{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc4e5e4-51f6-4e7c-9f18-131deaab6bce",
   "metadata": {},
   "source": [
    "For the notebook concerned with generating data, see `../../paper_comparison.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a20e-d9e3-486b-b1b9-e30a8047ada9",
   "metadata": {},
   "source": [
    "# 2. Implement recall metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94097292-19e1-4556-8f63-32b14ddbf227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b294c801-7925-4464-823a-447e8977acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/generated_tables_with_high_quality_papers/total_tables_b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f.json\") as f:\n",
    "    table_0_total = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04a9b256-d50e-438b-8cb4-3fc067835cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_paper', 'pap_to_tab', 'cc_to_tab', 'multi_scheme', 'ours_table_question', 'ours_table_presupposition', 'ours_question_list', 'ours_final_table'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0cad87f-a4ed-45c9-9d29-fad1c7df36ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What is the main focus of the study?': {'paper_1': ['Creation of a subjectively annotated video quality assessment (VQA) database with real-world video sequences for the development and evaluation of objective VQA methods.'],\n",
       "  'paper_2': ['Construction of a large-scale video quality assessment database and conducting a comprehensive study with crowdsourced subjective video quality scores to advance no-reference video quality prediction.'],\n",
       "  'paper_3': ['Introduction of a large scale User Generated Content (UGC) dataset for video compression research, along with discussion of challenges for UGC compression and evaluation of no-reference objective quality metrics.'],\n",
       "  'paper_4': [\"Development of a large subjective video quality dataset and corresponding no-reference perceptual video quality assessment models that target 'in-the-wild' user-generated content (UGC).\"],\n",
       "  'paper_5': ['Introduction of a new in-the-wild video quality assessment dataset called KonVid-150k, alongside new efficient video quality assessment approaches based on multi-level spatially pooled features.'],\n",
       "  'paper_6': ['Empirical evaluation of Convolutional Neural Networks (CNNs) on large-scale video classification using a dataset of 1 million YouTube videos, and exploration of methods to extend CNNs in the time domain.'],\n",
       "  'paper_7': ['Description of the Kinetics human action video dataset for human action classification, containing 400 classes with an analysis of dataset imbalance and classifier bias.']},\n",
       " 'What types of data are presented in the study?': {'paper_1': ['Subjectively annotated VQA database with 1,200 video sequences representing a wide variety of content and distortions.'],\n",
       "  'paper_2': ['Video quality assessment database containing 585 videos with a large number of subjective quality scores collected via crowdsourcing.'],\n",
       "  'paper_3': ['Large scale UGC dataset with 1500 20-second video clips covering various categories and features like HDR, plus evaluation with three no-reference metrics.'],\n",
       "  'paper_4': ['Extensive video quality dataset with 39,000 distorted videos, 117,000 video patches, and 5.5 million human perceptual quality annotations.'],\n",
       "  'paper_5': ['VQA dataset called KonVid-150k with over 153,841 videos and multiple quality ratings, and 1,596 videos with extensive subjective assessment.'],\n",
       "  'paper_6': ['New dataset of 1 million YouTube videos across 487 classes for the evaluation of CNNs on large-scale video classification.'],\n",
       "  'paper_7': ['Kinetics human action video dataset comprising over 400 human action classes, with at least 400 video clips for each action.']},\n",
       " 'What is the methodology used for quality assessment?': {'paper_1': ['Subjective mean opinion scores (MOS) are used for video quality rating.'],\n",
       "  'paper_2': ['Crowdsourced subjective ratings were used for the quality assessment of video.'],\n",
       "  'paper_3': ['Evaluation is based on no-reference objective quality metrics such as Noise, Banding, and SLEEQ.'],\n",
       "  'paper_4': ['Creation of a region-based no-reference VQA architecture and a space-time video quality mapping engine for quality prediction and localization.'],\n",
       "  'paper_5': ['Proposed new efficient VQA approaches relying on multi-level spatially pooled deep-features (MLSP) for no-reference quality assessment.'],\n",
       "  'paper_6': [\"Empirical evaluation of CNNs, comparison with feature-based baselines, and studying model's performance improvements on UCF-101 dataset.\"],\n",
       "  'paper_7': ['Baseline performance figures for neural network architectures, and analysis of dataset imbalance impact on classifier performance.']},\n",
       " 'What are the proposed contributions?': {'paper_1': ['A new large dataset of video sequences aimed at helping development and evaluation of general-purpose VQA methods.'],\n",
       "  'paper_2': ['A large-scale video quality assessment database and comprehensive study to aid in the improvement of NR video quality predictors.'],\n",
       "  'paper_3': ['Introduction of a UGC dataset for research, novel sampling method, and discussion of challenges and shortcomings for UGC compression and quality evaluation.'],\n",
       "  'paper_4': ['Largest subjective video quality dataset for ‘in-the-wild’ distorted UGC videos and innovative models for local-to-global video quality assessment.'],\n",
       "  'paper_5': ['Introduction of a new VQA dataset that is larger and more diverse compared to existing datasets, as well as efficient VQA models suitable for in-the-wild videos.'],\n",
       "  'paper_6': ['Extensive evaluation of CNNs on large-scale video classification and suggestions on approaches to extend the spatio-temporal connectivity of CNNs.'],\n",
       "  'paper_7': ['New human action video dataset intended for more accurate human action classification and analysis of dataset bias.']}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['pap_to_tab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ade6dfa5-9afe-46be-9f96-45c1975b4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the gold table:\n",
    "with open(\"../data/arxiv_tables_2308_high_quality/tables.jsonl\") as f:\n",
    "    gold_tables = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b22e9079-eb38-46fd-ac86-d21cb698ae23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tabid': 'b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f',\n",
       " 'table': {'Dataset': {'9136312': ['KoNViD-1k '],\n",
       "   '52285071': ['LIVE-VQC '],\n",
       "   '119309258': ['YouTube-UGC '],\n",
       "   '227210156': ['LSVQ '],\n",
       "   '234788066': ['KoNViD-150k '],\n",
       "   '206592218': ['Sports-1M '],\n",
       "   '27300853': ['Kinetics-400 ']},\n",
       "  'Task': {'9136312': ['VQA'],\n",
       "   '52285071': ['VQA'],\n",
       "   '119309258': ['VQA'],\n",
       "   '227210156': ['VQA'],\n",
       "   '234788066': ['VQA'],\n",
       "   '206592218': ['classification'],\n",
       "   '27300853': ['classification']},\n",
       "  'Size': {'9136312': ['1,200'],\n",
       "   '52285071': ['585'],\n",
       "   '119309258': ['1,380'],\n",
       "   '227210156': ['39,075'],\n",
       "   '234788066': ['153,841'],\n",
       "   '206592218': ['1,133,158'],\n",
       "   '27300853': ['306,245']},\n",
       "  'Annotations': {'9136312': ['114'],\n",
       "   '52285071': ['240'],\n",
       "   '119309258': ['123'],\n",
       "   '227210156': ['35'],\n",
       "   '234788066': ['5'],\n",
       "   '206592218': ['- (auto.)'],\n",
       "   '27300853': ['3-5']}},\n",
       " 'row_bib_map': [{'bib_hash_or_arxiv_id': '24b4d04b01098cffe3cb975171aa05132c6a0903',\n",
       "   'row': 0,\n",
       "   'corpus_id': 9136312,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '8f3eb869445ed0622a9879a6f2010828f6039e8b',\n",
       "   'row': 1,\n",
       "   'corpus_id': 52285071,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '9be8207b7ba6b4f423a574f686110ed04d5a3d91',\n",
       "   'row': 2,\n",
       "   'corpus_id': 119309258,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '3dfd446db36563cc1eeb028310478bbb775a0237',\n",
       "   'row': 3,\n",
       "   'corpus_id': 227210156,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '9c04ab9115e73e7300a6077937ec1b23e3fdf820',\n",
       "   'row': 4,\n",
       "   'corpus_id': 234788066,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': 'fbaab966ad1e0efdeb5b96e80b5b467ba60eeced',\n",
       "   'row': 5,\n",
       "   'corpus_id': 206592218,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': 'dd352e3c3918b783377312dd4399a284589a24d1',\n",
       "   'row': 6,\n",
       "   'corpus_id': 27300853,\n",
       "   'type': 'ref'}]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f073d0d-2cec-4c51-aa5f-b315368a5dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tabid': 'b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f',\n",
       " 'table': {'Dataset': {'9136312': ['KoNViD-1k '],\n",
       "   '52285071': ['LIVE-VQC '],\n",
       "   '119309258': ['YouTube-UGC '],\n",
       "   '227210156': ['LSVQ '],\n",
       "   '234788066': ['KoNViD-150k '],\n",
       "   '206592218': ['Sports-1M '],\n",
       "   '27300853': ['Kinetics-400 ']},\n",
       "  'Task': {'9136312': ['VQA'],\n",
       "   '52285071': ['VQA'],\n",
       "   '119309258': ['VQA'],\n",
       "   '227210156': ['VQA'],\n",
       "   '234788066': ['VQA'],\n",
       "   '206592218': ['classification'],\n",
       "   '27300853': ['classification']},\n",
       "  'Size': {'9136312': ['1,200'],\n",
       "   '52285071': ['585'],\n",
       "   '119309258': ['1,380'],\n",
       "   '227210156': ['39,075'],\n",
       "   '234788066': ['153,841'],\n",
       "   '206592218': ['1,133,158'],\n",
       "   '27300853': ['306,245']},\n",
       "  'Annotations': {'9136312': ['114'],\n",
       "   '52285071': ['240'],\n",
       "   '119309258': ['123'],\n",
       "   '227210156': ['35'],\n",
       "   '234788066': ['5'],\n",
       "   '206592218': ['- (auto.)'],\n",
       "   '27300853': ['3-5']}},\n",
       " 'row_bib_map': [{'bib_hash_or_arxiv_id': '24b4d04b01098cffe3cb975171aa05132c6a0903',\n",
       "   'row': 0,\n",
       "   'corpus_id': 9136312,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '8f3eb869445ed0622a9879a6f2010828f6039e8b',\n",
       "   'row': 1,\n",
       "   'corpus_id': 52285071,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '9be8207b7ba6b4f423a574f686110ed04d5a3d91',\n",
       "   'row': 2,\n",
       "   'corpus_id': 119309258,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '3dfd446db36563cc1eeb028310478bbb775a0237',\n",
       "   'row': 3,\n",
       "   'corpus_id': 227210156,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': '9c04ab9115e73e7300a6077937ec1b23e3fdf820',\n",
       "   'row': 4,\n",
       "   'corpus_id': 234788066,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': 'fbaab966ad1e0efdeb5b96e80b5b467ba60eeced',\n",
       "   'row': 5,\n",
       "   'corpus_id': 206592218,\n",
       "   'type': 'ref'},\n",
       "  {'bib_hash_or_arxiv_id': 'dd352e3c3918b783377312dd4399a284589a24d1',\n",
       "   'row': 6,\n",
       "   'corpus_id': 27300853,\n",
       "   'type': 'ref'}],\n",
       " 'context': {'caption': 'Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.',\n",
       "  'mention_paragraphs': ['DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. 1, public VQA datasets are significantly smaller in size when compared to video classification datasets (Karpathy et al., 2014; Kay et al., 2017). The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality (Chen et al., 2010; Hoßfeld et al., 2014; Shahid et al., 2014; Chen et al., 2015), which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset (Hosu et al., 2017) requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets (Ying et al., 2021; Götz-Hahn et al., 2021). However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset (Götz-Hahn et al., 2021) revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.'],\n",
       "  'glossary': {'Dataset': 'The dataset the paper uses to evaluate its task.',\n",
       "   'Task': 'The task the dataset was designed for.',\n",
       "   'VQA': 'An acronym that stands for video quality assessment.',\n",
       "   'classification': 'The video classification task.',\n",
       "   'Annotations': 'The number of annotations per video.',\n",
       "   'Size': 'The number of annotated videos in the dataset'}},\n",
       " 'table_decontext': {'The dataset the paper uses to evaluate its task.': ['The dataset the paper uses to evaluate its task: 9136312',\n",
       "   'The dataset the paper uses to evaluate its task: 52285071',\n",
       "   'The dataset the paper uses to evaluate its task: 119309258',\n",
       "   'The dataset the paper uses to evaluate its task: 227210156',\n",
       "   'The dataset the paper uses to evaluate its task: 234788066',\n",
       "   'The dataset the paper uses to evaluate its task: 206592218',\n",
       "   'The dataset the paper uses to evaluate its task: 27300853'],\n",
       "  'The task the dataset was designed for.': ['The task the dataset was designed for: 9136312',\n",
       "   'The task the dataset was designed for: 52285071',\n",
       "   'The task the dataset was designed for: 119309258',\n",
       "   'The task the dataset was designed for: 227210156',\n",
       "   'The task the dataset was designed for: 234788066',\n",
       "   'The task the dataset was designed for: 206592218',\n",
       "   'The task the dataset was designed for: 27300853'],\n",
       "  'The number of annotated videos in the dataset': ['The number of annotated videos in the dataset: 9136312',\n",
       "   'The number of annotated videos in the dataset: 52285071',\n",
       "   'The number of annotated videos in the dataset: 119309258',\n",
       "   'The number of annotated videos in the dataset: 227210156',\n",
       "   'The number of annotated videos in the dataset: 234788066',\n",
       "   'The number of annotated videos in the dataset: 206592218',\n",
       "   'The number of annotated videos in the dataset: 27300853'],\n",
       "  'The number of annotations per video.': ['The number of annotations per video: 9136312',\n",
       "   'The number of annotations per video: 52285071',\n",
       "   'The number of annotations per video: 119309258',\n",
       "   'The number of annotations per video: 227210156',\n",
       "   'The number of annotations per video: 234788066',\n",
       "   'The number of annotations per video: 206592218',\n",
       "   'The number of annotations per video: 27300853']}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f5bf1845-0f65-4f8c-8561-7e7d736b5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_table = gold_tables[0]\n",
    "table_decontext = {}\n",
    "for column_name in gold_table[\"table\"]:\n",
    "    col_context = gold_table[\"context\"][\"glossary\"].get(column_name)\n",
    "    if col_context is None:\n",
    "        table_decontext[column_name] = gold_table[\"table\"][column_name]\n",
    "        continue\n",
    "\n",
    "    col_name_and_context =f\"{column_name} ( {col_context} )\"\n",
    "    table_decontext[col_name_and_context] = {}\n",
    "    for corpus_id in gold_table[\"table\"][column_name]:\n",
    "        # value = \", \".join(gold_table[\"table\"][column_name][corpus_id])\n",
    "        value = gold_table[\"table\"][column_name][corpus_id][0]\n",
    "        value_context = gold_table[\"context\"][\"glossary\"].get(value)\n",
    "        if value_context is None:\n",
    "            table_decontext[col_name_and_context][corpus_id] = [f\"{col_context}: {value}\"]\n",
    "        else:\n",
    "            table_decontext[col_name_and_context][corpus_id] = [f\"{col_context}: {value} ( {value_context} )\"]\n",
    "\n",
    "gold_table[\"table_decontext\"] = table_decontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8bd1b2d4-1e41-4584-aa38-de99fcd1d7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dataset ( The dataset the paper uses to evaluate its task. )': {'9136312': ['The dataset the paper uses to evaluate its task.: KoNViD-1k '],\n",
       "  '52285071': ['The dataset the paper uses to evaluate its task.: LIVE-VQC '],\n",
       "  '119309258': ['The dataset the paper uses to evaluate its task.: YouTube-UGC '],\n",
       "  '227210156': ['The dataset the paper uses to evaluate its task.: LSVQ '],\n",
       "  '234788066': ['The dataset the paper uses to evaluate its task.: KoNViD-150k '],\n",
       "  '206592218': ['The dataset the paper uses to evaluate its task.: Sports-1M '],\n",
       "  '27300853': ['The dataset the paper uses to evaluate its task.: Kinetics-400 ']},\n",
       " 'Task ( The task the dataset was designed for. )': {'9136312': ['The task the dataset was designed for.: VQA ( An acronym that stands for video quality assessment. )'],\n",
       "  '52285071': ['The task the dataset was designed for.: VQA ( An acronym that stands for video quality assessment. )'],\n",
       "  '119309258': ['The task the dataset was designed for.: VQA ( An acronym that stands for video quality assessment. )'],\n",
       "  '227210156': ['The task the dataset was designed for.: VQA ( An acronym that stands for video quality assessment. )'],\n",
       "  '234788066': ['The task the dataset was designed for.: VQA ( An acronym that stands for video quality assessment. )'],\n",
       "  '206592218': ['The task the dataset was designed for.: classification ( The video classification task. )'],\n",
       "  '27300853': ['The task the dataset was designed for.: classification ( The video classification task. )']},\n",
       " 'Size ( The number of annotated videos in the dataset )': {'9136312': ['The number of annotated videos in the dataset: 1,200'],\n",
       "  '52285071': ['The number of annotated videos in the dataset: 585'],\n",
       "  '119309258': ['The number of annotated videos in the dataset: 1,380'],\n",
       "  '227210156': ['The number of annotated videos in the dataset: 39,075'],\n",
       "  '234788066': ['The number of annotated videos in the dataset: 153,841'],\n",
       "  '206592218': ['The number of annotated videos in the dataset: 1,133,158'],\n",
       "  '27300853': ['The number of annotated videos in the dataset: 306,245']},\n",
       " 'Annotations ( The number of annotations per video. )': {'9136312': ['The number of annotations per video.: 114'],\n",
       "  '52285071': ['The number of annotations per video.: 240'],\n",
       "  '119309258': ['The number of annotations per video.: 123'],\n",
       "  '227210156': ['The number of annotations per video.: 35'],\n",
       "  '234788066': ['The number of annotations per video.: 5'],\n",
       "  '206592218': ['The number of annotations per video.: - (auto.)'],\n",
       "  '27300853': ['The number of annotations per video.: 3-5']}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_table[\"table_decontext\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e12165-8f34-42c5-935e-c0e4badfe60a",
   "metadata": {},
   "source": [
    "In order to evaluate different metrics quickly, I need to have some gold alignments and compare them quickly to the predicted alignments.\n",
    "I'm concerned about the n-gram metrics not being good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85c63b97-5b7a-4b5d-9c11-68e3f7260406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['What is the main focus of the study?', 'What types of data are presented in the study?', 'What is the methodology used for quality assessment?', 'What are the proposed contributions?'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['pap_to_tab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2274b5a8-2efc-41e1-8737-8030da743b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset_size', 'dataset_focus', 'content_diversity', 'data_collection_methodology', 'UGC_focus', 'reference_to_real_world_conditions'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['cc_to_tab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c4b6183-b66e-4d7e-9625-20d8dfb0b34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_1': ['1,200 video sequences'],\n",
       " 'paper_2': ['585 videos'],\n",
       " 'paper_3': ['1,500 20-sec video clips'],\n",
       " 'paper_4': ['39,000 videos and 117,000 v-patches'],\n",
       " 'paper_5': ['153,841 videos, 1,596 densely annotated'],\n",
       " 'paper_6': ['1 million YouTube videos'],\n",
       " 'paper_7': ['At least 400 clips for each of the 400 actions']}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['cc_to_tab']['dataset_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1477a538-83d7-4624-945c-7ecb594d48e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_1': ['Creation of a subjectively annotated video quality assessment (VQA) database with real-world video sequences for the development and evaluation of objective VQA methods.'],\n",
       " 'paper_2': ['Construction of a large-scale video quality assessment database and conducting a comprehensive study with crowdsourced subjective video quality scores to advance no-reference video quality prediction.'],\n",
       " 'paper_3': ['Introduction of a large scale User Generated Content (UGC) dataset for video compression research, along with discussion of challenges for UGC compression and evaluation of no-reference objective quality metrics.'],\n",
       " 'paper_4': [\"Development of a large subjective video quality dataset and corresponding no-reference perceptual video quality assessment models that target 'in-the-wild' user-generated content (UGC).\"],\n",
       " 'paper_5': ['Introduction of a new in-the-wild video quality assessment dataset called KonVid-150k, alongside new efficient video quality assessment approaches based on multi-level spatially pooled features.'],\n",
       " 'paper_6': ['Empirical evaluation of Convolutional Neural Networks (CNNs) on large-scale video classification using a dataset of 1 million YouTube videos, and exploration of methods to extend CNNs in the time domain.'],\n",
       " 'paper_7': ['Description of the Kinetics human action video dataset for human action classification, containing 400 classes with an analysis of dataset imbalance and classifier bias.']}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['pap_to_tab']['What is the main focus of the study?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762c6a1-07ab-49ca-9a5d-d08fbe009b4b",
   "metadata": {},
   "source": [
    "`pap_to_tab`:\n",
    "- Dataset - 'What types of data are presented in the study'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30d15898-009f-4bf2-bdcf-5293f347b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da1eaf8a-c9e3-4556-a75c-e465bca95517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e11c3598984af58be3e89202c90df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58fe27b78b64028bdbb11f30dc550e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e587251da842349c5a0a989d94b7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad8968dd0714330ad3e79a4803ea840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880f19e474184118a4245b52496f9852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f776f160053408db308efe1b0c30ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cbd8339dbb4cf6b841574e56ad46ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdc4d8ada3847968538c9ab86de03e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264c6afbc1f44ef5b39dd5bff8a484e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f63011f9bd42f0b19990230c6509af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727fb4ef17eb4a89a3b61d82bcf26594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7017edf553d408f847bfca8e3dd2627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e847b61c814fd9b85e9e20383783a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9671746204654e568df85030c924217f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d07ff238-013c-46d0-af44-8fde2337896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_gold = model.encode(\n",
    "    [\" \".join(gold_table[\"table_decontext\"][c][paper_id]) for c in gold_table[\"table_decontext\"] for paper_id in gold_table[\"table_decontext\"][c]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "26ac81b4-416d-49f8-9e11-bfe5b8554141",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pred_pap_to_tab = model.encode(\n",
    "    [\" \".join(table_0_total[\"pap_to_tab\"][c][paper_id]) for c in table_0_total[\"pap_to_tab\"] for paper_id in table_0_total[\"pap_to_tab\"][c]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9e6974cc-be49-4176-bf93-fa7fd070482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pred_cc_to_tab = model.encode(\n",
    "    [\" \".join(table_0_total[\"cc_to_tab\"][c][paper_id]) for c in table_0_total[\"cc_to_tab\"] for paper_id in table_0_total[\"cc_to_tab\"][c]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "198773c2-6261-4042-ac40-ae75ee697fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_0_total[\"pap_to_tab\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b6bbee8b-8454-4a7f-89e4-e8224cf997a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0743842d-46fb-49a3-ba44-d1a6ed32f732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 384)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_gold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "189a00cf-bb86-4f9e-b238-52a4fe21c7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 384)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pred_pap_to_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "535728ad-d8e5-4ef4-abb6-1f81e1c69d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4f36cfa8-9a4b-4459-a853-4d58ad9bd2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999999 , 1.        , 1.        , 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 0.99999994, 1.0000001 ,\n",
       "       0.99999994, 1.        , 1.        , 0.99999994, 1.        ,\n",
       "       0.99999994, 1.        , 0.99999994, 0.99999994, 0.99999994,\n",
       "       1.        , 1.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(embeddings_gold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "803b402b-b295-420e-9268-17dae440beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_mat = np.matmul(embeddings_gold, embeddings_pred_pap_to_tab.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "838c7fc5-2660-4b20-928c-c893902babed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat_cosine = util.cos_sim(embeddings_gold, embeddings_pred_pap_to_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c63ab08f-7ce5-4092-8cef-5d7426dc7f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(sim_mat_cosine, sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "52f744c1-f93d-4898-bb66-466d7d528d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35967594, 0.51227462, 0.60144138, 0.5360809 ])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, iterate through the num_papers x num_papers blocks to aggregate similarities among papers\n",
    "sim_mat_cosine = util.cos_sim(embeddings_gold, embeddings_pred_pap_to_tab)\n",
    "num_papers = len(gold_table['row_bib_map'])\n",
    "column_scores = np.zeros((\n",
    "    len(gold_table[\"table_decontext\"]), len(table_0_total[\"pap_to_tab\"])\n",
    "))\n",
    "\n",
    "for i in range(0, len(sim_mat_cosine), num_papers):\n",
    "    for j in range(0, len(sim_mat_cosine[i]), num_papers):\n",
    "        column_scores[i//num_papers][j // num_papers] = np.mean(\n",
    "            sim_mat_cosine[i:i + num_papers, j:j + num_papers].numpy()\n",
    "        )\n",
    "        \n",
    "np.max(column_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "61db0f4c-f566-4d6f-aaa6-a20946e88c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat_cosine = util.cos_sim(embeddings_gold, embeddings_pred_cc_to_tab)\n",
    "\n",
    "# next, iterate through the num_papers x num_papers blocks to aggregate similarities among papers\n",
    "num_papers = len(gold_table['row_bib_map'])\n",
    "column_scores = np.zeros((\n",
    "    len(gold_table[\"table_decontext\"]), len(table_0_total[\"cc_to_tab\"])\n",
    "))\n",
    "\n",
    "for i in range(0, len(sim_mat_cosine), num_papers):\n",
    "    for j in range(0, len(sim_mat_cosine[i]), num_papers):\n",
    "        column_scores[i//num_papers][j // num_papers] = np.mean(\n",
    "            sim_mat_cosine[i:i + num_papers, j:j + num_papers].numpy()\n",
    "        )\n",
    "\n",
    "# column_scores[0] = the similarity scores for the first column of the gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "80fbc609-fd5f-4fe8-96da-b696e3785f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30438793, 0.43206748, 0.56763244, 0.53662711])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.4\n",
    "np.max(column_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e5c01879-c460-4f7b-b593-52e88ba3b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment(gold_table, pred_table, column_scores, threshold):\n",
    "    alignment = {}\n",
    "    for gold_col_i, gold_col_name in enumerate(gold_table):\n",
    "        alignment[(gold_col_name, gold_col_i)] = []\n",
    "        for pred_col_i, pred_col_name in enumerate(pred_table):\n",
    "            if column_scores[gold_col_i, pred_col_i] > threshold:\n",
    "                alignment[(gold_col_name, gold_col_i)].append((pred_col_name, pred_col_i))\n",
    "\n",
    "    return alignment, column_scores\n",
    "\n",
    "def get_alignment_values(gold_table, pred_table, threshold):\n",
    "    embeddings_gold = model.encode(\n",
    "        [\" \".join(gold_table[c][paper_id]) for c in gold_table for paper_id in gold_table[c]]\n",
    "    )\n",
    "    embeddings_pred = model.encode(\n",
    "        [\" \".join(pred_table[c][paper_id]) for c in pred_table for paper_id in pred_table[c]]\n",
    "    )\n",
    "    \n",
    "    sim_mat_cosine = util.cos_sim(embeddings_gold, embeddings_pred)\n",
    "\n",
    "    # next, iterate through the num_papers x num_papers blocks to aggregate similarities among papers\n",
    "    num_papers = len(list(gold_table.values())[0])\n",
    "    column_scores = np.zeros((\n",
    "        len(gold_table), len(pred_table)\n",
    "    ))\n",
    "    \n",
    "    for i in range(0, len(sim_mat_cosine), num_papers):\n",
    "        for j in range(0, len(sim_mat_cosine[i]), num_papers):\n",
    "            column_scores[i//num_papers][j // num_papers] = np.mean(\n",
    "                sim_mat_cosine[i:i + num_papers, j:j + num_papers].numpy()\n",
    "            )\n",
    "\n",
    "    # calculate the alignment\n",
    "    return get_alignment(gold_table, pred_table, column_scores, threshold)\n",
    "\n",
    "def get_alignment_columns(gold_table, pred_table, threshold):\n",
    "    embeddings_gold = model.encode(\n",
    "        list(gold_table.keys())\n",
    "    )\n",
    "    embeddings_pred = model.encode(\n",
    "        list(pred_table.keys())\n",
    "    )\n",
    "    \n",
    "    sim_mat_cosine = util.cos_sim(embeddings_gold, embeddings_pred)\n",
    "\n",
    "    # calculate the alignment\n",
    "    return get_alignment(gold_table, pred_table, sim_mat_cosine, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1ac0e1d7-8946-4f79-929f-a6728a6820f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_cc_to_tab, column_scores_cc_to_tab = get_alignment_values(gold_table[\"table_decontext\"], table_0_total[\"cc_to_tab\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "629509c0-3fba-4550-9bb6-7a43de993faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Dataset ( The dataset the paper uses to evaluate its task. )', 0): [],\n",
       " ('Task ( The task the dataset was designed for. )', 1): [],\n",
       " ('Size ( The number of annotated videos in the dataset )',\n",
       "  2): [('dataset_size', 0)],\n",
       " ('Annotations ( The number of annotations per video. )',\n",
       "  3): [('dataset_size', 0)]}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment_cc_to_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "56882c3f-2f65-4fc3-b6aa-b17d7521509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_cc_to_tab_cols, column_scores_cc_to_tab_cols = get_alignment_columns(gold_table[\"table_decontext\"], table_0_total[\"cc_to_tab\"], 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "66eef957-212e-49d9-a1ad-f6c1d87453c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Dataset ( The dataset the paper uses to evaluate its task. )', 0): [],\n",
       " ('Task ( The task the dataset was designed for. )', 1): [],\n",
       " ('Size ( The number of annotated videos in the dataset )',\n",
       "  2): [('dataset_size', 0)],\n",
       " ('Annotations ( The number of annotations per video. )', 3): []}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment_cc_to_tab_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1b6132ff-e494-4076-9c28-b9cab74544b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4252,  0.4232,  0.2144,  0.3453,  0.1487,  0.1917],\n",
       "        [ 0.2810,  0.3959,  0.1755,  0.3202,  0.1592,  0.1647],\n",
       "        [ 0.4677,  0.2154,  0.2187,  0.1279,  0.0396, -0.0273],\n",
       "        [ 0.1312,  0.1218,  0.2663,  0.0354,  0.1214,  0.0252]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_scores_cc_to_tab_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3057274b-8705-4f07-bf99-91897739b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_pap_to_tab, column_scores_pap_to_tab = get_alignment_values(gold_table[\"table_decontext\"], table_0_total[\"pap_to_tab\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6c9edb17-7e17-4b50-bed4-595923afeca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Dataset ( The dataset the paper uses to evaluate its task. )', 0): [],\n",
       " ('Task ( The task the dataset was designed for. )',\n",
       "  1): [('What types of data are presented in the study?', 1)],\n",
       " ('Size ( The number of annotated videos in the dataset )',\n",
       "  2): [('What types of data are presented in the study?', 1)],\n",
       " ('Annotations ( The number of annotations per video. )',\n",
       "  3): [('What types of data are presented in the study?', 1)]}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment_pap_to_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "85b6d8b0-615d-4ccb-96a7-5eb7433c8867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29249093, 0.35967594, 0.27512306, 0.3139298 ],\n",
       "       [0.48544809, 0.51227462, 0.38085705, 0.49473956],\n",
       "       [0.48843396, 0.60144138, 0.2822994 , 0.44557634],\n",
       "       [0.43911833, 0.5360809 , 0.27533525, 0.38838932]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_scores_pap_to_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0f48ffd3-75f1-4f11-aa06-205c2c26bf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_1': ['Subjectively annotated VQA database with 1,200 video sequences representing a wide variety of content and distortions.'],\n",
       " 'paper_2': ['Video quality assessment database containing 585 videos with a large number of subjective quality scores collected via crowdsourcing.'],\n",
       " 'paper_3': ['Large scale UGC dataset with 1500 20-second video clips covering various categories and features like HDR, plus evaluation with three no-reference metrics.'],\n",
       " 'paper_4': ['Extensive video quality dataset with 39,000 distorted videos, 117,000 video patches, and 5.5 million human perceptual quality annotations.'],\n",
       " 'paper_5': ['VQA dataset called KonVid-150k with over 153,841 videos and multiple quality ratings, and 1,596 videos with extensive subjective assessment.'],\n",
       " 'paper_6': ['New dataset of 1 million YouTube videos across 487 classes for the evaluation of CNNs on large-scale video classification.'],\n",
       " 'paper_7': ['Kinetics human action video dataset comprising over 400 human action classes, with at least 400 video clips for each action.']}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total[\"pap_to_tab\"]['What types of data are presented in the study?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0a716-df33-483f-8010-29e328b4d6bf",
   "metadata": {},
   "source": [
    "There are other things we could try, but we need to find the context for all of the tables. This requires continuing what I was doing last week:\n",
    "- I need to extract the full texts\n",
    "- Would be nice to do get some gold alignments too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d4c90a04-d455-4141-bd12-61c7d7dd47c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(gold_table[\"table_decontext\"].values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c1a2278-b2b9-407b-9c7f-2ccbb49afdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2a5a1c9d0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sim_mat_cosine)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d011fc62-22f5-45fc-a09a-d43b0d644bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2885b1390>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sim_mat)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38fdbd7b-c220-49d3-b73b-78b82dd077c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Creation of a subjectively annotated video quality assessment (VQA) database with real-world video sequences for the development and evaluation of objective VQA methods.',\n",
       " 'Construction of a large-scale video quality assessment database and conducting a comprehensive study with crowdsourced subjective video quality scores to advance no-reference video quality prediction.',\n",
       " 'Introduction of a large scale User Generated Content (UGC) dataset for video compression research, along with discussion of challenges for UGC compression and evaluation of no-reference objective quality metrics.',\n",
       " \"Development of a large subjective video quality dataset and corresponding no-reference perceptual video quality assessment models that target 'in-the-wild' user-generated content (UGC).\",\n",
       " 'Introduction of a new in-the-wild video quality assessment dataset called KonVid-150k, alongside new efficient video quality assessment approaches based on multi-level spatially pooled features.',\n",
       " 'Empirical evaluation of Convolutional Neural Networks (CNNs) on large-scale video classification using a dataset of 1 million YouTube videos, and exploration of methods to extend CNNs in the time domain.',\n",
       " 'Description of the Kinetics human action video dataset for human action classification, containing 400 classes with an analysis of dataset imbalance and classifier bias.',\n",
       " 'Subjectively annotated VQA database with 1,200 video sequences representing a wide variety of content and distortions.',\n",
       " 'Video quality assessment database containing 585 videos with a large number of subjective quality scores collected via crowdsourcing.',\n",
       " 'Large scale UGC dataset with 1500 20-second video clips covering various categories and features like HDR, plus evaluation with three no-reference metrics.',\n",
       " 'Extensive video quality dataset with 39,000 distorted videos, 117,000 video patches, and 5.5 million human perceptual quality annotations.',\n",
       " 'VQA dataset called KonVid-150k with over 153,841 videos and multiple quality ratings, and 1,596 videos with extensive subjective assessment.',\n",
       " 'New dataset of 1 million YouTube videos across 487 classes for the evaluation of CNNs on large-scale video classification.',\n",
       " 'Kinetics human action video dataset comprising over 400 human action classes, with at least 400 video clips for each action.',\n",
       " 'Subjective mean opinion scores (MOS) are used for video quality rating.',\n",
       " 'Crowdsourced subjective ratings were used for the quality assessment of video.',\n",
       " 'Evaluation is based on no-reference objective quality metrics such as Noise, Banding, and SLEEQ.',\n",
       " 'Creation of a region-based no-reference VQA architecture and a space-time video quality mapping engine for quality prediction and localization.',\n",
       " 'Proposed new efficient VQA approaches relying on multi-level spatially pooled deep-features (MLSP) for no-reference quality assessment.',\n",
       " \"Empirical evaluation of CNNs, comparison with feature-based baselines, and studying model's performance improvements on UCF-101 dataset.\",\n",
       " 'Baseline performance figures for neural network architectures, and analysis of dataset imbalance impact on classifier performance.',\n",
       " 'A new large dataset of video sequences aimed at helping development and evaluation of general-purpose VQA methods.',\n",
       " 'A large-scale video quality assessment database and comprehensive study to aid in the improvement of NR video quality predictors.',\n",
       " 'Introduction of a UGC dataset for research, novel sampling method, and discussion of challenges and shortcomings for UGC compression and quality evaluation.',\n",
       " 'Largest subjective video quality dataset for ‘in-the-wild’ distorted UGC videos and innovative models for local-to-global video quality assessment.',\n",
       " 'Introduction of a new VQA dataset that is larger and more diverse compared to existing datasets, as well as efficient VQA models suitable for in-the-wild videos.',\n",
       " 'Extensive evaluation of CNNs on large-scale video classification and suggestions on approaches to extend the spatio-temporal connectivity of CNNs.',\n",
       " 'New human action video dataset intended for more accurate human action classification and analysis of dataset bias.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(table_0_total[\"pap_to_tab\"][c][paper_id]) for c in table_0_total[\"pap_to_tab\"] for paper_id in table_0_total[\"pap_to_tab\"][c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1bb88e28-41b5-4d71-9f2e-5cafa24434e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tables[0][\"context\"] = {\n",
    "    \"caption\": \"Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\",\n",
    "    \"mention_paragraphs\": [\"DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. 1, public VQA datasets are significantly smaller in size when compared to video classification datasets (Karpathy et al., 2014; Kay et al., 2017). The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality (Chen et al., 2010; Hoßfeld et al., 2014; Shahid et al., 2014; Chen et al., 2015), which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset (Hosu et al., 2017) requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets (Ying et al., 2021; Götz-Hahn et al., 2021). However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset (Götz-Hahn et al., 2021) revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\"],\n",
    "    \"glossary\": {\n",
    "        \"Dataset\": \"The dataset the paper uses to evaluate its task.\",\n",
    "        \"Task\": \"The task the dataset was designed for.\",\n",
    "        \"VQA\": \"An acronym that stands for video quality assessment.\",\n",
    "        \"classification\": \"The video classification task.\",\n",
    "        \"Annotations\": \"The number of annotations per video.\",\n",
    "        \"Size\": \"The number of annotated videos in the dataset\",\n",
    "    }\n",
    "}\n",
    "\n",
    "gold_tables[1][\"context\"] = {\n",
    "    \"caption\": \"Physical adversarial attacks against Semantic Segmentation tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.\",\n",
    "    \"mention_paragraphs\": [\"Table XII presents a comprehensive comparison of various adversarial attack methods in the semantic segmentation task. It provides an overview of their attack goals, patch placement strategies, consideration of changing viewpoints, testing in the physical domain, and transferability to other models. Table XIII offers detailed information on adversarial attacks, including the attacker’s knowledge level, robustness techniques, stealthiness techniques, physical test types, and space of operation. Table XIV provides information on the datasets used, the evaluated networks, and the links to open-source code for the experiments conducted in the semantic segmentation task.\"],\n",
    "    \"glossary\": {\n",
    "        \"IPatch\": \"An adversarial attack that targets a particular region in an image by inserting a adversarially optimized patch.\",\n",
    "        # \"SSAttack\": \"\",\n",
    "        \"Attacker's Knowledge\": \"The knowledge required by the attacker. Can be one of White-box, Black-box or Grey-box\",\n",
    "        \"White-box\": \"refer to adversarial attacks where the attacker possesses complete knowledge of the training and testing data used to train the victim model, as well as the architecture and parameters of the target model.\",\n",
    "        \"Robustness Technique\": \"Technique for building systems that are robust to this attack\",\n",
    "        \"EOT\": \"Expectation of Transformation. for adversarial attacks, which takes potential transformations in the real world into account during the optimization, resulting in better robustness. EOT adds random distortions in the optimization to make the perturbation more robust.\",\n",
    "        \"Stealthiness Technique\": \"Techniques for evading detection that the attack was used.\",\n",
    "        \"Static\": \"Attacks that do not change once inserted into the scene.\",\n",
    "        \"Space\": \"Whether the attack takes place in two dimensions (eg like a sticker) or three (eg an object)\",\n",
    "        \"2D\": \"The attack takes place in two dimensions\", \n",
    "    }\n",
    "}\n",
    "\n",
    "gold_tables[2][\"context\"] = {\n",
    "    \"caption\": \"The comparison of existing data augmentation methods powered by LLMs for training retrieval models.\",\n",
    "    \"mention_paragraphs\": [\"Additionally, to highlight the similarities and differences among the corresponding methods, we present a comparative result in Table III. It compares the aforementioned methods from various perspectives, including the number of examples, the generator employed, the type of synthetic data produced, the method applied to filter synthetic data, and whether LLMs are fine-tuned. This table serves to facilitate a clearer understanding of the landscape of these methods.\"],\n",
    "    \"glossary\": {\n",
    "        \"Methods\": \"The name of the data augmentation method used by the paper\",\n",
    "        \"# Examples\": {\"text\": \"The number of in-context examples used in the prompt the paper's method proposes.\", \"notes\": \"guess\"},\n",
    "        \"Generator\": \"The generation model used for data augmentation\",\n",
    "        \"Synthetic data\": \"The type of synthetic data produced by the method in the paper.\",\n",
    "        \"Relevant query\": \"The data augmentation mehtod produces synthetic queries that are relevant to a set of documents.\",\n",
    "        \"Soft relevance labels\": {\"text\": \"In some downstream tasks of retrieval, such as question-answering, the collection of questions is also sufficient. However, the relevance labels connecting these questions with the passages of supporting evidence are very limited. In this context, leveraging the capability of LLMs for relevance label generation is a promising approach that can augment the training corpus for retrievers. LLMs produce the generation probabilities of the question conditioned on these top passages. After a normalization process, these probabilities serve as soft relevance labels for the training of the retriever.\", \"notes\": \"quote\"},\n",
    "        \"Filter Method\": \"How the synthetic data produced by the method in the paper is filtered to produce high quality data.\",\n",
    "        \"Generation probability\": {\"text\": \"The probability of the synthetic query given the documents.\", \"notes\": \"guess\"},\n",
    "        \"Round-trip filtering\": \"First a retriever is fine-tuned on generated samples and then it is used to filter the samples.\",\n",
    "        \"LLMs' tuning\": \"Whether the LLMs are finetuned, and what method is used if they are.\",\n",
    "        \"Fixed\": {\"text\": \"the LLMs are not fintetuned\", \"notes\": \"guess\"},\n",
    "        \"Soft Prompt tuning\": \"only the prompts' embedding layer is optimized during the training process.\",\n",
    "    }\n",
    "}\n",
    "\n",
    "gold_tables[3][\"context\"] = {\n",
    "    \"caption\": \"The comparison of retrievers that leverage LLMs as the foundation.\",\n",
    "    \"mention_paragraphs\": [\"To provide a comprehensive understanding of this topic, Table 4 summarizes the common and unique characteristics of the LLM-based retrievers discussed above.\"],\n",
    "    \"glossary\": {\n",
    "        \"Methods\": \"The method introduced by the paper\",\n",
    "        \"Backbone\": {\"text\": \"The model that the LLM-based retriever is built off of\", \"notes\": \"guess\"},\n",
    "        \"Architecture\": \"The architecture of the model used for the LLM-based retriever.\",\n",
    "        \"Encoder-based\": \"LLMs, like the T5-family of models, are used to embed passages which are then placed in an index.\",\n",
    "        \"Generative\": \"LLMs generate document identifiers or URLs directly rather than using the 'index-retrieval-rank' paradigm.\",\n",
    "        \"LLM's tuning\": \"How LLM's need to be tuned for the paper's method.\",\n",
    "        \"Training from scratch\": {\"text\": \"The model has to be trained from scratch\", \"notes\": \"guess\"},\n",
    "        \"Fine-tuning\": {\"text\": \"The model has to be fine-tuned\", \"notes\": \"guess\"},\n",
    "        \"Fine-tuning & Prompting\": {\"text\": \"The model has to be fine-tuned and prompted in a specific way to perform retrieval\", \"notes\": \"guess\"},\n",
    "        \"Prompting\": {\"text\": \"The model does not have to be fine-tuned and can instead just be prompted\", \"notes\": \"guess\"},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06807757-27dc-42d1-9ee3-c6dd61cc5453",
   "metadata": {},
   "source": [
    "# 1. Analyzing High Quality Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0770aa68-73b2-4e96-ba06-8449c7fc054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/generated_tables_with_high_quality_papers/total_tables_b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f.json\") as f:\n",
    "    table_0_total = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc89288-632f-4f06-9d18-5ee629203305",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/generated_tables_with_high_quality_papers/baseline_table_b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f.json\") as f:\n",
    "    table_0_baseline = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2891c607-86bc-4ef5-a03c-d8d28bebbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/generated_tables_with_high_quality_papers/ours_table_b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f.json\") as f:\n",
    "    table_0_ours = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a40b7d6c-62b6-49ff-a5d6-b6f94381f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold table\n",
    "with open(\"../data/arxiv_tables_2308_high_quality/tables.jsonl\") as f:\n",
    "    tables_gold = [json.loads(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfb6f5ad-f43f-48bb-acd2-60bbb3eb933e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Dataset', 'Task', 'Size', 'Annotations'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_gold[0]['table'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62b9e10d-154d-49bd-95b8-0841b7075bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Task</th>\n",
       "      <th>Size</th>\n",
       "      <th>Annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9136312</th>\n",
       "      <td>[KoNViD-1k ]</td>\n",
       "      <td>[VQA]</td>\n",
       "      <td>[1,200]</td>\n",
       "      <td>[114]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52285071</th>\n",
       "      <td>[LIVE-VQC ]</td>\n",
       "      <td>[VQA]</td>\n",
       "      <td>[585]</td>\n",
       "      <td>[240]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119309258</th>\n",
       "      <td>[YouTube-UGC ]</td>\n",
       "      <td>[VQA]</td>\n",
       "      <td>[1,380]</td>\n",
       "      <td>[123]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227210156</th>\n",
       "      <td>[LSVQ ]</td>\n",
       "      <td>[VQA]</td>\n",
       "      <td>[39,075]</td>\n",
       "      <td>[35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234788066</th>\n",
       "      <td>[KoNViD-150k ]</td>\n",
       "      <td>[VQA]</td>\n",
       "      <td>[153,841]</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206592218</th>\n",
       "      <td>[Sports-1M ]</td>\n",
       "      <td>[classification]</td>\n",
       "      <td>[1,133,158]</td>\n",
       "      <td>[- (auto.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27300853</th>\n",
       "      <td>[Kinetics-400 ]</td>\n",
       "      <td>[classification]</td>\n",
       "      <td>[306,245]</td>\n",
       "      <td>[3-5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Dataset              Task         Size  Annotations\n",
       "9136312       [KoNViD-1k ]             [VQA]      [1,200]        [114]\n",
       "52285071       [LIVE-VQC ]             [VQA]        [585]        [240]\n",
       "119309258   [YouTube-UGC ]             [VQA]      [1,380]        [123]\n",
       "227210156          [LSVQ ]             [VQA]     [39,075]         [35]\n",
       "234788066   [KoNViD-150k ]             [VQA]    [153,841]          [5]\n",
       "206592218     [Sports-1M ]  [classification]  [1,133,158]  [- (auto.)]\n",
       "27300853   [Kinetics-400 ]  [classification]    [306,245]        [3-5]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tables_gold[0]['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b24dcf3-d48b-4e57-849b-b6e340e785c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_0</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>question_3</th>\n",
       "      <th>question_4</th>\n",
       "      <th>question_5</th>\n",
       "      <th>question_6</th>\n",
       "      <th>question_7</th>\n",
       "      <th>question_8</th>\n",
       "      <th>question_9</th>\n",
       "      <th>question_10</th>\n",
       "      <th>question_11</th>\n",
       "      <th>question_12</th>\n",
       "      <th>question_13</th>\n",
       "      <th>question_14</th>\n",
       "      <th>question_15</th>\n",
       "      <th>question_16</th>\n",
       "      <th>question_17</th>\n",
       "      <th>question_18</th>\n",
       "      <th>question_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_0</th>\n",
       "      <td>The paper addresses the lack of large VQA databases with real-world video sequences and subjective MOS.</td>\n",
       "      <td></td>\n",
       "      <td>VQA is video quality assessment, and a large, diverse database aids in developing and evaluating objective VQA methods.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The issue is the lack of large, diverse datasets with real-world distortions for video quality assessment.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>The paper addresses the problem of no-reference video quality models' poor performance on diverse real-world video data.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Performance measured by comparing no-reference video quality predictors on a new large-scale video quality database with crowdsourced scores.</td>\n",
       "      <td></td>\n",
       "      <td>The issue is current no-reference video quality models' inability to handle diverse video impairments.</td>\n",
       "      <td>No-reference video quality models struggle with diverse, authentic, commingled distortions in real-world videos.</td>\n",
       "      <td></td>\n",
       "      <td>The paper proposes a large-scale video quality database to enhance no-reference video quality models.</td>\n",
       "      <td></td>\n",
       "      <td>The paper addresses the inadequacy of existing databases to represent real-world video diversity and authentic distortions.</td>\n",
       "      <td>The focus is on improving no-reference video quality prediction models for diverse real-world conditions.</td>\n",
       "      <td>The paper addresses shortcomings in current no-reference video quality models due to limited databases.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>The paper tackles the shortcomings of traditional metrics for UGC video compression and quality assessment.</td>\n",
       "      <td>The approach is the introduction of a large scale UGC dataset to improve video compression research.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper focuses on the issue of assessing and improving compression and quality metrics for UGC videos.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>No-reference video quality models struggle with User Generated Content (UGC) that is non-pristine.</td>\n",
       "      <td>The paper considers BD-Rate and PSNR inadequate for UGC video compression and quality assessment.</td>\n",
       "      <td>The paper proposes using no-reference objective quality metrics to assess UGC video quality for compression.</td>\n",
       "      <td>The paper introduces no-reference objective quality metrics for evaluating UGC video compression and quality.</td>\n",
       "      <td>The paper addresses shortcomings of traditional metrics like BD-Rate and PSNR on non-pristine originals in UGC.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td></td>\n",
       "      <td>The approach is a ‘local-to-global’ region-based NR VQA architecture and spatio-temporal quality mapping engine.</td>\n",
       "      <td>VQA is Video Quality Assessment; large real-world database with MOS needed for accurate \"in-the-wild\" content quality prediction.</td>\n",
       "      <td></td>\n",
       "      <td>The paper used subjective annotations to create a large video quality dataset and developed NR-VQA models.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Performance is measured by a subjective video quality dataset with human perceptual quality annotations.</td>\n",
       "      <td></td>\n",
       "      <td>The paper discusses the limited prediction capabilities of current no-reference models on real-world user-generated content video quality.</td>\n",
       "      <td>No-reference video quality models struggle with real-world user-generated content (UGC) video data.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper introduces a local-to-global region-based NR VQA architecture and a space-time video quality mapping engine.</td>\n",
       "      <td></td>\n",
       "      <td>The paper focuses on improving no-reference perceptual video quality assessment for user-generated content.</td>\n",
       "      <td>The paper addresses the shortcomings in no-reference perceptual video quality assessment for user-generated content.</td>\n",
       "      <td>The main problem addressed is the inefficient no-reference perceptual video quality assessment for user-generated content.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>The paper addresses the lack of large, diverse datasets for no-reference video quality assessment in-the-wild.</td>\n",
       "      <td>The approach proposed is a new dataset, KonVid-150k, and efficient VQA methods using multi-level spatially pooled deep-features (MLSP).</td>\n",
       "      <td>VQA is Video Quality Assessment, and a large, diverse dataset with subjective Mean Opinion Scores (MOS) aids in developing robust models for real-world applications.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The focus is on the issue of limited VQA performance on diverse, in-the-wild video content.</td>\n",
       "      <td>The paper highlights a lack of large and diverse in-the-wild VQA datasets.</td>\n",
       "      <td>The paper introduces a large and diverse dataset and efficient Multi-Level Spatially Pooled feature-based methods.</td>\n",
       "      <td>Performance is measured by Spearman rank-order correlation coefficient on KoNViD-1k, with improved scores using MLSP-VQA-FF.</td>\n",
       "      <td></td>\n",
       "      <td>The paper addresses under-performance of traditional VQA methods on diverse, in-the-wild video content.</td>\n",
       "      <td>They struggle with authentically distorted videos \"in-the-wild\".</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper introduces MLSP-VQA approaches for efficient no-reference video quality assessment in-the-wild.</td>\n",
       "      <td>The paper addresses issues with traditional VQA metrics' underperformance on diverse, in-the-wild video content.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>The paper tackles large-scale video classification using CNNs and performance improvements over baselines.</td>\n",
       "      <td>The paper proposes using CNNs with extended time domain connectivity for large-scale video classification.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The focus of the paper is improving large-scale video classification using Convolutional Neural Networks.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Performance improvement measured by comparing CNNs to baselines on a dataset of 1 million YouTube videos.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper focuses on enhancing large-scale video classification using Convolutional Neural Networks.</td>\n",
       "      <td>The paper addresses shortcomings in large-scale video classification using CNNs on new extensive datasets.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>what problem does this paper tackle?</td>\n",
       "      <td>what is the approach this paper proposed?</td>\n",
       "      <td>What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?</td>\n",
       "      <td>How does the paper propose to overcome the lack of large VQA databases?</td>\n",
       "      <td>What methods were used in the paper to construct and validate a new VQA database?</td>\n",
       "      <td>What are the potential applications of improving VQA databases as discussed in the paper?</td>\n",
       "      <td>What issue is the focus of this paper?</td>\n",
       "      <td>What is missing in VQA databases according to this paper?</td>\n",
       "      <td>What methodologies does the paper introduce to improve the performance of no-reference video quality models?</td>\n",
       "      <td>How does the paper measure the performance improvement on real-world video data?</td>\n",
       "      <td>What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?</td>\n",
       "      <td>What is the performance issue discussed in the paper concerning video quality models?</td>\n",
       "      <td>What type of video data do the no-reference video quality models struggle with?</td>\n",
       "      <td>What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?</td>\n",
       "      <td>How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?</td>\n",
       "      <td>What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?</td>\n",
       "      <td>What issues with traditional metrics does this paper address?</td>\n",
       "      <td>What is the focus of this paper's problem-solving?</td>\n",
       "      <td>In what context is the paper addressing shortcomings?</td>\n",
       "      <td>What is the main problem addressed by this paper?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>initial</td>\n",
       "      <td>initial</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presup</th>\n",
       "      <td>Presupposition: This paper tackles a problem.</td>\n",
       "      <td>The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.</td>\n",
       "      <td>Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.</td>\n",
       "      <td>The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.</td>\n",
       "      <td>Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.</td>\n",
       "      <td>The paper discusses potential applications of improving Visual Question Answering (VQA) databases.</td>\n",
       "      <td>Presupposition: This paper focuses on a specific issue.</td>\n",
       "      <td>Presupposition: This paper identifies something as missing in Visual Question Answering (VQA) databases.</td>\n",
       "      <td>Presupposition:\\n\\n1. The paper introduces methodologies.\\n2. These methodologies are aimed at improving performance.\\n3. The context is no-reference video quality models.\\n4. The performance that is subject to improvement pertains to no-reference video quality models.</td>\n",
       "      <td>The presuppositions underlying the third question are as follows:\\n\\n1. The paper includes a performance measurement or evaluation component.\\n2. The paper's focus, at least in part, involves real-world video data.\\n3. There is an implied performance improvement that the paper investigates or reports.\\n4. There is a specific methodology or metric used in the paper to measure said performance improvement.</td>\n",
       "      <td>Here are the presuppositions of the given question:\\n\\n1. A solution for no-reference video quality assessment is proposed in the paper.\\n2. The paper uses datasets or benchmarks to evaluate the proposed solution.\\n3. No-reference video quality assessment is a topic addressed within the paper.</td>\n",
       "      <td>Presupposition: The paper discusses a performance issue concerning video quality models.</td>\n",
       "      <td>Presupposition: No-reference video quality models struggle with certain types of video data.</td>\n",
       "      <td>- Traditional metrics for UGC video compression and quality assessment are considered by this paper.\\n- The paper finds the existing metrics for UGC video compression and quality assessment to be inadequate.\\n- The paper discusses UGC video compression and quality assessment.\\n- The paper is oriented towards improving or critiquing aspects of UGC video compression and quality assessment.</td>\n",
       "      <td>1. The paper proposes a method or solution for overcoming certain shortcomings.\\n2. The paper is focused on User-Generated Content (UGC) video quality assessment and compression.\\n3. There are known shortcomings in UGC video quality assessment and compression that the paper addresses.\\n4. The subject matter of the paper is relevant to the field of video quality assessment and/or compression.</td>\n",
       "      <td>1. The paper introduces new metrics or methodologies.\\n2. The paper's focus is on evaluating UGC (User-Generated Content) video compression and quality.</td>\n",
       "      <td>This paper addresses issues with traditional metrics.</td>\n",
       "      <td>Presupposition: This paper's focus is on problem-solving.</td>\n",
       "      <td>The paper is addressing some shortcomings.</td>\n",
       "      <td>The presuppositions associated with the question \"What is the main problem addressed by this paper?\" could include:\\n\\n1. The paper addresses a specific problem.\\n2. There is a \"main\" problem, indicating that the paper may address multiple issues, but there is a primary focus.\\n3. The paper is written with the intention to discuss and potentially offer solutions or insights into a problem.\\n4. The term \"this\" indicates that there is a specific paper being referenced, and that paper is accessible to or known by both the person asking and the person answering the question.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        question_0  \\\n",
       "paper_0                    The paper addresses the lack of large VQA databases with real-world video sequences and subjective MOS.   \n",
       "paper_1   The paper addresses the problem of no-reference video quality models' poor performance on diverse real-world video data.   \n",
       "paper_2                The paper tackles the shortcomings of traditional metrics for UGC video compression and quality assessment.   \n",
       "paper_3                                                                                                                              \n",
       "paper_4             The paper addresses the lack of large, diverse datasets for no-reference video quality assessment in-the-wild.   \n",
       "paper_5                 The paper tackles large-scale video classification using CNNs and performance improvements over baselines.   \n",
       "paper_6                                                                                                                              \n",
       "question                                                                                      what problem does this paper tackle?   \n",
       "type                                                                                                                       initial   \n",
       "presup                                                                               Presupposition: This paper tackles a problem.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              question_1  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The approach is the introduction of a large scale UGC dataset to improve video compression research.   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The approach is a ‘local-to-global’ region-based NR VQA architecture and spatio-temporal quality mapping engine.   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The approach proposed is a new dataset, KonVid-150k, and efficient VQA methods using multi-level spatially pooled deep-features (MLSP).   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The paper proposes using CNNs with extended time domain connectivity for large-scale video classification.   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       what is the approach this paper proposed?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             initial   \n",
       "presup    The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            question_2  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        VQA is video quality assessment, and a large, diverse database aids in developing and evaluating objective VQA methods.   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              VQA is Video Quality Assessment; large real-world database with MOS needed for accurate \"in-the-wild\" content quality prediction.   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          VQA is Video Quality Assessment, and a large, diverse dataset with subjective Mean Opinion Scores (MOS) aids in developing robust models for real-world applications.   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          followup   \n",
       "presup    Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.   \n",
       "\n",
       "                                                                                                 question_3  \\\n",
       "paper_0                                                                                                       \n",
       "paper_1                                                                                                       \n",
       "paper_2                                                                                                       \n",
       "paper_3                                                                                                       \n",
       "paper_4                                                                                                       \n",
       "paper_5                                                                                                       \n",
       "paper_6                                                                                                       \n",
       "question                            How does the paper propose to overcome the lack of large VQA databases?   \n",
       "type                                                                                               followup   \n",
       "presup    The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.   \n",
       "\n",
       "                                                                                                                         question_4  \\\n",
       "paper_0                                                                                                                               \n",
       "paper_1                                                                                                                               \n",
       "paper_2                                                                                                                               \n",
       "paper_3                  The paper used subjective annotations to create a large video quality dataset and developed NR-VQA models.   \n",
       "paper_4                                                                                                                               \n",
       "paper_5                                                                                                                               \n",
       "paper_6                                                                                                                               \n",
       "question                                          What methods were used in the paper to construct and validate a new VQA database?   \n",
       "type                                                                                                                       followup   \n",
       "presup    Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.   \n",
       "\n",
       "                                                                                                  question_5  \\\n",
       "paper_0                                                                                                        \n",
       "paper_1                                                                                                        \n",
       "paper_2                                                                                                        \n",
       "paper_3                                                                                                        \n",
       "paper_4                                                                                                        \n",
       "paper_5                                                                                                        \n",
       "paper_6                                                                                                        \n",
       "question           What are the potential applications of improving VQA databases as discussed in the paper?   \n",
       "type                                                                                                followup   \n",
       "presup    The paper discusses potential applications of improving Visual Question Answering (VQA) databases.   \n",
       "\n",
       "                                                                                                         question_6  \\\n",
       "paper_0                                                                                                               \n",
       "paper_1                                                                                                               \n",
       "paper_2   The paper focuses on the issue of assessing and improving compression and quality metrics for UGC videos.   \n",
       "paper_3                                                                                                               \n",
       "paper_4                 The focus is on the issue of limited VQA performance on diverse, in-the-wild video content.   \n",
       "paper_5   The focus of the paper is improving large-scale video classification using Convolutional Neural Networks.   \n",
       "paper_6                                                                                                               \n",
       "question                                                                     What issue is the focus of this paper?   \n",
       "type                                                                                                       lowlevel   \n",
       "presup                                                      Presupposition: This paper focuses on a specific issue.   \n",
       "\n",
       "                                                                                                        question_7  \\\n",
       "paper_0                                                                                                              \n",
       "paper_1                                                                                                              \n",
       "paper_2                                                                                                              \n",
       "paper_3                                                                                                              \n",
       "paper_4                                 The paper highlights a lack of large and diverse in-the-wild VQA datasets.   \n",
       "paper_5                                                                                                              \n",
       "paper_6                                                                                                              \n",
       "question                                                 What is missing in VQA databases according to this paper?   \n",
       "type                                                                                                      lowlevel   \n",
       "presup    Presupposition: This paper identifies something as missing in Visual Question Answering (VQA) databases.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                             question_8  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                   \n",
       "paper_1                                                                                                                                                                                                                                                                                   \n",
       "paper_2                                                                                                                                                                                                                                                                                   \n",
       "paper_3                                                                                                                                                                                                                                                                                   \n",
       "paper_4                                                                                                                                                              The paper introduces a large and diverse dataset and efficient Multi-Level Spatially Pooled feature-based methods.   \n",
       "paper_5                                                                                                                                                                                                                                                                                   \n",
       "paper_6                                                                                                                                                                                                                                                                                   \n",
       "question                                                                                                                                                                   What methodologies does the paper introduce to improve the performance of no-reference video quality models?   \n",
       "type                                                                                                                                                                                                                                                                           followup   \n",
       "presup    Presupposition:\\n\\n1. The paper introduces methodologies.\\n2. These methodologies are aimed at improving performance.\\n3. The context is no-reference video quality models.\\n4. The performance that is subject to improvement pertains to no-reference video quality models.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                       question_9  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "paper_1                                                                                                                                                                                                                                                                             Performance measured by comparing no-reference video quality predictors on a new large-scale video quality database with crowdsourced scores.   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "paper_3                                                                                                                                                                                                                                                                                                                  Performance is measured by a subjective video quality dataset with human perceptual quality annotations.   \n",
       "paper_4                                                                                                                                                                                                                                                                                              Performance is measured by Spearman rank-order correlation coefficient on KoNViD-1k, with improved scores using MLSP-VQA-FF.   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                 Performance improvement measured by comparing CNNs to baselines on a dataset of 1 million YouTube videos.   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "question                                                                                                                                                                                                                                                                                                                                         How does the paper measure the performance improvement on real-world video data?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                     followup   \n",
       "presup    The presuppositions underlying the third question are as follows:\\n\\n1. The paper includes a performance measurement or evaluation component.\\n2. The paper's focus, at least in part, involves real-world video data.\\n3. There is an implied performance improvement that the paper investigates or reports.\\n4. There is a specific methodology or metric used in the paper to measure said performance improvement.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     question_10  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                            \n",
       "paper_1                                                                                                                                                                                                                                                                                                            \n",
       "paper_2                                                                                                                                                                                                                                                                                                            \n",
       "paper_3                                                                                                                                                                                                                                                                                                            \n",
       "paper_4                                                                                                                                                                                                                                                                                                            \n",
       "paper_5                                                                                                                                                                                                                                                                                                            \n",
       "paper_6                                                                                                                                                                                                                                                                                                            \n",
       "question                                                                                                                                                                          What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?   \n",
       "type                                                                                                                                                                                                                                                                                                    followup   \n",
       "presup    Here are the presuppositions of the given question:\\n\\n1. A solution for no-reference video quality assessment is proposed in the paper.\\n2. The paper uses datasets or benchmarks to evaluate the proposed solution.\\n3. No-reference video quality assessment is a topic addressed within the paper.   \n",
       "\n",
       "                                                                                                                                         question_11  \\\n",
       "paper_0                                   The issue is the lack of large, diverse datasets with real-world distortions for video quality assessment.   \n",
       "paper_1                                       The issue is current no-reference video quality models' inability to handle diverse video impairments.   \n",
       "paper_2                                                                                                                                                \n",
       "paper_3   The paper discusses the limited prediction capabilities of current no-reference models on real-world user-generated content video quality.   \n",
       "paper_4                                      The paper addresses under-performance of traditional VQA methods on diverse, in-the-wild video content.   \n",
       "paper_5                                                                                                                                                \n",
       "paper_6                                                                                                                                                \n",
       "question                                                       What is the performance issue discussed in the paper concerning video quality models?   \n",
       "type                                                                                                                                        lowlevel   \n",
       "presup                                                      Presupposition: The paper discusses a performance issue concerning video quality models.   \n",
       "\n",
       "                                                                                                               question_12  \\\n",
       "paper_0                                                                                                                      \n",
       "paper_1   No-reference video quality models struggle with diverse, authentic, commingled distortions in real-world videos.   \n",
       "paper_2                 No-reference video quality models struggle with User Generated Content (UGC) that is non-pristine.   \n",
       "paper_3                No-reference video quality models struggle with real-world user-generated content (UGC) video data.   \n",
       "paper_4                                                   They struggle with authentically distorted videos \"in-the-wild\".   \n",
       "paper_5                                                                                                                      \n",
       "paper_6                                                                                                                      \n",
       "question                                   What type of video data do the no-reference video quality models struggle with?   \n",
       "type                                                                                                              lowlevel   \n",
       "presup                        Presupposition: No-reference video quality models struggle with certain types of video data.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     question_13  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_2                                                                                                                                                                                                                                                                                                        The paper considers BD-Rate and PSNR inadequate for UGC video compression and quality assessment.   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "question                                                                                                                                                                                                                                                             What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                    followup   \n",
       "presup    - Traditional metrics for UGC video compression and quality assessment are considered by this paper.\\n- The paper finds the existing metrics for UGC video compression and quality assessment to be inadequate.\\n- The paper discusses UGC video compression and quality assessment.\\n- The paper is oriented towards improving or critiquing aspects of UGC video compression and quality assessment.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                         question_14  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_1                                                                                                                                                                                                                                                                                                        The paper proposes a large-scale video quality database to enhance no-reference video quality models.   \n",
       "paper_2                                                                                                                                                                                                                                                                                                 The paper proposes using no-reference objective quality metrics to assess UGC video quality for compression.   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "question                                                                                                                                                                                                                                                                                                  How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                        followup   \n",
       "presup    1. The paper proposes a method or solution for overcoming certain shortcomings.\\n2. The paper is focused on User-Generated Content (UGC) video quality assessment and compression.\\n3. There are known shortcomings in UGC video quality assessment and compression that the paper addresses.\\n4. The subject matter of the paper is relevant to the field of video quality assessment and/or compression.   \n",
       "\n",
       "                                                                                                                                                       question_15  \\\n",
       "paper_0                                                                                                                                                              \n",
       "paper_1                                                                                                                                                              \n",
       "paper_2                                              The paper introduces no-reference objective quality metrics for evaluating UGC video compression and quality.   \n",
       "paper_3                                     The paper introduces a local-to-global region-based NR VQA architecture and a space-time video quality mapping engine.   \n",
       "paper_4                                                  The paper introduces MLSP-VQA approaches for efficient no-reference video quality assessment in-the-wild.   \n",
       "paper_5                                                                                                                                                              \n",
       "paper_6                                                                                                                                                              \n",
       "question                                              What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?   \n",
       "type                                                                                                                                                      followup   \n",
       "presup    1. The paper introduces new metrics or methodologies.\\n2. The paper's focus is on evaluating UGC (User-Generated Content) video compression and quality.   \n",
       "\n",
       "                                                                                                                          question_16  \\\n",
       "paper_0                                                                                                                                 \n",
       "paper_1   The paper addresses the inadequacy of existing databases to represent real-world video diversity and authentic distortions.   \n",
       "paper_2               The paper addresses shortcomings of traditional metrics like BD-Rate and PSNR on non-pristine originals in UGC.   \n",
       "paper_3                                                                                                                                 \n",
       "paper_4              The paper addresses issues with traditional VQA metrics' underperformance on diverse, in-the-wild video content.   \n",
       "paper_5                                                                                                                                 \n",
       "paper_6                                                                                                                                 \n",
       "question                                                                What issues with traditional metrics does this paper address?   \n",
       "type                                                                                                                         lowlevel   \n",
       "presup                                                                          This paper addresses issues with traditional metrics.   \n",
       "\n",
       "                                                                                                          question_17  \\\n",
       "paper_0                                                                                                                 \n",
       "paper_1     The focus is on improving no-reference video quality prediction models for diverse real-world conditions.   \n",
       "paper_2                                                                                                                 \n",
       "paper_3   The paper focuses on improving no-reference perceptual video quality assessment for user-generated content.   \n",
       "paper_4                                                                                                                 \n",
       "paper_5          The paper focuses on enhancing large-scale video classification using Convolutional Neural Networks.   \n",
       "paper_6                                                                                                                 \n",
       "question                                                           What is the focus of this paper's problem-solving?   \n",
       "type                                                                                                         lowlevel   \n",
       "presup                                                      Presupposition: This paper's focus is on problem-solving.   \n",
       "\n",
       "                                                                                                                   question_18  \\\n",
       "paper_0                                                                                                                          \n",
       "paper_1                The paper addresses shortcomings in current no-reference video quality models due to limited databases.   \n",
       "paper_2                                                                                                                          \n",
       "paper_3   The paper addresses the shortcomings in no-reference perceptual video quality assessment for user-generated content.   \n",
       "paper_4                                                                                                                          \n",
       "paper_5             The paper addresses shortcomings in large-scale video classification using CNNs on new extensive datasets.   \n",
       "paper_6                                                                                                                          \n",
       "question                                                                 In what context is the paper addressing shortcomings?   \n",
       "type                                                                                                                  lowlevel   \n",
       "presup                                                                              The paper is addressing some shortcomings.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                question_19  \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The main problem addressed is the inefficient no-reference perceptual video quality assessment for user-generated content.  \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  What is the main problem addressed by this paper?  \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                generic  \n",
       "presup    The presuppositions associated with the question \"What is the main problem addressed by this paper?\" could include:\\n\\n1. The paper addresses a specific problem.\\n2. There is a \"main\" problem, indicating that the paper may address multiple issues, but there is a primary focus.\\n3. The paper is written with the intention to discuss and potentially offer solutions or insights into a problem.\\n4. The term \"this\" indicates that there is a specific paper being referenced, and that paper is accessible to or known by both the person asking and the person answering the question.  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table_0_total['ours_table_question'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7f26b0f-db1e-462e-a102-b71b806ef3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_0': '',\n",
       " 'paper_1': \"The 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper does not provide details about the number of human perceptual quality annotations within the dataset it discusses.\",\n",
       " 'paper_2': '',\n",
       " 'paper_3': \"The dataset in the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6,300 subjects.\",\n",
       " 'paper_4': '5.5M',\n",
       " 'paper_5': \"The 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper states that the final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches.\",\n",
       " 'paper_6': 'A total of 5.5 million perceptual quality judgments on videos and v-patches from almost 6,300 subjects'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_baseline['multi_scheme'][\"What is the number of human perceptual quality annotations provided in the dataset mentioned in the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dd1adf4-3935-40da-b2c4-b3b3558304e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>How are the videos in the UGC dataset sampled?</th>\n",
       "      <th>What is the purpose of the PVQ Mapper mentioned in the paper?</th>\n",
       "      <th>What are the shortcomings of traditional reference-based metrics on UGC according to the paper?</th>\n",
       "      <th>What is the performance difference between spatio-temporal networks and single-frame models reported in the paper?</th>\n",
       "      <th>On what measures is this study the largest video quality assessment study ever conducted?</th>\n",
       "      <th>To what extent does the LIVE-VQC database represent real world videos?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>This study is the largest video quality assessment study ever conducted in terms of the number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.</td>\n",
       "      <td>The LIVE-VQC database is designed to represent real world videos that contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>For the 'YouTube UGC Dataset for Video Compression Research', videos in the UGC dataset are sampled using a novel sampling method based on features extracted from encoding.</td>\n",
       "      <td>The purpose of the PVQ Mapper mentioned in the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper is to predict first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships, which helps localize, visualize, and act on video distortions.</td>\n",
       "      <td>The shortcomings of traditional reference-based metrics on UGC according to the 'YouTube UGC Dataset for Video Compression Research' paper include their significant drop in accuracy when applied to non-pristine originals which make up the majority of UGC.</td>\n",
       "      <td>The 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper suggests that their new model achieves top performance on the new database as well as on smaller 'in-the-wild' databases without fine-tuning. However, specific performance difference values between spatio-temporal networks and single-frame models are not provided in the given introduction.</td>\n",
       "      <td>The 'Large-Scale Study of Perceptual Video Quality’ study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.</td>\n",
       "      <td>The LIVE-VQC database represents real-world videos to a considerable extent as it contains 585 videos of unique content, captured by a large number of users, with a wide range of levels of complex, authentic distortions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td></td>\n",
       "      <td>The PVQ Mapper is a prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>In the 'YouTube UGC Dataset for Video Compression Research' paper, the videos in the dataset are sampled using a novel method based on features extracted from encoding.</td>\n",
       "      <td>The purpose of the PVQ Mapper, as mentioned in the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper, is to predict first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships, helping to localize, visualize, and act on video distortions.</td>\n",
       "      <td>The 'YouTube UGC Dataset for Video Compression Research' paper and 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper both address the shortcomings of traditional reference-based metrics on UGC, which include their decreased accuracy when applied to non-pristine originals that are typical of UGC, as they were designed for pristine originals.</td>\n",
       "      <td>The 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper seems to imply that spatio-temporal networks have better performance due to their ability to capture both spatial and temporal impairments, unlike single-frame models which may not capture diverse temporal impairments effectively.</td>\n",
       "      <td>According to the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper, this study is the largest ever conducted based on the number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.</td>\n",
       "      <td>The 'Large-Scale Study of Perceptual Video Quality' paper claims that the LIVE-VQC database contains a large variety of videos captured by numerous users under a wide range of levels of complex, authentic distortions, thereby adequately representing real world videos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>The method of video sampling in the UGC dataset is not detailed in the provided paper introductions or abstracts.</td>\n",
       "      <td>The purpose of the PVQ Mapper is to predict space-time maps of video quality by learning global-to-local quality relationships, helping to localize, visualize, and act on video distortions.</td>\n",
       "      <td>Traditional reference-based metrics on UGC, such as BD-Rate and PSNR, are said to be less accurate for non-pristine originals and their shortcomings include not accounting for the diverse and authentic distortions found in UGC.</td>\n",
       "      <td>The performance difference between spatio-temporal networks and single-frame models is not provided in the given paper content.</td>\n",
       "      <td>This study is the largest video quality assessment study ever conducted in terms of number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.</td>\n",
       "      <td>The extent to which the LIVE-VQC database represents real world videos is characterized by its large and diverse collection of videos with complex, authentic distortions, but specific representational details are not provided in the given text.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       How are the videos in the UGC dataset sampled?  \\\n",
       "paper_0                                                                                                                                                                                 \n",
       "paper_1                                                                                                                                                                                 \n",
       "paper_2                                                                                                                                                                                 \n",
       "paper_3  For the 'YouTube UGC Dataset for Video Compression Research', videos in the UGC dataset are sampled using a novel sampling method based on features extracted from encoding.   \n",
       "paper_4                                                                                                                                                                                 \n",
       "paper_5      In the 'YouTube UGC Dataset for Video Compression Research' paper, the videos in the dataset are sampled using a novel method based on features extracted from encoding.   \n",
       "paper_6                                                             The method of video sampling in the UGC dataset is not detailed in the provided paper introductions or abstracts.   \n",
       "\n",
       "                                                                                                                                                                                                                                         What is the purpose of the PVQ Mapper mentioned in the paper?  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                  \n",
       "paper_1                                                                                                                                                                                                                                                                                                  \n",
       "paper_2                                                                                                                                                                                                                                                                                                  \n",
       "paper_3      The purpose of the PVQ Mapper mentioned in the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper is to predict first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships, which helps localize, visualize, and act on video distortions.   \n",
       "paper_4                                                                                                                                         The PVQ Mapper is a prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships.   \n",
       "paper_5  The purpose of the PVQ Mapper, as mentioned in the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper, is to predict first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships, helping to localize, visualize, and act on video distortions.   \n",
       "paper_6                                                                                                  The purpose of the PVQ Mapper is to predict space-time maps of video quality by learning global-to-local quality relationships, helping to localize, visualize, and act on video distortions.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        What are the shortcomings of traditional reference-based metrics on UGC according to the paper?  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                   \n",
       "paper_3                                                                                                 The shortcomings of traditional reference-based metrics on UGC according to the 'YouTube UGC Dataset for Video Compression Research' paper include their significant drop in accuracy when applied to non-pristine originals which make up the majority of UGC.   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                   \n",
       "paper_5  The 'YouTube UGC Dataset for Video Compression Research' paper and 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper both address the shortcomings of traditional reference-based metrics on UGC, which include their decreased accuracy when applied to non-pristine originals that are typical of UGC, as they were designed for pristine originals.   \n",
       "paper_6                                                                                                                             Traditional reference-based metrics on UGC, such as BD-Rate and PSNR, are said to be less accurate for non-pristine originals and their shortcomings include not accounting for the diverse and authentic distortions found in UGC.   \n",
       "\n",
       "                                                                                                                                                                                                                                                       What is the performance difference between spatio-temporal networks and single-frame models reported in the paper?  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                     \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                     \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                     \n",
       "paper_3  The 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper suggests that their new model achieves top performance on the new database as well as on smaller 'in-the-wild' databases without fine-tuning. However, specific performance difference values between spatio-temporal networks and single-frame models are not provided in the given introduction.   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                     \n",
       "paper_5                                                              The 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper seems to imply that spatio-temporal networks have better performance due to their ability to capture both spatial and temporal impairments, unlike single-frame models which may not capture diverse temporal impairments effectively.   \n",
       "paper_6                                                                                                                                                                                                                                   The performance difference between spatio-temporal networks and single-frame models is not provided in the given paper content.   \n",
       "\n",
       "                                                                                                                                                                                                                      On what measures is this study the largest video quality assessment study ever conducted?  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                           \n",
       "paper_1                                                               This study is the largest video quality assessment study ever conducted in terms of the number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.   \n",
       "paper_2                                                                                                                                                                                                                                                                                                           \n",
       "paper_3  The 'Large-Scale Study of Perceptual Video Quality’ study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.   \n",
       "paper_4                                                                                                                                                       Number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.   \n",
       "paper_5                     According to the 'Patch-VQ: ‘Patching Up’ the Video Quality Problem' paper, this study is the largest ever conducted based on the number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.   \n",
       "paper_6                                                                   This study is the largest video quality assessment study ever conducted in terms of number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.   \n",
       "\n",
       "                                                                                                                                                                                                               To what extent does the LIVE-VQC database represent real world videos?  \n",
       "paper_0                                                                                                                                                                                                                                                                                \n",
       "paper_1                    The LIVE-VQC database is designed to represent real world videos that contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate.  \n",
       "paper_2                                                                                                                                                                                                                                                                                \n",
       "paper_3                                                  The LIVE-VQC database represents real-world videos to a considerable extent as it contains 585 videos of unique content, captured by a large number of users, with a wide range of levels of complex, authentic distortions.  \n",
       "paper_4                                                                                                                                                                                                                                                                                \n",
       "paper_5  The 'Large-Scale Study of Perceptual Video Quality' paper claims that the LIVE-VQC database contains a large variety of videos captured by numerous users under a wide range of levels of complex, authentic distortions, thereby adequately representing real world videos.  \n",
       "paper_6                          The extent to which the LIVE-VQC database represents real world videos is characterized by its large and diverse collection of videos with complex, authentic distortions, but specific representational details are not provided in the given text.  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({k: v for k, v in table_0_total['multi_scheme'].items() if \"_\" not in k}); df[df.columns[6:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0c49898-8f88-4583-a6f4-545c867133b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What is the main focus of the study?</th>\n",
       "      <th>What types of data are presented in the study?</th>\n",
       "      <th>What is the methodology used for quality assessment?</th>\n",
       "      <th>What are the proposed contributions?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>[Creation of a subjectively annotated video quality assessment (VQA) database with real-world video sequences for the development and evaluation of objective VQA methods.]</td>\n",
       "      <td>[Subjectively annotated VQA database with 1,200 video sequences representing a wide variety of content and distortions.]</td>\n",
       "      <td>[Subjective mean opinion scores (MOS) are used for video quality rating.]</td>\n",
       "      <td>[A new large dataset of video sequences aimed at helping development and evaluation of general-purpose VQA methods.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>[Construction of a large-scale video quality assessment database and conducting a comprehensive study with crowdsourced subjective video quality scores to advance no-reference video quality prediction.]</td>\n",
       "      <td>[Video quality assessment database containing 585 videos with a large number of subjective quality scores collected via crowdsourcing.]</td>\n",
       "      <td>[Crowdsourced subjective ratings were used for the quality assessment of video.]</td>\n",
       "      <td>[A large-scale video quality assessment database and comprehensive study to aid in the improvement of NR video quality predictors.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>[Introduction of a large scale User Generated Content (UGC) dataset for video compression research, along with discussion of challenges for UGC compression and evaluation of no-reference objective quality metrics.]</td>\n",
       "      <td>[Large scale UGC dataset with 1500 20-second video clips covering various categories and features like HDR, plus evaluation with three no-reference metrics.]</td>\n",
       "      <td>[Evaluation is based on no-reference objective quality metrics such as Noise, Banding, and SLEEQ.]</td>\n",
       "      <td>[Introduction of a UGC dataset for research, novel sampling method, and discussion of challenges and shortcomings for UGC compression and quality evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>[Development of a large subjective video quality dataset and corresponding no-reference perceptual video quality assessment models that target 'in-the-wild' user-generated content (UGC).]</td>\n",
       "      <td>[Extensive video quality dataset with 39,000 distorted videos, 117,000 video patches, and 5.5 million human perceptual quality annotations.]</td>\n",
       "      <td>[Creation of a region-based no-reference VQA architecture and a space-time video quality mapping engine for quality prediction and localization.]</td>\n",
       "      <td>[Largest subjective video quality dataset for ‘in-the-wild’ distorted UGC videos and innovative models for local-to-global video quality assessment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>[Introduction of a new in-the-wild video quality assessment dataset called KonVid-150k, alongside new efficient video quality assessment approaches based on multi-level spatially pooled features.]</td>\n",
       "      <td>[VQA dataset called KonVid-150k with over 153,841 videos and multiple quality ratings, and 1,596 videos with extensive subjective assessment.]</td>\n",
       "      <td>[Proposed new efficient VQA approaches relying on multi-level spatially pooled deep-features (MLSP) for no-reference quality assessment.]</td>\n",
       "      <td>[Introduction of a new VQA dataset that is larger and more diverse compared to existing datasets, as well as efficient VQA models suitable for in-the-wild videos.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>[Empirical evaluation of Convolutional Neural Networks (CNNs) on large-scale video classification using a dataset of 1 million YouTube videos, and exploration of methods to extend CNNs in the time domain.]</td>\n",
       "      <td>[New dataset of 1 million YouTube videos across 487 classes for the evaluation of CNNs on large-scale video classification.]</td>\n",
       "      <td>[Empirical evaluation of CNNs, comparison with feature-based baselines, and studying model's performance improvements on UCF-101 dataset.]</td>\n",
       "      <td>[Extensive evaluation of CNNs on large-scale video classification and suggestions on approaches to extend the spatio-temporal connectivity of CNNs.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_7</th>\n",
       "      <td>[Description of the Kinetics human action video dataset for human action classification, containing 400 classes with an analysis of dataset imbalance and classifier bias.]</td>\n",
       "      <td>[Kinetics human action video dataset comprising over 400 human action classes, with at least 400 video clips for each action.]</td>\n",
       "      <td>[Baseline performance figures for neural network architectures, and analysis of dataset imbalance impact on classifier performance.]</td>\n",
       "      <td>[New human action video dataset intended for more accurate human action classification and analysis of dataset bias.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                           What is the main focus of the study?  \\\n",
       "paper_1                                             [Creation of a subjectively annotated video quality assessment (VQA) database with real-world video sequences for the development and evaluation of objective VQA methods.]   \n",
       "paper_2              [Construction of a large-scale video quality assessment database and conducting a comprehensive study with crowdsourced subjective video quality scores to advance no-reference video quality prediction.]   \n",
       "paper_3  [Introduction of a large scale User Generated Content (UGC) dataset for video compression research, along with discussion of challenges for UGC compression and evaluation of no-reference objective quality metrics.]   \n",
       "paper_4                             [Development of a large subjective video quality dataset and corresponding no-reference perceptual video quality assessment models that target 'in-the-wild' user-generated content (UGC).]   \n",
       "paper_5                    [Introduction of a new in-the-wild video quality assessment dataset called KonVid-150k, alongside new efficient video quality assessment approaches based on multi-level spatially pooled features.]   \n",
       "paper_6           [Empirical evaluation of Convolutional Neural Networks (CNNs) on large-scale video classification using a dataset of 1 million YouTube videos, and exploration of methods to extend CNNs in the time domain.]   \n",
       "paper_7                                             [Description of the Kinetics human action video dataset for human action classification, containing 400 classes with an analysis of dataset imbalance and classifier bias.]   \n",
       "\n",
       "                                                                                                                        What types of data are presented in the study?  \\\n",
       "paper_1                                       [Subjectively annotated VQA database with 1,200 video sequences representing a wide variety of content and distortions.]   \n",
       "paper_2                        [Video quality assessment database containing 585 videos with a large number of subjective quality scores collected via crowdsourcing.]   \n",
       "paper_3  [Large scale UGC dataset with 1500 20-second video clips covering various categories and features like HDR, plus evaluation with three no-reference metrics.]   \n",
       "paper_4                   [Extensive video quality dataset with 39,000 distorted videos, 117,000 video patches, and 5.5 million human perceptual quality annotations.]   \n",
       "paper_5                 [VQA dataset called KonVid-150k with over 153,841 videos and multiple quality ratings, and 1,596 videos with extensive subjective assessment.]   \n",
       "paper_6                                   [New dataset of 1 million YouTube videos across 487 classes for the evaluation of CNNs on large-scale video classification.]   \n",
       "paper_7                                 [Kinetics human action video dataset comprising over 400 human action classes, with at least 400 video clips for each action.]   \n",
       "\n",
       "                                                                                                      What is the methodology used for quality assessment?  \\\n",
       "paper_1                                                                          [Subjective mean opinion scores (MOS) are used for video quality rating.]   \n",
       "paper_2                                                                   [Crowdsourced subjective ratings were used for the quality assessment of video.]   \n",
       "paper_3                                                 [Evaluation is based on no-reference objective quality metrics such as Noise, Banding, and SLEEQ.]   \n",
       "paper_4  [Creation of a region-based no-reference VQA architecture and a space-time video quality mapping engine for quality prediction and localization.]   \n",
       "paper_5          [Proposed new efficient VQA approaches relying on multi-level spatially pooled deep-features (MLSP) for no-reference quality assessment.]   \n",
       "paper_6         [Empirical evaluation of CNNs, comparison with feature-based baselines, and studying model's performance improvements on UCF-101 dataset.]   \n",
       "paper_7               [Baseline performance figures for neural network architectures, and analysis of dataset imbalance impact on classifier performance.]   \n",
       "\n",
       "                                                                                                                                        What are the proposed contributions?  \n",
       "paper_1                                                 [A new large dataset of video sequences aimed at helping development and evaluation of general-purpose VQA methods.]  \n",
       "paper_2                                  [A large-scale video quality assessment database and comprehensive study to aid in the improvement of NR video quality predictors.]  \n",
       "paper_3       [Introduction of a UGC dataset for research, novel sampling method, and discussion of challenges and shortcomings for UGC compression and quality evaluation.]  \n",
       "paper_4                [Largest subjective video quality dataset for ‘in-the-wild’ distorted UGC videos and innovative models for local-to-global video quality assessment.]  \n",
       "paper_5  [Introduction of a new VQA dataset that is larger and more diverse compared to existing datasets, as well as efficient VQA models suitable for in-the-wild videos.]  \n",
       "paper_6                 [Extensive evaluation of CNNs on large-scale video classification and suggestions on approaches to extend the spatio-temporal connectivity of CNNs.]  \n",
       "paper_7                                                [New human action video dataset intended for more accurate human action classification and analysis of dataset bias.]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(table_0_total['pap_to_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8960444e-9473-4688-a56f-aec7a00f2c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>What is the main focus of the research?</th>\n",
       "      <th>What type of dataset is introduced or used?</th>\n",
       "      <th>How were the video sequences obtained for the dataset?</th>\n",
       "      <th>What are the applications of the proposed method or dataset?</th>\n",
       "      <th>What is the scale of the dataset in terms of video count and variety?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>[Creating a large VQA database of real-world video sequences with subjective mean opinion scores to benefit the development and evaluation of VQA methods.]</td>\n",
       "      <td>[The KoNViD-1k database consisting of 1,200 public-domain video sequences with subjective annotations.]</td>\n",
       "      <td>[Fairly sampled from a large public video dataset, YFCC100m.]</td>\n",
       "      <td>[Training and validation of general-purpose VQA methods, especially useful for deep learning approaches.]</td>\n",
       "      <td>[1,200 video sequences depicting a wide variety of content and authentic distortions.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>[Conducting a large-scale study of perceptual video quality and constructing a large video quality assessment database to advance NR video quality prediction.]</td>\n",
       "      <td>[The LIVE Video Quality Challenge Database (LIVE-VQC) containing 585 videos with a large number of subjective scores collected via crowdsourcing.]</td>\n",
       "      <td>[Captured by a large number of users, with wide ranges of levels of complex, authentic distortions.]</td>\n",
       "      <td>[Advancing no-reference video quality prediction by providing a resource for evaluating video quality predictors.]</td>\n",
       "      <td>[585 videos of unique content, captured by numerous users and a wide range of authentically distorted videos.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>[Introducing a large scale UGC dataset for video compression and quality assessment research, discussing its challenges, and evaluating UGC quality by no-reference metrics.]</td>\n",
       "      <td>[A large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos, covering popular categories and features like HDR.]</td>\n",
       "      <td>[Sampled from millions of YouTube videos through a novel sampling method based on features extracted from encoding.]</td>\n",
       "      <td>[Improving video compression and quality assessment in the context of UGC by providing a dataset representative of real-world conditions.]</td>\n",
       "      <td>[1500 video clips representing popular categories and various features such as HDR.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>[Creating the largest subjective video quality dataset and developing region-based NR VQA models to improve the prediction of video quality on UGC.]</td>\n",
       "      <td>[A subjective video quality dataset containing 39,000 real-world distorted videos and 117,000 'v-patches', along with 5.5M human quality annotations.]</td>\n",
       "      <td>[Collected real-world UGC video data and space-time localized video patches.]</td>\n",
       "      <td>[Improving NR perceptual VQA algorithms for social and streaming media applications, localization, and visualization of perceptual distortions.]</td>\n",
       "      <td>[A dataset containing 39,000 videos and 117,000 video patches for localized quality assessment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>[Introducing a new VQA dataset 'KonVid-150k' for videos in-the-wild and proposing new efficient VQA approaches with multi-level spatially pooled deep-features.]</td>\n",
       "      <td>[KonVid-150k, a dataset of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each.]</td>\n",
       "      <td>[Automatically gathered from the web and coarsely annotated.]</td>\n",
       "      <td>[Providing a basis for developing and benchmarking VQA methods suited for videos in-the-wild.]</td>\n",
       "      <td>[153,841 videos for coarsely annotated set, and 1,596 videos for densely annotated set.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>[Providing an extensive empirical evaluation of CNNs on a new large-scale video classification dataset of YouTube videos and exploring different architectures.]</td>\n",
       "      <td>[A new dataset of 1 million YouTube videos belonging to 487 classes for large-scale video classification.]</td>\n",
       "      <td>[Automatically extracted from YouTube using video metadata.]</td>\n",
       "      <td>[Evaluating the performance of CNN architectures for large-scale video classification tasks and the extraction of spatio-temporal features.]</td>\n",
       "      <td>[1 million YouTube videos belonging to 487 classes, providing significant variety in terms of content.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_7</th>\n",
       "      <td>[Describing the DeepMind Kinetics human action video dataset and analyzing the classification performance and potential dataset biases.]</td>\n",
       "      <td>[The Kinetics human action video dataset containing 400 action classes, with over 400 video clips per action sourced from YouTube.]</td>\n",
       "      <td>[Manually selected from YouTube, ensuring a variety of human actions and contexts.]</td>\n",
       "      <td>[Training and testing neural network architectures for human action classification and analyzing dataset imbalance and its effects.]</td>\n",
       "      <td>[Approximately 400 video clips for each of the 400 human action classes, totaling over 160,000 clips.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               What is the main focus of the research?  \\\n",
       "paper_1                    [Creating a large VQA database of real-world video sequences with subjective mean opinion scores to benefit the development and evaluation of VQA methods.]   \n",
       "paper_2                [Conducting a large-scale study of perceptual video quality and constructing a large video quality assessment database to advance NR video quality prediction.]   \n",
       "paper_3  [Introducing a large scale UGC dataset for video compression and quality assessment research, discussing its challenges, and evaluating UGC quality by no-reference metrics.]   \n",
       "paper_4                           [Creating the largest subjective video quality dataset and developing region-based NR VQA models to improve the prediction of video quality on UGC.]   \n",
       "paper_5               [Introducing a new VQA dataset 'KonVid-150k' for videos in-the-wild and proposing new efficient VQA approaches with multi-level spatially pooled deep-features.]   \n",
       "paper_6               [Providing an extensive empirical evaluation of CNNs on a new large-scale video classification dataset of YouTube videos and exploring different architectures.]   \n",
       "paper_7                                       [Describing the DeepMind Kinetics human action video dataset and analyzing the classification performance and potential dataset biases.]   \n",
       "\n",
       "                                                                                                                    What type of dataset is introduced or used?  \\\n",
       "paper_1                                                 [The KoNViD-1k database consisting of 1,200 public-domain video sequences with subjective annotations.]   \n",
       "paper_2      [The LIVE Video Quality Challenge Database (LIVE-VQC) containing 585 videos with a large number of subjective scores collected via crowdsourcing.]   \n",
       "paper_3       [A large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos, covering popular categories and features like HDR.]   \n",
       "paper_4  [A subjective video quality dataset containing 39,000 real-world distorted videos and 117,000 'v-patches', along with 5.5M human quality annotations.]   \n",
       "paper_5                        [KonVid-150k, a dataset of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each.]   \n",
       "paper_6                                              [A new dataset of 1 million YouTube videos belonging to 487 classes for large-scale video classification.]   \n",
       "paper_7                     [The Kinetics human action video dataset containing 400 action classes, with over 400 video clips per action sourced from YouTube.]   \n",
       "\n",
       "                                                                       How were the video sequences obtained for the dataset?  \\\n",
       "paper_1                                                         [Fairly sampled from a large public video dataset, YFCC100m.]   \n",
       "paper_2                  [Captured by a large number of users, with wide ranges of levels of complex, authentic distortions.]   \n",
       "paper_3  [Sampled from millions of YouTube videos through a novel sampling method based on features extracted from encoding.]   \n",
       "paper_4                                         [Collected real-world UGC video data and space-time localized video patches.]   \n",
       "paper_5                                                         [Automatically gathered from the web and coarsely annotated.]   \n",
       "paper_6                                                          [Automatically extracted from YouTube using video metadata.]   \n",
       "paper_7                                   [Manually selected from YouTube, ensuring a variety of human actions and contexts.]   \n",
       "\n",
       "                                                                                             What are the applications of the proposed method or dataset?  \\\n",
       "paper_1                                         [Training and validation of general-purpose VQA methods, especially useful for deep learning approaches.]   \n",
       "paper_2                                [Advancing no-reference video quality prediction by providing a resource for evaluating video quality predictors.]   \n",
       "paper_3        [Improving video compression and quality assessment in the context of UGC by providing a dataset representative of real-world conditions.]   \n",
       "paper_4  [Improving NR perceptual VQA algorithms for social and streaming media applications, localization, and visualization of perceptual distortions.]   \n",
       "paper_5                                                    [Providing a basis for developing and benchmarking VQA methods suited for videos in-the-wild.]   \n",
       "paper_6      [Evaluating the performance of CNN architectures for large-scale video classification tasks and the extraction of spatio-temporal features.]   \n",
       "paper_7              [Training and testing neural network architectures for human action classification and analyzing dataset imbalance and its effects.]   \n",
       "\n",
       "                                                  What is the scale of the dataset in terms of video count and variety?  \n",
       "paper_1                          [1,200 video sequences depicting a wide variety of content and authentic distortions.]  \n",
       "paper_2  [585 videos of unique content, captured by numerous users and a wide range of authentically distorted videos.]  \n",
       "paper_3                            [1500 video clips representing popular categories and various features such as HDR.]  \n",
       "paper_4                [A dataset containing 39,000 videos and 117,000 video patches for localized quality assessment.]  \n",
       "paper_5                        [153,841 videos for coarsely annotated set, and 1,596 videos for densely annotated set.]  \n",
       "paper_6         [1 million YouTube videos belonging to 487 classes, providing significant variety in terms of content.]  \n",
       "paper_7          [Approximately 400 video clips for each of the 400 human action classes, totaling over 160,000 clips.]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table_0_baseline['pap_to_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aa674ec-0f32-41f9-94a5-201c1ca72375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_size</th>\n",
       "      <th>dataset_focus</th>\n",
       "      <th>content_diversity</th>\n",
       "      <th>data_collection_methodology</th>\n",
       "      <th>UGC_focus</th>\n",
       "      <th>reference_to_real_world_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>[1,200 video sequences]</td>\n",
       "      <td>[No-reference video quality assessment]</td>\n",
       "      <td>[Variety of content and authentic distortions]</td>\n",
       "      <td>[Subjectively annotated]</td>\n",
       "      <td>[No specific focus on UGC]</td>\n",
       "      <td>['In the wild' authentic distortions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>[585 videos]</td>\n",
       "      <td>[No-reference video quality assessment]</td>\n",
       "      <td>[Unique contents, capture devices, distortion types]</td>\n",
       "      <td>[Crowdsourcing with 4776 participants, 205000 opinion scores]</td>\n",
       "      <td>[No specific focus on UGC]</td>\n",
       "      <td>[Real-world video data with authentic distortions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>[1,500 20-sec video clips]</td>\n",
       "      <td>[Compression and quality assessment of UGC]</td>\n",
       "      <td>[Popular categories like Gaming, Sports, HDR features]</td>\n",
       "      <td>[Sampling based on features from encoding]</td>\n",
       "      <td>[UGC compression and quality evaluation]</td>\n",
       "      <td>[Non-pristine originals from YouTube]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>[39,000 videos and 117,000 v-patches]</td>\n",
       "      <td>[No-reference perceptual video quality assessment]</td>\n",
       "      <td>[Real-world distorted videos and local video patches]</td>\n",
       "      <td>[5.5M human perceptual quality annotations]</td>\n",
       "      <td>[Emphasis on UGC video data]</td>\n",
       "      <td>[Real-world UGC video data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>[153,841 videos, 1,596 densely annotated]</td>\n",
       "      <td>[No-reference video quality assessment of in-the-wild videos]</td>\n",
       "      <td>[Substantially larger and diverse, in-the-wild videos]</td>\n",
       "      <td>[Coarsely annotated with five quality ratings each, some with at least 89 ratings]</td>\n",
       "      <td>[Focus on in-the-wild UGC]</td>\n",
       "      <td>[In-the-wild videos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>[1 million YouTube videos]</td>\n",
       "      <td>[Video classification with CNNs]</td>\n",
       "      <td>[Large-scale video classification across 487 classes]</td>\n",
       "      <td>[Empirical evaluation on a new dataset]</td>\n",
       "      <td>[Videos from YouTube, UGC mentioned but not the focus]</td>\n",
       "      <td>[Use of YouTube videos implies real-world relevance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_7</th>\n",
       "      <td>[At least 400 clips for each of the 400 actions]</td>\n",
       "      <td>[Human action video dataset]</td>\n",
       "      <td>[400 human action classes, broad range]</td>\n",
       "      <td>[Detailed collection description and baseline figures]</td>\n",
       "      <td>[YouTube videos are source, but focus is on human actions rather than UGC as a category]</td>\n",
       "      <td>[YouTube videos, but real-world conditions not explicitly stated]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             dataset_size  \\\n",
       "paper_1                           [1,200 video sequences]   \n",
       "paper_2                                      [585 videos]   \n",
       "paper_3                        [1,500 20-sec video clips]   \n",
       "paper_4             [39,000 videos and 117,000 v-patches]   \n",
       "paper_5         [153,841 videos, 1,596 densely annotated]   \n",
       "paper_6                        [1 million YouTube videos]   \n",
       "paper_7  [At least 400 clips for each of the 400 actions]   \n",
       "\n",
       "                                                         dataset_focus  \\\n",
       "paper_1                        [No-reference video quality assessment]   \n",
       "paper_2                        [No-reference video quality assessment]   \n",
       "paper_3                    [Compression and quality assessment of UGC]   \n",
       "paper_4             [No-reference perceptual video quality assessment]   \n",
       "paper_5  [No-reference video quality assessment of in-the-wild videos]   \n",
       "paper_6                               [Video classification with CNNs]   \n",
       "paper_7                                   [Human action video dataset]   \n",
       "\n",
       "                                              content_diversity  \\\n",
       "paper_1          [Variety of content and authentic distortions]   \n",
       "paper_2    [Unique contents, capture devices, distortion types]   \n",
       "paper_3  [Popular categories like Gaming, Sports, HDR features]   \n",
       "paper_4   [Real-world distorted videos and local video patches]   \n",
       "paper_5  [Substantially larger and diverse, in-the-wild videos]   \n",
       "paper_6   [Large-scale video classification across 487 classes]   \n",
       "paper_7                 [400 human action classes, broad range]   \n",
       "\n",
       "                                                                data_collection_methodology  \\\n",
       "paper_1                                                            [Subjectively annotated]   \n",
       "paper_2                       [Crowdsourcing with 4776 participants, 205000 opinion scores]   \n",
       "paper_3                                          [Sampling based on features from encoding]   \n",
       "paper_4                                         [5.5M human perceptual quality annotations]   \n",
       "paper_5  [Coarsely annotated with five quality ratings each, some with at least 89 ratings]   \n",
       "paper_6                                             [Empirical evaluation on a new dataset]   \n",
       "paper_7                              [Detailed collection description and baseline figures]   \n",
       "\n",
       "                                                                                        UGC_focus  \\\n",
       "paper_1                                                                [No specific focus on UGC]   \n",
       "paper_2                                                                [No specific focus on UGC]   \n",
       "paper_3                                                  [UGC compression and quality evaluation]   \n",
       "paper_4                                                              [Emphasis on UGC video data]   \n",
       "paper_5                                                                [Focus on in-the-wild UGC]   \n",
       "paper_6                                    [Videos from YouTube, UGC mentioned but not the focus]   \n",
       "paper_7  [YouTube videos are source, but focus is on human actions rather than UGC as a category]   \n",
       "\n",
       "                                        reference_to_real_world_conditions  \n",
       "paper_1                              ['In the wild' authentic distortions]  \n",
       "paper_2                 [Real-world video data with authentic distortions]  \n",
       "paper_3                              [Non-pristine originals from YouTube]  \n",
       "paper_4                                        [Real-world UGC video data]  \n",
       "paper_5                                               [In-the-wild videos]  \n",
       "paper_6               [Use of YouTube videos implies real-world relevance]  \n",
       "paper_7  [YouTube videos, but real-world conditions not explicitly stated]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table_0_total['cc_to_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99521df7-4480-4239-a19e-a3a0fe9d5eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Focus of VQA Database</th>\n",
       "      <th>Methodology for VQA</th>\n",
       "      <th>Dataset Contribution</th>\n",
       "      <th>Content and Distortion Types</th>\n",
       "      <th>Model or Metric Proposed</th>\n",
       "      <th>Use of Authentic or User-Generated Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>['in the wild' videos, a wide variety of content and authentic distortions]</td>\n",
       "      <td>[Subjective VQA database creation]</td>\n",
       "      <td>[1,200 public-domain video sequences with subjective mean opinion scores (MOS)]</td>\n",
       "      <td>[Authentic distortions]</td>\n",
       "      <td>[None, database for development of future models]</td>\n",
       "      <td>[Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>[Unique content and wide range of video impairments including complex, authentic distortions]</td>\n",
       "      <td>[Subjective VQA database with crowdsourced opinion scores and a study of leading NR video quality predictors]</td>\n",
       "      <td>[Large-scale VQA database with 585 videos and over 205,000 opinion scores]</td>\n",
       "      <td>[Complex, authentic distortions]</td>\n",
       "      <td>[NR video quality predictors tested and comparison conducted]</td>\n",
       "      <td>[Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>[Video compression research for UGC, various categories like Gaming, Sports, and HDR features]</td>\n",
       "      <td>[UGC dataset sampling based on features extracted from encoding, and UGC quality evaluation]</td>\n",
       "      <td>[Large scale UGC dataset (1500 20 sec clips) sampled from YouTube videos]</td>\n",
       "      <td>[Variety of content like Gaming, Sports, HDR]</td>\n",
       "      <td>[No-reference objective quality metrics evaluation for UGC]</td>\n",
       "      <td>[Yes, with focus on UGC for compression and quality assessment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>[Space-time localized video patches ('v-patches'), real-world distorted videos]</td>\n",
       "      <td>[Local-to-global NR VQA architecture, space-time video quality mapping engine]</td>\n",
       "      <td>[Largest subjective video quality dataset with 39,000 videos, 117,000 v-patches, and 5.5M quality annotations]</td>\n",
       "      <td>[Real-world distorted videos]</td>\n",
       "      <td>[Local-to-global region-based NR VQA architecture (PVQ), space-time video quality mapping engine (PVQ Mapper)]</td>\n",
       "      <td>[Yes, focusing on real-world UGC imperfections]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>[Large and diverse VQA dataset with in-the-wild videos, coarsely and finely annotated]</td>\n",
       "      <td>[Introduction of MLSP-VQA approaches for in-the-wild videos training at scale]</td>\n",
       "      <td>[KonVid-150k dataset with 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each]</td>\n",
       "      <td>[VQA for authentically distorted videos in-the-wild]</td>\n",
       "      <td>[MLSP-VQA approaches with multi-level spatially pooled deep-features]</td>\n",
       "      <td>[Yes, in-the-wild VQA dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>[Large-scale video classification, not focused on video quality assessment]</td>\n",
       "      <td>[Empirical evaluation of CNNs for video classification]</td>\n",
       "      <td>[Dataset of 1 million YouTube videos across 487 classes for video classification]</td>\n",
       "      <td>[Diverse YouTube video content for classification]</td>\n",
       "      <td>[Exploration of CNN architectures for local spatio-temporal information]</td>\n",
       "      <td>[Yes, 1 million YouTube videos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_7</th>\n",
       "      <td>[Human action classification, not focused on video quality assessment]</td>\n",
       "      <td>[Dataset for neural networks trained on human action classification]</td>\n",
       "      <td>[400 human action classes with at least 400 video clips for each action]</td>\n",
       "      <td>[Human action videos]</td>\n",
       "      <td>[Neural network architectures for human action classification]</td>\n",
       "      <td>[Yes, YouTube video clips for human actions]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  Focus of VQA Database  \\\n",
       "paper_1                     ['in the wild' videos, a wide variety of content and authentic distortions]   \n",
       "paper_2   [Unique content and wide range of video impairments including complex, authentic distortions]   \n",
       "paper_3  [Video compression research for UGC, various categories like Gaming, Sports, and HDR features]   \n",
       "paper_4                 [Space-time localized video patches ('v-patches'), real-world distorted videos]   \n",
       "paper_5          [Large and diverse VQA dataset with in-the-wild videos, coarsely and finely annotated]   \n",
       "paper_6                     [Large-scale video classification, not focused on video quality assessment]   \n",
       "paper_7                          [Human action classification, not focused on video quality assessment]   \n",
       "\n",
       "                                                                                                   Methodology for VQA  \\\n",
       "paper_1                                                                             [Subjective VQA database creation]   \n",
       "paper_2  [Subjective VQA database with crowdsourced opinion scores and a study of leading NR video quality predictors]   \n",
       "paper_3                   [UGC dataset sampling based on features extracted from encoding, and UGC quality evaluation]   \n",
       "paper_4                                 [Local-to-global NR VQA architecture, space-time video quality mapping engine]   \n",
       "paper_5                                 [Introduction of MLSP-VQA approaches for in-the-wild videos training at scale]   \n",
       "paper_6                                                        [Empirical evaluation of CNNs for video classification]   \n",
       "paper_7                                           [Dataset for neural networks trained on human action classification]   \n",
       "\n",
       "                                                                                                                   Dataset Contribution  \\\n",
       "paper_1                                                 [1,200 public-domain video sequences with subjective mean opinion scores (MOS)]   \n",
       "paper_2                                                      [Large-scale VQA database with 585 videos and over 205,000 opinion scores]   \n",
       "paper_3                                                       [Large scale UGC dataset (1500 20 sec clips) sampled from YouTube videos]   \n",
       "paper_4                  [Largest subjective video quality dataset with 39,000 videos, 117,000 v-patches, and 5.5M quality annotations]   \n",
       "paper_5  [KonVid-150k dataset with 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each]   \n",
       "paper_6                                               [Dataset of 1 million YouTube videos across 487 classes for video classification]   \n",
       "paper_7                                                        [400 human action classes with at least 400 video clips for each action]   \n",
       "\n",
       "                                 Content and Distortion Types  \\\n",
       "paper_1                               [Authentic distortions]   \n",
       "paper_2                      [Complex, authentic distortions]   \n",
       "paper_3         [Variety of content like Gaming, Sports, HDR]   \n",
       "paper_4                         [Real-world distorted videos]   \n",
       "paper_5  [VQA for authentically distorted videos in-the-wild]   \n",
       "paper_6    [Diverse YouTube video content for classification]   \n",
       "paper_7                                 [Human action videos]   \n",
       "\n",
       "                                                                                               Model or Metric Proposed  \\\n",
       "paper_1                                                               [None, database for development of future models]   \n",
       "paper_2                                                   [NR video quality predictors tested and comparison conducted]   \n",
       "paper_3                                                     [No-reference objective quality metrics evaluation for UGC]   \n",
       "paper_4  [Local-to-global region-based NR VQA architecture (PVQ), space-time video quality mapping engine (PVQ Mapper)]   \n",
       "paper_5                                           [MLSP-VQA approaches with multi-level spatially pooled deep-features]   \n",
       "paper_6                                        [Exploration of CNN architectures for local spatio-temporal information]   \n",
       "paper_7                                                  [Neural network architectures for human action classification]   \n",
       "\n",
       "                              Use of Authentic or User-Generated Content  \n",
       "paper_1                                                            [Yes]  \n",
       "paper_2                                                            [Yes]  \n",
       "paper_3  [Yes, with focus on UGC for compression and quality assessment]  \n",
       "paper_4                  [Yes, focusing on real-world UGC imperfections]  \n",
       "paper_5                                   [Yes, in-the-wild VQA dataset]  \n",
       "paper_6                                  [Yes, 1 million YouTube videos]  \n",
       "paper_7                     [Yes, YouTube video clips for human actions]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table_0_baseline['cc_to_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be6aaf90-71a1-4ff8-a770-426bb2c191ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>what problem does this paper tackle?</th>\n",
       "      <th>what is the approach this paper proposed?</th>\n",
       "      <th>What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?</th>\n",
       "      <th>How does the paper propose to overcome the lack of large VQA databases?</th>\n",
       "      <th>What methods were used in the paper to construct and validate a new VQA database?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_0</th>\n",
       "      <td>[The paper addresses the lack of large VQA databases with real-world video sequences and subjective MOS.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[VQA is video quality assessment, and a large, diverse database aids in developing and evaluating objective VQA methods.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>[The paper addresses the problem of no-reference video quality models' poor performance on diverse real-world video data.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>[The paper tackles the shortcomings of traditional metrics for UGC video compression and quality assessment.]</td>\n",
       "      <td>[The approach is the introduction of a large scale UGC dataset to improve video compression research.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[The approach is a ‘local-to-global’ region-based NR VQA architecture and spatio-temporal quality mapping engine.]</td>\n",
       "      <td>[VQA is Video Quality Assessment; large real-world database with MOS needed for accurate \"in-the-wild\" content quality prediction.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[The paper used subjective annotations to create a large video quality dataset and developed NR-VQA models.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>[The paper addresses the lack of large, diverse datasets for no-reference video quality assessment in-the-wild.]</td>\n",
       "      <td>[The approach proposed is a new dataset, KonVid-150k, and efficient VQA methods using multi-level spatially pooled deep-features (MLSP).]</td>\n",
       "      <td>[VQA is Video Quality Assessment, and a large, diverse dataset with subjective Mean Opinion Scores (MOS) aids in developing robust models for real-world applications.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>[The paper tackles large-scale video classification using CNNs and performance improvements over baselines.]</td>\n",
       "      <td>[The paper proposes using CNNs with extended time domain connectivity for large-scale video classification.]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>[initial]</td>\n",
       "      <td>[initial]</td>\n",
       "      <td>[followup]</td>\n",
       "      <td>[followup]</td>\n",
       "      <td>[followup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presup</th>\n",
       "      <td>[Presupposition: This paper tackles a problem.]</td>\n",
       "      <td>[The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.]</td>\n",
       "      <td>[Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.]</td>\n",
       "      <td>[The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.]</td>\n",
       "      <td>[Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               what problem does this paper tackle?  \\\n",
       "paper_0                   [The paper addresses the lack of large VQA databases with real-world video sequences and subjective MOS.]   \n",
       "paper_1  [The paper addresses the problem of no-reference video quality models' poor performance on diverse real-world video data.]   \n",
       "paper_2               [The paper tackles the shortcomings of traditional metrics for UGC video compression and quality assessment.]   \n",
       "paper_3                                                                                                                          []   \n",
       "paper_4            [The paper addresses the lack of large, diverse datasets for no-reference video quality assessment in-the-wild.]   \n",
       "paper_5                [The paper tackles large-scale video classification using CNNs and performance improvements over baselines.]   \n",
       "paper_6                                                                                                                          []   \n",
       "type                                                                                                                      [initial]   \n",
       "presup                                                                              [Presupposition: This paper tackles a problem.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                what is the approach this paper proposed?  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                []   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                []   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [The approach is the introduction of a large scale UGC dataset to improve video compression research.]   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [The approach is a ‘local-to-global’ region-based NR VQA architecture and spatio-temporal quality mapping engine.]   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [The approach proposed is a new dataset, KonVid-150k, and efficient VQA methods using multi-level spatially pooled deep-features (MLSP).]   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [The paper proposes using CNNs with extended time domain connectivity for large-scale video classification.]   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                []   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [initial]   \n",
       "presup   [The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [VQA is video quality assessment, and a large, diverse database aids in developing and evaluating objective VQA methods.]   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              []   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              []   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [VQA is Video Quality Assessment; large real-world database with MOS needed for accurate \"in-the-wild\" content quality prediction.]   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [VQA is Video Quality Assessment, and a large, diverse dataset with subjective Mean Opinion Scores (MOS) aids in developing robust models for real-world applications.]   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              []   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              []   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [followup]   \n",
       "presup   [Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.]   \n",
       "\n",
       "                                     How does the paper propose to overcome the lack of large VQA databases?  \\\n",
       "paper_0                                                                                                   []   \n",
       "paper_1                                                                                                   []   \n",
       "paper_2                                                                                                   []   \n",
       "paper_3                                                                                                   []   \n",
       "paper_4                                                                                                   []   \n",
       "paper_5                                                                                                   []   \n",
       "paper_6                                                                                                   []   \n",
       "type                                                                                              [followup]   \n",
       "presup   [The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.]   \n",
       "\n",
       "                                                   What methods were used in the paper to construct and validate a new VQA database?  \n",
       "paper_0                                                                                                                           []  \n",
       "paper_1                                                                                                                           []  \n",
       "paper_2                                                                                                                           []  \n",
       "paper_3                 [The paper used subjective annotations to create a large video quality dataset and developed NR-VQA models.]  \n",
       "paper_4                                                                                                                           []  \n",
       "paper_5                                                                                                                           []  \n",
       "paper_6                                                                                                                           []  \n",
       "type                                                                                                                      [followup]  \n",
       "presup   [Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_ours_df = pd.DataFrame(table_0_ours['final_table'])\n",
    "table_0_ours_df[table_0_ours_df.columns[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f81951c6-fc0a-4193-8e1c-5aefc896bc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_0</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>question_3</th>\n",
       "      <th>question_4</th>\n",
       "      <th>question_5</th>\n",
       "      <th>question_6</th>\n",
       "      <th>question_7</th>\n",
       "      <th>question_8</th>\n",
       "      <th>question_9</th>\n",
       "      <th>question_10</th>\n",
       "      <th>question_11</th>\n",
       "      <th>question_12</th>\n",
       "      <th>question_13</th>\n",
       "      <th>question_14</th>\n",
       "      <th>question_15</th>\n",
       "      <th>question_16</th>\n",
       "      <th>question_17</th>\n",
       "      <th>question_18</th>\n",
       "      <th>question_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper_0</th>\n",
       "      <td>The paper addresses the lack of large VQA databases with real-world video sequences and subjective MOS.</td>\n",
       "      <td></td>\n",
       "      <td>VQA is video quality assessment, and a large, diverse database aids in developing and evaluating objective VQA methods.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The issue is the lack of large, diverse datasets with real-world distortions for video quality assessment.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>The paper addresses the problem of no-reference video quality models' poor performance on diverse real-world video data.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Performance measured by comparing no-reference video quality predictors on a new large-scale video quality database with crowdsourced scores.</td>\n",
       "      <td></td>\n",
       "      <td>The issue is current no-reference video quality models' inability to handle diverse video impairments.</td>\n",
       "      <td>No-reference video quality models struggle with diverse, authentic, commingled distortions in real-world videos.</td>\n",
       "      <td></td>\n",
       "      <td>The paper proposes a large-scale video quality database to enhance no-reference video quality models.</td>\n",
       "      <td></td>\n",
       "      <td>The paper addresses the inadequacy of existing databases to represent real-world video diversity and authentic distortions.</td>\n",
       "      <td>The focus is on improving no-reference video quality prediction models for diverse real-world conditions.</td>\n",
       "      <td>The paper addresses shortcomings in current no-reference video quality models due to limited databases.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>The paper tackles the shortcomings of traditional metrics for UGC video compression and quality assessment.</td>\n",
       "      <td>The approach is the introduction of a large scale UGC dataset to improve video compression research.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper focuses on the issue of assessing and improving compression and quality metrics for UGC videos.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>No-reference video quality models struggle with User Generated Content (UGC) that is non-pristine.</td>\n",
       "      <td>The paper considers BD-Rate and PSNR inadequate for UGC video compression and quality assessment.</td>\n",
       "      <td>The paper proposes using no-reference objective quality metrics to assess UGC video quality for compression.</td>\n",
       "      <td>The paper introduces no-reference objective quality metrics for evaluating UGC video compression and quality.</td>\n",
       "      <td>The paper addresses shortcomings of traditional metrics like BD-Rate and PSNR on non-pristine originals in UGC.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td></td>\n",
       "      <td>The approach is a ‘local-to-global’ region-based NR VQA architecture and spatio-temporal quality mapping engine.</td>\n",
       "      <td>VQA is Video Quality Assessment; large real-world database with MOS needed for accurate \"in-the-wild\" content quality prediction.</td>\n",
       "      <td></td>\n",
       "      <td>The paper used subjective annotations to create a large video quality dataset and developed NR-VQA models.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Performance is measured by a subjective video quality dataset with human perceptual quality annotations.</td>\n",
       "      <td></td>\n",
       "      <td>The paper discusses the limited prediction capabilities of current no-reference models on real-world user-generated content video quality.</td>\n",
       "      <td>No-reference video quality models struggle with real-world user-generated content (UGC) video data.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper introduces a local-to-global region-based NR VQA architecture and a space-time video quality mapping engine.</td>\n",
       "      <td></td>\n",
       "      <td>The paper focuses on improving no-reference perceptual video quality assessment for user-generated content.</td>\n",
       "      <td>The paper addresses the shortcomings in no-reference perceptual video quality assessment for user-generated content.</td>\n",
       "      <td>The main problem addressed is the inefficient no-reference perceptual video quality assessment for user-generated content.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>The paper addresses the lack of large, diverse datasets for no-reference video quality assessment in-the-wild.</td>\n",
       "      <td>The approach proposed is a new dataset, KonVid-150k, and efficient VQA methods using multi-level spatially pooled deep-features (MLSP).</td>\n",
       "      <td>VQA is Video Quality Assessment, and a large, diverse dataset with subjective Mean Opinion Scores (MOS) aids in developing robust models for real-world applications.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The focus is on the issue of limited VQA performance on diverse, in-the-wild video content.</td>\n",
       "      <td>The paper highlights a lack of large and diverse in-the-wild VQA datasets.</td>\n",
       "      <td>The paper introduces a large and diverse dataset and efficient Multi-Level Spatially Pooled feature-based methods.</td>\n",
       "      <td>Performance is measured by Spearman rank-order correlation coefficient on KoNViD-1k, with improved scores using MLSP-VQA-FF.</td>\n",
       "      <td></td>\n",
       "      <td>The paper addresses under-performance of traditional VQA methods on diverse, in-the-wild video content.</td>\n",
       "      <td>They struggle with authentically distorted videos \"in-the-wild\".</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper introduces MLSP-VQA approaches for efficient no-reference video quality assessment in-the-wild.</td>\n",
       "      <td>The paper addresses issues with traditional VQA metrics' underperformance on diverse, in-the-wild video content.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>The paper tackles large-scale video classification using CNNs and performance improvements over baselines.</td>\n",
       "      <td>The paper proposes using CNNs with extended time domain connectivity for large-scale video classification.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The focus of the paper is improving large-scale video classification using Convolutional Neural Networks.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Performance improvement measured by comparing CNNs to baselines on a dataset of 1 million YouTube videos.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The paper focuses on enhancing large-scale video classification using Convolutional Neural Networks.</td>\n",
       "      <td>The paper addresses shortcomings in large-scale video classification using CNNs on new extensive datasets.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>what problem does this paper tackle?</td>\n",
       "      <td>what is the approach this paper proposed?</td>\n",
       "      <td>What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?</td>\n",
       "      <td>How does the paper propose to overcome the lack of large VQA databases?</td>\n",
       "      <td>What methods were used in the paper to construct and validate a new VQA database?</td>\n",
       "      <td>What are the potential applications of improving VQA databases as discussed in the paper?</td>\n",
       "      <td>What issue is the focus of this paper?</td>\n",
       "      <td>What is missing in VQA databases according to this paper?</td>\n",
       "      <td>What methodologies does the paper introduce to improve the performance of no-reference video quality models?</td>\n",
       "      <td>How does the paper measure the performance improvement on real-world video data?</td>\n",
       "      <td>What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?</td>\n",
       "      <td>What is the performance issue discussed in the paper concerning video quality models?</td>\n",
       "      <td>What type of video data do the no-reference video quality models struggle with?</td>\n",
       "      <td>What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?</td>\n",
       "      <td>How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?</td>\n",
       "      <td>What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?</td>\n",
       "      <td>What issues with traditional metrics does this paper address?</td>\n",
       "      <td>What is the focus of this paper's problem-solving?</td>\n",
       "      <td>In what context is the paper addressing shortcomings?</td>\n",
       "      <td>What is the main problem addressed by this paper?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>initial</td>\n",
       "      <td>initial</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>followup</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>lowlevel</td>\n",
       "      <td>generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presup</th>\n",
       "      <td>Presupposition: This paper tackles a problem.</td>\n",
       "      <td>The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.</td>\n",
       "      <td>Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.</td>\n",
       "      <td>The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.</td>\n",
       "      <td>Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.</td>\n",
       "      <td>The paper discusses potential applications of improving Visual Question Answering (VQA) databases.</td>\n",
       "      <td>Presupposition: This paper focuses on a specific issue.</td>\n",
       "      <td>Presupposition: This paper identifies something as missing in Visual Question Answering (VQA) databases.</td>\n",
       "      <td>Presupposition:\\n\\n1. The paper introduces methodologies.\\n2. These methodologies are aimed at improving performance.\\n3. The context is no-reference video quality models.\\n4. The performance that is subject to improvement pertains to no-reference video quality models.</td>\n",
       "      <td>The presuppositions underlying the third question are as follows:\\n\\n1. The paper includes a performance measurement or evaluation component.\\n2. The paper's focus, at least in part, involves real-world video data.\\n3. There is an implied performance improvement that the paper investigates or reports.\\n4. There is a specific methodology or metric used in the paper to measure said performance improvement.</td>\n",
       "      <td>Here are the presuppositions of the given question:\\n\\n1. A solution for no-reference video quality assessment is proposed in the paper.\\n2. The paper uses datasets or benchmarks to evaluate the proposed solution.\\n3. No-reference video quality assessment is a topic addressed within the paper.</td>\n",
       "      <td>Presupposition: The paper discusses a performance issue concerning video quality models.</td>\n",
       "      <td>Presupposition: No-reference video quality models struggle with certain types of video data.</td>\n",
       "      <td>- Traditional metrics for UGC video compression and quality assessment are considered by this paper.\\n- The paper finds the existing metrics for UGC video compression and quality assessment to be inadequate.\\n- The paper discusses UGC video compression and quality assessment.\\n- The paper is oriented towards improving or critiquing aspects of UGC video compression and quality assessment.</td>\n",
       "      <td>1. The paper proposes a method or solution for overcoming certain shortcomings.\\n2. The paper is focused on User-Generated Content (UGC) video quality assessment and compression.\\n3. There are known shortcomings in UGC video quality assessment and compression that the paper addresses.\\n4. The subject matter of the paper is relevant to the field of video quality assessment and/or compression.</td>\n",
       "      <td>1. The paper introduces new metrics or methodologies.\\n2. The paper's focus is on evaluating UGC (User-Generated Content) video compression and quality.</td>\n",
       "      <td>This paper addresses issues with traditional metrics.</td>\n",
       "      <td>Presupposition: This paper's focus is on problem-solving.</td>\n",
       "      <td>The paper is addressing some shortcomings.</td>\n",
       "      <td>The presuppositions associated with the question \"What is the main problem addressed by this paper?\" could include:\\n\\n1. The paper addresses a specific problem.\\n2. There is a \"main\" problem, indicating that the paper may address multiple issues, but there is a primary focus.\\n3. The paper is written with the intention to discuss and potentially offer solutions or insights into a problem.\\n4. The term \"this\" indicates that there is a specific paper being referenced, and that paper is accessible to or known by both the person asking and the person answering the question.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        question_0  \\\n",
       "paper_0                    The paper addresses the lack of large VQA databases with real-world video sequences and subjective MOS.   \n",
       "paper_1   The paper addresses the problem of no-reference video quality models' poor performance on diverse real-world video data.   \n",
       "paper_2                The paper tackles the shortcomings of traditional metrics for UGC video compression and quality assessment.   \n",
       "paper_3                                                                                                                              \n",
       "paper_4             The paper addresses the lack of large, diverse datasets for no-reference video quality assessment in-the-wild.   \n",
       "paper_5                 The paper tackles large-scale video classification using CNNs and performance improvements over baselines.   \n",
       "paper_6                                                                                                                              \n",
       "question                                                                                      what problem does this paper tackle?   \n",
       "type                                                                                                                       initial   \n",
       "presup                                                                               Presupposition: This paper tackles a problem.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              question_1  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The approach is the introduction of a large scale UGC dataset to improve video compression research.   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The approach is a ‘local-to-global’ region-based NR VQA architecture and spatio-temporal quality mapping engine.   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The approach proposed is a new dataset, KonVid-150k, and efficient VQA methods using multi-level spatially pooled deep-features (MLSP).   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The paper proposes using CNNs with extended time domain connectivity for large-scale video classification.   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       what is the approach this paper proposed?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             initial   \n",
       "presup    The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            question_2  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        VQA is video quality assessment, and a large, diverse database aids in developing and evaluating objective VQA methods.   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              VQA is Video Quality Assessment; large real-world database with MOS needed for accurate \"in-the-wild\" content quality prediction.   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          VQA is Video Quality Assessment, and a large, diverse dataset with subjective Mean Opinion Scores (MOS) aids in developing robust models for real-world applications.   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          followup   \n",
       "presup    Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.   \n",
       "\n",
       "                                                                                                 question_3  \\\n",
       "paper_0                                                                                                       \n",
       "paper_1                                                                                                       \n",
       "paper_2                                                                                                       \n",
       "paper_3                                                                                                       \n",
       "paper_4                                                                                                       \n",
       "paper_5                                                                                                       \n",
       "paper_6                                                                                                       \n",
       "question                            How does the paper propose to overcome the lack of large VQA databases?   \n",
       "type                                                                                               followup   \n",
       "presup    The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.   \n",
       "\n",
       "                                                                                                                         question_4  \\\n",
       "paper_0                                                                                                                               \n",
       "paper_1                                                                                                                               \n",
       "paper_2                                                                                                                               \n",
       "paper_3                  The paper used subjective annotations to create a large video quality dataset and developed NR-VQA models.   \n",
       "paper_4                                                                                                                               \n",
       "paper_5                                                                                                                               \n",
       "paper_6                                                                                                                               \n",
       "question                                          What methods were used in the paper to construct and validate a new VQA database?   \n",
       "type                                                                                                                       followup   \n",
       "presup    Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.   \n",
       "\n",
       "                                                                                                  question_5  \\\n",
       "paper_0                                                                                                        \n",
       "paper_1                                                                                                        \n",
       "paper_2                                                                                                        \n",
       "paper_3                                                                                                        \n",
       "paper_4                                                                                                        \n",
       "paper_5                                                                                                        \n",
       "paper_6                                                                                                        \n",
       "question           What are the potential applications of improving VQA databases as discussed in the paper?   \n",
       "type                                                                                                followup   \n",
       "presup    The paper discusses potential applications of improving Visual Question Answering (VQA) databases.   \n",
       "\n",
       "                                                                                                         question_6  \\\n",
       "paper_0                                                                                                               \n",
       "paper_1                                                                                                               \n",
       "paper_2   The paper focuses on the issue of assessing and improving compression and quality metrics for UGC videos.   \n",
       "paper_3                                                                                                               \n",
       "paper_4                 The focus is on the issue of limited VQA performance on diverse, in-the-wild video content.   \n",
       "paper_5   The focus of the paper is improving large-scale video classification using Convolutional Neural Networks.   \n",
       "paper_6                                                                                                               \n",
       "question                                                                     What issue is the focus of this paper?   \n",
       "type                                                                                                       lowlevel   \n",
       "presup                                                      Presupposition: This paper focuses on a specific issue.   \n",
       "\n",
       "                                                                                                        question_7  \\\n",
       "paper_0                                                                                                              \n",
       "paper_1                                                                                                              \n",
       "paper_2                                                                                                              \n",
       "paper_3                                                                                                              \n",
       "paper_4                                 The paper highlights a lack of large and diverse in-the-wild VQA datasets.   \n",
       "paper_5                                                                                                              \n",
       "paper_6                                                                                                              \n",
       "question                                                 What is missing in VQA databases according to this paper?   \n",
       "type                                                                                                      lowlevel   \n",
       "presup    Presupposition: This paper identifies something as missing in Visual Question Answering (VQA) databases.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                             question_8  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                   \n",
       "paper_1                                                                                                                                                                                                                                                                                   \n",
       "paper_2                                                                                                                                                                                                                                                                                   \n",
       "paper_3                                                                                                                                                                                                                                                                                   \n",
       "paper_4                                                                                                                                                              The paper introduces a large and diverse dataset and efficient Multi-Level Spatially Pooled feature-based methods.   \n",
       "paper_5                                                                                                                                                                                                                                                                                   \n",
       "paper_6                                                                                                                                                                                                                                                                                   \n",
       "question                                                                                                                                                                   What methodologies does the paper introduce to improve the performance of no-reference video quality models?   \n",
       "type                                                                                                                                                                                                                                                                           followup   \n",
       "presup    Presupposition:\\n\\n1. The paper introduces methodologies.\\n2. These methodologies are aimed at improving performance.\\n3. The context is no-reference video quality models.\\n4. The performance that is subject to improvement pertains to no-reference video quality models.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                       question_9  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "paper_1                                                                                                                                                                                                                                                                             Performance measured by comparing no-reference video quality predictors on a new large-scale video quality database with crowdsourced scores.   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "paper_3                                                                                                                                                                                                                                                                                                                  Performance is measured by a subjective video quality dataset with human perceptual quality annotations.   \n",
       "paper_4                                                                                                                                                                                                                                                                                              Performance is measured by Spearman rank-order correlation coefficient on KoNViD-1k, with improved scores using MLSP-VQA-FF.   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                 Performance improvement measured by comparing CNNs to baselines on a dataset of 1 million YouTube videos.   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "question                                                                                                                                                                                                                                                                                                                                         How does the paper measure the performance improvement on real-world video data?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                     followup   \n",
       "presup    The presuppositions underlying the third question are as follows:\\n\\n1. The paper includes a performance measurement or evaluation component.\\n2. The paper's focus, at least in part, involves real-world video data.\\n3. There is an implied performance improvement that the paper investigates or reports.\\n4. There is a specific methodology or metric used in the paper to measure said performance improvement.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     question_10  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                            \n",
       "paper_1                                                                                                                                                                                                                                                                                                            \n",
       "paper_2                                                                                                                                                                                                                                                                                                            \n",
       "paper_3                                                                                                                                                                                                                                                                                                            \n",
       "paper_4                                                                                                                                                                                                                                                                                                            \n",
       "paper_5                                                                                                                                                                                                                                                                                                            \n",
       "paper_6                                                                                                                                                                                                                                                                                                            \n",
       "question                                                                                                                                                                          What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?   \n",
       "type                                                                                                                                                                                                                                                                                                    followup   \n",
       "presup    Here are the presuppositions of the given question:\\n\\n1. A solution for no-reference video quality assessment is proposed in the paper.\\n2. The paper uses datasets or benchmarks to evaluate the proposed solution.\\n3. No-reference video quality assessment is a topic addressed within the paper.   \n",
       "\n",
       "                                                                                                                                         question_11  \\\n",
       "paper_0                                   The issue is the lack of large, diverse datasets with real-world distortions for video quality assessment.   \n",
       "paper_1                                       The issue is current no-reference video quality models' inability to handle diverse video impairments.   \n",
       "paper_2                                                                                                                                                \n",
       "paper_3   The paper discusses the limited prediction capabilities of current no-reference models on real-world user-generated content video quality.   \n",
       "paper_4                                      The paper addresses under-performance of traditional VQA methods on diverse, in-the-wild video content.   \n",
       "paper_5                                                                                                                                                \n",
       "paper_6                                                                                                                                                \n",
       "question                                                       What is the performance issue discussed in the paper concerning video quality models?   \n",
       "type                                                                                                                                        lowlevel   \n",
       "presup                                                      Presupposition: The paper discusses a performance issue concerning video quality models.   \n",
       "\n",
       "                                                                                                               question_12  \\\n",
       "paper_0                                                                                                                      \n",
       "paper_1   No-reference video quality models struggle with diverse, authentic, commingled distortions in real-world videos.   \n",
       "paper_2                 No-reference video quality models struggle with User Generated Content (UGC) that is non-pristine.   \n",
       "paper_3                No-reference video quality models struggle with real-world user-generated content (UGC) video data.   \n",
       "paper_4                                                   They struggle with authentically distorted videos \"in-the-wild\".   \n",
       "paper_5                                                                                                                      \n",
       "paper_6                                                                                                                      \n",
       "question                                   What type of video data do the no-reference video quality models struggle with?   \n",
       "type                                                                                                              lowlevel   \n",
       "presup                        Presupposition: No-reference video quality models struggle with certain types of video data.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     question_13  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_2                                                                                                                                                                                                                                                                                                        The paper considers BD-Rate and PSNR inadequate for UGC video compression and quality assessment.   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "question                                                                                                                                                                                                                                                             What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                    followup   \n",
       "presup    - Traditional metrics for UGC video compression and quality assessment are considered by this paper.\\n- The paper finds the existing metrics for UGC video compression and quality assessment to be inadequate.\\n- The paper discusses UGC video compression and quality assessment.\\n- The paper is oriented towards improving or critiquing aspects of UGC video compression and quality assessment.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                         question_14  \\\n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_1                                                                                                                                                                                                                                                                                                        The paper proposes a large-scale video quality database to enhance no-reference video quality models.   \n",
       "paper_2                                                                                                                                                                                                                                                                                                 The paper proposes using no-reference objective quality metrics to assess UGC video quality for compression.   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "question                                                                                                                                                                                                                                                                                                  How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?   \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                        followup   \n",
       "presup    1. The paper proposes a method or solution for overcoming certain shortcomings.\\n2. The paper is focused on User-Generated Content (UGC) video quality assessment and compression.\\n3. There are known shortcomings in UGC video quality assessment and compression that the paper addresses.\\n4. The subject matter of the paper is relevant to the field of video quality assessment and/or compression.   \n",
       "\n",
       "                                                                                                                                                       question_15  \\\n",
       "paper_0                                                                                                                                                              \n",
       "paper_1                                                                                                                                                              \n",
       "paper_2                                              The paper introduces no-reference objective quality metrics for evaluating UGC video compression and quality.   \n",
       "paper_3                                     The paper introduces a local-to-global region-based NR VQA architecture and a space-time video quality mapping engine.   \n",
       "paper_4                                                  The paper introduces MLSP-VQA approaches for efficient no-reference video quality assessment in-the-wild.   \n",
       "paper_5                                                                                                                                                              \n",
       "paper_6                                                                                                                                                              \n",
       "question                                              What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?   \n",
       "type                                                                                                                                                      followup   \n",
       "presup    1. The paper introduces new metrics or methodologies.\\n2. The paper's focus is on evaluating UGC (User-Generated Content) video compression and quality.   \n",
       "\n",
       "                                                                                                                          question_16  \\\n",
       "paper_0                                                                                                                                 \n",
       "paper_1   The paper addresses the inadequacy of existing databases to represent real-world video diversity and authentic distortions.   \n",
       "paper_2               The paper addresses shortcomings of traditional metrics like BD-Rate and PSNR on non-pristine originals in UGC.   \n",
       "paper_3                                                                                                                                 \n",
       "paper_4              The paper addresses issues with traditional VQA metrics' underperformance on diverse, in-the-wild video content.   \n",
       "paper_5                                                                                                                                 \n",
       "paper_6                                                                                                                                 \n",
       "question                                                                What issues with traditional metrics does this paper address?   \n",
       "type                                                                                                                         lowlevel   \n",
       "presup                                                                          This paper addresses issues with traditional metrics.   \n",
       "\n",
       "                                                                                                          question_17  \\\n",
       "paper_0                                                                                                                 \n",
       "paper_1     The focus is on improving no-reference video quality prediction models for diverse real-world conditions.   \n",
       "paper_2                                                                                                                 \n",
       "paper_3   The paper focuses on improving no-reference perceptual video quality assessment for user-generated content.   \n",
       "paper_4                                                                                                                 \n",
       "paper_5          The paper focuses on enhancing large-scale video classification using Convolutional Neural Networks.   \n",
       "paper_6                                                                                                                 \n",
       "question                                                           What is the focus of this paper's problem-solving?   \n",
       "type                                                                                                         lowlevel   \n",
       "presup                                                      Presupposition: This paper's focus is on problem-solving.   \n",
       "\n",
       "                                                                                                                   question_18  \\\n",
       "paper_0                                                                                                                          \n",
       "paper_1                The paper addresses shortcomings in current no-reference video quality models due to limited databases.   \n",
       "paper_2                                                                                                                          \n",
       "paper_3   The paper addresses the shortcomings in no-reference perceptual video quality assessment for user-generated content.   \n",
       "paper_4                                                                                                                          \n",
       "paper_5             The paper addresses shortcomings in large-scale video classification using CNNs on new extensive datasets.   \n",
       "paper_6                                                                                                                          \n",
       "question                                                                 In what context is the paper addressing shortcomings?   \n",
       "type                                                                                                                  lowlevel   \n",
       "presup                                                                              The paper is addressing some shortcomings.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                question_19  \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The main problem addressed is the inefficient no-reference perceptual video quality assessment for user-generated content.  \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  What is the main problem addressed by this paper?  \n",
       "type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                generic  \n",
       "presup    The presuppositions associated with the question \"What is the main problem addressed by this paper?\" could include:\\n\\n1. The paper addresses a specific problem.\\n2. There is a \"main\" problem, indicating that the paper may address multiple issues, but there is a primary focus.\\n3. The paper is written with the intention to discuss and potentially offer solutions or insights into a problem.\\n4. The term \"this\" indicates that there is a specific paper being referenced, and that paper is accessible to or known by both the person asking and the person answering the question.  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table_0_ours['table_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ed29ec5-c335-4835-81dc-2f9d2bd94d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>presup_0</th>\n",
       "      <th>presup_1</th>\n",
       "      <th>presup_2</th>\n",
       "      <th>presup_3</th>\n",
       "      <th>presup_4</th>\n",
       "      <th>presup_5</th>\n",
       "      <th>presup_6</th>\n",
       "      <th>presup_7</th>\n",
       "      <th>presup_8</th>\n",
       "      <th>presup_9</th>\n",
       "      <th>presup_10</th>\n",
       "      <th>presup_11</th>\n",
       "      <th>presup_12</th>\n",
       "      <th>presup_13</th>\n",
       "      <th>presup_14</th>\n",
       "      <th>presup_15</th>\n",
       "      <th>presup_16</th>\n",
       "      <th>presup_17</th>\n",
       "      <th>presup_18</th>\n",
       "      <th>presup_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>what problem does this paper tackle?</td>\n",
       "      <td>what is the approach this paper proposed?</td>\n",
       "      <td>What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?</td>\n",
       "      <td>How does the paper propose to overcome the lack of large VQA databases?</td>\n",
       "      <td>What methods were used in the paper to construct and validate a new VQA database?</td>\n",
       "      <td>What are the potential applications of improving VQA databases as discussed in the paper?</td>\n",
       "      <td>What issue is the focus of this paper?</td>\n",
       "      <td>What is missing in VQA databases according to this paper?</td>\n",
       "      <td>What methodologies does the paper introduce to improve the performance of no-reference video quality models?</td>\n",
       "      <td>How does the paper measure the performance improvement on real-world video data?</td>\n",
       "      <td>What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?</td>\n",
       "      <td>What is the performance issue discussed in the paper concerning video quality models?</td>\n",
       "      <td>What type of video data do the no-reference video quality models struggle with?</td>\n",
       "      <td>What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?</td>\n",
       "      <td>How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?</td>\n",
       "      <td>What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?</td>\n",
       "      <td>What issues with traditional metrics does this paper address?</td>\n",
       "      <td>What is the focus of this paper's problem-solving?</td>\n",
       "      <td>In what context is the paper addressing shortcomings?</td>\n",
       "      <td>What is the main problem addressed by this paper?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presup</th>\n",
       "      <td>Presupposition: This paper tackles a problem.</td>\n",
       "      <td>The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.</td>\n",
       "      <td>Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.</td>\n",
       "      <td>The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.</td>\n",
       "      <td>Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.</td>\n",
       "      <td>The paper discusses potential applications of improving Visual Question Answering (VQA) databases.</td>\n",
       "      <td>Presupposition: This paper focuses on a specific issue.</td>\n",
       "      <td>Presupposition: This paper identifies something as missing in Visual Question Answering (VQA) databases.</td>\n",
       "      <td>Presupposition:\\n\\n1. The paper introduces methodologies.\\n2. These methodologies are aimed at improving performance.\\n3. The context is no-reference video quality models.\\n4. The performance that is subject to improvement pertains to no-reference video quality models.</td>\n",
       "      <td>The presuppositions underlying the third question are as follows:\\n\\n1. The paper includes a performance measurement or evaluation component.\\n2. The paper's focus, at least in part, involves real-world video data.\\n3. There is an implied performance improvement that the paper investigates or reports.\\n4. There is a specific methodology or metric used in the paper to measure said performance improvement.</td>\n",
       "      <td>Here are the presuppositions of the given question:\\n\\n1. A solution for no-reference video quality assessment is proposed in the paper.\\n2. The paper uses datasets or benchmarks to evaluate the proposed solution.\\n3. No-reference video quality assessment is a topic addressed within the paper.</td>\n",
       "      <td>Presupposition: The paper discusses a performance issue concerning video quality models.</td>\n",
       "      <td>Presupposition: No-reference video quality models struggle with certain types of video data.</td>\n",
       "      <td>- Traditional metrics for UGC video compression and quality assessment are considered by this paper.\\n- The paper finds the existing metrics for UGC video compression and quality assessment to be inadequate.\\n- The paper discusses UGC video compression and quality assessment.\\n- The paper is oriented towards improving or critiquing aspects of UGC video compression and quality assessment.</td>\n",
       "      <td>1. The paper proposes a method or solution for overcoming certain shortcomings.\\n2. The paper is focused on User-Generated Content (UGC) video quality assessment and compression.\\n3. There are known shortcomings in UGC video quality assessment and compression that the paper addresses.\\n4. The subject matter of the paper is relevant to the field of video quality assessment and/or compression.</td>\n",
       "      <td>1. The paper introduces new metrics or methodologies.\\n2. The paper's focus is on evaluating UGC (User-Generated Content) video compression and quality.</td>\n",
       "      <td>This paper addresses issues with traditional metrics.</td>\n",
       "      <td>Presupposition: This paper's focus is on problem-solving.</td>\n",
       "      <td>The paper is addressing some shortcomings.</td>\n",
       "      <td>The presuppositions associated with the question \"What is the main problem addressed by this paper?\" could include:\\n\\n1. The paper addresses a specific problem.\\n2. There is a \"main\" problem, indicating that the paper may address multiple issues, but there is a primary focus.\\n3. The paper is written with the intention to discuss and potentially offer solutions or insights into a problem.\\n4. The term \"this\" indicates that there is a specific paper being referenced, and that paper is accessible to or known by both the person asking and the person answering the question.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               presup_0  \\\n",
       "question           what problem does this paper tackle?   \n",
       "presup    Presupposition: This paper tackles a problem.   \n",
       "paper_0                                            True   \n",
       "paper_1                                            True   \n",
       "paper_2                                            True   \n",
       "paper_3                                           False   \n",
       "paper_4                                            True   \n",
       "paper_5                                            True   \n",
       "paper_6                                           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                presup_1  \\\n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       what is the approach this paper proposed?   \n",
       "presup    The presupposition for the question \"What is the approach this paper proposed?\" is that the paper proposes an approach. Specifically, the presupposition can be broken down into several sub-assumptions:\\n\\n1. There exists a paper related to the question being asked.\\n2. The paper makes a proposition or provides a solution or methodology.\\n3. The paper's content is relevant to a specific issue, problem, or area of study for which an approach is meaningful and expected.\\n4. The paper's proposed approach is a central aspect or key contribution of the paper, making it a suitable subject for the question.   \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            False   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            False   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             True   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             True   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             True   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             True   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              presup_2  \\\n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?   \n",
       "presup    Presuppositions:\\n\\n1. VQA stands for a concept or a technique that requires explanation.\\n2. There exists a thing called VQA that is relevant to the context of the discussion.\\n3. There is a significant relationship between VQA, a large database with real-world video sequences, and subjective MOS (Mean Opinion Scores).\\n4. The large database with real-world video sequences and subjective MOS is something that does not currently exist in a satisfactory state or needs improvement for the progress of VQA.\\n5. The progress of VQA is contingent upon or can be advanced by the presence of such a database.\\n6. The paper in question discusses or addresses VQA and the relevance or necessity of a database and MOS to its advancement.   \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           True   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          False   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          False   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           True   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           True   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          False   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          False   \n",
       "\n",
       "                                                                                                   presup_3  \\\n",
       "question                            How does the paper propose to overcome the lack of large VQA databases?   \n",
       "presup    The paper proposes a way to overcome the lack of large visual question answering (VQA) databases.   \n",
       "paper_0                                                                                               False   \n",
       "paper_1                                                                                               False   \n",
       "paper_2                                                                                               False   \n",
       "paper_3                                                                                               False   \n",
       "paper_4                                                                                               False   \n",
       "paper_5                                                                                               False   \n",
       "paper_6                                                                                               False   \n",
       "\n",
       "                                                                                                                           presup_4  \\\n",
       "question                                          What methods were used in the paper to construct and validate a new VQA database?   \n",
       "presup    Presupposition: The paper constructed and validated a new Visual Question Answering (VQA) database using certain methods.   \n",
       "paper_0                                                                                                                       False   \n",
       "paper_1                                                                                                                       False   \n",
       "paper_2                                                                                                                       False   \n",
       "paper_3                                                                                                                        True   \n",
       "paper_4                                                                                                                       False   \n",
       "paper_5                                                                                                                       False   \n",
       "paper_6                                                                                                                       False   \n",
       "\n",
       "                                                                                                    presup_5  \\\n",
       "question           What are the potential applications of improving VQA databases as discussed in the paper?   \n",
       "presup    The paper discusses potential applications of improving Visual Question Answering (VQA) databases.   \n",
       "paper_0                                                                                                False   \n",
       "paper_1                                                                                                False   \n",
       "paper_2                                                                                                False   \n",
       "paper_3                                                                                                False   \n",
       "paper_4                                                                                                False   \n",
       "paper_5                                                                                                False   \n",
       "paper_6                                                                                                False   \n",
       "\n",
       "                                                         presup_6  \\\n",
       "question                   What issue is the focus of this paper?   \n",
       "presup    Presupposition: This paper focuses on a specific issue.   \n",
       "paper_0                                                     False   \n",
       "paper_1                                                     False   \n",
       "paper_2                                                      True   \n",
       "paper_3                                                     False   \n",
       "paper_4                                                      True   \n",
       "paper_5                                                      True   \n",
       "paper_6                                                     False   \n",
       "\n",
       "                                                                                                          presup_7  \\\n",
       "question                                                 What is missing in VQA databases according to this paper?   \n",
       "presup    Presupposition: This paper identifies something as missing in Visual Question Answering (VQA) databases.   \n",
       "paper_0                                                                                                      False   \n",
       "paper_1                                                                                                      False   \n",
       "paper_2                                                                                                      False   \n",
       "paper_3                                                                                                      False   \n",
       "paper_4                                                                                                       True   \n",
       "paper_5                                                                                                      False   \n",
       "paper_6                                                                                                      False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               presup_8  \\\n",
       "question                                                                                                                                                                   What methodologies does the paper introduce to improve the performance of no-reference video quality models?   \n",
       "presup    Presupposition:\\n\\n1. The paper introduces methodologies.\\n2. These methodologies are aimed at improving performance.\\n3. The context is no-reference video quality models.\\n4. The performance that is subject to improvement pertains to no-reference video quality models.   \n",
       "paper_0                                                                                                                                                                                                                                                                           False   \n",
       "paper_1                                                                                                                                                                                                                                                                           False   \n",
       "paper_2                                                                                                                                                                                                                                                                           False   \n",
       "paper_3                                                                                                                                                                                                                                                                           False   \n",
       "paper_4                                                                                                                                                                                                                                                                            True   \n",
       "paper_5                                                                                                                                                                                                                                                                           False   \n",
       "paper_6                                                                                                                                                                                                                                                                           False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                         presup_9  \\\n",
       "question                                                                                                                                                                                                                                                                                                                                         How does the paper measure the performance improvement on real-world video data?   \n",
       "presup    The presuppositions underlying the third question are as follows:\\n\\n1. The paper includes a performance measurement or evaluation component.\\n2. The paper's focus, at least in part, involves real-world video data.\\n3. There is an implied performance improvement that the paper investigates or reports.\\n4. There is a specific methodology or metric used in the paper to measure said performance improvement.   \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                     False   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                      True   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                     False   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                      True   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                      True   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                      True   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                     False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                       presup_10  \\\n",
       "question                                                                                                                                                                          What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?   \n",
       "presup    Here are the presuppositions of the given question:\\n\\n1. A solution for no-reference video quality assessment is proposed in the paper.\\n2. The paper uses datasets or benchmarks to evaluate the proposed solution.\\n3. No-reference video quality assessment is a topic addressed within the paper.   \n",
       "paper_0                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_1                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_2                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_3                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_4                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_5                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_6                                                                                                                                                                                                                                                                                                    False   \n",
       "\n",
       "                                                                                         presup_11  \\\n",
       "question     What is the performance issue discussed in the paper concerning video quality models?   \n",
       "presup    Presupposition: The paper discusses a performance issue concerning video quality models.   \n",
       "paper_0                                                                                       True   \n",
       "paper_1                                                                                       True   \n",
       "paper_2                                                                                      False   \n",
       "paper_3                                                                                       True   \n",
       "paper_4                                                                                       True   \n",
       "paper_5                                                                                      False   \n",
       "paper_6                                                                                      False   \n",
       "\n",
       "                                                                                             presup_12  \\\n",
       "question               What type of video data do the no-reference video quality models struggle with?   \n",
       "presup    Presupposition: No-reference video quality models struggle with certain types of video data.   \n",
       "paper_0                                                                                          False   \n",
       "paper_1                                                                                           True   \n",
       "paper_2                                                                                           True   \n",
       "paper_3                                                                                           True   \n",
       "paper_4                                                                                           True   \n",
       "paper_5                                                                                          False   \n",
       "paper_6                                                                                          False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                       presup_13  \\\n",
       "question                                                                                                                                                                                                                                                             What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?   \n",
       "presup    - Traditional metrics for UGC video compression and quality assessment are considered by this paper.\\n- The paper finds the existing metrics for UGC video compression and quality assessment to be inadequate.\\n- The paper discusses UGC video compression and quality assessment.\\n- The paper is oriented towards improving or critiquing aspects of UGC video compression and quality assessment.   \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                     True   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                    False   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                    False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                           presup_14  \\\n",
       "question                                                                                                                                                                                                                                                                                                  How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?   \n",
       "presup    1. The paper proposes a method or solution for overcoming certain shortcomings.\\n2. The paper is focused on User-Generated Content (UGC) video quality assessment and compression.\\n3. There are known shortcomings in UGC video quality assessment and compression that the paper addresses.\\n4. The subject matter of the paper is relevant to the field of video quality assessment and/or compression.   \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                        False   \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                         True   \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                         True   \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                        False   \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                        False   \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                        False   \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                        False   \n",
       "\n",
       "                                                                                                                                                         presup_15  \\\n",
       "question                                              What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?   \n",
       "presup    1. The paper introduces new metrics or methodologies.\\n2. The paper's focus is on evaluating UGC (User-Generated Content) video compression and quality.   \n",
       "paper_0                                                                                                                                                      False   \n",
       "paper_1                                                                                                                                                      False   \n",
       "paper_2                                                                                                                                                       True   \n",
       "paper_3                                                                                                                                                       True   \n",
       "paper_4                                                                                                                                                       True   \n",
       "paper_5                                                                                                                                                      False   \n",
       "paper_6                                                                                                                                                      False   \n",
       "\n",
       "                                                              presup_16  \\\n",
       "question  What issues with traditional metrics does this paper address?   \n",
       "presup            This paper addresses issues with traditional metrics.   \n",
       "paper_0                                                           False   \n",
       "paper_1                                                            True   \n",
       "paper_2                                                            True   \n",
       "paper_3                                                           False   \n",
       "paper_4                                                            True   \n",
       "paper_5                                                           False   \n",
       "paper_6                                                           False   \n",
       "\n",
       "                                                          presup_17  \\\n",
       "question         What is the focus of this paper's problem-solving?   \n",
       "presup    Presupposition: This paper's focus is on problem-solving.   \n",
       "paper_0                                                       False   \n",
       "paper_1                                                        True   \n",
       "paper_2                                                       False   \n",
       "paper_3                                                        True   \n",
       "paper_4                                                       False   \n",
       "paper_5                                                        True   \n",
       "paper_6                                                       False   \n",
       "\n",
       "                                                      presup_18  \\\n",
       "question  In what context is the paper addressing shortcomings?   \n",
       "presup               The paper is addressing some shortcomings.   \n",
       "paper_0                                                   False   \n",
       "paper_1                                                    True   \n",
       "paper_2                                                   False   \n",
       "paper_3                                                    True   \n",
       "paper_4                                                   False   \n",
       "paper_5                                                    True   \n",
       "paper_6                                                   False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  presup_19  \n",
       "question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  What is the main problem addressed by this paper?  \n",
       "presup    The presuppositions associated with the question \"What is the main problem addressed by this paper?\" could include:\\n\\n1. The paper addresses a specific problem.\\n2. There is a \"main\" problem, indicating that the paper may address multiple issues, but there is a primary focus.\\n3. The paper is written with the intention to discuss and potentially offer solutions or insights into a problem.\\n4. The term \"this\" indicates that there is a specific paper being referenced, and that paper is accessible to or known by both the person asking and the person answering the question.  \n",
       "paper_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               False  \n",
       "paper_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               False  \n",
       "paper_2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               False  \n",
       "paper_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                True  \n",
       "paper_4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               False  \n",
       "paper_5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               False  \n",
       "paper_6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               False  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table_0_ours['table_presupposition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f045a11-5faa-47f0-8b8f-0e1170d2fe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['final_table', 'table_question', 'table_presupposition', 'question_list'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_ours.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998777fa-8a16-4d60-85d2-a7b4e401629b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_paper', 'pap_to_tab', 'cc_to_tab', 'multi_scheme'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_baseline.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897a9d18-2b72-4ee0-b451-cbed9e02f9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_paper', 'pap_to_tab', 'cc_to_tab', 'multi_scheme'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c43a2d-8a86-4788-830b-f930f8e22b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['What is the main focus of the study?', 'What types of data are presented in the study?', 'What is the methodology used for quality assessment?', 'What are the proposed contributions?'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['pap_to_tab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f8a4b20-167a-4e35-a189-5d21945795bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['What is the main focus of the research?', 'What type of dataset is introduced or used?', 'How were the video sequences obtained for the dataset?', 'What are the applications of the proposed method or dataset?', 'What is the scale of the dataset in terms of video count and variety?'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_baseline['pap_to_tab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7815700f-425c-4521-b177-b7e2cb2c9db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_1': ['Creating a large VQA database of real-world video sequences with subjective mean opinion scores to benefit the development and evaluation of VQA methods.'],\n",
       " 'paper_2': ['Conducting a large-scale study of perceptual video quality and constructing a large video quality assessment database to advance NR video quality prediction.'],\n",
       " 'paper_3': ['Introducing a large scale UGC dataset for video compression and quality assessment research, discussing its challenges, and evaluating UGC quality by no-reference metrics.'],\n",
       " 'paper_4': ['Creating the largest subjective video quality dataset and developing region-based NR VQA models to improve the prediction of video quality on UGC.'],\n",
       " 'paper_5': [\"Introducing a new VQA dataset 'KonVid-150k' for videos in-the-wild and proposing new efficient VQA approaches with multi-level spatially pooled deep-features.\"],\n",
       " 'paper_6': ['Providing an extensive empirical evaluation of CNNs on a new large-scale video classification dataset of YouTube videos and exploring different architectures.'],\n",
       " 'paper_7': ['Describing the DeepMind Kinetics human action video dataset and analyzing the classification performance and potential dataset biases.']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(table_0_baseline['pap_to_tab'].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b8520ce-68d7-4ba7-b9b4-fe6c8a4ef4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset_size', 'dataset_focus', 'content_diversity', 'data_collection_methodology', 'UGC_focus', 'reference_to_real_world_conditions'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_total['cc_to_tab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0ed87f3-5690-4858-bc9a-61c991628140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Focus of VQA Database', 'Methodology for VQA', 'Dataset Contribution', 'Content and Distortion Types', 'Model or Metric Proposed', 'Use of Authentic or User-Generated Content'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_baseline['cc_to_tab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e73a349e-c8c2-4a9a-a6a2-680a22d739ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['what problem does this paper tackle?', 'what is the approach this paper proposed?', 'What is VQA, and why is a large database with real-world video sequences and subjective MOS necessary for its progress?', 'How does the paper propose to overcome the lack of large VQA databases?', 'What methods were used in the paper to construct and validate a new VQA database?', 'What are the potential applications of improving VQA databases as discussed in the paper?', 'What issue is the focus of this paper?', 'What is missing in VQA databases according to this paper?', 'What methodologies does the paper introduce to improve the performance of no-reference video quality models?', 'How does the paper measure the performance improvement on real-world video data?', 'What datasets or benchmarks are used in the paper to evaluate the solution proposed for no-reference video quality assessment?', 'What is the performance issue discussed in the paper concerning video quality models?', 'What type of video data do the no-reference video quality models struggle with?', 'What traditional metrics for UGC (User-Generated Content) video compression and quality assessment are considered inadequate in this paper?', 'How does the paper propose overcoming these shortcomings for UGC video quality assessment and compression?', 'What new metrics or methodologies does the paper introduce for evaluating UGC video compression and quality?', 'What issues with traditional metrics does this paper address?', \"What is the focus of this paper's problem-solving?\", 'In what context is the paper addressing shortcomings?', 'What is the main problem addressed by this paper?'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_0_ours['final_table'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f138e5-cbb5-4f93-a584-8cf8452c648f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
