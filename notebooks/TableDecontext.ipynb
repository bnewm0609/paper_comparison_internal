{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5e6e8a-98e4-4604-9637-d6cad0c2914c",
   "metadata": {},
   "source": [
    "# 1. Decontextualize Tables\n",
    "We want to do this by generating glossaries for each entry in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63b19d3-25f8-4d69-9157-ea74cb502c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e400fc-2327-43e5-99d1-a5a8b08af872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/arxiv_tables_2308_high_quality/dataset.jsonl\") as f:\n",
    "with open(\"../data/arxiv_tables_2308_high_quality/dataset_autocontext.jsonl\") as f:\n",
    "    dataset_2308_high_quality_sample = [json.loads(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86318063-4378-486e-b501-f93283b3196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/arxiv_tables_2308_high_quality/full_texts_with_tables_v2.jsonl\") as f:\n",
    "    full_texts_2308_high_quality = [json.loads(line) for line in f]\n",
    "    full_texts_2308_high_quality_map = {ft[\"paper_id\"]: ft for ft in full_texts_2308_high_quality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb647eae-b540-405a-8eb2-2cfdcbfa3fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '2308.00729v1',\n",
       " '_pdf_hash': None,\n",
       " '_source_hash': 'add89d24da98870765fefba429d689dc4a83485c',\n",
       " '_source_name': '2308.00729v1',\n",
       " 'metadata': {},\n",
       " 'abstract': {'section': 'Abstract',\n",
       "  'text': '',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " 'ref_entries': {},\n",
       " 'tables': {'43c9402e-2914-42dc-99c4-b2424f71f6ad': {'table': '<table rend=\"display\" id-text=\"1\" id=\"uid1\" place=\"t\"><head>Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.</head>\\n<row><cell right-border=\"true\" halign=\"center\">Dataset</cell>\\n<cell right-border=\"true\" halign=\"center\">Task</cell>\\n<cell halign=\"center\">Size</cell>\\n<cell>Annotations</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">KoNViD-1k <cit sha=\"24b4d04b01098cffe3cb975171aa05132c6a0903\"><ref target=\"bid14\"/></cit>{{cite:24b4d04}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">1,200</cell>\\n<cell>114</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">LIVE-VQC <cit sha=\"8f3eb869445ed0622a9879a6f2010828f6039e8b\"><ref target=\"bid13\"/></cit>{{cite:8f3eb86}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">585</cell>\\n<cell>240</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">YouTube-UGC <cit sha=\"9be8207b7ba6b4f423a574f686110ed04d5a3d91\"><ref target=\"bid15\"/></cit>{{cite:9be8207}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">1,380</cell>\\n<cell>123</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">LSVQ <cit sha=\"3dfd446db36563cc1eeb028310478bbb775a0237\"><ref target=\"bid9\"/></cit>{{cite:3dfd446}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">39,075</cell>\\n<cell>35</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">KoNViD-150k <cit sha=\"9c04ab9115e73e7300a6077937ec1b23e3fdf820\"><ref target=\"bid10\"/></cit>{{cite:9c04ab9}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">153,841</cell>\\n<cell>5</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Sports-1M <cit sha=\"fbaab966ad1e0efdeb5b96e80b5b467ba60eeced\"><ref target=\"bid1\"/></cit>{{cite:fbaab96}}</cell>\\n<cell right-border=\"true\" halign=\"center\">classification</cell>\\n<cell halign=\"center\">1,133,158</cell>\\n<cell>- (<hi rend=\"it\">auto.</hi>)</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Kinetics-400 <cit sha=\"dd352e3c3918b783377312dd4399a284589a24d1\"><ref target=\"bid2\"/></cit>{{cite:dd352e3}}</cell>\\n<cell right-border=\"true\" halign=\"center\">classification</cell>\\n<cell halign=\"center\">306,245</cell>\\n<cell>3-5</cell>\\n</row></table>',\n",
       "   'caption': 'Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.',\n",
       "   'type': 'table'},\n",
       "  'a8b624dc-b470-4046-8f35-b6e5534343a4': {'table': '<table rend=\"display\" id-text=\"2\" id=\"uid29\" starred=\"true\" place=\"t\"><head>Comparisons with SOTA methods. The up arrow “{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are <hi rend=\"bold\">highlighted</hi> and <hi rend=\"underline\">underlined</hi>. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.</head>\\n<row><cell right-border=\"true\" halign=\"center\">2*Method</cell>\\n<cell right-border=\"true\" halign=\"center\" cols=\"3\">KoNViD-1k</cell>\\n<cell right-border=\"true\" halign=\"center\" cols=\"3\">LIVE-VQC</cell>\\n<cell halign=\"center\" cols=\"3\">YouTube-UGC</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">SRCC{{formula:585eb95b-1547-4785-a382-4d02cb7a7274}}</cell>\\n<cell right-border=\"true\" halign=\"center\">PLCC{{formula:2f43609f-d5b9-4947-a668-77916eaab08b}}</cell>\\n<cell halign=\"center\">Mean</cell>\\n<cell halign=\"center\">SRCC{{formula:0ab3c965-7cc8-475b-9cc8-a434f557a90f}}</cell>\\n<cell right-border=\"true\" halign=\"center\">PLCC{{formula:91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0}}</cell>\\n<cell halign=\"center\">Mean</cell>\\n<cell halign=\"center\">SRCC{{formula:fe38b572-b993-4f84-b520-4ca75bc3ba79}}</cell>\\n<cell halign=\"center\">PLCC{{formula:ef0f4492-775e-4bdf-b2d5-8c30a10f8946}}</cell>\\n<cell>Mean</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">VIIDEO <cit sha=\"0832a8c9419aa6a0897ba06ab4d4528070bf0fa7\"><ref target=\"bid25\"/></cit>{{cite:0832a8c}}</cell>\\n<cell halign=\"center\">0.2980</cell>\\n<cell right-border=\"true\" halign=\"center\">0.3030</cell>\\n<cell halign=\"center\">0.3005</cell>\\n<cell halign=\"center\">0.0332</cell>\\n<cell right-border=\"true\" halign=\"center\">0.2164</cell>\\n<cell halign=\"center\">0.1248</cell>\\n<cell halign=\"center\">0.0580</cell>\\n<cell halign=\"center\">0.1534</cell>\\n<cell>0.1057</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">NIQE <cit sha=\"c619c6de8497b9acc2901779f3d6f41bb0653a99\"><ref target=\"bid57\"/></cit>{{cite:c619c6d}}</cell>\\n<cell halign=\"center\">0.5417</cell>\\n<cell right-border=\"true\" halign=\"center\">0.5530</cell>\\n<cell halign=\"center\">0.5474</cell>\\n<cell halign=\"center\">0.5957</cell>\\n<cell right-border=\"true\" halign=\"center\">0.6286</cell>\\n<cell halign=\"center\">0.6122</cell>\\n<cell halign=\"center\">0.2379</cell>\\n<cell halign=\"center\">0.2776</cell>\\n<cell>0.2578</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">BRISQUE <cit sha=\"18dcc024a728c82f0671adaeb65b81b7185d7acd\"><ref target=\"bid58\"/></cit>{{cite:18dcc02}}</cell>\\n<cell halign=\"center\">0.654</cell>\\n<cell right-border=\"true\" halign=\"center\">0.626</cell>\\n<cell halign=\"center\">0.640</cell>\\n<cell halign=\"center\">0.592</cell>\\n<cell right-border=\"true\" halign=\"center\">0.638</cell>\\n<cell halign=\"center\">0.615</cell>\\n<cell halign=\"center\">0.382</cell>\\n<cell halign=\"center\">0.395</cell>\\n<cell>0.389</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">VSFA <cit sha=\"7da41796bcbf3c567677b6c3fc55a27f94ef63b0\"><ref target=\"bid7\"/></cit>{{cite:7da4179}}</cell>\\n<cell halign=\"center\">0.755</cell>\\n<cell right-border=\"true\" halign=\"center\">0.744</cell>\\n<cell halign=\"center\">0.750</cell>\\n<cell halign=\"center\">-</cell>\\n<cell right-border=\"true\" halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">TLVQM <cit sha=\"1af68c6a2b2f94cf4ab35e14311272390f599453\"><ref target=\"bid59\"/></cit>{{cite:1af68c6}}</cell>\\n<cell halign=\"center\">0.7729</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7688</cell>\\n<cell halign=\"center\">0.7709</cell>\\n<cell halign=\"center\">0.7988</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8025</cell>\\n<cell halign=\"center\">0.8807</cell>\\n<cell halign=\"center\">0.6693</cell>\\n<cell halign=\"center\">0.6590</cell>\\n<cell>0.6642</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">RIRNet <cit sha=\"04485d0956438899571e65d6b1847ec05a6a9c5b\"><ref target=\"bid60\"/></cit>{{cite:04485d0}}</cell>\\n<cell halign=\"center\">0.7755</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7812</cell>\\n<cell halign=\"center\">0.7784</cell>\\n<cell halign=\"center\">0.7713</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7982</cell>\\n<cell halign=\"center\">0.7848</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">MDTVSFA <cit sha=\"1ad644b3f88e49a2de5aeab132462a51198265de\"><ref target=\"bid8\"/></cit>{{cite:1ad644b}}</cell>\\n<cell halign=\"center\">0.7812</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7856</cell>\\n<cell halign=\"center\">0.7834</cell>\\n<cell halign=\"center\">0.7382</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7728</cell>\\n<cell halign=\"center\">0.7555</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">RIRNet+<hi rend=\"it\">CSPT</hi> <cit sha=\"b519bb8aedbcaca80c324c191ad0f5a2144c0534\"><ref target=\"bid61\"/></cit>{{cite:b519bb8}}</cell>\\n<cell halign=\"center\">0.8008</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8062</cell>\\n<cell halign=\"center\">0.8035</cell>\\n<cell halign=\"center\">0.7989</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8194</cell>\\n<cell halign=\"center\">0.8092</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">CoINVQ <cit sha=\"80cfe8864bda775820fbc62d4bb618efafdb8421\"><ref target=\"bid22\"/></cit>{{cite:80cfe88}}</cell>\\n<cell halign=\"center\">0.802</cell>\\n<cell right-border=\"true\" halign=\"center\">0.816</cell>\\n<cell halign=\"center\">0.809</cell>\\n<cell halign=\"center\">-</cell>\\n<cell right-border=\"true\" halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">0.764</cell>\\n<cell halign=\"center\">0.767</cell>\\n<cell>0.766</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">RAPIQUE <cit sha=\"f7fae7386c3d4b0aa93baaf42e1c16f1fb782b70\"><ref target=\"bid62\"/></cit>{{cite:f7fae73}}</cell>\\n<cell halign=\"center\">0.8031</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8175</cell>\\n<cell halign=\"center\">0.8103</cell>\\n<cell halign=\"center\">0.7548</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7863</cell>\\n<cell halign=\"center\">0.7706</cell>\\n<cell halign=\"center\">0.7591</cell>\\n<cell halign=\"center\">0.7684</cell>\\n<cell>0.7638</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">StarVQA <cit sha=\"1c7237b9ad351ad4648f2a9a0d988f0a5b415dea\"><ref target=\"bid35\"/></cit>{{cite:1c7237b}}</cell>\\n<cell halign=\"center\">0.812</cell>\\n<cell right-border=\"true\" halign=\"center\">0.796</cell>\\n<cell halign=\"center\">0.804</cell>\\n<cell halign=\"center\">0.732</cell>\\n<cell right-border=\"true\" halign=\"center\">0.808</cell>\\n<cell halign=\"center\">0.770</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">BVQA* <cit sha=\"48735e5e29aaa49bca85d0dc76a113b1778c92c0\"><ref target=\"bid23\"/></cit>{{cite:48735e5}}</cell>\\n<cell halign=\"center\">0.8362</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8335</cell>\\n<cell halign=\"center\">0.8349</cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8412</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"underline\">0.8415</hi></cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8414</hi></cell>\\n<cell halign=\"center\">0.8312</cell>\\n<cell halign=\"center\">0.8194</cell>\\n<cell>0.8253</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">STDAM <cit sha=\"c6c0872f065d72e0370d386102cdb2bfbac88d47\"><ref target=\"bid6\"/></cit>{{cite:c6c0872}}</cell>\\n<cell halign=\"center\">0.8448</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8415</cell>\\n<cell halign=\"center\">0.8432</cell>\\n<cell halign=\"center\">0.7931</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8204</cell>\\n<cell halign=\"center\">0.8068</cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8341</hi></cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8297</hi></cell>\\n<cell><hi rend=\"underline\">0.8319</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">DisCoVQA <cit sha=\"ebacaf2da3f7d792f0824641375633af46d4b099\"><ref target=\"bid40\"/></cit>{{cite:ebacaf2}}</cell>\\n<cell halign=\"center\">0.847</cell>\\n<cell right-border=\"true\" halign=\"center\">0.847</cell>\\n<cell halign=\"center\">0.847</cell>\\n<cell halign=\"center\">0.820</cell>\\n<cell right-border=\"true\" halign=\"center\">0.826</cell>\\n<cell halign=\"center\">0.823</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">FastVQA <cit sha=\"f3075fdd875c3fd0c662a61a2aa2cf6f27c14fd2\"><ref target=\"bid37\"/></cit>{{cite:f3075fd}}</cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.859</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"underline\">0.855</hi></cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.857</hi></cell>\\n<cell halign=\"center\">0.823</cell>\\n<cell right-border=\"true\" halign=\"center\">0.844</cell>\\n<cell halign=\"center\">0.834</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Video Swin Tiny</cell>\\n<cell halign=\"center\">0.8316</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8694</cell>\\n<cell halign=\"center\">0.8505</cell>\\n<cell halign=\"center\">0.8335</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8316</cell>\\n<cell halign=\"center\">0.8326</cell>\\n<cell halign=\"center\">0.8566</cell>\\n<cell halign=\"center\">0.8499</cell>\\n<cell>0.8533</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Ada-DQA</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">0.8831</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8741</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8591</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">0.8587</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8589</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8729</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8800</hi></cell>\\n<cell><hi rend=\"bold\">0.8765</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:16e73452-3bb6-49a1-9157-f959b7b592f5}}  than the previous best</cell>\\n<cell halign=\"center\">+0.6%</cell>\\n<cell right-border=\"true\" halign=\"center\">+2.8%</cell>\\n<cell halign=\"center\">+1.7%</cell>\\n<cell halign=\"center\">+1.79%</cell>\\n<cell right-border=\"true\" halign=\"center\">+1.72%</cell>\\n<cell halign=\"center\">+1.75%</cell>\\n<cell halign=\"center\">+3.88%</cell>\\n<cell halign=\"center\">+5.03%</cell>\\n<cell>+4.46%</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:0818a4f1-d597-4afa-8fd6-78b317784b57}}  than <hi rend=\"it\">w/o</hi> pretrained models</cell>\\n<cell halign=\"center\">+3.35%</cell>\\n<cell right-border=\"true\" halign=\"center\">+1.37%</cell>\\n<cell halign=\"center\">+2.36%</cell>\\n<cell halign=\"center\">+2.56%</cell>\\n<cell right-border=\"true\" halign=\"center\">+2.71%</cell>\\n<cell halign=\"center\">+2.63%</cell>\\n<cell halign=\"center\">+1.63%</cell>\\n<cell halign=\"center\">+3.01%</cell>\\n<cell>+2.32%</cell>\\n</row></table>',\n",
       "   'caption': 'Comparisons with SOTA methods. The up arrow “{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are highlighted and underlined. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.',\n",
       "   'type': 'table'},\n",
       "  '8d9a3ded-086a-4687-a726-4cc648dbc022': {'table': '<table rend=\"display\" id-text=\"3\" id=\"uid32\" place=\"t\"><head>Experimental analysis on different numbers of selected pretrained models <hi rend=\"it\">with</hi> or <hi rend=\"it\">without</hi> the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of <hi rend=\"it\">with</hi> or <hi rend=\"it\">without</hi> is bolded.</head>\\n<row><cell halign=\"center\">Number</cell>\\n<cell right-border=\"true\" halign=\"center\">QAM</cell>\\n<cell halign=\"center\">KoNViD-1k</cell>\\n<cell halign=\"center\">LIVE-VQC</cell>\\n<cell>YouTube-UGC</cell>\\n</row><row><cell halign=\"center\">3</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:520102f0-4552-44e5-b853-ef384910ec7d}}</cell>\\n<cell halign=\"center\">0.8517</cell>\\n<cell halign=\"center\">0.8432</cell>\\n<cell>0.8630</cell>\\n</row><row><cell halign=\"center\">4</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:5c208282-8d62-42d5-8381-fcf71ca99c11}}</cell>\\n<cell halign=\"center\">0.8613</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8510</hi></cell>\\n<cell><hi rend=\"bold\">0.8659</hi></cell>\\n</row><row><cell halign=\"center\">5</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:55c39428-0e5d-40b5-b2d8-c5511775ee50}}</cell>\\n<cell halign=\"center\">0.8601</cell>\\n<cell halign=\"center\">0.8490</cell>\\n<cell>0.8591</cell>\\n</row><row><cell halign=\"center\">6</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:4fbff708-17bc-428a-b5fd-7a2dec20e294}}</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8615</hi></cell>\\n<cell halign=\"center\">0.8448</cell>\\n<cell>0.8589</cell>\\n</row><row><cell halign=\"center\">7</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:d8c752b2-c211-409d-a42a-164151334db2}}</cell>\\n<cell halign=\"center\">0.8573</cell>\\n<cell halign=\"center\">0.8459</cell>\\n<cell>0.8621</cell>\\n</row><row><cell halign=\"center\">3</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8432</cell>\\n<cell halign=\"center\">0.8433</cell>\\n<cell>0.8621</cell>\\n</row><row><cell halign=\"center\">4</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8623</cell>\\n<cell halign=\"center\">0.8514</cell>\\n<cell>0.8695</cell>\\n</row><row><cell halign=\"center\">5</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8621</cell>\\n<cell halign=\"center\">0.8546</cell>\\n<cell>0.8679</cell>\\n</row><row><cell halign=\"center\">6</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8645</cell>\\n<cell halign=\"center\">0.8542</cell>\\n<cell>0.8644</cell>\\n</row><row><cell halign=\"center\">7</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8591</hi></cell>\\n<cell><hi rend=\"bold\">0.8729</hi></cell>\\n</row><row><cell halign=\"center\">8</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8641</cell>\\n<cell halign=\"center\">0.8560</cell>\\n<cell>0.8711</cell>\\n</row></table>',\n",
       "   'caption': 'Experimental analysis on different numbers of selected pretrained models with or without the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of with or without is bolded.',\n",
       "   'type': 'table'},\n",
       "  'b84d94e4-9350-4014-bc1b-c8b4434ebf24': {'table': '<table rend=\"display\" id-text=\"4\" id=\"uid33\" place=\"t\"><head>Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.</head>\\n<row><cell right-border=\"true\" halign=\"center\">Pretrained Model</cell>\\n<cell halign=\"center\">KoNViD-1k</cell>\\n<cell halign=\"center\">LIVE-VQC</cell>\\n<cell>YouTube-UGC</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\"><hi rend=\"it\">w/o</hi></cell>\\n<cell halign=\"center\">0.8316</cell>\\n<cell halign=\"center\">0.8335</cell>\\n<cell>0.8566</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">EfficientNet-b7</cell>\\n<cell halign=\"center\">0.8412</cell>\\n<cell halign=\"center\">0.8495</cell>\\n<cell>0.8587</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Video Swin Base</cell>\\n<cell halign=\"center\">0.8390</cell>\\n<cell halign=\"center\">0.8173</cell>\\n<cell>0.8603</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Swin Base</cell>\\n<cell halign=\"center\">0.8391</cell>\\n<cell halign=\"center\">0.8475</cell>\\n<cell>0.8497</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">TimeSformer</cell>\\n<cell halign=\"center\">0.8409</cell>\\n<cell halign=\"center\">0.8302</cell>\\n<cell><hi rend=\"bold\">0.8618</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">CLIP</cell>\\n<cell halign=\"center\">0.8404</cell>\\n<cell halign=\"center\">0.8341</cell>\\n<cell>0.8608</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">ir-CNS-152</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8458</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8521</hi></cell>\\n<cell>0.8518</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">SlowFast</cell>\\n<cell halign=\"center\">0.8423</cell>\\n<cell halign=\"center\">0.8041</cell>\\n<cell>0.8523</cell>\\n</row></table>',\n",
       "   'caption': 'Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.',\n",
       "   'type': 'table'},\n",
       "  '141b825f-8848-4126-9985-77db3f3c4705': {'table': '<table rend=\"display\" id-text=\"5\" id=\"uid36\" place=\"t\"><head>Experiments on different distillation losses.</head>\\n<row><cell right-border=\"true\" halign=\"center\">distillation loss</cell>\\n<cell halign=\"center\">SRCC{{formula:c4c815c4-2362-448d-a544-b0fd44f79d70}}</cell>\\n<cell>PLCC{{formula:11c7a23f-f47a-40f7-83f2-342c6590e0c2}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:a29e8124-9675-4061-9d6d-977cb9100a57}}</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:101515c0-83a5-489c-9c6f-dbb3358b1e5e}}</cell>\\n<cell halign=\"center\">0.8092</cell>\\n<cell>0.8310</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Jesen-Shannon</cell>\\n<cell halign=\"center\">0.8455</cell>\\n<cell>0.8646</cell>\\n</row></table>',\n",
       "   'caption': 'Experiments on different distillation losses.',\n",
       "   'type': 'table'},\n",
       "  '56c4b01b-b705-42bd-a314-8bed4c21515d': {'table': '<table rend=\"array\" id-text=\"6\" id=\"uid39\" place=\"t\"><head>Selection of the hyper-parameters of {{formula:a9eb32d2-012d-403d-a083-c08a11dcce5f}}  and {{formula:f0ad8386-af17-423d-b208-6cd0a2cb4525}} .</head>\\n<p><table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}</cell>\\n<cell halign=\"center\">SRCC{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}</cell>\\n<cell>PLCC{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.1</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8462</cell>\\n<cell>0.8663</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8406</cell>\\n<cell>0.8698</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8320</cell>\\n<cell>0.8549</cell>\\n</row></table><table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}</cell>\\n<cell halign=\"center\">SRCC{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}</cell>\\n<cell>PLCC{{formula:1b522aaf-e830-438c-a148-584b695753ab}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.8</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8449</cell>\\n<cell>0.8671</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8528</cell>\\n<cell>0.8716</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8602</cell>\\n<cell>0.8780</cell>\\n</row></table></p><unexpected>\\n\\n[t]0.23\\n<caption>hyper-parameter {{formula:28690097-c6c4-4e25-93c5-fa850bcd8cdf}}</caption><hfill/>[t]0.23\\n<caption>hyper-parameter {{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}} .</caption></unexpected></table>',\n",
       "   'caption': 'hyper-parameter {{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}} .',\n",
       "   'type': 'table'},\n",
       "  '85a7afd7-6fb3-452c-bc87-76bfffd63be7': {'table': '<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}</cell>\\n<cell halign=\"center\">SRCC{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}</cell>\\n<cell>PLCC{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.1</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8462</cell>\\n<cell>0.8663</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8406</cell>\\n<cell>0.8698</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8320</cell>\\n<cell>0.8549</cell>\\n</row></table>',\n",
       "   'caption': 'NO_CAPTION',\n",
       "   'type': 'table'},\n",
       "  '34357c34-625c-4c92-b9f3-1cbd27a380a5': {'table': '<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}</cell>\\n<cell halign=\"center\">SRCC{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}</cell>\\n<cell>PLCC{{formula:1b522aaf-e830-438c-a148-584b695753ab}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.8</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8449</cell>\\n<cell>0.8671</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8528</cell>\\n<cell>0.8716</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8602</cell>\\n<cell>0.8780</cell>\\n</row></table>',\n",
       "   'caption': 'NO_CAPTION',\n",
       "   'type': 'table'},\n",
       "  '4588288c-950a-42b5-9e99-a9926c75ff3a': {'table': '<table rend=\"display\" id-text=\"7\" id=\"uid40\" place=\"t\"><head>Contributions of different pretrained models by {{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}} .</head>\\n<row><cell right-border=\"true\" halign=\"center\">Model</cell>\\n<cell halign=\"center\">{{formula:cb8e8db9-0b6f-4095-8935-43c745009f52}}  for LQ videos</cell>\\n<cell>{{formula:5e218773-7abe-4ee6-836b-4e4e6998017a}}  for HQ videos</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">EfficientNet-b7</cell>\\n<cell halign=\"center\">0.1303</cell>\\n<cell>0.7208</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Video Swin Base</cell>\\n<cell halign=\"center\">0.2021</cell>\\n<cell>0.0455</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Swin Base</cell>\\n<cell halign=\"center\">0.1676</cell>\\n<cell>0.6805</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">TimeSformer</cell>\\n<cell halign=\"center\">0.4679</cell>\\n<cell>0.2317</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">CLIP</cell>\\n<cell halign=\"center\">0.1279</cell>\\n<cell>0.3567</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">ir-CNS-152</cell>\\n<cell halign=\"center\">0.8690</cell>\\n<cell>0.2788</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">SlowFast</cell>\\n<cell halign=\"center\">0.4210</cell>\\n<cell>0.0018</cell>\\n</row></table>',\n",
       "   'caption': 'Contributions of different pretrained models by {{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}} .',\n",
       "   'type': 'table'}},\n",
       " 'bib_entries': {'445c84720802ddacc214925b8a18a5764162b41d': {'bib_entry_raw': 'Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. 2021. ViViT: A Video Vision Transformer. CoRR abs/2103.15691 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1cd332654b46fa3bbe4feac69d56da09340034b9': {'bib_entry_raw': 'Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is Space-Time Attention All You Need for Video Understanding?. In ICML, Vol. 139. PMLR, 813–824.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f2782ba1fc48bd553d272241a97ab2f07c7e3202': {'bib_entry_raw': 'João Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. 2018. A Short Note about Kinetics-600. CoRR abs/1808.01340 (2018).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9b70eb43e82b6795b3bad723ddf018c158705cb4': {'bib_entry_raw': 'Aaron Chadha and Yiannis Andreopoulos. 2021. Deep Perceptual Preprocessing for Video Coding. In CVPR. IEEE, 14852–14861.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'b5fb90b58169d81c40acf45ac8e6e11cb382c1b0': {'bib_entry_raw': 'Kuan-Ta Chen, Chi-Jui Chang, Chen-Chi Wu, Yu-Chun Chang, and Chin-Laung Lei. 2010. Quadrant of euphoria: a crowdsourcing platform for QoE assessment. IEEE Netw. 24, 2 (2010), 28–35. https://doi.org/10.1109/MNET.2010.5430141',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/MNET.2010.5430141',\n",
       "     'text': 'https://doi.org/10.1109/MNET.2010.5430141',\n",
       "     'start': 182,\n",
       "     'end': 223}]},\n",
       "  '04485d0956438899571e65d6b1847ec05a6a9c5b': {'bib_entry_raw': 'Pengfei Chen, Leida Li, Lei Ma, Jinjian Wu, and Guangming Shi. 2020b. RIRNet: Recurrent-In-Recurrent Network for Video Quality Assessment. In ACM MM. ACM, 834–842.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'b519bb8aedbcaca80c324c191ad0f5a2144c0534': {'bib_entry_raw': 'Pengfei Chen, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. 2022. Contrastive Self-Supervised Pre-Training for Video Quality Assessment. IEEE Trans. Image Process. 31 (2022), 458–471.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '06016b9532826c93f4afeac2abe91b104e7fe580': {'bib_entry_raw': 'Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020a. A Simple Framework for Contrastive Learning of Visual Representations. In ICML, Vol. 119. PMLR, 1597–1607.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '11e5a31df37f23435d9bf5bc45b27b1de884c03a': {'bib_entry_raw': 'Yanjiao Chen, Kaishun Wu, and Qian Zhang. 2015. From QoS to QoE: A Tutorial on Video Quality Assessment. IEEE Commun. Surv. Tutorials 17, 2 (2015), 1126–1165. https://doi.org/10.1109/COMST.2014.2363139',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/COMST.2014.2363139',\n",
       "     'text': 'https://doi.org/10.1109/COMST.2014.2363139',\n",
       "     'start': 159,\n",
       "     'end': 201}]},\n",
       "  '08d1075d1f740185bbd331ea03ac31a73c2587c8': {'bib_entry_raw': 'Shyamprasad Chikkerur, Vijay Sundaram, Martin Reisslein, and Lina J. Karam. 2011. Objective Video Quality Assessment Methods: A Classification, Review, and Performance Comparison. IEEE Trans. Broadcast. 57, 2 (2011), 165–182.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'cef100d09a20adbadb11d206f7d3125c4ecdd5fd': {'bib_entry_raw': 'Cisco. 2021. Cisco annual internet report white paper. https://www.cisco.com/c/en/us/solutions/collateral/ executive-perspectives/annual-internet-report/white-paper-c11-741490.html.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://www.cisco.com/c/en/us/solutions/collateral/ executive-perspectives/annual-internet-report/white-paper-c11-741490.html',\n",
       "     'text': 'https://www.cisco.com/c/en/us/solutions/collateral/ executive-perspectives/annual-internet-report/white-paper-c11-741490.html.',\n",
       "     'start': 55,\n",
       "     'end': 181}]},\n",
       "  'a718e63e60682d859a31a46983da9514e83d45b5': {'bib_entry_raw': \"MMAction2 Contributors. 2020. OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark. https://github.com/open-mmlab/mmaction2.\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://github.com/open-mmlab/mmaction2',\n",
       "     'text': 'https://github.com/open-mmlab/mmaction2.',\n",
       "     'start': 101,\n",
       "     'end': 141}]},\n",
       "  'bad8aed6a62f21179fc07856d2f1fb32e9e0bfd2': {'bib_entry_raw': 'Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In CVPR. IEEE Computer Society, 248–255.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1af68c6a2b2f94cf4ab35e14311272390f599453': {'bib_entry_raw': 'Joshua Peter Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, and Alan C. Bovik. 2020. No-Reference Video Quality Assessment Using Space-Time Chips. In MMSP. IEEE, 1–6.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '55db95fd02d5d42c08599ed5a8077d0ff0fd430f': {'bib_entry_raw': 'Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. SlowFast Networks for Video Recognition. In ICCV. 6201–6210.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9306c461bfb29b796bb723f589f12d12ab4003a9': {'bib_entry_raw': 'Bent Fuglede and Flemming Topsøe. 2004. Jensen-Shannon divergence and Hilbert space embedding. In ISIT. IEEE, 31.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9c04ab9115e73e7300a6077937ec1b23e3fdf820': {'bib_entry_raw': 'Franz Götz-Hahn, Vlad Hosu, Hanhe Lin, and Dietmar Saupe. 2021. KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild. IEEE Access 9 (2021), 72139–72160. https://doi.org/10.1109/ACCESS.2021.3077642',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/ACCESS.2021.3077642',\n",
       "     'text': 'https://doi.org/10.1109/ACCESS.2021.3077642',\n",
       "     'start': 187,\n",
       "     'end': 230}]},\n",
       "  'b2e0c1508c6aff960f71bc2d31c91555d192f868': {'bib_entry_raw': 'Jie Gu, Gaofeng Meng, Cheng Da, Shiming Xiang, and Chunhong Pan. 2019a. No-Reference Image Quality Assessment with Reinforcement Recursive List-Wise Ranking. In AAAI. AAAI Press, 8336–8343.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'd5c7273f6adc0feb2e8d22ea7031df71fcfb3f99': {'bib_entry_raw': 'Jie Gu, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. 2019b. Blind image quality assessment via learnable attention-based pooling. Pattern Recognit. 91 (2019), 332–344.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'e64e98e251fef4d7713d86c975cc21749bb61550': {'bib_entry_raw': 'Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In CVPR. Computer Vision Foundation / IEEE, 9726–9735.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3ff8959facdc02de0e570002ac51fdf3b0b392d7': {'bib_entry_raw': 'Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowledge in a Neural Network. CoRR abs/1503.02531 (2015).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '2222c26a7c1e762ad3063dc13ad2c553a6d739be': {'bib_entry_raw': 'Tobias Hoßfeld, Christian Keimel, Matthias Hirth, Bruno Gardlo, Julian Habigt, Klaus Diepold, and Phuoc Tran-Gia. 2014. Best Practices for QoE Crowdtesting: QoE Assessment With Crowdsourcing. IEEE Trans. Multim. 16, 2 (2014), 541–558. https://doi.org/10.1109/TMM.2013.2291663',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/TMM.2013.2291663',\n",
       "     'text': 'https://doi.org/10.1109/TMM.2013.2291663',\n",
       "     'start': 235,\n",
       "     'end': 275}]},\n",
       "  '24b4d04b01098cffe3cb975171aa05132c6a0903': {'bib_entry_raw': 'Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tamás Szirányi, Shujun Li, and Dietmar Saupe. 2017. The Konstanz natural video database (KoNViD-1k). In QoMEX. IEEE, 1–6.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '65d772704ddf1c8224f0b3a96a41b1ae8c93c395': {'bib_entry_raw': 'Rui Hou, YunHao Zhao, Yang Hu, and Huan Liu. 2020. No-reference video quality evaluation by a deep transfer CNN architecture. SPIC 83 (2020), 115782.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'fbaab966ad1e0efdeb5b96e80b5b467ba60eeced': {'bib_entry_raw': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. 2014. Large-scale Video Classification with Convolutional Neural Networks. In CVPR.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'dd352e3c3918b783377312dd4399a284589a24d1': {'bib_entry_raw': 'Will Kay, João Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. 2017. The Kinetics Human Action Video Dataset. CoRR abs/1705.06950 (2017).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'dbe6597a54c9f5c71a1c35c272930b6bd7510e99': {'bib_entry_raw': 'Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. 2021. MUSIQ: Multi-scale Image Quality Transformer. (October 2021), 5148–5157.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '87e3b5086562892a8e0130a9c50711fab689a72a': {'bib_entry_raw': 'Jari Korhonen. 2019. Two-Level Approach for No-Reference Consumer Video Quality Assessment. IEEE Trans. Image Process. 28, 12 (2019), 5923–5938.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3237048360325037902afaa52cc965b5a48c70c3': {'bib_entry_raw': \"Jari Korhonen, Yicheng Su, and Junyong You. 2020. Blind Natural Video Quality Prediction via Statistical Temporal Features and Deep Spatial Features. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann (Eds.). ACM, 3311–3319. https://doi.org/10.1145/3394171.3413845\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1145/3394171.3413845',\n",
       "     'text': 'https://doi.org/10.1145/3394171.3413845',\n",
       "     'start': 403,\n",
       "     'end': 442}]},\n",
       "  '48735e5e29aaa49bca85d0dc76a113b1778c92c0': {'bib_entry_raw': 'Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, and Xianpei Wang. 2021b. Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. CoRR abs/2108.08505 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '7da41796bcbf3c567677b6c3fc55a27f94ef63b0': {'bib_entry_raw': 'Dingquan Li, Tingting Jiang, and Ming Jiang. 2019. Quality Assessment of In-the-Wild Videos. In ACM Multimedia. ACM, 2351–2359.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1ad644b3f88e49a2de5aeab132462a51198265de': {'bib_entry_raw': 'Dingquan Li, Tingting Jiang, and Ming Jiang. 2021a. Unified Quality Assessment of in-the-Wild Videos with Mixed Datasets Training. IJCV 129, 4 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'db5dff2e2ed6e62a80e58067cbdb74063c18a0bf': {'bib_entry_raw': \"Liang Liao, Kangmin Xu, Haoning Wu, Chaofeng Chen, Wenxiu Sun, Qiong Yan, and Weisi Lin. 2022. Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment. In MM '22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, João Magalhães, Alberto Del Bimbo, Shin'ichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, and Laura Toni (Eds.). ACM, 837–846. https://doi.org/10.1145/3503161.3547849\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1145/3503161.3547849',\n",
       "     'text': 'https://doi.org/10.1145/3503161.3547849',\n",
       "     'start': 444,\n",
       "     'end': 483}]},\n",
       "  '942a083c331580e3d3b7ea7de148a99be5c5f998': {'bib_entry_raw': 'Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021a. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In ICCV. 10012–10022.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3b57340bcff1ec73fee2f7513ac2159950fbd1d9': {'bib_entry_raw': 'Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. 2021b. Video Swin Transformer. CoRR abs/2106.13230 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '18dcc024a728c82f0671adaeb65b81b7185d7acd': {'bib_entry_raw': 'Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012. No-Reference Image Quality Assessment in the Spatial Domain. IEEE Trans. Image Process. 21, 12 (2012), 4695–4708.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '0832a8c9419aa6a0897ba06ab4d4528070bf0fa7': {'bib_entry_raw': 'Anish Mittal, Michele A. Saad, and Alan C. Bovik. 2016. A Completely Blind Video Integrity Oracle. IEEE Trans. Image Process. 25, 1 (2016), 289–300.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'c619c6de8497b9acc2901779f3d6f41bb0653a99': {'bib_entry_raw': 'Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. Making a \"Completely Blind\" Image Quality Analyzer. IEEE SPL 20, 3 (2013), 209–212.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '2e556d9cae64c69740bb3a2284c192df6db50d34': {'bib_entry_raw': 'Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS. 8024–8035.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'c682601a81559291eba0b36bbfd8204f86fca999': {'bib_entry_raw': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML. PMLR, 8748–8763.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '7c1267c9415d3a71add5a7b3777ccea8cab6dfe0': {'bib_entry_raw': 'Michele A. Saad, Alan C. Bovik, and Christophe Charrier. 2012. Blind Image Quality Assessment: A Natural Scene Statistics Approach in the DCT Domain. IEEE Trans. Image Process. 21, 8 (2012), 3339–3352. https://doi.org/10.1109/TIP.2012.2191563',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/TIP.2012.2191563',\n",
       "     'text': 'https://doi.org/10.1109/TIP.2012.2191563',\n",
       "     'start': 202,\n",
       "     'end': 242}]},\n",
       "  '5f9c3ce33b51d81f001155e15bc1eeb419614930': {'bib_entry_raw': 'Michele A. Saad, Alan C. Bovik, and Christophe Charrier. 2014. Blind Prediction of Natural Video Quality. IEEE Trans. Image Process. 23, 3 (2014), 1352–1365.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'ecf1e0df49a10250089dc0a0aac8acef6e77c03c': {'bib_entry_raw': 'Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In ICCV. IEEE Computer Society, 618–626.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '689f6563165a43df2f9743153047793b7bfada1e': {'bib_entry_raw': 'Muhammad Shahid, Jacob Søgaard, Jeevan Pokhrel, Kjell Brunnström, Kun Wang, Samira Tavakoli, and Narciso García. 2014. Crowdsourcing based subjective quality assessment of adaptive video streaming. In Sixth International Workshop on Quality of Multimedia Experience, QoMEX 2014, Singapore, September 18-20, 2014. IEEE, 53–54. https://doi.org/10.1109/QoMEX.2014.6982289',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/QoMEX.2014.6982289',\n",
       "     'text': 'https://doi.org/10.1109/QoMEX.2014.6982289',\n",
       "     'start': 326,\n",
       "     'end': 368}]},\n",
       "  '8f3eb869445ed0622a9879a6f2010828f6039e8b': {'bib_entry_raw': 'Zeina Sinno and Alan Conrad Bovik. 2019. Large-Scale Study of Perceptual Video Quality. IEEE Trans. Image Process. 28, 2 (2019), 612–627.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'd48f3caea220408a74618213f7a143765c70f8a5': {'bib_entry_raw': 'Mingxing Tan and Quoc V. Le. 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In ICML (Proceedings of Machine Learning Research, Vol. 97). PMLR, 6105–6114.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f69f321b9fac8171bb5acda04e263ebcedbd706d': {'bib_entry_raw': 'Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. 2019. Video Classification With Channel-Separated Convolutional Networks. In ICCV. IEEE, 5551–5560.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'b58ab68c5d2dfbe0f0b04af2023d37b40675ed95': {'bib_entry_raw': 'Zhengzhong Tu, Chia-Ju Chen, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021a. Efficient User-Generated Video Quality Prediction. In Picture Coding Symposium, PCS 2021, Bristol, United Kingdom, June 29 - July 2, 2021. IEEE, 1–5. https://doi.org/10.1109/PCS50896.2021.9477483',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/PCS50896.2021.9477483',\n",
       "     'text': 'https://doi.org/10.1109/PCS50896.2021.9477483',\n",
       "     'start': 248,\n",
       "     'end': 293}]},\n",
       "  'e20cc9f23a038321af050adbe9b1c391b073144e': {'bib_entry_raw': 'Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021b. UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content. IEEE TIP 30 (2021), 4449–4464.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '6b6881a46c72b96eaeeda4a1488cbc4688ab1ffc': {'bib_entry_raw': 'Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021c. UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content. IEEE TIP 30 (2021), 4449–4464.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f7fae7386c3d4b0aa93baaf42e1c16f1fb782b70': {'bib_entry_raw': 'Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021d. RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content. CoRR abs/2101.10955 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1cad59a8b730c5f05e99b511bdea2006d47c9e1f': {'bib_entry_raw': 'Domonkos Varga and Tamás Szirányi. 2019. No-reference video quality assessment via pretrained CNN and LSTM networks. Signal Image Video Process. 13, 8 (2019), 1569–1576.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9be8207b7ba6b4f423a574f686110ed04d5a3d91': {'bib_entry_raw': 'Yilin Wang, Sasi Inguva, and Balu Adsumilli. 2019. YouTube UGC Dataset for Video Compression Research. In MMSP. IEEE, 1–5.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '80cfe8864bda775820fbc62d4bb618efafdb8421': {'bib_entry_raw': 'Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, and Feng Yang. 2021. Rich Features for Perceptual Quality Assessment of UGC Videos. In CVPR. 13435–13444.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3d9999db65fc542e1b4b4c9bb78077ba11472c4e': {'bib_entry_raw': 'Zhou Wang, Ligang Lu, and Alan C. Bovik. 2004. Video quality assessment based on structural distortion measurement. Signal Process. Image Commun. 19, 2 (2004), 121–132.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f3075fdd875c3fd0c662a61a2aa2cf6f27c14fd2': {'bib_entry_raw': 'Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. 2022a. FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling. In ECCV (6), Vol. 13666. 538–554.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'ebacaf2da3f7d792f0824641375633af46d4b099': {'bib_entry_raw': 'Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Qiong Yan, and Weisi Lin. 2022b. DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment. CoRR abs/2206.09853 (2022).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1c7237b9ad351ad4648f2a9a0d988f0a5b415dea': {'bib_entry_raw': 'Fengchuang Xing, Yuan-Gen Wang, Hanpin Wang, Leida Li, and Guopu Zhu. 2021. StarVQA: Space-Time Attention for Video Quality Assessment. CoRR abs/2108.09635 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'c6c0872f065d72e0370d386102cdb2bfbac88d47': {'bib_entry_raw': 'Jiahua Xu, Jing Li, Xingguang Zhou, Wei Zhou, Baichao Wang, and Zhibo Chen. 2021. Perceptual Quality Assessment of Internet Videos. In ACM Multimedia. ACM, 1248–1257.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '4bce23f631f9ae481757708cc0ea26efae5b89ab': {'bib_entry_raw': 'Jia Yan, Weixia Zhang, and Tianpeng Feng. 2016. Blind Image Quality Assessment Based on Natural Redundancy Statistics. In Computer Vision - ACCV 2016 - 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part IV (Lecture Notes in Computer Science, Vol. 10114), Shang-Hong Lai, Vincent Lepetit, Ko Nishino, and Yoichi Sato (Eds.). Springer, 3–18. https://doi.org/10.1007/978-3-319-54190-7_1',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1007/978-3-319-54190-7_1',\n",
       "     'text': 'https://doi.org/10.1007/978-3-319-54190-7_1',\n",
       "     'start': 399,\n",
       "     'end': 442}]},\n",
       "  '3dfd446db36563cc1eeb028310478bbb775a0237': {'bib_entry_raw': \"Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik. 2021. Patch-VQ: 'Patching Up' the Video Quality Problem. In CVPR. Computer Vision Foundation / IEEE, 14019–14029.\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '83e016cddc45f2a6e168da7fcc15222f30b62750': {'bib_entry_raw': 'Junyong You. 2021. Long Short-term Convolutional Transformer for No-Reference Video Quality Assessment. In ACM Multimedia. ACM, 2112–2120.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '5d0c0fc387c200be63a04d7091ff071e6b1d7e15': {'bib_entry_raw': 'Junyong You and Jari Korhonen. 2019. Deep Neural Networks for No-Reference Video Quality Assessment. In 2019 IEEE International Conference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019. IEEE, 2349–2353. https://doi.org/10.1109/ICIP.2019.8803395',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/ICIP.2019.8803395',\n",
       "     'text': 'https://doi.org/10.1109/ICIP.2019.8803395',\n",
       "     'start': 227,\n",
       "     'end': 268}]},\n",
       "  'b0d4c59ca1f40082c912ba330d487bbb8795000a': {'bib_entry_raw': 'Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen. 2023b. Quality-Aware Pre-Trained Models for Blind Image Quality Assessment. In CVPR. IEEE Computer Society, 22302–22313.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'fab2895d792fae2caac7291068c07fb9d37da6b1': {'bib_entry_raw': 'Kai Zhao, Kun Yuan, Ming Sun, and Xing Wen. 2023a. Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment. In CVPR Workshops. IEEE Computer Society, 1302–1310.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []}},\n",
       " 'body_text': [{'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': \"With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\\n\",\n",
       "   'cite_spans': [{'start': 128,\n",
       "     'end': 144,\n",
       "     'text': '{{cite:cef100d}}',\n",
       "     'ref_id': 'cef100d'},\n",
       "    {'start': 306,\n",
       "     'end': 322,\n",
       "     'text': '{{cite:9b70eb4}}',\n",
       "     'ref_id': '9b70eb4'},\n",
       "    {'start': 324,\n",
       "     'end': 340,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'},\n",
       "    {'start': 419,\n",
       "     'end': 435,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 437,\n",
       "     'end': 453,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 455,\n",
       "     'end': 471,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'},\n",
       "    {'start': 473,\n",
       "     'end': 489,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'},\n",
       "    {'start': 491,\n",
       "     'end': 507,\n",
       "     'text': '{{cite:5d0c0fc}}',\n",
       "     'ref_id': '5d0c0fc'},\n",
       "    {'start': 509,\n",
       "     'end': 525,\n",
       "     'text': '{{cite:3237048}}',\n",
       "     'ref_id': '3237048'},\n",
       "    {'start': 581,\n",
       "     'end': 597,\n",
       "     'text': '{{cite:8f3eb86}}',\n",
       "     'ref_id': '8f3eb86'},\n",
       "    {'start': 599,\n",
       "     'end': 615,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 617,\n",
       "     'end': 633,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'},\n",
       "    {'start': 635,\n",
       "     'end': 651,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab.\\xa0{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\\n',\n",
       "   'cite_spans': [{'start': 256,\n",
       "     'end': 272,\n",
       "     'text': '{{cite:fbaab96}}',\n",
       "     'ref_id': 'fbaab96'},\n",
       "    {'start': 274,\n",
       "     'end': 290,\n",
       "     'text': '{{cite:dd352e3}}',\n",
       "     'ref_id': 'dd352e3'},\n",
       "    {'start': 407,\n",
       "     'end': 423,\n",
       "     'text': '{{cite:b5fb90b}}',\n",
       "     'ref_id': 'b5fb90b'},\n",
       "    {'start': 425,\n",
       "     'end': 441,\n",
       "     'text': '{{cite:2222c26}}',\n",
       "     'ref_id': '2222c26'},\n",
       "    {'start': 443,\n",
       "     'end': 459,\n",
       "     'text': '{{cite:689f656}}',\n",
       "     'ref_id': '689f656'},\n",
       "    {'start': 461,\n",
       "     'end': 477,\n",
       "     'text': '{{cite:11e5a31}}',\n",
       "     'ref_id': '11e5a31'},\n",
       "    {'start': 619,\n",
       "     'end': 635,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 767,\n",
       "     'end': 783,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'},\n",
       "    {'start': 785,\n",
       "     'end': 801,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'},\n",
       "    {'start': 1022,\n",
       "     'end': 1038,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'}],\n",
       "   'ref_spans': [{'start': 106,\n",
       "     'end': 152,\n",
       "     'text': '{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "     'ref_id': '43c9402e-2914-42dc-99c4-b2424f71f6ad'}]},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\\nDataset\\nTask\\nSize\\nAnnotations\\nKoNViD-1k {{cite:24b4d04}}\\nVQA\\n1,200\\n114\\nLIVE-VQC {{cite:8f3eb86}}\\nVQA\\n585\\n240\\nYouTube-UGC {{cite:9be8207}}\\nVQA\\n1,380\\n123\\nLSVQ {{cite:3dfd446}}\\nVQA\\n39,075\\n35\\nKoNViD-150k {{cite:9c04ab9}}\\nVQA\\n153,841\\n5\\nSports-1M {{cite:fbaab96}}\\nclassification\\n1,133,158\\n- (auto.)\\nKinetics-400 {{cite:dd352e3}}\\nclassification\\n306,245\\n3-5\\n{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "   'cite_spans': [{'start': 240,\n",
       "     'end': 256,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 280,\n",
       "     'end': 296,\n",
       "     'text': '{{cite:8f3eb86}}',\n",
       "     'ref_id': '8f3eb86'},\n",
       "    {'start': 321,\n",
       "     'end': 337,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'},\n",
       "    {'start': 357,\n",
       "     'end': 373,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'},\n",
       "    {'start': 400,\n",
       "     'end': 416,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'},\n",
       "    {'start': 441,\n",
       "     'end': 457,\n",
       "     'text': '{{cite:fbaab96}}',\n",
       "     'ref_id': 'fbaab96'},\n",
       "    {'start': 506,\n",
       "     'end': 522,\n",
       "     'text': '{{cite:dd352e3}}',\n",
       "     'ref_id': 'dd352e3'}],\n",
       "   'ref_spans': [{'start': 550,\n",
       "     'end': 596,\n",
       "     'text': '{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "     'ref_id': '43c9402e-2914-42dc-99c4-b2424f71f6ad'}]},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig.\\xa0{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\\n',\n",
       "   'cite_spans': [{'start': 72,\n",
       "     'end': 88,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 90, 'end': 106, 'text': '{{cite:1ad644b}}', 'ref_id': '1ad644b'},\n",
       "    {'start': 108,\n",
       "     'end': 124,\n",
       "     'text': '{{cite:65d7727}}',\n",
       "     'ref_id': '65d7727'},\n",
       "    {'start': 126,\n",
       "     'end': 142,\n",
       "     'text': '{{cite:1cad59a}}',\n",
       "     'ref_id': '1cad59a'},\n",
       "    {'start': 299,\n",
       "     'end': 315,\n",
       "     'text': '{{cite:bad8aed}}',\n",
       "     'ref_id': 'bad8aed'},\n",
       "    {'start': 779,\n",
       "     'end': 795,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'},\n",
       "    {'start': 942,\n",
       "     'end': 958,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 960,\n",
       "     'end': 976,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'}],\n",
       "   'ref_spans': [{'start': 1181,\n",
       "     'end': 1227,\n",
       "     'text': '{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "     'ref_id': '43c9402e-2914-42dc-99c4-b2424f71f6ad'}]},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\\n',\n",
       "   'cite_spans': [{'start': 653,\n",
       "     'end': 669,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\\nOur contributions are as follows:\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'list',\n",
       "   'text': '\\nTo the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\\n\\nWe propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\\n\\nWe evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\\n\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Related work',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'According to the availability of reference videos, VQA methods can be classified into full-reference (FR), reduced-reference (RR), and no-reference (NR) {{cite:48735e5}} ones. As reference videos are always hard to obtain, NR-VQA becomes more practical in the real-world VQA scenario, which is investigated in this paper. According to the difference in construction schema, VQA methods can be classified into traditional hand-crafted and learning-based ones.\\n',\n",
       "   'cite_spans': [{'start': 153,\n",
       "     'end': 169,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Related work',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Overview of our proposed Ada-DQA framework. First, in-the-wild pretrained models are selected as candidates according to diverse aspects. Second, features generated by these frozen pretrained models are aggregated per sample using the QAM adaptively. This approach allows acquiring of quality-related representations. Third, during training, the integrated feature is utilized as supplementary supervision, along with the labeled quality score, to guide the training of a lightweight VQA model. During inference, only the optimized VQA model is used, reducing the computational cost largely.\\n{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 592,\n",
       "     'end': 639,\n",
       "     'text': '{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}}',\n",
       "     'ref_id': 'fa4ceb3d-fea6-4d86-9c04-31b91570ede3'}]},\n",
       "  {'section': 'Classical VQA Approaches',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Classic VQA methods {{cite:7c1267c}}, {{cite:0832a8c}}, {{cite:87e3b50}}, {{cite:e20cc9f}}, {{cite:b58ab68}}, {{cite:db5dff2}} rely on handcrafted features to evaluate video quality. With an underlying assumption that the perceptual quality can be measured by the disturbance of natural scene statics (NSS) {{cite:4bce23f}}, these work attempts at designing handcrafted features with richer representation for VQA. The work {{cite:5f9c3ce}} based on the 2D discrete-time transform (DCT) features of video frame-difference statistics, and motion information is further introduced to level up the representation capacity. TLVQM {{cite:87e3b50}} utilizes a combination of spatial high-complexity and temporal low-complexity handcraft features. Whereas, handcrafted features are gradually replaced by the DNN-based features, due to their sensitivity to distortion types and the superiority DNN features demonstrated in various computer vision tasks.\\n',\n",
       "   'cite_spans': [{'start': 20,\n",
       "     'end': 36,\n",
       "     'text': '{{cite:7c1267c}}',\n",
       "     'ref_id': '7c1267c'},\n",
       "    {'start': 38, 'end': 54, 'text': '{{cite:0832a8c}}', 'ref_id': '0832a8c'},\n",
       "    {'start': 56, 'end': 72, 'text': '{{cite:87e3b50}}', 'ref_id': '87e3b50'},\n",
       "    {'start': 74, 'end': 90, 'text': '{{cite:e20cc9f}}', 'ref_id': 'e20cc9f'},\n",
       "    {'start': 92, 'end': 108, 'text': '{{cite:b58ab68}}', 'ref_id': 'b58ab68'},\n",
       "    {'start': 110,\n",
       "     'end': 126,\n",
       "     'text': '{{cite:db5dff2}}',\n",
       "     'ref_id': 'db5dff2'},\n",
       "    {'start': 307,\n",
       "     'end': 323,\n",
       "     'text': '{{cite:4bce23f}}',\n",
       "     'ref_id': '4bce23f'},\n",
       "    {'start': 424,\n",
       "     'end': 440,\n",
       "     'text': '{{cite:5f9c3ce}}',\n",
       "     'ref_id': '5f9c3ce'},\n",
       "    {'start': 626,\n",
       "     'end': 642,\n",
       "     'text': '{{cite:87e3b50}}',\n",
       "     'ref_id': '87e3b50'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'DNN-based VQA Methods',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Recently, CNN-based methods {{cite:7da4179}}, {{cite:e20cc9f}}, {{cite:d5c7273}}, {{cite:b2e0c15}} and Transformer-based methods {{cite:dbe6597}}, {{cite:1c7237b}}, {{cite:fab2895}}, {{cite:f3075fd}} have taken the lead in the QA domain. However, due to the data-driven characteristics of deep learning, most of the current VQA models suffer from the lack of sufficient high-quality-labeled datasets. There are some attempts to relieve this insufficient data challenge, either from patch-level/frame-level augmentation {{cite:7da4179}}, {{cite:83e016c}} or fine-tuning from other large computer vision models pretrained on large general knowledge-based datasets {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}}. VSFA {{cite:7da4179}} extracts frame-wise features with ResNet and uses a gated-recurrent unit to model temporal information. LSCT {{cite:83e016c}} adopts a Transformer to predict video quality based on the frame features extracted by an IQA model. But frame-level augmentation dismissed the effect brought by temporal concealment, which is widely noticed nowadays.\\n',\n",
       "   'cite_spans': [{'start': 28,\n",
       "     'end': 44,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 46, 'end': 62, 'text': '{{cite:e20cc9f}}', 'ref_id': 'e20cc9f'},\n",
       "    {'start': 64, 'end': 80, 'text': '{{cite:d5c7273}}', 'ref_id': 'd5c7273'},\n",
       "    {'start': 82, 'end': 98, 'text': '{{cite:b2e0c15}}', 'ref_id': 'b2e0c15'},\n",
       "    {'start': 129,\n",
       "     'end': 145,\n",
       "     'text': '{{cite:dbe6597}}',\n",
       "     'ref_id': 'dbe6597'},\n",
       "    {'start': 147,\n",
       "     'end': 163,\n",
       "     'text': '{{cite:1c7237b}}',\n",
       "     'ref_id': '1c7237b'},\n",
       "    {'start': 165,\n",
       "     'end': 181,\n",
       "     'text': '{{cite:fab2895}}',\n",
       "     'ref_id': 'fab2895'},\n",
       "    {'start': 183,\n",
       "     'end': 199,\n",
       "     'text': '{{cite:f3075fd}}',\n",
       "     'ref_id': 'f3075fd'},\n",
       "    {'start': 519,\n",
       "     'end': 535,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 537,\n",
       "     'end': 553,\n",
       "     'text': '{{cite:83e016c}}',\n",
       "     'ref_id': '83e016c'},\n",
       "    {'start': 662,\n",
       "     'end': 678,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 680,\n",
       "     'end': 696,\n",
       "     'text': '{{cite:65d7727}}',\n",
       "     'ref_id': '65d7727'},\n",
       "    {'start': 698,\n",
       "     'end': 714,\n",
       "     'text': '{{cite:1cad59a}}',\n",
       "     'ref_id': '1cad59a'},\n",
       "    {'start': 721,\n",
       "     'end': 737,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 847,\n",
       "     'end': 863,\n",
       "     'text': '{{cite:83e016c}}',\n",
       "     'ref_id': '83e016c'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'DNN-based VQA Methods',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': \"Most fine-tuning work {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} utilizes models pretrained on classification datasets, where learned information mainly covers content-awareness and is not tailor for the task of VQA.\\nSeveral work {{cite:80cfe88}}, {{cite:48735e5}}, {{cite:b0d4c59}} has noticed the insufficiency of content-aware information.\\nCoINVQ {{cite:80cfe88}} leverage distortion-aware and compression-aware representation besides the content-aware representation. Nevertheless, the distorted information is learned from synthetic datasets and the generalized ability to in-the-wild distorted data needs to be verified. BVQA {{cite:48735e5}} introduces motion-aware information learned from the action classification dataset. But they dismiss the fact that distortion awareness is crucial to VQA.\\nWhat's more, these works either utilize a temporal-sampling and concatenating strategy to aggregate features or employ temporal average pooling for feature fusion. The final features are not acquired in an adaptive and flexible manner, which prohibits the diverse feature representations from unleashing full potential. More recent work focus on building spatiotemporal relation.\\nStarVQA {{cite:1c7237b}} builds a Transformer by using divided space-time attention. DisCoVQA {{cite:ebacaf2}} design a transformer-based Spatial-Temporal Distortion Extraction module to tackle temporal quality attention. FastVQA {{cite:f3075fd}} attempts to assess local quality by sampling patches at their raw resolution and covers global quality with contextual relations.\\n\",\n",
       "   'cite_spans': [{'start': 22,\n",
       "     'end': 38,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 40, 'end': 56, 'text': '{{cite:65d7727}}', 'ref_id': '65d7727'},\n",
       "    {'start': 58, 'end': 74, 'text': '{{cite:1cad59a}}', 'ref_id': '1cad59a'},\n",
       "    {'start': 240,\n",
       "     'end': 256,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 258,\n",
       "     'end': 274,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'},\n",
       "    {'start': 276,\n",
       "     'end': 292,\n",
       "     'text': '{{cite:b0d4c59}}',\n",
       "     'ref_id': 'b0d4c59'},\n",
       "    {'start': 360,\n",
       "     'end': 376,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 642,\n",
       "     'end': 658,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'},\n",
       "    {'start': 1202,\n",
       "     'end': 1218,\n",
       "     'text': '{{cite:1c7237b}}',\n",
       "     'ref_id': '1c7237b'},\n",
       "    {'start': 1288,\n",
       "     'end': 1304,\n",
       "     'text': '{{cite:ebacaf2}}',\n",
       "     'ref_id': 'ebacaf2'},\n",
       "    {'start': 1424,\n",
       "     'end': 1440,\n",
       "     'text': '{{cite:f3075fd}}',\n",
       "     'ref_id': 'f3075fd'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Method',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To surmount the constraint of limited labeled data availability and to obtain the quality-related features inherent in diverse modalities, we introduce the Ada-DQA framework for VQA tasks. In Sec.REF , we provide an overview of the framework. In Sec.REF , we explicate the construction of pretrained models from various aspects. In Sec.REF , we elucidate the process of acquiring quality representation using the proposed Quality-aware Acquisition Module (QAM). Finally, in Sec.REF , we present the optimization objective during training based on knowledge distillation and regression loss.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Ada-DQA Framework',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'As shown in Fig.\\xa0{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}} , the framework of Ada-DQA can be divided into three components. First, {{formula:0a2dda82-f469-4a2c-a7e4-e3eb50eb52d6}}  pretrained models, which act as feature extractors, are selected as candidates from the wild. Given an input video {{formula:70d4f318-edc8-4a87-adf4-4481112155d1}} , features are generated by these pretrained models, whose weights are frozen. This significantly reduces the training cost of multiple heavy pretrained models. According to the training paradigm of pretrained models, these features may contain quality-related information (e.g., , content, distortions, and motion). However, since factors that may affect quality vary in different videos, the correlation between the quality of different videos and these features is also different. Second, to adaptively capture desired quality-related features sample-by-sample during training, the proposed QAM is used to raise dynamic weights for feature aggregation. An extra sparsity constraint is attached to the distribution of these gating weights, promoting attention to more critical and relevant features for quality representation. Then the video quality feature can be obtained by a weighted summation. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner. During inference, only the optimized VQA model is used, reducing the computational cost largely. More details will be provided below.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 17,\n",
       "     'end': 64,\n",
       "     'text': '{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}}',\n",
       "     'ref_id': 'fa4ceb3d-fea6-4d86-9c04-31b91570ede3'},\n",
       "    {'start': 137,\n",
       "     'end': 185,\n",
       "     'text': '{{formula:0a2dda82-f469-4a2c-a7e4-e3eb50eb52d6}}',\n",
       "     'ref_id': '0a2dda82-f469-4a2c-a7e4-e3eb50eb52d6'},\n",
       "    {'start': 302,\n",
       "     'end': 350,\n",
       "     'text': '{{formula:70d4f318-edc8-4a87-adf4-4481112155d1}}',\n",
       "     'ref_id': '70d4f318-edc8-4a87-adf4-4481112155d1'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Videos sampled from the YouTube-UGC dataset {{cite:9be8207}} and their corresponding labeled MOS, ranging from 1.0 to 5.0. It can be seen that video quality is affected by various aspects, including content, distortions, and motion.\\n{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}}',\n",
       "   'cite_spans': [{'start': 44,\n",
       "     'end': 60,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'}],\n",
       "   'ref_spans': [{'start': 233,\n",
       "     'end': 280,\n",
       "     'text': '{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}}',\n",
       "     'ref_id': '11ec1a15-65b0-4d3a-b435-b8e6372c8c66'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Inspired by the current success of the “pretraining and fine-tuning” paradigm in deep learning {{cite:e64e98e}}, {{cite:06016b9}}, {{cite:c682601}}, we aim to utilize in-the-wild pretrained models to benefit VQA from diverse aspects of the video, in order to enhance better understanding of video quality and enable personalized treatments to improve it. We contemplate the choice of pretrained models through the lens of multiple factors, as shown in Fig.\\xa0{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}} , that may impact the quality of videos as follows:\\n',\n",
       "   'cite_spans': [{'start': 95,\n",
       "     'end': 111,\n",
       "     'text': '{{cite:e64e98e}}',\n",
       "     'ref_id': 'e64e98e'},\n",
       "    {'start': 113,\n",
       "     'end': 129,\n",
       "     'text': '{{cite:06016b9}}',\n",
       "     'ref_id': '06016b9'},\n",
       "    {'start': 131,\n",
       "     'end': 147,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'}],\n",
       "   'ref_spans': [{'start': 457,\n",
       "     'end': 504,\n",
       "     'text': '{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}}',\n",
       "     'ref_id': '11ec1a15-65b0-4d3a-b435-b8e6372c8c66'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'list',\n",
       "   'text': \"\\nContent. Human judgments of visual quality are content-dependent according to previous studies {{cite:7da4179}}, {{cite:80cfe88}}. When a video is visually appealing, engaging, and relevant to the viewer's interests (e.g., cute puppy and beautiful scenery), it can capture their attention and make them more receptive to the video's content. In contrast, if a video is dull, uninteresting, or irrelevant (e.g., black screen and messy corners), viewers are more likely to rate low quality. Introducing models pretrained on the task of object recognition (e.g., EfficientNet, Swin Transformer) may benefit VQA.\\n\\nDistortion. In addition to content, distortions introduced during the phase of video capturing and compression also determine the video quality {{cite:6b6881a}}. Thus a pretrained model that has been trained on a dataset of images or videos (e.g., ImageNet, Kinetics-400) with compression artifacts will have learned to identify the specific patterns and features that are associated with compression artifacts, such as blockiness, blurriness, or pixelation.\\n\\nMotion. Unlike the image scenario, motion blur can significantly affect the quality of videos {{cite:3d9999d}}, {{cite:08d1075}}. It occurs when there is rapid motion, and the camera or objects in the scene are moving too quickly for the camera's shutter to capture. A pretrained action recognition model (e.g., SlowFast, TimeSformer) may detect specific actions or movements such as running, jumping, or throwing. These can be useful for analyzing the amount of motion or by looking for specific visual cues that are associated with motion blur, such as streaking around the edges of moving objects.\\n\\n\",\n",
       "   'cite_spans': [{'start': 96,\n",
       "     'end': 112,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 114,\n",
       "     'end': 130,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 755,\n",
       "     'end': 771,\n",
       "     'text': '{{cite:6b6881a}}',\n",
       "     'ref_id': '6b6881a'},\n",
       "    {'start': 1165,\n",
       "     'end': 1181,\n",
       "     'text': '{{cite:3d9999d}}',\n",
       "     'ref_id': '3d9999d'},\n",
       "    {'start': 1183,\n",
       "     'end': 1199,\n",
       "     'text': '{{cite:08d1075}}',\n",
       "     'ref_id': '08d1075'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Responses of different pretrained models to synthetic sequences generated by distortions (i.e., compression, sharpness). The correlation of SRCC is computed according to distortion degrees. It is evident that pretrained models may detect certain types of distortions, but their ability to perceive distortion varies across models.\\n{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 331,\n",
       "     'end': 378,\n",
       "     'text': '{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}}',\n",
       "     'ref_id': '23154bd4-20ea-41e7-9f68-e344eb273514'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'However, it is important to note that individual pretrained models may not be able to identify all types of quality-related factors, or may not be as accurate in identifying certain types. Some evidences are given in Fig.\\xa0{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}} . When encountering different types of distortions, there will be obvious differences in the perception ability of the pretrained model. In detail, ConvNext-base (SRCC=0.968) outperforms EfficientNet-b7 (SRCC=0.038) when facing compression. When it comes to sharpness, EfficientNet-b7 performs better. Therefore, it is important to use a diverse set of models and combine their results to get a more robust assessment. Thus, we propose to construct a pool with a large diversity of candidate models, considering the following aspects:\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 222,\n",
       "     'end': 269,\n",
       "     'text': '{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}}',\n",
       "     'ref_id': '23154bd4-20ea-41e7-9f68-e344eb273514'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'list',\n",
       "   'text': '\\nArchitecture. The efficacy of a network architecture (e.g., CNN, Transformer) hinges upon its capacity to assimilate and convey information. A well-crafted architecture can discern finer details and patterns in the input video, while also influencing the manner in which spatial and temporal information, containing quality-related features, is processed.\\n\\nPretrained pretext task. The type of supervision in the pretext task has an impact on the ability of the pretrained model to VQA tasks. When the distribution of data is similar, a supervised pretext task may lead to superior performance. Conversely, self-supervised pretext tasks, where the model is trained on unlabeled data, may facilitate better generalization when confronting unfamiliar VQA domains.\\n\\nPretrained dataset. Pretrained datasets on a large scale can be advantageous to VQA by providing diverse content, distortion, and motion-related data. A desired pretrained dataset should include a wide range of categories that closely resemble real-world scenarios, as well as other multi-modal information that can aid in describing video quality. For instance, the WebImageText {{cite:c682601}} dataset, which combines text and images, can be helpful in this regard.\\n\\n',\n",
       "   'cite_spans': [{'start': 1144,\n",
       "     'end': 1160,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Based on the above considerations, in this paper, we select several pretrained models that obtain top performance in their original fields, including (1) EfficientNet-b7 {{cite:d48f3ca}} trained on ImageNet-1k {{cite:bad8aed}}, (2) ir-CSN-152 {{cite:f69f321}} trained on Sports-1M {{cite:fbaab96}}, (3) CLIP trained on WebImageText {{cite:c682601}}, (4) Swin Transformer Base {{cite:942a083}} trained on ImageNet-21k {{cite:bad8aed}}, (5) TimeSformer {{cite:1cd3326}} trained on Kinetics-400 {{cite:dd352e3}}, (6) Video Swin Transformer Base {{cite:3b57340}} trained on Kinetics-600 {{cite:f2782ba}}, and (7) SlowFast {{cite:55db95f}} trained on Kinetics-400.\\n',\n",
       "   'cite_spans': [{'start': 170,\n",
       "     'end': 186,\n",
       "     'text': '{{cite:d48f3ca}}',\n",
       "     'ref_id': 'd48f3ca'},\n",
       "    {'start': 210,\n",
       "     'end': 226,\n",
       "     'text': '{{cite:bad8aed}}',\n",
       "     'ref_id': 'bad8aed'},\n",
       "    {'start': 243,\n",
       "     'end': 259,\n",
       "     'text': '{{cite:f69f321}}',\n",
       "     'ref_id': 'f69f321'},\n",
       "    {'start': 281,\n",
       "     'end': 297,\n",
       "     'text': '{{cite:fbaab96}}',\n",
       "     'ref_id': 'fbaab96'},\n",
       "    {'start': 332,\n",
       "     'end': 348,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'},\n",
       "    {'start': 376,\n",
       "     'end': 392,\n",
       "     'text': '{{cite:942a083}}',\n",
       "     'ref_id': '942a083'},\n",
       "    {'start': 417,\n",
       "     'end': 433,\n",
       "     'text': '{{cite:bad8aed}}',\n",
       "     'ref_id': 'bad8aed'},\n",
       "    {'start': 451,\n",
       "     'end': 467,\n",
       "     'text': '{{cite:1cd3326}}',\n",
       "     'ref_id': '1cd3326'},\n",
       "    {'start': 492,\n",
       "     'end': 508,\n",
       "     'text': '{{cite:dd352e3}}',\n",
       "     'ref_id': 'dd352e3'},\n",
       "    {'start': 542,\n",
       "     'end': 558,\n",
       "     'text': '{{cite:3b57340}}',\n",
       "     'ref_id': '3b57340'},\n",
       "    {'start': 583,\n",
       "     'end': 599,\n",
       "     'text': '{{cite:f2782ba}}',\n",
       "     'ref_id': 'f2782ba'},\n",
       "    {'start': 618,\n",
       "     'end': 634,\n",
       "     'text': '{{cite:55db95f}}',\n",
       "     'ref_id': '55db95f'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'As the distribution of content and distortions in videos can be quite complex, a static combination of pretrained models may not always yield optimal performance. In order to adaptively capture the diversity and complementary information from different pretrained models, we propose a Quality-aware Acquisition Module (QAM). It takes extracted features from various pretrained models as input and produces a consolidated feature as output for the ultimate representation of quality. The computational process can be partitioned into two main parts. The first part is responsible for transforming the extracted features initially into a uniform feature dimension to enable subsequent aggregation. Structurally, this transformation block comprises two fully-connected layers followed by a normalization layer and a GELU activation layer. The second part generates gating weights {{formula:37337360-9a58-4f2e-bc9c-4579edb73e1e}}  to control the aggregation process. The gating network takes the concatenated feature vector as input and outputs a set of gating weights that represent the relative contribution of each pretrained model to the final quality representation. Structurally, this gating network is stacked using a fully-connected layer and a sigmoid layer. Then the quality representation {{formula:e87cca53-60ab-4242-8a5c-36ca3249b7a0}}  can be obtained by a weighted sum according to the gating weights. Given the extracted features by different pretrained models {{formula:ee60e35d-98bc-4620-aad3-5377af55c0f7}} , these procedures can be noted as:\\n{{formula:3adab10d-4dfb-4b57-a64d-b4f79084d696}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 877,\n",
       "     'end': 925,\n",
       "     'text': '{{formula:37337360-9a58-4f2e-bc9c-4579edb73e1e}}',\n",
       "     'ref_id': '37337360-9a58-4f2e-bc9c-4579edb73e1e'},\n",
       "    {'start': 1296,\n",
       "     'end': 1344,\n",
       "     'text': '{{formula:e87cca53-60ab-4242-8a5c-36ca3249b7a0}}',\n",
       "     'ref_id': 'e87cca53-60ab-4242-8a5c-36ca3249b7a0'},\n",
       "    {'start': 1473,\n",
       "     'end': 1521,\n",
       "     'text': '{{formula:ee60e35d-98bc-4620-aad3-5377af55c0f7}}',\n",
       "     'ref_id': 'ee60e35d-98bc-4620-aad3-5377af55c0f7'},\n",
       "    {'start': 1558,\n",
       "     'end': 1606,\n",
       "     'text': '{{formula:3adab10d-4dfb-4b57-a64d-b4f79084d696}}',\n",
       "     'ref_id': '3adab10d-4dfb-4b57-a64d-b4f79084d696'}]},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'where {{formula:ca5a3201-6391-490a-be08-7bb67bfadc33}}  denotes the mapping function for the {{formula:2595f6fe-5e54-4f7c-9762-3d684fbaf4f8}} -th transformation block, and {{formula:0ed6c418-804c-41b2-89d0-42b5853ea868}}  represents the mapping function for the gating network. And {{formula:248a0551-587d-4a12-8d93-a4a03cb39f54}}  is the number of aligned feature dimensions.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 6,\n",
       "     'end': 54,\n",
       "     'text': '{{formula:ca5a3201-6391-490a-be08-7bb67bfadc33}}',\n",
       "     'ref_id': 'ca5a3201-6391-490a-be08-7bb67bfadc33'},\n",
       "    {'start': 93,\n",
       "     'end': 141,\n",
       "     'text': '{{formula:2595f6fe-5e54-4f7c-9762-3d684fbaf4f8}}',\n",
       "     'ref_id': '2595f6fe-5e54-4f7c-9762-3d684fbaf4f8'},\n",
       "    {'start': 172,\n",
       "     'end': 220,\n",
       "     'text': '{{formula:0ed6c418-804c-41b2-89d0-42b5853ea868}}',\n",
       "     'ref_id': '0ed6c418-804c-41b2-89d0-42b5853ea868'},\n",
       "    {'start': 282,\n",
       "     'end': 330,\n",
       "     'text': '{{formula:248a0551-587d-4a12-8d93-a4a03cb39f54}}',\n",
       "     'ref_id': '248a0551-587d-4a12-8d93-a4a03cb39f54'}]},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In addition, to emphasize the importance of critical features and enhance the generalization ability, we propose to impose a sparsity constraint as a regularization on the distribution of gating weights. The {{formula:c6eb3ce7-3574-4a93-9668-81f00f6ef30b}}  loss is utilized to penalize non-zero weights resulting in more weights near zero. This constraint can be written as:\\n{{formula:67a151a6-206b-4fae-9e99-152c5f039648}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 208,\n",
       "     'end': 256,\n",
       "     'text': '{{formula:c6eb3ce7-3574-4a93-9668-81f00f6ef30b}}',\n",
       "     'ref_id': 'c6eb3ce7-3574-4a93-9668-81f00f6ef30b'},\n",
       "    {'start': 376,\n",
       "     'end': 424,\n",
       "     'text': '{{formula:67a151a6-206b-4fae-9e99-152c5f039648}}',\n",
       "     'ref_id': '67a151a6-206b-4fae-9e99-152c5f039648'}]},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In this way, QAM allows for capturing a broader range of quality-related features, thereby enabling better adaptation to various types of video content, distortions, or movement. Then the aggregated feature is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:3f2b822c-30dd-45b4-ba0b-beb2946e3eea}} .\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 317,\n",
       "     'end': 365,\n",
       "     'text': '{{formula:3f2b822c-30dd-45b4-ba0b-beb2946e3eea}}',\n",
       "     'ref_id': '3f2b822c-30dd-45b4-ba0b-beb2946e3eea'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In practical scenarios, using these large pretrained models for inference can be computationally expensive. To reduce the computational cost and increase flexibility, we propose to use knowledge distillation {{cite:3ff8959}} to transfer the knowledge from large and complex models to a lightweight VQA model.\\n',\n",
       "   'cite_spans': [{'start': 208,\n",
       "     'end': 224,\n",
       "     'text': '{{cite:3ff8959}}',\n",
       "     'ref_id': '3ff8959'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In this paper, a Video Swin Transformer-Tiny {{cite:3b57340}} is selected as the backbone. For an input video {{formula:8867c3e5-3d24-4636-8ff8-d5750ff52bd6}} , the quality representation can be achieved by {{formula:bb5f7f42-117d-46f5-be87-d4e217711613}} , where {{formula:c7ff177d-8ef7-45e2-8103-88b4eea6fb93}}  represents the mapping function of the lightweight backbone, and {{formula:7c479112-73be-4fee-8400-e83b7742006f}} . Then {{formula:779587b6-9f41-4a76-8eb3-f73e18564535}}  is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:24a53d27-11e1-49b4-954e-749af6b21b65}} . Note that both {{formula:7773c82b-4644-48fe-bb90-fe64464414c7}}  and {{formula:71835fc2-410b-434f-a922-c99ec67fed5b}}  are supervised by the labeled MOS using a smooth {{formula:fd34ed25-a357-4022-a448-0d9ab5498d99}}  loss. Additionally, we apply a similarity consistency in knowledge distillation between {{formula:7700fca0-2c77-4558-8af4-f8293fa8fa5e}}  and {{formula:85d627d9-b864-4f30-8fe0-48f9cb3eb9e9}} . This allows the VQA model to simulate the robust quality representation generated by diverse pretrained models, further enhancing its performance. Given the labeled MOS {{formula:b1c614c9-d41b-499f-a879-5ff2c29c8c1f}} , the regression loss for pretrained models can be noted as:\\n{{formula:c0f5e532-617b-4ffd-b550-cc7b40bed394}} \\n',\n",
       "   'cite_spans': [{'start': 45,\n",
       "     'end': 61,\n",
       "     'text': '{{cite:3b57340}}',\n",
       "     'ref_id': '3b57340'}],\n",
       "   'ref_spans': [{'start': 110,\n",
       "     'end': 158,\n",
       "     'text': '{{formula:8867c3e5-3d24-4636-8ff8-d5750ff52bd6}}',\n",
       "     'ref_id': '8867c3e5-3d24-4636-8ff8-d5750ff52bd6'},\n",
       "    {'start': 207,\n",
       "     'end': 255,\n",
       "     'text': '{{formula:bb5f7f42-117d-46f5-be87-d4e217711613}}',\n",
       "     'ref_id': 'bb5f7f42-117d-46f5-be87-d4e217711613'},\n",
       "    {'start': 264,\n",
       "     'end': 312,\n",
       "     'text': '{{formula:c7ff177d-8ef7-45e2-8103-88b4eea6fb93}}',\n",
       "     'ref_id': 'c7ff177d-8ef7-45e2-8103-88b4eea6fb93'},\n",
       "    {'start': 379,\n",
       "     'end': 427,\n",
       "     'text': '{{formula:7c479112-73be-4fee-8400-e83b7742006f}}',\n",
       "     'ref_id': '7c479112-73be-4fee-8400-e83b7742006f'},\n",
       "    {'start': 435,\n",
       "     'end': 483,\n",
       "     'text': '{{formula:779587b6-9f41-4a76-8eb3-f73e18564535}}',\n",
       "     'ref_id': '779587b6-9f41-4a76-8eb3-f73e18564535'},\n",
       "    {'start': 595,\n",
       "     'end': 643,\n",
       "     'text': '{{formula:24a53d27-11e1-49b4-954e-749af6b21b65}}',\n",
       "     'ref_id': '24a53d27-11e1-49b4-954e-749af6b21b65'},\n",
       "    {'start': 661,\n",
       "     'end': 709,\n",
       "     'text': '{{formula:7773c82b-4644-48fe-bb90-fe64464414c7}}',\n",
       "     'ref_id': '7773c82b-4644-48fe-bb90-fe64464414c7'},\n",
       "    {'start': 715,\n",
       "     'end': 763,\n",
       "     'text': '{{formula:71835fc2-410b-434f-a922-c99ec67fed5b}}',\n",
       "     'ref_id': '71835fc2-410b-434f-a922-c99ec67fed5b'},\n",
       "    {'start': 814,\n",
       "     'end': 862,\n",
       "     'text': '{{formula:fd34ed25-a357-4022-a448-0d9ab5498d99}}',\n",
       "     'ref_id': 'fd34ed25-a357-4022-a448-0d9ab5498d99'},\n",
       "    {'start': 952,\n",
       "     'end': 1000,\n",
       "     'text': '{{formula:7700fca0-2c77-4558-8af4-f8293fa8fa5e}}',\n",
       "     'ref_id': '7700fca0-2c77-4558-8af4-f8293fa8fa5e'},\n",
       "    {'start': 1006,\n",
       "     'end': 1054,\n",
       "     'text': '{{formula:85d627d9-b864-4f30-8fe0-48f9cb3eb9e9}}',\n",
       "     'ref_id': '85d627d9-b864-4f30-8fe0-48f9cb3eb9e9'},\n",
       "    {'start': 1226,\n",
       "     'end': 1274,\n",
       "     'text': '{{formula:b1c614c9-d41b-499f-a879-5ff2c29c8c1f}}',\n",
       "     'ref_id': 'b1c614c9-d41b-499f-a879-5ff2c29c8c1f'},\n",
       "    {'start': 1336,\n",
       "     'end': 1384,\n",
       "     'text': '{{formula:c0f5e532-617b-4ffd-b550-cc7b40bed394}}',\n",
       "     'ref_id': 'c0f5e532-617b-4ffd-b550-cc7b40bed394'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'And the regression loss for the lightweight VQA model {{formula:0c537334-c228-44a1-a7f5-d96b35f54545}}  share the same formulation. A {{formula:1e204ae6-ff4f-43e9-8682-70a86a9a8ed0}}  loss is used for the process of knowledge distillation, which can be written as:\\n{{formula:ad9e366d-6efb-40d9-91ac-fb8b2c2903f1}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 54,\n",
       "     'end': 102,\n",
       "     'text': '{{formula:0c537334-c228-44a1-a7f5-d96b35f54545}}',\n",
       "     'ref_id': '0c537334-c228-44a1-a7f5-d96b35f54545'},\n",
       "    {'start': 134,\n",
       "     'end': 182,\n",
       "     'text': '{{formula:1e204ae6-ff4f-43e9-8682-70a86a9a8ed0}}',\n",
       "     'ref_id': '1e204ae6-ff4f-43e9-8682-70a86a9a8ed0'},\n",
       "    {'start': 265,\n",
       "     'end': 313,\n",
       "     'text': '{{formula:ad9e366d-6efb-40d9-91ac-fb8b2c2903f1}}',\n",
       "     'ref_id': 'ad9e366d-6efb-40d9-91ac-fb8b2c2903f1'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'The whole optimization objective can be formulated as:\\n{{formula:fb3a642c-c877-4f46-9f28-33e9bc338a34}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 55,\n",
       "     'end': 103,\n",
       "     'text': '{{formula:fb3a642c-c877-4f46-9f28-33e9bc338a34}}',\n",
       "     'ref_id': 'fb3a642c-c877-4f46-9f28-33e9bc338a34'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'where {{formula:c8adb2cf-77ae-4bfa-8dc6-03e60eb65f95}}  is a balancing weight for knowledge distillation, and {{formula:6947ab90-a396-45c6-9cf7-8bb891969bbc}}  is a hyper-parameter to balance the level of sparsity.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 6,\n",
       "     'end': 54,\n",
       "     'text': '{{formula:c8adb2cf-77ae-4bfa-8dc6-03e60eb65f95}}',\n",
       "     'ref_id': 'c8adb2cf-77ae-4bfa-8dc6-03e60eb65f95'},\n",
       "    {'start': 110,\n",
       "     'end': 158,\n",
       "     'text': '{{formula:6947ab90-a396-45c6-9cf7-8bb891969bbc}}',\n",
       "     'ref_id': '6947ab90-a396-45c6-9cf7-8bb891969bbc'}]},\n",
       "  {'section': 'Dataset.',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Our method is evaluated on three widely-adopted public NR-VQA datasets, including KoNViD-1k {{cite:24b4d04}}, LIVE-VQC {{cite:8f3eb86}}, and YouTube-UGC {{cite:9be8207}}. Mean opinion scores (MOS) are provided along with training videos.\\nSpecifically, KoNViD-1k contains 1,200 videos that are fairly filtered from a large public video dataset YFCC-100M. The time duration of the video is 8 seconds. And these videos have a frame rate of 24/25/30 FPS and a resolution of {{formula:fdead041-49aa-4d88-b0fb-641cbcf6acb7}} .\\nLIVE-VQC consists of 585 videos with complex authentic distortions, which are captured by 80 users using 101 different devices, ranging from 240P to 1080P.\\nYouTube-UGC has 1,380 UGC videos sampled from YouTube with a duration of 20 seconds and resolutions from 360P to 4K.\\nAll these datasets contain no pristine videos, thus only NR methods can be evaluated on them.\\nFollowing {{cite:c6c0872}}, we split all the dataset into 80% training videos and 20% testing videos randomly.\\n',\n",
       "   'cite_spans': [{'start': 92,\n",
       "     'end': 108,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 119,\n",
       "     'end': 135,\n",
       "     'text': '{{cite:8f3eb86}}',\n",
       "     'ref_id': '8f3eb86'},\n",
       "    {'start': 153,\n",
       "     'end': 169,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'},\n",
       "    {'start': 898,\n",
       "     'end': 914,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'}],\n",
       "   'ref_spans': [{'start': 470,\n",
       "     'end': 518,\n",
       "     'text': '{{formula:fdead041-49aa-4d88-b0fb-641cbcf6acb7}}',\n",
       "     'ref_id': 'fdead041-49aa-4d88-b0fb-641cbcf6acb7'}]},\n",
       "  {'section': 'Evaluation Metric.',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Spearman’s Rank-Order Correlation Coefficient (SRCC) and Pearson’s Linear Correlation Coefficient (PLCC) are selected as metrics to measure the monotonicity and accuracy, respectively. They are in the range of 0.0 to 1.0, and larger values indicate better results. Besides, the mean average of PLCC and SRCC is also reported as a comprehensive criterion.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Implementation Details',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Our method is implemented based on PyTorch {{cite:2e556d9}} and MMAction2 {{cite:a718e63}}. All experiments are conducted on 4 NVIDIA V100 GPUs.\\nFor all datasets, we select EfficientNet-b7, ir-CSN-152, CLIP, Swin Transformer Base, TimeSformer, Video Swin Transformer Base and SlowFast as candidate pretrained models, and choose the Video Swin Transformer Tiny as the lightweight VQA model.\\nFrames are sampled in each video with a fixed temporal step to form a clip input. For frame-wise models (e.g., EfficientNet, CLIP), the feature representation can be calculated through the average features of all frames. For video clip-based models (e.g., SlowFast, ir-CSN-152), the extracted features can be used directly for the video representation.\\nFor KoNViD-1k, we sample 16 frames with a frame interval of 2.\\nAs videos in LIVE-VQC and YouTube-UGC have longer time durations, we sample 64 frames with an interval of 2, and 32 frames with an interval of 8, respectively.\\nSince most augmentations will introduce extra interference to the quality of videos (e.g., resize, color jitter) {{cite:dbe6597}}, we only choose the center crop to produce inputs with a resolution of {{formula:5e2b8317-630a-4bbc-9eaf-21292c9c7555}} .\\nDuring the optimization procedure, we use the AdamW optimizer with a weight decay of 2e-2.\\nA cosine annealing scheduler with a warmup of 2 epochs is adopted to control the learning rate. The initial learning rate is 1e-3. And {{formula:4fed9173-0d04-409f-bbc8-27f03d1fad19}}  is 0.1 by default. {{formula:3f1077df-d8c8-4672-a8ed-9a8461b6450e}}  is set to 0.8. {{formula:1d004647-ed88-4208-a7e8-057ebf756691}}  is set to 32. The batch size of the input is set to 1. All models are trained for 60 epochs. And the checkpoint generated by the last iteration is used for evaluation.\\nFor inference, we follow a similar procedure as {{cite:445c847}} by using {{formula:fb9a7adc-6b8b-4c6a-8343-9dbbe16247f6}}  views. In the procedure, a video is uniformly sampled as 4 clips in the temporal dimension, and for each clip, the shorter spatial side is scaled to 256 pixels and we take 5 crops in the four corners and the center. The final score is computed as the average score of all the views. The average result of 10 repeat runs with different random splits is used as the final score for the experiments in Tab.\\xa0{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}} .\\n',\n",
       "   'cite_spans': [{'start': 43,\n",
       "     'end': 59,\n",
       "     'text': '{{cite:2e556d9}}',\n",
       "     'ref_id': '2e556d9'},\n",
       "    {'start': 74, 'end': 90, 'text': '{{cite:a718e63}}', 'ref_id': 'a718e63'},\n",
       "    {'start': 1079,\n",
       "     'end': 1095,\n",
       "     'text': '{{cite:dbe6597}}',\n",
       "     'ref_id': 'dbe6597'},\n",
       "    {'start': 1844,\n",
       "     'end': 1860,\n",
       "     'text': '{{cite:445c847}}',\n",
       "     'ref_id': '445c847'}],\n",
       "   'ref_spans': [{'start': 1167,\n",
       "     'end': 1215,\n",
       "     'text': '{{formula:5e2b8317-630a-4bbc-9eaf-21292c9c7555}}',\n",
       "     'ref_id': '5e2b8317-630a-4bbc-9eaf-21292c9c7555'},\n",
       "    {'start': 1444,\n",
       "     'end': 1492,\n",
       "     'text': '{{formula:4fed9173-0d04-409f-bbc8-27f03d1fad19}}',\n",
       "     'ref_id': '4fed9173-0d04-409f-bbc8-27f03d1fad19'},\n",
       "    {'start': 1513,\n",
       "     'end': 1561,\n",
       "     'text': '{{formula:3f1077df-d8c8-4672-a8ed-9a8461b6450e}}',\n",
       "     'ref_id': '3f1077df-d8c8-4672-a8ed-9a8461b6450e'},\n",
       "    {'start': 1578,\n",
       "     'end': 1626,\n",
       "     'text': '{{formula:1d004647-ed88-4208-a7e8-057ebf756691}}',\n",
       "     'ref_id': '1d004647-ed88-4208-a7e8-057ebf756691'},\n",
       "    {'start': 1870,\n",
       "     'end': 1918,\n",
       "     'text': '{{formula:fb9a7adc-6b8b-4c6a-8343-9dbbe16247f6}}',\n",
       "     'ref_id': 'fb9a7adc-6b8b-4c6a-8343-9dbbe16247f6'},\n",
       "    {'start': 2324,\n",
       "     'end': 2370,\n",
       "     'text': '{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "     'ref_id': 'a8b624dc-b470-4046-8f35-b6e5534343a4'}]},\n",
       "  {'section': 'Implementation Details',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Comparisons with SOTA methods. The up arrow “{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are highlighted and underlined. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.\\n2*Method\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\n\\nSRCC{{formula:585eb95b-1547-4785-a382-4d02cb7a7274}}\\nPLCC{{formula:2f43609f-d5b9-4947-a668-77916eaab08b}}\\nMean\\nSRCC{{formula:0ab3c965-7cc8-475b-9cc8-a434f557a90f}}\\nPLCC{{formula:91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0}}\\nMean\\nSRCC{{formula:fe38b572-b993-4f84-b520-4ca75bc3ba79}}\\nPLCC{{formula:ef0f4492-775e-4bdf-b2d5-8c30a10f8946}}\\nMean\\nVIIDEO {{cite:0832a8c}}\\n0.2980\\n0.3030\\n0.3005\\n0.0332\\n0.2164\\n0.1248\\n0.0580\\n0.1534\\n0.1057\\nNIQE {{cite:c619c6d}}\\n0.5417\\n0.5530\\n0.5474\\n0.5957\\n0.6286\\n0.6122\\n0.2379\\n0.2776\\n0.2578\\nBRISQUE {{cite:18dcc02}}\\n0.654\\n0.626\\n0.640\\n0.592\\n0.638\\n0.615\\n0.382\\n0.395\\n0.389\\nVSFA {{cite:7da4179}}\\n0.755\\n0.744\\n0.750\\n-\\n-\\n-\\n-\\n-\\n-\\nTLVQM {{cite:1af68c6}}\\n0.7729\\n0.7688\\n0.7709\\n0.7988\\n0.8025\\n0.8807\\n0.6693\\n0.6590\\n0.6642\\nRIRNet {{cite:04485d0}}\\n0.7755\\n0.7812\\n0.7784\\n0.7713\\n0.7982\\n0.7848\\n-\\n-\\n-\\nMDTVSFA {{cite:1ad644b}}\\n0.7812\\n0.7856\\n0.7834\\n0.7382\\n0.7728\\n0.7555\\n-\\n-\\n-\\nRIRNet+CSPT {{cite:b519bb8}}\\n0.8008\\n0.8062\\n0.8035\\n0.7989\\n0.8194\\n0.8092\\n-\\n-\\n-\\nCoINVQ {{cite:80cfe88}}\\n0.802\\n0.816\\n0.809\\n-\\n-\\n-\\n0.764\\n0.767\\n0.766\\nRAPIQUE {{cite:f7fae73}}\\n0.8031\\n0.8175\\n0.8103\\n0.7548\\n0.7863\\n0.7706\\n0.7591\\n0.7684\\n0.7638\\nStarVQA {{cite:1c7237b}}\\n0.812\\n0.796\\n0.804\\n0.732\\n0.808\\n0.770\\n-\\n-\\n-\\nBVQA* {{cite:48735e5}}\\n0.8362\\n0.8335\\n0.8349\\n0.8412\\n0.8415\\n0.8414\\n0.8312\\n0.8194\\n0.8253\\nSTDAM {{cite:c6c0872}}\\n0.8448\\n0.8415\\n0.8432\\n0.7931\\n0.8204\\n0.8068\\n0.8341\\n0.8297\\n0.8319\\nDisCoVQA {{cite:ebacaf2}}\\n0.847\\n0.847\\n0.847\\n0.820\\n0.826\\n0.823\\n-\\n-\\n-\\nFastVQA {{cite:f3075fd}}\\n0.859\\n0.855\\n0.857\\n0.823\\n0.844\\n0.834\\n-\\n-\\n-\\nVideo Swin Tiny\\n0.8316\\n0.8694\\n0.8505\\n0.8335\\n0.8316\\n0.8326\\n0.8566\\n0.8499\\n0.8533\\nAda-DQA\\n0.8651\\n0.8831\\n0.8741\\n0.8591\\n0.8587\\n0.8589\\n0.8729\\n0.8800\\n0.8765\\n{{formula:16e73452-3bb6-49a1-9157-f959b7b592f5}}  than the previous best\\n+0.6%\\n+2.8%\\n+1.7%\\n+1.79%\\n+1.72%\\n+1.75%\\n+3.88%\\n+5.03%\\n+4.46%\\n{{formula:0818a4f1-d597-4afa-8fd6-78b317784b57}}  than w/o pretrained models\\n+3.35%\\n+1.37%\\n+2.36%\\n+2.56%\\n+2.71%\\n+2.63%\\n+1.63%\\n+3.01%\\n+2.32%\\n{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "   'cite_spans': [{'start': 918,\n",
       "     'end': 934,\n",
       "     'text': '{{cite:0832a8c}}',\n",
       "     'ref_id': '0832a8c'},\n",
       "    {'start': 1003,\n",
       "     'end': 1019,\n",
       "     'text': '{{cite:c619c6d}}',\n",
       "     'ref_id': 'c619c6d'},\n",
       "    {'start': 1091,\n",
       "     'end': 1107,\n",
       "     'text': '{{cite:18dcc02}}',\n",
       "     'ref_id': '18dcc02'},\n",
       "    {'start': 1167,\n",
       "     'end': 1183,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 1220,\n",
       "     'end': 1236,\n",
       "     'text': '{{cite:1af68c6}}',\n",
       "     'ref_id': '1af68c6'},\n",
       "    {'start': 1307,\n",
       "     'end': 1323,\n",
       "     'text': '{{cite:04485d0}}',\n",
       "     'ref_id': '04485d0'},\n",
       "    {'start': 1380,\n",
       "     'end': 1396,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 1457,\n",
       "     'end': 1473,\n",
       "     'text': '{{cite:b519bb8}}',\n",
       "     'ref_id': 'b519bb8'},\n",
       "    {'start': 1529,\n",
       "     'end': 1545,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 1596,\n",
       "     'end': 1612,\n",
       "     'text': '{{cite:f7fae73}}',\n",
       "     'ref_id': 'f7fae73'},\n",
       "    {'start': 1684,\n",
       "     'end': 1700,\n",
       "     'text': '{{cite:1c7237b}}',\n",
       "     'ref_id': '1c7237b'},\n",
       "    {'start': 1749,\n",
       "     'end': 1765,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'},\n",
       "    {'start': 1835,\n",
       "     'end': 1851,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'},\n",
       "    {'start': 1924,\n",
       "     'end': 1940,\n",
       "     'text': '{{cite:ebacaf2}}',\n",
       "     'ref_id': 'ebacaf2'},\n",
       "    {'start': 1991,\n",
       "     'end': 2007,\n",
       "     'text': '{{cite:f3075fd}}',\n",
       "     'ref_id': 'f3075fd'}],\n",
       "   'ref_spans': [{'start': 45,\n",
       "     'end': 93,\n",
       "     'text': '{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}}',\n",
       "     'ref_id': '4e18f964-3331-45fa-b22d-7351dde0b937'},\n",
       "    {'start': 582,\n",
       "     'end': 630,\n",
       "     'text': '{{formula:585eb95b-1547-4785-a382-4d02cb7a7274}}',\n",
       "     'ref_id': '585eb95b-1547-4785-a382-4d02cb7a7274'},\n",
       "    {'start': 635,\n",
       "     'end': 683,\n",
       "     'text': '{{formula:2f43609f-d5b9-4947-a668-77916eaab08b}}',\n",
       "     'ref_id': '2f43609f-d5b9-4947-a668-77916eaab08b'},\n",
       "    {'start': 693,\n",
       "     'end': 741,\n",
       "     'text': '{{formula:0ab3c965-7cc8-475b-9cc8-a434f557a90f}}',\n",
       "     'ref_id': '0ab3c965-7cc8-475b-9cc8-a434f557a90f'},\n",
       "    {'start': 746,\n",
       "     'end': 794,\n",
       "     'text': '{{formula:91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0}}',\n",
       "     'ref_id': '91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0'},\n",
       "    {'start': 804,\n",
       "     'end': 852,\n",
       "     'text': '{{formula:fe38b572-b993-4f84-b520-4ca75bc3ba79}}',\n",
       "     'ref_id': 'fe38b572-b993-4f84-b520-4ca75bc3ba79'},\n",
       "    {'start': 857,\n",
       "     'end': 905,\n",
       "     'text': '{{formula:ef0f4492-775e-4bdf-b2d5-8c30a10f8946}}',\n",
       "     'ref_id': 'ef0f4492-775e-4bdf-b2d5-8c30a10f8946'},\n",
       "    {'start': 2200,\n",
       "     'end': 2248,\n",
       "     'text': '{{formula:16e73452-3bb6-49a1-9157-f959b7b592f5}}',\n",
       "     'ref_id': '16e73452-3bb6-49a1-9157-f959b7b592f5'},\n",
       "    {'start': 2333,\n",
       "     'end': 2381,\n",
       "     'text': '{{formula:0818a4f1-d597-4afa-8fd6-78b317784b57}}',\n",
       "     'ref_id': '0818a4f1-d597-4afa-8fd6-78b317784b57'},\n",
       "    {'start': 2473,\n",
       "     'end': 2519,\n",
       "     'text': '{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "     'ref_id': 'a8b624dc-b470-4046-8f35-b6e5534343a4'}]},\n",
       "  {'section': 'Comparison with SOTA methods',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'We report the SRCC and PLCC performance with current SOTA methods on KoNViD-1k, LIVE-VQC, and YouTube-UGC.\\nAs shown in Tab.\\xa0{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}} , our method achieves new state-of-the-art results on all these datasets without using extra training data of QA.\\nSome observations can also be found through these results.\\nWe can observe those deep learning-based methods outperform the traditional hand-crafted method (e.g., VIIDEO, NIQE) largely.\\nBesides, within deep learning-based methods, VQA methods produce much better performance than IQA methods (e.g., BRISQUE). Since there exist many temporal-distributed distortions in these datasets, VQA models can capture this temporal information.\\nAda-DQA outperforms StarVQA, which builds a Transformer model to capture spatiotemporal information, in large margins.\\nBVQA incorporates extra training data from other QA datasets for the feature extractor. And our method still outperforms it without using external training data of QA (+2.89% of SRCC, and + 4.96% of PLCC in KoNViD-1k). This shows the advantage of leveraging quality-related knowledge from pretrained models.\\nCompare with the current best method of STDAM, DiscoVQA, and FastVQA, Ada-DQA improves the best performance in large margins to {{formula:b96bdb34-83cc-4e41-a47f-eded16f61bd3}}  of SRCC in KoNVid-1k (+0.6%), {{formula:4a49b392-ff35-4621-856e-a4b467efec3e}}  of SRCC in LIVE-VQC (+1.79%) and {{formula:172716b6-1c7e-4113-a4cc-f6314208c987}}  of SRCC in YouTube-UGC (+3.88%). The accuracy and consistency of prediction results have been significantly improved.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 124,\n",
       "     'end': 170,\n",
       "     'text': '{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "     'ref_id': 'a8b624dc-b470-4046-8f35-b6e5534343a4'},\n",
       "    {'start': 1273,\n",
       "     'end': 1321,\n",
       "     'text': '{{formula:b96bdb34-83cc-4e41-a47f-eded16f61bd3}}',\n",
       "     'ref_id': 'b96bdb34-83cc-4e41-a47f-eded16f61bd3'},\n",
       "    {'start': 1353,\n",
       "     'end': 1401,\n",
       "     'text': '{{formula:4a49b392-ff35-4621-856e-a4b467efec3e}}',\n",
       "     'ref_id': '4a49b392-ff35-4621-856e-a4b467efec3e'},\n",
       "    {'start': 1436,\n",
       "     'end': 1484,\n",
       "     'text': '{{formula:172716b6-1c7e-4113-a4cc-f6314208c987}}',\n",
       "     'ref_id': '172716b6-1c7e-4113-a4cc-f6314208c987'}]},\n",
       "  {'section': 'Experimental Analysis and Ablation Studies',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Experimental analysis on different numbers of selected pretrained models with or without the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of with or without is bolded.\\nNumber\\nQAM\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\n3\\n{{formula:520102f0-4552-44e5-b853-ef384910ec7d}}\\n0.8517\\n0.8432\\n0.8630\\n4\\n{{formula:5c208282-8d62-42d5-8381-fcf71ca99c11}}\\n0.8613\\n0.8510\\n0.8659\\n5\\n{{formula:55c39428-0e5d-40b5-b2d8-c5511775ee50}}\\n0.8601\\n0.8490\\n0.8591\\n6\\n{{formula:4fbff708-17bc-428a-b5fd-7a2dec20e294}}\\n0.8615\\n0.8448\\n0.8589\\n7\\n{{formula:d8c752b2-c211-409d-a42a-164151334db2}}\\n0.8573\\n0.8459\\n0.8621\\n3\\n\\n0.8432\\n0.8433\\n0.8621\\n4\\n\\n0.8623\\n0.8514\\n0.8695\\n5\\n\\n0.8621\\n0.8546\\n0.8679\\n6\\n\\n0.8645\\n0.8542\\n0.8644\\n7\\n\\n0.8651\\n0.8591\\n0.8729\\n8\\n\\n0.8641\\n0.8560\\n0.8711\\n{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 265,\n",
       "     'end': 313,\n",
       "     'text': '{{formula:520102f0-4552-44e5-b853-ef384910ec7d}}',\n",
       "     'ref_id': '520102f0-4552-44e5-b853-ef384910ec7d'},\n",
       "    {'start': 337,\n",
       "     'end': 385,\n",
       "     'text': '{{formula:5c208282-8d62-42d5-8381-fcf71ca99c11}}',\n",
       "     'ref_id': '5c208282-8d62-42d5-8381-fcf71ca99c11'},\n",
       "    {'start': 409,\n",
       "     'end': 457,\n",
       "     'text': '{{formula:55c39428-0e5d-40b5-b2d8-c5511775ee50}}',\n",
       "     'ref_id': '55c39428-0e5d-40b5-b2d8-c5511775ee50'},\n",
       "    {'start': 481,\n",
       "     'end': 529,\n",
       "     'text': '{{formula:4fbff708-17bc-428a-b5fd-7a2dec20e294}}',\n",
       "     'ref_id': '4fbff708-17bc-428a-b5fd-7a2dec20e294'},\n",
       "    {'start': 553,\n",
       "     'end': 601,\n",
       "     'text': '{{formula:d8c752b2-c211-409d-a42a-164151334db2}}',\n",
       "     'ref_id': 'd8c752b2-c211-409d-a42a-164151334db2'},\n",
       "    {'start': 767,\n",
       "     'end': 813,\n",
       "     'text': '{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}}',\n",
       "     'ref_id': '8d9a3ded-086a-4687-a726-4cc648dbc022'}]},\n",
       "  {'section': 'Experimental Analysis and Ablation Studies',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.\\nPretrained Model\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\nw/o\\n0.8316\\n0.8335\\n0.8566\\nEfficientNet-b7\\n0.8412\\n0.8495\\n0.8587\\nVideo Swin Base\\n0.8390\\n0.8173\\n0.8603\\nSwin Base\\n0.8391\\n0.8475\\n0.8497\\nTimeSformer\\n0.8409\\n0.8302\\n0.8618\\nCLIP\\n0.8404\\n0.8341\\n0.8608\\nir-CNS-152\\n0.8458\\n0.8521\\n0.8518\\nSlowFast\\n0.8423\\n0.8041\\n0.8523\\n{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 471,\n",
       "     'end': 517,\n",
       "     'text': '{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}}',\n",
       "     'ref_id': 'b84d94e4-9350-4014-bc1b-c8b4434ebf24'}]},\n",
       "  {'section': 'Number of pretrained models and effectiveness of sparsity constraint in QAM',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To investigate the impact of the number of pretrained models, we performed experiments by reducing the number of models from 7 to 3. As depicted in Tab.\\xa0{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}} , increasing the number of models does not always lead to better performance without the use of sparsity constraints in QAM. With the help of sparsity constraint, the model can achieve continuous improvement as the number of pretrained models increases. However, adding more models beyond 8 (introducing an extra model of ViT Base) does not yield any further improvements. This may indicate that the quality-related information provided by the pretrained models has reached a saturation point. Therefore, we set the number of pretrained models to 7 in our experiments. In these experiments, the pretrained model is randomly selected and removed.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 153,\n",
       "     'end': 199,\n",
       "     'text': '{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}}',\n",
       "     'ref_id': '8d9a3ded-086a-4687-a726-4cc648dbc022'}]},\n",
       "  {'section': 'The necessary of diverse pretrained models',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Moreover, we also conduct experiments solely utilizing a singular pretrained model to demonstrate the disparity among models. As depicted in Tab.\\xa0{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}} , a solitary pretrained model cannot consistently attain optimal outcomes across all VQA datasets. For instance, while CLIP excels on YouTube-UGC, it falls considerably short on the other two datasets. We posit that this is influenced by the correlation between pretrained models, such as pre-text task, dataset, architecture, and VQA tasks. Additionally, the results obtained by utilizing singular pretrained models are notably distant from the state-of-the-art. These findings substantiate that a solitary pretrained model is inadequate for diverse application scenarios, and leveraging a variety of pretrained models is imperative.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 146,\n",
       "     'end': 192,\n",
       "     'text': '{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}}',\n",
       "     'ref_id': 'b84d94e4-9350-4014-bc1b-c8b4434ebf24'}]},\n",
       "  {'section': 'The necessary of diverse pretrained models',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Experiments on different distillation losses.\\ndistillation loss\\nSRCC{{formula:c4c815c4-2362-448d-a544-b0fd44f79d70}}\\nPLCC{{formula:11c7a23f-f47a-40f7-83f2-342c6590e0c2}}\\n{{formula:a29e8124-9675-4061-9d6d-977cb9100a57}}\\n0.8651\\n0.8831\\n{{formula:101515c0-83a5-489c-9c6f-dbb3358b1e5e}}\\n0.8092\\n0.8310\\nJesen-Shannon\\n0.8455\\n0.8646\\n{{table:141b825f-8848-4126-9985-77db3f3c4705}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 68,\n",
       "     'end': 116,\n",
       "     'text': '{{formula:c4c815c4-2362-448d-a544-b0fd44f79d70}}',\n",
       "     'ref_id': 'c4c815c4-2362-448d-a544-b0fd44f79d70'},\n",
       "    {'start': 121,\n",
       "     'end': 169,\n",
       "     'text': '{{formula:11c7a23f-f47a-40f7-83f2-342c6590e0c2}}',\n",
       "     'ref_id': '11c7a23f-f47a-40f7-83f2-342c6590e0c2'},\n",
       "    {'start': 170,\n",
       "     'end': 218,\n",
       "     'text': '{{formula:a29e8124-9675-4061-9d6d-977cb9100a57}}',\n",
       "     'ref_id': 'a29e8124-9675-4061-9d6d-977cb9100a57'},\n",
       "    {'start': 233,\n",
       "     'end': 281,\n",
       "     'text': '{{formula:101515c0-83a5-489c-9c6f-dbb3358b1e5e}}',\n",
       "     'ref_id': '101515c0-83a5-489c-9c6f-dbb3358b1e5e'},\n",
       "    {'start': 324,\n",
       "     'end': 370,\n",
       "     'text': '{{table:141b825f-8848-4126-9985-77db3f3c4705}}',\n",
       "     'ref_id': '141b825f-8848-4126-9985-77db3f3c4705'}]},\n",
       "  {'section': 'Different types of distillation loss',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Experiments on different knowledge distillation losses are performed in KoNViD-1k, including the {{formula:addaeb8c-543e-4208-9329-786e534ef54b}}  loss, the {{formula:fa25327b-5f45-4ab7-a127-f2ffeccd55ae}}  loss and the Jesen-Shannon (JS) loss {{cite:9306c46}}. As shown in Tab.\\xa0{{table:141b825f-8848-4126-9985-77db3f3c4705}} , the {{formula:37cec503-545d-43b4-b8f9-58076991d335}}  loss owns the best performance in transferring aggregated features by multiple teacher models.\\n',\n",
       "   'cite_spans': [{'start': 244,\n",
       "     'end': 260,\n",
       "     'text': '{{cite:9306c46}}',\n",
       "     'ref_id': '9306c46'}],\n",
       "   'ref_spans': [{'start': 97,\n",
       "     'end': 145,\n",
       "     'text': '{{formula:addaeb8c-543e-4208-9329-786e534ef54b}}',\n",
       "     'ref_id': 'addaeb8c-543e-4208-9329-786e534ef54b'},\n",
       "    {'start': 157,\n",
       "     'end': 205,\n",
       "     'text': '{{formula:fa25327b-5f45-4ab7-a127-f2ffeccd55ae}}',\n",
       "     'ref_id': 'fa25327b-5f45-4ab7-a127-f2ffeccd55ae'},\n",
       "    {'start': 279,\n",
       "     'end': 325,\n",
       "     'text': '{{table:141b825f-8848-4126-9985-77db3f3c4705}}',\n",
       "     'ref_id': '141b825f-8848-4126-9985-77db3f3c4705'},\n",
       "    {'start': 332,\n",
       "     'end': 380,\n",
       "     'text': '{{formula:37cec503-545d-43b4-b8f9-58076991d335}}',\n",
       "     'ref_id': '37cec503-545d-43b4-b8f9-58076991d335'}]},\n",
       "  {'section': 'Selection of hyper-parameters',\n",
       "   'sec_number': '6',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'We conduct experiments to show how the hyper-parameters {{formula:40079db8-ea5e-4e4e-872e-d09c7bdd43c8}}  and {{formula:b59cd732-43a5-4abc-b5fb-2b9d28fbbbc6}}  in Equ.\\xa0REF  will influence the final results in KoNViD-1k. The results are listed in Tab.\\xa0{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}  and {{table:56c4b01b-b705-42bd-a314-8bed4c21515d}} . When {{formula:026ff505-2042-4124-aaf5-6b083b5b9fd3}}  is 0.1, and {{formula:aa5777b2-7128-4723-9351-4d3a83c1855b}}  is 0.8, which are used in our experiments, the best performance can be obtained.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 56,\n",
       "     'end': 104,\n",
       "     'text': '{{formula:40079db8-ea5e-4e4e-872e-d09c7bdd43c8}}',\n",
       "     'ref_id': '40079db8-ea5e-4e4e-872e-d09c7bdd43c8'},\n",
       "    {'start': 110,\n",
       "     'end': 158,\n",
       "     'text': '{{formula:b59cd732-43a5-4abc-b5fb-2b9d28fbbbc6}}',\n",
       "     'ref_id': 'b59cd732-43a5-4abc-b5fb-2b9d28fbbbc6'},\n",
       "    {'start': 251,\n",
       "     'end': 297,\n",
       "     'text': '{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "     'ref_id': '56c4b01b-b705-42bd-a314-8bed4c21515d'},\n",
       "    {'start': 303,\n",
       "     'end': 349,\n",
       "     'text': '{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "     'ref_id': '56c4b01b-b705-42bd-a314-8bed4c21515d'},\n",
       "    {'start': 357,\n",
       "     'end': 405,\n",
       "     'text': '{{formula:026ff505-2042-4124-aaf5-6b083b5b9fd3}}',\n",
       "     'ref_id': '026ff505-2042-4124-aaf5-6b083b5b9fd3'},\n",
       "    {'start': 419,\n",
       "     'end': 467,\n",
       "     'text': '{{formula:aa5777b2-7128-4723-9351-4d3a83c1855b}}',\n",
       "     'ref_id': 'aa5777b2-7128-4723-9351-4d3a83c1855b'}]},\n",
       "  {'section': 'Selection of hyper-parameters',\n",
       "   'sec_number': '6',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Selection of the hyper-parameters of {{formula:a9eb32d2-012d-403d-a083-c08a11dcce5f}}  and {{formula:f0ad8386-af17-423d-b208-6cd0a2cb4525}} .\\n{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}\\nSRCC{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}\\nPLCC{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}\\n0.1\\n0.8651\\n0.8831\\n0.2\\n0.8462\\n0.8663\\n0.5\\n0.8406\\n0.8698\\n1.0\\n0.8320\\n0.8549\\n{{table:85a7afd7-6fb3-452c-bc87-76bfffd63be7}}{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}\\nSRCC{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}\\nPLCC{{formula:1b522aaf-e830-438c-a148-584b695753ab}}\\n0.8\\n0.8651\\n0.8831\\n0.2\\n0.8449\\n0.8671\\n0.5\\n0.8528\\n0.8716\\n1.0\\n0.8602\\n0.8780\\n{{table:34357c34-625c-4c92-b9f3-1cbd27a380a5}}\\n\\n[t]0.23\\nhyper-parameter {{formula:28690097-c6c4-4e25-93c5-fa850bcd8cdf}}[t]0.23\\nhyper-parameter {{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}} .{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 37,\n",
       "     'end': 85,\n",
       "     'text': '{{formula:a9eb32d2-012d-403d-a083-c08a11dcce5f}}',\n",
       "     'ref_id': 'a9eb32d2-012d-403d-a083-c08a11dcce5f'},\n",
       "    {'start': 91,\n",
       "     'end': 139,\n",
       "     'text': '{{formula:f0ad8386-af17-423d-b208-6cd0a2cb4525}}',\n",
       "     'ref_id': 'f0ad8386-af17-423d-b208-6cd0a2cb4525'},\n",
       "    {'start': 142,\n",
       "     'end': 190,\n",
       "     'text': '{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}',\n",
       "     'ref_id': '6aabc355-4353-495e-8754-ba127a2ca29a'},\n",
       "    {'start': 195,\n",
       "     'end': 243,\n",
       "     'text': '{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}',\n",
       "     'ref_id': '6d8e62cd-c09b-448d-97de-3ce5403c6544'},\n",
       "    {'start': 248,\n",
       "     'end': 296,\n",
       "     'text': '{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}',\n",
       "     'ref_id': '8616041b-c2cc-4e7a-9dee-a47db9687eb5'},\n",
       "    {'start': 369,\n",
       "     'end': 415,\n",
       "     'text': '{{table:85a7afd7-6fb3-452c-bc87-76bfffd63be7}}',\n",
       "     'ref_id': '85a7afd7-6fb3-452c-bc87-76bfffd63be7'},\n",
       "    {'start': 415,\n",
       "     'end': 463,\n",
       "     'text': '{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}',\n",
       "     'ref_id': '66f56494-555a-41eb-94ca-a691e8a05223'},\n",
       "    {'start': 468,\n",
       "     'end': 516,\n",
       "     'text': '{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}',\n",
       "     'ref_id': '37c6ba32-0e69-4ace-b358-e5868c67c743'},\n",
       "    {'start': 521,\n",
       "     'end': 569,\n",
       "     'text': '{{formula:1b522aaf-e830-438c-a148-584b695753ab}}',\n",
       "     'ref_id': '1b522aaf-e830-438c-a148-584b695753ab'},\n",
       "    {'start': 642,\n",
       "     'end': 688,\n",
       "     'text': '{{table:34357c34-625c-4c92-b9f3-1cbd27a380a5}}',\n",
       "     'ref_id': '34357c34-625c-4c92-b9f3-1cbd27a380a5'},\n",
       "    {'start': 714,\n",
       "     'end': 762,\n",
       "     'text': '{{formula:28690097-c6c4-4e25-93c5-fa850bcd8cdf}}',\n",
       "     'ref_id': '28690097-c6c4-4e25-93c5-fa850bcd8cdf'},\n",
       "    {'start': 786,\n",
       "     'end': 834,\n",
       "     'text': '{{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}}',\n",
       "     'ref_id': '1b960ffa-8317-4b49-9ab6-0973f5604b07'},\n",
       "    {'start': 836,\n",
       "     'end': 882,\n",
       "     'text': '{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "     'ref_id': '56c4b01b-b705-42bd-a314-8bed4c21515d'}]},\n",
       "  {'section': 'Selection of hyper-parameters',\n",
       "   'sec_number': '6',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Contributions of different pretrained models by {{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}} .\\nModel\\n{{formula:cb8e8db9-0b6f-4095-8935-43c745009f52}}  for LQ videos\\n{{formula:5e218773-7abe-4ee6-836b-4e4e6998017a}}  for HQ videos\\nEfficientNet-b7\\n0.1303\\n0.7208\\nVideo Swin Base\\n0.2021\\n0.0455\\nSwin Base\\n0.1676\\n0.6805\\nTimeSformer\\n0.4679\\n0.2317\\nCLIP\\n0.1279\\n0.3567\\nir-CNS-152\\n0.8690\\n0.2788\\nSlowFast\\n0.4210\\n0.0018\\n{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 48,\n",
       "     'end': 96,\n",
       "     'text': '{{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}}',\n",
       "     'ref_id': '56ce4a4e-539b-45af-9a31-8eacdf6234a6'},\n",
       "    {'start': 105,\n",
       "     'end': 153,\n",
       "     'text': '{{formula:cb8e8db9-0b6f-4095-8935-43c745009f52}}',\n",
       "     'ref_id': 'cb8e8db9-0b6f-4095-8935-43c745009f52'},\n",
       "    {'start': 169,\n",
       "     'end': 217,\n",
       "     'text': '{{formula:5e218773-7abe-4ee6-836b-4e4e6998017a}}',\n",
       "     'ref_id': '5e218773-7abe-4ee6-836b-4e4e6998017a'},\n",
       "    {'start': 410,\n",
       "     'end': 456,\n",
       "     'text': '{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}}',\n",
       "     'ref_id': '4588288c-950a-42b5-9e99-a9926c75ff3a'}]},\n",
       "  {'section': 'Contribution of different pretrained models',\n",
       "   'sec_number': '7',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To investigate the contribution, we analyze the gating weights {{formula:45adcc6d-d8f1-43bf-97ef-41f131219f5d}}  generated by the QAM in KoNViD-1k. The statistical average scores for different models are calculated.\\nWe count responses of low-quality (LQ, MOS<3.5) and high-quality (HQ, MOS>3.5) videos in Tab.\\xa0{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}} . It can be seen that for LQ videos, models that can provide distortion and motion-related information (e.g., ir-CSN-152) have larger weights; for HQ videos, models that can provide content-related one (e.g., EfficientNet-b7) own larger weights.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 63,\n",
       "     'end': 111,\n",
       "     'text': '{{formula:45adcc6d-d8f1-43bf-97ef-41f131219f5d}}',\n",
       "     'ref_id': '45adcc6d-d8f1-43bf-97ef-41f131219f5d'},\n",
       "    {'start': 310,\n",
       "     'end': 356,\n",
       "     'text': '{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}}',\n",
       "     'ref_id': '4588288c-950a-42b5-9e99-a9926c75ff3a'}]},\n",
       "  {'section': 'Computational cost',\n",
       "   'sec_number': '8',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'We compare the #Params, #FLOPs, and SRCC of Ada-DQA over some SOTA methods whose models are available: Ada-DQA (29M, 88T, 0.8651), MDTVSFA (24M, 168T, 0.7812), StarVQA (121M, 75T, 0.812), BVQA (24M, 240T, 0.8362). With the help of pretrained models during training, Ada-DQA obtains higher results with a fair cost during inference.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Visualization of the Attention',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Some representative videos in KoNViD-1k are selected to show the performance improvement brought by Ada-DQA. Visualization of the feature attention maps using GradCAM {{cite:ecf1e0d}} are shown in Fig.\\xa0{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}} . After introducing the adaptive acquisition strategy, Ada-DQA generates more accurate results and the attention maps highlight more quality-related regions.\\nFor instance, in the first video clip, attention from the vast surface of the ocean (left) is transferred to the sailboat, with some attention on ocean waves kept (right). Ada-DQA focuses on areas more related to the action (boat sailing) or giving clues about the perceptual quality (edges of waves).\\n',\n",
       "   'cite_spans': [{'start': 167,\n",
       "     'end': 183,\n",
       "     'text': '{{cite:ecf1e0d}}',\n",
       "     'ref_id': 'ecf1e0d'}],\n",
       "   'ref_spans': [{'start': 202,\n",
       "     'end': 249,\n",
       "     'text': '{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}}',\n",
       "     'ref_id': 'f37ff774-dbb6-49f8-afdf-a7ec9619c959'}]},\n",
       "  {'section': 'Visualization of the Attention',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Comparison of predictions and attention visualizations. For each video, video frames (left) and attention maps (right) redbefore and greenafter using Ada-DQA are illustrated.\\n{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 175,\n",
       "     'end': 222,\n",
       "     'text': '{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}}',\n",
       "     'ref_id': 'f37ff774-dbb6-49f8-afdf-a7ec9619c959'}]},\n",
       "  {'section': 'Conclusion',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To address the issue of insufficient training data in VQA, this paper analyzes the entire spectrum of video distribution diversity that impacts quality and proposes the Ada-DQA framework, which employs a range of diverse pretrained models to improve quality representation. With Ada-DQA, it becomes possible to extract critical and relevant features generated by different frozen pretrained models adaptively. Experimental results on three mainstream NR-VQA benchmarks show the effectiveness in the context of limited data. Thorough analysis and ablation studies also validate the necessity of each component. This work hopes to inspire future research that leverages pretrained models to aid in a wider array of tasks.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Acknowledgments',\n",
       "   'sec_number': '-1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'This research was partly supported by the National Key R&D Program of China (Grant No. 2020AAA0108303), and Shenzhen Science and Technology Project (Grant No. JCYJ20200109143041798) and Shenzhen Stable Supporting Program (No. WDZC20200820200655001) and Shenzhen Key Laboratory of next-generation interactive media innovative technology (No. ZDSYS20210623092001004).\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Acknowledgments',\n",
       "   'sec_number': '-1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'Bibliography',\n",
       "   'text': 'Anurag Arnab, Mostafa\\nDehghani, Georg Heigold, Chen Sun,\\nMario Lucic, and Cordelia Schmid.\\n2021.\\nViViT: A Video Vision Transformer.\\nCoRR abs/2103.15691\\n(2021).\\nGedas Bertasius, Heng\\nWang, and Lorenzo Torresani.\\n2021.\\nIs Space-Time Attention All You Need for Video\\nUnderstanding?. In ICML,\\nVol.\\xa0139. PMLR,\\n813–824.\\nJoão Carreira, Eric\\nNoland, Andras Banki-Horvath, Chloe\\nHillier, and Andrew Zisserman.\\n2018.\\nA Short Note about Kinetics-600.\\nCoRR abs/1808.01340\\n(2018).\\nAaron Chadha and Yiannis\\nAndreopoulos. 2021.\\nDeep Perceptual Preprocessing for Video Coding. In\\nCVPR. IEEE,\\n14852–14861.\\nKuan-Ta Chen, Chi-Jui\\nChang, Chen-Chi Wu, Yu-Chun Chang,\\nand Chin-Laung Lei. 2010.\\nQuadrant of euphoria: a crowdsourcing platform for\\nQoE assessment.\\nIEEE Netw. 24,\\n2 (2010), 28–35.\\nhttps://doi.org/10.1109/MNET.2010.5430141\\nPengfei Chen, Leida Li,\\nLei Ma, Jinjian Wu, and\\nGuangming Shi. 2020b.\\nRIRNet: Recurrent-In-Recurrent Network for Video\\nQuality Assessment. In ACM MM.\\nACM, 834–842.\\nPengfei Chen, Leida Li,\\nJinjian Wu, Weisheng Dong, and\\nGuangming Shi. 2022.\\nContrastive Self-Supervised Pre-Training for Video\\nQuality Assessment.\\nIEEE Trans. Image Process.\\n31 (2022), 458–471.\\nTing Chen, Simon\\nKornblith, Mohammad Norouzi, and\\nGeoffrey\\xa0E. Hinton. 2020a.\\nA Simple Framework for Contrastive Learning of\\nVisual Representations. In ICML,\\nVol.\\xa0119. PMLR,\\n1597–1607.\\nYanjiao Chen, Kaishun Wu,\\nand Qian Zhang. 2015.\\nFrom QoS to QoE: A Tutorial on Video Quality\\nAssessment.\\nIEEE Commun. Surv. Tutorials\\n17, 2 (2015),\\n1126–1165.\\nhttps://doi.org/10.1109/COMST.2014.2363139\\nShyamprasad Chikkerur,\\nVijay Sundaram, Martin Reisslein, and\\nLina\\xa0J. Karam. 2011.\\nObjective Video Quality Assessment Methods: A\\nClassification, Review, and Performance Comparison.\\nIEEE Trans. Broadcast.\\n57, 2 (2011),\\n165–182.\\nCisco. 2021.\\nCisco annual internet report white paper.\\nhttps://www.cisco.com/c/en/us/solutions/collateral/\\nexecutive-perspectives/annual-internet-report/white-paper-c11-741490.html.\\nMMAction2 Contributors.\\n2020.\\nOpenMMLab\\'s Next Generation Video Understanding\\nToolbox and Benchmark.\\nhttps://github.com/open-mmlab/mmaction2.\\nJia Deng, Wei Dong,\\nRichard Socher, Li-Jia Li,\\nKai Li, and Li Fei-Fei.\\n2009.\\nImageNet: A large-scale hierarchical image\\ndatabase. In CVPR. IEEE\\nComputer Society, 248–255.\\nJoshua\\xa0Peter Ebenezer,\\nZaixi Shang, Yongjun Wu,\\nHai Wei, and Alan\\xa0C. Bovik.\\n2020.\\nNo-Reference Video Quality Assessment Using\\nSpace-Time Chips. In MMSP.\\nIEEE, 1–6.\\nChristoph Feichtenhofer,\\nHaoqi Fan, Jitendra Malik, and\\nKaiming He. 2019.\\nSlowFast Networks for Video Recognition. In\\nICCV. 6201–6210.\\nBent Fuglede and\\nFlemming Topsøe. 2004.\\nJensen-Shannon divergence and Hilbert space\\nembedding. In ISIT.\\nIEEE, 31.\\nFranz Götz-Hahn,\\nVlad Hosu, Hanhe Lin, and\\nDietmar Saupe. 2021.\\nKonVid-150k: A Dataset for No-Reference Video\\nQuality Assessment of Videos in-the-Wild.\\nIEEE Access 9\\n(2021), 72139–72160.\\nhttps://doi.org/10.1109/ACCESS.2021.3077642\\nJie Gu, Gaofeng Meng,\\nCheng Da, Shiming Xiang, and\\nChunhong Pan. 2019a.\\nNo-Reference Image Quality Assessment with\\nReinforcement Recursive List-Wise Ranking. In\\nAAAI. AAAI Press,\\n8336–8343.\\nJie Gu, Gaofeng Meng,\\nShiming Xiang, and Chunhong Pan.\\n2019b.\\nBlind image quality assessment via learnable\\nattention-based pooling.\\nPattern Recognit. 91\\n(2019), 332–344.\\nKaiming He, Haoqi Fan,\\nYuxin Wu, Saining Xie, and\\nRoss\\xa0B. Girshick. 2020.\\nMomentum Contrast for Unsupervised Visual\\nRepresentation Learning. In CVPR.\\nComputer Vision Foundation / IEEE,\\n9726–9735.\\nGeoffrey\\xa0E. Hinton, Oriol\\nVinyals, and Jeffrey Dean.\\n2015.\\nDistilling the Knowledge in a Neural Network.\\nCoRR abs/1503.02531\\n(2015).\\nTobias Hoßfeld,\\nChristian Keimel, Matthias Hirth,\\nBruno Gardlo, Julian Habigt,\\nKlaus Diepold, and Phuoc Tran-Gia.\\n2014.\\nBest Practices for QoE Crowdtesting: QoE Assessment\\nWith Crowdsourcing.\\nIEEE Trans. Multim. 16,\\n2 (2014), 541–558.\\nhttps://doi.org/10.1109/TMM.2013.2291663\\nVlad Hosu, Franz Hahn,\\nMohsen Jenadeleh, Hanhe Lin,\\nHui Men, Tamás Szirányi,\\nShujun Li, and Dietmar Saupe.\\n2017.\\nThe Konstanz natural video database (KoNViD-1k).\\nIn QoMEX. IEEE,\\n1–6.\\nRui Hou, YunHao Zhao,\\nYang Hu, and Huan Liu.\\n2020.\\nNo-reference video quality evaluation by a deep\\ntransfer CNN architecture.\\nSPIC 83\\n(2020), 115782.\\nAndrej Karpathy, George\\nToderici, Sanketh Shetty, Thomas Leung,\\nRahul Sukthankar, and Li Fei-Fei.\\n2014.\\nLarge-scale Video Classification with Convolutional\\nNeural Networks. In CVPR.\\nWill Kay, João\\nCarreira, Karen Simonyan, Brian Zhang,\\nChloe Hillier, Sudheendra\\nVijayanarasimhan, Fabio Viola, Tim\\nGreen, Trevor Back, Paul Natsev,\\nMustafa Suleyman, and Andrew\\nZisserman. 2017.\\nThe Kinetics Human Action Video Dataset.\\nCoRR abs/1705.06950\\n(2017).\\nJunjie Ke, Qifei Wang,\\nYilin Wang, Peyman Milanfar, and\\nFeng Yang. 2021.\\nMUSIQ: Multi-scale Image Quality Transformer.\\n(October 2021),\\n5148–5157.\\nJari Korhonen.\\n2019.\\nTwo-Level Approach for No-Reference Consumer Video\\nQuality Assessment.\\nIEEE Trans. Image Process.\\n28, 12 (2019),\\n5923–5938.\\nJari Korhonen, Yicheng\\nSu, and Junyong You. 2020.\\nBlind Natural Video Quality Prediction via\\nStatistical Temporal Features and Deep Spatial Features. In\\nMM \\'20: The 28th ACM International Conference\\non Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020,\\nChang\\xa0Wen Chen, Rita\\nCucchiara, Xian-Sheng Hua, Guo-Jun\\nQi, Elisa Ricci, Zhengyou Zhang, and\\nRoger Zimmermann (Eds.). ACM,\\n3311–3319.\\nhttps://doi.org/10.1145/3394171.3413845\\nBowen Li, Weixia Zhang,\\nMeng Tian, Guangtao Zhai, and\\nXianpei Wang. 2021b.\\nBlindly Assess Quality of In-the-Wild Videos via\\nQuality-aware Pre-training and Motion Perception.\\nCoRR abs/2108.08505\\n(2021).\\nDingquan Li, Tingting\\nJiang, and Ming Jiang. 2019.\\nQuality Assessment of In-the-Wild Videos. In\\nACM Multimedia. ACM,\\n2351–2359.\\nDingquan Li, Tingting\\nJiang, and Ming Jiang.\\n2021a.\\nUnified Quality Assessment of in-the-Wild Videos\\nwith Mixed Datasets Training.\\nIJCV 129,\\n4 (2021).\\nLiang Liao, Kangmin Xu,\\nHaoning Wu, Chaofeng Chen,\\nWenxiu Sun, Qiong Yan, and\\nWeisi Lin. 2022.\\nExploring the Effectiveness of Video Perceptual\\nRepresentation in Blind Video Quality Assessment. In\\nMM \\'22: The 30th ACM International Conference\\non Multimedia, Lisboa, Portugal, October 10 - 14, 2022,\\nJoão Magalhães,\\nAlberto\\xa0Del Bimbo, Shin\\'ichi Satoh,\\nNicu Sebe, Xavier Alameda-Pineda,\\nQin Jin, Vincent Oria, and\\nLaura Toni (Eds.). ACM,\\n837–846.\\nhttps://doi.org/10.1145/3503161.3547849\\nZe Liu, Yutong Lin,\\nYue Cao, Han Hu, Yixuan\\nWei, Zheng Zhang, Stephen Lin, and\\nBaining Guo. 2021a.\\nSwin Transformer: Hierarchical Vision Transformer\\nUsing Shifted Windows. In ICCV.\\n10012–10022.\\nZe Liu, Jia Ning,\\nYue Cao, Yixuan Wei,\\nZheng Zhang, Stephen Lin, and\\nHan Hu. 2021b.\\nVideo Swin Transformer.\\nCoRR abs/2106.13230\\n(2021).\\nAnish Mittal,\\nAnush\\xa0Krishna Moorthy, and Alan\\xa0Conrad\\nBovik. 2012.\\nNo-Reference Image Quality Assessment in the\\nSpatial Domain.\\nIEEE Trans. Image Process.\\n21, 12 (2012),\\n4695–4708.\\nAnish Mittal, Michele\\xa0A.\\nSaad, and Alan\\xa0C. Bovik.\\n2016.\\nA Completely Blind Video Integrity Oracle.\\nIEEE Trans. Image Process.\\n25, 1 (2016),\\n289–300.\\nAnish Mittal, Rajiv\\nSoundararajan, and Alan\\xa0C. Bovik.\\n2013.\\nMaking a \"Completely Blind\" Image Quality\\nAnalyzer.\\nIEEE SPL 20,\\n3 (2013), 209–212.\\nAdam Paszke, Sam Gross,\\nFrancisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin,\\nNatalia Gimelshein, Luca Antiga,\\nAlban Desmaison, Andreas Köpf,\\nEdward\\xa0Z. Yang, Zachary DeVito,\\nMartin Raison, Alykhan Tejani,\\nSasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and\\nSoumith Chintala. 2019.\\nPyTorch: An Imperative Style, High-Performance Deep\\nLearning Library. In NeurIPS.\\n8024–8035.\\nAlec Radford, Jong\\xa0Wook\\nKim, Chris Hallacy, Aditya Ramesh,\\nGabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell,\\nPamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever.\\n2021.\\nLearning Transferable Visual Models From Natural\\nLanguage Supervision. In ICML.\\nPMLR, 8748–8763.\\nMichele\\xa0A. Saad, Alan\\xa0C.\\nBovik, and Christophe Charrier.\\n2012.\\nBlind Image Quality Assessment: A Natural Scene\\nStatistics Approach in the DCT Domain.\\nIEEE Trans. Image Process.\\n21, 8 (2012),\\n3339–3352.\\nhttps://doi.org/10.1109/TIP.2012.2191563\\nMichele\\xa0A. Saad, Alan\\xa0C.\\nBovik, and Christophe Charrier.\\n2014.\\nBlind Prediction of Natural Video Quality.\\nIEEE Trans. Image Process.\\n23, 3 (2014),\\n1352–1365.\\nRamprasaath\\xa0R. Selvaraju,\\nMichael Cogswell, Abhishek Das,\\nRamakrishna Vedantam, Devi Parikh, and\\nDhruv Batra. 2017.\\nGrad-CAM: Visual Explanations from Deep Networks\\nvia Gradient-Based Localization. In ICCV.\\nIEEE Computer Society, 618–626.\\nMuhammad Shahid, Jacob\\nSøgaard, Jeevan Pokhrel, Kjell\\nBrunnström, Kun Wang, Samira\\nTavakoli, and Narciso García.\\n2014.\\nCrowdsourcing based subjective quality assessment\\nof adaptive video streaming. In Sixth\\nInternational Workshop on Quality of Multimedia Experience, QoMEX 2014,\\nSingapore, September 18-20, 2014. IEEE,\\n53–54.\\nhttps://doi.org/10.1109/QoMEX.2014.6982289\\nZeina Sinno and\\nAlan\\xa0Conrad Bovik. 2019.\\nLarge-Scale Study of Perceptual Video Quality.\\nIEEE Trans. Image Process.\\n28, 2 (2019),\\n612–627.\\nMingxing Tan and Quoc\\xa0V.\\nLe. 2019.\\nEfficientNet: Rethinking Model Scaling for\\nConvolutional Neural Networks. In ICML\\n(Proceedings of Machine Learning Research,\\nVol.\\xa097). PMLR,\\n6105–6114.\\nDu Tran, Heng Wang,\\nMatt Feiszli, and Lorenzo Torresani.\\n2019.\\nVideo Classification With Channel-Separated\\nConvolutional Networks. In ICCV.\\nIEEE, 5551–5560.\\nZhengzhong Tu, Chia-Ju\\nChen, Yilin Wang, Neil Birkbeck,\\nBalu Adsumilli, and Alan\\xa0C. Bovik.\\n2021a.\\nEfficient User-Generated Video Quality Prediction.\\nIn Picture Coding Symposium, PCS 2021, Bristol,\\nUnited Kingdom, June 29 - July 2, 2021. IEEE,\\n1–5.\\nhttps://doi.org/10.1109/PCS50896.2021.9477483\\nZhengzhong Tu, Yilin\\nWang, Neil Birkbeck, Balu Adsumilli,\\nand Alan\\xa0C. Bovik. 2021b.\\nUGC-VQA: Benchmarking Blind Video Quality\\nAssessment for User Generated Content.\\nIEEE TIP 30\\n(2021), 4449–4464.\\nZhengzhong Tu, Yilin\\nWang, Neil Birkbeck, Balu Adsumilli,\\nand Alan\\xa0C. Bovik. 2021c.\\nUGC-VQA: Benchmarking Blind Video Quality\\nAssessment for User Generated Content.\\nIEEE TIP 30\\n(2021), 4449–4464.\\nZhengzhong Tu, Xiangxu\\nYu, Yilin Wang, Neil Birkbeck,\\nBalu Adsumilli, and Alan\\xa0C. Bovik.\\n2021d.\\nRAPIQUE: Rapid and Accurate Video Quality\\nPrediction of User Generated Content.\\nCoRR abs/2101.10955\\n(2021).\\nDomonkos Varga and\\nTamás Szirányi.\\n2019.\\nNo-reference video quality assessment via\\npretrained CNN and LSTM networks.\\nSignal Image Video Process.\\n13, 8 (2019),\\n1569–1576.\\nYilin Wang, Sasi Inguva,\\nand Balu Adsumilli. 2019.\\nYouTube UGC Dataset for Video Compression\\nResearch. In MMSP.\\nIEEE, 1–5.\\nYilin Wang, Junjie Ke,\\nHossein Talebi, Joong\\xa0Gon Yim,\\nNeil Birkbeck, Balu Adsumilli,\\nPeyman Milanfar, and Feng Yang.\\n2021.\\nRich Features for Perceptual Quality Assessment of\\nUGC Videos. In CVPR.\\n13435–13444.\\nZhou Wang, Ligang Lu,\\nand Alan\\xa0C. Bovik. 2004.\\nVideo quality assessment based on structural\\ndistortion measurement.\\nSignal Process. Image Commun.\\n19, 2 (2004),\\n121–132.\\nHaoning Wu, Chaofeng\\nChen, Jingwen Hou, Liang Liao,\\nAnnan Wang, Wenxiu Sun,\\nQiong Yan, and Weisi Lin.\\n2022a.\\nFAST-VQA: Efficient End-to-End Video Quality\\nAssessment with Fragment Sampling. In ECCV\\n(6), Vol.\\xa013666. 538–554.\\nHaoning Wu, Chaofeng\\nChen, Liang Liao, Jingwen Hou,\\nQiong Yan, and Weisi Lin.\\n2022b.\\nDisCoVQA: Temporal Distortion-Content Transformers\\nfor Video Quality Assessment.\\nCoRR abs/2206.09853\\n(2022).\\nFengchuang Xing,\\nYuan-Gen Wang, Hanpin Wang,\\nLeida Li, and Guopu Zhu.\\n2021.\\nStarVQA: Space-Time Attention for Video Quality\\nAssessment.\\nCoRR abs/2108.09635\\n(2021).\\nJiahua Xu, Jing Li,\\nXingguang Zhou, Wei Zhou,\\nBaichao Wang, and Zhibo Chen.\\n2021.\\nPerceptual Quality Assessment of Internet Videos.\\nIn ACM Multimedia. ACM,\\n1248–1257.\\nJia Yan, Weixia Zhang,\\nand Tianpeng Feng. 2016.\\nBlind Image Quality Assessment Based on Natural\\nRedundancy Statistics. In Computer Vision - ACCV\\n2016 - 13th Asian Conference on Computer Vision, Taipei, Taiwan, November\\n20-24, 2016, Revised Selected Papers, Part IV\\n(Lecture Notes in Computer Science,\\nVol.\\xa010114),\\nShang-Hong Lai,\\nVincent Lepetit, Ko\\xa0Nishino, and\\nYoichi Sato (Eds.). Springer,\\n3–18.\\nhttps://doi.org/10.1007/978-3-319-54190-7_1\\nZhenqiang Ying, Maniratnam\\nMandal, Deepti Ghadiyaram, and Alan\\nBovik. 2021.\\nPatch-VQ: \\'Patching Up\\' the Video Quality Problem.\\nIn CVPR. Computer Vision\\nFoundation / IEEE, 14019–14029.\\nJunyong You.\\n2021.\\nLong Short-term Convolutional Transformer for\\nNo-Reference Video Quality Assessment. In ACM\\nMultimedia. ACM, 2112–2120.\\nJunyong You and Jari\\nKorhonen. 2019.\\nDeep Neural Networks for No-Reference Video Quality\\nAssessment. In 2019 IEEE International\\nConference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25,\\n2019. IEEE, 2349–2353.\\nhttps://doi.org/10.1109/ICIP.2019.8803395\\nKai Zhao, Kun Yuan,\\nMing Sun, Mading Li, and\\nXing Wen. 2023b.\\nQuality-Aware Pre-Trained Models for Blind Image\\nQuality Assessment. In CVPR.\\nIEEE Computer Society, 22302–22313.\\nKai Zhao, Kun Yuan,\\nMing Sun, and Xing Wen.\\n2023a.\\nZoom-VQA: Patches, Frames and Clips Integration for\\nVideo Quality Assessment. In CVPR Workshops.\\nIEEE Computer Society, 1302–1310.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52b79fa-7671-47e2-85cb-cf5c1201e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/arxiv_tables_2308_high_quality/papers_expanded.jsonl\") as f:\n",
    "    papers_2308_high_quality = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41205f6-13d8-4f12-9138-4cd285774539",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map = {paper['arxiv_id']: paper for paper in papers_2308_high_quality if 'arxiv_id' in paper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570a14c3-8bcb-4515-b53d-78558c25d52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2308.00729': {'tabids': [],\n",
       "  'corpus_id': 260378902,\n",
       "  'arxiv_id': '2308.00729',\n",
       "  'paper_id': 'b5e4c46ac09d5de2051998c9f741ef480fab2ebe',\n",
       "  'title': 'Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment',\n",
       "  'abstract': 'Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.'},\n",
       " '2308.06173': {'tabids': [],\n",
       "  'corpus_id': 260865937,\n",
       "  'arxiv_id': '2308.06173',\n",
       "  'paper_id': '89a0f417d5d41e5acaebaa8fd695d9dcb9632a04',\n",
       "  'title': 'Physical Adversarial Attacks for Camera-Based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook',\n",
       "  'abstract': 'Deep Neural Networks (DNNs) have shown impressive performance in computer vision tasks; however, their vulnerability to adversarial attacks raises concerns regarding their security and reliability. Extensive research has shown that DNNs can be compromised by carefully crafted perturbations, leading to significant performance degradation in both digital and physical domains. Therefore, ensuring the security of DNN-based systems is crucial, particularly in safety-critical domains such as autonomous driving, robotics, smart homes/cities, smart industries, video surveillance, and healthcare. In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.'},\n",
       " '2308.07107': {'tabids': [],\n",
       "  'corpus_id': 260887838,\n",
       "  'arxiv_id': '2308.07107',\n",
       "  'paper_id': '0b220041eb83c23b7b10d32a5d08c0309d528071',\n",
       "  'title': 'Large Language Models for Information Retrieval: A Survey',\n",
       "  'abstract': 'As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.'},\n",
       " '2308.00937': {'tabids': [],\n",
       "  'corpus_id': 260378734,\n",
       "  'arxiv_id': '2308.00937',\n",
       "  'paper_id': '92f6346346a46f5813b6996b9c1f10d9602eb39a',\n",
       "  'title': 'LEMMA: Learning Language-Conditioned Multi-Robot Manipulation',\n",
       "  'abstract': \"Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for <underline>L</underline>anguag<underline>E</underline>-Conditioned <underline>M</underline>ulti-robot <underline>MA</underline>nipulation (<monospace>LEMMA</monospace>) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. <monospace>LEMMA</monospace> features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. <monospace>LEMMA</monospace> poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of <monospace>LEMMA</monospace> for developing future language-conditioned multi-robot systems.\"},\n",
       " '2308.08407': {'tabids': [],\n",
       "  'corpus_id': 260926590,\n",
       "  'arxiv_id': '2308.08407',\n",
       "  'paper_id': 'fea19ea351f65115c470bdd6a2446d7e41ee799f',\n",
       "  'title': 'Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities',\n",
       "  'abstract': \"Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.\"},\n",
       " '2308.06966': {'tabids': [],\n",
       "  'corpus_id': 260887693,\n",
       "  'arxiv_id': '2308.06966',\n",
       "  'paper_id': '64e802ea8e9dbe247c31fb06184c04dbf9e55e4e',\n",
       "  'title': 'EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce',\n",
       "  'abstract': 'Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.'},\n",
       " '2308.14430': {'tabids': [],\n",
       "  'corpus_id': 261242529,\n",
       "  'arxiv_id': '2308.14430',\n",
       "  'paper_id': '9f432d2500758cd1182fe47fb09c2065bf7b9123',\n",
       "  'title': 'TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models',\n",
       "  'abstract': 'Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/'},\n",
       " '2308.14371': {'tabids': [],\n",
       "  'corpus_id': 261242415,\n",
       "  'arxiv_id': '2308.14371',\n",
       "  'paper_id': 'f0f8471873132d02c18d030aaf72cd21988e657e',\n",
       "  'title': 'SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction',\n",
       "  'abstract': 'Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code will be released after accteptance.'},\n",
       " '2308.09090': {'tabids': [],\n",
       "  'corpus_id': 261030390,\n",
       "  'arxiv_id': '2308.09090',\n",
       "  'paper_id': 'ee5896108a9cb78dffddc4ff94a1ea43e0e4f656',\n",
       "  'title': 'Data-driven Integrated Sensing and Communication: Recent Advances, Challenges, and Future Prospects',\n",
       "  'abstract': 'Integrated Sensing and Communication (ISAC), combined with data-driven approaches, has emerged as a highly significant field, garnering considerable attention from academia and industry. Its potential to enable wide-scale applications in the future sixth-generation (6G) networks has led to extensive recent research efforts. Machine learning (ML) techniques, including $K$-nearest neighbors (KNN), support vector machines (SVM), deep learning (DL) architectures, and reinforcement learning (RL) algorithms, have been deployed to address various design aspects of ISAC and its diverse applications. Therefore, this paper aims to explore integrating various ML techniques into ISAC systems, covering various applications. These applications span intelligent vehicular networks, encompassing unmanned aerial vehicles (UAVs) and autonomous cars, as well as radar applications, localization and tracking, millimeter wave (mmWave) and Terahertz (THz) communication, and beamforming. The contributions of this paper lie in its comprehensive survey of ML-based works in the ISAC domain and its identification of challenges and future research directions. By synthesizing the existing knowledge and proposing new research avenues, this survey serves as a valuable resource for researchers, practitioners, and stakeholders involved in advancing the capabilities of ISAC systems in the context of 6G networks.'},\n",
       " '2308.08794': {'tabids': [],\n",
       "  'corpus_id': 261030871,\n",
       "  'arxiv_id': '2308.08794',\n",
       "  'paper_id': '939f35d721c33187a71b43351a371a799ee409ab',\n",
       "  'title': 'Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces',\n",
       "  'abstract': 'Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points.'},\n",
       " '2308.03149': {'tabids': [],\n",
       "  'corpus_id': 260189388,\n",
       "  'arxiv_id': '2308.03149',\n",
       "  'paper_id': '29742526f4e6cd3727789815053395c266a28650',\n",
       "  'title': 'A Survey of mmWave-Based Human Sensing: Technology, Platforms and Applications',\n",
       "  'abstract': 'With the rapid development of the Internet of Things (IoT) and the rise of 5G communication networks and automatic driving, millimeter wave (mmWave) sensing is emerging and starts impacting our life and workspace. mmWave sensing can sense humans and objects in a contactless way, providing fine-grained sensing ability. In the past few years, many mmWave sensing techniques have been proposed and applied in various human sensing applications (e.g., human localization, gesture recognition, and vital monitoring). We discover the need of a comprehensive survey to summarize the technology, platforms and applications of mmWave-based human sensing. In this survey, we first present the mmWave hardware platforms and some key techniques of mmWave sensing. We then provide a comprehensive review of existing mmWave-based human sensing works. Specifically, we divide existing works into four categories according to the sensing granularity: human tracking and localization, motion recognition, biometric measurement and human imaging. Finally, we discuss the potential research challenges and present future directions in this area.'},\n",
       " '2308.13266': {'tabids': [],\n",
       "  'corpus_id': 261214723,\n",
       "  'arxiv_id': '2308.13266',\n",
       "  'paper_id': '483a8014ac9c0048c4bcf10b404ab3f5b0f46e4f',\n",
       "  'title': 'Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation',\n",
       "  'abstract': 'Tracking any given object(s) spatially and temporally is a common purpose in Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint tracking and segmentation have been attempted in some studies but they often lack full compatibility of both box and mask in initialization and prediction, and mainly focus on single-object scenarios. To address these limitations, this paper proposes a Multi-object Mask-box Integrated framework for unified Tracking and Segmentation, dubbed MITS. Firstly, the unified identification module is proposed to support both box and mask reference for initialization, where detailed object information is inferred from boxes or directly retained from masks. Additionally, a novel pinpoint box predictor is proposed for accurate multi-object box prediction, facilitating target-oriented representation learning. All target objects are processed simultaneously from encoding to propagation and decoding, as a unified pipeline for VOT and VOS. Experimental results show MITS achieves state-of-the-art performance on both VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor by around 6% on the GOT-10k test set, and significantly improves the performance of box initialization on VOS benchmarks. The code is available at https://github.com/yoxu515/MITS.'},\n",
       " '2308.08643': {'tabids': [],\n",
       "  'corpus_id': 261031130,\n",
       "  'arxiv_id': '2308.08643',\n",
       "  'paper_id': '99a0f0f6ada73216389307133e04e9468639fbb2',\n",
       "  'title': 'Towards Personalized Federated Learning via Heterogeneous Model Reassembly',\n",
       "  'abstract': 'This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner.'},\n",
       " '2308.07444': {'tabids': [],\n",
       "  'corpus_id': 260900310,\n",
       "  'arxiv_id': '2308.07444',\n",
       "  'paper_id': '7402bd2fca6ec71744604f4aeb84cab02daa8b6c',\n",
       "  'title': 'The Performance of Transferability Metrics does not Translate to Medical Tasks',\n",
       "  'abstract': 'Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contexts, inviting further work in that direction.'},\n",
       " '2308.10631': {'tabids': [],\n",
       "  'corpus_id': 261049754,\n",
       "  'arxiv_id': '2308.10631',\n",
       "  'paper_id': '78b8f98c133c876d6f9ee151821d271100733cd5',\n",
       "  'title': 'PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait',\n",
       "  'abstract': 'Psychological trait estimation from external factors such as movement and appearance is a challenging and long-standing problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totalling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.'},\n",
       " '2308.02151': {'tabids': [],\n",
       "  'corpus_id': 260611249,\n",
       "  'arxiv_id': '2308.02151',\n",
       "  'paper_id': '81b10e64133e775dab53153cc82277d276efe1f7',\n",
       "  'title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization',\n",
       "  'abstract': 'Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.'},\n",
       " '2308.03108': {'tabids': [],\n",
       "  'corpus_id': 260682704,\n",
       "  'arxiv_id': '2308.03108',\n",
       "  'paper_id': '79411a597f24ef32126c388b3813a8070f93c75d',\n",
       "  'title': 'SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation',\n",
       "  'abstract': 'In this paper, we investigate the vulnerability of MDE to adversarial patches. We propose a novel \\\\underline{S}tealthy \\\\underline{A}dversarial \\\\underline{A}ttacks on \\\\underline{M}DE (SAAM) that compromises MDE by either corrupting the estimated distance or causing an object to seamlessly blend into its surroundings. Our experiments, demonstrate that the designed stealthy patch successfully causes a DNN-based MDE to misestimate the depth of objects. In fact, our proposed adversarial patch achieves a significant 60\\\\% depth error with 99\\\\% ratio of the affected region. Importantly, despite its adversarial nature, the patch maintains a naturalistic appearance, making it inconspicuous to human observers. We believe that this work sheds light on the threat of adversarial attacks in the context of MDE on edge devices. We hope it raises awareness within the community about the potential real-life harm of such attacks and encourages further research into developing more robust and adaptive defense mechanisms.'},\n",
       " '2308.00247': {'tabids': [],\n",
       "  'corpus_id': 260351426,\n",
       "  'arxiv_id': '2308.00247',\n",
       "  'paper_id': 'e4561cf0cee2139b7c9085822a4899ce6a5aace3',\n",
       "  'title': 'Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review',\n",
       "  'abstract': 'The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.'},\n",
       " '2308.09592': {'tabids': [],\n",
       "  'corpus_id': 261031087,\n",
       "  'arxiv_id': '2308.09592',\n",
       "  'paper_id': '05cbac9a5101f47a6fabad72398616506572c9fa',\n",
       "  'title': 'StableVideo: Text-driven Consistency-aware Diffusion Video Editing',\n",
       "  'abstract': 'Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at this https URL.'},\n",
       " '2308.03151': {'tabids': [],\n",
       "  'corpus_id': 260681484,\n",
       "  'arxiv_id': '2308.03151',\n",
       "  'paper_id': '36d23a17309e5e31a5b966c0158386ebe6ce719c',\n",
       "  'title': 'Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models',\n",
       "  'abstract': \"Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.\"},\n",
       " '2308.03258': {'tabids': [],\n",
       "  'corpus_id': 260680386,\n",
       "  'arxiv_id': '2308.03258',\n",
       "  'paper_id': 'c0c06de39c1fd89db3bdb52b56851bfbabba0b09',\n",
       "  'title': 'APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses',\n",
       "  'abstract': 'The efficacy of availability poisoning, a method of poisoning data by injecting imperceptible perturbations to prevent its use in model training, has been a hot subject of investigation. Previous research suggested that it was difficult to effectively counteract such poisoning attacks. However, the introduction of various defense methods has challenged this notion. Due to the rapid progress in this field, the performance of different novel methods cannot be accurately validated due to variations in experimental setups. To further evaluate the attack and defense capabilities of these poisoning methods, we have developed a benchmark -- APBench for assessing the efficacy of adversarial poisoning. APBench consists of 9 state-of-the-art availability poisoning attacks, 8 defense algorithms, and 4 conventional data augmentation techniques. We also have set up experiments with varying different poisoning ratios, and evaluated the attacks on multiple datasets and their transferability across model architectures. We further conducted a comprehensive evaluation of 2 additional attacks specifically targeting unsupervised models. Our results reveal the glaring inadequacy of existing attacks in safeguarding individual privacy. APBench is open source and available to the deep learning community: https://github.com/lafeat/apbench.'},\n",
       " '2308.10737': {'tabids': [],\n",
       "  'corpus_id': 261049286,\n",
       "  'arxiv_id': '2308.10737',\n",
       "  'paper_id': 'b706a48c83e146c843948e0b67f73cadf01fcc8d',\n",
       "  'title': 'UGSL: A Unified Framework for Benchmarking Graph Structure Learning',\n",
       "  'abstract': 'Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications. While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided. The GNN parameters and a graph structure are jointly learned. Previous studies adopt different experimentation setups, making it difficult to compare their merits. In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework. Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model. We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework. Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses. The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl.'},\n",
       " '2308.06595': {'tabids': [],\n",
       "  'corpus_id': 260887670,\n",
       "  'arxiv_id': '2308.06595',\n",
       "  'paper_id': 'd6c2523ab97416c2692cbbeab082ed1790e8e55e',\n",
       "  'title': 'VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use',\n",
       "  'abstract': \"We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at visit-bench.github.io.\"},\n",
       " '2308.01529': {'tabids': [],\n",
       "  'corpus_id': 260438699,\n",
       "  'arxiv_id': '2308.01529',\n",
       "  'paper_id': '01a3050bb42a8426e125ede7694294de26c71f13',\n",
       "  'title': 'Towards Fair and Privacy Preserving Federated Learning for the Healthcare Domain',\n",
       "  'abstract': 'Federated learning enables data sharing in healthcare contexts where it might otherwise be difficult due to data-use-ordinances or security and communication constraints. Distributed and shared data models allow models to become generalizable and learn from heterogeneous clients. While addressing data security, privacy, and vulnerability considerations, data itself is not shared across nodes in a given learning network. On the other hand, FL models often struggle with variable client data distributions and operate on an assumption of independent and identically distributed data. As the field has grown, the notion of fairness-aware federated learning mechanisms has also been introduced and is of distinct significance to the healthcare domain where many sensitive groups and protected classes exist. In this paper, we create a benchmark methodology for FAFL mechanisms under various heterogeneous conditions on datasets in the healthcare domain typically outside the scope of current federated learning benchmarks, such as medical imaging and waveform data formats. Our results indicate considerable variation in how various FAFL schemes respond to high levels of data heterogeneity. Additionally, doing so under privacy-preserving conditions can create significant increases in network communication cost and latency compared to the typical federated learning scheme.'},\n",
       " '2308.06953': {'tabids': [],\n",
       "  'corpus_id': 260886925,\n",
       "  'arxiv_id': '2308.06953',\n",
       "  'paper_id': '5901358b0df3491e12263a39f52f45249e0aa7b9',\n",
       "  'title': 'Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation',\n",
       "  'abstract': 'Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs; and, the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. With a single YAML configuration file, users can build and test an annotation interface for any framework within minutes -- all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained frameworks and corresponding annotations made and collected by the community, covering a wide range of NLP tasks. For deployment, Thresh offers multiple options for any scale of annotation projects from small manual inspections to large crowdsourcing ones. Additionally, we introduce a Python library to streamline the entire process from typology design and deployment to annotation processing. Thresh is publicly accessible at https://thresh.tools.'},\n",
       " '2308.15701': {'tabids': [],\n",
       "  'corpus_id': 261339556,\n",
       "  'arxiv_id': '2308.15701',\n",
       "  'paper_id': 'ca9f41ad1c3f0ed51781eef6cfcd035bf4d01d1a',\n",
       "  'title': 'A Survey on Multi-Behavior Sequential Recommendation',\n",
       "  'abstract': 'Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.'},\n",
       " '2308.06507': {'tabids': [],\n",
       "  'corpus_id': 259370708,\n",
       "  'arxiv_id': '2308.06507',\n",
       "  'paper_id': '8d36390a430845849b62646a8c8be4c79f2b3d62',\n",
       "  'title': 'AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models',\n",
       "  'abstract': 'Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.'},\n",
       " '2308.05787': {'tabids': [],\n",
       "  'corpus_id': 260866000,\n",
       "  'arxiv_id': '2308.05787',\n",
       "  'paper_id': 'fe779d52ef85c0387ed9a68cd90ca11033689bfa',\n",
       "  'title': 'Temporally-Adaptive Models for Efficient Video Understanding',\n",
       "  'abstract': 'Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2147fe-6800-4485-8ebf-1cb6bebd2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(full_texts_2308_high_quality)):\n",
    "#     full_texts_2308_high_quality[i] = {k:, \n",
    "\n",
    "for ft in full_texts_2308_high_quality:\n",
    "    arxiv_id_no_version = re.sub(\"v\\d+\", \"\", ft['paper_id'])\n",
    "    ft['title'] = metadata_map[arxiv_id_no_version]['title']\n",
    "    ft['abstract']['text'] = metadata_map[arxiv_id_no_version]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a482f231-b197-4c3c-86e7-c63ca1e18cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/arxiv_tables_2308_high_quality/full_texts_with_tables.jsonl\", \"w\") as f:\n",
    "    for full_text in full_texts_2308_high_quality:\n",
    "        f.write(json.dumps(full_text) + \"\\n\")\n",
    "    # full_texts_2308_high_quality = [json.loads(line) for line in f]\n",
    "    # full_texts_2308_high_quality_map = {ft[\"paper_id\"]: ft for ft in full_texts_2308_high_quality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b36e78-04bd-491e-bfe1-a10a2811df93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '2308.00729v1',\n",
       " '_pdf_hash': None,\n",
       " '_source_hash': 'add89d24da98870765fefba429d689dc4a83485c',\n",
       " '_source_name': '2308.00729v1',\n",
       " 'metadata': {},\n",
       " 'abstract': {'section': 'Abstract',\n",
       "  'text': 'Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " 'ref_entries': {},\n",
       " 'tables': {'43c9402e-2914-42dc-99c4-b2424f71f6ad': {'table': '<table rend=\"display\" id-text=\"1\" id=\"uid1\" place=\"t\"><head>Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.</head>\\n<row><cell right-border=\"true\" halign=\"center\">Dataset</cell>\\n<cell right-border=\"true\" halign=\"center\">Task</cell>\\n<cell halign=\"center\">Size</cell>\\n<cell>Annotations</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">KoNViD-1k <cit sha=\"24b4d04b01098cffe3cb975171aa05132c6a0903\"><ref target=\"bid14\"/></cit>{{cite:24b4d04}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">1,200</cell>\\n<cell>114</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">LIVE-VQC <cit sha=\"8f3eb869445ed0622a9879a6f2010828f6039e8b\"><ref target=\"bid13\"/></cit>{{cite:8f3eb86}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">585</cell>\\n<cell>240</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">YouTube-UGC <cit sha=\"9be8207b7ba6b4f423a574f686110ed04d5a3d91\"><ref target=\"bid15\"/></cit>{{cite:9be8207}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">1,380</cell>\\n<cell>123</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">LSVQ <cit sha=\"3dfd446db36563cc1eeb028310478bbb775a0237\"><ref target=\"bid9\"/></cit>{{cite:3dfd446}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">39,075</cell>\\n<cell>35</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">KoNViD-150k <cit sha=\"9c04ab9115e73e7300a6077937ec1b23e3fdf820\"><ref target=\"bid10\"/></cit>{{cite:9c04ab9}}</cell>\\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\\n<cell halign=\"center\">153,841</cell>\\n<cell>5</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Sports-1M <cit sha=\"fbaab966ad1e0efdeb5b96e80b5b467ba60eeced\"><ref target=\"bid1\"/></cit>{{cite:fbaab96}}</cell>\\n<cell right-border=\"true\" halign=\"center\">classification</cell>\\n<cell halign=\"center\">1,133,158</cell>\\n<cell>- (<hi rend=\"it\">auto.</hi>)</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Kinetics-400 <cit sha=\"dd352e3c3918b783377312dd4399a284589a24d1\"><ref target=\"bid2\"/></cit>{{cite:dd352e3}}</cell>\\n<cell right-border=\"true\" halign=\"center\">classification</cell>\\n<cell halign=\"center\">306,245</cell>\\n<cell>3-5</cell>\\n</row></table>',\n",
       "   'caption': 'Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.',\n",
       "   'type': 'table'},\n",
       "  'a8b624dc-b470-4046-8f35-b6e5534343a4': {'table': '<table rend=\"display\" id-text=\"2\" id=\"uid29\" starred=\"true\" place=\"t\"><head>Comparisons with SOTA methods. The up arrow “{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are <hi rend=\"bold\">highlighted</hi> and <hi rend=\"underline\">underlined</hi>. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.</head>\\n<row><cell right-border=\"true\" halign=\"center\">2*Method</cell>\\n<cell right-border=\"true\" halign=\"center\" cols=\"3\">KoNViD-1k</cell>\\n<cell right-border=\"true\" halign=\"center\" cols=\"3\">LIVE-VQC</cell>\\n<cell halign=\"center\" cols=\"3\">YouTube-UGC</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">SRCC{{formula:585eb95b-1547-4785-a382-4d02cb7a7274}}</cell>\\n<cell right-border=\"true\" halign=\"center\">PLCC{{formula:2f43609f-d5b9-4947-a668-77916eaab08b}}</cell>\\n<cell halign=\"center\">Mean</cell>\\n<cell halign=\"center\">SRCC{{formula:0ab3c965-7cc8-475b-9cc8-a434f557a90f}}</cell>\\n<cell right-border=\"true\" halign=\"center\">PLCC{{formula:91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0}}</cell>\\n<cell halign=\"center\">Mean</cell>\\n<cell halign=\"center\">SRCC{{formula:fe38b572-b993-4f84-b520-4ca75bc3ba79}}</cell>\\n<cell halign=\"center\">PLCC{{formula:ef0f4492-775e-4bdf-b2d5-8c30a10f8946}}</cell>\\n<cell>Mean</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">VIIDEO <cit sha=\"0832a8c9419aa6a0897ba06ab4d4528070bf0fa7\"><ref target=\"bid25\"/></cit>{{cite:0832a8c}}</cell>\\n<cell halign=\"center\">0.2980</cell>\\n<cell right-border=\"true\" halign=\"center\">0.3030</cell>\\n<cell halign=\"center\">0.3005</cell>\\n<cell halign=\"center\">0.0332</cell>\\n<cell right-border=\"true\" halign=\"center\">0.2164</cell>\\n<cell halign=\"center\">0.1248</cell>\\n<cell halign=\"center\">0.0580</cell>\\n<cell halign=\"center\">0.1534</cell>\\n<cell>0.1057</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">NIQE <cit sha=\"c619c6de8497b9acc2901779f3d6f41bb0653a99\"><ref target=\"bid57\"/></cit>{{cite:c619c6d}}</cell>\\n<cell halign=\"center\">0.5417</cell>\\n<cell right-border=\"true\" halign=\"center\">0.5530</cell>\\n<cell halign=\"center\">0.5474</cell>\\n<cell halign=\"center\">0.5957</cell>\\n<cell right-border=\"true\" halign=\"center\">0.6286</cell>\\n<cell halign=\"center\">0.6122</cell>\\n<cell halign=\"center\">0.2379</cell>\\n<cell halign=\"center\">0.2776</cell>\\n<cell>0.2578</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">BRISQUE <cit sha=\"18dcc024a728c82f0671adaeb65b81b7185d7acd\"><ref target=\"bid58\"/></cit>{{cite:18dcc02}}</cell>\\n<cell halign=\"center\">0.654</cell>\\n<cell right-border=\"true\" halign=\"center\">0.626</cell>\\n<cell halign=\"center\">0.640</cell>\\n<cell halign=\"center\">0.592</cell>\\n<cell right-border=\"true\" halign=\"center\">0.638</cell>\\n<cell halign=\"center\">0.615</cell>\\n<cell halign=\"center\">0.382</cell>\\n<cell halign=\"center\">0.395</cell>\\n<cell>0.389</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">VSFA <cit sha=\"7da41796bcbf3c567677b6c3fc55a27f94ef63b0\"><ref target=\"bid7\"/></cit>{{cite:7da4179}}</cell>\\n<cell halign=\"center\">0.755</cell>\\n<cell right-border=\"true\" halign=\"center\">0.744</cell>\\n<cell halign=\"center\">0.750</cell>\\n<cell halign=\"center\">-</cell>\\n<cell right-border=\"true\" halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">TLVQM <cit sha=\"1af68c6a2b2f94cf4ab35e14311272390f599453\"><ref target=\"bid59\"/></cit>{{cite:1af68c6}}</cell>\\n<cell halign=\"center\">0.7729</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7688</cell>\\n<cell halign=\"center\">0.7709</cell>\\n<cell halign=\"center\">0.7988</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8025</cell>\\n<cell halign=\"center\">0.8807</cell>\\n<cell halign=\"center\">0.6693</cell>\\n<cell halign=\"center\">0.6590</cell>\\n<cell>0.6642</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">RIRNet <cit sha=\"04485d0956438899571e65d6b1847ec05a6a9c5b\"><ref target=\"bid60\"/></cit>{{cite:04485d0}}</cell>\\n<cell halign=\"center\">0.7755</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7812</cell>\\n<cell halign=\"center\">0.7784</cell>\\n<cell halign=\"center\">0.7713</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7982</cell>\\n<cell halign=\"center\">0.7848</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">MDTVSFA <cit sha=\"1ad644b3f88e49a2de5aeab132462a51198265de\"><ref target=\"bid8\"/></cit>{{cite:1ad644b}}</cell>\\n<cell halign=\"center\">0.7812</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7856</cell>\\n<cell halign=\"center\">0.7834</cell>\\n<cell halign=\"center\">0.7382</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7728</cell>\\n<cell halign=\"center\">0.7555</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">RIRNet+<hi rend=\"it\">CSPT</hi> <cit sha=\"b519bb8aedbcaca80c324c191ad0f5a2144c0534\"><ref target=\"bid61\"/></cit>{{cite:b519bb8}}</cell>\\n<cell halign=\"center\">0.8008</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8062</cell>\\n<cell halign=\"center\">0.8035</cell>\\n<cell halign=\"center\">0.7989</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8194</cell>\\n<cell halign=\"center\">0.8092</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">CoINVQ <cit sha=\"80cfe8864bda775820fbc62d4bb618efafdb8421\"><ref target=\"bid22\"/></cit>{{cite:80cfe88}}</cell>\\n<cell halign=\"center\">0.802</cell>\\n<cell right-border=\"true\" halign=\"center\">0.816</cell>\\n<cell halign=\"center\">0.809</cell>\\n<cell halign=\"center\">-</cell>\\n<cell right-border=\"true\" halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">0.764</cell>\\n<cell halign=\"center\">0.767</cell>\\n<cell>0.766</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">RAPIQUE <cit sha=\"f7fae7386c3d4b0aa93baaf42e1c16f1fb782b70\"><ref target=\"bid62\"/></cit>{{cite:f7fae73}}</cell>\\n<cell halign=\"center\">0.8031</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8175</cell>\\n<cell halign=\"center\">0.8103</cell>\\n<cell halign=\"center\">0.7548</cell>\\n<cell right-border=\"true\" halign=\"center\">0.7863</cell>\\n<cell halign=\"center\">0.7706</cell>\\n<cell halign=\"center\">0.7591</cell>\\n<cell halign=\"center\">0.7684</cell>\\n<cell>0.7638</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">StarVQA <cit sha=\"1c7237b9ad351ad4648f2a9a0d988f0a5b415dea\"><ref target=\"bid35\"/></cit>{{cite:1c7237b}}</cell>\\n<cell halign=\"center\">0.812</cell>\\n<cell right-border=\"true\" halign=\"center\">0.796</cell>\\n<cell halign=\"center\">0.804</cell>\\n<cell halign=\"center\">0.732</cell>\\n<cell right-border=\"true\" halign=\"center\">0.808</cell>\\n<cell halign=\"center\">0.770</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">BVQA* <cit sha=\"48735e5e29aaa49bca85d0dc76a113b1778c92c0\"><ref target=\"bid23\"/></cit>{{cite:48735e5}}</cell>\\n<cell halign=\"center\">0.8362</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8335</cell>\\n<cell halign=\"center\">0.8349</cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8412</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"underline\">0.8415</hi></cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8414</hi></cell>\\n<cell halign=\"center\">0.8312</cell>\\n<cell halign=\"center\">0.8194</cell>\\n<cell>0.8253</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">STDAM <cit sha=\"c6c0872f065d72e0370d386102cdb2bfbac88d47\"><ref target=\"bid6\"/></cit>{{cite:c6c0872}}</cell>\\n<cell halign=\"center\">0.8448</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8415</cell>\\n<cell halign=\"center\">0.8432</cell>\\n<cell halign=\"center\">0.7931</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8204</cell>\\n<cell halign=\"center\">0.8068</cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8341</hi></cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.8297</hi></cell>\\n<cell><hi rend=\"underline\">0.8319</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">DisCoVQA <cit sha=\"ebacaf2da3f7d792f0824641375633af46d4b099\"><ref target=\"bid40\"/></cit>{{cite:ebacaf2}}</cell>\\n<cell halign=\"center\">0.847</cell>\\n<cell right-border=\"true\" halign=\"center\">0.847</cell>\\n<cell halign=\"center\">0.847</cell>\\n<cell halign=\"center\">0.820</cell>\\n<cell right-border=\"true\" halign=\"center\">0.826</cell>\\n<cell halign=\"center\">0.823</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">FastVQA <cit sha=\"f3075fdd875c3fd0c662a61a2aa2cf6f27c14fd2\"><ref target=\"bid37\"/></cit>{{cite:f3075fd}}</cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.859</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"underline\">0.855</hi></cell>\\n<cell halign=\"center\"><hi rend=\"underline\">0.857</hi></cell>\\n<cell halign=\"center\">0.823</cell>\\n<cell right-border=\"true\" halign=\"center\">0.844</cell>\\n<cell halign=\"center\">0.834</cell>\\n<cell halign=\"center\">-</cell>\\n<cell halign=\"center\">-</cell>\\n<cell>-</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Video Swin Tiny</cell>\\n<cell halign=\"center\">0.8316</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8694</cell>\\n<cell halign=\"center\">0.8505</cell>\\n<cell halign=\"center\">0.8335</cell>\\n<cell right-border=\"true\" halign=\"center\">0.8316</cell>\\n<cell halign=\"center\">0.8326</cell>\\n<cell halign=\"center\">0.8566</cell>\\n<cell halign=\"center\">0.8499</cell>\\n<cell>0.8533</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Ada-DQA</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">0.8831</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8741</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8591</hi></cell>\\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">0.8587</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8589</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8729</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8800</hi></cell>\\n<cell><hi rend=\"bold\">0.8765</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:16e73452-3bb6-49a1-9157-f959b7b592f5}}  than the previous best</cell>\\n<cell halign=\"center\">+0.6%</cell>\\n<cell right-border=\"true\" halign=\"center\">+2.8%</cell>\\n<cell halign=\"center\">+1.7%</cell>\\n<cell halign=\"center\">+1.79%</cell>\\n<cell right-border=\"true\" halign=\"center\">+1.72%</cell>\\n<cell halign=\"center\">+1.75%</cell>\\n<cell halign=\"center\">+3.88%</cell>\\n<cell halign=\"center\">+5.03%</cell>\\n<cell>+4.46%</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:0818a4f1-d597-4afa-8fd6-78b317784b57}}  than <hi rend=\"it\">w/o</hi> pretrained models</cell>\\n<cell halign=\"center\">+3.35%</cell>\\n<cell right-border=\"true\" halign=\"center\">+1.37%</cell>\\n<cell halign=\"center\">+2.36%</cell>\\n<cell halign=\"center\">+2.56%</cell>\\n<cell right-border=\"true\" halign=\"center\">+2.71%</cell>\\n<cell halign=\"center\">+2.63%</cell>\\n<cell halign=\"center\">+1.63%</cell>\\n<cell halign=\"center\">+3.01%</cell>\\n<cell>+2.32%</cell>\\n</row></table>',\n",
       "   'caption': 'Comparisons with SOTA methods. The up arrow “{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are highlighted and underlined. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.',\n",
       "   'type': 'table'},\n",
       "  '8d9a3ded-086a-4687-a726-4cc648dbc022': {'table': '<table rend=\"display\" id-text=\"3\" id=\"uid32\" place=\"t\"><head>Experimental analysis on different numbers of selected pretrained models <hi rend=\"it\">with</hi> or <hi rend=\"it\">without</hi> the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of <hi rend=\"it\">with</hi> or <hi rend=\"it\">without</hi> is bolded.</head>\\n<row><cell halign=\"center\">Number</cell>\\n<cell right-border=\"true\" halign=\"center\">QAM</cell>\\n<cell halign=\"center\">KoNViD-1k</cell>\\n<cell halign=\"center\">LIVE-VQC</cell>\\n<cell>YouTube-UGC</cell>\\n</row><row><cell halign=\"center\">3</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:520102f0-4552-44e5-b853-ef384910ec7d}}</cell>\\n<cell halign=\"center\">0.8517</cell>\\n<cell halign=\"center\">0.8432</cell>\\n<cell>0.8630</cell>\\n</row><row><cell halign=\"center\">4</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:5c208282-8d62-42d5-8381-fcf71ca99c11}}</cell>\\n<cell halign=\"center\">0.8613</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8510</hi></cell>\\n<cell><hi rend=\"bold\">0.8659</hi></cell>\\n</row><row><cell halign=\"center\">5</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:55c39428-0e5d-40b5-b2d8-c5511775ee50}}</cell>\\n<cell halign=\"center\">0.8601</cell>\\n<cell halign=\"center\">0.8490</cell>\\n<cell>0.8591</cell>\\n</row><row><cell halign=\"center\">6</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:4fbff708-17bc-428a-b5fd-7a2dec20e294}}</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8615</hi></cell>\\n<cell halign=\"center\">0.8448</cell>\\n<cell>0.8589</cell>\\n</row><row><cell halign=\"center\">7</cell>\\n<cell right-border=\"true\" halign=\"center\">{{formula:d8c752b2-c211-409d-a42a-164151334db2}}</cell>\\n<cell halign=\"center\">0.8573</cell>\\n<cell halign=\"center\">0.8459</cell>\\n<cell>0.8621</cell>\\n</row><row><cell halign=\"center\">3</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8432</cell>\\n<cell halign=\"center\">0.8433</cell>\\n<cell>0.8621</cell>\\n</row><row><cell halign=\"center\">4</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8623</cell>\\n<cell halign=\"center\">0.8514</cell>\\n<cell>0.8695</cell>\\n</row><row><cell halign=\"center\">5</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8621</cell>\\n<cell halign=\"center\">0.8546</cell>\\n<cell>0.8679</cell>\\n</row><row><cell halign=\"center\">6</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8645</cell>\\n<cell halign=\"center\">0.8542</cell>\\n<cell>0.8644</cell>\\n</row><row><cell halign=\"center\">7</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8591</hi></cell>\\n<cell><hi rend=\"bold\">0.8729</hi></cell>\\n</row><row><cell halign=\"center\">8</cell>\\n<cell right-border=\"true\" halign=\"center\"/>\\n<cell halign=\"center\">0.8641</cell>\\n<cell halign=\"center\">0.8560</cell>\\n<cell>0.8711</cell>\\n</row></table>',\n",
       "   'caption': 'Experimental analysis on different numbers of selected pretrained models with or without the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of with or without is bolded.',\n",
       "   'type': 'table'},\n",
       "  'b84d94e4-9350-4014-bc1b-c8b4434ebf24': {'table': '<table rend=\"display\" id-text=\"4\" id=\"uid33\" place=\"t\"><head>Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.</head>\\n<row><cell right-border=\"true\" halign=\"center\">Pretrained Model</cell>\\n<cell halign=\"center\">KoNViD-1k</cell>\\n<cell halign=\"center\">LIVE-VQC</cell>\\n<cell>YouTube-UGC</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\"><hi rend=\"it\">w/o</hi></cell>\\n<cell halign=\"center\">0.8316</cell>\\n<cell halign=\"center\">0.8335</cell>\\n<cell>0.8566</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">EfficientNet-b7</cell>\\n<cell halign=\"center\">0.8412</cell>\\n<cell halign=\"center\">0.8495</cell>\\n<cell>0.8587</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Video Swin Base</cell>\\n<cell halign=\"center\">0.8390</cell>\\n<cell halign=\"center\">0.8173</cell>\\n<cell>0.8603</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Swin Base</cell>\\n<cell halign=\"center\">0.8391</cell>\\n<cell halign=\"center\">0.8475</cell>\\n<cell>0.8497</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">TimeSformer</cell>\\n<cell halign=\"center\">0.8409</cell>\\n<cell halign=\"center\">0.8302</cell>\\n<cell><hi rend=\"bold\">0.8618</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">CLIP</cell>\\n<cell halign=\"center\">0.8404</cell>\\n<cell halign=\"center\">0.8341</cell>\\n<cell>0.8608</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">ir-CNS-152</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8458</hi></cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8521</hi></cell>\\n<cell>0.8518</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">SlowFast</cell>\\n<cell halign=\"center\">0.8423</cell>\\n<cell halign=\"center\">0.8041</cell>\\n<cell>0.8523</cell>\\n</row></table>',\n",
       "   'caption': 'Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.',\n",
       "   'type': 'table'},\n",
       "  '141b825f-8848-4126-9985-77db3f3c4705': {'table': '<table rend=\"display\" id-text=\"5\" id=\"uid36\" place=\"t\"><head>Experiments on different distillation losses.</head>\\n<row><cell right-border=\"true\" halign=\"center\">distillation loss</cell>\\n<cell halign=\"center\">SRCC{{formula:c4c815c4-2362-448d-a544-b0fd44f79d70}}</cell>\\n<cell>PLCC{{formula:11c7a23f-f47a-40f7-83f2-342c6590e0c2}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:a29e8124-9675-4061-9d6d-977cb9100a57}}</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">{{formula:101515c0-83a5-489c-9c6f-dbb3358b1e5e}}</cell>\\n<cell halign=\"center\">0.8092</cell>\\n<cell>0.8310</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Jesen-Shannon</cell>\\n<cell halign=\"center\">0.8455</cell>\\n<cell>0.8646</cell>\\n</row></table>',\n",
       "   'caption': 'Experiments on different distillation losses.',\n",
       "   'type': 'table'},\n",
       "  '56c4b01b-b705-42bd-a314-8bed4c21515d': {'table': '<table rend=\"array\" id-text=\"6\" id=\"uid39\" place=\"t\"><head>Selection of the hyper-parameters of {{formula:a9eb32d2-012d-403d-a083-c08a11dcce5f}}  and {{formula:f0ad8386-af17-423d-b208-6cd0a2cb4525}} .</head>\\n<p><table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}</cell>\\n<cell halign=\"center\">SRCC{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}</cell>\\n<cell>PLCC{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.1</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8462</cell>\\n<cell>0.8663</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8406</cell>\\n<cell>0.8698</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8320</cell>\\n<cell>0.8549</cell>\\n</row></table><table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}</cell>\\n<cell halign=\"center\">SRCC{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}</cell>\\n<cell>PLCC{{formula:1b522aaf-e830-438c-a148-584b695753ab}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.8</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8449</cell>\\n<cell>0.8671</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8528</cell>\\n<cell>0.8716</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8602</cell>\\n<cell>0.8780</cell>\\n</row></table></p><unexpected>\\n\\n[t]0.23\\n<caption>hyper-parameter {{formula:28690097-c6c4-4e25-93c5-fa850bcd8cdf}}</caption><hfill/>[t]0.23\\n<caption>hyper-parameter {{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}} .</caption></unexpected></table>',\n",
       "   'caption': 'hyper-parameter {{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}} .',\n",
       "   'type': 'table'},\n",
       "  '85a7afd7-6fb3-452c-bc87-76bfffd63be7': {'table': '<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}</cell>\\n<cell halign=\"center\">SRCC{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}</cell>\\n<cell>PLCC{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.1</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8462</cell>\\n<cell>0.8663</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8406</cell>\\n<cell>0.8698</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8320</cell>\\n<cell>0.8549</cell>\\n</row></table>',\n",
       "   'caption': 'NO_CAPTION',\n",
       "   'type': 'table'},\n",
       "  '34357c34-625c-4c92-b9f3-1cbd27a380a5': {'table': '<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\">{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}</cell>\\n<cell halign=\"center\">SRCC{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}</cell>\\n<cell>PLCC{{formula:1b522aaf-e830-438c-a148-584b695753ab}}</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.8</cell>\\n<cell halign=\"center\"><hi rend=\"bold\">0.8651</hi></cell>\\n<cell><hi rend=\"bold\">0.8831</hi></cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.2</cell>\\n<cell halign=\"center\">0.8449</cell>\\n<cell>0.8671</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">0.5</cell>\\n<cell halign=\"center\">0.8528</cell>\\n<cell>0.8716</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">1.0</cell>\\n<cell halign=\"center\">0.8602</cell>\\n<cell>0.8780</cell>\\n</row></table>',\n",
       "   'caption': 'NO_CAPTION',\n",
       "   'type': 'table'},\n",
       "  '4588288c-950a-42b5-9e99-a9926c75ff3a': {'table': '<table rend=\"display\" id-text=\"7\" id=\"uid40\" place=\"t\"><head>Contributions of different pretrained models by {{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}} .</head>\\n<row><cell right-border=\"true\" halign=\"center\">Model</cell>\\n<cell halign=\"center\">{{formula:cb8e8db9-0b6f-4095-8935-43c745009f52}}  for LQ videos</cell>\\n<cell>{{formula:5e218773-7abe-4ee6-836b-4e4e6998017a}}  for HQ videos</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">EfficientNet-b7</cell>\\n<cell halign=\"center\">0.1303</cell>\\n<cell>0.7208</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Video Swin Base</cell>\\n<cell halign=\"center\">0.2021</cell>\\n<cell>0.0455</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">Swin Base</cell>\\n<cell halign=\"center\">0.1676</cell>\\n<cell>0.6805</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">TimeSformer</cell>\\n<cell halign=\"center\">0.4679</cell>\\n<cell>0.2317</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">CLIP</cell>\\n<cell halign=\"center\">0.1279</cell>\\n<cell>0.3567</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">ir-CNS-152</cell>\\n<cell halign=\"center\">0.8690</cell>\\n<cell>0.2788</cell>\\n</row><row><cell right-border=\"true\" halign=\"center\">SlowFast</cell>\\n<cell halign=\"center\">0.4210</cell>\\n<cell>0.0018</cell>\\n</row></table>',\n",
       "   'caption': 'Contributions of different pretrained models by {{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}} .',\n",
       "   'type': 'table'}},\n",
       " 'bib_entries': {'445c84720802ddacc214925b8a18a5764162b41d': {'bib_entry_raw': 'Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. 2021. ViViT: A Video Vision Transformer. CoRR abs/2103.15691 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1cd332654b46fa3bbe4feac69d56da09340034b9': {'bib_entry_raw': 'Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is Space-Time Attention All You Need for Video Understanding?. In ICML, Vol. 139. PMLR, 813–824.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f2782ba1fc48bd553d272241a97ab2f07c7e3202': {'bib_entry_raw': 'João Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. 2018. A Short Note about Kinetics-600. CoRR abs/1808.01340 (2018).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9b70eb43e82b6795b3bad723ddf018c158705cb4': {'bib_entry_raw': 'Aaron Chadha and Yiannis Andreopoulos. 2021. Deep Perceptual Preprocessing for Video Coding. In CVPR. IEEE, 14852–14861.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'b5fb90b58169d81c40acf45ac8e6e11cb382c1b0': {'bib_entry_raw': 'Kuan-Ta Chen, Chi-Jui Chang, Chen-Chi Wu, Yu-Chun Chang, and Chin-Laung Lei. 2010. Quadrant of euphoria: a crowdsourcing platform for QoE assessment. IEEE Netw. 24, 2 (2010), 28–35. https://doi.org/10.1109/MNET.2010.5430141',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/MNET.2010.5430141',\n",
       "     'text': 'https://doi.org/10.1109/MNET.2010.5430141',\n",
       "     'start': 182,\n",
       "     'end': 223}]},\n",
       "  '04485d0956438899571e65d6b1847ec05a6a9c5b': {'bib_entry_raw': 'Pengfei Chen, Leida Li, Lei Ma, Jinjian Wu, and Guangming Shi. 2020b. RIRNet: Recurrent-In-Recurrent Network for Video Quality Assessment. In ACM MM. ACM, 834–842.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'b519bb8aedbcaca80c324c191ad0f5a2144c0534': {'bib_entry_raw': 'Pengfei Chen, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. 2022. Contrastive Self-Supervised Pre-Training for Video Quality Assessment. IEEE Trans. Image Process. 31 (2022), 458–471.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '06016b9532826c93f4afeac2abe91b104e7fe580': {'bib_entry_raw': 'Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020a. A Simple Framework for Contrastive Learning of Visual Representations. In ICML, Vol. 119. PMLR, 1597–1607.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '11e5a31df37f23435d9bf5bc45b27b1de884c03a': {'bib_entry_raw': 'Yanjiao Chen, Kaishun Wu, and Qian Zhang. 2015. From QoS to QoE: A Tutorial on Video Quality Assessment. IEEE Commun. Surv. Tutorials 17, 2 (2015), 1126–1165. https://doi.org/10.1109/COMST.2014.2363139',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/COMST.2014.2363139',\n",
       "     'text': 'https://doi.org/10.1109/COMST.2014.2363139',\n",
       "     'start': 159,\n",
       "     'end': 201}]},\n",
       "  '08d1075d1f740185bbd331ea03ac31a73c2587c8': {'bib_entry_raw': 'Shyamprasad Chikkerur, Vijay Sundaram, Martin Reisslein, and Lina J. Karam. 2011. Objective Video Quality Assessment Methods: A Classification, Review, and Performance Comparison. IEEE Trans. Broadcast. 57, 2 (2011), 165–182.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'cef100d09a20adbadb11d206f7d3125c4ecdd5fd': {'bib_entry_raw': 'Cisco. 2021. Cisco annual internet report white paper. https://www.cisco.com/c/en/us/solutions/collateral/ executive-perspectives/annual-internet-report/white-paper-c11-741490.html.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://www.cisco.com/c/en/us/solutions/collateral/ executive-perspectives/annual-internet-report/white-paper-c11-741490.html',\n",
       "     'text': 'https://www.cisco.com/c/en/us/solutions/collateral/ executive-perspectives/annual-internet-report/white-paper-c11-741490.html.',\n",
       "     'start': 55,\n",
       "     'end': 181}]},\n",
       "  'a718e63e60682d859a31a46983da9514e83d45b5': {'bib_entry_raw': \"MMAction2 Contributors. 2020. OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark. https://github.com/open-mmlab/mmaction2.\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://github.com/open-mmlab/mmaction2',\n",
       "     'text': 'https://github.com/open-mmlab/mmaction2.',\n",
       "     'start': 101,\n",
       "     'end': 141}]},\n",
       "  'bad8aed6a62f21179fc07856d2f1fb32e9e0bfd2': {'bib_entry_raw': 'Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In CVPR. IEEE Computer Society, 248–255.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1af68c6a2b2f94cf4ab35e14311272390f599453': {'bib_entry_raw': 'Joshua Peter Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, and Alan C. Bovik. 2020. No-Reference Video Quality Assessment Using Space-Time Chips. In MMSP. IEEE, 1–6.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '55db95fd02d5d42c08599ed5a8077d0ff0fd430f': {'bib_entry_raw': 'Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. SlowFast Networks for Video Recognition. In ICCV. 6201–6210.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9306c461bfb29b796bb723f589f12d12ab4003a9': {'bib_entry_raw': 'Bent Fuglede and Flemming Topsøe. 2004. Jensen-Shannon divergence and Hilbert space embedding. In ISIT. IEEE, 31.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9c04ab9115e73e7300a6077937ec1b23e3fdf820': {'bib_entry_raw': 'Franz Götz-Hahn, Vlad Hosu, Hanhe Lin, and Dietmar Saupe. 2021. KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild. IEEE Access 9 (2021), 72139–72160. https://doi.org/10.1109/ACCESS.2021.3077642',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/ACCESS.2021.3077642',\n",
       "     'text': 'https://doi.org/10.1109/ACCESS.2021.3077642',\n",
       "     'start': 187,\n",
       "     'end': 230}]},\n",
       "  'b2e0c1508c6aff960f71bc2d31c91555d192f868': {'bib_entry_raw': 'Jie Gu, Gaofeng Meng, Cheng Da, Shiming Xiang, and Chunhong Pan. 2019a. No-Reference Image Quality Assessment with Reinforcement Recursive List-Wise Ranking. In AAAI. AAAI Press, 8336–8343.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'd5c7273f6adc0feb2e8d22ea7031df71fcfb3f99': {'bib_entry_raw': 'Jie Gu, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. 2019b. Blind image quality assessment via learnable attention-based pooling. Pattern Recognit. 91 (2019), 332–344.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'e64e98e251fef4d7713d86c975cc21749bb61550': {'bib_entry_raw': 'Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In CVPR. Computer Vision Foundation / IEEE, 9726–9735.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3ff8959facdc02de0e570002ac51fdf3b0b392d7': {'bib_entry_raw': 'Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowledge in a Neural Network. CoRR abs/1503.02531 (2015).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '2222c26a7c1e762ad3063dc13ad2c553a6d739be': {'bib_entry_raw': 'Tobias Hoßfeld, Christian Keimel, Matthias Hirth, Bruno Gardlo, Julian Habigt, Klaus Diepold, and Phuoc Tran-Gia. 2014. Best Practices for QoE Crowdtesting: QoE Assessment With Crowdsourcing. IEEE Trans. Multim. 16, 2 (2014), 541–558. https://doi.org/10.1109/TMM.2013.2291663',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/TMM.2013.2291663',\n",
       "     'text': 'https://doi.org/10.1109/TMM.2013.2291663',\n",
       "     'start': 235,\n",
       "     'end': 275}]},\n",
       "  '24b4d04b01098cffe3cb975171aa05132c6a0903': {'bib_entry_raw': 'Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tamás Szirányi, Shujun Li, and Dietmar Saupe. 2017. The Konstanz natural video database (KoNViD-1k). In QoMEX. IEEE, 1–6.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '65d772704ddf1c8224f0b3a96a41b1ae8c93c395': {'bib_entry_raw': 'Rui Hou, YunHao Zhao, Yang Hu, and Huan Liu. 2020. No-reference video quality evaluation by a deep transfer CNN architecture. SPIC 83 (2020), 115782.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'fbaab966ad1e0efdeb5b96e80b5b467ba60eeced': {'bib_entry_raw': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. 2014. Large-scale Video Classification with Convolutional Neural Networks. In CVPR.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'dd352e3c3918b783377312dd4399a284589a24d1': {'bib_entry_raw': 'Will Kay, João Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. 2017. The Kinetics Human Action Video Dataset. CoRR abs/1705.06950 (2017).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'dbe6597a54c9f5c71a1c35c272930b6bd7510e99': {'bib_entry_raw': 'Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. 2021. MUSIQ: Multi-scale Image Quality Transformer. (October 2021), 5148–5157.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '87e3b5086562892a8e0130a9c50711fab689a72a': {'bib_entry_raw': 'Jari Korhonen. 2019. Two-Level Approach for No-Reference Consumer Video Quality Assessment. IEEE Trans. Image Process. 28, 12 (2019), 5923–5938.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3237048360325037902afaa52cc965b5a48c70c3': {'bib_entry_raw': \"Jari Korhonen, Yicheng Su, and Junyong You. 2020. Blind Natural Video Quality Prediction via Statistical Temporal Features and Deep Spatial Features. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, Chang Wen Chen, Rita Cucchiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann (Eds.). ACM, 3311–3319. https://doi.org/10.1145/3394171.3413845\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1145/3394171.3413845',\n",
       "     'text': 'https://doi.org/10.1145/3394171.3413845',\n",
       "     'start': 403,\n",
       "     'end': 442}]},\n",
       "  '48735e5e29aaa49bca85d0dc76a113b1778c92c0': {'bib_entry_raw': 'Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, and Xianpei Wang. 2021b. Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. CoRR abs/2108.08505 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '7da41796bcbf3c567677b6c3fc55a27f94ef63b0': {'bib_entry_raw': 'Dingquan Li, Tingting Jiang, and Ming Jiang. 2019. Quality Assessment of In-the-Wild Videos. In ACM Multimedia. ACM, 2351–2359.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1ad644b3f88e49a2de5aeab132462a51198265de': {'bib_entry_raw': 'Dingquan Li, Tingting Jiang, and Ming Jiang. 2021a. Unified Quality Assessment of in-the-Wild Videos with Mixed Datasets Training. IJCV 129, 4 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'db5dff2e2ed6e62a80e58067cbdb74063c18a0bf': {'bib_entry_raw': \"Liang Liao, Kangmin Xu, Haoning Wu, Chaofeng Chen, Wenxiu Sun, Qiong Yan, and Weisi Lin. 2022. Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment. In MM '22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, João Magalhães, Alberto Del Bimbo, Shin'ichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, and Laura Toni (Eds.). ACM, 837–846. https://doi.org/10.1145/3503161.3547849\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1145/3503161.3547849',\n",
       "     'text': 'https://doi.org/10.1145/3503161.3547849',\n",
       "     'start': 444,\n",
       "     'end': 483}]},\n",
       "  '942a083c331580e3d3b7ea7de148a99be5c5f998': {'bib_entry_raw': 'Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021a. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In ICCV. 10012–10022.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3b57340bcff1ec73fee2f7513ac2159950fbd1d9': {'bib_entry_raw': 'Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. 2021b. Video Swin Transformer. CoRR abs/2106.13230 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '18dcc024a728c82f0671adaeb65b81b7185d7acd': {'bib_entry_raw': 'Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012. No-Reference Image Quality Assessment in the Spatial Domain. IEEE Trans. Image Process. 21, 12 (2012), 4695–4708.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '0832a8c9419aa6a0897ba06ab4d4528070bf0fa7': {'bib_entry_raw': 'Anish Mittal, Michele A. Saad, and Alan C. Bovik. 2016. A Completely Blind Video Integrity Oracle. IEEE Trans. Image Process. 25, 1 (2016), 289–300.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'c619c6de8497b9acc2901779f3d6f41bb0653a99': {'bib_entry_raw': 'Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. Making a \"Completely Blind\" Image Quality Analyzer. IEEE SPL 20, 3 (2013), 209–212.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '2e556d9cae64c69740bb3a2284c192df6db50d34': {'bib_entry_raw': 'Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS. 8024–8035.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'c682601a81559291eba0b36bbfd8204f86fca999': {'bib_entry_raw': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML. PMLR, 8748–8763.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '7c1267c9415d3a71add5a7b3777ccea8cab6dfe0': {'bib_entry_raw': 'Michele A. Saad, Alan C. Bovik, and Christophe Charrier. 2012. Blind Image Quality Assessment: A Natural Scene Statistics Approach in the DCT Domain. IEEE Trans. Image Process. 21, 8 (2012), 3339–3352. https://doi.org/10.1109/TIP.2012.2191563',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/TIP.2012.2191563',\n",
       "     'text': 'https://doi.org/10.1109/TIP.2012.2191563',\n",
       "     'start': 202,\n",
       "     'end': 242}]},\n",
       "  '5f9c3ce33b51d81f001155e15bc1eeb419614930': {'bib_entry_raw': 'Michele A. Saad, Alan C. Bovik, and Christophe Charrier. 2014. Blind Prediction of Natural Video Quality. IEEE Trans. Image Process. 23, 3 (2014), 1352–1365.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'ecf1e0df49a10250089dc0a0aac8acef6e77c03c': {'bib_entry_raw': 'Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In ICCV. IEEE Computer Society, 618–626.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '689f6563165a43df2f9743153047793b7bfada1e': {'bib_entry_raw': 'Muhammad Shahid, Jacob Søgaard, Jeevan Pokhrel, Kjell Brunnström, Kun Wang, Samira Tavakoli, and Narciso García. 2014. Crowdsourcing based subjective quality assessment of adaptive video streaming. In Sixth International Workshop on Quality of Multimedia Experience, QoMEX 2014, Singapore, September 18-20, 2014. IEEE, 53–54. https://doi.org/10.1109/QoMEX.2014.6982289',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/QoMEX.2014.6982289',\n",
       "     'text': 'https://doi.org/10.1109/QoMEX.2014.6982289',\n",
       "     'start': 326,\n",
       "     'end': 368}]},\n",
       "  '8f3eb869445ed0622a9879a6f2010828f6039e8b': {'bib_entry_raw': 'Zeina Sinno and Alan Conrad Bovik. 2019. Large-Scale Study of Perceptual Video Quality. IEEE Trans. Image Process. 28, 2 (2019), 612–627.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'd48f3caea220408a74618213f7a143765c70f8a5': {'bib_entry_raw': 'Mingxing Tan and Quoc V. Le. 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In ICML (Proceedings of Machine Learning Research, Vol. 97). PMLR, 6105–6114.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f69f321b9fac8171bb5acda04e263ebcedbd706d': {'bib_entry_raw': 'Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. 2019. Video Classification With Channel-Separated Convolutional Networks. In ICCV. IEEE, 5551–5560.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'b58ab68c5d2dfbe0f0b04af2023d37b40675ed95': {'bib_entry_raw': 'Zhengzhong Tu, Chia-Ju Chen, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021a. Efficient User-Generated Video Quality Prediction. In Picture Coding Symposium, PCS 2021, Bristol, United Kingdom, June 29 - July 2, 2021. IEEE, 1–5. https://doi.org/10.1109/PCS50896.2021.9477483',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/PCS50896.2021.9477483',\n",
       "     'text': 'https://doi.org/10.1109/PCS50896.2021.9477483',\n",
       "     'start': 248,\n",
       "     'end': 293}]},\n",
       "  'e20cc9f23a038321af050adbe9b1c391b073144e': {'bib_entry_raw': 'Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021b. UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content. IEEE TIP 30 (2021), 4449–4464.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '6b6881a46c72b96eaeeda4a1488cbc4688ab1ffc': {'bib_entry_raw': 'Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021c. UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content. IEEE TIP 30 (2021), 4449–4464.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f7fae7386c3d4b0aa93baaf42e1c16f1fb782b70': {'bib_entry_raw': 'Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C. Bovik. 2021d. RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated Content. CoRR abs/2101.10955 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1cad59a8b730c5f05e99b511bdea2006d47c9e1f': {'bib_entry_raw': 'Domonkos Varga and Tamás Szirányi. 2019. No-reference video quality assessment via pretrained CNN and LSTM networks. Signal Image Video Process. 13, 8 (2019), 1569–1576.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '9be8207b7ba6b4f423a574f686110ed04d5a3d91': {'bib_entry_raw': 'Yilin Wang, Sasi Inguva, and Balu Adsumilli. 2019. YouTube UGC Dataset for Video Compression Research. In MMSP. IEEE, 1–5.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '80cfe8864bda775820fbc62d4bb618efafdb8421': {'bib_entry_raw': 'Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, and Feng Yang. 2021. Rich Features for Perceptual Quality Assessment of UGC Videos. In CVPR. 13435–13444.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '3d9999db65fc542e1b4b4c9bb78077ba11472c4e': {'bib_entry_raw': 'Zhou Wang, Ligang Lu, and Alan C. Bovik. 2004. Video quality assessment based on structural distortion measurement. Signal Process. Image Commun. 19, 2 (2004), 121–132.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'f3075fdd875c3fd0c662a61a2aa2cf6f27c14fd2': {'bib_entry_raw': 'Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. 2022a. FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling. In ECCV (6), Vol. 13666. 538–554.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'ebacaf2da3f7d792f0824641375633af46d4b099': {'bib_entry_raw': 'Haoning Wu, Chaofeng Chen, Liang Liao, Jingwen Hou, Qiong Yan, and Weisi Lin. 2022b. DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment. CoRR abs/2206.09853 (2022).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '1c7237b9ad351ad4648f2a9a0d988f0a5b415dea': {'bib_entry_raw': 'Fengchuang Xing, Yuan-Gen Wang, Hanpin Wang, Leida Li, and Guopu Zhu. 2021. StarVQA: Space-Time Attention for Video Quality Assessment. CoRR abs/2108.09635 (2021).',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'c6c0872f065d72e0370d386102cdb2bfbac88d47': {'bib_entry_raw': 'Jiahua Xu, Jing Li, Xingguang Zhou, Wei Zhou, Baichao Wang, and Zhibo Chen. 2021. Perceptual Quality Assessment of Internet Videos. In ACM Multimedia. ACM, 1248–1257.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '4bce23f631f9ae481757708cc0ea26efae5b89ab': {'bib_entry_raw': 'Jia Yan, Weixia Zhang, and Tianpeng Feng. 2016. Blind Image Quality Assessment Based on Natural Redundancy Statistics. In Computer Vision - ACCV 2016 - 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part IV (Lecture Notes in Computer Science, Vol. 10114), Shang-Hong Lai, Vincent Lepetit, Ko Nishino, and Yoichi Sato (Eds.). Springer, 3–18. https://doi.org/10.1007/978-3-319-54190-7_1',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1007/978-3-319-54190-7_1',\n",
       "     'text': 'https://doi.org/10.1007/978-3-319-54190-7_1',\n",
       "     'start': 399,\n",
       "     'end': 442}]},\n",
       "  '3dfd446db36563cc1eeb028310478bbb775a0237': {'bib_entry_raw': \"Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik. 2021. Patch-VQ: 'Patching Up' the Video Quality Problem. In CVPR. Computer Vision Foundation / IEEE, 14019–14029.\",\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '83e016cddc45f2a6e168da7fcc15222f30b62750': {'bib_entry_raw': 'Junyong You. 2021. Long Short-term Convolutional Transformer for No-Reference Video Quality Assessment. In ACM Multimedia. ACM, 2112–2120.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  '5d0c0fc387c200be63a04d7091ff071e6b1d7e15': {'bib_entry_raw': 'Junyong You and Jari Korhonen. 2019. Deep Neural Networks for No-Reference Video Quality Assessment. In 2019 IEEE International Conference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019. IEEE, 2349–2353. https://doi.org/10.1109/ICIP.2019.8803395',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': [{'url': 'https://doi.org/10.1109/ICIP.2019.8803395',\n",
       "     'text': 'https://doi.org/10.1109/ICIP.2019.8803395',\n",
       "     'start': 227,\n",
       "     'end': 268}]},\n",
       "  'b0d4c59ca1f40082c912ba330d487bbb8795000a': {'bib_entry_raw': 'Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen. 2023b. Quality-Aware Pre-Trained Models for Blind Image Quality Assessment. In CVPR. IEEE Computer Society, 22302–22313.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []},\n",
       "  'fab2895d792fae2caac7291068c07fb9d37da6b1': {'bib_entry_raw': 'Kai Zhao, Kun Yuan, Ming Sun, and Xing Wen. 2023a. Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment. In CVPR Workshops. IEEE Computer Society, 1302–1310.',\n",
       "   'contained_arXiv_ids': [],\n",
       "   'contained_links': []}},\n",
       " 'body_text': [{'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': \"With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\\n\",\n",
       "   'cite_spans': [{'start': 128,\n",
       "     'end': 144,\n",
       "     'text': '{{cite:cef100d}}',\n",
       "     'ref_id': 'cef100d'},\n",
       "    {'start': 306,\n",
       "     'end': 322,\n",
       "     'text': '{{cite:9b70eb4}}',\n",
       "     'ref_id': '9b70eb4'},\n",
       "    {'start': 324,\n",
       "     'end': 340,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'},\n",
       "    {'start': 419,\n",
       "     'end': 435,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 437,\n",
       "     'end': 453,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 455,\n",
       "     'end': 471,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'},\n",
       "    {'start': 473,\n",
       "     'end': 489,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'},\n",
       "    {'start': 491,\n",
       "     'end': 507,\n",
       "     'text': '{{cite:5d0c0fc}}',\n",
       "     'ref_id': '5d0c0fc'},\n",
       "    {'start': 509,\n",
       "     'end': 525,\n",
       "     'text': '{{cite:3237048}}',\n",
       "     'ref_id': '3237048'},\n",
       "    {'start': 581,\n",
       "     'end': 597,\n",
       "     'text': '{{cite:8f3eb86}}',\n",
       "     'ref_id': '8f3eb86'},\n",
       "    {'start': 599,\n",
       "     'end': 615,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 617,\n",
       "     'end': 633,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'},\n",
       "    {'start': 635,\n",
       "     'end': 651,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab.\\xa0{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\\n',\n",
       "   'cite_spans': [{'start': 256,\n",
       "     'end': 272,\n",
       "     'text': '{{cite:fbaab96}}',\n",
       "     'ref_id': 'fbaab96'},\n",
       "    {'start': 274,\n",
       "     'end': 290,\n",
       "     'text': '{{cite:dd352e3}}',\n",
       "     'ref_id': 'dd352e3'},\n",
       "    {'start': 407,\n",
       "     'end': 423,\n",
       "     'text': '{{cite:b5fb90b}}',\n",
       "     'ref_id': 'b5fb90b'},\n",
       "    {'start': 425,\n",
       "     'end': 441,\n",
       "     'text': '{{cite:2222c26}}',\n",
       "     'ref_id': '2222c26'},\n",
       "    {'start': 443,\n",
       "     'end': 459,\n",
       "     'text': '{{cite:689f656}}',\n",
       "     'ref_id': '689f656'},\n",
       "    {'start': 461,\n",
       "     'end': 477,\n",
       "     'text': '{{cite:11e5a31}}',\n",
       "     'ref_id': '11e5a31'},\n",
       "    {'start': 619,\n",
       "     'end': 635,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 767,\n",
       "     'end': 783,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'},\n",
       "    {'start': 785,\n",
       "     'end': 801,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'},\n",
       "    {'start': 1022,\n",
       "     'end': 1038,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'}],\n",
       "   'ref_spans': [{'start': 106,\n",
       "     'end': 152,\n",
       "     'text': '{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "     'ref_id': '43c9402e-2914-42dc-99c4-b2424f71f6ad'}]},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\\nDataset\\nTask\\nSize\\nAnnotations\\nKoNViD-1k {{cite:24b4d04}}\\nVQA\\n1,200\\n114\\nLIVE-VQC {{cite:8f3eb86}}\\nVQA\\n585\\n240\\nYouTube-UGC {{cite:9be8207}}\\nVQA\\n1,380\\n123\\nLSVQ {{cite:3dfd446}}\\nVQA\\n39,075\\n35\\nKoNViD-150k {{cite:9c04ab9}}\\nVQA\\n153,841\\n5\\nSports-1M {{cite:fbaab96}}\\nclassification\\n1,133,158\\n- (auto.)\\nKinetics-400 {{cite:dd352e3}}\\nclassification\\n306,245\\n3-5\\n{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "   'cite_spans': [{'start': 240,\n",
       "     'end': 256,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 280,\n",
       "     'end': 296,\n",
       "     'text': '{{cite:8f3eb86}}',\n",
       "     'ref_id': '8f3eb86'},\n",
       "    {'start': 321,\n",
       "     'end': 337,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'},\n",
       "    {'start': 357,\n",
       "     'end': 373,\n",
       "     'text': '{{cite:3dfd446}}',\n",
       "     'ref_id': '3dfd446'},\n",
       "    {'start': 400,\n",
       "     'end': 416,\n",
       "     'text': '{{cite:9c04ab9}}',\n",
       "     'ref_id': '9c04ab9'},\n",
       "    {'start': 441,\n",
       "     'end': 457,\n",
       "     'text': '{{cite:fbaab96}}',\n",
       "     'ref_id': 'fbaab96'},\n",
       "    {'start': 506,\n",
       "     'end': 522,\n",
       "     'text': '{{cite:dd352e3}}',\n",
       "     'ref_id': 'dd352e3'}],\n",
       "   'ref_spans': [{'start': 550,\n",
       "     'end': 596,\n",
       "     'text': '{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "     'ref_id': '43c9402e-2914-42dc-99c4-b2424f71f6ad'}]},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig.\\xa0{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\\n',\n",
       "   'cite_spans': [{'start': 72,\n",
       "     'end': 88,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 90, 'end': 106, 'text': '{{cite:1ad644b}}', 'ref_id': '1ad644b'},\n",
       "    {'start': 108,\n",
       "     'end': 124,\n",
       "     'text': '{{cite:65d7727}}',\n",
       "     'ref_id': '65d7727'},\n",
       "    {'start': 126,\n",
       "     'end': 142,\n",
       "     'text': '{{cite:1cad59a}}',\n",
       "     'ref_id': '1cad59a'},\n",
       "    {'start': 299,\n",
       "     'end': 315,\n",
       "     'text': '{{cite:bad8aed}}',\n",
       "     'ref_id': 'bad8aed'},\n",
       "    {'start': 779,\n",
       "     'end': 795,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'},\n",
       "    {'start': 942,\n",
       "     'end': 958,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 960,\n",
       "     'end': 976,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'}],\n",
       "   'ref_spans': [{'start': 1181,\n",
       "     'end': 1227,\n",
       "     'text': '{{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}}',\n",
       "     'ref_id': '43c9402e-2914-42dc-99c4-b2424f71f6ad'}]},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\\n',\n",
       "   'cite_spans': [{'start': 653,\n",
       "     'end': 669,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\\nOur contributions are as follows:\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Introduction',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'list',\n",
       "   'text': '\\nTo the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\\n\\nWe propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\\n\\nWe evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\\n\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Related work',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'According to the availability of reference videos, VQA methods can be classified into full-reference (FR), reduced-reference (RR), and no-reference (NR) {{cite:48735e5}} ones. As reference videos are always hard to obtain, NR-VQA becomes more practical in the real-world VQA scenario, which is investigated in this paper. According to the difference in construction schema, VQA methods can be classified into traditional hand-crafted and learning-based ones.\\n',\n",
       "   'cite_spans': [{'start': 153,\n",
       "     'end': 169,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Related work',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Overview of our proposed Ada-DQA framework. First, in-the-wild pretrained models are selected as candidates according to diverse aspects. Second, features generated by these frozen pretrained models are aggregated per sample using the QAM adaptively. This approach allows acquiring of quality-related representations. Third, during training, the integrated feature is utilized as supplementary supervision, along with the labeled quality score, to guide the training of a lightweight VQA model. During inference, only the optimized VQA model is used, reducing the computational cost largely.\\n{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 592,\n",
       "     'end': 639,\n",
       "     'text': '{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}}',\n",
       "     'ref_id': 'fa4ceb3d-fea6-4d86-9c04-31b91570ede3'}]},\n",
       "  {'section': 'Classical VQA Approaches',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Classic VQA methods {{cite:7c1267c}}, {{cite:0832a8c}}, {{cite:87e3b50}}, {{cite:e20cc9f}}, {{cite:b58ab68}}, {{cite:db5dff2}} rely on handcrafted features to evaluate video quality. With an underlying assumption that the perceptual quality can be measured by the disturbance of natural scene statics (NSS) {{cite:4bce23f}}, these work attempts at designing handcrafted features with richer representation for VQA. The work {{cite:5f9c3ce}} based on the 2D discrete-time transform (DCT) features of video frame-difference statistics, and motion information is further introduced to level up the representation capacity. TLVQM {{cite:87e3b50}} utilizes a combination of spatial high-complexity and temporal low-complexity handcraft features. Whereas, handcrafted features are gradually replaced by the DNN-based features, due to their sensitivity to distortion types and the superiority DNN features demonstrated in various computer vision tasks.\\n',\n",
       "   'cite_spans': [{'start': 20,\n",
       "     'end': 36,\n",
       "     'text': '{{cite:7c1267c}}',\n",
       "     'ref_id': '7c1267c'},\n",
       "    {'start': 38, 'end': 54, 'text': '{{cite:0832a8c}}', 'ref_id': '0832a8c'},\n",
       "    {'start': 56, 'end': 72, 'text': '{{cite:87e3b50}}', 'ref_id': '87e3b50'},\n",
       "    {'start': 74, 'end': 90, 'text': '{{cite:e20cc9f}}', 'ref_id': 'e20cc9f'},\n",
       "    {'start': 92, 'end': 108, 'text': '{{cite:b58ab68}}', 'ref_id': 'b58ab68'},\n",
       "    {'start': 110,\n",
       "     'end': 126,\n",
       "     'text': '{{cite:db5dff2}}',\n",
       "     'ref_id': 'db5dff2'},\n",
       "    {'start': 307,\n",
       "     'end': 323,\n",
       "     'text': '{{cite:4bce23f}}',\n",
       "     'ref_id': '4bce23f'},\n",
       "    {'start': 424,\n",
       "     'end': 440,\n",
       "     'text': '{{cite:5f9c3ce}}',\n",
       "     'ref_id': '5f9c3ce'},\n",
       "    {'start': 626,\n",
       "     'end': 642,\n",
       "     'text': '{{cite:87e3b50}}',\n",
       "     'ref_id': '87e3b50'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'DNN-based VQA Methods',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Recently, CNN-based methods {{cite:7da4179}}, {{cite:e20cc9f}}, {{cite:d5c7273}}, {{cite:b2e0c15}} and Transformer-based methods {{cite:dbe6597}}, {{cite:1c7237b}}, {{cite:fab2895}}, {{cite:f3075fd}} have taken the lead in the QA domain. However, due to the data-driven characteristics of deep learning, most of the current VQA models suffer from the lack of sufficient high-quality-labeled datasets. There are some attempts to relieve this insufficient data challenge, either from patch-level/frame-level augmentation {{cite:7da4179}}, {{cite:83e016c}} or fine-tuning from other large computer vision models pretrained on large general knowledge-based datasets {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}}. VSFA {{cite:7da4179}} extracts frame-wise features with ResNet and uses a gated-recurrent unit to model temporal information. LSCT {{cite:83e016c}} adopts a Transformer to predict video quality based on the frame features extracted by an IQA model. But frame-level augmentation dismissed the effect brought by temporal concealment, which is widely noticed nowadays.\\n',\n",
       "   'cite_spans': [{'start': 28,\n",
       "     'end': 44,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 46, 'end': 62, 'text': '{{cite:e20cc9f}}', 'ref_id': 'e20cc9f'},\n",
       "    {'start': 64, 'end': 80, 'text': '{{cite:d5c7273}}', 'ref_id': 'd5c7273'},\n",
       "    {'start': 82, 'end': 98, 'text': '{{cite:b2e0c15}}', 'ref_id': 'b2e0c15'},\n",
       "    {'start': 129,\n",
       "     'end': 145,\n",
       "     'text': '{{cite:dbe6597}}',\n",
       "     'ref_id': 'dbe6597'},\n",
       "    {'start': 147,\n",
       "     'end': 163,\n",
       "     'text': '{{cite:1c7237b}}',\n",
       "     'ref_id': '1c7237b'},\n",
       "    {'start': 165,\n",
       "     'end': 181,\n",
       "     'text': '{{cite:fab2895}}',\n",
       "     'ref_id': 'fab2895'},\n",
       "    {'start': 183,\n",
       "     'end': 199,\n",
       "     'text': '{{cite:f3075fd}}',\n",
       "     'ref_id': 'f3075fd'},\n",
       "    {'start': 519,\n",
       "     'end': 535,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 537,\n",
       "     'end': 553,\n",
       "     'text': '{{cite:83e016c}}',\n",
       "     'ref_id': '83e016c'},\n",
       "    {'start': 662,\n",
       "     'end': 678,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 680,\n",
       "     'end': 696,\n",
       "     'text': '{{cite:65d7727}}',\n",
       "     'ref_id': '65d7727'},\n",
       "    {'start': 698,\n",
       "     'end': 714,\n",
       "     'text': '{{cite:1cad59a}}',\n",
       "     'ref_id': '1cad59a'},\n",
       "    {'start': 721,\n",
       "     'end': 737,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 847,\n",
       "     'end': 863,\n",
       "     'text': '{{cite:83e016c}}',\n",
       "     'ref_id': '83e016c'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'DNN-based VQA Methods',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': \"Most fine-tuning work {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} utilizes models pretrained on classification datasets, where learned information mainly covers content-awareness and is not tailor for the task of VQA.\\nSeveral work {{cite:80cfe88}}, {{cite:48735e5}}, {{cite:b0d4c59}} has noticed the insufficiency of content-aware information.\\nCoINVQ {{cite:80cfe88}} leverage distortion-aware and compression-aware representation besides the content-aware representation. Nevertheless, the distorted information is learned from synthetic datasets and the generalized ability to in-the-wild distorted data needs to be verified. BVQA {{cite:48735e5}} introduces motion-aware information learned from the action classification dataset. But they dismiss the fact that distortion awareness is crucial to VQA.\\nWhat's more, these works either utilize a temporal-sampling and concatenating strategy to aggregate features or employ temporal average pooling for feature fusion. The final features are not acquired in an adaptive and flexible manner, which prohibits the diverse feature representations from unleashing full potential. More recent work focus on building spatiotemporal relation.\\nStarVQA {{cite:1c7237b}} builds a Transformer by using divided space-time attention. DisCoVQA {{cite:ebacaf2}} design a transformer-based Spatial-Temporal Distortion Extraction module to tackle temporal quality attention. FastVQA {{cite:f3075fd}} attempts to assess local quality by sampling patches at their raw resolution and covers global quality with contextual relations.\\n\",\n",
       "   'cite_spans': [{'start': 22,\n",
       "     'end': 38,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 40, 'end': 56, 'text': '{{cite:65d7727}}', 'ref_id': '65d7727'},\n",
       "    {'start': 58, 'end': 74, 'text': '{{cite:1cad59a}}', 'ref_id': '1cad59a'},\n",
       "    {'start': 240,\n",
       "     'end': 256,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 258,\n",
       "     'end': 274,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'},\n",
       "    {'start': 276,\n",
       "     'end': 292,\n",
       "     'text': '{{cite:b0d4c59}}',\n",
       "     'ref_id': 'b0d4c59'},\n",
       "    {'start': 360,\n",
       "     'end': 376,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 642,\n",
       "     'end': 658,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'},\n",
       "    {'start': 1202,\n",
       "     'end': 1218,\n",
       "     'text': '{{cite:1c7237b}}',\n",
       "     'ref_id': '1c7237b'},\n",
       "    {'start': 1288,\n",
       "     'end': 1304,\n",
       "     'text': '{{cite:ebacaf2}}',\n",
       "     'ref_id': 'ebacaf2'},\n",
       "    {'start': 1424,\n",
       "     'end': 1440,\n",
       "     'text': '{{cite:f3075fd}}',\n",
       "     'ref_id': 'f3075fd'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Method',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To surmount the constraint of limited labeled data availability and to obtain the quality-related features inherent in diverse modalities, we introduce the Ada-DQA framework for VQA tasks. In Sec.REF , we provide an overview of the framework. In Sec.REF , we explicate the construction of pretrained models from various aspects. In Sec.REF , we elucidate the process of acquiring quality representation using the proposed Quality-aware Acquisition Module (QAM). Finally, in Sec.REF , we present the optimization objective during training based on knowledge distillation and regression loss.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Ada-DQA Framework',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'As shown in Fig.\\xa0{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}} , the framework of Ada-DQA can be divided into three components. First, {{formula:0a2dda82-f469-4a2c-a7e4-e3eb50eb52d6}}  pretrained models, which act as feature extractors, are selected as candidates from the wild. Given an input video {{formula:70d4f318-edc8-4a87-adf4-4481112155d1}} , features are generated by these pretrained models, whose weights are frozen. This significantly reduces the training cost of multiple heavy pretrained models. According to the training paradigm of pretrained models, these features may contain quality-related information (e.g., , content, distortions, and motion). However, since factors that may affect quality vary in different videos, the correlation between the quality of different videos and these features is also different. Second, to adaptively capture desired quality-related features sample-by-sample during training, the proposed QAM is used to raise dynamic weights for feature aggregation. An extra sparsity constraint is attached to the distribution of these gating weights, promoting attention to more critical and relevant features for quality representation. Then the video quality feature can be obtained by a weighted summation. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner. During inference, only the optimized VQA model is used, reducing the computational cost largely. More details will be provided below.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 17,\n",
       "     'end': 64,\n",
       "     'text': '{{figure:fa4ceb3d-fea6-4d86-9c04-31b91570ede3}}',\n",
       "     'ref_id': 'fa4ceb3d-fea6-4d86-9c04-31b91570ede3'},\n",
       "    {'start': 137,\n",
       "     'end': 185,\n",
       "     'text': '{{formula:0a2dda82-f469-4a2c-a7e4-e3eb50eb52d6}}',\n",
       "     'ref_id': '0a2dda82-f469-4a2c-a7e4-e3eb50eb52d6'},\n",
       "    {'start': 302,\n",
       "     'end': 350,\n",
       "     'text': '{{formula:70d4f318-edc8-4a87-adf4-4481112155d1}}',\n",
       "     'ref_id': '70d4f318-edc8-4a87-adf4-4481112155d1'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Videos sampled from the YouTube-UGC dataset {{cite:9be8207}} and their corresponding labeled MOS, ranging from 1.0 to 5.0. It can be seen that video quality is affected by various aspects, including content, distortions, and motion.\\n{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}}',\n",
       "   'cite_spans': [{'start': 44,\n",
       "     'end': 60,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'}],\n",
       "   'ref_spans': [{'start': 233,\n",
       "     'end': 280,\n",
       "     'text': '{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}}',\n",
       "     'ref_id': '11ec1a15-65b0-4d3a-b435-b8e6372c8c66'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Inspired by the current success of the “pretraining and fine-tuning” paradigm in deep learning {{cite:e64e98e}}, {{cite:06016b9}}, {{cite:c682601}}, we aim to utilize in-the-wild pretrained models to benefit VQA from diverse aspects of the video, in order to enhance better understanding of video quality and enable personalized treatments to improve it. We contemplate the choice of pretrained models through the lens of multiple factors, as shown in Fig.\\xa0{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}} , that may impact the quality of videos as follows:\\n',\n",
       "   'cite_spans': [{'start': 95,\n",
       "     'end': 111,\n",
       "     'text': '{{cite:e64e98e}}',\n",
       "     'ref_id': 'e64e98e'},\n",
       "    {'start': 113,\n",
       "     'end': 129,\n",
       "     'text': '{{cite:06016b9}}',\n",
       "     'ref_id': '06016b9'},\n",
       "    {'start': 131,\n",
       "     'end': 147,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'}],\n",
       "   'ref_spans': [{'start': 457,\n",
       "     'end': 504,\n",
       "     'text': '{{figure:11ec1a15-65b0-4d3a-b435-b8e6372c8c66}}',\n",
       "     'ref_id': '11ec1a15-65b0-4d3a-b435-b8e6372c8c66'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'list',\n",
       "   'text': \"\\nContent. Human judgments of visual quality are content-dependent according to previous studies {{cite:7da4179}}, {{cite:80cfe88}}. When a video is visually appealing, engaging, and relevant to the viewer's interests (e.g., cute puppy and beautiful scenery), it can capture their attention and make them more receptive to the video's content. In contrast, if a video is dull, uninteresting, or irrelevant (e.g., black screen and messy corners), viewers are more likely to rate low quality. Introducing models pretrained on the task of object recognition (e.g., EfficientNet, Swin Transformer) may benefit VQA.\\n\\nDistortion. In addition to content, distortions introduced during the phase of video capturing and compression also determine the video quality {{cite:6b6881a}}. Thus a pretrained model that has been trained on a dataset of images or videos (e.g., ImageNet, Kinetics-400) with compression artifacts will have learned to identify the specific patterns and features that are associated with compression artifacts, such as blockiness, blurriness, or pixelation.\\n\\nMotion. Unlike the image scenario, motion blur can significantly affect the quality of videos {{cite:3d9999d}}, {{cite:08d1075}}. It occurs when there is rapid motion, and the camera or objects in the scene are moving too quickly for the camera's shutter to capture. A pretrained action recognition model (e.g., SlowFast, TimeSformer) may detect specific actions or movements such as running, jumping, or throwing. These can be useful for analyzing the amount of motion or by looking for specific visual cues that are associated with motion blur, such as streaking around the edges of moving objects.\\n\\n\",\n",
       "   'cite_spans': [{'start': 96,\n",
       "     'end': 112,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 114,\n",
       "     'end': 130,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 755,\n",
       "     'end': 771,\n",
       "     'text': '{{cite:6b6881a}}',\n",
       "     'ref_id': '6b6881a'},\n",
       "    {'start': 1165,\n",
       "     'end': 1181,\n",
       "     'text': '{{cite:3d9999d}}',\n",
       "     'ref_id': '3d9999d'},\n",
       "    {'start': 1183,\n",
       "     'end': 1199,\n",
       "     'text': '{{cite:08d1075}}',\n",
       "     'ref_id': '08d1075'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Responses of different pretrained models to synthetic sequences generated by distortions (i.e., compression, sharpness). The correlation of SRCC is computed according to distortion degrees. It is evident that pretrained models may detect certain types of distortions, but their ability to perceive distortion varies across models.\\n{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 331,\n",
       "     'end': 378,\n",
       "     'text': '{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}}',\n",
       "     'ref_id': '23154bd4-20ea-41e7-9f68-e344eb273514'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'However, it is important to note that individual pretrained models may not be able to identify all types of quality-related factors, or may not be as accurate in identifying certain types. Some evidences are given in Fig.\\xa0{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}} . When encountering different types of distortions, there will be obvious differences in the perception ability of the pretrained model. In detail, ConvNext-base (SRCC=0.968) outperforms EfficientNet-b7 (SRCC=0.038) when facing compression. When it comes to sharpness, EfficientNet-b7 performs better. Therefore, it is important to use a diverse set of models and combine their results to get a more robust assessment. Thus, we propose to construct a pool with a large diversity of candidate models, considering the following aspects:\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 222,\n",
       "     'end': 269,\n",
       "     'text': '{{figure:23154bd4-20ea-41e7-9f68-e344eb273514}}',\n",
       "     'ref_id': '23154bd4-20ea-41e7-9f68-e344eb273514'}]},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'list',\n",
       "   'text': '\\nArchitecture. The efficacy of a network architecture (e.g., CNN, Transformer) hinges upon its capacity to assimilate and convey information. A well-crafted architecture can discern finer details and patterns in the input video, while also influencing the manner in which spatial and temporal information, containing quality-related features, is processed.\\n\\nPretrained pretext task. The type of supervision in the pretext task has an impact on the ability of the pretrained model to VQA tasks. When the distribution of data is similar, a supervised pretext task may lead to superior performance. Conversely, self-supervised pretext tasks, where the model is trained on unlabeled data, may facilitate better generalization when confronting unfamiliar VQA domains.\\n\\nPretrained dataset. Pretrained datasets on a large scale can be advantageous to VQA by providing diverse content, distortion, and motion-related data. A desired pretrained dataset should include a wide range of categories that closely resemble real-world scenarios, as well as other multi-modal information that can aid in describing video quality. For instance, the WebImageText {{cite:c682601}} dataset, which combines text and images, can be helpful in this regard.\\n\\n',\n",
       "   'cite_spans': [{'start': 1144,\n",
       "     'end': 1160,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Quality-related Pretrained Models',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Based on the above considerations, in this paper, we select several pretrained models that obtain top performance in their original fields, including (1) EfficientNet-b7 {{cite:d48f3ca}} trained on ImageNet-1k {{cite:bad8aed}}, (2) ir-CSN-152 {{cite:f69f321}} trained on Sports-1M {{cite:fbaab96}}, (3) CLIP trained on WebImageText {{cite:c682601}}, (4) Swin Transformer Base {{cite:942a083}} trained on ImageNet-21k {{cite:bad8aed}}, (5) TimeSformer {{cite:1cd3326}} trained on Kinetics-400 {{cite:dd352e3}}, (6) Video Swin Transformer Base {{cite:3b57340}} trained on Kinetics-600 {{cite:f2782ba}}, and (7) SlowFast {{cite:55db95f}} trained on Kinetics-400.\\n',\n",
       "   'cite_spans': [{'start': 170,\n",
       "     'end': 186,\n",
       "     'text': '{{cite:d48f3ca}}',\n",
       "     'ref_id': 'd48f3ca'},\n",
       "    {'start': 210,\n",
       "     'end': 226,\n",
       "     'text': '{{cite:bad8aed}}',\n",
       "     'ref_id': 'bad8aed'},\n",
       "    {'start': 243,\n",
       "     'end': 259,\n",
       "     'text': '{{cite:f69f321}}',\n",
       "     'ref_id': 'f69f321'},\n",
       "    {'start': 281,\n",
       "     'end': 297,\n",
       "     'text': '{{cite:fbaab96}}',\n",
       "     'ref_id': 'fbaab96'},\n",
       "    {'start': 332,\n",
       "     'end': 348,\n",
       "     'text': '{{cite:c682601}}',\n",
       "     'ref_id': 'c682601'},\n",
       "    {'start': 376,\n",
       "     'end': 392,\n",
       "     'text': '{{cite:942a083}}',\n",
       "     'ref_id': '942a083'},\n",
       "    {'start': 417,\n",
       "     'end': 433,\n",
       "     'text': '{{cite:bad8aed}}',\n",
       "     'ref_id': 'bad8aed'},\n",
       "    {'start': 451,\n",
       "     'end': 467,\n",
       "     'text': '{{cite:1cd3326}}',\n",
       "     'ref_id': '1cd3326'},\n",
       "    {'start': 492,\n",
       "     'end': 508,\n",
       "     'text': '{{cite:dd352e3}}',\n",
       "     'ref_id': 'dd352e3'},\n",
       "    {'start': 542,\n",
       "     'end': 558,\n",
       "     'text': '{{cite:3b57340}}',\n",
       "     'ref_id': '3b57340'},\n",
       "    {'start': 583,\n",
       "     'end': 599,\n",
       "     'text': '{{cite:f2782ba}}',\n",
       "     'ref_id': 'f2782ba'},\n",
       "    {'start': 618,\n",
       "     'end': 634,\n",
       "     'text': '{{cite:55db95f}}',\n",
       "     'ref_id': '55db95f'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'As the distribution of content and distortions in videos can be quite complex, a static combination of pretrained models may not always yield optimal performance. In order to adaptively capture the diversity and complementary information from different pretrained models, we propose a Quality-aware Acquisition Module (QAM). It takes extracted features from various pretrained models as input and produces a consolidated feature as output for the ultimate representation of quality. The computational process can be partitioned into two main parts. The first part is responsible for transforming the extracted features initially into a uniform feature dimension to enable subsequent aggregation. Structurally, this transformation block comprises two fully-connected layers followed by a normalization layer and a GELU activation layer. The second part generates gating weights {{formula:37337360-9a58-4f2e-bc9c-4579edb73e1e}}  to control the aggregation process. The gating network takes the concatenated feature vector as input and outputs a set of gating weights that represent the relative contribution of each pretrained model to the final quality representation. Structurally, this gating network is stacked using a fully-connected layer and a sigmoid layer. Then the quality representation {{formula:e87cca53-60ab-4242-8a5c-36ca3249b7a0}}  can be obtained by a weighted sum according to the gating weights. Given the extracted features by different pretrained models {{formula:ee60e35d-98bc-4620-aad3-5377af55c0f7}} , these procedures can be noted as:\\n{{formula:3adab10d-4dfb-4b57-a64d-b4f79084d696}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 877,\n",
       "     'end': 925,\n",
       "     'text': '{{formula:37337360-9a58-4f2e-bc9c-4579edb73e1e}}',\n",
       "     'ref_id': '37337360-9a58-4f2e-bc9c-4579edb73e1e'},\n",
       "    {'start': 1296,\n",
       "     'end': 1344,\n",
       "     'text': '{{formula:e87cca53-60ab-4242-8a5c-36ca3249b7a0}}',\n",
       "     'ref_id': 'e87cca53-60ab-4242-8a5c-36ca3249b7a0'},\n",
       "    {'start': 1473,\n",
       "     'end': 1521,\n",
       "     'text': '{{formula:ee60e35d-98bc-4620-aad3-5377af55c0f7}}',\n",
       "     'ref_id': 'ee60e35d-98bc-4620-aad3-5377af55c0f7'},\n",
       "    {'start': 1558,\n",
       "     'end': 1606,\n",
       "     'text': '{{formula:3adab10d-4dfb-4b57-a64d-b4f79084d696}}',\n",
       "     'ref_id': '3adab10d-4dfb-4b57-a64d-b4f79084d696'}]},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'where {{formula:ca5a3201-6391-490a-be08-7bb67bfadc33}}  denotes the mapping function for the {{formula:2595f6fe-5e54-4f7c-9762-3d684fbaf4f8}} -th transformation block, and {{formula:0ed6c418-804c-41b2-89d0-42b5853ea868}}  represents the mapping function for the gating network. And {{formula:248a0551-587d-4a12-8d93-a4a03cb39f54}}  is the number of aligned feature dimensions.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 6,\n",
       "     'end': 54,\n",
       "     'text': '{{formula:ca5a3201-6391-490a-be08-7bb67bfadc33}}',\n",
       "     'ref_id': 'ca5a3201-6391-490a-be08-7bb67bfadc33'},\n",
       "    {'start': 93,\n",
       "     'end': 141,\n",
       "     'text': '{{formula:2595f6fe-5e54-4f7c-9762-3d684fbaf4f8}}',\n",
       "     'ref_id': '2595f6fe-5e54-4f7c-9762-3d684fbaf4f8'},\n",
       "    {'start': 172,\n",
       "     'end': 220,\n",
       "     'text': '{{formula:0ed6c418-804c-41b2-89d0-42b5853ea868}}',\n",
       "     'ref_id': '0ed6c418-804c-41b2-89d0-42b5853ea868'},\n",
       "    {'start': 282,\n",
       "     'end': 330,\n",
       "     'text': '{{formula:248a0551-587d-4a12-8d93-a4a03cb39f54}}',\n",
       "     'ref_id': '248a0551-587d-4a12-8d93-a4a03cb39f54'}]},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In addition, to emphasize the importance of critical features and enhance the generalization ability, we propose to impose a sparsity constraint as a regularization on the distribution of gating weights. The {{formula:c6eb3ce7-3574-4a93-9668-81f00f6ef30b}}  loss is utilized to penalize non-zero weights resulting in more weights near zero. This constraint can be written as:\\n{{formula:67a151a6-206b-4fae-9e99-152c5f039648}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 208,\n",
       "     'end': 256,\n",
       "     'text': '{{formula:c6eb3ce7-3574-4a93-9668-81f00f6ef30b}}',\n",
       "     'ref_id': 'c6eb3ce7-3574-4a93-9668-81f00f6ef30b'},\n",
       "    {'start': 376,\n",
       "     'end': 424,\n",
       "     'text': '{{formula:67a151a6-206b-4fae-9e99-152c5f039648}}',\n",
       "     'ref_id': '67a151a6-206b-4fae-9e99-152c5f039648'}]},\n",
       "  {'section': 'Quality-aware Acquisition Module',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In this way, QAM allows for capturing a broader range of quality-related features, thereby enabling better adaptation to various types of video content, distortions, or movement. Then the aggregated feature is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:3f2b822c-30dd-45b4-ba0b-beb2946e3eea}} .\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 317,\n",
       "     'end': 365,\n",
       "     'text': '{{formula:3f2b822c-30dd-45b4-ba0b-beb2946e3eea}}',\n",
       "     'ref_id': '3f2b822c-30dd-45b4-ba0b-beb2946e3eea'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In practical scenarios, using these large pretrained models for inference can be computationally expensive. To reduce the computational cost and increase flexibility, we propose to use knowledge distillation {{cite:3ff8959}} to transfer the knowledge from large and complex models to a lightweight VQA model.\\n',\n",
       "   'cite_spans': [{'start': 208,\n",
       "     'end': 224,\n",
       "     'text': '{{cite:3ff8959}}',\n",
       "     'ref_id': '3ff8959'}],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'In this paper, a Video Swin Transformer-Tiny {{cite:3b57340}} is selected as the backbone. For an input video {{formula:8867c3e5-3d24-4636-8ff8-d5750ff52bd6}} , the quality representation can be achieved by {{formula:bb5f7f42-117d-46f5-be87-d4e217711613}} , where {{formula:c7ff177d-8ef7-45e2-8103-88b4eea6fb93}}  represents the mapping function of the lightweight backbone, and {{formula:7c479112-73be-4fee-8400-e83b7742006f}} . Then {{formula:779587b6-9f41-4a76-8eb3-f73e18564535}}  is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:24a53d27-11e1-49b4-954e-749af6b21b65}} . Note that both {{formula:7773c82b-4644-48fe-bb90-fe64464414c7}}  and {{formula:71835fc2-410b-434f-a922-c99ec67fed5b}}  are supervised by the labeled MOS using a smooth {{formula:fd34ed25-a357-4022-a448-0d9ab5498d99}}  loss. Additionally, we apply a similarity consistency in knowledge distillation between {{formula:7700fca0-2c77-4558-8af4-f8293fa8fa5e}}  and {{formula:85d627d9-b864-4f30-8fe0-48f9cb3eb9e9}} . This allows the VQA model to simulate the robust quality representation generated by diverse pretrained models, further enhancing its performance. Given the labeled MOS {{formula:b1c614c9-d41b-499f-a879-5ff2c29c8c1f}} , the regression loss for pretrained models can be noted as:\\n{{formula:c0f5e532-617b-4ffd-b550-cc7b40bed394}} \\n',\n",
       "   'cite_spans': [{'start': 45,\n",
       "     'end': 61,\n",
       "     'text': '{{cite:3b57340}}',\n",
       "     'ref_id': '3b57340'}],\n",
       "   'ref_spans': [{'start': 110,\n",
       "     'end': 158,\n",
       "     'text': '{{formula:8867c3e5-3d24-4636-8ff8-d5750ff52bd6}}',\n",
       "     'ref_id': '8867c3e5-3d24-4636-8ff8-d5750ff52bd6'},\n",
       "    {'start': 207,\n",
       "     'end': 255,\n",
       "     'text': '{{formula:bb5f7f42-117d-46f5-be87-d4e217711613}}',\n",
       "     'ref_id': 'bb5f7f42-117d-46f5-be87-d4e217711613'},\n",
       "    {'start': 264,\n",
       "     'end': 312,\n",
       "     'text': '{{formula:c7ff177d-8ef7-45e2-8103-88b4eea6fb93}}',\n",
       "     'ref_id': 'c7ff177d-8ef7-45e2-8103-88b4eea6fb93'},\n",
       "    {'start': 379,\n",
       "     'end': 427,\n",
       "     'text': '{{formula:7c479112-73be-4fee-8400-e83b7742006f}}',\n",
       "     'ref_id': '7c479112-73be-4fee-8400-e83b7742006f'},\n",
       "    {'start': 435,\n",
       "     'end': 483,\n",
       "     'text': '{{formula:779587b6-9f41-4a76-8eb3-f73e18564535}}',\n",
       "     'ref_id': '779587b6-9f41-4a76-8eb3-f73e18564535'},\n",
       "    {'start': 595,\n",
       "     'end': 643,\n",
       "     'text': '{{formula:24a53d27-11e1-49b4-954e-749af6b21b65}}',\n",
       "     'ref_id': '24a53d27-11e1-49b4-954e-749af6b21b65'},\n",
       "    {'start': 661,\n",
       "     'end': 709,\n",
       "     'text': '{{formula:7773c82b-4644-48fe-bb90-fe64464414c7}}',\n",
       "     'ref_id': '7773c82b-4644-48fe-bb90-fe64464414c7'},\n",
       "    {'start': 715,\n",
       "     'end': 763,\n",
       "     'text': '{{formula:71835fc2-410b-434f-a922-c99ec67fed5b}}',\n",
       "     'ref_id': '71835fc2-410b-434f-a922-c99ec67fed5b'},\n",
       "    {'start': 814,\n",
       "     'end': 862,\n",
       "     'text': '{{formula:fd34ed25-a357-4022-a448-0d9ab5498d99}}',\n",
       "     'ref_id': 'fd34ed25-a357-4022-a448-0d9ab5498d99'},\n",
       "    {'start': 952,\n",
       "     'end': 1000,\n",
       "     'text': '{{formula:7700fca0-2c77-4558-8af4-f8293fa8fa5e}}',\n",
       "     'ref_id': '7700fca0-2c77-4558-8af4-f8293fa8fa5e'},\n",
       "    {'start': 1006,\n",
       "     'end': 1054,\n",
       "     'text': '{{formula:85d627d9-b864-4f30-8fe0-48f9cb3eb9e9}}',\n",
       "     'ref_id': '85d627d9-b864-4f30-8fe0-48f9cb3eb9e9'},\n",
       "    {'start': 1226,\n",
       "     'end': 1274,\n",
       "     'text': '{{formula:b1c614c9-d41b-499f-a879-5ff2c29c8c1f}}',\n",
       "     'ref_id': 'b1c614c9-d41b-499f-a879-5ff2c29c8c1f'},\n",
       "    {'start': 1336,\n",
       "     'end': 1384,\n",
       "     'text': '{{formula:c0f5e532-617b-4ffd-b550-cc7b40bed394}}',\n",
       "     'ref_id': 'c0f5e532-617b-4ffd-b550-cc7b40bed394'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'And the regression loss for the lightweight VQA model {{formula:0c537334-c228-44a1-a7f5-d96b35f54545}}  share the same formulation. A {{formula:1e204ae6-ff4f-43e9-8682-70a86a9a8ed0}}  loss is used for the process of knowledge distillation, which can be written as:\\n{{formula:ad9e366d-6efb-40d9-91ac-fb8b2c2903f1}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 54,\n",
       "     'end': 102,\n",
       "     'text': '{{formula:0c537334-c228-44a1-a7f5-d96b35f54545}}',\n",
       "     'ref_id': '0c537334-c228-44a1-a7f5-d96b35f54545'},\n",
       "    {'start': 134,\n",
       "     'end': 182,\n",
       "     'text': '{{formula:1e204ae6-ff4f-43e9-8682-70a86a9a8ed0}}',\n",
       "     'ref_id': '1e204ae6-ff4f-43e9-8682-70a86a9a8ed0'},\n",
       "    {'start': 265,\n",
       "     'end': 313,\n",
       "     'text': '{{formula:ad9e366d-6efb-40d9-91ac-fb8b2c2903f1}}',\n",
       "     'ref_id': 'ad9e366d-6efb-40d9-91ac-fb8b2c2903f1'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'The whole optimization objective can be formulated as:\\n{{formula:fb3a642c-c877-4f46-9f28-33e9bc338a34}} \\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 55,\n",
       "     'end': 103,\n",
       "     'text': '{{formula:fb3a642c-c877-4f46-9f28-33e9bc338a34}}',\n",
       "     'ref_id': 'fb3a642c-c877-4f46-9f28-33e9bc338a34'}]},\n",
       "  {'section': 'Optimization Objective',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'where {{formula:c8adb2cf-77ae-4bfa-8dc6-03e60eb65f95}}  is a balancing weight for knowledge distillation, and {{formula:6947ab90-a396-45c6-9cf7-8bb891969bbc}}  is a hyper-parameter to balance the level of sparsity.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 6,\n",
       "     'end': 54,\n",
       "     'text': '{{formula:c8adb2cf-77ae-4bfa-8dc6-03e60eb65f95}}',\n",
       "     'ref_id': 'c8adb2cf-77ae-4bfa-8dc6-03e60eb65f95'},\n",
       "    {'start': 110,\n",
       "     'end': 158,\n",
       "     'text': '{{formula:6947ab90-a396-45c6-9cf7-8bb891969bbc}}',\n",
       "     'ref_id': '6947ab90-a396-45c6-9cf7-8bb891969bbc'}]},\n",
       "  {'section': 'Dataset.',\n",
       "   'sec_number': '1',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Our method is evaluated on three widely-adopted public NR-VQA datasets, including KoNViD-1k {{cite:24b4d04}}, LIVE-VQC {{cite:8f3eb86}}, and YouTube-UGC {{cite:9be8207}}. Mean opinion scores (MOS) are provided along with training videos.\\nSpecifically, KoNViD-1k contains 1,200 videos that are fairly filtered from a large public video dataset YFCC-100M. The time duration of the video is 8 seconds. And these videos have a frame rate of 24/25/30 FPS and a resolution of {{formula:fdead041-49aa-4d88-b0fb-641cbcf6acb7}} .\\nLIVE-VQC consists of 585 videos with complex authentic distortions, which are captured by 80 users using 101 different devices, ranging from 240P to 1080P.\\nYouTube-UGC has 1,380 UGC videos sampled from YouTube with a duration of 20 seconds and resolutions from 360P to 4K.\\nAll these datasets contain no pristine videos, thus only NR methods can be evaluated on them.\\nFollowing {{cite:c6c0872}}, we split all the dataset into 80% training videos and 20% testing videos randomly.\\n',\n",
       "   'cite_spans': [{'start': 92,\n",
       "     'end': 108,\n",
       "     'text': '{{cite:24b4d04}}',\n",
       "     'ref_id': '24b4d04'},\n",
       "    {'start': 119,\n",
       "     'end': 135,\n",
       "     'text': '{{cite:8f3eb86}}',\n",
       "     'ref_id': '8f3eb86'},\n",
       "    {'start': 153,\n",
       "     'end': 169,\n",
       "     'text': '{{cite:9be8207}}',\n",
       "     'ref_id': '9be8207'},\n",
       "    {'start': 898,\n",
       "     'end': 914,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'}],\n",
       "   'ref_spans': [{'start': 470,\n",
       "     'end': 518,\n",
       "     'text': '{{formula:fdead041-49aa-4d88-b0fb-641cbcf6acb7}}',\n",
       "     'ref_id': 'fdead041-49aa-4d88-b0fb-641cbcf6acb7'}]},\n",
       "  {'section': 'Evaluation Metric.',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Spearman’s Rank-Order Correlation Coefficient (SRCC) and Pearson’s Linear Correlation Coefficient (PLCC) are selected as metrics to measure the monotonicity and accuracy, respectively. They are in the range of 0.0 to 1.0, and larger values indicate better results. Besides, the mean average of PLCC and SRCC is also reported as a comprehensive criterion.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Implementation Details',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Our method is implemented based on PyTorch {{cite:2e556d9}} and MMAction2 {{cite:a718e63}}. All experiments are conducted on 4 NVIDIA V100 GPUs.\\nFor all datasets, we select EfficientNet-b7, ir-CSN-152, CLIP, Swin Transformer Base, TimeSformer, Video Swin Transformer Base and SlowFast as candidate pretrained models, and choose the Video Swin Transformer Tiny as the lightweight VQA model.\\nFrames are sampled in each video with a fixed temporal step to form a clip input. For frame-wise models (e.g., EfficientNet, CLIP), the feature representation can be calculated through the average features of all frames. For video clip-based models (e.g., SlowFast, ir-CSN-152), the extracted features can be used directly for the video representation.\\nFor KoNViD-1k, we sample 16 frames with a frame interval of 2.\\nAs videos in LIVE-VQC and YouTube-UGC have longer time durations, we sample 64 frames with an interval of 2, and 32 frames with an interval of 8, respectively.\\nSince most augmentations will introduce extra interference to the quality of videos (e.g., resize, color jitter) {{cite:dbe6597}}, we only choose the center crop to produce inputs with a resolution of {{formula:5e2b8317-630a-4bbc-9eaf-21292c9c7555}} .\\nDuring the optimization procedure, we use the AdamW optimizer with a weight decay of 2e-2.\\nA cosine annealing scheduler with a warmup of 2 epochs is adopted to control the learning rate. The initial learning rate is 1e-3. And {{formula:4fed9173-0d04-409f-bbc8-27f03d1fad19}}  is 0.1 by default. {{formula:3f1077df-d8c8-4672-a8ed-9a8461b6450e}}  is set to 0.8. {{formula:1d004647-ed88-4208-a7e8-057ebf756691}}  is set to 32. The batch size of the input is set to 1. All models are trained for 60 epochs. And the checkpoint generated by the last iteration is used for evaluation.\\nFor inference, we follow a similar procedure as {{cite:445c847}} by using {{formula:fb9a7adc-6b8b-4c6a-8343-9dbbe16247f6}}  views. In the procedure, a video is uniformly sampled as 4 clips in the temporal dimension, and for each clip, the shorter spatial side is scaled to 256 pixels and we take 5 crops in the four corners and the center. The final score is computed as the average score of all the views. The average result of 10 repeat runs with different random splits is used as the final score for the experiments in Tab.\\xa0{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}} .\\n',\n",
       "   'cite_spans': [{'start': 43,\n",
       "     'end': 59,\n",
       "     'text': '{{cite:2e556d9}}',\n",
       "     'ref_id': '2e556d9'},\n",
       "    {'start': 74, 'end': 90, 'text': '{{cite:a718e63}}', 'ref_id': 'a718e63'},\n",
       "    {'start': 1079,\n",
       "     'end': 1095,\n",
       "     'text': '{{cite:dbe6597}}',\n",
       "     'ref_id': 'dbe6597'},\n",
       "    {'start': 1844,\n",
       "     'end': 1860,\n",
       "     'text': '{{cite:445c847}}',\n",
       "     'ref_id': '445c847'}],\n",
       "   'ref_spans': [{'start': 1167,\n",
       "     'end': 1215,\n",
       "     'text': '{{formula:5e2b8317-630a-4bbc-9eaf-21292c9c7555}}',\n",
       "     'ref_id': '5e2b8317-630a-4bbc-9eaf-21292c9c7555'},\n",
       "    {'start': 1444,\n",
       "     'end': 1492,\n",
       "     'text': '{{formula:4fed9173-0d04-409f-bbc8-27f03d1fad19}}',\n",
       "     'ref_id': '4fed9173-0d04-409f-bbc8-27f03d1fad19'},\n",
       "    {'start': 1513,\n",
       "     'end': 1561,\n",
       "     'text': '{{formula:3f1077df-d8c8-4672-a8ed-9a8461b6450e}}',\n",
       "     'ref_id': '3f1077df-d8c8-4672-a8ed-9a8461b6450e'},\n",
       "    {'start': 1578,\n",
       "     'end': 1626,\n",
       "     'text': '{{formula:1d004647-ed88-4208-a7e8-057ebf756691}}',\n",
       "     'ref_id': '1d004647-ed88-4208-a7e8-057ebf756691'},\n",
       "    {'start': 1870,\n",
       "     'end': 1918,\n",
       "     'text': '{{formula:fb9a7adc-6b8b-4c6a-8343-9dbbe16247f6}}',\n",
       "     'ref_id': 'fb9a7adc-6b8b-4c6a-8343-9dbbe16247f6'},\n",
       "    {'start': 2324,\n",
       "     'end': 2370,\n",
       "     'text': '{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "     'ref_id': 'a8b624dc-b470-4046-8f35-b6e5534343a4'}]},\n",
       "  {'section': 'Implementation Details',\n",
       "   'sec_number': '2',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Comparisons with SOTA methods. The up arrow “{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are highlighted and underlined. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.\\n2*Method\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\n\\nSRCC{{formula:585eb95b-1547-4785-a382-4d02cb7a7274}}\\nPLCC{{formula:2f43609f-d5b9-4947-a668-77916eaab08b}}\\nMean\\nSRCC{{formula:0ab3c965-7cc8-475b-9cc8-a434f557a90f}}\\nPLCC{{formula:91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0}}\\nMean\\nSRCC{{formula:fe38b572-b993-4f84-b520-4ca75bc3ba79}}\\nPLCC{{formula:ef0f4492-775e-4bdf-b2d5-8c30a10f8946}}\\nMean\\nVIIDEO {{cite:0832a8c}}\\n0.2980\\n0.3030\\n0.3005\\n0.0332\\n0.2164\\n0.1248\\n0.0580\\n0.1534\\n0.1057\\nNIQE {{cite:c619c6d}}\\n0.5417\\n0.5530\\n0.5474\\n0.5957\\n0.6286\\n0.6122\\n0.2379\\n0.2776\\n0.2578\\nBRISQUE {{cite:18dcc02}}\\n0.654\\n0.626\\n0.640\\n0.592\\n0.638\\n0.615\\n0.382\\n0.395\\n0.389\\nVSFA {{cite:7da4179}}\\n0.755\\n0.744\\n0.750\\n-\\n-\\n-\\n-\\n-\\n-\\nTLVQM {{cite:1af68c6}}\\n0.7729\\n0.7688\\n0.7709\\n0.7988\\n0.8025\\n0.8807\\n0.6693\\n0.6590\\n0.6642\\nRIRNet {{cite:04485d0}}\\n0.7755\\n0.7812\\n0.7784\\n0.7713\\n0.7982\\n0.7848\\n-\\n-\\n-\\nMDTVSFA {{cite:1ad644b}}\\n0.7812\\n0.7856\\n0.7834\\n0.7382\\n0.7728\\n0.7555\\n-\\n-\\n-\\nRIRNet+CSPT {{cite:b519bb8}}\\n0.8008\\n0.8062\\n0.8035\\n0.7989\\n0.8194\\n0.8092\\n-\\n-\\n-\\nCoINVQ {{cite:80cfe88}}\\n0.802\\n0.816\\n0.809\\n-\\n-\\n-\\n0.764\\n0.767\\n0.766\\nRAPIQUE {{cite:f7fae73}}\\n0.8031\\n0.8175\\n0.8103\\n0.7548\\n0.7863\\n0.7706\\n0.7591\\n0.7684\\n0.7638\\nStarVQA {{cite:1c7237b}}\\n0.812\\n0.796\\n0.804\\n0.732\\n0.808\\n0.770\\n-\\n-\\n-\\nBVQA* {{cite:48735e5}}\\n0.8362\\n0.8335\\n0.8349\\n0.8412\\n0.8415\\n0.8414\\n0.8312\\n0.8194\\n0.8253\\nSTDAM {{cite:c6c0872}}\\n0.8448\\n0.8415\\n0.8432\\n0.7931\\n0.8204\\n0.8068\\n0.8341\\n0.8297\\n0.8319\\nDisCoVQA {{cite:ebacaf2}}\\n0.847\\n0.847\\n0.847\\n0.820\\n0.826\\n0.823\\n-\\n-\\n-\\nFastVQA {{cite:f3075fd}}\\n0.859\\n0.855\\n0.857\\n0.823\\n0.844\\n0.834\\n-\\n-\\n-\\nVideo Swin Tiny\\n0.8316\\n0.8694\\n0.8505\\n0.8335\\n0.8316\\n0.8326\\n0.8566\\n0.8499\\n0.8533\\nAda-DQA\\n0.8651\\n0.8831\\n0.8741\\n0.8591\\n0.8587\\n0.8589\\n0.8729\\n0.8800\\n0.8765\\n{{formula:16e73452-3bb6-49a1-9157-f959b7b592f5}}  than the previous best\\n+0.6%\\n+2.8%\\n+1.7%\\n+1.79%\\n+1.72%\\n+1.75%\\n+3.88%\\n+5.03%\\n+4.46%\\n{{formula:0818a4f1-d597-4afa-8fd6-78b317784b57}}  than w/o pretrained models\\n+3.35%\\n+1.37%\\n+2.36%\\n+2.56%\\n+2.71%\\n+2.63%\\n+1.63%\\n+3.01%\\n+2.32%\\n{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "   'cite_spans': [{'start': 918,\n",
       "     'end': 934,\n",
       "     'text': '{{cite:0832a8c}}',\n",
       "     'ref_id': '0832a8c'},\n",
       "    {'start': 1003,\n",
       "     'end': 1019,\n",
       "     'text': '{{cite:c619c6d}}',\n",
       "     'ref_id': 'c619c6d'},\n",
       "    {'start': 1091,\n",
       "     'end': 1107,\n",
       "     'text': '{{cite:18dcc02}}',\n",
       "     'ref_id': '18dcc02'},\n",
       "    {'start': 1167,\n",
       "     'end': 1183,\n",
       "     'text': '{{cite:7da4179}}',\n",
       "     'ref_id': '7da4179'},\n",
       "    {'start': 1220,\n",
       "     'end': 1236,\n",
       "     'text': '{{cite:1af68c6}}',\n",
       "     'ref_id': '1af68c6'},\n",
       "    {'start': 1307,\n",
       "     'end': 1323,\n",
       "     'text': '{{cite:04485d0}}',\n",
       "     'ref_id': '04485d0'},\n",
       "    {'start': 1380,\n",
       "     'end': 1396,\n",
       "     'text': '{{cite:1ad644b}}',\n",
       "     'ref_id': '1ad644b'},\n",
       "    {'start': 1457,\n",
       "     'end': 1473,\n",
       "     'text': '{{cite:b519bb8}}',\n",
       "     'ref_id': 'b519bb8'},\n",
       "    {'start': 1529,\n",
       "     'end': 1545,\n",
       "     'text': '{{cite:80cfe88}}',\n",
       "     'ref_id': '80cfe88'},\n",
       "    {'start': 1596,\n",
       "     'end': 1612,\n",
       "     'text': '{{cite:f7fae73}}',\n",
       "     'ref_id': 'f7fae73'},\n",
       "    {'start': 1684,\n",
       "     'end': 1700,\n",
       "     'text': '{{cite:1c7237b}}',\n",
       "     'ref_id': '1c7237b'},\n",
       "    {'start': 1749,\n",
       "     'end': 1765,\n",
       "     'text': '{{cite:48735e5}}',\n",
       "     'ref_id': '48735e5'},\n",
       "    {'start': 1835,\n",
       "     'end': 1851,\n",
       "     'text': '{{cite:c6c0872}}',\n",
       "     'ref_id': 'c6c0872'},\n",
       "    {'start': 1924,\n",
       "     'end': 1940,\n",
       "     'text': '{{cite:ebacaf2}}',\n",
       "     'ref_id': 'ebacaf2'},\n",
       "    {'start': 1991,\n",
       "     'end': 2007,\n",
       "     'text': '{{cite:f3075fd}}',\n",
       "     'ref_id': 'f3075fd'}],\n",
       "   'ref_spans': [{'start': 45,\n",
       "     'end': 93,\n",
       "     'text': '{{formula:4e18f964-3331-45fa-b22d-7351dde0b937}}',\n",
       "     'ref_id': '4e18f964-3331-45fa-b22d-7351dde0b937'},\n",
       "    {'start': 582,\n",
       "     'end': 630,\n",
       "     'text': '{{formula:585eb95b-1547-4785-a382-4d02cb7a7274}}',\n",
       "     'ref_id': '585eb95b-1547-4785-a382-4d02cb7a7274'},\n",
       "    {'start': 635,\n",
       "     'end': 683,\n",
       "     'text': '{{formula:2f43609f-d5b9-4947-a668-77916eaab08b}}',\n",
       "     'ref_id': '2f43609f-d5b9-4947-a668-77916eaab08b'},\n",
       "    {'start': 693,\n",
       "     'end': 741,\n",
       "     'text': '{{formula:0ab3c965-7cc8-475b-9cc8-a434f557a90f}}',\n",
       "     'ref_id': '0ab3c965-7cc8-475b-9cc8-a434f557a90f'},\n",
       "    {'start': 746,\n",
       "     'end': 794,\n",
       "     'text': '{{formula:91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0}}',\n",
       "     'ref_id': '91ed1fe3-12b3-46fe-b0f7-85bf3eea74f0'},\n",
       "    {'start': 804,\n",
       "     'end': 852,\n",
       "     'text': '{{formula:fe38b572-b993-4f84-b520-4ca75bc3ba79}}',\n",
       "     'ref_id': 'fe38b572-b993-4f84-b520-4ca75bc3ba79'},\n",
       "    {'start': 857,\n",
       "     'end': 905,\n",
       "     'text': '{{formula:ef0f4492-775e-4bdf-b2d5-8c30a10f8946}}',\n",
       "     'ref_id': 'ef0f4492-775e-4bdf-b2d5-8c30a10f8946'},\n",
       "    {'start': 2200,\n",
       "     'end': 2248,\n",
       "     'text': '{{formula:16e73452-3bb6-49a1-9157-f959b7b592f5}}',\n",
       "     'ref_id': '16e73452-3bb6-49a1-9157-f959b7b592f5'},\n",
       "    {'start': 2333,\n",
       "     'end': 2381,\n",
       "     'text': '{{formula:0818a4f1-d597-4afa-8fd6-78b317784b57}}',\n",
       "     'ref_id': '0818a4f1-d597-4afa-8fd6-78b317784b57'},\n",
       "    {'start': 2473,\n",
       "     'end': 2519,\n",
       "     'text': '{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "     'ref_id': 'a8b624dc-b470-4046-8f35-b6e5534343a4'}]},\n",
       "  {'section': 'Comparison with SOTA methods',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'We report the SRCC and PLCC performance with current SOTA methods on KoNViD-1k, LIVE-VQC, and YouTube-UGC.\\nAs shown in Tab.\\xa0{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}} , our method achieves new state-of-the-art results on all these datasets without using extra training data of QA.\\nSome observations can also be found through these results.\\nWe can observe those deep learning-based methods outperform the traditional hand-crafted method (e.g., VIIDEO, NIQE) largely.\\nBesides, within deep learning-based methods, VQA methods produce much better performance than IQA methods (e.g., BRISQUE). Since there exist many temporal-distributed distortions in these datasets, VQA models can capture this temporal information.\\nAda-DQA outperforms StarVQA, which builds a Transformer model to capture spatiotemporal information, in large margins.\\nBVQA incorporates extra training data from other QA datasets for the feature extractor. And our method still outperforms it without using external training data of QA (+2.89% of SRCC, and + 4.96% of PLCC in KoNViD-1k). This shows the advantage of leveraging quality-related knowledge from pretrained models.\\nCompare with the current best method of STDAM, DiscoVQA, and FastVQA, Ada-DQA improves the best performance in large margins to {{formula:b96bdb34-83cc-4e41-a47f-eded16f61bd3}}  of SRCC in KoNVid-1k (+0.6%), {{formula:4a49b392-ff35-4621-856e-a4b467efec3e}}  of SRCC in LIVE-VQC (+1.79%) and {{formula:172716b6-1c7e-4113-a4cc-f6314208c987}}  of SRCC in YouTube-UGC (+3.88%). The accuracy and consistency of prediction results have been significantly improved.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 124,\n",
       "     'end': 170,\n",
       "     'text': '{{table:a8b624dc-b470-4046-8f35-b6e5534343a4}}',\n",
       "     'ref_id': 'a8b624dc-b470-4046-8f35-b6e5534343a4'},\n",
       "    {'start': 1273,\n",
       "     'end': 1321,\n",
       "     'text': '{{formula:b96bdb34-83cc-4e41-a47f-eded16f61bd3}}',\n",
       "     'ref_id': 'b96bdb34-83cc-4e41-a47f-eded16f61bd3'},\n",
       "    {'start': 1353,\n",
       "     'end': 1401,\n",
       "     'text': '{{formula:4a49b392-ff35-4621-856e-a4b467efec3e}}',\n",
       "     'ref_id': '4a49b392-ff35-4621-856e-a4b467efec3e'},\n",
       "    {'start': 1436,\n",
       "     'end': 1484,\n",
       "     'text': '{{formula:172716b6-1c7e-4113-a4cc-f6314208c987}}',\n",
       "     'ref_id': '172716b6-1c7e-4113-a4cc-f6314208c987'}]},\n",
       "  {'section': 'Experimental Analysis and Ablation Studies',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Experimental analysis on different numbers of selected pretrained models with or without the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of with or without is bolded.\\nNumber\\nQAM\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\n3\\n{{formula:520102f0-4552-44e5-b853-ef384910ec7d}}\\n0.8517\\n0.8432\\n0.8630\\n4\\n{{formula:5c208282-8d62-42d5-8381-fcf71ca99c11}}\\n0.8613\\n0.8510\\n0.8659\\n5\\n{{formula:55c39428-0e5d-40b5-b2d8-c5511775ee50}}\\n0.8601\\n0.8490\\n0.8591\\n6\\n{{formula:4fbff708-17bc-428a-b5fd-7a2dec20e294}}\\n0.8615\\n0.8448\\n0.8589\\n7\\n{{formula:d8c752b2-c211-409d-a42a-164151334db2}}\\n0.8573\\n0.8459\\n0.8621\\n3\\n\\n0.8432\\n0.8433\\n0.8621\\n4\\n\\n0.8623\\n0.8514\\n0.8695\\n5\\n\\n0.8621\\n0.8546\\n0.8679\\n6\\n\\n0.8645\\n0.8542\\n0.8644\\n7\\n\\n0.8651\\n0.8591\\n0.8729\\n8\\n\\n0.8641\\n0.8560\\n0.8711\\n{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 265,\n",
       "     'end': 313,\n",
       "     'text': '{{formula:520102f0-4552-44e5-b853-ef384910ec7d}}',\n",
       "     'ref_id': '520102f0-4552-44e5-b853-ef384910ec7d'},\n",
       "    {'start': 337,\n",
       "     'end': 385,\n",
       "     'text': '{{formula:5c208282-8d62-42d5-8381-fcf71ca99c11}}',\n",
       "     'ref_id': '5c208282-8d62-42d5-8381-fcf71ca99c11'},\n",
       "    {'start': 409,\n",
       "     'end': 457,\n",
       "     'text': '{{formula:55c39428-0e5d-40b5-b2d8-c5511775ee50}}',\n",
       "     'ref_id': '55c39428-0e5d-40b5-b2d8-c5511775ee50'},\n",
       "    {'start': 481,\n",
       "     'end': 529,\n",
       "     'text': '{{formula:4fbff708-17bc-428a-b5fd-7a2dec20e294}}',\n",
       "     'ref_id': '4fbff708-17bc-428a-b5fd-7a2dec20e294'},\n",
       "    {'start': 553,\n",
       "     'end': 601,\n",
       "     'text': '{{formula:d8c752b2-c211-409d-a42a-164151334db2}}',\n",
       "     'ref_id': 'd8c752b2-c211-409d-a42a-164151334db2'},\n",
       "    {'start': 767,\n",
       "     'end': 813,\n",
       "     'text': '{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}}',\n",
       "     'ref_id': '8d9a3ded-086a-4687-a726-4cc648dbc022'}]},\n",
       "  {'section': 'Experimental Analysis and Ablation Studies',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.\\nPretrained Model\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\nw/o\\n0.8316\\n0.8335\\n0.8566\\nEfficientNet-b7\\n0.8412\\n0.8495\\n0.8587\\nVideo Swin Base\\n0.8390\\n0.8173\\n0.8603\\nSwin Base\\n0.8391\\n0.8475\\n0.8497\\nTimeSformer\\n0.8409\\n0.8302\\n0.8618\\nCLIP\\n0.8404\\n0.8341\\n0.8608\\nir-CNS-152\\n0.8458\\n0.8521\\n0.8518\\nSlowFast\\n0.8423\\n0.8041\\n0.8523\\n{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 471,\n",
       "     'end': 517,\n",
       "     'text': '{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}}',\n",
       "     'ref_id': 'b84d94e4-9350-4014-bc1b-c8b4434ebf24'}]},\n",
       "  {'section': 'Number of pretrained models and effectiveness of sparsity constraint in QAM',\n",
       "   'sec_number': '3',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To investigate the impact of the number of pretrained models, we performed experiments by reducing the number of models from 7 to 3. As depicted in Tab.\\xa0{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}} , increasing the number of models does not always lead to better performance without the use of sparsity constraints in QAM. With the help of sparsity constraint, the model can achieve continuous improvement as the number of pretrained models increases. However, adding more models beyond 8 (introducing an extra model of ViT Base) does not yield any further improvements. This may indicate that the quality-related information provided by the pretrained models has reached a saturation point. Therefore, we set the number of pretrained models to 7 in our experiments. In these experiments, the pretrained model is randomly selected and removed.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 153,\n",
       "     'end': 199,\n",
       "     'text': '{{table:8d9a3ded-086a-4687-a726-4cc648dbc022}}',\n",
       "     'ref_id': '8d9a3ded-086a-4687-a726-4cc648dbc022'}]},\n",
       "  {'section': 'The necessary of diverse pretrained models',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Moreover, we also conduct experiments solely utilizing a singular pretrained model to demonstrate the disparity among models. As depicted in Tab.\\xa0{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}} , a solitary pretrained model cannot consistently attain optimal outcomes across all VQA datasets. For instance, while CLIP excels on YouTube-UGC, it falls considerably short on the other two datasets. We posit that this is influenced by the correlation between pretrained models, such as pre-text task, dataset, architecture, and VQA tasks. Additionally, the results obtained by utilizing singular pretrained models are notably distant from the state-of-the-art. These findings substantiate that a solitary pretrained model is inadequate for diverse application scenarios, and leveraging a variety of pretrained models is imperative.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 146,\n",
       "     'end': 192,\n",
       "     'text': '{{table:b84d94e4-9350-4014-bc1b-c8b4434ebf24}}',\n",
       "     'ref_id': 'b84d94e4-9350-4014-bc1b-c8b4434ebf24'}]},\n",
       "  {'section': 'The necessary of diverse pretrained models',\n",
       "   'sec_number': '4',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Experiments on different distillation losses.\\ndistillation loss\\nSRCC{{formula:c4c815c4-2362-448d-a544-b0fd44f79d70}}\\nPLCC{{formula:11c7a23f-f47a-40f7-83f2-342c6590e0c2}}\\n{{formula:a29e8124-9675-4061-9d6d-977cb9100a57}}\\n0.8651\\n0.8831\\n{{formula:101515c0-83a5-489c-9c6f-dbb3358b1e5e}}\\n0.8092\\n0.8310\\nJesen-Shannon\\n0.8455\\n0.8646\\n{{table:141b825f-8848-4126-9985-77db3f3c4705}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 68,\n",
       "     'end': 116,\n",
       "     'text': '{{formula:c4c815c4-2362-448d-a544-b0fd44f79d70}}',\n",
       "     'ref_id': 'c4c815c4-2362-448d-a544-b0fd44f79d70'},\n",
       "    {'start': 121,\n",
       "     'end': 169,\n",
       "     'text': '{{formula:11c7a23f-f47a-40f7-83f2-342c6590e0c2}}',\n",
       "     'ref_id': '11c7a23f-f47a-40f7-83f2-342c6590e0c2'},\n",
       "    {'start': 170,\n",
       "     'end': 218,\n",
       "     'text': '{{formula:a29e8124-9675-4061-9d6d-977cb9100a57}}',\n",
       "     'ref_id': 'a29e8124-9675-4061-9d6d-977cb9100a57'},\n",
       "    {'start': 233,\n",
       "     'end': 281,\n",
       "     'text': '{{formula:101515c0-83a5-489c-9c6f-dbb3358b1e5e}}',\n",
       "     'ref_id': '101515c0-83a5-489c-9c6f-dbb3358b1e5e'},\n",
       "    {'start': 324,\n",
       "     'end': 370,\n",
       "     'text': '{{table:141b825f-8848-4126-9985-77db3f3c4705}}',\n",
       "     'ref_id': '141b825f-8848-4126-9985-77db3f3c4705'}]},\n",
       "  {'section': 'Different types of distillation loss',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Experiments on different knowledge distillation losses are performed in KoNViD-1k, including the {{formula:addaeb8c-543e-4208-9329-786e534ef54b}}  loss, the {{formula:fa25327b-5f45-4ab7-a127-f2ffeccd55ae}}  loss and the Jesen-Shannon (JS) loss {{cite:9306c46}}. As shown in Tab.\\xa0{{table:141b825f-8848-4126-9985-77db3f3c4705}} , the {{formula:37cec503-545d-43b4-b8f9-58076991d335}}  loss owns the best performance in transferring aggregated features by multiple teacher models.\\n',\n",
       "   'cite_spans': [{'start': 244,\n",
       "     'end': 260,\n",
       "     'text': '{{cite:9306c46}}',\n",
       "     'ref_id': '9306c46'}],\n",
       "   'ref_spans': [{'start': 97,\n",
       "     'end': 145,\n",
       "     'text': '{{formula:addaeb8c-543e-4208-9329-786e534ef54b}}',\n",
       "     'ref_id': 'addaeb8c-543e-4208-9329-786e534ef54b'},\n",
       "    {'start': 157,\n",
       "     'end': 205,\n",
       "     'text': '{{formula:fa25327b-5f45-4ab7-a127-f2ffeccd55ae}}',\n",
       "     'ref_id': 'fa25327b-5f45-4ab7-a127-f2ffeccd55ae'},\n",
       "    {'start': 279,\n",
       "     'end': 325,\n",
       "     'text': '{{table:141b825f-8848-4126-9985-77db3f3c4705}}',\n",
       "     'ref_id': '141b825f-8848-4126-9985-77db3f3c4705'},\n",
       "    {'start': 332,\n",
       "     'end': 380,\n",
       "     'text': '{{formula:37cec503-545d-43b4-b8f9-58076991d335}}',\n",
       "     'ref_id': '37cec503-545d-43b4-b8f9-58076991d335'}]},\n",
       "  {'section': 'Selection of hyper-parameters',\n",
       "   'sec_number': '6',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'We conduct experiments to show how the hyper-parameters {{formula:40079db8-ea5e-4e4e-872e-d09c7bdd43c8}}  and {{formula:b59cd732-43a5-4abc-b5fb-2b9d28fbbbc6}}  in Equ.\\xa0REF  will influence the final results in KoNViD-1k. The results are listed in Tab.\\xa0{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}  and {{table:56c4b01b-b705-42bd-a314-8bed4c21515d}} . When {{formula:026ff505-2042-4124-aaf5-6b083b5b9fd3}}  is 0.1, and {{formula:aa5777b2-7128-4723-9351-4d3a83c1855b}}  is 0.8, which are used in our experiments, the best performance can be obtained.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 56,\n",
       "     'end': 104,\n",
       "     'text': '{{formula:40079db8-ea5e-4e4e-872e-d09c7bdd43c8}}',\n",
       "     'ref_id': '40079db8-ea5e-4e4e-872e-d09c7bdd43c8'},\n",
       "    {'start': 110,\n",
       "     'end': 158,\n",
       "     'text': '{{formula:b59cd732-43a5-4abc-b5fb-2b9d28fbbbc6}}',\n",
       "     'ref_id': 'b59cd732-43a5-4abc-b5fb-2b9d28fbbbc6'},\n",
       "    {'start': 251,\n",
       "     'end': 297,\n",
       "     'text': '{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "     'ref_id': '56c4b01b-b705-42bd-a314-8bed4c21515d'},\n",
       "    {'start': 303,\n",
       "     'end': 349,\n",
       "     'text': '{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "     'ref_id': '56c4b01b-b705-42bd-a314-8bed4c21515d'},\n",
       "    {'start': 357,\n",
       "     'end': 405,\n",
       "     'text': '{{formula:026ff505-2042-4124-aaf5-6b083b5b9fd3}}',\n",
       "     'ref_id': '026ff505-2042-4124-aaf5-6b083b5b9fd3'},\n",
       "    {'start': 419,\n",
       "     'end': 467,\n",
       "     'text': '{{formula:aa5777b2-7128-4723-9351-4d3a83c1855b}}',\n",
       "     'ref_id': 'aa5777b2-7128-4723-9351-4d3a83c1855b'}]},\n",
       "  {'section': 'Selection of hyper-parameters',\n",
       "   'sec_number': '6',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Selection of the hyper-parameters of {{formula:a9eb32d2-012d-403d-a083-c08a11dcce5f}}  and {{formula:f0ad8386-af17-423d-b208-6cd0a2cb4525}} .\\n{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}\\nSRCC{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}\\nPLCC{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}\\n0.1\\n0.8651\\n0.8831\\n0.2\\n0.8462\\n0.8663\\n0.5\\n0.8406\\n0.8698\\n1.0\\n0.8320\\n0.8549\\n{{table:85a7afd7-6fb3-452c-bc87-76bfffd63be7}}{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}\\nSRCC{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}\\nPLCC{{formula:1b522aaf-e830-438c-a148-584b695753ab}}\\n0.8\\n0.8651\\n0.8831\\n0.2\\n0.8449\\n0.8671\\n0.5\\n0.8528\\n0.8716\\n1.0\\n0.8602\\n0.8780\\n{{table:34357c34-625c-4c92-b9f3-1cbd27a380a5}}\\n\\n[t]0.23\\nhyper-parameter {{formula:28690097-c6c4-4e25-93c5-fa850bcd8cdf}}[t]0.23\\nhyper-parameter {{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}} .{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 37,\n",
       "     'end': 85,\n",
       "     'text': '{{formula:a9eb32d2-012d-403d-a083-c08a11dcce5f}}',\n",
       "     'ref_id': 'a9eb32d2-012d-403d-a083-c08a11dcce5f'},\n",
       "    {'start': 91,\n",
       "     'end': 139,\n",
       "     'text': '{{formula:f0ad8386-af17-423d-b208-6cd0a2cb4525}}',\n",
       "     'ref_id': 'f0ad8386-af17-423d-b208-6cd0a2cb4525'},\n",
       "    {'start': 142,\n",
       "     'end': 190,\n",
       "     'text': '{{formula:6aabc355-4353-495e-8754-ba127a2ca29a}}',\n",
       "     'ref_id': '6aabc355-4353-495e-8754-ba127a2ca29a'},\n",
       "    {'start': 195,\n",
       "     'end': 243,\n",
       "     'text': '{{formula:6d8e62cd-c09b-448d-97de-3ce5403c6544}}',\n",
       "     'ref_id': '6d8e62cd-c09b-448d-97de-3ce5403c6544'},\n",
       "    {'start': 248,\n",
       "     'end': 296,\n",
       "     'text': '{{formula:8616041b-c2cc-4e7a-9dee-a47db9687eb5}}',\n",
       "     'ref_id': '8616041b-c2cc-4e7a-9dee-a47db9687eb5'},\n",
       "    {'start': 369,\n",
       "     'end': 415,\n",
       "     'text': '{{table:85a7afd7-6fb3-452c-bc87-76bfffd63be7}}',\n",
       "     'ref_id': '85a7afd7-6fb3-452c-bc87-76bfffd63be7'},\n",
       "    {'start': 415,\n",
       "     'end': 463,\n",
       "     'text': '{{formula:66f56494-555a-41eb-94ca-a691e8a05223}}',\n",
       "     'ref_id': '66f56494-555a-41eb-94ca-a691e8a05223'},\n",
       "    {'start': 468,\n",
       "     'end': 516,\n",
       "     'text': '{{formula:37c6ba32-0e69-4ace-b358-e5868c67c743}}',\n",
       "     'ref_id': '37c6ba32-0e69-4ace-b358-e5868c67c743'},\n",
       "    {'start': 521,\n",
       "     'end': 569,\n",
       "     'text': '{{formula:1b522aaf-e830-438c-a148-584b695753ab}}',\n",
       "     'ref_id': '1b522aaf-e830-438c-a148-584b695753ab'},\n",
       "    {'start': 642,\n",
       "     'end': 688,\n",
       "     'text': '{{table:34357c34-625c-4c92-b9f3-1cbd27a380a5}}',\n",
       "     'ref_id': '34357c34-625c-4c92-b9f3-1cbd27a380a5'},\n",
       "    {'start': 714,\n",
       "     'end': 762,\n",
       "     'text': '{{formula:28690097-c6c4-4e25-93c5-fa850bcd8cdf}}',\n",
       "     'ref_id': '28690097-c6c4-4e25-93c5-fa850bcd8cdf'},\n",
       "    {'start': 786,\n",
       "     'end': 834,\n",
       "     'text': '{{formula:1b960ffa-8317-4b49-9ab6-0973f5604b07}}',\n",
       "     'ref_id': '1b960ffa-8317-4b49-9ab6-0973f5604b07'},\n",
       "    {'start': 836,\n",
       "     'end': 882,\n",
       "     'text': '{{table:56c4b01b-b705-42bd-a314-8bed4c21515d}}',\n",
       "     'ref_id': '56c4b01b-b705-42bd-a314-8bed4c21515d'}]},\n",
       "  {'section': 'Selection of hyper-parameters',\n",
       "   'sec_number': '6',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'table',\n",
       "   'text': 'Contributions of different pretrained models by {{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}} .\\nModel\\n{{formula:cb8e8db9-0b6f-4095-8935-43c745009f52}}  for LQ videos\\n{{formula:5e218773-7abe-4ee6-836b-4e4e6998017a}}  for HQ videos\\nEfficientNet-b7\\n0.1303\\n0.7208\\nVideo Swin Base\\n0.2021\\n0.0455\\nSwin Base\\n0.1676\\n0.6805\\nTimeSformer\\n0.4679\\n0.2317\\nCLIP\\n0.1279\\n0.3567\\nir-CNS-152\\n0.8690\\n0.2788\\nSlowFast\\n0.4210\\n0.0018\\n{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 48,\n",
       "     'end': 96,\n",
       "     'text': '{{formula:56ce4a4e-539b-45af-9a31-8eacdf6234a6}}',\n",
       "     'ref_id': '56ce4a4e-539b-45af-9a31-8eacdf6234a6'},\n",
       "    {'start': 105,\n",
       "     'end': 153,\n",
       "     'text': '{{formula:cb8e8db9-0b6f-4095-8935-43c745009f52}}',\n",
       "     'ref_id': 'cb8e8db9-0b6f-4095-8935-43c745009f52'},\n",
       "    {'start': 169,\n",
       "     'end': 217,\n",
       "     'text': '{{formula:5e218773-7abe-4ee6-836b-4e4e6998017a}}',\n",
       "     'ref_id': '5e218773-7abe-4ee6-836b-4e4e6998017a'},\n",
       "    {'start': 410,\n",
       "     'end': 456,\n",
       "     'text': '{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}}',\n",
       "     'ref_id': '4588288c-950a-42b5-9e99-a9926c75ff3a'}]},\n",
       "  {'section': 'Contribution of different pretrained models',\n",
       "   'sec_number': '7',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To investigate the contribution, we analyze the gating weights {{formula:45adcc6d-d8f1-43bf-97ef-41f131219f5d}}  generated by the QAM in KoNViD-1k. The statistical average scores for different models are calculated.\\nWe count responses of low-quality (LQ, MOS<3.5) and high-quality (HQ, MOS>3.5) videos in Tab.\\xa0{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}} . It can be seen that for LQ videos, models that can provide distortion and motion-related information (e.g., ir-CSN-152) have larger weights; for HQ videos, models that can provide content-related one (e.g., EfficientNet-b7) own larger weights.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 63,\n",
       "     'end': 111,\n",
       "     'text': '{{formula:45adcc6d-d8f1-43bf-97ef-41f131219f5d}}',\n",
       "     'ref_id': '45adcc6d-d8f1-43bf-97ef-41f131219f5d'},\n",
       "    {'start': 310,\n",
       "     'end': 356,\n",
       "     'text': '{{table:4588288c-950a-42b5-9e99-a9926c75ff3a}}',\n",
       "     'ref_id': '4588288c-950a-42b5-9e99-a9926c75ff3a'}]},\n",
       "  {'section': 'Computational cost',\n",
       "   'sec_number': '8',\n",
       "   'sec_type': '',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'We compare the #Params, #FLOPs, and SRCC of Ada-DQA over some SOTA methods whose models are available: Ada-DQA (29M, 88T, 0.8651), MDTVSFA (24M, 168T, 0.7812), StarVQA (121M, 75T, 0.812), BVQA (24M, 240T, 0.8362). With the help of pretrained models during training, Ada-DQA obtains higher results with a fair cost during inference.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Visualization of the Attention',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'Some representative videos in KoNViD-1k are selected to show the performance improvement brought by Ada-DQA. Visualization of the feature attention maps using GradCAM {{cite:ecf1e0d}} are shown in Fig.\\xa0{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}} . After introducing the adaptive acquisition strategy, Ada-DQA generates more accurate results and the attention maps highlight more quality-related regions.\\nFor instance, in the first video clip, attention from the vast surface of the ocean (left) is transferred to the sailboat, with some attention on ocean waves kept (right). Ada-DQA focuses on areas more related to the action (boat sailing) or giving clues about the perceptual quality (edges of waves).\\n',\n",
       "   'cite_spans': [{'start': 167,\n",
       "     'end': 183,\n",
       "     'text': '{{cite:ecf1e0d}}',\n",
       "     'ref_id': 'ecf1e0d'}],\n",
       "   'ref_spans': [{'start': 202,\n",
       "     'end': 249,\n",
       "     'text': '{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}}',\n",
       "     'ref_id': 'f37ff774-dbb6-49f8-afdf-a7ec9619c959'}]},\n",
       "  {'section': 'Visualization of the Attention',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': 'subsection',\n",
       "   'content_type': 'figure',\n",
       "   'text': 'Comparison of predictions and attention visualizations. For each video, video frames (left) and attention maps (right) redbefore and greenafter using Ada-DQA are illustrated.\\n{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}}',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 175,\n",
       "     'end': 222,\n",
       "     'text': '{{figure:f37ff774-dbb6-49f8-afdf-a7ec9619c959}}',\n",
       "     'ref_id': 'f37ff774-dbb6-49f8-afdf-a7ec9619c959'}]},\n",
       "  {'section': 'Conclusion',\n",
       "   'sec_number': '5',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'To address the issue of insufficient training data in VQA, this paper analyzes the entire spectrum of video distribution diversity that impacts quality and proposes the Ada-DQA framework, which employs a range of diverse pretrained models to improve quality representation. With Ada-DQA, it becomes possible to extract critical and relevant features generated by different frozen pretrained models adaptively. Experimental results on three mainstream NR-VQA benchmarks show the effectiveness in the context of limited data. Thorough analysis and ablation studies also validate the necessity of each component. This work hopes to inspire future research that leverages pretrained models to aid in a wider array of tasks.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Acknowledgments',\n",
       "   'sec_number': '-1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'paragraph',\n",
       "   'text': 'This research was partly supported by the National Key R&D Program of China (Grant No. 2020AAA0108303), and Shenzhen Science and Technology Project (Grant No. JCYJ20200109143041798) and Shenzhen Stable Supporting Program (No. WDZC20200820200655001) and Shenzhen Key Laboratory of next-generation interactive media innovative technology (No. ZDSYS20210623092001004).\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []},\n",
       "  {'section': 'Acknowledgments',\n",
       "   'sec_number': '-1',\n",
       "   'sec_type': 'section',\n",
       "   'content_type': 'Bibliography',\n",
       "   'text': 'Anurag Arnab, Mostafa\\nDehghani, Georg Heigold, Chen Sun,\\nMario Lucic, and Cordelia Schmid.\\n2021.\\nViViT: A Video Vision Transformer.\\nCoRR abs/2103.15691\\n(2021).\\nGedas Bertasius, Heng\\nWang, and Lorenzo Torresani.\\n2021.\\nIs Space-Time Attention All You Need for Video\\nUnderstanding?. In ICML,\\nVol.\\xa0139. PMLR,\\n813–824.\\nJoão Carreira, Eric\\nNoland, Andras Banki-Horvath, Chloe\\nHillier, and Andrew Zisserman.\\n2018.\\nA Short Note about Kinetics-600.\\nCoRR abs/1808.01340\\n(2018).\\nAaron Chadha and Yiannis\\nAndreopoulos. 2021.\\nDeep Perceptual Preprocessing for Video Coding. In\\nCVPR. IEEE,\\n14852–14861.\\nKuan-Ta Chen, Chi-Jui\\nChang, Chen-Chi Wu, Yu-Chun Chang,\\nand Chin-Laung Lei. 2010.\\nQuadrant of euphoria: a crowdsourcing platform for\\nQoE assessment.\\nIEEE Netw. 24,\\n2 (2010), 28–35.\\nhttps://doi.org/10.1109/MNET.2010.5430141\\nPengfei Chen, Leida Li,\\nLei Ma, Jinjian Wu, and\\nGuangming Shi. 2020b.\\nRIRNet: Recurrent-In-Recurrent Network for Video\\nQuality Assessment. In ACM MM.\\nACM, 834–842.\\nPengfei Chen, Leida Li,\\nJinjian Wu, Weisheng Dong, and\\nGuangming Shi. 2022.\\nContrastive Self-Supervised Pre-Training for Video\\nQuality Assessment.\\nIEEE Trans. Image Process.\\n31 (2022), 458–471.\\nTing Chen, Simon\\nKornblith, Mohammad Norouzi, and\\nGeoffrey\\xa0E. Hinton. 2020a.\\nA Simple Framework for Contrastive Learning of\\nVisual Representations. In ICML,\\nVol.\\xa0119. PMLR,\\n1597–1607.\\nYanjiao Chen, Kaishun Wu,\\nand Qian Zhang. 2015.\\nFrom QoS to QoE: A Tutorial on Video Quality\\nAssessment.\\nIEEE Commun. Surv. Tutorials\\n17, 2 (2015),\\n1126–1165.\\nhttps://doi.org/10.1109/COMST.2014.2363139\\nShyamprasad Chikkerur,\\nVijay Sundaram, Martin Reisslein, and\\nLina\\xa0J. Karam. 2011.\\nObjective Video Quality Assessment Methods: A\\nClassification, Review, and Performance Comparison.\\nIEEE Trans. Broadcast.\\n57, 2 (2011),\\n165–182.\\nCisco. 2021.\\nCisco annual internet report white paper.\\nhttps://www.cisco.com/c/en/us/solutions/collateral/\\nexecutive-perspectives/annual-internet-report/white-paper-c11-741490.html.\\nMMAction2 Contributors.\\n2020.\\nOpenMMLab\\'s Next Generation Video Understanding\\nToolbox and Benchmark.\\nhttps://github.com/open-mmlab/mmaction2.\\nJia Deng, Wei Dong,\\nRichard Socher, Li-Jia Li,\\nKai Li, and Li Fei-Fei.\\n2009.\\nImageNet: A large-scale hierarchical image\\ndatabase. In CVPR. IEEE\\nComputer Society, 248–255.\\nJoshua\\xa0Peter Ebenezer,\\nZaixi Shang, Yongjun Wu,\\nHai Wei, and Alan\\xa0C. Bovik.\\n2020.\\nNo-Reference Video Quality Assessment Using\\nSpace-Time Chips. In MMSP.\\nIEEE, 1–6.\\nChristoph Feichtenhofer,\\nHaoqi Fan, Jitendra Malik, and\\nKaiming He. 2019.\\nSlowFast Networks for Video Recognition. In\\nICCV. 6201–6210.\\nBent Fuglede and\\nFlemming Topsøe. 2004.\\nJensen-Shannon divergence and Hilbert space\\nembedding. In ISIT.\\nIEEE, 31.\\nFranz Götz-Hahn,\\nVlad Hosu, Hanhe Lin, and\\nDietmar Saupe. 2021.\\nKonVid-150k: A Dataset for No-Reference Video\\nQuality Assessment of Videos in-the-Wild.\\nIEEE Access 9\\n(2021), 72139–72160.\\nhttps://doi.org/10.1109/ACCESS.2021.3077642\\nJie Gu, Gaofeng Meng,\\nCheng Da, Shiming Xiang, and\\nChunhong Pan. 2019a.\\nNo-Reference Image Quality Assessment with\\nReinforcement Recursive List-Wise Ranking. In\\nAAAI. AAAI Press,\\n8336–8343.\\nJie Gu, Gaofeng Meng,\\nShiming Xiang, and Chunhong Pan.\\n2019b.\\nBlind image quality assessment via learnable\\nattention-based pooling.\\nPattern Recognit. 91\\n(2019), 332–344.\\nKaiming He, Haoqi Fan,\\nYuxin Wu, Saining Xie, and\\nRoss\\xa0B. Girshick. 2020.\\nMomentum Contrast for Unsupervised Visual\\nRepresentation Learning. In CVPR.\\nComputer Vision Foundation / IEEE,\\n9726–9735.\\nGeoffrey\\xa0E. Hinton, Oriol\\nVinyals, and Jeffrey Dean.\\n2015.\\nDistilling the Knowledge in a Neural Network.\\nCoRR abs/1503.02531\\n(2015).\\nTobias Hoßfeld,\\nChristian Keimel, Matthias Hirth,\\nBruno Gardlo, Julian Habigt,\\nKlaus Diepold, and Phuoc Tran-Gia.\\n2014.\\nBest Practices for QoE Crowdtesting: QoE Assessment\\nWith Crowdsourcing.\\nIEEE Trans. Multim. 16,\\n2 (2014), 541–558.\\nhttps://doi.org/10.1109/TMM.2013.2291663\\nVlad Hosu, Franz Hahn,\\nMohsen Jenadeleh, Hanhe Lin,\\nHui Men, Tamás Szirányi,\\nShujun Li, and Dietmar Saupe.\\n2017.\\nThe Konstanz natural video database (KoNViD-1k).\\nIn QoMEX. IEEE,\\n1–6.\\nRui Hou, YunHao Zhao,\\nYang Hu, and Huan Liu.\\n2020.\\nNo-reference video quality evaluation by a deep\\ntransfer CNN architecture.\\nSPIC 83\\n(2020), 115782.\\nAndrej Karpathy, George\\nToderici, Sanketh Shetty, Thomas Leung,\\nRahul Sukthankar, and Li Fei-Fei.\\n2014.\\nLarge-scale Video Classification with Convolutional\\nNeural Networks. In CVPR.\\nWill Kay, João\\nCarreira, Karen Simonyan, Brian Zhang,\\nChloe Hillier, Sudheendra\\nVijayanarasimhan, Fabio Viola, Tim\\nGreen, Trevor Back, Paul Natsev,\\nMustafa Suleyman, and Andrew\\nZisserman. 2017.\\nThe Kinetics Human Action Video Dataset.\\nCoRR abs/1705.06950\\n(2017).\\nJunjie Ke, Qifei Wang,\\nYilin Wang, Peyman Milanfar, and\\nFeng Yang. 2021.\\nMUSIQ: Multi-scale Image Quality Transformer.\\n(October 2021),\\n5148–5157.\\nJari Korhonen.\\n2019.\\nTwo-Level Approach for No-Reference Consumer Video\\nQuality Assessment.\\nIEEE Trans. Image Process.\\n28, 12 (2019),\\n5923–5938.\\nJari Korhonen, Yicheng\\nSu, and Junyong You. 2020.\\nBlind Natural Video Quality Prediction via\\nStatistical Temporal Features and Deep Spatial Features. In\\nMM \\'20: The 28th ACM International Conference\\non Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020,\\nChang\\xa0Wen Chen, Rita\\nCucchiara, Xian-Sheng Hua, Guo-Jun\\nQi, Elisa Ricci, Zhengyou Zhang, and\\nRoger Zimmermann (Eds.). ACM,\\n3311–3319.\\nhttps://doi.org/10.1145/3394171.3413845\\nBowen Li, Weixia Zhang,\\nMeng Tian, Guangtao Zhai, and\\nXianpei Wang. 2021b.\\nBlindly Assess Quality of In-the-Wild Videos via\\nQuality-aware Pre-training and Motion Perception.\\nCoRR abs/2108.08505\\n(2021).\\nDingquan Li, Tingting\\nJiang, and Ming Jiang. 2019.\\nQuality Assessment of In-the-Wild Videos. In\\nACM Multimedia. ACM,\\n2351–2359.\\nDingquan Li, Tingting\\nJiang, and Ming Jiang.\\n2021a.\\nUnified Quality Assessment of in-the-Wild Videos\\nwith Mixed Datasets Training.\\nIJCV 129,\\n4 (2021).\\nLiang Liao, Kangmin Xu,\\nHaoning Wu, Chaofeng Chen,\\nWenxiu Sun, Qiong Yan, and\\nWeisi Lin. 2022.\\nExploring the Effectiveness of Video Perceptual\\nRepresentation in Blind Video Quality Assessment. In\\nMM \\'22: The 30th ACM International Conference\\non Multimedia, Lisboa, Portugal, October 10 - 14, 2022,\\nJoão Magalhães,\\nAlberto\\xa0Del Bimbo, Shin\\'ichi Satoh,\\nNicu Sebe, Xavier Alameda-Pineda,\\nQin Jin, Vincent Oria, and\\nLaura Toni (Eds.). ACM,\\n837–846.\\nhttps://doi.org/10.1145/3503161.3547849\\nZe Liu, Yutong Lin,\\nYue Cao, Han Hu, Yixuan\\nWei, Zheng Zhang, Stephen Lin, and\\nBaining Guo. 2021a.\\nSwin Transformer: Hierarchical Vision Transformer\\nUsing Shifted Windows. In ICCV.\\n10012–10022.\\nZe Liu, Jia Ning,\\nYue Cao, Yixuan Wei,\\nZheng Zhang, Stephen Lin, and\\nHan Hu. 2021b.\\nVideo Swin Transformer.\\nCoRR abs/2106.13230\\n(2021).\\nAnish Mittal,\\nAnush\\xa0Krishna Moorthy, and Alan\\xa0Conrad\\nBovik. 2012.\\nNo-Reference Image Quality Assessment in the\\nSpatial Domain.\\nIEEE Trans. Image Process.\\n21, 12 (2012),\\n4695–4708.\\nAnish Mittal, Michele\\xa0A.\\nSaad, and Alan\\xa0C. Bovik.\\n2016.\\nA Completely Blind Video Integrity Oracle.\\nIEEE Trans. Image Process.\\n25, 1 (2016),\\n289–300.\\nAnish Mittal, Rajiv\\nSoundararajan, and Alan\\xa0C. Bovik.\\n2013.\\nMaking a \"Completely Blind\" Image Quality\\nAnalyzer.\\nIEEE SPL 20,\\n3 (2013), 209–212.\\nAdam Paszke, Sam Gross,\\nFrancisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin,\\nNatalia Gimelshein, Luca Antiga,\\nAlban Desmaison, Andreas Köpf,\\nEdward\\xa0Z. Yang, Zachary DeVito,\\nMartin Raison, Alykhan Tejani,\\nSasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and\\nSoumith Chintala. 2019.\\nPyTorch: An Imperative Style, High-Performance Deep\\nLearning Library. In NeurIPS.\\n8024–8035.\\nAlec Radford, Jong\\xa0Wook\\nKim, Chris Hallacy, Aditya Ramesh,\\nGabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell,\\nPamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever.\\n2021.\\nLearning Transferable Visual Models From Natural\\nLanguage Supervision. In ICML.\\nPMLR, 8748–8763.\\nMichele\\xa0A. Saad, Alan\\xa0C.\\nBovik, and Christophe Charrier.\\n2012.\\nBlind Image Quality Assessment: A Natural Scene\\nStatistics Approach in the DCT Domain.\\nIEEE Trans. Image Process.\\n21, 8 (2012),\\n3339–3352.\\nhttps://doi.org/10.1109/TIP.2012.2191563\\nMichele\\xa0A. Saad, Alan\\xa0C.\\nBovik, and Christophe Charrier.\\n2014.\\nBlind Prediction of Natural Video Quality.\\nIEEE Trans. Image Process.\\n23, 3 (2014),\\n1352–1365.\\nRamprasaath\\xa0R. Selvaraju,\\nMichael Cogswell, Abhishek Das,\\nRamakrishna Vedantam, Devi Parikh, and\\nDhruv Batra. 2017.\\nGrad-CAM: Visual Explanations from Deep Networks\\nvia Gradient-Based Localization. In ICCV.\\nIEEE Computer Society, 618–626.\\nMuhammad Shahid, Jacob\\nSøgaard, Jeevan Pokhrel, Kjell\\nBrunnström, Kun Wang, Samira\\nTavakoli, and Narciso García.\\n2014.\\nCrowdsourcing based subjective quality assessment\\nof adaptive video streaming. In Sixth\\nInternational Workshop on Quality of Multimedia Experience, QoMEX 2014,\\nSingapore, September 18-20, 2014. IEEE,\\n53–54.\\nhttps://doi.org/10.1109/QoMEX.2014.6982289\\nZeina Sinno and\\nAlan\\xa0Conrad Bovik. 2019.\\nLarge-Scale Study of Perceptual Video Quality.\\nIEEE Trans. Image Process.\\n28, 2 (2019),\\n612–627.\\nMingxing Tan and Quoc\\xa0V.\\nLe. 2019.\\nEfficientNet: Rethinking Model Scaling for\\nConvolutional Neural Networks. In ICML\\n(Proceedings of Machine Learning Research,\\nVol.\\xa097). PMLR,\\n6105–6114.\\nDu Tran, Heng Wang,\\nMatt Feiszli, and Lorenzo Torresani.\\n2019.\\nVideo Classification With Channel-Separated\\nConvolutional Networks. In ICCV.\\nIEEE, 5551–5560.\\nZhengzhong Tu, Chia-Ju\\nChen, Yilin Wang, Neil Birkbeck,\\nBalu Adsumilli, and Alan\\xa0C. Bovik.\\n2021a.\\nEfficient User-Generated Video Quality Prediction.\\nIn Picture Coding Symposium, PCS 2021, Bristol,\\nUnited Kingdom, June 29 - July 2, 2021. IEEE,\\n1–5.\\nhttps://doi.org/10.1109/PCS50896.2021.9477483\\nZhengzhong Tu, Yilin\\nWang, Neil Birkbeck, Balu Adsumilli,\\nand Alan\\xa0C. Bovik. 2021b.\\nUGC-VQA: Benchmarking Blind Video Quality\\nAssessment for User Generated Content.\\nIEEE TIP 30\\n(2021), 4449–4464.\\nZhengzhong Tu, Yilin\\nWang, Neil Birkbeck, Balu Adsumilli,\\nand Alan\\xa0C. Bovik. 2021c.\\nUGC-VQA: Benchmarking Blind Video Quality\\nAssessment for User Generated Content.\\nIEEE TIP 30\\n(2021), 4449–4464.\\nZhengzhong Tu, Xiangxu\\nYu, Yilin Wang, Neil Birkbeck,\\nBalu Adsumilli, and Alan\\xa0C. Bovik.\\n2021d.\\nRAPIQUE: Rapid and Accurate Video Quality\\nPrediction of User Generated Content.\\nCoRR abs/2101.10955\\n(2021).\\nDomonkos Varga and\\nTamás Szirányi.\\n2019.\\nNo-reference video quality assessment via\\npretrained CNN and LSTM networks.\\nSignal Image Video Process.\\n13, 8 (2019),\\n1569–1576.\\nYilin Wang, Sasi Inguva,\\nand Balu Adsumilli. 2019.\\nYouTube UGC Dataset for Video Compression\\nResearch. In MMSP.\\nIEEE, 1–5.\\nYilin Wang, Junjie Ke,\\nHossein Talebi, Joong\\xa0Gon Yim,\\nNeil Birkbeck, Balu Adsumilli,\\nPeyman Milanfar, and Feng Yang.\\n2021.\\nRich Features for Perceptual Quality Assessment of\\nUGC Videos. In CVPR.\\n13435–13444.\\nZhou Wang, Ligang Lu,\\nand Alan\\xa0C. Bovik. 2004.\\nVideo quality assessment based on structural\\ndistortion measurement.\\nSignal Process. Image Commun.\\n19, 2 (2004),\\n121–132.\\nHaoning Wu, Chaofeng\\nChen, Jingwen Hou, Liang Liao,\\nAnnan Wang, Wenxiu Sun,\\nQiong Yan, and Weisi Lin.\\n2022a.\\nFAST-VQA: Efficient End-to-End Video Quality\\nAssessment with Fragment Sampling. In ECCV\\n(6), Vol.\\xa013666. 538–554.\\nHaoning Wu, Chaofeng\\nChen, Liang Liao, Jingwen Hou,\\nQiong Yan, and Weisi Lin.\\n2022b.\\nDisCoVQA: Temporal Distortion-Content Transformers\\nfor Video Quality Assessment.\\nCoRR abs/2206.09853\\n(2022).\\nFengchuang Xing,\\nYuan-Gen Wang, Hanpin Wang,\\nLeida Li, and Guopu Zhu.\\n2021.\\nStarVQA: Space-Time Attention for Video Quality\\nAssessment.\\nCoRR abs/2108.09635\\n(2021).\\nJiahua Xu, Jing Li,\\nXingguang Zhou, Wei Zhou,\\nBaichao Wang, and Zhibo Chen.\\n2021.\\nPerceptual Quality Assessment of Internet Videos.\\nIn ACM Multimedia. ACM,\\n1248–1257.\\nJia Yan, Weixia Zhang,\\nand Tianpeng Feng. 2016.\\nBlind Image Quality Assessment Based on Natural\\nRedundancy Statistics. In Computer Vision - ACCV\\n2016 - 13th Asian Conference on Computer Vision, Taipei, Taiwan, November\\n20-24, 2016, Revised Selected Papers, Part IV\\n(Lecture Notes in Computer Science,\\nVol.\\xa010114),\\nShang-Hong Lai,\\nVincent Lepetit, Ko\\xa0Nishino, and\\nYoichi Sato (Eds.). Springer,\\n3–18.\\nhttps://doi.org/10.1007/978-3-319-54190-7_1\\nZhenqiang Ying, Maniratnam\\nMandal, Deepti Ghadiyaram, and Alan\\nBovik. 2021.\\nPatch-VQ: \\'Patching Up\\' the Video Quality Problem.\\nIn CVPR. Computer Vision\\nFoundation / IEEE, 14019–14029.\\nJunyong You.\\n2021.\\nLong Short-term Convolutional Transformer for\\nNo-Reference Video Quality Assessment. In ACM\\nMultimedia. ACM, 2112–2120.\\nJunyong You and Jari\\nKorhonen. 2019.\\nDeep Neural Networks for No-Reference Video Quality\\nAssessment. In 2019 IEEE International\\nConference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25,\\n2019. IEEE, 2349–2353.\\nhttps://doi.org/10.1109/ICIP.2019.8803395\\nKai Zhao, Kun Yuan,\\nMing Sun, Mading Li, and\\nXing Wen. 2023b.\\nQuality-Aware Pre-Trained Models for Blind Image\\nQuality Assessment. In CVPR.\\nIEEE Computer Society, 22302–22313.\\nKai Zhao, Kun Yuan,\\nMing Sun, and Xing Wen.\\n2023a.\\nZoom-VQA: Patches, Frames and Clips Integration for\\nVideo Quality Assessment. In CVPR Workshops.\\nIEEE Computer Society, 1302–1310.\\n',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': []}],\n",
       " 'title': 'Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_texts_2308_high_quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "18c3db33-6f58-46cb-a86d-8ca56a14ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/arxiv_tables_2308_high_quality/papers_expanded.jsonl\", \"w\") as f:\n",
    "#     for line in papers_2308_high_quality:\n",
    "#         f.write(json.dumps(line) + \"\\n\")\n",
    "    # papers_2308_high_quality = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bee47-545b-4edb-bad3-79f69ee967d8",
   "metadata": {},
   "source": [
    "The next step is actually to get the context for the table from the full-text. There are a few pieces of this:\n",
    "1. The caption: this is easiest because it's included in the info extracted from the table\n",
    "2. The table mention: this is next easiest because we can just see where the table is referenced\n",
    "3. The glossary: this is hard because it involves some kind of paperqa. It is also the most important which is kind of unfortunate... I can use my gpt pipeline, but because we need to decontextualize *every* table entry, that's gonna be kinda expensive. What are our options?\n",
    "    1. Use mistral? (good to try, but I would need to move over some data to s2 cluster, etc.)\n",
    "    2. DSPY? not enough in-context examples probably\n",
    "    3. Exact string match? eh quite brittle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "9a60418b-c3f3-4c2e-a76a-4adc32379e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tabids': ['b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f'],\n",
       " 'corpus_id': 9136312,\n",
       " 'title': 'The Konstanz natural video database (KoNViD-1k)',\n",
       " 'paper_id': 'ae95abd2406ddfab1aa2ffa2413d68b98b1ba21b',\n",
       " 'abstract': 'Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be ‘general purpose’ requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at ‘in the wild’ authentic distortions, depicting a wide variety of content.',\n",
       " 'intro': 'Most of the Internet traffic today stems from user-generated videos on sharing web-sites and social networks. Video sequences pass through several stages of processing before they reach consumers, which often deteriorate visual quality. Moreover, the vast amount of user-generated video content and the increased diversity of end user devices (ranging from smaller and power-constrained mobile devices to large displays such as 4K Ultra HDTVs and TV walls) calls for a broad range of video quality to be supported. Adapting video quality to different use cases has become an important topic for researchers, content providers and distributors [1].\\n\\nAutomatic and accurate prediction of video quality is a basic operation for many video processing applications such as video quality monitoring in transmission protocols, video quality filtering in sharing services, automatic and recommended camera parameter settings during video capturing, and video enhancement. Specifically, no-reference methods attempt to judge the quality of a video sequence without any additional information about the original recorded scene. Such blind methods may apply machine learning techniques to learn from large amounts of annotated data. However, current video quality assessment (VQA) databases contain only a small number of video sequences with little content diversity, thus offering limited support for designing and evaluating noreference VQA methods effectively and fairly.\\n\\nAdditionally, these databases were mostly designed to include only artificially distorted video sequences to simulate quality loss in compression, transmission, and other parts of the video processing and distribution pipeline. Some databases capture imagery with a variety of cameras to encompass authentic video acquisition distortions, however, with content restricted to a small number of physical scenes.\\n\\nWinkler [2] proposed several criteria for quantitative comparisons of source content, test conditions, and subjective ratings, applying them to 27 image and video databases. Most collections have not been found satisfactory in terms of content range and uniformity. Only few databases showed good uniformity for test conditions (image/video quality), but not over the whole quality range. Also the distortion variety was found lacking in most databases covering mainly compression and transmission, but not the many other types of natural distortions found \"in the wild\" [3].\\n\\nTo overcome these limitations we introduce KoNViD-1k, a large publicly available database of video sequences based on YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset [4] with a diverse set of video content. In this paper we report the filtering mechanisms and sampling procedures necessary to construct high-quality VQA databases of this kind, focusing on their usefulness in a variety of applications.\\n\\nIn the next section, we describe the database creation procedure and the set of attributes we have considered to maximise its diversity. Additional information regarding the removal of non-natural video sequences and sampling techniques are provided as well. Next, in Sec. III, we review our crowdsourcing-based process of collecting subjective mean opinion scores (MOS) and detail our results as well as crowd worker statistics. In Sec. IV we relate our database characteristics and creation methodology with other existing works and outline the differences, before discussing conclusions of our work and considering possible future work.'}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_2308_high_quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42f6d041-2978-4286-a71d-085acf70c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start, let's see how we would do this with the first table:\n",
    "full_text = full_texts_2308_high_quality_map[dataset_2308_high_quality_sample[0]['paper_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940a616-af9f-4437-bdc7-3b7e432b85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "('Inference Time', '29min22s')\n",
    "('Inference Time', '21min56s')\n",
    "('Inference Time', '9s')\n",
    "\n",
    "('Baseband BW', '1 GHz')\n",
    "('Baseband BW', '4 GHz')\n",
    "('Baseband BW', '2 GHz')\n",
    "('Baseband BW', '450 MHz')\n",
    "('Baseband BW', '7 GHz')\n",
    "('Carries Freq.', 'Carries Freq.')\n",
    "('Carries Freq.', '57 ∼ 64 GHz')\n",
    "('Carries Freq.', '60 GHz')\n",
    "('Carries Freq.', '24 GHz')\n",
    "('Cost', '$15K')\n",
    "('Cost', 'below $15K')\n",
    "('Cost', '$40K')\n",
    "('Cost', 'below $100')\n",
    "('Video Training', '∼ 10 hr')\n",
    "('Video Training', '∼ -')\n",
    "('Edit Training', '∼ 1 hours')\n",
    "('Edit Training', '30 min')\n",
    "('Edit Inference', '∼ 10 sec')\n",
    "('Edit Inference', '∼ 4 min')\n",
    "('Edit Inference', '∼ 30 sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0a42b80d-d246-4a50-addc-1c5bd1817918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Try to apply the pattern at the start of the string, returning\n",
       "a Match object, or None if no match was found.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/jupyter/lib/python3.9/re.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7fc3c91b-0fd7-4705-ba68-0fa2b4bd5bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10hr'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[∼∼\\s]\", \"\", '∼ 10 hr'.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f27d2032-e04a-4d0a-a750-8df4d3027639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carriesfreq.\n",
      "carriesfreq.\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_numeric('Carries Freq.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7cc24678-7c96-4f86-8146-a1317d6e03d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any((None, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1ab1d5f8-46bb-48ff-a8f8-ff2d0644c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below$100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_numeric('below $100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f0a405bc-988c-4975-83d0-1da6d861388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10hr\n",
      "10hr\n",
      "None\n",
      "('10hr', None, None)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_numeric('∼ 10 hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "fdd4be4f-b905-4d10-8e31-07505bdcfcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Try to apply the pattern at the start of the string, returning\n",
       "a Match object, or None if no match was found.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/jupyter/lib/python3.9/re.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "71457f90-c48a-4b70-a1ac-5e1dbbf3fba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False or False or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf9b10-306b-44ff-bc4b-3620634f2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "'below $15K', '–', '×', '∼ -'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d70a258b-1ffc-4ac2-adf2-d8fe3c0b09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_na('∼ -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ac00a489-de91-4a5e-8919-19f431371e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15k\n",
      "15k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_numeric('below $15K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c8ee9f-bbe6-4253-8728-3caeefb24b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    # value = value.lower()\n",
    "    value = value.replace(\"below\", \"<\")\n",
    "    value = re.sub(r\"[<$∼~∼\\s]\", \"\", value.lower().strip())\n",
    "    # print(value)\n",
    "    units_regex = r\"[<-]?\\d+[km]\"\n",
    "    time_regex = r\"(\\d+(?:hrs?|hours?))?(\\d+(?:ms?|mins?|minutes?))?(\\d+(?:s|sec|seconds?))?\"\n",
    "    freq_regex = r\"\\d+[gm]hz\"\n",
    "    # print(value)\n",
    "    # print(re.match(units_regex, value) is not None or any(re.match(time_regex, value).groups()) or re.match(freq_regex, value) is not None)\n",
    "    if re.match(units_regex, value) is not None or any(re.match(time_regex, value).groups()) or re.match(freq_regex, value) is not None:\n",
    "        # print(re.match(units_regex, value) is not None)\n",
    "        # print(any(re.match(time_regex, value).groups()))\n",
    "        # print(re.match(freq_regex, value) is not None)\n",
    "        return True\n",
    "    # this is dumb, but easy and fast\n",
    "    try:\n",
    "        float(value.replace(\",\", \"\").replace(\"-\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_binary(value):\n",
    "    value = value.strip()\n",
    "    return value.lower() in {'yes', 'no'} or value in {'✘', '✗', '×', '✔', '✓'}\n",
    "\n",
    "def is_na(value):\n",
    "    value = re.sub(\"[∼~\\s]\", \"\", value.lower())\n",
    "    return value in {\"-\", '–', '-'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d466ca6f-7607-415b-878c-08aa8ce68931",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_keys = [\n",
    "    (scheme.strip(), entry.strip())\n",
    "    for scheme, column in\n",
    "    dataset_2308_high_quality_sample[0]['table_json']['table_dict'].items()\n",
    "    for entry in [scheme] + column\n",
    "    if scheme != \"References\" and not is_numeric(str(entry)) and not is_na(str(entry)) and not is_binary(str(entry))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dddb5d38-c50a-4576-a4b4-dad18d30c054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dataset', 'Dataset'),\n",
       " ('Dataset', 'KoNViD-1k'),\n",
       " ('Dataset', 'LIVE-VQC'),\n",
       " ('Dataset', 'YouTube-UGC'),\n",
       " ('Dataset', 'LSVQ'),\n",
       " ('Dataset', 'KoNViD-150k'),\n",
       " ('Dataset', 'Sports-1M'),\n",
       " ('Dataset', 'Kinetics-400'),\n",
       " ('Task', 'Task'),\n",
       " ('Task', 'VQA'),\n",
       " ('Task', 'VQA'),\n",
       " ('Task', 'VQA'),\n",
       " ('Task', 'VQA'),\n",
       " ('Task', 'VQA'),\n",
       " ('Task', 'classification'),\n",
       " ('Task', 'classification'),\n",
       " ('Size', 'Size'),\n",
       " ('Annotations', 'Annotations'),\n",
       " ('Annotations', '- (auto.)')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f0e1c6b-9b44-4674-91c6-1fa3f9cae97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scheme, glossary_key in glossary_keys:\n",
    "    if scheme == glossary_key:\n",
    "        query = f\"In the context of the following table, what does '{glossary_key}' refer to?\\n\"\n",
    "    else:\n",
    "        query = f\"In the context of the following table, what does the '{scheme}' '{glossary_key}' refer to?\"\n",
    "    table = str(pd.DataFrame(dataset_2308_high_quality_sample[0]['table_json']['table_dict'])) + \"\\n\"\n",
    "    table += \"Caption: \" + dataset_2308_high_quality_sample[0]['table_unfiltered']['caption']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40c3e1fc-8605-4fb6-b068-b71b70f3a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of the following table, what does the 'Dataset' 'KoNViD-1k' refer to?\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2620186e-9c0c-40b9-a68f-45de713cba63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': \"With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\\n\",\n",
       "  'cite_spans': [{'start': 128,\n",
       "    'end': 144,\n",
       "    'text': '{{cite:cef100d}}',\n",
       "    'ref_id': 'cef100d'},\n",
       "   {'start': 306, 'end': 322, 'text': '{{cite:9b70eb4}}', 'ref_id': '9b70eb4'},\n",
       "   {'start': 324, 'end': 340, 'text': '{{cite:c6c0872}}', 'ref_id': 'c6c0872'},\n",
       "   {'start': 419, 'end': 435, 'text': '{{cite:7da4179}}', 'ref_id': '7da4179'},\n",
       "   {'start': 437, 'end': 453, 'text': '{{cite:1ad644b}}', 'ref_id': '1ad644b'},\n",
       "   {'start': 455, 'end': 471, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'},\n",
       "   {'start': 473, 'end': 489, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'},\n",
       "   {'start': 491, 'end': 507, 'text': '{{cite:5d0c0fc}}', 'ref_id': '5d0c0fc'},\n",
       "   {'start': 509, 'end': 525, 'text': '{{cite:3237048}}', 'ref_id': '3237048'},\n",
       "   {'start': 581, 'end': 597, 'text': '{{cite:8f3eb86}}', 'ref_id': '8f3eb86'},\n",
       "   {'start': 599, 'end': 615, 'text': '{{cite:24b4d04}}', 'ref_id': '24b4d04'},\n",
       "   {'start': 617, 'end': 633, 'text': '{{cite:9be8207}}', 'ref_id': '9be8207'},\n",
       "   {'start': 635,\n",
       "    'end': 651,\n",
       "    'text': '{{cite:3dfd446}}',\n",
       "    'ref_id': '3dfd446'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab.\\xa0REF , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\\n',\n",
       "  'cite_spans': [{'start': 213,\n",
       "    'end': 229,\n",
       "    'text': '{{cite:fbaab96}}',\n",
       "    'ref_id': 'fbaab96'},\n",
       "   {'start': 231, 'end': 247, 'text': '{{cite:dd352e3}}', 'ref_id': 'dd352e3'},\n",
       "   {'start': 364, 'end': 380, 'text': '{{cite:b5fb90b}}', 'ref_id': 'b5fb90b'},\n",
       "   {'start': 382, 'end': 398, 'text': '{{cite:2222c26}}', 'ref_id': '2222c26'},\n",
       "   {'start': 400, 'end': 416, 'text': '{{cite:689f656}}', 'ref_id': '689f656'},\n",
       "   {'start': 418, 'end': 434, 'text': '{{cite:11e5a31}}', 'ref_id': '11e5a31'},\n",
       "   {'start': 576, 'end': 592, 'text': '{{cite:24b4d04}}', 'ref_id': '24b4d04'},\n",
       "   {'start': 724, 'end': 740, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'},\n",
       "   {'start': 742, 'end': 758, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'},\n",
       "   {'start': 979,\n",
       "    'end': 995,\n",
       "    'text': '{{cite:9c04ab9}}',\n",
       "    'ref_id': '9c04ab9'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\\nDataset\\nTask\\nSize\\nAnnotations\\nKoNViD-1k {{cite:24b4d04}}\\nVQA\\n1,200\\n114\\nLIVE-VQC {{cite:8f3eb86}}\\nVQA\\n585\\n240\\nYouTube-UGC {{cite:9be8207}}\\nVQA\\n1,380\\n123\\nLSVQ {{cite:3dfd446}}\\nVQA\\n39,075\\n35\\nKoNViD-150k {{cite:9c04ab9}}\\nVQA\\n153,841\\n5\\nSports-1M {{cite:fbaab96}}\\nclassification\\n1,133,158\\n- (auto.)\\nKinetics-400 {{cite:dd352e3}}\\nclassification\\n306,245\\n3-5\\n{{table:24da7cab-1a2b-4e9c-b0e0-fa3182383d36}}',\n",
       "  'cite_spans': [{'start': 240,\n",
       "    'end': 256,\n",
       "    'text': '{{cite:24b4d04}}',\n",
       "    'ref_id': '24b4d04'},\n",
       "   {'start': 280, 'end': 296, 'text': '{{cite:8f3eb86}}', 'ref_id': '8f3eb86'},\n",
       "   {'start': 321, 'end': 337, 'text': '{{cite:9be8207}}', 'ref_id': '9be8207'},\n",
       "   {'start': 357, 'end': 373, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'},\n",
       "   {'start': 400, 'end': 416, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'},\n",
       "   {'start': 441, 'end': 457, 'text': '{{cite:fbaab96}}', 'ref_id': 'fbaab96'},\n",
       "   {'start': 506,\n",
       "    'end': 522,\n",
       "    'text': '{{cite:dd352e3}}',\n",
       "    'ref_id': 'dd352e3'}],\n",
       "  'ref_spans': [{'start': 550,\n",
       "    'end': 596,\n",
       "    'text': '{{table:24da7cab-1a2b-4e9c-b0e0-fa3182383d36}}',\n",
       "    'ref_id': '24da7cab-1a2b-4e9c-b0e0-fa3182383d36'}]},\n",
       " {'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig.\\xa0REF , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\\n',\n",
       "  'cite_spans': [{'start': 72,\n",
       "    'end': 88,\n",
       "    'text': '{{cite:7da4179}}',\n",
       "    'ref_id': '7da4179'},\n",
       "   {'start': 90, 'end': 106, 'text': '{{cite:1ad644b}}', 'ref_id': '1ad644b'},\n",
       "   {'start': 108, 'end': 124, 'text': '{{cite:65d7727}}', 'ref_id': '65d7727'},\n",
       "   {'start': 126, 'end': 142, 'text': '{{cite:1cad59a}}', 'ref_id': '1cad59a'},\n",
       "   {'start': 299, 'end': 315, 'text': '{{cite:bad8aed}}', 'ref_id': 'bad8aed'},\n",
       "   {'start': 779, 'end': 795, 'text': '{{cite:c6c0872}}', 'ref_id': 'c6c0872'},\n",
       "   {'start': 942, 'end': 958, 'text': '{{cite:80cfe88}}', 'ref_id': '80cfe88'},\n",
       "   {'start': 960,\n",
       "    'end': 976,\n",
       "    'text': '{{cite:48735e5}}',\n",
       "    'ref_id': '48735e5'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\\n',\n",
       "  'cite_spans': [{'start': 653,\n",
       "    'end': 669,\n",
       "    'text': '{{cite:c682601}}',\n",
       "    'ref_id': 'c682601'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\\nOur contributions are as follows:\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Introduction',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'list',\n",
       "  'text': '\\nTo the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\\n\\nWe propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\\n\\nWe evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\\n\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Related work',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'According to the availability of reference videos, VQA methods can be classified into full-reference (FR), reduced-reference (RR), and no-reference (NR) {{cite:48735e5}} ones. As reference videos are always hard to obtain, NR-VQA becomes more practical in the real-world VQA scenario, which is investigated in this paper. According to the difference in construction schema, VQA methods can be classified into traditional hand-crafted and learning-based ones.\\n',\n",
       "  'cite_spans': [{'start': 153,\n",
       "    'end': 169,\n",
       "    'text': '{{cite:48735e5}}',\n",
       "    'ref_id': '48735e5'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Related work',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'figure',\n",
       "  'text': 'Overview of our proposed Ada-DQA framework. First, in-the-wild pretrained models are selected as candidates according to diverse aspects. Second, features generated by these frozen pretrained models are aggregated per sample using the QAM adaptively. This approach allows acquiring of quality-related representations. Third, during training, the integrated feature is utilized as supplementary supervision, along with the labeled quality score, to guide the training of a lightweight VQA model. During inference, only the optimized VQA model is used, reducing the computational cost largely.\\n{{figure:5fade9f4-1b6b-43a3-9b67-d972b61a11e3}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 592,\n",
       "    'end': 639,\n",
       "    'text': '{{figure:5fade9f4-1b6b-43a3-9b67-d972b61a11e3}}',\n",
       "    'ref_id': '5fade9f4-1b6b-43a3-9b67-d972b61a11e3'}]},\n",
       " {'section': 'Classical VQA Approaches',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Classic VQA methods {{cite:7c1267c}}, {{cite:0832a8c}}, {{cite:87e3b50}}, {{cite:e20cc9f}}, {{cite:b58ab68}}, {{cite:db5dff2}} rely on handcrafted features to evaluate video quality. With an underlying assumption that the perceptual quality can be measured by the disturbance of natural scene statics (NSS) {{cite:4bce23f}}, these work attempts at designing handcrafted features with richer representation for VQA. The work {{cite:5f9c3ce}} based on the 2D discrete-time transform (DCT) features of video frame-difference statistics, and motion information is further introduced to level up the representation capacity. TLVQM {{cite:87e3b50}} utilizes a combination of spatial high-complexity and temporal low-complexity handcraft features. Whereas, handcrafted features are gradually replaced by the DNN-based features, due to their sensitivity to distortion types and the superiority DNN features demonstrated in various computer vision tasks.\\n',\n",
       "  'cite_spans': [{'start': 20,\n",
       "    'end': 36,\n",
       "    'text': '{{cite:7c1267c}}',\n",
       "    'ref_id': '7c1267c'},\n",
       "   {'start': 38, 'end': 54, 'text': '{{cite:0832a8c}}', 'ref_id': '0832a8c'},\n",
       "   {'start': 56, 'end': 72, 'text': '{{cite:87e3b50}}', 'ref_id': '87e3b50'},\n",
       "   {'start': 74, 'end': 90, 'text': '{{cite:e20cc9f}}', 'ref_id': 'e20cc9f'},\n",
       "   {'start': 92, 'end': 108, 'text': '{{cite:b58ab68}}', 'ref_id': 'b58ab68'},\n",
       "   {'start': 110, 'end': 126, 'text': '{{cite:db5dff2}}', 'ref_id': 'db5dff2'},\n",
       "   {'start': 307, 'end': 323, 'text': '{{cite:4bce23f}}', 'ref_id': '4bce23f'},\n",
       "   {'start': 424, 'end': 440, 'text': '{{cite:5f9c3ce}}', 'ref_id': '5f9c3ce'},\n",
       "   {'start': 626,\n",
       "    'end': 642,\n",
       "    'text': '{{cite:87e3b50}}',\n",
       "    'ref_id': '87e3b50'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'DNN-based VQA Methods',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Recently, CNN-based methods {{cite:7da4179}}, {{cite:e20cc9f}}, {{cite:d5c7273}}, {{cite:b2e0c15}} and Transformer-based methods {{cite:dbe6597}}, {{cite:1c7237b}}, {{cite:fab2895}}, {{cite:f3075fd}} have taken the lead in the QA domain. However, due to the data-driven characteristics of deep learning, most of the current VQA models suffer from the lack of sufficient high-quality-labeled datasets. There are some attempts to relieve this insufficient data challenge, either from patch-level/frame-level augmentation {{cite:7da4179}}, {{cite:83e016c}} or fine-tuning from other large computer vision models pretrained on large general knowledge-based datasets {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}}. VSFA {{cite:7da4179}} extracts frame-wise features with ResNet and uses a gated-recurrent unit to model temporal information. LSCT {{cite:83e016c}} adopts a Transformer to predict video quality based on the frame features extracted by an IQA model. But frame-level augmentation dismissed the effect brought by temporal concealment, which is widely noticed nowadays.\\n',\n",
       "  'cite_spans': [{'start': 28,\n",
       "    'end': 44,\n",
       "    'text': '{{cite:7da4179}}',\n",
       "    'ref_id': '7da4179'},\n",
       "   {'start': 46, 'end': 62, 'text': '{{cite:e20cc9f}}', 'ref_id': 'e20cc9f'},\n",
       "   {'start': 64, 'end': 80, 'text': '{{cite:d5c7273}}', 'ref_id': 'd5c7273'},\n",
       "   {'start': 82, 'end': 98, 'text': '{{cite:b2e0c15}}', 'ref_id': 'b2e0c15'},\n",
       "   {'start': 129, 'end': 145, 'text': '{{cite:dbe6597}}', 'ref_id': 'dbe6597'},\n",
       "   {'start': 147, 'end': 163, 'text': '{{cite:1c7237b}}', 'ref_id': '1c7237b'},\n",
       "   {'start': 165, 'end': 181, 'text': '{{cite:fab2895}}', 'ref_id': 'fab2895'},\n",
       "   {'start': 183, 'end': 199, 'text': '{{cite:f3075fd}}', 'ref_id': 'f3075fd'},\n",
       "   {'start': 519, 'end': 535, 'text': '{{cite:7da4179}}', 'ref_id': '7da4179'},\n",
       "   {'start': 537, 'end': 553, 'text': '{{cite:83e016c}}', 'ref_id': '83e016c'},\n",
       "   {'start': 662, 'end': 678, 'text': '{{cite:1ad644b}}', 'ref_id': '1ad644b'},\n",
       "   {'start': 680, 'end': 696, 'text': '{{cite:65d7727}}', 'ref_id': '65d7727'},\n",
       "   {'start': 698, 'end': 714, 'text': '{{cite:1cad59a}}', 'ref_id': '1cad59a'},\n",
       "   {'start': 721, 'end': 737, 'text': '{{cite:7da4179}}', 'ref_id': '7da4179'},\n",
       "   {'start': 847,\n",
       "    'end': 863,\n",
       "    'text': '{{cite:83e016c}}',\n",
       "    'ref_id': '83e016c'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'DNN-based VQA Methods',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': \"Most fine-tuning work {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} utilizes models pretrained on classification datasets, where learned information mainly covers content-awareness and is not tailor for the task of VQA.\\nSeveral work {{cite:80cfe88}}, {{cite:48735e5}}, {{cite:b0d4c59}} has noticed the insufficiency of content-aware information.\\nCoINVQ {{cite:80cfe88}} leverage distortion-aware and compression-aware representation besides the content-aware representation. Nevertheless, the distorted information is learned from synthetic datasets and the generalized ability to in-the-wild distorted data needs to be verified. BVQA {{cite:48735e5}} introduces motion-aware information learned from the action classification dataset. But they dismiss the fact that distortion awareness is crucial to VQA.\\nWhat's more, these works either utilize a temporal-sampling and concatenating strategy to aggregate features or employ temporal average pooling for feature fusion. The final features are not acquired in an adaptive and flexible manner, which prohibits the diverse feature representations from unleashing full potential. More recent work focus on building spatiotemporal relation.\\nStarVQA {{cite:1c7237b}} builds a Transformer by using divided space-time attention. DisCoVQA {{cite:ebacaf2}} design a transformer-based Spatial-Temporal Distortion Extraction module to tackle temporal quality attention. FastVQA {{cite:f3075fd}} attempts to assess local quality by sampling patches at their raw resolution and covers global quality with contextual relations.\\n\",\n",
       "  'cite_spans': [{'start': 22,\n",
       "    'end': 38,\n",
       "    'text': '{{cite:1ad644b}}',\n",
       "    'ref_id': '1ad644b'},\n",
       "   {'start': 40, 'end': 56, 'text': '{{cite:65d7727}}', 'ref_id': '65d7727'},\n",
       "   {'start': 58, 'end': 74, 'text': '{{cite:1cad59a}}', 'ref_id': '1cad59a'},\n",
       "   {'start': 240, 'end': 256, 'text': '{{cite:80cfe88}}', 'ref_id': '80cfe88'},\n",
       "   {'start': 258, 'end': 274, 'text': '{{cite:48735e5}}', 'ref_id': '48735e5'},\n",
       "   {'start': 276, 'end': 292, 'text': '{{cite:b0d4c59}}', 'ref_id': 'b0d4c59'},\n",
       "   {'start': 360, 'end': 376, 'text': '{{cite:80cfe88}}', 'ref_id': '80cfe88'},\n",
       "   {'start': 642, 'end': 658, 'text': '{{cite:48735e5}}', 'ref_id': '48735e5'},\n",
       "   {'start': 1202,\n",
       "    'end': 1218,\n",
       "    'text': '{{cite:1c7237b}}',\n",
       "    'ref_id': '1c7237b'},\n",
       "   {'start': 1288,\n",
       "    'end': 1304,\n",
       "    'text': '{{cite:ebacaf2}}',\n",
       "    'ref_id': 'ebacaf2'},\n",
       "   {'start': 1424,\n",
       "    'end': 1440,\n",
       "    'text': '{{cite:f3075fd}}',\n",
       "    'ref_id': 'f3075fd'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Method',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'To surmount the constraint of limited labeled data availability and to obtain the quality-related features inherent in diverse modalities, we introduce the Ada-DQA framework for VQA tasks. In Sec.REF , we provide an overview of the framework. In Sec.REF , we explicate the construction of pretrained models from various aspects. In Sec.REF , we elucidate the process of acquiring quality representation using the proposed Quality-aware Acquisition Module (QAM). Finally, in Sec.REF , we present the optimization objective during training based on knowledge distillation and regression loss.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Ada-DQA Framework',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'As shown in Fig.\\xa0REF , the framework of Ada-DQA can be divided into three components. First, {{formula:c7dda1fd-8b15-4be8-8370-ccf75814f6d0}}  pretrained models, which act as feature extractors, are selected as candidates from the wild. Given an input video {{formula:c553ddb9-8fbf-42df-8469-7e75fbac32b0}} , features are generated by these pretrained models, whose weights are frozen. This significantly reduces the training cost of multiple heavy pretrained models. According to the training paradigm of pretrained models, these features may contain quality-related information (e.g., , content, distortions, and motion). However, since factors that may affect quality vary in different videos, the correlation between the quality of different videos and these features is also different. Second, to adaptively capture desired quality-related features sample-by-sample during training, the proposed QAM is used to raise dynamic weights for feature aggregation. An extra sparsity constraint is attached to the distribution of these gating weights, promoting attention to more critical and relevant features for quality representation. Then the video quality feature can be obtained by a weighted summation. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner. During inference, only the optimized VQA model is used, reducing the computational cost largely. More details will be provided below.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 93,\n",
       "    'end': 141,\n",
       "    'text': '{{formula:c7dda1fd-8b15-4be8-8370-ccf75814f6d0}}',\n",
       "    'ref_id': 'c7dda1fd-8b15-4be8-8370-ccf75814f6d0'},\n",
       "   {'start': 258,\n",
       "    'end': 306,\n",
       "    'text': '{{formula:c553ddb9-8fbf-42df-8469-7e75fbac32b0}}',\n",
       "    'ref_id': 'c553ddb9-8fbf-42df-8469-7e75fbac32b0'}]},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'figure',\n",
       "  'text': 'Videos sampled from the YouTube-UGC dataset {{cite:9be8207}} and their corresponding labeled MOS, ranging from 1.0 to 5.0. It can be seen that video quality is affected by various aspects, including content, distortions, and motion.\\n{{figure:7ce08525-212e-4804-9184-79dc25fdca7a}}',\n",
       "  'cite_spans': [{'start': 44,\n",
       "    'end': 60,\n",
       "    'text': '{{cite:9be8207}}',\n",
       "    'ref_id': '9be8207'}],\n",
       "  'ref_spans': [{'start': 233,\n",
       "    'end': 280,\n",
       "    'text': '{{figure:7ce08525-212e-4804-9184-79dc25fdca7a}}',\n",
       "    'ref_id': '7ce08525-212e-4804-9184-79dc25fdca7a'}]},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Inspired by the current success of the “pretraining and fine-tuning” paradigm in deep learning {{cite:e64e98e}}, {{cite:06016b9}}, {{cite:c682601}}, we aim to utilize in-the-wild pretrained models to benefit VQA from diverse aspects of the video, in order to enhance better understanding of video quality and enable personalized treatments to improve it. We contemplate the choice of pretrained models through the lens of multiple factors, as shown in Fig.\\xa0REF , that may impact the quality of videos as follows:\\n',\n",
       "  'cite_spans': [{'start': 95,\n",
       "    'end': 111,\n",
       "    'text': '{{cite:e64e98e}}',\n",
       "    'ref_id': 'e64e98e'},\n",
       "   {'start': 113, 'end': 129, 'text': '{{cite:06016b9}}', 'ref_id': '06016b9'},\n",
       "   {'start': 131,\n",
       "    'end': 147,\n",
       "    'text': '{{cite:c682601}}',\n",
       "    'ref_id': 'c682601'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'list',\n",
       "  'text': \"\\nContent. Human judgments of visual quality are content-dependent according to previous studies {{cite:7da4179}}, {{cite:80cfe88}}. When a video is visually appealing, engaging, and relevant to the viewer's interests (e.g., cute puppy and beautiful scenery), it can capture their attention and make them more receptive to the video's content. In contrast, if a video is dull, uninteresting, or irrelevant (e.g., black screen and messy corners), viewers are more likely to rate low quality. Introducing models pretrained on the task of object recognition (e.g., EfficientNet, Swin Transformer) may benefit VQA.\\n\\nDistortion. In addition to content, distortions introduced during the phase of video capturing and compression also determine the video quality {{cite:6b6881a}}. Thus a pretrained model that has been trained on a dataset of images or videos (e.g., ImageNet, Kinetics-400) with compression artifacts will have learned to identify the specific patterns and features that are associated with compression artifacts, such as blockiness, blurriness, or pixelation.\\n\\nMotion. Unlike the image scenario, motion blur can significantly affect the quality of videos {{cite:3d9999d}}, {{cite:08d1075}}. It occurs when there is rapid motion, and the camera or objects in the scene are moving too quickly for the camera's shutter to capture. A pretrained action recognition model (e.g., SlowFast, TimeSformer) may detect specific actions or movements such as running, jumping, or throwing. These can be useful for analyzing the amount of motion or by looking for specific visual cues that are associated with motion blur, such as streaking around the edges of moving objects.\\n\\n\",\n",
       "  'cite_spans': [{'start': 96,\n",
       "    'end': 112,\n",
       "    'text': '{{cite:7da4179}}',\n",
       "    'ref_id': '7da4179'},\n",
       "   {'start': 114, 'end': 130, 'text': '{{cite:80cfe88}}', 'ref_id': '80cfe88'},\n",
       "   {'start': 755, 'end': 771, 'text': '{{cite:6b6881a}}', 'ref_id': '6b6881a'},\n",
       "   {'start': 1165,\n",
       "    'end': 1181,\n",
       "    'text': '{{cite:3d9999d}}',\n",
       "    'ref_id': '3d9999d'},\n",
       "   {'start': 1183,\n",
       "    'end': 1199,\n",
       "    'text': '{{cite:08d1075}}',\n",
       "    'ref_id': '08d1075'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'figure',\n",
       "  'text': 'Responses of different pretrained models to synthetic sequences generated by distortions (i.e., compression, sharpness). The correlation of SRCC is computed according to distortion degrees. It is evident that pretrained models may detect certain types of distortions, but their ability to perceive distortion varies across models.\\n{{figure:e3af1672-5720-4013-bd7b-5b4ff0a7008c}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 331,\n",
       "    'end': 378,\n",
       "    'text': '{{figure:e3af1672-5720-4013-bd7b-5b4ff0a7008c}}',\n",
       "    'ref_id': 'e3af1672-5720-4013-bd7b-5b4ff0a7008c'}]},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'However, it is important to note that individual pretrained models may not be able to identify all types of quality-related factors, or may not be as accurate in identifying certain types. Some evidences are given in Fig.\\xa0REF . When encountering different types of distortions, there will be obvious differences in the perception ability of the pretrained model. In detail, ConvNext-base (SRCC=0.968) outperforms EfficientNet-b7 (SRCC=0.038) when facing compression. When it comes to sharpness, EfficientNet-b7 performs better. Therefore, it is important to use a diverse set of models and combine their results to get a more robust assessment. Thus, we propose to construct a pool with a large diversity of candidate models, considering the following aspects:\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'list',\n",
       "  'text': '\\nArchitecture. The efficacy of a network architecture (e.g., CNN, Transformer) hinges upon its capacity to assimilate and convey information. A well-crafted architecture can discern finer details and patterns in the input video, while also influencing the manner in which spatial and temporal information, containing quality-related features, is processed.\\n\\nPretrained pretext task. The type of supervision in the pretext task has an impact on the ability of the pretrained model to VQA tasks. When the distribution of data is similar, a supervised pretext task may lead to superior performance. Conversely, self-supervised pretext tasks, where the model is trained on unlabeled data, may facilitate better generalization when confronting unfamiliar VQA domains.\\n\\nPretrained dataset. Pretrained datasets on a large scale can be advantageous to VQA by providing diverse content, distortion, and motion-related data. A desired pretrained dataset should include a wide range of categories that closely resemble real-world scenarios, as well as other multi-modal information that can aid in describing video quality. For instance, the WebImageText {{cite:c682601}} dataset, which combines text and images, can be helpful in this regard.\\n\\n',\n",
       "  'cite_spans': [{'start': 1144,\n",
       "    'end': 1160,\n",
       "    'text': '{{cite:c682601}}',\n",
       "    'ref_id': 'c682601'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Quality-related Pretrained Models',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Based on the above considerations, in this paper, we select several pretrained models that obtain top performance in their original fields, including (1) EfficientNet-b7 {{cite:d48f3ca}} trained on ImageNet-1k {{cite:bad8aed}}, (2) ir-CSN-152 {{cite:f69f321}} trained on Sports-1M {{cite:fbaab96}}, (3) CLIP trained on WebImageText {{cite:c682601}}, (4) Swin Transformer Base {{cite:942a083}} trained on ImageNet-21k {{cite:bad8aed}}, (5) TimeSformer {{cite:1cd3326}} trained on Kinetics-400 {{cite:dd352e3}}, (6) Video Swin Transformer Base {{cite:3b57340}} trained on Kinetics-600 {{cite:f2782ba}}, and (7) SlowFast {{cite:55db95f}} trained on Kinetics-400.\\n',\n",
       "  'cite_spans': [{'start': 170,\n",
       "    'end': 186,\n",
       "    'text': '{{cite:d48f3ca}}',\n",
       "    'ref_id': 'd48f3ca'},\n",
       "   {'start': 210, 'end': 226, 'text': '{{cite:bad8aed}}', 'ref_id': 'bad8aed'},\n",
       "   {'start': 243, 'end': 259, 'text': '{{cite:f69f321}}', 'ref_id': 'f69f321'},\n",
       "   {'start': 281, 'end': 297, 'text': '{{cite:fbaab96}}', 'ref_id': 'fbaab96'},\n",
       "   {'start': 332, 'end': 348, 'text': '{{cite:c682601}}', 'ref_id': 'c682601'},\n",
       "   {'start': 376, 'end': 392, 'text': '{{cite:942a083}}', 'ref_id': '942a083'},\n",
       "   {'start': 417, 'end': 433, 'text': '{{cite:bad8aed}}', 'ref_id': 'bad8aed'},\n",
       "   {'start': 451, 'end': 467, 'text': '{{cite:1cd3326}}', 'ref_id': '1cd3326'},\n",
       "   {'start': 492, 'end': 508, 'text': '{{cite:dd352e3}}', 'ref_id': 'dd352e3'},\n",
       "   {'start': 542, 'end': 558, 'text': '{{cite:3b57340}}', 'ref_id': '3b57340'},\n",
       "   {'start': 583, 'end': 599, 'text': '{{cite:f2782ba}}', 'ref_id': 'f2782ba'},\n",
       "   {'start': 618,\n",
       "    'end': 634,\n",
       "    'text': '{{cite:55db95f}}',\n",
       "    'ref_id': '55db95f'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Quality-aware Acquisition Module',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'As the distribution of content and distortions in videos can be quite complex, a static combination of pretrained models may not always yield optimal performance. In order to adaptively capture the diversity and complementary information from different pretrained models, we propose a Quality-aware Acquisition Module (QAM). It takes extracted features from various pretrained models as input and produces a consolidated feature as output for the ultimate representation of quality. The computational process can be partitioned into two main parts. The first part is responsible for transforming the extracted features initially into a uniform feature dimension to enable subsequent aggregation. Structurally, this transformation block comprises two fully-connected layers followed by a normalization layer and a GELU activation layer. The second part generates gating weights {{formula:5fbebeaa-6e29-429f-af6d-7028234a6145}}  to control the aggregation process. The gating network takes the concatenated feature vector as input and outputs a set of gating weights that represent the relative contribution of each pretrained model to the final quality representation. Structurally, this gating network is stacked using a fully-connected layer and a sigmoid layer. Then the quality representation {{formula:62670a2b-af0d-4d99-9e72-2692062d8936}}  can be obtained by a weighted sum according to the gating weights. Given the extracted features by different pretrained models {{formula:4e5c4529-850d-4f24-85ea-b1e6c1b2641b}} , these procedures can be noted as:\\n{{formula:6c571f1d-29e5-4dd9-ba12-c75931535cf1}} \\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 877,\n",
       "    'end': 925,\n",
       "    'text': '{{formula:5fbebeaa-6e29-429f-af6d-7028234a6145}}',\n",
       "    'ref_id': '5fbebeaa-6e29-429f-af6d-7028234a6145'},\n",
       "   {'start': 1296,\n",
       "    'end': 1344,\n",
       "    'text': '{{formula:62670a2b-af0d-4d99-9e72-2692062d8936}}',\n",
       "    'ref_id': '62670a2b-af0d-4d99-9e72-2692062d8936'},\n",
       "   {'start': 1473,\n",
       "    'end': 1521,\n",
       "    'text': '{{formula:4e5c4529-850d-4f24-85ea-b1e6c1b2641b}}',\n",
       "    'ref_id': '4e5c4529-850d-4f24-85ea-b1e6c1b2641b'},\n",
       "   {'start': 1558,\n",
       "    'end': 1606,\n",
       "    'text': '{{formula:6c571f1d-29e5-4dd9-ba12-c75931535cf1}}',\n",
       "    'ref_id': '6c571f1d-29e5-4dd9-ba12-c75931535cf1'}]},\n",
       " {'section': 'Quality-aware Acquisition Module',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'where {{formula:1a4fa986-fed6-46a0-ad53-e32075b16491}}  denotes the mapping function for the {{formula:6938802c-1c0a-4d61-a681-68041a1f3c24}} -th transformation block, and {{formula:7e3de11c-a75e-4bac-ba2f-d235480454f2}}  represents the mapping function for the gating network. And {{formula:b5d1a473-8334-4e62-962b-cfe026c2de46}}  is the number of aligned feature dimensions.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 6,\n",
       "    'end': 54,\n",
       "    'text': '{{formula:1a4fa986-fed6-46a0-ad53-e32075b16491}}',\n",
       "    'ref_id': '1a4fa986-fed6-46a0-ad53-e32075b16491'},\n",
       "   {'start': 93,\n",
       "    'end': 141,\n",
       "    'text': '{{formula:6938802c-1c0a-4d61-a681-68041a1f3c24}}',\n",
       "    'ref_id': '6938802c-1c0a-4d61-a681-68041a1f3c24'},\n",
       "   {'start': 172,\n",
       "    'end': 220,\n",
       "    'text': '{{formula:7e3de11c-a75e-4bac-ba2f-d235480454f2}}',\n",
       "    'ref_id': '7e3de11c-a75e-4bac-ba2f-d235480454f2'},\n",
       "   {'start': 282,\n",
       "    'end': 330,\n",
       "    'text': '{{formula:b5d1a473-8334-4e62-962b-cfe026c2de46}}',\n",
       "    'ref_id': 'b5d1a473-8334-4e62-962b-cfe026c2de46'}]},\n",
       " {'section': 'Quality-aware Acquisition Module',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'In addition, to emphasize the importance of critical features and enhance the generalization ability, we propose to impose a sparsity constraint as a regularization on the distribution of gating weights. The {{formula:4f78fa84-5998-4285-9d7e-f60e003beb79}}  loss is utilized to penalize non-zero weights resulting in more weights near zero. This constraint can be written as:\\n{{formula:6b11ca0a-9bbf-49c3-aad9-66d13b5dbba7}} \\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 208,\n",
       "    'end': 256,\n",
       "    'text': '{{formula:4f78fa84-5998-4285-9d7e-f60e003beb79}}',\n",
       "    'ref_id': '4f78fa84-5998-4285-9d7e-f60e003beb79'},\n",
       "   {'start': 376,\n",
       "    'end': 424,\n",
       "    'text': '{{formula:6b11ca0a-9bbf-49c3-aad9-66d13b5dbba7}}',\n",
       "    'ref_id': '6b11ca0a-9bbf-49c3-aad9-66d13b5dbba7'}]},\n",
       " {'section': 'Quality-aware Acquisition Module',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'In this way, QAM allows for capturing a broader range of quality-related features, thereby enabling better adaptation to various types of video content, distortions, or movement. Then the aggregated feature is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:1f2628ca-68ff-4f09-a605-753c1f0c7044}} .\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 317,\n",
       "    'end': 365,\n",
       "    'text': '{{formula:1f2628ca-68ff-4f09-a605-753c1f0c7044}}',\n",
       "    'ref_id': '1f2628ca-68ff-4f09-a605-753c1f0c7044'}]},\n",
       " {'section': 'Optimization Objective',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'In practical scenarios, using these large pretrained models for inference can be computationally expensive. To reduce the computational cost and increase flexibility, we propose to use knowledge distillation {{cite:3ff8959}} to transfer the knowledge from large and complex models to a lightweight VQA model.\\n',\n",
       "  'cite_spans': [{'start': 208,\n",
       "    'end': 224,\n",
       "    'text': '{{cite:3ff8959}}',\n",
       "    'ref_id': '3ff8959'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Optimization Objective',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'In this paper, a Video Swin Transformer-Tiny {{cite:3b57340}} is selected as the backbone. For an input video {{formula:49738d73-40f7-4464-ac8d-9ec9eff4df6b}} , the quality representation can be achieved by {{formula:259222d6-7c40-4718-9b20-0ec19a8fc322}} , where {{formula:85ae2fac-e0db-4930-ac5d-baa57705ad3e}}  represents the mapping function of the lightweight backbone, and {{formula:82aa382a-efba-4170-bd5f-d80a50f432a1}} . Then {{formula:ed99ea9d-a43a-4fcd-bf92-b424f31da861}}  is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:f6172209-4096-47e0-aad6-1a9b172623e6}} . Note that both {{formula:cf3e1bd5-75c4-4a7d-b209-48f578459c8b}}  and {{formula:92e2e32a-bfab-42db-912f-27dd5c5391de}}  are supervised by the labeled MOS using a smooth {{formula:1033e1d1-4365-4d1e-8ec4-3519b2b975ea}}  loss. Additionally, we apply a similarity consistency in knowledge distillation between {{formula:46d43316-7075-4c76-b0c3-1b1bb9a49b48}}  and {{formula:5d9433c4-bb4c-42d7-be06-a7d148d989b8}} . This allows the VQA model to simulate the robust quality representation generated by diverse pretrained models, further enhancing its performance. Given the labeled MOS {{formula:1d2e4ebc-fefb-46b0-9c43-2a9324db9bcd}} , the regression loss for pretrained models can be noted as:\\n{{formula:f6db4960-52c2-445e-b97e-07032e4ee4df}} \\n',\n",
       "  'cite_spans': [{'start': 45,\n",
       "    'end': 61,\n",
       "    'text': '{{cite:3b57340}}',\n",
       "    'ref_id': '3b57340'}],\n",
       "  'ref_spans': [{'start': 110,\n",
       "    'end': 158,\n",
       "    'text': '{{formula:49738d73-40f7-4464-ac8d-9ec9eff4df6b}}',\n",
       "    'ref_id': '49738d73-40f7-4464-ac8d-9ec9eff4df6b'},\n",
       "   {'start': 207,\n",
       "    'end': 255,\n",
       "    'text': '{{formula:259222d6-7c40-4718-9b20-0ec19a8fc322}}',\n",
       "    'ref_id': '259222d6-7c40-4718-9b20-0ec19a8fc322'},\n",
       "   {'start': 264,\n",
       "    'end': 312,\n",
       "    'text': '{{formula:85ae2fac-e0db-4930-ac5d-baa57705ad3e}}',\n",
       "    'ref_id': '85ae2fac-e0db-4930-ac5d-baa57705ad3e'},\n",
       "   {'start': 379,\n",
       "    'end': 427,\n",
       "    'text': '{{formula:82aa382a-efba-4170-bd5f-d80a50f432a1}}',\n",
       "    'ref_id': '82aa382a-efba-4170-bd5f-d80a50f432a1'},\n",
       "   {'start': 435,\n",
       "    'end': 483,\n",
       "    'text': '{{formula:ed99ea9d-a43a-4fcd-bf92-b424f31da861}}',\n",
       "    'ref_id': 'ed99ea9d-a43a-4fcd-bf92-b424f31da861'},\n",
       "   {'start': 595,\n",
       "    'end': 643,\n",
       "    'text': '{{formula:f6172209-4096-47e0-aad6-1a9b172623e6}}',\n",
       "    'ref_id': 'f6172209-4096-47e0-aad6-1a9b172623e6'},\n",
       "   {'start': 661,\n",
       "    'end': 709,\n",
       "    'text': '{{formula:cf3e1bd5-75c4-4a7d-b209-48f578459c8b}}',\n",
       "    'ref_id': 'cf3e1bd5-75c4-4a7d-b209-48f578459c8b'},\n",
       "   {'start': 715,\n",
       "    'end': 763,\n",
       "    'text': '{{formula:92e2e32a-bfab-42db-912f-27dd5c5391de}}',\n",
       "    'ref_id': '92e2e32a-bfab-42db-912f-27dd5c5391de'},\n",
       "   {'start': 814,\n",
       "    'end': 862,\n",
       "    'text': '{{formula:1033e1d1-4365-4d1e-8ec4-3519b2b975ea}}',\n",
       "    'ref_id': '1033e1d1-4365-4d1e-8ec4-3519b2b975ea'},\n",
       "   {'start': 952,\n",
       "    'end': 1000,\n",
       "    'text': '{{formula:46d43316-7075-4c76-b0c3-1b1bb9a49b48}}',\n",
       "    'ref_id': '46d43316-7075-4c76-b0c3-1b1bb9a49b48'},\n",
       "   {'start': 1006,\n",
       "    'end': 1054,\n",
       "    'text': '{{formula:5d9433c4-bb4c-42d7-be06-a7d148d989b8}}',\n",
       "    'ref_id': '5d9433c4-bb4c-42d7-be06-a7d148d989b8'},\n",
       "   {'start': 1226,\n",
       "    'end': 1274,\n",
       "    'text': '{{formula:1d2e4ebc-fefb-46b0-9c43-2a9324db9bcd}}',\n",
       "    'ref_id': '1d2e4ebc-fefb-46b0-9c43-2a9324db9bcd'},\n",
       "   {'start': 1336,\n",
       "    'end': 1384,\n",
       "    'text': '{{formula:f6db4960-52c2-445e-b97e-07032e4ee4df}}',\n",
       "    'ref_id': 'f6db4960-52c2-445e-b97e-07032e4ee4df'}]},\n",
       " {'section': 'Optimization Objective',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'And the regression loss for the lightweight VQA model {{formula:48f98f1b-4648-49eb-bc17-0d581a35d692}}  share the same formulation. A {{formula:b8d641e6-56f4-479a-962e-18a5f2f0280f}}  loss is used for the process of knowledge distillation, which can be written as:\\n{{formula:da4128b2-ca63-4245-b9e0-17d09737c759}} \\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 54,\n",
       "    'end': 102,\n",
       "    'text': '{{formula:48f98f1b-4648-49eb-bc17-0d581a35d692}}',\n",
       "    'ref_id': '48f98f1b-4648-49eb-bc17-0d581a35d692'},\n",
       "   {'start': 134,\n",
       "    'end': 182,\n",
       "    'text': '{{formula:b8d641e6-56f4-479a-962e-18a5f2f0280f}}',\n",
       "    'ref_id': 'b8d641e6-56f4-479a-962e-18a5f2f0280f'},\n",
       "   {'start': 265,\n",
       "    'end': 313,\n",
       "    'text': '{{formula:da4128b2-ca63-4245-b9e0-17d09737c759}}',\n",
       "    'ref_id': 'da4128b2-ca63-4245-b9e0-17d09737c759'}]},\n",
       " {'section': 'Optimization Objective',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'The whole optimization objective can be formulated as:\\n{{formula:9c235282-5762-4e7a-8779-65f819b582d0}} \\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 55,\n",
       "    'end': 103,\n",
       "    'text': '{{formula:9c235282-5762-4e7a-8779-65f819b582d0}}',\n",
       "    'ref_id': '9c235282-5762-4e7a-8779-65f819b582d0'}]},\n",
       " {'section': 'Optimization Objective',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'where {{formula:edd3ecf5-e9ba-4766-805c-6c6d9c365904}}  is a balancing weight for knowledge distillation, and {{formula:cccc0ef1-171b-45e8-800e-aa911b903103}}  is a hyper-parameter to balance the level of sparsity.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 6,\n",
       "    'end': 54,\n",
       "    'text': '{{formula:edd3ecf5-e9ba-4766-805c-6c6d9c365904}}',\n",
       "    'ref_id': 'edd3ecf5-e9ba-4766-805c-6c6d9c365904'},\n",
       "   {'start': 110,\n",
       "    'end': 158,\n",
       "    'text': '{{formula:cccc0ef1-171b-45e8-800e-aa911b903103}}',\n",
       "    'ref_id': 'cccc0ef1-171b-45e8-800e-aa911b903103'}]},\n",
       " {'section': 'Dataset.',\n",
       "  'sec_number': '1',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Our method is evaluated on three widely-adopted public NR-VQA datasets, including KoNViD-1k {{cite:24b4d04}}, LIVE-VQC {{cite:8f3eb86}}, and YouTube-UGC {{cite:9be8207}}. Mean opinion scores (MOS) are provided along with training videos.\\nSpecifically, KoNViD-1k contains 1,200 videos that are fairly filtered from a large public video dataset YFCC-100M. The time duration of the video is 8 seconds. And these videos have a frame rate of 24/25/30 FPS and a resolution of {{formula:88c19727-b56a-4f6d-8e99-0aec9bb05750}} .\\nLIVE-VQC consists of 585 videos with complex authentic distortions, which are captured by 80 users using 101 different devices, ranging from 240P to 1080P.\\nYouTube-UGC has 1,380 UGC videos sampled from YouTube with a duration of 20 seconds and resolutions from 360P to 4K.\\nAll these datasets contain no pristine videos, thus only NR methods can be evaluated on them.\\nFollowing {{cite:c6c0872}}, we split all the dataset into 80% training videos and 20% testing videos randomly.\\n',\n",
       "  'cite_spans': [{'start': 92,\n",
       "    'end': 108,\n",
       "    'text': '{{cite:24b4d04}}',\n",
       "    'ref_id': '24b4d04'},\n",
       "   {'start': 119, 'end': 135, 'text': '{{cite:8f3eb86}}', 'ref_id': '8f3eb86'},\n",
       "   {'start': 153, 'end': 169, 'text': '{{cite:9be8207}}', 'ref_id': '9be8207'},\n",
       "   {'start': 898,\n",
       "    'end': 914,\n",
       "    'text': '{{cite:c6c0872}}',\n",
       "    'ref_id': 'c6c0872'}],\n",
       "  'ref_spans': [{'start': 470,\n",
       "    'end': 518,\n",
       "    'text': '{{formula:88c19727-b56a-4f6d-8e99-0aec9bb05750}}',\n",
       "    'ref_id': '88c19727-b56a-4f6d-8e99-0aec9bb05750'}]},\n",
       " {'section': 'Evaluation Metric.',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Spearman’s Rank-Order Correlation Coefficient (SRCC) and Pearson’s Linear Correlation Coefficient (PLCC) are selected as metrics to measure the monotonicity and accuracy, respectively. They are in the range of 0.0 to 1.0, and larger values indicate better results. Besides, the mean average of PLCC and SRCC is also reported as a comprehensive criterion.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Implementation Details',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Our method is implemented based on PyTorch {{cite:2e556d9}} and MMAction2 {{cite:a718e63}}. All experiments are conducted on 4 NVIDIA V100 GPUs.\\nFor all datasets, we select EfficientNet-b7, ir-CSN-152, CLIP, Swin Transformer Base, TimeSformer, Video Swin Transformer Base and SlowFast as candidate pretrained models, and choose the Video Swin Transformer Tiny as the lightweight VQA model.\\nFrames are sampled in each video with a fixed temporal step to form a clip input. For frame-wise models (e.g., EfficientNet, CLIP), the feature representation can be calculated through the average features of all frames. For video clip-based models (e.g., SlowFast, ir-CSN-152), the extracted features can be used directly for the video representation.\\nFor KoNViD-1k, we sample 16 frames with a frame interval of 2.\\nAs videos in LIVE-VQC and YouTube-UGC have longer time durations, we sample 64 frames with an interval of 2, and 32 frames with an interval of 8, respectively.\\nSince most augmentations will introduce extra interference to the quality of videos (e.g., resize, color jitter) {{cite:dbe6597}}, we only choose the center crop to produce inputs with a resolution of {{formula:f65cb7a8-aa8f-40f4-8620-58c4befba7dd}} .\\nDuring the optimization procedure, we use the AdamW optimizer with a weight decay of 2e-2.\\nA cosine annealing scheduler with a warmup of 2 epochs is adopted to control the learning rate. The initial learning rate is 1e-3. And {{formula:90617cdd-14f1-4198-92b5-f5cc82cb932a}}  is 0.1 by default. {{formula:d9f6a3fd-7cd3-47a3-a9a4-22cb40b1ee3d}}  is set to 0.8. {{formula:20e7ac50-f536-450f-aef3-b5f454e999f9}}  is set to 32. The batch size of the input is set to 1. All models are trained for 60 epochs. And the checkpoint generated by the last iteration is used for evaluation.\\nFor inference, we follow a similar procedure as {{cite:445c847}} by using {{formula:32f86a77-28dd-4e84-9816-ca280897ad39}}  views. In the procedure, a video is uniformly sampled as 4 clips in the temporal dimension, and for each clip, the shorter spatial side is scaled to 256 pixels and we take 5 crops in the four corners and the center. The final score is computed as the average score of all the views. The average result of 10 repeat runs with different random splits is used as the final score for the experiments in Tab.\\xa0REF .\\n',\n",
       "  'cite_spans': [{'start': 43,\n",
       "    'end': 59,\n",
       "    'text': '{{cite:2e556d9}}',\n",
       "    'ref_id': '2e556d9'},\n",
       "   {'start': 74, 'end': 90, 'text': '{{cite:a718e63}}', 'ref_id': 'a718e63'},\n",
       "   {'start': 1079,\n",
       "    'end': 1095,\n",
       "    'text': '{{cite:dbe6597}}',\n",
       "    'ref_id': 'dbe6597'},\n",
       "   {'start': 1844,\n",
       "    'end': 1860,\n",
       "    'text': '{{cite:445c847}}',\n",
       "    'ref_id': '445c847'}],\n",
       "  'ref_spans': [{'start': 1167,\n",
       "    'end': 1215,\n",
       "    'text': '{{formula:f65cb7a8-aa8f-40f4-8620-58c4befba7dd}}',\n",
       "    'ref_id': 'f65cb7a8-aa8f-40f4-8620-58c4befba7dd'},\n",
       "   {'start': 1444,\n",
       "    'end': 1492,\n",
       "    'text': '{{formula:90617cdd-14f1-4198-92b5-f5cc82cb932a}}',\n",
       "    'ref_id': '90617cdd-14f1-4198-92b5-f5cc82cb932a'},\n",
       "   {'start': 1513,\n",
       "    'end': 1561,\n",
       "    'text': '{{formula:d9f6a3fd-7cd3-47a3-a9a4-22cb40b1ee3d}}',\n",
       "    'ref_id': 'd9f6a3fd-7cd3-47a3-a9a4-22cb40b1ee3d'},\n",
       "   {'start': 1578,\n",
       "    'end': 1626,\n",
       "    'text': '{{formula:20e7ac50-f536-450f-aef3-b5f454e999f9}}',\n",
       "    'ref_id': '20e7ac50-f536-450f-aef3-b5f454e999f9'},\n",
       "   {'start': 1870,\n",
       "    'end': 1918,\n",
       "    'text': '{{formula:32f86a77-28dd-4e84-9816-ca280897ad39}}',\n",
       "    'ref_id': '32f86a77-28dd-4e84-9816-ca280897ad39'}]},\n",
       " {'section': 'Implementation Details',\n",
       "  'sec_number': '2',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Comparisons with SOTA methods. The up arrow “{{formula:9cdebb9e-6b3f-4b6f-b9ab-97667fb7a327}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are highlighted and underlined. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.\\n2*Method\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\n\\nSRCC{{formula:d8a7d31e-d904-4b1e-8f77-b44182b0adc0}}\\nPLCC{{formula:80f9cab4-f37a-4236-a62f-f73bf32c5751}}\\nMean\\nSRCC{{formula:908447c2-5ea5-403f-8de1-ecc8f566c8e9}}\\nPLCC{{formula:c5d0133b-0266-4c10-a568-387e30798da5}}\\nMean\\nSRCC{{formula:00fa2371-042a-46d0-a7df-1eceae187c4c}}\\nPLCC{{formula:01663790-9e8e-4df9-9f95-902a56657579}}\\nMean\\nVIIDEO {{cite:0832a8c}}\\n0.2980\\n0.3030\\n0.3005\\n0.0332\\n0.2164\\n0.1248\\n0.0580\\n0.1534\\n0.1057\\nNIQE {{cite:c619c6d}}\\n0.5417\\n0.5530\\n0.5474\\n0.5957\\n0.6286\\n0.6122\\n0.2379\\n0.2776\\n0.2578\\nBRISQUE {{cite:18dcc02}}\\n0.654\\n0.626\\n0.640\\n0.592\\n0.638\\n0.615\\n0.382\\n0.395\\n0.389\\nVSFA {{cite:7da4179}}\\n0.755\\n0.744\\n0.750\\n-\\n-\\n-\\n-\\n-\\n-\\nTLVQM {{cite:1af68c6}}\\n0.7729\\n0.7688\\n0.7709\\n0.7988\\n0.8025\\n0.8807\\n0.6693\\n0.6590\\n0.6642\\nRIRNet {{cite:04485d0}}\\n0.7755\\n0.7812\\n0.7784\\n0.7713\\n0.7982\\n0.7848\\n-\\n-\\n-\\nMDTVSFA {{cite:1ad644b}}\\n0.7812\\n0.7856\\n0.7834\\n0.7382\\n0.7728\\n0.7555\\n-\\n-\\n-\\nRIRNet+CSPT {{cite:b519bb8}}\\n0.8008\\n0.8062\\n0.8035\\n0.7989\\n0.8194\\n0.8092\\n-\\n-\\n-\\nCoINVQ {{cite:80cfe88}}\\n0.802\\n0.816\\n0.809\\n-\\n-\\n-\\n0.764\\n0.767\\n0.766\\nRAPIQUE {{cite:f7fae73}}\\n0.8031\\n0.8175\\n0.8103\\n0.7548\\n0.7863\\n0.7706\\n0.7591\\n0.7684\\n0.7638\\nStarVQA {{cite:1c7237b}}\\n0.812\\n0.796\\n0.804\\n0.732\\n0.808\\n0.770\\n-\\n-\\n-\\nBVQA* {{cite:48735e5}}\\n0.8362\\n0.8335\\n0.8349\\n0.8412\\n0.8415\\n0.8414\\n0.8312\\n0.8194\\n0.8253\\nSTDAM {{cite:c6c0872}}\\n0.8448\\n0.8415\\n0.8432\\n0.7931\\n0.8204\\n0.8068\\n0.8341\\n0.8297\\n0.8319\\nDisCoVQA {{cite:ebacaf2}}\\n0.847\\n0.847\\n0.847\\n0.820\\n0.826\\n0.823\\n-\\n-\\n-\\nFastVQA {{cite:f3075fd}}\\n0.859\\n0.855\\n0.857\\n0.823\\n0.844\\n0.834\\n-\\n-\\n-\\nVideo Swin Tiny\\n0.8316\\n0.8694\\n0.8505\\n0.8335\\n0.8316\\n0.8326\\n0.8566\\n0.8499\\n0.8533\\nAda-DQA\\n0.8651\\n0.8831\\n0.8741\\n0.8591\\n0.8587\\n0.8589\\n0.8729\\n0.8800\\n0.8765\\n{{formula:a1eede44-0117-469e-89b9-f15da50d4c96}}  than the previous best\\n+0.6%\\n+2.8%\\n+1.7%\\n+1.79%\\n+1.72%\\n+1.75%\\n+3.88%\\n+5.03%\\n+4.46%\\n{{formula:a6bb876c-531a-40c9-afe7-f5d8ea915687}}  than w/o pretrained models\\n+3.35%\\n+1.37%\\n+2.36%\\n+2.56%\\n+2.71%\\n+2.63%\\n+1.63%\\n+3.01%\\n+2.32%\\n{{table:d7264495-8d09-4a2f-ab91-921442bf5f78}}',\n",
       "  'cite_spans': [{'start': 918,\n",
       "    'end': 934,\n",
       "    'text': '{{cite:0832a8c}}',\n",
       "    'ref_id': '0832a8c'},\n",
       "   {'start': 1003,\n",
       "    'end': 1019,\n",
       "    'text': '{{cite:c619c6d}}',\n",
       "    'ref_id': 'c619c6d'},\n",
       "   {'start': 1091,\n",
       "    'end': 1107,\n",
       "    'text': '{{cite:18dcc02}}',\n",
       "    'ref_id': '18dcc02'},\n",
       "   {'start': 1167,\n",
       "    'end': 1183,\n",
       "    'text': '{{cite:7da4179}}',\n",
       "    'ref_id': '7da4179'},\n",
       "   {'start': 1220,\n",
       "    'end': 1236,\n",
       "    'text': '{{cite:1af68c6}}',\n",
       "    'ref_id': '1af68c6'},\n",
       "   {'start': 1307,\n",
       "    'end': 1323,\n",
       "    'text': '{{cite:04485d0}}',\n",
       "    'ref_id': '04485d0'},\n",
       "   {'start': 1380,\n",
       "    'end': 1396,\n",
       "    'text': '{{cite:1ad644b}}',\n",
       "    'ref_id': '1ad644b'},\n",
       "   {'start': 1457,\n",
       "    'end': 1473,\n",
       "    'text': '{{cite:b519bb8}}',\n",
       "    'ref_id': 'b519bb8'},\n",
       "   {'start': 1529,\n",
       "    'end': 1545,\n",
       "    'text': '{{cite:80cfe88}}',\n",
       "    'ref_id': '80cfe88'},\n",
       "   {'start': 1596,\n",
       "    'end': 1612,\n",
       "    'text': '{{cite:f7fae73}}',\n",
       "    'ref_id': 'f7fae73'},\n",
       "   {'start': 1684,\n",
       "    'end': 1700,\n",
       "    'text': '{{cite:1c7237b}}',\n",
       "    'ref_id': '1c7237b'},\n",
       "   {'start': 1749,\n",
       "    'end': 1765,\n",
       "    'text': '{{cite:48735e5}}',\n",
       "    'ref_id': '48735e5'},\n",
       "   {'start': 1835,\n",
       "    'end': 1851,\n",
       "    'text': '{{cite:c6c0872}}',\n",
       "    'ref_id': 'c6c0872'},\n",
       "   {'start': 1924,\n",
       "    'end': 1940,\n",
       "    'text': '{{cite:ebacaf2}}',\n",
       "    'ref_id': 'ebacaf2'},\n",
       "   {'start': 1991,\n",
       "    'end': 2007,\n",
       "    'text': '{{cite:f3075fd}}',\n",
       "    'ref_id': 'f3075fd'}],\n",
       "  'ref_spans': [{'start': 45,\n",
       "    'end': 93,\n",
       "    'text': '{{formula:9cdebb9e-6b3f-4b6f-b9ab-97667fb7a327}}',\n",
       "    'ref_id': '9cdebb9e-6b3f-4b6f-b9ab-97667fb7a327'},\n",
       "   {'start': 582,\n",
       "    'end': 630,\n",
       "    'text': '{{formula:d8a7d31e-d904-4b1e-8f77-b44182b0adc0}}',\n",
       "    'ref_id': 'd8a7d31e-d904-4b1e-8f77-b44182b0adc0'},\n",
       "   {'start': 635,\n",
       "    'end': 683,\n",
       "    'text': '{{formula:80f9cab4-f37a-4236-a62f-f73bf32c5751}}',\n",
       "    'ref_id': '80f9cab4-f37a-4236-a62f-f73bf32c5751'},\n",
       "   {'start': 693,\n",
       "    'end': 741,\n",
       "    'text': '{{formula:908447c2-5ea5-403f-8de1-ecc8f566c8e9}}',\n",
       "    'ref_id': '908447c2-5ea5-403f-8de1-ecc8f566c8e9'},\n",
       "   {'start': 746,\n",
       "    'end': 794,\n",
       "    'text': '{{formula:c5d0133b-0266-4c10-a568-387e30798da5}}',\n",
       "    'ref_id': 'c5d0133b-0266-4c10-a568-387e30798da5'},\n",
       "   {'start': 804,\n",
       "    'end': 852,\n",
       "    'text': '{{formula:00fa2371-042a-46d0-a7df-1eceae187c4c}}',\n",
       "    'ref_id': '00fa2371-042a-46d0-a7df-1eceae187c4c'},\n",
       "   {'start': 857,\n",
       "    'end': 905,\n",
       "    'text': '{{formula:01663790-9e8e-4df9-9f95-902a56657579}}',\n",
       "    'ref_id': '01663790-9e8e-4df9-9f95-902a56657579'},\n",
       "   {'start': 2200,\n",
       "    'end': 2248,\n",
       "    'text': '{{formula:a1eede44-0117-469e-89b9-f15da50d4c96}}',\n",
       "    'ref_id': 'a1eede44-0117-469e-89b9-f15da50d4c96'},\n",
       "   {'start': 2333,\n",
       "    'end': 2381,\n",
       "    'text': '{{formula:a6bb876c-531a-40c9-afe7-f5d8ea915687}}',\n",
       "    'ref_id': 'a6bb876c-531a-40c9-afe7-f5d8ea915687'},\n",
       "   {'start': 2473,\n",
       "    'end': 2519,\n",
       "    'text': '{{table:d7264495-8d09-4a2f-ab91-921442bf5f78}}',\n",
       "    'ref_id': 'd7264495-8d09-4a2f-ab91-921442bf5f78'}]},\n",
       " {'section': 'Comparison with SOTA methods',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'We report the SRCC and PLCC performance with current SOTA methods on KoNViD-1k, LIVE-VQC, and YouTube-UGC.\\nAs shown in Tab.\\xa0REF , our method achieves new state-of-the-art results on all these datasets without using extra training data of QA.\\nSome observations can also be found through these results.\\nWe can observe those deep learning-based methods outperform the traditional hand-crafted method (e.g., VIIDEO, NIQE) largely.\\nBesides, within deep learning-based methods, VQA methods produce much better performance than IQA methods (e.g., BRISQUE). Since there exist many temporal-distributed distortions in these datasets, VQA models can capture this temporal information.\\nAda-DQA outperforms StarVQA, which builds a Transformer model to capture spatiotemporal information, in large margins.\\nBVQA incorporates extra training data from other QA datasets for the feature extractor. And our method still outperforms it without using external training data of QA (+2.89% of SRCC, and + 4.96% of PLCC in KoNViD-1k). This shows the advantage of leveraging quality-related knowledge from pretrained models.\\nCompare with the current best method of STDAM, DiscoVQA, and FastVQA, Ada-DQA improves the best performance in large margins to {{formula:6f5c3ff3-bb6d-4fb0-a8bc-2a1c99c97f35}}  of SRCC in KoNVid-1k (+0.6%), {{formula:6ddb6487-ef20-43d8-af75-df8bc01255e5}}  of SRCC in LIVE-VQC (+1.79%) and {{formula:52cb05d2-3405-4e58-9a6f-f51d25476aa2}}  of SRCC in YouTube-UGC (+3.88%). The accuracy and consistency of prediction results have been significantly improved.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 1230,\n",
       "    'end': 1278,\n",
       "    'text': '{{formula:6f5c3ff3-bb6d-4fb0-a8bc-2a1c99c97f35}}',\n",
       "    'ref_id': '6f5c3ff3-bb6d-4fb0-a8bc-2a1c99c97f35'},\n",
       "   {'start': 1310,\n",
       "    'end': 1358,\n",
       "    'text': '{{formula:6ddb6487-ef20-43d8-af75-df8bc01255e5}}',\n",
       "    'ref_id': '6ddb6487-ef20-43d8-af75-df8bc01255e5'},\n",
       "   {'start': 1393,\n",
       "    'end': 1441,\n",
       "    'text': '{{formula:52cb05d2-3405-4e58-9a6f-f51d25476aa2}}',\n",
       "    'ref_id': '52cb05d2-3405-4e58-9a6f-f51d25476aa2'}]},\n",
       " {'section': 'Experimental Analysis and Ablation Studies',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Experimental analysis on different numbers of selected pretrained models with or without the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of with or without is bolded.\\nNumber\\nQAM\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\n3\\n{{formula:221dcedf-f5a4-4aaf-80a0-269d15f534be}}\\n0.8517\\n0.8432\\n0.8630\\n4\\n{{formula:1b371620-17ef-403e-9fda-3e25f7c434a7}}\\n0.8613\\n0.8510\\n0.8659\\n5\\n{{formula:e5bb12eb-de29-46bb-be7a-2f57fdab0797}}\\n0.8601\\n0.8490\\n0.8591\\n6\\n{{formula:d8a70087-e5e2-4795-a70a-ae3803ab7f30}}\\n0.8615\\n0.8448\\n0.8589\\n7\\n{{formula:88da022f-02c7-45d4-9301-e9e20bb5773f}}\\n0.8573\\n0.8459\\n0.8621\\n3\\n\\n0.8432\\n0.8433\\n0.8621\\n4\\n\\n0.8623\\n0.8514\\n0.8695\\n5\\n\\n0.8621\\n0.8546\\n0.8679\\n6\\n\\n0.8645\\n0.8542\\n0.8644\\n7\\n\\n0.8651\\n0.8591\\n0.8729\\n8\\n\\n0.8641\\n0.8560\\n0.8711\\n{{table:9ad28e89-2a09-4b7b-a6b7-58d313f28dc1}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 265,\n",
       "    'end': 313,\n",
       "    'text': '{{formula:221dcedf-f5a4-4aaf-80a0-269d15f534be}}',\n",
       "    'ref_id': '221dcedf-f5a4-4aaf-80a0-269d15f534be'},\n",
       "   {'start': 337,\n",
       "    'end': 385,\n",
       "    'text': '{{formula:1b371620-17ef-403e-9fda-3e25f7c434a7}}',\n",
       "    'ref_id': '1b371620-17ef-403e-9fda-3e25f7c434a7'},\n",
       "   {'start': 409,\n",
       "    'end': 457,\n",
       "    'text': '{{formula:e5bb12eb-de29-46bb-be7a-2f57fdab0797}}',\n",
       "    'ref_id': 'e5bb12eb-de29-46bb-be7a-2f57fdab0797'},\n",
       "   {'start': 481,\n",
       "    'end': 529,\n",
       "    'text': '{{formula:d8a70087-e5e2-4795-a70a-ae3803ab7f30}}',\n",
       "    'ref_id': 'd8a70087-e5e2-4795-a70a-ae3803ab7f30'},\n",
       "   {'start': 553,\n",
       "    'end': 601,\n",
       "    'text': '{{formula:88da022f-02c7-45d4-9301-e9e20bb5773f}}',\n",
       "    'ref_id': '88da022f-02c7-45d4-9301-e9e20bb5773f'},\n",
       "   {'start': 767,\n",
       "    'end': 813,\n",
       "    'text': '{{table:9ad28e89-2a09-4b7b-a6b7-58d313f28dc1}}',\n",
       "    'ref_id': '9ad28e89-2a09-4b7b-a6b7-58d313f28dc1'}]},\n",
       " {'section': 'Experimental Analysis and Ablation Studies',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.\\nPretrained Model\\nKoNViD-1k\\nLIVE-VQC\\nYouTube-UGC\\nw/o\\n0.8316\\n0.8335\\n0.8566\\nEfficientNet-b7\\n0.8412\\n0.8495\\n0.8587\\nVideo Swin Base\\n0.8390\\n0.8173\\n0.8603\\nSwin Base\\n0.8391\\n0.8475\\n0.8497\\nTimeSformer\\n0.8409\\n0.8302\\n0.8618\\nCLIP\\n0.8404\\n0.8341\\n0.8608\\nir-CNS-152\\n0.8458\\n0.8521\\n0.8518\\nSlowFast\\n0.8423\\n0.8041\\n0.8523\\n{{table:05f0fe78-77f1-4f23-82a6-1cb1620402b6}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 471,\n",
       "    'end': 517,\n",
       "    'text': '{{table:05f0fe78-77f1-4f23-82a6-1cb1620402b6}}',\n",
       "    'ref_id': '05f0fe78-77f1-4f23-82a6-1cb1620402b6'}]},\n",
       " {'section': 'Number of pretrained models and effectiveness of sparsity constraint in QAM',\n",
       "  'sec_number': '3',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'To investigate the impact of the number of pretrained models, we performed experiments by reducing the number of models from 7 to 3. As depicted in Tab.\\xa0REF , increasing the number of models does not always lead to better performance without the use of sparsity constraints in QAM. With the help of sparsity constraint, the model can achieve continuous improvement as the number of pretrained models increases. However, adding more models beyond 8 (introducing an extra model of ViT Base) does not yield any further improvements. This may indicate that the quality-related information provided by the pretrained models has reached a saturation point. Therefore, we set the number of pretrained models to 7 in our experiments. In these experiments, the pretrained model is randomly selected and removed.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'The necessary of diverse pretrained models',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Moreover, we also conduct experiments solely utilizing a singular pretrained model to demonstrate the disparity among models. As depicted in Tab.\\xa0REF , a solitary pretrained model cannot consistently attain optimal outcomes across all VQA datasets. For instance, while CLIP excels on YouTube-UGC, it falls considerably short on the other two datasets. We posit that this is influenced by the correlation between pretrained models, such as pre-text task, dataset, architecture, and VQA tasks. Additionally, the results obtained by utilizing singular pretrained models are notably distant from the state-of-the-art. These findings substantiate that a solitary pretrained model is inadequate for diverse application scenarios, and leveraging a variety of pretrained models is imperative.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'The necessary of diverse pretrained models',\n",
       "  'sec_number': '4',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Experiments on different distillation losses.\\ndistillation loss\\nSRCC{{formula:dd46be6a-65c3-43f3-946d-92949282cc11}}\\nPLCC{{formula:eda80638-c5e0-4244-a398-8894e6f85a54}}\\n{{formula:31f9fd54-46b1-4afd-9db2-1b91ddfc43e0}}\\n0.8651\\n0.8831\\n{{formula:c3bcc9df-6e3f-4bf1-b323-74173e83d89d}}\\n0.8092\\n0.8310\\nJesen-Shannon\\n0.8455\\n0.8646\\n{{table:3b4dbfb8-49c7-4dda-9e65-5f6f6d95a4d3}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 68,\n",
       "    'end': 116,\n",
       "    'text': '{{formula:dd46be6a-65c3-43f3-946d-92949282cc11}}',\n",
       "    'ref_id': 'dd46be6a-65c3-43f3-946d-92949282cc11'},\n",
       "   {'start': 121,\n",
       "    'end': 169,\n",
       "    'text': '{{formula:eda80638-c5e0-4244-a398-8894e6f85a54}}',\n",
       "    'ref_id': 'eda80638-c5e0-4244-a398-8894e6f85a54'},\n",
       "   {'start': 170,\n",
       "    'end': 218,\n",
       "    'text': '{{formula:31f9fd54-46b1-4afd-9db2-1b91ddfc43e0}}',\n",
       "    'ref_id': '31f9fd54-46b1-4afd-9db2-1b91ddfc43e0'},\n",
       "   {'start': 233,\n",
       "    'end': 281,\n",
       "    'text': '{{formula:c3bcc9df-6e3f-4bf1-b323-74173e83d89d}}',\n",
       "    'ref_id': 'c3bcc9df-6e3f-4bf1-b323-74173e83d89d'},\n",
       "   {'start': 324,\n",
       "    'end': 370,\n",
       "    'text': '{{table:3b4dbfb8-49c7-4dda-9e65-5f6f6d95a4d3}}',\n",
       "    'ref_id': '3b4dbfb8-49c7-4dda-9e65-5f6f6d95a4d3'}]},\n",
       " {'section': 'Different types of distillation loss',\n",
       "  'sec_number': '5',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Experiments on different knowledge distillation losses are performed in KoNViD-1k, including the {{formula:a767cdb6-563d-46b0-8abe-24b0e712c561}}  loss, the {{formula:3740432c-1fce-4101-b4b5-e072e1b6b55a}}  loss and the Jesen-Shannon (JS) loss {{cite:9306c46}}. As shown in Tab.\\xa0REF , the {{formula:84f55c13-be98-4d51-be6a-45e7140ec7d6}}  loss owns the best performance in transferring aggregated features by multiple teacher models.\\n',\n",
       "  'cite_spans': [{'start': 244,\n",
       "    'end': 260,\n",
       "    'text': '{{cite:9306c46}}',\n",
       "    'ref_id': '9306c46'}],\n",
       "  'ref_spans': [{'start': 97,\n",
       "    'end': 145,\n",
       "    'text': '{{formula:a767cdb6-563d-46b0-8abe-24b0e712c561}}',\n",
       "    'ref_id': 'a767cdb6-563d-46b0-8abe-24b0e712c561'},\n",
       "   {'start': 157,\n",
       "    'end': 205,\n",
       "    'text': '{{formula:3740432c-1fce-4101-b4b5-e072e1b6b55a}}',\n",
       "    'ref_id': '3740432c-1fce-4101-b4b5-e072e1b6b55a'},\n",
       "   {'start': 289,\n",
       "    'end': 337,\n",
       "    'text': '{{formula:84f55c13-be98-4d51-be6a-45e7140ec7d6}}',\n",
       "    'ref_id': '84f55c13-be98-4d51-be6a-45e7140ec7d6'}]},\n",
       " {'section': 'Selection of hyper-parameters',\n",
       "  'sec_number': '6',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'We conduct experiments to show how the hyper-parameters {{formula:ed351050-05c6-4413-9435-db5841c75807}}  and {{formula:1b42aa12-0802-4f1e-b163-0319b4aa0451}}  in Equ.\\xa0REF  will influence the final results in KoNViD-1k. The results are listed in Tab.\\xa0REF  and REF . When {{formula:66e76ccc-bbaf-42ce-a3af-6b78dbfec06e}}  is 0.1, and {{formula:20053bad-d395-4fd8-9a94-ba68fd319ff9}}  is 0.8, which are used in our experiments, the best performance can be obtained.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 56,\n",
       "    'end': 104,\n",
       "    'text': '{{formula:ed351050-05c6-4413-9435-db5841c75807}}',\n",
       "    'ref_id': 'ed351050-05c6-4413-9435-db5841c75807'},\n",
       "   {'start': 110,\n",
       "    'end': 158,\n",
       "    'text': '{{formula:1b42aa12-0802-4f1e-b163-0319b4aa0451}}',\n",
       "    'ref_id': '1b42aa12-0802-4f1e-b163-0319b4aa0451'},\n",
       "   {'start': 271,\n",
       "    'end': 319,\n",
       "    'text': '{{formula:66e76ccc-bbaf-42ce-a3af-6b78dbfec06e}}',\n",
       "    'ref_id': '66e76ccc-bbaf-42ce-a3af-6b78dbfec06e'},\n",
       "   {'start': 333,\n",
       "    'end': 381,\n",
       "    'text': '{{formula:20053bad-d395-4fd8-9a94-ba68fd319ff9}}',\n",
       "    'ref_id': '20053bad-d395-4fd8-9a94-ba68fd319ff9'}]},\n",
       " {'section': 'Selection of hyper-parameters',\n",
       "  'sec_number': '6',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Selection of the hyper-parameters of {{formula:f16bc710-2160-46d5-b84a-64fa45cfb291}}  and {{formula:fd5fe218-5800-4333-b038-da96402e6a57}} .\\n{{formula:29169695-a810-4688-9399-808f691d97bc}}\\nSRCC{{formula:db3a9c0b-5328-4299-84d7-f440e728d271}}\\nPLCC{{formula:1e9c8d41-bc4b-4137-85cc-218df131f13d}}\\n0.1\\n0.8651\\n0.8831\\n0.2\\n0.8462\\n0.8663\\n0.5\\n0.8406\\n0.8698\\n1.0\\n0.8320\\n0.8549\\n{{table:ddd2f8c3-e328-4637-b41f-6bbaccf59efa}}{{formula:7390f3e9-af3d-4608-acc3-2f8eb18dbb6c}}\\nSRCC{{formula:414262e2-d916-4698-abf5-34c10fc1b10f}}\\nPLCC{{formula:a70efd62-92c7-4ad4-9534-15c65066653b}}\\n0.8\\n0.8651\\n0.8831\\n0.2\\n0.8449\\n0.8671\\n0.5\\n0.8528\\n0.8716\\n1.0\\n0.8602\\n0.8780\\n{{table:075d524d-ee5a-49ae-a3f0-bf8151a446a6}}\\n\\n[t]0.23\\nhyper-parameter {{formula:49c4784c-769b-446c-8656-ababa402404d}}[t]0.23\\nhyper-parameter {{formula:7200fc51-23ac-40a3-a533-1b2f724d97a6}} .{{table:de0d28e6-3c30-41b2-8264-b12f873fb795}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 37,\n",
       "    'end': 85,\n",
       "    'text': '{{formula:f16bc710-2160-46d5-b84a-64fa45cfb291}}',\n",
       "    'ref_id': 'f16bc710-2160-46d5-b84a-64fa45cfb291'},\n",
       "   {'start': 91,\n",
       "    'end': 139,\n",
       "    'text': '{{formula:fd5fe218-5800-4333-b038-da96402e6a57}}',\n",
       "    'ref_id': 'fd5fe218-5800-4333-b038-da96402e6a57'},\n",
       "   {'start': 142,\n",
       "    'end': 190,\n",
       "    'text': '{{formula:29169695-a810-4688-9399-808f691d97bc}}',\n",
       "    'ref_id': '29169695-a810-4688-9399-808f691d97bc'},\n",
       "   {'start': 195,\n",
       "    'end': 243,\n",
       "    'text': '{{formula:db3a9c0b-5328-4299-84d7-f440e728d271}}',\n",
       "    'ref_id': 'db3a9c0b-5328-4299-84d7-f440e728d271'},\n",
       "   {'start': 248,\n",
       "    'end': 296,\n",
       "    'text': '{{formula:1e9c8d41-bc4b-4137-85cc-218df131f13d}}',\n",
       "    'ref_id': '1e9c8d41-bc4b-4137-85cc-218df131f13d'},\n",
       "   {'start': 369,\n",
       "    'end': 415,\n",
       "    'text': '{{table:ddd2f8c3-e328-4637-b41f-6bbaccf59efa}}',\n",
       "    'ref_id': 'ddd2f8c3-e328-4637-b41f-6bbaccf59efa'},\n",
       "   {'start': 415,\n",
       "    'end': 463,\n",
       "    'text': '{{formula:7390f3e9-af3d-4608-acc3-2f8eb18dbb6c}}',\n",
       "    'ref_id': '7390f3e9-af3d-4608-acc3-2f8eb18dbb6c'},\n",
       "   {'start': 468,\n",
       "    'end': 516,\n",
       "    'text': '{{formula:414262e2-d916-4698-abf5-34c10fc1b10f}}',\n",
       "    'ref_id': '414262e2-d916-4698-abf5-34c10fc1b10f'},\n",
       "   {'start': 521,\n",
       "    'end': 569,\n",
       "    'text': '{{formula:a70efd62-92c7-4ad4-9534-15c65066653b}}',\n",
       "    'ref_id': 'a70efd62-92c7-4ad4-9534-15c65066653b'},\n",
       "   {'start': 642,\n",
       "    'end': 688,\n",
       "    'text': '{{table:075d524d-ee5a-49ae-a3f0-bf8151a446a6}}',\n",
       "    'ref_id': '075d524d-ee5a-49ae-a3f0-bf8151a446a6'},\n",
       "   {'start': 714,\n",
       "    'end': 762,\n",
       "    'text': '{{formula:49c4784c-769b-446c-8656-ababa402404d}}',\n",
       "    'ref_id': '49c4784c-769b-446c-8656-ababa402404d'},\n",
       "   {'start': 786,\n",
       "    'end': 834,\n",
       "    'text': '{{formula:7200fc51-23ac-40a3-a533-1b2f724d97a6}}',\n",
       "    'ref_id': '7200fc51-23ac-40a3-a533-1b2f724d97a6'},\n",
       "   {'start': 836,\n",
       "    'end': 882,\n",
       "    'text': '{{table:de0d28e6-3c30-41b2-8264-b12f873fb795}}',\n",
       "    'ref_id': 'de0d28e6-3c30-41b2-8264-b12f873fb795'}]},\n",
       " {'section': 'Selection of hyper-parameters',\n",
       "  'sec_number': '6',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'table',\n",
       "  'text': 'Contributions of different pretrained models by {{formula:277c600d-4c40-49ad-8127-54194614bd53}} .\\nModel\\n{{formula:c4f89f23-5b45-4835-99b1-dd94ba22dfaa}}  for LQ videos\\n{{formula:d8aee694-bab5-4b79-a1ec-b094d047475f}}  for HQ videos\\nEfficientNet-b7\\n0.1303\\n0.7208\\nVideo Swin Base\\n0.2021\\n0.0455\\nSwin Base\\n0.1676\\n0.6805\\nTimeSformer\\n0.4679\\n0.2317\\nCLIP\\n0.1279\\n0.3567\\nir-CNS-152\\n0.8690\\n0.2788\\nSlowFast\\n0.4210\\n0.0018\\n{{table:a1a6fc9d-c459-41de-bec4-a56b500b92d4}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 48,\n",
       "    'end': 96,\n",
       "    'text': '{{formula:277c600d-4c40-49ad-8127-54194614bd53}}',\n",
       "    'ref_id': '277c600d-4c40-49ad-8127-54194614bd53'},\n",
       "   {'start': 105,\n",
       "    'end': 153,\n",
       "    'text': '{{formula:c4f89f23-5b45-4835-99b1-dd94ba22dfaa}}',\n",
       "    'ref_id': 'c4f89f23-5b45-4835-99b1-dd94ba22dfaa'},\n",
       "   {'start': 169,\n",
       "    'end': 217,\n",
       "    'text': '{{formula:d8aee694-bab5-4b79-a1ec-b094d047475f}}',\n",
       "    'ref_id': 'd8aee694-bab5-4b79-a1ec-b094d047475f'},\n",
       "   {'start': 410,\n",
       "    'end': 456,\n",
       "    'text': '{{table:a1a6fc9d-c459-41de-bec4-a56b500b92d4}}',\n",
       "    'ref_id': 'a1a6fc9d-c459-41de-bec4-a56b500b92d4'}]},\n",
       " {'section': 'Contribution of different pretrained models',\n",
       "  'sec_number': '7',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'To investigate the contribution, we analyze the gating weights {{formula:ab91ab50-65bf-4e4f-bc2c-6d61ca1dcfb9}}  generated by the QAM in KoNViD-1k. The statistical average scores for different models are calculated.\\nWe count responses of low-quality (LQ, MOS<3.5) and high-quality (HQ, MOS>3.5) videos in Tab.\\xa0REF . It can be seen that for LQ videos, models that can provide distortion and motion-related information (e.g., ir-CSN-152) have larger weights; for HQ videos, models that can provide content-related one (e.g., EfficientNet-b7) own larger weights.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 63,\n",
       "    'end': 111,\n",
       "    'text': '{{formula:ab91ab50-65bf-4e4f-bc2c-6d61ca1dcfb9}}',\n",
       "    'ref_id': 'ab91ab50-65bf-4e4f-bc2c-6d61ca1dcfb9'}]},\n",
       " {'section': 'Computational cost',\n",
       "  'sec_number': '8',\n",
       "  'sec_type': '',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'We compare the #Params, #FLOPs, and SRCC of Ada-DQA over some SOTA methods whose models are available: Ada-DQA (29M, 88T, 0.8651), MDTVSFA (24M, 168T, 0.7812), StarVQA (121M, 75T, 0.812), BVQA (24M, 240T, 0.8362). With the help of pretrained models during training, Ada-DQA obtains higher results with a fair cost during inference.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Visualization of the Attention',\n",
       "  'sec_number': '5',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'Some representative videos in KoNViD-1k are selected to show the performance improvement brought by Ada-DQA. Visualization of the feature attention maps using GradCAM {{cite:ecf1e0d}} are shown in Fig.\\xa0REF . After introducing the adaptive acquisition strategy, Ada-DQA generates more accurate results and the attention maps highlight more quality-related regions.\\nFor instance, in the first video clip, attention from the vast surface of the ocean (left) is transferred to the sailboat, with some attention on ocean waves kept (right). Ada-DQA focuses on areas more related to the action (boat sailing) or giving clues about the perceptual quality (edges of waves).\\n',\n",
       "  'cite_spans': [{'start': 167,\n",
       "    'end': 183,\n",
       "    'text': '{{cite:ecf1e0d}}',\n",
       "    'ref_id': 'ecf1e0d'}],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Visualization of the Attention',\n",
       "  'sec_number': '5',\n",
       "  'sec_type': 'subsection',\n",
       "  'content_type': 'figure',\n",
       "  'text': 'Comparison of predictions and attention visualizations. For each video, video frames (left) and attention maps (right) redbefore and greenafter using Ada-DQA are illustrated.\\n{{figure:1658c38e-34bd-4fc9-9e2a-02fd2548f5cc}}',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 175,\n",
       "    'end': 222,\n",
       "    'text': '{{figure:1658c38e-34bd-4fc9-9e2a-02fd2548f5cc}}',\n",
       "    'ref_id': '1658c38e-34bd-4fc9-9e2a-02fd2548f5cc'}]},\n",
       " {'section': 'Conclusion',\n",
       "  'sec_number': '5',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'To address the issue of insufficient training data in VQA, this paper analyzes the entire spectrum of video distribution diversity that impacts quality and proposes the Ada-DQA framework, which employs a range of diverse pretrained models to improve quality representation. With Ada-DQA, it becomes possible to extract critical and relevant features generated by different frozen pretrained models adaptively. Experimental results on three mainstream NR-VQA benchmarks show the effectiveness in the context of limited data. Thorough analysis and ablation studies also validate the necessity of each component. This work hopes to inspire future research that leverages pretrained models to aid in a wider array of tasks.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Acknowledgments',\n",
       "  'sec_number': '-1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'paragraph',\n",
       "  'text': 'This research was partly supported by the National Key R&D Program of China (Grant No. 2020AAA0108303), and Shenzhen Science and Technology Project (Grant No. JCYJ20200109143041798) and Shenzhen Stable Supporting Program (No. WDZC20200820200655001) and Shenzhen Key Laboratory of next-generation interactive media innovative technology (No. ZDSYS20210623092001004).\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []},\n",
       " {'section': 'Acknowledgments',\n",
       "  'sec_number': '-1',\n",
       "  'sec_type': 'section',\n",
       "  'content_type': 'Bibliography',\n",
       "  'text': 'Anurag Arnab, Mostafa\\nDehghani, Georg Heigold, Chen Sun,\\nMario Lucic, and Cordelia Schmid.\\n2021.\\nViViT: A Video Vision Transformer.\\nCoRR abs/2103.15691\\n(2021).\\nGedas Bertasius, Heng\\nWang, and Lorenzo Torresani.\\n2021.\\nIs Space-Time Attention All You Need for Video\\nUnderstanding?. In ICML,\\nVol.\\xa0139. PMLR,\\n813–824.\\nJoão Carreira, Eric\\nNoland, Andras Banki-Horvath, Chloe\\nHillier, and Andrew Zisserman.\\n2018.\\nA Short Note about Kinetics-600.\\nCoRR abs/1808.01340\\n(2018).\\nAaron Chadha and Yiannis\\nAndreopoulos. 2021.\\nDeep Perceptual Preprocessing for Video Coding. In\\nCVPR. IEEE,\\n14852–14861.\\nKuan-Ta Chen, Chi-Jui\\nChang, Chen-Chi Wu, Yu-Chun Chang,\\nand Chin-Laung Lei. 2010.\\nQuadrant of euphoria: a crowdsourcing platform for\\nQoE assessment.\\nIEEE Netw. 24,\\n2 (2010), 28–35.\\nhttps://doi.org/10.1109/MNET.2010.5430141\\nPengfei Chen, Leida Li,\\nLei Ma, Jinjian Wu, and\\nGuangming Shi. 2020b.\\nRIRNet: Recurrent-In-Recurrent Network for Video\\nQuality Assessment. In ACM MM.\\nACM, 834–842.\\nPengfei Chen, Leida Li,\\nJinjian Wu, Weisheng Dong, and\\nGuangming Shi. 2022.\\nContrastive Self-Supervised Pre-Training for Video\\nQuality Assessment.\\nIEEE Trans. Image Process.\\n31 (2022), 458–471.\\nTing Chen, Simon\\nKornblith, Mohammad Norouzi, and\\nGeoffrey\\xa0E. Hinton. 2020a.\\nA Simple Framework for Contrastive Learning of\\nVisual Representations. In ICML,\\nVol.\\xa0119. PMLR,\\n1597–1607.\\nYanjiao Chen, Kaishun Wu,\\nand Qian Zhang. 2015.\\nFrom QoS to QoE: A Tutorial on Video Quality\\nAssessment.\\nIEEE Commun. Surv. Tutorials\\n17, 2 (2015),\\n1126–1165.\\nhttps://doi.org/10.1109/COMST.2014.2363139\\nShyamprasad Chikkerur,\\nVijay Sundaram, Martin Reisslein, and\\nLina\\xa0J. Karam. 2011.\\nObjective Video Quality Assessment Methods: A\\nClassification, Review, and Performance Comparison.\\nIEEE Trans. Broadcast.\\n57, 2 (2011),\\n165–182.\\nCisco. 2021.\\nCisco annual internet report white paper.\\nhttps://www.cisco.com/c/en/us/solutions/collateral/\\nexecutive-perspectives/annual-internet-report/white-paper-c11-741490.html.\\nMMAction2 Contributors.\\n2020.\\nOpenMMLab\\'s Next Generation Video Understanding\\nToolbox and Benchmark.\\nhttps://github.com/open-mmlab/mmaction2.\\nJia Deng, Wei Dong,\\nRichard Socher, Li-Jia Li,\\nKai Li, and Li Fei-Fei.\\n2009.\\nImageNet: A large-scale hierarchical image\\ndatabase. In CVPR. IEEE\\nComputer Society, 248–255.\\nJoshua\\xa0Peter Ebenezer,\\nZaixi Shang, Yongjun Wu,\\nHai Wei, and Alan\\xa0C. Bovik.\\n2020.\\nNo-Reference Video Quality Assessment Using\\nSpace-Time Chips. In MMSP.\\nIEEE, 1–6.\\nChristoph Feichtenhofer,\\nHaoqi Fan, Jitendra Malik, and\\nKaiming He. 2019.\\nSlowFast Networks for Video Recognition. In\\nICCV. 6201–6210.\\nBent Fuglede and\\nFlemming Topsøe. 2004.\\nJensen-Shannon divergence and Hilbert space\\nembedding. In ISIT.\\nIEEE, 31.\\nFranz Götz-Hahn,\\nVlad Hosu, Hanhe Lin, and\\nDietmar Saupe. 2021.\\nKonVid-150k: A Dataset for No-Reference Video\\nQuality Assessment of Videos in-the-Wild.\\nIEEE Access 9\\n(2021), 72139–72160.\\nhttps://doi.org/10.1109/ACCESS.2021.3077642\\nJie Gu, Gaofeng Meng,\\nCheng Da, Shiming Xiang, and\\nChunhong Pan. 2019a.\\nNo-Reference Image Quality Assessment with\\nReinforcement Recursive List-Wise Ranking. In\\nAAAI. AAAI Press,\\n8336–8343.\\nJie Gu, Gaofeng Meng,\\nShiming Xiang, and Chunhong Pan.\\n2019b.\\nBlind image quality assessment via learnable\\nattention-based pooling.\\nPattern Recognit. 91\\n(2019), 332–344.\\nKaiming He, Haoqi Fan,\\nYuxin Wu, Saining Xie, and\\nRoss\\xa0B. Girshick. 2020.\\nMomentum Contrast for Unsupervised Visual\\nRepresentation Learning. In CVPR.\\nComputer Vision Foundation / IEEE,\\n9726–9735.\\nGeoffrey\\xa0E. Hinton, Oriol\\nVinyals, and Jeffrey Dean.\\n2015.\\nDistilling the Knowledge in a Neural Network.\\nCoRR abs/1503.02531\\n(2015).\\nTobias Hoßfeld,\\nChristian Keimel, Matthias Hirth,\\nBruno Gardlo, Julian Habigt,\\nKlaus Diepold, and Phuoc Tran-Gia.\\n2014.\\nBest Practices for QoE Crowdtesting: QoE Assessment\\nWith Crowdsourcing.\\nIEEE Trans. Multim. 16,\\n2 (2014), 541–558.\\nhttps://doi.org/10.1109/TMM.2013.2291663\\nVlad Hosu, Franz Hahn,\\nMohsen Jenadeleh, Hanhe Lin,\\nHui Men, Tamás Szirányi,\\nShujun Li, and Dietmar Saupe.\\n2017.\\nThe Konstanz natural video database (KoNViD-1k).\\nIn QoMEX. IEEE,\\n1–6.\\nRui Hou, YunHao Zhao,\\nYang Hu, and Huan Liu.\\n2020.\\nNo-reference video quality evaluation by a deep\\ntransfer CNN architecture.\\nSPIC 83\\n(2020), 115782.\\nAndrej Karpathy, George\\nToderici, Sanketh Shetty, Thomas Leung,\\nRahul Sukthankar, and Li Fei-Fei.\\n2014.\\nLarge-scale Video Classification with Convolutional\\nNeural Networks. In CVPR.\\nWill Kay, João\\nCarreira, Karen Simonyan, Brian Zhang,\\nChloe Hillier, Sudheendra\\nVijayanarasimhan, Fabio Viola, Tim\\nGreen, Trevor Back, Paul Natsev,\\nMustafa Suleyman, and Andrew\\nZisserman. 2017.\\nThe Kinetics Human Action Video Dataset.\\nCoRR abs/1705.06950\\n(2017).\\nJunjie Ke, Qifei Wang,\\nYilin Wang, Peyman Milanfar, and\\nFeng Yang. 2021.\\nMUSIQ: Multi-scale Image Quality Transformer.\\n(October 2021),\\n5148–5157.\\nJari Korhonen.\\n2019.\\nTwo-Level Approach for No-Reference Consumer Video\\nQuality Assessment.\\nIEEE Trans. Image Process.\\n28, 12 (2019),\\n5923–5938.\\nJari Korhonen, Yicheng\\nSu, and Junyong You. 2020.\\nBlind Natural Video Quality Prediction via\\nStatistical Temporal Features and Deep Spatial Features. In\\nMM \\'20: The 28th ACM International Conference\\non Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020,\\nChang\\xa0Wen Chen, Rita\\nCucchiara, Xian-Sheng Hua, Guo-Jun\\nQi, Elisa Ricci, Zhengyou Zhang, and\\nRoger Zimmermann (Eds.). ACM,\\n3311–3319.\\nhttps://doi.org/10.1145/3394171.3413845\\nBowen Li, Weixia Zhang,\\nMeng Tian, Guangtao Zhai, and\\nXianpei Wang. 2021b.\\nBlindly Assess Quality of In-the-Wild Videos via\\nQuality-aware Pre-training and Motion Perception.\\nCoRR abs/2108.08505\\n(2021).\\nDingquan Li, Tingting\\nJiang, and Ming Jiang. 2019.\\nQuality Assessment of In-the-Wild Videos. In\\nACM Multimedia. ACM,\\n2351–2359.\\nDingquan Li, Tingting\\nJiang, and Ming Jiang.\\n2021a.\\nUnified Quality Assessment of in-the-Wild Videos\\nwith Mixed Datasets Training.\\nIJCV 129,\\n4 (2021).\\nLiang Liao, Kangmin Xu,\\nHaoning Wu, Chaofeng Chen,\\nWenxiu Sun, Qiong Yan, and\\nWeisi Lin. 2022.\\nExploring the Effectiveness of Video Perceptual\\nRepresentation in Blind Video Quality Assessment. In\\nMM \\'22: The 30th ACM International Conference\\non Multimedia, Lisboa, Portugal, October 10 - 14, 2022,\\nJoão Magalhães,\\nAlberto\\xa0Del Bimbo, Shin\\'ichi Satoh,\\nNicu Sebe, Xavier Alameda-Pineda,\\nQin Jin, Vincent Oria, and\\nLaura Toni (Eds.). ACM,\\n837–846.\\nhttps://doi.org/10.1145/3503161.3547849\\nZe Liu, Yutong Lin,\\nYue Cao, Han Hu, Yixuan\\nWei, Zheng Zhang, Stephen Lin, and\\nBaining Guo. 2021a.\\nSwin Transformer: Hierarchical Vision Transformer\\nUsing Shifted Windows. In ICCV.\\n10012–10022.\\nZe Liu, Jia Ning,\\nYue Cao, Yixuan Wei,\\nZheng Zhang, Stephen Lin, and\\nHan Hu. 2021b.\\nVideo Swin Transformer.\\nCoRR abs/2106.13230\\n(2021).\\nAnish Mittal,\\nAnush\\xa0Krishna Moorthy, and Alan\\xa0Conrad\\nBovik. 2012.\\nNo-Reference Image Quality Assessment in the\\nSpatial Domain.\\nIEEE Trans. Image Process.\\n21, 12 (2012),\\n4695–4708.\\nAnish Mittal, Michele\\xa0A.\\nSaad, and Alan\\xa0C. Bovik.\\n2016.\\nA Completely Blind Video Integrity Oracle.\\nIEEE Trans. Image Process.\\n25, 1 (2016),\\n289–300.\\nAnish Mittal, Rajiv\\nSoundararajan, and Alan\\xa0C. Bovik.\\n2013.\\nMaking a \"Completely Blind\" Image Quality\\nAnalyzer.\\nIEEE SPL 20,\\n3 (2013), 209–212.\\nAdam Paszke, Sam Gross,\\nFrancisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin,\\nNatalia Gimelshein, Luca Antiga,\\nAlban Desmaison, Andreas Köpf,\\nEdward\\xa0Z. Yang, Zachary DeVito,\\nMartin Raison, Alykhan Tejani,\\nSasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and\\nSoumith Chintala. 2019.\\nPyTorch: An Imperative Style, High-Performance Deep\\nLearning Library. In NeurIPS.\\n8024–8035.\\nAlec Radford, Jong\\xa0Wook\\nKim, Chris Hallacy, Aditya Ramesh,\\nGabriel Goh, Sandhini Agarwal,\\nGirish Sastry, Amanda Askell,\\nPamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever.\\n2021.\\nLearning Transferable Visual Models From Natural\\nLanguage Supervision. In ICML.\\nPMLR, 8748–8763.\\nMichele\\xa0A. Saad, Alan\\xa0C.\\nBovik, and Christophe Charrier.\\n2012.\\nBlind Image Quality Assessment: A Natural Scene\\nStatistics Approach in the DCT Domain.\\nIEEE Trans. Image Process.\\n21, 8 (2012),\\n3339–3352.\\nhttps://doi.org/10.1109/TIP.2012.2191563\\nMichele\\xa0A. Saad, Alan\\xa0C.\\nBovik, and Christophe Charrier.\\n2014.\\nBlind Prediction of Natural Video Quality.\\nIEEE Trans. Image Process.\\n23, 3 (2014),\\n1352–1365.\\nRamprasaath\\xa0R. Selvaraju,\\nMichael Cogswell, Abhishek Das,\\nRamakrishna Vedantam, Devi Parikh, and\\nDhruv Batra. 2017.\\nGrad-CAM: Visual Explanations from Deep Networks\\nvia Gradient-Based Localization. In ICCV.\\nIEEE Computer Society, 618–626.\\nMuhammad Shahid, Jacob\\nSøgaard, Jeevan Pokhrel, Kjell\\nBrunnström, Kun Wang, Samira\\nTavakoli, and Narciso García.\\n2014.\\nCrowdsourcing based subjective quality assessment\\nof adaptive video streaming. In Sixth\\nInternational Workshop on Quality of Multimedia Experience, QoMEX 2014,\\nSingapore, September 18-20, 2014. IEEE,\\n53–54.\\nhttps://doi.org/10.1109/QoMEX.2014.6982289\\nZeina Sinno and\\nAlan\\xa0Conrad Bovik. 2019.\\nLarge-Scale Study of Perceptual Video Quality.\\nIEEE Trans. Image Process.\\n28, 2 (2019),\\n612–627.\\nMingxing Tan and Quoc\\xa0V.\\nLe. 2019.\\nEfficientNet: Rethinking Model Scaling for\\nConvolutional Neural Networks. In ICML\\n(Proceedings of Machine Learning Research,\\nVol.\\xa097). PMLR,\\n6105–6114.\\nDu Tran, Heng Wang,\\nMatt Feiszli, and Lorenzo Torresani.\\n2019.\\nVideo Classification With Channel-Separated\\nConvolutional Networks. In ICCV.\\nIEEE, 5551–5560.\\nZhengzhong Tu, Chia-Ju\\nChen, Yilin Wang, Neil Birkbeck,\\nBalu Adsumilli, and Alan\\xa0C. Bovik.\\n2021a.\\nEfficient User-Generated Video Quality Prediction.\\nIn Picture Coding Symposium, PCS 2021, Bristol,\\nUnited Kingdom, June 29 - July 2, 2021. IEEE,\\n1–5.\\nhttps://doi.org/10.1109/PCS50896.2021.9477483\\nZhengzhong Tu, Yilin\\nWang, Neil Birkbeck, Balu Adsumilli,\\nand Alan\\xa0C. Bovik. 2021b.\\nUGC-VQA: Benchmarking Blind Video Quality\\nAssessment for User Generated Content.\\nIEEE TIP 30\\n(2021), 4449–4464.\\nZhengzhong Tu, Yilin\\nWang, Neil Birkbeck, Balu Adsumilli,\\nand Alan\\xa0C. Bovik. 2021c.\\nUGC-VQA: Benchmarking Blind Video Quality\\nAssessment for User Generated Content.\\nIEEE TIP 30\\n(2021), 4449–4464.\\nZhengzhong Tu, Xiangxu\\nYu, Yilin Wang, Neil Birkbeck,\\nBalu Adsumilli, and Alan\\xa0C. Bovik.\\n2021d.\\nRAPIQUE: Rapid and Accurate Video Quality\\nPrediction of User Generated Content.\\nCoRR abs/2101.10955\\n(2021).\\nDomonkos Varga and\\nTamás Szirányi.\\n2019.\\nNo-reference video quality assessment via\\npretrained CNN and LSTM networks.\\nSignal Image Video Process.\\n13, 8 (2019),\\n1569–1576.\\nYilin Wang, Sasi Inguva,\\nand Balu Adsumilli. 2019.\\nYouTube UGC Dataset for Video Compression\\nResearch. In MMSP.\\nIEEE, 1–5.\\nYilin Wang, Junjie Ke,\\nHossein Talebi, Joong\\xa0Gon Yim,\\nNeil Birkbeck, Balu Adsumilli,\\nPeyman Milanfar, and Feng Yang.\\n2021.\\nRich Features for Perceptual Quality Assessment of\\nUGC Videos. In CVPR.\\n13435–13444.\\nZhou Wang, Ligang Lu,\\nand Alan\\xa0C. Bovik. 2004.\\nVideo quality assessment based on structural\\ndistortion measurement.\\nSignal Process. Image Commun.\\n19, 2 (2004),\\n121–132.\\nHaoning Wu, Chaofeng\\nChen, Jingwen Hou, Liang Liao,\\nAnnan Wang, Wenxiu Sun,\\nQiong Yan, and Weisi Lin.\\n2022a.\\nFAST-VQA: Efficient End-to-End Video Quality\\nAssessment with Fragment Sampling. In ECCV\\n(6), Vol.\\xa013666. 538–554.\\nHaoning Wu, Chaofeng\\nChen, Liang Liao, Jingwen Hou,\\nQiong Yan, and Weisi Lin.\\n2022b.\\nDisCoVQA: Temporal Distortion-Content Transformers\\nfor Video Quality Assessment.\\nCoRR abs/2206.09853\\n(2022).\\nFengchuang Xing,\\nYuan-Gen Wang, Hanpin Wang,\\nLeida Li, and Guopu Zhu.\\n2021.\\nStarVQA: Space-Time Attention for Video Quality\\nAssessment.\\nCoRR abs/2108.09635\\n(2021).\\nJiahua Xu, Jing Li,\\nXingguang Zhou, Wei Zhou,\\nBaichao Wang, and Zhibo Chen.\\n2021.\\nPerceptual Quality Assessment of Internet Videos.\\nIn ACM Multimedia. ACM,\\n1248–1257.\\nJia Yan, Weixia Zhang,\\nand Tianpeng Feng. 2016.\\nBlind Image Quality Assessment Based on Natural\\nRedundancy Statistics. In Computer Vision - ACCV\\n2016 - 13th Asian Conference on Computer Vision, Taipei, Taiwan, November\\n20-24, 2016, Revised Selected Papers, Part IV\\n(Lecture Notes in Computer Science,\\nVol.\\xa010114),\\nShang-Hong Lai,\\nVincent Lepetit, Ko\\xa0Nishino, and\\nYoichi Sato (Eds.). Springer,\\n3–18.\\nhttps://doi.org/10.1007/978-3-319-54190-7_1\\nZhenqiang Ying, Maniratnam\\nMandal, Deepti Ghadiyaram, and Alan\\nBovik. 2021.\\nPatch-VQ: \\'Patching Up\\' the Video Quality Problem.\\nIn CVPR. Computer Vision\\nFoundation / IEEE, 14019–14029.\\nJunyong You.\\n2021.\\nLong Short-term Convolutional Transformer for\\nNo-Reference Video Quality Assessment. In ACM\\nMultimedia. ACM, 2112–2120.\\nJunyong You and Jari\\nKorhonen. 2019.\\nDeep Neural Networks for No-Reference Video Quality\\nAssessment. In 2019 IEEE International\\nConference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25,\\n2019. IEEE, 2349–2353.\\nhttps://doi.org/10.1109/ICIP.2019.8803395\\nKai Zhao, Kun Yuan,\\nMing Sun, Mading Li, and\\nXing Wen. 2023b.\\nQuality-Aware Pre-Trained Models for Blind Image\\nQuality Assessment. In CVPR.\\nIEEE Computer Society, 22302–22313.\\nKai Zhao, Kun Yuan,\\nMing Sun, and Xing Wen.\\n2023a.\\nZoom-VQA: Patches, Frames and Clips Integration for\\nVideo Quality Assessment. In CVPR Workshops.\\nIEEE Computer Society, 1302–1310.\\n',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': []}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text['body_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9c178c46-c35d-460b-8a9b-0138fe613f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2308.00729v1'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7b25e30-cb37-4011-b872-e556899065f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\n",
      "\n",
      "\n",
      "DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. REF , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\n",
      "\n",
      "\n",
      "Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\n",
      "Dataset\n",
      "Task\n",
      "Size\n",
      "Annotations\n",
      "KoNViD-1k {{cite:24b4d04}}\n",
      "VQA\n",
      "1,200\n",
      "114\n",
      "LIVE-VQC {{cite:8f3eb86}}\n",
      "VQA\n",
      "585\n",
      "240\n",
      "YouTube-UGC {{cite:9be8207}}\n",
      "VQA\n",
      "1,380\n",
      "123\n",
      "LSVQ {{cite:3dfd446}}\n",
      "VQA\n",
      "39,075\n",
      "35\n",
      "KoNViD-150k {{cite:9c04ab9}}\n",
      "VQA\n",
      "153,841\n",
      "5\n",
      "Sports-1M {{cite:fbaab96}}\n",
      "classification\n",
      "1,133,158\n",
      "- (auto.)\n",
      "Kinetics-400 {{cite:dd352e3}}\n",
      "classification\n",
      "306,245\n",
      "3-5\n",
      "{{table:24da7cab-1a2b-4e9c-b0e0-fa3182383d36}}\n",
      "\n",
      "To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig. REF , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\n",
      "\n",
      "\n",
      "To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\n",
      "\n",
      "\n",
      "Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\n",
      "Our contributions are as follows:\n",
      "\n",
      "\n",
      "\n",
      "To the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\n",
      "\n",
      "We propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\n",
      "\n",
      "We evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\n",
      "\n",
      "\n",
      "\n",
      "According to the availability of reference videos, VQA methods can be classified into full-reference (FR), reduced-reference (RR), and no-reference (NR) {{cite:48735e5}} ones. As reference videos are always hard to obtain, NR-VQA becomes more practical in the real-world VQA scenario, which is investigated in this paper. According to the difference in construction schema, VQA methods can be classified into traditional hand-crafted and learning-based ones.\n",
      "\n",
      "\n",
      "Overview of our proposed Ada-DQA framework. First, in-the-wild pretrained models are selected as candidates according to diverse aspects. Second, features generated by these frozen pretrained models are aggregated per sample using the QAM adaptively. This approach allows acquiring of quality-related representations. Third, during training, the integrated feature is utilized as supplementary supervision, along with the labeled quality score, to guide the training of a lightweight VQA model. During inference, only the optimized VQA model is used, reducing the computational cost largely.\n",
      "{{figure:5fade9f4-1b6b-43a3-9b67-d972b61a11e3}}\n",
      "\n",
      "Classic VQA methods {{cite:7c1267c}}, {{cite:0832a8c}}, {{cite:87e3b50}}, {{cite:e20cc9f}}, {{cite:b58ab68}}, {{cite:db5dff2}} rely on handcrafted features to evaluate video quality. With an underlying assumption that the perceptual quality can be measured by the disturbance of natural scene statics (NSS) {{cite:4bce23f}}, these work attempts at designing handcrafted features with richer representation for VQA. The work {{cite:5f9c3ce}} based on the 2D discrete-time transform (DCT) features of video frame-difference statistics, and motion information is further introduced to level up the representation capacity. TLVQM {{cite:87e3b50}} utilizes a combination of spatial high-complexity and temporal low-complexity handcraft features. Whereas, handcrafted features are gradually replaced by the DNN-based features, due to their sensitivity to distortion types and the superiority DNN features demonstrated in various computer vision tasks.\n",
      "\n",
      "\n",
      "Recently, CNN-based methods {{cite:7da4179}}, {{cite:e20cc9f}}, {{cite:d5c7273}}, {{cite:b2e0c15}} and Transformer-based methods {{cite:dbe6597}}, {{cite:1c7237b}}, {{cite:fab2895}}, {{cite:f3075fd}} have taken the lead in the QA domain. However, due to the data-driven characteristics of deep learning, most of the current VQA models suffer from the lack of sufficient high-quality-labeled datasets. There are some attempts to relieve this insufficient data challenge, either from patch-level/frame-level augmentation {{cite:7da4179}}, {{cite:83e016c}} or fine-tuning from other large computer vision models pretrained on large general knowledge-based datasets {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}}. VSFA {{cite:7da4179}} extracts frame-wise features with ResNet and uses a gated-recurrent unit to model temporal information. LSCT {{cite:83e016c}} adopts a Transformer to predict video quality based on the frame features extracted by an IQA model. But frame-level augmentation dismissed the effect brought by temporal concealment, which is widely noticed nowadays.\n",
      "\n",
      "\n",
      "Most fine-tuning work {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} utilizes models pretrained on classification datasets, where learned information mainly covers content-awareness and is not tailor for the task of VQA.\n",
      "Several work {{cite:80cfe88}}, {{cite:48735e5}}, {{cite:b0d4c59}} has noticed the insufficiency of content-aware information.\n",
      "CoINVQ {{cite:80cfe88}} leverage distortion-aware and compression-aware representation besides the content-aware representation. Nevertheless, the distorted information is learned from synthetic datasets and the generalized ability to in-the-wild distorted data needs to be verified. BVQA {{cite:48735e5}} introduces motion-aware information learned from the action classification dataset. But they dismiss the fact that distortion awareness is crucial to VQA.\n",
      "What's more, these works either utilize a temporal-sampling and concatenating strategy to aggregate features or employ temporal average pooling for feature fusion. The final features are not acquired in an adaptive and flexible manner, which prohibits the diverse feature representations from unleashing full potential. More recent work focus on building spatiotemporal relation.\n",
      "StarVQA {{cite:1c7237b}} builds a Transformer by using divided space-time attention. DisCoVQA {{cite:ebacaf2}} design a transformer-based Spatial-Temporal Distortion Extraction module to tackle temporal quality attention. FastVQA {{cite:f3075fd}} attempts to assess local quality by sampling patches at their raw resolution and covers global quality with contextual relations.\n",
      "\n",
      "\n",
      "To surmount the constraint of limited labeled data availability and to obtain the quality-related features inherent in diverse modalities, we introduce the Ada-DQA framework for VQA tasks. In Sec.REF , we provide an overview of the framework. In Sec.REF , we explicate the construction of pretrained models from various aspects. In Sec.REF , we elucidate the process of acquiring quality representation using the proposed Quality-aware Acquisition Module (QAM). Finally, in Sec.REF , we present the optimization objective during training based on knowledge distillation and regression loss.\n",
      "\n",
      "\n",
      "As shown in Fig. REF , the framework of Ada-DQA can be divided into three components. First, {{formula:c7dda1fd-8b15-4be8-8370-ccf75814f6d0}}  pretrained models, which act as feature extractors, are selected as candidates from the wild. Given an input video {{formula:c553ddb9-8fbf-42df-8469-7e75fbac32b0}} , features are generated by these pretrained models, whose weights are frozen. This significantly reduces the training cost of multiple heavy pretrained models. According to the training paradigm of pretrained models, these features may contain quality-related information (e.g., , content, distortions, and motion). However, since factors that may affect quality vary in different videos, the correlation between the quality of different videos and these features is also different. Second, to adaptively capture desired quality-related features sample-by-sample during training, the proposed QAM is used to raise dynamic weights for feature aggregation. An extra sparsity constraint is attached to the distribution of these gating weights, promoting attention to more critical and relevant features for quality representation. Then the video quality feature can be obtained by a weighted summation. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner. During inference, only the optimized VQA model is used, reducing the computational cost largely. More details will be provided below.\n",
      "\n",
      "\n",
      "Videos sampled from the YouTube-UGC dataset {{cite:9be8207}} and their corresponding labeled MOS, ranging from 1.0 to 5.0. It can be seen that video quality is affected by various aspects, including content, distortions, and motion.\n",
      "{{figure:7ce08525-212e-4804-9184-79dc25fdca7a}}\n",
      "\n",
      "Inspired by the current success of the “pretraining and fine-tuning” paradigm in deep learning {{cite:e64e98e}}, {{cite:06016b9}}, {{cite:c682601}}, we aim to utilize in-the-wild pretrained models to benefit VQA from diverse aspects of the video, in order to enhance better understanding of video quality and enable personalized treatments to improve it. We contemplate the choice of pretrained models through the lens of multiple factors, as shown in Fig. REF , that may impact the quality of videos as follows:\n",
      "\n",
      "\n",
      "\n",
      "Content. Human judgments of visual quality are content-dependent according to previous studies {{cite:7da4179}}, {{cite:80cfe88}}. When a video is visually appealing, engaging, and relevant to the viewer's interests (e.g., cute puppy and beautiful scenery), it can capture their attention and make them more receptive to the video's content. In contrast, if a video is dull, uninteresting, or irrelevant (e.g., black screen and messy corners), viewers are more likely to rate low quality. Introducing models pretrained on the task of object recognition (e.g., EfficientNet, Swin Transformer) may benefit VQA.\n",
      "\n",
      "Distortion. In addition to content, distortions introduced during the phase of video capturing and compression also determine the video quality {{cite:6b6881a}}. Thus a pretrained model that has been trained on a dataset of images or videos (e.g., ImageNet, Kinetics-400) with compression artifacts will have learned to identify the specific patterns and features that are associated with compression artifacts, such as blockiness, blurriness, or pixelation.\n",
      "\n",
      "Motion. Unlike the image scenario, motion blur can significantly affect the quality of videos {{cite:3d9999d}}, {{cite:08d1075}}. It occurs when there is rapid motion, and the camera or objects in the scene are moving too quickly for the camera's shutter to capture. A pretrained action recognition model (e.g., SlowFast, TimeSformer) may detect specific actions or movements such as running, jumping, or throwing. These can be useful for analyzing the amount of motion or by looking for specific visual cues that are associated with motion blur, such as streaking around the edges of moving objects.\n",
      "\n",
      "\n",
      "\n",
      "Responses of different pretrained models to synthetic sequences generated by distortions (i.e., compression, sharpness). The correlation of SRCC is computed according to distortion degrees. It is evident that pretrained models may detect certain types of distortions, but their ability to perceive distortion varies across models.\n",
      "{{figure:e3af1672-5720-4013-bd7b-5b4ff0a7008c}}\n",
      "\n",
      "However, it is important to note that individual pretrained models may not be able to identify all types of quality-related factors, or may not be as accurate in identifying certain types. Some evidences are given in Fig. REF . When encountering different types of distortions, there will be obvious differences in the perception ability of the pretrained model. In detail, ConvNext-base (SRCC=0.968) outperforms EfficientNet-b7 (SRCC=0.038) when facing compression. When it comes to sharpness, EfficientNet-b7 performs better. Therefore, it is important to use a diverse set of models and combine their results to get a more robust assessment. Thus, we propose to construct a pool with a large diversity of candidate models, considering the following aspects:\n",
      "\n",
      "\n",
      "\n",
      "Architecture. The efficacy of a network architecture (e.g., CNN, Transformer) hinges upon its capacity to assimilate and convey information. A well-crafted architecture can discern finer details and patterns in the input video, while also influencing the manner in which spatial and temporal information, containing quality-related features, is processed.\n",
      "\n",
      "Pretrained pretext task. The type of supervision in the pretext task has an impact on the ability of the pretrained model to VQA tasks. When the distribution of data is similar, a supervised pretext task may lead to superior performance. Conversely, self-supervised pretext tasks, where the model is trained on unlabeled data, may facilitate better generalization when confronting unfamiliar VQA domains.\n",
      "\n",
      "Pretrained dataset. Pretrained datasets on a large scale can be advantageous to VQA by providing diverse content, distortion, and motion-related data. A desired pretrained dataset should include a wide range of categories that closely resemble real-world scenarios, as well as other multi-modal information that can aid in describing video quality. For instance, the WebImageText {{cite:c682601}} dataset, which combines text and images, can be helpful in this regard.\n",
      "\n",
      "\n",
      "\n",
      "Based on the above considerations, in this paper, we select several pretrained models that obtain top performance in their original fields, including (1) EfficientNet-b7 {{cite:d48f3ca}} trained on ImageNet-1k {{cite:bad8aed}}, (2) ir-CSN-152 {{cite:f69f321}} trained on Sports-1M {{cite:fbaab96}}, (3) CLIP trained on WebImageText {{cite:c682601}}, (4) Swin Transformer Base {{cite:942a083}} trained on ImageNet-21k {{cite:bad8aed}}, (5) TimeSformer {{cite:1cd3326}} trained on Kinetics-400 {{cite:dd352e3}}, (6) Video Swin Transformer Base {{cite:3b57340}} trained on Kinetics-600 {{cite:f2782ba}}, and (7) SlowFast {{cite:55db95f}} trained on Kinetics-400.\n",
      "\n",
      "\n",
      "As the distribution of content and distortions in videos can be quite complex, a static combination of pretrained models may not always yield optimal performance. In order to adaptively capture the diversity and complementary information from different pretrained models, we propose a Quality-aware Acquisition Module (QAM). It takes extracted features from various pretrained models as input and produces a consolidated feature as output for the ultimate representation of quality. The computational process can be partitioned into two main parts. The first part is responsible for transforming the extracted features initially into a uniform feature dimension to enable subsequent aggregation. Structurally, this transformation block comprises two fully-connected layers followed by a normalization layer and a GELU activation layer. The second part generates gating weights {{formula:5fbebeaa-6e29-429f-af6d-7028234a6145}}  to control the aggregation process. The gating network takes the concatenated feature vector as input and outputs a set of gating weights that represent the relative contribution of each pretrained model to the final quality representation. Structurally, this gating network is stacked using a fully-connected layer and a sigmoid layer. Then the quality representation {{formula:62670a2b-af0d-4d99-9e72-2692062d8936}}  can be obtained by a weighted sum according to the gating weights. Given the extracted features by different pretrained models {{formula:4e5c4529-850d-4f24-85ea-b1e6c1b2641b}} , these procedures can be noted as:\n",
      "{{formula:6c571f1d-29e5-4dd9-ba12-c75931535cf1}} \n",
      "\n",
      "\n",
      "where {{formula:1a4fa986-fed6-46a0-ad53-e32075b16491}}  denotes the mapping function for the {{formula:6938802c-1c0a-4d61-a681-68041a1f3c24}} -th transformation block, and {{formula:7e3de11c-a75e-4bac-ba2f-d235480454f2}}  represents the mapping function for the gating network. And {{formula:b5d1a473-8334-4e62-962b-cfe026c2de46}}  is the number of aligned feature dimensions.\n",
      "\n",
      "\n",
      "In addition, to emphasize the importance of critical features and enhance the generalization ability, we propose to impose a sparsity constraint as a regularization on the distribution of gating weights. The {{formula:4f78fa84-5998-4285-9d7e-f60e003beb79}}  loss is utilized to penalize non-zero weights resulting in more weights near zero. This constraint can be written as:\n",
      "{{formula:6b11ca0a-9bbf-49c3-aad9-66d13b5dbba7}} \n",
      "\n",
      "\n",
      "In this way, QAM allows for capturing a broader range of quality-related features, thereby enabling better adaptation to various types of video content, distortions, or movement. Then the aggregated feature is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:1f2628ca-68ff-4f09-a605-753c1f0c7044}} .\n",
      "\n",
      "\n",
      "In practical scenarios, using these large pretrained models for inference can be computationally expensive. To reduce the computational cost and increase flexibility, we propose to use knowledge distillation {{cite:3ff8959}} to transfer the knowledge from large and complex models to a lightweight VQA model.\n",
      "\n",
      "\n",
      "In this paper, a Video Swin Transformer-Tiny {{cite:3b57340}} is selected as the backbone. For an input video {{formula:49738d73-40f7-4464-ac8d-9ec9eff4df6b}} , the quality representation can be achieved by {{formula:259222d6-7c40-4718-9b20-0ec19a8fc322}} , where {{formula:85ae2fac-e0db-4930-ac5d-baa57705ad3e}}  represents the mapping function of the lightweight backbone, and {{formula:82aa382a-efba-4170-bd5f-d80a50f432a1}} . Then {{formula:ed99ea9d-a43a-4fcd-bf92-b424f31da861}}  is sent into a regression head, which is a single fully-connected layer, for quality prediction, resulting in {{formula:f6172209-4096-47e0-aad6-1a9b172623e6}} . Note that both {{formula:cf3e1bd5-75c4-4a7d-b209-48f578459c8b}}  and {{formula:92e2e32a-bfab-42db-912f-27dd5c5391de}}  are supervised by the labeled MOS using a smooth {{formula:1033e1d1-4365-4d1e-8ec4-3519b2b975ea}}  loss. Additionally, we apply a similarity consistency in knowledge distillation between {{formula:46d43316-7075-4c76-b0c3-1b1bb9a49b48}}  and {{formula:5d9433c4-bb4c-42d7-be06-a7d148d989b8}} . This allows the VQA model to simulate the robust quality representation generated by diverse pretrained models, further enhancing its performance. Given the labeled MOS {{formula:1d2e4ebc-fefb-46b0-9c43-2a9324db9bcd}} , the regression loss for pretrained models can be noted as:\n",
      "{{formula:f6db4960-52c2-445e-b97e-07032e4ee4df}} \n",
      "\n",
      "\n",
      "And the regression loss for the lightweight VQA model {{formula:48f98f1b-4648-49eb-bc17-0d581a35d692}}  share the same formulation. A {{formula:b8d641e6-56f4-479a-962e-18a5f2f0280f}}  loss is used for the process of knowledge distillation, which can be written as:\n",
      "{{formula:da4128b2-ca63-4245-b9e0-17d09737c759}} \n",
      "\n",
      "\n",
      "The whole optimization objective can be formulated as:\n",
      "{{formula:9c235282-5762-4e7a-8779-65f819b582d0}} \n",
      "\n",
      "\n",
      "where {{formula:edd3ecf5-e9ba-4766-805c-6c6d9c365904}}  is a balancing weight for knowledge distillation, and {{formula:cccc0ef1-171b-45e8-800e-aa911b903103}}  is a hyper-parameter to balance the level of sparsity.\n",
      "\n",
      "\n",
      "Our method is evaluated on three widely-adopted public NR-VQA datasets, including KoNViD-1k {{cite:24b4d04}}, LIVE-VQC {{cite:8f3eb86}}, and YouTube-UGC {{cite:9be8207}}. Mean opinion scores (MOS) are provided along with training videos.\n",
      "Specifically, KoNViD-1k contains 1,200 videos that are fairly filtered from a large public video dataset YFCC-100M. The time duration of the video is 8 seconds. And these videos have a frame rate of 24/25/30 FPS and a resolution of {{formula:88c19727-b56a-4f6d-8e99-0aec9bb05750}} .\n",
      "LIVE-VQC consists of 585 videos with complex authentic distortions, which are captured by 80 users using 101 different devices, ranging from 240P to 1080P.\n",
      "YouTube-UGC has 1,380 UGC videos sampled from YouTube with a duration of 20 seconds and resolutions from 360P to 4K.\n",
      "All these datasets contain no pristine videos, thus only NR methods can be evaluated on them.\n",
      "Following {{cite:c6c0872}}, we split all the dataset into 80% training videos and 20% testing videos randomly.\n",
      "\n",
      "\n",
      "Spearman’s Rank-Order Correlation Coefficient (SRCC) and Pearson’s Linear Correlation Coefficient (PLCC) are selected as metrics to measure the monotonicity and accuracy, respectively. They are in the range of 0.0 to 1.0, and larger values indicate better results. Besides, the mean average of PLCC and SRCC is also reported as a comprehensive criterion.\n",
      "\n",
      "\n",
      "Our method is implemented based on PyTorch {{cite:2e556d9}} and MMAction2 {{cite:a718e63}}. All experiments are conducted on 4 NVIDIA V100 GPUs.\n",
      "For all datasets, we select EfficientNet-b7, ir-CSN-152, CLIP, Swin Transformer Base, TimeSformer, Video Swin Transformer Base and SlowFast as candidate pretrained models, and choose the Video Swin Transformer Tiny as the lightweight VQA model.\n",
      "Frames are sampled in each video with a fixed temporal step to form a clip input. For frame-wise models (e.g., EfficientNet, CLIP), the feature representation can be calculated through the average features of all frames. For video clip-based models (e.g., SlowFast, ir-CSN-152), the extracted features can be used directly for the video representation.\n",
      "For KoNViD-1k, we sample 16 frames with a frame interval of 2.\n",
      "As videos in LIVE-VQC and YouTube-UGC have longer time durations, we sample 64 frames with an interval of 2, and 32 frames with an interval of 8, respectively.\n",
      "Since most augmentations will introduce extra interference to the quality of videos (e.g., resize, color jitter) {{cite:dbe6597}}, we only choose the center crop to produce inputs with a resolution of {{formula:f65cb7a8-aa8f-40f4-8620-58c4befba7dd}} .\n",
      "During the optimization procedure, we use the AdamW optimizer with a weight decay of 2e-2.\n",
      "A cosine annealing scheduler with a warmup of 2 epochs is adopted to control the learning rate. The initial learning rate is 1e-3. And {{formula:90617cdd-14f1-4198-92b5-f5cc82cb932a}}  is 0.1 by default. {{formula:d9f6a3fd-7cd3-47a3-a9a4-22cb40b1ee3d}}  is set to 0.8. {{formula:20e7ac50-f536-450f-aef3-b5f454e999f9}}  is set to 32. The batch size of the input is set to 1. All models are trained for 60 epochs. And the checkpoint generated by the last iteration is used for evaluation.\n",
      "For inference, we follow a similar procedure as {{cite:445c847}} by using {{formula:32f86a77-28dd-4e84-9816-ca280897ad39}}  views. In the procedure, a video is uniformly sampled as 4 clips in the temporal dimension, and for each clip, the shorter spatial side is scaled to 256 pixels and we take 5 crops in the four corners and the center. The final score is computed as the average score of all the views. The average result of 10 repeat runs with different random splits is used as the final score for the experiments in Tab. REF .\n",
      "\n",
      "\n",
      "Comparisons with SOTA methods. The up arrow “{{formula:9cdebb9e-6b3f-4b6f-b9ab-97667fb7a327}} \" means that a larger value indicates better performance. The mark “-” means the results are not reported originally. Mark “*\" indicates that the model uses external QA data for training. The best and second best performances are highlighted and underlined. Ada-DQA outperforms existing SOTA methods by large margins on three datasets. We also report the performance of the Video Swin Tiny without the assistance of diverse pretrained models.\n",
      "2*Method\n",
      "KoNViD-1k\n",
      "LIVE-VQC\n",
      "YouTube-UGC\n",
      "\n",
      "SRCC{{formula:d8a7d31e-d904-4b1e-8f77-b44182b0adc0}}\n",
      "PLCC{{formula:80f9cab4-f37a-4236-a62f-f73bf32c5751}}\n",
      "Mean\n",
      "SRCC{{formula:908447c2-5ea5-403f-8de1-ecc8f566c8e9}}\n",
      "PLCC{{formula:c5d0133b-0266-4c10-a568-387e30798da5}}\n",
      "Mean\n",
      "SRCC{{formula:00fa2371-042a-46d0-a7df-1eceae187c4c}}\n",
      "PLCC{{formula:01663790-9e8e-4df9-9f95-902a56657579}}\n",
      "Mean\n",
      "VIIDEO {{cite:0832a8c}}\n",
      "0.2980\n",
      "0.3030\n",
      "0.3005\n",
      "0.0332\n",
      "0.2164\n",
      "0.1248\n",
      "0.0580\n",
      "0.1534\n",
      "0.1057\n",
      "NIQE {{cite:c619c6d}}\n",
      "0.5417\n",
      "0.5530\n",
      "0.5474\n",
      "0.5957\n",
      "0.6286\n",
      "0.6122\n",
      "0.2379\n",
      "0.2776\n",
      "0.2578\n",
      "BRISQUE {{cite:18dcc02}}\n",
      "0.654\n",
      "0.626\n",
      "0.640\n",
      "0.592\n",
      "0.638\n",
      "0.615\n",
      "0.382\n",
      "0.395\n",
      "0.389\n",
      "VSFA {{cite:7da4179}}\n",
      "0.755\n",
      "0.744\n",
      "0.750\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "TLVQM {{cite:1af68c6}}\n",
      "0.7729\n",
      "0.7688\n",
      "0.7709\n",
      "0.7988\n",
      "0.8025\n",
      "0.8807\n",
      "0.6693\n",
      "0.6590\n",
      "0.6642\n",
      "RIRNet {{cite:04485d0}}\n",
      "0.7755\n",
      "0.7812\n",
      "0.7784\n",
      "0.7713\n",
      "0.7982\n",
      "0.7848\n",
      "-\n",
      "-\n",
      "-\n",
      "MDTVSFA {{cite:1ad644b}}\n",
      "0.7812\n",
      "0.7856\n",
      "0.7834\n",
      "0.7382\n",
      "0.7728\n",
      "0.7555\n",
      "-\n",
      "-\n",
      "-\n",
      "RIRNet+CSPT {{cite:b519bb8}}\n",
      "0.8008\n",
      "0.8062\n",
      "0.8035\n",
      "0.7989\n",
      "0.8194\n",
      "0.8092\n",
      "-\n",
      "-\n",
      "-\n",
      "CoINVQ {{cite:80cfe88}}\n",
      "0.802\n",
      "0.816\n",
      "0.809\n",
      "-\n",
      "-\n",
      "-\n",
      "0.764\n",
      "0.767\n",
      "0.766\n",
      "RAPIQUE {{cite:f7fae73}}\n",
      "0.8031\n",
      "0.8175\n",
      "0.8103\n",
      "0.7548\n",
      "0.7863\n",
      "0.7706\n",
      "0.7591\n",
      "0.7684\n",
      "0.7638\n",
      "StarVQA {{cite:1c7237b}}\n",
      "0.812\n",
      "0.796\n",
      "0.804\n",
      "0.732\n",
      "0.808\n",
      "0.770\n",
      "-\n",
      "-\n",
      "-\n",
      "BVQA* {{cite:48735e5}}\n",
      "0.8362\n",
      "0.8335\n",
      "0.8349\n",
      "0.8412\n",
      "0.8415\n",
      "0.8414\n",
      "0.8312\n",
      "0.8194\n",
      "0.8253\n",
      "STDAM {{cite:c6c0872}}\n",
      "0.8448\n",
      "0.8415\n",
      "0.8432\n",
      "0.7931\n",
      "0.8204\n",
      "0.8068\n",
      "0.8341\n",
      "0.8297\n",
      "0.8319\n",
      "DisCoVQA {{cite:ebacaf2}}\n",
      "0.847\n",
      "0.847\n",
      "0.847\n",
      "0.820\n",
      "0.826\n",
      "0.823\n",
      "-\n",
      "-\n",
      "-\n",
      "FastVQA {{cite:f3075fd}}\n",
      "0.859\n",
      "0.855\n",
      "0.857\n",
      "0.823\n",
      "0.844\n",
      "0.834\n",
      "-\n",
      "-\n",
      "-\n",
      "Video Swin Tiny\n",
      "0.8316\n",
      "0.8694\n",
      "0.8505\n",
      "0.8335\n",
      "0.8316\n",
      "0.8326\n",
      "0.8566\n",
      "0.8499\n",
      "0.8533\n",
      "Ada-DQA\n",
      "0.8651\n",
      "0.8831\n",
      "0.8741\n",
      "0.8591\n",
      "0.8587\n",
      "0.8589\n",
      "0.8729\n",
      "0.8800\n",
      "0.8765\n",
      "{{formula:a1eede44-0117-469e-89b9-f15da50d4c96}}  than the previous best\n",
      "+0.6%\n",
      "+2.8%\n",
      "+1.7%\n",
      "+1.79%\n",
      "+1.72%\n",
      "+1.75%\n",
      "+3.88%\n",
      "+5.03%\n",
      "+4.46%\n",
      "{{formula:a6bb876c-531a-40c9-afe7-f5d8ea915687}}  than w/o pretrained models\n",
      "+3.35%\n",
      "+1.37%\n",
      "+2.36%\n",
      "+2.56%\n",
      "+2.71%\n",
      "+2.63%\n",
      "+1.63%\n",
      "+3.01%\n",
      "+2.32%\n",
      "{{table:d7264495-8d09-4a2f-ab91-921442bf5f78}}\n",
      "\n",
      "We report the SRCC and PLCC performance with current SOTA methods on KoNViD-1k, LIVE-VQC, and YouTube-UGC.\n",
      "As shown in Tab. REF , our method achieves new state-of-the-art results on all these datasets without using extra training data of QA.\n",
      "Some observations can also be found through these results.\n",
      "We can observe those deep learning-based methods outperform the traditional hand-crafted method (e.g., VIIDEO, NIQE) largely.\n",
      "Besides, within deep learning-based methods, VQA methods produce much better performance than IQA methods (e.g., BRISQUE). Since there exist many temporal-distributed distortions in these datasets, VQA models can capture this temporal information.\n",
      "Ada-DQA outperforms StarVQA, which builds a Transformer model to capture spatiotemporal information, in large margins.\n",
      "BVQA incorporates extra training data from other QA datasets for the feature extractor. And our method still outperforms it without using external training data of QA (+2.89% of SRCC, and + 4.96% of PLCC in KoNViD-1k). This shows the advantage of leveraging quality-related knowledge from pretrained models.\n",
      "Compare with the current best method of STDAM, DiscoVQA, and FastVQA, Ada-DQA improves the best performance in large margins to {{formula:6f5c3ff3-bb6d-4fb0-a8bc-2a1c99c97f35}}  of SRCC in KoNVid-1k (+0.6%), {{formula:6ddb6487-ef20-43d8-af75-df8bc01255e5}}  of SRCC in LIVE-VQC (+1.79%) and {{formula:52cb05d2-3405-4e58-9a6f-f51d25476aa2}}  of SRCC in YouTube-UGC (+3.88%). The accuracy and consistency of prediction results have been significantly improved.\n",
      "\n",
      "\n",
      "Experimental analysis on different numbers of selected pretrained models with or without the usage of sparsity constraint in QAM. SRCC results are reported. The best result under the setting of with or without is bolded.\n",
      "Number\n",
      "QAM\n",
      "KoNViD-1k\n",
      "LIVE-VQC\n",
      "YouTube-UGC\n",
      "3\n",
      "{{formula:221dcedf-f5a4-4aaf-80a0-269d15f534be}}\n",
      "0.8517\n",
      "0.8432\n",
      "0.8630\n",
      "4\n",
      "{{formula:1b371620-17ef-403e-9fda-3e25f7c434a7}}\n",
      "0.8613\n",
      "0.8510\n",
      "0.8659\n",
      "5\n",
      "{{formula:e5bb12eb-de29-46bb-be7a-2f57fdab0797}}\n",
      "0.8601\n",
      "0.8490\n",
      "0.8591\n",
      "6\n",
      "{{formula:d8a70087-e5e2-4795-a70a-ae3803ab7f30}}\n",
      "0.8615\n",
      "0.8448\n",
      "0.8589\n",
      "7\n",
      "{{formula:88da022f-02c7-45d4-9301-e9e20bb5773f}}\n",
      "0.8573\n",
      "0.8459\n",
      "0.8621\n",
      "3\n",
      "\n",
      "0.8432\n",
      "0.8433\n",
      "0.8621\n",
      "4\n",
      "\n",
      "0.8623\n",
      "0.8514\n",
      "0.8695\n",
      "5\n",
      "\n",
      "0.8621\n",
      "0.8546\n",
      "0.8679\n",
      "6\n",
      "\n",
      "0.8645\n",
      "0.8542\n",
      "0.8644\n",
      "7\n",
      "\n",
      "0.8651\n",
      "0.8591\n",
      "0.8729\n",
      "8\n",
      "\n",
      "0.8641\n",
      "0.8560\n",
      "0.8711\n",
      "{{table:9ad28e89-2a09-4b7b-a6b7-58d313f28dc1}}\n",
      "\n",
      "Experiments using a single pretrained model for knowledge distillation. The SRCC results in three datasets are reported. The best results in different datasets are bolded.\n",
      "Pretrained Model\n",
      "KoNViD-1k\n",
      "LIVE-VQC\n",
      "YouTube-UGC\n",
      "w/o\n",
      "0.8316\n",
      "0.8335\n",
      "0.8566\n",
      "EfficientNet-b7\n",
      "0.8412\n",
      "0.8495\n",
      "0.8587\n",
      "Video Swin Base\n",
      "0.8390\n",
      "0.8173\n",
      "0.8603\n",
      "Swin Base\n",
      "0.8391\n",
      "0.8475\n",
      "0.8497\n",
      "TimeSformer\n",
      "0.8409\n",
      "0.8302\n",
      "0.8618\n",
      "CLIP\n",
      "0.8404\n",
      "0.8341\n",
      "0.8608\n",
      "ir-CNS-152\n",
      "0.8458\n",
      "0.8521\n",
      "0.8518\n",
      "SlowFast\n",
      "0.8423\n",
      "0.8041\n",
      "0.8523\n",
      "{{table:05f0fe78-77f1-4f23-82a6-1cb1620402b6}}\n",
      "\n",
      "To investigate the impact of the number of pretrained models, we performed experiments by reducing the number of models from 7 to 3. As depicted in Tab. REF , increasing the number of models does not always lead to better performance without the use of sparsity constraints in QAM. With the help of sparsity constraint, the model can achieve continuous improvement as the number of pretrained models increases. However, adding more models beyond 8 (introducing an extra model of ViT Base) does not yield any further improvements. This may indicate that the quality-related information provided by the pretrained models has reached a saturation point. Therefore, we set the number of pretrained models to 7 in our experiments. In these experiments, the pretrained model is randomly selected and removed.\n",
      "\n",
      "\n",
      "Moreover, we also conduct experiments solely utilizing a singular pretrained model to demonstrate the disparity among models. As depicted in Tab. REF , a solitary pretrained model cannot consistently attain optimal outcomes across all VQA datasets. For instance, while CLIP excels on YouTube-UGC, it falls considerably short on the other two datasets. We posit that this is influenced by the correlation between pretrained models, such as pre-text task, dataset, architecture, and VQA tasks. Additionally, the results obtained by utilizing singular pretrained models are notably distant from the state-of-the-art. These findings substantiate that a solitary pretrained model is inadequate for diverse application scenarios, and leveraging a variety of pretrained models is imperative.\n",
      "\n",
      "\n",
      "Experiments on different distillation losses.\n",
      "distillation loss\n",
      "SRCC{{formula:dd46be6a-65c3-43f3-946d-92949282cc11}}\n",
      "PLCC{{formula:eda80638-c5e0-4244-a398-8894e6f85a54}}\n",
      "{{formula:31f9fd54-46b1-4afd-9db2-1b91ddfc43e0}}\n",
      "0.8651\n",
      "0.8831\n",
      "{{formula:c3bcc9df-6e3f-4bf1-b323-74173e83d89d}}\n",
      "0.8092\n",
      "0.8310\n",
      "Jesen-Shannon\n",
      "0.8455\n",
      "0.8646\n",
      "{{table:3b4dbfb8-49c7-4dda-9e65-5f6f6d95a4d3}}\n",
      "\n",
      "Experiments on different knowledge distillation losses are performed in KoNViD-1k, including the {{formula:a767cdb6-563d-46b0-8abe-24b0e712c561}}  loss, the {{formula:3740432c-1fce-4101-b4b5-e072e1b6b55a}}  loss and the Jesen-Shannon (JS) loss {{cite:9306c46}}. As shown in Tab. REF , the {{formula:84f55c13-be98-4d51-be6a-45e7140ec7d6}}  loss owns the best performance in transferring aggregated features by multiple teacher models.\n",
      "\n",
      "\n",
      "We conduct experiments to show how the hyper-parameters {{formula:ed351050-05c6-4413-9435-db5841c75807}}  and {{formula:1b42aa12-0802-4f1e-b163-0319b4aa0451}}  in Equ. REF  will influence the final results in KoNViD-1k. The results are listed in Tab. REF  and REF . When {{formula:66e76ccc-bbaf-42ce-a3af-6b78dbfec06e}}  is 0.1, and {{formula:20053bad-d395-4fd8-9a94-ba68fd319ff9}}  is 0.8, which are used in our experiments, the best performance can be obtained.\n",
      "\n",
      "\n",
      "Selection of the hyper-parameters of {{formula:f16bc710-2160-46d5-b84a-64fa45cfb291}}  and {{formula:fd5fe218-5800-4333-b038-da96402e6a57}} .\n",
      "{{formula:29169695-a810-4688-9399-808f691d97bc}}\n",
      "SRCC{{formula:db3a9c0b-5328-4299-84d7-f440e728d271}}\n",
      "PLCC{{formula:1e9c8d41-bc4b-4137-85cc-218df131f13d}}\n",
      "0.1\n",
      "0.8651\n",
      "0.8831\n",
      "0.2\n",
      "0.8462\n",
      "0.8663\n",
      "0.5\n",
      "0.8406\n",
      "0.8698\n",
      "1.0\n",
      "0.8320\n",
      "0.8549\n",
      "{{table:ddd2f8c3-e328-4637-b41f-6bbaccf59efa}}{{formula:7390f3e9-af3d-4608-acc3-2f8eb18dbb6c}}\n",
      "SRCC{{formula:414262e2-d916-4698-abf5-34c10fc1b10f}}\n",
      "PLCC{{formula:a70efd62-92c7-4ad4-9534-15c65066653b}}\n",
      "0.8\n",
      "0.8651\n",
      "0.8831\n",
      "0.2\n",
      "0.8449\n",
      "0.8671\n",
      "0.5\n",
      "0.8528\n",
      "0.8716\n",
      "1.0\n",
      "0.8602\n",
      "0.8780\n",
      "{{table:075d524d-ee5a-49ae-a3f0-bf8151a446a6}}\n",
      "\n",
      "[t]0.23\n",
      "hyper-parameter {{formula:49c4784c-769b-446c-8656-ababa402404d}}[t]0.23\n",
      "hyper-parameter {{formula:7200fc51-23ac-40a3-a533-1b2f724d97a6}} .{{table:de0d28e6-3c30-41b2-8264-b12f873fb795}}\n",
      "\n",
      "Contributions of different pretrained models by {{formula:277c600d-4c40-49ad-8127-54194614bd53}} .\n",
      "Model\n",
      "{{formula:c4f89f23-5b45-4835-99b1-dd94ba22dfaa}}  for LQ videos\n",
      "{{formula:d8aee694-bab5-4b79-a1ec-b094d047475f}}  for HQ videos\n",
      "EfficientNet-b7\n",
      "0.1303\n",
      "0.7208\n",
      "Video Swin Base\n",
      "0.2021\n",
      "0.0455\n",
      "Swin Base\n",
      "0.1676\n",
      "0.6805\n",
      "TimeSformer\n",
      "0.4679\n",
      "0.2317\n",
      "CLIP\n",
      "0.1279\n",
      "0.3567\n",
      "ir-CNS-152\n",
      "0.8690\n",
      "0.2788\n",
      "SlowFast\n",
      "0.4210\n",
      "0.0018\n",
      "{{table:a1a6fc9d-c459-41de-bec4-a56b500b92d4}}\n",
      "\n",
      "To investigate the contribution, we analyze the gating weights {{formula:ab91ab50-65bf-4e4f-bc2c-6d61ca1dcfb9}}  generated by the QAM in KoNViD-1k. The statistical average scores for different models are calculated.\n",
      "We count responses of low-quality (LQ, MOS<3.5) and high-quality (HQ, MOS>3.5) videos in Tab. REF . It can be seen that for LQ videos, models that can provide distortion and motion-related information (e.g., ir-CSN-152) have larger weights; for HQ videos, models that can provide content-related one (e.g., EfficientNet-b7) own larger weights.\n",
      "\n",
      "\n",
      "We compare the #Params, #FLOPs, and SRCC of Ada-DQA over some SOTA methods whose models are available: Ada-DQA (29M, 88T, 0.8651), MDTVSFA (24M, 168T, 0.7812), StarVQA (121M, 75T, 0.812), BVQA (24M, 240T, 0.8362). With the help of pretrained models during training, Ada-DQA obtains higher results with a fair cost during inference.\n",
      "\n",
      "\n",
      "Some representative videos in KoNViD-1k are selected to show the performance improvement brought by Ada-DQA. Visualization of the feature attention maps using GradCAM {{cite:ecf1e0d}} are shown in Fig. REF . After introducing the adaptive acquisition strategy, Ada-DQA generates more accurate results and the attention maps highlight more quality-related regions.\n",
      "For instance, in the first video clip, attention from the vast surface of the ocean (left) is transferred to the sailboat, with some attention on ocean waves kept (right). Ada-DQA focuses on areas more related to the action (boat sailing) or giving clues about the perceptual quality (edges of waves).\n",
      "\n",
      "\n",
      "Comparison of predictions and attention visualizations. For each video, video frames (left) and attention maps (right) redbefore and greenafter using Ada-DQA are illustrated.\n",
      "{{figure:1658c38e-34bd-4fc9-9e2a-02fd2548f5cc}}\n",
      "\n",
      "To address the issue of insufficient training data in VQA, this paper analyzes the entire spectrum of video distribution diversity that impacts quality and proposes the Ada-DQA framework, which employs a range of diverse pretrained models to improve quality representation. With Ada-DQA, it becomes possible to extract critical and relevant features generated by different frozen pretrained models adaptively. Experimental results on three mainstream NR-VQA benchmarks show the effectiveness in the context of limited data. Thorough analysis and ablation studies also validate the necessity of each component. This work hopes to inspire future research that leverages pretrained models to aid in a wider array of tasks.\n",
      "\n",
      "\n",
      "This research was partly supported by the National Key R&D Program of China (Grant No. 2020AAA0108303), and Shenzhen Science and Technology Project (Grant No. JCYJ20200109143041798) and Shenzhen Stable Supporting Program (No. WDZC20200820200655001) and Shenzhen Key Laboratory of next-generation interactive media innovative technology (No. ZDSYS20210623092001004).\n",
      "\n",
      "\n",
      "Anurag Arnab, Mostafa\n",
      "Dehghani, Georg Heigold, Chen Sun,\n",
      "Mario Lucic, and Cordelia Schmid.\n",
      "2021.\n",
      "ViViT: A Video Vision Transformer.\n",
      "CoRR abs/2103.15691\n",
      "(2021).\n",
      "Gedas Bertasius, Heng\n",
      "Wang, and Lorenzo Torresani.\n",
      "2021.\n",
      "Is Space-Time Attention All You Need for Video\n",
      "Understanding?. In ICML,\n",
      "Vol. 139. PMLR,\n",
      "813–824.\n",
      "João Carreira, Eric\n",
      "Noland, Andras Banki-Horvath, Chloe\n",
      "Hillier, and Andrew Zisserman.\n",
      "2018.\n",
      "A Short Note about Kinetics-600.\n",
      "CoRR abs/1808.01340\n",
      "(2018).\n",
      "Aaron Chadha and Yiannis\n",
      "Andreopoulos. 2021.\n",
      "Deep Perceptual Preprocessing for Video Coding. In\n",
      "CVPR. IEEE,\n",
      "14852–14861.\n",
      "Kuan-Ta Chen, Chi-Jui\n",
      "Chang, Chen-Chi Wu, Yu-Chun Chang,\n",
      "and Chin-Laung Lei. 2010.\n",
      "Quadrant of euphoria: a crowdsourcing platform for\n",
      "QoE assessment.\n",
      "IEEE Netw. 24,\n",
      "2 (2010), 28–35.\n",
      "https://doi.org/10.1109/MNET.2010.5430141\n",
      "Pengfei Chen, Leida Li,\n",
      "Lei Ma, Jinjian Wu, and\n",
      "Guangming Shi. 2020b.\n",
      "RIRNet: Recurrent-In-Recurrent Network for Video\n",
      "Quality Assessment. In ACM MM.\n",
      "ACM, 834–842.\n",
      "Pengfei Chen, Leida Li,\n",
      "Jinjian Wu, Weisheng Dong, and\n",
      "Guangming Shi. 2022.\n",
      "Contrastive Self-Supervised Pre-Training for Video\n",
      "Quality Assessment.\n",
      "IEEE Trans. Image Process.\n",
      "31 (2022), 458–471.\n",
      "Ting Chen, Simon\n",
      "Kornblith, Mohammad Norouzi, and\n",
      "Geoffrey E. Hinton. 2020a.\n",
      "A Simple Framework for Contrastive Learning of\n",
      "Visual Representations. In ICML,\n",
      "Vol. 119. PMLR,\n",
      "1597–1607.\n",
      "Yanjiao Chen, Kaishun Wu,\n",
      "and Qian Zhang. 2015.\n",
      "From QoS to QoE: A Tutorial on Video Quality\n",
      "Assessment.\n",
      "IEEE Commun. Surv. Tutorials\n",
      "17, 2 (2015),\n",
      "1126–1165.\n",
      "https://doi.org/10.1109/COMST.2014.2363139\n",
      "Shyamprasad Chikkerur,\n",
      "Vijay Sundaram, Martin Reisslein, and\n",
      "Lina J. Karam. 2011.\n",
      "Objective Video Quality Assessment Methods: A\n",
      "Classification, Review, and Performance Comparison.\n",
      "IEEE Trans. Broadcast.\n",
      "57, 2 (2011),\n",
      "165–182.\n",
      "Cisco. 2021.\n",
      "Cisco annual internet report white paper.\n",
      "https://www.cisco.com/c/en/us/solutions/collateral/\n",
      "executive-perspectives/annual-internet-report/white-paper-c11-741490.html.\n",
      "MMAction2 Contributors.\n",
      "2020.\n",
      "OpenMMLab's Next Generation Video Understanding\n",
      "Toolbox and Benchmark.\n",
      "https://github.com/open-mmlab/mmaction2.\n",
      "Jia Deng, Wei Dong,\n",
      "Richard Socher, Li-Jia Li,\n",
      "Kai Li, and Li Fei-Fei.\n",
      "2009.\n",
      "ImageNet: A large-scale hierarchical image\n",
      "database. In CVPR. IEEE\n",
      "Computer Society, 248–255.\n",
      "Joshua Peter Ebenezer,\n",
      "Zaixi Shang, Yongjun Wu,\n",
      "Hai Wei, and Alan C. Bovik.\n",
      "2020.\n",
      "No-Reference Video Quality Assessment Using\n",
      "Space-Time Chips. In MMSP.\n",
      "IEEE, 1–6.\n",
      "Christoph Feichtenhofer,\n",
      "Haoqi Fan, Jitendra Malik, and\n",
      "Kaiming He. 2019.\n",
      "SlowFast Networks for Video Recognition. In\n",
      "ICCV. 6201–6210.\n",
      "Bent Fuglede and\n",
      "Flemming Topsøe. 2004.\n",
      "Jensen-Shannon divergence and Hilbert space\n",
      "embedding. In ISIT.\n",
      "IEEE, 31.\n",
      "Franz Götz-Hahn,\n",
      "Vlad Hosu, Hanhe Lin, and\n",
      "Dietmar Saupe. 2021.\n",
      "KonVid-150k: A Dataset for No-Reference Video\n",
      "Quality Assessment of Videos in-the-Wild.\n",
      "IEEE Access 9\n",
      "(2021), 72139–72160.\n",
      "https://doi.org/10.1109/ACCESS.2021.3077642\n",
      "Jie Gu, Gaofeng Meng,\n",
      "Cheng Da, Shiming Xiang, and\n",
      "Chunhong Pan. 2019a.\n",
      "No-Reference Image Quality Assessment with\n",
      "Reinforcement Recursive List-Wise Ranking. In\n",
      "AAAI. AAAI Press,\n",
      "8336–8343.\n",
      "Jie Gu, Gaofeng Meng,\n",
      "Shiming Xiang, and Chunhong Pan.\n",
      "2019b.\n",
      "Blind image quality assessment via learnable\n",
      "attention-based pooling.\n",
      "Pattern Recognit. 91\n",
      "(2019), 332–344.\n",
      "Kaiming He, Haoqi Fan,\n",
      "Yuxin Wu, Saining Xie, and\n",
      "Ross B. Girshick. 2020.\n",
      "Momentum Contrast for Unsupervised Visual\n",
      "Representation Learning. In CVPR.\n",
      "Computer Vision Foundation / IEEE,\n",
      "9726–9735.\n",
      "Geoffrey E. Hinton, Oriol\n",
      "Vinyals, and Jeffrey Dean.\n",
      "2015.\n",
      "Distilling the Knowledge in a Neural Network.\n",
      "CoRR abs/1503.02531\n",
      "(2015).\n",
      "Tobias Hoßfeld,\n",
      "Christian Keimel, Matthias Hirth,\n",
      "Bruno Gardlo, Julian Habigt,\n",
      "Klaus Diepold, and Phuoc Tran-Gia.\n",
      "2014.\n",
      "Best Practices for QoE Crowdtesting: QoE Assessment\n",
      "With Crowdsourcing.\n",
      "IEEE Trans. Multim. 16,\n",
      "2 (2014), 541–558.\n",
      "https://doi.org/10.1109/TMM.2013.2291663\n",
      "Vlad Hosu, Franz Hahn,\n",
      "Mohsen Jenadeleh, Hanhe Lin,\n",
      "Hui Men, Tamás Szirányi,\n",
      "Shujun Li, and Dietmar Saupe.\n",
      "2017.\n",
      "The Konstanz natural video database (KoNViD-1k).\n",
      "In QoMEX. IEEE,\n",
      "1–6.\n",
      "Rui Hou, YunHao Zhao,\n",
      "Yang Hu, and Huan Liu.\n",
      "2020.\n",
      "No-reference video quality evaluation by a deep\n",
      "transfer CNN architecture.\n",
      "SPIC 83\n",
      "(2020), 115782.\n",
      "Andrej Karpathy, George\n",
      "Toderici, Sanketh Shetty, Thomas Leung,\n",
      "Rahul Sukthankar, and Li Fei-Fei.\n",
      "2014.\n",
      "Large-scale Video Classification with Convolutional\n",
      "Neural Networks. In CVPR.\n",
      "Will Kay, João\n",
      "Carreira, Karen Simonyan, Brian Zhang,\n",
      "Chloe Hillier, Sudheendra\n",
      "Vijayanarasimhan, Fabio Viola, Tim\n",
      "Green, Trevor Back, Paul Natsev,\n",
      "Mustafa Suleyman, and Andrew\n",
      "Zisserman. 2017.\n",
      "The Kinetics Human Action Video Dataset.\n",
      "CoRR abs/1705.06950\n",
      "(2017).\n",
      "Junjie Ke, Qifei Wang,\n",
      "Yilin Wang, Peyman Milanfar, and\n",
      "Feng Yang. 2021.\n",
      "MUSIQ: Multi-scale Image Quality Transformer.\n",
      "(October 2021),\n",
      "5148–5157.\n",
      "Jari Korhonen.\n",
      "2019.\n",
      "Two-Level Approach for No-Reference Consumer Video\n",
      "Quality Assessment.\n",
      "IEEE Trans. Image Process.\n",
      "28, 12 (2019),\n",
      "5923–5938.\n",
      "Jari Korhonen, Yicheng\n",
      "Su, and Junyong You. 2020.\n",
      "Blind Natural Video Quality Prediction via\n",
      "Statistical Temporal Features and Deep Spatial Features. In\n",
      "MM '20: The 28th ACM International Conference\n",
      "on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020,\n",
      "Chang Wen Chen, Rita\n",
      "Cucchiara, Xian-Sheng Hua, Guo-Jun\n",
      "Qi, Elisa Ricci, Zhengyou Zhang, and\n",
      "Roger Zimmermann (Eds.). ACM,\n",
      "3311–3319.\n",
      "https://doi.org/10.1145/3394171.3413845\n",
      "Bowen Li, Weixia Zhang,\n",
      "Meng Tian, Guangtao Zhai, and\n",
      "Xianpei Wang. 2021b.\n",
      "Blindly Assess Quality of In-the-Wild Videos via\n",
      "Quality-aware Pre-training and Motion Perception.\n",
      "CoRR abs/2108.08505\n",
      "(2021).\n",
      "Dingquan Li, Tingting\n",
      "Jiang, and Ming Jiang. 2019.\n",
      "Quality Assessment of In-the-Wild Videos. In\n",
      "ACM Multimedia. ACM,\n",
      "2351–2359.\n",
      "Dingquan Li, Tingting\n",
      "Jiang, and Ming Jiang.\n",
      "2021a.\n",
      "Unified Quality Assessment of in-the-Wild Videos\n",
      "with Mixed Datasets Training.\n",
      "IJCV 129,\n",
      "4 (2021).\n",
      "Liang Liao, Kangmin Xu,\n",
      "Haoning Wu, Chaofeng Chen,\n",
      "Wenxiu Sun, Qiong Yan, and\n",
      "Weisi Lin. 2022.\n",
      "Exploring the Effectiveness of Video Perceptual\n",
      "Representation in Blind Video Quality Assessment. In\n",
      "MM '22: The 30th ACM International Conference\n",
      "on Multimedia, Lisboa, Portugal, October 10 - 14, 2022,\n",
      "João Magalhães,\n",
      "Alberto Del Bimbo, Shin'ichi Satoh,\n",
      "Nicu Sebe, Xavier Alameda-Pineda,\n",
      "Qin Jin, Vincent Oria, and\n",
      "Laura Toni (Eds.). ACM,\n",
      "837–846.\n",
      "https://doi.org/10.1145/3503161.3547849\n",
      "Ze Liu, Yutong Lin,\n",
      "Yue Cao, Han Hu, Yixuan\n",
      "Wei, Zheng Zhang, Stephen Lin, and\n",
      "Baining Guo. 2021a.\n",
      "Swin Transformer: Hierarchical Vision Transformer\n",
      "Using Shifted Windows. In ICCV.\n",
      "10012–10022.\n",
      "Ze Liu, Jia Ning,\n",
      "Yue Cao, Yixuan Wei,\n",
      "Zheng Zhang, Stephen Lin, and\n",
      "Han Hu. 2021b.\n",
      "Video Swin Transformer.\n",
      "CoRR abs/2106.13230\n",
      "(2021).\n",
      "Anish Mittal,\n",
      "Anush Krishna Moorthy, and Alan Conrad\n",
      "Bovik. 2012.\n",
      "No-Reference Image Quality Assessment in the\n",
      "Spatial Domain.\n",
      "IEEE Trans. Image Process.\n",
      "21, 12 (2012),\n",
      "4695–4708.\n",
      "Anish Mittal, Michele A.\n",
      "Saad, and Alan C. Bovik.\n",
      "2016.\n",
      "A Completely Blind Video Integrity Oracle.\n",
      "IEEE Trans. Image Process.\n",
      "25, 1 (2016),\n",
      "289–300.\n",
      "Anish Mittal, Rajiv\n",
      "Soundararajan, and Alan C. Bovik.\n",
      "2013.\n",
      "Making a \"Completely Blind\" Image Quality\n",
      "Analyzer.\n",
      "IEEE SPL 20,\n",
      "3 (2013), 209–212.\n",
      "Adam Paszke, Sam Gross,\n",
      "Francisco Massa, Adam Lerer,\n",
      "James Bradbury, Gregory Chanan,\n",
      "Trevor Killeen, Zeming Lin,\n",
      "Natalia Gimelshein, Luca Antiga,\n",
      "Alban Desmaison, Andreas Köpf,\n",
      "Edward Z. Yang, Zachary DeVito,\n",
      "Martin Raison, Alykhan Tejani,\n",
      "Sasank Chilamkurthy, Benoit Steiner,\n",
      "Lu Fang, Junjie Bai, and\n",
      "Soumith Chintala. 2019.\n",
      "PyTorch: An Imperative Style, High-Performance Deep\n",
      "Learning Library. In NeurIPS.\n",
      "8024–8035.\n",
      "Alec Radford, Jong Wook\n",
      "Kim, Chris Hallacy, Aditya Ramesh,\n",
      "Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell,\n",
      "Pamela Mishkin, Jack Clark,\n",
      "Gretchen Krueger, and Ilya Sutskever.\n",
      "2021.\n",
      "Learning Transferable Visual Models From Natural\n",
      "Language Supervision. In ICML.\n",
      "PMLR, 8748–8763.\n",
      "Michele A. Saad, Alan C.\n",
      "Bovik, and Christophe Charrier.\n",
      "2012.\n",
      "Blind Image Quality Assessment: A Natural Scene\n",
      "Statistics Approach in the DCT Domain.\n",
      "IEEE Trans. Image Process.\n",
      "21, 8 (2012),\n",
      "3339–3352.\n",
      "https://doi.org/10.1109/TIP.2012.2191563\n",
      "Michele A. Saad, Alan C.\n",
      "Bovik, and Christophe Charrier.\n",
      "2014.\n",
      "Blind Prediction of Natural Video Quality.\n",
      "IEEE Trans. Image Process.\n",
      "23, 3 (2014),\n",
      "1352–1365.\n",
      "Ramprasaath R. Selvaraju,\n",
      "Michael Cogswell, Abhishek Das,\n",
      "Ramakrishna Vedantam, Devi Parikh, and\n",
      "Dhruv Batra. 2017.\n",
      "Grad-CAM: Visual Explanations from Deep Networks\n",
      "via Gradient-Based Localization. In ICCV.\n",
      "IEEE Computer Society, 618–626.\n",
      "Muhammad Shahid, Jacob\n",
      "Søgaard, Jeevan Pokhrel, Kjell\n",
      "Brunnström, Kun Wang, Samira\n",
      "Tavakoli, and Narciso García.\n",
      "2014.\n",
      "Crowdsourcing based subjective quality assessment\n",
      "of adaptive video streaming. In Sixth\n",
      "International Workshop on Quality of Multimedia Experience, QoMEX 2014,\n",
      "Singapore, September 18-20, 2014. IEEE,\n",
      "53–54.\n",
      "https://doi.org/10.1109/QoMEX.2014.6982289\n",
      "Zeina Sinno and\n",
      "Alan Conrad Bovik. 2019.\n",
      "Large-Scale Study of Perceptual Video Quality.\n",
      "IEEE Trans. Image Process.\n",
      "28, 2 (2019),\n",
      "612–627.\n",
      "Mingxing Tan and Quoc V.\n",
      "Le. 2019.\n",
      "EfficientNet: Rethinking Model Scaling for\n",
      "Convolutional Neural Networks. In ICML\n",
      "(Proceedings of Machine Learning Research,\n",
      "Vol. 97). PMLR,\n",
      "6105–6114.\n",
      "Du Tran, Heng Wang,\n",
      "Matt Feiszli, and Lorenzo Torresani.\n",
      "2019.\n",
      "Video Classification With Channel-Separated\n",
      "Convolutional Networks. In ICCV.\n",
      "IEEE, 5551–5560.\n",
      "Zhengzhong Tu, Chia-Ju\n",
      "Chen, Yilin Wang, Neil Birkbeck,\n",
      "Balu Adsumilli, and Alan C. Bovik.\n",
      "2021a.\n",
      "Efficient User-Generated Video Quality Prediction.\n",
      "In Picture Coding Symposium, PCS 2021, Bristol,\n",
      "United Kingdom, June 29 - July 2, 2021. IEEE,\n",
      "1–5.\n",
      "https://doi.org/10.1109/PCS50896.2021.9477483\n",
      "Zhengzhong Tu, Yilin\n",
      "Wang, Neil Birkbeck, Balu Adsumilli,\n",
      "and Alan C. Bovik. 2021b.\n",
      "UGC-VQA: Benchmarking Blind Video Quality\n",
      "Assessment for User Generated Content.\n",
      "IEEE TIP 30\n",
      "(2021), 4449–4464.\n",
      "Zhengzhong Tu, Yilin\n",
      "Wang, Neil Birkbeck, Balu Adsumilli,\n",
      "and Alan C. Bovik. 2021c.\n",
      "UGC-VQA: Benchmarking Blind Video Quality\n",
      "Assessment for User Generated Content.\n",
      "IEEE TIP 30\n",
      "(2021), 4449–4464.\n",
      "Zhengzhong Tu, Xiangxu\n",
      "Yu, Yilin Wang, Neil Birkbeck,\n",
      "Balu Adsumilli, and Alan C. Bovik.\n",
      "2021d.\n",
      "RAPIQUE: Rapid and Accurate Video Quality\n",
      "Prediction of User Generated Content.\n",
      "CoRR abs/2101.10955\n",
      "(2021).\n",
      "Domonkos Varga and\n",
      "Tamás Szirányi.\n",
      "2019.\n",
      "No-reference video quality assessment via\n",
      "pretrained CNN and LSTM networks.\n",
      "Signal Image Video Process.\n",
      "13, 8 (2019),\n",
      "1569–1576.\n",
      "Yilin Wang, Sasi Inguva,\n",
      "and Balu Adsumilli. 2019.\n",
      "YouTube UGC Dataset for Video Compression\n",
      "Research. In MMSP.\n",
      "IEEE, 1–5.\n",
      "Yilin Wang, Junjie Ke,\n",
      "Hossein Talebi, Joong Gon Yim,\n",
      "Neil Birkbeck, Balu Adsumilli,\n",
      "Peyman Milanfar, and Feng Yang.\n",
      "2021.\n",
      "Rich Features for Perceptual Quality Assessment of\n",
      "UGC Videos. In CVPR.\n",
      "13435–13444.\n",
      "Zhou Wang, Ligang Lu,\n",
      "and Alan C. Bovik. 2004.\n",
      "Video quality assessment based on structural\n",
      "distortion measurement.\n",
      "Signal Process. Image Commun.\n",
      "19, 2 (2004),\n",
      "121–132.\n",
      "Haoning Wu, Chaofeng\n",
      "Chen, Jingwen Hou, Liang Liao,\n",
      "Annan Wang, Wenxiu Sun,\n",
      "Qiong Yan, and Weisi Lin.\n",
      "2022a.\n",
      "FAST-VQA: Efficient End-to-End Video Quality\n",
      "Assessment with Fragment Sampling. In ECCV\n",
      "(6), Vol. 13666. 538–554.\n",
      "Haoning Wu, Chaofeng\n",
      "Chen, Liang Liao, Jingwen Hou,\n",
      "Qiong Yan, and Weisi Lin.\n",
      "2022b.\n",
      "DisCoVQA: Temporal Distortion-Content Transformers\n",
      "for Video Quality Assessment.\n",
      "CoRR abs/2206.09853\n",
      "(2022).\n",
      "Fengchuang Xing,\n",
      "Yuan-Gen Wang, Hanpin Wang,\n",
      "Leida Li, and Guopu Zhu.\n",
      "2021.\n",
      "StarVQA: Space-Time Attention for Video Quality\n",
      "Assessment.\n",
      "CoRR abs/2108.09635\n",
      "(2021).\n",
      "Jiahua Xu, Jing Li,\n",
      "Xingguang Zhou, Wei Zhou,\n",
      "Baichao Wang, and Zhibo Chen.\n",
      "2021.\n",
      "Perceptual Quality Assessment of Internet Videos.\n",
      "In ACM Multimedia. ACM,\n",
      "1248–1257.\n",
      "Jia Yan, Weixia Zhang,\n",
      "and Tianpeng Feng. 2016.\n",
      "Blind Image Quality Assessment Based on Natural\n",
      "Redundancy Statistics. In Computer Vision - ACCV\n",
      "2016 - 13th Asian Conference on Computer Vision, Taipei, Taiwan, November\n",
      "20-24, 2016, Revised Selected Papers, Part IV\n",
      "(Lecture Notes in Computer Science,\n",
      "Vol. 10114),\n",
      "Shang-Hong Lai,\n",
      "Vincent Lepetit, Ko Nishino, and\n",
      "Yoichi Sato (Eds.). Springer,\n",
      "3–18.\n",
      "https://doi.org/10.1007/978-3-319-54190-7_1\n",
      "Zhenqiang Ying, Maniratnam\n",
      "Mandal, Deepti Ghadiyaram, and Alan\n",
      "Bovik. 2021.\n",
      "Patch-VQ: 'Patching Up' the Video Quality Problem.\n",
      "In CVPR. Computer Vision\n",
      "Foundation / IEEE, 14019–14029.\n",
      "Junyong You.\n",
      "2021.\n",
      "Long Short-term Convolutional Transformer for\n",
      "No-Reference Video Quality Assessment. In ACM\n",
      "Multimedia. ACM, 2112–2120.\n",
      "Junyong You and Jari\n",
      "Korhonen. 2019.\n",
      "Deep Neural Networks for No-Reference Video Quality\n",
      "Assessment. In 2019 IEEE International\n",
      "Conference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25,\n",
      "2019. IEEE, 2349–2353.\n",
      "https://doi.org/10.1109/ICIP.2019.8803395\n",
      "Kai Zhao, Kun Yuan,\n",
      "Ming Sun, Mading Li, and\n",
      "Xing Wen. 2023b.\n",
      "Quality-Aware Pre-Trained Models for Blind Image\n",
      "Quality Assessment. In CVPR.\n",
      "IEEE Computer Society, 22302–22313.\n",
      "Kai Zhao, Kun Yuan,\n",
      "Ming Sun, and Xing Wen.\n",
      "2023a.\n",
      "Zoom-VQA: Patches, Frames and Clips Integration for\n",
      "Video Quality Assessment. In CVPR Workshops.\n",
      "IEEE Computer Society, 1302–1310.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paragraph in full_text['body_text']:\n",
    "    print(paragraph['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "11c92f00-b861-4cb4-a3c7-ff6c5a30fb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'section': 'Introduction',\n",
       " 'sec_number': '1',\n",
       " 'sec_type': 'section',\n",
       " 'content_type': 'paragraph',\n",
       " 'text': 'DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab.\\xa0REF , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\\n',\n",
       " 'cite_spans': [{'start': 213,\n",
       "   'end': 229,\n",
       "   'text': '{{cite:fbaab96}}',\n",
       "   'ref_id': 'fbaab96'},\n",
       "  {'start': 231, 'end': 247, 'text': '{{cite:dd352e3}}', 'ref_id': 'dd352e3'},\n",
       "  {'start': 364, 'end': 380, 'text': '{{cite:b5fb90b}}', 'ref_id': 'b5fb90b'},\n",
       "  {'start': 382, 'end': 398, 'text': '{{cite:2222c26}}', 'ref_id': '2222c26'},\n",
       "  {'start': 400, 'end': 416, 'text': '{{cite:689f656}}', 'ref_id': '689f656'},\n",
       "  {'start': 418, 'end': 434, 'text': '{{cite:11e5a31}}', 'ref_id': '11e5a31'},\n",
       "  {'start': 576, 'end': 592, 'text': '{{cite:24b4d04}}', 'ref_id': '24b4d04'},\n",
       "  {'start': 724, 'end': 740, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'},\n",
       "  {'start': 742, 'end': 758, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'},\n",
       "  {'start': 979, 'end': 995, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'}],\n",
       " 'ref_spans': []}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text['body_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf00b6c0-2fb4-49ac-ad94-0a990b3eb122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': 'Introduction', 'sec_number': '1', 'sec_type': 'section', 'content_type': 'paragraph', 'text': \"With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\\n\", 'cite_spans': [{'start': 128, 'end': 144, 'text': '{{cite:cef100d}}', 'ref_id': 'cef100d'}, {'start': 306, 'end': 322, 'text': '{{cite:9b70eb4}}', 'ref_id': '9b70eb4'}, {'start': 324, 'end': 340, 'text': '{{cite:c6c0872}}', 'ref_id': 'c6c0872'}, {'start': 419, 'end': 435, 'text': '{{cite:7da4179}}', 'ref_id': '7da4179'}, {'start': 437, 'end': 453, 'text': '{{cite:1ad644b}}', 'ref_id': '1ad644b'}, {'start': 455, 'end': 471, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'}, {'start': 473, 'end': 489, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'}, {'start': 491, 'end': 507, 'text': '{{cite:5d0c0fc}}', 'ref_id': '5d0c0fc'}, {'start': 509, 'end': 525, 'text': '{{cite:3237048}}', 'ref_id': '3237048'}, {'start': 581, 'end': 597, 'text': '{{cite:8f3eb86}}', 'ref_id': '8f3eb86'}, {'start': 599, 'end': 615, 'text': '{{cite:24b4d04}}', 'ref_id': '24b4d04'}, {'start': 617, 'end': 633, 'text': '{{cite:9be8207}}', 'ref_id': '9be8207'}, {'start': 635, 'end': 651, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'}], 'ref_spans': []}\n",
      "{'section': 'Introduction', 'sec_number': '1', 'sec_type': 'section', 'content_type': 'paragraph', 'text': 'DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab.\\xa0REF , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\\n', 'cite_spans': [{'start': 213, 'end': 229, 'text': '{{cite:fbaab96}}', 'ref_id': 'fbaab96'}, {'start': 231, 'end': 247, 'text': '{{cite:dd352e3}}', 'ref_id': 'dd352e3'}, {'start': 364, 'end': 380, 'text': '{{cite:b5fb90b}}', 'ref_id': 'b5fb90b'}, {'start': 382, 'end': 398, 'text': '{{cite:2222c26}}', 'ref_id': '2222c26'}, {'start': 400, 'end': 416, 'text': '{{cite:689f656}}', 'ref_id': '689f656'}, {'start': 418, 'end': 434, 'text': '{{cite:11e5a31}}', 'ref_id': '11e5a31'}, {'start': 576, 'end': 592, 'text': '{{cite:24b4d04}}', 'ref_id': '24b4d04'}, {'start': 724, 'end': 740, 'text': '{{cite:3dfd446}}', 'ref_id': '3dfd446'}, {'start': 742, 'end': 758, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'}, {'start': 979, 'end': 995, 'text': '{{cite:9c04ab9}}', 'ref_id': '9c04ab9'}], 'ref_spans': []}\n"
     ]
    }
   ],
   "source": [
    "for paragraph in full_text['body_text']:\n",
    "    # for ref_span in paragraph['ref_spans']:\n",
    "    if \"DNN-based VQA method\" in paragraph[\"text\"]:\n",
    "        print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ffca20d1-89c7-4cc8-bdc0-cbdabf001e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table\n"
     ]
    }
   ],
   "source": [
    "# as a baseline, let's use title, abstract, and section where the table comes from as input\n",
    "section_names = []\n",
    "for paragraph in full_text['body_text']:\n",
    "    for ref_span in paragraph['ref_spans']:\n",
    "        # if \"DNN-based VQA method\" in paragraph[\"body_text\"]:\n",
    "        #     print(paragraph)\n",
    "        if ref_span[\"ref_id\"] == dataset_2308_high_quality_sample[0]['_full_text_table_hash']:\n",
    "            # print(paragraph['content_type'])\n",
    "            # print(paragraph.keys())\n",
    "            section_names.append(paragraph['section'])\n",
    "\n",
    "\n",
    "def listify(list_paragraph):\n",
    "    return \" * \" + \"\\n * \".join(list_paragraph.strip().split(\"\\n\\n\"))\n",
    "\n",
    "relevant_sections = {sn: [] for sn in section_names}\n",
    "for paragraph in full_text['body_text']:\n",
    "    if paragraph['section'] in relevant_sections:\n",
    "        if paragraph['content_type'] == \"paragraph\":\n",
    "            relevant_sections[paragraph['section']].append(paragraph['text'])\n",
    "        elif paragraph['content_type'] == \"list\":\n",
    "            relevant_sections[paragraph['section']].append(listify(paragraph['text']))\n",
    "        else:\n",
    "            print(paragraph['content_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6d49ec4c-2185-4a6d-9606-e15e3565addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * To the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\n",
      " * We propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\n",
      " * We evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\n"
     ]
    }
   ],
   "source": [
    "print(\" * \" + \"\\n * \".join(relevant_sections['Introduction'][-1].strip().split(\"\\n\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0bcf2d6d-ef49-4fa8-9f21-636d38f385d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\n",
      "\n",
      "DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. {{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\n",
      "\n",
      "To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig. {{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\n",
      "\n",
      "To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\n",
      "\n",
      "Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\n",
      "Our contributions are as follows:\n",
      "\n",
      " * To the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\n",
      " * We propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\n",
      " * We evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(relevant_sections['Introduction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16ef87f8-d99f-48a8-9faf-a2fce6ee66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOL, I don't actually have the title or abstract of the paper with the table! That's hilarious.\n",
    "# I have to use s2 to get it!\n",
    "def get_metadata_s2_public(corpus_ids_batch, prefix=\"CorpusId:\"):\n",
    "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/batch?fields=corpusId,isOpenAccess,openAccessPdf,externalIds,title,abstract\"\n",
    "    ids_json = {\"ids\": [f\"{prefix}{corpus_id}\" for corpus_id in corpus_ids_batch]}\n",
    "    response = requests.post(base_url, json=ids_json)\n",
    "    metadata = response.json()\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68a59e5c-4dcd-4782-9b93-b2992ac452f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_ids_batch = [ft['paper_id'] for ft in full_texts_2308_high_quality]\n",
    "arxiv_ids_batch_stripped = [re.sub(\"v\\d+\", \"\", arxiv_id) for arxiv_id in arxiv_ids_batch]\n",
    "metadata_batch = get_metadata_s2_public(arxiv_ids_batch_stripped, prefix=\"ARXIV:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17735944-18eb-4c67-b201-4572eddc6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/arxiv_tables_2308_high_quality/full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f484fc5c-9777-4dfe-80d9-fb872564c620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': 'b5e4c46ac09d5de2051998c9f741ef480fab2ebe',\n",
       "  'externalIds': {'DBLP': 'conf/mm/LiuWYSTZWL23',\n",
       "   'ArXiv': '2308.00729',\n",
       "   'DOI': '10.1145/3581783.3611795',\n",
       "   'CorpusId': 260378902},\n",
       "  'corpusId': 260378902,\n",
       "  'title': 'Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment',\n",
       "  'abstract': 'Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://dl.acm.org/doi/pdf/10.1145/3581783.3611795',\n",
       "   'status': 'HYBRID'}},\n",
       " {'paperId': '89a0f417d5d41e5acaebaa8fd695d9dcb9632a04',\n",
       "  'externalIds': {'ArXiv': '2308.06173',\n",
       "   'DBLP': 'journals/access/GuesmiHOS23',\n",
       "   'DOI': '10.1109/ACCESS.2023.3321118',\n",
       "   'CorpusId': 260865937},\n",
       "  'corpusId': 260865937,\n",
       "  'title': 'Physical Adversarial Attacks for Camera-Based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook',\n",
       "  'abstract': 'Deep Neural Networks (DNNs) have shown impressive performance in computer vision tasks; however, their vulnerability to adversarial attacks raises concerns regarding their security and reliability. Extensive research has shown that DNNs can be compromised by carefully crafted perturbations, leading to significant performance degradation in both digital and physical domains. Therefore, ensuring the security of DNN-based systems is crucial, particularly in safety-critical domains such as autonomous driving, robotics, smart homes/cities, smart industries, video surveillance, and healthcare. In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://ieeexplore.ieee.org/ielx7/6287639/6514899/10268441.pdf',\n",
       "   'status': 'GOLD'}},\n",
       " {'paperId': '0b220041eb83c23b7b10d32a5d08c0309d528071',\n",
       "  'externalIds': {'ArXiv': '2308.07107',\n",
       "   'DBLP': 'journals/corr/abs-2308-07107',\n",
       "   'DOI': '10.48550/arXiv.2308.07107',\n",
       "   'CorpusId': 260887838},\n",
       "  'corpusId': 260887838,\n",
       "  'title': 'Large Language Models for Information Retrieval: A Survey',\n",
       "  'abstract': 'As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.07107',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '92f6346346a46f5813b6996b9c1f10d9602eb39a',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2308-00937',\n",
       "   'ArXiv': '2308.00937',\n",
       "   'DOI': '10.1109/LRA.2023.3313058',\n",
       "   'CorpusId': 260378734},\n",
       "  'corpusId': 260378734,\n",
       "  'title': 'LEMMA: Learning Language-Conditioned Multi-Robot Manipulation',\n",
       "  'abstract': \"Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for <underline>L</underline>anguag<underline>E</underline>-Conditioned <underline>M</underline>ulti-robot <underline>MA</underline>nipulation (<monospace>LEMMA</monospace>) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. <monospace>LEMMA</monospace> features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. <monospace>LEMMA</monospace> poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of <monospace>LEMMA</monospace> for developing future language-conditioned multi-robot systems.\",\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.00937',\n",
       "   'status': 'GREEN'}},\n",
       " {'paperId': 'fea19ea351f65115c470bdd6a2446d7e41ee799f',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2308-08407',\n",
       "   'ArXiv': '2308.08407',\n",
       "   'DOI': '10.48550/arXiv.2308.08407',\n",
       "   'CorpusId': 260926590},\n",
       "  'corpusId': 260926590,\n",
       "  'title': 'Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities',\n",
       "  'abstract': \"Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.\",\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.08407',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '64e802ea8e9dbe247c31fb06184c04dbf9e55e4e',\n",
       "  'externalIds': {'ArXiv': '2308.06966',\n",
       "   'DBLP': 'journals/corr/abs-2308-06966',\n",
       "   'DOI': '10.48550/arXiv.2308.06966',\n",
       "   'CorpusId': 260887693},\n",
       "  'corpusId': 260887693,\n",
       "  'title': 'EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce',\n",
       "  'abstract': 'Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.06966',\n",
       "   'status': 'GREEN'}},\n",
       " {'paperId': '9f432d2500758cd1182fe47fb09c2065bf7b9123',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2308-14430',\n",
       "   'ArXiv': '2308.14430',\n",
       "   'DOI': '10.48550/arXiv.2308.14430',\n",
       "   'CorpusId': 261242529},\n",
       "  'corpusId': 261242529,\n",
       "  'title': 'TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models',\n",
       "  'abstract': 'Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.14430',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': 'f0f8471873132d02c18d030aaf72cd21988e657e',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2308-14371',\n",
       "   'ArXiv': '2308.14371',\n",
       "   'DOI': '10.48550/arXiv.2308.14371',\n",
       "   'CorpusId': 261242415,\n",
       "   'PubMed': '37738188'},\n",
       "  'corpusId': 261242415,\n",
       "  'title': 'SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction',\n",
       "  'abstract': 'Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code will be released after accteptance.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.14371',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': 'ee5896108a9cb78dffddc4ff94a1ea43e0e4f656',\n",
       "  'externalIds': {'ArXiv': '2308.09090', 'CorpusId': 261030390},\n",
       "  'corpusId': 261030390,\n",
       "  'title': 'Data-driven Integrated Sensing and Communication: Recent Advances, Challenges, and Future Prospects',\n",
       "  'abstract': 'Integrated Sensing and Communication (ISAC), combined with data-driven approaches, has emerged as a highly significant field, garnering considerable attention from academia and industry. Its potential to enable wide-scale applications in the future sixth-generation (6G) networks has led to extensive recent research efforts. Machine learning (ML) techniques, including $K$-nearest neighbors (KNN), support vector machines (SVM), deep learning (DL) architectures, and reinforcement learning (RL) algorithms, have been deployed to address various design aspects of ISAC and its diverse applications. Therefore, this paper aims to explore integrating various ML techniques into ISAC systems, covering various applications. These applications span intelligent vehicular networks, encompassing unmanned aerial vehicles (UAVs) and autonomous cars, as well as radar applications, localization and tracking, millimeter wave (mmWave) and Terahertz (THz) communication, and beamforming. The contributions of this paper lie in its comprehensive survey of ML-based works in the ISAC domain and its identification of challenges and future research directions. By synthesizing the existing knowledge and proposing new research avenues, this survey serves as a valuable resource for researchers, practitioners, and stakeholders involved in advancing the capabilities of ISAC systems in the context of 6G networks.',\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None},\n",
       " {'paperId': '939f35d721c33187a71b43351a371a799ee409ab',\n",
       "  'externalIds': {'ArXiv': '2308.08794',\n",
       "   'DBLP': 'journals/corr/abs-2308-08794',\n",
       "   'DOI': '10.48550/arXiv.2308.08794',\n",
       "   'CorpusId': 261030871},\n",
       "  'corpusId': 261030871,\n",
       "  'title': 'Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces',\n",
       "  'abstract': 'Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.08794',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '29742526f4e6cd3727789815053395c266a28650',\n",
       "  'externalIds': {'ArXiv': '2308.03149',\n",
       "   'DBLP': 'journals/corr/abs-2308-03149',\n",
       "   'DOI': '10.1109/COMST.2023.3298300',\n",
       "   'CorpusId': 260189388},\n",
       "  'corpusId': 260189388,\n",
       "  'title': 'A Survey of mmWave-Based Human Sensing: Technology, Platforms and Applications',\n",
       "  'abstract': 'With the rapid development of the Internet of Things (IoT) and the rise of 5G communication networks and automatic driving, millimeter wave (mmWave) sensing is emerging and starts impacting our life and workspace. mmWave sensing can sense humans and objects in a contactless way, providing fine-grained sensing ability. In the past few years, many mmWave sensing techniques have been proposed and applied in various human sensing applications (e.g., human localization, gesture recognition, and vital monitoring). We discover the need of a comprehensive survey to summarize the technology, platforms and applications of mmWave-based human sensing. In this survey, we first present the mmWave hardware platforms and some key techniques of mmWave sensing. We then provide a comprehensive review of existing mmWave-based human sensing works. Specifically, we divide existing works into four categories according to the sensing granularity: human tracking and localization, motion recognition, biometric measurement and human imaging. Finally, we discuss the potential research challenges and present future directions in this area.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.03149',\n",
       "   'status': 'GREEN'}},\n",
       " {'paperId': '483a8014ac9c0048c4bcf10b404ab3f5b0f46e4f',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2308-13266',\n",
       "   'ArXiv': '2308.13266',\n",
       "   'DOI': '10.1109/ICCV51070.2023.00893',\n",
       "   'CorpusId': 261214723},\n",
       "  'corpusId': 261214723,\n",
       "  'title': 'Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation',\n",
       "  'abstract': 'Tracking any given object(s) spatially and temporally is a common purpose in Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint tracking and segmentation have been attempted in some studies but they often lack full compatibility of both box and mask in initialization and prediction, and mainly focus on single-object scenarios. To address these limitations, this paper proposes a Multi-object Mask-box Integrated framework for unified Tracking and Segmentation, dubbed MITS. Firstly, the unified identification module is proposed to support both box and mask reference for initialization, where detailed object information is inferred from boxes or directly retained from masks. Additionally, a novel pinpoint box predictor is proposed for accurate multi-object box prediction, facilitating target-oriented representation learning. All target objects are processed simultaneously from encoding to propagation and decoding, as a unified pipeline for VOT and VOS. Experimental results show MITS achieves state-of-the-art performance on both VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor by around 6% on the GOT-10k test set, and significantly improves the performance of box initialization on VOS benchmarks. The code is available at https://github.com/yoxu515/MITS.',\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None},\n",
       " {'paperId': '99a0f0f6ada73216389307133e04e9468639fbb2',\n",
       "  'externalIds': {'ArXiv': '2308.08643',\n",
       "   'DBLP': 'journals/corr/abs-2308-08643',\n",
       "   'DOI': '10.48550/arXiv.2308.08643',\n",
       "   'CorpusId': 261031130},\n",
       "  'corpusId': 261031130,\n",
       "  'title': 'Towards Personalized Federated Learning via Heterogeneous Model Reassembly',\n",
       "  'abstract': 'This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.08643',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '7402bd2fca6ec71744604f4aeb84cab02daa8b6c',\n",
       "  'externalIds': {'ArXiv': '2308.07444',\n",
       "   'DBLP': 'journals/corr/abs-2308-07444',\n",
       "   'DOI': '10.48550/arXiv.2308.07444',\n",
       "   'CorpusId': 260900310},\n",
       "  'corpusId': 260900310,\n",
       "  'title': 'The Performance of Transferability Metrics does not Translate to Medical Tasks',\n",
       "  'abstract': 'Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contexts, inviting further work in that direction.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.07444',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '78b8f98c133c876d6f9ee151821d271100733cd5',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2308-10631',\n",
       "   'ArXiv': '2308.10631',\n",
       "   'DOI': '10.48550/arXiv.2308.10631',\n",
       "   'CorpusId': 261049754},\n",
       "  'corpusId': 261049754,\n",
       "  'title': 'PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait',\n",
       "  'abstract': 'Psychological trait estimation from external factors such as movement and appearance is a challenging and long-standing problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totalling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.10631',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '81b10e64133e775dab53153cc82277d276efe1f7',\n",
       "  'externalIds': {'ArXiv': '2308.02151',\n",
       "   'DBLP': 'journals/corr/abs-2308-02151',\n",
       "   'DOI': '10.48550/arXiv.2308.02151',\n",
       "   'CorpusId': 260611249},\n",
       "  'corpusId': 260611249,\n",
       "  'title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization',\n",
       "  'abstract': 'Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.02151',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '79411a597f24ef32126c388b3813a8070f93c75d',\n",
       "  'externalIds': {'ArXiv': '2308.03108',\n",
       "   'DOI': '10.1109/access.2024.3353042',\n",
       "   'CorpusId': 260682704},\n",
       "  'corpusId': 260682704,\n",
       "  'title': 'SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation',\n",
       "  'abstract': 'In this paper, we investigate the vulnerability of MDE to adversarial patches. We propose a novel \\\\underline{S}tealthy \\\\underline{A}dversarial \\\\underline{A}ttacks on \\\\underline{M}DE (SAAM) that compromises MDE by either corrupting the estimated distance or causing an object to seamlessly blend into its surroundings. Our experiments, demonstrate that the designed stealthy patch successfully causes a DNN-based MDE to misestimate the depth of objects. In fact, our proposed adversarial patch achieves a significant 60\\\\% depth error with 99\\\\% ratio of the affected region. Importantly, despite its adversarial nature, the patch maintains a naturalistic appearance, making it inconspicuous to human observers. We believe that this work sheds light on the threat of adversarial attacks in the context of MDE on edge devices. We hope it raises awareness within the community about the potential real-life harm of such attacks and encourages further research into developing more robust and adaptive defense mechanisms.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://ieeexplore.ieee.org/ielx7/6287639/6514899/10388324.pdf',\n",
       "   'status': 'GOLD'}},\n",
       " {'paperId': 'e4561cf0cee2139b7c9085822a4899ce6a5aace3',\n",
       "  'externalIds': {'ArXiv': '2308.00247',\n",
       "   'DBLP': 'journals/corr/abs-2308-00247',\n",
       "   'DOI': '10.48550/arXiv.2308.00247',\n",
       "   'CorpusId': 260351426},\n",
       "  'corpusId': 260351426,\n",
       "  'title': 'Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review',\n",
       "  'abstract': 'The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.00247',\n",
       "   'status': 'GREEN'}},\n",
       " {'paperId': '05cbac9a5101f47a6fabad72398616506572c9fa',\n",
       "  'externalIds': {'ArXiv': '2308.09592',\n",
       "   'DBLP': 'journals/corr/abs-2308-09592',\n",
       "   'DOI': '10.1109/ICCV51070.2023.02106',\n",
       "   'CorpusId': 261031087},\n",
       "  'corpusId': 261031087,\n",
       "  'title': 'StableVideo: Text-driven Consistency-aware Diffusion Video Editing',\n",
       "  'abstract': 'Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at this https URL.',\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None},\n",
       " {'paperId': '36d23a17309e5e31a5b966c0158386ebe6ce719c',\n",
       "  'externalIds': {'ArXiv': '2308.03151',\n",
       "   'DBLP': 'journals/corr/abs-2308-03151',\n",
       "   'DOI': '10.1145/3581783.3611994',\n",
       "   'CorpusId': 260681484},\n",
       "  'corpusId': 260681484,\n",
       "  'title': 'Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models',\n",
       "  'abstract': \"Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.\",\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://dl.acm.org/doi/pdf/10.1145/3581783.3611994',\n",
       "   'status': 'BRONZE'}},\n",
       " {'paperId': 'c0c06de39c1fd89db3bdb52b56851bfbabba0b09',\n",
       "  'externalIds': {'ArXiv': '2308.03258',\n",
       "   'DBLP': 'journals/corr/abs-2308-03258',\n",
       "   'DOI': '10.48550/arXiv.2308.03258',\n",
       "   'CorpusId': 260680386},\n",
       "  'corpusId': 260680386,\n",
       "  'title': 'APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses',\n",
       "  'abstract': 'The efficacy of availability poisoning, a method of poisoning data by injecting imperceptible perturbations to prevent its use in model training, has been a hot subject of investigation. Previous research suggested that it was difficult to effectively counteract such poisoning attacks. However, the introduction of various defense methods has challenged this notion. Due to the rapid progress in this field, the performance of different novel methods cannot be accurately validated due to variations in experimental setups. To further evaluate the attack and defense capabilities of these poisoning methods, we have developed a benchmark -- APBench for assessing the efficacy of adversarial poisoning. APBench consists of 9 state-of-the-art availability poisoning attacks, 8 defense algorithms, and 4 conventional data augmentation techniques. We also have set up experiments with varying different poisoning ratios, and evaluated the attacks on multiple datasets and their transferability across model architectures. We further conducted a comprehensive evaluation of 2 additional attacks specifically targeting unsupervised models. Our results reveal the glaring inadequacy of existing attacks in safeguarding individual privacy. APBench is open source and available to the deep learning community: https://github.com/lafeat/apbench.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.03258',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': 'b706a48c83e146c843948e0b67f73cadf01fcc8d',\n",
       "  'externalIds': {'ArXiv': '2308.10737',\n",
       "   'DBLP': 'journals/corr/abs-2308-10737',\n",
       "   'DOI': '10.48550/arXiv.2308.10737',\n",
       "   'CorpusId': 261049286},\n",
       "  'corpusId': 261049286,\n",
       "  'title': 'UGSL: A Unified Framework for Benchmarking Graph Structure Learning',\n",
       "  'abstract': 'Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications. While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided. The GNN parameters and a graph structure are jointly learned. Previous studies adopt different experimentation setups, making it difficult to compare their merits. In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework. Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model. We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework. Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses. The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.10737',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': 'd6c2523ab97416c2692cbbeab082ed1790e8e55e',\n",
       "  'externalIds': {'ArXiv': '2308.06595',\n",
       "   'DBLP': 'journals/corr/abs-2308-06595',\n",
       "   'DOI': '10.48550/arXiv.2308.06595',\n",
       "   'CorpusId': 260887670},\n",
       "  'corpusId': 260887670,\n",
       "  'title': 'VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use',\n",
       "  'abstract': \"We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at visit-bench.github.io.\",\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.06595',\n",
       "   'status': 'GREEN'}},\n",
       " {'paperId': '01a3050bb42a8426e125ede7694294de26c71f13',\n",
       "  'externalIds': {'ArXiv': '2308.01529',\n",
       "   'DBLP': 'journals/corr/abs-2308-01529',\n",
       "   'DOI': '10.48550/arXiv.2308.01529',\n",
       "   'CorpusId': 260438699},\n",
       "  'corpusId': 260438699,\n",
       "  'title': 'Towards Fair and Privacy Preserving Federated Learning for the Healthcare Domain',\n",
       "  'abstract': 'Federated learning enables data sharing in healthcare contexts where it might otherwise be difficult due to data-use-ordinances or security and communication constraints. Distributed and shared data models allow models to become generalizable and learn from heterogeneous clients. While addressing data security, privacy, and vulnerability considerations, data itself is not shared across nodes in a given learning network. On the other hand, FL models often struggle with variable client data distributions and operate on an assumption of independent and identically distributed data. As the field has grown, the notion of fairness-aware federated learning mechanisms has also been introduced and is of distinct significance to the healthcare domain where many sensitive groups and protected classes exist. In this paper, we create a benchmark methodology for FAFL mechanisms under various heterogeneous conditions on datasets in the healthcare domain typically outside the scope of current federated learning benchmarks, such as medical imaging and waveform data formats. Our results indicate considerable variation in how various FAFL schemes respond to high levels of data heterogeneity. Additionally, doing so under privacy-preserving conditions can create significant increases in network communication cost and latency compared to the typical federated learning scheme.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.01529',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '5901358b0df3491e12263a39f52f45249e0aa7b9',\n",
       "  'externalIds': {'ArXiv': '2308.06953',\n",
       "   'DBLP': 'journals/corr/abs-2308-06953',\n",
       "   'DOI': '10.48550/arXiv.2308.06953',\n",
       "   'CorpusId': 260886925},\n",
       "  'corpusId': 260886925,\n",
       "  'title': 'Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation',\n",
       "  'abstract': 'Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs; and, the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. With a single YAML configuration file, users can build and test an annotation interface for any framework within minutes -- all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained frameworks and corresponding annotations made and collected by the community, covering a wide range of NLP tasks. For deployment, Thresh offers multiple options for any scale of annotation projects from small manual inspections to large crowdsourcing ones. Additionally, we introduce a Python library to streamline the entire process from typology design and deployment to annotation processing. Thresh is publicly accessible at https://thresh.tools.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.06953',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': 'ca9f41ad1c3f0ed51781eef6cfcd035bf4d01d1a',\n",
       "  'externalIds': {'ArXiv': '2308.15701',\n",
       "   'DBLP': 'journals/corr/abs-2308-15701',\n",
       "   'DOI': '10.48550/arXiv.2308.15701',\n",
       "   'CorpusId': 261339556},\n",
       "  'corpusId': 261339556,\n",
       "  'title': 'A Survey on Multi-Behavior Sequential Recommendation',\n",
       "  'abstract': 'Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.15701',\n",
       "   'status': 'CLOSED'}},\n",
       " {'paperId': '8d36390a430845849b62646a8c8be4c79f2b3d62',\n",
       "  'externalIds': {'ArXiv': '2308.06507',\n",
       "   'DBLP': 'journals/corr/abs-2308-06507',\n",
       "   'ACL': '2023.acl-short.149',\n",
       "   'DOI': '10.18653/v1/2023.acl-short.149',\n",
       "   'CorpusId': 259370708},\n",
       "  'corpusId': 259370708,\n",
       "  'title': 'AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models',\n",
       "  'abstract': 'Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://aclanthology.org/2023.acl-short.149.pdf',\n",
       "   'status': 'HYBRID'}},\n",
       " {'paperId': 'fe779d52ef85c0387ed9a68cd90ca11033689bfa',\n",
       "  'externalIds': {'ArXiv': '2308.05787',\n",
       "   'DBLP': 'journals/corr/abs-2308-05787',\n",
       "   'DOI': '10.48550/arXiv.2308.05787',\n",
       "   'CorpusId': 260866000},\n",
       "  'corpusId': 260866000,\n",
       "  'title': 'Temporally-Adaptive Models for Efficient Video Understanding',\n",
       "  'abstract': 'Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.',\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.05787',\n",
       "   'status': 'CLOSED'}}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "67e74323-056e-456f-a801-4908e76217cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map = {md['externalIds']['ArXiv']: md for md in metadata_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "abb0e307-6e0e-47a1-8d27-5c8633331951",
   "metadata": {},
   "outputs": [],
   "source": [
    "for md in metadata_batch:\n",
    "    papers_2308_high_quality.append({\n",
    "        'tabids': [],\n",
    "        'corpus_id': md['externalIds']['CorpusId'],\n",
    "        'arxiv_id': md['externalIds']['ArXiv'],\n",
    "        'paper_id': md['paperId'],\n",
    "        'title': md['title'],\n",
    "        'abstract': md['abstract'],\n",
    "    })\n",
    "    \n",
    "# papers_2308_high_quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ff974ac8-4a1e-45c7-9540-86ccd648654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now we have the title and abstract, let'd combine this into a prompt\n",
    "title = metadata_batch[0][\"title\"]\n",
    "abstract = metadata_batch[0][\"abstract\"]\n",
    "table_section = \"\\n\".join(relevant_sections['Introduction'])\n",
    "\n",
    "for scheme, glossary_key in glossary_keys:\n",
    "    if scheme == glossary_key:\n",
    "        query = f\"In the context of the following table, what does '{glossary_key}' refer to?\\n\"\n",
    "    else:\n",
    "        query = f\"In the context of the following table, what does the '{scheme}' '{glossary_key}' refer to?\"\n",
    "    table = str(pd.DataFrame(dataset_2308_high_quality_sample[0]['table_json']['table_dict'])) + \"\\n\"\n",
    "    table += \"Caption: \" + dataset_2308_high_quality_sample[0]['table_unfiltered']['caption']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ef4b873b-0a26-45e3-8728-7381512d8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = f\"\"\"\\\n",
    "Based on the following text from a scientific paper, answer the question about the table that follows:\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "Abstract: {abstract}\n",
    "\n",
    "Section: {list(relevant_sections.keys())[0]}\n",
    "{table_section}\n",
    "\n",
    "---\n",
    "Based on the above text, in the context of the following table, what does 'Dataset' refer to? Answer in a single sentence.\n",
    "Table: 43c9402e-2914-42dc-99c4-b2424f71f6ad\n",
    "{table}\\\n",
    "\"\"\"\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5489b465-f459-47ff-8991-504f734c0612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the following text from a scientific paper, answer the question about the table that follows:\n",
      "\n",
      "Title: Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment\n",
      "\n",
      "Abstract: Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.\n",
      "\n",
      "Section: Introduction\n",
      "With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\n",
      "\n",
      "DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. {{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\n",
      "\n",
      "To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig. {{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\n",
      "\n",
      "To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\n",
      "\n",
      "Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\n",
      "Our contributions are as follows:\n",
      "\n",
      " * To the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\n",
      " * We propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\n",
      " * We evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\n",
      "\n",
      "---\n",
      "Based on the above, in the context of the following table, what does 'Dataset' refer to?\n",
      "Table: 43c9402e-2914-42dc-99c4-b2424f71f6ad\n",
      "         References        Dataset            Task       Size Annotations\n",
      "0  {{cite:24b4d04}}     KoNViD-1k              VQA      1,200         114\n",
      "1  {{cite:8f3eb86}}      LIVE-VQC              VQA        585         240\n",
      "2  {{cite:9be8207}}   YouTube-UGC              VQA      1,380         123\n",
      "3  {{cite:3dfd446}}          LSVQ              VQA     39,075          35\n",
      "4  {{cite:9c04ab9}}   KoNViD-150k              VQA    153,841           5\n",
      "5  {{cite:fbaab96}}     Sports-1M   classification  1,133,158   - (auto.)\n",
      "6  {{cite:dd352e3}}  Kinetics-400   classification    306,245         3-5\n",
      "Caption: Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\n"
     ]
    }
   ],
   "source": [
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be6af09-b03d-4f6f-97d6-b2a03f7a37dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3142688693b4294826cc83ac279d060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=\"/gscratch/xlab/blnewman/models/transformers/\")\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    cache_dir=\"/gscratch/xlab/blnewman/models/transformers/\",\n",
    "    load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    # device_map=DEVICE,\n",
    ")\n",
    "mistral_model.config.pad_token_id = mistral_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "48e702bb-2b3f-43ee-8b0f-3135233739cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8b953965-f393-4a5e-9a95-6f2d7ff1e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8041021440"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5a5e8614-beed-4e9e-80b2-521ea5f3f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c7b803cd-0f6e-43fb-9136-fb939bc816aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mistral_tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "generated_ids = mistral_model.generate(inputs, max_new_tokens=1000, do_sample=True, num_return_sequences=1)\n",
    "mistral_tokenizer.batch_decode(generated_ids[:, inputs.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "79a08d2a-2cfb-412e-b516-9f5a726325f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In the context of the table, 'Dataset' refers to the specific videocontent dataset mentioned in each row, for example, KoNViD-1k, LIVE-VQC, YouTube-UGC, LSVQ, KoNViD-150k, Sports-1M, and Kinetics-400. These datasets are used for different tasks, such as Video Quality Assessment (VQA) or video classification, and vary in size and number of annotations available.\"]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_tokenizer.batch_decode(generated_ids[:, inputs.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b6c2bca8-6e8c-4b7f-a906-9638bc0944ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.\"]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Answer in a single sentence\"\n",
    "inputs = mistral_tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "generated_ids = mistral_model.generate(inputs, max_new_tokens=1000, do_sample=True, num_return_sequences=1)\n",
    "mistral_tokenizer.batch_decode(generated_ids[:, inputs.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d0fe59c0-8702-4285-aed9-89bb06265b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_keys_unique = []\n",
    "for key in glossary_keys:\n",
    "    if key not in glossary_keys_unique:\n",
    "        glossary_keys_unique.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "eb3a4aa9-8b93-41cd-bb2e-e49e3ab9bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = pd.DataFrame(dataset_2308_high_quality_sample[0]['table_json']['table_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7bc2690a-f8b8-4887-85cc-4d5302ef952f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>References</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{{cite:24b4d04}}</td>\n",
       "      <td>VQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{{cite:8f3eb86}}</td>\n",
       "      <td>VQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{{cite:9be8207}}</td>\n",
       "      <td>VQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{{cite:3dfd446}}</td>\n",
       "      <td>VQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{{cite:9c04ab9}}</td>\n",
       "      <td>VQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{{cite:fbaab96}}</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{{cite:dd352e3}}</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         References            Task\n",
       "0  {{cite:24b4d04}}             VQA\n",
       "1  {{cite:8f3eb86}}             VQA\n",
       "2  {{cite:9be8207}}             VQA\n",
       "3  {{cite:3dfd446}}             VQA\n",
       "4  {{cite:9c04ab9}}             VQA\n",
       "5  {{cite:fbaab96}}  classification\n",
       "6  {{cite:dd352e3}}  classification"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_df[[\"References\", \"Task\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94e462a8-f4b2-4311-94c5-37e0daa06cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the whole glossary:\n",
    "# table = str(pd.DataFrame(dataset_2308_high_quality_sample[0]['table_json']['table_dict'])) + \"\\n\"\n",
    "# table += \"Caption: \" + dataset_2308_high_quality_sample[0]['table_unfiltered']['caption']\n",
    "def query_mistral(prompt):\n",
    "    generated_ids = []\n",
    "    inputs = mistral_tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    try:\n",
    "        generated_ids = mistral_model.generate(inputs, max_new_tokens=1000, do_sample=True, num_return_sequences=1)\n",
    "        response = mistral_tokenizer.batch_decode(generated_ids[:, inputs.shape[1]:], skip_special_tokens=True)\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"oom:\", inputs.shape, flush=True)\n",
    "        raise torch.cuda.OutOfMemoryError\n",
    "    finally:\n",
    "        del inputs\n",
    "        del generated_ids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return response\n",
    "\n",
    "def create_glossary_instructions(title, abstract, section_name, table_section, glossary_query, table_hash, table):\n",
    "        instruction = f\"\"\"\\\n",
    "Based on the following text from a scientific paper, answer the question about the table that follows:\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "Abstract: {abstract}\n",
    "\n",
    "Section: {section_name}\n",
    "{table_section}\n",
    "\n",
    "---\n",
    "Based on the above text, in the context of the following table, what does {glossary_query} refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\n",
    "Table: {table_hash}\n",
    "{table}\\\n",
    "\"\"\"\n",
    "        \n",
    "        return instruction\n",
    "\n",
    "def create_glossary(dataset_table, glossary_keys, title, abstract, section_name, table_section):\n",
    "    table_df = pd.DataFrame(dataset_table['table_json']['table_dict'])\n",
    "    # glossary = {(\"Dataset\", \"Dataset\"): \"In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.\"}\n",
    "    glossary = {}\n",
    "    for scheme, glossary_key in glossary_keys:\n",
    "        # Avoid repeating glossary terms\n",
    "        if (scheme, glossary_key) in glossary:\n",
    "            continue\n",
    "        print((scheme, glossary_key), flush=True)\n",
    "        if scheme == glossary_key:\n",
    "            glossary_query = f\"'{glossary_key}'\"\n",
    "        else:\n",
    "            glossary_query = f\"'{glossary_key}' in the column '{scheme}'\"\n",
    "    \n",
    "        # mask out the columns that aren't being asked about:\n",
    "        table = str(table_df[[\"References\", scheme]]) + \"\\n\"\n",
    "        table += \"Caption: \" + dataset_table['table_unfiltered']['caption']\n",
    "\n",
    "        instruction = create_glossary_instructions(\n",
    "            title, abstract, section_name, table_section, glossary_query, dataset_table['_full_text_table_hash'], table\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            prompt = [{\"role\": \"user\", \"content\": instruction}]\n",
    "            try:\n",
    "                response = query_mistral(prompt)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\"OOM num chars:\", len(instruction))\n",
    "                table_section = \"\\n\".join(table_section.split(\"\\n\")[:-1])\n",
    "                instruction = create_glossary_instructions(\n",
    "                    title, abstract, section_name, table_section, glossary_query, dataset_table['_full_text_table_hash'], table\n",
    "                )\n",
    "        # inputs = mistral_tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        # try:\n",
    "        #     # generated_ids = mistral_model.generate(inputs, max_new_tokens=1000, do_sample=True, num_return_sequences=1)\n",
    "        #     response = query_mistral(prompt)\n",
    "        # except torch.cuda.OutOfMemoryError:\n",
    "        #     print(\"oom:\", flush=True)\n",
    "        #     # print(\"oom:\", inputs.shape)\n",
    "        #     # del inputs\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     new_table_section = \"\\n\".join(table_section.split(\"\\n\")[:-1])\n",
    "        #     return create_glossary(dataset_table, glossary_keys, title, abstract, section_name, new_table_section)\n",
    "        #     # remove the last paragraph from the table_section and try again lol\n",
    "        # response = mistral_tokenizer.batch_decode(generated_ids[:, inputs.shape[1]:], skip_special_tokens=True)\n",
    "        # response = [\"test\"]\n",
    "        glossary[(scheme, glossary_key)] = response[0]\n",
    "        # del inputs\n",
    "        # del generated_ids\n",
    "        # torch.cuda.empty_cache()\n",
    "    return instruction, glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d75d031-c5cb-46f9-acbe-5d90b208f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(list_paragraph):\n",
    "        return \" * \" + \"\\n * \".join(list_paragraph.strip().split(\"\\n\\n\"))\n",
    "\n",
    "def get_section_with_table(dataset_table, full_text):\n",
    "    section_names = []\n",
    "    for paragraph in full_text['body_text']:\n",
    "        for ref_span in paragraph['ref_spans']:\n",
    "            # if \"DNN-based VQA method\" in paragraph[\"body_text\"]:\n",
    "            #     print(paragraph)\n",
    "            if ref_span[\"ref_id\"] == dataset_table['_full_text_table_hash']:\n",
    "                # print(paragraph['content_type'])\n",
    "                # print(paragraph.keys())\n",
    "                section_names.append(paragraph['section'])\n",
    "    \n",
    "    relevant_sections = {sn: [] for sn in section_names}\n",
    "    for paragraph in full_text['body_text']:\n",
    "        if paragraph['section'] in relevant_sections:\n",
    "            if paragraph['content_type'] == \"paragraph\":\n",
    "                relevant_sections[paragraph['section']].append(paragraph['text'])\n",
    "            elif paragraph['content_type'] == \"list\":\n",
    "                relevant_sections[paragraph['section']].append(listify(paragraph['text']))\n",
    "            # else:\n",
    "            #     print(paragraph['content_type'])\n",
    "\n",
    "    # just return the first section for now\n",
    "    table_section = \"\\n\".join(list(relevant_sections.values())[0])\n",
    "    return list(relevant_sections.keys())[0], table_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ac6f59dc-0a23-43da-9227-20d5a0713f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "\n",
      "0 2308.00729v1 43c9402e-2914-42dc-99c4-b2424f71f6ad\n",
      "[('Dataset', 'Dataset'), ('Dataset', 'KoNViD-1k'), ('Dataset', 'LIVE-VQC'), ('Dataset', 'YouTube-UGC'), ('Dataset', 'LSVQ'), ('Dataset', 'KoNViD-150k'), ('Dataset', 'Sports-1M'), ('Dataset', 'Kinetics-400'), ('Task', 'Task'), ('Task', 'VQA'), ('Task', 'classification'), ('Size', 'Size'), ('Annotations', 'Annotations'), ('Annotations', '- (auto.)')]\n",
      "\n",
      "----------\n",
      "\n",
      "1 2308.06173v1 8750716b-7f93-4922-8d97-ff903f750be7\n",
      "[('Attack', 'Attack'), ('Attack', 'IPatch'), ('Attack', 'SSAttack'), (\"Attacker's Knowledge\", \"Attacker's Knowledge\"), (\"Attacker's Knowledge\", 'White-box'), ('Robustness Technique', 'Robustness Technique'), ('Robustness Technique', 'EOT'), ('Stealthiness Technique', 'Stealthiness Technique'), ('Physical test type', 'Physical test type'), ('Physical test type', 'Static'), ('Space', 'Space'), ('Space', '2D')]\n",
      "\n",
      "----------\n",
      "\n",
      "2 2308.07107v2 e9b07ad7-35e6-4776-8561-9658e198363e\n",
      "[('Methods', 'Methods'), ('Methods', 'InPairs'), ('Methods', 'PROMPTAGATOR'), ('Methods', 'TQGen'), ('Methods', 'UDAPDR'), ('Methods', 'SPTAR'), ('Methods', 'ART'), ('# Examples', '# Examples'), ('Generator', 'Generator'), ('Generator', 'Curie'), ('Generator', 'FLAN'), ('Generator', 'T0'), ('Generator', 'GPT3 & FLAN-T5-XXL'), ('Generator', 'LLaMA-7B & Vicuna-7B'), ('Generator', 'T5-XL & T5-XXL'), ('Synthetic Data', 'Synthetic Data'), ('Synthetic Data', 'Relevant query'), ('Synthetic Data', 'Soft relevance labels'), ('Filter Method', 'Filter Method'), ('Filter Method', 'Generation probability'), ('Filter Method', 'Round-trip filtering'), ('Filter Method', 'BM25 filtering'), (\"LLMs' tuning\", \"LLMs' tuning\"), (\"LLMs' tuning\", 'Fixed'), (\"LLMs' tuning\", 'Soft Prompt tuning')]\n",
      "\n",
      "----------\n",
      "\n",
      "3 2308.07107v2 5a5d90c5-3c97-4eb9-a25f-e1e2e71bac32\n",
      "[('Methods', 'Methods'), ('Methods', 'cpt-text'), ('Methods', 'GTR'), ('Methods', 'TART'), ('Methods', 'DSI'), ('Methods', 'LLM-URL'), ('Backbone', 'Backbone'), ('Backbone', 'cpt-text'), ('Backbone', 'T5'), ('Backbone', 'GPT-3'), ('Architecture', 'Architecture'), ('Architecture', 'Encoder-based'), ('Architecture', 'Generative'), (\"LLM's tuning\", \"LLM's tuning\"), (\"LLM's tuning\", 'Training from scratch'), (\"LLM's tuning\", 'Fine-tuning'), (\"LLM's tuning\", 'Fine-tuning & Prompting'), (\"LLM's tuning\", 'Prompting')]\n",
      "\n",
      "----------\n",
      "\n",
      "4 2308.07107v2 19cc0a2d-1d22-44a1-9312-a47546fc8491\n",
      "[('Methods', 'Methods'), ('Methods', 'REALM'), ('Methods', 'RAG'), ('Methods', 'REPLUG'), ('Methods', 'Atlas'), ('Methods', 'RETA-LLM'), ('Methods', 'RALM'), ('Methods', 'RETRO'), ('Methods', 'IRCoT'), ('Methods', 'FLARE'), ('Backbone models', 'Backbone models'), ('Backbone models', 'BERT'), ('Backbone models', 'BART'), ('Backbone models', 'GPT'), ('Backbone models', 'T5'), ('Backbone models', 'Gopher'), ('Backbone models', 'LLaMA & GLM & GPT'), ('Backbone models', 'LLaMA & OPT & GPT'), ('Backbone models', 'Transformer'), ('Backbone models', 'Flan-T5 & GPT'), ('Where to incorporate retrieval', 'Where to incorporate retrieval'), ('Where to incorporate retrieval', 'Input layer'), ('Where to incorporate retrieval', 'Attention layer'), ('When to retrieve', 'When to retrieve'), ('When to retrieve', 'In the beginning'), ('When to retrieve', 'During generation (every n tokens)'), ('When to retrieve', 'During generation (every sentence)'), ('When to retrieve', 'During generation (aperiodic)'), ('How to use LLMs', 'How to use LLMs'), ('How to use LLMs', 'Fine-tuning'), ('How to use LLMs', 'Prompting'), ('How to use LLMs', 'Training from scratch')]\n",
      "\n",
      "----------\n",
      "\n",
      "5 2308.00937v1 0ce6f366-69ae-4e8e-8498-48bf64bcab6f\n",
      "[('Benchmark', 'Benchmark'), ('Benchmark', 'Alfred'), ('Benchmark', 'MQA'), ('Benchmark', 'Calvin'), ('Benchmark', 'M-EQA'), ('Benchmark', 'Ravens'), ('Benchmark', 'Vlmbench'), ('Benchmark', 'CH-MARL'), ('Benchmark', 'TBP'), ('Benchmark', 'EMATP'), ('Benchmark', 'LEMMA'), ('Language', 'Language'), ('Multi-task', 'Multi-task'), ('Manipulation', 'Manipulation'), ('Multi-agent', 'Multi-agent'), ('Tool Use', 'Tool Use'), ('Temporal Dep.', 'Temporal Dep.')]\n",
      "\n",
      "----------\n",
      "\n",
      "6 2308.08407v1 1e5afe33-474f-48fc-9bf9-3e365069c06d\n",
      "[('Year', 'Year'), ('Problem-Task', 'Problem-Task'), ('Problem-Task', 'AMR'), ('Problem-Task', 'mortality'), ('Problem-Task', 'COVID-19 severity'), ('Problem-Task', 'phenotyping'), ('Problem-Task', 'macular degeneration'), ('Problem-Model', 'Problem-Model'), ('Problem-Model', 'regression'), ('Problem-Model', 'neural network'), ('Dataset-Size', 'Dataset-Size'), ('XAI-Method', 'XAI-Method'), ('XAI-Method', 'inherent'), ('XAI-Method', 'LIME'), ('XAI-C.V.', 'XAI-C.V.'), ('XAI-Q.E.', 'XAI-Q.E.'), ('XAI-O.A.', 'XAI-O.A.')]\n",
      "\n",
      "----------\n",
      "\n",
      "7 2308.06953v3 3b4a9beb-d2ea-4f3b-9301-e62bcb2ba2a0\n",
      "[('Framework', 'Framework'), ('Framework', 'MQM'), ('Framework', 'FRANK'), ('Framework', 'SNaC'), ('Framework', 'Scarecrow'), ('Framework', 'SALSA'), ('Framework', 'ERRANT'), ('Framework', 'FG-RLHF'), ('Framework', 'MultiPIT'), ('Framework', 'CWZCC'), ('Framework', 'Propaganda'), ('Framework', 'arXivEdits'), ('Task', 'Task'), ('Task', 'Translation'), ('Task', 'Summarization'), ('Task', 'Narrative Summarization'), ('Task', 'Open-ended Generation'), ('Task', 'Simplification'), ('Task', 'Grammar Error Correction'), ('Task', 'Fine-Grained RLHF'), ('Task', 'Paraphrase Generation'), ('Task', 'Zamboanga Chavacano Spell Checking'), ('Task', 'Propaganda Analysis'), ('Task', 'Scientific Text Revision'), ('Released', 'Released'), ('Link', 'Link'), ('Link', 'thresh.tools/mqm'), ('Link', 'thresh.tools/frank'), ('Link', 'thresh.tools/snac'), ('Link', 'thresh.tools/scarecrow'), ('Link', 'thresh.tools/salsa'), ('Link', 'thresh.tools/errant'), ('Link', 'thresh.tools/fg-rlhf'), ('Link', 'thresh.tools/multipit'), ('Link', 'thresh.tools/cwzcc'), ('Link', 'thresh.tools/propaganda'), ('Link', 'thresh.tools/arxivedits')]\n",
      "\n",
      "----------\n",
      "\n",
      "8 2308.06966v1 c2fdbeb9-936a-4437-8bc9-1636a337dcce\n",
      "[('Dataset', 'Dataset'), ('Dataset', 'Lenove'), ('Dataset', 'Reddit'), ('Dataset', 'ABSA'), ('Dataset', 'MEPAVE'), ('Dataset', 'Multi-CPR'), ('Dataset', 'OpenBG https://github.com/OpenBGBenchmark'), ('Lang.', 'Lang.'), ('Lang.', 'EN'), ('Lang.', 'ZH'), ('Task', 'Task'), ('Task', \"['Named Entity Recognization', 'Entity Span Detection']\"), ('Task', 'Extractive QA'), ('Task', 'Review Topic Classification'), ('Task', \"['Attribute Value Recognization', 'Attribute Value Detection']\"), ('Task', 'Product Select'), ('Task', \"['Product Align', 'Title Attritube Matching', 'Fine-grain Product Classify', 'Coarse-grain Product Classify', 'Title Generate']\"), ('Metric', 'Metric'), ('Metric', \"['F1, Rouge', 'Rouge']\"), ('Metric', 'Rouge'), ('Metric', 'F1, Rouge'), ('Metric', \"['F1, Rouge', 'F1, Rouge', 'F1, Rouge', 'F1, Rouge', 'Rouge']\")]\n",
      "\n",
      "----------\n",
      "\n",
      "9 2308.14430v1 c6fc0305-b065-4cbc-8dcc-3e48ce4c4e5b\n",
      "[('Dataset', 'Dataset'), ('Dataset', 'StylePrompt'), ('Dataset', 'NLSpeech'), ('Dataset', 'PromptSpeech'), ('Dataset', 'TextrolSpeech'), ('Open source', 'Open source'), ('Open source', 'yes (part)'), ('Hours', 'Hours'), ('Text items', 'Text items'), ('Prompt diversity', 'Prompt diversity'), ('Speaker', 'Speaker'), ('Emotion', 'Emotion')]\n",
      "\n",
      "----------\n",
      "\n",
      "10 2308.14371v2 6b6fb748-8152-401d-a920-7b8f265c5a6a\n",
      "[('Method', 'Method'), ('Method', 'NP'), ('Method', 'CAP'), ('Method', 'Ours'), ('Open surface', 'Open surface'), ('Inference Time', 'Inference Time'), ('Sparse Input', 'Sparse Input'), ('Sparse Input', 'Hard'), ('Sparse Input', 'Easy')]\n",
      "\n",
      "----------\n",
      "\n",
      "11 2308.09090v1 67e1500f-559c-4454-a570-60b8d6b5992c\n",
      "[('Technology', 'Technology'), ('Technology', 'IRS-assisted V2I'), ('Technology', 'General applications'), ('Technology', 'IRS-assisted ISAC MISO'), ('AI technique', 'AI technique'), ('AI technique', 'DL'), ('Criteria', 'Criteria'), ('Criteria', 'CAP-Net (conv layers and LSTM)'), ('Criteria', 'CNN (Trained as a denoiser)'), ('Criteria', '1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)'), ('Input', 'Input'), ('Input', 'Historical covariance of the received echo'), ('Input', 'Channel estimated by LS method'), ('Input', '1.Received direct signals2.Total received signals and DE-CNN estimation'), ('Output', 'Output'), ('Output', 'AoAs'), ('Output', 'Channel estimation'), ('Output', 'Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels')]\n",
      "\n",
      "----------\n",
      "\n",
      "12 2308.08794v1 bdb02ddc-f6e2-4652-b298-deb8ecca3c20\n",
      "[('Method', 'Method'), ('Method', 'EWS'), ('Method', 'Bury et al.'), ('Method', 'Patel and Ott'), ('Method', 'Ours'), ('Generality', 'Generality'), ('Function space', 'Function space'), ('Partial physics', 'Partial physics'), ('Speed', 'Speed'), ('Pre-tip data', 'Pre-tip data')]\n",
      "\n",
      "----------\n",
      "\n",
      "13 2308.03149v1 0efac130-e8ec-4bc3-bac3-932eca9420ba\n",
      "[('Reference', 'Reference'), ('Reference', '[c]Openmili'), ('Reference', '[c]M 3'), ('Reference', '[c]mm-Flex'), ('Reference', '[c]'), ('Reference', '[c]Soli'), ('Baseband BW', 'Baseband BW'), ('Carries Freq.', 'Carries Freq.'), ('Antenna', 'Antenna'), ('Antenna', 'Horn/Phased-array'), ('Antenna', 'Phased-array'), ('Antenna', 'Phased-antenna'), ('Antenna', 'Array'), ('Cost', 'Cost')]\n",
      "\n",
      "----------\n",
      "\n",
      "14 2308.13266v2 d0e58d4f-e4c7-43dc-adb6-0f0526bd3b35\n",
      "[('Method', 'Method'), ('Method', 'SiamMask'), ('Method', 'D3S'), ('Method', 'SiamR-CNN'), ('Method', 'UniTrack'), ('Method', 'Unicorn'), ('Method', 'RTS'), ('Method', 'MITS (Ours)'), ('Initialization-Box', 'Initialization-Box'), ('Initialization-Mask', 'Initialization-Mask'), ('Prediction-Box', 'Prediction-Box'), ('Prediction-Mask', 'Prediction-Mask'), ('Extra Model', 'Extra Model'), ('Extra Model', 'Box2Seg {{cite:4dd3024}}'), ('Extra Model', 'STA {{cite:9e8cd2a}}'), ('Multi-Object', 'Multi-Object')]\n",
      "\n",
      "----------\n",
      "\n",
      "15 2308.08643v1 b8a7fba1-a12b-4b99-ad7d-1ff1b0bc8ec3\n",
      "[('Approach', 'Approach'), ('Approach', 'FedDF'), ('Approach', 'FedKEMF'), ('Approach', 'FCCL'), ('Approach', 'FedMD'), ('Approach', 'FedGH'), ('Approach', 'pFedHR'), ('Public Dataset-W. Label', 'Public Dataset-W. Label'), ('Public Dataset-W.o. Label', 'Public Dataset-W.o. Label'), ('Model Characteristics-Upload and Download', 'Model Characteristics-Upload and Download'), ('Model Characteristics-Upload and Download', 'parameters'), ('Model Characteristics-Upload and Download', 'logits'), ('Model Characteristics-Upload and Download', 'class scores'), ('Model Characteristics-Upload and Download', 'label-wise representations'), ('Model Characteristics-Aggregation', 'Model Characteristics-Aggregation'), ('Model Characteristics-Aggregation', 'ensemble distillation'), ('Model Characteristics-Aggregation', 'mutual learning'), ('Model Characteristics-Aggregation', 'average'), ('Model Characteristics-Aggregation', 'model reassembly'), ('Model Characteristics-Personalization', 'Model Characteristics-Personalization')]\n",
      "\n",
      "----------\n",
      "\n",
      "16 2308.10631v2 e147f394-6cdd-44f4-b641-08fd1a3d771f\n",
      "[('Dataset', 'Dataset'), ('Dataset', 'FVG'), ('Dataset', 'CASIA-B'), ('Dataset', 'OU-ISIR'), ('Dataset', 'Gait3D'), ('Dataset', 'GREW'), ('Dataset', 'DenseGait'), ('Dataset', 'PsyMo (ours)'), ('Type', 'Type'), ('Type', 'Controlled'), ('Type', 'In the Wild'), ('# IDs', '# IDs'), ('# Seq.', '# Seq.'), ('Variations', 'Variations'), ('Variations', 'NM, CL, BG, WS, CBG'), ('Variations', 'NM, CL, BG'), ('Variations', 'NM, CL, BG, WSS, WSF, TXT, PH'), ('Views', 'Views'), ('Env.', 'Env.'), ('Env.', 'Outdoor'), ('Env.', 'Indoor'), ('Demogr.', 'Demogr.'), ('BMI', 'BMI'), ('Particularity', 'Particularity'), ('Particularity', 'time difference'), ('Particularity', 'treadmill'), ('Particularity', 'collected in a supermarket'), ('Particularity', 'auto labelled with 42 attributes'), ('Particularity', '17 psychological traits')]\n",
      "\n",
      "----------\n",
      "\n",
      "17 2308.02151v1 3df6c6f3-c102-45a6-9330-01e293d766e3\n",
      "[('Approach', 'Approach'), ('Approach', 'CoT'), ('Approach', 'ReAct'), ('Approach', 'Self-refine'), ('Approach', 'RAP'), ('Approach', 'Reflexion'), ('Approach', 'Retroformer (our method)'), ('Gradient learning', 'Gradient learning'), ('Arbitrary reward', 'Arbitrary reward'), ('Iterative refinement', 'Iterative refinement'), ('Hidden constraints', 'Hidden constraints'), ('Decision making', 'Decision making'), ('Memory', 'Memory')]\n",
      "\n",
      "----------\n",
      "\n",
      "18 2308.03108v1 a2ad42a4-2eac-4b0c-b638-3f0f2aca9d66\n",
      "[('Method', 'Method'), ('Method', 'Adversarial patch'), ('Method', 'AOP'), ('Method', 'APARATE'), ('Method', 'SAAM (ours)'), ('Attack goal', 'Attack goal'), ('Attack goal', 'M'), ('Attack goal', 'M, H'), (\"Attacker's Knowledge\", \"Attacker's Knowledge\"), (\"Attacker's Knowledge\", 'White-box'), ('Stealthy', 'Stealthy'), ('Placement', 'Placement'), ('Placement', 'A'), ('Placement', 'O'), ('Placement', 'O, A'), ('Setting', 'Setting'), ('Setting', 'Outdoor'), ('Setting', 'Indoor')]\n",
      "\n",
      "----------\n",
      "\n",
      "19 2308.00247v1 f79e32ed-1780-4efd-a93a-7764dca79f4a\n",
      "[('Method', 'Method'), ('Method', 'N2V'), ('Method', 'N2S'), ('Method', 'PN2V'), ('Method', 'Noise2Same'), ('Method', 'S2S'), ('Method', 'B2UB'), ('Other needs', 'Other needs'), ('Other needs', 'An arbitrary noise model'), ('Applications (denoising type)', 'Applications (denoising type)'), ('Applications (denoising type)', 'Gaussian noise and some biomedical image noise denoising.'), ('Applications (denoising type)', 'Blind Gaussian noise denoising'), ('Applications (denoising type)', 'Microscopy and low-light condition image noise denoising'), ('Applications (denoising type)', 'Gaussian noise denoising.'), ('Applications (denoising type)', 'Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising'), ('Applications (denoising type)', 'FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising'), ('Key words (remarks)', 'Key words (remarks)'), ('Key words (remarks)', 'Pixel-wise independent noise, randomly select several pixel to mask in the input images.'), ('Key words (remarks)', 'J -invariant function determines the mask distribution, and replaces the pixels at J with random numbers.'), ('Key words (remarks)', 'Mask input images, probabilistic model, predict per-pixel intensity distributions.'), ('Key words (remarks)', 'J-invariant function determines the mask distribution, and replaces the pixels at J with local averages.'), ('Key words (remarks)', 'Bernoulli-sampled instances of the input image results on noisy pairs'), ('Key words (remarks)', 'Gobal-aware mask mapper, re-visible loss.')]\n",
      "\n",
      "----------\n",
      "\n",
      "20 2308.09592v1 35daceae-5f26-45a2-87f0-2ecf8bd94123\n",
      "[('Method', 'Method'), ('Method', 'Text2LIVE'), ('Method', 'Tune-A-Video'), ('Method', 'StableVideo (ours)'), ('Video Training', 'Video Training'), ('Edit Training', 'Edit Training'), ('Edit Inference', 'Edit Inference')]\n",
      "\n",
      "----------\n",
      "\n",
      "21 2308.03151v1 286ec480-7a24-4385-a36e-5b211c8d67a6\n",
      "[('Dataset', 'Dataset'), ('Dataset', 'Recipe1M+'), ('Dataset', 'FoodSeg103'), ('Dataset', 'UPMC Food-101'), ('Dataset', 'UEC Food256'), ('Dataset', 'VIREO Food-172'), ('Dataset', 'Sushi-50'), ('Dataset', 'ChineseFoodNet'), ('Dataset', 'Yummly-66k'), ('Dataset', 'ISIA Food-500'), ('Dataset', 'Food-500 Cap'), ('Image Number', 'Image Number'), ('Category-Number', 'Category-Number'), ('Category-Coverage', 'Category-Coverage'), ('Category-Coverage', 'Worldwide'), ('Category-Coverage', 'Western'), ('Category-Coverage', 'Japanese'), ('Category-Coverage', 'Chinese'), ('Annotation-type', 'Annotation-type'), ('Annotation-type', 'Ingredients & Cooking instructions'), ('Annotation-type', 'Ingredients'), ('Annotation-type', 'Related web text'), ('Annotation-type', 'Course & ingredients & region'), ('Annotation-type', 'Image Captions & region'), ('Annotation-source', 'Annotation-source'), ('Annotation-source', 'Web'), ('Annotation-source', 'Manual')]\n",
      "\n",
      "----------\n",
      "\n",
      "22 2308.03258v1 31c3c548-50f8-4775-94d1-36cedf85de2f\n",
      "[('Defense Method', 'Defense Method'), ('Defense Method', 'Standard'), ('Defense Method', 'CutOut'), ('Defense Method', 'MixUp'), ('Defense Method', 'CutMix'), ('Defense Method', 'Gaussian (used in )'), ('Defense Method', 'BDR (used in )'), ('Defense Method', 'Gray (used in )'), ('Defense Method', 'JPEG (used in )'), ('Defense Method', 'AVATAR'), ('Defense Method', 'U-Lite'), ('Defense Method', 'U-Max'), ('Defense Method', 'AT'), ('Type', 'Type'), ('Type', 'Data augmentations'), ('Type', 'Data preprocessing'), ('Type', 'Training-phase defense'), ('Time Cost', 'Time Cost'), ('Time Cost', 'Low'), ('Time Cost', 'High'), ('Description', 'Description'), ('Description', 'Random image cropping and flipping'), ('Description', 'Random image erasing'), ('Description', 'Random image blending'), ('Description', 'Random image cutting and stitching'), ('Description', 'Image blurring with a Gaussian kernel'), ('Description', 'Bit-depth reduction'), ('Description', 'Image grayscale transformation'), ('Description', 'Image compression'), ('Description', 'Image corruption and restoration'), ('Description', 'Stronger data augmentations'), ('Description', 'Adversarial augmentations'), ('Description', 'Adversarial training')]\n",
      "\n",
      "----------\n",
      "\n",
      "23 2308.10737v1 1dc9e3f8-9028-413c-8117-084d9777cea4\n",
      "[('Model', 'Model'), ('Model', 'GLCN'), ('Model', 'JLGCN'), ('Model', 'DGCNN'), ('Model', 'LDS'), ('Model', 'IDGL'), ('Model', 'Graph-Bert'), ('Model', 'GRCN'), ('Model', 'SLAPS'), ('Model', 'SUBLIME'), ('Model', 'VIB-GSL'), ('Input', 'Input'), ('Input', 'features'), ('Input', 'features, WL and spectral'), ('Edge scorer', 'Edge scorer'), ('Edge scorer', 'MLP'), ('Edge scorer', 'FP'), ('Edge scorer', 'ATT'), ('Edge scorer', 'FP, MLP, ATT'), ('Sparsifier', 'Sparsifier'), ('Sparsifier', 'none'), ('Sparsifier', 'kNN'), ('Sparsifier', 'Bernoulli'), ('Sparsifier', 'ϵNN'), ('Processor', 'Processor'), ('Processor', 'activation'), ('Processor', 'none'), ('Processor', 'activation-sym'), ('Regularizers', 'Regularizers'), ('Regularizers', 'sparse-connect, closeness, log-barrier'), ('Regularizers', 'smoothness'), ('Regularizers', 'none'), ('Regularizers', 'sparse-connect, log-barrier'), ('Unsupervised Losses', 'Unsupervised Losses'), ('Unsupervised Losses', 'none'), ('Unsupervised Losses', 'denoising'), ('Unsupervised Losses', 'contrastive')]\n",
      "\n",
      "----------\n",
      "\n",
      "24 2308.06595v1 1d5cb0b2-a9e1-403d-9802-1f8545379f7d\n",
      "[('Method', 'Method'), ('Method', 'MultiInstruct'), ('Method', 'Owl'), ('Method', 'InstructBLIP'), ('Method', 'M 3 IT'), ('Method', 'LVLM'), ('Method', 'GAVIE'), ('Method', 'VisIT-Bench'), ('Number of Models', 'Number of Models'), ('Number of Skills Tested', 'Number of Skills Tested'), ('Multiple-Images', 'Multiple-Images'), ('Video', 'Video'), ('Multi-Turn Conversations', 'Multi-Turn Conversations'), ('Multilingual Conversations', 'Multilingual Conversations'), ('Instruction-conditioned Captions', 'Instruction-conditioned Captions'), ('Chatbot-style Responses', 'Chatbot-style Responses'), ('Dataset-specific Evaluation', 'Dataset-specific Evaluation'), ('Human Evaluation', 'Human Evaluation'), ('Auto/GPT-4 Evaluation', 'Auto/GPT-4 Evaluation'), ('Win-rates*', 'Win-rates*'), ('Elo Rating', 'Elo Rating')]\n",
      "\n",
      "----------\n",
      "\n",
      "25 2308.01529v1 a35ebfbd-baf8-48b3-a15c-ac2b7c4a11c0\n",
      "[('Dataset', 'Dataset'), ('Dataset', 'NIH Chest X-Ray'), ('Dataset', 'Fast MRI'), ('Dataset', 'PTB Dataset'), ('Dataset', 'MIMIC II'), ('Data Type', 'Data Type'), ('Data Type', 'Adult X-Ray Images'), ('Data Type', 'Brain Tissue Scans'), ('Data Type', 'ECG Waveforms'), ('Data Type', 'Clinical and Vital Signs'), ('Format', 'Format'), ('Format', 'PNG'), ('Format', '3D Arrays'), ('Format', 'Signal'), ('Format', 'Tabular, Signal'), ('No. Classes', 'No. Classes'), ('Size', 'Size'), ('Base Model', 'Base Model'), ('Base Model', 'ResNet-34'), ('Base Model', 'U-Net'), ('Base Model', 'ResNet-18'), ('Base Model', 'XGBoost')]\n",
      "\n",
      "----------\n",
      "\n",
      "26 2308.15701v1 ef95492e-6dde-41e2-89b4-70db1dab2cf5\n",
      "[('Works', 'Works'), ('Works', 'RLBL'), ('Works', 'RIB'), ('Works', 'BINN'), ('Works', 'CBS'), ('Works', 'DIPN'), ('Works', 'HUP'), ('Works', 'IARS'), ('Works', 'DeepRec'), ('Works', 'MBN'), ('Data Perspective', 'Data Perspective'), ('Data Perspective', 'A sequence of (item, behavior) pairs'), ('Data Perspective', 'Some behavior-specific subsequences of items'), ('Model Perspective', 'Model Perspective'), ('Model Perspective', 'Local'), ('Model Perspective', 'Local + Global'), ('Features', 'Features'), ('Features', 'Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.'), ('Features', 'Leverage GRU and attention mechanism simultaneously.'), ('Features', 'Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.'), ('Features', 'Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.'), ('Features', 'Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.'), ('Features', 'Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.'), ('Features', 'Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.'), ('Features', 'Utilizing multi-behavior sequence data to make privacy-preserving recommendation.'), ('Features', 'The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.')]\n",
      "\n",
      "----------\n",
      "\n",
      "27 2308.06507v1 6acfd98a-95fa-4602-8898-8b48434cefe8\n",
      "[('Method', 'Method'), ('Method', 'EDA'), ('Method', 'Back-Translation'), ('Method', 'SeemSeek'), ('Method', 'Dialog Inpainting'), ('Method', 'AutoConv (Ours)'), ('DG', 'DG'), ('Data Needs', 'Data Needs'), ('Data Needs', 'Large'), ('Data Needs', 'Few')]\n",
      "\n",
      "----------\n",
      "\n",
      "28 2308.15701v1 ec69d321-0332-4d1c-a502-d66a5faa52e5\n",
      "[('Works', 'Works'), ('Works', 'MGNN-SPred'), ('Works', 'DMBGN'), ('Works', 'GPG4HSR'), ('Works', 'BGNN'), ('Works', 'BA-GNN'), ('Data Perspective', 'Data Perspective'), ('Data Perspective', 'Some behavior-specific subsequences of items'), ('Data Perspective', 'A sequence of (item, behavior) pairs'), ('Data Perspective', 'Some behavior-specific subsequence of items'), ('Model Perspective', 'Model Perspective'), ('Model Perspective', 'Global'), ('Model Perspective', 'Local + Global'), ('Features', 'Features'), ('Features', 'Modeling behavior from behavior transition relations, containing homogeneous behavior transitions intra each kind of behavior-specific subsequences.'), ('Features', 'Focus on the task of voucher redemption rate prediction and model the relationship between multiple behaviors and vouchers effectively.'), ('Features', 'Learn various behavior transition relations from the global graph and the personalized graph, respectively.'), ('Features', 'Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information.'), ('Features', 'Construct directed graphs for different behavior-specific sequences respectively.')]\n",
      "\n",
      "----------\n",
      "\n",
      "29 2308.07444v1 df31e046-9b0a-4970-a7bc-832f3a45a1bd\n",
      "[('Tr.scorer', 'Tr.scorer'), ('Tr.scorer', 'H-Score'), ('Tr.scorer', 'NCE'), ('Tr.scorer', 'LEEP'), ('Tr.scorer', 'N-LEEP'), ('Tr.scorer', 'LogME'), ('Tr.scorer', 'Regularized H-Score'), ('Tr.scorer', 'GBC'), ('Cat.', 'Cat.'), ('Cat.', 'fb'), ('Cat.', 'lb'), ('Scorer input', 'Scorer input'), ('Scorer input', 'source feature extractor & labels'), ('Scorer input', 'source classification head & labels'), ('Details', 'Details'), ('Details', 'transferability correlates to inter-class variance and feature redundancy'), ('Details', 'negative conditional entropy between source and target labels'), ('Details', 'log-likelihood between target labels and source model predictions'), ('Details', 'log-likelihood between target labels and Gaussian mixture model fit to target extracted features'), ('Details', 'probability of target labels conditioned on target image embeddings'), ('Details', 'shrinkage estimators for stable covariance'), ('Details', 'Bhattacharyya coeff. between multivariate Gaussians fit to each class’ feature estimating overlap with target task classes')]\n",
      "\n",
      "----------\n",
      "\n",
      "30 2308.05787v1 7d565ad8-2153-41b3-a135-79f90f8fb9da\n",
      "[('Operations', 'Operations'), ('Operations', 'CondConv'), ('Operations', 'DynamicFilter'), ('Operations', 'DDF'), ('Operations', 'TAM'), ('Operations', 'midgreyTAdaConv(V2)'), ('Temporal modeling', 'Temporal modeling'), ('Location adaptive', 'Location adaptive'), ('Pretrained weights', 'Pretrained weights')]\n"
     ]
    }
   ],
   "source": [
    "for dataset_table_i, dataset_table in enumerate(dataset_2308_high_quality_sample):\n",
    "    print(\"\\n----------\\n\")\n",
    "    print(dataset_table_i, dataset_table[\"paper_id\"], dataset_table[\"_full_text_table_hash\"])\n",
    "    full_text = full_texts_2308_high_quality_map[dataset_table['paper_id']]\n",
    "    # first, create the unique glossary keys:\n",
    "    glossary_keys = [\n",
    "        (scheme.strip(), str(entry).strip())\n",
    "        for scheme, column in\n",
    "        dataset_table['table_json']['table_dict'].items()\n",
    "        for entry in [scheme] + column\n",
    "        if scheme != \"References\" and not is_numeric(str(entry)) and not is_na(str(entry)) and not is_binary(str(entry))\n",
    "    ]\n",
    "    glossary_keys_unique = []\n",
    "    for key in glossary_keys:\n",
    "        if key not in glossary_keys_unique:\n",
    "            glossary_keys_unique.append(key)\n",
    "\n",
    "    print(glossary_keys_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1c111b7-afee-4e35-a8df-81941660918d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "\n",
      "26 2308.15701v1 ef95492e-6dde-41e2-89b4-70db1dab2cf5\n",
      "Methods in MBSR\n",
      "('Works', 'Works')\n",
      "oom: torch.Size([1, 17582])\n",
      "OOM num chars: 54114\n",
      "oom: torch.Size([1, 17581])\n",
      "OOM num chars: 54113\n",
      "oom: torch.Size([1, 17522])\n",
      "OOM num chars: 53786\n",
      "oom: torch.Size([1, 17371])\n",
      "OOM num chars: 53232\n",
      "oom: torch.Size([1, 17370])\n",
      "OOM num chars: 53231\n",
      "oom: torch.Size([1, 17350])\n",
      "OOM num chars: 53108\n",
      "oom: torch.Size([1, 17282])\n",
      "OOM num chars: 52762\n",
      "oom: torch.Size([1, 17281])\n",
      "OOM num chars: 52761\n",
      "oom: torch.Size([1, 17227])\n",
      "OOM num chars: 52473\n",
      "oom: torch.Size([1, 17161])\n",
      "OOM num chars: 52158\n",
      "oom: torch.Size([1, 17122])\n",
      "OOM num chars: 51971\n",
      "oom: torch.Size([1, 17047])\n",
      "OOM num chars: 51656\n",
      "oom: torch.Size([1, 17042])\n",
      "OOM num chars: 51650\n",
      "oom: torch.Size([1, 17041])\n",
      "OOM num chars: 51649\n",
      "oom: torch.Size([1, 16949])\n",
      "OOM num chars: 51228\n",
      "oom: torch.Size([1, 16931])\n",
      "OOM num chars: 51148\n",
      "oom: torch.Size([1, 16912])\n",
      "OOM num chars: 51069\n",
      "oom: torch.Size([1, 16911])\n",
      "OOM num chars: 51068\n",
      "oom: torch.Size([1, 16867])\n",
      "OOM num chars: 50876\n",
      "oom: torch.Size([1, 16846])\n",
      "OOM num chars: 50777\n",
      "oom: torch.Size([1, 16704])\n",
      "OOM num chars: 50251\n",
      "oom: torch.Size([1, 16697])\n",
      "OOM num chars: 50243\n",
      "oom: torch.Size([1, 16696])\n",
      "OOM num chars: 50242\n",
      "oom: torch.Size([1, 16492])\n",
      "OOM num chars: 49605\n",
      "oom: torch.Size([1, 16491])\n",
      "OOM num chars: 49604\n",
      "oom: torch.Size([1, 16406])\n",
      "OOM num chars: 49133\n",
      "oom: torch.Size([1, 16405])\n",
      "OOM num chars: 49132\n",
      "oom: torch.Size([1, 16305])\n",
      "OOM num chars: 48914\n",
      "oom: torch.Size([1, 16267])\n",
      "OOM num chars: 48719\n",
      "oom: torch.Size([1, 16266])\n",
      "OOM num chars: 48718\n",
      "oom: torch.Size([1, 16080])\n",
      "OOM num chars: 48256\n",
      "oom: torch.Size([1, 16079])\n",
      "OOM num chars: 48255\n",
      "oom: torch.Size([1, 16078])\n",
      "OOM num chars: 48254\n",
      "oom: torch.Size([1, 16037])\n",
      "OOM num chars: 48204\n",
      "oom: torch.Size([1, 15838])\n",
      "OOM num chars: 47746\n",
      "oom: torch.Size([1, 15703])\n",
      "OOM num chars: 47499\n",
      "oom: torch.Size([1, 15682])\n",
      "OOM num chars: 47386\n",
      "oom: torch.Size([1, 15681])\n",
      "OOM num chars: 47385\n",
      "oom: torch.Size([1, 15601])\n",
      "OOM num chars: 47125\n",
      "oom: torch.Size([1, 15597])\n",
      "OOM num chars: 47120\n",
      "oom: torch.Size([1, 15596])\n",
      "OOM num chars: 47119\n",
      "oom: torch.Size([1, 15488])\n",
      "OOM num chars: 46589\n",
      "oom: torch.Size([1, 15487])\n",
      "OOM num chars: 46588\n",
      "oom: torch.Size([1, 15380])\n",
      "OOM num chars: 46037\n",
      "oom: torch.Size([1, 15379])\n",
      "OOM num chars: 46036\n",
      "oom: torch.Size([1, 15246])\n",
      "OOM num chars: 45298\n",
      "oom: torch.Size([1, 15245])\n",
      "OOM num chars: 45297\n",
      "oom: torch.Size([1, 15013])\n",
      "OOM num chars: 44789\n",
      "oom: torch.Size([1, 14998])\n",
      "OOM num chars: 44733\n",
      "oom: torch.Size([1, 14886])\n",
      "OOM num chars: 44466\n",
      "oom: torch.Size([1, 14701])\n",
      "OOM num chars: 44098\n",
      "oom: torch.Size([1, 14700])\n",
      "OOM num chars: 44097\n",
      "oom: torch.Size([1, 14699])\n",
      "OOM num chars: 44096\n",
      "oom: torch.Size([1, 14659])\n",
      "OOM num chars: 44046\n",
      "oom: torch.Size([1, 14658])\n",
      "OOM num chars: 44045\n",
      "oom: torch.Size([1, 14579])\n",
      "OOM num chars: 43790\n",
      "oom: torch.Size([1, 14578])\n",
      "OOM num chars: 43789\n",
      "oom: torch.Size([1, 14549])\n",
      "OOM num chars: 43627\n",
      "oom: torch.Size([1, 14486])\n",
      "OOM num chars: 43457\n",
      "oom: torch.Size([1, 14449])\n",
      "OOM num chars: 43308\n",
      "oom: torch.Size([1, 14270])\n",
      "OOM num chars: 42863\n",
      "oom: torch.Size([1, 14269])\n",
      "OOM num chars: 42862\n",
      "oom: torch.Size([1, 14268])\n",
      "OOM num chars: 42861\n",
      "oom: torch.Size([1, 14211])\n",
      "OOM num chars: 42573\n",
      "oom: torch.Size([1, 14168])\n",
      "OOM num chars: 42388\n",
      "oom: torch.Size([1, 14165])\n",
      "OOM num chars: 42382\n",
      "oom: torch.Size([1, 14164])\n",
      "OOM num chars: 42381\n",
      "oom: torch.Size([1, 14099])\n",
      "OOM num chars: 42077\n",
      "oom: torch.Size([1, 13992])\n",
      "OOM num chars: 41850\n",
      "oom: torch.Size([1, 13935])\n",
      "OOM num chars: 41557\n",
      "oom: torch.Size([1, 13934])\n",
      "OOM num chars: 41556\n",
      "oom: torch.Size([1, 13753])\n",
      "OOM num chars: 40771\n",
      "oom: torch.Size([1, 13748])\n",
      "OOM num chars: 40763\n",
      "oom: torch.Size([1, 13747])\n",
      "OOM num chars: 40762\n",
      "oom: torch.Size([1, 13637])\n",
      "OOM num chars: 40189\n",
      "oom: torch.Size([1, 13636])\n",
      "OOM num chars: 40188\n",
      "oom: torch.Size([1, 13567])\n",
      "OOM num chars: 39828\n",
      "oom: torch.Size([1, 13566])\n",
      "OOM num chars: 39827\n",
      "oom: torch.Size([1, 13426])\n",
      "OOM num chars: 39085\n",
      "oom: torch.Size([1, 13393])\n",
      "OOM num chars: 38938\n",
      "oom: torch.Size([1, 13392])\n",
      "OOM num chars: 38937\n",
      "oom: torch.Size([1, 13265])\n",
      "OOM num chars: 38338\n",
      "oom: torch.Size([1, 13261])\n",
      "OOM num chars: 38330\n",
      "oom: torch.Size([1, 13260])\n",
      "OOM num chars: 38329\n",
      "oom: torch.Size([1, 13151])\n",
      "OOM num chars: 37874\n",
      "oom: torch.Size([1, 13150])\n",
      "OOM num chars: 37873\n",
      "oom: torch.Size([1, 12912])\n",
      "OOM num chars: 37433\n",
      "oom: torch.Size([1, 12736])\n",
      "OOM num chars: 37140\n",
      "oom: torch.Size([1, 12735])\n",
      "OOM num chars: 37139\n",
      "oom: torch.Size([1, 12734])\n",
      "OOM num chars: 37138\n",
      "oom: torch.Size([1, 12693])\n",
      "OOM num chars: 37088\n",
      "oom: torch.Size([1, 12520])\n",
      "OOM num chars: 36645\n",
      "oom: torch.Size([1, 12400])\n",
      "OOM num chars: 36113\n",
      "oom: torch.Size([1, 12399])\n",
      "OOM num chars: 36112\n",
      "oom: torch.Size([1, 12333])\n",
      "OOM num chars: 35830\n",
      "oom: torch.Size([1, 12329])\n",
      "OOM num chars: 35825\n",
      "oom: torch.Size([1, 12328])\n",
      "OOM num chars: 35824\n",
      "oom: torch.Size([1, 12262])\n",
      "OOM num chars: 35452\n",
      "oom: torch.Size([1, 12261])\n",
      "OOM num chars: 35451\n",
      "oom: torch.Size([1, 12199])\n",
      "OOM num chars: 35186\n",
      "oom: torch.Size([1, 12050])\n",
      "OOM num chars: 34888\n",
      "oom: torch.Size([1, 11901])\n",
      "OOM num chars: 34502\n",
      "oom: torch.Size([1, 11900])\n",
      "OOM num chars: 34501\n",
      "oom: torch.Size([1, 11899])\n",
      "OOM num chars: 34500\n",
      "oom: torch.Size([1, 11857])\n",
      "OOM num chars: 34450\n",
      "oom: torch.Size([1, 11841])\n",
      "OOM num chars: 34374\n",
      "oom: torch.Size([1, 11810])\n",
      "OOM num chars: 34231\n",
      "oom: torch.Size([1, 11809])\n",
      "OOM num chars: 34230\n",
      "oom: torch.Size([1, 11666])\n",
      "OOM num chars: 33774\n",
      "oom: torch.Size([1, 11662])\n",
      "OOM num chars: 33769\n",
      "oom: torch.Size([1, 11661])\n",
      "OOM num chars: 33768\n",
      "oom: torch.Size([1, 11480])\n",
      "OOM num chars: 33298\n",
      "oom: torch.Size([1, 11479])\n",
      "OOM num chars: 33297\n",
      "oom: torch.Size([1, 11377])\n",
      "OOM num chars: 32748\n",
      "oom: torch.Size([1, 11376])\n",
      "OOM num chars: 32747\n",
      "oom: torch.Size([1, 11240])\n",
      "OOM num chars: 32037\n",
      "oom: torch.Size([1, 11239])\n",
      "OOM num chars: 32036\n",
      "oom: torch.Size([1, 11190])\n",
      "OOM num chars: 31762\n",
      "oom: torch.Size([1, 11099])\n",
      "OOM num chars: 31212\n",
      "oom: torch.Size([1, 11032])\n",
      "OOM num chars: 30872\n",
      "oom: torch.Size([1, 11031])\n",
      "OOM num chars: 30871\n",
      "oom: torch.Size([1, 10917])\n",
      "OOM num chars: 30314\n",
      "oom: torch.Size([1, 10913])\n",
      "OOM num chars: 30308\n",
      "oom: torch.Size([1, 10912])\n",
      "OOM num chars: 30307\n",
      "oom: torch.Size([1, 10860])\n",
      "OOM num chars: 29997\n",
      "oom: torch.Size([1, 10755])\n",
      "OOM num chars: 29757\n",
      "oom: torch.Size([1, 10686])\n",
      "OOM num chars: 29428\n",
      "oom: torch.Size([1, 10685])\n",
      "OOM num chars: 29427\n",
      "oom: torch.Size([1, 10613])\n",
      "OOM num chars: 29101\n",
      "oom: torch.Size([1, 10507])\n",
      "OOM num chars: 28845\n",
      "oom: torch.Size([1, 10506])\n",
      "OOM num chars: 28844\n",
      "oom: torch.Size([1, 10505])\n",
      "OOM num chars: 28843\n",
      "oom: torch.Size([1, 10464])\n",
      "OOM num chars: 28793\n",
      "oom: torch.Size([1, 10286])\n",
      "OOM num chars: 28456\n",
      "oom: torch.Size([1, 10143])\n",
      "OOM num chars: 28040\n",
      "oom: torch.Size([1, 10142])\n",
      "OOM num chars: 28039\n",
      "oom: torch.Size([1, 9856])\n",
      "OOM num chars: 27438\n",
      "oom: torch.Size([1, 9855])\n",
      "OOM num chars: 27437\n",
      "oom: torch.Size([1, 9854])\n",
      "OOM num chars: 27436\n",
      "oom: torch.Size([1, 9815])\n",
      "OOM num chars: 27386\n",
      "oom: torch.Size([1, 9754])\n",
      "OOM num chars: 27216\n",
      "oom: torch.Size([1, 9503])\n",
      "OOM num chars: 26867\n",
      "oom: torch.Size([1, 9245])\n",
      "OOM num chars: 26436\n",
      "oom: torch.Size([1, 9123])\n",
      "OOM num chars: 26132\n",
      "oom: torch.Size([1, 9085])\n",
      "OOM num chars: 25908\n",
      "oom: torch.Size([1, 8797])\n",
      "OOM num chars: 25378\n",
      "oom: torch.Size([1, 8746])\n",
      "OOM num chars: 25153\n",
      "oom: torch.Size([1, 8647])\n",
      "OOM num chars: 24795\n",
      "oom: torch.Size([1, 8640])\n",
      "OOM num chars: 24786\n",
      "oom: torch.Size([1, 8639])\n",
      "OOM num chars: 24785\n",
      "oom: torch.Size([1, 8519])\n",
      "OOM num chars: 24156\n",
      "oom: torch.Size([1, 8518])\n",
      "OOM num chars: 24155\n",
      "oom: torch.Size([1, 8446])\n",
      "OOM num chars: 23792\n",
      "oom: torch.Size([1, 8261])\n",
      "OOM num chars: 23475\n",
      "oom: torch.Size([1, 8221])\n",
      "OOM num chars: 23344\n",
      "oom: torch.Size([1, 7477])\n",
      "OOM num chars: 22038\n",
      "oom: torch.Size([1, 7476])\n",
      "OOM num chars: 22037\n",
      "oom: torch.Size([1, 7451])\n",
      "OOM num chars: 21937\n",
      "oom: torch.Size([1, 7406])\n",
      "OOM num chars: 21773\n",
      "oom: torch.Size([1, 7401])\n",
      "OOM num chars: 21766\n",
      "oom: torch.Size([1, 7400])\n",
      "OOM num chars: 21765\n",
      "oom: torch.Size([1, 7230])\n",
      "OOM num chars: 21209\n",
      "oom: torch.Size([1, 7117])\n",
      "OOM num chars: 20636\n",
      "oom: torch.Size([1, 7116])\n",
      "OOM num chars: 20635\n",
      "oom: torch.Size([1, 7042])\n",
      "OOM num chars: 20213\n",
      "oom: torch.Size([1, 6990])\n",
      "OOM num chars: 20099\n",
      "oom: torch.Size([1, 6809])\n",
      "OOM num chars: 19803\n",
      "oom: torch.Size([1, 6808])\n",
      "OOM num chars: 19802\n",
      "oom: torch.Size([1, 6807])\n",
      "OOM num chars: 19801\n",
      "oom: torch.Size([1, 6768])\n",
      "OOM num chars: 19751\n",
      "oom: torch.Size([1, 6612])\n",
      "OOM num chars: 19390\n",
      "oom: torch.Size([1, 6565])\n",
      "OOM num chars: 19312\n",
      "oom: torch.Size([1, 6564])\n",
      "OOM num chars: 19311\n",
      "oom: torch.Size([1, 6563])\n",
      "OOM num chars: 19310\n",
      "oom: torch.Size([1, 6525])\n",
      "OOM num chars: 19260\n",
      "oom: torch.Size([1, 6146])\n",
      "OOM num chars: 18575\n",
      "('Works', 'RLBL')\n",
      "('Works', 'RIB')\n",
      "('Works', 'BINN')\n",
      "('Works', 'CBS')\n",
      "('Works', 'DIPN')\n",
      "('Works', 'HUP')\n",
      "('Works', 'IARS')\n",
      "('Works', 'DeepRec')\n",
      "('Works', 'MBN')\n",
      "('Data Perspective', 'Data Perspective')\n",
      "('Data Perspective', 'A sequence of (item, behavior) pairs')\n",
      "('Data Perspective', 'Some behavior-specific subsequences of items')\n",
      "('Model Perspective', 'Model Perspective')\n",
      "('Model Perspective', 'Local')\n",
      "('Model Perspective', 'Local + Global')\n",
      "('Features', 'Features')\n",
      "('Features', 'Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.')\n",
      "('Features', 'Leverage GRU and attention mechanism simultaneously.')\n",
      "('Features', 'Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.')\n",
      "('Features', 'Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.')\n",
      "('Features', 'Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.')\n",
      "('Features', 'Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.')\n",
      "('Features', 'Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.')\n",
      "('Features', 'Utilizing multi-behavior sequence data to make privacy-preserving recommendation.')\n",
      "('Features', 'The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.')\n"
     ]
    }
   ],
   "source": [
    "# contextualization_inputs = {}\n",
    "# alright, let's put this in a loop to get all of the glossaries:\n",
    "start_i = 26\n",
    "for dataset_table_i, dataset_table in enumerate(dataset_2308_high_quality_sample[start_i:]):\n",
    "    dataset_table_i += start_i\n",
    "    print(\"\\n----------\\n\")\n",
    "    print(dataset_table_i, dataset_table[\"paper_id\"], dataset_table[\"_full_text_table_hash\"])\n",
    "    full_text = full_texts_2308_high_quality_map[dataset_table['paper_id']]\n",
    "    # first, create the unique glossary keys:\n",
    "    glossary_keys = [\n",
    "        (scheme.strip(), str(entry).strip())\n",
    "        for scheme, column in\n",
    "        dataset_table['table_json']['table_dict'].items()\n",
    "        for entry in [scheme] + column\n",
    "        if scheme != \"References\" and not is_numeric(str(entry)) and not is_na(str(entry)) and not is_binary(str(entry))\n",
    "    ]\n",
    "    # glossary_keys_unique = []\n",
    "    # for key in glossary_keys:\n",
    "    #     if key not in glossary_keys_unique:\n",
    "    #         glossary_keys_unique.append(key)\n",
    "\n",
    "    # next, get the section, title and abstract\n",
    "    section_name, table_section = get_section_with_table(dataset_table, full_text)\n",
    "    arxiv_id = re.sub(\"v\\d+\", \"\", dataset_table[\"paper_id\"])\n",
    "    title = metadata_map[arxiv_id]['title']\n",
    "    abstract = metadata_map[arxiv_id]['abstract']\n",
    "    print(section_name)\n",
    "    # print(table_section)\n",
    "    instruction_sample, glossary = create_glossary(dataset_table, glossary_keys, title, abstract, section_name, table_section)\n",
    "\n",
    "    dataset_table[\"context_autogenerated\"] = {\n",
    "        \"glossary\": {\"<~>\".join(k): v for k, v in glossary.items()},\n",
    "        \"inputs\": {\n",
    "            \"section_name\": section_name,\n",
    "            \"instruction_sample\": instruction_sample,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    dataset_table[\"title\"] = title\n",
    "    dataset_table[\"abstract\"] = abstract\n",
    "    break\n",
    "    # \"title\": title,\n",
    "    # \"abstract\": abstract,\n",
    "\n",
    "    # contextualization_inputs[dataset_table[\"_table_hash\"]] = {\n",
    "    #     \"glossary\": {\"<~>\".join(k): v for k, v in glossary.items()},\n",
    "    #     \"inputs\": {\n",
    "    #         \"title\": title,\n",
    "    #         \"abstract\": abstract,\n",
    "    #         \"section_name\": section_name,\n",
    "    #         \"table_section\": table_section,\n",
    "    #         \"instruction_sample\": instruction_sample,\n",
    "    #     }\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489d691-9ab9-49e2-a5f8-b6771c9736ea",
   "metadata": {},
   "source": [
    "Weird ones (thing isn't included): 2, 4, 5, 7, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7649f2ce-d8e3-4b90-a4c6-dd89e88a3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/arxiv_tables_2308_high_quality/dataset_autocontext.jsonl\", \"w\") as f:\n",
    "    for line in dataset_2308_high_quality_sample:\n",
    "        f.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "# with open(\"../data/arxiv_tables_2308_high_quality/context_info.jsonl\", \"w\") as f:\n",
    "#     json.dump(contextualization_inputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "f99ff06b-fb87-403b-9e28-8db21e8988ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', '_pdf_hash', '_source_hash', '_source_name', '_table_hash', 'table_html', 'table_json', 'bib_hash', 'row_bib_map', 'table_unfiltered', '_table_hash_full_text', '_full_text_table_hash', 'context_autogenerated', 'title', 'abstract'])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47124cd8-e304-409f-b680-d1628d52dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dti, dataset_table in enumerate(dataset_2308_high_quality_sample):\n",
    "    # print(list(dataset_table[\"context_autogenerated\"][\"glossary\"].values())[1])\n",
    "    if list(dataset_table[\"context_autogenerated\"][\"glossary\"].values())[1] == \"test\":\n",
    "        print(dti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2968abb0-2fda-433e-a7fc-6e6dcf897805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Works<~>Works': \"In the context of the table, 'Works' refers to the specific research works or models that have been mentioned in the text and are listed in the first column of the table, each with a corresponding reference.\",\n",
       " 'Works<~>RLBL': 'RLBL refers to the recurrent log-bilinear model for next-item recommendation as described in the given text.',\n",
       " 'Works<~>RIB': 'RIB in the table refers to the interpretable recommendation framework from the micro behavior perspective (RIB), which models heterogeneous behaviors and dwell time to capture more fine-grained user information using GRU.',\n",
       " 'Works<~>BINN': 'BINN in the context of the table refers to Behavior-intensive neural network, which is one of the works mentioned in the table that uses RNN learning architecture for multi-behavior sequential recommendation.',\n",
       " 'Works<~>CBS': 'CBS stands for CBS model, which is mentioned as one of the works in the table that utilizes RNN learning architecture for multi-behavior sequential recommendation.',\n",
       " 'Works<~>DIPN': 'DIPN in the table refers to the \"Deep Interest-aware Persistent Neural Network\" proposed in the paper with the citation {{cite:9641864}}.',\n",
       " 'Works<~>HUP': 'HUP stands for Hierarchical User Modeling for Personalized Recommendation, which is one of the works mentioned in the text using an RNN-based learning architecture for recommendation.',\n",
       " 'Works<~>IARS': 'IARS in the table refers to Intention-aware recommenders system, a work that incorporates the item category to perform next-item recommendation.',\n",
       " 'Works<~>DeepRec': \"'DeepRec' in the column 'Works' refers to the Deep Recurrent model for multi-behavior sequential recommendation discussed in the text.\",\n",
       " 'Works<~>MBN': 'MBN refers to Multi-behavior network, which is a work that models multi-behavior sequences towards the next-basket recommendation problem using a combination of basket encoder, meta multi-behavior sequence encoder, and recurring-item-aware predictor.',\n",
       " 'Data Perspective<~>Data Perspective': \"In the context of the given table, 'Data Perspective' refers to the type of input data used in the mentioned works, specifically whether the data consists of a sequence of (item, behavior) pairs or some behavior-specific subsequences of items.\",\n",
       " 'Data Perspective<~>A sequence of (item, behavior) pairs': \"In the given table, under the column 'Data Perspective,' 'A sequence of (item, behavior) pairs' refers to a series of data points, each consisting of an item and an associated user behavior.\",\n",
       " 'Data Perspective<~>Some behavior-specific subsequences of items': \"'Some behavior-specific subsequences of items' in the column 'Data Perspective' refers to a subset of items related to specific user behaviors.\",\n",
       " 'Model Perspective<~>Model Perspective': \"The 'Model Perspective' in the table refers to the perspective from which the behavior types are modeled in the given works, specifically whether it is modeled from a local or a local plus global perspective.\",\n",
       " 'Model Perspective<~>Local': \"In the context of the given table, 'Local' in the 'Model Perspective' column refers to modeling approaches that only consider individual data points or sequences without considering their relations to other data points or sequences.\",\n",
       " 'Model Perspective<~>Local + Global': \"In the context of the table, 'Local + Global' in the 'Model Perspective' column for reference [b1e6c16] refers to a model that utilizes both local and global perspectives. The local perspective refers to modeling user behavior from their own client, while the global perspective involves modeling behavior from the cloud.\",\n",
       " 'Features<~>Features': \"In the context of the table, 'Features' refers to the unique aspects or characteristics of each cited work in the field of multi-behavior sequential recommendation based on RNN learning architecture.\",\n",
       " 'Features<~>Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.': \"'Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix' in the context of the table refers to the use of a matrix to model the transitions between different behavior types in a user's sequence of interactions, allowing for the modeling of the influence of heterogeneous behaviors on each other.\",\n",
       " 'Features<~>Leverage GRU and attention mechanism simultaneously.': \"'Leverage GRU and attention mechanism simultaneously' in the context of the table refers to using GRU (Gated Recurrent Unit) and attention mechanism together in the modeling process.\",\n",
       " 'Features<~>Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.': \"In the context of the table, 'Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.' in the column 'Features' refers to creating the Contextual Long Short-Term Memory (CLSTM) and Bidirectional Contextual Long Short-Term Memory (Bi-CLSTM) models, where the behavior vector acts as input context in LSTM.\",\n",
       " 'Features<~>Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.': \"In the context of the table, 'Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.' in the Features column refers to designing models that handle multiple behavior types in a basket recommendation system, with some parameters shared between the behavior types and others unique to each behavior type.\",\n",
       " 'Features<~>Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.': \"The statement 'Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior' in the 'Features' column of the table refers to the use of GRU and attention mechanism together in modeling specific interactive behaviors, such as swipe, touch, and browse, in a given sequence.\",\n",
       " 'Features<~>Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.': \"In the context of the table, 'Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.' in the column 'Features' refers to designing a variant of LSTM called Behavior-LSTM, which includes behavior gate and time gate in addition to attention mechanism, and takes into consideration the category of the items to improve sequential recommendation performance.\",\n",
       " 'Features<~>Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.': \"The feature 'Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items' in the context of the table refers to the use of Soft-MGRU, which is a multi-behavior gated recurrent unit that shares parameters between behaviors, utilizes an attention mechanism to better capture user intentions, and takes into account the category of the items being considered. Specifically, Soft-MGRU is one type of multi-behavior GRU units used in the Intention-aware recommender system (IARS) to capture multiple intentions of the user.\",\n",
       " 'Features<~>Utilizing multi-behavior sequence data to make privacy-preserving recommendation.': \"'Utilizing multi-behavior sequence data to make privacy-preserving recommendation.' in the column 'Features' refers to the use of multi-behavior sequence data to make recommendations in a privacy-preserving manner using RNN learning architecture.\",\n",
       " 'Features<~>The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.': \"In the context of the table, 'The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.' in the 'Features' column refers to a method in Multi-behavior Network (MBN) where the learned multi-behavior information is assigned to different Behavior-RNN layers at the Meta RNN layer by gathering and then scattering, which is a more explicit way to model intra-behavioral and inter-behavioral sequence information.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[26][\"context_autogenerated\"][\"glossary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d2dd09c-61a5-426f-8b70-725729b5d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for dataset_table in dataset_2308_high_quality_sample:\n",
    "    total += len(dataset_table[\"context_autogenerated\"][\"glossary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4139981c-213c-4a75-a7ef-31f652e784c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d966f96-1a2a-4f38-8bb2-16336a00c5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table': '<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"left\">Defense Method</cell>\\n<cell right-border=\"true\" halign=\"left\">Type</cell>\\n<cell halign=\"left\">Time Cost</cell>\\n<cell>Description</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">Standard</cell>\\n<cell right-border=\"true\" halign=\"left\">4*Data augmentations</cell>\\n<cell halign=\"left\">Low</cell>\\n<cell>Random image cropping and flipping</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">CutOut\\xa0<cit sha=\"401c9eff85fb4a8b8b4fdfa18023550fbebfded4\"><ref target=\"bid24\"/></cit>{{cite:401c9ef}}</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">Low</cell>\\n<cell>Random image erasing</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">MixUp\\xa0<cit sha=\"01545b6a8afa3b8e3be1dba5a5dba35ae9a68e34\"><ref target=\"bid26\"/></cit>{{cite:01545b6}}</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">Low</cell>\\n<cell>Random image blending</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">CutMix\\xa0<cit sha=\"bb672ed9b35a0ef2c6004398687964331e2b4cf8\"><ref target=\"bid25\"/></cit>{{cite:bb672ed}}</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">Low</cell>\\n<cell>Random image cutting and stitching</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">Gaussian (used in\\xa0<cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\\n<cell right-border=\"true\" halign=\"left\">5*Data preprocessing</cell>\\n<cell halign=\"left\">Low</cell>\\n<cell>Image blurring with a Gaussian kernel</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">BDR (used in\\xa0<cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">Low</cell>\\n<cell>Bit-depth reduction</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">Gray\\xa0(used in <cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">Low</cell>\\n<cell>Image grayscale transformation</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">JPEG\\xa0(used in <cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">Low</cell>\\n<cell>Image compression</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">AVATAR\\xa0<cit sha=\"8efa7c360ad3a59b806714d667cbe16e2e5a0c6d\"><ref target=\"bid20\"/></cit>{{cite:8efa7c3}}</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">High</cell>\\n<cell>Image corruption and restoration</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">U-Lite\\xa0<cit sha=\"c89b242d22c52c5dfa79a8f3ee52f346731520d1\"><ref target=\"bid19\"/></cit>{{cite:c89b242}}</cell>\\n<cell right-border=\"true\" halign=\"left\">3*Training-phase defense</cell>\\n<cell halign=\"left\">Low</cell>\\n<cell>Stronger data augmentations</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">U-Max\\xa0<cit sha=\"c89b242d22c52c5dfa79a8f3ee52f346731520d1\"><ref target=\"bid19\"/></cit>{{cite:c89b242}}</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">High</cell>\\n<cell>Adversarial augmentations</cell>\\n</row><row><cell right-border=\"true\" halign=\"left\">AT\\xa0<cit sha=\"5437669c43abbb22403f84917671e44a916b8d56\"><ref target=\"bid33\"/></cit>{{cite:5437669}}</cell>\\n<cell right-border=\"true\" halign=\"left\"/>\\n<cell halign=\"left\">High</cell>\\n<cell>Adversarial training</cell>\\n</row></table>',\n",
       " 'caption': 'NO_CAPTION',\n",
       " 'type': 'table'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[22][\"table_unfiltered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70536873-62d3-4d56-8484-e42010298b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'References': ['{{cite:bcf84d7}}',\n",
       "  '{{cite:2f67c58}}',\n",
       "  '{{cite:909f272}}',\n",
       "  '{{cite:8dbb1e8}}',\n",
       "  '{{cite:9641864}}',\n",
       "  '{{cite:c76954f}}',\n",
       "  '{{cite:82d42f0}}',\n",
       "  '{{cite:b1e6c16}}',\n",
       "  '{{cite:15341b1}}'],\n",
       " 'Works': ['RLBL',\n",
       "  'RIB',\n",
       "  'BINN',\n",
       "  'CBS',\n",
       "  'DIPN',\n",
       "  'HUP',\n",
       "  'IARS',\n",
       "  'DeepRec',\n",
       "  'MBN'],\n",
       " 'Data Perspective': ['A sequence of (item, behavior) pairs',\n",
       "  'A sequence of (item, behavior) pairs',\n",
       "  'A sequence of (item, behavior) pairs',\n",
       "  'Some behavior-specific subsequences of items',\n",
       "  'Some behavior-specific subsequences of items',\n",
       "  'A sequence of (item, behavior) pairs',\n",
       "  'A sequence of (item, behavior) pairs',\n",
       "  'Some behavior-specific subsequences of items',\n",
       "  'Some behavior-specific subsequences of items'],\n",
       " 'Model Perspective': ['Local',\n",
       "  'Local',\n",
       "  'Local',\n",
       "  'Local',\n",
       "  'Local',\n",
       "  'Local',\n",
       "  'Local',\n",
       "  'Local + Global',\n",
       "  'Local'],\n",
       " 'Features': ['Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.',\n",
       "  'Leverage GRU and attention mechanism simultaneously.',\n",
       "  'Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.',\n",
       "  'Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.',\n",
       "  'Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.',\n",
       "  'Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.',\n",
       "  'Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.',\n",
       "  'Utilizing multi-behavior sequence data to make privacy-preserving recommendation.',\n",
       "  'The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[26][\"table_json\"][\"table_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "facfd4a1-bf36-4cf3-9b2f-5193a26f2383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for table in dataset_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b4d481a1-9ad8-43b9-b7e5-c16e34d9edbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2308.00729v1', '2308.06173v1', '2308.07107v2', '2308.00937v1', '2308.08407v1', '2308.06953v3', '2308.06966v1', '2308.14430v1', '2308.14371v2', '2308.09090v1', '2308.08794v1', '2308.03149v1', '2308.13266v2', '2308.08643v1', '2308.10631v2', '2308.02151v1', '2308.03108v1'])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualization_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ca4f72cf-71f1-4534-b190-2fd877a3a5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(contextualization_inputs.keys()).index('2308.09090v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6ffc6357-7d1b-458d-aba0-810b34b8589f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the following text from a scientific paper, answer the question about the table that follows:\n",
      "\n",
      "Title: Data-driven Integrated Sensing and Communication: Recent Advances, Challenges, and Future Prospects\n",
      "\n",
      "Abstract: Integrated Sensing and Communication (ISAC), combined with data-driven approaches, has emerged as a highly significant field, garnering considerable attention from academia and industry. Its potential to enable wide-scale applications in the future sixth-generation (6G) networks has led to extensive recent research efforts. Machine learning (ML) techniques, including $K$-nearest neighbors (KNN), support vector machines (SVM), deep learning (DL) architectures, and reinforcement learning (RL) algorithms, have been deployed to address various design aspects of ISAC and its diverse applications. Therefore, this paper aims to explore integrating various ML techniques into ISAC systems, covering various applications. These applications span intelligent vehicular networks, encompassing unmanned aerial vehicles (UAVs) and autonomous cars, as well as radar applications, localization and tracking, millimeter wave (mmWave) and Terahertz (THz) communication, and beamforming. The contributions of this paper lie in its comprehensive survey of ML-based works in the ISAC domain and its identification of challenges and future research directions. By synthesizing the existing knowledge and proposing new research avenues, this survey serves as a valuable resource for researchers, practitioners, and stakeholders involved in advancing the capabilities of ISAC systems in the context of 6G networks.\n",
      "\n",
      "Section: Data-driven methods for channel estimation and IRS\n",
      "Efficient CE is crucial for characterizing received symbols in wireless systems with time-varying and/or frequency-selective channels. Huang et al. {{cite:cc21066}} proposed a MIMO radar-based CE framework that divides into two stages: AoA/AoDs estimation at the radar module and gain estimation at the communication module. arrival/departure (AoA/AoDs) were obtained using a model-based algorithm, while gains were obtained using a residual denoising autoencoder (RDAE). The RDAE was trained to denoise input signals, where the least squares (LS) method is applied to the output for gain estimation.\n",
      "\n",
      "On the other hand, IRS is a physical surface with reflecting elements that are set up in propagation environments to overcome the coverage holes that occur in wireless communication {{cite:e041f88}}. Fig. {{figure:bb1ceac7-96e9-4708-9af7-1c17ed29f445}}  shows examples for IRS deployment in different propagation settings. Dynamic V2I scenarios in IRS-assisted ISAC systems were considered by Wang et al. {{cite:0af48c5}}. To estimate AoAs at both the base station and deployed IRS, the authors proposed CAP-Net, a DNN consisting of convolutional layers and LSTM units. CAP-Net was trained using historical covariance data of received echo to predict AoAs.\n",
      "\n",
      "CE becomes critical in IRS-assisted systems as passive IRS cannot perform signal processing. Therefore, Zhang et al. {{cite:1b511a4}} presented a self-supervised learning approach to IRS-CE. During training, a DNN learns to output signals, similar to the original, when provided with the noisy version of the signal. The trained denoiser is used to improve channel estimation. The approach is shown to be suitable for various communication systems, including ISAC.\n",
      "\n",
      "CE in multiple-input-single-output (MISO) IRS-assisted ISAC systems was considered in {{cite:23432d7}}. The problem was divided into three stages: estimating direct SAC channels in the first stage, individually reflecting communication and sensing channels in the second and third stages, respectively. To accommodate the inherent propagation differences between direct and reflected channels in a full-duplex ISAC base station, two CNNs were designed and trained to make predictions at all three stages. Table {{table:67e1500f-559c-4454-a570-60b8d6b5992c}}  summarizes the works using DL-ISAC techniques in for IRS/CE.\n",
      "\n",
      "\n",
      "---\n",
      "Based on the above text, in the context of the following table, what does 'Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels' in the column 'Output' refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\n",
      "Table: 67e1500f-559c-4454-a570-60b8d6b5992c\n",
      "         References                                             Output\n",
      "0  {{cite:0af48c5}}                                               AoAs\n",
      "1  {{cite:1b511a4}}                                 Channel estimation\n",
      "2  {{cite:23432d7}}  Direct (DE-CNN) and reflected (RE-CNN) sensing...\n",
      "Caption: Summary of key works in DL-ISAC for channel estimation and IRS.\n"
     ]
    }
   ],
   "source": [
    "print(contextualization_inputs['2308.09090v1']['inputs']['instruction_sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "3e506786-c539-4cfb-827c-3e65a848d9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['References', 'Methods', '# Examples', 'Generator', 'Synthetic Data', 'Filter Method', \"LLMs' tuning\"])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in dataset_2308_high_quality_sample if t['paper_id'] == '2308.07107v2'][0]['table_json']['table_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "c32e6c4c-cc9f-4efc-a5e5-bcb2684c9442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Dataset<~>Dataset', 'Technology<~>Technology', 'Technology<~>IRS-assisted V2I', 'Technology<~>General applications', 'Technology<~>IRS-assisted ISAC MISO', 'AI technique<~>AI technique', 'AI technique<~>DL', 'Criteria<~>Criteria', 'Criteria<~>CAP-Net (conv layers and LSTM)', 'Criteria<~>CNN (Trained as a denoiser)', 'Criteria<~>1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)', 'Input<~>Input', 'Input<~>Historical covariance of the received echo', 'Input<~>Channel estimated by LS method', 'Input<~>1.Received direct signals2.Total received signals and DE-CNN estimation', 'Output<~>Output', 'Output<~>AoAs', 'Output<~>Channel estimation', 'Output<~>Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels'])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualization_inputs['2308.09090v1']['glossary'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5baddab5-4e08-4fb0-a23b-fd0002ae9e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Dataset<~>Dataset', 'Methods<~>Methods', 'Methods<~>REALM', 'Methods<~>RAG', 'Methods<~>REPLUG', 'Methods<~>Atlas', 'Methods<~>RETA-LLM', 'Methods<~>RALM', 'Methods<~>RETRO', 'Methods<~>IRCoT', 'Methods<~>FLARE', 'Backbone models<~>Backbone models', 'Backbone models<~>BERT', 'Backbone models<~>BART', 'Backbone models<~>GPT', 'Backbone models<~>T5', 'Backbone models<~>Gopher', 'Backbone models<~>LLaMA & GLM & GPT', 'Backbone models<~>LLaMA & OPT & GPT', 'Backbone models<~>Transformer', 'Backbone models<~>Flan-T5 & GPT', 'Where to incorporate retrieval<~>Where to incorporate retrieval', 'Where to incorporate retrieval<~>Input layer', 'Where to incorporate retrieval<~>Attention layer', 'When to retrieve<~>When to retrieve', 'When to retrieve<~>In the beginning', 'When to retrieve<~>During generation (every n tokens)', 'When to retrieve<~>During generation (every sentence)', 'When to retrieve<~>During generation (aperiodic)', 'How to use LLMs<~>How to use LLMs', 'How to use LLMs<~>Fine-tuning', 'How to use LLMs<~>Prompting', 'How to use LLMs<~>Training from scratch'])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualization_inputs['2308.07107v2']['glossary'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "c8e4262a-3129-4e19-86d4-8af37a8b4121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Dataset<~>Dataset\": \"In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.\",\n",
      "  \"Technology<~>Technology\": \"In the context of the table, 'Technology' refers to the specific application domain of the DL-ISAC techniques used for channel estimation and IRS, such as IRS-assisted V2I, general applications, and IRS-assisted ISAC MISO.\",\n",
      "  \"Technology<~>IRS-assisted V2I\": \"IRS-assisted V2I refers to Intelligent Reflecting Surface (IRS) assisted vehicle-to-infrastructure (V2I) communication systems.\",\n",
      "  \"Technology<~>General applications\": \"'General applications' in the column 'Technology' refers to applications of deep learning in ISAC systems beyond Intelligent Reflecting Surface (IRS)-assisted ISAC and V2I communications.\",\n",
      "  \"Technology<~>IRS-assisted ISAC MISO\": \"IRS-assisted ISAC MISO refers to Integrated Sensing and Communication systems that are assisted by an Intelligent Reflecting Surface (IRS) in a Multiple-Input-Single-Output (MISO) configuration.\",\n",
      "  \"AI technique<~>AI technique\": \"In the context of the table, 'AI technique' refers to deep learning (DL) algorithms used for channel estimation and IRS in Integrated Sensing and Communication (ISAC) systems.\",\n",
      "  \"AI technique<~>DL\": \"The 'DL' in the column 'AI technique' refers to Deep Learning.\",\n",
      "  \"Criteria<~>Criteria\": \"The term 'Criteria' in the table refers to the specific methods or techniques used in the referenced works for channel estimation and IRS in DL-ISAC systems.\",\n",
      "  \"Criteria<~>CAP-Net (conv layers and LSTM)\": \"CAP-Net in the table refers to a Deep Neural Network consisting of convolutional layers and Long Short-Term Memory (LSTM) units for channel estimation and Angle of Arrival estimation in IRS-assisted ISAC systems.\",\n",
      "  \"Criteria<~>CNN (Trained as a denoiser)\": \"The term 'CNN (Trained as a denoiser)' in the table refers to a convolutional neural network that has been trained as a denoiser to improve channel estimation in IRS-assisted ISAC systems.\",\n",
      "  \"Criteria<~>1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)\": \"The criteria column '1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)' in the table refers to two different types of convolutional neural networks used for channel estimation in IRS-assisted ISAC systems, one for direct estimation and the other for reflected estimation.\",\n",
      "  \"Input<~>Input\": \"The 'Input' column in the table refers to the specific input data used in each of the key works summarized in the table for deep learning-based integrated sensing and communication (DL-ISAC) techniques in channel estimation and intelligent reflecting surface (IRS) applications. For example, the first row uses historical covariance of the received echo as input, while the second row uses the channel estimated by the least squares method as input, and the third row uses received direct signals and the total received signals as inputs.\",\n",
      "  \"Input<~>Historical covariance of the received echo\": \"The 'Historical covariance of the received echo' in the table refers to the historical data of the received echo signals used for training a deep neural network (CAP-Net) to estimate Angle of Arrival (AoAs) in IRS-assisted ISAC systems.\",\n",
      "  \"Input<~>Channel estimated by LS method\": \"The 'Channel estimated by LS method' in the table refers to a channel estimate obtained using the least squares (LS) method.\",\n",
      "  \"Input<~>1.Received direct signals2.Total received signals and DE-CNN estimation\": \"The second input in the table for the reference [23432d7] refers to both received direct signals and total received signals used for DE-CNN estimation in DL-ISAC for channel estimation and IRS.\",\n",
      "  \"Output<~>Output\": \"The 'Output' column in the table refers to the results or outcomes of the studies or techniques mentioned in each corresponding reference. In the given context, the output for reference [0] is AoAs, for reference [1] is channel estimation, and for reference [2] it is direct (DE-CNN) and reflected (RE-CNN) sensing.\",\n",
      "  \"Output<~>AoAs\": \"The output column 'AoAs' in the table refers to Angle of Arrivals used for channel estimation and estimation of AoAs in IRS-assisted systems.\",\n",
      "  \"Output<~>Channel estimation\": \"In the context of the table, 'Channel estimation' in the Output column refers to the estimation of wireless communication channels.\",\n",
      "  \"Output<~>Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels\": \"The term \\\"Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels\\\" in the output column of the table refers to the estimation and processing of direct and reflected channels in multiple-input-single-output (MISO) IRS-assisted ISAC systems using convolutional neural networks (CNNs).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(contextualization_inputs['2308.09090v1']['glossary'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bc3992b5-1090-464c-9ccb-0428741abe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', '_pdf_hash', '_source_hash', '_source_name', '_table_hash', 'table_html', 'table_json', 'bib_hash', 'row_bib_map', 'table_unfiltered', '_table_hash_full_text', 'context', '_full_text_table_hash'])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bbf26638-7514-4d9a-bbb3-71d822bfba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': 'b5e4c46ac09d5de2051998c9f741ef480fab2ebe',\n",
       " 'externalIds': {'DBLP': 'conf/mm/LiuWYSTZWL23',\n",
       "  'ArXiv': '2308.00729',\n",
       "  'DOI': '10.1145/3581783.3611795',\n",
       "  'CorpusId': 260378902},\n",
       " 'corpusId': 260378902,\n",
       " 'title': 'Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment',\n",
       " 'abstract': 'Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.',\n",
       " 'isOpenAccess': True,\n",
       " 'openAccessPdf': {'url': 'https://dl.acm.org/doi/pdf/10.1145/3581783.3611795',\n",
       "  'status': 'HYBRID'}}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_map = {md['ArXiv']: md for md in metadata_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e7141401-ce17-4aa0-87b9-98be7a983743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dataset', 'KoNViD-1k')\n",
      "('Dataset', 'LIVE-VQC')\n",
      "('Dataset', 'YouTube-UGC')\n",
      "('Dataset', 'LSVQ')\n",
      "('Dataset', 'KoNViD-150k')\n",
      "('Dataset', 'Sports-1M')\n",
      "('Dataset', 'Kinetics-400')\n",
      "('Task', 'Task')\n",
      "('Task', 'VQA')\n",
      "('Task', 'classification')\n",
      "('Size', 'Size')\n",
      "('Annotations', 'Annotations')\n",
      "('Annotations', '- (auto.)')\n"
     ]
    }
   ],
   "source": [
    "glossary_v3 = create_glossary(dataset_2308_high_quality_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1f32bed1-d153-4ac0-87ef-793248152fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Dataset',\n",
       "  'Dataset'): \"In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.\",\n",
       " ('Dataset',\n",
       "  'KoNViD-1k'): 'KoNViD-1k is a publicly available no-reference video quality assessment dataset that consists of 1000 videos for training and testing purposes.',\n",
       " ('Dataset',\n",
       "  'LIVE-VQC'): 'LIVE-VQC is a mainstream high-quality annotated Non-Reference Video Quality Assessment (NR-VQA) dataset.',\n",
       " ('Dataset',\n",
       "  'YouTube-UGC'): 'YouTube-UGC in the table refers to a specific dataset named \"YouTube-UGC\" used in the context of video quality assessment.',\n",
       " ('Dataset',\n",
       "  'LSVQ'): 'LSVQ in the given table refers to a public VQA dataset named Large Scale Video Quality (LSVQ) dataset.',\n",
       " ('Dataset',\n",
       "  'KoNViD-150k'): 'KoNViD-150k is a large-scale video quality assessment dataset referenced in the text.',\n",
       " ('Dataset',\n",
       "  'Sports-1M'): 'Sports-1M is a video classification dataset that contains over a million videos. It is not specifically a VQA (Video Quality Assessment) dataset, but rather a video classification dataset. Therefore, it does not directly relate to the context of VQA or quality assessment in the given text.',\n",
       " ('Dataset',\n",
       "  'Kinetics-400'): 'Kinetics-400 is a video classification dataset.',\n",
       " ('Task',\n",
       "  'Task'): \"'Task' in this context refers to the type of dataset being compared, specifically whether it is a Video Quality Assessment (VQA) dataset or a video classification dataset.\",\n",
       " ('Task', 'VQA'): 'VQA in the table refers to Video Quality Assessment tasks.',\n",
       " ('Task',\n",
       "  'classification'): \"The term 'classification' in the 'Task' column of the table refers to datasets that focus on identifying the categories or labels of video content, rather than assessing video quality as in VQA datasets.\",\n",
       " ('Size',\n",
       "  'Size'): \"In the given context, the 'Size' column refers to the number of annotations or data points in each dataset.\",\n",
       " ('Annotations',\n",
       "  'Annotations'): \"In the context of the table, 'Annotations' refer to the number of subjective scores required to produce a valid label for each dataset in the table.\",\n",
       " ('Annotations',\n",
       "  '- (auto.)'): \"The '- (auto.)' in the column 'Annotations' for the reference {{cite:fbaab96}} indicates that the annotations for this dataset were automatically generated rather than manually labeled by human annotators.\"}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "db562348-375e-48ab-b5dc-bfb602f5d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask for \"unanswerable\", \"single sentence\"\n",
    "glossary_v2 = {k: v for k, v in glossary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a73b0797-93ed-42bf-9a5a-b24d8ef5bfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Dataset',\n",
       "  'Dataset'): \"In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.\",\n",
       " ('Dataset',\n",
       "  'KoNViD-1k'): 'KoNViD-1k is a publicly available VQA dataset with 1,200 videos and an average of 114 subjective quality scores per video.',\n",
       " ('Dataset',\n",
       "  'LIVE-VQC'): 'LIVE-VQC is a publicly available VQA dataset with 585 videos and 240 annotations per video.',\n",
       " ('Dataset',\n",
       "  'YouTube-UGC'): 'YouTube-UGC in the table refers to the YouTube Unlabelled Generic Video Dataset used for VQA tasks.',\n",
       " ('Dataset',\n",
       "  'LSVQ'): \"LSVQ in the column 'Dataset' refers to a publicly available Video Quality Assessment dataset with a size of 39,075 videos and 35 annotations per video.\",\n",
       " ('Dataset',\n",
       "  'KoNViD-150k'): 'KoNViD-150k is a VQA dataset mentioned in the text, which contains 153,841 videos.',\n",
       " ('Dataset',\n",
       "  'Sports-1M'): \"'Sports-1M' in the column 'Dataset' refers to a dataset used for video classification, which has a size of 1,133,158 and does not provide annotations for video quality.\",\n",
       " ('Dataset',\n",
       "  'Kinetics-400'): 'Kinetics-400 in the table refers to a dataset used for video classification with a size of 306,245 and 3-5 annotations per video.',\n",
       " ('Task',\n",
       "  'Task'): \"In the given table, 'Task' refers to the type of data analysis being performed on the respective datasets, specifically Video Quality Assessment (VQA) in the case of the first three datasets, and video classification in the case of the last two datasets.\",\n",
       " ('Task',\n",
       "  'VQA'): \"In the context of the table, 'VQA' in the 'Task' column refers to Video Quality Assessment.\",\n",
       " ('Task',\n",
       "  'classification'): \"In the context of the table, 'classification' in the column 'Task' for references 5 and 6 refers to the task of classifying videos into different categories.\",\n",
       " ('Size',\n",
       "  'Size'): 'In the context of the table, \"Size\" refers to the number of videos or data samples in each dataset.',\n",
       " ('Annotations',\n",
       "  'Annotations'): \"In the context of the table, 'Annotations' refer to the number of subjective scores provided for each video in a VQA dataset.\",\n",
       " ('Annotations',\n",
       "  '- (auto.)'): \"'- (auto.)' in the column 'Annotations' for the Sports-1M dataset refers to the fact that the annotations were automatically generated through the use of algorithms or models, rather than manually through crowdsourcing or other subjective methods.\"}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194388f-2f62-4812-9bcf-4717247b328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is ok... but it relies a bit too much on the table... I wonder if we could somehow subset the table and if that would be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2a959af4-5971-428f-89ba-188db5f1a6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the following text from a scientific paper, answer the question about the table that follows:\n",
      "\n",
      "Title: Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment\n",
      "\n",
      "Abstract: Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA.\n",
      "\n",
      "Section: Introduction\n",
      "With the explosive growth of video content-based social media, there has been a tremendous amount of videos produced and shared {{cite:cef100d}}. To guarantee optimal video quality and ensure users' quality of experience (QoE), VQA plays a crucial role in guiding image processing and video coding systems {{cite:9b70eb4}}, {{cite:c6c0872}}. Benefit from the thrive of Deep Neural Networks (DNN), DNN-based VQA methods {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:3dfd446}}, {{cite:9c04ab9}}, {{cite:5d0c0fc}}, {{cite:3237048}} have shown great results on in-the-wild VQA benchmarks {{cite:8f3eb86}}, {{cite:24b4d04}}, {{cite:9be8207}}, {{cite:3dfd446}}.\n",
      "\n",
      "DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. {{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , public VQA datasets are significantly smaller in size when compared to video classification datasets {{cite:fbaab96}}, {{cite:dd352e3}}. The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality {{cite:b5fb90b}}, {{cite:2222c26}}, {{cite:689f656}}, {{cite:11e5a31}}, which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset {{cite:24b4d04}} requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets {{cite:3dfd446}}, {{cite:9c04ab9}}. However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset {{cite:9c04ab9}} revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data.\n",
      "\n",
      "To surmount the constraint of insufficient training data, certain works {{cite:7da4179}}, {{cite:1ad644b}}, {{cite:65d7727}}, {{cite:1cad59a}} attempt to utilize DNNs that have been pre-trained on other extensive datasets, for the purpose of fine-tuning. The majority of these works employ ImageNet {{cite:bad8aed}}, which comprises a vast array of object categories, as a pre-training dataset. By introducing content-aware knowledge which meets the diverse distribution of in-the-wild videos, a considerable enhancement in performance can be obtained in downstream VQA tasks. However, videos captured in-the-wild often encounter unavoidable distortions resulting from extreme shooting conditions, compression, transmission, or other unprofessional operations performed by users {{cite:c6c0872}}. In such scenarios, relying solely on content-aware information is insufficient to ensure quality representation. Furthermore, some other studies {{cite:80cfe88}}, {{cite:48735e5}} seek to leverage motion-aware and compression-aware features obtained from models that are pretrained using spatiotemporal information, simply by concatenating or averaging the features. As shown in Fig. {{table:43c9402e-2914-42dc-99c4-b2424f71f6ad}} , quality-related factors are encompassed within the information of different modalities, and the contribution of each modality may vary depending on the input. Nevertheless, most existing methods do not consider the complete range of video distribution diversity or employ simplistic methods to integrate various types of features, which restricts further improvements in VQA.\n",
      "\n",
      "To address the aforementioned limitations, in this paper, we aim to employ diverse in-the-wild pretrained models to enhance VQA performance from various aspects that may affect video quality. In the case of the image modality, owing to the presence of content labels and a vast number of images closely resembling the actual distribution, quality-related information pertaining to content and distortion perception is included. As for the video modality, the data distribution contains quality-related information such as compression distortions (e.g., blocky artifacts) or motion blur. For the cross-modality of image-text, we use a trained CLIP model {{cite:c682601}}, which efficiently learns visual concepts from natural language supervision, for analysis. By devising appropriate templates, CLIP exhibits remarkable predictive ability without access to VQA annotations. We surmise that during the training of CLIP, certain texts may contain emotional descriptions associated with the quality. These phenomena attest to the practicality of employing in-the-wild models that have been pretrained on diverse modalities of data.\n",
      "\n",
      "Based on the above observations, in this paper, we introduce a new Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework for VQA tasks. Initially, we establish a pool of pre-trained models exhibiting a wide range of diversity, taking into account various aspects of their architectures, pretrained pretext tasks, and pretrained datasets. Subsequently, to dynamically capture the desired quality-related features on a per-sample basis during training, we present a Quality-aware Acquisition Module (QAM) that outputs gating weights of features generated by different pretrained models for aggregation. We also impose an additional sparsity constraint on the distribution of the gating weights, encouraging attention to be focused on more crucial and pertinent features for quality representation. Lastly, the learned quality representation is employed as auxiliary supervisory information, in conjunction with the supervision of the labeled quality score, to facilitate the training of a comparatively lightweight VQA model using knowledge distillation. The effectiveness of our method is evaluated through extensive experiments on three mainstream high-quality annotated NR-VQA datasets. Adapt-DQA models achieve 0.8651, 0.8591, and 0.8729 of SRCC on KoNViD-1k, LIVE-VQC, and YouTube-UGC, improving the state-of-the-art results for these datasets by absolute margins of 0.6%, 1.79%, and 3.88%, respectively.\n",
      "Our contributions are as follows:\n",
      "\n",
      " * To the best of our knowledge, this is the first study to comprehensively investigate the relationships between pretrained models and video quality. We construct a diverse pool that encompasses a broad spectrum of quality-related factors.\n",
      " * We propose the Ada-DQA to leverage these pretrained models for VQA, where the QAM is proposed to capture quality-related features adaptively. Additionally, a sparsity constraint is also attached for the most crucial and relevant features.\n",
      " * We evaluate Ada-DQA on three mainstream NR-VQA benchmarks, surpassing current state-of-the-art methods without using extra training data of QA. Sufficient ablation studies validated the effectiveness of each component.\n",
      "\n",
      "---\n",
      "Based on the above text, in the context of the following table, what does '- (auto.)' in the column 'Annotations' refer to? Answer in a single sentence.\n",
      "Table: 43c9402e-2914-42dc-99c4-b2424f71f6ad\n",
      "         References        Dataset            Task       Size Annotations\n",
      "0  {{cite:24b4d04}}     KoNViD-1k              VQA      1,200         114\n",
      "1  {{cite:8f3eb86}}      LIVE-VQC              VQA        585         240\n",
      "2  {{cite:9be8207}}   YouTube-UGC              VQA      1,380         123\n",
      "3  {{cite:3dfd446}}          LSVQ              VQA     39,075          35\n",
      "4  {{cite:9c04ab9}}   KoNViD-150k              VQA    153,841           5\n",
      "5  {{cite:fbaab96}}     Sports-1M   classification  1,133,158   - (auto.)\n",
      "6  {{cite:dd352e3}}  Kinetics-400   classification    306,245         3-5\n",
      "Caption: Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.\n"
     ]
    }
   ],
   "source": [
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "41044ccd-8428-4a3b-9d88-984299854b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Dataset',\n",
       "  'Dataset'): \"In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.\",\n",
       " ('Dataset', 'KoNViD-1k'): 'test',\n",
       " ('Dataset', 'LIVE-VQC'): 'test',\n",
       " ('Dataset', 'YouTube-UGC'): 'test',\n",
       " ('Dataset', 'LSVQ'): 'test',\n",
       " ('Dataset', 'KoNViD-150k'): 'test',\n",
       " ('Dataset', 'Sports-1M'): 'test',\n",
       " ('Dataset', 'Kinetics-400'): 'test',\n",
       " ('Task', 'Task'): 'test',\n",
       " ('Task', 'VQA'): 'test',\n",
       " ('Task', 'classification'): 'test',\n",
       " ('Size', 'Size'): 'test',\n",
       " ('Annotations', 'Annotations'): 'test',\n",
       " ('Annotations', '- (auto.)'): 'test'}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0339b58-7f0e-45e4-a809-13e00d110b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2483])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a35aa7f8-3827-44c7-913a-e88785c74a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a9c2ca5-6dcf-4a48-8461-223cb39e8d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24da7cab-1a2b-4e9c-b0e0-fa3182383d36'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[0]['_full_text_table_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3306c168-9877-4dd5-8088-c5cf9bd679ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2308.00729v1'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2308_high_quality_sample[0]['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b74fd35b-794f-495e-aa16-3a71c64cb6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2308.00729v1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acbca909-a039-4fbf-aa84-4625eade070e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tabids': ['b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f'],\n",
       " 'corpus_id': 9136312,\n",
       " 'title': 'The Konstanz natural video database (KoNViD-1k)',\n",
       " 'paper_id': 'ae95abd2406ddfab1aa2ffa2413d68b98b1ba21b',\n",
       " 'abstract': 'Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be ‘general purpose’ requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at ‘in the wild’ authentic distortions, depicting a wide variety of content.',\n",
       " 'intro': 'Most of the Internet traffic today stems from user-generated videos on sharing web-sites and social networks. Video sequences pass through several stages of processing before they reach consumers, which often deteriorate visual quality. Moreover, the vast amount of user-generated video content and the increased diversity of end user devices (ranging from smaller and power-constrained mobile devices to large displays such as 4K Ultra HDTVs and TV walls) calls for a broad range of video quality to be supported. Adapting video quality to different use cases has become an important topic for researchers, content providers and distributors [1].\\n\\nAutomatic and accurate prediction of video quality is a basic operation for many video processing applications such as video quality monitoring in transmission protocols, video quality filtering in sharing services, automatic and recommended camera parameter settings during video capturing, and video enhancement. Specifically, no-reference methods attempt to judge the quality of a video sequence without any additional information about the original recorded scene. Such blind methods may apply machine learning techniques to learn from large amounts of annotated data. However, current video quality assessment (VQA) databases contain only a small number of video sequences with little content diversity, thus offering limited support for designing and evaluating noreference VQA methods effectively and fairly.\\n\\nAdditionally, these databases were mostly designed to include only artificially distorted video sequences to simulate quality loss in compression, transmission, and other parts of the video processing and distribution pipeline. Some databases capture imagery with a variety of cameras to encompass authentic video acquisition distortions, however, with content restricted to a small number of physical scenes.\\n\\nWinkler [2] proposed several criteria for quantitative comparisons of source content, test conditions, and subjective ratings, applying them to 27 image and video databases. Most collections have not been found satisfactory in terms of content range and uniformity. Only few databases showed good uniformity for test conditions (image/video quality), but not over the whole quality range. Also the distortion variety was found lacking in most databases covering mainly compression and transmission, but not the many other types of natural distortions found \"in the wild\" [3].\\n\\nTo overcome these limitations we introduce KoNViD-1k, a large publicly available database of video sequences based on YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset [4] with a diverse set of video content. In this paper we report the filtering mechanisms and sampling procedures necessary to construct high-quality VQA databases of this kind, focusing on their usefulness in a variety of applications.\\n\\nIn the next section, we describe the database creation procedure and the set of attributes we have considered to maximise its diversity. Additional information regarding the removal of non-natural video sequences and sampling techniques are provided as well. Next, in Sec. III, we review our crowdsourcing-based process of collecting subjective mean opinion scores (MOS) and detail our results as well as crowd worker statistics. In Sec. IV we relate our database characteristics and creation methodology with other existing works and outline the differences, before discussing conclusions of our work and considering possible future work.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_2308_high_quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1d75a-84b4-4731-97f0-bcd7e5e4f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
