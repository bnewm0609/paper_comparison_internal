{"paper_id": "2308.00729v1", "_pdf_hash": null, "_source_hash": "add89d24da98870765fefba429d689dc4a83485c", "_source_name": "2308.00729v1", "_table_hash": "b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid1\" id-text=\"1\" place=\"t\" rend=\"display\"><head>Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.</head>\n<tr><td halign=\"center\" right-border=\"true\">Dataset</td>\n<td halign=\"center\" right-border=\"true\">Task</td>\n<td halign=\"center\">Size</td>\n<td>Annotations</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">KoNViD-1k <cit sha=\"24b4d04b01098cffe3cb975171aa05132c6a0903\"><ref target=\"bid14\"/></cit>{{cite:24b4d04}}</td>\n<td halign=\"center\" right-border=\"true\">VQA</td>\n<td halign=\"center\">1,200</td>\n<td>114</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">LIVE-VQC <cit sha=\"8f3eb869445ed0622a9879a6f2010828f6039e8b\"><ref target=\"bid13\"/></cit>{{cite:8f3eb86}}</td>\n<td halign=\"center\" right-border=\"true\">VQA</td>\n<td halign=\"center\">585</td>\n<td>240</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">YouTube-UGC <cit sha=\"9be8207b7ba6b4f423a574f686110ed04d5a3d91\"><ref target=\"bid15\"/></cit>{{cite:9be8207}}</td>\n<td halign=\"center\" right-border=\"true\">VQA</td>\n<td halign=\"center\">1,380</td>\n<td>123</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">LSVQ <cit sha=\"3dfd446db36563cc1eeb028310478bbb775a0237\"><ref target=\"bid9\"/></cit>{{cite:3dfd446}}</td>\n<td halign=\"center\" right-border=\"true\">VQA</td>\n<td halign=\"center\">39,075</td>\n<td>35</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">KoNViD-150k <cit sha=\"9c04ab9115e73e7300a6077937ec1b23e3fdf820\"><ref target=\"bid10\"/></cit>{{cite:9c04ab9}}</td>\n<td halign=\"center\" right-border=\"true\">VQA</td>\n<td halign=\"center\">153,841</td>\n<td>5</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">Sports-1M <cit sha=\"fbaab966ad1e0efdeb5b96e80b5b467ba60eeced\"><ref target=\"bid1\"/></cit>{{cite:fbaab96}}</td>\n<td halign=\"center\" right-border=\"true\">classification</td>\n<td halign=\"center\">1,133,158</td>\n<td>- (<hi rend=\"it\">auto.</hi>)</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">Kinetics-400 <cit sha=\"dd352e3c3918b783377312dd4399a284589a24d1\"><ref target=\"bid2\"/></cit>{{cite:dd352e3}}</td>\n<td halign=\"center\" right-border=\"true\">classification</td>\n<td halign=\"center\">306,245</td>\n<td>3-5</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Dataset", "Task", "Size", "Annotations"], ["KoNViD-1k {{cite:24b4d04}}", "VQA", "1,200", "114"], ["LIVE-VQC {{cite:8f3eb86}}", "VQA", "585", "240"], ["YouTube-UGC {{cite:9be8207}}", "VQA", "1,380", "123"], ["LSVQ {{cite:3dfd446}}", "VQA", "39,075", "35"], ["KoNViD-150k {{cite:9c04ab9}}", "VQA", "153,841", "5"], ["Sports-1M {{cite:fbaab96}}", "classification", "1,133,158", "- (auto.)"], ["Kinetics-400 {{cite:dd352e3}}", "classification", "306,245", "3-5"]], "table_dict": {"References": ["{{cite:24b4d04}}", "{{cite:8f3eb86}}", "{{cite:9be8207}}", "{{cite:3dfd446}}", "{{cite:9c04ab9}}", "{{cite:fbaab96}}", "{{cite:dd352e3}}"], "Dataset": ["KoNViD-1k ", "LIVE-VQC ", "YouTube-UGC ", "LSVQ ", "KoNViD-150k ", "Sports-1M ", "Kinetics-400 "], "Task": ["VQA", "VQA", "VQA", "VQA", "VQA", "classification", "classification"], "Size": ["1,200", "585", "1,380", "39,075", "153,841", "1,133,158", "306,245"], "Annotations": ["114", "240", "123", "35", "5", "- (auto.)", "3-5"]}}, "bib_hash": ["24b4d04b01098cffe3cb975171aa05132c6a0903", "8f3eb869445ed0622a9879a6f2010828f6039e8b", "9be8207b7ba6b4f423a574f686110ed04d5a3d91", "3dfd446db36563cc1eeb028310478bbb775a0237", "9c04ab9115e73e7300a6077937ec1b23e3fdf820", "fbaab966ad1e0efdeb5b96e80b5b467ba60eeced", "dd352e3c3918b783377312dd4399a284589a24d1"], "row_bib_map": [{"bib_hash_or_arxiv_id": "24b4d04b01098cffe3cb975171aa05132c6a0903", "row": 0, "corpus_id": 9136312, "type": "ref"}, {"bib_hash_or_arxiv_id": "8f3eb869445ed0622a9879a6f2010828f6039e8b", "row": 1, "corpus_id": 52285071, "type": "ref"}, {"bib_hash_or_arxiv_id": "9be8207b7ba6b4f423a574f686110ed04d5a3d91", "row": 2, "corpus_id": 119309258, "type": "ref"}, {"bib_hash_or_arxiv_id": "3dfd446db36563cc1eeb028310478bbb775a0237", "row": 3, "corpus_id": 227210156, "type": "ref"}, {"bib_hash_or_arxiv_id": "9c04ab9115e73e7300a6077937ec1b23e3fdf820", "row": 4, "corpus_id": 234788066, "type": "ref"}, {"bib_hash_or_arxiv_id": "fbaab966ad1e0efdeb5b96e80b5b467ba60eeced", "row": 5, "corpus_id": 206592218, "type": "ref"}, {"bib_hash_or_arxiv_id": "dd352e3c3918b783377312dd4399a284589a24d1", "row": 6, "corpus_id": 27300853, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid1\" place=\"t\"><head>Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.</head>\n<row><cell right-border=\"true\" halign=\"center\">Dataset</cell>\n<cell right-border=\"true\" halign=\"center\">Task</cell>\n<cell halign=\"center\">Size</cell>\n<cell>Annotations</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">KoNViD-1k <cit sha=\"24b4d04b01098cffe3cb975171aa05132c6a0903\"><ref target=\"bid14\"/></cit>{{cite:24b4d04}}</cell>\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\n<cell halign=\"center\">1,200</cell>\n<cell>114</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">LIVE-VQC <cit sha=\"8f3eb869445ed0622a9879a6f2010828f6039e8b\"><ref target=\"bid13\"/></cit>{{cite:8f3eb86}}</cell>\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\n<cell halign=\"center\">585</cell>\n<cell>240</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">YouTube-UGC <cit sha=\"9be8207b7ba6b4f423a574f686110ed04d5a3d91\"><ref target=\"bid15\"/></cit>{{cite:9be8207}}</cell>\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\n<cell halign=\"center\">1,380</cell>\n<cell>123</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">LSVQ <cit sha=\"3dfd446db36563cc1eeb028310478bbb775a0237\"><ref target=\"bid9\"/></cit>{{cite:3dfd446}}</cell>\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\n<cell halign=\"center\">39,075</cell>\n<cell>35</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">KoNViD-150k <cit sha=\"9c04ab9115e73e7300a6077937ec1b23e3fdf820\"><ref target=\"bid10\"/></cit>{{cite:9c04ab9}}</cell>\n<cell right-border=\"true\" halign=\"center\">VQA</cell>\n<cell halign=\"center\">153,841</cell>\n<cell>5</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">Sports-1M <cit sha=\"fbaab966ad1e0efdeb5b96e80b5b467ba60eeced\"><ref target=\"bid1\"/></cit>{{cite:fbaab96}}</cell>\n<cell right-border=\"true\" halign=\"center\">classification</cell>\n<cell halign=\"center\">1,133,158</cell>\n<cell>- (<hi rend=\"it\">auto.</hi>)</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">Kinetics-400 <cit sha=\"dd352e3c3918b783377312dd4399a284589a24d1\"><ref target=\"bid2\"/></cit>{{cite:dd352e3}}</cell>\n<cell right-border=\"true\" halign=\"center\">classification</cell>\n<cell halign=\"center\">306,245</cell>\n<cell>3-5</cell>\n</row></table>", "caption": "Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.", "type": "table"}, "_table_hash_full_text": "f235824f-a02e-4be0-83c8-b5c2551e9a8f", "context": {"caption": "Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.", "mention_paragraphs": ["DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. 1, public VQA datasets are significantly smaller in size when compared to video classification datasets (Karpathy et al., 2014; Kay et al., 2017). The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality (Chen et al., 2010; Ho\u00dffeld et al., 2014; Shahid et al., 2014; Chen et al., 2015), which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset (Hosu et al., 2017) requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets (Ying et al., 2021; G\u00f6tz-Hahn et al., 2021). However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset (G\u00f6tz-Hahn et al., 2021) revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data."], "glossary": {"dataset": "The dataset the paper uses for to evaluate its task.", "task": "The task the dataset was designed for.", "VQA": "An acronym that stands for video quality assessment.", "classification": "The video classification task.", "Annotations": "The number of annotations per video.", "Size": "The number of annotated videos in the dataset"}}, "_full_text_table_hash": "43c9402e-2914-42dc-99c4-b2424f71f6ad", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Dataset<~>KoNViD-1k": "KoNViD-1k refers to a public VQA dataset with a size of 1,000 hours.", "Dataset<~>LIVE-VQC": "LIVE-VQC refers to a mainstream no-reference video quality assessment benchmark.", "Dataset<~>YouTube-UGC": "YouTube-UGC in the table refers to the Unlabelled YouTube Universal Dataset, which is a publicly available dataset for video understanding research. It is commonly used for video classification tasks but is also used in VQA research.", "Dataset<~>LSVQ": "LSVQ in the table refers to a specific public VQA dataset. However, without additional context, it is unclear what the acronym stands for in this particular case.", "Dataset<~>KoNViD-150k": "KoNViD-150k is a public VQA dataset with a significantly larger size compared to KoNViD-1k.", "Dataset<~>Sports-1M": "Sports-1M in the table refers to the Sports-1M dataset, which is a large-scale video dataset that contains over one million videos of various sports events. It is a video classification dataset and is different from the video quality assessment (VQA) datasets mentioned in the text.", "Dataset<~>Kinetics-400": "Kinetics-400 is a large-scale video classification dataset.", "Task<~>Task": "In the context of the table provided, 'Task' refers to the type of dataset being compared, specifically either Video Quality Assessment (VQA) or video classification.", "Task<~>VQA": "VQA in the table refers to Video Quality Assessment.", "Task<~>classification": "The column 'Task' in the table refers to the type of dataset: 'VQA' stands for Video Quality Assessment, while 'classification' refers to the task of labeling video data based on pre-defined categories or classes.", "Size<~>Size": "In the given table, \"Size\" refers to the number of data samples or records in each dataset.", "Annotations<~>Annotations": "In the context of the table, 'Annotations' refer to the number of subjective scores required for obtaining a valid label for each video in public VQA datasets.", "Annotations<~>- (auto.)": "'- (auto.)' in the column 'Annotations' for the reference {{cite:fbaab96}} refers to automatically generated annotations, likely obtained through methods other than crowdsourcing."}}, "title": "Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment", "abstract": "Video quality assessment (VQA) has attracted growing attention in recent years. While the great expense of annotating large-scale VQA datasets has become the main obstacle for current deep-learning methods. To surmount the constraint of insufficient training data, in this paper, we first consider the complete range of video distribution diversity (i.e. content, distortion, motion) and employ diverse pretrained models (e.g. architecture, pretext task, pre-training dataset) to benefit quality representation. An Adaptive Diverse Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture desired quality-related features generated by these frozen pretrained models. By leveraging the Quality-aware Acquisition Module (QAM), the framework is able to extract more essential and relevant features to represent quality. Finally, the learned quality representation is utilized as supplementary supervisory information, along with the supervision of the labeled quality score, to guide the training of a relatively lightweight VQA model in a knowledge distillation manner, which largely reduces the computational cost during inference. Experimental results on three mainstream no-reference VQA benchmarks clearly show the superior performance of Ada-DQA in comparison with current state-of-the-art approaches without using extra training data of VQA."}
{"paper_id": "2308.06173v1", "_pdf_hash": null, "_source_hash": "1a10f4628b02b2eb510343398882e048e9294789", "_source_name": "2308.06173v1", "_table_hash": "da17cb75-5655-4f95-bffc-c918a7ecb473", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid174\" id-text=\"14\" place=\"!htp\" rend=\"display\" starred=\"true\"><head>Physical adversarial attacks against <hi rend=\"bold\">Semantic Segmentation</hi> tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.</head>\n<tr><td halign=\"left\"><hi rend=\"bold\">Attack</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Attacker\u2019s</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Robustness</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Stealthiness</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Physical</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Space</hi></td>\n</tr><tr><td halign=\"left\"/>\n<td halign=\"left\"><hi rend=\"bold\">Knowledge</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Technique</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Technique</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">test type</hi></td>\n<td halign=\"left\"/>\n</tr><tr><td halign=\"left\">IPatch <cit sha=\"5e18118b535c0e44d7d004b4c8114668660f16e2\"><ref target=\"bid131\"/></cit>{{cite:5e18118}}</td>\n<td halign=\"left\">White-box</td>\n<td halign=\"left\">EOT</td>\n<td halign=\"left\">-</td>\n<td halign=\"left\">Static</td>\n<td halign=\"left\">2D</td>\n</tr><tr><td halign=\"left\">SSAttack <cit sha=\"935b9f53040da5646e8ad6f12786a48e460e2eba\"><ref target=\"bid132\"/></cit>{{cite:935b9f5}}</td>\n<td halign=\"left\">White-box</td>\n<td halign=\"left\">EOT</td>\n<td halign=\"left\">-</td>\n<td halign=\"left\">Static</td>\n<td halign=\"left\">2D</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Attack", "Attacker\u2019s", "Robustness", "Stealthiness", "Physical", "Space"], ["", "Knowledge", "Technique", "Technique", "test type", ""], ["IPatch {{cite:5e18118}}", "White-box", "EOT", "-", "Static", "2D"], ["SSAttack {{cite:935b9f5}}", "White-box", "EOT", "-", "Static", "2D"]], "table_dict": {"References": ["{{cite:5e18118}}", "{{cite:935b9f5}}"], "Attack": ["IPatch ", "SSAttack "], "Attacker's Knowledge": ["White-box", "White-box"], "Robustness Technique": ["EOT", "EOT"], "Stealthiness Technique": ["-", "-"], "Physical test type": ["Static", "Static"], "Space": ["2D", "2D"]}}, "bib_hash": ["5e18118b535c0e44d7d004b4c8114668660f16e2", "935b9f53040da5646e8ad6f12786a48e460e2eba"], "row_bib_map": [{"bib_hash_or_arxiv_id": "5e18118b535c0e44d7d004b4c8114668660f16e2", "row": 0, "corpus_id": 233481695, "type": "ref"}, {"bib_hash_or_arxiv_id": "935b9f53040da5646e8ad6f12786a48e460e2eba", "row": 1, "corpus_id": 237048246, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"14\" id=\"uid174\" starred=\"true\" place=\"!htp\"><head>Physical adversarial attacks against <hi rend=\"bold\">Semantic Segmentation</hi> tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.</head>\n<row><cell halign=\"left\"><hi rend=\"bold\">Attack</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Attacker\u2019s</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Robustness</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Stealthiness</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Physical</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Space</hi></cell>\n</row><row><cell halign=\"left\"/>\n<cell halign=\"left\"><hi rend=\"bold\">Knowledge</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Technique</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Technique</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">test type</hi></cell>\n<cell halign=\"left\"/>\n</row><row><cell halign=\"left\">IPatch <cit sha=\"5e18118b535c0e44d7d004b4c8114668660f16e2\"><ref target=\"bid131\"/></cit>{{cite:5e18118}}</cell>\n<cell halign=\"left\">White-box</cell>\n<cell halign=\"left\">EOT</cell>\n<cell halign=\"left\">-</cell>\n<cell halign=\"left\">Static</cell>\n<cell halign=\"left\">2D</cell>\n</row><row><cell halign=\"left\">SSAttack <cit sha=\"935b9f53040da5646e8ad6f12786a48e460e2eba\"><ref target=\"bid132\"/></cit>{{cite:935b9f5}}</cell>\n<cell halign=\"left\">White-box</cell>\n<cell halign=\"left\">EOT</cell>\n<cell halign=\"left\">-</cell>\n<cell halign=\"left\">Static</cell>\n<cell halign=\"left\">2D</cell>\n</row></table>", "caption": "Physical adversarial attacks against Semantic Segmentation tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.", "type": "table"}, "_table_hash_full_text": "f90106a9-614d-4127-88d7-57614d687c75", "context": {"caption": "Physical adversarial attacks against Semantic Segmentation tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.", "mention_paragraphs": ["Table XII presents a comprehensive comparison of various adversarial attack methods in the semantic segmentation task. It provides an overview of their attack goals, patch placement strategies, consideration of changing viewpoints, testing in the physical domain, and transferability to other models. Table XIII offers detailed information on adversarial attacks, including the attacker\u2019s knowledge level, robustness techniques, stealthiness techniques, physical test types, and space of operation. Table XIV provides information on the datasets used, the evaluated networks, and the links to open-source code for the experiments conducted in the semantic segmentation task."], "glossary": {"IPatch": "An adversarial attack that targets a particular region in an image by inserting a adversarially optimized patch.", "Attacker's Knowledge": "The knowledge required by the attacker. Can be one of White-box, Black-box or Grey-box", "White-box": "refer to adversarial attacks where the attacker possesses complete knowledge of the training and testing data used to train the victim model, as well as the architecture and parameters of the target model.", "Robustness Technique": "Techniques for maintaining robustness to attacks.", "EOT": "Expectation of Transformation. for adversarial attacks, which takes potential transformations in the real world into account during the optimization, resulting in better robustness. EOT adds random distortions in the optimization to make the perturbation more robust.", "Stealthiness Technique": "Techniques for evading detection that the attack was used.", "Static": "Attacks that do not change once inserted into the scene.", "Space": "Whether the attack takes place in two dimensions (eg like a sticker) or three (eg an object)", "2D": "The attack takes place in two dimensions"}}, "_full_text_table_hash": "8750716b-7f93-4922-8d97-ff903f750be7", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Attack<~>Attack": "In this context, 'Attack' refers to different methods used by adversaries to manipulate or deceive semantic segmentation models in computer vision. The table provides information on various attack methods along with their attacker's knowledge level, robustness techniques, stealthiness techniques, physical test types, and space of operation.", "Attack<~>IPatch": "IPatch refers to a specific adversarial attack method against Semantic Segmentation tasks.", "Attack<~>SSAttack": "SSAttack refers to a specific type of physical adversarial attack method against Semantic Segmentation tasks.", "Attacker's Knowledge<~>Attacker's Knowledge": "'Attacker's Knowledge' in this context refers to the level of information the attacker has about the target semantic segmentation model, specifically whether it is white-box (full knowledge of model architecture and parameters) or black-box (no knowledge beyond input-output behavior).", "Attacker's Knowledge<~>White-box": "'White-box' in the column 'Attacker's Knowledge' refers to an attacker having full access to the targeted semantic segmentation model, including its architecture and weights.", "Robustness Technique<~>Robustness Technique": "A robustness technique is a method employed to defend against or mitigate the impact of adversarial attacks on semantic segmentation models.", "Robustness Technique<~>EOT": "EOT in the context of the table refers to Early Opt-out, which is a robustness technique used in physical adversarial attacks against Semantic Segmentation tasks.", "Stealthiness Technique<~>Stealthiness Technique": "Stealthiness technique refers to the methods used by attackers to make their adversarial attacks hard to detect in the physical domain.", "Physical test type<~>Physical test type": "'Physical test type' refers to the methodology used to test the effectiveness of adversarial attacks in the physical world.", "Physical test type<~>Static": "In the context of the given table, 'Static' in the column 'Physical test type' refers to experiments conducted under controlled conditions where the physical environment is not changing.", "Space<~>Space": "In this context, 'Space' refers to the dimension or domain in which the adversarial attacks against semantic segmentation tasks are conducted, such as 2D or 3D space.", "Space<~>2D": "The '2D' in the 'Space' column refers to the two-dimensional space of images in the context of physical adversarial attacks against Semantic Segmentation tasks."}}, "title": "Physical Adversarial Attacks for Camera-Based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook", "abstract": "Deep Neural Networks (DNNs) have shown impressive performance in computer vision tasks; however, their vulnerability to adversarial attacks raises concerns regarding their security and reliability. Extensive research has shown that DNNs can be compromised by carefully crafted perturbations, leading to significant performance degradation in both digital and physical domains. Therefore, ensuring the security of DNN-based systems is crucial, particularly in safety-critical domains such as autonomous driving, robotics, smart homes/cities, smart industries, video surveillance, and healthcare. In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems."}
{"paper_id": "2308.07107v2", "_pdf_hash": null, "_source_hash": "0a27543ade8282918c934e7f1af77e75924f7c5f", "_source_name": "2308.07107v2", "_table_hash": "361b11db-c0a0-4286-9b88-15e2b801a181", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid30\" id-text=\"3\" place=\"t\" rend=\"display\" starred=\"true\"><head>The comparison of existing data augmentation methods powered by LLMs for training retrieval models.</head>\n<tr><td halign=\"left\"><hi rend=\"bold\">Methods</hi></td>\n<td halign=\"center\"><hi rend=\"bold\"># Examples</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Generator</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Synthetic Data</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Filter Method</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">LLMs' tuning</hi></td>\n</tr><tr><td halign=\"left\">InPairs<cit sha=\"e8f69a7963ced27f18703e2ac5a4d6ba10b7361f\"><ref target=\"bid106\"/></cit>{{cite:e8f69a7}}</td>\n<td halign=\"center\">3</td>\n<td halign=\"center\">Curie</td>\n<td halign=\"center\">Relevant query</td>\n<td halign=\"center\">Generation probability</td>\n<td halign=\"center\">Fixed</td>\n</tr><tr><td halign=\"left\">InPairs-v2<cit sha=\"70d063b52cdf076c52bcbfaecccdfbd4108a16b8\"><ref target=\"bid107\"/></cit>{{cite:70d063b}}</td>\n<td halign=\"center\">3</td>\n<td halign=\"center\">GPT-J</td>\n<td halign=\"center\">Relevant query</td>\n<td halign=\"center\">Relevance score from</td>\n</tr><tr><td halign=\"left\">fine-tuned monoT5-3B</td>\n<td halign=\"center\">Fixed</td>\n</tr><tr><td halign=\"left\">PROMPTAGATOR<cit sha=\"7de5c111a8024faf01e605b3183946b73c31c16f\"><ref target=\"bid108\"/></cit>{{cite:7de5c11}}</td>\n<td halign=\"center\">0-8</td>\n<td halign=\"center\">FLAN</td>\n<td halign=\"center\">Relevant query</td>\n<td halign=\"center\">Round-trip filtering</td>\n<td halign=\"center\">Fixed</td>\n</tr><tr><td halign=\"left\">TQGen<cit sha=\"a275fd96e36bcd6295a3934721b89c2bb31ac005\"><ref target=\"bid109\"/></cit>{{cite:a275fd9}}</td>\n<td halign=\"center\">0</td>\n<td halign=\"center\">T0</td>\n<td halign=\"center\">Relevant query</td>\n<td halign=\"center\">Generation probability</td>\n<td halign=\"center\">Fixed</td>\n</tr><tr><td halign=\"left\">UDAPDR<cit sha=\"294d6e3ca2a32db8e60dcc2680b1904928a82c5b\"><ref target=\"bid110\"/></cit>{{cite:294d6e3}}</td>\n<td halign=\"center\">0-3</td>\n<td halign=\"center\">GPT3 &amp; FLAN-T5-XXL</td>\n<td halign=\"center\">Relevant query</td>\n<td halign=\"center\">Round-trip filtering</td>\n<td halign=\"center\">Fixed</td>\n</tr><tr><td halign=\"left\">SPTAR<cit sha=\"5d896302f59c613de7fbccda14176b6b381d46b8\"><ref target=\"bid111\"/></cit>{{cite:5d89630}}</td>\n<td halign=\"center\">1-2</td>\n<td halign=\"center\">LLaMA-7B &amp; Vicuna-7B</td>\n<td halign=\"center\">Relevant query</td>\n<td halign=\"center\">BM25 filtering</td>\n<td halign=\"center\">Soft Prompt tuning</td>\n</tr><tr><td halign=\"left\">ART<cit sha=\"42df0e7079ec632c185d17a4377815de82c44a62\"><ref target=\"bid112\"/></cit>{{cite:42df0e7}}</td>\n<td halign=\"center\">0</td>\n<td halign=\"center\">T5-XL &amp; T5-XXL</td>\n<td halign=\"center\">Soft relevance labels</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">Fixed</td>\n</tr></table>", "table_json": {"incomplete_rows": [{"row_idx": 2, "text": "InPairs-v2{{cite:70d063b}}", "cols": 1}, {"row_idx": 2, "text": "3", "cols": 1}, {"row_idx": 2, "text": "GPT-J", "cols": 1}, {"row_idx": 2, "text": "Relevant query", "cols": 1}, {"row_idx": 2, "text": "Relevance score from", "cols": 1}, {"row_idx": 3, "text": "fine-tuned monoT5-3B", "cols": 1}, {"row_idx": 3, "text": "Fixed", "cols": 1}], "table": [["Methods", "# Examples", "Generator", "Synthetic Data", "Filter Method", "LLMs' tuning"], ["InPairs{{cite:e8f69a7}}", "3", "Curie", "Relevant query", "Generation probability", "Fixed"], ["PROMPTAGATOR{{cite:7de5c11}}", "0-8", "FLAN", "Relevant query", "Round-trip filtering", "Fixed"], ["TQGen{{cite:a275fd9}}", "0", "T0", "Relevant query", "Generation probability", "Fixed"], ["UDAPDR{{cite:294d6e3}}", "0-3", "GPT3 & FLAN-T5-XXL", "Relevant query", "Round-trip filtering", "Fixed"], ["SPTAR{{cite:5d89630}}", "1-2", "LLaMA-7B & Vicuna-7B", "Relevant query", "BM25 filtering", "Soft Prompt tuning"], ["ART{{cite:42df0e7}}", "0", "T5-XL & T5-XXL", "Soft relevance labels", "-", "Fixed"]], "table_dict": {"References": ["{{cite:e8f69a7}}", "{{cite:7de5c11}}", "{{cite:a275fd9}}", "{{cite:294d6e3}}", "{{cite:5d89630}}", "{{cite:42df0e7}}"], "Methods": ["InPairs", "PROMPTAGATOR", "TQGen", "UDAPDR", "SPTAR", "ART"], "# Examples": ["3", "0-8", "0", "0-3", "1-2", "0"], "Generator": ["Curie", "FLAN", "T0", "GPT3 & FLAN-T5-XXL", "LLaMA-7B & Vicuna-7B", "T5-XL & T5-XXL"], "Synthetic Data": ["Relevant query", "Relevant query", "Relevant query", "Relevant query", "Relevant query", "Soft relevance labels"], "Filter Method": ["Generation probability", "Round-trip filtering", "Generation probability", "Round-trip filtering", "BM25 filtering", "-"], "LLMs' tuning": ["Fixed", "Fixed", "Fixed", "Fixed", "Soft Prompt tuning", "Fixed"]}}, "bib_hash": ["e8f69a7963ced27f18703e2ac5a4d6ba10b7361f", "70d063b52cdf076c52bcbfaecccdfbd4108a16b8", "7de5c111a8024faf01e605b3183946b73c31c16f", "a275fd96e36bcd6295a3934721b89c2bb31ac005", "294d6e3ca2a32db8e60dcc2680b1904928a82c5b", "5d896302f59c613de7fbccda14176b6b381d46b8", "42df0e7079ec632c185d17a4377815de82c44a62"], "row_bib_map": [{"bib_hash_or_arxiv_id": "e8f69a7963ced27f18703e2ac5a4d6ba10b7361f", "row": 0, "corpus_id": 246705967, "type": "ref"}, {"bib_hash_or_arxiv_id": "7de5c111a8024faf01e605b3183946b73c31c16f", "row": 1, "corpus_id": 252519173, "type": "ref"}, {"bib_hash_or_arxiv_id": "a275fd96e36bcd6295a3934721b89c2bb31ac005", "row": 2, "corpus_id": 257405222, "type": "ref"}, {"bib_hash_or_arxiv_id": "294d6e3ca2a32db8e60dcc2680b1904928a82c5b", "row": 3, "corpus_id": 257279774, "type": "ref"}, {"bib_hash_or_arxiv_id": "5d896302f59c613de7fbccda14176b6b381d46b8", "row": 4, "corpus_id": 259937100, "type": "ref"}, {"bib_hash_or_arxiv_id": "42df0e7079ec632c185d17a4377815de82c44a62", "row": 5, "corpus_id": 249926985, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"3\" id=\"uid30\" starred=\"true\" place=\"t\"><head>The comparison of existing data augmentation methods powered by LLMs for training retrieval models.</head>\n<row><cell halign=\"left\"><hi rend=\"bold\">Methods</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\"># Examples</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Generator</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Synthetic Data</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Filter Method</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">LLMs' tuning</hi></cell>\n</row><row><cell halign=\"left\">InPairs\u00a0<cit sha=\"e8f69a7963ced27f18703e2ac5a4d6ba10b7361f\"><ref target=\"bid106\"/></cit>{{cite:e8f69a7}}</cell>\n<cell halign=\"center\">3</cell>\n<cell halign=\"center\">Curie</cell>\n<cell halign=\"center\">Relevant query</cell>\n<cell halign=\"center\">Generation probability</cell>\n<cell halign=\"center\">Fixed</cell>\n</row><row><cell halign=\"left\">InPairs-v2\u00a0<cit sha=\"70d063b52cdf076c52bcbfaecccdfbd4108a16b8\"><ref target=\"bid107\"/></cit>{{cite:70d063b}}</cell>\n<cell halign=\"center\">3</cell>\n<cell halign=\"center\">GPT-J</cell>\n<cell halign=\"center\">Relevant query</cell>\n<cell halign=\"center\">Relevance score from</cell>\n</row><row><cell halign=\"left\">fine-tuned monoT5-3B</cell>\n<cell halign=\"center\">Fixed</cell>\n</row><row><cell halign=\"left\">PROMPTAGATOR\u00a0<cit sha=\"7de5c111a8024faf01e605b3183946b73c31c16f\"><ref target=\"bid108\"/></cit>{{cite:7de5c11}}</cell>\n<cell halign=\"center\">0-8</cell>\n<cell halign=\"center\">FLAN</cell>\n<cell halign=\"center\">Relevant query</cell>\n<cell halign=\"center\">Round-trip filtering</cell>\n<cell halign=\"center\">Fixed</cell>\n</row><row><cell halign=\"left\">TQGen\u00a0<cit sha=\"a275fd96e36bcd6295a3934721b89c2bb31ac005\"><ref target=\"bid109\"/></cit>{{cite:a275fd9}}</cell>\n<cell halign=\"center\">0</cell>\n<cell halign=\"center\">T0</cell>\n<cell halign=\"center\">Relevant query</cell>\n<cell halign=\"center\">Generation probability</cell>\n<cell halign=\"center\">Fixed</cell>\n</row><row><cell halign=\"left\">UDAPDR\u00a0<cit sha=\"294d6e3ca2a32db8e60dcc2680b1904928a82c5b\"><ref target=\"bid110\"/></cit>{{cite:294d6e3}}</cell>\n<cell halign=\"center\">0-3</cell>\n<cell halign=\"center\">GPT3 &amp; FLAN-T5-XXL</cell>\n<cell halign=\"center\">Relevant query</cell>\n<cell halign=\"center\">Round-trip filtering</cell>\n<cell halign=\"center\">Fixed</cell>\n</row><row><cell halign=\"left\">SPTAR\u00a0<cit sha=\"5d896302f59c613de7fbccda14176b6b381d46b8\"><ref target=\"bid111\"/></cit>{{cite:5d89630}}</cell>\n<cell halign=\"center\">1-2</cell>\n<cell halign=\"center\">LLaMA-7B &amp; Vicuna-7B</cell>\n<cell halign=\"center\">Relevant query</cell>\n<cell halign=\"center\">BM25 filtering</cell>\n<cell halign=\"center\">Soft Prompt tuning</cell>\n</row><row><cell halign=\"left\">ART\u00a0<cit sha=\"42df0e7079ec632c185d17a4377815de82c44a62\"><ref target=\"bid112\"/></cit>{{cite:42df0e7}}</cell>\n<cell halign=\"center\">0</cell>\n<cell halign=\"center\">T5-XL &amp; T5-XXL</cell>\n<cell halign=\"center\">Soft relevance labels</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">Fixed</cell>\n</row></table>", "caption": "The comparison of existing data augmentation methods powered by LLMs for training retrieval models.", "type": "table"}, "_table_hash_full_text": "027dc2f7-4d24-499b-82d2-01a85ae991ae", "context": {"caption": "The comparison of existing data augmentation methods powered by LLMs for training retrieval models.", "mention_paragraphs": ["Additionally, to highlight the similarities and differences among the corresponding methods, we present a comparative result in Table III. It compares the aforementioned methods from various perspectives, including the number of examples, the generator employed, the type of synthetic data produced, the method applied to filter synthetic data, and whether LLMs are fine-tuned. This table serves to facilitate a clearer understanding of the landscape of these methods."], "glossary": {"Methods": "The name of the data augmentation method used by the paper", "# Examples": {"text": "The number of in-context examples used in the prompt the paper's method proposes.", "notes": "guess"}, "Generator": "The generation model used for data augmentation", "Synthetic data": "The type of synthetic data produced by the method in the paper.", "Relevant query": "The data augmentation mehtod produces synthetic queries that are relevant to a set of documents.", "Soft relevance labels": {"text": "In some downstream tasks of retrieval, such as question-answering, the collection of questions is also sufficient. However, the relevance labels connecting these questions with the passages of supporting evidence are very limited. In this context, leveraging the capability of LLMs for relevance label generation is a promising approach that can augment the training corpus for retrievers. LLMs produce the generation probabilities of the question conditioned on these top passages. After a normalization process, these probabilities serve as soft relevance labels for the training of the retriever.", "notes": "quote"}, "Filter Method": "How the synthetic data produced by the method in the paper is filtered to produce high quality data.", "Generation probability": {"text": "The probability of the synthetic query given the documents.", "notes": "guess"}, "Round-trip filtering": "First a retriever is fine-tuned on generated samples and then it is used to filter the samples.", "LLMs' tuning": "Whether the LLMs are finetuned, and what method is used if they are.", "Fixed": {"text": "the LLMs are not fintetuned", "notes": "guess"}, "Soft Prompt tuning": "only the prompts' embedding layer is optimized during the training process."}}, "_full_text_table_hash": "e9b07ad7-35e6-4776-8561-9658e198363e", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Methods<~>Methods": "The 'Methods' column in the table refers to the specific data augmentation methods used in each study to train retrieval models using large language models.", "Methods<~>InPairs": "InPairs refers to a data augmentation method that uses LLMs to generate query-document pairs.", "Methods<~>PROMPTAGATOR": "'PROMPTAGATOR' is a data augmentation method in the context of the table that utilizes LLMs to generate query-document pairs for training retrieval models.", "Methods<~>TQGen": "TQGen in the context of the table refers to a data augmentation method that uses LLMs to generate query-document pairs.", "Methods<~>UDAPDR": "UDAPDR stands for Universal Data Augmentation via Dense Paragraph Retrieval in the context of the table.", "Methods<~>SPTAR": "SPTAR stands for \"Statistical Paraphrase and Triple Association Rule\" method.", "Methods<~>ART": "In the context of the table, 'ART' in the 'Methods' column refers to Data Augmentation with Retrieved Text method.", "# Examples<~># Examples": "In the context of the table, '# Examples' refers to the number of examples used for each data augmentation method powered by large language models (LLMs) for training retrieval models.", "Generator<~>Generator": "In the context of the table, 'Generator' refers to specific language models, including Curie, FLAN, T0, GPT3 & FLAN-T5-XXL, LLaMA-7B & Vicuna-7B, and T5-XL & T5-XXL, used for data augmentation in training retrieval models.", "Generator<~>Curie": "'Curie' in the column 'Generator' refers to a specific language model or system, likely one of the large language models (LLMs) mentioned in the text. The exact identity of Curie is not clear from the provided context.", "Generator<~>FLAN": "'FLAN' in the column 'Generator' refers to a specific language model, possibly named FLAN-T5-XXL.", "Generator<~>T0": "'T0' in the column 'Generator' could refer to a specific language model or a version of a language model used in the study, but without additional context from the text or the table's source, it's unanswerable to determine exactly which one it is.", "Generator<~>GPT3 & FLAN-T5-XXL": "GPT3 & FLAN-T5-XXL in the table refers to the combination of the large language model GPT-3 with FLAN-T5-XXL, which is an extension of the T5 model.", "Generator<~>LLaMA-7B & Vicuna-7B": "LlaMA-7B & Vicuna-7B refer to large language models used as generators in the context of data augmentation methods for training retrieval models.", "Generator<~>T5-XL & T5-XXL": "T5-XL & T5-XXL in the 'Generator' column refer to large language models, specifically the T5 models with sizes XL and XXL.", "Synthetic Data<~>Synthetic Data": "In the context of the table, 'Synthetic Data' refers to the generation of additional data using large language models (LLMs).", "Synthetic Data<~>Relevant query": "In the context of the table, 'Relevant query' in the 'Synthetic Data' column refers to a query used as input for generating synthetic data using large language models for training retrieval models.", "Synthetic Data<~>Soft relevance labels": "In the context of the table, 'Soft relevance labels' in the column 'Synthetic Data' refer to labels generated by large language models to denote the level of relevance between queries and documents.", "Filter Method<~>Filter Method": "The 'Filter Method' column in the table refers to different methods used for filtering and ranking documents in information retrieval systems, such as generation probability, round-trip filtering, and BM25 filtering.", "Filter Method<~>Generation probability": "In the context of the given table, 'Generation probability' in the 'Filter Method' column refers to the probability generated by a large language model for a given query or document during the data augmentation process.", "Filter Method<~>Round-trip filtering": "In the context of the table, 'Round-trip filtering' in the 'Filter Method' column refers to a filtering method used for refining search results by leveraging the strong text understanding and generation capabilities of large language models.", "Filter Method<~>BM25 filtering": "BM25 filtering is a filtering method used in information retrieval to rank documents based on the likelihood of relevance to a given query using the Best Matching Ranking (BM25) algorithm.", "LLMs' tuning<~>LLMs' tuning": "In the context of the table, 'LLMs' tuning' refers to the specific method of adjusting or fine-tuning large language models used for data augmentation in training retrieval models. The table indicates that some references use a fixed LLM, while reference 4 explores the use of soft prompt tuning for LLMs.", "LLMs' tuning<~>Fixed": "In the context of the table, 'Fixed' in the column 'LLMs' tuning refers to the use of pre-trained language models without any fine-tuning or adjustment for the specific retrieval task.", "LLMs' tuning<~>Soft Prompt tuning": "In the context of the table, 'Soft Prompt tuning' in the 'LLMs' tuning' column refers to fine-tuning the prompts used to interact with the language model."}}, "title": "Large Language Models for Information Retrieval: A Survey", "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field."}
{"paper_id": "2308.07107v2", "_pdf_hash": null, "_source_hash": "0a27543ade8282918c934e7f1af77e75924f7c5f", "_source_name": "2308.07107v2", "_table_hash": "ff0a2460-7c6f-409b-8f91-30d45d25268d", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid35\" id-text=\"4\" place=\"t\" rend=\"display\"><head>The comparison of retrievers that leverage LLMs as the foundation.</head>\n<tr><td halign=\"left\"><hi rend=\"bold\">Methods</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Backbone</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Architecture</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">LLM's tuning</hi></td>\n</tr><tr><td halign=\"left\">cpt-text<cit sha=\"13f2a1f9b27999a697ba6890b06ba81c198871ee\"><ref target=\"bid114\"/></cit>{{cite:13f2a1f}}</td>\n<td halign=\"center\">cpt-text</td>\n<td halign=\"center\">Encoder-based</td>\n<td halign=\"center\">Training from scratch</td>\n</tr><tr><td halign=\"left\">GTR<cit sha=\"48695786b484f08152e4e1fd756ccaea64ded8e2\"><ref target=\"bid115\"/></cit>{{cite:4869578}}</td>\n<td halign=\"center\">T5</td>\n<td halign=\"center\">Encoder-based</td>\n<td halign=\"center\">Fine-tuning</td>\n</tr><tr><td halign=\"left\">TART<cit sha=\"fc129165ab79b08e5c086a8a5b7677831d827b83\"><ref target=\"bid116\"/></cit>{{cite:fc12916}}</td>\n<td halign=\"center\">T5</td>\n<td halign=\"center\">Encoder-based</td>\n<td halign=\"center\">Fine-tuning &amp;</td>\n</tr><tr><td halign=\"left\">Prompting</td>\n</tr><tr><td halign=\"left\">DSI<cit sha=\"fe72bb9d4b6a456a1bffa8897b36bb9af88a1629\"><ref target=\"bid120\"/></cit>{{cite:fe72bb9}}</td>\n<td halign=\"center\">T5</td>\n<td halign=\"center\">Generative</td>\n<td halign=\"center\">Fine-tuning</td>\n</tr><tr><td halign=\"left\">LLM-URL<cit sha=\"7c88d85c30d455c9de475355302f7501fe1c745e\"><ref target=\"bid121\"/></cit>{{cite:7c88d85}}</td>\n<td halign=\"center\">GPT-3</td>\n<td halign=\"center\">Generative</td>\n<td halign=\"center\">Prompting</td>\n</tr></table>", "table_json": {"incomplete_rows": [{"row_idx": 4, "text": "Prompting", "cols": 1}], "table": [["Methods", "Backbone", "Architecture", "LLM's tuning"], ["cpt-text{{cite:13f2a1f}}", "cpt-text", "Encoder-based", "Training from scratch"], ["GTR{{cite:4869578}}", "T5", "Encoder-based", "Fine-tuning"], ["TART{{cite:fc12916}}", "T5", "Encoder-based", "Fine-tuning &"], ["DSI{{cite:fe72bb9}}", "T5", "Generative", "Fine-tuning"], ["LLM-URL{{cite:7c88d85}}", "GPT-3", "Generative", "Prompting"]], "table_dict": {"References": ["{{cite:13f2a1f}}", "{{cite:4869578}}", "{{cite:fc12916}}", "{{cite:fe72bb9}}", "{{cite:7c88d85}}"], "Methods": ["cpt-text ", "GTR ", "TART ", "DSI ", "LLM-URL "], "Backbone": ["cpt-text", "T5", "T5", "T5", "GPT-3"], "Architecture": ["Encoder-based", "Encoder-based", "Encoder-based", "Generative", "Generative"], "LLM's tuning": ["Training from scratch", "Fine-tuning", "Fine-tuning & Prompting", "Fine-tuning", "Prompting"]}}, "bib_hash": ["13f2a1f9b27999a697ba6890b06ba81c198871ee", "48695786b484f08152e4e1fd756ccaea64ded8e2", "fc129165ab79b08e5c086a8a5b7677831d827b83", "fe72bb9d4b6a456a1bffa8897b36bb9af88a1629", "7c88d85c30d455c9de475355302f7501fe1c745e"], "row_bib_map": [{"bib_hash_or_arxiv_id": "13f2a1f9b27999a697ba6890b06ba81c198871ee", "row": 0, "corpus_id": 246275593, "type": "ref"}, {"bib_hash_or_arxiv_id": "48695786b484f08152e4e1fd756ccaea64ded8e2", "row": 1, "corpus_id": 245144556, "type": "ref"}, {"bib_hash_or_arxiv_id": "fc129165ab79b08e5c086a8a5b7677831d827b83", "row": 2, "corpus_id": 253581733, "type": "ref"}, {"bib_hash_or_arxiv_id": "fe72bb9d4b6a456a1bffa8897b36bb9af88a1629", "row": 3, "corpus_id": 246863488, "type": "ref"}, {"bib_hash_or_arxiv_id": "7c88d85c30d455c9de475355302f7501fe1c745e", "row": 4, "corpus_id": 258714822, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"4\" id=\"uid35\" place=\"t\"><head>The comparison of retrievers that leverage LLMs as the foundation.</head>\n<row><cell halign=\"left\"><hi rend=\"bold\">Methods</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Backbone</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Architecture</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">LLM's tuning</hi></cell>\n</row><row><cell halign=\"left\">cpt-text\u00a0<cit sha=\"13f2a1f9b27999a697ba6890b06ba81c198871ee\"><ref target=\"bid114\"/></cit>{{cite:13f2a1f}}</cell>\n<cell halign=\"center\">cpt-text</cell>\n<cell halign=\"center\">Encoder-based</cell>\n<cell halign=\"center\">Training from scratch</cell>\n</row><row><cell halign=\"left\">GTR\u00a0<cit sha=\"48695786b484f08152e4e1fd756ccaea64ded8e2\"><ref target=\"bid115\"/></cit>{{cite:4869578}}</cell>\n<cell halign=\"center\">T5</cell>\n<cell halign=\"center\">Encoder-based</cell>\n<cell halign=\"center\">Fine-tuning</cell>\n</row><row><cell halign=\"left\">TART\u00a0<cit sha=\"fc129165ab79b08e5c086a8a5b7677831d827b83\"><ref target=\"bid116\"/></cit>{{cite:fc12916}}</cell>\n<cell halign=\"center\">T5</cell>\n<cell halign=\"center\">Encoder-based</cell>\n<cell halign=\"center\">Fine-tuning &amp;</cell>\n</row><row><cell halign=\"left\">Prompting</cell>\n</row><row><cell halign=\"left\">DSI\u00a0<cit sha=\"fe72bb9d4b6a456a1bffa8897b36bb9af88a1629\"><ref target=\"bid120\"/></cit>{{cite:fe72bb9}}</cell>\n<cell halign=\"center\">T5</cell>\n<cell halign=\"center\">Generative</cell>\n<cell halign=\"center\">Fine-tuning</cell>\n</row><row><cell halign=\"left\">LLM-URL\u00a0<cit sha=\"7c88d85c30d455c9de475355302f7501fe1c745e\"><ref target=\"bid121\"/></cit>{{cite:7c88d85}}</cell>\n<cell halign=\"center\">GPT-3</cell>\n<cell halign=\"center\">Generative</cell>\n<cell halign=\"center\">Prompting</cell>\n</row></table>", "caption": "The comparison of retrievers that leverage LLMs as the foundation.", "type": "table"}, "_table_hash_full_text": "c87c74e9-ffc5-405d-a753-98ec9225a111", "context": {"caption": "The comparison of retrievers that leverage LLMs as the foundation.", "mention_paragraphs": ["To provide a comprehensive understanding of this topic, Table 4 summarizes the common and unique characteristics of the LLM-based retrievers discussed above."], "glossary": {"Methods": "The method introduced by the paper", "Backbone": {"text": "The model that the LLM-based retriever is built off of", "notes": "guess"}, "Architecture": "The architecture of the model used for the LLM-based retriever.", "Encoder-based": "LLMs, like the T5-family of models, are used to embed passages which are then placed in an index.", "Generative": "LLMs generate document identifiers or URLs directly rather than using the 'index-retrieval-rank' paradigm.", "LLM's tuning": "How LLM's need to be tuned for the paper's method.", "Training from scratch": {"text": "The model has to be trained from scratch", "notes": "guess"}, "Fine-tuning": {"text": "The model has to be fine-tuned", "notes": "guess"}, "Fine-tuning & Prompting": {"text": "The model has to be fine-tuned and prompted in a specific way to perform retrieval", "notes": "guess"}, "Prompting": {"text": "The model does not have to be fine-tuned and can instead just be prompted", "notes": "guess"}}}, "_full_text_table_hash": "5a5d90c5-3c97-4eb9-a25f-e1e2e71bac32", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Methods<~>Methods": "The 'Methods' column in the table refers to the specific types of LLM-based retrieval methods mentioned in the text, including cpt-text, GTR, TART, DSI, and LLM-URL.", "Methods<~>cpt-text": "'cpt-text' in the column 'Methods' refers to a specific retriever method, but the text does not provide enough context to determine what 'cpt-text' stands for.", "Methods<~>GTR": "GTR in the context of the table refers to Generative Text Retrieval, which is one of the LLM-based retrieval methods mentioned in the text.", "Methods<~>TART": "TART in the table refers to the Method \"Transformer-agonist Retrieval Transformer\" as described in the reference {{cite:fc12916}}.", "Methods<~>DSI": "DSI refers to the method called Dense Passage Retrieval (DPR) based on the DenserSIFT information retrieval model, which uses fine-tuning of pre-trained language models for retrieval tasks.", "Methods<~>LLM-URL": "LLM-URL refers to a specific method for LLM-based retrieval that utilizes the GPT-3 text-davinci-003 model to generate and extract URLs based on user queries.", "Backbone<~>Backbone": "In the context of the table, 'Backbone' refers to the type of large language model (LLM) used as the foundation for the generative retriever system.", "Backbone<~>cpt-text": "'Cpt-text' in the column 'Backbone' refers to a specific pre-trained language model, as mentioned in the citation {{cite:13f2a1f}} in the table. The exact nature or relation of 'cpt-text' to other LLMs, such as T5 or GPT-3, is not clear in the given context.", "Backbone<~>T5": "The 'T5' in the 'Backbone' column refers to the T5 model, which is a type of pre-trained transformer model used in the mentioned research for generative retrieval methods.", "Backbone<~>GPT-3": "'GPT-3' in the column 'Backbone' refers to the specific language model (GPT-3) used in one of the LLM-based retrievers mentioned in the table.", "Architecture<~>Architecture": "In the context of the table, 'Architecture' refers to the type of LLM-based retrieval method used, specifically whether it is encoder-based or generative.", "Architecture<~>Encoder-based": "'Encoder-based' in the column 'Architecture' refers to retrieval models that use LLMs as an encoder to transform queries and documents into a representation space for retrieval.", "Architecture<~>Generative": "In the context of the table, 'Generative' in the column 'Architecture' refers to model-based retrieval methods that use LLMs to directly generate document identifiers relevant to queries, eliminating the need for additional storage space for the index.", "LLM's tuning<~>LLM's tuning": "'LLM's tuning' refers to the process of adjusting or modifying large language models to enhance their performance in specific tasks, such as retrieval, by training them on relevant datasets or using prompting techniques.", "LLM's tuning<~>Training from scratch": "In the context of the table, 'Training from scratch' in the column 'LLM's tuning' for reference [{{cite:13f2a1f}}] refers to training a large language model (LLM) from scratch, meaning starting the training process from the very beginning with no pre-existing weights or knowledge. This is different from fine-tuning or prompting, which involve adding or making adjustments to pre-trained models.", "LLM's tuning<~>Fine-tuning": "In the context of the table, 'Fine-tuning' in the column 'LLM's tuning' refers to the process of adjusting the parameters of a pre-trained LLM to improve its performance on a specific task, such as retrieval.", "LLM's tuning<~>Fine-tuning & Prompting": "In the context of the table, 'Fine-tuning & Prompting' in the column 'LLM's tuning' refers to methods that use both fine-tuning and prompting techniques to leverage large language models (LLMs) for retrieval tasks. Fine-tuning involves adjusting the parameters of a pre-trained LLM to better perform a specific task, while prompting involves providing specific input to the LLM to elicit a desired output.", "LLM's tuning<~>Prompting": "In the context of the table, 'Prompting' in the column 'LLM's tuning' refers to using LLMs to directly generate document identifiers or URLs for user queries by providing a few demonstrations or prompts."}}, "title": "Large Language Models for Information Retrieval: A Survey", "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field."}
{"paper_id": "2308.07107v2", "_pdf_hash": null, "_source_hash": "0a27543ade8282918c934e7f1af77e75924f7c5f", "_source_name": "2308.07107v2", "_table_hash": "f83bb26c-2ea8-4a4f-b121-f69e4ebe9619", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid49\" id-text=\"5\" place=\"t\" rend=\"display\" starred=\"true\"><head>The comparison of existing methods that have a passive reader module. REALM and RAG do not use LLMs, but their frameworks have been widely applied in many following approaches.</head>\n<tr><td halign=\"left\"><hi rend=\"bold\">Methods</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Backbone models</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Where to incorporate retrieval</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">When to retrieve</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">How to use LLMs</hi></td>\n</tr><tr><td halign=\"left\">REALM<cit sha=\"0b8b82d5172c22d00106fc2b733655ca694c9a1b\"><ref target=\"bid136\"/></cit>{{cite:0b8b82d}}</td>\n<td halign=\"center\">BERT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Fine-tuning</td>\n</tr><tr><td halign=\"left\">RAG<cit sha=\"872cc6e211987815bfab4346ada6a2ce580c833f\"><ref target=\"bid137\"/></cit>{{cite:872cc6e}}</td>\n<td halign=\"center\">BART</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Fine-tuning</td>\n</tr><tr><td halign=\"left\">REPLUG<cit sha=\"9624f5d5e6b00d6f79c96ce6652f6345b746b8fb\"><ref target=\"bid138\"/></cit>{{cite:9624f5d}}</td>\n<td halign=\"center\">GPT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Fine-tuning</td>\n</tr><tr><td halign=\"left\">Atlas<cit sha=\"519a30c21d1562a0a7654ae4e79da6aef3ada884\"><ref target=\"bid139\"/></cit>{{cite:519a30c}}</td>\n<td halign=\"center\">T5</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Fine-tuning</td>\n</tr><tr><td halign=\"left\"><cit sha=\"28a8adf06ab383d636fef0ccec1e1b7f60ccc986\"><ref target=\"bid140\"/></cit>{{cite:28a8adf}}</td>\n<td halign=\"center\">Gopher</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Prompting</td>\n</tr><tr><td halign=\"left\"><cit sha=\"095710a2546c78bd6b9837961cfd2940720ec55a\"><ref target=\"bid141\"/></cit>{{cite:095710a}}</td>\n<td halign=\"center\">GPT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Prompting</td>\n</tr><tr><td halign=\"left\">RETA-LLM<cit sha=\"7500f3f7039d4b002b93bbeacf4a3354d447b60d\"><ref target=\"bid142\"/></cit>{{cite:7500f3f}}</td>\n<td halign=\"center\">LLaMA &amp; GLM &amp; GPT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">In the beginning</td>\n<td halign=\"center\">Prompting</td>\n</tr><tr><td halign=\"left\">RALM<cit sha=\"3b7c7857ccb7cb3e7df9cd553d0aa76e2ed8e1c8\"><ref target=\"bid143\"/></cit>{{cite:3b7c785}}</td>\n<td halign=\"center\">LLaMA &amp; OPT &amp; GPT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">During generation (every <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></formula> tokens)</td>\n<td halign=\"center\">Prompting</td>\n</tr><tr><td halign=\"left\">RETRO<cit sha=\"5078187aee4b6e66e706c35300e8b1d38ca94ca6\"><ref target=\"bid22\"/></cit>{{cite:5078187}}</td>\n<td halign=\"center\">Transformer</td>\n<td halign=\"center\">Attention layer</td>\n<td halign=\"center\">During generation (every <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></formula> tokens)</td>\n<td halign=\"center\">Training from scratch</td>\n</tr><tr><td halign=\"left\">IRCoT<cit sha=\"8ba5b280c23865fc4b4006243188b1f3775fb29a\"><ref target=\"bid144\"/></cit>{{cite:8ba5b28}}</td>\n<td halign=\"center\">Flan-T5 &amp; GPT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">During generation (every sentence)</td>\n<td halign=\"center\">Prompting</td>\n</tr><tr><td halign=\"left\">FLARE<cit sha=\"82d2780da367028be328fa6315248e7fb61142db\"><ref target=\"bid145\"/></cit>{{cite:82d2780}}</td>\n<td halign=\"center\">GPT</td>\n<td halign=\"center\">Input layer</td>\n<td halign=\"center\">During generation (aperiodic)</td>\n<td halign=\"center\">Prompting</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Methods", "Backbone models", "Where to incorporate retrieval", "When to retrieve", "How to use LLMs"], ["REALM{{cite:0b8b82d}}", "BERT", "Input layer", "In the beginning", "Fine-tuning"], ["RAG{{cite:872cc6e}}", "BART", "Input layer", "In the beginning", "Fine-tuning"], ["REPLUG{{cite:9624f5d}}", "GPT", "Input layer", "In the beginning", "Fine-tuning"], ["Atlas{{cite:519a30c}}", "T5", "Input layer", "In the beginning", "Fine-tuning"], ["{{cite:28a8adf}}", "Gopher", "Input layer", "In the beginning", "Prompting"], ["{{cite:095710a}}", "GPT", "Input layer", "In the beginning", "Prompting"], ["RETA-LLM{{cite:7500f3f}}", "LLaMA & GLM & GPT", "Input layer", "In the beginning", "Prompting"], ["RALM{{cite:3b7c785}}", "LLaMA & OPT & GPT", "Input layer", "During generation (every n tokens)", "Prompting"], ["RETRO{{cite:5078187}}", "Transformer", "Attention layer", "During generation (every n tokens)", "Training from scratch"], ["IRCoT{{cite:8ba5b28}}", "Flan-T5 & GPT", "Input layer", "During generation (every sentence)", "Prompting"], ["FLARE{{cite:82d2780}}", "GPT", "Input layer", "During generation (aperiodic)", "Prompting"]], "table_dict": {"References": ["{{cite:0b8b82d}}", "{{cite:872cc6e}}", "{{cite:9624f5d}}", "{{cite:519a30c}}", "{{cite:28a8adf}}", "{{cite:095710a}}", "{{cite:7500f3f}}", "{{cite:3b7c785}}", "{{cite:5078187}}", "{{cite:8ba5b28}}", "{{cite:82d2780}}"], "Methods": ["REALM", "RAG", "REPLUG", "Atlas", "-", "-", "RETA-LLM", "RALM", "RETRO", "IRCoT", "FLARE"], "Backbone models": ["BERT", "BART", "GPT", "T5", "Gopher", "GPT", "LLaMA & GLM & GPT", "LLaMA & OPT & GPT", "Transformer", "Flan-T5 & GPT", "GPT"], "Where to incorporate retrieval": ["Input layer", "Input layer", "Input layer", "Input layer", "Input layer", "Input layer", "Input layer", "Input layer", "Attention layer", "Input layer", "Input layer"], "When to retrieve": ["In the beginning", "In the beginning", "In the beginning", "In the beginning", "In the beginning", "In the beginning", "In the beginning", "During generation (every n tokens)", "During generation (every n tokens)", "During generation (every sentence)", "During generation (aperiodic)"], "How to use LLMs": ["Fine-tuning", "Fine-tuning", "Fine-tuning", "Fine-tuning", "Prompting", "Prompting", "Prompting", "Prompting", "Training from scratch", "Prompting", "Prompting"]}}, "bib_hash": ["0b8b82d5172c22d00106fc2b733655ca694c9a1b", "872cc6e211987815bfab4346ada6a2ce580c833f", "9624f5d5e6b00d6f79c96ce6652f6345b746b8fb", "519a30c21d1562a0a7654ae4e79da6aef3ada884", "28a8adf06ab383d636fef0ccec1e1b7f60ccc986", "095710a2546c78bd6b9837961cfd2940720ec55a", "7500f3f7039d4b002b93bbeacf4a3354d447b60d", "3b7c7857ccb7cb3e7df9cd553d0aa76e2ed8e1c8", "5078187aee4b6e66e706c35300e8b1d38ca94ca6", "8ba5b280c23865fc4b4006243188b1f3775fb29a", "82d2780da367028be328fa6315248e7fb61142db"], "row_bib_map": [{"bib_hash_or_arxiv_id": "0b8b82d5172c22d00106fc2b733655ca694c9a1b", "row": 0, "corpus_id": 211204736, "type": "ref"}, {"bib_hash_or_arxiv_id": "872cc6e211987815bfab4346ada6a2ce580c833f", "row": 1, "corpus_id": 218869575, "type": "ref"}, {"bib_hash_or_arxiv_id": "9624f5d5e6b00d6f79c96ce6652f6345b746b8fb", "row": 2, "corpus_id": 256389797, "type": "ref"}, {"bib_hash_or_arxiv_id": "519a30c21d1562a0a7654ae4e79da6aef3ada884", "row": 3, "corpus_id": 251371732, "type": "ref"}, {"bib_hash_or_arxiv_id": "28a8adf06ab383d636fef0ccec1e1b7f60ccc986", "row": 4, "corpus_id": 247362809, "type": "ref"}, {"bib_hash_or_arxiv_id": "095710a2546c78bd6b9837961cfd2940720ec55a", "row": 5, "corpus_id": 255372320, "type": "ref"}, {"bib_hash_or_arxiv_id": "7500f3f7039d4b002b93bbeacf4a3354d447b60d", "row": 6, "corpus_id": 259108339, "type": "ref"}, {"bib_hash_or_arxiv_id": "3b7c7857ccb7cb3e7df9cd553d0aa76e2ed8e1c8", "row": 7, "corpus_id": 256459451, "type": "ref"}, {"bib_hash_or_arxiv_id": "5078187aee4b6e66e706c35300e8b1d38ca94ca6", "row": 8, "corpus_id": 244954723, "type": "ref"}, {"bib_hash_or_arxiv_id": "8ba5b280c23865fc4b4006243188b1f3775fb29a", "row": 9, "corpus_id": 254877499, "type": "ref"}, {"bib_hash_or_arxiv_id": "82d2780da367028be328fa6315248e7fb61142db", "row": 10, "corpus_id": 258615731, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"5\" id=\"uid49\" starred=\"true\" place=\"t\"><head>The comparison of existing methods that have a passive reader module. REALM and RAG do not use LLMs, but their frameworks have been widely applied in many following approaches.</head>\n<row><cell halign=\"left\"><hi rend=\"bold\">Methods</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Backbone models</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Where to incorporate retrieval</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">When to retrieve</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">How to use LLMs</hi></cell>\n</row><row><cell halign=\"left\">REALM\u00a0<cit sha=\"0b8b82d5172c22d00106fc2b733655ca694c9a1b\"><ref target=\"bid136\"/></cit>{{cite:0b8b82d}}</cell>\n<cell halign=\"center\">BERT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Fine-tuning</cell>\n</row><row><cell halign=\"left\">RAG\u00a0<cit sha=\"872cc6e211987815bfab4346ada6a2ce580c833f\"><ref target=\"bid137\"/></cit>{{cite:872cc6e}}</cell>\n<cell halign=\"center\">BART</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Fine-tuning</cell>\n</row><row><cell halign=\"left\">REPLUG\u00a0<cit sha=\"9624f5d5e6b00d6f79c96ce6652f6345b746b8fb\"><ref target=\"bid138\"/></cit>{{cite:9624f5d}}</cell>\n<cell halign=\"center\">GPT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Fine-tuning</cell>\n</row><row><cell halign=\"left\">Atlas\u00a0<cit sha=\"519a30c21d1562a0a7654ae4e79da6aef3ada884\"><ref target=\"bid139\"/></cit>{{cite:519a30c}}</cell>\n<cell halign=\"center\">T5</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Fine-tuning</cell>\n</row><row><cell halign=\"left\"><cit sha=\"28a8adf06ab383d636fef0ccec1e1b7f60ccc986\"><ref target=\"bid140\"/></cit>{{cite:28a8adf}}</cell>\n<cell halign=\"center\">Gopher</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Prompting</cell>\n</row><row><cell halign=\"left\"><cit sha=\"095710a2546c78bd6b9837961cfd2940720ec55a\"><ref target=\"bid141\"/></cit>{{cite:095710a}}</cell>\n<cell halign=\"center\">GPT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Prompting</cell>\n</row><row><cell halign=\"left\">RETA-LLM\u00a0<cit sha=\"7500f3f7039d4b002b93bbeacf4a3354d447b60d\"><ref target=\"bid142\"/></cit>{{cite:7500f3f}}</cell>\n<cell halign=\"center\">LLaMA &amp; GLM &amp; GPT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">In the beginning</cell>\n<cell halign=\"center\">Prompting</cell>\n</row><row><cell halign=\"left\">RALM\u00a0<cit sha=\"3b7c7857ccb7cb3e7df9cd553d0aa76e2ed8e1c8\"><ref target=\"bid143\"/></cit>{{cite:3b7c785}}</cell>\n<cell halign=\"center\">LLaMA &amp; OPT &amp; GPT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">During generation (every <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math><texmath>n</texmath></formula> tokens)</cell>\n<cell halign=\"center\">Prompting</cell>\n</row><row><cell halign=\"left\">RETRO\u00a0<cit sha=\"5078187aee4b6e66e706c35300e8b1d38ca94ca6\"><ref target=\"bid22\"/></cit>{{cite:5078187}}</cell>\n<cell halign=\"center\">Transformer</cell>\n<cell halign=\"center\">Attention layer</cell>\n<cell halign=\"center\">During generation (every <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math><texmath>n</texmath></formula> tokens)</cell>\n<cell halign=\"center\">Training from scratch</cell>\n</row><row><cell halign=\"left\">IRCoT\u00a0<cit sha=\"8ba5b280c23865fc4b4006243188b1f3775fb29a\"><ref target=\"bid144\"/></cit>{{cite:8ba5b28}}</cell>\n<cell halign=\"center\">Flan-T5 &amp; GPT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">During generation (every sentence)</cell>\n<cell halign=\"center\">Prompting</cell>\n</row><row><cell halign=\"left\">FLARE\u00a0<cit sha=\"82d2780da367028be328fa6315248e7fb61142db\"><ref target=\"bid145\"/></cit>{{cite:82d2780}}</cell>\n<cell halign=\"center\">GPT</cell>\n<cell halign=\"center\">Input layer</cell>\n<cell halign=\"center\">During generation (aperiodic)</cell>\n<cell halign=\"center\">Prompting</cell>\n</row></table>", "caption": "The comparison of existing methods that have a passive reader module. REALM and RAG do not use LLMs, but their frameworks have been widely applied in many following approaches.", "type": "table"}, "_table_hash_full_text": "23419b8a-3d7e-466b-8f87-0e600eb03421", "_full_text_table_hash": "19cc0a2d-1d22-44a1-9312-a47546fc8491", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Methods<~>Methods": "The 'Methods' column in the table refers to the specific IR systems or approaches that utilize a passive reader module. In this context, 'methods' refers to the named systems or techniques listed, such as REALM, RAG, REPLUG, Atlas, RETA-LLM, RALM, RETRO, IRCoT, and FLARE.", "Methods<~>REALM": "REALM in the table refers to the Reconstructed Empathy and Abstract Reasoning (REALM) model, which is a non-LLM based method used in information retrieval systems with a passive reader module.", "Methods<~>RAG": "RAG stands for \"Retrieved and Generated\" and refers to a method that uses retrieved references along with a language model to generate answers in IR systems.", "Methods<~>REPLUG": "'REPLUG' in the column 'Methods' refers to a specific method named REPLUG that uses large language models in a passive reader module for information retrieval and answer generation.", "Methods<~>Atlas": "Atlas in the given table refers to a specific method or system that uses LLMs in a passive reader module for information retrieval, as indicated in the text.", "Methods<~>RETA-LLM": "RETA-LLM refers to a method that uses a large language model (LLM) in a passive reader module for information retrieval systems.", "Methods<~>RALM": "RALM stands for Retrieval-Augmented Language Model in the context of the table.", "Methods<~>RETRO": "RETRO in the table refers to the method RETRO (Retrieval-augmented Transformer for Open-domain Question Answering), which is a recently proposed approach that uses LLMs in combination with retrieved references to generate answers.", "Methods<~>IRCoT": "IRCoT in the table refers to the Interactive Reasoning and Collaborative Thinking system for IR.", "Methods<~>FLARE": "FLARE refers to a specific method mentioned in the table, which is not described in the text provided. Therefore, it is unanswerable what exactly FLARE is without additional context.", "Backbone models<~>Backbone models": "In the context of the given table, 'Backbone models' refer to the foundational models like BERT, BART, GPT, T5, Gopher, LLaMA, OPT, and Transformer, which form the base for the development of the reader module in Information Retrieval systems.", "Backbone models<~>BERT": "BERT in the context of the table refers to the Bidirectional Encoder Representations from Transformers, a popular natural language processing model developed by Google.", "Backbone models<~>BART": "BART in the table refers to the Backbone model named BART, which is a denoising autoencoder-based model for natural language processing.", "Backbone models<~>GPT": "GPT in the column 'Backbone models' refers to the family of Generative PreTrained Transformer models, specifically mentioned in citations 2, 5, 10, and 7. In the context of the table, these models are used as backbone components in various IR systems with reader modules.", "Backbone models<~>T5": "T5 in the context of the table refers to a specific language model developed by Google, which is used as a backbone model in some IR systems with a reader module.", "Backbone models<~>Gopher": "'Gopher' in the context of the table refers to a specific backbone model that has been used in some IR systems.", "Backbone models<~>LLaMA & GLM & GPT": "LLaMA, GLM, and GPT in the table's 'Backbone models' column refer to large language models, specifically LlaMA, GLM (likely Google's Language Model), and GPT (likely GPT-3 from OpenAI).", "Backbone models<~>LLaMA & OPT & GPT": "The column 'Backbone models' in the table refers to the specific language models used in each approach, with 'LLaMA & OPT & GPT' being a combination of the large language models LLaMA, OPT, and GPT.", "Backbone models<~>Transformer": "The 'Transformer' in the column 'Backbone models' refers to the Transformer architecture used in language models, specifically mentioned in reference 8: \"8ba5b28\".", "Backbone models<~>Flan-T5 & GPT": "'Flan-T5 & GPT' in the column 'Backbone models' refers to the combination of the T5 model and the GPT model in the FLAN model for a passive reader module in IR systems.", "Where to incorporate retrieval<~>Where to incorporate retrieval": "The table describes the placement of retrieval references in different models with a passive reader module. 'Where to incorporate retrieval' refers to the layer in each model where the retrieved references are incorporated.", "Where to incorporate retrieval<~>Input layer": "In the table context, 'Input layer' in the 'Where to incorporate retrieval' column for each citation refers to the stage where retrieved references are introduced as inputs to the language model for generating answers.", "Where to incorporate retrieval<~>Attention layer": "The 'Attention layer' in the column 'Where to incorporate retrieval' refers to a specific layer in deep learning models where the focus is selectively given to certain parts of the input while processing the data. In the context of the table, it indicates that retrieved references are incorporated at this attention layer in some IR systems for generating answers.", "When to retrieve<~>When to retrieve": "'When to retrieve' refers to the timing of retrieving references for language models in the context of answer generation in information retrieval systems. In this table, references are retrieved in the beginning for some approaches, while others retrieve them during the generation process.", "When to retrieve<~>In the beginning": "In the context of the table, 'In the beginning' in the column 'When to retrieve' refers to the point at which references are retrieved in the process of LLM-powered answer generation in IR systems, specifically at the beginning of the generation process.", "When to retrieve<~>During generation (every n tokens)": "During generation (every n tokens) in the column 'When to retrieve' refers to retrieving references every n tokens during the language model's generation process.", "When to retrieve<~>During generation (every sentence)": "During generation (every sentence) in the column 'When to retrieve' refers to retrieving references every sentence during the language model's generation process.", "When to retrieve<~>During generation (aperiodic)": "During generation (aperiodic) in the column 'When to retrieve' refers to retrieving references repeatedly throughout the language model's generation process in an irregular or non-periodic manner.", "How to use LLMs<~>How to use LLMs": "\"How to use LLMs\" in the context of the table refers to the methods mentioned in the table for utilizing Large Language Models, specifically through fine-tuning or prompting.", "How to use LLMs<~>Fine-tuning": "In the context of the table provided, 'Fine-tuning' in the 'How to use LLMs' column refers to the process of adjusting or optimizing the performance of a language model by training it on new data, typically with the goal of adapting it to a specific task or domain.", "How to use LLMs<~>Prompting": "In the context of the table, 'Prompting' in the column 'How to use LLMs' refers to providing specific inputs or instructions to the language model for generating desired outputs.", "How to use LLMs<~>Training from scratch": "\"Training from scratch\" in the column 'How to use LLMs' refers to the process of teaching a large language model from the ground up, without using any pre-trained weights or fine-tuning on specific tasks."}}, "title": "Large Language Models for Information Retrieval: A Survey", "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field."}
{"paper_id": "2308.00937v1", "_pdf_hash": null, "_source_hash": "3e5100b1d5f49ef45de6921425cb3e1d39777d0e", "_source_name": "2308.00937v1", "_table_hash": "d63d3952-e30a-4a05-8415-b13226eff27e", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid13\" id-text=\"1\" place=\"t!\" rend=\"display\" starred=\"true\"><head><hi rend=\"small\"><hi rend=\"bold\">Comparison with other benchmarks.</hi></hi><hi rend=\"small\"> </hi><hi rend=\"small\"><hi rend=\"tt\">LEMMA</hi></hi><hi rend=\"small\"> evaluates the performance of language-conditioned multi-agent object manipulation in long-horizon tasks. </hi><hi rend=\"small\"><hi rend=\"bold\">Multi-task</hi></hi><hi rend=\"small\">: using a multi-task setting. </hi><hi rend=\"small\"><hi rend=\"bold\">Language</hi></hi><hi rend=\"small\">: language instructions to specify goal. </hi><hi rend=\"small\"><hi rend=\"bold\">Manipulation</hi></hi><hi rend=\"small\">: physical object manipulation. </hi><hi rend=\"small\"><hi rend=\"bold\">Multi-agent</hi></hi><hi rend=\"small\">: requiring multiple agents for task completion. </hi><hi rend=\"small\"><hi rend=\"bold\">Tool use</hi></hi><hi rend=\"small\">: requiring the robot to use a tool to interact with other objects. </hi><hi rend=\"small\"><hi rend=\"bold\">Temporal Dep</hi></hi><hi rend=\"small\">: temporal dependency between sub-tasks.</hi></head>\n<unexpected><p rend=\"center\"><hi rend=\"small\"><resizebox width=\"427.0pt\"><table rend=\"inline\"><tr><td halign=\"center\">Benchmark</td>\n<td halign=\"center\">Alfred<cit sha=\"4e416278743059f32ce493e712a6c5c9af902cf4\"><ref target=\"bid5\"/></cit>{{cite:4e41627}}</td>\n<td halign=\"center\">MQA<cit sha=\"4ec7009044a9ed586281f8496022e82be2630330\"><ref target=\"bid20\"/></cit>{{cite:4ec7009}}</td>\n<td halign=\"center\">Calvin<cit sha=\"cbc3708d4c18205739d95e02ea2639ebb5d8a586\"><ref target=\"bid1\"/></cit>{{cite:cbc3708}}</td>\n<td halign=\"center\">M-EQA<cit sha=\"0712b6a7c80b6cb66a5dc3af29082c4b04e1c924\"><ref target=\"bid12\"/></cit>{{cite:0712b6a}}</td>\n<td halign=\"center\">Ravens<cit sha=\"785eb16415601e9ec40a21602ff401caaa557dc8\"><ref target=\"bid21\"/></cit>{{cite:785eb16}}</td>\n<td halign=\"center\">Vlmbench<cit sha=\"82f243c35f10e98cfb3a307f4524e88961c83d80\"><ref target=\"bid2\"/></cit>{{cite:82f243c}}</td>\n<td halign=\"center\">CH-MARL<cit sha=\"e34be003de2fa0d4eed0191dedd8135d457ae9a8\"><ref target=\"bid22\"/></cit>{{cite:e34be00}}</td>\n<td halign=\"center\">TBP <cit sha=\"3d1ff60450bc9702302fa6c1be194a4bbb756f36\"><ref target=\"bid15\"/></cit>{{cite:3d1ff60}}</td>\n<td halign=\"center\">EMATP<cit sha=\"e60b1262453e3d9d72560f8b766427c97024fbc5\"><ref target=\"bid13\"/></cit>{{cite:e60b126}}</td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\"/></hi><hi rend=\"small\"><hi rend=\"tt\"><hi rend=\"bold\">LEMMA</hi></hi></hi><hi rend=\"small\"><hi rend=\"bold\"/></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"center\">Language</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n</tr><tr><td halign=\"center\">Multi-task</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n</tr><tr><td halign=\"center\">Manipulation</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n</tr><tr><td halign=\"center\">Multi-agent</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n</tr><tr><td halign=\"center\">Tool Use</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n</tr><tr><td halign=\"center\">Temporal Dep.</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n</tr><tr><td halign=\"center\"/>\n</tr></table>\n</resizebox>\n</hi></p><p rend=\"center\" spacebefore=\"-15.0pt\"><hi rend=\"small\"/></p></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Benchmark", "Alfred{{cite:4e41627}}", "MQA{{cite:4ec7009}}", "Calvin{{cite:cbc3708}}", "M-EQA{{cite:0712b6a}}", "Ravens{{cite:785eb16}}", "Vlmbench{{cite:82f243c}}", "CH-MARL{{cite:e34be00}}", "TBP {{cite:3d1ff60}}", "EMATP{{cite:e60b126}}", "LEMMA"], ["Language", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2713", "\u2713", "\u2717", "\u2713", "\u2713"], ["Multi-task", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2713"], ["Manipulation", "\u2717", "\u2713", "\u2713", "\u2717", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717", "\u2713"], ["Multi-agent", "\u2717", "\u2717", "\u2717", "\u2713", "\u2717", "\u2717", "\u2713", "\u2713", "\u2713", "\u2713"], ["Tool Use", "\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2713"], ["Temporal Dep.", "\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2713"]], "table_dict": {"References": ["{{cite:4e41627}}", "{{cite:4ec7009}}", "{{cite:cbc3708}}", "{{cite:0712b6a}}", "{{cite:785eb16}}", "{{cite:82f243c}}", "{{cite:e34be00}}", "{{cite:3d1ff60}}", "{{cite:e60b126}}", "-"], "Benchmark": ["Alfred", "MQA", "Calvin", "M-EQA", "Ravens", "Vlmbench", "CH-MARL", "TBP ", "EMATP", "LEMMA"], "Language": ["\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2713", "\u2713", "\u2717", "\u2713", "\u2713"], "Multi-task": ["\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2713"], "Manipulation": ["\u2717", "\u2713", "\u2713", "\u2717", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717", "\u2713"], "Multi-agent": ["\u2717", "\u2717", "\u2717", "\u2713", "\u2717", "\u2717", "\u2713", "\u2713", "\u2713", "\u2713"], "Tool Use": ["\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2713"], "Temporal Dep.": ["\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2713"]}}, "bib_hash": ["4e416278743059f32ce493e712a6c5c9af902cf4", "4ec7009044a9ed586281f8496022e82be2630330", "cbc3708d4c18205739d95e02ea2639ebb5d8a586", "0712b6a7c80b6cb66a5dc3af29082c4b04e1c924", "785eb16415601e9ec40a21602ff401caaa557dc8", "82f243c35f10e98cfb3a307f4524e88961c83d80", "e34be003de2fa0d4eed0191dedd8135d457ae9a8", "3d1ff60450bc9702302fa6c1be194a4bbb756f36", "e60b1262453e3d9d72560f8b766427c97024fbc5"], "row_bib_map": [{"bib_hash_or_arxiv_id": "4e416278743059f32ce493e712a6c5c9af902cf4", "row": 0, "corpus_id": 208617407, "type": "ref"}, {"bib_hash_or_arxiv_id": "4ec7009044a9ed586281f8496022e82be2630330", "row": 1, "corpus_id": 212644876, "type": "ref"}, {"bib_hash_or_arxiv_id": "cbc3708d4c18205739d95e02ea2639ebb5d8a586", "row": 2, "corpus_id": 244908821, "type": "ref"}, {"bib_hash_or_arxiv_id": "0712b6a7c80b6cb66a5dc3af29082c4b04e1c924", "row": 3, "corpus_id": 227232796, "type": "ref"}, {"bib_hash_or_arxiv_id": "785eb16415601e9ec40a21602ff401caaa557dc8", "row": 4, "corpus_id": 225076003, "type": "ref"}, {"bib_hash_or_arxiv_id": "82f243c35f10e98cfb3a307f4524e88961c83d80", "row": 5, "corpus_id": 249848175, "type": "ref"}, {"bib_hash_or_arxiv_id": "e34be003de2fa0d4eed0191dedd8135d457ae9a8", "row": 6, "corpus_id": 251903958, "type": "ref"}, {"bib_hash_or_arxiv_id": "3d1ff60450bc9702302fa6c1be194a4bbb756f36", "row": 7, "corpus_id": 109933186, "type": "ref"}, {"bib_hash_or_arxiv_id": "e60b1262453e3d9d72560f8b766427c97024fbc5", "row": 8, "corpus_id": 248941435, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.00937v1", "row": 9, "corpus_id": 260378734, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid13\" starred=\"true\" place=\"t!\"><head><hi rend=\"small\"><hi rend=\"bold\">Comparison with other benchmarks.</hi></hi><hi rend=\"small\"> </hi><hi rend=\"small\"><hi rend=\"tt\">LEMMA</hi></hi><hi rend=\"small\"> evaluates the performance of language-conditioned multi-agent object manipulation in long-horizon tasks. </hi><hi rend=\"small\"><hi rend=\"bold\">Multi-task</hi></hi><hi rend=\"small\">: using a multi-task setting. </hi><hi rend=\"small\"><hi rend=\"bold\">Language</hi></hi><hi rend=\"small\">: language instructions to specify goal. </hi><hi rend=\"small\"><hi rend=\"bold\">Manipulation</hi></hi><hi rend=\"small\">: physical object manipulation. </hi><hi rend=\"small\"><hi rend=\"bold\">Multi-agent</hi></hi><hi rend=\"small\">: requiring multiple agents for task completion. </hi><hi rend=\"small\"><hi rend=\"bold\">Tool use</hi></hi><hi rend=\"small\">: requiring the robot to use a tool to interact with other objects. </hi><hi rend=\"small\"><hi rend=\"bold\">Temporal Dep</hi></hi><hi rend=\"small\">: temporal dependency between sub-tasks.</hi></head>\n<unexpected><p rend=\"center\"><hi rend=\"small\"><resizebox width=\"427.0pt\"><table rend=\"inline\"><row><cell halign=\"center\">Benchmark</cell>\n<cell halign=\"center\">Alfred<cit sha=\"4e416278743059f32ce493e712a6c5c9af902cf4\"><ref target=\"bid5\"/></cit>{{cite:4e41627}}</cell>\n<cell halign=\"center\">MQA<cit sha=\"4ec7009044a9ed586281f8496022e82be2630330\"><ref target=\"bid20\"/></cit>{{cite:4ec7009}}</cell>\n<cell halign=\"center\">Calvin<cit sha=\"cbc3708d4c18205739d95e02ea2639ebb5d8a586\"><ref target=\"bid1\"/></cit>{{cite:cbc3708}}</cell>\n<cell halign=\"center\">M-EQA<cit sha=\"0712b6a7c80b6cb66a5dc3af29082c4b04e1c924\"><ref target=\"bid12\"/></cit>{{cite:0712b6a}}</cell>\n<cell halign=\"center\">Ravens<cit sha=\"785eb16415601e9ec40a21602ff401caaa557dc8\"><ref target=\"bid21\"/></cit>{{cite:785eb16}}</cell>\n<cell halign=\"center\">Vlmbench<cit sha=\"82f243c35f10e98cfb3a307f4524e88961c83d80\"><ref target=\"bid2\"/></cit>{{cite:82f243c}}</cell>\n<cell halign=\"center\">CH-MARL<cit sha=\"e34be003de2fa0d4eed0191dedd8135d457ae9a8\"><ref target=\"bid22\"/></cit>{{cite:e34be00}}</cell>\n<cell halign=\"center\">TBP <cit sha=\"3d1ff60450bc9702302fa6c1be194a4bbb756f36\"><ref target=\"bid15\"/></cit>{{cite:3d1ff60}}</cell>\n<cell halign=\"center\">EMATP<cit sha=\"e60b1262453e3d9d72560f8b766427c97024fbc5\"><ref target=\"bid13\"/></cit>{{cite:e60b126}}</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\"/></hi><hi rend=\"small\"><hi rend=\"tt\"><hi rend=\"bold\">LEMMA</hi></hi></hi><hi rend=\"small\"><hi rend=\"bold\"/></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"center\">Language</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n</row><row><cell halign=\"center\">Multi-task</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n</row><row><cell halign=\"center\">Manipulation</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n</row><row><cell halign=\"center\">Multi-agent</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n</row><row><cell halign=\"center\">Tool Use</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n</row><row><cell halign=\"center\">Temporal Dep.</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n</row><row><cell halign=\"center\"/>\n</row></table>\n</resizebox>\n</hi></p><p spacebefore=\"-15.0pt\" rend=\"center\"><hi rend=\"small\"/></p></unexpected></table>", "caption": "Comparison with other benchmarks. LEMMA evaluates the performance of language-conditioned multi-agent object manipulation in long-horizon tasks. Multi-task: using a multi-task setting. Language: language instructions to specify goal. Manipulation: physical object manipulation. Multi-agent: requiring multiple agents for task completion. Tool use: requiring the robot to use a tool to interact with other objects. Temporal Dep: temporal dependency between sub-tasks.", "type": "table"}, "_table_hash_full_text": "3553f297-17d4-4a36-b64f-d45fa82fdddb", "_full_text_table_hash": "0ce6f366-69ae-4e8e-8498-48bf64bcab6f", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Benchmark<~>Benchmark": "'LEMMA' referred to in the table is the benchmark for Language-Conditioned Multi-robot Manipulation introduced in the text.", "Benchmark<~>Alfred": "Alfred in the 'Benchmark' column of the table refers to the language-conditioned multi-robot manipulation benchmark introduced in the LEMMA paper.", "Benchmark<~>MQA": "MQA in the 'Benchmark' column of the table stands for Multi-Question Answer, a benchmark for evaluating language-conditioned multi-agent systems that require answering a series of questions based on textual descriptions.", "Benchmark<~>Calvin": "Calvin in the 'Benchmark' column of the table refers to a specific benchmark named Calvin, as mentioned in citation {{cite:cbc3708}}.", "Benchmark<~>M-EQA": "'M-EQA' in the 'Benchmark' column refers to the Multi-agent Equivalence Reasoning benchmark.", "Benchmark<~>Ravens": "Ravens in the table 'Benchmark' column refers to the Ravens Progressive Matrices benchmark, which is a popular test of abstract reasoning ability in animals and artificial intelligence. However, it is not clear how it is related to the other benchmarks in the table as there is no mention of it being a language-conditioned multi-agent object manipulation benchmark like LEMMA.", "Benchmark<~>Vlmbench": "Vlmbench in the table refers to a specific benchmark for multi-agent object manipulation that focuses on language-conditioned manipulation and evaluation in long-horizon tasks.", "Benchmark<~>CH-MARL": "CH-MARL in the table 'Benchmark' column refers to the Coordinated Helicopter Multi-Agent learning Environment benchmark for multi-robot cooperation in dynamic and changing environments.", "Benchmark<~>TBP": "'TBP' in the column 'Benchmark' likely refers to the name of a specific benchmark, but without additional context, it is unclear what the expansion of the abbreviation is.", "Benchmark<~>EMATP": "EMATP in the table refers to the Ethz Manipulation and Assembly benchmark for multi-robot manipulation and assembly tasks.", "Benchmark<~>LEMMA": "LEMMA in the 'Benchmark' column of the table refers to the Language-Conditioned Multi-robot Manipulation benchmark introduced in the given scientific paper.", "Language<~>Language": "In the given table, 'Language' refers to the use of human language instructions to specify goals for multi-agent object manipulation tasks in the LEMMA benchmark.", "Multi-task<~>Multi-task": "In the context of the table, 'Multi-task' refers to the use of a multi-task setting in the benchmark.", "Manipulation<~>Manipulation": "In the context of the table, 'Manipulation' refers to physical object manipulation by robots.", "Multi-agent<~>Multi-agent": "In the context of the table, 'Multi-agent' refers to requiring multiple agents for task completion.", "Tool Use<~>Tool Use": "Tool use in the context of the table refers to the requirement for robots to use tools to interact with other objects during the manipulation tasks in LEMMA benchmark.", "Temporal Dep.<~>Temporal Dep.": "In the context of the table, 'Temporal Dep.' refers to the presence of temporal dependencies between sub-tasks in multi-agent object manipulation tasks."}}, "title": "LEMMA: Learning Language-Conditioned Multi-Robot Manipulation", "abstract": "Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for <underline>L</underline>anguag<underline>E</underline>-Conditioned <underline>M</underline>ulti-robot <underline>MA</underline>nipulation (<monospace>LEMMA</monospace>) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. <monospace>LEMMA</monospace> features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. <monospace>LEMMA</monospace> poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of <monospace>LEMMA</monospace> for developing future language-conditioned multi-robot systems."}
{"paper_id": "2308.08407v1", "_pdf_hash": null, "_source_hash": "15fb09333b596e0d30ca25fbd7c6d4062ae2041d", "_source_name": "2308.08407v1", "_table_hash": "d3b56d4e-6535-4c7e-a634-dae53c7aa8f5", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid48\" id-text=\"5\" place=\"htp\" rend=\"display\"><head>XAI for clinical risk prediction for genomic data for different methods, as well as evaluation criteria they might satisfy (continued). C.V. stands for clinical validation, ie. whether the explainability was at all clinically evaluated, Q.E. for quantitative evaluation of the explainability process, and O.A. whether the application is open access</head>\n<tr><td halign=\"left\">2*Reference</td>\n<td halign=\"center\">2*Year</td>\n<td cols=\"2\" halign=\"center\">Problem</td>\n<td cols=\"1\" halign=\"center\">Dataset</td>\n<td cols=\"4\" halign=\"center\">XAI</td>\n</tr><tr><td halign=\"left\">(r)3-4\n(r)5-6\n(r)6-9</td>\n<td halign=\"center\"/>\n<td halign=\"center\">Task</td>\n<td halign=\"center\">Model</td>\n<td halign=\"center\">Size</td>\n<td halign=\"center\">Method</td>\n<td halign=\"center\">C.V.</td>\n<td halign=\"center\">Q.E.</td>\n<td halign=\"center\">O.A.</td>\n</tr><tr><td halign=\"left\">(r)1-9\n<cit sha=\"349d172410558efeb2ff6cdac24969e39e9cacb2\"><ref target=\"bid230\"/></cit>{{cite:349d172}}</td>\n<td halign=\"center\">2020</td>\n<td halign=\"center\">AMR</td>\n<td halign=\"center\">regression</td>\n<td halign=\"center\">1,595</td>\n<td halign=\"center\">inherent</td>\n<td halign=\"center\"/>\n<td halign=\"center\"/>\n<td halign=\"center\"/>\n</tr><tr><td halign=\"left\"><cit sha=\"b0a80f1e2e80e64e3d25ff33844191522050ff69\"><ref target=\"bid231\"/></cit>{{cite:b0a80f1}}</td>\n<td halign=\"center\">2020</td>\n<td halign=\"center\">mortality</td>\n<td halign=\"center\">neural network</td>\n<td halign=\"center\">7,803</td>\n<td halign=\"center\">LIME</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\"/>\n<td halign=\"center\">X</td>\n</tr><tr><td halign=\"left\"><cit sha=\"5e866e6cd84eaef4301e3f762a6944933b1ccd9a\"><ref target=\"bid232\"/></cit>{{cite:5e866e6}}</td>\n<td halign=\"center\">2021</td>\n<td halign=\"center\">COVID-19 severity</td>\n<td halign=\"center\">regression</td>\n<td halign=\"center\">12,965</td>\n<td halign=\"center\">inherent</td>\n<td halign=\"center\"/>\n<td halign=\"center\"/>\n<td halign=\"center\">X</td>\n</tr><tr><td halign=\"left\"><cit sha=\"d763228e24fd2b2e04b6cb758e3c56d770f44d75\"><ref target=\"bid233\"/></cit>{{cite:d763228}}</td>\n<td halign=\"center\">2021</td>\n<td halign=\"center\">phenotyping</td>\n<td halign=\"center\">neural network</td>\n<td halign=\"center\">11,214</td>\n<td halign=\"center\">inherent</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\"/>\n</tr><tr><td halign=\"left\"><cit sha=\"59bf3b0fdc4bf78431f253c21e5d683052129f02\"><ref target=\"bid234\"/></cit>{{cite:59bf3b0}}</td>\n<td halign=\"center\">2021</td>\n<td halign=\"center\">macular degeneration</td>\n<td halign=\"center\">neural network</td>\n<td halign=\"center\">32,215</td>\n<td halign=\"center\">LIME</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\"/>\n</tr><tr><td halign=\"left\"><cit sha=\"d8e786e8e3038a999aef0093dc66cb7113805dbb\"><ref target=\"bid235\"/></cit>{{cite:d8e786e}}</td>\n<td halign=\"center\">2021</td>\n<td halign=\"center\">mortality</td>\n<td halign=\"center\">neural network</td>\n<td halign=\"center\">3,431</td>\n<td halign=\"center\">inherent</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\">X</td>\n<td halign=\"center\"/>\n</tr></table>", "table_json": {"incomplete_rows": [{"row_idx": 0, "text": "2*Reference", "cols": 1}, {"row_idx": 0, "text": "2*Year", "cols": 1}, {"row_idx": 0, "text": "Problem", "cols": 2}, {"row_idx": 0, "text": "Dataset", "cols": 1}, {"row_idx": 0, "text": "XAI", "cols": 4}], "table": [["2*Reference-(r)3-4\n(r)5-6\n(r)6-9", "2*Year-", "Problem-Task", "Problem-Model", "Dataset-Size", "XAI-Method", "XAI-C.V.", "XAI-Q.E.", "XAI-O.A."], ["(r)1-9\n{{cite:349d172}}", "2020", "AMR", "regression", "1,595", "inherent", "", "", ""], ["{{cite:b0a80f1}}", "2020", "mortality", "neural network", "7,803", "LIME", "X", "", "X"], ["{{cite:5e866e6}}", "2021", "COVID-19 severity", "regression", "12,965", "inherent", "", "", "X"], ["{{cite:d763228}}", "2021", "phenotyping", "neural network", "11,214", "inherent", "X", "X", ""], ["{{cite:59bf3b0}}", "2021", "macular degeneration", "neural network", "32,215", "LIME", "X", "X", ""], ["{{cite:d8e786e}}", "2021", "mortality", "neural network", "3,431", "inherent", "X", "X", ""]], "table_dict": {"References": ["{{cite:349d172}}", "{{cite:b0a80f1}}", "{{cite:5e866e6}}", "{{cite:d763228}}", "{{cite:59bf3b0}}", "{{cite:d8e786e}}"], "Year": ["2020", "2020", "2021", "2021", "2021", "2021"], "Problem-Task": ["AMR", "mortality", "COVID-19 severity", "phenotyping", "macular degeneration", "mortality"], "Problem-Model": ["regression", "neural network", "regression", "neural network", "neural network", "neural network"], "Dataset-Size": ["1,595", "7,803", "12,965", "11,214", "32,215", "3,431"], "XAI-Method": ["inherent", "LIME", "inherent", "inherent", "LIME", "inherent"], "XAI-C.V.": ["\u2713", "\u2717", "\u2713", "\u2717", "\u2717", "\u2717"], "XAI-Q.E.": ["\u2713", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717"], "XAI-O.A.": ["\u2713", "\u2717", "\u2717", "\u2713", "\u2713", "\u2713"]}}, "bib_hash": ["349d172410558efeb2ff6cdac24969e39e9cacb2", "b0a80f1e2e80e64e3d25ff33844191522050ff69", "5e866e6cd84eaef4301e3f762a6944933b1ccd9a", "d763228e24fd2b2e04b6cb758e3c56d770f44d75", "59bf3b0fdc4bf78431f253c21e5d683052129f02", "d8e786e8e3038a999aef0093dc66cb7113805dbb"], "row_bib_map": [{"bib_hash_or_arxiv_id": "349d172410558efeb2ff6cdac24969e39e9cacb2", "row": 0, "corpus_id": 218772411, "type": "ref"}, {"bib_hash_or_arxiv_id": "b0a80f1e2e80e64e3d25ff33844191522050ff69", "row": 1, "corpus_id": 221912722, "type": "ref"}, {"bib_hash_or_arxiv_id": "5e866e6cd84eaef4301e3f762a6944933b1ccd9a", "row": 2, "corpus_id": 232333726, "type": "ref"}, {"bib_hash_or_arxiv_id": "d763228e24fd2b2e04b6cb758e3c56d770f44d75", "row": 3, "corpus_id": 237556679, "type": "ref"}, {"bib_hash_or_arxiv_id": "59bf3b0fdc4bf78431f253c21e5d683052129f02", "row": 4, "corpus_id": 232055058, "type": "ref"}, {"bib_hash_or_arxiv_id": "d8e786e8e3038a999aef0093dc66cb7113805dbb", "row": 5, "corpus_id": 235333227, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"5\" id=\"uid48\" place=\"htp\"><head>XAI for clinical risk prediction for genomic data for different methods, as well as evaluation criteria they might satisfy (continued). C.V. stands for clinical validation, ie. whether the explainability was at all clinically evaluated, Q.E. for quantitative evaluation of the explainability process, and O.A. whether the application is open access</head>\n<row><cell halign=\"left\">2*Reference</cell>\n<cell halign=\"center\">2*Year</cell>\n<cell halign=\"center\" cols=\"2\">Problem</cell>\n<cell halign=\"center\" cols=\"1\">Dataset</cell>\n<cell halign=\"center\" cols=\"4\">XAI</cell>\n</row><row><cell halign=\"left\">(r)3-4\n(r)5-6\n(r)6-9</cell>\n<cell halign=\"center\"/>\n<cell halign=\"center\">Task</cell>\n<cell halign=\"center\">Model</cell>\n<cell halign=\"center\">Size</cell>\n<cell halign=\"center\">Method</cell>\n<cell halign=\"center\">C.V.</cell>\n<cell halign=\"center\">Q.E.</cell>\n<cell halign=\"center\">O.A.</cell>\n</row><row><cell halign=\"left\">(r)1-9\n<cit sha=\"349d172410558efeb2ff6cdac24969e39e9cacb2\"><ref target=\"bid230\"/></cit>{{cite:349d172}}</cell>\n<cell halign=\"center\">2020</cell>\n<cell halign=\"center\">AMR</cell>\n<cell halign=\"center\">regression</cell>\n<cell halign=\"center\">1,595</cell>\n<cell halign=\"center\">inherent</cell>\n<cell halign=\"center\"/>\n<cell halign=\"center\"/>\n<cell halign=\"center\"/>\n</row><row><cell halign=\"left\"><cit sha=\"b0a80f1e2e80e64e3d25ff33844191522050ff69\"><ref target=\"bid231\"/></cit>{{cite:b0a80f1}}</cell>\n<cell halign=\"center\">2020</cell>\n<cell halign=\"center\">mortality</cell>\n<cell halign=\"center\">neural network</cell>\n<cell halign=\"center\">7,803</cell>\n<cell halign=\"center\">LIME</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\"/>\n<cell halign=\"center\">X</cell>\n</row><row><cell halign=\"left\"><cit sha=\"5e866e6cd84eaef4301e3f762a6944933b1ccd9a\"><ref target=\"bid232\"/></cit>{{cite:5e866e6}}</cell>\n<cell halign=\"center\">2021</cell>\n<cell halign=\"center\">COVID-19 severity</cell>\n<cell halign=\"center\">regression</cell>\n<cell halign=\"center\">12,965</cell>\n<cell halign=\"center\">inherent</cell>\n<cell halign=\"center\"/>\n<cell halign=\"center\"/>\n<cell halign=\"center\">X</cell>\n</row><row><cell halign=\"left\"><cit sha=\"d763228e24fd2b2e04b6cb758e3c56d770f44d75\"><ref target=\"bid233\"/></cit>{{cite:d763228}}</cell>\n<cell halign=\"center\">2021</cell>\n<cell halign=\"center\">phenotyping</cell>\n<cell halign=\"center\">neural network</cell>\n<cell halign=\"center\">11,214</cell>\n<cell halign=\"center\">inherent</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\"/>\n</row><row><cell halign=\"left\"><cit sha=\"59bf3b0fdc4bf78431f253c21e5d683052129f02\"><ref target=\"bid234\"/></cit>{{cite:59bf3b0}}</cell>\n<cell halign=\"center\">2021</cell>\n<cell halign=\"center\">macular degeneration</cell>\n<cell halign=\"center\">neural network</cell>\n<cell halign=\"center\">32,215</cell>\n<cell halign=\"center\">LIME</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\"/>\n</row><row><cell halign=\"left\"><cit sha=\"d8e786e8e3038a999aef0093dc66cb7113805dbb\"><ref target=\"bid235\"/></cit>{{cite:d8e786e}}</cell>\n<cell halign=\"center\">2021</cell>\n<cell halign=\"center\">mortality</cell>\n<cell halign=\"center\">neural network</cell>\n<cell halign=\"center\">3,431</cell>\n<cell halign=\"center\">inherent</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\">X</cell>\n<cell halign=\"center\"/>\n</row></table>", "caption": "XAI for clinical risk prediction for genomic data for different methods, as well as evaluation criteria they might satisfy (continued). C.V. stands for clinical validation, ie. whether the explainability was at all clinically evaluated, Q.E. for quantitative evaluation of the explainability process, and O.A. whether the application is open access", "type": "table"}, "_table_hash_full_text": "e1d04f7d-127d-47d6-8ace-29301ba213a2", "_full_text_table_hash": "1e5afe33-474f-48fc-9bf9-3e365069c06d", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Year<~>Year": "In the context of the table, 'Year' refers to the year when the referenced research was published.", "Problem-Task<~>Problem-Task": "The 'Problem-Task' column in the table refers to the specific prediction task or domain for which the XAI (explainable artificial intelligence) method was applied in clinical risk prediction for genomic data. Examples of problem-tasks include AMR (antimicrobial resistance), mortality, COVID-19 severity, phenotyping, and macular degeneration.", "Problem-Task<~>AMR": "In the context of the given table, 'AMR' in the 'Problem-Task' column refers to Antimicrobial Resistance.", "Problem-Task<~>mortality": "In the context of the table, 'mortality' in the column 'Problem-Task' refers to the prediction of mortality using genomic data.", "Problem-Task<~>COVID-19 severity": "COVID-19 severity in the context of the table refers to the prediction of the severity of COVID-19 infections using explainable AI methods.", "Problem-Task<~>phenotyping": "Phenotyping refers to the identification and classification of an individual's observable traits or characteristics, often used in medical research and diagnosis.", "Problem-Task<~>macular degeneration": "Macular degeneration is a problem-task in the table related to clinical risk prediction for genomic data, specifically in reference [59bf3b0] of the table.", "Problem-Model<~>Problem-Model": "The 'Problem-Model' column in the table refers to the specific method used for clinical risk prediction with genomic data, such as regression or neural network.", "Problem-Model<~>regression": "Regression in the context of the table refers to a statistical problem-model used in some XAI applications for clinical risk prediction in genomics.", "Problem-Model<~>neural network": "In the context of the table, 'neural network' in the column 'Problem-Model' refers to the use of artificial neural network models for genomic data analysis in XAI for clinical risk prediction.", "Dataset-Size<~>Dataset-Size": "In the context of the table, 'Dataset-Size' refers to the number of data samples used in the XAI applications for clinical risk prediction in genomic data.", "XAI-Method<~>XAI-Method": "In the context of the table, 'XAI-Method' refers to the specific explainability method used in each clinical risk prediction application for genomic data.", "XAI-Method<~>inherent": "In the context of the table, 'inherent' in the column 'XAI-Method' refers to the fact that the specific explainability method is an inherent property or feature of the underlying machine learning or AI model used in the study and not an additional separate tool applied post-hoc.", "XAI-Method<~>LIME": "LIME in the context of the table refers to Local Interpretable Model-agnostic Explanations, a post-hoc explainability method used for interpreting the predictions of complex models.", "XAI-C.V.<~>XAI-C.V.": "XAI-C.V. refers to the clinical validation of explanatory artificial intelligence (XAI) applications for clinical risk prediction using genomic data.", "XAI-Q.E.<~>XAI-Q.E.": "XAI-Q.E. in the context of the table refers to Quantitative Evaluation of the explainability process for XAI (Explainable AI) applications for clinical risk prediction with genomic data.", "XAI-O.A.<~>XAI-O.A.": "XAI-O.A. in the context of the table refers to open access of the explainable artificial intelligence (XAI) applications for clinical risk prediction related to genomic data."}}, "title": "Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities", "abstract": "Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success."}
{"paper_id": "2308.06953v3", "_pdf_hash": null, "_source_hash": "4d0802e715398febfcc4dc0c05bee2d12af992a4", "_source_name": "2308.06953v3", "_table_hash": "b9e280ca-7a97-4486-b953-059737120e77", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr><td halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Framework</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Task</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Released</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Link</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\"><hi rend=\"small\"><hi rend=\"it\">Evaluation</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">MQM <cit sha=\"dfb8dac8b692020a79eb1146551ae568faa69a13\"><ref target=\"bid6\"/></cit>{{cite:dfb8dac}}</td>\n<td halign=\"left\">Translation</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/mqm\">thresh.tools/mqm</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">FRANK <cit sha=\"d8551c559c57cb8aa75817e5bb346f1afecc4576\"><ref target=\"bid7\"/></cit>{{cite:d8551c5}}</td>\n<td halign=\"left\">Summarization</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/frank\">thresh.tools/frank</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">SNaC <cit sha=\"6f8893a90f18cc4df6b960e9609e50e41d93c5e0\"><ref target=\"bid8\"/></cit>{{cite:6f8893a}}</td>\n<td halign=\"left\">Narrative Summarization</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/snac\">thresh.tools/snac</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">Scarecrow <cit sha=\"0dbeb3ade924e2f01ca44cfca4dcce5651251c78\"><ref target=\"bid4\"/></cit>{{cite:0dbeb3a}}</td>\n<td halign=\"left\">Open-ended Generation</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/scarecrow\">thresh.tools/scarecrow</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">SALSA <cit sha=\"f8e4914651055bea09ee2e1da320c2368e4fe42e\"><ref target=\"bid5\"/></cit>{{cite:f8e4914}}</td>\n<td halign=\"left\">Simplification</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/salsa\">thresh.tools/salsa</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">ERRANT <cit sha=\"ce925217a8b3a9df2dc6197d748781e44feabacc\"><ref target=\"bid9\"/></cit>{{cite:ce92521}}</td>\n<td halign=\"left\">Grammar Error Correction</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/errant\">thresh.tools/errant</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">FG-RLHF <cit sha=\"1266e7bc98cc5e968c31869a5daa0b30ed57fe4e\"><ref target=\"bid10\"/></cit>{{cite:1266e7b}}</td>\n<td halign=\"left\">Fine-Grained RLHF</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/fg-rlhf\">thresh.tools/fg-rlhf</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\"><hi rend=\"small\"><hi rend=\"it\">Inspection</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">MultiPIT <cit sha=\"aedecdf68c2494abded77e56fd5c2c665d83c820\"><ref target=\"bid11\"/></cit>{{cite:aedecdf}}</td>\n<td halign=\"left\">Paraphrase Generation</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/multipit\">thresh.tools/multipit</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">CWZCC <cit sha=\"85a32a63576c643d6511c34723d453c63f23de26\"><ref target=\"bid12\"/></cit>{{cite:85a32a6}}</td>\n<td halign=\"left\">Zamboanga Chavacano Spell Checking</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/cwzcc\">thresh.tools/cwzcc</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">Propaganda <cit sha=\"f0128f8fe6d16e9bcfd3f7e14d223caa641e0970\"><ref target=\"bid13\"/></cit>{{cite:f0128f8}}</td>\n<td halign=\"left\">Propaganda Analysis</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/propaganda\">thresh.tools/propaganda</xref></hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">arXivEdits <cit sha=\"253e6ae729d8fb35cc135434fc9f6b8a70619784\"><ref target=\"bid14\"/></cit>{{cite:253e6ae}}</td>\n<td halign=\"left\">Scientific Text Revision</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/arxivedits\">thresh.tools/arxivedits</xref></hi></hi><hi rend=\"small\"/></td>\n</tr></table>", "table_json": {"incomplete_rows": [{"row_idx": 1, "text": "Evaluation", "cols": 1}, {"row_idx": 9, "text": "Inspection", "cols": 1}], "table": [["Framework", "Task", "Released", "Link"], ["MQM {{cite:dfb8dac}}", "Translation", "\u2713", "thresh.tools/mqm"], ["FRANK {{cite:d8551c5}}", "Summarization", "\u2713", "thresh.tools/frank"], ["SNaC {{cite:6f8893a}}", "Narrative Summarization", "\u2713", "thresh.tools/snac"], ["Scarecrow {{cite:0dbeb3a}}", "Open-ended Generation", "\u2713", "thresh.tools/scarecrow"], ["SALSA {{cite:f8e4914}}", "Simplification", "\u2713", "thresh.tools/salsa"], ["ERRANT {{cite:ce92521}}", "Grammar Error Correction", "\u2717", "thresh.tools/errant"], ["FG-RLHF {{cite:1266e7b}}", "Fine-Grained RLHF", "\u2713", "thresh.tools/fg-rlhf"], ["MultiPIT {{cite:aedecdf}}", "Paraphrase Generation", "\u2717", "thresh.tools/multipit"], ["CWZCC {{cite:85a32a6}}", "Zamboanga Chavacano Spell Checking", "\u2717", "thresh.tools/cwzcc"], ["Propaganda {{cite:f0128f8}}", "Propaganda Analysis", "\u2713", "thresh.tools/propaganda"], ["arXivEdits {{cite:253e6ae}}", "Scientific Text Revision", "\u2713", "thresh.tools/arxivedits"]], "table_dict": {"References": ["{{cite:dfb8dac}}", "{{cite:d8551c5}}", "{{cite:6f8893a}}", "{{cite:0dbeb3a}}", "{{cite:f8e4914}}", "{{cite:ce92521}}", "{{cite:1266e7b}}", "{{cite:aedecdf}}", "{{cite:85a32a6}}", "{{cite:f0128f8}}", "{{cite:253e6ae}}"], "Framework": ["MQM ", "FRANK ", "SNaC ", "Scarecrow ", "SALSA ", "ERRANT ", "FG-RLHF ", "MultiPIT ", "CWZCC ", "Propaganda ", "arXivEdits "], "Task": ["Translation", "Summarization", "Narrative Summarization", "Open-ended Generation", "Simplification", "Grammar Error Correction", "Fine-Grained RLHF", "Paraphrase Generation", "Zamboanga Chavacano Spell Checking", "Propaganda Analysis", "Scientific Text Revision"], "Released": ["\u2713", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2713", "\u2717", "\u2717", "\u2713", "\u2713"], "Link": ["thresh.tools/mqm", "thresh.tools/frank", "thresh.tools/snac", "thresh.tools/scarecrow", "thresh.tools/salsa", "thresh.tools/errant", "thresh.tools/fg-rlhf", "thresh.tools/multipit", "thresh.tools/cwzcc", "thresh.tools/propaganda", "thresh.tools/arxivedits"]}}, "bib_hash": ["dfb8dac8b692020a79eb1146551ae568faa69a13", "d8551c559c57cb8aa75817e5bb346f1afecc4576", "6f8893a90f18cc4df6b960e9609e50e41d93c5e0", "0dbeb3ade924e2f01ca44cfca4dcce5651251c78", "f8e4914651055bea09ee2e1da320c2368e4fe42e", "ce925217a8b3a9df2dc6197d748781e44feabacc", "1266e7bc98cc5e968c31869a5daa0b30ed57fe4e", "aedecdf68c2494abded77e56fd5c2c665d83c820", "85a32a63576c643d6511c34723d453c63f23de26", "f0128f8fe6d16e9bcfd3f7e14d223caa641e0970", "253e6ae729d8fb35cc135434fc9f6b8a70619784"], "row_bib_map": [{"bib_hash_or_arxiv_id": "dfb8dac8b692020a79eb1146551ae568faa69a13", "row": 0, "corpus_id": 233444275, "type": "ref"}, {"bib_hash_or_arxiv_id": "d8551c559c57cb8aa75817e5bb346f1afecc4576", "row": 1, "corpus_id": 233407441, "type": "ref"}, {"bib_hash_or_arxiv_id": "6f8893a90f18cc4df6b960e9609e50e41d93c5e0", "row": 2, "corpus_id": 248887364, "type": "ref"}, {"bib_hash_or_arxiv_id": "0dbeb3ade924e2f01ca44cfca4dcce5651251c78", "row": 3, "corpus_id": 247315430, "type": "ref"}, {"bib_hash_or_arxiv_id": "f8e4914651055bea09ee2e1da320c2368e4fe42e", "row": 4, "corpus_id": 258865481, "type": "ref"}, {"bib_hash_or_arxiv_id": "ce925217a8b3a9df2dc6197d748781e44feabacc", "row": 5, "corpus_id": 12122749, "type": "ref"}, {"bib_hash_or_arxiv_id": "1266e7bc98cc5e968c31869a5daa0b30ed57fe4e", "row": 6, "corpus_id": 259064099, "type": "ref"}, {"bib_hash_or_arxiv_id": "aedecdf68c2494abded77e56fd5c2c665d83c820", "row": 7, "corpus_id": 252762392, "type": "ref"}, {"bib_hash_or_arxiv_id": "85a32a63576c643d6511c34723d453c63f23de26", "row": 8, "corpus_id": 218974555, "type": "ref"}, {"bib_hash_or_arxiv_id": "f0128f8fe6d16e9bcfd3f7e14d223caa641e0970", "row": 9, "corpus_id": 202788575, "type": "ref"}, {"bib_hash_or_arxiv_id": "253e6ae729d8fb35cc135434fc9f6b8a70619784", "row": 10, "corpus_id": 253157749, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row><cell halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Framework</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Task</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Released</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Link</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\"><hi rend=\"small\"><hi rend=\"it\">Evaluation</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">MQM <cit sha=\"dfb8dac8b692020a79eb1146551ae568faa69a13\"><ref target=\"bid6\"/></cit>{{cite:dfb8dac}}</cell>\n<cell halign=\"left\">Translation</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/mqm\">thresh.tools/mqm</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">FRANK <cit sha=\"d8551c559c57cb8aa75817e5bb346f1afecc4576\"><ref target=\"bid7\"/></cit>{{cite:d8551c5}}</cell>\n<cell halign=\"left\">Summarization</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/frank\">thresh.tools/frank</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">SNaC <cit sha=\"6f8893a90f18cc4df6b960e9609e50e41d93c5e0\"><ref target=\"bid8\"/></cit>{{cite:6f8893a}}</cell>\n<cell halign=\"left\">Narrative Summarization</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/snac\">thresh.tools/snac</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">Scarecrow <cit sha=\"0dbeb3ade924e2f01ca44cfca4dcce5651251c78\"><ref target=\"bid4\"/></cit>{{cite:0dbeb3a}}</cell>\n<cell halign=\"left\">Open-ended Generation</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/scarecrow\">thresh.tools/scarecrow</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">SALSA <cit sha=\"f8e4914651055bea09ee2e1da320c2368e4fe42e\"><ref target=\"bid5\"/></cit>{{cite:f8e4914}}</cell>\n<cell halign=\"left\">Simplification</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/salsa\">thresh.tools/salsa</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">ERRANT <cit sha=\"ce925217a8b3a9df2dc6197d748781e44feabacc\"><ref target=\"bid9\"/></cit>{{cite:ce92521}}</cell>\n<cell halign=\"left\">Grammar Error Correction</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/errant\">thresh.tools/errant</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">FG-RLHF <cit sha=\"1266e7bc98cc5e968c31869a5daa0b30ed57fe4e\"><ref target=\"bid10\"/></cit>{{cite:1266e7b}}</cell>\n<cell halign=\"left\">Fine-Grained RLHF</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/fg-rlhf\">thresh.tools/fg-rlhf</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\"><hi rend=\"small\"><hi rend=\"it\">Inspection</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">MultiPIT <cit sha=\"aedecdf68c2494abded77e56fd5c2c665d83c820\"><ref target=\"bid11\"/></cit>{{cite:aedecdf}}</cell>\n<cell halign=\"left\">Paraphrase Generation</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/multipit\">thresh.tools/multipit</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">CWZCC <cit sha=\"85a32a63576c643d6511c34723d453c63f23de26\"><ref target=\"bid12\"/></cit>{{cite:85a32a6}}</cell>\n<cell halign=\"left\">Zamboanga Chavacano Spell Checking</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/cwzcc\">thresh.tools/cwzcc</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">Propaganda <cit sha=\"f0128f8fe6d16e9bcfd3f7e14d223caa641e0970\"><ref target=\"bid13\"/></cit>{{cite:f0128f8}}</cell>\n<cell halign=\"left\">Propaganda Analysis</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/propaganda\">thresh.tools/propaganda</xref></hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">arXivEdits <cit sha=\"253e6ae729d8fb35cc135434fc9f6b8a70619784\"><ref target=\"bid14\"/></cit>{{cite:253e6ae}}</cell>\n<cell halign=\"left\">Scientific Text Revision</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"left\"><hi rend=\"small\"><hi rend=\"tt\"><xref url=\"https://thresh.tools/arxivedits\">thresh.tools/arxivedits</xref></hi></hi><hi rend=\"small\"/></cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "3b4a9beb-d2ea-4f3b-9301-e62bcb2ba2a0", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Framework<~>Framework": "In the context of the given table, 'Framework' refers to the specific text evaluation methodology or tool mentioned in each row, such as MQM, FRANK, SNaC, etc.", "Framework<~>MQM": "MQM in the table refers to the Machine-reading Question Map framework.", "Framework<~>FRANK": "FRANK in the table refers to the \"Framework for Real-time Annotation and Review of Natural Language Applications\" described in {{cite:d8551c5}}.", "Framework<~>SNaC": "SNaC in the table refers to the Stanford Natural Language Annotation System for Coreference Resolution framework.", "Framework<~>Scarecrow": "Scarecrow in the table refers to a specific fine-grained evaluation framework mentioned in the paper. The name of the framework is not further explained in the given text.", "Framework<~>SALSA": "SALSA in the 'Framework' column refers to the Fine-grained Annotation and Evaluation platform for Text and Speech (SALSA).", "Framework<~>ERRANT": "'ERRANT' in the 'Framework' column refers to an unspecified fine-grained text evaluation framework mentioned in the scientific paper.", "Framework<~>FG-RLHF": "FG-RLHF in the table refers to the \"Fine-Grained Relevance and Linguistic Harmony Framework\" mentioned in the paper.", "Framework<~>MultiPIT": "MultiPIT refers to the Multi-granularity Policy-driven Information Extraction and Testing framework mentioned in the paper with cite key aedecdf.", "Framework<~>CWZCC": "CWZCC is a referencesto a specific fine-grained text evaluation framework mentioned in the table. The name of the framework is not explicitly stated in the provided text, so its meaning is unclear absence of the text.", "Framework<~>Propaganda": "Propaganda in the table refers to an evaluation framework named Propaganda. Without further context from the text, it's unclear what specific meaning or characteristics are associated with this framework.", "Framework<~>arXivEdits": "'arXivEdits' in the 'Framework' column refers to an unspecified fine-grained evaluation framework mentioned in the research paper.", "Task<~>Task": "Each row in the table represents a specific NLP task for which Thresh provides annotation interfaces, as indicated by the 'Task' column.", "Task<~>Translation": "The entry 'Translation' in the 'Task' column of the table refers to the evaluation of machine translation tasks in the context of the Thresh platform.", "Task<~>Summarization": "Summarization in the table refers to the task of generating a shorter version of a text while retaining its original meaning.", "Task<~>Narrative Summarization": "Narrative Summarization refers to the task of generating a shorter version of a text that accurately conveys the key points and ideas of the original text.", "Task<~>Open-ended Generation": "Open-ended Generation in the table refers to tasks where the AI system generates text without specific instructions or constraints, allowing for creative and diverse outputs.", "Task<~>Simplification": "'Simplification' in the context of the table refers to the text generation task of making language or ideas easier to understand, typically for a younger audience or for non-native speakers.", "Task<~>Grammar Error Correction": "'Grammar Error Correction' in the column 'Task' refers to the identification and correction of grammatical errors in text.", "Task<~>Fine-Grained RLHF": "'Fine-Grained RLHF' in the table refers to Fine-Grained Relevance and Human Fairness evaluation for text generation tasks.", "Task<~>Paraphrase Generation": "Paraphrase Generation in the table refers to the task of generating a new version of a text, expressing the same meaning as the original text, but with different words or sentence structure.", "Task<~>Zamboanga Chavacano Spell Checking": "Zamboanga Chavacano Spell Checking refers to the task of checking and correcting spelling errors in the Chavacano language of Zamboanga.", "Task<~>Propaganda Analysis": "Propaganda Analysis refers to the identification and evaluation of propagandistic content or rhetoric in text, often used in the context of media or political analysis.", "Task<~>Scientific Text Revision": "Scientific Text Revision refers to the task of revising or editing scientific text for clarity, accuracy, and coherence in the given table.", "Released<~>Released": "In the context of the table, 'Released' refers to whether or not the cited references have been made publicly available within the Thresh platform. A '\u2713' indicates that the reference has been released, while an '\u2717' indicates that it has not.", "Link<~>Link": "The 'Link' column in the table refers to the URLs of the corresponding fine-grained frameworks provided by Thresh platform, which can be accessed through the given links.", "Link<~>thresh.tools/mqm": "'thresh.tools/mqm' in the table refers to the link to the MQM (Multi-Annotator Agreement) evaluation framework on the Thresh platform.", "Link<~>thresh.tools/frank": "'thresh.tools/frank' in the 'Link' column refers to the specific fine-grained framework named 'frank' that is available on the Thresh platform.", "Link<~>thresh.tools/snac": "'thresh.tools/snac' in the table refers to one of the fine-grained frameworks hosted on Thresh's community hub, specifically named 'snac'.", "Link<~>thresh.tools/scarecrow": "'thresh.tools/scarecrow' in the table refers to one of the fine-grained frameworks hosted on Thresh's community hub, available at the specified link.", "Link<~>thresh.tools/salsa": "'thresh.tools/salsa' in the table refers to the link to the SALSA (Sense and Annotation Learning System for Annotation) framework hosted on Thresh.tools.", "Link<~>thresh.tools/errant": "'thresh.tools/errant' in the column 'Link' refers to the errant evaluation framework available on the Thresh platform.", "Link<~>thresh.tools/fg-rlhf": "The 'thresh.tools/fg-rlhf' in the 'Link' column refers to the location of the annotation tool 'fg-rlhf' on the Thresh platform.", "Link<~>thresh.tools/multipit": "'thresh.tools/multipit' in the table refers to a specific fine-grained evaluation tool or framework hosted on the Thresh platform. The exact nature or functionality of this framework is not clear from the provided text.", "Link<~>thresh.tools/cwzcc": "'thresh.tools/cwzcc' in the 'Link' column refers to the URL for the CWZCC fine-grained evaluation framework hosted on Thresh tools website.", "Link<~>thresh.tools/propaganda": "'thresh.tools/propaganda' in the context of the table refers to a specific evaluation framework or tool available on the Thresh platform. The exact nature of 'propaganda' is not defined in the provided text.", "Link<~>thresh.tools/arxivedits": "The link 'thresh.tools/arxivedits' in the table refers to the annotation interface or tool provided by Thresh for annotating and evaluating text related to scholarly articles or research papers, accessible at the given URL."}}, "title": "Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation", "abstract": "Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs; and, the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. With a single YAML configuration file, users can build and test an annotation interface for any framework within minutes -- all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained frameworks and corresponding annotations made and collected by the community, covering a wide range of NLP tasks. For deployment, Thresh offers multiple options for any scale of annotation projects from small manual inspections to large crowdsourcing ones. Additionally, we introduce a Python library to streamline the entire process from typology design and deployment to annotation processing. Thresh is publicly accessible at https://thresh.tools."}
{"paper_id": "2308.06966v1", "_pdf_hash": null, "_source_hash": "8425da8c832f42bc7f6b853e0ea3baea7638f237", "_source_name": "2308.06966v1", "_table_hash": "d94c8049-6696-4812-b3dc-1da0df7ffaee", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid19\" id-text=\"3\" rend=\"display\"><head>The details of our evaluation datasets.</head>\n<unexpected><scalebox scale=\"0.55\">\n<table rend=\"inline\"><tr><td halign=\"left\"><hi rend=\"bold\">Dataset</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Lang.</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Task</hi></td>\n<td halign=\"left\"><hi rend=\"bold\">Metric</hi></td>\n</tr><tr><td halign=\"left\">2*Lenove <cit sha=\"ab830b94082af95996e53e9f8f2b24066a494912\"><ref target=\"bid53\"/></cit>{{cite:ab830b9}}</td>\n<td halign=\"left\">2*EN</td>\n<td halign=\"left\">Named Entity Recognization</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">3-4</td>\n<td halign=\"left\"/>\n<td halign=\"left\">Entity Span Detection</td>\n<td halign=\"left\">Rouge</td>\n</tr><tr><td halign=\"left\">Reddit <cit sha=\"73f84c5eb640c5442d05e75007abb31189573fbc\"><ref target=\"bid54\"/></cit>{{cite:73f84c5}}</td>\n<td halign=\"left\">EN</td>\n<td halign=\"left\">Extractive QA</td>\n<td halign=\"left\">Rouge</td>\n</tr><tr><td halign=\"left\">ABSA <cit sha=\"a52a8f1899ed17f61810d29867fbf58dc6f6dae1\"><ref target=\"bid55\"/></cit>{{cite:a52a8f1}}</td>\n<td halign=\"left\">EN</td>\n<td halign=\"left\">Review Topic Classification</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">2*MEPAVE <cit sha=\"7225d18d0c275190e7978ee441b88afa5e6ba516\"><ref target=\"bid56\"/></cit>{{cite:7225d18}}</td>\n<td halign=\"left\">2*ZH</td>\n<td halign=\"left\">Attribute Value Recognization</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">3-4</td>\n<td halign=\"left\"/>\n<td halign=\"left\">Attribute Value Detection</td>\n<td halign=\"left\">Rouge</td>\n</tr><tr><td halign=\"left\">Multi-CPR <cit sha=\"110fa74f14465487676c9b20d962ac550fe76424\"><ref target=\"bid57\"/></cit>{{cite:110fa74}}</td>\n<td halign=\"left\">ZH</td>\n<td halign=\"left\">Product Select</td>\n<td halign=\"left\">Rouge</td>\n</tr><tr><td halign=\"left\">5*OpenBG <note id=\"uid20\" id-text=\"2\" place=\"foot\">https://github.com/OpenBGBenchmark</note></td>\n<td halign=\"left\">5*ZH</td>\n<td halign=\"left\">Product Align</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">3-4</td>\n<td halign=\"left\"/>\n<td halign=\"left\">Title Attritube Matching</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">3-4</td>\n<td halign=\"left\"/>\n<td halign=\"left\">Fine-grain Product Classify</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">3-4</td>\n<td halign=\"left\"/>\n<td halign=\"left\">Coarse-grain Product Classify</td>\n<td halign=\"left\">F1, Rouge</td>\n</tr><tr><td halign=\"left\">3-4</td>\n<td halign=\"left\"/>\n<td halign=\"left\">Title Generate</td>\n<td halign=\"left\">Rouge</td>\n</tr></table></scalebox></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Dataset", "Lang.", "Task", "Metric"], ["2*Lenove {{cite:ab830b9}}", "2*EN", "Named Entity Recognization", "F1, Rouge"], ["3-4", "", "Entity Span Detection", "Rouge"], ["Reddit {{cite:73f84c5}}", "EN", "Extractive QA", "Rouge"], ["ABSA {{cite:a52a8f1}}", "EN", "Review Topic Classification", "F1, Rouge"], ["2*MEPAVE {{cite:7225d18}}", "2*ZH", "Attribute Value Recognization", "F1, Rouge"], ["3-4", "", "Attribute Value Detection", "Rouge"], ["Multi-CPR {{cite:110fa74}}", "ZH", "Product Select", "Rouge"], ["5*OpenBG https://github.com/OpenBGBenchmark", "5*ZH", "Product Align", "F1, Rouge"], ["3-4", "", "Title Attritube Matching", "F1, Rouge"], ["3-4", "", "Fine-grain Product Classify", "F1, Rouge"], ["3-4", "", "Coarse-grain Product Classify", "F1, Rouge"], ["3-4", "", "Title Generate", "Rouge"]], "table_dict": {"References": ["{{cite:ab830b9}}", "{{cite:73f84c5}}", "{{cite:a52a8f1}}", "{{cite:7225d18}}", "{{cite:110fa74}}", "-"], "Dataset": ["Lenove ", "Reddit ", "ABSA ", "MEPAVE ", "Multi-CPR ", "OpenBG https://github.com/OpenBGBenchmark"], "Lang.": ["EN", "EN", "EN", "ZH", "ZH", "ZH"], "Task": [["Named Entity Recognization", "Entity Span Detection"], "Extractive QA", "Review Topic Classification", ["Attribute Value Recognization", "Attribute Value Detection"], "Product Select", ["Product Align", "Title Attritube Matching", "Fine-grain Product Classify", "Coarse-grain Product Classify", "Title Generate"]], "Metric": [["F1, Rouge", "Rouge"], "Rouge", "F1, Rouge", ["F1, Rouge", "Rouge"], "Rouge", ["F1, Rouge", "F1, Rouge", "F1, Rouge", "F1, Rouge", "Rouge"]]}}, "bib_hash": ["ab830b94082af95996e53e9f8f2b24066a494912", "73f84c5eb640c5442d05e75007abb31189573fbc", "a52a8f1899ed17f61810d29867fbf58dc6f6dae1", "7225d18d0c275190e7978ee441b88afa5e6ba516", "110fa74f14465487676c9b20d962ac550fe76424"], "row_bib_map": [{"bib_hash_or_arxiv_id": "ab830b94082af95996e53e9f8f2b24066a494912", "row": 0, "corpus_id": 218862749, "type": "ref"}, {"bib_hash_or_arxiv_id": "73f84c5eb640c5442d05e75007abb31189573fbc", "row": 1, "corpus_id": 216867120, "type": "ref"}, {"bib_hash_or_arxiv_id": "a52a8f1899ed17f61810d29867fbf58dc6f6dae1", "row": 2, "corpus_id": 61955135, "type": "ref"}, {"bib_hash_or_arxiv_id": "7225d18d0c275190e7978ee441b88afa5e6ba516", "row": 3, "corpus_id": 221703022, "type": "ref"}, {"bib_hash_or_arxiv_id": "110fa74f14465487676c9b20d962ac550fe76424", "row": 4, "corpus_id": 247292113, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.06966v1", "row": 5, "corpus_id": 260887693, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"3\" id=\"uid19\"><head>The details of our evaluation datasets.</head>\n<unexpected><scalebox scale=\"0.55\">\n<table rend=\"inline\"><row><cell halign=\"left\"><hi rend=\"bold\">Dataset</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Lang.</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Task</hi></cell>\n<cell halign=\"left\"><hi rend=\"bold\">Metric</hi></cell>\n</row><row><cell halign=\"left\">2*Lenove <cit sha=\"ab830b94082af95996e53e9f8f2b24066a494912\"><ref target=\"bid53\"/></cit>{{cite:ab830b9}}</cell>\n<cell halign=\"left\">2*EN</cell>\n<cell halign=\"left\">Named Entity Recognization</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">3-4</cell>\n<cell halign=\"left\"/>\n<cell halign=\"left\">Entity Span Detection</cell>\n<cell halign=\"left\">Rouge</cell>\n</row><row><cell halign=\"left\">Reddit <cit sha=\"73f84c5eb640c5442d05e75007abb31189573fbc\"><ref target=\"bid54\"/></cit>{{cite:73f84c5}}</cell>\n<cell halign=\"left\">EN</cell>\n<cell halign=\"left\">Extractive QA</cell>\n<cell halign=\"left\">Rouge</cell>\n</row><row><cell halign=\"left\">ABSA <cit sha=\"a52a8f1899ed17f61810d29867fbf58dc6f6dae1\"><ref target=\"bid55\"/></cit>{{cite:a52a8f1}}</cell>\n<cell halign=\"left\">EN</cell>\n<cell halign=\"left\">Review Topic Classification</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">2*MEPAVE <cit sha=\"7225d18d0c275190e7978ee441b88afa5e6ba516\"><ref target=\"bid56\"/></cit>{{cite:7225d18}}</cell>\n<cell halign=\"left\">2*ZH</cell>\n<cell halign=\"left\">Attribute Value Recognization</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">3-4</cell>\n<cell halign=\"left\"/>\n<cell halign=\"left\">Attribute Value Detection</cell>\n<cell halign=\"left\">Rouge</cell>\n</row><row><cell halign=\"left\">Multi-CPR <cit sha=\"110fa74f14465487676c9b20d962ac550fe76424\"><ref target=\"bid57\"/></cit>{{cite:110fa74}}</cell>\n<cell halign=\"left\">ZH</cell>\n<cell halign=\"left\">Product Select</cell>\n<cell halign=\"left\">Rouge</cell>\n</row><row><cell halign=\"left\">5*OpenBG <note id-text=\"2\" id=\"uid20\" place=\"foot\">https://github.com/OpenBGBenchmark</note></cell>\n<cell halign=\"left\">5*ZH</cell>\n<cell halign=\"left\">Product Align</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">3-4</cell>\n<cell halign=\"left\"/>\n<cell halign=\"left\">Title Attritube Matching</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">3-4</cell>\n<cell halign=\"left\"/>\n<cell halign=\"left\">Fine-grain Product Classify</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">3-4</cell>\n<cell halign=\"left\"/>\n<cell halign=\"left\">Coarse-grain Product Classify</cell>\n<cell halign=\"left\">F1, Rouge</cell>\n</row><row><cell halign=\"left\">3-4</cell>\n<cell halign=\"left\"/>\n<cell halign=\"left\">Title Generate</cell>\n<cell halign=\"left\">Rouge</cell>\n</row></table></scalebox></unexpected></table>", "caption": "The details of our evaluation datasets.", "type": "table"}, "_table_hash_full_text": "e1d39716-006e-49b3-aadd-f67a2dbb8880", "_full_text_table_hash": "c2fdbeb9-936a-4437-8bc9-1636a337dcce", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Dataset<~>Lenove": "'Lenove' in the column 'Dataset' refers to a specific dataset cited by the reference {{cite:ab830b9}}. The name of the dataset is not explained in the text and its nature is unknown without additional context.", "Dataset<~>Reddit": "The 'Dataset' column with the value 'Reddit' refers to a specific dataset used in the evaluation of the model mentioned in the text. The exact nature or context of this dataset in relation to e-commerce tasks is not clear without additional information.", "Dataset<~>ABSA": "ABSA in the table refers to the Aspect-based Sentiment Analysis dataset.", "Dataset<~>MEPAVE": "MEPAVE is a reference to an unspecified dataset named in cite:7225d18.", "Dataset<~>Multi-CPR": "Multi-CPR in the 'Dataset' column refers to a specific dataset named Multi-CPR. The name of the dataset is not explained in the provided text.", "Dataset<~>OpenBG https://github.com/OpenBGBenchmark": "OpenBG refers to the OpenBGBenchmark repository, which is mentioned in reference {{cite:ab830b9}} but not explicitly stated in the text as one of the evaluation datasets used in the study. It is important to note that the text explicitly states the datasets used for evaluation in the study, and thistable appears to be an additional table not directly related to the main study, as it does not correspond to the datasets mentioned in the text. Therefore, the table and its contents may not reflect the datasets used in the study and the reference to OpenBG in this table context is unrelated to the EcomInstruct dataset discussed in the text.", "Lang.<~>Lang.": "In the context of the table, 'Lang.' refers to the language of the cited references.", "Lang.<~>EN": "'EN' in the 'Lang.' column refers to English language.", "Lang.<~>ZH": "The 'ZH' in the 'Lang.' column of the table refers to the Chinese language.", "Task<~>Task": "In the context of the table, 'Task' refers to specific types of evaluation tasks, such as named entity recognition, extractive QA, review topic classification, attribute value recognition, product selection, and others.", "Task<~>['Named Entity Recognization', 'Entity Span Detection']": "The task 'Named Entity Recognization, Entity Span Detection' refers to the identification and extraction of named entities and their corresponding spans from text in Natural Language Processing.", "Task<~>Extractive QA": "Extractive QA in the 'Task' column refers to question answering tasks where the model is expected to extract the answer directly from the provided text, rather than generating new text.", "Task<~>Review Topic Classification": "Review Topic Classification refers to the task of categorizing customer reviews into different topics based on their content.", "Task<~>['Attribute Value Recognization', 'Attribute Value Detection']": "The tasks 'Attribute Value Recognization' and 'Attribute Value Detection' in the 'Task' column refer to the identification and extraction of attribute values in text data, respectively.", "Task<~>Product Select": "Product Select in the table refers to tasks related to selecting or filtering products based on certain criteria.", "Task<~>['Product Align', 'Title Attritube Matching', 'Fine-grain Product Classify', 'Coarse-grain Product Classify', 'Title Generate']": "The column 'Task' in the table with the label '[Product Align, Title Attribute Matching, Fine-grain Product Classify, Coarse-grain Product Classify, Title Generate]' refers to tasks related to aligning product information, matching title attributes, fine-grain and coarse-grain product classification, and generating product titles in the context of the EcomInstruct dataset.", "Metric<~>Metric": "In the context of the table, 'Metric' refers to the evaluation metrics used to assess the performance of the model on various tasks in the EcomInstruct dataset.", "Metric<~>['F1, Rouge', 'Rouge']": "In the given table, the 'Metric' column for some references lists '[F1, Rouge, Rouge]'. This indicates that these references use F1 score and ROUGE metric for evaluation. ROUGE is a set of metrics for evaluating the quality of summaries, and F1 score is a metric for evaluating binary classification problems. In this context, it seems that the studies are using both metrics, F1 score and ROUGE, for evaluating the performance of models on text generation tasks.", "Metric<~>Rouge": "Rouge in the context of the table refers to the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric used to evaluate text generation models, specifically in the context of this study for e-commerce tasks.", "Metric<~>F1, Rouge": "The table indicates that the evaluation metrics for different references include ROUGE and F1 for text generation tasks, and ROUGE specifically for previous works on text generation.", "Metric<~>['F1, Rouge', 'F1, Rouge', 'F1, Rouge', 'F1, Rouge', 'Rouge']": "The 'Metric' column in the table refers to the evaluation metrics used for different tasks in the EcomInstruct dataset, specifically F1 score and ROUGE for text generation tasks and ROUGE for classification and NER tasks. The repetition of the metrics in some rows indicates their usage for multiple related tasks."}}, "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce", "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks."}
{"paper_id": "2308.14430v1", "_pdf_hash": null, "_source_hash": "0f152b9720ff7da81cf94deb0b6ab6d81ad76df7", "_source_name": "2308.14430v1", "_table_hash": "3f8046c8-4fd5-40aa-a10f-b06c1fdfb796", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid1\" id-text=\"1\" place=\"t\" rend=\"display\" starred=\"true\"><head>Comparison between Text style prompt TTS datasets.</head>\n<tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\" right-border=\"true\">Dataset</td>\n<td halign=\"center\">Open source</td>\n<td halign=\"center\">Hours</td>\n<td halign=\"center\">Text items</td>\n<td halign=\"center\">Prompt diversity</td>\n<td halign=\"center\">Speaker</td>\n<td>Emotion</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">StylePrompt <cit sha=\"d77dcca00369731a762bc8bd4ef82e99ba5a4dbd\"><ref target=\"bid9\"/></cit>{{cite:d77dcca}}</td>\n<td halign=\"center\">no</td>\n<td halign=\"center\">12</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">8</td>\n<td>-</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">NLSpeech <cit sha=\"63b50a81563bcc58cf2f2ec82195effe0d7af490\"><ref target=\"bid10\"/></cit>{{cite:63b50a8}}</td>\n<td halign=\"center\">no</td>\n<td halign=\"center\">44</td>\n<td halign=\"center\">32000</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">7</td>\n<td><hi rend=\"bold\">yes</hi></td>\n</tr><tr><td halign=\"center\" right-border=\"true\">PromptSpeech <cit sha=\"e8b4f484242630840850365c105b895daab661aa\"><ref target=\"bid7\"/></cit>{{cite:e8b4f48}}</td>\n<td halign=\"center\"><hi rend=\"bold\">yes (part)</hi></td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">27893</td>\n<td halign=\"center\">5</td>\n<td halign=\"center\">-</td>\n<td>no</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\"><hi rend=\"bold\">TextrolSpeech</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">yes</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">330</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">236220</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">500</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">1324</hi></td>\n<td><hi rend=\"bold\">yes</hi></td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Dataset", "Open source", "Hours", "Text items", "Prompt diversity", "Speaker", "Emotion"], ["StylePrompt {{cite:d77dcca}}", "no", "12", "-", "-", "8", "-"], ["NLSpeech {{cite:63b50a8}}", "no", "44", "32000", "-", "7", "yes"], ["PromptSpeech {{cite:e8b4f48}}", "yes (part)", "-", "27893", "5", "-", "no"], ["TextrolSpeech", "yes", "330", "236220", "500", "1324", "yes"]], "table_dict": {"References": ["{{cite:d77dcca}}", "{{cite:63b50a8}}", "{{cite:e8b4f48}}", "-"], "Dataset": ["StylePrompt ", "NLSpeech ", "PromptSpeech ", "TextrolSpeech"], "Open source": ["no", "no", "yes (part)", "yes"], "Hours": ["12", "44", "-", "330"], "Text items": ["-", "32000", "27893", "236220"], "Prompt diversity": ["-", "-", "5", "500"], "Speaker": ["8", "7", "-", "1324"], "Emotion": ["-", "yes", "no", "yes"]}}, "bib_hash": ["d77dcca00369731a762bc8bd4ef82e99ba5a4dbd", "63b50a81563bcc58cf2f2ec82195effe0d7af490", "e8b4f484242630840850365c105b895daab661aa"], "row_bib_map": [{"bib_hash_or_arxiv_id": "d77dcca00369731a762bc8bd4ef82e99ba5a4dbd", "row": 0, "corpus_id": 258987676, "type": "ref"}, {"bib_hash_or_arxiv_id": "63b50a81563bcc58cf2f2ec82195effe0d7af490", "row": 1, "corpus_id": 256416291, "type": "ref"}, {"bib_hash_or_arxiv_id": "e8b4f484242630840850365c105b895daab661aa", "row": 2, "corpus_id": 253761189, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.14430v1", "row": 3, "corpus_id": 261242529, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid1\" starred=\"true\" place=\"t\"><head>Comparison between Text style prompt TTS datasets.</head>\n<row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"center\">Dataset</cell>\n<cell halign=\"center\">Open source</cell>\n<cell halign=\"center\">Hours</cell>\n<cell halign=\"center\">Text items</cell>\n<cell halign=\"center\">Prompt diversity</cell>\n<cell halign=\"center\">Speaker</cell>\n<cell>Emotion</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">StylePrompt <cit sha=\"d77dcca00369731a762bc8bd4ef82e99ba5a4dbd\"><ref target=\"bid9\"/></cit>{{cite:d77dcca}}</cell>\n<cell halign=\"center\">no</cell>\n<cell halign=\"center\">12</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">8</cell>\n<cell>-</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">NLSpeech <cit sha=\"63b50a81563bcc58cf2f2ec82195effe0d7af490\"><ref target=\"bid10\"/></cit>{{cite:63b50a8}}</cell>\n<cell halign=\"center\">no</cell>\n<cell halign=\"center\">44</cell>\n<cell halign=\"center\">32000</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">7</cell>\n<cell><hi rend=\"bold\">yes</hi></cell>\n</row><row><cell right-border=\"true\" halign=\"center\">PromptSpeech <cit sha=\"e8b4f484242630840850365c105b895daab661aa\"><ref target=\"bid7\"/></cit>{{cite:e8b4f48}}</cell>\n<cell halign=\"center\"><hi rend=\"bold\">yes (part)</hi></cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">27893</cell>\n<cell halign=\"center\">5</cell>\n<cell halign=\"center\">-</cell>\n<cell>no</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">TextrolSpeech</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">yes</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">330</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">236220</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">500</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">1324</hi></cell>\n<cell><hi rend=\"bold\">yes</hi></cell>\n</row></table>", "caption": "Comparison between Text style prompt TTS datasets.", "type": "table"}, "_table_hash_full_text": "9a42b01c-8679-4a42-a41a-5f54fa34390c", "_full_text_table_hash": "c6fc0305-b065-4cbc-8dcc-3e48ce4c4e5b", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Dataset<~>StylePrompt": "'StylePrompt' in the 'Dataset' column refers to the TextrolSpeech dataset, which is a large-scale, open-source text style prompt speech emotion dataset proposed in the text.", "Dataset<~>NLSpeech": "NLSpeech refers to a text style prompt speech dataset mentioned in one of the cited references.", "Dataset<~>PromptSpeech": "PromptSpeech in the context of the table refers to the unnamed dataset associated with the reference [[e8b4f48]] in the Text style prompt TTS datasets comparison. Based on the text provided, it can be inferred that it is the dataset proposed by PromptTTS for advanced text-controllable TTS models. However, the text does not mention the name 'PromptSpeech' for this dataset, which may create confusion with the other datasets listed in the table.", "Dataset<~>TextrolSpeech": "TextrolSpeech refers to the novel 330-hour clean text style prompt speech emotion dataset introduced in the paper.", "Open source<~>Open source": "In the context of the table, \"open source\" refers to the availability of the dataset for public use and modification. In this case, reference 3 ({{cite:e8b4f48}}) is marked as having partial open-source availability for TextrolSpeech.", "Open source<~>yes (part)": "'Yes (part)' in the 'Open source' column for reference [[e8b4f48]] in the table refers to the fact that only part of TextrolSpeech, the dataset proposed in the text, is open-source.", "Hours<~>Hours": "In the given table, 'Hours' refers to the duration of speech data in hours associated with each reference.", "Text items<~>Text items": "In the context of the table, 'Text items' likely refers to the number of text descriptions available in each dataset.", "Prompt diversity<~>Prompt diversity": "Prompt diversity in this context refers to the number of distinct natural language text descriptions available in each dataset for controlling the style of speech in text-to-speech systems. In the given table, the value \"5\" in the second row indicates that the TextrolSpeech dataset has five distinct text descriptions for each style, contributing to its high prompt diversity.", "Speaker<~>Speaker": "In the context of the table, 'Speaker' refers to the identification number of the cited reference.", "Emotion<~>Emotion": "In the given context, the term 'Emotion' in the table refers to whether or not the dataset contains emotions as a style factor in its text descriptions for Text-to-Speech (TTS) models. 'Yes' denotes that the dataset includes emotional text descriptions, while 'No' denotes the absence of such features. '-' signifies that there is no clear information available about the specific dataset regarding emotional text descriptions."}}, "title": "TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models", "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/"}
{"paper_id": "2308.14371v2", "_pdf_hash": null, "_source_hash": "ffcb0134e7eddfff78990d48a72e1453f060ce3e", "_source_name": "2308.14371v2", "_table_hash": "8b29f176-220d-4c3a-aecd-3ec67a6fb864", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid34\" id-text=\"1\" rend=\"display\"><head>The UDF estimation comparison between NP<cit sha=\"86d343a02fd22208d6e8b45e1356c40b8b6d6453\"><ref target=\"bid6\"/></cit>{{cite:86d343a}}, CAP<cit sha=\"c91528eb71713b335d30763b5083625233be1ca5\"><ref target=\"bid14\"/></cit>{{cite:c91528e}} and ours.</head>\n<tr><td halign=\"left\">Method</td>\n<td halign=\"left\">Open surface</td>\n<td halign=\"left\">Inference Time</td>\n<td halign=\"left\">Sparse Input</td>\n</tr><tr><td halign=\"left\">NP<cit sha=\"86d343a02fd22208d6e8b45e1356c40b8b6d6453\"><ref target=\"bid6\"/></cit>{{cite:86d343a}}</td>\n<td halign=\"left\">No</td>\n<td halign=\"left\">29min22s</td>\n<td halign=\"left\">Hard</td>\n</tr><tr><td halign=\"left\">CAP<cit sha=\"c91528eb71713b335d30763b5083625233be1ca5\"><ref target=\"bid14\"/></cit>{{cite:c91528e}}</td>\n<td halign=\"left\">Yes</td>\n<td halign=\"left\">21min56s</td>\n<td halign=\"left\">Hard</td>\n</tr><tr><td halign=\"left\">Ours</td>\n<td halign=\"left\">Yes</td>\n<td halign=\"left\">9s</td>\n<td halign=\"left\">Easy</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Method", "Open surface", "Inference Time", "Sparse Input"], ["NP{{cite:86d343a}}", "No", "29min22s", "Hard"], ["CAP{{cite:c91528e}}", "Yes", "21min56s", "Hard"], ["Ours", "Yes", "9s", "Easy"]], "table_dict": {"References": ["{{cite:86d343a}}", "{{cite:c91528e}}", "-"], "Method": ["NP", "CAP", "Ours"], "Open surface": ["No", "Yes", "Yes"], "Inference Time": ["29min22s", "21min56s", "9s"], "Sparse Input": ["Hard", "Hard", "Easy"]}}, "bib_hash": ["86d343a02fd22208d6e8b45e1356c40b8b6d6453", "c91528eb71713b335d30763b5083625233be1ca5", "86d343a02fd22208d6e8b45e1356c40b8b6d6453", "c91528eb71713b335d30763b5083625233be1ca5"], "row_bib_map": [{"bib_hash_or_arxiv_id": "86d343a02fd22208d6e8b45e1356c40b8b6d6453", "row": 0, "corpus_id": 227209266, "type": "ref"}, {"bib_hash_or_arxiv_id": "c91528eb71713b335d30763b5083625233be1ca5", "row": 1, "corpus_id": 252735231, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.14371v2", "row": 2, "corpus_id": 261242415, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid34\"><head>The UDF estimation comparison between NP\u00a0<cit sha=\"86d343a02fd22208d6e8b45e1356c40b8b6d6453\"><ref target=\"bid6\"/></cit>{{cite:86d343a}}, CAP\u00a0<cit sha=\"c91528eb71713b335d30763b5083625233be1ca5\"><ref target=\"bid14\"/></cit>{{cite:c91528e}} and ours.</head>\n<row><cell halign=\"left\">Method</cell>\n<cell halign=\"left\">Open surface</cell>\n<cell halign=\"left\">Inference Time</cell>\n<cell halign=\"left\">Sparse Input</cell>\n</row><row><cell halign=\"left\">NP\u00a0<cit sha=\"86d343a02fd22208d6e8b45e1356c40b8b6d6453\"><ref target=\"bid6\"/></cit>{{cite:86d343a}}</cell>\n<cell halign=\"left\">No</cell>\n<cell halign=\"left\">29min22s</cell>\n<cell halign=\"left\">Hard</cell>\n</row><row><cell halign=\"left\">CAP\u00a0<cit sha=\"c91528eb71713b335d30763b5083625233be1ca5\"><ref target=\"bid14\"/></cit>{{cite:c91528e}}</cell>\n<cell halign=\"left\">Yes</cell>\n<cell halign=\"left\">21min56s</cell>\n<cell halign=\"left\">Hard</cell>\n</row><row><cell halign=\"left\">Ours</cell>\n<cell halign=\"left\">Yes</cell>\n<cell halign=\"left\">9s</cell>\n<cell halign=\"left\">Easy</cell>\n</row></table>", "caption": "The UDF estimation comparison between NP\u00a0{{cite:86d343a}}, CAP\u00a0{{cite:c91528e}} and ours.", "type": "table"}, "_full_text_table_hash": "6b6fb748-8152-401d-a920-7b8f265c5a6a", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Method<~>Method": "In the given context of the table, 'Method' refers to the specific algorithms or methods being compared in the table, represented by their corresponding citations. In this case, the methods are Neural Pull (NP) represented by [[cite:86d343a]], CAP represented by [[cite:c91528e]], and the proposed method denoted by a hyphen (-) in the third row.", "Method<~>NP": "'NP' in the table refers to Neural Pull, as indicated in the references column.", "Method<~>CAP": "CAP in the context of the table refers to the method called \"Consistent Approximate Surface Reconstruction from Unordered Point Clouds\" as cited by {{cite:c91528e}}.", "Method<~>Ours": "In the context of the table, 'Ours' in the 'Method' column refers to the method proposed by the authors of the scientific paper, SuperUDF.", "Open surface<~>Open surface": "In the given context, 'open surface' refers to surfaces with boundaries or holes where the underlying surface is not completely connected.", "Inference Time<~>Inference Time": "Inference Time refers to the time taken to make a prediction using each method, specifically for UDF estimation, as indicated in the table and the caption.", "Sparse Input<~>Sparse Input": "In the given context, 'Sparse Input' refers to the requirement for dense points as input in Neural Pull and CAP methods, in contrast to ours which can work better on relatively sparse point clouds.", "Sparse Input<~>Hard": "In the context of the table, 'Hard' in the column 'Sparse Input' for references [{{cite:86d343a}}] and [{{cite:c91528e}}] refers to their difficulty in handling relatively sparse point clouds for UDF estimation.", "Sparse Input<~>Easy": "'Easy' in the column 'Sparse Input' for the reference with a hyphen (-) refers to the method that can handle relatively sparse input points more effectively compared to Neural Pull and CAP."}}, "title": "SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction", "abstract": "Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code will be released after accteptance."}
{"paper_id": "2308.09090v1", "_pdf_hash": null, "_source_hash": "22b10ac47563ab0e4cf9b6239b2fca3ee16a1c9a", "_source_name": "2308.09090v1", "_table_hash": "5ac6483b-03ae-448e-a80a-b453d252023b", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid29\" id-text=\"10\" rend=\"display\"><head>Summary of key works in DL-ISAC for channel estimation and IRS.</head>\n<unexpected><resizebox width=\"427.0pt\"><table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">Ref.</td>\n<td halign=\"center\" right-border=\"true\">Technology</td>\n<td halign=\"center\" right-border=\"true\">AI technique</td>\n<td halign=\"center\" right-border=\"true\">Criteria</td>\n<td halign=\"center\" right-border=\"true\">Input</td>\n<td halign=\"center\" right-border=\"true\">Output</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\"><cit sha=\"0af48c5fd0c63dc99b8ce98c456fe9caff8d218b\"><ref target=\"bid47\"/></cit>{{cite:0af48c5}}</td>\n<td halign=\"center\" right-border=\"true\">IRS-assisted V2I</td>\n<td halign=\"center\" right-border=\"true\">DL</td>\n<td halign=\"center\" right-border=\"true\">CAP-Net (conv layers and LSTM)</td>\n<td halign=\"center\" right-border=\"true\">Historical covariance of the received echo</td>\n<td halign=\"center\" right-border=\"true\">AoAs</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\"><cit sha=\"1b511a44aaae7d50389131410ca665fb8cfee3a4\"><ref target=\"bid93\"/></cit>{{cite:1b511a4}}</td>\n<td halign=\"center\" right-border=\"true\">General applications</td>\n<td halign=\"center\" right-border=\"true\">DL</td>\n<td halign=\"center\" right-border=\"true\">CNN (Trained as a denoiser)</td>\n<td halign=\"center\" right-border=\"true\">Channel estimated by LS method</td>\n<td halign=\"center\" right-border=\"true\">Channel estimation</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\"><cit sha=\"23432d7e146110fc770507a98d81365a48c96c41\"><ref target=\"bid94\"/></cit>{{cite:23432d7}}</td>\n<td halign=\"center\" right-border=\"true\">IRS-assisted ISAC MISO</td>\n<td halign=\"center\" right-border=\"true\">DL</td>\n<td halign=\"center\" right-border=\"true\">1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)</td>\n<td halign=\"center\" right-border=\"true\">1.Received direct signals2.Total received signals and DE-CNN estimation</td>\n<td halign=\"center\" right-border=\"true\">Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels</td>\n</tr></table>\n</resizebox></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Ref.", "Technology", "AI technique", "Criteria", "Input", "Output"], ["{{cite:0af48c5}}", "IRS-assisted V2I", "DL", "CAP-Net (conv layers and LSTM)", "Historical covariance of the received echo", "AoAs"], ["{{cite:1b511a4}}", "General applications", "DL", "CNN (Trained as a denoiser)", "Channel estimated by LS method", "Channel estimation"], ["{{cite:23432d7}}", "IRS-assisted ISAC MISO", "DL", "1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)", "1.Received direct signals2.Total received signals and DE-CNN estimation", "Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels"]], "table_dict": {"References": ["{{cite:0af48c5}}", "{{cite:1b511a4}}", "{{cite:23432d7}}"], "Technology": ["IRS-assisted V2I", "General applications", "IRS-assisted ISAC MISO"], "AI technique": ["DL", "DL", "DL"], "Criteria": ["CAP-Net (conv layers and LSTM)", "CNN (Trained as a denoiser)", "1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)"], "Input": ["Historical covariance of the received echo", "Channel estimated by LS method", "1.Received direct signals2.Total received signals and DE-CNN estimation"], "Output": ["AoAs", "Channel estimation", "Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels"]}}, "bib_hash": ["0af48c5fd0c63dc99b8ce98c456fe9caff8d218b", "1b511a44aaae7d50389131410ca665fb8cfee3a4", "23432d7e146110fc770507a98d81365a48c96c41"], "row_bib_map": [{"bib_hash_or_arxiv_id": "0af48c5fd0c63dc99b8ce98c456fe9caff8d218b", "row": 0, "corpus_id": 257801039, "type": "ref"}, {"bib_hash_or_arxiv_id": "1b511a44aaae7d50389131410ca665fb8cfee3a4", "row": 1, "corpus_id": 255658077, "type": "ref"}, {"bib_hash_or_arxiv_id": "23432d7e146110fc770507a98d81365a48c96c41", "row": 2, "corpus_id": 255080909, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"10\" id=\"uid29\"><head>Summary of key works in DL-ISAC for channel estimation and IRS.</head>\n<unexpected><resizebox width=\"427.0pt\"><table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">Ref.</cell>\n<cell right-border=\"true\" halign=\"center\">Technology</cell>\n<cell right-border=\"true\" halign=\"center\">AI technique</cell>\n<cell right-border=\"true\" halign=\"center\">Criteria</cell>\n<cell right-border=\"true\" halign=\"center\">Input</cell>\n<cell right-border=\"true\" halign=\"center\">Output</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\"><cit sha=\"0af48c5fd0c63dc99b8ce98c456fe9caff8d218b\"><ref target=\"bid47\"/></cit>{{cite:0af48c5}}</cell>\n<cell right-border=\"true\" halign=\"center\">IRS-assisted V2I</cell>\n<cell right-border=\"true\" halign=\"center\">DL</cell>\n<cell right-border=\"true\" halign=\"center\">CAP-Net (conv layers and LSTM)</cell>\n<cell right-border=\"true\" halign=\"center\">Historical covariance of the received echo</cell>\n<cell right-border=\"true\" halign=\"center\">AoAs</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\"><cit sha=\"1b511a44aaae7d50389131410ca665fb8cfee3a4\"><ref target=\"bid93\"/></cit>{{cite:1b511a4}}</cell>\n<cell right-border=\"true\" halign=\"center\">General applications</cell>\n<cell right-border=\"true\" halign=\"center\">DL</cell>\n<cell right-border=\"true\" halign=\"center\">CNN (Trained as a denoiser)</cell>\n<cell right-border=\"true\" halign=\"center\">Channel estimated by LS method</cell>\n<cell right-border=\"true\" halign=\"center\">Channel estimation</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\"><cit sha=\"23432d7e146110fc770507a98d81365a48c96c41\"><ref target=\"bid94\"/></cit>{{cite:23432d7}}</cell>\n<cell right-border=\"true\" halign=\"center\">IRS-assisted ISAC MISO</cell>\n<cell right-border=\"true\" halign=\"center\">DL</cell>\n<cell right-border=\"true\" halign=\"center\">1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)</cell>\n<cell right-border=\"true\" halign=\"center\">1.Received direct signals2.Total received signals and DE-CNN estimation</cell>\n<cell right-border=\"true\" halign=\"center\">Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels</cell>\n</row></table>\n</resizebox></unexpected></table>", "caption": "Summary of key works in DL-ISAC for channel estimation and IRS.", "type": "table"}, "_table_hash_full_text": "75d8c4b6-970b-4ffe-b202-cb4d4a726dee", "_full_text_table_hash": "67e1500f-559c-4454-a570-60b8d6b5992c", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Technology<~>Technology": "In the context of the table, 'Technology' refers to the specific application domain of the DL-ISAC techniques used for channel estimation and IRS, such as IRS-assisted V2I, general applications, and IRS-assisted ISAC MISO.", "Technology<~>IRS-assisted V2I": "IRS-assisted V2I refers to Intelligent Reflecting Surface (IRS) assisted vehicle-to-infrastructure (V2I) communication systems.", "Technology<~>General applications": "'General applications' in the column 'Technology' refers to applications of deep learning in ISAC systems beyond Intelligent Reflecting Surface (IRS)-assisted ISAC and V2I communications.", "Technology<~>IRS-assisted ISAC MISO": "IRS-assisted ISAC MISO refers to Integrated Sensing and Communication systems that are assisted by an Intelligent Reflecting Surface (IRS) in a Multiple-Input-Single-Output (MISO) configuration.", "AI technique<~>AI technique": "In the context of the table, 'AI technique' refers to deep learning (DL) algorithms used for channel estimation and IRS in Integrated Sensing and Communication (ISAC) systems.", "AI technique<~>DL": "The 'DL' in the column 'AI technique' refers to Deep Learning.", "Criteria<~>Criteria": "The term 'Criteria' in the table refers to the specific methods or techniques used in the referenced works for channel estimation and IRS in DL-ISAC systems.", "Criteria<~>CAP-Net (conv layers and LSTM)": "CAP-Net in the table refers to a Deep Neural Network consisting of convolutional layers and Long Short-Term Memory (LSTM) units for channel estimation and Angle of Arrival estimation in IRS-assisted ISAC systems.", "Criteria<~>CNN (Trained as a denoiser)": "The term 'CNN (Trained as a denoiser)' in the table refers to a convolutional neural network that has been trained as a denoiser to improve channel estimation in IRS-assisted ISAC systems.", "Criteria<~>1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)": "The criteria column '1.Direct estimation CNN (DE-CNN)2.Reflected estimation CNN (RE-CNN)' in the table refers to two different types of convolutional neural networks used for channel estimation in IRS-assisted ISAC systems, one for direct estimation and the other for reflected estimation.", "Input<~>Input": "The 'Input' column in the table refers to the specific input data used in each of the key works summarized in the table for deep learning-based integrated sensing and communication (DL-ISAC) techniques in channel estimation and intelligent reflecting surface (IRS) applications. For example, the first row uses historical covariance of the received echo as input, while the second row uses the channel estimated by the least squares method as input, and the third row uses received direct signals and the total received signals as inputs.", "Input<~>Historical covariance of the received echo": "The 'Historical covariance of the received echo' in the table refers to the historical data of the received echo signals used for training a deep neural network (CAP-Net) to estimate Angle of Arrival (AoAs) in IRS-assisted ISAC systems.", "Input<~>Channel estimated by LS method": "The 'Channel estimated by LS method' in the table refers to a channel estimate obtained using the least squares (LS) method.", "Input<~>1.Received direct signals2.Total received signals and DE-CNN estimation": "The second input in the table for the reference [23432d7] refers to both received direct signals and total received signals used for DE-CNN estimation in DL-ISAC for channel estimation and IRS.", "Output<~>Output": "The 'Output' column in the table refers to the results or outcomes of the studies or techniques mentioned in each corresponding reference. In the given context, the output for reference [0] is AoAs, for reference [1] is channel estimation, and for reference [2] it is direct (DE-CNN) and reflected (RE-CNN) sensing.", "Output<~>AoAs": "The output column 'AoAs' in the table refers to Angle of Arrivals used for channel estimation and estimation of AoAs in IRS-assisted systems.", "Output<~>Channel estimation": "In the context of the table, 'Channel estimation' in the Output column refers to the estimation of wireless communication channels.", "Output<~>Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels": "The term \"Direct (DE-CNN) and reflected (RE-CNN) sensing and communication channels\" in the output column of the table refers to the estimation and processing of direct and reflected channels in multiple-input-single-output (MISO) IRS-assisted ISAC systems using convolutional neural networks (CNNs)."}}, "title": "Data-driven Integrated Sensing and Communication: Recent Advances, Challenges, and Future Prospects", "abstract": "Integrated Sensing and Communication (ISAC), combined with data-driven approaches, has emerged as a highly significant field, garnering considerable attention from academia and industry. Its potential to enable wide-scale applications in the future sixth-generation (6G) networks has led to extensive recent research efforts. Machine learning (ML) techniques, including $K$-nearest neighbors (KNN), support vector machines (SVM), deep learning (DL) architectures, and reinforcement learning (RL) algorithms, have been deployed to address various design aspects of ISAC and its diverse applications. Therefore, this paper aims to explore integrating various ML techniques into ISAC systems, covering various applications. These applications span intelligent vehicular networks, encompassing unmanned aerial vehicles (UAVs) and autonomous cars, as well as radar applications, localization and tracking, millimeter wave (mmWave) and Terahertz (THz) communication, and beamforming. The contributions of this paper lie in its comprehensive survey of ML-based works in the ISAC domain and its identification of challenges and future research directions. By synthesizing the existing knowledge and proposing new research avenues, this survey serves as a valuable resource for researchers, practitioners, and stakeholders involved in advancing the capabilities of ISAC systems in the context of 6G networks."}
{"paper_id": "2308.08794v1", "_pdf_hash": null, "_source_hash": "fe2c4f9f22d2f3ed014ea3e06431ac63d9a3677a", "_source_name": "2308.08794v1", "_table_hash": "488bdd5f-7ba5-4295-8895-aa0a0b67a1ab", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid6\" id-text=\"1\" rend=\"display\" starred=\"true\"><head>Comparison of methods. \u201cGenerality\u201d denotes applicability to arbitrary types of tipping points. \u201cPre-tip data\u201d denotes the whether the method can forecast tipping points using only data from the pre-tipping regime.</head>\n<tr bottom-border=\"true\"><td cols=\"1\" halign=\"center\"><hi rend=\"bold\">Method</hi></td>\n<td cols=\"1\" halign=\"center\"><hi rend=\"bold\">Generality</hi></td>\n<td cols=\"1\" halign=\"center\"><hi rend=\"bold\">Function space</hi></td>\n<td cols=\"1\" halign=\"center\"><hi rend=\"bold\">Partial physics</hi></td>\n<td cols=\"1\" halign=\"center\"><hi rend=\"bold\">Speed</hi></td>\n<td cols=\"1\" halign=\"center\"><hi rend=\"bold\">Pre-tip data</hi></td>\n</tr><tr><td halign=\"left\" right-border=\"true\"><rule depth=\"0.0pt\" height=\"10.0pt\" width=\"0.0pt\"/> Solver</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">N/A</td>\n</tr><tr><td halign=\"left\" right-border=\"true\"><rule depth=\"0.0pt\" height=\"10.0pt\" width=\"0.0pt\"/> EWS<cit sha=\"1726ba3804c746fc9337197f9041139ca00d6b38\"><ref target=\"bid6\"/></cit>{{cite:1726ba3}}</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">N/A</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2714</td>\n</tr><tr><td halign=\"left\" right-border=\"true\"><rule depth=\"0.0pt\" height=\"10.0pt\" width=\"0.0pt\"/> Bury et al.<cit sha=\"e8009c84d1b4aea496ab1acb5baf4f5466c74a53\"><ref target=\"bid7\"/></cit>{{cite:e8009c8}}</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">N/A</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2718</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\" right-border=\"true\"><rule depth=\"0.0pt\" height=\"10.0pt\" width=\"0.0pt\"/> Patel and Ott<cit sha=\"27df2d506909e650987136ce52e3043bd9636a07\"><ref target=\"bid3\"/></cit>{{cite:27df2d5}}</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2718</td>\n<td halign=\"center\">N/A</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2718</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\" right-border=\"true\"><rule depth=\"0.0pt\" height=\"10.0pt\" width=\"0.0pt\"/> <hi rend=\"bold\">Ours</hi></td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2714</td>\n<td halign=\"center\">\u2714</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Method", "Generality", "Function space", "Partial physics", "Speed", "Pre-tip data"], [" Solver", "\u2714", "\u2714", "\u2718", "\u2718", "N/A"], [" EWS{{cite:1726ba3}}", "\u2718", "\u2718", "N/A", "\u2714", "\u2714"], [" Bury et al.{{cite:e8009c8}}", "\u2718", "\u2718", "N/A", "\u2714", "\u2718"], [" Patel and Ott{{cite:27df2d5}}", "\u2714", "\u2718", "N/A", "\u2714", "\u2718"], [" Ours", "\u2714", "\u2714", "\u2714", "\u2714", "\u2714"]], "table_dict": {"References": ["{{cite:1726ba3}}", "{{cite:e8009c8}}", "{{cite:27df2d5}}", "-"], "Method": ["EWS ", "Bury et al. ", "Patel and Ott ", "Ours"], "Generality": ["\u2718", "\u2718", "\u2714", "\u2714"], "Function space": ["\u2718", "\u2718", "\u2718", "\u2714"], "Partial physics": ["-", "-", "-", "\u2714"], "Speed": ["\u2714", "\u2714", "\u2714", "\u2714"], "Pre-tip data": ["\u2714", "\u2718", "\u2718", "\u2714"]}}, "bib_hash": ["1726ba3804c746fc9337197f9041139ca00d6b38", "e8009c84d1b4aea496ab1acb5baf4f5466c74a53", "27df2d506909e650987136ce52e3043bd9636a07"], "row_bib_map": [{"bib_hash_or_arxiv_id": "1726ba3804c746fc9337197f9041139ca00d6b38", "row": 0, "corpus_id": 4001553, "type": "ref"}, {"bib_hash_or_arxiv_id": "e8009c84d1b4aea496ab1acb5baf4f5466c74a53", "row": 1, "corpus_id": 237583501, "type": "ref"}, {"bib_hash_or_arxiv_id": "27df2d506909e650987136ce52e3043bd9636a07", "row": 2, "corpus_id": 250243662, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.08794v1", "row": 3, "corpus_id": 261030871, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid6\" starred=\"true\"><head>Comparison of methods. \u201cGenerality\u201d denotes applicability to arbitrary types of tipping points. \u201cPre-tip data\u201d denotes the whether the method can forecast tipping points using only data from the pre-tipping regime.</head>\n<row bottom-border=\"true\"><cell halign=\"center\" cols=\"1\"><hi rend=\"bold\">Method</hi></cell>\n<cell halign=\"center\" cols=\"1\"><hi rend=\"bold\">Generality</hi></cell>\n<cell halign=\"center\" cols=\"1\"><hi rend=\"bold\">Function space</hi></cell>\n<cell halign=\"center\" cols=\"1\"><hi rend=\"bold\">Partial physics</hi></cell>\n<cell halign=\"center\" cols=\"1\"><hi rend=\"bold\">Speed</hi></cell>\n<cell halign=\"center\" cols=\"1\"><hi rend=\"bold\">Pre-tip data</hi></cell>\n</row><row><cell right-border=\"true\" halign=\"left\"><rule width=\"0.0pt\" depth=\"0.0pt\" height=\"10.0pt\"/> Solver</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">N/A</cell>\n</row><row><cell right-border=\"true\" halign=\"left\"><rule width=\"0.0pt\" depth=\"0.0pt\" height=\"10.0pt\"/> EWS\u00a0<cit sha=\"1726ba3804c746fc9337197f9041139ca00d6b38\"><ref target=\"bid6\"/></cit>{{cite:1726ba3}}</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">N/A</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2714</cell>\n</row><row><cell right-border=\"true\" halign=\"left\"><rule width=\"0.0pt\" depth=\"0.0pt\" height=\"10.0pt\"/> Bury et al.\u00a0<cit sha=\"e8009c84d1b4aea496ab1acb5baf4f5466c74a53\"><ref target=\"bid7\"/></cit>{{cite:e8009c8}}</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">N/A</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2718</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"left\"><rule width=\"0.0pt\" depth=\"0.0pt\" height=\"10.0pt\"/> Patel and Ott\u00a0<cit sha=\"27df2d506909e650987136ce52e3043bd9636a07\"><ref target=\"bid3\"/></cit>{{cite:27df2d5}}</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2718</cell>\n<cell halign=\"center\">N/A</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2718</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"left\"><rule width=\"0.0pt\" depth=\"0.0pt\" height=\"10.0pt\"/> <hi rend=\"bold\">Ours</hi></cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2714</cell>\n<cell halign=\"center\">\u2714</cell>\n</row></table>", "caption": "Comparison of methods. \u201cGenerality\u201d denotes applicability to arbitrary types of tipping points. \u201cPre-tip data\u201d denotes the whether the method can forecast tipping points using only data from the pre-tipping regime.", "type": "table"}, "_table_hash_full_text": "bf085bc8-6c28-4027-b582-d5dfef720bfc", "_full_text_table_hash": "bdb02ddc-f6e2-4652-b298-deb8ecca3c20", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Method<~>Method": "The 'Method' column in the table refers to the name of the specific tipping point forecasting method mentioned in the corresponding literature reference.", "Method<~>EWS": "EWS in the table refers to Early Warning Signals, which is a method for forecasting tipping points based on the analysis of time series data to identify anomalous behavior preceding a tipping point.", "Method<~>Bury et al.": "Bury et al. refers to a specific group of researchers whose method for tipping point forecasting is mentioned in the reference [[e8009c8]]. The specifics of their method are not explicitly mentioned in the provided text.", "Method<~>Patel and Ott": "Patel and Ott in the context of the table refer to the authors of a method using reservoir computing mentioned in the text.", "Method<~>Ours": "In the given table, 'Ours' in the column 'Method' refers to the Recurrent Neural Operators (RNO) proposed by the authors in the scientific paper.", "Generality<~>Generality": "In the context of the table, 'Generality' refers to the applicability of different methods to arbitrary types of tipping points.", "Function space<~>Function space": "In the context of the given table, a function space refers to a mathematical space of functions that are used as inputs and outputs in variousmachine learning models, such as neural operators, in the study of tipping points in non-stationary dynamical systems.", "Partial physics<~>Partial physics": "'Partial physics' in the context of the table likely refers to the use of only some of the physics constraints or equations in the tipping point forecasting method.", "Speed<~>Speed": "In the given context, 'Speed' in the table likely refers to the computational or efficiency performance of the methods being compared.", "Pre-tip data<~>Pre-tip data": "'Pre-tip data' refers to the historical data from the pre-tipping regime used for forecasting tipping points in non-stationary dynamical systems."}}, "title": "Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces", "abstract": "Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points."}
{"paper_id": "2308.03149v1", "_pdf_hash": null, "_source_hash": "4ab2678028f58fc39fb12f14ef90063aa0319266", "_source_name": "2308.03149v1", "_table_hash": "8b1d0c59-2fa6-45b3-9b45-291056ee3725", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid7\" id-text=\"3\" place=\"h\" rend=\"display\" starred=\"true\"><head>Comparison of the customized hardware.</head>\n<tr bottom-border=\"true\"><td halign=\"left\">Reference</td>\n<td halign=\"center\">[c]Openmili <cit sha=\"1bc42af9f7fa0b06d7337e2b587355ab7c7ab4a3\"><ref target=\"bid15\"/></cit>{{cite:1bc42af}}</td>\n<td halign=\"center\">[c]<formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>M</mi> <mn>3</mn> </msup></math></formula> <cit sha=\"ac630111f02d70e760aa2267fdb6eabe2c445d83\"><ref target=\"bid16\"/></cit>{{cite:ac63011}}</td>\n<td halign=\"center\">[c]mm-Flex <cit sha=\"44af41ee3696f751ec455ec321049f38ba9abfb1\"><ref target=\"bid17\"/></cit>{{cite:44af41e}}</td>\n<td halign=\"center\">[c]<cit sha=\"63ae6d6c9b2db77b3262eeedf0b763343d0e895c\"><ref target=\"bid18\"/></cit>{{cite:63ae6d6}}</td>\n<td halign=\"center\">[c]<hi rend=\"it\">Soli</hi> <cit sha=\"5403921ab617a102f6d69ab16f58e65a96b053a5\"><ref target=\"bid1\"/></cit>{{cite:5403921}}</td>\n</tr><tr><td halign=\"left\">[l]Image</td>\n<td halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure extension=\"jpeg\" file=\"figure/openmilli\" rend=\"inline\" width=\"427.0pt\"/>{{figure:04598422-3d97-4990-a967-2d1c9db57129}}</raisebox>\n</p></minipage></td>\n<td halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure extension=\"jpg\" file=\"figure/m3\" rend=\"inline\" width=\"427.0pt\"/>{{figure:ec708beb-f065-4042-8fcd-b7e25d5ec901}}</raisebox>\n</p></minipage></td>\n<td halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure extension=\"jpeg\" file=\"figure/mmflex\" rend=\"inline\" width=\"427.0pt\"/>{{figure:5e0c00aa-c92d-4c5b-a928-206dc3fa035a}}</raisebox>\n</p></minipage></td>\n<td halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure extension=\"jpg\" file=\"figure/eeye\" rend=\"inline\" width=\"427.0pt\"/>{{figure:20b0f68d-a0fd-4c92-9db8-20e246449b43}}</raisebox>\n</p></minipage></td>\n<td halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure extension=\"jpeg\" file=\"figure/soli\" rend=\"inline\" width=\"427.0pt\"/>{{figure:066e6d10-79d9-46dd-8762-abf19e0ecfcb}}</raisebox>\n</p></minipage></td>\n</tr><tr><td halign=\"left\">Baseband BW</td>\n<td halign=\"center\">1 GHz</td>\n<td halign=\"center\">4 GHz</td>\n<td halign=\"center\">2 GHz</td>\n<td halign=\"center\">450 MHz</td>\n<td halign=\"center\">7 GHz</td>\n</tr><tr><td halign=\"left\">Carries Freq.</td>\n<td halign=\"center\">57 <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 64 GHz</td>\n<td halign=\"center\">60 GHz</td>\n<td halign=\"center\">60 GHz</td>\n<td halign=\"center\">24 GHz</td>\n<td halign=\"center\">60 GHz</td>\n</tr><tr><td halign=\"left\">Antenna</td>\n<td halign=\"center\">Horn/Phased-array</td>\n<td halign=\"center\">Phased-array</td>\n<td halign=\"center\">Phased-antenna</td>\n<td halign=\"center\">Array</td>\n<td halign=\"center\">Array</td>\n</tr><tr><td halign=\"left\">Cost</td>\n<td halign=\"center\">$15K</td>\n<td halign=\"center\">below $15K</td>\n<td halign=\"center\">$40K</td>\n<td halign=\"center\">below $100</td>\n<td halign=\"center\">-</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Reference", "[c]Openmili {{cite:1bc42af}}", "[c]M 3  {{cite:ac63011}}", "[c]mm-Flex {{cite:44af41e}}", "[c]{{cite:63ae6d6}}", "[c]Soli {{cite:5403921}}"], ["[l]Image", "{{figure:04598422-3d97-4990-a967-2d1c9db57129}}\n", "{{figure:ec708beb-f065-4042-8fcd-b7e25d5ec901}}\n", "{{figure:5e0c00aa-c92d-4c5b-a928-206dc3fa035a}}\n", "{{figure:20b0f68d-a0fd-4c92-9db8-20e246449b43}}\n", "{{figure:066e6d10-79d9-46dd-8762-abf19e0ecfcb}}\n"], ["Baseband BW", "1 GHz", "4 GHz", "2 GHz", "450 MHz", "7 GHz"], ["Carries Freq.", "57 \u223c 64 GHz", "60 GHz", "60 GHz", "24 GHz", "60 GHz"], ["Antenna", "Horn/Phased-array", "Phased-array", "Phased-antenna", "Array", "Array"], ["Cost", "$15K", "below $15K", "$40K", "below $100", "-"]], "table_dict": {"References": ["{{cite:1bc42af}}", "{{cite:ac63011}}", "{{cite:44af41e}}", "{{cite:63ae6d6}}", "{{cite:5403921}}"], "Reference": ["[c]Openmili ", "[c]M 3  ", "[c]mm-Flex ", "[c]", "[c]Soli "], "Baseband BW": ["1 GHz", "4 GHz", "2 GHz", "450 MHz", "7 GHz"], "Carries Freq.": ["57 \u223c 64 GHz", "60 GHz", "60 GHz", "24 GHz", "60 GHz"], "Antenna": ["Horn/Phased-array", "Phased-array", "Phased-antenna", "Array", "Array"], "Cost": ["$15K", "below $15K", "$40K", "below $100", "-"]}}, "bib_hash": ["1bc42af9f7fa0b06d7337e2b587355ab7c7ab4a3", "ac630111f02d70e760aa2267fdb6eabe2c445d83", "44af41ee3696f751ec455ec321049f38ba9abfb1", "63ae6d6c9b2db77b3262eeedf0b763343d0e895c", "5403921ab617a102f6d69ab16f58e65a96b053a5"], "row_bib_map": [{"bib_hash_or_arxiv_id": "1bc42af9f7fa0b06d7337e2b587355ab7c7ab4a3", "row": 0, "corpus_id": 16944951, "type": "ref"}, {"bib_hash_or_arxiv_id": "ac630111f02d70e760aa2267fdb6eabe2c445d83", "row": 1, "corpus_id": 211198441, "type": "ref"}, {"bib_hash_or_arxiv_id": "44af41ee3696f751ec455ec321049f38ba9abfb1", "row": 2, "corpus_id": 218569273, "type": "ref"}, {"bib_hash_or_arxiv_id": "63ae6d6c9b2db77b3262eeedf0b763343d0e895c", "row": 3, "corpus_id": 53095465, "type": "ref"}, {"bib_hash_or_arxiv_id": "5403921ab617a102f6d69ab16f58e65a96b053a5", "row": 4, "corpus_id": 6601027, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"3\" id=\"uid7\" starred=\"true\" place=\"h\"><head>Comparison of the customized hardware.</head>\n<row bottom-border=\"true\"><cell halign=\"left\">Reference</cell>\n<cell halign=\"center\">[c]Openmili <cit sha=\"1bc42af9f7fa0b06d7337e2b587355ab7c7ab4a3\"><ref target=\"bid15\"/></cit>{{cite:1bc42af}}</cell>\n<cell halign=\"center\">[c]<formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>M</mi> <mn>3</mn> </msup></math><texmath>M^3</texmath></formula> <cit sha=\"ac630111f02d70e760aa2267fdb6eabe2c445d83\"><ref target=\"bid16\"/></cit>{{cite:ac63011}}</cell>\n<cell halign=\"center\">[c]mm-Flex <cit sha=\"44af41ee3696f751ec455ec321049f38ba9abfb1\"><ref target=\"bid17\"/></cit>{{cite:44af41e}}</cell>\n<cell halign=\"center\">[c]<cit sha=\"63ae6d6c9b2db77b3262eeedf0b763343d0e895c\"><ref target=\"bid18\"/></cit>{{cite:63ae6d6}}</cell>\n<cell halign=\"center\">[c]<hi rend=\"it\">Soli</hi> <cit sha=\"5403921ab617a102f6d69ab16f58e65a96b053a5\"><ref target=\"bid1\"/></cit>{{cite:5403921}}</cell>\n</row><row><cell halign=\"left\">[l]Image</cell>\n<cell halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure rend=\"inline\" width=\"427.0pt\" file=\"figure/openmilli\" extension=\"jpeg\"/>{{figure:04598422-3d97-4990-a967-2d1c9db57129}}</raisebox>\n</p></minipage></cell>\n<cell halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure rend=\"inline\" width=\"427.0pt\" file=\"figure/m3\" extension=\"jpg\"/>{{figure:ec708beb-f065-4042-8fcd-b7e25d5ec901}}</raisebox>\n</p></minipage></cell>\n<cell halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure rend=\"inline\" width=\"427.0pt\" file=\"figure/mmflex\" extension=\"jpeg\"/>{{figure:5e0c00aa-c92d-4c5b-a928-206dc3fa035a}}</raisebox>\n</p></minipage></cell>\n<cell halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure rend=\"inline\" width=\"427.0pt\" file=\"figure/eeye\" extension=\"jpg\"/>{{figure:20b0f68d-a0fd-4c92-9db8-20e246449b43}}</raisebox>\n</p></minipage></cell>\n<cell halign=\"center\"><minipage pos=\"b\" width=\"128.1013pt\"><p rend=\"center\"><raisebox val=\"-.5\"><figure rend=\"inline\" width=\"427.0pt\" file=\"figure/soli\" extension=\"jpeg\"/>{{figure:066e6d10-79d9-46dd-8762-abf19e0ecfcb}}</raisebox>\n</p></minipage></cell>\n</row><row><cell halign=\"left\">Baseband BW</cell>\n<cell halign=\"center\">1 GHz</cell>\n<cell halign=\"center\">4 GHz</cell>\n<cell halign=\"center\">2 GHz</cell>\n<cell halign=\"center\">450 MHz</cell>\n<cell halign=\"center\">7 GHz</cell>\n</row><row><cell halign=\"left\">Carries Freq.</cell>\n<cell halign=\"center\">57 <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 64 GHz</cell>\n<cell halign=\"center\">60 GHz</cell>\n<cell halign=\"center\">60 GHz</cell>\n<cell halign=\"center\">24 GHz</cell>\n<cell halign=\"center\">60 GHz</cell>\n</row><row><cell halign=\"left\">Antenna</cell>\n<cell halign=\"center\">Horn/Phased-array</cell>\n<cell halign=\"center\">Phased-array</cell>\n<cell halign=\"center\">Phased-antenna</cell>\n<cell halign=\"center\">Array</cell>\n<cell halign=\"center\">Array</cell>\n</row><row><cell halign=\"left\">Cost</cell>\n<cell halign=\"center\">$15K</cell>\n<cell halign=\"center\">below $15K</cell>\n<cell halign=\"center\">$40K</cell>\n<cell halign=\"center\">below $100</cell>\n<cell halign=\"center\">-</cell>\n</row></table>", "caption": "Comparison of the customized hardware.", "type": "table"}, "_table_hash_full_text": "91839cdc-5304-4b3e-b2f2-ab342baa3181", "_full_text_table_hash": "0efac130-e8ec-4bc3-bac3-932eca9420ba", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Reference<~>Reference": "In the context of the table, 'References' refers to citations providing detailed information about the sources of the data or research mentioned in each row.", "Reference<~>[c]Openmili": "'Openmili' in the 'References' column refers to an Open-source millimeter-wave computing platform mentioned in the source {{cite:1bc42af}}.", "Reference<~>[c]M 3": "The '[c]M 3' in the 'Reference' column of the table likely refers to the third reference in a list of multiple references with the same prefix '[c]'. However, without more context, it is unclear what 'M 3' specifically signifies. It could be a code or label used by the table's creators or an abbreviation for a project or product name. Therefore, a definitive answer requires further information.", "Reference<~>[c]mm-Flex": "The term '[c]mm-Flex' in the 'Reference' column of the table refers to a specific hardware platform or system named 'mm-Flex'.", "Reference<~>[c]": "In the context of the table, '[c]' in the 'Reference' column refers to the citation key for the corresponding reference entry.", "Reference<~>[c]Soli": "The letter 'S' in the brackets '[c]Soli' in the 'References' column refers to a specific system or technology named 'Soli'.", "Baseband BW<~>Baseband BW": "In the context of the table, 'Baseband BW' refers to the bandwidth of the baseband signal processed by the digital components in a mmWave radar system.", "Carries Freq.<~>Carries Freq.": "'Carries Freq.' in the table refers to the frequency range of the mmWave signals used in each of the customized hardware referred to in the citations.", "Antenna<~>Antenna": "In the context of the table, 'Antenna' refers to various types of radio transmitting and receiving components, specifically horn, phased-array, phased-antenna, and arrays.", "Antenna<~>Horn/Phased-array": "'Horn/Phased-array' in the column 'Antenna' refers to a type of antenna array that uses a horn antenna in combination with phased-array technology.", "Antenna<~>Phased-array": "In the context of the table, 'Phased-array' in the column 'Antenna' refers to an array of antennas that can be electronically steered to point in different directions.", "Antenna<~>Phased-antenna": "'Phased-antenna' in the column 'Antenna' refers to an antenna system where signals are transmitted or received from multiple antennas in a synchronized manner, creating a beam that can be steered electronically to cover different angles.", "Antenna<~>Array": "In the context of the table, 'Array' in the column 'Antenna' refers to an arrangement of multiple antenna elements.", "Cost<~>Cost": "The 'Cost' column in the table refers to the price or cost of each customized hardware solution mentioned in the corresponding reference."}}, "title": "A Survey of mmWave-Based Human Sensing: Technology, Platforms and Applications", "abstract": "With the rapid development of the Internet of Things (IoT) and the rise of 5G communication networks and automatic driving, millimeter wave (mmWave) sensing is emerging and starts impacting our life and workspace. mmWave sensing can sense humans and objects in a contactless way, providing fine-grained sensing ability. In the past few years, many mmWave sensing techniques have been proposed and applied in various human sensing applications (e.g., human localization, gesture recognition, and vital monitoring). We discover the need of a comprehensive survey to summarize the technology, platforms and applications of mmWave-based human sensing. In this survey, we first present the mmWave hardware platforms and some key techniques of mmWave sensing. We then provide a comprehensive review of existing mmWave-based human sensing works. Specifically, we divide existing works into four categories according to the sensing granularity: human tracking and localization, motion recognition, biometric measurement and human imaging. Finally, we discuss the potential research challenges and present future directions in this area."}
{"paper_id": "2308.13266v2", "_pdf_hash": null, "_source_hash": "d6ab17c31d99ae6ef9ab06124384a760873d1ebe", "_source_name": "2308.13266v2", "_table_hash": "20317c3b-01c9-4813-9868-1b7139fc7c74", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr><td halign=\"left\" right-border=\"true\"/>\n<td cols=\"2\" halign=\"center\" right-border=\"true\">Initialization</td>\n<td cols=\"2\" halign=\"center\" right-border=\"true\">Prediction</td>\n<td cols=\"1\" halign=\"left\" right-border=\"true\"/>\n<td cols=\"1\" halign=\"left\"/>\n</tr><tr><td halign=\"left\" right-border=\"true\">Method</td>\n<td halign=\"center\" right-border=\"true\">Box</td>\n<td halign=\"center\">Mask</td>\n<td halign=\"center\" right-border=\"true\">Box</td>\n<td halign=\"center\" right-border=\"true\">Mask</td>\n<td>Extra Model</td>\n<td>Multi-Object</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">SiamMask <cit sha=\"ad732b30f6e466b3ff8cc7822f4b4831d9fdfd76\"><ref target=\"bid12\"/></cit>{{cite:ad732b3}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td>-</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">D3S <cit sha=\"3b41d17a9de8ee1855a3d193284e0d12529113c1\"><ref target=\"bid15\"/></cit>{{cite:3b41d17}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td>-</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">SiamR-CNN <cit sha=\"29eaa8ddf1cfabfc2d5952d4d19c36f1492d60b7\"><ref target=\"bid13\"/></cit>{{cite:29eaa8d}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td>Box2Seg <cit sha=\"4dd302477b2bb5f7d174b62f489e91ac908b5b84\"><ref target=\"bid9\"/></cit>{{cite:4dd3024}}</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">UniTrack <cit sha=\"09dbc3f21d890666ff1a1afa19c07dd4f762b9c6\"><ref target=\"bid16\"/></cit>{{cite:09dbc3f}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td>-</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">Unicorn <cit sha=\"76547ee06c85bceeb3e26a98760656f5fb7f5a6c\"><ref target=\"bid17\"/></cit>{{cite:76547ee}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td>-</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">RTS <cit sha=\"7abfa59a0fd51ad2a180eb812207ac9b75835c8b\"><ref target=\"bid14\"/></cit>{{cite:7abfa59}}</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td>STA <cit sha=\"9e8cd2a28bd153d103f21d5ffdb65765d0259253\"><ref target=\"bid11\"/></cit>{{cite:9e8cd2a}}</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">MITS (Ours)</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td>-</td>\n<td>\u2713</td>\n</tr></table>", "table_json": {"incomplete_rows": [{"row_idx": 0, "text": "", "cols": 1}, {"row_idx": 0, "text": "Initialization", "cols": 2}, {"row_idx": 0, "text": "Prediction", "cols": 2}, {"row_idx": 0, "text": "", "cols": 1}, {"row_idx": 0, "text": "", "cols": 1}], "table": [["-Method", "Initialization-Box", "Initialization-Mask", "Prediction-Box", "Prediction-Mask", "-Extra Model", "-Multi-Object"], ["SiamMask {{cite:ad732b3}}", "\u2713", "\u2717", "\u2713", "\u2713", "-", "\u2717"], ["D3S {{cite:3b41d17}}", "\u2713", "\u2713", "\u2717", "\u2713", "-", "\u2717"], ["SiamR-CNN {{cite:29eaa8d}}", "\u2713", "\u2717", "\u2713", "\u2717", "Box2Seg {{cite:4dd3024}}", "\u2717"], ["UniTrack {{cite:09dbc3f}}", "\u2713", "\u2713", "\u2713", "\u2713", "-", "\u2717"], ["Unicorn {{cite:76547ee}}", "\u2713", "\u2713", "\u2713", "\u2713", "-", "\u2717"], ["RTS {{cite:7abfa59}}", "\u2717", "\u2713", "\u2717", "\u2713", "STA {{cite:9e8cd2a}}", "\u2717"], ["MITS (Ours)", "\u2713", "\u2713", "\u2713", "\u2713", "-", "\u2713"]], "table_dict": {"References": ["{{cite:ad732b3}}", "{{cite:3b41d17}}", "{{cite:29eaa8d}}", "{{cite:09dbc3f}}", "{{cite:76547ee}}", "{{cite:7abfa59}}", "-"], "Method": ["SiamMask ", "D3S ", "SiamR-CNN ", "UniTrack ", "Unicorn ", "RTS ", "MITS (Ours)"], "Initialization-Box": ["\u2713", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2713"], "Initialization-Mask": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2713", "\u2713", "\u2713"], "Prediction-Box": ["\u2713", "\u2717", "\u2713", "\u2713", "\u2713", "\u2717", "\u2713"], "Prediction-Mask": ["\u2713", "\u2713", "\u2717", "\u2713", "\u2713", "\u2713", "\u2713"], "Extra Model": ["-", "-", "Box2Seg {{cite:4dd3024}}", "-", "-", "STA {{cite:9e8cd2a}}", "-"], "Multi-Object": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"]}}, "bib_hash": ["ad732b30f6e466b3ff8cc7822f4b4831d9fdfd76", "3b41d17a9de8ee1855a3d193284e0d12529113c1", "29eaa8ddf1cfabfc2d5952d4d19c36f1492d60b7", "4dd302477b2bb5f7d174b62f489e91ac908b5b84", "09dbc3f21d890666ff1a1afa19c07dd4f762b9c6", "76547ee06c85bceeb3e26a98760656f5fb7f5a6c", "7abfa59a0fd51ad2a180eb812207ac9b75835c8b", "9e8cd2a28bd153d103f21d5ffdb65765d0259253"], "row_bib_map": [{"bib_hash_or_arxiv_id": "ad732b30f6e466b3ff8cc7822f4b4831d9fdfd76", "row": 0, "corpus_id": 54475412, "type": "ref"}, {"bib_hash_or_arxiv_id": "3b41d17a9de8ee1855a3d193284e0d12529113c1", "row": 1, "corpus_id": 208175650, "type": "ref"}, {"bib_hash_or_arxiv_id": "29eaa8ddf1cfabfc2d5952d4d19c36f1492d60b7", "row": 2, "corpus_id": 208512936, "type": "ref"}, {"bib_hash_or_arxiv_id": "09dbc3f21d890666ff1a1afa19c07dd4f762b9c6", "row": 3, "corpus_id": 235732286, "type": "ref"}, {"bib_hash_or_arxiv_id": "76547ee06c85bceeb3e26a98760656f5fb7f5a6c", "row": 4, "corpus_id": 250526428, "type": "ref"}, {"bib_hash_or_arxiv_id": "7abfa59a0fd51ad2a180eb812207ac9b75835c8b", "row": 5, "corpus_id": 247593925, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.13266v2", "row": 6, "corpus_id": 261214723, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"left\"/>\n<cell right-border=\"true\" halign=\"center\" cols=\"2\">Initialization</cell>\n<cell right-border=\"true\" halign=\"center\" cols=\"2\">Prediction</cell>\n<cell right-border=\"true\" halign=\"left\" cols=\"1\"/>\n<cell halign=\"left\" cols=\"1\"/>\n</row><row><cell right-border=\"true\" halign=\"left\">Method</cell>\n<cell right-border=\"true\" halign=\"center\">Box</cell>\n<cell halign=\"center\">Mask</cell>\n<cell right-border=\"true\" halign=\"center\">Box</cell>\n<cell right-border=\"true\" halign=\"center\">Mask</cell>\n<cell>Extra Model</cell>\n<cell>Multi-Object</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">SiamMask <cit sha=\"ad732b30f6e466b3ff8cc7822f4b4831d9fdfd76\"><ref target=\"bid12\"/></cit>{{cite:ad732b3}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell>-</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">D3S <cit sha=\"3b41d17a9de8ee1855a3d193284e0d12529113c1\"><ref target=\"bid15\"/></cit>{{cite:3b41d17}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell>-</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">SiamR-CNN <cit sha=\"29eaa8ddf1cfabfc2d5952d4d19c36f1492d60b7\"><ref target=\"bid13\"/></cit>{{cite:29eaa8d}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell>Box2Seg <cit sha=\"4dd302477b2bb5f7d174b62f489e91ac908b5b84\"><ref target=\"bid9\"/></cit>{{cite:4dd3024}}</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">UniTrack <cit sha=\"09dbc3f21d890666ff1a1afa19c07dd4f762b9c6\"><ref target=\"bid16\"/></cit>{{cite:09dbc3f}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell>-</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">Unicorn <cit sha=\"76547ee06c85bceeb3e26a98760656f5fb7f5a6c\"><ref target=\"bid17\"/></cit>{{cite:76547ee}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell>-</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">RTS <cit sha=\"7abfa59a0fd51ad2a180eb812207ac9b75835c8b\"><ref target=\"bid14\"/></cit>{{cite:7abfa59}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell>STA <cit sha=\"9e8cd2a28bd153d103f21d5ffdb65765d0259253\"><ref target=\"bid11\"/></cit>{{cite:9e8cd2a}}</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">MITS (Ours)</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell>-</cell>\n<cell>\u2713</cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "d0e58d4f-e4c7-43dc-adb6-0f0526bd3b35", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Method<~>Method": "In the context of the table, 'Method' refers to the name of the specific tracking and segmentation algorithms used in each corresponding reference.", "Method<~>SiamMask": "SiamMask is a multi-object tracking and segmentation method referred to in the table.", "Method<~>D3S": "D3S in the table refers to the method \"Dynamic Regression for Discriminative Object Detection and Segmentation.\"", "Method<~>SiamR-CNN": "SiamR-CNN in the table refers to a specific method named Siam R-CNN used in the Multi-Object Tracking and Segmentation framework discussed in the text.", "Method<~>UniTrack": "UniTrack in the context of the table refers to a multi-object tracking and segmentation method mentioned in the scientific paper.", "Method<~>Unicorn": "'Unicorn' in the 'Method' column refers to the Unicorn method mentioned in the text as one of the methods that unified MOT/MOTS and VOT/VOS, but the table does not provide further context or information about the specific implementation of the Unicorn method used in the comparisons with MITS.", "Method<~>RTS": "RTS in the table refers to Real-time Multi-Object Segmentation proposed in the paper with citation {{cite:7abfa59}}.", "Method<~>MITS (Ours)": "MITS (Ours) refers to the Multi-object Mask-box Integrated framework for unified Tracking and Segmentation proposed in the given scientific paper.", "Initialization-Box<~>Initialization-Box": "In the context of the table, 'Initialization-Box' refers to the use of boxes as the initial reference for object tracking and segmentation in the Multi-Object Mask-box Integrated framework for unified Tracking and Segmentation (MITS).", "Initialization-Mask<~>Initialization-Mask": "\"Initialization-Mask\" in the context of the table refers to the use of masks for object initialization in Multi-Object Tracking and Segmentation tasks.", "Prediction-Box<~>Prediction-Box": "In the context of the table, 'Prediction-Box' refers to the predicted bounding boxes produced by the MITS framework for object tracking and segmentation.", "Prediction-Mask<~>Prediction-Mask": "In the context of the given table, 'Prediction-Mask' refers to a mask generated for object prediction in the Multi-Object Tracking and Segmentation (MOTS) or Video Object Segmentation (VOS) task as indicated by the checkmarks (\u2713) in the corresponding column.", "Extra Model<~>Extra Model": "The 'Extra Model' column in the table refers to additional models used in specific references for multi-object tracking and segmentation tasks. In the context of this table, the Extra Model for Box2Seg and STA are mentioned in reference 2 and 5 respectively.", "Extra Model<~>Box2Seg {{cite:4dd3024}}": "Box2Seg {{cite:4dd3024}} is a model used for segmenting objects based on box predictions.", "Extra Model<~>STA {{cite:9e8cd2a}}": "STA {{cite:9e8cd2a}} in the context of the table refers to a specific model or method named STA mentioned in {{cite:7abfa59}} under the 'Extra Model' column.", "Multi-Object<~>Multi-Object": "'Multi-Object' in the context of the table refers to the row where the 'References' column is empty and the 'Multi-Object' column is marked with a checkmark, indicating that the reference for this row is a multi-object related study."}}, "title": "Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation", "abstract": "Tracking any given object(s) spatially and temporally is a common purpose in Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint tracking and segmentation have been attempted in some studies but they often lack full compatibility of both box and mask in initialization and prediction, and mainly focus on single-object scenarios. To address these limitations, this paper proposes a Multi-object Mask-box Integrated framework for unified Tracking and Segmentation, dubbed MITS. Firstly, the unified identification module is proposed to support both box and mask reference for initialization, where detailed object information is inferred from boxes or directly retained from masks. Additionally, a novel pinpoint box predictor is proposed for accurate multi-object box prediction, facilitating target-oriented representation learning. All target objects are processed simultaneously from encoding to propagation and decoding, as a unified pipeline for VOT and VOS. Experimental results show MITS achieves state-of-the-art performance on both VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor by around 6% on the GOT-10k test set, and significantly improves the performance of box initialization on VOS benchmarks. The code is available at https://github.com/yoxu515/MITS."}
{"paper_id": "2308.08643v1", "_pdf_hash": null, "_source_hash": "4dfeb58c4d82db95fc5a5010de253ad83bc17a86", "_source_name": "2308.08643v1", "_table_hash": "a9d669ae-0620-464b-addf-d5c677d93c1e", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid8\" id-text=\"1\" place=\"t\" rend=\"display\" starred=\"true\"><head>A comparison between existing heterogeneous model cooperation works and our <hi rend=\"tt\">pFedHR</hi>.</head>\n<unexpected><resizebox width=\"341.6013pt\">\n<table rend=\"inline\"><tr><td halign=\"left\" right-border=\"true\">2*<hi rend=\"bold\">Approach</hi></td>\n<td cols=\"2\" halign=\"center\" right-border=\"true\"><hi rend=\"bold\">Public Dataset</hi></td>\n<td cols=\"3\" halign=\"center\"><hi rend=\"bold\">Model Characteristics</hi></td>\n</tr><tr><td halign=\"left\" right-border=\"true\">(lr)2-6</td>\n<td halign=\"center\" right-border=\"true\">W. Label</td>\n<td halign=\"center\">W.o. Label</td>\n<td halign=\"center\">Upload and Download</td>\n<td halign=\"center\">Aggregation</td>\n<td>Personalization</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">gray!20\nFedDF<cit sha=\"bc634cbc76983c021c5490753811d0862a21922c\"><ref target=\"bid15\"/></cit>{{cite:bc634cb}}</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">parameters</td>\n<td halign=\"center\">ensemble distillation</td>\n<td>\u2717</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">FedKEMF<cit sha=\"c51cb7c10dcae77792ad766ac99249a11c0d9da0\"><ref target=\"bid16\"/></cit>{{cite:c51cb7c}}</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">parameters</td>\n<td halign=\"center\">mutual learning</td>\n<td>\u2713</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">FCCL <cit sha=\"05dca6492de9192a01751bac89194ffdbbf82fff\"><ref target=\"bid14\"/></cit>{{cite:05dca64}}</td>\n<td halign=\"center\" right-border=\"true\">\u2717</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">logits</td>\n<td halign=\"center\">average</td>\n<td>\u2713</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">FedMD<cit sha=\"d9b8cb082e0970472eb3f9cbe819b094a21532cf\"><ref target=\"bid13\"/></cit>{{cite:d9b8cb0}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">class scores</td>\n<td halign=\"center\">average</td>\n<td>\u2713</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">FedGH <cit sha=\"cdf1d964c4bf1ecc1eca7f0fa1e5ff137c84f9e9\"><ref target=\"bid18\"/></cit>{{cite:cdf1d96}}</td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">label-wise representations</td>\n<td halign=\"center\">average</td>\n<td>\u2713</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">red!20\n<hi rend=\"tt\">pFedHR</hi></td>\n<td halign=\"center\" right-border=\"true\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">parameters</td>\n<td halign=\"center\">model reassembly</td>\n<td>\u2713</td>\n</tr></table>\n</resizebox></unexpected></table>", "table_json": {"incomplete_rows": [{"row_idx": 0, "text": "2*Approach", "cols": 1}, {"row_idx": 0, "text": "Public Dataset", "cols": 2}, {"row_idx": 0, "text": "Model Characteristics", "cols": 3}], "table": [["2*Approach-(lr)2-6", "Public Dataset-W. Label", "Public Dataset-W.o. Label", "Model Characteristics-Upload and Download", "Model Characteristics-Aggregation", "Model Characteristics-Personalization"], ["gray!20\nFedDF{{cite:bc634cb}}", "\u2717", "\u2713", "parameters", "ensemble distillation", "\u2717"], ["FedKEMF{{cite:c51cb7c}}", "\u2717", "\u2713", "parameters", "mutual learning", "\u2713"], ["FCCL {{cite:05dca64}}", "\u2717", "\u2713", "logits", "average", "\u2713"], ["FedMD{{cite:d9b8cb0}}", "\u2713", "\u2717", "class scores", "average", "\u2713"], ["FedGH {{cite:cdf1d96}}", "\u2713", "\u2717", "label-wise representations", "average", "\u2713"], ["red!20\npFedHR", "\u2713", "\u2713", "parameters", "model reassembly", "\u2713"]], "table_dict": {"References": ["{{cite:bc634cb}}", "{{cite:c51cb7c}}", "{{cite:05dca64}}", "{{cite:d9b8cb0}}", "{{cite:cdf1d96}}", "-"], "Approach": ["FedDF", "FedKEMF", "FCCL ", "FedMD", "FedGH ", "pFedHR"], "Public Dataset-W. Label": ["\u2717", "\u2717", "\u2717", "\u2713", "\u2713", "\u2713"], "Public Dataset-W.o. Label": ["\u2713", "\u2713", "\u2713", "\u2717", "\u2717", "\u2713"], "Model Characteristics-Upload and Download": ["parameters", "parameters", "logits", "class scores", "label-wise representations", "parameters"], "Model Characteristics-Aggregation": ["ensemble distillation", "mutual learning", "average", "average", "average", "model reassembly"], "Model Characteristics-Personalization": ["\u2717", "\u2713", "\u2713", "\u2713", "\u2713", "\u2713"]}}, "bib_hash": ["bc634cbc76983c021c5490753811d0862a21922c", "c51cb7c10dcae77792ad766ac99249a11c0d9da0", "05dca6492de9192a01751bac89194ffdbbf82fff", "d9b8cb082e0970472eb3f9cbe819b094a21532cf", "cdf1d964c4bf1ecc1eca7f0fa1e5ff137c84f9e9"], "row_bib_map": [{"bib_hash_or_arxiv_id": "bc634cbc76983c021c5490753811d0862a21922c", "row": 0, "corpus_id": 219636007, "type": "ref"}, {"bib_hash_or_arxiv_id": "c51cb7c10dcae77792ad766ac99249a11c0d9da0", "row": 1, "corpus_id": 263869520, "type": "ref"}, {"bib_hash_or_arxiv_id": "05dca6492de9192a01751bac89194ffdbbf82fff", "row": 2, "corpus_id": 250210682, "type": "ref"}, {"bib_hash_or_arxiv_id": "d9b8cb082e0970472eb3f9cbe819b094a21532cf", "row": 3, "corpus_id": 203951869, "type": "ref"}, {"bib_hash_or_arxiv_id": "cdf1d964c4bf1ecc1eca7f0fa1e5ff137c84f9e9", "row": 4, "corpus_id": 257687494, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.08643v1", "row": 5, "corpus_id": 261031130, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid8\" starred=\"true\" place=\"t\"><head>A comparison between existing heterogeneous model cooperation works and our <hi rend=\"tt\">pFedHR</hi>.</head>\n<unexpected><resizebox width=\"341.6013pt\">\n<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"left\">2*<hi rend=\"bold\">Approach</hi></cell>\n<cell right-border=\"true\" halign=\"center\" cols=\"2\"><hi rend=\"bold\">Public Dataset</hi></cell>\n<cell halign=\"center\" cols=\"3\"><hi rend=\"bold\">Model Characteristics</hi></cell>\n</row><row><cell right-border=\"true\" halign=\"left\">(lr)2-6</cell>\n<cell right-border=\"true\" halign=\"center\">W. Label</cell>\n<cell halign=\"center\">W.o. Label</cell>\n<cell halign=\"center\">Upload and Download</cell>\n<cell halign=\"center\">Aggregation</cell>\n<cell>Personalization</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">gray!20\nFedDF<cit sha=\"bc634cbc76983c021c5490753811d0862a21922c\"><ref target=\"bid15\"/></cit>{{cite:bc634cb}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">parameters</cell>\n<cell halign=\"center\">ensemble distillation</cell>\n<cell>\u2717</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">FedKEMF<cit sha=\"c51cb7c10dcae77792ad766ac99249a11c0d9da0\"><ref target=\"bid16\"/></cit>{{cite:c51cb7c}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">parameters</cell>\n<cell halign=\"center\">mutual learning</cell>\n<cell>\u2713</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">FCCL <cit sha=\"05dca6492de9192a01751bac89194ffdbbf82fff\"><ref target=\"bid14\"/></cit>{{cite:05dca64}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">logits</cell>\n<cell halign=\"center\">average</cell>\n<cell>\u2713</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">FedMD<cit sha=\"d9b8cb082e0970472eb3f9cbe819b094a21532cf\"><ref target=\"bid13\"/></cit>{{cite:d9b8cb0}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">class scores</cell>\n<cell halign=\"center\">average</cell>\n<cell>\u2713</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">FedGH <cit sha=\"cdf1d964c4bf1ecc1eca7f0fa1e5ff137c84f9e9\"><ref target=\"bid18\"/></cit>{{cite:cdf1d96}}</cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">label-wise representations</cell>\n<cell halign=\"center\">average</cell>\n<cell>\u2713</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">red!20\n<hi rend=\"tt\">pFedHR</hi></cell>\n<cell right-border=\"true\" halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">parameters</cell>\n<cell halign=\"center\">model reassembly</cell>\n<cell>\u2713</cell>\n</row></table>\n</resizebox></unexpected></table>", "caption": "A comparison between existing heterogeneous model cooperation works and our pFedHR.", "type": "table"}, "_table_hash_full_text": "bf5f3da7-78a5-4ad5-9e46-7b9a451d4cfe", "_full_text_table_hash": "b8a7fba1-a12b-4b99-ad7d-1ff1b0bc8ec3", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Approach<~>Approach": "In the context of the table, 'Approach' refers to the name of the specific heterogeneous model cooperation method referenced by the citation.", "Approach<~>FedDF": "FedDF refers to Federated Data Fusion, an approach for model cooperation in federated learning that employs unlabeled public data for model training.", "Approach<~>FedKEMF": "FedKEMF is a federated learning approach that performs mutual knowledge distillation learning on the server side to achieve model personalization, requiring predefined model structures.", "Approach<~>FCCL": "FCCL stands for Federated Contextual Comparison Learning, an approach mentioned in the text that uses a consensus logit as guidance during local model training to address model heterogeneity.", "Approach<~>FedMD": "FedMD refers to FedModel Distillation, an approach mentioned in the text that employs labeled public data for exchange of class information or representations between the server and clients to address model heterogeneity while introducing privacy leakage concerns.", "Approach<~>FedGH": "FedGH in the table refers to FedGHetero, a federated learning approach that employs labeled public data to exchange class information or representations between the server and clients for addressing model heterogeneity. (Source: cdf1d96)", "Approach<~>pFedHR": "pFedHR refers to the novel framework proposed in the paper for personalized federated learning via heterogeneous model reassembly.", "Public Dataset-W. Label<~>Public Dataset-W. Label": "'Public Dataset-W. Label' refers to the use of labeled public data in the existing heterogeneous model cooperation works mentioned in the text.", "Public Dataset-W.o. Label<~>Public Dataset-W.o. Label": "'Public Dataset-W.o. Label' in the context of the table referred to the use of unlabeled public data for model training in existing heterogeneous model cooperation approaches.", "Model Characteristics-Upload and Download<~>Model Characteristics-Upload and Download": "In the given table, 'Model Characteristics-Upload and Download' refers to the type of data that is sent and received between the server and the clients during the federated learning process. Specifically, it indicates the type of information that is transmitted in each row of the table. For example, for the first reference ({{cite:bc634cb}}), it is 'parameters', which means that only parameters are uploaded and downloaded in this case.", "Model Characteristics-Upload and Download<~>parameters": "In the context of the table, 'parameters' in the 'Model Characteristics-Upload and Download' column for references {{cite:bc634cb}} and {{cite:c51cb7c}} refer to the model weights or learnable parameters that are uploaded and downloaded during federated learning.", "Model Characteristics-Upload and Download<~>logits": "In the context of the table, 'logits' in the column 'Model Characteristics-Upload and Download' for reference [cite:05dca64] refer to the output of the final activation function before the softmax layer in a neural network.", "Model Characteristics-Upload and Download<~>class scores": "In the given table, 'class scores' in the column 'Model Characteristics-Upload and Download' refer to the outputs of a neural network model representing the probability distribution over the classes for a given input.", "Model Characteristics-Upload and Download<~>label-wise representations": "In the context of the table, 'label-wise representations' in the column 'Model Characteristics-Upload and Download' for reference {{cite:cdf1d96}} refer to the transfer of class labels and their corresponding model outputs or representations to the server for aggregation.", "Model Characteristics-Aggregation<~>Model Characteristics-Aggregation": "'Model Characteristics-Aggregation' in the context of the table refers to the methods used by existing heterogeneous model cooperation approaches to combine or aggregate the models from different clients. The table lists these methods, including ensemble distillation, mutual learning, and averaging, for some of the cited works.", "Model Characteristics-Aggregation<~>ensemble distillation": "In the context of the table, 'ensemble distillation' in the row referencing [[bc634cb]] under 'Model Characteristics-Aggregation' is a method used in FedDF for achieving model personalization by training a global model using the knowledge distilled from multiple client models.", "Model Characteristics-Aggregation<~>mutual learning": "In the context of the table, 'mutual learning' in the column 'Model Characteristics-Aggregation' for reference [[c51cb7c]] refers to a model training approach where multiple models learn from each other by exchanging information during the training process.", "Model Characteristics-Aggregation<~>average": "In the context of the table, 'average' in the column 'Model Characteristics-Aggregation' refers to the method used for aggregating the models on the server side, where the logits or parameters are averaged to produce a new model.", "Model Characteristics-Aggregation<~>model reassembly": "Model reassembly in the context of the table refers to the technique proposed by the authors of the paper for personalized federated learning through the aggregation of heterogeneous models.", "Model Characteristics-Personalization<~>Model Characteristics-Personalization": "'Model Characteristics-Personalization' in the context of the table refers to the capability of existing heterogeneous model cooperation approaches to generate personalized models. A '\u2713' indicates that the approach can generate personalized models, while an '\u2717' indicates that it cannot."}}, "title": "Towards Personalized Federated Learning via Heterogeneous Model Reassembly", "abstract": "This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner."}
{"paper_id": "2308.10631v2", "_pdf_hash": null, "_source_hash": "d5cd77a0a566fe93e975ed52b71d2ae56d2d73de", "_source_name": "2308.10631v2", "_table_hash": "6840a3c6-7ae9-412f-bed3-78e666859dcf", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid13\" id-text=\"1\" place=\"h\" rend=\"display\" starred=\"true\"><head><hi rend=\"small\">Comparison of popular datasets for gait recognition. PsyMo is similar in size and variations to other datasets, but is annotated with gender, height, weight and psychological attributes for each subject. GREW <cit sha=\"5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e\"><ref target=\"bid49\"/></cit>{{cite:5b905bf}}, Gait3D <cit sha=\"5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2\"><ref target=\"bid53\"/></cit>{{cite:5ab2560}} and DenseGait <cit sha=\"d69c6d56845da1b3a45871459adf15ce93741d8a\"><ref target=\"bid54\"/></cit>{{cite:d69c6d5}} are collected in the wild and have no clear delimitation between variations and viewpoints.</hi></head>\n<unexpected><hi rend=\"small\">\n<resizebox width=\"384.2974pt\">\n<table rend=\"inline\"><tr><td halign=\"left\" right-border=\"true\"><hi rend=\"small\"><hi rend=\"bold\">Dataset</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Type</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\"># IDs</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\"># Seq.</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Variations</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Views</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Env.</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Demogr.</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">BMI</hi></hi><hi rend=\"small\"/></td>\n<td><hi rend=\"small\"><hi rend=\"bold\">Particularity</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\" right-border=\"true\">FVG <cit sha=\"d2e8a57f36e52ecc8ebae992d562939cdcd4eb91\"><ref target=\"bid30\"/></cit>{{cite:d2e8a57}}</td>\n<td halign=\"center\">Controlled</td>\n<td halign=\"center\">226</td>\n<td halign=\"center\">2,857</td>\n<td halign=\"center\">NM, CL, BG, WS, CBG</td>\n<td halign=\"center\">3</td>\n<td halign=\"center\">Outdoor</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td>time difference</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">CASIA-B <cit sha=\"3d547cb557425d95a5290d9bc294ca29e87a61ac\"><ref target=\"bid29\"/></cit>{{cite:3d547cb}}</td>\n<td halign=\"center\">Controlled</td>\n<td halign=\"center\">124</td>\n<td halign=\"center\">13,640</td>\n<td halign=\"center\">NM, CL, BG</td>\n<td halign=\"center\">11</td>\n<td halign=\"center\">Indoor</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td>\u2013</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">OU-ISIR <cit sha=\"2accfb92d241cbbd4b607806a355d1de05c3b087\"><ref target=\"bid75\"/></cit>{{cite:2accfb9}}</td>\n<td halign=\"center\">Controlled</td>\n<td halign=\"center\">10,307</td>\n<td halign=\"center\">144,298</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">14</td>\n<td halign=\"center\">Indoor</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td>treadmill</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">Gait3D <cit sha=\"5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2\"><ref target=\"bid53\"/></cit>{{cite:5ab2560}}</td>\n<td halign=\"center\">In the Wild</td>\n<td halign=\"center\">4,000</td>\n<td halign=\"center\">25,309</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">Indoor</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td>collected in a supermarket</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">GREW <cit sha=\"5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e\"><ref target=\"bid49\"/></cit>{{cite:5b905bf}}</td>\n<td halign=\"center\">In the Wild</td>\n<td halign=\"center\">26,000</td>\n<td halign=\"center\">128,000</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">Outdoor</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2717</td>\n<td>\u2013</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">DenseGait <cit sha=\"d69c6d56845da1b3a45871459adf15ce93741d8a\"><ref target=\"bid54\"/></cit>{{cite:d69c6d5}}</td>\n<td halign=\"center\">In the Wild</td>\n<td halign=\"center\">217,954</td>\n<td halign=\"center\">217,954</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">Outdoor</td>\n<td halign=\"center\">\u2717</td>\n<td halign=\"center\">\u2717</td>\n<td>auto labelled with 42 attributes</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">PsyMo (<hi rend=\"small\"><hi rend=\"bold\">ours</hi></hi><hi rend=\"small\">)</hi></td>\n<td halign=\"center\">Controlled</td>\n<td halign=\"center\">312</td>\n<td halign=\"center\">14,976</td>\n<td halign=\"center\">NM, CL, BG, WSS, WSF, TXT, PH</td>\n<td halign=\"center\">6</td>\n<td halign=\"center\">Indoor</td>\n<td halign=\"center\">\u2713</td>\n<td halign=\"center\">\u2713</td>\n<td>17 psychological traits</td>\n</tr></table>\n</resizebox></hi></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Dataset", "Type", "# IDs", "# Seq.", "Variations", "Views", "Env.", "Demogr.", "BMI", "Particularity"], ["FVG {{cite:d2e8a57}}", "Controlled", "226", "2,857", "NM, CL, BG, WS, CBG", "3", "Outdoor", "\u2717", "\u2717", "time difference"], ["CASIA-B {{cite:3d547cb}}", "Controlled", "124", "13,640", "NM, CL, BG", "11", "Indoor", "\u2717", "\u2717", "\u2013"], ["OU-ISIR {{cite:2accfb9}}", "Controlled", "10,307", "144,298", "-", "14", "Indoor", "\u2713", "\u2717", "treadmill"], ["Gait3D {{cite:5ab2560}}", "In the Wild", "4,000", "25,309", "-", "-", "Indoor", "\u2717", "\u2717", "collected in a supermarket"], ["GREW {{cite:5b905bf}}", "In the Wild", "26,000", "128,000", "-", "-", "Outdoor", "\u2713", "\u2717", "\u2013"], ["DenseGait {{cite:d69c6d5}}", "In the Wild", "217,954", "217,954", "-", "-", "Outdoor", "\u2717", "\u2717", "auto labelled with 42 attributes"], ["PsyMo (ours)", "Controlled", "312", "14,976", "NM, CL, BG, WSS, WSF, TXT, PH", "6", "Indoor", "\u2713", "\u2713", "17 psychological traits"]], "table_dict": {"References": ["{{cite:d2e8a57}}", "{{cite:3d547cb}}", "{{cite:2accfb9}}", "{{cite:5ab2560}}", "{{cite:5b905bf}}", "{{cite:d69c6d5}}", "-"], "Dataset": ["FVG ", "CASIA-B ", "OU-ISIR ", "Gait3D ", "GREW ", "DenseGait ", "PsyMo (ours)"], "Type": ["Controlled", "Controlled", "Controlled", "In the Wild", "In the Wild", "In the Wild", "Controlled"], "# IDs": ["226", "124", "10,307", "4,000", "26,000", "217,954", "312"], "# Seq.": ["2,857", "13,640", "144,298", "25,309", "128,000", "217,954", "14,976"], "Variations": ["NM, CL, BG, WS, CBG", "NM, CL, BG", "-", "-", "-", "-", "NM, CL, BG, WSS, WSF, TXT, PH"], "Views": ["3", "11", "14", "-", "-", "-", "6"], "Env.": ["Outdoor", "Indoor", "Indoor", "Indoor", "Outdoor", "Outdoor", "Indoor"], "Demogr.": ["\u2717", "\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2713"], "BMI": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], "Particularity": ["time difference", "\u2013", "treadmill", "collected in a supermarket", "\u2013", "auto labelled with 42 attributes", "17 psychological traits"]}}, "bib_hash": ["5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e", "5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2", "d69c6d56845da1b3a45871459adf15ce93741d8a", "d2e8a57f36e52ecc8ebae992d562939cdcd4eb91", "3d547cb557425d95a5290d9bc294ca29e87a61ac", "2accfb92d241cbbd4b607806a355d1de05c3b087", "5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2", "5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e", "d69c6d56845da1b3a45871459adf15ce93741d8a"], "row_bib_map": [{"bib_hash_or_arxiv_id": "d2e8a57f36e52ecc8ebae992d562939cdcd4eb91", "row": 0, "corpus_id": 202540963, "type": "ref"}, {"bib_hash_or_arxiv_id": "3d547cb557425d95a5290d9bc294ca29e87a61ac", "row": 1, "corpus_id": 1815453, "type": "ref"}, {"bib_hash_or_arxiv_id": "2accfb92d241cbbd4b607806a355d1de05c3b087", "row": 2, "corpus_id": 4633086, "type": "ref"}, {"bib_hash_or_arxiv_id": "5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2", "row": 3, "corpus_id": 247996921, "type": "ref"}, {"bib_hash_or_arxiv_id": "5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e", "row": 4, "corpus_id": 244906176, "type": "ref"}, {"bib_hash_or_arxiv_id": "d69c6d56845da1b3a45871459adf15ce93741d8a", "row": 5, "corpus_id": 252263212, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.10631v2", "row": 6, "corpus_id": 261049754, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid13\" starred=\"true\" place=\"h\"><head><hi rend=\"small\">Comparison of popular datasets for gait recognition. PsyMo is similar in size and variations to other datasets, but is annotated with gender, height, weight and psychological attributes for each subject. GREW <cit sha=\"5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e\"><ref target=\"bid49\"/></cit>{{cite:5b905bf}}, Gait3D <cit sha=\"5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2\"><ref target=\"bid53\"/></cit>{{cite:5ab2560}} and DenseGait <cit sha=\"d69c6d56845da1b3a45871459adf15ce93741d8a\"><ref target=\"bid54\"/></cit>{{cite:d69c6d5}} are collected in the wild and have no clear delimitation between variations and viewpoints.</hi></head>\n<unexpected><hi rend=\"small\">\n<resizebox width=\"384.2974pt\">\n<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Dataset</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Type</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\"># IDs</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\"># Seq.</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Variations</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Views</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Env.</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Demogr.</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">BMI</hi></hi><hi rend=\"small\"/></cell>\n<cell><hi rend=\"small\"><hi rend=\"bold\">Particularity</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell right-border=\"true\" halign=\"left\">FVG <cit sha=\"d2e8a57f36e52ecc8ebae992d562939cdcd4eb91\"><ref target=\"bid30\"/></cit>{{cite:d2e8a57}}</cell>\n<cell halign=\"center\">Controlled</cell>\n<cell halign=\"center\">226</cell>\n<cell halign=\"center\">2,857</cell>\n<cell halign=\"center\">NM, CL, BG, WS, CBG</cell>\n<cell halign=\"center\">3</cell>\n<cell halign=\"center\">Outdoor</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell>time difference</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">CASIA-B <cit sha=\"3d547cb557425d95a5290d9bc294ca29e87a61ac\"><ref target=\"bid29\"/></cit>{{cite:3d547cb}}</cell>\n<cell halign=\"center\">Controlled</cell>\n<cell halign=\"center\">124</cell>\n<cell halign=\"center\">13,640</cell>\n<cell halign=\"center\">NM, CL, BG</cell>\n<cell halign=\"center\">11</cell>\n<cell halign=\"center\">Indoor</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell>\u2013</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">OU-ISIR <cit sha=\"2accfb92d241cbbd4b607806a355d1de05c3b087\"><ref target=\"bid75\"/></cit>{{cite:2accfb9}}</cell>\n<cell halign=\"center\">Controlled</cell>\n<cell halign=\"center\">10,307</cell>\n<cell halign=\"center\">144,298</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">14</cell>\n<cell halign=\"center\">Indoor</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell>treadmill</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">Gait3D <cit sha=\"5ab25605dba54ac96ddf88b3c53a83b2ee83a6c2\"><ref target=\"bid53\"/></cit>{{cite:5ab2560}}</cell>\n<cell halign=\"center\">In the Wild</cell>\n<cell halign=\"center\">4,000</cell>\n<cell halign=\"center\">25,309</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">Indoor</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell>collected in a supermarket</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">GREW <cit sha=\"5b905bfc1f6a6436ed243faeb7ac61dcaf172f6e\"><ref target=\"bid49\"/></cit>{{cite:5b905bf}}</cell>\n<cell halign=\"center\">In the Wild</cell>\n<cell halign=\"center\">26,000</cell>\n<cell halign=\"center\">128,000</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">Outdoor</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell>\u2013</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">DenseGait <cit sha=\"d69c6d56845da1b3a45871459adf15ce93741d8a\"><ref target=\"bid54\"/></cit>{{cite:d69c6d5}}</cell>\n<cell halign=\"center\">In the Wild</cell>\n<cell halign=\"center\">217,954</cell>\n<cell halign=\"center\">217,954</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">Outdoor</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell halign=\"center\">\u2717</cell>\n<cell>auto labelled with 42 attributes</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">PsyMo (<hi rend=\"small\"><hi rend=\"bold\">ours</hi></hi><hi rend=\"small\">)</hi></cell>\n<cell halign=\"center\">Controlled</cell>\n<cell halign=\"center\">312</cell>\n<cell halign=\"center\">14,976</cell>\n<cell halign=\"center\">NM, CL, BG, WSS, WSF, TXT, PH</cell>\n<cell halign=\"center\">6</cell>\n<cell halign=\"center\">Indoor</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell halign=\"center\">\u2713</cell>\n<cell>17 psychological traits</cell>\n</row></table>\n</resizebox></hi></unexpected></table>", "caption": "Comparison of popular datasets for gait recognition. PsyMo is similar in size and variations to other datasets, but is annotated with gender, height, weight and psychological attributes for each subject. GREW {{cite:5b905bf}}, Gait3D {{cite:5ab2560}} and DenseGait {{cite:d69c6d5}} are collected in the wild and have no clear delimitation between variations and viewpoints.", "type": "table"}, "_full_text_table_hash": "e147f394-6cdd-44f4-b641-08fd1a3d771f", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Dataset<~>FVG": "FVG in the table refers to the Faces in the Wild (FVG) dataset, which is one of the smaller, more controlled datasets for gait recognition mentioned in the text.", "Dataset<~>CASIA-B": "CASIA-B in the table refers to the CASIA-B gait dataset, which is mentioned in the text as a smaller, more controlled dataset used for gait recognition.", "Dataset<~>OU-ISIR": "OU-ISIR in the table refers to the Optimal Unit for Intelligent Systems and Robotics laboratory dataset.", "Dataset<~>Gait3D": "'Gait3D' in the table refers to the Gait3D dataset, which is a popular dataset for gait recognition collected in the wild and has no clear delimitation between variations and viewpoints.", "Dataset<~>GREW": "GREW in the table refers to the Gait REcognition in the Wild dataset.", "Dataset<~>DenseGait": "DenseGait in the table refers to a dataset for gait recognition that is collected in the wild with no clear delimitation between variations and viewpoints.", "Dataset<~>PsyMo (ours)": "PsyMo (ours) in the table refers to the PsyMo dataset proposed in the text, which is not cited in the reference list but described as a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns, annotated with psychological attributes for each subject.", "Type<~>Type": "In the given table, the 'Type' column refers to the type of dataset, specifically whether it is a controlled dataset or an in the wild dataset.", "Type<~>Controlled": "In the context of the table, 'Controlled' in the column 'Type' refers to datasets where the conditions under which the data was collected are carefully managed and consistent.", "Type<~>In the Wild": "In the context of the table, 'In the Wild' in the column 'Type' refers to datasets that are collected in real-world conditions, as opposed to controlled environments.", "# IDs<~># IDs": "'# IDs' refers to the number of identities or subjects in each dataset, as indicated in the table.", "# Seq.<~># Seq.": "'# Seq.' in the table refers to the number of sequences in each dataset.", "Variations<~>Variations": "In the context of the table, 'Variations' refers to the different types of walking patterns or conditions captured in each dataset for the gait recognition task.", "Variations<~>NM, CL, BG, WS, CBG": "In the context of the table, 'NM', 'CL', 'BG', 'WS', 'CBG' in the 'Variations' column for the first row refer to Normal Motion, Crouched Position, Both Feet Grounded, Walking Slowly, and Crouched and Both Feet Grounded, respectively.", "Variations<~>NM, CL, BG": "In the context of the table, 'NM, CL, BG' in the column 'Variations' refer to normal motion, casual walk, and big gait variations in the PsyMo dataset.", "Variations<~>NM, CL, BG, WSS, WSF, TXT, PH": "In the context of the table, 'NM' refers to normal walking, 'CL' refers to climbing stairs, 'BG' refers to going backward, 'WSS' stands for walking while carrying a heavy weight sideways, 'WSF' stands for walking sideways with no weight, 'TXT' refers to texting while walking, and 'PH' stands for walking with a backpack.", "Views<~>Views": "In the context of the table, 'Views' in the second column refers to the number of different viewpoints in each dataset for gait recognition.", "Env.<~>Env.": "In the given table, 'Env.' refers to the environment where the data for gait recognition was collected, specifically whether it was indoors or outdoors.", "Env.<~>Outdoor": "In the context of the table, 'Outdoor' in the column 'Env.' refers to environments where the gait data was recorded outdoors.", "Env.<~>Indoor": "In the context of the table, 'Indoor' in the column 'Env.' refers to datasets that were collected in an indoor environment.", "Demogr.<~>Demogr.": "In the given table, 'Demogr.' refers to demographic information, which includes gender, height, and weight mentioned in the context of the PsyMo dataset.", "BMI<~>BMI": "BMI in the table refers to Body Mass Index. It is indicated by a checkmark (\u2713) for PsyMo dataset, but not for the other referenced datasets.", "Particularity<~>Particularity": "'Particularity' in the context of the table refers to the specific attributes or characteristics of each dataset. In this case, it refers to the type of data collected and annotated in each dataset for gait recognition. For PsyMo, it is annotated with gender, height, weight, and psychological attributes, while other datasets like GREW, Gait3D, and DenseGait are collected in the wild and have no clear delimitation between variations and viewpoints.", "Particularity<~>time difference": "In the context of the table, 'time difference' in the column 'Particularity' for the reference '[cite:d2e8a57]' likely refers to the difference in time between walking sequences used for training and testing in a gait recognition system.", "Particularity<~>treadmill": "In the context of the table, 'treadmill' in the column 'Particularity' for the CASIA-B reference refers to the type of surface used for capturing gait data, specifically a controlled environment using treadmills.", "Particularity<~>collected in a supermarket": "The entry in the 'Particularity' column for reference [{{cite:5ab2560}}] indicating 'collected in a supermarket' refers to the specific location where the data for this dataset was captured.", "Particularity<~>auto labelled with 42 attributes": "The 'auto labelled with 42 attributes' in the 'Particularity' column for 'DenseGait' (reference 5b905bf) indicates that the dataset is annotated with 42 different attributes in addition to gait information, including psychological traits.", "Particularity<~>17 psychological traits": "The '17 psychological traits' in the 'Particularity' column refer to the psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health that were filled in by participants in the PsyMo dataset."}}, "title": "PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait", "abstract": "Psychological trait estimation from external factors such as movement and appearance is a challenging and long-standing problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totalling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes."}
{"paper_id": "2308.02151v1", "_pdf_hash": null, "_source_hash": "b2563ccc60b424ac122691dbd4d201a9f91c9a55", "_source_name": "2308.02151v1", "_table_hash": "e816c01a-4fb4-4c23-ac5d-15b0242c82e4", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid5\" id-text=\"1\" place=\"ht\" rend=\"display\"><head>Related work on large language agents.</head>\n<unexpected><resizebox width=\"427.0pt\"><table rend=\"inline\"><tr><td halign=\"left\"><hi rend=\"bold\">Approach</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Gradient</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Arbitrary</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Iterative</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Hidden</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Decision </hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Memory</hi></td>\n</tr><tr><td halign=\"left\"/>\n<td halign=\"center\"><hi rend=\"bold\">learning</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">reward</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">refinement</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">constraints</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">making</hi></td>\n<td halign=\"center\"/>\n</tr><tr><td halign=\"left\">CoT<cit sha=\"5fbfa01cd48556124f2225cb26b7d091b409e4d9\"><ref target=\"bid12\"/></cit>{{cite:5fbfa01}}</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n</tr><tr><td halign=\"left\">ReAct<cit sha=\"712a0c190da8f9c8a44175afdbe10d248eea50bd\"><ref target=\"bid0\"/></cit>{{cite:712a0c1}}</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n</tr><tr><td halign=\"left\">Self-refine<cit sha=\"ff0016f1a7ba2154aa780af10489c75833d77db4\"><ref target=\"bid9\"/></cit>{{cite:ff0016f}}</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n</tr><tr><td halign=\"left\">RAP<cit sha=\"bd15724e879a1e63ccb412027106d8c8263972a5\"><ref target=\"bid13\"/></cit>{{cite:bd15724}}</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n</tr><tr><td halign=\"left\">Reflexion<cit sha=\"e12425d0ee9fb71c5e2cfeeefb57bce921d4d552\"><ref target=\"bid8\"/></cit>{{cite:e12425d}}</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">gray\u2717</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n</tr><tr><td halign=\"left\"><hi rend=\"bold\"/><hi rend=\"tt\"><hi rend=\"bold\">Retroformer</hi></hi><hi rend=\"bold\"/>(our method)</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n<td halign=\"center\">black\u2713</td>\n</tr></table>\n</resizebox></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Approach", "Gradient", "Arbitrary", "Iterative", "Hidden", "Decision ", "Memory"], ["", "learning", "reward", "refinement", "constraints", "making", ""], ["CoT{{cite:5fbfa01}}", "gray\u2717", "gray\u2717", "gray\u2717", "gray\u2717", "gray\u2717", "gray\u2717"], ["ReAct{{cite:712a0c1}}", "gray\u2717", "gray\u2717", "gray\u2717", "black\u2713", "black\u2713", "black\u2713"], ["Self-refine{{cite:ff0016f}}", "gray\u2717", "gray\u2717", "black\u2713", "gray\u2717", "gray\u2717", "gray\u2717"], ["RAP{{cite:bd15724}}", "gray\u2717", "gray\u2717", "black\u2713", "black\u2713", "black\u2713", "black\u2713"], ["Reflexion{{cite:e12425d}}", "gray\u2717", "gray\u2717", "black\u2713", "black\u2713", "black\u2713", "black\u2713"], ["Retroformer(our method)", "black\u2713", "black\u2713", "black\u2713", "black\u2713", "black\u2713", "black\u2713"]], "table_dict": {"References": ["{{cite:5fbfa01}}", "{{cite:712a0c1}}", "{{cite:ff0016f}}", "{{cite:bd15724}}", "{{cite:e12425d}}", "-"], "Approach": ["CoT ", "ReAct ", "Self-refine ", "RAP ", "Reflexion ", "Retroformer (our method)"], "Gradient learning": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], "Arbitrary reward": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], "Iterative refinement": ["\u2717", "\u2717", "\u2713", "\u2713", "\u2713", "\u2713"], "Hidden constraints": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2713", "\u2713"], "Decision making": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2713", "\u2713"], "Memory": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2713", "\u2713"]}}, "bib_hash": ["5fbfa01cd48556124f2225cb26b7d091b409e4d9", "712a0c190da8f9c8a44175afdbe10d248eea50bd", "ff0016f1a7ba2154aa780af10489c75833d77db4", "bd15724e879a1e63ccb412027106d8c8263972a5", "e12425d0ee9fb71c5e2cfeeefb57bce921d4d552"], "row_bib_map": [{"bib_hash_or_arxiv_id": "5fbfa01cd48556124f2225cb26b7d091b409e4d9", "row": 0, "corpus_id": 246411621, "type": "ref"}, {"bib_hash_or_arxiv_id": "712a0c190da8f9c8a44175afdbe10d248eea50bd", "row": 1, "corpus_id": 252762395, "type": "ref"}, {"bib_hash_or_arxiv_id": "ff0016f1a7ba2154aa780af10489c75833d77db4", "row": 2, "corpus_id": 257900871, "type": "ref"}, {"bib_hash_or_arxiv_id": "bd15724e879a1e63ccb412027106d8c8263972a5", "row": 3, "corpus_id": 258865812, "type": "ref"}, {"bib_hash_or_arxiv_id": "e12425d0ee9fb71c5e2cfeeefb57bce921d4d552", "row": 4, "corpus_id": 258833055, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.02151v1", "row": 5, "corpus_id": 260611249, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid5\" place=\"ht\"><head>Related work on large language agents.</head>\n<unexpected><resizebox width=\"427.0pt\"><table rend=\"inline\"><row><cell halign=\"left\"><hi rend=\"bold\">Approach</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Gradient</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Arbitrary</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Iterative</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Hidden</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Decision </hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Memory</hi></cell>\n</row><row><cell halign=\"left\"/>\n<cell halign=\"center\"><hi rend=\"bold\">learning</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">reward</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">refinement</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">constraints</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">making</hi></cell>\n<cell halign=\"center\"/>\n</row><row><cell halign=\"left\">CoT\u00a0<cit sha=\"5fbfa01cd48556124f2225cb26b7d091b409e4d9\"><ref target=\"bid12\"/></cit>{{cite:5fbfa01}}</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n</row><row><cell halign=\"left\">ReAct\u00a0<cit sha=\"712a0c190da8f9c8a44175afdbe10d248eea50bd\"><ref target=\"bid0\"/></cit>{{cite:712a0c1}}</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n</row><row><cell halign=\"left\">Self-refine\u00a0<cit sha=\"ff0016f1a7ba2154aa780af10489c75833d77db4\"><ref target=\"bid9\"/></cit>{{cite:ff0016f}}</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n</row><row><cell halign=\"left\">RAP\u00a0<cit sha=\"bd15724e879a1e63ccb412027106d8c8263972a5\"><ref target=\"bid13\"/></cit>{{cite:bd15724}}</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n</row><row><cell halign=\"left\">Reflexion\u00a0<cit sha=\"e12425d0ee9fb71c5e2cfeeefb57bce921d4d552\"><ref target=\"bid8\"/></cit>{{cite:e12425d}}</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">gray\u2717</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n</row><row><cell halign=\"left\"><hi rend=\"bold\"/><hi rend=\"tt\"><hi rend=\"bold\">Retroformer</hi></hi><hi rend=\"bold\"/>\u00a0(our method)</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n<cell halign=\"center\">black\u2713</cell>\n</row></table>\n</resizebox></unexpected></table>", "caption": "Related work on large language agents.", "type": "table"}, "_table_hash_full_text": "5bbcf5f8-5204-4eb7-a86c-cb3ffbdfeb6a", "_full_text_table_hash": "3df6c6f3-c102-45a6-9330-01e293d766e3", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Approach<~>Approach": "In the context of the table, 'Approach' refers to the specific language agent method or technique referenced in each row, such as CoT, ReAct, Self-refine, RAP, and Reflexion. Retroformer is the approach represented by the dash (-) in the fifth row, which is the method introduced in the paper.", "Approach<~>CoT": "CoT in the 'Approach' column refers to Chain-of-Thoughts, a pioneering work that prompts agents to decompose challenging reasoning tasks into smaller, more manageable steps.", "Approach<~>ReAct": "ReAct in the context of the table refers to the approach proposed in the paper with reference {{cite:712a0c1}}, which proposes the exploitation of reasoning and acting proficiency within large language models to encourage interaction with the environment by mapping observations to the generation of reasoning and action traces or API calls in natural language.", "Approach<~>Self-refine": "Self-refine in the context of the table refers to the approach presented in the reference [{{cite:ff0016f}}], which employs a single language model as a generator, refiner, and provider of feedback for iterative refinement of outputs.", "Approach<~>RAP": "RAP in the table refers to the Reinforced Approximate Policy learning approach introduced in the paper with reference [[bd15724]].", "Approach<~>Reflexion": "Reflexion in the table refers to the framework presented in {{cite:e12425d}} that equips agents with dynamic memory and self-reflection capabilities.", "Approach<~>Retroformer (our method)": "Retroformer (our method) refers to the approach introduced in the paper for reinforcing large language agents by learning a retrospective model that automatically tunes the language agent prompts from environment feedback through policy gradient optimization.", "Gradient learning<~>Gradient learning": "In the context of the given text and table, 'Gradient learning' refers to a reinforce learning method that uses gradients of the policy function with respect to the parameters to maximize the expected reward. (Based on the information provided in the text, it can be inferred that the last row of the table is indicating that reference 5 uses gradient learning.)", "Arbitrary reward<~>Arbitrary reward": "'Arbitrary reward' in the context of the table refers to the use of rewards that come from outside the specified agent architectures described in the references. It appears as a checkmark (\u2713) in the table for an unnamed reference (the fifth row), suggesting that this work employs arbitrary rewards. The other references do not seem to be explicitly stated as using arbitrary rewards, as indicated by the empty checkboxes (\u2717) in their rows.", "Iterative refinement<~>Iterative refinement": "Iterative refinement refers to the ability of an agent to iteratively improve its outputs or actions based on feedback, as demonstrated by references 2, 3, and 5 in the table.", "Hidden constraints<~>Hidden constraints": "In the context of the table, 'Hidden constraints' in relation to the references is unanswerable without additional context provided in the text.", "Decision making<~>Decision making": "In the context of the table, 'Decision making' refers to the ability of language agents to make choices or determinations based on given information or feedback from the environment. The '\u2713' symbols indicate works that incorporate decision making capabilities in their agent architectures, while the '\u2717' symbols indicate those that do not.", "Memory<~>Memory": "In the context of the table, 'Memory' refers to the presence or absence of dynamic memory capabilities in the mentioned language agent research."}}, "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time."}
{"paper_id": "2308.03108v1", "_pdf_hash": null, "_source_hash": "e5513c8a6380891a2342df129e2af5e89252a53a", "_source_name": "2308.03108v1", "_table_hash": "254278b5-ee90-4d52-88e1-fadc9531e8a0", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid9\" id-text=\"2\" place=\"!ht\" rend=\"display\" starred=\"true\"><head>Comparison of attack methods. M - Manipulate estimation; H \u2013 Hiding objects. A \u2013 Anywhere in the frame; O \u2013 On the target object(s)</head>\n<tr bottom-border=\"true\" top-border=\"true\"><td halign=\"left\" left-border=\"true\" right-border=\"true\"><hi rend=\"bold\">Method</hi></td>\n<td halign=\"center\" right-border=\"true\"><hi rend=\"bold\">Attack goal</hi></td>\n<td halign=\"center\" right-border=\"true\"><hi rend=\"bold\">Attacker\u2019s Knowledge</hi></td>\n<td halign=\"center\" right-border=\"true\"><hi rend=\"bold\">Stealthy</hi></td>\n<td halign=\"center\" right-border=\"true\"><hi rend=\"bold\">Placement</hi></td>\n<td halign=\"center\" right-border=\"true\"><hi rend=\"bold\">Setting</hi></td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\" left-border=\"true\" right-border=\"true\"><hi rend=\"bold\">Adversarial patch <cit sha=\"440d57105341eef5e4a5d266326e4e28ea3905ae\"><ref target=\"bid8\"/></cit>{{cite:440d571}}</hi></td>\n<td halign=\"center\" right-border=\"true\">M</td>\n<td halign=\"center\" right-border=\"true\">White-box</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u00d7</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">A</td>\n<td halign=\"center\" right-border=\"true\">Outdoor</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\" left-border=\"true\" right-border=\"true\"><hi rend=\"bold\">AOP <cit sha=\"267012d5e2f5b23c471635363455d4dddbd72534\"><ref target=\"bid9\"/></cit>{{cite:267012d}}</hi></td>\n<td halign=\"center\" right-border=\"true\">M</td>\n<td halign=\"center\" right-border=\"true\">White-box</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow/></math></formula></td>\n<td halign=\"center\" right-border=\"true\">O</td>\n<td halign=\"center\" right-border=\"true\">Outdoor</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\" left-border=\"true\" right-border=\"true\"><hi rend=\"bold\">APARATE <cit sha=\"a80ee042a02f54cfca874ce8337df4de092596c9\"><ref target=\"bid6\"/></cit>{{cite:a80ee04}}</hi></td>\n<td halign=\"center\" right-border=\"true\">M, H</td>\n<td halign=\"center\" right-border=\"true\">White-box</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u00d7</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">O</td>\n<td halign=\"center\" right-border=\"true\">Outdoor</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\" left-border=\"true\" right-border=\"true\"><hi rend=\"bold\">SAAM (ours)</hi></td>\n<td halign=\"center\" right-border=\"true\">M, H</td>\n<td halign=\"center\" right-border=\"true\">White-box</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow/></math></formula></td>\n<td halign=\"center\" right-border=\"true\">O, A</td>\n<td halign=\"center\" right-border=\"true\">Indoor</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Method", "Attack goal", "Attacker\u2019s Knowledge", "Stealthy", "Placement", "Setting"], ["Adversarial patch {{cite:440d571}}", "M", "White-box", "\u00d7", "A", "Outdoor"], ["AOP {{cite:267012d}}", "M", "White-box", "", "O", "Outdoor"], ["APARATE {{cite:a80ee04}}", "M, H", "White-box", "\u00d7", "O", "Outdoor"], ["SAAM (ours)", "M, H", "White-box", "", "O, A", "Indoor"]], "table_dict": {"References": ["{{cite:440d571}}", "{{cite:267012d}}", "{{cite:a80ee04}}", "-"], "Method": ["Adversarial patch ", "AOP ", "APARATE ", "SAAM (ours)"], "Attack goal": ["M", "M", "M, H", "M, H"], "Attacker's Knowledge": ["White-box", "White-box", "White-box", "White-box"], "Stealthy": ["\u00d7", "-", "\u00d7", "-"], "Placement": ["A", "O", "O", "O, A"], "Setting": ["Outdoor", "Outdoor", "Outdoor", "Indoor"]}}, "bib_hash": ["440d57105341eef5e4a5d266326e4e28ea3905ae", "267012d5e2f5b23c471635363455d4dddbd72534", "a80ee042a02f54cfca874ce8337df4de092596c9"], "row_bib_map": [{"bib_hash_or_arxiv_id": "440d57105341eef5e4a5d266326e4e28ea3905ae", "row": 0, "corpus_id": 222177159, "type": "ref"}, {"bib_hash_or_arxiv_id": "267012d5e2f5b23c471635363455d4dddbd72534", "row": 1, "corpus_id": 250426022, "type": "ref"}, {"bib_hash_or_arxiv_id": "a80ee042a02f54cfca874ce8337df4de092596c9", "row": 2, "corpus_id": 257279866, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.03108v1", "row": 3, "corpus_id": 260682704, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"2\" id=\"uid9\" starred=\"true\" place=\"!ht\"><head>Comparison of attack methods. M - Manipulate estimation; H \u2013 Hiding objects. A \u2013 Anywhere in the frame; O \u2013 On the target object(s)</head>\n<row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"left\" left-border=\"true\"><hi rend=\"bold\">Method</hi></cell>\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">Attack goal</hi></cell>\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">Attacker\u2019s Knowledge</hi></cell>\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">Stealthy</hi></cell>\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">Placement</hi></cell>\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"bold\">Setting</hi></cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"left\" left-border=\"true\"><hi rend=\"bold\">Adversarial patch <cit sha=\"440d57105341eef5e4a5d266326e4e28ea3905ae\"><ref target=\"bid8\"/></cit>{{cite:440d571}}</hi></cell>\n<cell right-border=\"true\" halign=\"center\">M</cell>\n<cell right-border=\"true\" halign=\"center\">White-box</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u00d7</mo></math><texmath>\\times </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">A</cell>\n<cell right-border=\"true\" halign=\"center\">Outdoor</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"left\" left-border=\"true\"><hi rend=\"bold\">AOP <cit sha=\"267012d5e2f5b23c471635363455d4dddbd72534\"><ref target=\"bid9\"/></cit>{{cite:267012d}}</hi></cell>\n<cell right-border=\"true\" halign=\"center\">M</cell>\n<cell right-border=\"true\" halign=\"center\">White-box</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow/></math><texmath/></formula></cell>\n<cell right-border=\"true\" halign=\"center\">O</cell>\n<cell right-border=\"true\" halign=\"center\">Outdoor</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"left\" left-border=\"true\"><hi rend=\"bold\">APARATE <cit sha=\"a80ee042a02f54cfca874ce8337df4de092596c9\"><ref target=\"bid6\"/></cit>{{cite:a80ee04}}</hi></cell>\n<cell right-border=\"true\" halign=\"center\">M, H</cell>\n<cell right-border=\"true\" halign=\"center\">White-box</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u00d7</mo></math><texmath>\\times </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">O</cell>\n<cell right-border=\"true\" halign=\"center\">Outdoor</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"left\" left-border=\"true\"><hi rend=\"bold\">SAAM (ours)</hi></cell>\n<cell right-border=\"true\" halign=\"center\">M, H</cell>\n<cell right-border=\"true\" halign=\"center\">White-box</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow/></math><texmath/></formula></cell>\n<cell right-border=\"true\" halign=\"center\">O, A</cell>\n<cell right-border=\"true\" halign=\"center\">Indoor</cell>\n</row></table>", "caption": "Comparison of attack methods. M - Manipulate estimation; H \u2013 Hiding objects. A \u2013 Anywhere in the frame; O \u2013 On the target object(s)", "type": "table"}, "_table_hash_full_text": "74831927-54d3-4868-9656-ee94fa2cd434", "_full_text_table_hash": "a2ad42a4-2eac-4b0c-b638-3f0f2aca9d66", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table in the text, 'Dataset' refers to a specific collection of video data along with associated annotations, used for the tasks of video quality assessment (VQA) or video classification.", "Method<~>Method": "The 'Method' column in the table refers to the name of the specific adversarial attack method used. In this case, \"Adversarial patch,\" \"AOP (Adversarial Object Patching),\" \"APARATE (Adversarial Pixel Adversarial Robustness Evaluation),\" and \"SAAM (Stealthy Adversarial Attacks on MDE)\" are the different attack methods listed in the comparison.", "Method<~>Adversarial patch": "An adversarial patch is a type of attack method used in the context of SAAM (Stealthy Adversarial Attacks on Monocular Depth Estimation) to compromise monocular depth estimation by corrupting the estimated distance or causing an object to blend seamlessly into its surroundings.", "Method<~>AOP": "In the given context of the table, 'AOP' in the 'Method' column for reference {{cite:267012d}} stands for Adversarial Object Patch.", "Method<~>APARATE": "APARATE is a method referred to in the table as proposed by the authors of the SAAM paper.", "Method<~>SAAM (ours)": "SAAM (ours) in the table refers to the stealthy adversarial attack method proposed in the paper titled \"SAAM: Stealthy Adversarial Attacks on Monocular Depth Estimation.\"", "Attack goal<~>Attack goal": "'Attack goal' refers to the objective of the attack methods listed in the table, which can be to manipulate the estimation of depth information (M) or to hide objects (H) in the monocular depth estimation process.", "Attack goal<~>M": "The 'M' in the 'Attack goal' column refers to manipulating the monocular depth estimation.", "Attack goal<~>M, H": "In the table, 'M, H' in the column 'Attack goal' refers to attacks that manipulate the estimation of depth and hide objects in the scene.", "Attacker's Knowledge<~>Attacker's Knowledge": "'Attacker's Knowledge' in the context of the table refers to the level of access the attacker has to the monocular depth estimation model or system, specifically whether it is a white-box attack (where the attacker has full knowledge of the model's architecture and parameters) or not.", "Attacker's Knowledge<~>White-box": "In the context of the table, 'White-box' refers to an attacker having full knowledge of the Monocular Depth Estimation system's architecture and implementation details.", "Stealthy<~>Stealthy": "In the given context, 'Stealthy' refers to the proposed SAAM (Stealthy Adversarial Attacks on MDE) method in the scientific paper that can compromise monocular depth estimation by corrupting the estimated depth or causing an object to blend seamlessly into its surroundings while maintaining a naturalistic appearance, making it inconspicuous to human observers.", "Placement<~>Placement": "In the context of the table, 'Placement' refers to the location of the attack method in the frame, specifically whether it is 'Anywhere in the frame (A)', 'On the target object(s) (O)', or 'Hiding objects (H)'.", "Placement<~>A": "A in the column 'Placement' refers to anywhere in the frame for the SAAM (Stealthy Adversarial Attacks on Monocular Depth Estimation) method.", "Placement<~>O": "In the given table, 'O' in the column 'Placement' refers to the placement of the attacks on the target object(s) in the scene.", "Placement<~>O, A": "'O, A' in the 'Placement' column refers to attacks that can be placed both on the target object(s) and anywhere in the frame.", "Setting<~>Setting": "In the context of the table, 'Setting' refers to the environment where the experiments or attacks were conducted, specifically whether they were conducted indoors or outdoors.", "Setting<~>Outdoor": "In the context of the text, 'Outdoor' in the 'Setting' column refers to experiments conducted in external environments.", "Setting<~>Indoor": "In the context of the table, 'Indoor' in the 'Setting' column refers to a scenario where the monocular depth estimation is being performed in an indoor environment."}}, "title": "SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation", "abstract": "In this paper, we investigate the vulnerability of MDE to adversarial patches. We propose a novel \\underline{S}tealthy \\underline{A}dversarial \\underline{A}ttacks on \\underline{M}DE (SAAM) that compromises MDE by either corrupting the estimated distance or causing an object to seamlessly blend into its surroundings. Our experiments, demonstrate that the designed stealthy patch successfully causes a DNN-based MDE to misestimate the depth of objects. In fact, our proposed adversarial patch achieves a significant 60\\% depth error with 99\\% ratio of the affected region. Importantly, despite its adversarial nature, the patch maintains a naturalistic appearance, making it inconspicuous to human observers. We believe that this work sheds light on the threat of adversarial attacks in the context of MDE on edge devices. We hope it raises awareness within the community about the potential real-life harm of such attacks and encourages further research into developing more robust and adaptive defense mechanisms."}
{"paper_id": "2308.00247v1", "_pdf_hash": null, "_source_hash": "474459265ec31ecba6dad48c948a5e0544b377aa", "_source_name": "2308.00247v1", "_table_hash": "ba04afbc-1ef6-407a-93db-56d82d5671d7", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid41\" id-text=\"2\" place=\"t\" rend=\"display\" starred=\"true\"><head>BSN-based self-supervised image denosing methods of mask in inputs.</head>\n<unexpected>0cm\n0cm\n<table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">Method</td>\n<td halign=\"center\" right-border=\"true\">Other needs</td>\n<td halign=\"center\" right-border=\"true\">Applications (denoising type)</td>\n<td halign=\"center\" right-border=\"true\">Key words (remarks)</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">N2V<cit sha=\"4e746e4c2e3ae2c5dd860abd2d018ea3106964f4\"><ref target=\"bid78\"/></cit>{{cite:4e746e4}}</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">Gaussian noise and some biomedical image noise denoising.</td>\n<td halign=\"center\" right-border=\"true\">Pixel-wise independent noise, randomly select several pixel to mask in the input images.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">N2S<cit sha=\"9429f75c3322db60f8643f58e0533e1dbb878214\"><ref target=\"bid82\"/></cit>{{cite:9429f75}}</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">Blind Gaussian noise denoising</td>\n<td halign=\"center\" right-border=\"true\">J -invariant function determines the mask distribution, and replaces the pixels at J with random numbers.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">PN2V <cit sha=\"73dc57bf3cbb047bd53e0990356797f2779fbc94\"><ref target=\"bid75\"/></cit>{{cite:73dc57b}}</td>\n<td halign=\"center\" right-border=\"true\">An arbitrary noise model</td>\n<td halign=\"center\" right-border=\"true\">Microscopy and low-light condition image noise denoising</td>\n<td halign=\"center\" right-border=\"true\">Mask input images, probabilistic model, predict per-pixel intensity distributions.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">Noise2Same <cit sha=\"0ecb0ab42f91f4707a193c9b9f0738aa34b972c9\"><ref target=\"bid85\"/></cit>{{cite:0ecb0ab}}</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">Gaussian noise denoising.</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>J</mi></math></formula>-invariant function determines the mask distribution, and replaces the pixels at <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>J</mi></math></formula> with local averages.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">S2S <cit sha=\"ad9c0f851fb597f37d23ed4aa719f6c8588635fb\"><ref target=\"bid86\"/></cit>{{cite:ad9c0f8}}</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising</td>\n<td halign=\"center\" right-border=\"true\">Bernoulli-sampled instances of the input image results on noisy pairs</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" left-border=\"true\" right-border=\"true\">B2UB<cit sha=\"b3bdc3666dc471c1d6be29f983c219834cd099e3\"><ref target=\"bid105\"/></cit>{{cite:b3bdc36}}</td>\n<td halign=\"center\" right-border=\"true\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math></formula></td>\n<td halign=\"center\" right-border=\"true\">FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising</td>\n<td halign=\"center\" right-border=\"true\">Gobal-aware mask mapper, re-visible loss.</td>\n</tr></table></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Method", "Other needs", "Applications (denoising type)", "Key words (remarks)"], ["N2V{{cite:4e746e4}}", "\u2216", "Gaussian noise and some biomedical image noise denoising.", "Pixel-wise independent noise, randomly select several pixel to mask in the input images."], ["N2S{{cite:9429f75}}", "\u2216", "Blind Gaussian noise denoising", "J -invariant function determines the mask distribution, and replaces the pixels at J with random numbers."], ["PN2V {{cite:73dc57b}}", "An arbitrary noise model", "Microscopy and low-light condition image noise denoising", "Mask input images, probabilistic model, predict per-pixel intensity distributions."], ["Noise2Same {{cite:0ecb0ab}}", "\u2216", "Gaussian noise denoising.", "J-invariant function determines the mask distribution, and replaces the pixels at J with local averages."], ["S2S {{cite:ad9c0f8}}", "\u2216", "Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising", "Bernoulli-sampled instances of the input image results on noisy pairs"], ["B2UB{{cite:b3bdc36}}", "\u2216", "FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising", "Gobal-aware mask mapper, re-visible loss."]], "table_dict": {"References": ["{{cite:4e746e4}}", "{{cite:9429f75}}", "{{cite:73dc57b}}", "{{cite:0ecb0ab}}", "{{cite:ad9c0f8}}", "{{cite:b3bdc36}}"], "Method": ["N2V", "N2S", "PN2V ", "Noise2Same ", "S2S ", "B2UB"], "Other needs": ["-", "-", "An arbitrary noise model", "-", "-", "-"], "Applications (denoising type)": ["Gaussian noise and some biomedical image noise denoising.", "Blind Gaussian noise denoising", "Microscopy and low-light condition image noise denoising", "Gaussian noise denoising.", "Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising", "FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising"], "Key words (remarks)": ["Pixel-wise independent noise, randomly select several pixel to mask in the input images.", "J -invariant function determines the mask distribution, and replaces the pixels at J with random numbers.", "Mask input images, probabilistic model, predict per-pixel intensity distributions.", "J-invariant function determines the mask distribution, and replaces the pixels at J with local averages.", "Bernoulli-sampled instances of the input image results on noisy pairs", "Gobal-aware mask mapper, re-visible loss."]}}, "bib_hash": ["4e746e4c2e3ae2c5dd860abd2d018ea3106964f4", "9429f75c3322db60f8643f58e0533e1dbb878214", "73dc57bf3cbb047bd53e0990356797f2779fbc94", "0ecb0ab42f91f4707a193c9b9f0738aa34b972c9", "ad9c0f851fb597f37d23ed4aa719f6c8588635fb", "b3bdc3666dc471c1d6be29f983c219834cd099e3"], "row_bib_map": [{"bib_hash_or_arxiv_id": "4e746e4c2e3ae2c5dd860abd2d018ea3106964f4", "row": 0, "corpus_id": 53751136, "type": "ref"}, {"bib_hash_or_arxiv_id": "9429f75c3322db60f8643f58e0533e1dbb878214", "row": 1, "corpus_id": 59523708, "type": "ref"}, {"bib_hash_or_arxiv_id": "73dc57bf3cbb047bd53e0990356797f2779fbc94", "row": 2, "corpus_id": 173990717, "type": "ref"}, {"bib_hash_or_arxiv_id": "0ecb0ab42f91f4707a193c9b9f0738aa34b972c9", "row": 3, "corpus_id": 225062070, "type": "ref"}, {"bib_hash_or_arxiv_id": "ad9c0f851fb597f37d23ed4aa719f6c8588635fb", "row": 4, "corpus_id": 219619080, "type": "ref"}, {"bib_hash_or_arxiv_id": "b3bdc3666dc471c1d6be29f983c219834cd099e3", "row": 5, "corpus_id": 247447122, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"2\" id=\"uid41\" starred=\"true\" place=\"t\"><head>BSN-based self-supervised image denosing methods of mask in inputs.</head>\n<unexpected>0cm\n0cm\n<table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">Method</cell>\n<cell right-border=\"true\" halign=\"center\">Other needs</cell>\n<cell right-border=\"true\" halign=\"center\">Applications (denoising type)</cell>\n<cell right-border=\"true\" halign=\"center\">Key words (remarks)</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">N2V<cit sha=\"4e746e4c2e3ae2c5dd860abd2d018ea3106964f4\"><ref target=\"bid78\"/></cit>{{cite:4e746e4}}</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math><texmath>\\backslash </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">Gaussian noise and some biomedical image noise denoising.</cell>\n<cell right-border=\"true\" halign=\"center\">Pixel-wise independent noise, randomly select several pixel to mask in the input images.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">N2S<cit sha=\"9429f75c3322db60f8643f58e0533e1dbb878214\"><ref target=\"bid82\"/></cit>{{cite:9429f75}}</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math><texmath>\\backslash </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">Blind Gaussian noise denoising</cell>\n<cell right-border=\"true\" halign=\"center\">J -invariant function determines the mask distribution, and replaces the pixels at J with random numbers.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">PN2V <cit sha=\"73dc57bf3cbb047bd53e0990356797f2779fbc94\"><ref target=\"bid75\"/></cit>{{cite:73dc57b}}</cell>\n<cell right-border=\"true\" halign=\"center\">An arbitrary noise model</cell>\n<cell right-border=\"true\" halign=\"center\">Microscopy and low-light condition image noise denoising</cell>\n<cell right-border=\"true\" halign=\"center\">Mask input images, probabilistic model, predict per-pixel intensity distributions.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">Noise2Same <cit sha=\"0ecb0ab42f91f4707a193c9b9f0738aa34b972c9\"><ref target=\"bid85\"/></cit>{{cite:0ecb0ab}}</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math><texmath>\\backslash </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">Gaussian noise denoising.</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>J</mi></math><texmath>J</texmath></formula>-invariant function determines the mask distribution, and replaces the pixels at <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>J</mi></math><texmath>J</texmath></formula> with local averages.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">S2S <cit sha=\"ad9c0f851fb597f37d23ed4aa719f6c8588635fb\"><ref target=\"bid86\"/></cit>{{cite:ad9c0f8}}</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math><texmath>\\backslash </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising</cell>\n<cell right-border=\"true\" halign=\"center\">Bernoulli-sampled instances of the input image results on noisy pairs</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\" left-border=\"true\">B2UB<cit sha=\"b3bdc3666dc471c1d6be29f983c219834cd099e3\"><ref target=\"bid105\"/></cit>{{cite:b3bdc36}}</cell>\n<cell right-border=\"true\" halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u2216</mo></math><texmath>\\backslash </texmath></formula></cell>\n<cell right-border=\"true\" halign=\"center\">FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising</cell>\n<cell right-border=\"true\" halign=\"center\">Gobal-aware mask mapper, re-visible loss.</cell>\n</row></table></unexpected></table>", "caption": "BSN-based self-supervised image denosing methods of mask in inputs.", "type": "table"}, "_table_hash_full_text": "41149cab-0446-47d3-beed-5c9a44366056", "_full_text_table_hash": "f79e32ed-1780-4efd-a93a-7764dca79f4a", "context_autogenerated": {"glossary": {"Method<~>Method": "The 'Method' column in the table refers to the name of the specific self-supervised image denoising method used for each reference in the given table, as described in the provided text. These methods include Noise2Void (N2V), Noise2Self (N2S), Probabilistic Noise2Void (PN2V), Noise2Same, Self2Self With Dropout (S2S), and Blind2Unblind (B2UB).", "Method<~>N2V": "N2V in the table refers to the self-supervised image denoising algorithm Noise2Void proposed by Krull et al. in 2018.", "Method<~>N2S": "N2S in the table refers to the self-supervised image denoising method proposed based on the assumption of noise with statistical independence, as discussed in the text.", "Method<~>PN2V": "PN2V refers to Probabilistic Noise2Void, a self-supervised image denoising method that models the noise distribution using a probabilistic approach for handling non-uniform noise.", "Method<~>Noise2Same": "Noise2Same refers to a self-supervised image denoising method that masks the noise image to a J-invariant distribution and replaces masked pixels with local average values to learn the invariance loss between the denoised image and the masked image.", "Method<~>S2S": "S2S in the table refers to Self2Self With Dropout, a denoising method that uses Bernoulli dropout to improve the performance of self-supervised denoising methods.", "Method<~>B2UB": "B2UB stands for Blind2Unblind, which is a self-supervised denoising algorithm based on blind spots proposed by Zejin Wang et al. in 2022.", "Other needs<~>Other needs": "The 'Other needs' column in the table refers to any additional requirements or dependencies for each specific self-supervised image denoising method mentioned in the references. In the given context, it specifically refers to the need for an arbitrary noise model in the case of PN2V.", "Other needs<~>An arbitrary noise model": "In the context of the table, 'An arbitrary noise model' in the column 'Other needs' refers to the specific type of noise model used in the PN2V denoising method presented in the text.", "Applications (denoising type)<~>Applications (denoising type)": "The 'Applications (denoising type)' column in the table refers to the type of noise that each denoising method is designed to handle or has been applied to.", "Applications (denoising type)<~>Gaussian noise and some biomedical image noise denoising.": "The first application in the table, 'Gaussian noise and some biomedical image noise denoising,' refers to the use of self-supervised image denoising methods based on Blind Spot Networks (BSNs) to remove Gaussian noise and some types of noise commonly found in biomedical images.", "Applications (denoising type)<~>Blind Gaussian noise denoising": "Blind Gaussian noise denoising refers to self-supervised image denoising methods based on Blind Spot Network (BSN), where the model is trained to predict missing pixels in a noisy image based on their spatial continuity with surrounding pixels, using a masked version of the image as input.", "Applications (denoising type)<~>Microscopy and low-light condition image noise denoising": "'Microscopy and low-light condition image noise denoising' in the context of the table refers to the denoising of images with noise commonly found in microscopy and low-light conditions.", "Applications (denoising type)<~>Gaussian noise denoising.": "In the context of the table, 'Gaussian noise denoising.' in the column 'Applications (denoising type)' refers to methods designed to remove or reduce Gaussian noise from an image.", "Applications (denoising type)<~>Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising": "The term 'Blind Gaussian, salt-and-pepper and real-world sRGB image noise denoising' in the 'Applications (denoising type)' column of the table refers to the use of Blind Self-Supervised (BSN) methods for denoising various types of image noise, including Gaussian, salt-and-pepper, and real-world sRGB noise.", "Applications (denoising type)<~>FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising": "The entry \"FMDD, Gaussian, Poisson and real-world rawRGB image noise denoising\" in the 'Applications (denoising type)' column refers to the denoising of noises with different distributions, including FMDD (Fully Markovian Deterministic Denoising Model), Gaussian, Poisson, and real-world rawRGB image noise using Blind Spot Network (BSN)-based self-supervised image denoising methods.", "Key words (remarks)<~>Key words (remarks)": "The \"Key words (remarks)\" in the table refer to important terms and concepts associated with each reference, as outlined in the brief summary provided in the table caption.", "Key words (remarks)<~>Pixel-wise independent noise, randomly select several pixel to mask in the input images.": "The term 'Pixel-wise independent noise, randomly select several pixels to mask in the input images' in the context of the table refers to self-supervised image denoising methods, particularly those based on Blind Spot Networks (BSNs), where pixels in input images are randomly masked to create training data for the denoising model, with the assumption that the noise is pixel-wise independent.", "Key words (remarks)<~>J -invariant function determines the mask distribution, and replaces the pixels at J with random numbers.": "'J-invariant function' in the context of the table refers to a function that determines the mask distribution and replaces the selected pixels at J with random numbers for the purpose of self-supervised image denoising methods.", "Key words (remarks)<~>Mask input images, probabilistic model, predict per-pixel intensity distributions.": "The phrase \"Mask input images, probabilistic model, predict per-pixel intensity distributions.\" in the table refers to methods that use masked input images, probabilistic models, and predict the per-pixel intensity distributions of the masked regions. This is the case for the \"PN2V\" method mentioned in the text.", "Key words (remarks)<~>J-invariant function determines the mask distribution, and replaces the pixels at J with local averages.": "In the context of the table, 'J-invariant function determines the mask distribution, and replaces the pixels at J with local averages' refers to a method where a J-invariant function is used to determine the mask distribution and replace the selected pixels with local averages as part of self-supervised image denoising using blind spot networks (BSNs).", "Key words (remarks)<~>Bernoulli-sampled instances of the input image results on noisy pairs": "Bernoulli-sampled instances of the input image refers to the sets of image pairs generated by randomly masking instances of the noisy image using a Bernoulli process for training the self-supervised denoising method S2S.", "Key words (remarks)<~>Gobal-aware mask mapper, re-visible loss.": "The 'Key words (remarks)' column for reference 5 in the table indicates that the method described uses a 'global-aware mask mapper' and 're-visible loss'. This refers to a masking strategy and a loss function used in the self-supervised denoising algorithm 'Blind2Unblind (B2UB)', where the masking is based on the global structure of the image, and the loss helps to restore more details in the denoised image."}}, "title": "Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review", "abstract": "The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements."}
{"paper_id": "2308.09592v1", "_pdf_hash": null, "_source_hash": "af39e7deec84cdb7f2a7656ace93d1848a5ca84f", "_source_name": "2308.09592v1", "_table_hash": "d3a4840b-f497-4bfb-9c6a-7d2e5c7c7b0a", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid38\" id-text=\"A\" place=\"t\" rend=\"display\"><head>The inference speed of three methods. Video Training: training once for each video. Edit Training: training once for each edit. Edit Inference: inference time. The approximated cost time is tested under the video with 768 <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u00d7</mo></math></formula> 432 resolution and 70 frames in a single NVIDIA A40. For StableVideo, we pick three key frames for foreground editing.</head>\n<unexpected><resizebox width=\"427.0pt\">\n<table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\" right-border=\"true\">Method</td>\n<td halign=\"center\">Video Training</td>\n<td halign=\"center\">Edit Training</td>\n<td>Edit Inference</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">Text2LIVE<cit sha=\"6b5497c14c76c939b407232e82f8356c8724ce1d\"><ref target=\"bid14\"/></cit>{{cite:6b5497c}}</td>\n<td halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 10 hr</td>\n<td halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 1 hours</td>\n<td><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 10 sec</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">Tune-A-Video<cit sha=\"5d7ad8193f4dd03e09dc53c34a56ed7e4fe6ec7e\"><ref target=\"bid8\"/></cit>{{cite:5d7ad81}}</td>\n<td halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> -</td>\n<td halign=\"center\">30 min</td>\n<td><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 4 min</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">StableVideo (<hi rend=\"it\">ours</hi>)</td>\n<td halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 10 hr</td>\n<td halign=\"center\">-</td>\n<td><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math></formula> 30 sec</td>\n</tr></table>\n</resizebox></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Method", "Video Training", "Edit Training", "Edit Inference"], ["Text2LIVE{{cite:6b5497c}}", "\u223c 10 hr", "\u223c 1 hours", "\u223c 10 sec"], ["Tune-A-Video{{cite:5d7ad81}}", "\u223c -", "30 min", "\u223c 4 min"], ["StableVideo (ours)", "\u223c 10 hr", "-", "\u223c 30 sec"]], "table_dict": {"References": ["{{cite:6b5497c}}", "{{cite:5d7ad81}}", "-"], "Method": ["Text2LIVE", "Tune-A-Video", "StableVideo (ours)"], "Video Training": ["\u223c 10 hr", "\u223c -", "\u223c 10 hr"], "Edit Training": ["\u223c 1 hours", "30 min", "-"], "Edit Inference": ["\u223c 10 sec", "\u223c 4 min", "\u223c 30 sec"]}}, "bib_hash": ["6b5497c14c76c939b407232e82f8356c8724ce1d", "5d7ad8193f4dd03e09dc53c34a56ed7e4fe6ec7e"], "row_bib_map": [{"bib_hash_or_arxiv_id": "6b5497c14c76c939b407232e82f8356c8724ce1d", "row": 0, "corpus_id": 247996703, "type": "ref"}, {"bib_hash_or_arxiv_id": "5d7ad8193f4dd03e09dc53c34a56ed7e4fe6ec7e", "row": 1, "corpus_id": 254974187, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.09592v1", "row": 2, "corpus_id": 261031087, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"A\" id=\"uid38\" place=\"t\"><head>The inference speed of three methods. Video Training: training once for each video. Edit Training: training once for each edit. Edit Inference: inference time. The approximated cost time is tested under the video with 768 <formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u00d7</mo></math><texmath>\\times </texmath></formula> 432 resolution and 70 frames in a single NVIDIA A40. For StableVideo, we pick three key frames for foreground editing.</head>\n<unexpected><resizebox width=\"427.0pt\">\n<table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"center\">Method</cell>\n<cell halign=\"center\">Video Training</cell>\n<cell halign=\"center\">Edit Training</cell>\n<cell>Edit Inference</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">Text2LIVE\u00a0<cit sha=\"6b5497c14c76c939b407232e82f8356c8724ce1d\"><ref target=\"bid14\"/></cit>{{cite:6b5497c}}</cell>\n<cell halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 10 hr</cell>\n<cell halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 1 hours</cell>\n<cell><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 10 sec</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">Tune-A-Video\u00a0<cit sha=\"5d7ad8193f4dd03e09dc53c34a56ed7e4fe6ec7e\"><ref target=\"bid8\"/></cit>{{cite:5d7ad81}}</cell>\n<cell halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> -</cell>\n<cell halign=\"center\">30 min</cell>\n<cell><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 4 min</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">StableVideo (<hi rend=\"it\">ours</hi>)</cell>\n<cell halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 10 hr</cell>\n<cell halign=\"center\">-</cell>\n<cell><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>\u223c</mo></math><texmath>\\sim </texmath></formula> 30 sec</cell>\n</row></table>\n</resizebox></unexpected></table>", "caption": "The inference speed of three methods. Video Training: training once for each video. Edit Training: training once for each edit. Edit Inference: inference time. The approximated cost time is tested under the video with 768 \u00d7\\times  432 resolution and 70 frames in a single NVIDIA A40. For StableVideo, we pick three key frames for foreground editing.", "type": "table"}, "_table_hash_full_text": "e4c15d7c-84c4-45ba-a57a-664a937cb7da", "_full_text_table_hash": "35daceae-5f26-45a2-87f0-2ecf8bd94123", "context_autogenerated": {"glossary": {"Method<~>Method": "'Method' in the table refers to the specific video editing techniques being compared, including Text2LIVE, Tune-A-Video, and StableVideo (the last one being the method suggested by the authors of the paper).", "Method<~>Text2LIVE": "Text2LIVE is one of the methods compared in the table, which is mentioned inReference [[cite:6b5497c]] in the first row.", "Method<~>Tune-A-Video": "Tune-A-Video is a reference cited in the table, denoted as Method in the second row, representing one of the state-of-the-art video editing methods compared to StableVideo in the text.", "Method<~>StableVideo (ours)": "StableVideo (ours) refers to the text-driven video editing framework introduced in the paper named StableVideo. It is the proposed approach by the authors for consistency-aware video editing.", "Video Training<~>Video Training": "In the given context, 'Video Training' refers to training a model once for each video in the case of StableVideo and Text2LIVE methods.", "Edit Training<~>Edit Training": "Edit Training refers to the training process for each individual edit in the video, as opposed to training once for the entire video (Video Training).", "Edit Inference<~>Edit Inference": "In the given context, 'Edit Inference' refers to the time required for the video editing process during inference stage for each method mentioned in the table."}}, "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing", "abstract": "Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at this https URL."}
{"paper_id": "2308.03151v1", "_pdf_hash": null, "_source_hash": "5805ed01fdb12832273f4fecccd8c996de2100d1", "_source_name": "2308.03151v1", "_table_hash": "e0bdb237-b51f-4c8a-af12-aa5107284905", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr><td halign=\"left\">2*<hi rend=\"small\"><hi rend=\"bold\">Dataset</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\">2*<hi rend=\"small\"><hi rend=\"bold\">Image Number</hi></hi><hi rend=\"small\"/></td>\n<td cols=\"2\" halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Category</hi></hi><hi rend=\"small\"/></td>\n<td cols=\"2\" halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Annotation</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\"/>\n<td halign=\"center\"/>\n<td halign=\"center\">Number</td>\n<td halign=\"center\">Coverage</td>\n<td halign=\"left\">type</td>\n<td halign=\"center\">source</td>\n</tr><tr><td halign=\"left\">Recipe1M+ <cit sha=\"e6651ea1245ba499e84dda5c63bfaad4746a9dc0\"><ref target=\"bid16\"/></cit>{{cite:e6651ea}}</td>\n<td halign=\"center\">13M</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">-</td>\n<td halign=\"left\">Ingredients &amp; Cooking instructions</td>\n<td halign=\"center\">Web</td>\n</tr><tr><td halign=\"left\">FoodSeg103 <cit sha=\"cbb485651ecc3079193ef6adf2b4cdc81770d3fd\"><ref target=\"bid17\"/></cit>{{cite:cbb4856}}</td>\n<td halign=\"center\">7,118</td>\n<td halign=\"center\">103</td>\n<td halign=\"center\">Worldwide</td>\n<td halign=\"left\">Ingredients</td>\n<td halign=\"center\">Manual</td>\n</tr><tr><td halign=\"left\">UPMC Food-101 <cit sha=\"fd50130c5c66074ba9687d1ad9e3a28b0175d299\"><ref target=\"bid29\"/></cit>{{cite:fd50130}}</td>\n<td halign=\"center\">90,840</td>\n<td halign=\"center\">101</td>\n<td halign=\"center\">Western</td>\n<td halign=\"left\">Related web text</td>\n<td halign=\"center\">Web</td>\n</tr><tr><td halign=\"left\">UEC Food256 <cit sha=\"67a05ad6043dbbd3c6d5ca9d71ad17fd5d23f5cc\"><ref target=\"bid30\"/></cit>{{cite:67a05ad}}</td>\n<td halign=\"center\">25,088</td>\n<td halign=\"center\">256</td>\n<td halign=\"center\">Japanese</td>\n<td halign=\"left\">-</td>\n<td halign=\"center\">-</td>\n</tr><tr><td halign=\"left\">VIREO Food-172 <cit sha=\"19d40d6509d6eb7308f731556954ce87f55d72c9\"><ref target=\"bid32\"/></cit>{{cite:19d40d6}}</td>\n<td halign=\"center\">110,241</td>\n<td halign=\"center\">172</td>\n<td halign=\"center\">Chinese</td>\n<td halign=\"left\">Ingredients &amp; Cooking instructions</td>\n<td halign=\"center\">Web</td>\n</tr><tr><td halign=\"left\">Sushi-50 <cit sha=\"97286d2c960bcd207a52f3ec9801e238bf97c2af\"><ref target=\"bid31\"/></cit>{{cite:97286d2}}</td>\n<td halign=\"center\">3,963</td>\n<td halign=\"center\">50</td>\n<td halign=\"center\">Japanese</td>\n<td halign=\"left\">-</td>\n<td halign=\"center\">-</td>\n</tr><tr><td halign=\"left\">ChineseFoodNet <cit sha=\"d1f290d1e4a8ee715e8936a6324e790f02c531db\"><ref target=\"bid18\"/></cit>{{cite:d1f290d}}</td>\n<td halign=\"center\">185,628</td>\n<td halign=\"center\">208</td>\n<td halign=\"center\">Chinese</td>\n<td halign=\"left\">-</td>\n<td halign=\"center\">-</td>\n</tr><tr><td halign=\"left\">Yummly-66k <cit sha=\"799e4f51a4a5b0deb46596a117153cb60afecf0e\"><ref target=\"bid33\"/></cit>{{cite:799e4f5}}</td>\n<td halign=\"center\">66,615</td>\n<td halign=\"center\">-</td>\n<td halign=\"center\">-</td>\n<td halign=\"left\">Course &amp; ingredients &amp; region</td>\n<td halign=\"center\">Web</td>\n</tr><tr><td halign=\"left\">ISIA Food-500 <cit sha=\"e0b9f819d470ac70ef82b8151a92c9535f461ef0\"><ref target=\"bid15\"/></cit>{{cite:e0b9f81}}</td>\n<td halign=\"center\">399,726</td>\n<td halign=\"center\">500</td>\n<td halign=\"center\">Worldwide</td>\n<td halign=\"left\">-</td>\n<td halign=\"center\">-</td>\n</tr><tr><td halign=\"left\">Food-500 Cap</td>\n<td halign=\"center\">24,700</td>\n<td halign=\"center\">494</td>\n<td halign=\"center\">Worldwide</td>\n<td halign=\"left\">Image Captions &amp; region</td>\n<td halign=\"center\">Manual</td>\n</tr></table>", "table_json": {"incomplete_rows": [{"row_idx": 0, "text": "2*Dataset", "cols": 1}, {"row_idx": 0, "text": "2*Image Number", "cols": 1}, {"row_idx": 0, "text": "Category", "cols": 2}, {"row_idx": 0, "text": "Annotation", "cols": 2}], "table": [["2*Dataset-", "2*Image Number-", "Category-Number", "Category-Coverage", "Annotation-type", "Annotation-source"], ["Recipe1M+ {{cite:e6651ea}}", "13M", "-", "-", "Ingredients & Cooking instructions", "Web"], ["FoodSeg103 {{cite:cbb4856}}", "7,118", "103", "Worldwide", "Ingredients", "Manual"], ["UPMC Food-101 {{cite:fd50130}}", "90,840", "101", "Western", "Related web text", "Web"], ["UEC Food256 {{cite:67a05ad}}", "25,088", "256", "Japanese", "-", "-"], ["VIREO Food-172 {{cite:19d40d6}}", "110,241", "172", "Chinese", "Ingredients & Cooking instructions", "Web"], ["Sushi-50 {{cite:97286d2}}", "3,963", "50", "Japanese", "-", "-"], ["ChineseFoodNet {{cite:d1f290d}}", "185,628", "208", "Chinese", "-", "-"], ["Yummly-66k {{cite:799e4f5}}", "66,615", "-", "-", "Course & ingredients & region", "Web"], ["ISIA Food-500 {{cite:e0b9f81}}", "399,726", "500", "Worldwide", "-", "-"], ["Food-500 Cap", "24,700", "494", "Worldwide", "Image Captions & region", "Manual"]], "table_dict": {"References": ["{{cite:e6651ea}}", "{{cite:cbb4856}}", "{{cite:fd50130}}", "{{cite:67a05ad}}", "{{cite:19d40d6}}", "{{cite:97286d2}}", "{{cite:d1f290d}}", "{{cite:799e4f5}}", "{{cite:e0b9f81}}", "-"], "Dataset": ["Recipe1M+ ", "FoodSeg103 ", "UPMC Food-101 ", "UEC Food256 ", "VIREO Food-172 ", "Sushi-50 ", "ChineseFoodNet ", "Yummly-66k ", "ISIA Food-500 ", "Food-500 Cap"], "Image Number": ["13M", "7,118", "90,840", "25,088", "110,241", "3,963", "185,628", "66,615", "399,726", "24,700"], "Category-Number": ["-", "103", "101", "256", "172", "50", "208", "-", "500", "494"], "Category-Coverage": ["-", "Worldwide", "Western", "Japanese", "Chinese", "Japanese", "Chinese", "-", "Worldwide", "Worldwide"], "Annotation-type": ["Ingredients & Cooking instructions", "Ingredients", "Related web text", "-", "Ingredients & Cooking instructions", "-", "-", "Course & ingredients & region", "-", "Image Captions & region"], "Annotation-source": ["Web", "Manual", "Web", "-", "Web", "-", "-", "Web", "-", "Manual"]}}, "bib_hash": ["e6651ea1245ba499e84dda5c63bfaad4746a9dc0", "cbb485651ecc3079193ef6adf2b4cdc81770d3fd", "fd50130c5c66074ba9687d1ad9e3a28b0175d299", "67a05ad6043dbbd3c6d5ca9d71ad17fd5d23f5cc", "19d40d6509d6eb7308f731556954ce87f55d72c9", "97286d2c960bcd207a52f3ec9801e238bf97c2af", "d1f290d1e4a8ee715e8936a6324e790f02c531db", "799e4f51a4a5b0deb46596a117153cb60afecf0e", "e0b9f819d470ac70ef82b8151a92c9535f461ef0"], "row_bib_map": [{"bib_hash_or_arxiv_id": "e6651ea1245ba499e84dda5c63bfaad4746a9dc0", "row": 0, "corpus_id": 7319196, "type": "ref"}, {"bib_hash_or_arxiv_id": "cbb485651ecc3079193ef6adf2b4cdc81770d3fd", "row": 1, "corpus_id": 234470115, "type": "ref"}, {"bib_hash_or_arxiv_id": "fd50130c5c66074ba9687d1ad9e3a28b0175d299", "row": 2, "corpus_id": 206822288, "type": "ref"}, {"bib_hash_or_arxiv_id": "67a05ad6043dbbd3c6d5ca9d71ad17fd5d23f5cc", "row": 3, "corpus_id": 14915460, "type": "ref"}, {"bib_hash_or_arxiv_id": "19d40d6509d6eb7308f731556954ce87f55d72c9", "row": 4, "corpus_id": 207240186, "type": "ref"}, {"bib_hash_or_arxiv_id": "97286d2c960bcd207a52f3ec9801e238bf97c2af", "row": 5, "corpus_id": 211117398, "type": "ref"}, {"bib_hash_or_arxiv_id": "d1f290d1e4a8ee715e8936a6324e790f02c531db", "row": 6, "corpus_id": 37105431, "type": "ref"}, {"bib_hash_or_arxiv_id": "799e4f51a4a5b0deb46596a117153cb60afecf0e", "row": 7, "corpus_id": 3914807, "type": "ref"}, {"bib_hash_or_arxiv_id": "e0b9f819d470ac70ef82b8151a92c9535f461ef0", "row": 8, "corpus_id": 221112548, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.03151v1", "row": 9, "corpus_id": 260681484, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row><cell halign=\"left\">2*<hi rend=\"small\"><hi rend=\"bold\">Dataset</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\">2*<hi rend=\"small\"><hi rend=\"bold\">Image Number</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\" cols=\"2\"><hi rend=\"small\"><hi rend=\"bold\">Category</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\" cols=\"2\"><hi rend=\"small\"><hi rend=\"bold\">Annotation</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\"/>\n<cell halign=\"center\"/>\n<cell halign=\"center\">Number</cell>\n<cell halign=\"center\">Coverage</cell>\n<cell halign=\"left\">type</cell>\n<cell halign=\"center\">source</cell>\n</row><row><cell halign=\"left\">Recipe1M+ <cit sha=\"e6651ea1245ba499e84dda5c63bfaad4746a9dc0\"><ref target=\"bid16\"/></cit>{{cite:e6651ea}}</cell>\n<cell halign=\"center\">13M</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"left\">Ingredients &amp; Cooking instructions</cell>\n<cell halign=\"center\">Web</cell>\n</row><row><cell halign=\"left\">FoodSeg103 <cit sha=\"cbb485651ecc3079193ef6adf2b4cdc81770d3fd\"><ref target=\"bid17\"/></cit>{{cite:cbb4856}}</cell>\n<cell halign=\"center\">7,118</cell>\n<cell halign=\"center\">103</cell>\n<cell halign=\"center\">Worldwide</cell>\n<cell halign=\"left\">Ingredients</cell>\n<cell halign=\"center\">Manual</cell>\n</row><row><cell halign=\"left\">UPMC Food-101 <cit sha=\"fd50130c5c66074ba9687d1ad9e3a28b0175d299\"><ref target=\"bid29\"/></cit>{{cite:fd50130}}</cell>\n<cell halign=\"center\">90,840</cell>\n<cell halign=\"center\">101</cell>\n<cell halign=\"center\">Western</cell>\n<cell halign=\"left\">Related web text</cell>\n<cell halign=\"center\">Web</cell>\n</row><row><cell halign=\"left\">UEC Food256 <cit sha=\"67a05ad6043dbbd3c6d5ca9d71ad17fd5d23f5cc\"><ref target=\"bid30\"/></cit>{{cite:67a05ad}}</cell>\n<cell halign=\"center\">25,088</cell>\n<cell halign=\"center\">256</cell>\n<cell halign=\"center\">Japanese</cell>\n<cell halign=\"left\">-</cell>\n<cell halign=\"center\">-</cell>\n</row><row><cell halign=\"left\">VIREO Food-172 <cit sha=\"19d40d6509d6eb7308f731556954ce87f55d72c9\"><ref target=\"bid32\"/></cit>{{cite:19d40d6}}</cell>\n<cell halign=\"center\">110,241</cell>\n<cell halign=\"center\">172</cell>\n<cell halign=\"center\">Chinese</cell>\n<cell halign=\"left\">Ingredients &amp; Cooking instructions</cell>\n<cell halign=\"center\">Web</cell>\n</row><row><cell halign=\"left\">Sushi-50 <cit sha=\"97286d2c960bcd207a52f3ec9801e238bf97c2af\"><ref target=\"bid31\"/></cit>{{cite:97286d2}}</cell>\n<cell halign=\"center\">3,963</cell>\n<cell halign=\"center\">50</cell>\n<cell halign=\"center\">Japanese</cell>\n<cell halign=\"left\">-</cell>\n<cell halign=\"center\">-</cell>\n</row><row><cell halign=\"left\">ChineseFoodNet <cit sha=\"d1f290d1e4a8ee715e8936a6324e790f02c531db\"><ref target=\"bid18\"/></cit>{{cite:d1f290d}}</cell>\n<cell halign=\"center\">185,628</cell>\n<cell halign=\"center\">208</cell>\n<cell halign=\"center\">Chinese</cell>\n<cell halign=\"left\">-</cell>\n<cell halign=\"center\">-</cell>\n</row><row><cell halign=\"left\">Yummly-66k <cit sha=\"799e4f51a4a5b0deb46596a117153cb60afecf0e\"><ref target=\"bid33\"/></cit>{{cite:799e4f5}}</cell>\n<cell halign=\"center\">66,615</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"center\">-</cell>\n<cell halign=\"left\">Course &amp; ingredients &amp; region</cell>\n<cell halign=\"center\">Web</cell>\n</row><row><cell halign=\"left\">ISIA Food-500 <cit sha=\"e0b9f819d470ac70ef82b8151a92c9535f461ef0\"><ref target=\"bid15\"/></cit>{{cite:e0b9f81}}</cell>\n<cell halign=\"center\">399,726</cell>\n<cell halign=\"center\">500</cell>\n<cell halign=\"center\">Worldwide</cell>\n<cell halign=\"left\">-</cell>\n<cell halign=\"center\">-</cell>\n</row><row><cell halign=\"left\">Food-500 Cap</cell>\n<cell halign=\"center\">24,700</cell>\n<cell halign=\"center\">494</cell>\n<cell halign=\"center\">Worldwide</cell>\n<cell halign=\"left\">Image Captions &amp; region</cell>\n<cell halign=\"center\">Manual</cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "286ec480-7a24-4385-a36e-5b211c8d67a6", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table, \"Dataset\" refers to the name of the specific food dataset mentioned in each row, such as Recipe1M+ or FoodSeg103.", "Dataset<~>Recipe1M+": "Recipe1M+ in the table refers to a food dataset with numerous images and recipes suitable for image-recipe retrieval tasks, as mentioned in the text.", "Dataset<~>FoodSeg103": "FoodSeg103 is a food image segmentation dataset that tags each image with multiple ingredients and draws the corresponding pixel-wise masks.", "Dataset<~>UPMC Food-101": "UPMC Food-101 refers to a food dataset proposed for food image classification with additional metadata such as related web text, ingredients, and cooking instructions.", "Dataset<~>UEC Food256": "UEC Food256 in the table refers to the food dataset proposed for food image classification with 256 food categories.", "Dataset<~>VIREO Food-172": "VIREO Food-172 in the table refers to a food dataset that contains 172 food categories with corresponding images.", "Dataset<~>Sushi-50": "Sushi-50 in the table refers to the Sushi-50 food dataset, which focuses on Japanese food images.", "Dataset<~>ChineseFoodNet": "ChineseFoodNet in the table refers to a food dataset annotated with food categories that focuses on Chinese food.", "Dataset<~>Yummly-66k": "Yummly-66k refers to a food dataset annotated with ingredients, courses, and regions for food topic models.", "Dataset<~>ISIA Food-500": "ISIA Food-500 is a food dataset referenced by [[e0b9f81]].", "Dataset<~>Food-500 Cap": "Food-500 Cap is a food image-caption dataset, where each image is accompanied by a detailed caption describing fine-grained visual content of the image, allowing VLM models to align images and texts effectively.", "Image Number<~>Image Number": "In the context of the table, 'Image Number' refers to the total number of images in each dataset as listed in the references.", "Category-Number<~>Category-Number": "\"Category-Number\" refers to the number of categories in the corresponding food dataset mentioned by the citation.", "Category-Coverage<~>Category-Coverage": "In the context of the given table, 'Category-Coverage' refers to the geographic origin of the food categories covered in each dataset indicated by their corresponding references.", "Category-Coverage<~>Worldwide": "In the context of the table, 'Worldwide' in the column 'Category-Coverage' for references [[e0b9f81]] and [[-]] refers to food categories from various geographic regions.", "Category-Coverage<~>Western": "The term \"Western\" in the 'Category-Coverage' column of the table refers to the food datasets that primarily cover western food categories.", "Category-Coverage<~>Japanese": "In the context of the table, 'Japanese' in the column 'Category-Coverage' refers to the food datasets that specifically cover Japanese food categories.", "Category-Coverage<~>Chinese": "The 'Chinese' in the column 'Category-Coverage' refers to the food databases that cover Chinese food categories.", "Annotation-type<~>Annotation-type": "In the context of the table, 'Annotation-type' refers to the type of information associated with each food dataset reference, such as ingredients, cooking instructions, related web text, or image captions.", "Annotation-type<~>Ingredients & Cooking instructions": "In the context of the table, 'Ingredients & Cooking instructions' in the 'Annotation-type' column refers to datasets that contain information about the ingredients and cooking instructions related to the images.", "Annotation-type<~>Ingredients": "In the context of the table, 'Ingredients' in the 'Annotation-type' column refers to datasets that contain information about the ingredients present in the food image.", "Annotation-type<~>Related web text": "In the context of the table, 'Related web text' in the 'Annotation-type' column refers to textual metadata that accompanies images in some food datasets, providing additional context or information.", "Annotation-type<~>Course & ingredients & region": "In the context of the table, 'Course & ingredients & region' in the column 'Annotation-type' refers to annotations that include information about the course, ingredients, and geographic region associated with the food image.", "Annotation-type<~>Image Captions & region": "The term 'Image Captions & region' in the 'Annotation-type' column of the table refers to food image-caption pairs where each image is described by a detailed caption that includes fine-grained visual attributes of the food, such as shape, color, and ingredient, as well as the geographic origin of the food. (Food-500 Cap dataset described in the given text)", "Annotation-source<~>Annotation-source": "'Annotation-source' in this context refers to where the food dataset annotations, specifically the citations, were obtained from. The table indicates whether the annotations were sourced from the web or were manually created.", "Annotation-source<~>Web": "In the context of the table, 'Web' in the 'Annotation-source' column refers to the source of the textual annotations being obtained from the web.", "Annotation-source<~>Manual": "In the context of the table, 'Manual' in the column 'Annotation-source' refers to annotations that were manually added or created by human annotators."}}, "title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models", "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue."}
{"paper_id": "2308.03258v1", "_pdf_hash": null, "_source_hash": "26c5fe7c33d24a163e1381c4c37fdd4e210a734e", "_source_name": "2308.03258v1", "_table_hash": "9a34d702-e100-4c93-b280-35b662fc1cbe", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr><td halign=\"left\" right-border=\"true\">Defense Method</td>\n<td halign=\"left\" right-border=\"true\">Type</td>\n<td halign=\"left\">Time Cost</td>\n<td>Description</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">Standard</td>\n<td halign=\"left\" right-border=\"true\">4*Data augmentations</td>\n<td halign=\"left\">Low</td>\n<td>Random image cropping and flipping</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">CutOut<cit sha=\"401c9eff85fb4a8b8b4fdfa18023550fbebfded4\"><ref target=\"bid24\"/></cit>{{cite:401c9ef}}</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">Low</td>\n<td>Random image erasing</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">MixUp<cit sha=\"01545b6a8afa3b8e3be1dba5a5dba35ae9a68e34\"><ref target=\"bid26\"/></cit>{{cite:01545b6}}</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">Low</td>\n<td>Random image blending</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">CutMix<cit sha=\"bb672ed9b35a0ef2c6004398687964331e2b4cf8\"><ref target=\"bid25\"/></cit>{{cite:bb672ed}}</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">Low</td>\n<td>Random image cutting and stitching</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">Gaussian (used in<cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</td>\n<td halign=\"left\" right-border=\"true\">5*Data preprocessing</td>\n<td halign=\"left\">Low</td>\n<td>Image blurring with a Gaussian kernel</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">BDR (used in<cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">Low</td>\n<td>Bit-depth reduction</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">Gray(used in <cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">Low</td>\n<td>Image grayscale transformation</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">JPEG(used in <cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">Low</td>\n<td>Image compression</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">AVATAR<cit sha=\"8efa7c360ad3a59b806714d667cbe16e2e5a0c6d\"><ref target=\"bid20\"/></cit>{{cite:8efa7c3}}</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">High</td>\n<td>Image corruption and restoration</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">U-Lite<cit sha=\"c89b242d22c52c5dfa79a8f3ee52f346731520d1\"><ref target=\"bid19\"/></cit>{{cite:c89b242}}</td>\n<td halign=\"left\" right-border=\"true\">3*Training-phase defense</td>\n<td halign=\"left\">Low</td>\n<td>Stronger data augmentations</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">U-Max<cit sha=\"c89b242d22c52c5dfa79a8f3ee52f346731520d1\"><ref target=\"bid19\"/></cit>{{cite:c89b242}}</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">High</td>\n<td>Adversarial augmentations</td>\n</tr><tr><td halign=\"left\" right-border=\"true\">AT<cit sha=\"5437669c43abbb22403f84917671e44a916b8d56\"><ref target=\"bid33\"/></cit>{{cite:5437669}}</td>\n<td halign=\"left\" right-border=\"true\"/>\n<td halign=\"left\">High</td>\n<td>Adversarial training</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Defense Method", "Type", "Time Cost", "Description"], ["Standard", "4*Data augmentations", "Low", "Random image cropping and flipping"], ["CutOut{{cite:401c9ef}}", "", "Low", "Random image erasing"], ["MixUp{{cite:01545b6}}", "", "Low", "Random image blending"], ["CutMix{{cite:bb672ed}}", "", "Low", "Random image cutting and stitching"], ["Gaussian (used in{{cite:ae7c4fc}})", "5*Data preprocessing", "Low", "Image blurring with a Gaussian kernel"], ["BDR (used in{{cite:ae7c4fc}})", "", "Low", "Bit-depth reduction"], ["Gray(used in {{cite:ae7c4fc}})", "", "Low", "Image grayscale transformation"], ["JPEG(used in {{cite:ae7c4fc}})", "", "Low", "Image compression"], ["AVATAR{{cite:8efa7c3}}", "", "High", "Image corruption and restoration"], ["U-Lite{{cite:c89b242}}", "3*Training-phase defense", "Low", "Stronger data augmentations"], ["U-Max{{cite:c89b242}}", "", "High", "Adversarial augmentations"], ["AT{{cite:5437669}}", "", "High", "Adversarial training"]], "table_dict": {"References": ["-", "{{cite:401c9ef}}", "{{cite:01545b6}}", "{{cite:bb672ed}}", "{{cite:ae7c4fc}}", "{{cite:ae7c4fc}}", "{{cite:ae7c4fc}}", "{{cite:ae7c4fc}}", "{{cite:8efa7c3}}", "{{cite:c89b242}}", "{{cite:c89b242}}", "{{cite:5437669}}"], "Defense Method": ["Standard", "CutOut ", "MixUp ", "CutMix ", "Gaussian (used in )", "BDR (used in )", "Gray (used in )", "JPEG (used in )", "AVATAR ", "U-Lite ", "U-Max ", "AT "], "Type": ["Data augmentations", "Data augmentations", "Data augmentations", "Data augmentations", "Data preprocessing", "Data preprocessing", "Data preprocessing", "Data preprocessing", "Data preprocessing", "Training-phase defense", "Training-phase defense", "Training-phase defense"], "Time Cost": ["Low", "Low", "Low", "Low", "Low", "Low", "Low", "Low", "High", "Low", "High", "High"], "Description": ["Random image cropping and flipping", "Random image erasing", "Random image blending", "Random image cutting and stitching", "Image blurring with a Gaussian kernel", "Bit-depth reduction", "Image grayscale transformation", "Image compression", "Image corruption and restoration", "Stronger data augmentations", "Adversarial augmentations", "Adversarial training"]}}, "bib_hash": ["401c9eff85fb4a8b8b4fdfa18023550fbebfded4", "01545b6a8afa3b8e3be1dba5a5dba35ae9a68e34", "bb672ed9b35a0ef2c6004398687964331e2b4cf8", "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "8efa7c360ad3a59b806714d667cbe16e2e5a0c6d", "c89b242d22c52c5dfa79a8f3ee52f346731520d1", "c89b242d22c52c5dfa79a8f3ee52f346731520d1", "5437669c43abbb22403f84917671e44a916b8d56"], "row_bib_map": [{"bib_hash_or_arxiv_id": "401c9eff85fb4a8b8b4fdfa18023550fbebfded4", "row": 1, "corpus_id": 23714201, "type": "ref"}, {"bib_hash_or_arxiv_id": "01545b6a8afa3b8e3be1dba5a5dba35ae9a68e34", "row": 2, "corpus_id": 3162051, "type": "ref"}, {"bib_hash_or_arxiv_id": "bb672ed9b35a0ef2c6004398687964331e2b4cf8", "row": 3, "corpus_id": 152282661, "type": "ref"}, {"bib_hash_or_arxiv_id": "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "row": 4, "corpus_id": 256416324, "type": "ref"}, {"bib_hash_or_arxiv_id": "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "row": 5, "corpus_id": 256416324, "type": "ref"}, {"bib_hash_or_arxiv_id": "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "row": 6, "corpus_id": 256416324, "type": "ref"}, {"bib_hash_or_arxiv_id": "ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab", "row": 7, "corpus_id": 256416324, "type": "ref"}, {"bib_hash_or_arxiv_id": "8efa7c360ad3a59b806714d667cbe16e2e5a0c6d", "row": 8, "corpus_id": 257532941, "type": "ref"}, {"bib_hash_or_arxiv_id": "c89b242d22c52c5dfa79a8f3ee52f346731520d1", "row": 9, "corpus_id": 257766788, "type": "ref"}, {"bib_hash_or_arxiv_id": "c89b242d22c52c5dfa79a8f3ee52f346731520d1", "row": 10, "corpus_id": 257766788, "type": "ref"}, {"bib_hash_or_arxiv_id": "5437669c43abbb22403f84917671e44a916b8d56", "row": 11, "corpus_id": 3488815, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.03258v1", "row": 0, "corpus_id": 260680386, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"left\">Defense Method</cell>\n<cell right-border=\"true\" halign=\"left\">Type</cell>\n<cell halign=\"left\">Time Cost</cell>\n<cell>Description</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">Standard</cell>\n<cell right-border=\"true\" halign=\"left\">4*Data augmentations</cell>\n<cell halign=\"left\">Low</cell>\n<cell>Random image cropping and flipping</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">CutOut\u00a0<cit sha=\"401c9eff85fb4a8b8b4fdfa18023550fbebfded4\"><ref target=\"bid24\"/></cit>{{cite:401c9ef}}</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">Low</cell>\n<cell>Random image erasing</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">MixUp\u00a0<cit sha=\"01545b6a8afa3b8e3be1dba5a5dba35ae9a68e34\"><ref target=\"bid26\"/></cit>{{cite:01545b6}}</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">Low</cell>\n<cell>Random image blending</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">CutMix\u00a0<cit sha=\"bb672ed9b35a0ef2c6004398687964331e2b4cf8\"><ref target=\"bid25\"/></cit>{{cite:bb672ed}}</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">Low</cell>\n<cell>Random image cutting and stitching</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">Gaussian (used in\u00a0<cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\n<cell right-border=\"true\" halign=\"left\">5*Data preprocessing</cell>\n<cell halign=\"left\">Low</cell>\n<cell>Image blurring with a Gaussian kernel</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">BDR (used in\u00a0<cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">Low</cell>\n<cell>Bit-depth reduction</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">Gray\u00a0(used in <cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">Low</cell>\n<cell>Image grayscale transformation</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">JPEG\u00a0(used in <cit sha=\"ae7c4fc071b5c76d3da66bad07c5ccf943ec0bab\"><ref target=\"bid18\"/></cit>{{cite:ae7c4fc}})</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">Low</cell>\n<cell>Image compression</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">AVATAR\u00a0<cit sha=\"8efa7c360ad3a59b806714d667cbe16e2e5a0c6d\"><ref target=\"bid20\"/></cit>{{cite:8efa7c3}}</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">High</cell>\n<cell>Image corruption and restoration</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">U-Lite\u00a0<cit sha=\"c89b242d22c52c5dfa79a8f3ee52f346731520d1\"><ref target=\"bid19\"/></cit>{{cite:c89b242}}</cell>\n<cell right-border=\"true\" halign=\"left\">3*Training-phase defense</cell>\n<cell halign=\"left\">Low</cell>\n<cell>Stronger data augmentations</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">U-Max\u00a0<cit sha=\"c89b242d22c52c5dfa79a8f3ee52f346731520d1\"><ref target=\"bid19\"/></cit>{{cite:c89b242}}</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">High</cell>\n<cell>Adversarial augmentations</cell>\n</row><row><cell right-border=\"true\" halign=\"left\">AT\u00a0<cit sha=\"5437669c43abbb22403f84917671e44a916b8d56\"><ref target=\"bid33\"/></cit>{{cite:5437669}}</cell>\n<cell right-border=\"true\" halign=\"left\"/>\n<cell halign=\"left\">High</cell>\n<cell>Adversarial training</cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "31c3c548-50f8-4775-94d1-36cedf85de2f", "context_autogenerated": {"glossary": {"Defense Method<~>Defense Method": "In the context of the given table, 'Defense Method' refers to specific techniques used to mitigate availability poisoning attacks mentioned in the corresponding references.", "Defense Method<~>Standard": "'Standard' in the 'Defense Method' column likely refers to no defense method being applied or a baseline comparison without any defense.", "Defense Method<~>CutOut": "CutOut is a defense method that introduces random rectangular masks of varying sizes into the training images during training to make the model more robust against adversarial attacks.", "Defense Method<~>MixUp": "MixUp is a defense method that involves interpolating between two examples during training to make models more robust to adversarial attacks.", "Defense Method<~>CutMix": "CutMix is a defense method mentioned in the table that refers to a data augmentation technique used in image classification tasks, which involves cutting out one image and pasting it onto another image, with some random cropping and horizontal flipping, to augment the training dataset and improve model robustness.", "Defense Method<~>Gaussian (used in )": "The mention of \"Gaussian (used in)\" in the 'Defense Method' column refers to the Gaussian noise defense method in the context of APBench benchmark. This defense method is also identified by its associated citation, {{cite:ae7c4fc}}. However, the specific implementation or usage of the Gaussian noise defense is not provided in the text.", "Defense Method<~>BDR (used in )": "BDR referred to in the table is likely an abbreviation for a specific defense method used in the APBench benchmark, but without more context it is unclear what that method is named.", "Defense Method<~>Gray (used in )": "The term 'Gray (used in)' in the 'Defense Method' column of the table refers to the use of the Gray defense method in the specific defense algorithm entry of the row. However, it is not explicitly stated in the text which defense algorithm uses the Gray defense method, and therefore, it is unclear without additional context which defense algorithm this entry corresponds to.", "Defense Method<~>JPEG (used in )": "In the context of the table, 'JPEG (used in)' in the 'Defense Method' column refers to the use of JPEG compression as a defense method against availability poisoning attacks.", "Defense Method<~>AVATAR": "AVATAR in the Defense Method column of the table refers to a specific defense algorithm, but without access to the text in \"app:methods\", it's not clear what exactly AVATAR stands for or what its methodology entails.", "Defense Method<~>U-Lite": "U-Lite in the column 'Defense Method' refers to a specific defense algorithm that is identified by the reference [[c89b242]]. The name of the defense method is not explicitly stated in the text, so it is not clear what 'U-Lite' stands for without additional context.", "Defense Method<~>U-Max": "U-Max in the table refers to U-Lite Max, a defense method mentioned in the text.", "Defense Method<~>AT": "In the context of the table, 'AT' in the column 'Defense Method' likely refers to AutoAugment or another data augmentation technique named \"AT\" mentioned in the paper, but without more context it is unclear which one specifically.", "Type<~>Type": "In the context of the given table, 'Type' refers to the category or methodology of each attack or defense algorithm mentioned in the references. In this case, all the listed references are for data augmentation or preprocessing methods or training-phase defenses.", "Type<~>Data augmentations": "In the context of the table, 'Data augmentations' in the column 'Type' refer to techniques used to artificially increase the size and variability of a dataset by applying transformations, rotations, flips, or other modifications to the existing data, without altering the fundamental information.", "Type<~>Data preprocessing": "In the context of the table, 'Data preprocessing' in the column 'Type' refers to techniques applied to the data before training a model.", "Type<~>Training-phase defense": "In the given table, the column 'Type' with the values 'Training-phase defense' refers to defense methods that mitigate data poisoning during the training phase of machine learning models.", "Time Cost<~>Time Cost": "In the context of the table, 'Time Cost' refers to the time required to implement or apply each specific availability poisoning attack or defense algorithm mentioned in the text.", "Time Cost<~>Low": "In the context of the table, 'Low' in the column 'Time Cost' refers to a relatively low cost or time requirement for each defense algorithm.", "Time Cost<~>High": "In the context of the table, 'High' in the column 'Time Cost' refers to methods that have a higher computational cost or time requirement compared to other methods listed.", "Description<~>Description": "The 'Description' column in the table refers to a brief summary of the type of data augmentation techniques mentioned in the corresponding row, as cited in the given references.", "Description<~>Random image cropping and flipping": "Random image cropping and flipping refers to randomly cropping and horizontally flipping images in the dataset during the data augmentation process.", "Description<~>Random image erasing": "Random image erasing refers to a type of data augmentation technique where random regions of an image are erased or masked out.", "Description<~>Random image blending": "'Random image blending' refers to a data augmentation technique where pixels from different images are blended together at random locations in the target image.", "Description<~>Random image cutting and stitching": "Random image cutting and stitching refers to randomly cutting parts of an image and then stitching them back together to create new data modifications in the context of the given table.", "Description<~>Image blurring with a Gaussian kernel": "The term 'Image blurring with a Gaussian kernel' in the given table refers to the application of a Gaussian filter to an image for introducing blurring or smoothing effect. This image augmentation technique is used to make the model learn more robust features by adding some level of randomness and noise to the images.", "Description<~>Bit-depth reduction": "Bit-depth reduction in the context of the table refers to reducing the number of bits used to represent the pixel values in an image.", "Description<~>Image grayscale transformation": "'Image grayscale transformation' in the table refers to the conversion of an image from its original color format to grayscale.", "Description<~>Image compression": "In the context of the table, 'Image compression' in the column 'Description' refers to reducing the size of an image by removing or compressing data without significantly affecting its perceptual quality.", "Description<~>Image corruption and restoration": "In the context of the table, 'Image corruption and restoration' in the column 'Description' refers to techniques that involve corrupting and then restoring an image, which can be used for various purposes such as data augmentation or as part of adversarial attacks.", "Description<~>Stronger data augmentations": "'Stronger data augmentations' in the table refers to unspecified advanced data augmentation techniques not further described in the text.", "Description<~>Adversarial augmentations": "Adversarial augmentations in the table refer to data augmentation techniques that involve generating adversarial examples, which are deliberately crafted input data with the intention of misleading a model during training. Unlike traditional augmentation methods, adversarial augmentations are specifically designed to cause model errors, making them relevant to the context of availability poisoning attacks and defenses.", "Description<~>Adversarial training": "Adversarial training refers to a defense method used in the context of APBench to generate robust models by training on adversarially perturbed data to improve the model's ability to resist availability poisoning attacks."}}, "title": "APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses", "abstract": "The efficacy of availability poisoning, a method of poisoning data by injecting imperceptible perturbations to prevent its use in model training, has been a hot subject of investigation. Previous research suggested that it was difficult to effectively counteract such poisoning attacks. However, the introduction of various defense methods has challenged this notion. Due to the rapid progress in this field, the performance of different novel methods cannot be accurately validated due to variations in experimental setups. To further evaluate the attack and defense capabilities of these poisoning methods, we have developed a benchmark -- APBench for assessing the efficacy of adversarial poisoning. APBench consists of 9 state-of-the-art availability poisoning attacks, 8 defense algorithms, and 4 conventional data augmentation techniques. We also have set up experiments with varying different poisoning ratios, and evaluated the attacks on multiple datasets and their transferability across model architectures. We further conducted a comprehensive evaluation of 2 additional attacks specifically targeting unsupervised models. Our results reveal the glaring inadequacy of existing attacks in safeguarding individual privacy. APBench is open source and available to the deep learning community: https://github.com/lafeat/apbench."}
{"paper_id": "2308.10737v1", "_pdf_hash": null, "_source_hash": "1bbe90984174d3b8b0a876f295b069aacb64cc1f", "_source_name": "2308.10737v1", "_table_hash": "a4751f48-857a-4785-ad8a-e690dd719028", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr><td halign=\"center\" right-border=\"true\"><hi rend=\"small\"><hi rend=\"bold\">Model</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Input</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Edge scorer</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Sparsifier</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Processor</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Regularizers</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Unsupervised Losses</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"center\" right-border=\"true\">GLCN<cit sha=\"a81495aa8567ea75eaa70c33a0ba9ba868e25d67\"><ref target=\"bid28\"/></cit>{{cite:a81495a}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">MLP</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">activation</td>\n<td halign=\"center\">sparse-connect, closeness, log-barrier</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">JLGCN<cit sha=\"2504eac7d07e7c0c8ae4c31e0e7aaace42021bdd\"><ref target=\"bid29\"/></cit>{{cite:2504eac}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">MLP</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">activation</td>\n<td halign=\"center\">smoothness</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">DGCNN<cit sha=\"df19576d9763622c410ef702b227476ba539d7d0\"><ref target=\"bid30\"/></cit>{{cite:df19576}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">MLP</td>\n<td halign=\"center\">kNN</td>\n<td halign=\"center\">activation</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">LDS<cit sha=\"42d0f04ddc897ab7c83702096c833e8a4bab8dc8\"><ref target=\"bid31\"/></cit>{{cite:42d0f04}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">FP</td>\n<td halign=\"center\">Bernoulli</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">IDGL<cit sha=\"7801bf7d614ea24e66569be71142fc40d9494863\"><ref target=\"bid32\"/></cit>{{cite:7801bf7}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">ATT</td>\n<td halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>\u03f5</mi></math></formula>NN</td>\n<td halign=\"center\">activation</td>\n<td halign=\"center\">sparse-connect, log-barrier</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">Graph-Bert<cit sha=\"6fa516df519c55001f2514d7f0557e7c6a30632d\"><ref target=\"bid14\"/></cit>{{cite:6fa516d}}</td>\n<td halign=\"center\">features, WL and spectral</td>\n<td halign=\"center\">MLP</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">activation</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">GRCN<cit sha=\"509d1471ba0cd5a849191666bcfa2ab036526961\"><ref target=\"bid33\"/></cit>{{cite:509d147}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">MLP</td>\n<td halign=\"center\">kNN</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">SLAPS<cit sha=\"eb0e320e869d1da3aa9a0a8376a6938e4e0c6182\"><ref target=\"bid27\"/></cit>{{cite:eb0e320}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">FP, MLP, ATT</td>\n<td halign=\"center\">kNN</td>\n<td halign=\"center\">activation-sym</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">denoising</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">SUBLIME<cit sha=\"eb5b1c008a6e0e46f1350ee6ec419a9900e71565\"><ref target=\"bid34\"/></cit>{{cite:eb5b1c0}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">FP, MLP, ATT</td>\n<td halign=\"center\">kNN</td>\n<td halign=\"center\">activation-sym</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">contrastive</td>\n</tr><tr><td halign=\"center\" right-border=\"true\">VIB-GSL<cit sha=\"200bf566e4af1f53b999242d2551c445d9a2bf82\"><ref target=\"bid21\"/></cit>{{cite:200bf56}}</td>\n<td halign=\"center\">features</td>\n<td halign=\"center\">MLP</td>\n<td halign=\"center\">Bernoulli</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">none</td>\n<td halign=\"center\">denoising</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Model", "Input", "Edge scorer", "Sparsifier", "Processor", "Regularizers", "Unsupervised Losses"], ["GLCN{{cite:a81495a}}", "features", "MLP", "none", "activation", "sparse-connect, closeness, log-barrier", "none"], ["JLGCN{{cite:2504eac}}", "features", "MLP", "none", "activation", "smoothness", "none"], ["DGCNN{{cite:df19576}}", "features", "MLP", "kNN", "activation", "none", "none"], ["LDS{{cite:42d0f04}}", "features", "FP", "Bernoulli", "none", "none", "none"], ["IDGL{{cite:7801bf7}}", "features", "ATT", "\u03f5NN", "activation", "sparse-connect, log-barrier", "none"], ["Graph-Bert{{cite:6fa516d}}", "features, WL and spectral", "MLP", "none", "activation", "none", "none"], ["GRCN{{cite:509d147}}", "features", "MLP", "kNN", "none", "none", "none"], ["SLAPS{{cite:eb0e320}}", "features", "FP, MLP, ATT", "kNN", "activation-sym", "none", "denoising"], ["SUBLIME{{cite:eb5b1c0}}", "features", "FP, MLP, ATT", "kNN", "activation-sym", "none", "contrastive"], ["VIB-GSL{{cite:200bf56}}", "features", "MLP", "Bernoulli", "none", "none", "denoising"]], "table_dict": {"References": ["{{cite:a81495a}}", "{{cite:2504eac}}", "{{cite:df19576}}", "{{cite:42d0f04}}", "{{cite:7801bf7}}", "{{cite:6fa516d}}", "{{cite:509d147}}", "{{cite:eb0e320}}", "{{cite:eb5b1c0}}", "{{cite:200bf56}}"], "Model": ["GLCN", "JLGCN", "DGCNN", "LDS", "IDGL", "Graph-Bert", "GRCN", "SLAPS", "SUBLIME", "VIB-GSL"], "Input": ["features", "features", "features", "features", "features", "features, WL and spectral", "features", "features", "features", "features"], "Edge scorer": ["MLP", "MLP", "MLP", "FP", "ATT", "MLP", "MLP", "FP, MLP, ATT", "FP, MLP, ATT", "MLP"], "Sparsifier": ["none", "none", "kNN", "Bernoulli", "\u03f5NN", "none", "kNN", "kNN", "kNN", "Bernoulli"], "Processor": ["activation", "activation", "activation", "none", "activation", "activation", "none", "activation-sym", "activation-sym", "none"], "Regularizers": ["sparse-connect, closeness, log-barrier", "smoothness", "none", "none", "sparse-connect, log-barrier", "none", "none", "none", "none", "none"], "Unsupervised Losses": ["none", "none", "none", "none", "none", "none", "none", "denoising", "contrastive", "denoising"]}}, "bib_hash": ["a81495aa8567ea75eaa70c33a0ba9ba868e25d67", "2504eac7d07e7c0c8ae4c31e0e7aaace42021bdd", "df19576d9763622c410ef702b227476ba539d7d0", "42d0f04ddc897ab7c83702096c833e8a4bab8dc8", "7801bf7d614ea24e66569be71142fc40d9494863", "6fa516df519c55001f2514d7f0557e7c6a30632d", "509d1471ba0cd5a849191666bcfa2ab036526961", "eb0e320e869d1da3aa9a0a8376a6938e4e0c6182", "eb5b1c008a6e0e46f1350ee6ec419a9900e71565", "200bf566e4af1f53b999242d2551c445d9a2bf82"], "row_bib_map": [{"bib_hash_or_arxiv_id": "a81495aa8567ea75eaa70c33a0ba9ba868e25d67", "row": 0, "corpus_id": 195497272, "type": "ref"}, {"bib_hash_or_arxiv_id": "2504eac7d07e7c0c8ae4c31e0e7aaace42021bdd", "row": 1, "corpus_id": 202558560, "type": "ref"}, {"bib_hash_or_arxiv_id": "df19576d9763622c410ef702b227476ba539d7d0", "row": 2, "corpus_id": 94822, "type": "ref"}, {"bib_hash_or_arxiv_id": "42d0f04ddc897ab7c83702096c833e8a4bab8dc8", "row": 3, "corpus_id": 85543335, "type": "ref"}, {"bib_hash_or_arxiv_id": "7801bf7d614ea24e66569be71142fc40d9494863", "row": 4, "corpus_id": 214003631, "type": "ref"}, {"bib_hash_or_arxiv_id": "6fa516df519c55001f2514d7f0557e7c6a30632d", "row": 5, "corpus_id": 210698881, "type": "ref"}, {"bib_hash_or_arxiv_id": "509d1471ba0cd5a849191666bcfa2ab036526961", "row": 6, "corpus_id": 208139383, "type": "ref"}, {"bib_hash_or_arxiv_id": "eb0e320e869d1da3aa9a0a8376a6938e4e0c6182", "row": 7, "corpus_id": 231855665, "type": "ref"}, {"bib_hash_or_arxiv_id": "eb5b1c008a6e0e46f1350ee6ec419a9900e71565", "row": 8, "corpus_id": 246015780, "type": "ref"}, {"bib_hash_or_arxiv_id": "200bf566e4af1f53b999242d2551c445d9a2bf82", "row": 9, "corpus_id": 245219275, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row><cell right-border=\"true\" halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Model</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Input</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Edge scorer</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Sparsifier</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Processor</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Regularizers</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Unsupervised Losses</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell right-border=\"true\" halign=\"center\">GLCN\u00a0<cit sha=\"a81495aa8567ea75eaa70c33a0ba9ba868e25d67\"><ref target=\"bid28\"/></cit>{{cite:a81495a}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">MLP</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">activation</cell>\n<cell halign=\"center\">sparse-connect, closeness, log-barrier</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">JLGCN\u00a0<cit sha=\"2504eac7d07e7c0c8ae4c31e0e7aaace42021bdd\"><ref target=\"bid29\"/></cit>{{cite:2504eac}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">MLP</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">activation</cell>\n<cell halign=\"center\">smoothness</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">DGCNN\u00a0<cit sha=\"df19576d9763622c410ef702b227476ba539d7d0\"><ref target=\"bid30\"/></cit>{{cite:df19576}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">MLP</cell>\n<cell halign=\"center\">kNN</cell>\n<cell halign=\"center\">activation</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">LDS\u00a0<cit sha=\"42d0f04ddc897ab7c83702096c833e8a4bab8dc8\"><ref target=\"bid31\"/></cit>{{cite:42d0f04}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">FP</cell>\n<cell halign=\"center\">Bernoulli</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">IDGL\u00a0<cit sha=\"7801bf7d614ea24e66569be71142fc40d9494863\"><ref target=\"bid32\"/></cit>{{cite:7801bf7}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">ATT</cell>\n<cell halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>\u03f5</mi></math><texmath>\\epsilon </texmath></formula>NN</cell>\n<cell halign=\"center\">activation</cell>\n<cell halign=\"center\">sparse-connect, log-barrier</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">Graph-Bert\u00a0<cit sha=\"6fa516df519c55001f2514d7f0557e7c6a30632d\"><ref target=\"bid14\"/></cit>{{cite:6fa516d}}</cell>\n<cell halign=\"center\">features, WL and spectral</cell>\n<cell halign=\"center\">MLP</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">activation</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">GRCN\u00a0<cit sha=\"509d1471ba0cd5a849191666bcfa2ab036526961\"><ref target=\"bid33\"/></cit>{{cite:509d147}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">MLP</cell>\n<cell halign=\"center\">kNN</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">SLAPS\u00a0<cit sha=\"eb0e320e869d1da3aa9a0a8376a6938e4e0c6182\"><ref target=\"bid27\"/></cit>{{cite:eb0e320}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">FP, MLP, ATT</cell>\n<cell halign=\"center\">kNN</cell>\n<cell halign=\"center\">activation-sym</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">denoising</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">SUBLIME\u00a0<cit sha=\"eb5b1c008a6e0e46f1350ee6ec419a9900e71565\"><ref target=\"bid34\"/></cit>{{cite:eb5b1c0}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">FP, MLP, ATT</cell>\n<cell halign=\"center\">kNN</cell>\n<cell halign=\"center\">activation-sym</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">contrastive</cell>\n</row><row><cell right-border=\"true\" halign=\"center\">VIB-GSL\u00a0<cit sha=\"200bf566e4af1f53b999242d2551c445d9a2bf82\"><ref target=\"bid21\"/></cit>{{cite:200bf56}}</cell>\n<cell halign=\"center\">features</cell>\n<cell halign=\"center\">MLP</cell>\n<cell halign=\"center\">Bernoulli</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">none</cell>\n<cell halign=\"center\">denoising</cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "1dc9e3f8-9028-413c-8117-084d9777cea4", "context_autogenerated": {"glossary": {"Model<~>Model": "In the context of the table, 'Model' refers to the name of the specific Graph Structure Learning model referred to by the corresponding citation.", "Model<~>GLCN": "GLCN in the table refers to Graph Convolutional Network (GCN), as mentioned in the reference {{cite:a81495a}}.", "Model<~>JLGCN": "JLGCN in the table refers to the model \"Joint Learning for Graph Convolutional Networks\" described in the reference [[cite:2504eac]].", "Model<~>DGCNN": "DGCNN in the table refers to Deep Graph Convolutional Networks.", "Model<~>LDS": "'LDS' in the column 'Model' likely refers to \"Local Differential Privacy Graph Convolutional Network\" as suggested by the reference {{cite:42d0f04}}.", "Model<~>IDGL": "IDGL stands for Inductive Deep Graph Learning model.", "Model<~>Graph-Bert": "Graph-Bert in the table refers to a specific model named \"Graph-Bert\" mentioned in the reference with citation {{cite:6fa516d}}.", "Model<~>GRCN": "GRCN in the table refers to Graph Convolutional Network with Residual Connection as mentioned in [[509d147]].", "Model<~>SLAPS": "SLAPS in the table refers to Subgraph Attention Pooling Scholarship model.", "Model<~>SUBLIME": "Sublime in the column 'Model' refers to a specific graph structure learning model referred to by its citation {{cite:eb5b1c0}} in the text.", "Model<~>VIB-GSL": "VIB-GSL is a model referred to incite:200bf56. It is not explicitly defined in the text, but it can be inferred that it is a type of Graph Structure Learning model mentioned in the reference.", "Input<~>Input": "In this context, 'Input' refers to the features used by the models described in the references in the table.", "Input<~>features": "In the context of the table, 'features' in the 'Input' column refers to the raw features used as input to various models in the Unified Graph Structure Learning (UGSL) framework.", "Input<~>features, WL and spectral": "The 'Input' column in the table refers to the datasets used in the experiments, and for reference [6fa516d], the Input includes 'features, WL and spectral', which likely refers to the input features, positional encodings based on Weisfeiler-Lehman roles, and spectral roles.", "Edge scorer<~>Edge scorer": "The table lists different references and the edge scorer used in each reference, which can be MLP, FP, or ATT.", "Edge scorer<~>MLP": "In the context of the table, 'MLP' in the column 'Edge scorer' refers to a multilayer perceptron edge scorer.", "Edge scorer<~>FP": "'FP' in the column 'Edge scorer' refers to a specific edge scorer method, possibly standing for 'Full Parameterization'.", "Edge scorer<~>ATT": "In the context of the table, 'ATT' in the 'Edge scorer' column refers to Attentive edge scorer.", "Edge scorer<~>FP, MLP, ATT": "FP, MLP, and ATT in the 'Edge scorer' column refer to Full parameterization, Multilayer perceptron, and Attentive edge scorers, respectively.", "Sparsifier<~>Sparsifier": "In the given table, 'Sparsifier' refers to a method used to reduce the density of a graph by removing some edges while preserving its underlying structure. The specific methods mentioned in the table include kNN and Bernoulli.", "Sparsifier<~>none": "'None' in the column 'Sparsifier' refers to the absence of a specific sparsification method mentioned in the text.", "Sparsifier<~>kNN": "The 'kNN' in the 'Sparsifier' column for the table refers to \"k-nearest neighbors.\"", "Sparsifier<~>Bernoulli": "Bernoulli refers to the relaxation of the Bernoulli distribution used as a sparsifier in some GSL models.", "Sparsifier<~>\u03f5NN": "'\u03f5NN' in the column 'Sparsifier' refers to the epsilon-neighborhood graph, a type of graph sparsifier where each node is connected to its k-nearest neighbors within a radius of epsilon.", "Processor<~>Processor": "In the context of the given table, 'Processor' refers to the application of different processing techniques on the output of sparsifiers in graph structure learning models. These processing techniques can include applying non-linearities on edge weights, symmetrizing the output of sparsifiers, or applying both.", "Processor<~>activation": "In the context of the table, 'activation' in the 'Processor' column refers to the application of non-linearities on the edge weights to remove negative values.", "Processor<~>none": "In the context of the table, 'none' in the 'Processor' column refers to a lack of specific processing techniques applied in the corresponding experiment.", "Processor<~>activation-sym": "In the context of the table, 'activation-sym' in the 'Processor' column refers to a processing technique that applies both non-linearities on edge weights and symmetrization of the output of sparsifiers.", "Regularizers<~>Regularizers": "In the context of the table, 'Regularizers' refers to the constraint functions applied during the training of a model to improve the model's performance and prevent overfitting.", "Regularizers<~>sparse-connect, closeness, log-barrier": "The table lists the references for different Graph Structure Learning (GSL) models and the regularizers used in each model. The regularizers mentioned in the first reference, \"sparse-connect, closeness, log-barrier,\" are types of regularization techniques used in the GSL model described by that reference.", "Regularizers<~>smoothness": "In the context of the table, 'smoothness' in the column 'Regularizers' likely refers to the measure of the lack of fluctuation or irregularity in the node representations learned by the graph neural network, as it is mentioned as a regularizer in the given literature cited.", "Regularizers<~>none": "In the given table, 'none' in the 'Regularizers' column refers to the absence of any regularizer being used in the experimental setup for the corresponding reference.", "Regularizers<~>sparse-connect, log-barrier": "In the context of the table, 'sparse-connect, log-barrier' in the 'Regularizers' column refers to using the sparse-connect regularizer along with the log-barrier regularizer.", "Unsupervised Losses<~>Unsupervised Losses": "Unsupervised losses refer to the loss functions used in the UGSL framework that do not rely on labeled data for training but aim to learn useful representations from the input data. In the given table, \"denoising\" and \"contrative\" denote two specific types of unsupervised losses used in the cited studies.", "Unsupervised Losses<~>none": "In the context of the table, 'none' in the column 'Unsupervised Losses' refers to the absence of an unsupervised loss function being used in the cited references.", "Unsupervised Losses<~>denoising": "The 'denoising' in the 'Unsupervised Losses' column of the table refers to a type of unsupervised loss function used in the UGSL framework, specifically mentioned in the citations for rows 7 and 9. The exact nature of the denoising loss function is not specified in the given text.", "Unsupervised Losses<~>contrastive": "The entry \"contrastive\" in the column 'Unsupervised Losses' refers to the contrastive loss function used in the UGSL framework."}}, "title": "UGSL: A Unified Framework for Benchmarking Graph Structure Learning", "abstract": "Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications. While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided. The GNN parameters and a graph structure are jointly learned. Previous studies adopt different experimentation setups, making it difficult to compare their merits. In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework. Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model. We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework. Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses. The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl."}
{"paper_id": "2308.06595v1", "_pdf_hash": null, "_source_hash": "bdfa42015c4fcaf96c3437faa0ef900b0cd81dcb", "_source_name": "2308.06595v1", "_table_hash": "199fe872-0610-48fc-a187-4394add0d91f", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"left\"/>\n<td halign=\"center\">MultiInstruct<cit sha=\"61276bdc50a13a469a8525f61a564a33fb027772\"><ref target=\"bid31\"/></cit>{{cite:61276bd}}</td>\n<td halign=\"center\">Owl<cit sha=\"db475512af1ac2b57776a52505a70f0451ea07cb\"><ref target=\"bid16\"/></cit>{{cite:db47551}}</td>\n<td halign=\"center\">InstructBLIP<cit sha=\"bd9a08c5111e8cd1f437bea3e63ddc376431c67f\"><ref target=\"bid13\"/></cit>{{cite:bd9a08c}}</td>\n<td halign=\"center\">M<formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow/> <mn>3</mn> </msup></math></formula>IT<cit sha=\"50db30023b116aae88ba6dca45ee992af5b190af\"><ref target=\"bid32\"/></cit>{{cite:50db300}}</td>\n<td halign=\"center\">LVLM<cit sha=\"6e5cb721496d22b7b0002238313f0c3e15b7a7ca\"><ref target=\"bid33\"/></cit>{{cite:6e5cb72}}</td>\n<td halign=\"center\">GAVIE<cit sha=\"843607eec66be63c8bc521796fb7a5d3ff979cc8\"><ref target=\"bid34\"/></cit>{{cite:843607e}}</td>\n<td halign=\"center\"><hi rend=\"bold\">VisIT-Bench</hi></td>\n</tr><tr><td halign=\"left\">Number of Models</td>\n<td halign=\"center\">1</td>\n<td halign=\"center\">5</td>\n<td halign=\"center\">3</td>\n<td halign=\"center\">4</td>\n<td halign=\"center\">8</td>\n<td halign=\"center\">5</td>\n<td halign=\"center\">10</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\">Number of Skills Tested</td>\n<td halign=\"center\">9</td>\n<td halign=\"center\">6</td>\n<td halign=\"center\">13</td>\n<td halign=\"center\">13</td>\n<td halign=\"center\">47</td>\n<td halign=\"center\">16</td>\n<td halign=\"center\">70</td>\n</tr><tr><td halign=\"left\">Multiple-Images</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr><tr><td halign=\"left\">Video</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n</tr><tr><td halign=\"left\">Multi-Turn Conversations</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\">Multilingual Conversations</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n</tr><tr><td halign=\"left\">Instruction-conditioned Captions</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\">Chatbot-style Responses</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr><tr><td halign=\"left\">Dataset-specific Evaluation</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n</tr><tr><td halign=\"left\">Human Evaluation</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\">Auto/GPT-4 Evaluation</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr><tr><td halign=\"left\">Win-rates*</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\">Elo Rating</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">darkgreen\u2713</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["", "MultiInstruct{{cite:61276bd}}", "Owl{{cite:db47551}}", "InstructBLIP{{cite:bd9a08c}}", "M 3 IT{{cite:50db300}}", "LVLM{{cite:6e5cb72}}", "GAVIE{{cite:843607e}}", "VisIT-Bench"], ["Number of Models", "1", "5", "3", "4", "8", "5", "10"], ["Number of Skills Tested", "9", "6", "13", "13", "47", "16", "70"], ["Multiple-Images", "red\u2717", "darkgreen\u2713", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "darkgreen\u2713"], ["Video", "red\u2717", "red\u2717", "darkgreen\u2713", "darkgreen\u2713", "red\u2717", "red\u2717", "red\u2717"], ["Multi-Turn Conversations", "darkgreen\u2713", "darkgreen\u2713", "darkgreen\u2713", "darkgreen\u2713", "darkgreen\u2713", "red\u2717", "red\u2717"], ["Multilingual Conversations", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713", "red\u2717", "red\u2717", "red\u2717"], ["Instruction-conditioned Captions", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "darkgreen\u2713"], ["Chatbot-style Responses", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "darkgreen\u2713"], ["Dataset-specific Evaluation", "darkgreen\u2713", "darkgreen\u2713", "darkgreen\u2713", "darkgreen\u2713", "darkgreen\u2713", "red\u2717", "red\u2717"], ["Human Evaluation", "red\u2717", "darkgreen\u2713", "red\u2717", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713"], ["Auto/GPT-4 Evaluation", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713", "darkgreen\u2713"], ["Win-rates*", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713", "darkgreen\u2713"], ["Elo Rating", "red\u2717", "red\u2717", "red\u2717", "red\u2717", "darkgreen\u2713", "red\u2717", "darkgreen\u2713"]], "table_dict": {"References": ["{{cite:61276bd}}", "{{cite:db47551}}", "{{cite:bd9a08c}}", "{{cite:50db300}}", "{{cite:6e5cb72}}", "{{cite:843607e}}", "-"], "Method": ["MultiInstruct ", "Owl ", "InstructBLIP", "M 3 IT ", "LVLM", "GAVIE", "VisIT-Bench"], "Number of Models": ["1", "5", "3", "4", "8", "5", "10"], "Number of Skills Tested": ["9", "6", "13", "13", "47", "16", "70"], "Multiple-Images": ["\u2717", "\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], "Video": ["\u2717", "\u2717", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717"], "Multi-Turn Conversations": ["\u2713", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2717"], "Multilingual Conversations": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2717", "\u2717"], "Instruction-conditioned Captions": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], "Chatbot-style Responses": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], "Dataset-specific Evaluation": ["\u2713", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2717"], "Human Evaluation": ["\u2717", "\u2713", "\u2717", "\u2717", "\u2713", "\u2717", "\u2713"], "Auto/GPT-4 Evaluation": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2713", "\u2713"], "Win-rates*": ["\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2713", "\u2713"], "Elo Rating": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2717", "\u2713"]}}, "bib_hash": ["61276bdc50a13a469a8525f61a564a33fb027772", "db475512af1ac2b57776a52505a70f0451ea07cb", "bd9a08c5111e8cd1f437bea3e63ddc376431c67f", "50db30023b116aae88ba6dca45ee992af5b190af", "6e5cb721496d22b7b0002238313f0c3e15b7a7ca", "843607eec66be63c8bc521796fb7a5d3ff979cc8"], "row_bib_map": [{"bib_hash_or_arxiv_id": "61276bdc50a13a469a8525f61a564a33fb027772", "row": 0, "corpus_id": 254926784, "type": "ref"}, {"bib_hash_or_arxiv_id": "db475512af1ac2b57776a52505a70f0451ea07cb", "row": 1, "corpus_id": 258352455, "type": "ref"}, {"bib_hash_or_arxiv_id": "bd9a08c5111e8cd1f437bea3e63ddc376431c67f", "row": 2, "corpus_id": 258615266, "type": "ref"}, {"bib_hash_or_arxiv_id": "50db30023b116aae88ba6dca45ee992af5b190af", "row": 3, "corpus_id": 259095896, "type": "ref"}, {"bib_hash_or_arxiv_id": "6e5cb721496d22b7b0002238313f0c3e15b7a7ca", "row": 4, "corpus_id": 259165040, "type": "ref"}, {"bib_hash_or_arxiv_id": "843607eec66be63c8bc521796fb7a5d3ff979cc8", "row": 5, "corpus_id": 263860779, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.06595v1", "row": 6, "corpus_id": 260887670, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell halign=\"left\"/>\n<cell halign=\"center\">MultiInstruct\u00a0<cit sha=\"61276bdc50a13a469a8525f61a564a33fb027772\"><ref target=\"bid31\"/></cit>{{cite:61276bd}}</cell>\n<cell halign=\"center\">Owl\u00a0<cit sha=\"db475512af1ac2b57776a52505a70f0451ea07cb\"><ref target=\"bid16\"/></cit>{{cite:db47551}}</cell>\n<cell halign=\"center\">InstructBLIP\u00a0<cit sha=\"bd9a08c5111e8cd1f437bea3e63ddc376431c67f\"><ref target=\"bid13\"/></cit>{{cite:bd9a08c}}</cell>\n<cell halign=\"center\">M<formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow/> <mn>3</mn> </msup></math><texmath>^3</texmath></formula>IT\u00a0<cit sha=\"50db30023b116aae88ba6dca45ee992af5b190af\"><ref target=\"bid32\"/></cit>{{cite:50db300}}</cell>\n<cell halign=\"center\">LVLM\u00a0<cit sha=\"6e5cb721496d22b7b0002238313f0c3e15b7a7ca\"><ref target=\"bid33\"/></cit>{{cite:6e5cb72}}</cell>\n<cell halign=\"center\">GAVIE\u00a0<cit sha=\"843607eec66be63c8bc521796fb7a5d3ff979cc8\"><ref target=\"bid34\"/></cit>{{cite:843607e}}</cell>\n<cell halign=\"center\"><hi rend=\"bold\">VisIT-Bench</hi></cell>\n</row><row><cell halign=\"left\">Number of Models</cell>\n<cell halign=\"center\">1</cell>\n<cell halign=\"center\">5</cell>\n<cell halign=\"center\">3</cell>\n<cell halign=\"center\">4</cell>\n<cell halign=\"center\">8</cell>\n<cell halign=\"center\">5</cell>\n<cell halign=\"center\">10</cell>\n</row><row bottom-border=\"true\"><cell halign=\"left\">Number of Skills Tested</cell>\n<cell halign=\"center\">9</cell>\n<cell halign=\"center\">6</cell>\n<cell halign=\"center\">13</cell>\n<cell halign=\"center\">13</cell>\n<cell halign=\"center\">47</cell>\n<cell halign=\"center\">16</cell>\n<cell halign=\"center\">70</cell>\n</row><row><cell halign=\"left\">Multiple-Images</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row><row><cell halign=\"left\">Video</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n</row><row><cell halign=\"left\">Multi-Turn Conversations</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n</row><row bottom-border=\"true\"><cell halign=\"left\">Multilingual Conversations</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n</row><row><cell halign=\"left\">Instruction-conditioned Captions</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row><row bottom-border=\"true\"><cell halign=\"left\">Chatbot-style Responses</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row><row><cell halign=\"left\">Dataset-specific Evaluation</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n</row><row><cell halign=\"left\">Human Evaluation</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row><row bottom-border=\"true\"><cell halign=\"left\">Auto/GPT-4 Evaluation</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row><row><cell halign=\"left\">Win-rates*</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row><row bottom-border=\"true\"><cell halign=\"left\">Elo Rating</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">darkgreen\u2713</cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "1d5cb0b2-a9e1-403d-9802-1f8545379f7d", "context_autogenerated": {"glossary": {"Method<~>Method": "The 'Method' column in the table refers to the name of the method or model used in each reference.", "Method<~>MultiInstruct": "MultiInstruct refers to a method used in a prior work for evaluating multimodal chatbots, specifically mentioned in the first row of the table.", "Method<~>Owl": "In the given table, 'Owl' in the 'Method' column for the second row refers to the method named 'Owl' used in the study mentioned in the reference {{cite:db47551}}. The specific details about the 'Owl' method are not provided in the text.", "Method<~>InstructBLIP": "In the context of the given table, 'InstructBLIP' in the column 'Method' refers to a specific multimodal chatbot evaluation method mentioned in {{cite:bd9a08c}}.", "Method<~>M 3 IT": "M 3 IT in the table refers to the method \"M3 IT\" mentioned in [[50db300]], which is likely an instruction-following model or a variant of it.", "Method<~>LVLM": "LVLM in the table refers to the multimodal instruction-following model named Visual and Language Interaction Model mentioned in [[5e5cb72]] in the references section of the text.", "Method<~>GAVIE": "GAVIE refers to a specific method for evaluating multimodal chatbots mentioned in the text.", "Method<~>VisIT-Bench": "VisIT-Bench in the context of the table refers to the Visual InsTruction Benchmark method used for evaluating multimodal chatbots.", "Number of Models<~>Number of Models": "The 'Number of Models' column in the table refers to the number of multimodal instruction-following models that have been evaluated using the VisIT-Bench data for comparison.", "Number of Skills Tested<~>Number of Skills Tested": "In the context of the table presented in the text, 'Number of Skills Tested' refers to the number of distinct instruction families or tasks evaluated by each referenced work in the evaluation of multimodal chatbots.", "Multiple-Images<~>Multiple-Images": "Multiple-Images in the table refers to the presence or absence of multiple images in the datasets used by the referred works for evaluating multimodal chatbots. The marked '\u2713' indicates that the dataset includes multiple images.", "Video<~>Video": "It is unclear what 'Video' refers to in the context of the given table.", "Multi-Turn Conversations<~>Multi-Turn Conversations": "'Multi-Turn Conversations' refers to the ability of models to engage in back-and-forth interactions with users, as opposed to simply generating a single response. The table indicates which of the cited works support multi-turn conversations by marking the checkbox with '\u2713'.", "Multilingual Conversations<~>Multilingual Conversations": "Multilingual Conversations refers to the support for multi-language conversations in the mentioned works. The table indicates which of the referenced works support multilingual conversations with a check mark (\u2713) and which do not with an X mark (\u2717).", "Instruction-conditioned Captions<~>Instruction-conditioned Captions": "Instruction-conditioned Captions refer to human-crafted descriptions that provide information specifically targeted to the instruction and the image, enabling quantitative evaluation of multimodal generation quality. (From the text, it is clear that these captions are used in the VisIT-Bench benchmark but are not explicitly mentioned in the table.)", "Chatbot-style Responses<~>Chatbot-style Responses": "\"Chatbot-style Responses\" in the context of the table refers to model responses that are designed to mimic conversational interactions with a human in a chatbot format. The table differentiates models that have been evaluated with such responses (marked with a checkmark) from those that have not (marked with a blank or an 'X' symbol).", "Dataset-specific Evaluation<~>Dataset-specific Evaluation": "In the context of the table provided, 'Dataset-specific Evaluation' refers to the evaluation of multimodal chatbots using datasets specific to their domain or task. The table indicates that references exist for datasets 0-5, which all underwent dataset-specific evaluation, while there is no reference for dataset 6, which did not undergo this evaluation.", "Human Evaluation<~>Human Evaluation": "In this context, 'Human Evaluation' refers to the assessment of model outputs by human annotators to determine their quality or preference over other model outputs. The table shows whether human evaluation was conducted for different references or not.", "Auto/GPT-4 Evaluation<~>Auto/GPT-4 Evaluation": "The 'Auto/GPT-4 Evaluation' column in the table refers to whether the automated GPT-4 evaluation aligns best with human preferences for the corresponding multimodal chatbot. A '\u2713' indicates agreement, while an '\u2717' indicates disagreement.", "Win-rates*<~>Win-rates*": "\"Win-rates*\" in the table refers to the percentage of times each Model's output was preferred over the Human Reference in the study conducted by the authors.", "Elo Rating<~>Elo Rating": "Elo Rating is a metric used to estimate the relative skill or strength of different models or systems, providing an estimate of the probability that one model will be preferred over another."}}, "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", "abstract": "We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at visit-bench.github.io."}
{"paper_id": "2308.01529v1", "_pdf_hash": null, "_source_hash": "6612990fc91913097d8d5c44699e721d5bdff500", "_source_name": "2308.01529v1", "_table_hash": "01fcc1e8-bb38-4360-851a-3ba035d3b355", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\"><hi rend=\"bold\">Dataset</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Data Type</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Format</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">No. Classes</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Size</hi></td>\n<td halign=\"center\"><hi rend=\"bold\">Base Model</hi></td>\n</tr><tr><td halign=\"center\">NIH Chest X-Ray <cit sha=\"dd24223a908700078ad155e8d6bc7ee5900c33ea\"><ref target=\"bid15\"/></cit>{{cite:dd24223}}</td>\n<td halign=\"center\">Adult X-Ray Images</td>\n<td halign=\"center\">PNG</td>\n<td halign=\"center\">13</td>\n<td halign=\"center\">25k</td>\n<td halign=\"center\">ResNet-34</td>\n</tr><tr><td halign=\"center\">Fast MRI <cit sha=\"54a33035d17a7a45ffaa4670d977985240afd842\"><ref target=\"bid16\"/></cit>{{cite:54a3303}}</td>\n<td halign=\"center\">Brain Tissue Scans</td>\n<td halign=\"center\">3D Arrays</td>\n<td halign=\"center\">2</td>\n<td halign=\"center\">8k</td>\n<td halign=\"center\">U-Net</td>\n</tr><tr><td halign=\"center\">PTB Dataset <cit sha=\"4a92373b531319e788fc25d48419e5386ae322ea\"><ref target=\"bid17\"/></cit>{{cite:4a92373}}</td>\n<td halign=\"center\">ECG Waveforms</td>\n<td halign=\"center\">Signal</td>\n<td halign=\"center\">10</td>\n<td halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>&lt;</mo></math></formula>1k</td>\n<td halign=\"center\">ResNet-18</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\">MIMIC II <cit sha=\"418dc6a794db400ac6fdcdeade3798b7c3ea0d35\"><ref target=\"bid18\"/></cit>{{cite:418dc6a}}</td>\n<td halign=\"center\">Clinical and Vital Signs</td>\n<td halign=\"center\">Tabular, Signal</td>\n<td halign=\"center\">59</td>\n<td halign=\"center\">30k</td>\n<td halign=\"center\">XGBoost</td>\n</tr></table>", "table_json": {"incomplete_rows": [], "table": [["Dataset", "Data Type", "Format", "No. Classes", "Size", "Base Model"], ["NIH Chest X-Ray {{cite:dd24223}}", "Adult X-Ray Images", "PNG", "13", "25k", "ResNet-34"], ["Fast MRI {{cite:54a3303}}", "Brain Tissue Scans", "3D Arrays", "2", "8k", "U-Net"], ["PTB Dataset {{cite:4a92373}}", "ECG Waveforms", "Signal", "10", "<1k", "ResNet-18"], ["MIMIC II {{cite:418dc6a}}", "Clinical and Vital Signs", "Tabular, Signal", "59", "30k", "XGBoost"]], "table_dict": {"References": ["{{cite:dd24223}}", "{{cite:54a3303}}", "{{cite:4a92373}}", "{{cite:418dc6a}}"], "Dataset": ["NIH Chest X-Ray ", "Fast MRI ", "PTB Dataset ", "MIMIC II "], "Data Type": ["Adult X-Ray Images", "Brain Tissue Scans", "ECG Waveforms", "Clinical and Vital Signs"], "Format": ["PNG", "3D Arrays", "Signal", "Tabular, Signal"], "No. Classes": ["13", "2", "10", "59"], "Size": ["25k", "8k", "<1k", "30k"], "Base Model": ["ResNet-34", "U-Net", "ResNet-18", "XGBoost"]}}, "bib_hash": ["dd24223a908700078ad155e8d6bc7ee5900c33ea", "54a33035d17a7a45ffaa4670d977985240afd842", "4a92373b531319e788fc25d48419e5386ae322ea", "418dc6a794db400ac6fdcdeade3798b7c3ea0d35"], "row_bib_map": [{"bib_hash_or_arxiv_id": "dd24223a908700078ad155e8d6bc7ee5900c33ea", "row": 0, "corpus_id": 8945673, "type": "ref"}, {"bib_hash_or_arxiv_id": "54a33035d17a7a45ffaa4670d977985240afd842", "row": 1, "corpus_id": 53759905, "type": "ref"}, {"bib_hash_or_arxiv_id": "4a92373b531319e788fc25d48419e5386ae322ea", "row": 2, "corpus_id": 218865062, "type": "ref"}, {"bib_hash_or_arxiv_id": "418dc6a794db400ac6fdcdeade3798b7c3ea0d35", "row": 3, "corpus_id": 33285731, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell halign=\"center\"><hi rend=\"bold\">Dataset</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Data Type</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Format</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">No. Classes</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Size</hi></cell>\n<cell halign=\"center\"><hi rend=\"bold\">Base Model</hi></cell>\n</row><row><cell halign=\"center\">NIH Chest X-Ray <cit sha=\"dd24223a908700078ad155e8d6bc7ee5900c33ea\"><ref target=\"bid15\"/></cit>{{cite:dd24223}}</cell>\n<cell halign=\"center\">Adult X-Ray Images</cell>\n<cell halign=\"center\">PNG</cell>\n<cell halign=\"center\">13</cell>\n<cell halign=\"center\">25k</cell>\n<cell halign=\"center\">ResNet-34</cell>\n</row><row><cell halign=\"center\">Fast MRI <cit sha=\"54a33035d17a7a45ffaa4670d977985240afd842\"><ref target=\"bid16\"/></cit>{{cite:54a3303}}</cell>\n<cell halign=\"center\">Brain Tissue Scans</cell>\n<cell halign=\"center\">3D Arrays</cell>\n<cell halign=\"center\">2</cell>\n<cell halign=\"center\">8k</cell>\n<cell halign=\"center\">U-Net</cell>\n</row><row><cell halign=\"center\">PTB Dataset <cit sha=\"4a92373b531319e788fc25d48419e5386ae322ea\"><ref target=\"bid17\"/></cit>{{cite:4a92373}}</cell>\n<cell halign=\"center\">ECG Waveforms</cell>\n<cell halign=\"center\">Signal</cell>\n<cell halign=\"center\">10</cell>\n<cell halign=\"center\"><formula type=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>&lt;</mo></math><texmath>&lt;</texmath></formula>1k</cell>\n<cell halign=\"center\">ResNet-18</cell>\n</row><row bottom-border=\"true\"><cell halign=\"center\">MIMIC II <cit sha=\"418dc6a794db400ac6fdcdeade3798b7c3ea0d35\"><ref target=\"bid18\"/></cit>{{cite:418dc6a}}</cell>\n<cell halign=\"center\">Clinical and Vital Signs</cell>\n<cell halign=\"center\">Tabular, Signal</cell>\n<cell halign=\"center\">59</cell>\n<cell halign=\"center\">30k</cell>\n<cell halign=\"center\">XGBoost</cell>\n</row></table>", "caption": "NO_CAPTION", "type": "table"}, "_table_hash_full_text": "3656f752-a94b-4c07-8acb-ead182f95513", "_full_text_table_hash": "a35ebfbd-baf8-48b3-a15c-ac2b7c4a11c0", "context_autogenerated": {"glossary": {"Dataset<~>Dataset": "In the context of the table, 'Dataset' refers to the specific dataset used in various federated learning research studies, as identified by their corresponding citations.", "Dataset<~>NIH Chest X-Ray": "The 'NIH Chest X-Ray' in the 'Dataset' column refers to the NIH (National Institutes of Health) Chest X-Ray dataset.", "Dataset<~>Fast MRI": "Fast MRI in the table refers to Magnetic Resonance Imaging data collected in a fast manner.", "Dataset<~>PTB Dataset": "PTB Dataset refers to a specific dataset mentioned in one of the cited references. Without access to the references, it's unclear what type of data is contained in the PTB Dataset.", "Dataset<~>MIMIC II": "MIMIC II stands for Medical Information Mart for Intensive Care, which is a large, freely available database of deidentified health data records and related information for intensive care unit patients.", "Data Type<~>Data Type": "In the given context, 'Data Type' refers to the specific type of data being used, such as Adult X-Ray Images, Brain Tissue Scans, ECG Waveforms, or Clinical and Vital Signs, as indicated in the table.", "Data Type<~>Adult X-Ray Images": "Adult X-Ray Images refers to images used for medical diagnosis through X-ray technology. (Assuming this is the case based on the common use of the term \"Adult X-Ray\" in the medical field.)", "Data Type<~>Brain Tissue Scans": "Brain Tissue Scans in the table referred to in the text refer to medical imaging data specifically showing scans of brain tissue.", "Data Type<~>ECG Waveforms": "ECG Waveforms refer to electrical signals recorded from the heart through electrodes attached to the skin.", "Data Type<~>Clinical and Vital Signs": "Clinical and Vital Signs in the table refer to health data that includes measures and observations of patients' physiological and biological status, such as temperature, heart rate, blood pressure, and lab results.", "Format<~>Format": "The 'Format' column in the table refers to the file format of the cited references.", "Format<~>PNG": "PNG in the table refers to Portable Network Graphics format.", "Format<~>3D Arrays": "In the given table, '3D Arrays' in the 'Format' column refers to data formats consisting of three-dimensional arrays.", "Format<~>Signal": "In the context of the table, 'Signal' in the 'Format' column likely refers to continuous data or time-series data, as it is mentioned in the same row as the reference {{cite:4a92373}} which uses the term \"signal data\" in the provided scientific paper text.", "Format<~>Tabular, Signal": "The 'Tabular, Signal' format in the table refers to data that is represented both in tabular form and as signals.", "No. Classes<~>No. Classes": "In the context of the table, 'No. Classes' refers to the number of classes present in the dataset or model associated with each cited reference.", "Size<~>Size": "The 'Size' column in the table refers to the size of the references, likely indicating the number of data points or records they represent.", "Base Model<~>Base Model": "In the context of the table, 'Base Model' refers to the specific deep learning or machine learning model mentioned in each row, which is identified by the corresponding citation number.", "Base Model<~>ResNet-34": "'ResNet-34' in the column 'Base Model' refers to the use of the ResNet-34 model as a base architecture in the experiments mentioned in the scientific paper.", "Base Model<~>U-Net": "U-Net is a base model referred to in the table, which is a specific convolutional neural network architecture often used for biomedical image segmentation tasks.", "Base Model<~>ResNet-18": "The column 'Base Model' in the table refers to the type of machine learning model used as a foundation for the fair and privacy-preserving federated learning approaches being evaluated in the study. In particular, the third row indicates that for reference [4a92373], the base model is ResNet-18. ResNet-18 is a commonly-used deep residual neural network architecture with 18 layers.", "Base Model<~>XGBoost": "XGBoost in the table refers to the Extremely Gradient Boosting machine learning algorithm."}}, "title": "Towards Fair and Privacy Preserving Federated Learning for the Healthcare Domain", "abstract": "Federated learning enables data sharing in healthcare contexts where it might otherwise be difficult due to data-use-ordinances or security and communication constraints. Distributed and shared data models allow models to become generalizable and learn from heterogeneous clients. While addressing data security, privacy, and vulnerability considerations, data itself is not shared across nodes in a given learning network. On the other hand, FL models often struggle with variable client data distributions and operate on an assumption of independent and identically distributed data. As the field has grown, the notion of fairness-aware federated learning mechanisms has also been introduced and is of distinct significance to the healthcare domain where many sensitive groups and protected classes exist. In this paper, we create a benchmark methodology for FAFL mechanisms under various heterogeneous conditions on datasets in the healthcare domain typically outside the scope of current federated learning benchmarks, such as medical imaging and waveform data formats. Our results indicate considerable variation in how various FAFL schemes respond to high levels of data heterogeneity. Additionally, doing so under privacy-preserving conditions can create significant increases in network communication cost and latency compared to the typical federated learning scheme."}
{"paper_id": "2308.15701v1", "_pdf_hash": null, "_source_hash": "ea08a3ecc996b3092887adf8f6675fed4219ea4b", "_source_name": "2308.15701v1", "_table_hash": "ca93e9fc-b206-43af-957e-5f635558fde0", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid32\" id-text=\"2\" place=\"htbp\" rend=\"display\" starred=\"true\"><head>Data &amp; modeling perspectives and features used in works based on RNN learning architecture.</head>\n<unexpected><hi rend=\"small\"><table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\" right-border=\"true\">Works</td>\n<td halign=\"center\" right-border=\"true\">Data Perspective</td>\n<td halign=\"center\">Model Perspective</td>\n<td>Features</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">RLBL<cit sha=\"bcf84d7793ec7649f4933e930216476056e83046\"><ref target=\"bid13\"/></cit>{{cite:bcf84d7}}</td>\n<td halign=\"center\" right-border=\"true\">A sequence of (item, behavior) pairs</td>\n<td halign=\"center\">Local</td>\n<td><hi rend=\"small\"><hi color=\"colid1\">Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.</hi></hi><hi rend=\"small\"/></td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">RIB<cit sha=\"2f67c58f5f8132925c0fd2bb1519d25ce40f2212\"><ref target=\"bid25\"/></cit>{{cite:2f67c58}}</td>\n<td halign=\"center\" right-border=\"true\">A sequence of (item, behavior) pairs</td>\n<td halign=\"center\">Local</td>\n<td>Leverage GRU and attention mechanism simultaneously.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">BINN<cit sha=\"909f2729ba27471fc01b41abc19cb7cfccaf5ab7\"><ref target=\"bid21\"/></cit>{{cite:909f272}}</td>\n<td halign=\"center\" right-border=\"true\">A sequence of (item, behavior) pairs</td>\n<td halign=\"center\">Local</td>\n<td>Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">CBS<cit sha=\"8dbb1e8e34ffc8cb061a0700182110ff9ab381cb\"><ref target=\"bid62\"/></cit>{{cite:8dbb1e8}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequences of items</td>\n<td halign=\"center\">Local</td>\n<td>Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">DIPN<cit sha=\"96418646514091907a1c7a59a16ebdebef53de70\"><ref target=\"bid63\"/></cit>{{cite:9641864}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequences of items</td>\n<td halign=\"center\">Local</td>\n<td>Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">HUP<cit sha=\"c76954ff0ca62f301ea0bea6c87223ae813cd345\"><ref target=\"bid26\"/></cit>{{cite:c76954f}}</td>\n<td halign=\"center\" right-border=\"true\">A sequence of (item, behavior) pairs</td>\n<td halign=\"center\">Local</td>\n<td>Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">IARS<cit sha=\"82d42f087719a41c6710f89ce7293d970ac4e1eb\"><ref target=\"bid27\"/></cit>{{cite:82d42f0}}</td>\n<td halign=\"center\" right-border=\"true\">A sequence of (item, behavior) pairs</td>\n<td halign=\"center\">Local</td>\n<td>Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">DeepRec<cit sha=\"b1e6c1692c53e7a4e81915f2f240751c2abc6eaf\"><ref target=\"bid61\"/></cit>{{cite:b1e6c16}}</td>\n<td halign=\"center\" right-border=\"true\"><hi rend=\"small\"><hi color=\"colid1\">Some behavior-specific subsequences of items</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\">Local + Global</td>\n<td>Utilizing multi-behavior sequence data to make privacy-preserving recommendation.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">MBN<cit sha=\"15341b17dc891668520b21abdc88e1c309179cf8\"><ref target=\"bid64\"/></cit>{{cite:15341b1}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequences of items</td>\n<td halign=\"center\">Local</td>\n<td>The <hi rend=\"small\"><hi color=\"colid1\">overall</hi></hi><hi rend=\"small\"> Meta-RNN and the </hi><hi rend=\"small\"><hi color=\"colid1\">separate</hi></hi><hi rend=\"small\"> Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.</hi></td>\n</tr></table></hi></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Works", "Data Perspective", "Model Perspective", "Features"], ["RLBL{{cite:bcf84d7}}", "A sequence of (item, behavior) pairs", "Local", "Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix."], ["RIB{{cite:2f67c58}}", "A sequence of (item, behavior) pairs", "Local", "Leverage GRU and attention mechanism simultaneously."], ["BINN{{cite:909f272}}", "A sequence of (item, behavior) pairs", "Local", "Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM."], ["CBS{{cite:8dbb1e8}}", "Some behavior-specific subsequences of items", "Local", "Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation."], ["DIPN{{cite:9641864}}", "Some behavior-specific subsequences of items", "Local", "Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior."], ["HUP{{cite:c76954f}}", "A sequence of (item, behavior) pairs", "Local", "Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items."], ["IARS{{cite:82d42f0}}", "A sequence of (item, behavior) pairs", "Local", "Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items."], ["DeepRec{{cite:b1e6c16}}", "Some behavior-specific subsequences of items", "Local + Global", "Utilizing multi-behavior sequence data to make privacy-preserving recommendation."], ["MBN{{cite:15341b1}}", "Some behavior-specific subsequences of items", "Local", "The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation."]], "table_dict": {"References": ["{{cite:bcf84d7}}", "{{cite:2f67c58}}", "{{cite:909f272}}", "{{cite:8dbb1e8}}", "{{cite:9641864}}", "{{cite:c76954f}}", "{{cite:82d42f0}}", "{{cite:b1e6c16}}", "{{cite:15341b1}}"], "Works": ["RLBL", "RIB", "BINN", "CBS", "DIPN", "HUP", "IARS", "DeepRec", "MBN"], "Data Perspective": ["A sequence of (item, behavior) pairs", "A sequence of (item, behavior) pairs", "A sequence of (item, behavior) pairs", "Some behavior-specific subsequences of items", "Some behavior-specific subsequences of items", "A sequence of (item, behavior) pairs", "A sequence of (item, behavior) pairs", "Some behavior-specific subsequences of items", "Some behavior-specific subsequences of items"], "Model Perspective": ["Local", "Local", "Local", "Local", "Local", "Local", "Local", "Local + Global", "Local"], "Features": ["Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.", "Leverage GRU and attention mechanism simultaneously.", "Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.", "Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.", "Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.", "Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.", "Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.", "Utilizing multi-behavior sequence data to make privacy-preserving recommendation.", "The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation."]}}, "bib_hash": ["bcf84d7793ec7649f4933e930216476056e83046", "2f67c58f5f8132925c0fd2bb1519d25ce40f2212", "909f2729ba27471fc01b41abc19cb7cfccaf5ab7", "8dbb1e8e34ffc8cb061a0700182110ff9ab381cb", "96418646514091907a1c7a59a16ebdebef53de70", "c76954ff0ca62f301ea0bea6c87223ae813cd345", "82d42f087719a41c6710f89ce7293d970ac4e1eb", "b1e6c1692c53e7a4e81915f2f240751c2abc6eaf", "15341b17dc891668520b21abdc88e1c309179cf8"], "row_bib_map": [{"bib_hash_or_arxiv_id": "bcf84d7793ec7649f4933e930216476056e83046", "row": 0, "corpus_id": 14093453, "type": "ref"}, {"bib_hash_or_arxiv_id": "2f67c58f5f8132925c0fd2bb1519d25ce40f2212", "row": 1, "corpus_id": 32370905, "type": "ref"}, {"bib_hash_or_arxiv_id": "909f2729ba27471fc01b41abc19cb7cfccaf5ab7", "row": 2, "corpus_id": 50768534, "type": "ref"}, {"bib_hash_or_arxiv_id": "8dbb1e8e34ffc8cb061a0700182110ff9ab381cb", "row": 3, "corpus_id": 51609715, "type": "ref"}, {"bib_hash_or_arxiv_id": "96418646514091907a1c7a59a16ebdebef53de70", "row": 4, "corpus_id": 196171043, "type": "ref"}, {"bib_hash_or_arxiv_id": "c76954ff0ca62f301ea0bea6c87223ae813cd345", "row": 5, "corpus_id": 210883769, "type": "ref"}, {"bib_hash_or_arxiv_id": "82d42f087719a41c6710f89ce7293d970ac4e1eb", "row": 6, "corpus_id": 235571722, "type": "ref"}, {"bib_hash_or_arxiv_id": "b1e6c1692c53e7a4e81915f2f240751c2abc6eaf", "row": 7, "corpus_id": 235324699, "type": "ref"}, {"bib_hash_or_arxiv_id": "15341b17dc891668520b21abdc88e1c309179cf8", "row": 8, "corpus_id": 247398476, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"2\" id=\"uid32\" starred=\"true\" place=\"htbp\"><head>Data &amp; modeling perspectives and features used in works based on RNN learning architecture.</head>\n<unexpected><hi rend=\"small\"><table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"center\">Works</cell>\n<cell right-border=\"true\" halign=\"center\">Data Perspective</cell>\n<cell halign=\"center\">Model Perspective</cell>\n<cell>Features</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">RLBL\u00a0<cit sha=\"bcf84d7793ec7649f4933e930216476056e83046\"><ref target=\"bid13\"/></cit>{{cite:bcf84d7}}</cell>\n<cell right-border=\"true\" halign=\"center\">A sequence of (item, behavior) pairs</cell>\n<cell halign=\"center\">Local</cell>\n<cell><hi rend=\"small\"><hi color=\"colid1\">Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.</hi></hi><hi rend=\"small\"/></cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">RIB\u00a0<cit sha=\"2f67c58f5f8132925c0fd2bb1519d25ce40f2212\"><ref target=\"bid25\"/></cit>{{cite:2f67c58}}</cell>\n<cell right-border=\"true\" halign=\"center\">A sequence of (item, behavior) pairs</cell>\n<cell halign=\"center\">Local</cell>\n<cell>Leverage GRU and attention mechanism simultaneously.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">BINN\u00a0<cit sha=\"909f2729ba27471fc01b41abc19cb7cfccaf5ab7\"><ref target=\"bid21\"/></cit>{{cite:909f272}}</cell>\n<cell right-border=\"true\" halign=\"center\">A sequence of (item, behavior) pairs</cell>\n<cell halign=\"center\">Local</cell>\n<cell>Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">CBS\u00a0<cit sha=\"8dbb1e8e34ffc8cb061a0700182110ff9ab381cb\"><ref target=\"bid62\"/></cit>{{cite:8dbb1e8}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequences of items</cell>\n<cell halign=\"center\">Local</cell>\n<cell>Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">DIPN\u00a0<cit sha=\"96418646514091907a1c7a59a16ebdebef53de70\"><ref target=\"bid63\"/></cit>{{cite:9641864}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequences of items</cell>\n<cell halign=\"center\">Local</cell>\n<cell>Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">HUP\u00a0<cit sha=\"c76954ff0ca62f301ea0bea6c87223ae813cd345\"><ref target=\"bid26\"/></cit>{{cite:c76954f}}</cell>\n<cell right-border=\"true\" halign=\"center\">A sequence of (item, behavior) pairs</cell>\n<cell halign=\"center\">Local</cell>\n<cell>Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">IARS\u00a0<cit sha=\"82d42f087719a41c6710f89ce7293d970ac4e1eb\"><ref target=\"bid27\"/></cit>{{cite:82d42f0}}</cell>\n<cell right-border=\"true\" halign=\"center\">A sequence of (item, behavior) pairs</cell>\n<cell halign=\"center\">Local</cell>\n<cell>Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">DeepRec\u00a0<cit sha=\"b1e6c1692c53e7a4e81915f2f240751c2abc6eaf\"><ref target=\"bid61\"/></cit>{{cite:b1e6c16}}</cell>\n<cell right-border=\"true\" halign=\"center\"><hi rend=\"small\"><hi color=\"colid1\">Some behavior-specific subsequences of items</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\">Local + Global</cell>\n<cell>Utilizing multi-behavior sequence data to make privacy-preserving recommendation.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">MBN\u00a0<cit sha=\"15341b17dc891668520b21abdc88e1c309179cf8\"><ref target=\"bid64\"/></cit>{{cite:15341b1}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequences of items</cell>\n<cell halign=\"center\">Local</cell>\n<cell>The <hi rend=\"small\"><hi color=\"colid1\">overall</hi></hi><hi rend=\"small\"> Meta-RNN and the </hi><hi rend=\"small\"><hi color=\"colid1\">separate</hi></hi><hi rend=\"small\"> Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.</hi></cell>\n</row></table></hi></unexpected></table>", "caption": "Data & modeling perspectives and features used in works based on RNN learning architecture.", "type": "table"}, "_table_hash_full_text": "b9a1dd5d-86bf-4bcd-baba-8056e66b95de", "_full_text_table_hash": "ef95492e-6dde-41e2-89b4-70db1dab2cf5", "context_autogenerated": {"glossary": {"Works<~>Works": "In the context of the table, 'Works' refers to the specific research works or models that have been mentioned in the text and are listed in the first column of the table, each with a corresponding reference.", "Works<~>RLBL": "RLBL refers to the recurrent log-bilinear model for next-item recommendation as described in the given text.", "Works<~>RIB": "RIB in the table refers to the interpretable recommendation framework from the micro behavior perspective (RIB), which models heterogeneous behaviors and dwell time to capture more fine-grained user information using GRU.", "Works<~>BINN": "BINN in the context of the table refers to Behavior-intensive neural network, which is one of the works mentioned in the table that uses RNN learning architecture for multi-behavior sequential recommendation.", "Works<~>CBS": "CBS stands for CBS model, which is mentioned as one of the works in the table that utilizes RNN learning architecture for multi-behavior sequential recommendation.", "Works<~>DIPN": "DIPN in the table refers to the \"Deep Interest-aware Persistent Neural Network\" proposed in the paper with the citation {{cite:9641864}}.", "Works<~>HUP": "HUP stands for Hierarchical User Modeling for Personalized Recommendation, which is one of the works mentioned in the text using an RNN-based learning architecture for recommendation.", "Works<~>IARS": "IARS in the table refers to Intention-aware recommenders system, a work that incorporates the item category to perform next-item recommendation.", "Works<~>DeepRec": "'DeepRec' in the column 'Works' refers to the Deep Recurrent model for multi-behavior sequential recommendation discussed in the text.", "Works<~>MBN": "MBN refers to Multi-behavior network, which is a work that models multi-behavior sequences towards the next-basket recommendation problem using a combination of basket encoder, meta multi-behavior sequence encoder, and recurring-item-aware predictor.", "Data Perspective<~>Data Perspective": "In the context of the given table, 'Data Perspective' refers to the type of input data used in the mentioned works, specifically whether the data consists of a sequence of (item, behavior) pairs or some behavior-specific subsequences of items.", "Data Perspective<~>A sequence of (item, behavior) pairs": "In the given table, under the column 'Data Perspective,' 'A sequence of (item, behavior) pairs' refers to a series of data points, each consisting of an item and an associated user behavior.", "Data Perspective<~>Some behavior-specific subsequences of items": "'Some behavior-specific subsequences of items' in the column 'Data Perspective' refers to a subset of items related to specific user behaviors.", "Model Perspective<~>Model Perspective": "The 'Model Perspective' in the table refers to the perspective from which the behavior types are modeled in the given works, specifically whether it is modeled from a local or a local plus global perspective.", "Model Perspective<~>Local": "In the context of the given table, 'Local' in the 'Model Perspective' column refers to modeling approaches that only consider individual data points or sequences without considering their relations to other data points or sequences.", "Model Perspective<~>Local + Global": "In the context of the table, 'Local + Global' in the 'Model Perspective' column for reference [b1e6c16] refers to a model that utilizes both local and global perspectives. The local perspective refers to modeling user behavior from their own client, while the global perspective involves modeling behavior from the cloud.", "Features<~>Features": "In the context of the table, 'Features' refers to the unique aspects or characteristics of each cited work in the field of multi-behavior sequential recommendation based on RNN learning architecture.", "Features<~>Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix.": "'Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix' in the context of the table refers to the use of a matrix to model the transitions between different behavior types in a user's sequence of interactions, allowing for the modeling of the influence of heterogeneous behaviors on each other.", "Features<~>Leverage GRU and attention mechanism simultaneously.": "'Leverage GRU and attention mechanism simultaneously' in the context of the table refers to using GRU (Gated Recurrent Unit) and attention mechanism together in the modeling process.", "Features<~>Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.": "In the context of the table, 'Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM.' in the column 'Features' refers to creating the Contextual Long Short-Term Memory (CLSTM) and Bidirectional Contextual Long Short-Term Memory (Bi-CLSTM) models, where the behavior vector acts as input context in LSTM.", "Features<~>Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.": "In the context of the table, 'Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation.' in the Features column refers to designing models that handle multiple behavior types in a basket recommendation system, with some parameters shared between the behavior types and others unique to each behavior type.", "Features<~>Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior.": "The statement 'Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior' in the 'Features' column of the table refers to the use of GRU and attention mechanism together in modeling specific interactive behaviors, such as swipe, touch, and browse, in a given sequence.", "Features<~>Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.": "In the context of the table, 'Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items.' in the column 'Features' refers to designing a variant of LSTM called Behavior-LSTM, which includes behavior gate and time gate in addition to attention mechanism, and takes into consideration the category of the items to improve sequential recommendation performance.", "Features<~>Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items.": "The feature 'Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items' in the context of the table refers to the use of Soft-MGRU, which is a multi-behavior gated recurrent unit that shares parameters between behaviors, utilizes an attention mechanism to better capture user intentions, and takes into account the category of the items being considered. Specifically, Soft-MGRU is one type of multi-behavior GRU units used in the Intention-aware recommender system (IARS) to capture multiple intentions of the user.", "Features<~>Utilizing multi-behavior sequence data to make privacy-preserving recommendation.": "'Utilizing multi-behavior sequence data to make privacy-preserving recommendation.' in the column 'Features' refers to the use of multi-behavior sequence data to make recommendations in a privacy-preserving manner using RNN learning architecture.", "Features<~>The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.": "In the context of the table, 'The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.' in the 'Features' column refers to a method in Multi-behavior Network (MBN) where the learned multi-behavior information is assigned to different Behavior-RNN layers at the Meta RNN layer by gathering and then scattering, which is a more explicit way to model intra-behavioral and inter-behavioral sequence information."}, "inputs": {"section_name": "Methods in MBSR", "instruction_sample": "Based on the following text from a scientific paper, answer the question about the table that follows:\n\nTitle: A Survey on Multi-Behavior Sequential Recommendation\n\nAbstract: Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.\n\nSection: Methods in MBSR\nThere are some research works on RNN-based neural network architectures for solving MBSR problems, which differ in terms of the perspective of the input sequences and the perspective of modeling the behavior types. Specifically, from the data perspective, most of the works have an input sequence of (item, behavior) pairs, such as RLBL\u00a0{{cite:bcf84d7}}, RIB\u00a0{{cite:2f67c58}}, BINN\u00a0{{cite:909f272}}, HUP\u00a0{{cite:c76954f}}, IARS\u00a0{{cite:82d42f0}} and DeepRec\u00a0{{cite:b1e6c16}}. In contrast, other works have some behavior-specific subsequences of items, such as CBS\u00a0{{cite:8dbb1e8}}, DIPN\u00a0{{cite:9641864}} and MBN\u00a0{{cite:15341b1}}. From the modeling perspective, DeepRec models a user's behavior types in the cloud from a global perspective and in the user's own client from a local perspective, while other works mentioned above utilize a local perspective only to model the behavior types. We distinguish and summarize these works in Table\u00a0{{table:ef95492e-6dde-41e2-89b4-70db1dab2cf5}}  and describe some of them in detail as shown below.\n\nRLBL.\nThe recurrent log-bilinear model (RLBL)\u00a0{{cite:bcf84d7}} illustrated in Figure\u00a0 {{figure:924391bc-a4e3-45e5-8302-e2d8dc37883b}} is the first work oriented towards next-item recommendation. RLBL integrates the ideas of RNN and log-bilinear (LBL) to address the challenge of long-term and short-term preference modeling. Specifically, RLBL uses behavior-specific transition matrices to distinguish between heterogeneous behaviors in a user's historical interaction sequence, and splits the sequence into multiple windows. Then RLBL captures the short-term contextual information for each window by LBL, and finally integrates these features at the granularity of the window by RNN to construct the user's long-term contextual information.\n\nIn RLBL, each window contains a sequence of (item, behavior) pairs of length {{formula:71254e03-fa15-428c-9679-40a8f4e4e138}} , i.e., {{formula:ed030f53-33a2-46e8-b0ab-9848a277f8b9}}  {{formula:f8d71b3c-1d13-4716-8448-942cfa28426c}} . In the pair {{formula:fa4d2cdc-9dbd-4326-85d0-7012e87810c4}}  of the sequence, RLBL uses an item embedding {{formula:de869d98-485e-408b-97dd-e1159d3ac488}}  to represent the historically interacted item {{formula:66f5ad46-9593-462e-a7fd-0d42e144be55}}  of user {{formula:6e8102d4-5ca7-4b68-939d-f932f414789f}} , a behavior correlation embedding {{formula:b29be62b-99e9-44ad-9dd7-032ce14d9096}}  to represent the user\u2019s behavior {{formula:8c5c7f9b-9441-4194-b586-25050943ccc8}}  for item {{formula:8540a652-ea42-483a-b449-ab966c019085}} , and a position transition embedding {{formula:cfe80008-62be-48d6-bf43-d888593360c2}}  to separately capture the position context information of each position in the window {{formula:feb4b45d-e08b-4772-9950-6fee473ecd9a}} . Hence, the hidden state {{formula:91693d79-0d6f-4e10-8f90-72177af57303}}  at the {{formula:800ec798-344c-4ecc-a2f5-4a107a56f209}} th time step is calculated below:\n{{formula:24461df2-58e9-4b09-a071-02af435f8c39}} \n\n\nwhere {{formula:61803f26-267a-43ae-9146-7ba06cbd86c9}}  is utilized to capture the sequential information between the hidden state {{formula:3b7c4cc2-cb26-48f0-9ae2-d60e2454b73b}}  and the hidden state {{formula:aa1635fd-3d88-4d17-857a-995fb198f518}} . And then the predicted preference that user {{formula:d628f4d6-1737-4d47-89b2-60d56232a417}}  generates behavior {{formula:1d93017b-18a3-41b7-a035-cbbf61133aee}}  on item {{formula:01f92168-ccca-4002-ab88-16f6dde07e01}}  at the {{formula:09037df1-80e9-4cb4-8f31-9932b4c4d894}} th time step is calculated as follows:\n{{formula:665d234d-160a-4bc2-b2d7-7b915041e734}} \n\n\nwhere {{formula:6aef73ea-49ed-4442-b4a5-ba34d58570d3}}  is the user embedding, and {{formula:111ac98f-8ed7-42c1-8e1d-e2e29d9023e5}}  is the representation incorporating the long-term and short-term preferences of user {{formula:89d16a87-ec33-4187-92f1-42ff3da438b0}} .\n\nRLBL and its extended version TA-RLBL\u00a0{{cite:bcf84d7}}, which considers continuous time differences, can model the long-term and short-term context information for data well with the consideration of the sequential and heterogeneous nature of user behaviors. However, the modeling of user behavior is relatively straightforward, and there are some important issues that are overlooked. For example, the transition matrix is the same for all users, and it does not take into account the feature information of the items.\n\nRIB.\nAn interpretable recommendation framework from the micro behavior perspective (RIB)\u00a0{{cite:2f67c58}}, another classic work towards next-item recommendation, models heterogeneous behaviors and dwell time to capture more fine-grained user information by GRU. Specifically, RIB takes a sequence of (item, behavior) pairs as input, taking items and behaviors encoded as item embeddings and behavior embeddings via an embedding layer, respectively. Then the embedding {{formula:0b4698c0-fbcb-49dc-adc2-0d0ac8c7e49d}}  is obtained by concatenating the above two embeddings and fed into a GRU layer to obtain the hidden state at each time step. The calculation equations of the reset gate {{formula:b704b33c-3f79-49a1-9b94-a49e49edad91}} , the update gate {{formula:f29a963c-e222-46f0-b11a-ab077d3f8353}} , the internal state {{formula:9b90e903-43ef-4e96-b435-158100425d91}}  and the external state {{formula:0d0e682d-f307-442e-9f7a-32762ca4e981}}  at the {{formula:62a27fb8-a775-40bf-b6c8-3b5db134b59f}} th time step in GRU are shown below:\n{{formula:7456c28c-d79b-43c8-ab8b-bc05d5a4f050}} \n\n\nwhere {{formula:5bb7cb6b-8a29-4ee1-aba9-c551a49dc465}} , {{formula:d404b4a4-f40e-4973-9c38-619ade78fe0d}} , {{formula:91aa41df-79f7-4976-a63a-90f87b47e6ee}} , and {{formula:9131b026-dce3-4bc0-a7e1-3c81f6d38b14}} , {{formula:0a62f34a-3a4d-42de-8851-dab69fa59f2e}} , {{formula:694128e2-fefe-4033-85ce-cec851808dbe}}  are the learnable weight parameters inside GRU.\nThen the hidden state is passed into an attention layer to get the attention score for each time step. Finally, in the output layer, the hidden states of each time step are multiplied with the corresponding attention scores, where the results are added to obtain a latent representation for predicting the user's preference value for an item.\n\nSimilar to RLBL, RIB introduces different behavioral information into the input side of the RNN (in this case GRU), but the difference is that RIB uses an embedding matrix to represent multiple behavioral information, where each behavior corresponds to an embedding vector. RIB also uses an attention layer to capture the importance of different behaviors, and in the original paper, modeling of dwell time was also considered. Nevertheless, RIB may not capture real user behavior information since it uses an embedding matrix to represent behavior types and then concatenates them directly with the item embedding.\n\nBINN.\nBehavior-intensive neural network (BINN)\u00a0{{cite:909f272}}, based on LSTM, models users' long-term and short-term preferences to improve the next-item recommendation performance.\nBINN takes a sequence of (item, behavior) pairs as input and models each sequence from a local perspective. BINN contains two modules, session behaviors learning (SBL) to model a user's current consumption motivation and preference behaviors learning (PBL) to learn the user's historical stable preference. In SBL, a context-aware LSTM (CLSTM) incorporating the behavioral information as input is built,\nwhose input gate {{formula:32825de9-893c-485a-a064-3bd8b3680f7d}} , forgetting gate {{formula:e4a2bbcb-1221-4740-a390-074f30b98f81}} , output gate {{formula:3b8e7333-243b-46d5-b186-8eaaa8d8cbe6}} , internal state {{formula:7a839976-b468-4246-9a6d-a09daf15773c}}  and external state {{formula:2f2b2546-001f-41ce-8f17-5052038daa33}}  at the {{formula:e5a60b31-480c-468e-9024-eff7b590eec8}} th time step are as follows:\n{{formula:32b69b6d-04e6-4e09-ba68-671c8512d388}} \n\n\nwhere {{formula:1ad11f8f-901a-491f-b831-02cae63a8689}}  are the internal model parameters of the LSTM. Then the output {{formula:b811b7ca-fdb9-4d1c-9b0c-1aae34cab428}}  at the last time step {{formula:4d963c2e-d83c-40f2-8e78-132fd24d06ac}}  can be served as the user's current consumption motivation representation {{formula:ba234b6f-d044-491a-9179-bcb82d7999d0}} .\nIn PBL, BINN adopts a bidirectional CLSTM (Bi-CLSTM) which considers both forward and backward input sequences to obtain the long-term preference representation {{formula:d445b951-25ea-4912-8fdd-9c8991f1202f}} . By concatenating {{formula:a223bd95-c2f8-4286-9a49-0783bda9f8be}}  with {{formula:984c037a-86ee-44e0-ba57-6504405e08ed}} , the obtained representation is utilized to make predictions and generate recommended items.\n\nBINN proposes a novel gating structure, consisting of Bi-CLSTM and CLSTM, which enables the memorization of multi-behavior information in sequences. In contrast to RLBL and RIB in how to introduce multi-behavior information, BINN modifies the internal structure of the LSTM by feeding behavior embedding matrix into the Bi-CLSTM and CLSTM to make it suitable for multi-behavior sequences. However, the limitations of BINN are similar to those of RIB, as both represent multiple types of user behavior directly in embedding matrix, which might make it challenging to capture real user behavioral information.\n\nIARS.\nIntention-aware recommender system (IARS)\u00a0{{cite:82d42f0}} is also a work that incorporates the item category to perform the next-item recommendation task. IARS consists of four blocks in total, which are an RNN-based encoder for perceiving user intent, and three decoders, i.e., a judgment or prediction task based on user intent, so as to learn the complex and co-existing intent of the user. Specifically, the encoder takes a sequence of (item, behavior, category) tuples as input, adopts a local modeling perspective, processes the behavior types through\nmultiple multi-behavior GRU units (MGRUs) to capture multiple intentions of the user. Note that we only discuss Soft-MGRU, one type of MGRU, for its lower spatial complexity and better performance by sharing the same set of parameters between different behaviors. After an embedding layer, the item embedding, category embedding and behavior embedding are fed into Soft-MGRU to obtain the hidden state {{formula:8f7468d5-3009-4e11-8645-e232b0b8c7af}}  at the time step {{formula:7dabbd91-d7fb-49ff-9077-1562585e0f2e}} .\n\nSoft-MGRU encodes the dependencies of items in multi-behavior sequences and obtains hidden states that characterize user intentions.\nIt takes into account item categories and utilizes an attention network to capture the user's purchase intention for candidate items. The introduction of multi-behavior information is also achieved through the GRU's input which concatenates the embeddings of behavior, item, and category. However, the behavior embedding only participates in the computation of the reset gate and update gate, which may not be sufficient to represent the complete behavior information of the user.\n\nMBN.\nMulti-behavior network (MBN)\u00a0{{cite:15341b1}} models multi-behavior sequences towards the next-basket recommendation problem.\nThe MBN architecture is composed of three modules, i.e., basket encoder, meta multi-behavior sequence encoder and recurring-item-aware predictor.\nSpecifically, the basket encoder converts the item representation {{formula:1117a756-cdd8-4c3b-be68-c8cdba83b39e}}  to the basket representation of the items {{formula:ed044bec-712f-4286-a3c6-31dfab475be6}}  by a max pooling method.\nIn the meta multi-behavior sequence encoder, multiple behavior-specific subsequences of items are taken as input and go through Behavior-RNN layers to learn behavior-specific information, which is local in the perspective of modeling. In addition to the Behavior-RNN layers, this work also proposes a Meta-RNN layer to learn the collective knowledge of multi-behavior sequences. Then a gathering-scattering scheme is utilized to correlate the Meta-RNN layer and the Behavior-RNN layer. The representations learned by the Behavior-RNN layers are gathered to the Meta-RNN layer to learn the collective knowledge of multi-behavior sequences, and then the representations learned by the Meta-RNN layer are scattered to the individual Behavior-RNN layer to calibrate behavioral modeling.\nIn the recurring-item-aware predictor, a mixed probabilistic function in the generate mode and the repeat mode is proposed to predict the probability of each item in the next basket, which can simulate the distribution of items with biased repetition.\n\nMBN introduces a method of gathering and then scattering to fuse and assign the learned multi-behavior information to different Behavior-RNNs layers at the Meta RNN layer, which is a more explicit way to model intra-behavioral and inter-behavioral sequence information. In addition, any type of user's behavior can be treated as the target behavior. Nonetheless, as the number of behavior types increases, the number of behavioral RNN layers and associated parameters also increases, resulting in heightened computational complexity. Moreover, the division of the item basket in MBN is based on the time span, which may not align with the real-world scenario of purchasing a basket of items at the same time.\n\nIn addition to the above works, several efforts employ RNN-based learning architecture to model the sequentiality and heterogeneity of user behaviors. CBS\u00a0{{cite:8dbb1e8}} models longer sequences rather than short-term dependencies for the next-basket recommendation problem with the use of a LSTM with or without shared parameters for each of the two behaviors (or the representation obtained from the embedding layer directly for the target behavior sequence). DIPN\u00a0{{cite:9641864}} employs a GRU and a hierarchical attention mechanism to effectively capture heterogeneous user behaviors and utilizes a multi-task module to capture short-term and long-term purchase preferences. HUP utilizes the attention mechanism, and designs LSTMs with the addition of behavior gate and time gate at the micro-, item-, and category-levels to capture different granularities of information from session-based recommendation. In terms of federated recommendation, DeepRec\u00a0{{cite:b1e6c16}} applies GRU on the historical interaction data of all users on the cloud, and is then pushed to users' devices, which makes it possible to fine-tune it for the individuals to obtain a personal recommendation model for each of them.\n\nIn summary, the RNN-based learning architecture is suitable for sequence problems and can store short-term memories, but suffers from gradient disappearance and gradient explosion problems. In addition, RNN is inefficient and has difficulty in predicting information about future sequences since the output of the current moment depends on the computation and the output of the previous moment. At present, the industry has rarely leveraged RNN-based learning architecture for recommendation.\n\nIn MBSR, there are lots of works achieving great recommendation performance based on GNN, such as MGNN-SPred\u00a0{{cite:ef27bb1}}, DMBGN\u00a0{{cite:51fcf28}}, GPG4HSR\u00a0{{cite:eafaa65}}, BGNN\u00a0{{cite:2fe2021}} and BA-GNN\u00a0{{cite:c5008f0}}. We describe some of them in detail below, and summarize the data perspective, the modeling perspective and the characteristics of these works in Table\u00a0{{table:ec69d321-0332-4d1c-a502-d66a5faa52e5}} .\n\nMGNN-SPred.\nMulti-relational graph neural network model for session-based target behavior prediction (MGNN-SPred)\u00a0{{cite:ef27bb1}} also utilizes GNN to model multi-behavior sequences in session-based recommendation scenarios from a global modeling perspective.\n\n\n---\nBased on the above text, in the context of the following table, what does 'The overall Meta-RNN and the separate Behavior-RNN share the learned potential representations by gathering and then scattering; towards the next-basket recommendation.' in the column 'Features' refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\nTable: ef95492e-6dde-41e2-89b4-70db1dab2cf5\n         References                                           Features\n0  {{cite:bcf84d7}}  Capture the influence of heterogeneous behavio...\n1  {{cite:2f67c58}}  Leverage GRU and attention mechanism simultane...\n2  {{cite:909f272}}  Design the CLSTM and the Bi-CLSTM, where the b...\n3  {{cite:8dbb1e8}}  Design of models with and without shared param...\n4  {{cite:9641864}}  Leverage GRU and attention mechanism simultane...\n5  {{cite:c76954f}}  Design the Behavior-LSTM where adds behavior g...\n6  {{cite:82d42f0}}  Propose Soft-MGRU (a multi-behavior gated recu...\n7  {{cite:b1e6c16}}  Utilizing multi-behavior sequence data to make...\n8  {{cite:15341b1}}  The overall Meta-RNN and the separate Behavior...\nCaption: Data & modeling perspectives and features used in works based on RNN learning architecture."}}, "title": "A Survey on Multi-Behavior Sequential Recommendation", "abstract": "Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR."}
{"paper_id": "2308.06507v1", "_pdf_hash": null, "_source_hash": "3c4a079ff1d8bf3c750a009e971b6b4ff4b3d28d", "_source_name": "2308.06507v1", "_table_hash": "d5efb5c3-ff9c-4944-b60d-c70574c8ed7c", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid1\" id-text=\"1\" place=\"t\" rend=\"display\"><head><hi rend=\"small\">The differences between AutoConv and others. DG represents whether the augmentation is document grounded, and Data Needs denotes the scale of human conversations used for augmentation.</hi></head>\n<unexpected><p rend=\"center\"><hi rend=\"small\"><table rend=\"inline\"><tr bottom-border=\"true\"><td halign=\"left\"><rule depth=\"0.0pt\" height=\"1.5pt\"/>\n<hi rend=\"small\"><hi rend=\"bold\">Method</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">DG</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Data Needs</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">EDA <cit sha=\"ff1c82f64a859c31a98d2df0ef19ea86c0e8ab58\"><ref target=\"bid6\"/></cit>{{cite:ff1c82f}}</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">-</td>\n</tr><tr><td halign=\"left\">Back-Translation <cit sha=\"2f58ff567386b6e3be66e954eba439629c09486f\"><ref target=\"bid7\"/></cit>{{cite:2f58ff5}}</td>\n<td halign=\"center\">red\u2717</td>\n<td halign=\"center\">-</td>\n</tr><tr><td halign=\"left\">SeemSeek <cit sha=\"8449650be7ca3aafa136f7763cbbd03ea74a45f8\"><ref target=\"bid9\"/></cit>{{cite:8449650}}</td>\n<td halign=\"center\">green\u2714</td>\n<td halign=\"center\">Large</td>\n</tr><tr><td halign=\"left\">Dialog Inpainting <cit sha=\"e3d2ed90e6ec3cf61fb40eda6a7981bc98e30f94\"><ref target=\"bid3\"/></cit>{{cite:e3d2ed9}}</td>\n<td halign=\"center\">green\u2714</td>\n<td halign=\"center\">Large</td>\n</tr><tr><td halign=\"left\">aliceblue!60\n<hi rend=\"small\"><hi rend=\"bold\">AutoConv</hi></hi><hi rend=\"small\">(Ours)</hi></td>\n<td halign=\"center\">green\u2714</td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Few</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\"><rule depth=\"0.0pt\" height=\"1.5pt\"/></td>\n</tr></table>\n</hi></p><p rend=\"center\" spacebefore=\"-10.0pt\"><hi rend=\"small\"/></p></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["\nMethod", "DG", "Data Needs"], ["EDA {{cite:ff1c82f}}", "red\u2717", "-"], ["Back-Translation {{cite:2f58ff5}}", "red\u2717", "-"], ["SeemSeek {{cite:8449650}}", "green\u2714", "Large"], ["Dialog Inpainting {{cite:e3d2ed9}}", "green\u2714", "Large"], ["aliceblue!60\nAutoConv(Ours)", "green\u2714", "Few"]], "table_dict": {"References": ["{{cite:ff1c82f}}", "{{cite:2f58ff5}}", "{{cite:8449650}}", "{{cite:e3d2ed9}}", "-"], "Method": ["EDA ", "Back-Translation ", "SeemSeek ", "Dialog Inpainting ", "AutoConv (Ours)"], "DG": ["\u2717", "\u2717", "\u2714", "\u2714", "\u2714"], "Data Needs": ["-", "-", "Large", "Large", "Few"]}}, "bib_hash": ["ff1c82f64a859c31a98d2df0ef19ea86c0e8ab58", "2f58ff567386b6e3be66e954eba439629c09486f", "8449650be7ca3aafa136f7763cbbd03ea74a45f8", "e3d2ed90e6ec3cf61fb40eda6a7981bc98e30f94"], "row_bib_map": [{"bib_hash_or_arxiv_id": "ff1c82f64a859c31a98d2df0ef19ea86c0e8ab58", "row": 0, "corpus_id": 59523656, "type": "ref"}, {"bib_hash_or_arxiv_id": "2f58ff567386b6e3be66e954eba439629c09486f", "row": 1, "corpus_id": 15600925, "type": "ref"}, {"bib_hash_or_arxiv_id": "8449650be7ca3aafa136f7763cbbd03ea74a45f8", "row": 2, "corpus_id": 249062776, "type": "ref"}, {"bib_hash_or_arxiv_id": "e3d2ed90e6ec3cf61fb40eda6a7981bc98e30f94", "row": 3, "corpus_id": 248863311, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.06507v1", "row": 4, "corpus_id": 259370708, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid1\" place=\"t\"><head><hi rend=\"small\">The differences between AutoConv and others. DG represents whether the augmentation is document grounded, and Data Needs denotes the scale of human conversations used for augmentation.</hi></head>\n<unexpected><p rend=\"center\"><hi rend=\"small\"><table rend=\"inline\"><row bottom-border=\"true\"><cell halign=\"left\"><rule depth=\"0.0pt\" height=\"1.5pt\"/>\n<hi rend=\"small\"><hi rend=\"bold\">Method</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">DG</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Data Needs</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">EDA <cit sha=\"ff1c82f64a859c31a98d2df0ef19ea86c0e8ab58\"><ref target=\"bid6\"/></cit>{{cite:ff1c82f}}</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">-</cell>\n</row><row><cell halign=\"left\">Back-Translation <cit sha=\"2f58ff567386b6e3be66e954eba439629c09486f\"><ref target=\"bid7\"/></cit>{{cite:2f58ff5}}</cell>\n<cell halign=\"center\">red\u2717</cell>\n<cell halign=\"center\">-</cell>\n</row><row><cell halign=\"left\">SeemSeek <cit sha=\"8449650be7ca3aafa136f7763cbbd03ea74a45f8\"><ref target=\"bid9\"/></cit>{{cite:8449650}}</cell>\n<cell halign=\"center\">green\u2714</cell>\n<cell halign=\"center\">Large</cell>\n</row><row><cell halign=\"left\">Dialog Inpainting <cit sha=\"e3d2ed90e6ec3cf61fb40eda6a7981bc98e30f94\"><ref target=\"bid3\"/></cit>{{cite:e3d2ed9}}</cell>\n<cell halign=\"center\">green\u2714</cell>\n<cell halign=\"center\">Large</cell>\n</row><row><cell halign=\"left\">aliceblue!60\n<hi rend=\"small\"><hi rend=\"bold\">AutoConv</hi></hi><hi rend=\"small\">\u00a0(Ours)</hi></cell>\n<cell halign=\"center\">green\u2714</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Few</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\"><rule depth=\"0.0pt\" height=\"1.5pt\"/></cell>\n</row></table>\n</hi></p><p spacebefore=\"-10.0pt\" rend=\"center\"><hi rend=\"small\"/></p></unexpected></table>", "caption": "The differences between AutoConv and others. DG represents whether the augmentation is document grounded, and Data Needs denotes the scale of human conversations used for augmentation.", "type": "table"}, "_table_hash_full_text": "3a5f03eb-0347-41d1-980e-6f4e7499584f", "_full_text_table_hash": "6acfd98a-95fa-4602-8898-8b48434cefe8", "context_autogenerated": {"glossary": {"Method<~>Method": "'Method' in the table refers to the specific data augmentation techniques used, including EDA (Exploratory Data Analysis), Back-Translation, SeemSeek, Dialog Inpainting, and AutoConv (proposed by the authors).", "Method<~>EDA": "EDA in the context of the table refers to Exploratory Data Analysis. However, in the given context of the table, it is unclear why EDA is mentioned in reference ff1c82f as it is not mentioned in the provided text about AutoConv or the other methods.", "Method<~>Back-Translation": "In the context of the table, 'Back-Translation' in the 'Method' column refers to a specific data augmentation technique where the model is translated into another language and then back-translated back to the original language, which is believed to improve the linguistic diversity and fluency of the data.", "Method<~>SeemSeek": "'SeemSeek' in the column 'Method' refers to a specific simulation-based method proposed in [[8449650]] for generating information-seeking conversations.", "Method<~>Dialog Inpainting": "Dialog Inpainting is a method that creates information-seeking dialogues by inserting utterances between neighboring sentences in documents.", "Method<~>AutoConv (Ours)": "'AutoConv (Ours)' in the table refers to the method proposed by the researchers in the text, which utilizes large language models for automatically generating information-seeking conversations.", "DG<~>DG": "DG in the table refers to whether the data augmentation is document grounded.", "Data Needs<~>Data Needs": "In the context of the table, 'Data Needs' refers to the scale of human conversations used for data augmentation in different methods.", "Data Needs<~>Large": "In the context of the table, 'Large' in the column 'Data Needs' refers to the requirement of a significant amount of human conversations for some methods.", "Data Needs<~>Few": "In the context of the table, 'Few' in the column 'Data Needs' refers to a smaller scale of human conversations used for augmentation compared to 'Large'."}, "inputs": {"section_name": "Introduction", "instruction_sample": "Based on the following text from a scientific paper, answer the question about the table that follows:\n\nTitle: AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models\n\nAbstract: Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.\n\nSection: Introduction\nIn information-seeking conversations, users repeatedly ask questions based on their interests, and the dialogue system provides answers to fulfill their information needs {{cite:118345b}}, {{cite:3a72de5}}, {{cite:8f1d03b}}.\nThis scenario is important for addressing real-world open-ended questions, which requires discussions to explore in depth {{cite:e3d2ed9}}, e.g., How to learn more efficiently?\nThough great progress has been achieved in recent years, most existing researches depend on abundant human annotation, which can be highly costly and limited in knowledge coverage.\n\nA promising way to alleviate this problem is data augmentation {{cite:3f78c39}}.\nTraditional methods, including token-level manipulation {{cite:c02ab75}}, {{cite:ff1c82f}} and sentence-level paraphrasing {{cite:2f58ff5}}, improve the linguistic diversity of training data.\nHowever, they cannot create conversations grounded on new documents, which are indispensable for dealing with out-of-domain scenarios.\nAnother line of research focuses on simulation-based methods\n{{cite:97daeba}}, {{cite:8449650}}.\nSpecifically, they can iteratively generate conversations grounded on new documents based on a span extractor and an utterance generator.\nNevertheless, both the training of the extractor and the generator still require abundant human dialogues.\nBesides the above ways,\n{{cite:e3d2ed9}} propose Dialog Inpainting, which creates information-seeking dialogues by inserting utterances between neighboring sentences in documents.\nOne potential risk is the gap between the structure of documents and that of conversations.\nDocuments are tighter, while real-world conversations are more open-ended.\n\nTo alleviate the above issues, we propose a simple yet effective method AutoConv for Automatically generating information-seeking Conversations, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM) {{cite:de3097f}}.\nSpecifically, we formulate conversation generation as a language modeling task and utilize an LLM for generating synthetic conversations grounded on external documents.\nSurprisingly, finetuning with a few human dialogues can help LLM capture the characteristics of the information-seeking process (e.g., grounding, question answering) and generate high-quality synthetic conversations.\nThen, we can train a small task model with these dialogues.\nThe differences between AutoConv and others are shown in Table\u00a0{{table:6acfd98a-95fa-4602-8898-8b48434cefe8}} .\n\nWe conduct comprehensive experiments on two frequently-used datasets QuAC {{cite:3a72de5}} and CoQA {{cite:8f1d03b}} in the low-resource setting, where only dozens of human dialogues are available.\nThe results show that AutoConv has substantial improvements over several strong baselines.\nWhen scaling up the synthetic dialogues, AutoConv has the improvement of up to 5.06 F1 gain compared with directly finetuning, and thus largely reduces the labor force for annotation.\nIn addition, we find that the small task model trained with synthetic dialogues can even surpass finetuned LLM with only {{formula:0b3dfa51-be36-411e-aaf9-ee02dff1426b}}  parameters.\nMoreover, we also investigate the impact of decoding strategy and scaling laws for AutoConv.\n\n\n---\nBased on the above text, in the context of the following table, what does 'Few' in the column 'Data Needs' refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\nTable: 6acfd98a-95fa-4602-8898-8b48434cefe8\n         References Data Needs\n0  {{cite:ff1c82f}}          -\n1  {{cite:2f58ff5}}          -\n2  {{cite:8449650}}      Large\n3  {{cite:e3d2ed9}}      Large\n4                 -        Few\nCaption: The differences between AutoConv and others. DG represents whether the augmentation is document grounded, and Data Needs denotes the scale of human conversations used for augmentation."}}, "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models", "abstract": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research."}
{"paper_id": "2308.15701v1", "_pdf_hash": null, "_source_hash": "ea08a3ecc996b3092887adf8f6675fed4219ea4b", "_source_name": "2308.15701v1", "_table_hash": "7efdbf19-1e0e-4e51-830a-df5830384bc7", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid41\" id-text=\"3\" place=\"htbp\" rend=\"display\" starred=\"true\"><head>Data &amp; modeling perspectives and features used in works based on GNN learning architecture.</head>\n<unexpected><hi rend=\"small\"><table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\" right-border=\"true\">Works</td>\n<td halign=\"center\" right-border=\"true\">Data Perspective</td>\n<td halign=\"center\">Model Perspective</td>\n<td>Features</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">MGNN-SPred<cit sha=\"ef27bb128b97c97090b0638111cd58c3f702ea50\"><ref target=\"bid23\"/></cit>{{cite:ef27bb1}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequences of items</td>\n<td halign=\"center\">Global</td>\n<td>Modeling behavior from behavior transition relations, containing <hi rend=\"small\"><hi color=\"colid1\">homogeneous behavior transitions intra each kind of behavior-specific subsequences</hi></hi><hi rend=\"small\">.</hi></td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">DMBGN<cit sha=\"51fcf28c1e702b9b35ed6e1a7e94d647d0678676\"><ref target=\"bid70\"/></cit>{{cite:51fcf28}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequences of items</td>\n<td halign=\"center\">Global</td>\n<td>Focus on the task of voucher redemption rate prediction and <hi rend=\"small\"><hi color=\"colid1\">model the relationship between</hi></hi><hi rend=\"small\"> multiple behaviors and vouchers effectively.</hi></td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">GPG4HSR<cit sha=\"eafaa65b7bec60662cc14f62ec8510c7a919d787\"><ref target=\"bid71\"/></cit>{{cite:eafaa65}}</td>\n<td halign=\"center\" right-border=\"true\">A sequence of (item, behavior) pairs</td>\n<td halign=\"center\">Local + Global</td>\n<td>Learn various behavior transition relations from the global graph and the personalized graph, respectively.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">BGNN<cit sha=\"2fe20212cfb77eece0e595e6b641dc6bcf8966c7\"><ref target=\"bid72\"/></cit>{{cite:2fe2021}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequences of items</td>\n<td halign=\"center\">Global</td>\n<td>Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information.</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\" right-border=\"true\">BA-GNN<cit sha=\"c5008f0fbeeeb19928c47e6d6a6588e6991c418a\"><ref target=\"bid73\"/></cit>{{cite:c5008f0}}</td>\n<td halign=\"center\" right-border=\"true\">Some behavior-specific subsequence of items</td>\n<td halign=\"center\">Global</td>\n<td>Construct directed graphs for different behavior-specific sequences respectively.</td>\n</tr></table></hi></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Works", "Data Perspective", "Model Perspective", "Features"], ["MGNN-SPred{{cite:ef27bb1}}", "Some behavior-specific subsequences of items", "Global", "Modeling behavior from behavior transition relations, containing homogeneous behavior transitions intra each kind of behavior-specific subsequences."], ["DMBGN{{cite:51fcf28}}", "Some behavior-specific subsequences of items", "Global", "Focus on the task of voucher redemption rate prediction and model the relationship between multiple behaviors and vouchers effectively."], ["GPG4HSR{{cite:eafaa65}}", "A sequence of (item, behavior) pairs", "Local + Global", "Learn various behavior transition relations from the global graph and the personalized graph, respectively."], ["BGNN{{cite:2fe2021}}", "Some behavior-specific subsequences of items", "Global", "Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information."], ["BA-GNN{{cite:c5008f0}}", "Some behavior-specific subsequence of items", "Global", "Construct directed graphs for different behavior-specific sequences respectively."]], "table_dict": {"References": ["{{cite:ef27bb1}}", "{{cite:51fcf28}}", "{{cite:eafaa65}}", "{{cite:2fe2021}}", "{{cite:c5008f0}}"], "Works": ["MGNN-SPred", "DMBGN", "GPG4HSR", "BGNN", "BA-GNN"], "Data Perspective": ["Some behavior-specific subsequences of items", "Some behavior-specific subsequences of items", "A sequence of (item, behavior) pairs", "Some behavior-specific subsequences of items", "Some behavior-specific subsequence of items"], "Model Perspective": ["Global", "Global", "Local + Global", "Global", "Global"], "Features": ["Modeling behavior from behavior transition relations, containing homogeneous behavior transitions intra each kind of behavior-specific subsequences.", "Focus on the task of voucher redemption rate prediction and model the relationship between multiple behaviors and vouchers effectively.", "Learn various behavior transition relations from the global graph and the personalized graph, respectively.", "Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information.", "Construct directed graphs for different behavior-specific sequences respectively."]}}, "bib_hash": ["ef27bb128b97c97090b0638111cd58c3f702ea50", "51fcf28c1e702b9b35ed6e1a7e94d647d0678676", "eafaa65b7bec60662cc14f62ec8510c7a919d787", "2fe20212cfb77eece0e595e6b641dc6bcf8966c7", "c5008f0fbeeeb19928c47e6d6a6588e6991c418a"], "row_bib_map": [{"bib_hash_or_arxiv_id": "ef27bb128b97c97090b0638111cd58c3f702ea50", "row": 0, "corpus_id": 211171550, "type": "ref"}, {"bib_hash_or_arxiv_id": "51fcf28c1e702b9b35ed6e1a7e94d647d0678676", "row": 1, "corpus_id": 235358755, "type": "ref"}, {"bib_hash_or_arxiv_id": "eafaa65b7bec60662cc14f62ec8510c7a919d787", "row": 2, "corpus_id": 252216587, "type": "ref"}, {"bib_hash_or_arxiv_id": "2fe20212cfb77eece0e595e6b641dc6bcf8966c7", "row": 3, "corpus_id": 255894127, "type": "ref"}, {"bib_hash_or_arxiv_id": "c5008f0fbeeeb19928c47e6d6a6588e6991c418a", "row": 4, "corpus_id": 256832144, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"3\" id=\"uid41\" starred=\"true\" place=\"htbp\"><head>Data &amp; modeling perspectives and features used in works based on GNN learning architecture.</head>\n<unexpected><hi rend=\"small\"><table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell right-border=\"true\" halign=\"center\">Works</cell>\n<cell right-border=\"true\" halign=\"center\">Data Perspective</cell>\n<cell halign=\"center\">Model Perspective</cell>\n<cell>Features</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">MGNN-SPred\u00a0<cit sha=\"ef27bb128b97c97090b0638111cd58c3f702ea50\"><ref target=\"bid23\"/></cit>{{cite:ef27bb1}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequences of items</cell>\n<cell halign=\"center\">Global</cell>\n<cell>Modeling behavior from behavior transition relations, containing <hi rend=\"small\"><hi color=\"colid1\">homogeneous behavior transitions intra each kind of behavior-specific subsequences</hi></hi><hi rend=\"small\">.</hi></cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">DMBGN\u00a0<cit sha=\"51fcf28c1e702b9b35ed6e1a7e94d647d0678676\"><ref target=\"bid70\"/></cit>{{cite:51fcf28}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequences of items</cell>\n<cell halign=\"center\">Global</cell>\n<cell>Focus on the task of voucher redemption rate prediction and <hi rend=\"small\"><hi color=\"colid1\">model the relationship between</hi></hi><hi rend=\"small\"> multiple behaviors and vouchers effectively.</hi></cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">GPG4HSR\u00a0<cit sha=\"eafaa65b7bec60662cc14f62ec8510c7a919d787\"><ref target=\"bid71\"/></cit>{{cite:eafaa65}}</cell>\n<cell right-border=\"true\" halign=\"center\">A sequence of (item, behavior) pairs</cell>\n<cell halign=\"center\">Local + Global</cell>\n<cell>Learn various behavior transition relations from the global graph and the personalized graph, respectively.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">BGNN\u00a0<cit sha=\"2fe20212cfb77eece0e595e6b641dc6bcf8966c7\"><ref target=\"bid72\"/></cit>{{cite:2fe2021}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequences of items</cell>\n<cell halign=\"center\">Global</cell>\n<cell>Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information.</cell>\n</row><row bottom-border=\"true\"><cell right-border=\"true\" halign=\"center\">BA-GNN\u00a0<cit sha=\"c5008f0fbeeeb19928c47e6d6a6588e6991c418a\"><ref target=\"bid73\"/></cit>{{cite:c5008f0}}</cell>\n<cell right-border=\"true\" halign=\"center\">Some behavior-specific subsequence of items</cell>\n<cell halign=\"center\">Global</cell>\n<cell>Construct directed graphs for different behavior-specific sequences respectively.</cell>\n</row></table></hi></unexpected></table>", "caption": "Data & modeling perspectives and features used in works based on GNN learning architecture.", "type": "table"}, "_table_hash_full_text": "a92b8c12-63be-4f32-b185-05d25673f42a", "_full_text_table_hash": "ec69d321-0332-4d1c-a502-d66a5faa52e5", "context_autogenerated": {"glossary": {"Works<~>Works": "'Works' in the table refer to specific recommendation systems or models that have been developed and studied using Graph Neural Network (GNN) learning architecture, as listed in the references column.", "Works<~>MGNN-SPred": "MGNN-SPred refers to a specific work called Multi-Graph Neural Network with Symmetric Propagation for sequential recommendation.", "Works<~>DMBGN": "'DMBGN' in the column 'Works' refers to a specific work named \"DMBGN\" cited as {{cite:51fcf28}}. The exact meaning of DMBGN is not clear from the context provided in the text.", "Works<~>GPG4HSR": "GPG4HSR refers to a specific work mentioned in the table, but without additional context, it is not possible to determine what the acronym stands for.", "Works<~>BGNN": "BGNN in the context of the table refers to the work \"BGNN\" mentioned in the second row under the \"Works\" column. Without additional context, it is not clear what \"BGNN\" stands for.", "Works<~>BA-GNN": "BA-GNN in the table refers to Batched And Parallel Graph Neural Network.", "Data Perspective<~>Data Perspective": "In the context of the table, 'Data Perspective' refers to the type of data used in the referred works based on graph neural network (GNN) learning architecture.", "Data Perspective<~>Some behavior-specific subsequences of items": "The phrase 'Some behavior-specific subsequences of items' in the Data Perspective column refers to specific sequences of items that exhibit certain user behaviors, and are used as data inputs in the given works based on Graph Neural Network (GNN) learning architecture.", "Data Perspective<~>A sequence of (item, behavior) pairs": "The third entry in the 'Data Perspective' column, '{{cite:2fe2021}}', refers to a sequence of (item, behavior) pairs in the context of the data used in the work based on the Graph Neural Network (GNN) learning architecture.", "Data Perspective<~>Some behavior-specific subsequence of items": "'Some behavior-specific subsequences of items' in the table refers to a sequence of items that are related to a particular user behavior.", "Model Perspective<~>Model Perspective": "In the given context, 'Model Perspective' refers to the approach or methodology used in the models mentioned in the table. Specifically, it indicates whether the model uses a global or local approach in relation to the graph neural network (GNN) learning architecture.", "Model Perspective<~>Global": "In the context of the table, 'Global' in the 'Model Perspective' column refers to models that consider the entire graph or sequence data, rather than just local neighborhood information.", "Model Perspective<~>Local + Global": "In the given table, 'Local + Global' in the 'Model Perspective' column for reference [[eafaa65]] refers to a modeling approach that utilizes both local and global information in the graph neural network for effective learning.", "Features<~>Features": "In the context of the table, 'Features' refer to the specific characteristics or attributes of the works based on graph neural network (GNN) learning architecture that are mentioned in the references.", "Features<~>Modeling behavior from behavior transition relations, containing homogeneous behavior transitions intra each kind of behavior-specific subsequences.": "In the context of the table, 'Modeling behavior from behavior transition relations, containing homogeneous behavior transitions intra each kind of behavior-specific subsequences.' in the 'Features' column of reference [0] refers to modeling the relationships between different behaviors, specifically focusing on homogeneous transitions within each subsequence of behavior-specific transitions.", "Features<~>Focus on the task of voucher redemption rate prediction and model the relationship between multiple behaviors and vouchers effectively.": "The statement 'Focus on the task of voucher redemption rate prediction and model the relationship between multiple behaviors and vouchers effectively' in the 'Features' column of the table refers to utilizing GNN learning architecture for predicting voucher redemption rates and modeling the relationship between different user behaviors and vouchers.", "Features<~>Learn various behavior transition relations from the global graph and the personalized graph, respectively.": "'Learn various behavior transition relations from the global graph and the personalized graph, respectively' in the context of the table refers to learning behavior patterns and relationships from both the overall graph representing interactions among all users and items, and personalized graphs representing individual user interactions.", "Features<~>Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information.": "The phrase \"Construct directed graphs for different behavior transition (homogeneous and heterogeneous) information.\" in the 'Features' column refers to creating graph structures that represent the transitions between different behaviors (homogeneous and heterogeneous) for use in graph neural network (GNN) learning architecture.", "Features<~>Construct directed graphs for different behavior-specific sequences respectively.": "'Construct directed graphs for different behavior-specific sequences respectively' in the 'Features' column refers to creating directed graphs for different sequences of user behavior using the graph neural network (GNN) learning architecture."}, "inputs": {"section_name": "Basic Paradigm", "instruction_sample": "Based on the following text from a scientific paper, answer the question about the table that follows:\n\nTitle: A Survey on Multi-Behavior Sequential Recommendation\n\nAbstract: Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.\n\nSection: Basic Paradigm\nIn the case with implicit feedback data, the similarity between two items can be calculated using measures such as the Jaccard index and the cosine similarity.\nTaking item-based collaborative filtering with the Jaccard index as an example, the similarity of items {{formula:bf30d2f9-67e6-4754-adc0-7d3850f71000}}  and {{formula:d04eac34-ecbf-450c-b8df-6b9f1356c25b}}  is calculated as follows:\n{{formula:69fecd3f-f330-487c-a815-c641284ee486}} \n\n\nwhere {{formula:2b768011-c76b-4afc-9801-239ed974ba14}}  and {{formula:59b1ae4a-1b27-4e05-8b18-afc67e3ade14}}  denote the set of users who have interacted with item {{formula:0ef079fb-5dc2-4c10-a414-e0d5929beb5c}}  and item {{formula:7cd0251f-96ec-4dcc-b515-c9903519b1d7}} , respectively.\n\nBased on the calculated similarity, we may select the top-{{formula:65ebfa0d-06ce-411f-9442-3e19109f4a8d}}  nearest item set {{formula:d47306d0-4197-4818-93a0-9ac21feaff6b}}  for each item {{formula:a631a6f0-9ac1-4cf5-b8f6-0295afac9088}} , and then predict the score according to the following formula:\n{{formula:1712f2bb-af8d-45ef-ab22-13a32d7e6fe7}} \n\nAs the predicted rating of an item increases, the possibility that the user will be interested in it increases accordingly. Although there is almost no work for MBSR using neighborhood-based methods, we will introduce BIS\u00a0{{cite:663b593}}, a work toward the SBSR problem, to illustrate the idea of the use of similarity in sequential recommendation. It is expected to have some possibilities and inspirations to solve the MBSR problem.\n\nIn recommender systems, the idea of matrix factorization is mainly reflected in transforming the (user, item) interaction matrix into the inner product of two low-rank matrices, i.e., a user-specific matrix and an item-specific matrix. Taking the rating matrix {{formula:00c2a104-c231-4538-81d8-4423c5b59713}} formed by (user, item) interactions as an example, {{formula:569ef163-4430-47f6-aba8-ba85ca475887}}  is decomposed into a user matrix {{formula:fc3467b1-d2ea-451a-a307-57ccf348d2d4}} and an item matrix {{formula:bd1e24a9-2213-4ec4-84f7-b8133275511f}}, so that each missing value (i.e., a predicted value) {{formula:140e3bc4-63ff-4db0-8866-78799dd125d2}}  in the rating matrix can be obtained by multiplying the user embedding {{formula:d1db9bde-c0e5-4095-8729-48b256fd4437}}  and the item embedding {{formula:32a75a4a-ec6d-408a-966c-24868f711461}} :\n{{formula:4fe7fb32-1e57-4ab1-ad75-1fb700615e16}} \n\nRecurrent neural network (RNN)\u00a0{{cite:8d0592d}} is a classical deep learning method that can effectively process data with sequentiality. Currently, RNN has been applied to numerous fields, including information retrieval, speech recognition, machine translation and so on. Since RNN can take into account the characteristics of sequences, they have also been utilized to solve the SBSR problems and MBSR problems in the early works\u00a0{{cite:31c0718}}, {{cite:bcf84d7}}, {{cite:2f67c58}}.\n\nRNN contains multiple RNN cells, with its basic structure illustrated in Figure\u00a0{{figure:d8334c47-ac03-4f60-b958-8ea841213ca4}} . In the RNN learning architecture, the current time step receives the output of the previous time step as the input, and the output obtained from the RNN cell will be used as the input of the next time step, so as to capture the sequential nature of the data. Each cell of RNN is a layer of deep feedforward neural network, and a set of learning parameters will be shared across different time steps to capture sequential features and reduce the model complexity. The basic formula of RNN is as follows:\n{{formula:abff4ec3-9e65-4d46-bafa-3cca491e9ef9}} \n\n\nwhere {{formula:d3d6d9d8-9f07-4f7f-88c7-eaf37227118b}} , {{formula:e741902c-a6c2-4c3a-8b56-11791770e783}}  and {{formula:cf555fe4-5d5d-4a60-bb94-14fb0c388973}}  are the corresponding weight matrices, and {{formula:3770496b-fe6b-415e-b21e-b6f2901163cf}}  and {{formula:4d95e642-4e98-4b0a-9551-5c202b97e205}}  are the corresponding bias vectors.\n\nHowever, there is a certain challenge in the training process of RNN, that is, as the depth deepens, RNN has the problem of gradient disappearance or gradient explosion, and thus is prone to the difficulty in dealing with the long-term dependency of data\u00a0{{cite:f8e4660}}. To address this issue, many derivative methods based on RNN have been proposed, among which the most well-known ones are long short-term memory (LSTM)\u00a0{{cite:8b037c3}} and gated recurrent unit (GRU)\u00a0{{cite:328a947}}, a simplified version of LSTM. Both LSTM and GRU set up a hidden unit in the hidden layer to store long-term features, which enables to address the issue of modeling long-term data dependency.\n\nIn general, graph neural network models use graph convolution to allow nodes to obtain information about their neighbors. To make the procedure more specific, an example is shown in Figure\u00a0{{figure:111c5d1b-7226-4efa-9119-cc58af142a41}} , which depicts four nodes, labeled as node 1, node 2, node 3, and node 4. Node 1's first-order neighbors are node 2 and node 3. During the first-order graph convolution, the embeddings of node 2 and node 3 are aggregated into the embedding of node 1. In the second-order graph convolution, node 3 is a neighbor of node 4. Since node 3 has already obtained information about node 4 in the first-order graph convolution, node 1 is able to obtain information about its second-order neighbor, node 4, during the second-order graph convolution. This allows the graph convolution network to effectively utilize information from higher-order neighbors of nodes.\n\nThe basic architecture of the Transformer model is depicted in Figure\u00a0{{figure:794a6fb3-ceec-4439-8f9a-fa9602878c1d}} . It consists of two modules: the encoders and the decoders. In this discussion, we will focus on the encoders. The most crucial component within the encoder is the multi-head self-attention component. This component comprises several self-attention subcomponents, which are widely used in recommendation models. We will specifically examine the self-attention component by considering the representation of an examination sequence {{formula:198b7cd2-7ec5-4bba-9fcc-576a3ad5c14b}} . The calculation of the self-attention component is as follows:\n{{formula:2fce1f33-d6bb-4efb-a49d-ae8ecb6fe621}} \n\n\nwhere {{formula:8a019bf3-3f03-42ab-89d7-97ed32ee64ca}} , {{formula:1f4b8bb7-7437-4d23-b4c0-1c9a5a8059f1}} , {{formula:0307790b-511b-4b45-900d-feca15153170}}  and {{formula:75f587a1-ee51-4124-aceb-b4efb3a57cea}}  are the projection matrices. {{formula:9696c176-8d30-473b-9b93-8bdd1fe94a47}}  is the length of the sequence {{formula:1b42feb6-2ef8-4e05-8642-e422c5efe2f7}} , {{formula:7651e0d5-83b4-4cac-ba8c-7d9d8a49b563}}  is the dimension of {{formula:268b9cd2-f577-4d07-a757-40d158b27cf4}} , and {{formula:7ea8ea4c-c8ae-498a-8fee-28e602f79048}}  is the output of the self-attention component.\n\n\n---\nBased on the above text, in the context of the following table, what does 'Construct directed graphs for different behavior-specific sequences respectively.' in the column 'Features' refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\nTable: ec69d321-0332-4d1c-a502-d66a5faa52e5\n         References                                           Features\n0  {{cite:ef27bb1}}  Modeling behavior from behavior transition rel...\n1  {{cite:51fcf28}}  Focus on the task of voucher redemption rate p...\n2  {{cite:eafaa65}}  Learn various behavior transition relations fr...\n3  {{cite:2fe2021}}  Construct directed graphs for different behavi...\n4  {{cite:c5008f0}}  Construct directed graphs for different behavi...\nCaption: Data & modeling perspectives and features used in works based on GNN learning architecture."}}, "title": "A Survey on Multi-Behavior Sequential Recommendation", "abstract": "Recommender systems is set up to address the issue of information overload in traditional information retrieval systems, which is focused on recommending information that is of most interest to users from massive information. Generally, there is a sequential nature and heterogeneity to the behavior of a person interacting with a system, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR is a relatively new and worthy direction for in-depth research, which can achieve state-of-the-art recommendation through suitable modeling, and some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR."}
{"paper_id": "2308.07444v1", "_pdf_hash": null, "_source_hash": "08d21a4c9c2be33ed6ea92f48811e5fef8a8bdc4", "_source_name": "2308.07444v1", "_table_hash": "99ae82e7-1829-4360-825b-1c668277bc86", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid5\" id-text=\"1\" place=\"t\" rend=\"display\"><head>Summary of transferability scoring methods (Tr. scorer), sorted by publication date. Cat.: category; fb: feature-based; lb: label-based.</head>\n<unexpected><hi rend=\"small\"><table rend=\"inline\"><tr bottom-border=\"true\" top-border=\"true\"><td halign=\"center\">Tr.scorer</td>\n<td halign=\"left\">Cat.</td>\n<td halign=\"center\">Scorer input</td>\n<td halign=\"center\">Details</td>\n</tr><tr><td halign=\"center\">H-Score<cit sha=\"a6e7e0ef955e3cb4bf35abe9dda64c260eb0212e\"><ref target=\"bid4\"/></cit>{{cite:a6e7e0e}}</td>\n<td halign=\"left\">fb</td>\n<td halign=\"center\">source feature extractor &amp; labels</td>\n<td halign=\"center\">transferability correlates to inter-class variance and feature redundancy</td>\n</tr><tr><td halign=\"center\">NCE<cit sha=\"1e0504bc02a9b18a08c05a839db57cc278a8fcc0\"><ref target=\"bid5\"/></cit>{{cite:1e0504b}}</td>\n<td halign=\"left\">lb</td>\n<td halign=\"center\">source classification head &amp; labels</td>\n<td halign=\"center\">negative conditional entropy between source and target labels</td>\n</tr><tr><td halign=\"center\">LEEP<cit sha=\"06c96450711f6c1fe4216d0521ffb83f22a0ec07\"><ref target=\"bid6\"/></cit>{{cite:06c9645}}</td>\n<td halign=\"left\">lb</td>\n<td halign=\"center\">source classification head &amp; labels</td>\n<td halign=\"center\">log-likelihood between target labels and source model predictions</td>\n</tr><tr><td halign=\"center\">N-LEEP<cit sha=\"e31984323a9fb103b477eb045c833e58732b340f\"><ref target=\"bid7\"/></cit>{{cite:e319843}}</td>\n<td halign=\"left\">fb</td>\n<td halign=\"center\">source feature extractor &amp; labels</td>\n<td halign=\"center\">log-likelihood between target labels and Gaussian mixture model fit to target extracted features</td>\n</tr><tr><td halign=\"center\">LogME<cit sha=\"7485cbbef9cdd5dcb85b072fcb9bb513622bac67\"><ref target=\"bid8\"/></cit>{{cite:7485cbb}}</td>\n<td halign=\"left\">fb</td>\n<td halign=\"center\">source feature extractor &amp; labels</td>\n<td halign=\"center\">probability of target labels conditioned on target image embeddings</td>\n</tr><tr><td halign=\"center\">Regularized H-Score<cit sha=\"d0cfe30141635d85a486c1a6eabc1341a001a7a3\"><ref target=\"bid2\"/></cit>{{cite:d0cfe30}}</td>\n<td halign=\"left\">fb</td>\n<td halign=\"center\">source feature extractor &amp; labels</td>\n<td halign=\"center\">shrinkage estimators for stable covariance</td>\n</tr><tr bottom-border=\"true\"><td halign=\"center\">GBC<cit sha=\"32e25906259264945c049927609dddd9e6e1de12\"><ref target=\"bid9\"/></cit>{{cite:32e2590}}</td>\n<td halign=\"left\">fb</td>\n<td halign=\"center\">source feature extractor &amp; labels</td>\n<td halign=\"center\">Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 feature estimating overlap with target task classes</td>\n</tr></table></hi></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["Tr.scorer", "Cat.", "Scorer input", "Details"], ["H-Score{{cite:a6e7e0e}}", "fb", "source feature extractor & labels", "transferability correlates to inter-class variance and feature redundancy"], ["NCE{{cite:1e0504b}}", "lb", "source classification head & labels", "negative conditional entropy between source and target labels"], ["LEEP{{cite:06c9645}}", "lb", "source classification head & labels", "log-likelihood between target labels and source model predictions"], ["N-LEEP{{cite:e319843}}", "fb", "source feature extractor & labels", "log-likelihood between target labels and Gaussian mixture model fit to target extracted features"], ["LogME{{cite:7485cbb}}", "fb", "source feature extractor & labels", "probability of target labels conditioned on target image embeddings"], ["Regularized H-Score{{cite:d0cfe30}}", "fb", "source feature extractor & labels", "shrinkage estimators for stable covariance"], ["GBC{{cite:32e2590}}", "fb", "source feature extractor & labels", "Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 feature estimating overlap with target task classes"]], "table_dict": {"References": ["{{cite:a6e7e0e}}", "{{cite:1e0504b}}", "{{cite:06c9645}}", "{{cite:e319843}}", "{{cite:7485cbb}}", "{{cite:d0cfe30}}", "{{cite:32e2590}}"], "Tr.scorer": ["H-Score", "NCE", "LEEP", "N-LEEP", "LogME", "Regularized H-Score", "GBC"], "Cat.": ["fb", "lb", "lb", "fb", "fb", "fb", "fb"], "Scorer input": ["source feature extractor & labels", "source classification head & labels", "source classification head & labels", "source feature extractor & labels", "source feature extractor & labels", "source feature extractor & labels", "source feature extractor & labels"], "Details": ["transferability correlates to inter-class variance and feature redundancy", "negative conditional entropy between source and target labels", "log-likelihood between target labels and source model predictions", "log-likelihood between target labels and Gaussian mixture model fit to target extracted features", "probability of target labels conditioned on target image embeddings", "shrinkage estimators for stable covariance", "Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 feature estimating overlap with target task classes"]}}, "bib_hash": ["a6e7e0ef955e3cb4bf35abe9dda64c260eb0212e", "1e0504bc02a9b18a08c05a839db57cc278a8fcc0", "06c96450711f6c1fe4216d0521ffb83f22a0ec07", "e31984323a9fb103b477eb045c833e58732b340f", "7485cbbef9cdd5dcb85b072fcb9bb513622bac67", "d0cfe30141635d85a486c1a6eabc1341a001a7a3", "32e25906259264945c049927609dddd9e6e1de12"], "row_bib_map": [{"bib_hash_or_arxiv_id": "a6e7e0ef955e3cb4bf35abe9dda64c260eb0212e", "row": 0, "corpus_id": 202782600, "type": "ref"}, {"bib_hash_or_arxiv_id": "1e0504bc02a9b18a08c05a839db57cc278a8fcc0", "row": 1, "corpus_id": 201303557, "type": "ref"}, {"bib_hash_or_arxiv_id": "06c96450711f6c1fe4216d0521ffb83f22a0ec07", "row": 2, "corpus_id": 211572839, "type": "ref"}, {"bib_hash_or_arxiv_id": "e31984323a9fb103b477eb045c833e58732b340f", "row": 3, "corpus_id": 227126796, "type": "ref"}, {"bib_hash_or_arxiv_id": "7485cbbef9cdd5dcb85b072fcb9bb513622bac67", "row": 4, "corpus_id": 231985863, "type": "ref"}, {"bib_hash_or_arxiv_id": "d0cfe30141635d85a486c1a6eabc1341a001a7a3", "row": 5, "corpus_id": 238744475, "type": "ref"}, {"bib_hash_or_arxiv_id": "32e25906259264945c049927609dddd9e6e1de12", "row": 6, "corpus_id": 244709516, "type": "ref"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"1\" id=\"uid5\" place=\"t\"><head>Summary of transferability scoring methods (Tr. scorer), sorted by publication date. Cat.: category; fb: feature-based; lb: label-based.</head>\n<unexpected><hi rend=\"small\"><table rend=\"inline\"><row bottom-border=\"true\" top-border=\"true\"><cell halign=\"center\">Tr.\u00a0scorer</cell>\n<cell halign=\"left\">Cat.</cell>\n<cell halign=\"center\">Scorer input</cell>\n<cell halign=\"center\">Details</cell>\n</row><row><cell halign=\"center\">H-Score\u00a0<cit sha=\"a6e7e0ef955e3cb4bf35abe9dda64c260eb0212e\"><ref target=\"bid4\"/></cit>{{cite:a6e7e0e}}</cell>\n<cell halign=\"left\">fb</cell>\n<cell halign=\"center\">source feature extractor &amp; labels</cell>\n<cell halign=\"center\">transferability correlates to inter-class variance and feature redundancy</cell>\n</row><row><cell halign=\"center\">NCE\u00a0<cit sha=\"1e0504bc02a9b18a08c05a839db57cc278a8fcc0\"><ref target=\"bid5\"/></cit>{{cite:1e0504b}}</cell>\n<cell halign=\"left\">lb</cell>\n<cell halign=\"center\">source classification head &amp; labels</cell>\n<cell halign=\"center\">negative conditional entropy between source and target labels</cell>\n</row><row><cell halign=\"center\">LEEP\u00a0<cit sha=\"06c96450711f6c1fe4216d0521ffb83f22a0ec07\"><ref target=\"bid6\"/></cit>{{cite:06c9645}}</cell>\n<cell halign=\"left\">lb</cell>\n<cell halign=\"center\">source classification head &amp; labels</cell>\n<cell halign=\"center\">log-likelihood between target labels and source model predictions</cell>\n</row><row><cell halign=\"center\">N-LEEP\u00a0<cit sha=\"e31984323a9fb103b477eb045c833e58732b340f\"><ref target=\"bid7\"/></cit>{{cite:e319843}}</cell>\n<cell halign=\"left\">fb</cell>\n<cell halign=\"center\">source feature extractor &amp; labels</cell>\n<cell halign=\"center\">log-likelihood between target labels and Gaussian mixture model fit to target extracted features</cell>\n</row><row><cell halign=\"center\">LogME\u00a0<cit sha=\"7485cbbef9cdd5dcb85b072fcb9bb513622bac67\"><ref target=\"bid8\"/></cit>{{cite:7485cbb}}</cell>\n<cell halign=\"left\">fb</cell>\n<cell halign=\"center\">source feature extractor &amp; labels</cell>\n<cell halign=\"center\">probability of target labels conditioned on target image embeddings</cell>\n</row><row><cell halign=\"center\">Regularized H-Score\u00a0<cit sha=\"d0cfe30141635d85a486c1a6eabc1341a001a7a3\"><ref target=\"bid2\"/></cit>{{cite:d0cfe30}}</cell>\n<cell halign=\"left\">fb</cell>\n<cell halign=\"center\">source feature extractor &amp; labels</cell>\n<cell halign=\"center\">shrinkage estimators for stable covariance</cell>\n</row><row bottom-border=\"true\"><cell halign=\"center\">GBC\u00a0<cit sha=\"32e25906259264945c049927609dddd9e6e1de12\"><ref target=\"bid9\"/></cit>{{cite:32e2590}}</cell>\n<cell halign=\"left\">fb</cell>\n<cell halign=\"center\">source feature extractor &amp; labels</cell>\n<cell halign=\"center\">Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 feature estimating overlap with target task classes</cell>\n</row></table></hi></unexpected></table>", "caption": "Summary of transferability scoring methods (Tr. scorer), sorted by publication date. Cat.: category; fb: feature-based; lb: label-based.", "type": "table"}, "_table_hash_full_text": "1802dd9b-5a7a-470d-906b-2e37f0cb921c", "_full_text_table_hash": "df31e046-9b0a-4970-a7bc-832f3a45a1bd", "context_autogenerated": {"glossary": {"Tr.scorer<~>Tr.scorer": "'Tr.scorer' in the context of the table refers to a specific transferability scoring method mentioned in each row of the table, as indicated by the corresponding reference.", "Tr.scorer<~>H-Score": "H-Score is a transferability scoring method referred to in the table.", "Tr.scorer<~>NCE": "'NCE' in the table refers to Negative Cosine Similarity or contrastive loss, which is a transferability scoring method used in some deep learning models.", "Tr.scorer<~>LEEP": "N-LEEP is a transferability scoring method mentioned in the text.", "Tr.scorer<~>N-LEEP": "N-LEEP stands for Normalized Mutual Information Maximization for Large-scale Embedded Processes, which is a transferability scoring method for large-scale embedding spaces.", "Tr.scorer<~>LogME": "The 'LogME' in the 'Tr.scorer' column refers to a transferability scoring method named LogME (Logits-space Mutual Enterprise). It is a label-based transferability scoring method that calculates the mutual information between the logit scores of the source and target models.", "Tr.scorer<~>Regularized H-Score": "Regularized H-Score is a variant of the H-Score transferability metric, as mentioned in reference [[d0cfe30]].", "Tr.scorer<~>GBC": "GBC refers to the transferability scoring method named \"Gradient Boosting Certificate,\" as cited by {{cite:32e2590}}.", "Cat.<~>Cat.": "The 'Cat.' in the table refers to the category of each transferability scoring method, specifically whether it is feature-based (fb) or label-based (lb).", "Cat.<~>fb": "In the context of the table, 'fb' in the 'Cat.' column refers to 'feature-based' transferability scoring methods.", "Cat.<~>lb": "In the context of the table, 'lb' in the 'Cat.' column refers to label-based transferability scoring methods.", "Scorer input<~>Scorer input": "The 'Scorer input' refers to the type of data or model required by each transferability scoring method for estimating the model's transferability. In this case, all methods listed in the table require source feature extractors and labels.", "Scorer input<~>source feature extractor & labels": "'Source feature extractor & labels' in the column 'Scorer input' refer to the inputs of transferability scoring methods, which are the feature extractors from the source domain and its corresponding labels.", "Scorer input<~>source classification head & labels": "The term 'source classification head & labels' in the table refers to the use of the original classifications and their corresponding labels from the source dataset as input for some transferability scoring methods.", "Details<~>Details": "The term 'Details' in the table refers to additional information related to each transferability scoring method, such as the specific reference (cite) where the method was published and the category of the method (feature-based or label-based).", "Details<~>transferability correlates to inter-class variance and feature redundancy": "'transferability correlates to inter-class variance and feature redundancy' in the context of the table refers to one of the transferability scoring methods mentioned, specifically the one referenced by [[cite:a6e7e0e]], but the text does not provide enough context to fully understand what this means without additional information.", "Details<~>negative conditional entropy between source and target labels": "'Negative conditional entropy between source and target labels' refers to a measure of the dependency or relationship between the source and target labels used in some transferability scoring methods. It aims to quantify the transferability of a model by evaluating how much information the source labels provide about the target labels.", "Details<~>log-likelihood between target labels and source model predictions": "The entry 'log-likelihood between target labels and source model predictions' in the table refers to the calculation of the log-likelihood of the target labels given the predictions made by the source model.", "Details<~>log-likelihood between target labels and Gaussian mixture model fit to target extracted features": "The log-likelihood between target labels and a Gaussian mixture model fit to target extracted features is a transferability score method that calculates the likelihood of the target labels given the features extracted from the source domain using a Gaussian mixture model.", "Details<~>probability of target labels conditioned on target image embeddings": "'Probability of target labels conditioned on target image embeddings' in the context of the table refers to the calculation of the probability of the target labels given the target image embeddings, used as a transferability scoring method.", "Details<~>shrinkage estimators for stable covariance": "The reference [{{cite:d0cfe30}}] in the table describes a transferability scoring method that uses shrinkage estimators for stable covariance. This refers to a technique for estimating the covariance matrix in a more robust and stable manner, particularly useful when dealing with high-dimensional data or data with large covariance variability.", "Details<~>Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 feature estimating overlap with target task classes": "The term 'Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 in the 'Details' column refers to a measure of the similarity or overlap between the distribution of source task classes and target task classes using Bhattacharyya coefficients calculated between multivariate Gaussian distributions."}, "inputs": {"section_name": "Transferability Scores & Related Work", "instruction_sample": "Based on the following text from a scientific paper, answer the question about the table that follows:\n\nTitle: The Performance of Transferability Metrics does not Translate to Medical Tasks\n\nAbstract: Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contexts, inviting further work in that direction.\n\nSection: Transferability Scores & Related Work\nAn effective transferability scoring method exhibits computational efficiency while strongly correlating with the final performance metric of a fine-tuned model on the target dataset. Generally, the estimation of transferability involves extracting the embeddings or predictions from the target dataset. That extracted information is integrated with the target dataset's ground-truth labels to quantify the model's transferability. Transferability scoring methods can be categorized into feature-based (fb) and source-label-based (sb). Source-label-based scores assume access to the source classification head for calculating probability distribution or label predictions, whereas feature-based scores only require source models for feature extraction. Both methods require the true labels of the target dataset for computing the transferability score. We summarize the transferability scoring methods, sorted by publication date in Table\u00a0{{table:df31e046-9b0a-4970-a7bc-832f3a45a1bd}} .\n\nIbrahim et al.{{cite:d0cfe30}} and Agostinelli et al.{{cite:bf0737d}} evaluated transferability scores on general-purpose datasets for classification and segmentation tasks. Their findings suggest that these scores may be unstable, and minor variations in the experimental protocol could lead to different conclusions. N-LEEP and LogME deliver the best transferability estimation results depending on the experimental design of classification tasks. Our work focuses on classification tasks in scenarios where the dataset shift is significant. The experimental design of previous works assumes a lower dataset shift compared to what we consider in our paper. For instance, transferring from ImageNet to CIFAR is expected to be easier than any medical dataset due to some overlap between target-source classes and features. Additionally, we perform thorough hyperparameter tuning, which is essential in these works.\n\n\n---\nBased on the above text, in the context of the following table, what does 'Bhattacharyya coeff. between multivariate Gaussians fit to each class\u2019 feature estimating overlap with target task classes' in the column 'Details' refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\nTable: df31e046-9b0a-4970-a7bc-832f3a45a1bd\n         References                                            Details\n0  {{cite:a6e7e0e}}  transferability correlates to inter-class vari...\n1  {{cite:1e0504b}}  negative conditional entropy between source an...\n2  {{cite:06c9645}}  log-likelihood between target labels and sourc...\n3  {{cite:e319843}}  log-likelihood between target labels and Gauss...\n4  {{cite:7485cbb}}  probability of target labels conditioned on ta...\n5  {{cite:d0cfe30}}         shrinkage estimators for stable covariance\n6  {{cite:32e2590}}  Bhattacharyya coeff. between multivariate Gaus...\nCaption: Summary of transferability scoring methods (Tr. scorer), sorted by publication date. Cat.: category; fb: feature-based; lb: label-based."}}, "title": "The Performance of Transferability Metrics does not Translate to Medical Tasks", "abstract": "Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contexts, inviting further work in that direction."}
{"paper_id": "2308.05787v1", "_pdf_hash": null, "_source_hash": "d3c66eda0483ebf85fe4f2a5481979e7f3033dac", "_source_name": "2308.05787v1", "_table_hash": "8d85c51b-a012-4dfc-b2c7-c7930c473701", "table_html": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<table id=\"uid15\" id-text=\"2\" place=\"b\" rend=\"display\"><head><hi rend=\"small\">Comparison with existing dynamic filters in terms of temporal modeling capability, location adaptiveness and the ability to exploit pre-trained weights in existing models.</hi></head>\n<unexpected><hi rend=\"small\">\n<table rend=\"inline\"><tr><td halign=\"left\"></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Temporal</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Location</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Pretrained</hi></hi><hi rend=\"small\"/></td>\n</tr><tr bottom-border=\"true\"><td halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Operations</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">modeling</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">adaptive</hi></hi><hi rend=\"small\"/></td>\n<td halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">weights</hi></hi><hi rend=\"small\"/></td>\n</tr><tr><td halign=\"left\">CondConv<cit sha=\"8f566022aa35908d6a31548fbd6f047cca4b1f39\"><ref target=\"bid10\"/></cit>{{cite:8f56602}}</td>\n<td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n<hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"/></tr><tr><td halign=\"left\">DynamicFilter<cit sha=\"d55bb73d9fee3db69133f6cb51509e56e0a52317\"><ref target=\"bid20\"/></cit>{{cite:d55bb73}}</td>\n<td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n<hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"/></tr><tr><td halign=\"left\">DDF<cit sha=\"f5c1bfb55760070bd7f1131c74bd3284aaca6ea6\"><ref target=\"bid16\"/></cit>{{cite:f5c1bfb}}</td>\n<td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n<hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></td>\n</hi><hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"/></tr><tr><td halign=\"left\">TAM<cit sha=\"cc45c2f719c605335c854a6e984854f315926028\"><ref target=\"bid30\"/></cit>{{cite:cc45c2f}}</td>\n<td halign=\"center\"><hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></td>\n<hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"><td halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></td>\n</hi><hi rend=\"small\"/></tr><tr><td halign=\"left\">midgreyTAdaConv(V2)</td>\n<td halign=\"center\">midgrey<hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></td>\n<hi rend=\"small\"><td halign=\"center\">midgrey<hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></td>\n</hi><hi rend=\"small\"><td halign=\"center\">midgrey<hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></td>\n</hi><hi rend=\"small\"/></tr><tr><td halign=\"left\"/>\n</tr></table></hi></unexpected></table>", "table_json": {"incomplete_rows": [], "table": [["", "Temporal", "Location", "Pretrained"], ["Operations", "modeling", "adaptive", "weights"], ["CondConv{{cite:8f56602}}", "\u2717", "\u2717", "\u2717"], ["DynamicFilter{{cite:d55bb73}}", "\u2717", "\u2717", "\u2717"], ["DDF{{cite:f5c1bfb}}", "\u2717", "\u2713", "\u2717"], ["TAM{{cite:cc45c2f}}", "\u2713", "\u2717", "\u2717"], ["midgreyTAdaConv(V2)", "midgrey\u2713", "midgrey\u2713", "midgrey\u2713"]], "table_dict": {"References": ["{{cite:8f56602}}", "{{cite:d55bb73}}", "{{cite:f5c1bfb}}", "{{cite:cc45c2f}}", "-"], "Operations": ["CondConv ", "DynamicFilter ", "DDF ", "TAM ", "midgreyTAdaConv(V2)"], "Temporal modeling": ["\u2717", "\u2717", "\u2717", "\u2713", "\u2713"], "Location adaptive": ["\u2717", "\u2717", "\u2713", "\u2717", "\u2713"], "Pretrained weights": ["\u2717", "\u2717", "\u2717", "\u2717", "\u2713"]}}, "bib_hash": ["8f566022aa35908d6a31548fbd6f047cca4b1f39", "d55bb73d9fee3db69133f6cb51509e56e0a52317", "f5c1bfb55760070bd7f1131c74bd3284aaca6ea6", "cc45c2f719c605335c854a6e984854f315926028"], "row_bib_map": [{"bib_hash_or_arxiv_id": "8f566022aa35908d6a31548fbd6f047cca4b1f39", "row": 0, "corpus_id": 202775981, "type": "ref"}, {"bib_hash_or_arxiv_id": "d55bb73d9fee3db69133f6cb51509e56e0a52317", "row": 1, "corpus_id": 2097418, "type": "ref"}, {"bib_hash_or_arxiv_id": "f5c1bfb55760070bd7f1131c74bd3284aaca6ea6", "row": 2, "corpus_id": 233444201, "type": "ref"}, {"bib_hash_or_arxiv_id": "cc45c2f719c605335c854a6e984854f315926028", "row": 3, "corpus_id": 218630285, "type": "ref"}, {"bib_hash_or_arxiv_id": "2308.05787v1", "row": 4, "corpus_id": 260866000, "type": "ours"}], "table_unfiltered": {"table": "<table rend=\"display\" id-text=\"2\" id=\"uid15\" place=\"b\"><head><hi rend=\"small\">Comparison with existing dynamic filters in terms of temporal modeling capability, location adaptiveness and the ability to exploit pre-trained weights in existing models.</hi></head>\n<unexpected><hi rend=\"small\">\n\n<table rend=\"inline\"><row><cell halign=\"left\">\u00a0</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Temporal</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Location</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">Pretrained</hi></hi><hi rend=\"small\"/></cell>\n</row><row bottom-border=\"true\"><cell halign=\"left\"><hi rend=\"small\"><hi rend=\"bold\">Operations</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">modeling</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">adaptive</hi></hi><hi rend=\"small\"/></cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi rend=\"bold\">weights</hi></hi><hi rend=\"small\"/></cell>\n</row><row><cell halign=\"left\">CondConv\u00a0<cit sha=\"8f566022aa35908d6a31548fbd6f047cca4b1f39\"><ref target=\"bid10\"/></cit>{{cite:8f56602}}</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n<hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"/></row><row><cell halign=\"left\">DynamicFilter\u00a0<cit sha=\"d55bb73d9fee3db69133f6cb51509e56e0a52317\"><ref target=\"bid20\"/></cit>{{cite:d55bb73}}</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n<hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"/></row><row><cell halign=\"left\">DDF\u00a0<cit sha=\"f5c1bfb55760070bd7f1131c74bd3284aaca6ea6\"><ref target=\"bid16\"/></cit>{{cite:f5c1bfb}}</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n<hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></cell>\n</hi><hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"/></row><row><cell halign=\"left\">TAM\u00a0<cit sha=\"cc45c2f719c605335c854a6e984854f315926028\"><ref target=\"bid30\"/></cit>{{cite:cc45c2f}}</cell>\n<cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></cell>\n<hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"><cell halign=\"center\"><hi rend=\"small\"><hi color=\"colid7\">\u2717</hi></hi></cell>\n</hi><hi rend=\"small\"/></row><row><cell halign=\"left\">midgreyTAdaConv(V2)</cell>\n<cell halign=\"center\">midgrey<hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></cell>\n<hi rend=\"small\"><cell halign=\"center\">midgrey<hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></cell>\n</hi><hi rend=\"small\"><cell halign=\"center\">midgrey<hi rend=\"small\"><hi color=\"colid6\">\u2713</hi></hi></cell>\n</hi><hi rend=\"small\"/></row><row><cell halign=\"left\"/>\n</row></table></hi></unexpected></table>", "caption": "Comparison with existing dynamic filters in terms of temporal modeling capability, location adaptiveness and the ability to exploit pre-trained weights in existing models.", "type": "table"}, "_table_hash_full_text": "294ed366-3c4c-4633-9d58-f207640a76b6", "_full_text_table_hash": "7d565ad8-2153-41b3-a135-79f90f8fb9da", "context_autogenerated": {"glossary": {"Operations<~>Operations": "In the context of the table, 'Operations' refers to the specific dynamic filtering methods mentioned in each row, including CondConv, DynamicFilter, DDF, TAM, and midgreyTAdaConv(V2).", "Operations<~>CondConv": "CondConv in the table refers to Conditional Convolutions, a type of dynamic filtering operation used in TAdaConv.", "Operations<~>DynamicFilter": "'DynamicFilter' in the table refers to the dynamic filters mentioned in the text, including Dynamic Filtering (DDF) and Temporal Attention Modeling (TAM), which are approaches different from TAdaConv in generating dynamic weights.", "Operations<~>DDF": "DDF in the context of the table refers to Dynamic Filtering as mentioned in the text associated with reference [{{cite:f5c1bfb}}].", "Operations<~>TAM": "TAM in the table refers to Temporal Adaptive Modeling, which is a dynamic filtering approach mentioned in the text.", "Operations<~>midgreyTAdaConv(V2)": "MidgreyTAdaConv(V2) refers to the TAdaConvV2 operation described in the text, which is a temporally adaptive dynamic filter that can model complex temporal dynamics, generate weights based on local and global context, and exploit pre-trained weights in existing models.", "Temporal modeling<~>Temporal modeling": "In the context of the table, \"Temporal modeling\" refers to the ability of dynamic filters to model temporal dynamics in video data.", "Location adaptive<~>Location adaptive": "In the context of the table, 'Location adaptive' refers to the ability of dynamic filters to adapt the weights based on the spatial location of the input data in addition to the input content. The table indicates that some dynamic filters, such as the one referenced in {{cite:f5c1bfb}}, have this capability (marked as '\u2713'), while others, such as the ones in {{cite:8f56602}}, {{cite:d55bb73}}, and {{cite:cc45c2f}}, do not (marked as '\u2717' or not mentioned).", "Pretrained weights<~>Pretrained weights": "In the context of the table, 'Pretrained weights' refers to the weights learned by a model on a large dataset and used as a starting point for fine-tuning on a new dataset. In the context of the comparison with existing dynamic filters, it refers to the ability of TAdaConv to initialize its dynamic weights with pre-trained weights, while most other dynamic filters cannot."}, "inputs": {"section_name": "Calibration weight generation. ", "instruction_sample": "Based on the following text from a scientific paper, answer the question about the table that follows:\n\nTitle: Temporally-Adaptive Models for Efficient Video Understanding\n\nAbstract: Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.\n\nSection: Calibration weight generation. \nTo allow for the TAdaConv to model temporal dynamics, it is crucial that the calibration weight {{formula:b2edf0a8-914f-4a6a-a84f-34d7ab73a1bb}}  for the {{formula:b373af25-fb5c-480b-9de9-0f2039d80123}} -th frame takes into account not only the current frame, but more importantly, its temporal context, i.e., {{formula:71a1bec3-2341-48d7-9675-1cf94e1bbfe4}} .\nOtherwise, TAdaConv would degenerate to a set of unrelated spatial convolutions with different weights applied on different frames.\nIn practice, the calibration generation function can have various structural designs. In Fig.\u00a0{{figure:f2756d63-ee20-4710-b76b-3fc021332c06}} (b) and (c), we show two instantiations of the calibration generation function, which respectively correspond to TAdaConv and TAdaConvV2.\n\nTAdaConv. In our design, we aim for efficiency and the ability to capture inter-frame temporal dynamics.\nFor efficiency, we operate on the frame description vectors {{formula:457a0b27-9490-4b5b-921d-914fcdae0f8a}}  obtained by the global average pooling over the spatial dimension {{formula:677c7850-fe7e-4ff9-a111-2c8ad70c16ec}}  for each frame, i.e., {{formula:0a98b553-9e6c-4aec-938b-18b58247005c}} .\nFor temporal modeling, we apply two-layer 1D convolutions {{formula:0b4cf15d-cbab-41c0-9095-f734bcc673a6}}  with a dimension reduction ratio of {{formula:e496ef96-0ad7-464c-a4c5-a253eabeabc4}}  on the local temporal context {{formula:38514710-d780-48b7-934c-f9716c3bbbad}} :\n{{formula:df715a4e-2037-4a21-944f-d19008deebfd}} \n\nwhere we use ReLU\u00a0{{cite:eb060a9}} and batch normalizations\u00a0{{cite:dcbda4e}} for activation and normalization. {{formula:0ac841ea-1514-4c38-a1b8-f52ff3d0c4e8}}  denotes 1-D convolutions.\n\nIn order for a larger inter-frame field of view in complement to the local 1D convolution, we further incorporate global temporal information into the calibration weight generation process. For TAdaConv, we add a global descriptor\nto the weight generation process {{formula:8731ae0a-61c1-4505-a630-d2bc2afc5716}}  through a linear mapping function {{formula:86ef2ddc-126b-4e9f-8350-4e8b37aeaea2}} :\n{{formula:41ac6ee3-5cac-43a0-a4f3-86f1660ef9cb}} \n\nwhere {{formula:64c3ced9-852f-43c3-be25-8cdf66aae2f7}}  denotes global average pooling over the temporal dimension on the frame descriptors {{formula:e93aed49-4c04-420e-aaab-4ba127508cf9}} . This is equivalent to global average pooling over all spatiotemporal dimensions on the original input {{formula:4536b2a7-d848-4c33-a0b8-2bcf0510f058}} . Hence, {{formula:01bd01ba-98f0-467c-998a-a0698fe865a4}}  contains the global temporal context in the input videos.\n\nTAdaConvV2. The instantiation of TAdaConvV2 is generally similar to TAdaConv, with two improvements. (i) We alter the combination of ReLU and batch normalizations to GELU and layer normalizations to conform to the structures in ConvNeXt models. (ii) For global temporal context modeling, we take advantage of the powerful global modeling capability of self-attention\u00a0{{cite:3f3a49e}}. Specifically, the calibration weight generation function can be expressed as follows:\n{{formula:84687e6e-1a20-43d6-ba87-d37a80bdd955}} \n\nwhere MHSA denotes the multi-head self-attention\u00a0{{cite:3f3a49e}}. Since the 1D convolution before MHSA essentially provides a dynamic positional embedding for the frame descriptors {{formula:636fee06-6ca7-4273-9f7b-7e36fe36e45c}} , we do not add additional positional embeddings before the MHSA operation.\n\nInitialization. The TAdaConv is designed to be readily inserted into existing models by simply replacing the 2D convolutions.\nFor effective use of the pre-trained weights, TAdaConv is initialized to behave exactly the same as the standard convolution.\nThis is achieved by zero-initializing the weight of the last convolution in {{formula:85bb251a-4a70-49fd-be10-95ca96267457}}  and adding a constant vector {{formula:470c7592-a49a-4bf3-bbdc-1a95ecafc3ce}}  to the formulation:\n{{formula:35e8d0a1-827f-4b9b-83c9-72d983afc488}} \n\nIn this way, at initial state, {{formula:4275f0ba-670f-4572-8f3d-47e97ad38e19}} , where we load {{formula:ef410864-c748-45bb-8949-2daed32f90b8}}  with the pre-trained weights.\n\nCalibration dimension. \nThe base weight {{formula:42316890-ee83-4e98-99d3-85296cf6227b}}  can be calibrated in different dimensions.\nFor standard convolutions, we instantiate the calibration on the {{formula:6d648f36-6da7-434c-9e8e-de1d8ad336c0}}  dimension ({{formula:af68ee56-258b-4e04-9d92-e2501e2b6114}} ), as the weight generation based on the input features yields a more precise estimation for the relation of the input channels than the output channels or spatial structures (empirical analysis in Table\u00a0{{table:f1aa739d-caf9-44b7-b03f-46e8b734bb17}} ). For depthwise convolutions, since the convolution kernel does not have a {{formula:729563fb-9345-4c9a-a764-088c1c350c97}}  dimension, the calibration is directly applied on the {{formula:3b968f0d-3bcc-4b27-ba68-9072203a14f0}}  dimension of the convolution kernel.\n\nComparison with temporal convolutions. Table\u00a0{{table:802d1653-b24e-4549-bde6-9f0dda82656f}}  compares the TAdaConv with R(2+1)D in parameters and FLOPs, which shows most of our additional computation overhead on top of the spatial convolution is an order of magnitude less than the temporal convolution.\n\nComparison with existing dynamic filters. Table\u00a0{{table:7d565ad8-2153-41b3-a135-79f90f8fb9da}}  compares TAdaConv with existing dynamic filters.\nThe main difference between different dynamic filtering approaches lies in the way that the dynamic weights are generated.\nMixture-of-experts-based dynamic filters\u00a0{{cite:8f56602}} generate content-dependent weights to dynamically aggregate learnable convolution weights.\nOther types of dynamic filters\u00a0{{cite:d55bb73}}, {{cite:f5c1bfb}}, {{cite:cc45c2f}} generate dynamic weights entirely based on the input content.\nOur TAdaConv is different from existing dynamic filters in the following three aspects:\n(i) Compared to image-based dynamic filters\u00a0{{cite:d55bb73}}, {{cite:f5c1bfb}}, {{cite:8f56602}}, TAdaConv achieves temporal modeling by generating weights based on the local and global context.\n(ii) Compared to TANet\u00a0{{cite:cc45c2f}} in the video paradigm, TAdaConv could model more complex temporal dynamics because of the temporally adaptive weights.\n(iii) Most existing dynamic filters are incapable of exploiting existing pre-trained weights, while TAdaConv could be initialized to generate dynamic weights that are identical to pre-trained ones. This reduces the training difficulty in video applications.\nMore detailed comparisons of dynamic filters are included in Appendix.\n\n\n---\nBased on the above text, in the context of the following table, what does 'Pretrained weights' refer to? Answer in a single sentence. If the answer is not clear just write 'unanswerable'.\nTable: 7d565ad8-2153-41b3-a135-79f90f8fb9da\n         References Pretrained weights\n0  {{cite:8f56602}}                  \u2717\n1  {{cite:d55bb73}}                  \u2717\n2  {{cite:f5c1bfb}}                  \u2717\n3  {{cite:cc45c2f}}                  \u2717\n4                 -                  \u2713\nCaption: Comparison with existing dynamic filters in terms of temporal modeling capability, location adaptiveness and the ability to exploit pre-trained weights in existing models."}}, "title": "Temporally-Adaptive Models for Efficient Video Understanding", "abstract": "Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv."}
