{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 9136312, "title": "The Konstanz natural video database (KoNViD-1k)", "paper_id": "ae95abd2406ddfab1aa2ffa2413d68b98b1ba21b", "abstract": "Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be \u2018general purpose\u2019 requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at \u2018in the wild\u2019 authentic distortions, depicting a wide variety of content.", "intro": "Most of the Internet traffic today stems from user-generated videos on sharing web-sites and social networks. Video sequences pass through several stages of processing before they reach consumers, which often deteriorate visual quality. Moreover, the vast amount of user-generated video content and the increased diversity of end user devices (ranging from smaller and power-constrained mobile devices to large displays such as 4K Ultra HDTVs and TV walls) calls for a broad range of video quality to be supported. Adapting video quality to different use cases has become an important topic for researchers, content providers and distributors [1].\n\nAutomatic and accurate prediction of video quality is a basic operation for many video processing applications such as video quality monitoring in transmission protocols, video quality filtering in sharing services, automatic and recommended camera parameter settings during video capturing, and video enhancement. Specifically, no-reference methods attempt to judge the quality of a video sequence without any additional information about the original recorded scene. Such blind methods may apply machine learning techniques to learn from large amounts of annotated data. However, current video quality assessment (VQA) databases contain only a small number of video sequences with little content diversity, thus offering limited support for designing and evaluating noreference VQA methods effectively and fairly.\n\nAdditionally, these databases were mostly designed to include only artificially distorted video sequences to simulate quality loss in compression, transmission, and other parts of the video processing and distribution pipeline. Some databases capture imagery with a variety of cameras to encompass authentic video acquisition distortions, however, with content restricted to a small number of physical scenes.\n\nWinkler [2] proposed several criteria for quantitative comparisons of source content, test conditions, and subjective ratings, applying them to 27 image and video databases. Most collections have not been found satisfactory in terms of content range and uniformity. Only few databases showed good uniformity for test conditions (image/video quality), but not over the whole quality range. Also the distortion variety was found lacking in most databases covering mainly compression and transmission, but not the many other types of natural distortions found \"in the wild\" [3].\n\nTo overcome these limitations we introduce KoNViD-1k, a large publicly available database of video sequences based on YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset [4] with a diverse set of video content. In this paper we report the filtering mechanisms and sampling procedures necessary to construct high-quality VQA databases of this kind, focusing on their usefulness in a variety of applications.\n\nIn the next section, we describe the database creation procedure and the set of attributes we have considered to maximise its diversity. Additional information regarding the removal of non-natural video sequences and sampling techniques are provided as well. Next, in Sec. III, we review our crowdsourcing-based process of collecting subjective mean opinion scores (MOS) and detail our results as well as crowd worker statistics. In Sec. IV we relate our database characteristics and creation methodology with other existing works and outline the differences, before discussing conclusions of our work and considering possible future work."}
{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 52285071, "title": "Large-Scale Study of Perceptual Video Quality", "paper_id": "7954410ea395fe0b74db571e4bf87cc5b312ca6b", "abstract": "The great variations of videographic skills, camera designs, compression and processing protocols, and displays lead to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores, resulting in an average of 240 recorded human opinions per video. We demonstrate the value of the new resource, which we call the LIVE Video Quality Challenge Database (LIVE-VQC), by conducting a comparison of leading NR video quality predictors on it. This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores. The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html .", "intro": "The goal of Video quality assesement (VQA) research is to develop video quality models that produce predictions that are in close agreement with human judgments, regardless of the video contents or the type and severities of the distortions (or the mixtures of distortions) that have corrupted the videos. Over the past decade, we have experienced a surge in the number of videos recorded, shared and watched. Sharing \"inthe-moment\" experiences in the form of video has become quite popular using applications such as Instagram, Facebook, Twitter via Periscope, Snapchat, and so on. Online videos have also revolutionized modern journalism as they enable online news stories to unfold live, and allow the viewing audience to comment on or otherwise interact with it. Over the past year, Facebook alone generated more than 100 million of Zeina Sinno and Alan C. Bovik are with the Department of Electrical and Computer Engineering at The University of Texas at Austin, Austin, TX, 78712, USA (e-mails: zeina@utexas.edu -bovik@ece.utexas.edu). video watch hours each day [1]. On YouTube, the overall durations of the videos that are uploaded daily exceeds 65 years, and more than 1 billion hours of their video content is watched each day [2]. These numbers are continuing to rise and to reshape digital marketing, entertainment, journalism, and amateur videography. The volume of streaming video viewed online has become so large that more than 130 million people are now Netflix subscribers [3]. Streaming videos now comprises the majority of Internet traffic today. It is no surprise that videos account for the largest portion of Internet traffic, which is expected to eclipse 82% of all transmitted bits by 2021 [4]. These videos are captured using a very wide variety of camera devices by users having very diverse goals and expertise."}
{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 119309258, "title": "YouTube UGC Dataset for Video Compression Research", "paper_id": "093561bf2c2112be497873d2e36dcc1648a035e0", "abstract": "Non-professional video, commonly known as User Generated Content (UGC) has become very popular in today's video sharing applications. However, traditional metrics used in compression and quality assessment, like BD-Rate and PSNR, are designed for pristine originals. Thus, their accuracy drops significantly when being applied on non-pristine originals (the majority of UGC). Understanding difficulties for compression and quality assessment in the scenario of UGC is important, but there are few public UGC datasets available for research. This paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel sampling method based on features extracted from encoding, challenges for UGC compression and quality evaluation are also discussed. Shortcomings of traditional reference-based metrics on UGC are addressed. We demonstrate a promising way to evaluate UGC quality by no-reference objective quality metrics, and evaluate the current dataset with three no-reference metrics (Noise, Banding, and SLEEQ).", "intro": "Video makes up the majority of todays Internet traffic. Consequently, this motivates video service companies (e.g. YouTube) to spend substantial effort to control bandwidth usage [1]. The main remedy deployed is typically video bitrate reduction. However, aggressive bitrate reduction may hurt perceptual visual quality at the same time as both creators and viewers have increasing expectations for streaming media quality. The evolution of new codec technology (HEVC, VP9, AV1) continues to address this bitrate/quality tradeoff.\n\nTo measure the quality degradation, numerous quality metrics have been proposed in the last few decades. Some reference-based metrics (like PSNR, SSIM [2] and VMAF [3]) have been widely used in the industry. A common assumption held by most video quality and compression research is that the original video is perfect, and any operation on the original (processing, compression etc.) makes it worse. Most research measures how good the resulting video is by comparing it to the original. However, such an assumption does not hold for most of User Generated Content (UGC) due to the following reasons:\n\n\u2022 Non-pristine original When there are visual artifacts present in the original, it is not clear whether an encoder should be putting in efforts to accurately represent those artifacts. It is necessary to consider the effect that the encoding has on those undesired artifacts, but it is also necessary to consider the effect that the artifacts have on the ability to encode the video effectively. \u2022 Mismatched absolute and reference quality Using the original as a reference does not always make sense when the original isnt perfect. Quality improvement may be affected by pre/post processing before/after transcoding, but reference-based metrics (e.g. PSNR, SSIM) cannot fairly evaluate the impact of these tools in a compression chain. We created this large scale UGC dataset in order to encourage and facilitate research that considers the practical and realistic needs of video compression and quality assessment in video processing infrastructure.\n\nA major contribution of this work is the analysis of the enormous content in YouTube in a way that illustrates the breadth of visual quality in media worldwide. That analysis leads to the creation of a statistically representative set that is more amenable to academic research and computational resources. We built a large scale UGC dataset(Section III), and propose (Section IV) a novel sampling scheme based on features extracted from encoding logs, which achieves high coverage over millions of YouTube videos. Shortcomings of traditional reference-based metrics on UGC are discussed (Section V), and we evaluate the dataset with three noreference metrics: Noise [4], Banding [5], and Self-reference based LEarning-free Evaluator of Quality (SLEEQ) [6].\n\nThe dataset can be previewed and downloaded from https://media.withyoutube.com/ugc-dataset."}
{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 227210156, "title": "Patch-VQ: 'Patching Up' the Video Quality Problem", "paper_id": "2e41d8b57af02799b4f9e89977f62c748eb995fe", "abstract": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world,\"in-the-wild\"UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.", "intro": "User-generated content (UGC) and video streaming has exploded on social media platforms such as Facebook, Instagram, YouTube, and TikTok, each supporting millions and billions of users [1]. It has been estimated that each day, about 4 billion video views occur on Facebook [2] and 1 billion hours are viewed on YouTube [3]. Given the tremendous prevalence of Internet video, it would be of great value to measure and control the quality of UGC videos, both in capture devices and at social media sites where they are uploaded, encoded, processed, and analyzed.\n\nFull-reference (FR) video quality assessment (VQA) models perceptually compare quality against pristine videos, while no-reference (NR) models involve no such comparison. Thus, NR video quality monitoring could transform the processing and interpretation of videos on * \u2020 Equal contribution \u2021 The entity that conducted all of the data collection/experimentation. smartphones, social media, telemedicine, surveillance, and vision-guided robotics, in ways that FR models are unable to. Unfortunately, measuring video quality without a pristine reference is very hard. Hence, though FR models are successfully deployed at the largest scales [4], NR video quality prediction on UGC content remains largely unsolved, for several reasons. First, UGC video distortions arise from highly diverse capture conditions, unsteady hands of content creators, imperfect camera devices, processing and editing artifacts, frame rates, compression and transmission artifacts, and the way they are perceived by viewers. Inter-mixing of distortions is common, creating complex, composite distortions that are harder to model in videos. Moreover, it is wellknown that the technical degree of distortion (e.g. amount of blur, blocking, or noise) does not correlate well with perceptual quality [5], because of neurophysiological processes that induce masking [6]. Indeed, equal amounts of distortions may very differently affect the quality of two different videos [7].\n\nSecond, most existing video quality resources are too small and unrepresentative of the complex real-world distortions [8,9,10,11,12,13,14]. While three publicly avail-able databases of authentically distorted UGC videos are available [15,16,17], they are far too small to train modern, data-hungry deep neural networks. What is needed are very large databases of videos corrupted by real-world distortions, subjectively rated by large numbers of human viewers. However, conducting large-scale psychometric studies is much harder and time-consuming (per video) than standard object or action classification tasks.\n\nFinally, although a few NR algorithms achieve reasonable performance on small databases [18,19,20,21,22,23,24], most of them fail to account for the complex spacetime distortions common to UGC videos. UGC distortions are often transient (e.g., frame drops, focus changes, and transmission glitches) and yet may significantly impact the overall perceived quality of a video [25]. Most existing models are frame-based, or use sample frame differences, and cannot capture diverse temporal impairments.\n\nWe have made recent progress towards addressing these challenges, by learning to model the relationships that exist between local and global spatio-temporal distortions and perceptual quality. We built a large-scale public UGC video dataset of unprecedented size, comprising full videos and three kinds of spatio-temporal video patches (Fig. 1), and we conducted an online visual psychometric study to gather large numbers of human subjective quality scores on them. This unique data collection allowed us to successfully learn to exploit interactions between local and global video quality perception and to create algorithms that accurately predict video quality and space-time quality maps. We summarize our contributions below:\n\n\u2022 We built the largest video quality database in existence. We sampled hundreds of thousands of open source Internet UGC digital videos to match the feature distributions of social media UGC videos. Our final collection includes 39, 000 real-world videos of diverse sizes, contents, and distortions, 26 times larger than the most recent UGC dataset [17]. We also extracted three types of v-patches from each video, yielding 117, 000 space-time video patches (\"v-patches\") in total (Sec. 3.1). \u2022 We conducted the largest subjective video quality study to date. Our final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6, 300 subjects, more than 9 times larger than any prior UGC video quality study (Sec. 3.2). \u2022 We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D video features using PaQ2PiQ [29], in parallel with 3D features using ResNet3D [30]. The 2D and 3D features feed a time series regressor [31] that learns to accurately predict both global video, as well as local spacetime v-patch quality, by exploiting the relations between them. This new model, which we call Patch VQ (PVQ) achieves top performance on the new database as well as on smaller \"in-the-wild\" databases [16,15], without finetuning (Secs. 4.1 and 5.3). \u2022 We also create another unique prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships. This second model, called the PVQ Mapper, helps localize, visualize, and act on video distortions (Sec. 5.2)."}
{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 234788066, "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild", "paper_id": "0518d5abe3926ca8d6bd097c04c246e69f7d2da5", "abstract": "Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.", "intro": "Videos have become a central medium for business marketing [1], with over 81% of businesses using video as a marketing tool. Additionally, over 40% of businesses have adopted live video formats such as Facebook Live for marketing and user connection purposes [2]. For consumers, video is the primary source of media entertainment; for example the average US consumer spends 38 hours per week watching video content [3] and it is projected that online videos will make up more than 82% of all consumer internet traffic by 2022 [4]. Streaming platforms such as YouTube report that more than a billion hours of video are watched every day [5]. The success of online videos is due in part to the consumer belief that traditional TV offers an inferior quality [3]. Additionally, increased accessibility to video content The associate editor coordinating the review of this manuscript and approving it for publication was Zhang Lu. acquisition hardware, as well as improvements in overall image quality, are a central aspect in smartphone technology advancement. Similarly, user-generated content is produced at an increasing rate, but the resulting videos often suffer from quality defects.\n\nTherefore, a wide range of video producers and consumers should be able to get automated feedback on video quality. For example, user-generated video distribution platforms like YouTube or Vimeo may want to analyze new videos according to quality to separate professional from the amateur video content, instead of only indexing by video playback resolution. Additionally, with an automated video quality assessment (VQA) system, video streaming services can adjust video encoding parameters to minimize bandwidth requirements while ensuring the delivery of satisfactory video quality.\n\nA critical emerging challenge for VQA is to handle ecologically valid in-the-wild videos. In environmental psychology, VOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ ecological validity is defined as ''the applicability of the results of laboratory analogues to non-laboratory, real life settings'' [6]. In our case the term can be understood as a measure for the extent to which the data represented in a dataset can be generalized to data that would be naturally encountered in the use of a technology. Concretely, this would refer to the types and degree of distortions in visual media contents of internet videos, such as those consumed on YouTube, Flickr, or Vimeo. The term in-the-wild refers to datasets that are ''not constructed and designed with research questions in mind'' [7]. In the case of VQA this would mean datasets that are not recorded or altered with a specific research purpose in mind, such as artificially distorting videos at variable degrees. It comes as no surprise that no-reference VQA (NR-VQA), in particular, has been a field of intensive research in the past few years achieving significant performance gains [8]- [19]. However, state-of-the-art NR-VQA algorithms perform worse on in-the-wild videos than on synthetically distorted ones. These methods aggregate individual video frame quality characteristics that are engineered for specific purposes, such as detecting particular compression artifacts. Often, these features are a balance between precision and computational efficiency. Furthermore, since there is a lack of large-scale in-the-wild video quality datasets with authentic distortions, a thorough evaluation of NR-VQA methods is difficult. Most existing databases are intended as benchmarks for the detection of those specific artificial distortions that NR-VQA algorithms have classically been designed to detect.\n\nGiven the previous challenges, our first contribution is the creation of a large ecologically valid dataset, KonVid-150k. Similar to the dataset KoNViD-1k [20], the ecological validity of KonVid-150k stems from its size, content diversity, as well as naturally occurring, and thus representative degradations. However, being two orders of magnitude larger than existing datasets, it poses new challenges to VQA methods, requiring to train across a vast amount of content and a wide span of authentic distortions. Moreover, since a fixed budget usually constrains the development of a dataset, we needed to ensure a minimum level of annotation quality. Therefore, a part of KonVid-150k consists of 153,841 five seconds long videos that are annotated by five subjective opinions each. This set, from here on called KonVid-150k-A, is over 125 times larger than existing VQA datasets in terms of number of videos and with close to one million subjective ratings over eight times larger in number of annotations [20]- [23]. The dataset is accompanied by a benchmark set of nearly 1,600 videos (KonVid-150k-B) from the same source with a minimum of 89 opinion scores each. This presents a unique opportunity to analyze the trade-off between the number of training videos and the annotation noise/precision, in terms of the performance on the KonVid-150k-B benchmark dataset.\n\nThis new dataset exacerbates two problems of classical NR-VQA methods. First, the computational costs of hand-crafted feature-based approaches are increased through the sheer number of videos. Second, since hand-crafted features handle in-the-wild videos worse than conventional databases, this dataset is very challenging for classical NR-VQA methods. An alternative to hand-crafted features comes with the rise of deep convolutional neural networks (DCNNs), where stacked layers of increasingly complex feature detectors are learned directly from observations of input images. These features are often relatively generic and have been proven to transfer well to similar tasks that are not too different from the source domain [24], [25]. This suggests considering a DCNN as a feature extractor with a benefit over hand-crafted features in that the features are entirely learned from data.\n\nAs a second contribution, we propose to use a new way of extracting video features by aggregating activations of all layers of DCNNs, pre-trained for classification. We adopt a strategy similar to Hosu et al. [26] and extract narrow multi-level spatially pooled (MLSP) features of video frames from an InceptionResNet-v2 [27] architecture to learn VQA. By global average pooling the outputs of inception module activation blocks, we obtain fixed sized feature representations of the frames. We showcase the scalability of this approach by comparing it to the baseline of freezing the weights of the feature extraction network and training a new head, which is a technique that is commonly used in transfer learning.\n\nThe third contribution of this paper consists of two network variants trained on the frame feature vectors that surpass state-of-the-art NR-VQA methods on in-the-wild datasets and train at a rate that is able to scale to hundreds of thousands of videos. In a short ablation study we investigate the impact of architectural and hyperparameter choices of both models. Both approaches are then evaluated on existing VQA datasets consisting of authentic videos as well as those containing artificially degraded videos and show that on in-the-wild videos the proposed method outperforms classical methods based on hand-crafted features. In particular, training and testing on KoNViD-1k improves the state-of-the-art 0.80 to 0.82 SRCC. Finally, we show that training our proposed model on the new dataset we achieve a 0.83 SRCC when cross-testing on KoNViD-1k. This outperforms state-of-the-art intra-dataset test scenarios, where training and testing is performed on the same dataset. It is surprising, as intra-dataset tests have the benefit of not being affected by any domain shift [28].\n\nIn summary, our main contributions are:\n\n\u2022 KonVid-150k, an ecologically valid in-the-wild video quality assessment database, two orders of magnitude larger than existing ones.\n\n\u2022 The successful application of deep multi-layer spatially pooled features for video quality assessment, which allows training of state-of-the-art models at scale on conventional hardware.\n\n\u2022 Three deep neural network models (MLSP-VQA-FF, -RN, and -HYB). They surpass the intra-dataset stateof-the-art performance on KoNViD-1k with 0.82 SRCC versus the best existing 0.80 SRCC, and show excellent generalization in inter-dataset tests when trained on KonVid-150k, surpassing even the intra-dataset tests with 0.83 SRCC."}
{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 206592218, "title": "Large-Scale Video Classification with Convolutional Neural Networks", "paper_id": "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "intro": "Images and videos have become ubiquitous on the internet, which has encouraged the development of algorithms that can analyze their semantic content for various applications, including search and summarization. Recently, Convolutional Neural Networks (CNNs) [15] have been demonstrated as an effective class of models for understanding image content, giving state-of-the-art results on image recognition, segmentation, detection and retrieval [11,3,2,20,9,18]. The key enabling factors behind these results were techniques for scaling up the networks to tens of millions of parameters and massive labeled datasets that can support the learning process. Under these conditions, CNNs have been shown to learn powerful and interpretable image features [28]. Encouraged by positive results in domain of images, we study the performance of CNNs in large-scale video classification, where the networks have access to not only the appearance information present in single, static images, but also their complex temporal evolution. There are several challenges to extending and applying CNNs in this setting.\n\nFrom a practical standpoint, there are currently no video classification benchmarks that match the scale and variety of existing image datasets because videos are significantly more difficult to collect, annotate and store. To obtain sufficient amount of data needed to train our CNN architectures, we collected a new Sports-1M dataset, which consists of 1 million YouTube videos belonging to a taxonomy of 487 classes of sports. We make Sports-1M available to the research community to support future work in this area.\n\nFrom a modeling perspective, we are interested in answering the following questions: what temporal connectivity pattern in a CNN architecture is best at taking advantage of local motion information present in the video? How does the additional motion information influence the predictions of a CNN and how much does it improve performance overall? We examine these questions empirically by evaluating multiple CNN architectures that each take a different approach to combining information across the time domain.\n\nFrom a computational perspective, CNNs require extensively long periods of training time to effectively optimize the millions of parameters that parametrize the model. This difficulty is further compounded when extending the connectivity of the architecture in time because the network must process not just one image but several frames of video at a time. To mitigate this issue, we show that an effective approach to speeding up the runtime performance of CNNs is to modify the architecture to contain two separate streams of processing: a context stream that learns features on low-resolution frames and a high-resolution fovea stream that only operates on the middle portion of the frame. We observe a 2-4x increase in runtime performance of the network due to the reduced dimensionality of the input, while retaining the classification accuracy.\n\nFinally, a natural question that arises is whether features learned on the Sports-1M dataset are generic enough to generalize to a different, smaller dataset. We investigate the transfer learning problem empirically, achieving significantly better performance (65.4%, up from 41.3%) on UCF-101 by re-purposing low-level features learned on the Sports-1M dataset than by training the entire network on UCF-101 alone. Furthermore, since only some classes in UCF-101 are related to sports, we can quantify the relative improvements of the transfer learning in both settings.\n\nOur contributions can be summarized as follows:\n\n\u2022 We provide extensive experimental evaluation of multiple approaches for extending CNNs into video classification on a large-scale dataset of 1 million videos with 487 categories (which we release as Sports-1M dataset) and report significant gains in performance over strong feature-based baselines.\n\n\u2022 We highlight an architecture that processes input at two spatial resolutions -a low-resolution context stream and a high-resolution fovea stream -as a promising way of improving the runtime performance of CNNs at no cost in accuracy.\n\n\u2022 We apply our networks to the UCF-101 dataset and report significant improvement over feature-based stateof-the-art results and baselines established by training networks on UCF-101 alone."}
{"tabids": ["b42af37c-c83f-4cc0-8cc8-e7221c6a8d0f"], "corpus_id": 27300853, "title": "The Kinetics Human Action Video Dataset", "paper_id": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6", "abstract": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.", "intro": "In this paper we introduce a new, large, video dataset for human action classification. We developed this dataset principally because there is a lack of such datasets for human action classification, and we believe that having one will facilitate research in this area -both because the dataset is large enough to train deep networks from scratch, and also because the dataset is challenging enough to act as a performance benchmark where the advantages of different architectures can be teased apart.\n\nOur aim is to provide a large scale high quality dataset, covering a diverse range of human actions, that can be used for human action classification, rather than temporal localization. Since the use case is classification, only short clips of around 10s containing the action are included, and there are no untrimmed videos. However, the clips also contain sound so the dataset can potentially be used for many purposes, including multi-modal analysis. Our inspiration in providing a dataset for classification is ImageNet [18], where the significant benefits of first training deep networks on this dataset for classification, and then using the trained network for other purposes (detection, image segmentation, non-visual modalities (e.g. sound, depth), etc) are well known.\n\nThe Kinetics dataset can be seen as the successor to the two human action video datasets that have emerged as the standard benchmarks for this area: HMDB-51 [15] and UCF-101 [20]. These datasets have served the community very well, but their usefulness is now expiring. This is because they are simply not large enough or have sufficient variation to train and test the current generation of human action classification models based on deep learning. Coincidentally, one of the motivations for introducing the HMDB dataset was that the then current generation of action datasets was too small. The increase then was from 10 to 51 classes, and we in turn increase this to 400 classes. Table 1 compares the size of Kinetics to a number of recent human action datasets. In terms of variation, although the UCF-101 dataset contains 101 actions with 100+ clips for each action, all the clips are taken from only 2.5k distinct videos. For example there are 7 clips from one video of the same person brushing their hair. This means that there is far less variation than if the action in each clip was performed by a different person (and different viewpoint, lighting, etc). This problem is avoided in Kinetics as each clip is taken from a different video.\n\nThe clips are sourced from YouTube videos. Consequently, for the most part, they are not professionally videoed and edited material (as in TV and film videos). There can be considerable camera motion/shake, illumination variations, shadows, background clutter, etc. More im-"}
{"tabids": ["da17cb75-5655-4f95-bffc-c918a7ecb473"], "corpus_id": 233481695, "title": "IPatch: A Remote Adversarial Patch", "paper_id": "6b60da73fd18a73d12482c09558c07d6a48a575f", "abstract": "Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image. In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples `remote adversarial patches' (RAP). We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.", "intro": "Deep learning has become the go-to method for automating image-based tasks.This is because, deep neural networks (DNNs) are excellent at learning and identifying spatial patterns and abstract concepts. With advances in both hardware and neural architectures, deep learning has become both a practical and reliable solution. Companies now use image-based deep learning to automate tasks in life critical operations such as autonomous driving [1], [2], surveillance [3], and medical image screening [4].\n\nIn tasks such as these, multiple objects must be identified per image. One way to accomplish this is to predict a class probability for each pixel in the input image . This approach is called image segmentation and companies such as Telsa use it to guide their autonomous vehicles safely through an environment [2]. Another approach is called object detection where is split into a grid of cells or regions and the model predicts both a class probability and a bounding box for each of them [5], [6]. In both cases, these models rely on image semantics to successfully parse and interpret a scene.\n\nJust like other deep learning models, these semantic models are also susceptible to adversarial attacks. In 2017, researchers demonstrated how a small 'adversarial' patch can be placed in a real world scene and override an image-classifier's prediction, regardless of the patch's location or orientation [7]. This gave rise to a number of works which demonstrated the concept of adversarial patches against image segmentation and object detection models [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19]. However, current adversarial patches are limited in the following ways: Location Only predictions around the patch itself are explicitly affected. This limits where objects can be made to 'appear'\n\nThe author is with Ben-Gurion University (e-mail: yisroel@post.bgu.ac.il see https://ymirsky.github.io/)."}
{"tabids": ["da17cb75-5655-4f95-bffc-c918a7ecb473"], "corpus_id": 237048246, "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks", "paper_id": "fc8a115c92896d0aaa5a84e6fc776630af7ade22", "abstract": "Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.", "intro": "The rise of deep learning unlocked unprecedented performance in several scientific areas [24]. Convolutional (a) (b) (c) (d) (e) (f) Figure 1: Proposed adversarial patches on Cityscapes [5] (b) and CARLA Simulator [6] (e); (c/f) show the corresponding SS predicted by BiSeNet [40]; (a/d) show the corresponding predictions obtained using random patches instead of adversarial ones.\n\nneural networks [16] (CNNs) yielded super-human performance for many different computer vision tasks, such as image recognition [9], object detection [27] [26], and image segmentation [20]. Image segmentation, and semantic segmentation (SS) in particular, is used in autonomous driving perception pipelines [30], mainly for object detection [20]. Despite their high performance, CNNs are prone to adversarial attacks [31]. Most of the literature on adversarial attacks focuses on directly manipulating the pixels of the whole image, hence making the assumption that the attacker has control over the digital representation of the environment obtained by the on-board cameras. This kind of unsafe inputs are called digital adversarial examples.\n\nAlthough such digital attacks do not transfer well into the real world, they continue to be used to evaluate the robustness of models in safety-critical systems [12,3,19]. Realworld adversarial examples (RWAEs), on the other hand, are physical objects that can be placed in the field of view of a camera, such that the resulting image acts as an adversarial example for the neural network under attack [17]. Thus, RWAEs can induce errors in neural networks without requiring the attacker to access the digital representation of the image, thereby making them a more realistic and dangerous threat to safety-critical systems. This paper. This work focuses on RWAEs, as they repre-sent a potential threat to tasks in autonomous driving today. Although the effects of RWAEs have been studied extensively in the literature for classification and object detection, those on SS remain relatively unexplored. However, SS is an integral part of autonomous driving pipelines [30]. Thus, this paper examines various state-of-the-art models for real-time SS aiming at benchmarking their robustness to RWAEs in autonomous driving scenarios.\n\nOf the several types of RWAEs proposed in the literature [32], the form of attack used in this paper is adversarial patches [4]. This is because attacks that perturb the whole image are not practically feasible in the real world. Conversely, such patches can be easily printed and attached to any visible 2D surface in the driving environment, such as billboards and road signs, thus making them a simple, yet effective attack strategy.\n\nThe paper starts by recognizing the shortcomings of the standard cross-entropy loss for optimizing adversarial patches for SS. Thus, an extension to the cross-entropy loss is proposed and integrated in all the performed attacks. This extension forces the optimization to focus on pixels that are not yet misclassified, thus obtaining patches that are more powerful compared to those generated with the standard cross-entropy-based setting [21].\n\nFollowing this rationale, the robustness of real-time SS models to RWAEs attacks is benchmarked. The paper starts by first examining the case of driving images, crafting adversarial patches on the Cityscapes dataset [5], a popular benchmark of high-resolution images of urban driving. Robust real-world patches are crafted by following the Expectation Over Transformation (EOT) [2] paradigm, which has been extended in this work to attack SS models. Furthermore, a comparison against non-robust patches (without EOT) is presented to question their effectiveness on driving scenes.\n\nAnother set of experiments targeted a virtual 3D scenario, for which a stronger adversarial attack is presented and tested. The proposed scene-specific attack, defined in Section 3.4, is a more practical tool for crafting adversarial patches in a realistic autonomous driving scenario. It assumes that the attacker is interested in targeting an autonomous driving scene at a particular corner of a specific town, where information about the position of the attackable 2D surface (in our case, a billboard) is available. To satisfy such requirements we developed and tested this attack using the CARLA simulator, which provides all the needed geometric information. These experiments include a comparison with the EOT-based and non-robust patches, performed by importing them into the CARLA world and placing them on billboards to simulate a realistic study. Figure 1 provides some examples of the effect of our patches on Cityscapes and CARLA.\n\nThe last set of experiments were conducted on a real-world driving scenario, which required collecting a dataset within the city, optimizing a patch on it, physically printing said patch on a billboard, and finally evaluating SS models on images containing the printed patch.\n\nTo the best of our knowledge, this work represents the first exhaustive evaluation of the robustness of SS models against RWAEs for autonomous driving systems. The results of the experiments state important observations that should be taken into consideration while evaluating the trustworthiness of SS models in autonomous driving. First, they demonstrate that non-robust patches are not good candidates for assessing the practical robustness of an SS model to adversarial examples. Indeed, while they proved to be effective in attacking images related to driving scenes (from Cityscapes), they do not induce any real-world adversarial effect when crafted and tested in a virtual 3D world (based on CARLA). Conversely, robust patches, crafted with EOT or the proposed scene-specific approach, resulted to be less effective than non-robust ones on Cityscapes images, but were capable to accomplish the attack in both virtual 3D world and the real world. Nevertheless, their effectiveness in the latter two cases still resulted to be quite limited, hence questioning the practical relevance of RWAEs.\n\nIn summary, the paper makes the following contributions:\n\n\u2022 It proposes an extension to the pixel-wise crossentropy loss to enable crafting strong patches for the semantic segmentation setting. \u2022 It proposes a novel technique for crafting adversarial patches for autonomous driving scenarios that utilize geometric information of the 3D world. \u2022 It finally reports an extensive evaluation of RWAEbased attacks on a set of real-time semantic segmentation models using data from the Cityscapes dataset, CARLA, and the real world. The remainder of this paper is organized as follows: Section 2 provides a brief overview of related work existing in the literature, Section 3 formalizes the proposed loss function, pipeline, and attack strategy, Section 4 reports the experimental results, and Section 5 states the conclusions and proposes ideas for future work."}
{"tabids": ["361b11db-c0a0-4286-9b88-15e2b801a181"], "corpus_id": 246705967, "title": "InPars: Data Augmentation for Information Retrieval using Large Language Models", "paper_id": "4e36db22808c1d677438137b10979a9279fb6c1f", "abstract": "The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars .", "intro": "Language models (LMs) such as GPT-3 (Brown et al., 2020), FLAN (Wei et al., 2022), Gopher (Rae et al., 2021), and T0++ (Sanh et al., 2021) have demonstrated impressive performance on many NLP tasks. Additionally, when sufficient supervised information is not available for a task, they have been shown to be effective and at times yield compelling results (Winata et al., 2021;Schick and Sch\u00fctze, 2021b).\n\nDespite the appealing capabilities of large LMs, multi-billion parameter models are rarely used in information retrieval (IR), with p q p q What are the effects of caffeine during pregnancy? Document: We don't know a lot about the effects of caffeine during pregnancy on you and your baby. So it's best to limit the amount you get each day. Question:"}
{"tabids": ["361b11db-c0a0-4286-9b88-15e2b801a181"], "corpus_id": 252519173, "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "paper_id": "e86009d9f9b1cdf083a48d087552bc4153784451", "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.", "intro": "Recently, major progress has been made on neural retrieval models such as dual encoders, which can retrieve knowledge from a large collection of documents containing millions to billions of passages (Yih et al., 2011;Karpukhin et al., 2020). However, Thakur et al. (2021) recently proposed the BEIR heterogeneous retrieval benchmark, and showed that it is still difficult for neural retrievers to perform well on a wide variety of retrieval tasks that lack dedicated training data. Thus, previous approaches focus on transferring knowledge from question answering (QA) datasets such as MS MARCO (Nguyen et al., 2016). To best transfer from QA datasets, expressive retrievers are developed that allow fine-grained token-level interaction such as ColBERT (Khattab & Zaharia, 2020;Santhanam et al., 2022) and SPLADE (Formal et al., 2021) but with higher inference cost. Data augmentation via synthetic question generation has previously been explored Shakeri et al., 2020), but these question generators are typically only trained on popular QA datasets.\n\nWe argue that it is hard to expect models based on one or two QA datasets to perform well across different retrieval tasks. First, different retrieval tasks have very different search intents; in other words, different definitions of \"relevance\". For example, as illustrated in Figure 1(a), both Dbpedia-Entity (Hasibi et al., 2017) and FEVER (Thorne et al., 2018) are tasks to retrieve documents from Wikipedia. Dbpedia-Entity is a task to retrieve entities that are mentioned in the query, while FEVER is a task to find evidence that either supports or refutes a given statement. Which document is relevant to the query can be very different from one task to another task even if they share the same (2) which trains model on a large QA retrieval datasets and transfer to other retrieval tasks. Right (c): Few-shot PROMPTAGATOR performance. Average nDCG@10 on 11 datasets from BEIR from our PROMPTAGATOR models and previously MS MARCO-supervised models (SPLADE v2).\n\ndomain. Moreover, different tasks have distinct distributions of queries even when their search intents are similar. For example, in the BEIR benchmark, queries in HotpotQA (Yang et al., 2018) are long compositional questions, while queries in FiQA (Maia et al., 2018) are short financial questions.\n\nIn this paper, we advocate to work on the setting of Few-shot Retrieval for diverse retrieval ( \u00a72), where each task comes with a short description and a few annotated examples to clearly illustrate the search intents. Given that only a few examples are available, we propose Prompt-base Query Generation for Retriever (PROMPTAGATOR) ( \u00a73) which aims to resolve the data scarcity issue while retaining the efficiency of a small dual encoder, by harnessing the power of large language models (LLM) such as FLAN (Wei et al., 2022a). PROMPTAGATOR combines prompting with LLMs as a query generator without fine-tuning ( \u00a73.1), and can generate good queries with minimal supervision -shown in Figure 1(b), it solely relies on a few supervised examples from the target task without using annotated query-document pairs from Natural Questions (Kwiatkowski et al., 2019) or MS MARCO (Nguyen et al., 2016) to train the retriever directly. The key insight of PROMPTAGATOR is to amplify the power of few-shot examples by creating task-specific prompting, which in turn enables generating a large set of synthetic queries for training retrievers suited for the task. To ensure the generated data quality, we develop a filtering technique that ensures round-trip consistency using generated data only ( \u00a73.2). Our filter is tailored to retrieval, which removes ambiguous, generic, and low-quality questions, and significantly improves retrieval performance.\n\nWhile PROMPTAGATOR is not the first application of LLM for retrieval, prior attempts of using LLMs often come with higher serving cost. Neelakantan et al. (2022) proposes to use the GPT-3 (Brown et al., 2020) embeddings in dual encoder models. However, the embedding size is 12k and hence makes the search index footprint and inference cost high. Sachan et al. (2022) and Bonifacio et al. (2022) have applied prompting and LLMs for reranking, while leaving the retriever untouched. With PROMPTAGATOR, we show that LLMs can be used to generate efficient end-to-end retriever with high accuracy. The contributions of the paper are as follows:\n\n\u2022 We analyze the previously overlooked differences across retrieval tasks in their search intents and query distributions, and propose a Few-Shot Retrieval setting for the BEIR dataset. Our prompt and fewshot examples will be released to facilitate future research.\n\n\u2022 We propose PROMPTAGATOR, a simple recipe for few-shot retrieval by prompting with a LLM to generate synthetic task-specific training data. For the first time, end-to-end retrievers solely based on a few supervised examples can be strong and efficient to serve with PROMPTAGATOR.\n\n\u2022 Our experimental results show that, surprisingly, PROMPTAGATOR with two-to-eight examples produced significantly better retrievers compared to recent models trained on MS MARCO or NQ that have over 500K human annotated examples (Figure 1(c)). PROMPTA-GATOR outperforms ColBERT v2 and SPLADE v2 on 11 retrieval tasks we tested, while reranking boosts results by another 5 points on standard retrieval evaluation metric."}
{"tabids": ["361b11db-c0a0-4286-9b88-15e2b801a181"], "corpus_id": 257405222, "title": "AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation", "paper_id": "209afcf7480c6b9ebefce8c8daa54a3ccefecd79", "abstract": "Dense retrievers have made significant strides in text retrieval and open-domain question answering, even though most achievements were made possible only with large amounts of human supervision. In this work, we aim to develop unsupervised methods by proposing two methods that create pseudo query-document pairs and train dense retrieval models in an annotation-free and scalable manner: query extraction and transferred query generation. The former method produces pseudo queries by selecting salient spans from the original document. The latter utilizes generation models trained for other NLP tasks (e.g., summarization) to produce pseudo queries. Extensive experiments show that models trained with the proposed augmentation methods can perform comparably well (or better) to multiple strong baselines. Combining those strategies leads to further improvements, achieving the state-of-the-art performance of unsupervised dense retrieval on both BEIR and ODQA datasets.", "intro": "Text retrieval is one of the most impactful artificial intelligence applications nowadays. Billions of users access massive amounts of data on the Internet through common internet services powered by information retrieval techniques, such as web search and product search. Despite the fact that traditional lexical retrieval remains a simple yet effective solution, neural network based models, namely dense retrievers, have made significant progress in recent years and demonstrated advantages in scenarios concerning semantic matching.\n\nNevertheless, most dense retrievers heavily rely on training with a large amount of annotated data. For example, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are the two most widely used datasets, and models trained with them have obtained outstanding performance. But each of these datasets has hundreds of thousands of human annotated pairs, which can be prohibitively costly to collect, and the models trained with them may not perform well in unseen domains (Thakur et al., 2021). Training dense retrieval models without human-annotated data remains an unsolved challenge. Nevertheless, most dense retrievers heavily rely on training with a large amount of annotated data. For example, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are the two most widely used datasets, and models trained with them have obtained outstanding performance. But each of these datasets has hundreds of thousands of human annotated query-document pairs, which can be prohibitively costly to collect, and the models trained with them may not perform well in unseen domains (Thakur et al., 2021). Training dense retrieval models without human-annotated data remains an unsolved challenge.\n\nRecently, there have been some efforts showing the feasibility of training dense retrievers in an annotation-free way (Izacard et al., 2021;Ram et al., 2021;Neelakantan et al., 2022). Following the regular paradigm of self-supervised learning, a pretext task is designed by taking two different views of a single document (a positive pair). Then a dual-encoder model is trained using contrastive learning, mapping the two views of data to hidden representations as similar as possible. When directly applied on downstream retrieval tasks, the scores of those unsupervised models lag behind the classic lexical method BM25 by large margins, but a boost in performance is observed after tuning with annotated query-doc pairs. This motivates us to focus on the gap between the pretext task and downstream retrieval tasks. Existing strategies for constructing positive pairs are very heuristic, e.g. Contriever (Izacard et al., 2021) samples two random text spans from a document to form a positive pair. One can see that the quality of the positive pairs is poorly controlled, and the resulting pseudo queries bear little resemblance to real-world ones. As a result, the models are negatively impacted by the noisy pseudo pairs, and they achieve inferior performance on down-stream tasks.\n\nIn this study, we propose two new categories of strategies for constructing pseudo query-doc pairs, namely query extraction and transferred query generation. Given a document of an arbitrary domain, both methods can produce pseudo queries in an unsupervised manner, and the resulting queries are paired with the original document to train dense retrievers using contrastive learning. The contributions of this study can be summarized as follows:\n\n1. We propose query extraction (QEXT), a novel data augmentation method for training dense retrievers. Given a document, a list of random spans is sampled from it. We utilize various methods to gauge their salience to the document and select the spans with the highest scores as pseudo queries.\n\n2. We propose transferred query generation (TQGEN). Pseudo queries are produced by language generation models for other NLP tasks, e.g., summarization. Even though query generation (QGen) has been studied for long, existing studies require annotated data for training query generation models. Instead, we utilize out-of-distribution generation models for this end, and the result demonstrates that the inductive bias of specific NLP tasks can be effective for training dense retrievers.\n\n3. We introduce two datasets AUGQ-WIKI and AUGQ-CC, consisting of 22.6M and 52.4M pseudo query-document pairs, by applying the proposed augmentation methods on two large corpora.\n\n4. Extensive experiments show that the retrievers trained with AUGQ, named AUGTRIEVER, outperform strong baselines and achieve state-ofthe-art results on both BEIR and open-domain QA benchmarks, greatly bridging the gap between unsupervised dense methods and BM25."}
{"tabids": ["361b11db-c0a0-4286-9b88-15e2b801a181"], "corpus_id": 257279774, "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers", "paper_id": "44b0d2e884efa5344e50424dbe2edf616981f201", "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.", "intro": "The advent of neural information retrieval (IR) has led to notable performance improvements on document and passage retrieval tasks (Nogueira and Cho, 2019;Khattab and Zaharia, 2020;Formal et al., 2021) as well as downstream knowledgeintensive NLP tasks such as open-domain questionanswering and fact verification (Guu et al., 2020;Lewis et al., 2020;Khattab et al., 2021;Izacard et al., 2022).Neural retrievers for these tasks often benefit from fine-tuning on large labeled datasets such as SQuAD (Rajpurkar et al., 2018), Natural Questions (NQ) (Kwiatkowski et al., 2019), and KILT (Petroni et al., 2021).However, IR models can experience significant drops in accuracy due to distribution shifts from the training to the target domain (Thakur et al., 2021;Santhanam et al., 2022b).For example, dense retrieval models trained on MS MARCO (Nguyen et al., 2016) might not generalize well to queries about COVID-19 scientific publications (Voorhees et al., 2021;Wang et al., Figure 1: Overview of UDAPDR.An expensive LLM like GPT-3 is used to create an initial set of synthetic queries.These are incorporated into a set of prompts for a less expensive LLM that can generate large numbers of synthetic queries cheaply.The queries stemming from each prompt are used to train separate rerankers, and these are distilled into a single ColBERTv2 retriever for use in the target domain.2020), considering for instance that MS MARCO predates COVID-19 and thus lacks related topics.\n\nRecent work has sought to adapt IR models to new domains by using large language models (LLMs) to create synthetic target-domain datasets for fine-tuning retrievers (Bonifacio et al., 2022;Meng et al., 2022;Dua et al., 2022).For example, using synthetic queries, Thakur et al. (2021) and Dai et al. (2022) fine-tune the retriever itself and train a cross-encoder to serve as a passage reranker for improving retrieval accuracy.This significantly improves retriever performance in novel domains, but it comes at a high computational cost stemming from extensive use of LLMs.This has limited the applicability of these methods for researchers and practitioners, particularly in high-demand, user-facing settings.\n\nIn this paper, we develop Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers (UDAPDR), 1 an efficient strategy for using LLMs to facilitate unsupervised domain adaptation of neural retriever models.We show that UDAPDR leads to large gains in zero-shot settings on a diverse range of domains.\n\nThe approach is outlined in Figure 1.We begin with a collection of passages from a target domain (no in-domain queries or labels are required) as well as a prompting strategy incorporating these passages with the goal of query generation.A powerful (and perhaps expensive) language model like GPT-3 is used to create a modest number of synthetic queries.These queries form the basis for corpus-adapted prompts that provide examples of passages with good and bad queries, with the goal of generating good queries for new target domain passages.These prompts are fed to a smaller (and presumably less expensive) LM that can generate a very large number of queries for fine-tuning neural rerankers.We train a separate reranker on the queries from each of these corpus-adapted prompts, and these rerankers are distilled into a single student ColBERTv2 retriever (Khattab and Zaharia, 2020;Santhanam et al., 2022b,a), which is evaluated on the target domain.\n\nBy distilling from multiple passage rerankers instead of a single one, we improve the utility of ColBERTv2, preserving more retrieval accuracy gains while reducing latency at inference.Our core contributions are as follows:\n\n\u2022 We propose UDAPDR, a novel unsupervised domain adaptation method for neural IR that strategically leverages expensive LLMs like GPT-3 (Brown et al., 2020) and less expensive ones like Flan-T5 XXL (Chung et al., 2022), as well as multiple passage rerankers.Our approach improves retrieval accuracy in zeroshot settings for LoTTE (Santhanam et al., 2022b), SQuAD, and NQ.\n\n\u2022 We preserve the accuracy gains of these rerankers while maintaining the competitive latency of ColBERTv2.This leads to substantial reductions in query latency.\n\n\u2022 Unlike a number of previous domain adaptation approaches that utilize millions of synthetic queries, our technique only requires 1 pronounced: Yoo-Dap-ter 1000s of synthetic queries to prove effective and is compatible with various LLMs designed for handling instruction-based tasks like creating synthetic queries (e.g., GPT-3, T5, Flan-T5).\n\n\u2022 We generate synthetic queries using multiple prompting strategies that leverage GPT-3 and Flan-T5 XXL.This bolsters the effectiveness of our unsupervised domain adaptation approach.The broader set of synthetic queries allows us to fine-tune multiple passage rerankers and distill them more effectively.\n\n2 Related Work"}
{"tabids": ["361b11db-c0a0-4286-9b88-15e2b801a181"], "corpus_id": 259937100, "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models", "paper_id": "d44031f253668c61ac6d68b95bbe9cac57730d51", "abstract": "Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.", "intro": "Traditional informational retrieval (IR) methods like TF-IDF and BM25 [25] are based on token-level similarity matching and then suffer from lexical gap [1]. Inspired by the progress in deep learning, researchers have proposed to utilize neural networks to overcome the lexical gap. DR is such a kind of method based on neural networks. DR models like DPR [10] and Col-BERT [11] [27] encode each query or document to a dense vector, the dimensionality of which is determined by the neural networks. In practice, dense retrievers pre-compute the embeddings of documents and then on which build an approximate nearest neighbor (ANN) index for fast search. When a new query comes in, only its embedding is computed and fed into the following ANN search system. Unlike TF-IDF and BM25, DR cares more about the similarity of the overall semantic meaning.\n\nEven though neural retrieval mitigates the lexical gap, it still suffers from the challenge of lacking domain-specific training data. Some researchers have proposed to leverage transfer learning to tackle this challenge. To tackle this problem. Evidence [32] [6] shows that not all DR models and domains can benefit from transfer learning equally. Recently, LLMs like CPT-3 [4], LLaMA [33], and Vicuna [5] show the strong ability of zero-shot and few-shot learning. Instead of fine-tuning the LLMs on task-specific data, prompting concatenates the instructions for certain tasks (e.g., TL;DR translate to English) and a few corresponding examples as input and obtains the answers from the output of large language model (LLM). These kinds of human-written prompts are also called hard prompts. Researchers [28] recently have estimated that a good language classifier prompt is worth hundreds to thousands of extra data points. InPars [2] and PROMPTAGATOR [6] both utilize hard prompts to prompt the LLMs to tag the unlabeled documents with weak queries and then train task-specific retrievers. However, hard prompts have some drawbacks: a) It is not easy to find good hard prompts. Hard prompts must be hand-crafted by humans through trial and error, and sometimes intuition and luck are needed; b) Even with handcrafted prompts, the downstream tasks still underperform tuned models. For instance, compared with the performance of fine-tuned T5-XXL [24] on SuperGLUE [35], GPT-3 175B few-shot gets a 17.5 points smaller score despite using 16 times more parameters [12].\n\nInstead of utilizing humanly readable words as a hard prompt [22], soft prompt [12] [13] is a list of embeddings, unrecognizable to the human eye, appended to the input of the neural network. During the soft prompt tuning, the parameters of the LLM are frozen, and only the parameters associated with soft prompt are updated. Even though [12] and [13] both demonstrate that soft prompt outperforms the hard prompt, there is no work utilizing soft prompt tuning to augment DR. In this paper, we propose soft prompt tuning for augmenting DR (SPTAR). Specifically, for each task, we leverage soft prompt tuning to optimize the parameters associated with soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific retrievers. Moreover, we find that even with the optimized soft prompt, the quality of generated weak queries is sensitive to the example document-query pairs in the prompt. So, we design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries as well as the DR tasks. Our main contributions can be summarized as follows:\n\n\u2022 To the best of our knowledge, this is the first work that utilizes LLMs with soft prompt tuning for augmenting DR tasks. \u2022 We introduce a novel soft prompt filter to select high-quality document-query pairs in the prompt to further improve the quality of generated weak data. \u2022 Comprehensive experiments are conducted to demonstrate our approach outperforming BM25 and InPars [2]. \u2022 Experiments are based on the recent open-source LLMs, and we will make the code publicly available upon paper acceptance.  [27] share the same BERT but utilize a different special token after \"[CLS]\" to distinguish query and document. Unlike DPR directly measures the similarity between query embedding and document embeddings, Col-BERT introduces a late interaction mechanism. Specifically, for each token in the query, ColBERT computes its similarity with all the tokens in the document and applies a maximum pooling on these similarity scores. The similarity score of a pair of query and document is the summarization of all the scores after the maximum pooling. Given a query with one positive document and one negative document, ColBERT is optimized by the pairwise softmax cross-entropy loss over the computed scores of the positive and negative documents. ANCE [39] is a bi-encoder trained on (query, positive document, negative document) tuples where the negative document is retrieved from an ANN built on the checkpoint of the last step. BM25CE [36] is a re-ranking-based DR. BM25CE first applies BM25 to retrieve documents and then employs the trained crossed-encoder to re-rank the retrieved documents. Our contribution is not to propose new dense retrievers but to propose a novel method to augment the existing dense retrievers."}
{"tabids": ["361b11db-c0a0-4286-9b88-15e2b801a181"], "corpus_id": 249926985, "title": "Questions Are All You Need to Train a Dense Passage Retriever", "paper_id": "624b2ea0ba57ce67a4524e029ac11f748a0782fa", "abstract": "We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g., questions and potential answer passages). It uses a new passage-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence passages, and (2) the passages are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both passage and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.1 Our code and model checkpoints are available at: https://github.com/DevSinghSachan/art.", "intro": "Dense passage retrieval methods (Karpukhin et al., 2020;Xiong et al., 2021), initialized with encoders such as BERT  and trained using supervised contrastive losses (Oord et al., 2018), have surpassed the performance achieved by previously popular keyword-based approaches like BM25 (Robertson and Zaragoza, 2009). Such retrievers are core components in models for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hardnegative mining and denoising of positive examples. In this paper, we introduce the first unsupervised method, based on a new corpus-level autoencoding approach, that can match or surpass strong supervised performance levels with no labeled training data or task-specific losses.\n\nWe propose ART, Autoencoding-based Retriever Training, which only assumes access to sets of unpaired questions and passages. Given an input question, ART first retrieves a small set of possible evidence passages. It then reconstructs the original question by attending to these passages (see Figure 1 for an overview). The key idea in ART is to consider the retrieved passages as a noisy representation of the original question and question reconstruction probability as a way of denoising that provides soft-labels for how likely each passage is to have been the correct result.\n\nTo bootstrap the training of a strong model, it is important to both have a strong initial retrieval model and to be able to compute reliable initial estimates of question reconstruction probability when conditioned on a (retrieved) passage. Although passage representations from BERTstyle models are known to be reasonable retrieval baselines, it is less clear how to do zero-shot question generation. We use a generative pre-trained language model (PLM) and prompt it with the passage as input to generate the question tokens using teacher-forcing. As finetuning of the questiongeneration PLM is not needed, only the retrieval model, ART can use large PLMs and obtain accurate soft-label estimates of which passages are likely to be the highest quality.\n\nThe retriever is trained to penalize the divergence of a passage likelihood from its soft-label score. For example, if the question is ''Where is the bowling hall of fame located?'' as shown in Figure 1, then the training process will boost the retrieval likelihood of the passage ''Bowling Hall of Fame is located in Arlington,'' as it is relevant and would lead to a higher question reconstruction likelihood, while the likelihood of the passage ''Hall of Fame is a song by ...'' would be penalized as it is irrelevant. In this manner, the training process encourages correct retrieval results and vice-versa, leading to an iterative improvement in passage retrieval.\n\nComprehensive experiments on five benchmark QA datasets demonstrate the usefulness of our proposed training approach. By simply using questions from the training set, ART outperforms models like DPR by an average of 5 points absolute in top-20 and 4 points absolute in top-100 accuracy. We also train using all the questions contained in the Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) and find that even with a mix of answerable and unanswerable questions, ART achieves strong generalization on outof-distribution datasets due to relying on PLM. Our analysis further reveals that ART is highly sample-efficient, outperforming BM25 and DPR with just 100 and 1000 questions, respectively, on the NQ-Open dataset, and that scaling up to larger retriever models consistently improves performance."}
{"tabids": ["ff0a2460-7c6f-409b-8f91-30d45d25268d"], "corpus_id": 246275593, "title": "Text and Code Embeddings by Contrastive Pre-Training", "paper_id": "6d7d4fca9840504f630e9bea6acaa07322a6e889", "abstract": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.", "intro": "Deep unsupervised learning with generative and embedding models has seen dramatic success in the past few years. Generative models (Peters et al., 2018;Raffel et al., 2019;van den Oord et al., 2016;Ramesh et al., 2021;Brown et al., 2020;Chen et al., 2021)  imize the likelihood of observed data while embedding models are trained to distinguish observed data from noise (Sohn, 2016;van den Oord et al., 2018;Radford et al., 2021;Jia et al., 2021;Gao et al., 2021;Izacard et al., 2021). Generative models have been shown to produce realistic content and benefit many downstream applications, reducing the need for labeled training datasets. In generative models, the information about the input is typically distributed over multiple hidden states of the model. While some generative models (Kingma & Welling, 2014;Kiros et al., 2015) can learn a single representation of the input, most autoregressive Transformer (Vaswani et al., 2017) models do not (Raffel et al., 2019;Brown et al., 2020;Chen et al., 2021;Ramesh et al., 2021). However, learning such a representation (or embedding) is necessary for many tasks. Systems that search over millions or billions of items require each entry to be embedded as a dense representation and build an index in advance to save computational costs at query time. These embeddings are useful features for classification tasks and can also enable data visualization applications via techniques such as clustering. Embedding models are explicitly optimized to learn a low dimensional representation that captures the semantic meaning of the input Jia et al., 2021;Giorgi et al., 2020;Gao et al., 2021;Izacard et al., 2021).\n\nIn this work, we train embedding models using a contrastive learning objective with in-batch negatives (Sohn, 2016;Yih et al., 2011) on unlabeled data. The input is encoded with a Transformer encoder (Vaswani et al., 2017) and we leverage naturally occurring paired data to construct training data with no explicit labels. Text embedding models are trained on paired text data where we consider neighboring pieces of text on the Internet as positive pairs. Code embedding models treat the top-level docstring in a function along with its implementation as a (text, code) pair. The training signal of the contrastive objective on its own is not sufficient to learn useful representations and we overcome this by initializing our model with other pretrained models (Brown et al., 2020;Chen et al., 2021). Finally, we find that it is critical to use a sufficiently large batch to achieve the optimal performance. We show that this simple recipe combining pre-trained model initialization, large-batch contrastive learning and training at scale, can produce text and code embeddings that possess a broad range of capabilities.\n\nWe train a series of unsupervised text embedding models (cpt-text) of different sizes, ranging from 300M to 175B parameters, and observe a consistent performance improvement with increasing model sizes ( Figure  1). On classification accuracy averaging across 7 linearprobe classification tasks in SentEval (Conneau & Kiela, 2018), our largest unsupervised model achieves new stateof-the-art results with a relative improvement of 4% and 1.8% over the previous best unsupervised (Giorgi et al., 2020) and supervised (Gao et al., 2021) text embedding models, respectively.\n\nText embedding in previous work was studied under different domains, varying in data, training objective and model architecture. Precisely, sentence embedding (Reimers & Gurevych, 2019;Gao et al., 2021;Giorgi et al., 2020) and neural information retrieval Guu et al., 2020;Karpukhin et al., 2020a;Sachan et al., 2021;Izacard et al., 2021) have remained different research topics evaluated on distinct benchmarks, even though both aim to learn high-quality text representation. However, we find the same model that achieves good performance on sentence embedding benchmarks, as discussed above, is also able to obtain impressive results on large-scale information retrieval. When evaluated on the MSMARCO passage ranking task (Nguyen et al., 2016) to search over 4M passages, cpt-text gets a relative improvement of 23.4% over previous best unsupervised methods (Robertson, 2009). On the task of searching on 21M documents from Wikipedia, cpt-text obtains a relative improvement of 14.7%, and 10.6% over previous unsupervised methods (Izacard et al., 2021) for Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), respectively. On Triv-iaQA, our unsupervised method is even competitive with fine-tuned models.\n\nNext, we train code embedding models (cpt-code) using the same recipe. Our models learn via (text, code) pairs, extracted from open source code. We evaluate our model on CodeSearchNet (Husain et al., 2020), a commonly used code search benchmark, where the task is to find the most relevant code snippet given a natural language query. Our models achieve new state-of-the-art results with a 20.8% relative improvement over the previous best result (Guo et al., 2021). Unlike text embedding models, we observe no performance improvement on code search when increasing the number of parameters of cpt-code from 300M to 1.2B.\n\nFinally, we experiment with fine-tuning our models on several supervised datasets and study the transfer learning performance. When fine-tuned on NLI (Natural Language Inference) datasets, we see a further boost in linearprobe classification, outperforming the previous best transfer method (Gao et al., 2021) by 2.2%. On SST-2 sentiment classification (Socher et al., 2013), we find that our representations are sufficiently descriptive that even a simple k-NN classifier achieves results comparable to a linearprobe classifier. Interestingly, zero-shot performance with our embeddings outperforms the supervised neural network models introduced along with the release of the SST-2 dataset. We also fine-tune the unsupervised model on MS-MARCO and evaluate it on a suite of zero-shot search tasks in the BEIR benchmark (Thakur et al., 2021). In the transfer setting, our models achieve a 5.2% relative improvement over previous methods (Izacard et al., 2021) and is comparable even with methods (Santhanam et al., 2021;Formal et al., 2021;Wang et al., 2020) that demand substantially more computation at test time."}
{"tabids": ["ff0a2460-7c6f-409b-8f91-30d45d25268d"], "corpus_id": 245144556, "title": "Large Dual Encoders Are Generalizable Retrievers", "paper_id": "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e", "abstract": "It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.", "intro": "Typical neural retrieval models follow a dual encoder paradigm (Gillick et al., 2018;Karpukhin et al., 2020). In this setup, queries and documents are encoded separately into a shared fixed-dimensional embedding space, where relevant queries and documents are represented in each other's proximity. Then, approximated nearest neighbor search (Vanderkam et al., 2013;Johnson et al., 2021) is applied to efficiently retrieve relevant documents given an encoded input query. * Correspondence to jianmon@google.com \u2020 Work done while at Google Research While dual encoders are popular neural retrievers, the expressiveness of the model is limited by a bottleneck layer consisting of only a simple dotproduct between query and passage embeddings. Lu et al. (2021); Khattab and Zaharia (2020) argued that the dot-product (or cosine similarity) between the embeddings might not be powerful enough to capture the semantic relevance. Similarly, Thakur et al. (2021) suggested that dual encoder models have \"issues for out-of-distribution data\" and models with more interactions between queries and documents have better generalization ability.\n\nIn this paper, we challenge this belief by scaling up the dual encoder model size while keeping the bottleneck as a single dot-product with a fixed size. Note that scaling up a dual encoder is different from scaling up pretrained language models, such as BERT  and T5 (Raffel et al., 2020), because of the presence of the bottleneck layer. While increasing the model size can greatly increase model capacity, for dual encoders with a fixed bottleneck embedding size, the interactions between queries and documents are still limited by a simple dot-product.\n\nTo test this hypothesis, we take advantage of the T5 architecture and checkpoints to build encoders of up to 5 billion parameters while keeping the  Figure 2: Architecture of Generalizable T5-based dense Retrievers. The research question we ask is: can scaling up dual encoder model size improve the retrieval performance while keeping the bottleneck layers as a single dot-product with a fixed size? Only the encoder is taken from the pre-trained T5 models, and the two towers of the dual encoder share parameters.\n\nbottleneck embedding dimension of 768 in all configurations, as illustrated in Figure 2. Following , we build dual encoders by taking the encoder part of T5. To effectively leverage the power of large models, we collect two billion web question-answer pairs as generic pre-training data. By combining pre-training with generic data and fine-tuning with MS Marco (Nguyen et al., 2016), we are able to train large-scale dual encoder retrieval models. We call the resulting models Generalizable T5-based dense Retrievers (GTR). We assess the zero-shot performance of GTR on the BEIR benchmark (Thakur et al., 2021), which has 18 information retrieval tasks across 9 domains. We showed that scaling up leads to better generalization despite the fixed single-dot product bottleneck. Second, pre-training on community questionanswer pairs and fine-tuning on human curated data are both important to fully utilize the power of the scaled up model. In addition, with scaling and pre-training, we found GTR to be highly data efficient in terms of human annotated queries, as it only needs to use 10% of MS Marco to match the overall out-of-domain performance."}
{"tabids": ["ff0a2460-7c6f-409b-8f91-30d45d25268d"], "corpus_id": 253581733, "title": "Task-aware Retrieval with Instructions", "paper_id": "8cf05ed2b7cd3b0f601c454914a678c24d393de3", "abstract": "We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.", "intro": "Information retrieval (IR) is the task of finding relevant documents from a large collection of texts to fulfill a user's information need, typically expressed in the form of a textual query (Singhal et al., 2001). The notion of relevance from the user's perspective (i.e., intent) can be amorphous (Mizzaro, 1998), and a query alone may not fully capture user information needs (Ruthven and Lalmas, 2003;Taylor, 1962). As illustrated in Figure 1 (top), given the same query, \"implementing batch normalization in Python,\" a user may want to retrieve a passage that describes how to do the task or to identify a similar query, or even to directly locate a code snippet.  Figure 1: User intents are not fully captured in query q only (top). Conventional approaches (bottom left) take a query and retrieve documents from a closed corpus using a task-specific retriever. Retrieval with instructions (bottom right) takes a query and explicit intent and retrieves documents aligning with the user's expectations.\n\nMost existing work tries to learn those implicit intents from labeled data (e.g., pairs of queries and relevant documents), yielding separate models for different intents as shown in the bottom left of Figure 1. This approach has several limitations. First, a vast number of annotated examples may be required to train a model to capture the task-specific notion of relevance, while they could benefit from the abundance of data available from related tasks. Second, a model trained on one task may not easily transfer to new tasks that are not closely related.\n\nIn this work we advocate for a new task formulation, retrieval with instructions, to explicitly model a user's search intent by providing a natural language description of the search task (i.e., an instruction). Here, the goal of retrieval systems is to retrieve documents that are both relevant to the query and well-suited to the instructions.\n\nDespite active research in other settings, instruction-following has not been systematically explored in retrieval, partly due to the lack of annotated resources. To facilitate research in retrieval with instructions, we introduce BERRI (Bank of Explicit RetRieval Instructions), a collection of approximately 40 retrieval datasets with diverse instructions in a unified format, covering 10 diverse domains. Each task has on average 3.5 diverse instructions annotated by experts, following our novel instruction schema for retrieval tasks.\n\nWe use BERRI to train TART (Task-aware ReTriever), a single multi-task retrieval system that follows instructions to perform diverse tasks with no parameter updates on each task. We employ two widely explored architectures: TART-dual is a dense dual-encoder architecture, retrieving documents based on the similarity of independently encoded query and document embeddings; TARTfull calculates probabilities of a document being relevant to the query according to the instruction using a cross-encoder. TART is trained with carefully designed negative samples, including our novel instruction-unfollowing negatives samples.\n\nThe TART models, particularly TART-full yields state-of-the-art results on two popular zeroshot retrieval benchmarks, BEIR (Thakur et al., 2021) and LOTTE-pooled (Santhanam et al., 2022), outperforming systems using three times more parameters (Nogueira et al. 2020;Ni et al. 2021; Muennighoff 2022) as well as task-specific retrievers trained on millions of automatically generated examples (Dai et al., 2022;Wang et al., 2022a).\n\nWe further introduce a new evaluation setup, X 2 -Retrieval (Cross-task Cross-domain Retrieval), where a system needs to handle queries with diverse intents to find relevant documents from a large-scale, cross-domain pooled corpus, simulating challenges in real-world retrieval applications. In this under-explored setting, TART outperforms other state-of-the-art methods, demonstrating its ability to find documents in a large-scale opendomain corpus by leveraging explicit textual intents. Our analysis shows that training a model on diverse tasks with instructions, our new negative samples leveraging instructions and giving informative instructions are crucial.\n\nIn summary, our contributions are as follows:\n\n\u2022 Retrieval with instructions, a new formulation to model users' intent explicitly (Section 3).\n\n\u2022 BERRI, a new large-scale collection of approximately 40 retrieval datasets in diverse domains with instructions (Section 4).\n\n\u2022 TART, a task-aware retriever trained on BERRI that advances state of the art on zeroshot and cross-task retrieval (Section 5)."}
{"tabids": ["ff0a2460-7c6f-409b-8f91-30d45d25268d"], "corpus_id": 246863488, "title": "Transformer Memory as a Differentiable Search Index", "paper_id": "9d40837175577bb0009b138269b422f6d5820d00", "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.", "intro": "Information retrieval (IR) systems map a user query q \u2208 Q to a ranked list of relevant documents {d 1 , . . . , d n } \u2286 D, typically represented by integers or short strings called document identifiers (docids). The most widely used IR approaches are based on pipelined retrieve-then-rank strategies. For retrieval, approaches based on inverted indexes or nearest neighbor search are common where contrastive learning based dual encoders (DEs) (Gillick et al., 2018;Karpukhin et al., 2020;Ni et al., 2021) are the present state-of-the-art. This paper proposes an alternative architecture, wherein a sequence-to-sequence (seq2seq) learning system (Sutskever et al., 2014) is used to directly map a query q to a relevant docid j \u2208 Y. This proposal is shown in the bottom half of Figure 1, for a sequence-to-sequence encoder-decoder architecture.\n\nWe call this proposed architecture a differentiable search index (DSI), and implement it with a large pre-trained Transformer (Vaswani et al., 2017) model, building on the recent success of large generative language models (LMs) Raffel et al., 2019;Devlin et al., 2018;Thoppilan et al., 2022;Du et al., 2021). In this proposed architecture, all information of the corpus is encoded within the parameters of the Transformer language model. At inference time, the trained model takes as input a text query q and outputs a docid j. If desired, beam search can be used to produce a ranked list of potentially-relevant docids. As we show, this process can work surprisingly well when trained properly. In our experiments it can consistently outperform DE baselines, sometimes drastically: for a base-sized T5 model, Hits@1 on the smallest corpus is improved by more than 20 points, from 12.4% for a DE to 33.9% for DSI; and on a corpus   \neach term t \u2192 {dj 1 , . . . , dj k } each docvec v d j \u2192 j to map dj \u2192 j retrieval approximate sparse matmul approximate MIPS run trained model (top-1) to find argmax j v T q v d j to find argmax j v T q v d j to find argmax j Pr(j|q)\n30\u00d7 larger, performance is improved by nearly 7 points. These gains increase when larger models are used: for an 11B-parameter T5 model, Hits@1 performance improves by more than 25 points over DE on the small corpus, and more than 15 points on the large corpus. DSI also performs extremely well in a zero-shot setting, e.g., improving Hits@1 by 14 points over BM25.\n\nIn addition to these quantitative gains, the DSI architecture is much simpler than a DE (see Table 1).\n\nA DE system fixes a search procedure (MIPS) and learns internal representations that optimize performance for that search procedure; in contrast, a DSI system contains no special-purpose fixed search procedure, instead using standard model inference to map from encodings to docids.\n\nOf particular interest to the machine learning community, as Table 1 shows, in DSI all aspects of retrieval are mapped into well-understood ML tasks. This may lead to new potential approaches to solving long-standing IR problems. As one example, since indexing is now a special case of model training, incrementally updating an index becomes a special case of model updating .\n\nIn this paper, DSI is applied to moderate-sized corpora (from 10k to 320k documents), all of which are derived from one challenging retrieval task, and we leave the important question of the scaling DSI to larger corpora to future work. The task considered is retrieving supporting passages given questions from the Natural Questions (NQ) dataset, a challenging task for lexical models.\n\nWhile the idea of DSI is simple, there are a number of ways it can be realized, some of which work surprisingly well, and some of which work surprisingly poorly. Below we explore a number of variations of the DSI architecture.\n\nDocument representation. We explore several approaches to representing documents, including a \"naive\" approach of using the document's full text, as well as variants of the bag-of-words representation used by traditional IR engines.\n\nDocid representation. We look at several ways to represent docids. In addition to naively representing integers as text strings, we also consider unstructured atomic docids, where each document is assigned a unique token, and some simple baselines for constructing structured semantic docids that describe how to navigate to a document through a hierarchical clustering of the corpus. Structured docidseither semantically structured via clustering, or naively structured as tokenized integers-scale better to large corpora, since the size of the vocabulary used in the decoder is made larger.\n\nIndexing. A trainable IR system traditionally has two phases: indexing a corpus (i.e., memorizing information about each document), and learning how to effectively retrieve from the index. In DSI, the index is stored in the model parameters, and indexing is simply another kind of model training. Figure 1 suggests one approach to indexing a corpus: namely, to train on (1) (2) alone do not provide enough information for a system to generalize to novel retrievals, there are many alternatives to examples of type (1) that might plausibly \"teach\" a model about the associations between documents and docids. We explore a number of these below, and show that some plausible-seeming techniques perform very poorly. We also explore a number of alternative multi-task optimization and curriculum learning schemes for combining these types of examples.\n\nEffects of model and corpus size. Since recent results suggest that some properties of large LMs emerge only for very large model sizes , we explore the performance of DSI for a range of model sizes and corpus sizes of 10k, 100k, and 320k documents.\n\nSummary. We show that even naive representations for documents and docids, coupled with appropriate training procedures to fine-tune modern large LMs, can perform surprisingly well; we present two improved docid representations, unstructured docids and semantically-structured docids, which improve the naive representation choice. We show that there is substantial variation in performance among indexing/training strategies and we show that performance of DSI significantly and consistently improves with model scale. To our knowledge this is the first case of generative indexing improving performance over strong baselines for a well-studied document retrieval task."}
{"tabids": ["ff0a2460-7c6f-409b-8f91-30d45d25268d"], "corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "paper_id": "a19193ef534619654d3252d4034ef154407562dc", "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.", "intro": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;. For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022). To reduce the high training cost of autoregressive search engine, a smaller model size is preferred. However, the results of our pilot study in Figure 1 show smaller language models are significantly worse at mapping passages to document identifiers than larger ones. Moreover, different retrieval tasks can have unique retrieval requirements. One task may require a model to retrieve factual evidence to support or refute a claim (i.e., fact checking) (Onoe et al., 2021) while another may require a model to retrieve specific trivia information about an entity (i.e., entity linking) (Petroni et al., 2021;. It would be better if the retriever was capable of generalizing to new retrieval tasks with only a few examples.\n\nIn this work, we explore the use of in-context demonstrations to prompt LLMs to directly generate web URLs for document retrieval, namely LLM-URL. Surprisingly, we find that by providing a few (query, URL) pairs as contextual demonstrations, large language models (e.g. GPT-3) generate Web URLs where nearly 90% of the corresponding documents contain answers to opendomain questions. In this way, LLMs can be thought of as built-in search engines, as they have not been explicitly trained to map questions or documents to identifiers. Instead of using newlycreated document identifiers, LLM-URL leverages existing and widely used document identifiers directly, i.e., URLs. We compare our approach to existing document retrieval methods on three different open-domain question answering (QA) datasets: WebQ (Berant et al., 2013), NQ (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017). Further, to avoid exceeding the limit on the number of input tokens of LLMs, we employ an unsupervised passage filtering module to remove irrelevant portions of supporting documents. To summarize, our main contributions are as follows:\n\n1. We reveal that LLMs are built-in autoregressive search engines capable of document re-trieval by directly generating Web page URLs under both zero and few-shot settings.\n\n2. We show retrieving documents by generating URLs with LLMs significantly outperforms existing methods for document retrieval, as measured by Recall@K. Further, we show that breaking the retrieved documents into passages then using a ranker to filter the passages significantly reduces the number of supporting passages while maintaining high recall.\n\n3. We show the retrieved documents improve downstream QA performance as measured by EM when compared to baseline methods.\n\n2 Related Work"}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "paper_id": "832fff14d2ed50eb7969c4c4b976c35776548f56", "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.", "intro": "Recent advances in language model pre-training have shown that models such as BERT (Devlin et al., 2018), RoBERTa  and T5 (Raffel et al., 2019) store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on (Petroni et al., 2019). For example, BERT is able to * Equal contribution 1 Google Research. Correspondence to: Kelvin Guu <kguu@google.com>, Kenton Lee <ken-tonl@google.com>, Zora Tung <gatoatigrado@google.com>, Panupong Pasupat <ppasupat@google.com>, Ming-Wei Chang <mingweichang@google.com>. Figure 1. REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus, Z (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in Z-a significant computational challenge that we address. correctly predict the missing word in the following sentence: \"The is the currency of the United\n\nKingdom\" (answer: \"pound\").\n\nIn these language models, the learned world knowledge is stored implicitly in the parameters of the underlying neural network. This makes it difficult to determine what knowledge is stored in the network and where. Furthermore, storage space is limited by the size of the network-to capture more world knowledge, one must train ever-larger networks, which can be prohibitively slow or expensive.\n\nTo capture knowledge in a more interpretable and modular way, we propose a novel framework, Retrieval-Augmented Language Model (REALM) pre-training, which augments language model pre-training algorithms with a learned textual knowledge retriever. In contrast to models that store knowledge in their parameters, this approach explicitly exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during inference. Before making each prediction, the language model uses the retriever to retrieve documents 1 from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction. Learning this model end-toend requires backpropagating through a retrieval step that considers an entire corpus of textual knowledge, as shown in Figure 1.\n\nThe key intuition of REALM is to train the retriever using a performance-based signal from unsupervised text: a retrieval that improves the language model's perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized. For example, in Figure 1, if the model needs to fill the blank in \"the at the top of the pyramid\", the retriever should be rewarded for selecting a document containing \"The pyramidion on top allows for less material higher up the pyramid\". We achieve this behavior by modeling our retrieve-then-predict approach as a latent variable language model and optimizing the marginal likelihood.\n\nIncorporating a large-scale neural retrieval module during pre-training constitutes a significant computational challenge, since the retriever must consider millions of candidate documents for each pre-training step, and we must backpropagate through its decisions. To address this, we structure the retriever such that the computation performed for each document can be cached and asynchronously updated, and selection of the best documents can be formulated as Maximum Inner Product Search (MIPS).\n\nNumerous prior works have demonstrated the benefit of adding a discrete retrieval step to neural networks (Miller et al., 2016;Chen et al., 2017), but did not apply the framework to language model pre-training and employed non-learned retrievers to handle large-scale document collections. In the language modeling literature, the k-Nearest Neighbor Language Model (Khandelwal et al., 2019) (kNN-LM) retrieves similar LM examples to improve memorization. However, kNN-LM was not finetuned for downstream tasks, perhaps because it is unclear how to adapt the retrieval mechanism: a kNN can only use examples labeled for the target task-during fine-tuning, this precludes LM examples, which contain the desired world knowledge. In contrast, REALM's retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.\n\nWe evaluate our approach by fine-tuning the models pre-trained with REALM on the task of Opendomain Question Answering (Open-QA), one of the most knowledge-intensive tasks in natural language processing. We evaluate on three popular Open-QA benchmarks (NATURALQUESTIONS-OPEN, WEBQUESTIONS, and CURATEDTREC) and compare to state-of-the-art Open-QA models, including both extremely large models that store knowledge implicitly (such as T5) as well as previous approaches that also use a knowledge retriever to access external knowledge, but implement retrieval in a more heuristic fashion Min et al., 2019a;Asai et al., 2019). REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy. We also demonstrate qualitative benefits of REALM, including interpretability and modularity."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "paper_id": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.", "intro": "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [41]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [45,46]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can't straightforwardly provide insight into their predictions, and may produce \"hallucinations\" [34]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [18,22,42] can address some of these issues because knowledge can be directly revised and expanded, and its access can be inspected and interpreted. REALM [18] and ORQA [27], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results, Figure 1: An overview of retrieval-augmented generation (RAG). We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained encoder-decoder (Generator) and fine-tune end-to-end. For some query x, we use Maximum Inner Product Search (MIPS) to find the top-K most relevant documents of all documents z i . To make the final prediction y, we treat z as a latent variable and marginalize over the encoder-decoder predictions given different documents.\n\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \"workhorse of NLP,\" i.e. sequence-to-sequence (seq2seq) models.\n\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained generative seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed using a pre-trained neural retriever. We combine these components in an end-to-end probabilistic model; the document retriever (Dense Passage Retriever [22], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [28]) then conditions on both these latent documents and the input to generate the output. We marginalize the latent variables through a top-K approximation, either on a per answer basis (assuming the same document is responsible for all tokens) or a per answer token basis (assuming different documents can be responsible for different tokens). Just like T5 [45] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the sequence generator and retriever are jointly learned.\n\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks-e.g. in memory networks [58,49], stackaugmented networks [21] and memory layers for transformers [26]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained knowledge-access mechanisms, the ability to access knowledge is present without additional training.\n\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks. Our RAG models achieve state-of-the-art results on open Natural Questions [25], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [20]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For the FEVER [50] fact verification task, we achieve results within 4% of sophisticated, state-of-the-art pipeline models which use strong supervision. Finally, we show that the non-parametric memory can be replaced in order to control generation, demonstrating a simple mechanism to update the knowledge that the model uses as facts about the world change."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "paper_id": "07b14c24833400b79978b0a5f084803337e30a15", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.", "intro": "Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022b;Yasunaga et al., 2022), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model Figure 1. Different from previous retrieval-augmented approaches  that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served via APIs. Izacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.\n\nIn this work, we introduce REPLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy. As shown in arXiv:2301.12652v4 [cs.CL] 24 May 2023 Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.\n\nWe also introduce REPLUG LSR (REPLUG with LM-Supervised Retrieval), a training scheme that can further improve the initial retrieval model in REPLUG with supervision signals from a black-box language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work  that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\n\nOur experiments show that REPLUG can improve the performance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019;Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) leads to additional improvements, including up to 6.3% increase in GPT-3 175B language modeling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs (>100B model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:\n\n\u2022 We introduce REPLUG ( \u00a73), the first retrievalaugmented language modeling framework for enhancing large black-box language models with retrieval.\n\n\u2022 We propose a training scheme ( \u00a74) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.\n\n\u2022 Evaluations on language modeling ( \u00a76), open-domain QA and MMLU demonstrate that REPLUG can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 251371732, "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "paper_id": "398e4061dde8f5c80606869cebfa2031de7b5b74", "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.", "intro": "Large language models (LLMs) are impressive few-shot learners Rae et al., 2021;Hoffmann et al., 2022;Chowdhery et al., 2022). They are able to learn new tasks with very few examples or even from instructions alone. For this generalisation ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the larger training data. While it is intuitive to assume that increased reasoning abilities lead to better generalisation, and hence few-shot learning, the same is not true for in-parameter memorisation. Specifically, it is unclear to what extent effective few-shot learning requires vast knowledge in the parameters of the model.\n\nIn this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from generalisation. To do so, we leverage the fact that memory can be outsourced and replaced by an external non-parametric knowledge source by employing a retrieval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, interpretability and efficiency (Guu et al., 2020;Borgeaud et al., 2021, inter alia). However, retrieval-augmented models have yet to \u2026 \u2026 Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot performance on knowledge tasks, and uses retrieval during both pre-training and fine-tuning.\n\ndemonstrate compelling few-shot learning capabilities. In this work we address this gap, and present Atlas, a retrieval-augmented language model capable of strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners.\n\nAtlas retrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder architecture, based on the Contriever . The retrieved documents are processed, along with the current context, by a sequence-to-sequence model using the Fusion-in-Decoder architecture ) that generates the corresponding output. We study the impact of different techniques to train Atlas on its few-shot performance on a range of downstream tasks, including question answering and fact checking. We find that jointly pre-training the components is crucial for few-shot performance, and we carefully evaluate a number of existing and novel pre-training tasks and schemes for this purpose. Atlas achieves strong downstream performance in both few-shot and resource-rich settings. For example, with only 11B parameters, Atlas achieves an accuracy of 42.4% on NaturalQuestions using 64 training examples (45.1% with a Wikipedia-only index), outperforming PaLM (Chowdhery et al., 2022), a 540B parameter model by almost 3 points, and 64.0% in a full-dataset setting with a Wikipedia index, establishing a new state of the art by 8 points.\n\nIn summary we make the following contributions:\n\n\u2022 A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample efficiency.\n\n\u2022 The findings of this study lead to a retrieval-augmented language model, called Atlas, that exhibits few-shot abilities that emerge at lower scale than standard LLM.\n\n\u2022 We provide an exploration of fine-tuning strategies to efficiently adapt both the retriever and the language model to the task at hand.\n\n\u2022 Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15\u00d7 more parameters on MMLU.\n\n\u2022 Experiments investigating full-dataset finetuning, setting new state-of-the-art results in NaturalQuestions (+8.1%), TriviaQA (+9.3%) and 5 KILT Tasks.\n\n\u2022 Experiments demonstrating the updatability and interpretability characteristics of Atlas.\n\n\u2022 Experiments demonstrating that a compressed index using product quantisation achieves comparable performance as an uncompressed index while resulting in a 5x memory reduction.\n\nOur code, pretrained Atlas checkpoints, and various supporting data are available at https://github.com/ facebookresearch/atlas"}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 247362809, "title": "Internet-augmented language models through few-shot prompting for open-domain question answering", "paper_id": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd", "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.", "intro": "Undoubtedly, large-scale language models (LSLMs) present a research breakthrough for language research, particularly for their state-of-the-art language modeling results [1,2] and impressive generative capabilities. Above all, increasing scale has made few-shot learning a defining new paradigm for language models (LMs). Due to the versatility of prompting, these models can now be quickly adapted using only a handful of examples to perform tasks ranging from question answering and numeric reasoning to creative writing [3]. All these considerations place few-shot LSLMs at an excellent position to be used as building blocks for open-ended and \"in the wild\" user interactions.\n\nDespite these successes, few-shot LSLMs still lack a key ingredient; they are susceptible to hallucinations [4] caused by incorrect retrieval of knowledge stored in their weights or due to the model having incomplete or outdated knowledge. As for many user interactions we expect factuality to play an important role, it is imperative to find ways to keep LSLMs up-to-date and grounded to factual and new information as it becomes available. As the current trend sees the size of these models to continually grow, mitigating those issues should rely on flexible and robust approaches that can be easily transferred to different domains and tasks.\n\nHere, we aim to capitalize on the unique benefits offered by pre-trained LSLMs and propose to overcome some of their limitations by drawing ideas from semi-parametric models [5][6][7][8] that ground their decisions in external retrieved evidence to reduce hallucinations and improve factuality [9]. Specifically, we use the Internet as a source of up-to-date knowledge, and rely on the powerful few-shot capabilities of these LSLMs to learn how to use it effectively for answering questions. Taking open-domain question answering as a task where factual correctness is vital, we design a system that given a question uses a retrieval model to retrieve relevant documents from the Internet. Then, using few-shot learning we prompt the model to answer the question via conditioning on the retrieved documents, without the need to fine-tune or learn extra parameters. As a retrieval system we use a search engine -in particular Google Search -allowing us to treat the whole web as a knowledge source. While Wikipedia has been the dominant knowledge source driving progress on a multitude of tasks, given the current progress and the quest towards more complex interactions, there has never been a better time to widen their scope, embracing the opportunities working with the whole web, such as considering a wider range of topics and views, as well as the many challenges, such as working with more noisy and potentially uncurated and unsafe text in the wild. Indeed, there is momentum building up in breaking away from Wikipedia-only research [10][11][12][13].\n\nTo test the effectiveness of equipping LSLMs with Internet search on open-domain question answering, we use a mix of single-hop and multi-hop, language generation and classification tasks. We find that our biggest LSLMs benefit from conditioning on the web through few-shot prompting. For the language generation tasks, we see a relative performance increase of 15%-30% over the commonly used closed-book few-shot approach. Surprisingly, we find that our method achieves gains, albeit smaller, even on complex multi-hop questions, despite the fact that these questions suffer from higher retrieval errors. Moreover, we see that in certain cases conditioning models on the Internet makes up performance-wise for their smaller size. While perhaps the mainstream view places scaling models' parameters as the primary way to increase their few-shot performance, our results add to the stream of work that emphasizes instead better use of the models' powerful prompting abilities [14,15]. As such, our approach presents a lightweight method applicable to virtually any pre-trained LM without the need for fine-tuning or adding extra learnable parameters. Finally, increasing the inferencetime compute of models via sampling multiple answers and reranking using scores computed from the same LSLMs not only adds further performance gains, but also alleviates generally decreased performance of smaller few-shot LMs, partly closing their performance gap with larger models.\n\nAll in all, our findings hint at the possibility of slowing down the race towards the biggest model and instead shifting the attention to more targeted and effective use of models' few-shot capabilities in combination with increasing inference-time compute, a generally more scalable approach."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 255372320, "title": "Rethinking with Retrieval: Faithful Large Language Model Inference", "paper_id": "490d8006851b1562cfd9ec1f057471f2868289d1", "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.", "intro": "Large language models (LLMs) have shown exceptional performance across various tasks through in-context learning without task-specific training or fine-tuning (Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;Ouyang et al., 2022). Recent progress in prompting Kojima et al., 2022) and decoding  has made it feasible for LLMs to tackle tasks that demand complex reasoning. * Part of this work was done while the author was at the University of Pennsylvania. 1 Our code is publicly available at https://github. com/HornHehhf/RR.  Figure 1: An overview of three approaches for using LLMs: (a) Standard prompting for generating a prediction in response to a query. (b) Chain-of-thought prompting for generating both an explanation and a prediction in response to a query. (c) Rethinking with retrieval, our proposed approach for using the decomposed reasoning steps obtained from chain-of-thought prompting to retrieve relevant external knowledge for LLMs, leading to more faithful explanations and improved predictions in response to a query.\n\nHowever, the knowledge stored in LLMs might inevitably be incomplete, out-of-date, or incorrect. As a result, external sources of knowledge, such as Wikipedia, may be essential for the successful deployment of LLMs for real-world applications. Previously, people tried to utilize knowledge for smaller language models (LMs), such as T5 (Raffel et al., 2020), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019). However, these methods often require additional training or fine-tuning, which can be costly and thus impractical for LLMs.\n\nIn this paper, we present a post-processing approach called rethinking with retrieval (RR) for utilizing external knowledge in LLMs. Our method begins by using the chain-of-thought (CoT) prompting method  to generate a diverse set of reasoning paths, as described in . We then use each reasoning step in those paths to retrieve relevant external knowledge, which enables RR to provide more faithful explanations and more accurate predictions, as illustrated in Figure 1.\n\nWe evaluate the effectiveness of our proposed method, RR, on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning, using GPT-3 175B (Brown et al., 2020) and different external knowledge sources: Wikipedia, Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), WordNet (Miller, 1995), and Conceptnet (Speer et al., 2017). The results demonstrate that RR consistently outperforms all baselines on all three tasks without requiring additional training or fine-tuning, indicating the superiority of our approach in leveraging external knowledge to enhance the performance of LLMs.\n\n2 Related Work Enhancing LMs through retrieval. Retrievalenhanced LMs have received significant attention as a means of improving performance through the incorporation of external knowledge. For example, the k-most similar training contexts can be retrieved to improve the estimation of the next word distribution in both the training stage (Borgeaud et al., 2021) and the inference stage (Khandelwal et al., 2020). Furthermore, search query generators have been adopted to generate search queries for search engines to retrieve relevant documents (Komeili et al., 2022;Shuster et al., 2022;Thoppilan et al., 2022). Other approaches have utilized retrieved documents as the additional context in generation tasks (Joshi et al., 2020;Guu et al., 2020;Lewis et al., 2020). Nakano et al. (2021) instead use human feedback in a text-based web-browsing environment. Among these previous works, Khandelwal et al. (2020) is most closely related to our approach. However, they focus on improving local inference by using the nearest neighbor datastore constructed from training data, whereas we focus on conducting faithful inference using external knowledge. In contrast to other aforementioned approaches, which require training or fine-tuning to incorporate retrieved knowledge, we propose a post-processing method for leveraging retrieved knowledge without additional training or fine-tuning.\n\nIncorporating external knowledge into LMs. Significant effort has been devoted to leveraging external knowledge to improve the reasoning ability of LMs. Previous work has incorporated external knowledge sources such as WordNet (Miller, 1995) and ConceptNet (Speer et al., 2017) to enhance LMs for tabular reasoning tasks (Neeraja et al., 2021;Varun et al., 2022). Explicit rules have also been added to inputs to improve reasoning ability over implicit knowledge (Talmor et al., 2020). In addition, explicit knowledge from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and implicit knowledge in LLMs have been integrated into a transformer (Vaswani et al., 2017) for visual question answering (Gui et al., 2021). Nye et al. (2021) instead introduces a symbolic reasoning module to improve coherence and consistency in LLMs. Among these previous works, Nye et al. (2021) is the most relevant to our approach. Still, they focus on incorporating logical constraints to improve coherence and consistency, whereas we aim to improve the faithfulness of explanations through the use of external knowledge. In contrast to other aforementioned approaches that incorporate external knowledge before generation and require additional training or fine-tuning, our proposal leverages external knowledge in a postprocessing manner to enhance LMs without additional training or fine-tuning.\n\nUncovering latent Knowledge in LLMs. There has been a line of work exploring the knowledge hidden within LLMs for reasoning. This has included the use of careful prompting to encourage LLMs to generate explanations in the reasoning process, such as through chain of thought prompting in few-shot  or zero-shot (Kojima et al., 2022) learning, or through the use of scratchpads for intermediate computation (Nye et al., 2022). In addition, various methods based on sampling a diverse set of reasoning paths in LLMs have been proposed, including training verifiers to judge the correctness of model completions (Cobbe et al., 2021), calibrating model predictions based on the reliability of the explanations (Ye and Durrett, 2022), and promoting selfconsistency over diverse reasoning paths . Zelikman et al. (2022) instead iteratively bootstrap the ability of LLMs to generate high-quality rationales from a few initial examples. Liu et al. (2022) further propose generating knowledge from LLMs, which is then used as additional input to improve commonsense reasoning. In contrast to this line of work, our proposal focuses on leveraging external knowledge to enhance LLMs, while they aim to explore the knowledge hidden within LLMs.\n\nLLMs have been shown to generate incorrect supporting facts from time to time, even when they accurately capture the perspective needed to answer a question. This phenomenon highlights intrinsic issues in the way LLMs store and retrieve knowledge, including (1) the presence of out-of-date, incorrect, or missing relevant knowledge in the pre-training corpus; (2) incorrect memorization of relevant knowledge during pre-training; and (3) incorrect retrieval of relevant knowledge during the inference stage. To address these issues, we propose the use of RR, which leverages external knowledge through the retrieval of relevant information based on decomposed reasoning steps.\n\nOverview. Given a query Q, we utilize chain-ofthought prompting to generate a diverse set of reasoning paths R 1 , R 2 , \u00b7 \u00b7 \u00b7 R N , where each reasoning path R i consists of an explanation E i followed by a prediction P i . After that, we retrieve relevant knowledge K 1 , \u00b7 \u00b7 \u00b7 K M from a suitable knowledge base KB to support the explanation in each reasoning path, and select the predictionP that is most faithful to this knowledge. To better illustrate our proposal, we use \"Did Aristotle use a laptop?\" as a running example in this work.\n\nChain-of-thought prompting. In contrast to standard prompting, CoT prompting  includes demonstrations of step-by-step reasoning examples in the prompt to produce a series of short sentences that capture the reasoning process. For instance, given the question \"Did Aristotle use a laptop?\", CoT prompting aims to generate the complete reasoning path \"Aristotle died in 322 BC. The first laptop was invented in 1980. Thus, Aristotle did not use a laptop. So the answer is no.\" rather than simply outputs \"No.\" Empirical results show that CoT prompting significantly improves the performance of LLMs on many multistep reasoning tasks. Therefore, we adopt CoT prompting to obtain both explanation E and prediction P for the query Q.\n\nSampling diverse reasoning paths. Similar to , we sample a diverse set of reasoning paths R 1 , R 2 , \u00b7 \u00b7 \u00b7 R N rather than only considering the greedy path as in . For the question \"Did Aristotle use a laptop?\", the potential reasoning paths can be as follows:\n\n(R 1 ) Aristotle died in 2000. The first laptop was invented in 1980. Thus, Aristotle used a laptop. So the answer is yes.\n\n(R 2 ) Aristotle died in 322BC. The first laptop was invented in 2000. Thus, Aristotle did not use a laptop. So the answer is no.\n\n(R 3 ) Aristotle died in 322BC. The first laptop was invented in 1980. Thus, Aristotle did not use a laptop. So the answer is no.\n\nKnowledge retrieval. Different knowledge bases can be used to address different tasks. For example, to address the question \"Did Aristotle use a laptop?\", we can use Wikipedia as the external knowledge base KB. Information retrieval techniques can be applied to retrieve the relevant knowledge K 1 , \u00b7 \u00b7 \u00b7 K M from Wikipedia based on the decomposed reasoning steps. Ideally, we would obtain the following two paragraphs from Wikipedia for this question:\n\n(K 1 ) Aristotle (384-322 BC) was a Greek philosopher and polymath during the Classical period in Ancient Greece. ...\n\n(K 2 ) The Epson HX-20, the first laptop computer, was invented in 1980. ..."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 259108339, "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit", "paper_id": "cc78babfacce48e715dac56886d7dd9746cfcab0", "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.", "intro": "Large language models (LLMs) have attracted increasing attention from both research community and industry (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023;Chowdhery et al., 2022;Zeng et al., 2022). With tremendous world knowledge stored in parameters (Petroni et al., 2019;Roberts et al., 2020;Jiang et al., 2020) and the Reinforcement Learning * Corresponding author.\n\nfrom Human Feedback (RLHF) techniques (Christiano et al., 2017;Ziegler et al., 2019), LLMs can generate helpful, detailed, and polite texts in response to user inputs. Many studies have demonstrated LLMs' extraordinary abilities in various areas, including nature language processing (Moslem et al., 2023), information retrieval (Sun et al., 2023;Wang et al., 2023;Mao et al., 2023), and recommendation .\n\nHowever, LLMs still tend to hallucinate and sometimes generate texts opposite to facts (Zhou et al., 2021;. To tackle these problems, researchers have proposed a new paradigm to strengthen LLMs with information retrieval systems (retrieval-augmented LLMs) (Shi et al., 2023;Jiang et al., 2023;Nakano et al., 2022), which enables LLMs to retrieve relevant contents from an external repository (knowledge corpus) to generate texts based on them. It has been verified that retrieval-augmented LLMs can generate texts in response to user input with fewer hallucinations (Nakano et al., 2022). Furthermore, by incorporating customized private data resources, retrieval-augmented LLMs can respond to in-domain queries that cannot be answered by LLMs trained with public data.\n\nTo support research in this area and help users build their own in-domain LLM-based systems, we devise RETA-LLM, a RETreival-Augmented LLM toolkit. Different from previous general LLMenhanced toolkits such as LangChain, 1 RETA-LLM focuses on the retrieval-augmented LLMs and provides more plug-in modules. Typically, retrieval-augmented LLMs use a retrieve-andgenerate strategy with two modules: First, they retrieve documents or passages based on user request (document retrieval module); then, they generate answers utilizing these relevant documents as references (answer generation module). In addi-"}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 256459451, "title": "In-Context Retrieval-Augmented Language Models", "paper_id": "465471bb5bf1a945549d6291c2d23367966b4957", "abstract": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1", "intro": "Recent advances in language models (LMs) have dramatically increased the usefulness of machine-generated text across a wide range of use-cases and domains (Brown et al., 2020).However, the mainstream paradigm of generating text with LMs bears inherent limitations in access to external knowledge.First, LMs are not coupled with any source attribution, and must be trained in order to incorporate up-to-date information that was not seen during training.More importantly, they tend to produce factual inaccuracies and errors (Lin et al., 2022;Maynez et al., 2020;Huang et al., 2020).This problem is present in any LM generation scenario, and is exacerbated when generation is made in uncommon domains or private data.A promising approach for addressing the above is Retrieval-Augmented Language Modeling (RALM), grounding the LM during generation by conditioning on relevant documents retrieved from an external knowledge source.RALM systems include two high level components: (i) document selection, selecting the set of documents upon which to condition; and (ii) document reading, determining how to incorporate the selected documents into the LM generation process.\n\nLeading RALM systems introduced recently tend to be focused on altering the language model architecture (Khandelwal et al., 2020;Borgeaud et al., 2022;Zhong et al., 2022;Levine et al., 2022c;Li et al., 2022).Notably, Borgeaud et al. (2022) introduced RETRO, featuring document reading via nontrivial modifications that require further training to the LM architecture, while using an off-the-shelf frozen BERT retriever for document selection.Although the paper's experimental findings showed impressive performance gains, the need for changes in architecture and dedicated retraining has hindered the wide adoption of such models.\n\nIn this paper, we show that a very simple document reading mechanism can have a large impact, and that substantial gains can also be made by adapting the document selection mechanism to the task of language modeling.Thus, we show that many of the benefits of RALM can be achieved while working with off-the-shelf LMs, even via API access.Specifically, we consider a simple but powerful RALM framework, dubbed In-Context RALM (presented in Section 3), which employs a zero-effort document reading mechanism: We simply prepend the selected documents to the LM's input text (Figure 1).\n\nSection 4 describes our experimental setup.To show the wide applicability of our framework, we performed LM experiments on a suite of five diverse corpora: WikiText-103 (Merity et al., 2016), RealNews (Zellers et al., 2019), and three datasets from The Pile (Gao et al., 2021): ArXiv, Stack Exchange, and FreeLaw.We use open-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT, and LLaMA model families).\n\nIn Section 5 we evaluate the application of off-the-shelf retrievers to our framework.In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM's number of parameters by 2-3\u00d7 across all of the text corpora we examined.In Section 6 we investigate methods for adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom.Our adaptation methods range from using a small LM to perform zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision from the LM signal.These methods lead to further gains in the LM task corresponding to an additional size increase of 2\u00d7 in the LM architecture.As a concrete example of the gains, a 345M parameter GPT-2 enhanced by In-Context RALM outperforms a 762M parameter GPT-2 when employing an off-the-shelf BM25 retriever (Robertson and Zaragoza, 2009), and outperforms a 1.5B parameter GPT-2 when employing our trained LM-oriented reranker (see Figure 2).For large model sizes, our method is even more effective: In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter parameter OPT model (see Figure 4).(Robertson and Zaragoza, 2009) to the LM task ( \u00a75) yields significant gains, and choosing the grounding documents via our new class of Predictive Rerankers ( \u00a76) provides a further boost.See Table 1 for the full results on five diverse corpora.\n\nIn Section 7 we demonstrate the applicability of In-Context RALM to downstream open-domain questions answering (ODQA) tasks.\n\nIn a concurrent work, Shi et al. (2023) also suggest to augment off-the-shelf LMs with retrieved texts by prepending them to the input.Their results are based on training a dedicated retriever for language modeling.In contrast, we focus on the gains achievable in using off-the-shelf retrievers for this task.We show strong gains of this simpler setting by investigating: (1) which off-the-shelf retriever is best suited for language modeling, (2) the frequency of retrieval operations, and (3) the optimal query length.In addition, we boost the off-the-shelf retrieval performance by introducing two reranking methods that demonstrate further gains in perplexity.\n\nWe believe that In-Context RALM can play two important roles in making RALM systems more powerful and more prevalent.First, given its simple reading mechanism, In-Context RALM can serve as a clean probe for developing document retrieval methods that are specialized for the LM task.These in turn can be used to improve both In-Context RALM and other more elaborate RALM methods that currently leverage general purpose retrievers.Second, due to its compatibility with off-the-shelf LMs, In-Context RALM can help drive wider deployment of RALM systems."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "paper_id": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.", "intro": "Language modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions ( 1 , . . . , ) = ( | < ). Neural networks have proven to be powerful language models, first in the form of recurrent architectures (Graves, 2013;Jozefowicz et al., 2016;Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters (Brown et al., 2020;Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020). The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data.\n\nIn this work, we endeavor to decouple these, by exploring efficient means of augmenting language models with a massive-scale memory without significantly increasing computations. Specifically, we suggest retrieval from a large text database as a complementary path to scaling language models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions-a semi-parametric approach. At a high level, our Retrieval Transformer (R ) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing retrieval for language modelling work only considers small transformers (100 millions parameters) and databases of limited size (up to billions of tokens) (Guu et al., 2020;Khandelwal et al., 2020;Lewis et al., 2020;Yogatama et al., 2021). To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by \u223c 10\u00d7. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours. Past this, performance begins to degrade, perhaps due to the reduced quality. At evaluation R can be used without retrieval data (R [OFF]), bringing limited performance degradation compared to baseline transformers.\n\ncontributions are the following.\n\n\u2022 We introduce R , a retrieval-enhanced autoregressive language model ( \u00a72.2). We use a chunked cross-attention module to incorporate the retrieved text ( \u00a72.4), with time complexity linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen B model ( \u00a72.3) works at scale, removing the need for training and updating a retriever network.\n\n\u2022 We show that our method scales well with model size and database size ( Fig. 1): R provides a constant gain for models ranging from 150M to 7B parameters, and R can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) ( \u00a74). We show that R can be fine-tuned to achieve competitive performance on downstream tasks such as question answering ( \u00a74.3).\n\n\u2022 We propose an evaluation aware of proximity of test documents with the training set ( \u00a72.6), addressing the problem of test set leakage (Lee et al., 2021). This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation. Using this methodology, we show that the performance of R comes from both explicit neighbour copying and general knowledge extraction ( \u00a74.4)."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 254877499, "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "paper_id": "f208ea909fa7f54fea82def9a92fd81dfc758c39", "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.", "intro": "Large language models are capable of answering complex questions by generating step-bystep natural language reasoning steps-so called chains of thoughts (CoT)-when prompted appropriately (Wei et al., 2022). This approach has been successful when all information needed to answer the question is either provided as context (e.g., algebra questions) or assumed to be present in the model's parameters (e.g., commonsense reasoning). 1 Code, data, and prompts are available at https:// github.com/stonybrooknlp/ircot\n\nIn what country was Lost Gravity manufactured?\n\nThe Lost Gravity was manufactured by Mack Rides.\n\nMack Rides is a company from Germany.\n\nThe answer is Germany.\n\ncumulate docs cumulate docs cumulate docs Figure 1: IRCoT interleaves chain-of-thought (CoT) generation and knowledge retrieval steps in order to guide the retrieval by CoT and vice-versa. This interleaving allows retrieving more relevant information for later reasoning steps, compared to standard retrieval using solely the question as the query.\n\nHowever, for many open-domain questions, all required knowledge is not always available or up-todate in models' parameters and it's beneficial to retrieve knowledge from external sources (Lazaridou et al., 2022;Kasai et al., 2022). How can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks that require complex, multi-step reasoning?\n\nWhile a one-shot retrieval from a knowledge source based solely on the question can successfully augment LMs with relevant knowledge for many factoid-based tasks (Lewis et al., 2020;Guu et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022), this strategy has clear limitations for more complex multi-step reasoning questions. For such questions, one often must retrieve partial knowledge, perform partial reasoning, retrieve additional information based on the outcome of the partial reasoning done so far, and iterate. As an example, consider the question illustrated in Fig. 1, \"In what country was Lost Gravity manufactured?\". The Wikipedia document retrieved using the question (in particular, the roller coaster Lost Gravity) as the query does not mention where Lost Gravity was manufactured. Instead, one must first infer that it was manufactured by a company called Mack Rides, and then perform further retrieval, guided by the inferred company name, to obtain evidence pointing to the manufacturing country.\n\nThus, the retrieval and reasoning steps must inform each other. Without retrieval, a model is likely to generate an incorrect reasoning step due to hallucination. Additionally, without generating the first reasoning step, the text supporting the second step can't be identified easily given the lack of lexical or even semantic overlap with the question. In other words, we need retrieved facts in order to generate factually correct reasoning steps and the reasoning steps to retrieve relevant facts.\n\nBased on this intuition, we propose an interleaving approach to this problem, where the idea is to use retrieval to guide the chain-of-thought (CoT) reasoning steps and use CoT reasoning to guide the retrieval. Fig. 1 shows an overview of our retrieval method, which we call IRCoT. 2 We begin by retrieving a base set of paragraphs using the question as a query. Subsequently, we alternate between the following two steps: (i) extend CoT: use the question, the paragraphs collected thus far, and the CoT sentences generated thus far to generate the next CoT sentence; (ii) expand retrieved information: use the last CoT sentence as a query to retrieve additional paragraphs to add to the collected set. We repeat these steps till the CoT reports an answer or we reach the maximum allowed number of reasoning steps. Upon termination, all collected paragraphs are returned as the retrieval outcome. Finally, we use these as the context for answering the question via direct QA prompting (Brown et al., 2020) or CoT prompting (Wei et al., 2022).\n\nWe evaluate the efficacy of our system on 4 multi-step reasoning datasets under an open-domain setting: HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and IIRC (Ferguson et al., 2020). Our experiments using OpenAI GPT3 (code-davinci-002) (Brown et al., 2020;Ouyang et al., 2022;Chen et al., 2021) demon-2 Interleaved Retrieval guided by Chain-of-Thought. strate that retrieval using IRCoT is substantially more effective than the baseline, one-step, questionbased retrieval by 11-21 recall points under a fixedbudget optimal recall setup. 3 When IRCoT is used in conjunction with a prompting-based reader, it also leads to substantial improvement (up to 15 F1 points) in downstream few-shot QA performance and reduces factual errors in generated CoT by up to 50%. Our approach also works on much smaller Flan-T5 models (11B, 3B, and 0.7B) showing similar trends. In particular, we find QA using Flan-T5-XL (3B) with IRCoT even outperforms the 58X larger GPT3 with a one-step questionbased retrieval. Furthermore, these improvements also hold up in an out-of-distribution (OOD) setting where the demonstrations from one dataset are used when testing on another dataset. Lastly, we note that our QA scores exceed those reported by recent works on few-shot prompting for open-domain QA (ODQA) (Khot et al., 2023;Press et al., 2022;Yao et al., 2022), although a fair apples-to-apples comparison with them isn't possible (cf. Appendix C).\n\nIn summary, our main contribution is a novel retrieval method, IRCoT, that leverages LMs' chainof-thought generation capabilities to guide retrieval and uses retrieval in turn to improve CoT reasoning. We demonstrate that IRCoT:\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain QA datasets, in both IID and OOD settings; 2. reduces factual errors in generated CoTs; and 3. improves performance with both large-scale (175B models) as well as smaller-scale models (Flan-T5-*, \u226411B) without any training."}
{"tabids": ["f83bb26c-2ea8-4a4f-b121-f69e4ebe9619"], "corpus_id": 258615731, "title": "Active Retrieval Augmented Generation", "paper_id": "88884b8806262a4095036041e3567d450dba39f7", "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.", "intro": "Generative language models (LMs) (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023;Chowdhery et al., 2022;Zhang et al., 2022;Touvron et al., 2023;Zhao et al., 2023) have become a foundational component in natural language processing (NLP) systems with their remarkable abilities.Although LMs have memorized some world knowledge during training (Petroni et al., 2019;Roberts et al., 2020;Jiang et al., 2020), they still tend to hallucinate and create imaginary content (Maynez et al., 2020;Zhou et al., 2021).Augmenting LMs with retrieval components that look up relevant information from external knowledge resources is a promising direction to address hallucination (Khandelwal et al., 2020;Izacard et al., 2022).\n\nRetrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve documents based on the user's input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017;Guu et al., 2020;Lewis et al., 2020;Izacard and Grave, 2021;Sachan et al., 2021;Lee et al., 2021;Jiang et al., 2022;Izacard et al., 2022;Nakano et al., 2021;Qian et al., 2023;Lazaridou et al., 2022;Shi et al., 2023).These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for shortform knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019;Joshi et al., 2017), where the information needs are clear in the user's input, and it is sufficient to retrieve relevant knowledge once solely based on the input.\n\nIncreasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as longform QA (Fan et al., 2019;Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021;Hayashi et al., 2021;Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022;Ho et al., 2020;Geva et al., 2021;Hendrycks et al., 2020).In contrast to short-form generation, long-form generation presents complex information needs that are not always evident from the input alone.Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowledge throughout the generation process.For example, to generate a summary about a particular topic, the initial retrieval based on the topic name Joe Biden (born November 20, 1942) is the 46th president of the United States.\n\nHe graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.\n\nJoe Biden attended the University of Pennsylvania, where he earned a law degree."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 208617407, "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks", "paper_id": "f4cf4246f3882aa6337e9c05d5675a3b8463a32e", "abstract": "We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like \u201cRinse off a mug and place it in the coffee maker.\u201d and low-level language instructions like \u201cWalk to the coffee maker on the right.\u201d ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision- and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.", "intro": "A robot operating in a human spaces needs to connect natural language to the world. This symbol grounding [21] problem has largely focused on connecting language to static images. However, robots need to understand taskoriented language, for example \"Rinse off a mug and place it in the coffee maker\" illustrated in Figure 1.\n\nPlatforms for translating language to action have become increasingly popular, spawning new test-beds [12,3,14,41]. These benchmarks include language-driven navigation and embodied question answering, which have seen dramatic improvements in modeling thanks to environments like Matterport 3D [11,3], AI2-THOR [25], and AI Habi- 1 Paul G. Allen School of Computer Sci & Eng, Univ of Washington 2 Carnegie Mellon University LTI & Microsoft Research AI 3 Allen Institute for AI 4 NVIDIA \"walk to the coffee maker on the right\" \"wash the mug in the sink\" \"put the clean mug in the coffee maker\"\n\n\"pick up the mug and go back to the coffee maker\"\n\n\"pick up the dirty mug from the coffee maker\" \"turn and walk to the sink\"  Figure 1: ALFRED consists of 25k+ language directives corresponding to expert demonstrations on household tasks. Above, we highlight several action sequence frames corresponding to portions of the accompanying language instruction. Unlike related datasets that focus only on navigation, ALFRED requires interactions with objects, keeping track of state changes, and callbacks to previous instructions.\n\ntat [43]. However, these datasets ignore complexities arising from describing task-oriented behaviors with objects.\n\nWe introduce ALFRED, a new benchmark for connecting human language to actions, behaviors, and objects in an interactive visual environment. Expert task demonstrations are accompanied by both high-and low-level human language instructions in 120 indoor scenes in the new AI2-THOR 2.0 [25]. These demonstrations involve partial observability, long action horizons, underspecified natural language, and irreversible actions.\n\nALFRED includes 25,743 English language directives describing 8,055 expert demonstrations averaging 50 steps each, resulting in 428,322 image-action pairs. Motivated by work in robotics on segmentation-based grasping [36], agents in ALFRED interact with objects visually, specifying a pixelwise interaction mask of the target object. This TACoS [42] 17k+ High&Low Photos ---R2R [3]; Touchdown [14] 21k+; 9.3k+ Low Photos Ego Graph EQA [15] High Low Ego Discrete Matterport EQA [53] High Photos Ego Discrete IQA [20] High High Ego Discrete Discrete VirtualHome [41] 2.7k+ High&Low High 3 rd Person Discrete VSP [56] High High Ego Discrete ALFRED 25k+ High&Low High Ego Discrete Discrete + Mask Table 1: Dataset comparison. ALFRED is the first interactive visual dataset to include high-and low-level natural language instructions for object and environment interactions. TACoS [42] provides detailed high-and low-level text descriptions of cooking videos, but does not facilitate task execution. For navigation, ALFRED enables discretized, grid-based movement, while other datasets use topological graph navigation or avoid navigation altogether. ALFRED requires an agent to generate spatially located interaction masks for action commands. By contrast, other datasets only require choosing from a discrete set of available interactions and object classes or offer no interactive capability.\n\ninference is more realistic than simple object class prediction, where localization is treated as a solved problem.\n\nExisting beam-search [17,51,46] and backtracking solutions [23,28] are infeasible due to the larger action and state space, long horizon, and inability to undo certain actions.\n\nTo establish baseline performance levels, we evaluate a sequence-to-sequence model shown to be successful on vision-and-language navigation tasks [27]. This model is not effective on the complex tasks in ALFRED, achieving less than 5% success rates. For analysis, we also evaluate individual sub-goals like the routine of cooking something in a microwave. While performance is better for isolated sub-goals, the model lacks the reasoning capacity for longhorizon and compositional task planning.\n\nIn summary, ALFRED facilitates learning models that translate from language to sequences of actions and predictions of visual interaction masks for object interactions. This benchmark captures many reasoning challenges present in real-world settings for translating human language to robot actions for accomplishing household tasks. Models that can overcome these challenges will begin to close the gap towards real-world, language-driven robotics. Table 1 summarizes the benefits of ALFRED relative to other visual action datasets with language annotations. Vision & Language Navigation. In vision-and-language navigation tasks, either natural or templated language describes a route to a goal location through egocentric visual observations [30,13,12,3,14]. Since the proposal of R2R [3], researchers have dramatically improved the navigation performance of models [52,17,51,23,28] with techniques like progress monitoring [27], as well as introduced task variants with additional, on-route instructions [38,37,49]. Much of this research is limited to static environments. By contrast, ALFRED tasks include navigation, object interactions, and state changes. Vision & Language Task Completion. There are several existing benchmarks based on simple block worlds and fully observable scenes [9,33]. ALFRED provides more difficult tasks in richer, visually complex scenes, and uses partially observable environments. The CHAI benchmark [32] evaluates agents performing household instructions, but includes only a single interact action outside navigation. ALFRED has seven manipulation actions, such as pick up, turn on, and open, state changes like clean versus dirty, and variation in language and visual complexity."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 212644876, "title": "MQA: Answering the Question via Robotic Manipulation", "paper_id": "88e8f7636a2a8d5d3d0a99708bea332770471eb3", "abstract": "In this paper, we propose a novel task, Manipulation Question Answering (MQA), where the robot performs manipulation actions to change the environment in order to answer a given question. To solve this problem, a framework consisting of a QA module and a manipulation module is proposed. For the QA module, we adopt the method for the Visual Question Answering (VQA) task. For the manipulation module, a Deep Q Network (DQN) model is designed to generate manipulation actions for the robot to interact with the environment. We consider the situation where the robot continuously manipulating objects inside a bin until the answer to the question is found. Besides, a novel dataset that contains a variety of object models, scenarios and corresponding question-answer pairs is established in a simulation environment. Extensive experiments have been conducted to validate the effectiveness of the proposed framework.", "intro": "People have long anticipated the day when humans can ask questions to an intelligent robot directly with natural language and the robot knows to interact with the environment to respond. Imagine there is a bin in your kitchen which contains a variety of items, and you would like to know how many cans are left in it so that you can decide whether some replenishment should be done. Then you call your assistant robot in the kitchen and ask \"How many cans are there in the bin?\" Having the question well understood, the robot starts to explore the bin, where all kinds of objects may be mixed together. As some cans may be occluded by some other objects and can not be seen directly, the robot has to generate a sequence of manipulation actions to change the current scenario in order to explore the bin thoroughly. As shown in Fig.1, the robot keeps exploring the bin until all possible areas are explored and then it is able to report the answer to the user.\n\nRecently, the task of Question Answering (QA) has attracted increasing attention from many researchers worldwide. In the big family of QA research, the popular QA chatbot tries to communicate with humans by scraping the Internet or database to get the answer to the question [1]. One of the representative tasks among it is the Visual Question Answering (VQA) task [2][3] [16] [17], where the robot is required to have the ability to reason about the visual content in order to answer a question \u2020 indicates the authors with equal contributions. The authors are also with Beijing National Research Center for Information Science and Technology. Y. Deng is also with the Center of Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Tsinghua University. This work was jointly supported by the National Natural Science Fund for Distinguished Young Scholars (62025304), and in part by the Seed Fund of Tsinghua University (Department of Computer Science and Technology)-Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things. * Corresponding author: hpliu@tsinghua.edu.cn Fig. 1. Given a question, the manipulator explores the bin with a series of manipulation actions to find the answer. In this example, two cans are directly visible in the initial scene, while there may also be some cans occluded by other objects. Therefore, the manipulator tries to push a food box to the side and another can is revealed. After exploring all possible areas, three cans are found at last. And the robot report the answer \"Three\" to the user. about the given visual input. As a step forward to realize the natural human-robot interaction, a much more challenging task, Visual Dialog, is proposed, where the robot needs to answer a coherent series of questions to the visual content [9] [22]. However, they only try to answer the question passively from the visual input and the robot's ability to move in the environment is ignored.\n\nIn the real-world environment, the perception should never be passive but an active process [4] [7]. Considering the embodiment of intelligent agents, next emerges a body of work on Embodied Question Answering (EQA), where the mobile robot is able to actively explore the environment to find the answer to the question [10] [11]. In the EQA task, the robot needs to understand the acquired visual information and perform a series of actions accordingly to actively explore the environment to answer the question. A most important characteristic of the EQA task is that the perception and action ability of the agent are combined together. Additionally, under the large scope of EQA, Gordon et al. propose an Interactive Question Answering (IQA) task [14], which points out that besides merely navigating the environment, the robot should also be able to execute some interactive actions based on the object's affordance, such as opening the door of the refrigerator to better find the answer to the question. But they are limited EQA [10] IQA [14] MQA (Ours)  Understanding  Exploration  -Interaction  --Manipulation  ---to some simple standard actions, and lack of manipulation. In the real world environment, it is far more complex and highly unstructured. For example, in the cluttered scene, a target object may be occluded by other objects, which results in an even higher requirement on the robotic manipulation ability. To tackle this problem, we propose a novel task of Manipulation Question Answering (MQA), where the robot is required to find the answer by performing manipulation actions to actively explore the environment, rather than simply doing some predefined actions for the interaction. A comprehensive comparison of VQA, EQA, IQA, and the proposed MQA tasks is illustrated in TABLE I. It can be seen that the VQA task only requires the agent to have the ability to understand the environment. Comparing to VQA task, the EQA task makes a big improvement by further leveraging the embodiment ability of the agent. The agent needs to explore the environment to find the answer. IQA task is an extension of the EQA task which also allows the interaction with the environment. And the proposed MQA contains all the characteristics of aforementioned tasks. Additionally, in the MQA task, the agent can perform manipulation actions to change the environment in order to answer the given question. Meanwhile, the MQA task we proposed poses several new challenges. First, the robot is expected to perform manipulation actions to change the environment in order to find the answer, instead of merely referencing the static environment. And then, a new set of metrics is required to evaluate this new task as currently available research lacks quantitative accuracy metrics and benchmarks for the proposed task. Besides, there is no existing dataset suitable for our MQA task.\n\nIn response to these challenges, we proposed a framework that integrates a QA module and a manipulation module to accomplish the newly defined MQA task. The contributions of this paper can be summarized as follows:\n\n\u2022 We formulate a novel Manipulation Question Answering (MQA) task and a solution framework is built to solve it. \u2022 We design a Deep Q Network (DQN) for the robot to effectively generate manipulations for the MQA task. \u2022 We build a novel MQA dataset including a variety of object models, bin scenarios and question-answer pairs. A corresponding benchmark is also established. The organization of this paper is as follows. The related work is introduced in Section II. We describe the MQA task in Section III. Section IV includes the establishment of the MQA dataset and its analysis. The proposed MQA framework is presented in Section V. Experimental results and analysis are demonstrated in Section VI. Finally, we come to the conclusion of the paper."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 244908821, "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks", "paper_id": "4be02694125b71876552900a53c85c47a2a83614", "abstract": "General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments and objects. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.", "intro": "A LONG-STANDING goal for robotics and embodied agents is to build systems that can perform tasks specified in natural language. Concepts expressed in natural language provide humans with an intuitive way to represent, summarize, and abstract diverse knowledge skills. By means of abstraction, concepts such as \"open the drawer and push the middle object into the drawer\" can be extended to a potentially infinite set of new and unseen entities. Additionally, humans leverage concepts to describe complex tasks as sequences of natural language instructions. This stands in contrast to current robots, which typically lack this generalization ability and learn individual tasks one at a time. Moreover, multi-task learning approaches traditionally assume that tasks are specified to the agent at test time via mechanisms such as goal images [1] and one-hot skill selectors [2], [3] that are not practical for non-expert users to instruct robots in everyday real-world settings. As robots become ubiquitous across human-centered environments the need for intuitive task specification grows: how can we scale robot learning systems to autonomously acquire general-purpose knowledge that allows them to compose long-horizon tasks by following unconstrained language instructions?\n\nTo address this problem we present CALVIN, a new opensource simulated benchmark that links human language to robot motor skills, behaviors, and objects in interactive visual environments. In this setting, a single agent must solve complex manipulation tasks by understanding a series of unconstrained language expressions in a row, e.g., \"open the drawer . . . pick up the blue block . . . push the block into the drawer . . . open the sliding door\". Furthermore, to evaluate the agents' ability for long-horizon planning, agents in this scenario are expected to be able to perform any combination of subtasks in any order. CALVIN has been developed from the ground up to support training, prototyping, and validation of language-conditioned continuous control policies over a range of four indoor manipulation environments, visualized in Figure  1. CALVIN includes \u223c24 hours teleoperated unstructured play data together with 20K language directives. Unscripted playful interactions have the advantage of being task-agnostic, diverse, and relatively cheap to obtain [1], [4]. The simulation platform supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing [5]. We believe that this flexible sensor suite will allow researchers to develop improved multimodal agents that can solve many tasks in real-world settings. This is the first public benchmark of instruction following, to our knowledge, that combines: natural language conditioning, multimodal highdimensional inputs, 7-DOF continuous control, and longhorizon robotic object manipulation. We provide an evaluation protocol with evaluation modes of varying difficulty by choosing different combinations of sensor suites and amounts of training environments. This effort joins the recent efforts to standardize robotics research for better benchmarks and more reproducible results. To open the door for future development of agents that can generalize abstract concepts to unseen entities the same way humans do, we include a challenging zeroshot evaluation by training on large play corpora covering three environments and testing on an unseen scene. The language instructions used for testing are not included in the training set and represent novel ways of describing the manipulation tasks seen during training.\n\nTo establish baseline performance levels, we evaluate the multi-context imitation learning (MCIL) approach that uses relabeled imitation learning to distill many reusable behaviors into a goal-directed policy [6]. This model is not effective on the complex long horizon robot manipulation tasks in CALVIN. While it achieves up to 53.9% success rate in short horizon tasks, it performs poorly in the long-horizon setting. We note that there is no constraint to use imitation learning approaches to solve CALVIN tasks, as approaches that use reinforcement learning to learn language-conditioned policies can also be applied [7].\n\nIn summary, CALVIN facilitates learning models that translate from language to sequences of motor skills in a realistic simulation environment. This benchmark captures many challenges present in real-world settings for relating human language to robot actions and perception for accomplishing long-horizon manipulation tasks. Models that can overcome these challenges will begin to close the gap towards scalable, general-purpose, language-driven robotics."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 227232796, "title": "Multi-Agent Embodied Question Answering in Interactive Environments", "paper_id": "5ed523871586b0f883e4ec890155075b96291a13", "abstract": "We investigate a new AI task \u2014 Multi-Agent Interactive Question Answering \u2014 where several agents explore the scene jointly in interactive environments to answer a question. To cooperate efficiently and answer accurately, agents must be well-organized to have balanced work division and share knowledge about the objects involved. We address this new problem in two stages: Multi-Agent 3D Reconstruction in Interactive Environments and Question Answering. Our proposed framework features multi-layer structural and semantic memories shared by all agents, as well as a question answering model built upon a 3D-CNN network to encode the scene memories. During the reconstruction, agents simultaneously explore and scan the scene with a clear division of work, organized by next viewpoints planning. We evaluate our framework on the IQuADv1 dataset and outperform the IQA baseline in a single-agent scenario. In multi-agent scenarios, our framework shows favorable speedups while remaining high accuracy.", "intro": "For decades, one of our best wishes has been to develop robots that can assist humans with the ability to understand the scene, to interact with environments, and to communicate with humans. For instance, a domestic robot might be asked: How many apples are in the house? To answer it, the agent must explore the house, open fridges & cabinets for possibly hidden apples, check the occurrence of apples, and answer the question by natural language.\n\nThis sort of problem refers to Embodied Question Answering (EQA) [4] : Being asked What color is the car?, an agent navigates to the car and observes it before it answers the question. Since the car may be out of sight initially, the agent must have common sense about possible locations of the car and a way to get there. However, point-to-point navigation is not enough -what if we want the agent to search for a missing fork which may be anywhere in the kitchen?\n\nTo be more practical, Interactive Question Answering (IQA) [7] takes both interactive actions (e.g., open a cabinet) and more generic questions (e.g., existence and counting) into consideration. To answer Is there a fork in the kitchen?, the agent must have comprehensive cognition to the kitchen, without missing any place where the target may exist, including interactive objects like containers. However, this process could be time-costing.\n\nParallelism has always been a fundamental but effective idea. Since several agents can search for an object simultaneously, the question will soon be answered if multiple robots can explore collaboratively. Therefore, we introduce Multi-Agent Interactive Question Answering, which presents additional challenges to AI systems. First, the multi-agent system must be well-organized to avoid duplicate work and unbalanced work. Second, the multi-agent QA system must integrate information from all agents and answer the question accurately without a repeat or a miss. Third, the multi-agent system should achieve as high speedup as possible while keeping the high accuracy.\n\nVery few studies have looked into multi-agent embodied question answering tasks. However, active 3D reconstruction [5] [24] is not a novel problem. Here we propose a two-stage framework for Multi-Agent IQA, which firstly executes a multi-agent (embodied) 3D reconstruction to construct 3D global structural and semantic memories and secondly encodes the scene via 3D memories to answer the question. To support interactive objects, we propose a multi-layer data structure as an extension to traditional voxel-based reconstructions.\n\nWe train and evaluate our proposed two-stage framework on the IQuADv1 IQA dataset [7] in both single-agent and multi-agent scenarios and observe promising results of highly effective and efficient in both cases.\n\nContributions. In summary, our main contributions include:\n\n-Problem. We introduce the Multi-Agent IQA, the task of organizing collaborative Interactive Question Answering for several agents. -Method. We propose a two-stage framework for Multi-Agent IQA, a method to efficiently construct 3D global memories via multi-agent 3D reconstruction and to answer the question by encoding the scene memories with 3D-CNN. -Results. Our 3D-memory-based framework surpasses the original IQA method in both answering accuracy and episode length, with a single agent on the IQuADv1 dataset. With 2, 3, and 4 agents, we show consistent high-level parallelism and affordable speedups in average episode length."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 225076003, "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation", "paper_id": "3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2", "abstract": "Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io", "intro": "End-to-end models that map directly from pixels to actions hold the capacity to learn complex manipulation skills, but are known to require copious amounts of data [1,2]. Integrating object-centric assumptions -e.g., object keypoints [3,4,5,6], embeddings [7,8], or dense descriptors [9,10,11] -has been shown to improve sample efficiency [10]. However, these representations often impose data collection burdens (i.e., configuring scenes with specific singulated objects) and still struggle to address challenging scenarios with unseen classes of objects, occluded objects, highly deformable objects, or piles of small objects [12]. This naturally leads us to ask: is there structure that we can incorporate into our end-to-end models to improve their learning efficiency, without imposing any of the limitations or burdens of explicit object representations?\n\nIn this work, we propose the Transporter Network, a simple end-to-end model architecture that preserves spatial structure for vision-based manipulation, without object-centric assumptions:\n\n\u2022 Manipulation involves rearranging things, which can be thought of as executing a sequence of spatial displacements: where the space being moved (i.e., transported) can encompass an object(s), part of an object, or end effector. We formulate vision for manipulation as estimating these displacements. Transporter Networks directly optimize for this by learning to 1) attend to a local region, and 2) predict its target spatial displacement via deep feature template matching -which then parameterizes robot actions for manipulation. This formulation enables high-level perceptual reasoning about which visual cues are important, and how they should be rearranged in a scene -the distributions of which can be learned from demonstrations. \u2022 Transporter Networks preserve the 3D spatial structure of the visual input. Prior end-to-end models [1,2] often use convolutional architectures with raw images, in which valuable spatial information can be lost to perspective distortions. Our method uses 3D reconstruction to project visual data onto a spatiallyconsistent representation as input, with which it is able to better exploit equivariance [13,14] for inductive biases that are present within the geometric symmetries [15] of the data for more efficient learning.\n\nIn our experiments, Transporter Networks exhibit superior sample efficiency on a number of tabletop manipulation tasks that involve changing the state of the robot's environment in a purposeful manner: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Transporter Networks excel in modeling 4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA. A Transporter Network is a simple model architecture that attends to a local region and predicts its spatial displacement (b) from visual input -which can parameterize robot actions. It is sample efficient in learning complex vision-based manipulation tasks: inserting blocks into fixtures (a), sequential pick-and-place in Towers of Hanoi (c), assembling kits with unseen objects (d), palletizing boxes (e), stacking a pyramid of blocks (f), manipulating rope (g), and pushing piles of small objects with closed-loop feedback (h) -and is practical to deploy on real production robots (k, m).\n\nmulti-modal spatial action distributions, and by construction generalize across rotations and translations of objects. They do not require any prior knowledge of the objects to be manipulated -they rely only on information contained within partial RGB-D data from demonstrations, and are capable of generalizing to new objects and configurations, and for some tasks, one-shot learning from a single demonstration.\n\nOur main contribution is a new perspective on the role of spatial structure and its capacity to improve end-toend learning for vision-based manipulation. We propose a simple model architecture that learns to attend to a local region and predict its spatial displacement, while retaining the spatial structure of the visual input. On 10 unique tabletop manipulation tasks, Transporter Networks trained from scratch are capable of achieving greater than 90% success on most tasks with objects in new configurations using 100 expert demonstrations, while other end-to-end alternatives struggle to generalize with the same amount of data. We also develop an extension to 6DoF tasks by combining 3DoF Transporter Networks with continuous regression to handle the remaining degrees of freedom. To facilitate further research in vision-based manipulation, we plan release code and open-source Ravens, our new simulated benchmark with all tasks. Ravens features a Gym-like API [16] with a built-in stochastic oracle to evaluate the sample efficiency of imitation learning methods."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 249848175, "title": "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation", "paper_id": "c6cb024e25ba5136aa914ee007453dd0af6891cc", "abstract": "Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents -- object manipulation by following human guidance, e.g.,\"move the red mug next to the box while keeping it upright.\"To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.", "intro": "\"Can you help me to clean the disks in the sink?\" -humans communicate with each other using language to issue tasks and specify the requirements. Although recent progress in embodied AI pushes intelligent robotic systems to reality closer than any other time before, it is still an open question how the agent learn to manipulate objects following language instructions. Therefore, we introduce the task of Vision-and-Language Manipulation (VLM), where the agent is required to follow language instructions to do robotic manipulation. There are recent benchmarks developed to evaluate robotic manipulation tasks with language guidance and visual input [10,1,33]. However, the collected task demonstrations are not modular and can hardly scale because they lack (1) adaptation to novel objects (2) categorization for modular and flexible composition to complex tasks. Additionally, the lack of variations in language also lead to biases for visual reasoning learning. To deal with these problems, we expect an inclusive, modular, and scalable benchmark to evaluate embodied agents for various language-guided manipulation tasks.\n\nAn ideal VLM benchmark should have at least three characteristics: The first one is scalability. Such a benchmark should automatically generate various physics-realistic 6 degrees of freedom (DoF) interactions with affordable objects and expand new tasks effortlessly. The second one is task categorization, which exploits commonality concerning robot motion between different semantic tasks and is almost ignored in existing works. The third one is reasonable language generation, Figure 1: Given the language instructions and observations, the VLMbench requires the agent to generate an executable manipulation trajectory for specific task goals. On the left, we show that the complex tasks can be divided into the unit tasks according to the constraints of the end-effector, like \"Open the door of the dishwasher\" and \"Open the door of the fridge\" should both follow the rotation constraints of the revolute joint. On the right, we show examples of object-centric representations, where all graspable objects or parts will generate local grasping poses as their attributes. Depending on the modular design, we can generate reasonable VLM data automatically.\n\nwhich requires the benchmark can generate language instructions for testing diverse visual reasoning abilities without biases. However, existing benchmarks [30,10,33,1] lack at least one characteristic for VLM tasks. Motivated by these attributes, we present VLMbench, a highly categorical robotic manipulation benchmark with compositional language for visual reasoning. To build and scale VLMbench, we propose AMSolver, an automatic unit task builder that can compose unit tasks to create complex multi-step tasks and seamlessly adapt to novel objects. Compared to previous benchmarks, VLMbench categorizes manipulation tasks into various meta manipulation actions according to the constraints of robot trajectories for the first time. Meanwhile, the combinations of compositional language templates and object-centric representations provide numerous variations for visual reasoning in VLMbench, as shown in Figure 1.\n\nTo investigate the difficulty of the benchmark, we test them with several partially modal methods and a keypoint-based method, 6D-CLIPort, modified from the state-of-the-art language-guided manipulation method CLIPort [25]. The results show that there is still a massive room for improvement in the robust manipulation action generations and accurate language-guided visual understanding. To sum up, our contributions in this work include:\n\n\u2022 AMSolver, an automatic demonstration generator for various task semantics, motion constraints, object types and states defined in a novel task template formulation.\n\n\u2022 VLMbench, a robot manipulation benchmark on 3D tasks with visual observation and compositional language instructions, where we categorize the manipulation tasks by constraints and provide variations with minimal biases in the first time.\n\n\u2022 6D-CLIPort, a general vision-and-language manipulation baseline model evaluated on all kinds of VLMbench tasks."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 251903958, "title": "CH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous Multi-Agent Reinforcement Learning", "paper_id": "47d4151ba4ff2f34aa439af18bec9a30a4fc54e0", "abstract": "We propose a multimodal (vision-and-language) benchmark for cooperative and heterogeneous multi-agent learning. We introduce a benchmark multimodal dataset with tasks involving collaboration between multiple simulated heterogeneous robots in a rich multi-room home environment. We provide an integrated learning framework, multimodal implementations of state-of-the-art multi-agent reinforcement learning techniques, and a consistent evaluation protocol. Our experiments investigate the impact of different modalities on multi-agent learning performance. We also introduce a simple message passing method between agents. The results suggest that multimodality introduces unique challenges for cooperative multi-agent learning and there is significant room for advancing multi-agent reinforcement learning methods in such settings.", "intro": "We posit that progress in multi-agent learning and its application to multi-robot problems could be sped up with the introduction of standard, sophisticated environments for training and evaluation. Prior work on cooperative multi-agent learning has focused on simplified environments [16]. Visually rich environments that support multi-agent, cooperative tasks have not been explored until very recently [24,8,9,18,23]. We propose the first multimodal benchmark on Cooperative Heterogeneous Multi-Agent Reinforcement Learning (CH-MARL) wherein two simulated robots must collaboratively find an object and place it at a target location.\n\nCH-MARL is built using visually rich scenes from VirtualHome [18], and includes language. We implement a language generator that procedurally provides feedback to guide embodied agents to achieve tasks. In addition to providing a novel large-scale vision and language dataset for collaborative task completion in simulated household environments, we conduct a comprehensive evaluation of several state of the art MARL algorithms under various setting for our collaborative robot benchmark task. We investigate and analyze the impact of various aspects of the collaborative MARL algorithms, including heterogeneity and multi-modality. We also propose and implement a message passing interface between agents to enable effective information sharing, especially in decentralized model setups where they would otherwise not have the ability to collaborate with each other. The results reveal interesting insights: 2. Vision and language grounding helps the learning process 3. Even simple multi-agent communication protocols substantially improve task performance by allowing effective collaboration.\n\nTo our knowledge, this is the first dataset to support multiple heterogeneous agents in a virtual environment collaboratively completing a specified task. A comparative study of state of the art embodied AI datasets is in Table 1. We expect this work to contribute towards a standard multi-modal testbed for MARL and foster research in this area. \u00d7 \u00d7 \u00d7 House3D [25] \u00d7 \u00d7 \u00d7 iGibson [13] \u00d7 \u00d7 Watch and Help [19] \u00d7 \u00d7 CH-MARL (Ours)"}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 109933186, "title": "Two Body Problem: Collaborative Visual Task Completion", "paper_id": "7cec763510b951fcf8bb93b508e7a49ec5162875", "abstract": "Collaboration is a necessary skill to perform tasks that are beyond one agent's capabilities. Addressed extensively in both conventional and modern AI, multi-agent collaboration has often been studied in the context of simple grid worlds. We argue that there are inherently visual aspects to collaboration which should be studied in visually rich environments. A key element in collaboration is communication that can be either explicit, through messages, or implicit, through perception of the other agents and the visual world. Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks. Refer to our project page for more details: https://prior.allenai.org/projects/two-body-problem", "intro": "Developing collaborative skills is known to be more cognitively demanding than learning to perform tasks independently. In AI, multi-agent collaboration has been studied in more conventional [32,43,9,58] and modern settings [53,28,79,35,56,61]. These studies have mainly been performed on grid-worlds and have factored out the role of perception in collaboration.\n\nIn this paper we argue that there are aspects of collaboration that are inherently visual. Studying collaboration in simplistic environments does not permit to observe the interplay between perception and communication, which is necessary for effective collaboration. Imagine moving a piece of furniture with a friend. Part of the collaboration is rooted in explicit communication through exchanging messages, and some part of it is done through implicit communication through interpreting perceivable cues about the other agents behavior. If you see your friend going around the furniture to grab it, you would naturally stay on the opposite side to avoid toppling it over. Additionally, communication and collaboration should be considered jointly with the task itself. The way you communicate, either explicitly or implicitly, in a soccer game is very different from when you move furniture. This suggests that factoring out per-ception and studying collaboration in isolation (grid-world) might not result in an ideal outcome.\n\nIn short, learning to perform tasks collaboratively in a visual environment entails joint learning of (1) how to perform tasks in that environment, (2) when and what to communicate, and (3) how to act based on implicit and explicit communication. In this work, we develop one of the first frameworks that enables the study of explicitly and implicitly communicating agents collaborating together in a photo-realistic environment.\n\nTo this end we consider the problem of finding and lifting bulky items, ones which cannot be lifted by a single agent. While conceptually simple, attaining proficiency in this task requires multiple stages of communication. The agents must search for the object of interest in the environment (possibly communicating their findings to each other), position themselves appropriately (for instance, opposing each other), and then lift the object simultaneously. If the agents position themselves incorrectly, lifting the object will cause it to topple over. Similarly, if the agents pick up the object at different time steps, they will not succeed.\n\nTo study this task, we use the AI2-THOR virtual environment [48], a photo-realistic, physics-enabled environment of indoor scenes used in past work to study single agent behavior. We extend AI2-THOR to enable multiple agents to communicate and interact.\n\nWe explore collaboration along several modes: (1) The benefits of communication for spatially constrained tasks (e.g., requiring agents to stand across one another while lifting an object) vs. unconstrained tasks. (2) The ability of agents to implicitly and explicitly communicate to solve these tasks. (3) The effect of the expressivity of the communication channel on the success of these tasks. (4) The efficacy of these developed communication protocols on known environments and their generalizability to new ones. (5) The challenges of egocentric visual environments vs. grid-world settings.\n\nWe propose a Two Body Network, or TBONE, for modeling the policies of agents in our environments. TBONE operates on a visual egocentric observation of the 3D world, a history of past observations and actions of the agent, as well as messages received from other agents in the scene. At each time step, agents go through two rounds of communication, akin to sending a message each and then replying to messages that are received in the first round. TBONE is trained with a warm start using a variant of DAgger [70], followed by a minimization of a sum of an A3C loss and a cross entropy loss between the agents actions and the actions of an expert policy.\n\nWe perform a detailed experimental analysis of the impact of communication using metrics including accuracy, number of failed pickup actions, and episode lengths. Following our above research questions, our findings show that: (1) Communication clearly benefits both constrained \nt \u2212 1 [ ] , a \u02c6 ( 1 ) t a \u02c6 ( 2 ) t Environment o ( 1 ) t o ( 2 ) t Comm. channel o ( 1 ) t + 1 o ( 2 ) t + 1\ns a m p le s a m p le Figure 2: A schematic depicting the inputs to the policy network. An agent's policy operates on a partial observation of the scene's state and a history of previous observations, actions, and messages received.\n\nand unconstrained tasks but is more advantageous for constrained tasks.\n\n(2) Both explicit and implicit communication are exploited by our agents and both are beneficial, individually and jointly. (3) For our tasks, large vocabulary sizes are beneficial. (4) Our agents generalize well to unseen environments. (5) Abstracting our environments towards a grid-world setting improves accuracy, confirming our notion that photo-realistic visual environments are more challenging than grid-world like settings. This is consistent with findings by past works for single agent scenarios. Finally we interpret the explicit mode of communication between agents by fitting logistic regression models to the messages to predict the values such as oracle distance to target, next action, etc., and find strong evidence matching our intuitions about the usage of messages between agents."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 248941435, "title": "Embodied Multi-Agent Task Planning from Ambiguous Instruction", "paper_id": "a2b2ff265ebe112626892703a233203348b3936e", "abstract": "\u2014In human-robots collaboration scenarios, a human would give robots an instruction that is intuitive for the human himself to accomplish. However, the instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction. This problem exhibits significant challenges in both language understanding and dynamic task planning with the perception information. In this work, an embodied multi-agent task planning framework is proposed to utilize external knowledge sources and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocate the decomposed tasks to multiple agents. Furthermore, we utilize the semantic information to perform environment perception and generate sub-goals to achieve the navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notori- ous sim2real problem. Finally, we build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit in instructions. We perform the evaluation experiments on the simulation platform and in physical scenarios, demonstrating that the proposed model can achieve promising results for multi-agent collaborative tasks.", "intro": "In real life, a group leader may release an ambiguous instruction, which contains his intention but lacks the implementation details. Nevertheless, intelligent group members may analyze the instruction to extract the intention and utilize their knowledge or shared-mental-mind with the leader to execute the necessary operational details to accomplish the task. Such a collaboration mechanism is also highly expected for humanrobot collaboration. For example, a human would give robots an instruction in which the process of completing the task is obvious to the human himself. However, the overall instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction, such as \"Put the book and newspaper away\". Although human knows that the book and newspaper are most likely to be put on the bookshelf, or the drawer if there is no bookshelf found, robots may not know where to put the book and newspaper directly from the instruction, let alone collaborating to complete this task. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction (Fig. 1). Fig. 1. An overview of the embodied multi-agent task planning from ambiguous instruction. Given a high-level instruction, several sub-tasks are generated and allocated to a group of agents. The agents explore the environment and implement the sub-tasks. With the change of the visual observation during the exploration process, the task decomposition and task allocation processes are also adjusted dynamically.\n\nIn the multi-agent task planning scenario, a complex task can be decomposed in multiple possible ways, and the decomposed sub-tasks are allocated to multiple agents for the execution [24]. Therefore, the task planning includes task decomposition, which focuses on the problem of what to do [21], task allocation which focuses on the problem of who does what [3], and task scheduling which focuses on the problem of how to arrange tasks in time [43]. Among them, the task decomposition is the problem of decomposing a complex task into simpler ones, down to the level of actionable tasks [21,28]; task allocation is the problem of determining which robot should execute which task in order to achieve the overall system goal [3], and task scheduling is the problem of sequencing tasks for execution [17]. The above problems have been extensively investigated in diversified works of literature, most of which transform the given task into a well-defined optimization problem that requires a clear, structured, and complete instruction [2,16]. In practical scenarios, task planning is highly coupled with the human-robot interaction and the perception of the environment. It should be dynamically adjusted due to the vagueness of the interaction and dynamics of the environment. In this work, we formulate such a problem as embodied multi-agent task planning from ambiguous instruction, which exhibits the following key challenges: 1) Ambiguous Instruction Due to the incompleteness and ambiguity of the given instruction, it is necessary to use external knowledge sources (such as domain knowledge in a specific field, knowledge graph, and industry rules) to reason and clarify the instruction. Based on the clarified instruction, combined with the characteristics of the robot, the given task is required to be decomposed into specific sub-tasks that the robot can perform. For example, the task \"Put the book and the newspaper away\" should be clarified to be \"Put the book and the newspaper on the bookshelf\" with the visual perceptions of robots and the knowledge that books and newspapers are always placed on the bookshelf. Then it should be initially decomposed into the sub-tasks of \"Find the book\", \"Find the newspaper\" and \"Find the bookshelf\".\n\n2) Dynamic Task Decomposition Since the initial visual perceptions of multiple robots are limited, the reasoning information based on the initial state may not be correct. Therefore, it is necessary to dynamically adjust the instruction reasoning and task decomposition with updated visual perceptions during the continuous execution process of agents. For example, the task \"Put the book and the newspaper away\" is clarified to be \"Put the book and the newspaper on the bookshelf\" with the initial visual perceptions. After several steps of exploration, agents find that there is no bookshelf but a drawer in the current scene based on their newly obtained visual perceptions. The book and newspaper can also be put in the drawer. Then agents need to re-reason the implicit information in the given instruction and obtain the new clarified instruction \"Put the book and the newspaper in the drawer\". Afterward, the subsequent decomposition and allocation processes are performed based on the clarified results.\n\n3) Dynamic Task Allocation Based on the specific decomposed sub-tasks, the sub-tasks need to be allocated to multiple robots considering the robots' perception and motion abilities so that each robot is assigned to a corresponding sub-task. More importantly, in the specific execution process, because of the ambiguity of instructions and the dynamic nature of the environment, robots are required to dynamically adjust their allocated sub-tasks according to the environment perception information. For example, the three decomposed tasks \"Find the book\", \"Find the newspaper\" and \"Find the bookshelf\" are allocated to Agent 1, Agent 2 and Agent 3 respectively based on their initial visual environment information. After several steps, if Agent 1 finds that it is actually closer to the bookshelf based on its obtained observations, they need to reallocate the sub-tasks and Agent 1 would change to perform \"Find the bookshelf\", and Agent 2 would change to \"Find the book\" accordingly.\n\nTo tackle the above issues, we propose an embodied multi-agent task planning framework demonstrated in Fig.1 which utilizes external knowledge sources, and dynamically perceives environment information to parse the high-level ambiguous instructions, dynamically allocates the decomposed sub-tasks and completes the distributed navigation tasks. In this framework, multiple agents are able to leverage the advantages of their embodiment attribute to dynamically and automatically adjust the instruction parsing results and efficiently complete the task. The main contributions are summarized as follows:\n\n1) Multi-agent embodied task planning framework: A multi-agent task planning framework is proposed to solve the multi-agent collaborative mission, which utilizes external knowledge sources, and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocates the decomposed tasks.\n\n2) Sim&Real learning method for embodied task planning: A dynamic task allocation model is developed based on multi-agent collaboration. We utilize the semantic information to perform the environment perception and generate sub-goals to achieve navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notorious sim2real problem.\n\n3) Evaluation and validation: We build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit. We perform the evaluation experiments both in the AI2-THOR [22] platform and physical scenarios including Easy and Hard settings, which demonstrate that the proposed model can achieve promising results for multi-agent collaborative tasks."}
{"tabids": ["d63d3952-e30a-4a05-8415-b13226eff27e"], "corpus_id": 260378734, "title": "LEMMA: Learning Language-Conditioned Multi-Robot Manipulation", "paper_id": "92f6346346a46f5813b6996b9c1f10d9602eb39a", "abstract": "Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.", "intro": "T HERE is growing interest in connecting human language to robot actions, particularly in single-agent systems [1], [2], [3], [4], [5]. However, there remains a research gap in enabling multi-robot systems to work together in response to language input.\n\nRecent vision and language tasks have primarily focused on navigation and object interactions [4], [6], [7]. However, the lack of physical manipulation in these works makes the settings oversimplified. Although some recent studies, such as [1], [5], address vision and language object manipulation in single-robot settings, the language instructions provided specify only short-term goals, neglecting long-term objectives. [8] attempt to address these limitations by exploring longhorizon planning with manipulation for individual robots. Nevertheless, there remains a need to investigate multi-robot systems capable of accomplishing a broader range of longhorizon tasks while following language instructions.\n\nLearning policies for multi-robot systems introduces distinct challenges, including diverse capabilities arising from physical Manuscript received: April, 18, 2023; Revised August, 1, 2023; Accepted August, 21, 2023. This paper was recommended for publication by Editor Aleksandra Faust upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by Amazon Alexa AI. Corresponding author: Xiaofeng Gao Project website: https://lemma-benchmark.github.io constraints such as the location and reach of different robots. Moreover, task planning heavily depends on the spatial and physical relations between the objects and robots, in addition to the geometries of the objects. To ensure suitable task assignments, an awareness of each robot's specific physical capabilities is needed.\n\nTo tackle the language-conditioned vision-based multi-robot object manipulation problem, we have developed LEMMA, a benchmark that contains 8 types of collaborative object manipulation tasks with varying degrees of complexity. Some tasks require the robot to use tools for object-object interactions. For each task, the object poses, appearances, and robot types are randomized, requiring object affordance estimation and robot capability understanding. To enable multi-task learning, each task is paired with an expert demonstration and several language instructions specifying the task at different granularities. As a result, LEMMA introduces a diverse range of challenges in multi-robot collaboration, including physics-based object manipulation, long-horizon task planning, scheduling and allocation, robot capability and object affordance estimation, tool use, and language grounding. Each aspect poses distinct challenges and is crucial for a multi-robot system that follows human instructions to complete tasks. To evaluate existing techniques on LEMMA, we further provide several baseline methods and compare their performance to each other. We assess task performance by utilizing the latest languageconditioned policy learning models. Our results indicate that current models for language-conditioned manipulation and task planning face significant challenges in LEMMA, especially when dealing with complex human instructions.\n\nWe make the following contributions: \u2022 We design eight novel collaborative object manipulation tasks involving robots with different physical configurations implemented in Nvidia Omniverse -Isaac Sim. \u2022 We provide an open-source dataset comprising 6,400 expert demonstrations and natural language instructions, including human and high-level instructions. \u2022 We implement a modular hierarchical planning approach as a baseline, which integrates language understanding, task planning, task allocation, and object manipulation."}
{"tabids": ["d3b56d4e-6535-4c7e-a634-dae53c7aa8f5"], "corpus_id": 218772411, "title": "A biochemically-interpretable machine learning classifier for microbial GWAS", "paper_id": "027d2a0fe82f42208196148a2d036eb8356b5744", "abstract": "Current machine learning classifiers have successfully been applied to whole-genome sequencing data to identify genetic determinants of antimicrobial resistance (AMR), but they lack causal interpretation. Here we present a metabolic model-based machine learning classifier, named Metabolic Allele Classifier (MAC), that uses flux balance analysis to estimate the biochemical effects of alleles. We apply the MAC to a dataset of 1595 drug-tested Mycobacterium tuberculosis strains and show that MACs predict AMR phenotypes with accuracy on par with mechanism-agnostic machine learning models (isoniazid AUC\u2009=\u20090.93) while enabling a biochemical interpretation of the genotype-phenotype map. Interpretation of MACs for three antibiotics (pyrazinamide, para-aminosalicylic acid, and isoniazid) recapitulates known AMR mechanisms and suggest a biochemical basis for how the identified alleles cause AMR. Extending flux balance analysis to identify accurate sequence classifiers thus contributes mechanistic insights to GWAS, a field thus far dominated by mechanism-agnostic results.", "intro": "M ycobacterium tuberculosis (TB) claims 1.6 million lives annually and resists eradication through evolution of antimicrobial resistance (AMR) 1 . To elucidate AMR mechanisms, researchers have applied machine learning approaches to large-scale genome sequencing and drug-testing datasets for identifying genetic determinants of AMR [2][3][4][5][6][7] . While current machine learning approaches have provided a predictive tool for microbial genome-wide association studies (GWAS), such black-box models are incapable of mechanistically interpreting genetic associations. Such a limitation has become increasingly apparent in TB, where numerous experimental studies have shown that AMR-associated genetic variants often reflect network-level metabolic adaptations to antibiotic-induced selection pressures ( Supplementary Fig. 1 [8][9][10][11][12] . These studies show that identified genetic associations have corresponding network-level associations that are highly informative of AMR mechanisms. However, current GWAS results only provide predictions for which alleles are most important, not their functional effects. Therefore, machine learning models that incorporate biochemical network structure may naturally extend GWAS results by estimating functional effects of identified alleles, leading to an enhanced understanding of AMR [13][14][15] .\n\nOver the past couple of decades, the computational analysis of biochemical networks in microorganisms has been advanced through the use of genome-scale models (GEMs) 16,17 . By computing metabolic flux states (see Glossary for definition of terms) consistent with imposed biological constraints, GEMs have been shown to predict a range of cellular functions, making them a valuable tool for analyzing multi-omics datasets 18 . Although GEMs are transparent genotype-phenotype models, they are largely outperformed by machine learning models in direct comparisons of prediction accuracy. Approaches have thus been developed that integrate meaningful GEM computations with predictive black-box machine learning to enable white-box interpretations of data 19 . These approaches have worked well for endogenous metabolomics data by using the GEM to directly transform the measurements to meaningful inputs for black box machine learning.\n\nThis approach, however, may not be amenable to analyzing microbial GWAS data, in which the genetic parameters of the GEM are not directly observed (see Supplementary Notes). GEMs have previously modeled genetic variation at the resolution of gene presence-absence 20-23 , but have not yet been used to link nucleotide-level genetic variation (i.e., alleles) to observed phenotypes (i.e., AMR) in a predictive manner 24 . Since alleles are the primary forms of causal variation identified in GWAS, an approach for mechanistically integrating information about alleles is of major interest 25 .\n\nHere we develop a GEM-based machine learning framework for modeling datasets used in GWAS and apply it to a sequencing dataset of drug-tested TB strains. We show that our framework achieves high performance in accurately classifying AMR phenotypes of TB strains. We then characterize the identified classifiers for pyrazinamide, isoniazid, and para-aminosalicylic acid AMR and show that they identify key genetic determinants and pathway activity discriminating between resistant and susceptible TB strains. This work demonstrates how GEMs can be used directly as an input-output machine learning model to extract both genetic and biochemical network-level insights from microbial GWAS datasets."}
{"tabids": ["d3b56d4e-6535-4c7e-a634-dae53c7aa8f5"], "corpus_id": 221912722, "title": "Genome\u2010wide association study\u2010based deep learning for survival prediction", "paper_id": "f08a8921817341a6491cd11788eb4e019bc06b52", "abstract": "Informative and accurate survival prediction with individualized dynamic risk profiles over time is critical for personalized disease prevention and clinical management. The massive genetic data, such as SNPs from genome\u2010wide association studies (GWAS), together with well\u2010characterized time\u2010to\u2010event phenotypes provide unprecedented opportunities for developing effective survival prediction models. Recent advances in deep learning have made extraordinary achievements in establishing powerful prediction models in the biomedical field. However, the applications of deep learning approaches in survival prediction are limited, especially with utilizing the wealthy GWAS data. Motivated by developing powerful prediction models for the progression of an eye disease, age\u2010related macular degeneration (AMD), we develop and implement a multilayer deep neural network (DNN) survival model to effectively extract features and make accurate and interpretable predictions. Various simulation studies are performed to compare the prediction performance of the DNN survival model with several other machine learning\u2010based survival models. Finally, using the GWAS data from two large\u2010scale randomized clinical trials in AMD with over 7800 observations, we show that the DNN survival model not only outperforms several existing survival prediction models in terms of prediction accuracy (eg, c\u2010index =0.76), but also successfully detects clinically meaningful risk subgroups by effectively learning the complex structures among genetic variants. Moreover, we obtain a subject\u2010specific importance measure for each predictor from the DNN survival model, which provides valuable insights into the personalized early prevention and clinical management for this disease.", "intro": "regarding the disease progression pattern in the future and shapes the physician's decision making for the treatment or clinical management strategy. Note that the survival prediction is fundamentally different from typical prediction models that predict a future event (whether occurs or not) by fixing the time of interest through a binary classification. 3,4 Despite its essential role in precision medicine, the survival prediction remains a challenging task, [5][6][7] largely due to the complex nature of diseases and the heterogeneity between patients. Therefore, there is an urgent need for developing accurate and personalized survival prediction models with improved capacity in learning the complex structures and interplays among predictors. Recent advances in high-throughput technologies have generated large volumes of molecular profiling data for each patient, which provides unprecedented opportunities in identifying potential biomarkers and further establishing accurate survival prediction models. [8][9][10] In particular, several national-wide large-scale longitudinal studies, such as the trans-omics for precision medicine and All of Us, are underway using whole-genome sequencing and other omics technologies, with the ultimate goal of accelerating precision medicine. However, how to effectively utilize the wealthy amount of data is challenging. The first challenge comes from how to connect high-dimensional predictors with the outcome of interest. This problem is particularly difficult in survival prediction because the events of interest are sometimes censored due to either a short study period or loss of follow-up during the study. The second challenge is how to model the complex structure among numerous biomarkers, where the specific structure is largely unknown. The third challenge is that given the heterogeneity of patients, how to interpret the importance of each predictor for each patient and further how to identify patient subgroups to provide personalized prevention or treatment strategy.\n\nThe recent advances in multilayer deep neural network (DNN) models have made extraordinary achievements in providing new effective risk prediction models from complex and high-dimensional biomedical data, such as omics and biomedical imaging. [11][12][13][14] However, the application of deep learning in survival prediction is still limited. Faraggi and Simon 15 proposed a single-layer neural network based on the Cox proportional hazards (PH) model. However, its performance did not exceed the regular Cox model in a prostate cancer survival dataset with 475 patents and only four clinical predictors. More recently, multiple efforts have been devoted to evaluating Cox-based neural network survival models using larger datasets with omic biomarkers. For example, Katzman et al 16 demonstrated that a single hidden layer neural network survival model performed marginally better than the Cox model and random survival forest (RSF) model in a breast cancer survival dataset with 1980 patients and nine predictors. In another study, Ching et al 17 applied a single hidden layer neural network survival model to 10 TCGA cancer survival datasets (sample sizes range from 302 to 1077) with high-throughput gene expression biomarkers, from which the neural network survival models resulted in comparable or better performance than the Cox model, the penalized Cox models such as Cox-LASSO and the RSF model. In another study 18 that also used TCGA cancer survival datasets (sample sizes range from 194 to 1092 with up to 17 000 gene expression biomarkers), the neural network survival models yielded comparable performance to the penalized Cox model and better performance than the RSF model. Hao et al 19 developed a pathway-based neural network survival model and applied it to a TCGA cancer dataset (sample size 522 with 860 pathways and 5567 genes). However, all these studies have limited sample sizes, particular in the presence of tens of thousands of predictors, and thus may lead to severe model overfitting problem. Moreover, the patient-specific predictor importance was not considered in those studies. The scenario of tied events, which is commonly seen in practice, especially when the sample size is large, was not carefully considered in these studies.\n\nIn this article, we propose and evaluate a multi-hidden-layer Cox-based DNN survival model to predict the progression of a progressive eye disease, namely, age-related macular degeneration (AMD). The genome-wide association study (GWAS) of AMD is the first and most successful GWAS research, where the massive GWAS data provide unprecedented opportunities to study disease risk and progression. Although some attempts have been tried to predict AMD progression risks using genetic information such as SNPs, most statistical models focus on the structured regression framework, which typically only accounts for (generalized) linear effects of the SNPs and thus have considerable limitations. To the best of our knowledge, there has no existing work on survival prediction using deep learning to effectively extract features from the GWAS data. Therefore, we develop and apply the DNN survival model to build an accurate and interpretable prediction model for the AMD progression.\n\nThe rest of the article is organized as follows. Section 2 describes the deep learning survival methods and prediction evaluation procedures. We assess the performance of three machine/deep learning survival prediction models (DNN, LASSO, RSF) through extensive simulation studies in Section 3 and apply them to the GWAS data from two large-scale clinical studies of AMD in Section 4. Discussion and conclusion are presented in Section 5."}
{"tabids": ["d3b56d4e-6535-4c7e-a634-dae53c7aa8f5"], "corpus_id": 232333726, "title": "Impact of Clinical and Genomic Factors on SARS-CoV2 Disease Severity", "paper_id": "cb2c0c1dd2ff3ceeb391df259d61bdf20a0b741b", "abstract": "The SARS-CoV2 virus behind the COVID-19 pandemic is manifesting itself in different ways among infected people. While many are experiencing mild flue-like symptoms or are even remaining asymptomatic after infection, the virus has also led to serious complications, overloading ICUs while claiming more than 2.6 million lives world-wide. In this work, we apply AI methods to better understand factors that drive the severity of the disease. From the UK BioBank dataset we analyzed both clinical and genomic data of patients infected by this virus. Leveraging positiveunlabeled machine learning algorithms coupled with RubricOE, a state-of-the-art genomic analysis framework forgenomic feature extraction, we propose severity prediction algorithms with high F1 score. Furthermore, we extracted insights on clinical and genomic factors driving the severity prediction. We also report on how these factors have evolved during the pandemic w.r.t. significant events such as the emergence of the B.1.1.7 SARS-CoV2 virus strain.", "intro": "COVID-19, caused by the deadly \"severe acute respiratory syndrome coronavirus 2\" (SARS-CoV2) virus, is one of the worst pandemic in human history inflicting severe casualties across the globe. So far, it has caused 117,332,262 confirmed cases while claiming 2,605,356 deaths worldwide 1 . This virus has stressed many health systems beyond their limits, screaming for an urgent reaction from research to help mitigate this unprecedented societal threat.\n\nThe outbreak of the virus first occurred in Wuhan province of China and then rapidly spread world-wide using humanto-human transmission 2 . One of the primary reason for the rapid spread is due to the wide spectrum of symptoms inhibited by the virus ranging from asymptomatic to mild to rapid progression to critical stage with pneumonia and likely death of respiratory failure 3 . In general, patients with mild symptoms cure easily whereas severe COVID-19 patients require further treatments such as ventilation in ICU. Such heterogeneity in terms of COVID-19 disease pose great challenges for designing treatment protocols. Typically, severe effects of infectious diseases like COVID-19 are hypothesized to be associated with the variations of host genome 4 . Several studies 5 were conducted recently to find the biological mechanisms of the severity of COVID-19 diseases such as genome wide association studies (GWAS) of severe COVID-19 6 . In addition, clinical factors such as demographic, socio-economic factors, lifestyles, prior conditions may also have impact on the spread, exposure, and severity. So does the viral load of the SARS-CoV2 virus. The availability of rich electronic health records (EHR) provides an opportunity for finding such important clinical factors associated with COVID-19 along with important clinical factors observed in EHRs 3,7,8 .\n\nFinding the bio-markers for severe COVID-19 patients requires rigorous study on clinical and genomic datasets. Most of the existing studies that aimed at finding the common risk factors for severe COVID-19 patients focused on either the clinical or the genomics factors, but not both. In this study, we aim to find the common risk factors associated COVID-19 severity using diverse factors from both clinical and genomics data available from a large-scale EHR dataset. Such kind of combined clinico-genomic factors can provide a holistic view of COVID-19 disease mechanisms. Specifically, it can provide the additional effects of genomic factors on the host genome as they relate to common clinical factors.\n\nHowever, there are few challenges in finding the clinical and genomic factors associated with COVID-19 that can be used as bio-markers. First, defining the COVID-19 severity from EHR datasets is often challenging, since it is not collected directly in these datasets. Hence, the severity has to be defined using some clinical knowledge as a surrogate phenotype which in turn may introduce label noise and data collection bias. Second, assessing the impact of genomic factors on COVID-19 severity may be confounded by several other clinical factors such as the prior comorbidities of patients and treatment protocols. The effect of such confounding factors has to be addressed carefully when conducting the combined clinico-genomic studies. Third, the impact of clinico-genomic factors may also vary depending on the variations of diverse COVID-19 strains that have been observed due to genetic mutations.  Origin=0 11,989  9,083 21,072  Origin=1 31,800  4,345 36,145  All  43,789  13,428 57,217 In this study, we propose a combined framework for finding the clinical and genomic factors associated with COVID-19 severity using a large COVID-19 dataset from the UK Biobank (UKBB). To address the above mentioned challenges, we first use the COVID-19 related hospitalization as a surrogate outcome for defining severe COVID-19 cases. Second, we use a machine learning technique called positive-unlabeled (PU) learning to address the noise and reporting bias present in COVID-19 severity labels. Moreover, we use a recently proposed genomic analysis framework entitled RubricOE 9 to select the set of genomic factors after adjusting for the common prior comorbid conditions which may act as potential confounders. Finally, we aim to assess how the importance of the extracted bio-markers evolve over the pandemic marked by event such as the emergence of the reportedly more contagious B.1.1.7 COVID-19 strain."}
{"tabids": ["d3b56d4e-6535-4c7e-a634-dae53c7aa8f5"], "corpus_id": 237556679, "title": "GenNet framework: interpretable deep learning for predicting phenotypes from genetic data", "paper_id": "e0e84f93df08251515546dee2c421c322302d6a3", "abstract": "Applying deep learning in population genomics is challenging because of computational issues and lack of interpretable models. Here, we propose GenNet, a novel open-source deep learning framework for predicting phenotypes from genetic variants. In this framework, interpretable and memory-efficient neural network architectures are constructed by embedding biologically knowledge from public databases, resulting in neural networks that contain only biologically plausible connections. We applied the framework to seventeen phenotypes and found well-replicated genes such as HERC2 and OCA2 for hair and eye color, and novel genes such as ZNF773 and PCNT for schizophrenia. Additionally, the framework identified ubiquitin mediated proteolysis, endocrine system and viral infectious diseases as most predictive biological pathways for schizophrenia. GenNet is a freely available, end-to-end deep learning framework that allows researchers to develop and use interpretable neural networks to obtain novel insights into the genetic architecture of complex traits and diseases.", "intro": "W hile genome-wide association studies (GWAS) have identified numerous genomic loci associated with complex traits and diseases, the biological interpretation of the underlying mechanisms often remains unclear. Recent GWAS studies with increasingly large sample sizes are resulting greater numbers of significant associations, at an increasing number of independent loci. To illustrate, the latest GWAS for body height based on 700,000 individuals identified more than 3000 near-independent significantly associated single nucleotide polymorphisms (SNPs) 1 . Uncovering a clear biological interpretation from all this information is a challenging task, in which causal variants, genes, and pathways need to be identified. In response, many methods such as MAGMA 2 , ALIGATOR 3 , and INRICH 4 , have been developed to obtain a biological interpretation from GWAS summary statistics, providing insights into relevant genes and pathways for defined phenotypes of interest. These methods explore GWAS summary statics and utilize knowledge from annotated biological databases such as NCBI RefSeq 5 , KEGG 6 , Reactome 7 , and GTEx 8 , which have proven to contain crucial information for understanding the underlying biological mechanisms of the human genome 9 . Additionally, it has been shown that embedding biological knowledge from these databases in polygenic risk scores can improve interpretation, trans-ancestry portability, and genetic risk prediction [10][11][12] .\n\nGiven the increasing amount of data available via biobanks and new developments to integrate data, it is now feasible to analyze raw data with more advanced methods. Deep learning is the state of the art in many domains such as medical image analysis 13 and natural language processing 14 because of its flexibility and modeling capabilities. In many cases, deep learning yields better performance than traditional approaches, since it scales very well with data size and can model highly non-linear relationships. However, a limitation to deep learning is that these algorithms are often uninterpretable because of their complexity 15,16 . Additionally, genetic data does not lend itself well to the convolution operation, the main driver of the success of deep learning in the imaging domain. Traditional fully connected neural networks have been successfully applied to predict genetic risk. Recently, Badre et al. 17 employed a fully connected neural network for improving polygenic risk scores for breast cancer, training a neural network with up to 528,620 input variants. However, these networks are very memory-intensive and therefore often require pre-selecting SNPs using GWAS summary statistics. Applying these fully connected neural networks for millions of input variants would require infeasible amounts of computational time and memory.\n\nTo overcome these limitations, we propose GenNet, a novel framework for predicting phenotype from genotype. Within the GenNet framework, biological information from annotated biological sources such as NCBI RefSeq, KEGG, and single RNA gene expression datasets, is used to define biologically plausible connections. As a result, neural networks based on this framework are memory efficient, interpretable, and yield biological interpretations for their predictions. GenNet is an end-to-end deep learning framework available as a command-line tool (https://github.com/ArnovanHilten/GenNet/)."}
{"tabids": ["d3b56d4e-6535-4c7e-a634-dae53c7aa8f5"], "corpus_id": 232055058, "title": "Genome-Wide Association Studies-Based Machine Learning for Prediction of Age-Related Macular Degeneration Risk", "paper_id": "b48f8a3a1872aa33b7547da3980eb190c7ab28ad", "abstract": "Purpose Because age-related macular degeneration (AMD) is a progressive disorder and advanced AMD is currently hard to cure, an accurate and informative prediction of a person's AMD risk using genetic information is desirable for early diagnosis and potential individualized clinical management. The objective of this study was to develop and validate novel prediction models for AMD risk using large genome-wide association studies datasets with different machine learning approaches. Methods Genotype data from 32,215 Caucasian individuals with age of \u226550 years from the International AMD Genomics Consortium in dbGaP were used to establish and test prediction models for AMD risk. Four different machine learning approaches\u2014neural network, lasso regression, support vector machine, and random forest\u2014were implemented. A standard logistic regression model using a genetic risk score was also considered. Results All machine learning\u2013based methods achieved satisfactory performance for predicting advanced AMD cases (vs. normal controls) (area under the curve = 0.81\u20130.82, Brier score = 0.17\u20130.18 in a separate test dataset) and any stage AMD (vs. normal controls) (area under the curve = 0.78\u20130.79, Brier score = 0.18\u20130.20 in a separate test dataset). The prediction performance was further validated in an independent dataset of 783 subjects from UK Biobank (area under the curve = 0.67). Conclusions By applying multiple state-of-art machine learning approaches on large AMD genome-wide association studies datasets, the predictive models we established can provide an accurate estimation of an individual's AMD risk profile based on genetic information along with age. The online prediction interface is available at: https://yanq.shinyapps.io/no_vs_amd_NN/. Translational Relevance The accurate and individualized risk prediction model interface will greatly improve early diagnosis and enhance tailored clinical management of AMD.", "intro": "Age-related macular degeneration (AMD) is a multifactorial neurodegenerative disease and a leading cause of vision loss among the elderly in the developed countries. 1,2 The disease affects the central vision and is progressive, starting with the appearance of drusen (i.e., the yellow or white deposits in the eye) and eventually leading to advanced AMD forms: wet AMD (choroidal neovascularization) and dry AMD (geographic atrophy). 3 Patients can progress to one or both forms of advanced AMD. Some patients with early AMD maintain good vision for a long time without progressing to advanced AMD, whereas others quickly developed advanced AMD.\n\nIn 2005, Fisher et al. 4 reported that the CFH gene on chromosome 1 and ARMS2/HTRA1 genes on chromosome 10 were the most replicated gene regions associated with AMD. Later, with the advances of technology, multiple genome-wide association studies (GWAS) were conducted to examine the association between AMD and a genome-wide set of single nucleotide polymorphisms (SNPs). In 2016, the International AMD Genomics Consortium identified or confirmed a total of 34 loci with 52 independent genetic variants to be associated with advanced AMD risk. 5 From this study, the phenotype and genotypes of 35,358 subjects were uploaded to dbGaP (phs001039.v1.p1) and the majority of them are Caucasians. Multiple studies demonstrated that the same AMD susceptibility loci were more strongly associated with AMD in Caucasians than in other ethnic groups. [6][7][8] Because advanced AMD is currently hard to cure, an accurate and informative prediction of a person's risk for advanced AMD at a young age using genetic information is desirable for early diagnosis, enhanced diet/behavior, and potential individualized clinical management. For example, for individuals with high predicted AMD risks, behaviors that could decrease AMD risk such as stopping smoking, keeping a healthy diet with more antioxidants, and taking appropriate vitamin supplements can be recommended. Earlier or more frequent clinical visits to monitor the development or progression of the disease can be also suggested to individuals with high AMD risks. In this study, our objective was to establish and validate prediction models for AMD risk based on genetic variants given any future age of a subject using the largest publicly available data for Caucasians."}
{"tabids": ["d3b56d4e-6535-4c7e-a634-dae53c7aa8f5"], "corpus_id": 235333227, "title": "DeepOmix: A scalable and interpretable multi-omics deep learning framework and application in cancer survival analysis", "paper_id": "8a9be6482128f1e5398647edc34bec65c31a251d", "abstract": "Graphical abstract", "intro": "The advent of high-throughput omics technologies, such as the next-generation sequencing [1], DNA microarrays [2], and DNA methylation arrays [3] enable the measurement of thousands of molecules at the same time from a biological sample comprehensively. Each type of omics data provides unbiased characterization on one aspect of genome, transcriptome, and epigenome, which raises opportunities for biological and medical research explorations [4,5]. However, analysis of single omics data is limited to exploring the underlying biological mechanisms and capturing intricacy for various complex diseases, which only can explain its molecular field respectively [6]. Therefore, integrating multiomics data at different levels yields a better understanding of overall disease alterations, has an enormous impact in cancer profiling, diagnosis, and treatment, and elucidates the relationships among different types of omics data for one specific disease [7].\n\nDeveloping accurate survival prediction models of cancer benefits the identification of effective prognostic biomarkers, improvement of risk stratification, and personalized treatment. With the accumulation of a tremendous number of multiple omics data in the past decades, it brings opportunities to build the prediction model and infer the systematic underlying biological mechanisms through making an integrative analysis. However, it raises new computational challenges in the data integration due to the heterogeneous characteristics and distribution of different types of data, the high dimensionality of each level of a molecular dataset, and a limited number of observations [8][9][10]. To address these issues, a variety of regression methods have been proposed to build the prognostic model through integrating multi-omics data [11].\n\nThere are mainly four kinds of approaches to predict the survival time, namely penalized regression, boosting, random forest, and deep learning-based methods [12]. Integrative LASSO with Penalty Factors (IPF-LASSO) [13], an extension of LASSO method, is designed to make the L1-penalized regression analysis by using different penalty weights for each type of omics data to train the  [14]. Both glmboost and IPF-LASSO can be applied to multi-omics data with high dimension, which perform the feature selection and reduce the complexity of the model. However, they could only detect the linear relationships between features and outcome variables through fitting linear models. Block forest, a variant from random forest, incorporates the block structure of multi-omics data and makes the non-linear prediction of the clinical outcomes [15]. The limitation of block forest is that they can't extrapolate the data.\n\nSome deep learning-based methods were also applied to train the survival model. DeepSurv was a method of the deep feedforward neural network to perform a prediction of time-to-event to make personalized treatment recommendations. It estimated each sample's effect on their hazard rates concerning parametrized weights of the network, which was configurable with multiple numbers of hidden layers [16]. DeepHIT was a deep neural network architecture to learn the joint distribution of survival time and event directly without making assumptions about the underlying stochastic process [17]. The parameters of the model and the form of the stochastic process depended on the features of the input dataset used for survival analysis. However, these two methods were not designed for integrating multi-omics data and lacked interpretability. It was urgent to propose an interpretable nonlinear model for multi-omics data integration and survival prediction.\n\nTo fill the gaps of the algorithm in this field, we presented Dee-pOmix, a scalable and interpretable deep learning framework for multi-omics data integration and survival prediction. DeepOmix learned meaningful information by incorporating prior biological knowledge of gene functional module networks as the function module layer since genes perform functions in cells in the form of a synergistic and regulatory system [18]. Getting the lowdimensional representations in the functional module layer facilitates extracting significant modules corresponding to the prognostic prediction result. The functional modules can be defined by users, including tissue networks [19], gene co-expression networks [20], or prior biological signaling pathways [21]. We performed experiments of benchmark comparison and elucidated that the performance of DeepOmix outperformed other existing state of art prediction models. Then, Low-Grade Glioma (LGG) was taken as the case study. Patients were grouped into two subtypes with significant differences in survival time based on the output layer of the prediction result. The difference of functional nodes on the module layers in these subtypes was tested and top-ranked functional modules were detected. DeepOmix can integrate multiomics data by incorporating prior biological knowledge to conduct prognosis prediction and learn the low representations on the module layers to understand the underlying mechanisms for further study."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 233444275, "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "paper_id": "476d79d1f5650c5361104ed468e75bfc4732622d", "abstract": "Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.", "intro": "Like many natural language generation tasks, machine translation (MT) is difficult to evaluate because the set of correct answers for each input is large and usually unknown. This limits the accuracy of automatic metrics, and necessitates costly human evaluation to provide a reliable gold standard for measuring MT quality and progress. Yet even human evaluation is problematic. For instance, we often wish to decide which of two translations is better, and by how much, but what should this take into account? If one translation sounds somewhat more natural than another, but contains a slight inaccuracy, what is the best way to quantify this? To what extent will different raters agree on their assessments?\n\nThe complexities of evaluating translationsboth machine and human-have been extensively studied, and there are many recommended best practices. However, due to expedience, human evaluation of MT is frequently carried out on isolated sentences by inexperienced raters with the aim of assigning a single score or ranking. When MT quality is poor, this can provide a useful signal; but as quality improves, there is a risk that the signal will become lost in rater noise or bias. Recent papers have argued that poor human evaluation practices have led to misleading results, including erroneous claims that MT has achieved human parity (Toral, 2020;L\u00e4ubli et al., 2018).\n\nThis paper aims to contribute to the evolution of standard practices for human evaluation of highquality MT. Our key insight is that any scoring or ranking of translations is implicitly based on an identification of errors and other imperfections. Making such an identification explicit by enumerating errors provides a \"platinum standard\" from which various gold-standard scorings can be derived, depending on the importance placed on different categories of errors for different downstream tasks. This is not a new insight: it is the conceptual basis for the Multidimensional Quality Metrics (MQM) framework developed in the EU QTLaunchPad and QT21 projects (www.qt21.eu), which we endorse and adopt for our experiments.\n\nMQM is a generic framework that provides a hierarchy of translation errors which can be tailored to specific applications. We identified a hierarchy appropriate for broad-coverage MT, and annotated outputs from 10 top-performing \"systems\" (including human references) for both the English\u2192German (EnDe) and Chinese\u2192English (ZhEn) language directions in the WMT 2020 news translation task (Barrault et al., 2020), using arXiv:2104.14478v1 [cs.CL] 29 Apr 2021 professional translators with access to full document context. For comparison purposes, we also collected scalar ratings on a 7-point scale from both professionals and crowd workers.\n\nWe analyze the resulting data along many different dimensions: comparing the system rankings resulting from different rating methods, including the original WMT scores; characterizing the error patterns of modern neural MT systems, including profiles of difficulty across documents, and comparing them to human translations (HT); measuring MQM inter-annotator agreement; and re-evaluating the performance of automatic metrics submitted to the WMT 2020 metrics task. Our most striking finding is that MQM ratings sharply revise the original WMT ranking of translations, exhibiting a clear preference for HT over MT, and promoting some low-ranked MT systems to much higher positions. This in turn changes the conclusions about the relative performance of different automatic metrics; interestingly, we find that most metrics correlate better with MQM rankings than WMT human scores do. We hope these results will underscore and help publicize the need for more careful human evaluation, particularly in shared tasks intended to assess MT or metric performance. We release our corpus to encourage further research. 1 Our main contributions are:\n\n\u2022 A proposal for a standard MQM scoring scheme appropriate for broad-coverage MT.\n\n\u2022 Release of a large-scale MQM corpus with annotations for over 100k HT and highquality-MT segments in two language pairs (EnDe and ZhEn) from WMT 2020. This is by far the largest study of human evaluation results released to the public.\n\n\u2022 Re-evaluation of the performance of MT systems and automatic metrics on our corpus, showing clear distinctions between HT and MT based on MQM ratings, adding to the evidence against claims of human parity.\n\n\u2022 Demonstration that crowd-worker evaluation has low correlation with our MQM-based evaluation, calling into question conclusions drawn on the basis of previous crowdsourced evaluations.\n\n1 https://github.com/google/ wmt-mqm-human-evaluation \u2022 Demonstration that automatic metrics based on pre-trained embeddings can outperform human crowd workers.\n\n\u2022 Characterization of current error types in HT and MT, identifying specific MT weaknesses.\n\n\u2022 Recommendations for the number of ratings needed to establish a reliable human benchmark, and for the most efficient way of distributing them across documents."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 233407441, "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics", "paper_id": "667bdd2a8dc997d40c106ff6761babebe4050762", "abstract": "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.", "intro": "Factuality is defined as a measure of \"whether eventualities are characterized as corresponding to facts, possibilities, or situations that do not hold in the world\" (Sauri, 2008;Saur\u00ed and Pustejovsky, 2012). In summarization, this \"world\" is the article, which is taken as ground-truth, and the output summary must be faithful to the article's facts. Despite advancements in neural abstractive summarization (Narayan et al., 2018;Liu and Lapata, 2019;Lewis et al., 2020), \u223c30% of summaries have factual inconsistencies (Cao et al., 2018). With summarization being an integral component of information consumption, this highlights a need for ensuring summarization systems are factually consistent and developing methods for evaluating them.\n\nCommon evaluation metrics for summarization based on n-gram overlap -BLEU, ROUGE, and METEOR (Papineni et al., 2002;Lin, 2004;Lavie and Agarwal, 2007) -are insufficient to measure the factual correctness of summaries and fail to correlate with the human judgements of factuality (Falke et al., 2019;Kryscinski et al., 2019). More recent metrics proposed to improve the evaluation of summarization factuality (Kryscinski et al., 2020;Durmus et al., 2020;Wang et al., 2020;Maynez et al., 2020) cannot be compared due to the lack of common benchmarks. More critically, while these approaches differ in the way they model factuality, they all consider factuality as a binary concept, labeling summaries of any length as factual or non-factual. They do not provide any finegrained understanding of the factual errors made by different systems that could serve as an actionable feedback on a system's limitations.\n\nThe binary factuality of a text can be difficult to determine. Falke et al. (2019) show relatively low crowd-expert agreement, indicating the presence of subjectivity in the annotation process. Moreover, not all factual errors are equally important and the number of errors can have a significant impact on the perceived factuality of a text. This suggests that non-factuality should be modeled as a multidimensional construct and not a label.\n\nIn this work, we propose a linguistically motivated typology of factual errors for fine-grained analysis of factuality in summarization systems ( \u00a72). Our typology is theoretically grounded in frame semantics (Fillmore et al., 1976;Palmer et al., 2005) and linguistic discourse theory (Brown and Yule, 1983). It provides several benefits. First, we find that decomposing the concept of factuality in (relatively) well-defined and grounded categories makes the final binary decision more objective leading to near perfect agreement between crowd and expert annotators (\u03ba = 0.86). Second, this approach provides some measure of the degree of non-factuality both in terms of the quantity and the category of factual violations that appear Figure 1: We propose a linguistically grounded typology of factual errors. We select crowd workers to annotate summaries from two datasets according to this typology achieving near perfect agreement with experts. We collect FRANK, the resulting dataset, to benchmark factuality metrics and state-of-art summarization systems.\n\nin the text. This typology also provides us with the means to categorize the types of errors made by summarization systems, helping us gain deeper insights than simply categorizing content as factual or hallucinated.\n\nWe define an annotation protocol of factuality based on our typology and collect a dataset of human judgements over a diverse set of model generated summaries on the CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) datasets ( \u00a73). Through this dataset, we aim to both assess the factuality of summarization systems and benchmark recently proposed factuality metrics. In \u00a74 we discuss various state-of-art models and show a detailed analysis of the factual errors they make. Finally, in \u00a75 we evaluate multiple summarization metrics against our benchmark and show their strengths and weaknesses in detecting specific types of factual errors. Figure 1 shows an overview of this work."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 248887364, "title": "SNaC: Coherence Error Detection for Narrative Summarization", "paper_id": "37cef4df24447b012f0671f0baabe6b9ffe38bf2", "abstract": "Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework for fine-grained annotations of long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowdworkers. Furthermore, we show that the collected annotations allow us to benchmark past work in coherence modeling and train a strong classifier for automatically localizing coherence errors in generated summaries. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction.", "intro": "As pre-trained models for news summarization (Lewis et al., 2020;Zhang et al., 2020;Brown et al., 2020) have improved drastically, researchers have begun tackling increasingly challenging settings, particularly long document summarization and generation of longer summaries (Kry\u015bci\u0144ski et al., 2021;Huang et al., 2021;Zhang et al., 2022;Wu et al., 2021). Summaries in these settings differ considerably from the newswire summaries of past research efforts (Nallapati et al., 2016;Narayan et al., 2018): models now need to extract salient information from different parts of a significantly Figure 1: Excerpt from a generated book summary by OpenAI's 175B model (Wu et al., 2021). Individual segments do not follow a coherent structure and extra information is often needed to understand the narrative.\n\nlonger document, and na\u00efvely combining these in a much longer output is less likely to yield a summary with coherent discourse structure.\n\nThis shift in the scope of the summarization task calls for a reexamination of the summarization evaluation framework. Even for short newswire summaries, Fabbri et al. (2021) showed that automated metrics are inadequate, and consequently, reporting results from a human evaluation study has become the standard practice. However, human evaluation is rarely done for longer summaries possibly due to the associated labor costs of reading and evaluating long text. It is also unclear whether A/B testing or Likert-scale based annotation frameworks transfer to long summary settings. Establishing human evaluation protocols is critical for comparing different modeling approaches and measuring progress.\n\nRecently, Wu et al. (2021) proposed a strong book summarization model but showed that although generated summaries covered important information from the books, they read like a list of events stapled together without any coherent narrative structure (see Figure 1). We found similar"}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 247315430, "title": "Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text", "paper_id": "ab847321118de056ff00325d58903b71bdcc76e1", "abstract": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow\u2014such as redundancy, commonsense errors, and incoherence\u2014are identified through several rounds of crowd annotation experiments without a predefined ontology.We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.", "intro": "Clark et al. (2021) demonstrated the challenges of human evaluation in the era of GPT-3 (Brown et al., 2020), as crowd workers are no longer able to reliably distinguish GPT-3\u2019s generations from human-written text.\n\nOr are they? In this paper, we propose a new framework for systematically scrutinizing machine text so that even crowd workers, despite the known challenges reported by recent literature, can successfully critique seemingly fluent generations. We not only quantify a measurable gap between machine text and human text, but reveal the distributions of specific categories of issues, and pinpoint their occurrences in text written by several sizes of language models as well as humans.\n\nTo achieve this, we develop SCARECROW, a methodology for eliciting categorical judgements of errors in machine-generated text from crowd workers. One goal in natural language generation (NLG) is to produce fluent outputs which can be read by laypeople. As such, we propose that important errors to address are those which are recognized by readers without NLP expertise. Our framework allows crowd workers to annotate problems in model outputs at the span level. A single such annotation is shown in Figure 1.\n\nTo make this possible, we establish a categorization of shortcomings commonly found in machine generated text (Table 1). This error schema covers a broad scope of problems as identified by experts, but has been honed according to what is salient to non-expert readers through several pilot rounds of crowd annotation without a fixed label set. The result is a framework that is usable by everyday people with minimal training, but covers the error phenomena found in real machine-generated text. Labeling spans of text using specific error types creates a picture of contemporary model generations with an unprecedented level of detail. In contrast to judging text holistically (Celikyilmaz et al., 2021), insights from this method are specific and practical, as it measures exactly how and where problems arise.\n\nWe conduct a large-scale analysis of humanwritten and machine-generated text using SCARECROW, collecting 13k annotations of 1.3k paragraphs, amassing 41k spans labeled with error type, severity, and an explanation. Through this, we characterize in which ways GPT-3\u2019s generations are better than those of previous models, and which aspects do not improve with increased data and parameters. We also provide a rigorous error analysis of text generated by several other contemporary language models, examining the impact of model size, training data, and decoding strategy.\n\nWe provide our detailed annotator training system and task interface so that future researchers may employ and refine them for error analyses of machine-generated text. We hope this will contribute to the standardization of NLG human evaluation (Howcroft et al., 2020)."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 258865481, "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA", "paper_id": "3d4a89217c8e361c2adc80dec773ff1607f88a43", "abstract": "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.", "intro": "Text simplification aims to improve a text's readability or content accessibility while preserving its fundamental meaning (Stajner, 2021;Chandrasekar et al., 1996).Traditional human evaluation for text simplification often relies on individual, shallow sentence-level ratings (Sulem et al., 2018c;Alva-Manchego et al., 2021), easily affected by the annotator's preference or bias.Maddela et al. (2023) recently proposes a more reliable and consistent human evaluation method by ranking and rating multiple simplifications altogether.However, as text simplification involves performing a series of transformations, or edits, such as paraphrasing, removing irrelevant details, or splitting a long sen-  tence into multiple shorter ones (Xu et al., 2012), sentence-level scoring remains difficult to interpret since it is not reflective of detailed information about the types of edits being performed.\n\nFine-grained human evaluation through span selection has been explored for machine translation (Lommel et al., 2014) and open-ended text generation (Dou et al., 2022).Yet, these evaluation methods are error-driven -i.e., focusing solely on evaluating failure -which punishes creative and diverse generations with minor errors in favor of generic ones.Additionally, machine translation and open-ended generation tasks usually retain none of the input words, while text simplification must balance the editing and preservation of words in the original input (Xu et al., 2016).We thus evaluate simplification quality as the aggregation of edit successes and failures, as depicted in Figure 1.\n\nWe introduce SALSA -Success and FAiluredriven Linguistic Simplification Annotation -an arXiv:2305.14458v2[cs.CL] 22 Oct 2023 edit-level human evaluation framework capturing a broad range of simplification transformations.SALSA is built on a comprehensive typology ( \u00a72) containing 21 quality and error edit types.Using SALSA, we develop an interactive interface and collect 19K edit annotations of 840 simplifications written by eleven state-of-the-art language models and two humans.With these annotations, we conduct a large-scale analysis of model and automatic metric performance, and further introduce the automatic word-level quality estimation task for text simplification.Our main findings are as follows:\n\n\u2022 Few-shot GPT-3.5 far surpasses existing models, particularly in making syntax and content edits.However, its simplifications are not aligned to the types of operations performed by human.( \u00a74) \u2022 Some fine-tuned models such as the MUSS (Martin et al., 2022) produce more diverse edits than GPT-3.5, yet suffer from incredibly high errors, while others (T5, Raffel et al., 2020) learn to minimize loss by making very few changes.( \u00a74) \u2022 Open-source instruction fine-tuned models such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) perform a similar number of edits as GPT-3.5, but at a cost of more conceptual errors due to the inherent limits of model imitation.( \u00a74) \u2022 Fine-tuned on SALSA annotations, our referencefree metric, LENS-SALSA, captures the subtleties of specific simplification approaches beyond existing automatic evaluation metrics.( \u00a75) \u2022 Leveraging our data, we present the automatic word-level quality estimation task for text simplification and establish several baseline approaches for future modeling efforts.( \u00a76)\n\nOur results demonstrate that SALSA provides an interpretable and exhaustive evaluation of text simplification."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 12122749, "title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction", "paper_id": "4cac1e1eb876ffdcfda5a62d5237f942b519a502", "abstract": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as \u201cGood\u201d or \u201cAcceptable\u201d in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.", "intro": "Grammatical Error Correction (GEC) systems are often only evaluated in terms of overall performance because system hypotheses are not annotated. This can be misleading however, and a system that performs poorly overall may in fact outperform others at specific error types. This is significant because a robust specialised system is actually more desirable than a mediocre general system. Without an error type analysis however, this information is completely unknown.\n\nThe main aim of this paper is hence to rectify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information. This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback to non-native learners. Given that different corpora are also annotated according to different standards, we also attempted to standardise existing datasets under a common error type framework.\n\nOur approach consists of two main steps. First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016) and second, we classify them according to a new, rule-based framework that relies solely on dataset-agnostic information such as lemma and part-of-speech. We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT) 1 , by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical error correction (Ng et al., 2014).\n\nIt is worth mentioning that despite an increased interest in GEC evaluation in recent years (Dahlmeier and Ng, 2012;Felice and Briscoe, 2015;Bryant and Ng, 2015;Napoles et al., 2015;Grundkiewicz et al., 2015;Sakaguchi et al., 2016), ERRANT is the only toolkit currently capable of producing error types scores."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 259064099, "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "paper_id": "e2e52461194bc81351da7caa978ac42e9e9549cc", "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.", "intro": "State-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans [29,28,9], with a combination of supervised learning and reinforcement learning.Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29,3,2,33,10].Many of these studies adopt reinforcement learning from human feedback (RLHF) [46], a framework that converts human feedback into an effective LM training signal to reach these goals.Specifically, humans are presented with two or more outputs and asked to select one or rank them, and this signal is then used to train a reward model, which computes a single scalar reward for each LM-generated the layer of gases, generally known as air\u2026 > = >\n\nFigure 1: Comparison of (a) RL with human preference and (b) our FINE-GRAINED RLHF on long-form QA.Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors.We train a fine-grained reward model for each type of error and optimize LM against these reward models.\n\nIn this example, we provide a relevance reward and a factuality reward after each sentence is generated.There is also a holistic information completeness reward after the whole text is generated.\n\nIn this paper, we propose that humans give fine-grained feedback to LM output, associating categories of undesired behavior (e.g., false or irrelevant generations) and a text span at a density (e.g., sentence or sub-sentence-level).To enable LMs to learn from such fine-grained feedback, we introduce the FINE-GRAINED RLHF framework.As shown in Figure 1, we first use collected human feedback to train fine-grained reward models such that each of them focuses on one category and provides rewards at the density associated with that category.We then integrate these reward models into Proximal Policy Optimization (PPO) [37], a commonly used RL algorithm for training LMs with preference-based human feedback ( \u00a72).\n\nWe conduct experiments on two language generation tasks-detoxification [12] ( \u00a73) and long-form question answering (QA) [39] ( \u00a74).For detoxification, toxicity is the only error category and we explore learning with a dense reward.We adopt PERSPECTIVE [1], a widely used language toxicity detection model trained on millions of human annotations, as our reward model.We use it to calculate a fine-grained reward after the generation of every sentence.Our experimental results show the efficacy and data efficiency of training models with dense reward compared to a holistic sequence-level reward, supported by automatic evaluation results.\n\nWith experiments on long-form QA, we aim to examine training models with fine-grained rewards at the two granularity dimensions (density and error category), for which we construct a long-form QA dataset, QA-FEEDBACK, along with our collected human feedback.We carefully develop a pipeline to collect fine-grained human feedback on three error categories at different density levels: i) irrelevance, repetition, or incoherence (sub-sentence), ii) incorrect or unverifiable facts (sentence), and iii) incomplete information (whole sequence; see Figure 1).Our experimental results show improved results in each error category by learning with such fine-grained feedback, supported by both automatic and human evaluation results.In a scenario with multiple reward models representing different error types, we also show FINE-GRAINED RLHF allows us to combine reward models with different weights and thus control the model training process towards a customized combination of desired behaviors."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 252762392, "title": "Improving Large-scale Paraphrase Acquisition and Generation", "paper_id": "dc62b87f2eae1683f5dc667d5035ffbd36bbe011", "abstract": "This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of 130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert (MultiPIT_expert) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MultiPIT_NMR) and a large automatically constructed training set (MultiPIT_Auto) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MultiPIT_Auto generate more diverse and high-quality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT.", "intro": "Paraphrases are alternative expressions that convey a similar meaning (Bhagat and Hovy, 2013). Studying paraphrase facilitates research in both natural language understanding and generation. For instance, identifying paraphrases on social media is important for tracking the spread of misinformation (Bakshy et al., 2011) and capturing emerging events (Vosoughi and Roy, 2016). On the other hand, paraphrase generation improves the linguistic diversity in conventional agents (Li et al., 2016) and machine translation (Thompson and Post, 2020). It has also been successfully applied in data argumentation to improve information extraction (Zhang et al., 2015;Ferguson et al., 2018) and question answering systems (Gan and Ng, 2019). 7. In Tibet, climate change causes bigger, faster avalanches.  Many researchers have been leveraging Twitter data to study paraphrase given its lexical and style diversity as well as coverage of up-to-date events. However, existing Twitter-based paraphrase datasets, namely PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017), suffer from quality issues such as topic unbalance and annotation noise, 1 which limit the performance of the models trained using them. Moreover, past efforts on creating paraphrase corpora only consider one paraphrase criteria without taking into account the fact that the desired \"strictness\" of semantic equivalence in paraphrases varies from task to task (Bhagat and Hovy, 2013;Liu and Soh, 2022). For example, for the purpose of tracking unfolding events, \"A tsunami hit Haiti.\" and \"303 people died because of the tsunami in Haiti\" are sufficiently close to be considered as paraphrases; whereas for paraphrase generation, the extra information \"303 people dead\" in the latter sentence may lead models to learn to  Table 1: Statistics of MULTIPIT CROWD and MULTIPIT EXPERT datasets. The sentence/tweet lengths are calculated based on the number of tokens per unique sentence/tweet. %Multi-Ref denotes the percentage of source sentences with more than one paraphrase. Compared with prior work, our MULTIPIT CROWD dataset has a significantly larger size, a higher portion of paraphrases, and a more balanced topic distribution.\n\nhallucinate and generate more unfaithful content.\n\nIn this paper, we present an effective data collection and annotation method to address these issues. We curate the Multi-Topic Paraphrase in Twitter (MULTIPIT) corpus, which includes MUL-TIPIT CROWD , a large crowdsourced set of 125K sentence pairs that is useful for tracking information on Twitter, and MULTIPIT EXPERT , an expert annotated set of 5.5K sentence pairs using a stricter definition that is more suitable for acquiring paraphrases for generation purpose. Compared to PIT-2015 and Twitter-URL, our corpus contains more than twice as much data with more balanced topic distribution and better annotation quality. Two sets of examples from MULTIPIT are shown in Figure 1.\n\nWe extensively evaluate several state-of-the-art neural language models on our datasets to demonstrate the importance of having task-specific paraphrase definition. Our best model achieves 84.2 F 1 for automatic paraphrase identification. In addition, we construct a continually growing paraphrase dataset, MULTIPIT AUTO , by applying the automatic identification model to unlabelled Twitter data. Empirical results and analysis show that generation models fine-tuned on MULTIPIT AUTO generate more diverse and high-quality paraphrases compared to models trained on other corpora, such as MSCOCO (Lin et al., 2014), ParaNMT (Wieting andGimpel, 2018), and Quora. 2 We hope our MULTIPIT corpus will facilitate future innovation in paraphrase research."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 218974555, "title": "Towards a Spell Checker for Zamboanga Chavacano Orthography", "paper_id": "6ccfd94b65aa1dc038389b1343c3ae6fb719d166", "abstract": "Zamboanga Chabacano (ZC) is the most vibrant variety of Philippine Creole Spanish, with over 400,000 native speakers in the Philippines (as of 2010). Following its introduction as a subject and a medium of instruction in the public schools of Zamboanga City from Grade 1 to 3 in 2012, an official orthography for this variety - the so-called \u201cZamboanga Chavacano Orthography\u201d - has been approved in 2014. Its complexity, however, is a barrier to most speakers, since it does not necessarily reflect the particular phonetic evolution in ZC, but favours etymology instead. The distance between the correct spelling and the different spelling variations is often so great that delivering acceptable performance with the current de facto spell checking technologies may be challenging. The goals of this research have been to propose i) a spelling error taxonomy for ZC, formalised as an ontology and ii) an adaptive spell checking approach using Character-Based Statistical Machine Translation to correct spelling errors in ZC. Our results show that this approach is suitable for the goals mentioned and that it could be combined with other current spell checking technologies to achieve even higher performance.", "intro": "According to a recent report from Komisyon sa Wikang Filipino (KWF, 2015), the Philippines is home to 135 living languages, among which Chabacano is the only creole language. In the 2010 Census of Population and Housing (CPH), 405,798 people claimed Zamboanga Chabacano (ZC, locally known simply as \"Chavacano\" or \"Zamboangue\u00f1o\") as their mother tongue, thus making it the most widely spoken variety of Philippine Creole Spanish in the country, and the only one still experiencing natural growth (National Statistics Office (NSO), 2003a, 2014a; Philippine Statistics Authority (PSA), 2014). With the implementation of the K-12 curriculum and the Mother-Tongue Based Multilingual Education (MTB-MLE) program nationwide in 2012, ZC is nowadays taught as a subject and is used as a medium of instruction in the public schools of Zamboanga City from Grade 1 to 3 (Government of the Philippines, 2011). This has motivated the local government to invest in the standardisation of ZC, resulting in the approval of an official orthography (Zamboanga Chavacano Orthography) in 2014, followed by the publication of a basic grammar, as well as some collections of texts and children's books; even a normative dictionary was published in late 2018. Despite all the official initiatives in the last few years, the testimonials of many Zamboangue\u00f1os suggest that there might be no room for optimism. ZC has been facing a multiglossic situation for decades and there are evidences that it is no longer the preferred language for socialisation among the younger generations, which raises some concerns regarding its future (Himoro, 2019). Besides (see Table 1), although the absolute number of native ZC speakers is still increasing, the census figures show that this increase is not proportional to the population growth, since the ratio of native ZC speakers in the Zamboanga City population is continuously decreasing. The on-going standardisation process is thus vital to enable the language to be used efficiently in higher written registers and counterbalance the  1970, 1980, 1990, 2000and 2010CPH. (National Census and Statistics Office (NCSO), 1974, 1983National Statistics Office (NSO), 1992, 2003b, 2014b hegemony of the official languages, namely English and Tagalog. The approval of the orthography, nevertheless, has raised some other issues. The first of them regards its applicability: the main principle of the orthography is that words should be written according to their original form in the language of origin (DepEd Zamboanga City Division, 2016), which means that the spelling deemed as correct does not always reflect the phonetic evolution of the language, requiring prior study and memorisation work from the writers. The second one is basically technical: the gap between the different spelling systems currently in use in ZC and the orthography is often so big that the current de facto spell checking technologies are unable to yield acceptable results. This research aims at addressing these issues and contributing to the standardisation of ZC by proposing i) a spelling error taxonomy for ZC formalised as an ontology and ii) an adaptive spell checking approach using Character-Based Statistical Machine Translation to correct spelling errors in ZC. We have also implemented a hunspell spell checker to be used as a baseline to evaluate our approach. We argue that this solution presents the following advantages over hunspell: (i) it can correct previously unseen words by recognising some patterns, and (ii) it has a better performance when not dealing with simple spelling errors, but rather with different writing systems, as is the case of ZC."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 202788575, "title": "Fine-Grained Analysis of Propaganda in News Article", "paper_id": "a12d22ff91ce159a0d3558ed5aaed115115beabd", "abstract": "Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.", "intro": "Research on detecting propaganda has focused primarily on articles (Barr\u00f3n-Cedeno et al., 2019;Rashkin et al., 2017). In many cases, there are no labeled data for individual articles, but there are such labels for entire news outlets. Thus, often all articles from the same news outlet get labeled the way that this outlet is labeled. Yet, it has been observed that propagandistic sources could post objective non-propagandistic articles periodically to increase their credibility (Horne et al., 2018). Similarly, media generally recognized as objective might occasionally post articles that promote a particular editorial agenda and are thus propagandistic. Thus, it is clear that transferring the label of the news outlet to each of its articles, could introduce noise. Such labels can still be useful for training robust systems, but they cannot be used to get a fair assessment of a system at testing time.\n\nOne option to deal with the lack of labels for articles is to crowdsource the annotation. However, in preliminary experiments we observed that the average annotator cannot detach her personal mindset from the judgment of propaganda and bias, i.e., if a clearly propagandistic text expresses ideas aligned with the annotator's beliefs, it is unlikely that she would judge it as such.\n\nWe argue that in order to study propaganda in a sound and reliable way, we need to rely on highquality trusted professional annotations and it is best to do so at the fragment level, targeting specific techniques rather than using a label for an entire document or an entire news outlet.\n\nOurs is the first work that goes at a fine-grained level: identifying specific instances of propaganda techniques used within an article. In particular, we create a corresponding corpus. For this purpose, we asked six experts to annotate articles from news outlets recognized as propagandistic and non-propagandistic, marking specific text spans with eighteen propaganda techniques. We also designed appropriate evaluation measures. Taken together, the annotated corpus and the evaluation measures represent the first manually-curated evaluation framework for the analysis of finegrained propaganda. We release the corpus (350K tokens) as well as our code in order to enable future research. 1 Our contributions are as follows:\n\n\u2022 We formulate a new problem: detect the use of specific propaganda techniques in text.\n\n\u2022 We build a new large corpus for this problem.\n\n\u2022 We propose a suitable evaluation measure.\n\n\u2022 We design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.\n\nOur corpus could enable research in propagandistic and non-objective news, including the development of explainable AI systems. A system that can detect instances of use of specific propagandistic techniques would be able to make it explicit to the users why a given article was predicted to be propagandistic. It could also help train the users to spot the use of such techniques in the news. The remainder of this paper is organized as follows: Section 2 presents the propagandistic techniques we focus on. Section 3 describes our corpus. Section 4 discusses an evaluation measures for comparing labeled fragments. Section 5 presents the formulation of the task and our proposed models. Section 6 describes our experiments and the evaluation results. Section 7 presents some relevant related work. Finally, Section 8 concludes and discusses future work."}
{"tabids": ["b9e280ca-7a97-4486-b953-059737120e77"], "corpus_id": 253157749, "title": "arXivEdits: Understanding the Human Revision Process in Scientific Writing", "paper_id": "758f216e2370bd517eeda23c2d7db963a95104f9", "abstract": "Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.", "intro": "Writing is essential for sharing scientific findings. Researchers devote a huge amount of effort to revising their papers by improving the writing quality or updating new discoveries. Valuable knowledge is encoded in this revision process. Up to January 1st, 2022, arXiv (https://arxiv.org/), an open access eprint service, has archived over 1.9 million papers, among which more than 600k papers have multiple versions available. This provides an amazing data * Work done as an undergraduate student. source for studying text revision in scientific writing. Specifically, revisions between different versions of papers contain valuable information about logical and structural improvements at documentlevel, as well as stylistic and grammatical refinements at sentence-and word-levels. It also can support various natural language processing (NLP) applications, including writing quality assessment and error correction (Louis and Nenkova, 2013;Xue and Hwa, 2014;Daudaravicius et al., 2016;Bryant et al., 2019), text simplification and compression (Xu et al., 2015;Filippova et al., 2015), style transfer (Xu et al., 2012;Krishna et al., 2020), hedge detection (Medlock and Briscoe, 2007), and paraphrase generation (Dou et al., 2022).\n\nIn this paper, we present a complete solution for studying the human revision process in the scientific writing domain, including annotated data, analysis, and system. We first construct ARXIVEDITS, which consists of 751 full arXiv papers with gold sentence alignment across their multiple versions of revisions, as shown in Figure 1. Our corpus spans 6 research areas, including physics, mathematics, computer science, quantitative biology, quantitative finance, and statistics, published in 23 years (from 1996 to 2019). To the best of our knowledge, this is the first text revision corpus that covers full multi-page research papers. To study sentence-level revision, we manually annotated fine-grained edits and their underlying intentions that reflect why the edits are being made for 1,000 sentence pairs, based on a taxonomy that we developed consisting of 7 categories.\n\nOur dataset addresses two major limitations in prior work. First, previous researchers mainly focus on the abstract (G\u00e1bor et al., 2018;Kang et al., 2018;Du et al., 2022) and introduction (Tan and Lee, 2014;Mita et al., 2022) sections, limiting the generalizability of their conclusions. In addition, a sentence-level revision may consist of multiple fine-grained edits made for different purposes (see"}
{"tabids": ["d94c8049-6696-4812-b3dc-1da0df7ffaee"], "corpus_id": 218862749, "title": "Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning", "paper_id": "123531e446449d92e8cbfbfb1733e26defd89a23", "abstract": "In this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.", "intro": "The vast majority of existing named entity recognition (NER) methods focus on a small set of prominent entity types, such as persons, organizations, diseases, and genes, for which labeled datasets are readily available (Tjong Kim Sang and De Meulder, 2003;Smith et al., 2008;Weischedel et al., 2011;Li et al., 2016). There is a marked lack of studies in many other domains, such as e-commerce, and for novel entity types, e.g. products and components.\n\nThe lack of annotated datasets in the ecommerce domain makes it hard to apply supervised NER methods. An alternative approach is to use dictionaries (Nadeau et al., 2006;Yang et al., 2018), but freely available knowledge resources, e.g. Wikidata (Vrande\u010dic and Kr\u00f6tzsch, 2014) or YAGO (Suchanek et al., 2007), contain only very limited information about e-commerce entities. Manually creating a dictionary of sufficient quality and coverage would be prohibitively expensive. This is amplified by the fact that in the e-commerce domain, entities are frequently ex-pressed as complex noun phrases instead of proper names. Product and component category terms are often combined with brand names, model numbers, and attributes (\"hard drive\" \u2192 \"SSD hard drive\" \u2192 \"WD Blue 500 GB SSD hard drive\"), which are almost impossible to enumerate exhaustively. In such a low-coverage setting, employing a simple dictionary-based approach would result in very low recall, and yield very noisy labels when used as a source of labels for a supervised machine learning algorithm. To address the drawbacks of dictionary-based labeling, Peng et al. (2019) propose a positive-unlabeled (PU) NER approach that labels positive instances using a seed dictionary, but makes no label assumptions for the remaining tokens (Bekker and Davis, 2018). The authors validate their approach on the CoNLL, MUC and Twitter datasets for standard entity types, but it is unclear how their approach transfers to the ecommerce domain and its entity types.\n\nContributions We adopt the PU algorithm of Peng et al. (2019) to the domain of consumer electronic product descriptions, and evaluate its effectiveness on four entity types: Product, Component, Brand and Attribute. Our algorithm bootstraps NER with a seed dictionary, iteratively labels more data and expands the dictionary, while accounting for accumulated errors from model predictions. During labeling, we utilize dependency parsing to efficiently expand dictionary matches in text. Our experiments on a novel dataset of product descriptions show that this labeling mechanism, combined with a PU learning strategy, consistently improves F1 scores over a standard BiLSTM classifier. Iterative learning quickly expands the dictionary, and further improves model performance. The proposed approach exhibits much better recall than the baseline model, and generalizes better to unseen entities.\n\nAlgorithm 1: Iterative Bootstrapping NER Input: Dictionary D seed , Corpus C, threshold K, max iterations I Result: Dictionary D + , Classifier L D + \u2190 D seed ; C dep \u2190 dependency parse(C); i \u2190 0; while not converged(D + ) and i < I do C lab \u2190 label(C, D + ); C exp \u2190 expand labels(C lab , C dep ); L \u2190 train classif ier(C exp ); C pred \u2190 predict(C exp , L); for e \u2190 C pred do if e / \u2208 D + and freq(e) > K then D + \u2190 add entity(D + , e); end end i \u2190 i + 1; end"}
{"tabids": ["d94c8049-6696-4812-b3dc-1da0df7ffaee"], "corpus_id": 216867120, "title": "The Effect of Natural Distribution Shift on Question Answering Models", "paper_id": "2fe6759b0e9757df70ca3db1e1dd9bd1c5a5bda5", "abstract": "We build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. Our first test set is from the original Wikipedia domain and measures the extent to which existing systems overfit the original test set. Despite several years of heavy test set re-use, we find no evidence of adaptive overfitting. The remaining three test sets are constructed from New York Times articles, Reddit posts, and Amazon product reviews and measure robustness to natural distribution shifts. Across a broad range of models, we observe average performance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.", "intro": "Since its release in 2016, the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has generated intense interest from the natural language processing community. At first glance, this intense interest has lead to impressive results. The best performing models in 2020  have F1 scores more than 40 points higher than the baseline presented by Rajpurkar et al. (2016). At the same time, it remains unclear to what extent progress on these benchmark numbers is a reliable indicator of progress more broadly.\n\nThe goal of building question answering systems is not merely to obtain high scores on the SQuAD leaderboard, but rather to generalize to new examples beyond the SQuAD test set. However, the competition format of SQuAD puts pressure on the validity of leaderboard scores. It is well-known that repeatedly evaluating models on a held-out test set can give overly optimistic estimates of model performance, a phenomenon known as adaptive overfitting Dwork et al. (2015). Moreover, the standard SQuAD evaluation only measures model performance on new examples from a single distribution, i.e., paragraphs derived from Wikipedia articles. Nevertheless, we often use models in settings different from the one in which they were trained. While Jia and Liang (2017) demonstrated that SQuAD models are not robust to adversarial distribution shifts, one might still hope that the models are more robust to natural distribution shifts, for instance changing from Wikipedia to newspaper articles.\n\nThis state of affairs raises two important questions:\n\nAre SQuAD models overfit to the SQuAD test set?\n\nAre SQuAD models robust to natural distribution shifts? In this work, we address both questions by replicating the SQuAD dataset creation process and generating four new SQuAD test sets on both the original Wikipedia domain, as well as three new domains: New York Times articles, Reddit posts, and Amazon product reviews.\n\nWe first show that there is no evidence of adaptive overfitting on SQuAD. Across a large collection of SQuAD models, there is little to no difference between the F1 scores from the original SQuAD test set and our replication. This even holds when comparing scores from the SQuAD development set (which was publicly released with answers) to our new test set. The lack of adaptive overfitting is consistent with recent replication studies in the context of image classification ; Yadav and Bottou (2019). These studies leave open the possibility that this phenomenon is specific to the data or models typical in computer vision research. Our result demonstrates this same phenomenon also holds for natural language processing.\n\nBeyond adaptive overfitting, we also demonstrate that SQuAD models exhibit robustness to some of our natural distribution shifts, though they still suffer substantial performance degradation on others. On the New York Times dataset, models in our testbed on average drop 3.8 F1 points. On the Reddit and Amazon datasets, the drop is on average 14.0 and 17.4 F1 points, respectively. All of our datasets were collected using the same data generation pipeline, so this degradation can be attributed purely to changes in the source text rather than differences in the annotation procedures across datasets.\n\nWe complement each of these experiments with a strong human baseline comprised of the authors of this paper. On the original SQuAD data, our human accuracy numbers are on par with the best SQuAD models  and significantly better than the Mechanical Turk baseline reported by Rajpurkar et al. (2016). On our new test sets, average human F1 scores decrease by 0.1 F1 on New York Times, 2.9 on Reddit, and 3.0 on Amazon. All of the resulting F1 scores are substantially higher than the best SQuAD models on the respective test sets. Figure 1 summarizes the main results of our experiments. Humans show consistent behavior on all four test sets, while models are substantially less robust against two of the distribution shifts. Although there has been steady progress on the SQuAD leaderboard, there has been markedly less progress in this robustness dimension.\n\nTo enable future research, all of our new tests sets are freely available online. 1"}
{"tabids": ["d94c8049-6696-4812-b3dc-1da0df7ffaee"], "corpus_id": 61955135, "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "paper_id": "2ff4b9ca75a37cec1607f1f49333b948b1c8d08b", "abstract": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval-2014 Task 4 aimed to foster research in the \ufb01eld of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.", "intro": "With the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span (Pang et al., 2002;Turney, 2002). However, the need for a more fine-grained approach, such as aspect-based (or 'feature-based') sentiment analysis (ABSA), soon became apparent (Liu, 2012). For example, laptop reviews not only express the overall sentiment about a specific model (e.g., \"This is a great This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ laptop\"), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc. Subsequently, a review may convey opposing sentiments (e.g., \"Its performance is ideal, I wish I could say the same about the price\") or objective information (e.g., \"This one still has the CD slot\") for different aspects of an entity.\n\nABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005;Titov and McDonald, 2008;Hu and Liu, 2004a;Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009;Brody and Elhadad, 2010).\n\nPrevious publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks. The restaurant reviews dataset of Ganu et al. (2009) uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral). Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, \"The restaurant was expensive, but the menu was great.\" would be assigned the aspect PRICE with negative polarity and FOOD with positive polarity. In the product reviews dataset of Hu and Liu (2004a;2004b), aspect terms, i.e., terms naming aspects (e.g., 'radio', 'voice dialing') together with strength scores (e.g., 'radio': +2, 'voice dialing': \u22123) are pro-vided. No predefined inventory of aspects is provided, unlike the dataset of Ganu et al.\n\nThe SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see Section 2). Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice."}
{"tabids": ["d94c8049-6696-4812-b3dc-1da0df7ffaee"], "corpus_id": 221703022, "title": "Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product", "paper_id": "97679b9616045bfa3723db15f360cf9f2c8b52ad", "abstract": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset will be released to the public.", "intro": "Product attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015;Yu et al., 2017), product recommendations (Gong,  2009; Cao et al., 2018), and product retrieval (Liao et al., 2018;Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers' shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1.\n\nThough plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011;More, 2016;Shinzato and Sekine, 2013;Zheng et al., 2018;Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task."}
{"tabids": ["d94c8049-6696-4812-b3dc-1da0df7ffaee"], "corpus_id": 247292113, "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval", "paper_id": "4529a0d6ba1720ce57ad81cc5c74ccca403262f7", "abstract": "Passage retrieval is a fundamental task in information retrieval (IR) research, which has drawn much attention recently. In the English field, the availability of large-scale annotated dataset (e.g, MS MARCO) and the emergence of deep pre-trained language models (e.g, BERT) has resulted in a substantial improvement of existing passage retrieval systems. However, in the Chinese field, especially for specific domains, passage retrieval systems are still immature due to quality-annotated dataset being limited by scale. Therefore, in this paper, we present a novel multi-domain Chinese dataset for passage retrieval (Multi-CPR). The dataset is collected from three different domains, including E-commerce, Entertainment video and Medical. Each dataset contains millions of passages and a certain amount of human annotated query-passage related pairs. We implement various representative passage retrieval methods as baselines. We find that the performance of retrieval models trained on dataset from general domain will inevitably decrease on specific domain. Nevertheless, a passage retrieval system built on in-domain annotated dataset can achieve significant improvement, which indeed demonstrates the necessity of domain labeled data for further optimization. We hope the release of the Multi-CPR dataset could benchmark Chinese passage retrieval task in specific domain and also make advances for future studies.", "intro": "Large scale passage retrieval is an important problem in information retrieval research field. Passage retrieval is often regarded Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '22, July 11-15, 2022 [26,30], machine reading comprehension [36,43] and web search systems [4], etc. Recent advances in deep learning have allowed state of the art performance on passage retrieval task compared to conventional statistical models [15-17, 26, 42]. However, these deep neural models usually contain millions of parameters that necessitate a large amount of training data. As such, high-quality public available benchmark dataset is critical for research progress with a deep-model fashion for the passage retrieval task.\n\nIn the English field, we observed that large, high-quality dataset enables the community rapidly develop new models for passage retrieval task, and at the same time, the research on model architecture also obtains a more deep understanding. As mentioned above, passage retrieval mainly serves downstream tasks such as question answering and machine reading comprehension. Therefore, existing datasets are also constructed based on the above two tasks. In term of question answering, there are several benchmark datasets like TREC QA [48], WikiPassageQA [6] and InsuranceQA [14]. For machine reading comprehension task, representative datasets including SQuAD [44], MS MARCO [5], CNN /Daily News [23] provide good benchmarks. In summary, dataset in the English field is relatively mature in terms of data scale and domain richness. On the other hand, in the field of Chinese, although some information retrieval datasets and machine reading comprehension datasets have been released in recent years like Sogou-QCL [55], Dureader [22] and SC-MRC [8], these datasets are mainly concentrated in the general domain, and dataset that can be adopted for specific domain passage retrieval research is still in shortage.\n\nTo push forward the quality and variety of Chinese passage retrieval dataset, we present Multi-CPR. There are three main properties of Multi-CPR: a) Multi-CPR is the first dataset that covers multiple specific domains for Chinese passage retrieval, including E-commerce, Entertainment video and Medical. There is a high degree of differentiation within the three domains. Furthermore, Only one (Medical) of these domains has been studied in previous research [54]. b) Multi-CPR is the largest domain specific Chinese passage retrieval dataset. For each domain, Multi-CPR contains millions of passages (e,g. 1,002,822 passages for the E-commerce domain) and sufficient human annotated query-passage related pairs. More detailed statistics of Multi-CPR and annotated examples can be found in Table 4 and Section 3.2. c) All Queries and passages in Multi-CPR are collected from real search engine systems within Alibaba Group. The authenticity of the samples allows Multi-CPR to meet the needs of both academia and industry fields.\n\nAs an attempt to tackle Multi-CPR and provide strong baselines, we implement various representative passage retrieval methods including both sparse and dense models. For the sparse models, except for the basic BM25 method, we also verified that previously proposed optimization methods based on the sparse strategy can indeed achieve significant improvement compared to the BM25 baseline (e,g, doc2query method). For the dense models, we mainly implemented methods based on the DPR model. Similarly, we also made some optimizations based on the DPR model. Compared to the sparse models, we found that the retrieval performance of the dense models trained on labeled dataset can be greatly improved. This observation empirically confirms the value of annotated data. In further, we verified that the retrieval-then-reranking two-stage framework based on the BERT model can further improve the overall retrieval performance on all three datasets in Multi-CPR, which once again corroborates the quality of Multi-CPR.\n\nIn summary, the major contributions of this paper are threefold:\n\n\u2022 We present Multi-CPR, the largest-scale Chinese multi domain passage retrieval dataset collected from practical search engine systems, and it covers E-commence, Entaitement vedio and Medical domain. \u2022 We conduct an in-depth analysis on Multi-CPR. Based on Multi-CPR, we have analyzed the characteristics of different passage retrieval methods along with their optimization strategies associated, which enables us to have a deeper understanding of Chinese passage retrieval task in specific domain. \u2022 We implement various representative methods as baselines and show the performance of existing methods on Multi-CPR, which provides an outlook for future research."}
{"tabids": ["d94c8049-6696-4812-b3dc-1da0df7ffaee"], "corpus_id": 260887693, "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce", "paper_id": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e", "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.", "intro": "In the field of E-commerce, the progress made in natural language processing (NLP) and deep learning (DL) has significantly contributed to the advancement of E-commerce technology. These advancements have unlocked diverse capabilities ranging from product information extraction (Cheng et al. 2021;Wang et al. 2021) to user query understanding (Zhao, Chen, and Yin 2019;Ahmadvand et al. 2020). Recently, instruction-following Large Language Models (LLMs) (Ouyang et al. 2022;Taori et al. 2023;Chiang et al. 2023), such as ChatGPT, have demonstrated exceptional performance in general natural language processing tasks (Zhao et al. 2023). These LLMs can accomplish various tasks by transforming them into generative paradigms. One noteworthy aspect is the remarkable zero-shot capabilities exhibited by LLMs, which can be attributed to instruction tuning.\n\nHowever, despite their numerous merits, general LLMs are not specifically designed for the E-commerce sector. This can lead to suboptimal performance for various E-commerce tasks. Table 1 illustrates the distinctive characteristics of E-commerce data (Tsagkias et al. 2021;Jiang et al. 2022) compared to general domains. Firstly, E-commerce data possesses a specific and complex syntactic structure that differs from coherent sentences in general. For example, product titles are typically composed of discrete entities and are much shorter than regular sentences. Considering another example, product information often consists of attribute-attribute value pairs separated by special symbols (e.g., \"##\"), which also poses challenges for general LLMs to comprehend. Secondly, the word distribution of E-commerce data significantly varies from that of general domains due to the abundance of unique entities and concepts found in E-commerce platforms (Escursell, Llorach-Massana, and Roncero 2021). Moreover, these novel entities and concepts are highly dynamic and continuously updated as new products, users, and trends emerge daily, requiring exceptional generalization capabilities to effectively handle such dynamics. Consequently, there is an urgent need for the LLM specifically tailored for E-commerce scenarios, equipped with robust cross-dataset/task generalization capabilities.\n\nIn the BERT era, numerous efforts (Zhang et al. 2021;Qiu et al. 2022;Xu et al. 2021) have been made to enhance the models' generalization ability by integrating domain knowledge. For instance, E-BERT (Poerner, Waltinger, and Sch\u00fctze 2020) further pre-trains BERT on the Amazon dataset to incorporate semantic knowledge of the E-commerce domain into BERT. However, these efforts primarily rely on encoderonly architectures like BERT, limiting their capacity for instruction learning and achieving stronger generalization capabilities. Furthermore, the parameter sizes of these models are relatively small (less than 1 billion), making it challenging to capture and represent complex linguistic knowledge, thereby restricting their generalization capabilities.\n\nTo enhance models' generalization ability cross dataset/tasks, this work presents the first E-commerce instruction dataset, EcomInstruct, comprising a total of 2.5 million instruction data and 134 tasks. EcomInstruct are built from two main sources. Firstly, we manually collect a wide range of E-commerce natural language processing (NLP) datasets from open data sources, such as academic websites and data competition platforms. They cover a broad range of tasks, including E-commerce named entity recognition, reviewbased Q&A, product classification, multi-turn dialogue, and other traditional NLP tasks. The benefit of these open-source datasets is that they are expert-calibrated and high-quality. Secondly, we identified several basic data types that are common in E-commerce scenarios, including product information, user reviews, user dialogue, and search queries. Around these basic data types, we build a large number of atomic tasks. Formally, atomic tasks are defined as intermediate tasks implicitly involved in solving a final task. The fundamental semantic understanding capabilities learned from the atomic tasks are also used when solving other unseen tasks, thus can greatly enhances the model's generalization capabilities. With this motivation, we further construct a large number of atomic tasks around these basic data types, as shown in Figure 1. Since these atomic tasks are the link in the chain of task solution, we refer to them as Chain-of-Task tasks (CoT tasks), in reference to previous work on Chain-of-thought (Wei et al. 2022;Wang et al. 2022a). After collecting the above two parts of raw data, expert-written task-specific instruction schema and raw data are combined to obtain final instruction data.\n\nBy training the backbone model BLOOMZ with EcomInstruct, we developed the instruction-following LLM EcomGPT for E-commerce. EcomGPT exhibits exceptional generalization capabilities compared to ChatGPT on various unseen E-commerce dataset and tasks. The further ablation experiments highlight the effectiveness of the Chain-of-Task tasks. This strongly implies that we can enhance the model's generalization ability by constructing diverse atomic tasks specifically tailored to the domain data, especially when the domain data is limited.\n\nIn summary, the contributions of this work are threefold:\n\nLang. Task Para. # task # train inst. # test inst. CLS  15  130,596  34,189  Ext  15  82,397  47,284  Gen  22  353,486  96,585  Other  10  61,756  36,481   ZH   CLS  18  324,062  362,845  Ext  9  131,814  54,725  Gen  37  444,503  353,486  Other  8  111,814  36,481   ALL  134  1,533,300  1,023,076   Table 2: Statistics for EcomInstruct."}
{"tabids": ["3f8046c8-4fd5-40aa-a10f-b06c1fdfb796"], "corpus_id": 258987676, "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions", "paper_id": "9b58ef6e29cccbc23578078e321f65848fde0335", "abstract": "Style transfer TTS has shown impressive performance in recent years. However, style control is often restricted to systems built on expressive speech recordings with discrete style categories. In practical situations, users may be interested in transferring style by typing text descriptions of desired styles, without the reference speech in the target style. The text-guided content generation techniques have drawn wide attention recently. In this work, we explore the possibility of controllable style transfer with natural language descriptions. To this end, we propose PromptStyle, a text prompt-guided cross-speaker style transfer system. Specifically, PromptStyle consists of an improved VITS and a cross-modal style encoder. The cross-modal style encoder constructs a shared space of stylistic and semantic representation through a two-stage training process. Experiments show that PromptStyle can achieve proper style transfer with text prompts while maintaining relatively high stability and speaker similarity. Audio samples are available in our demo page.", "intro": "Text-to-speech (TTS) [1,2] aims to produce human-like speech from input text. Recent progress in deep learning approaches has greatly improved the naturalness of speech [3]. With the wide applications of TTS in real-world human-computer interaction, expressive TTS with diverse styles attracts more attention. Generating stylistic speech for a specific speaker intuitively needs the same speaker's high-quality expressive speech recordings, which incurs a high cost for data collection. To solve the problem of synthesizing expressive speech for the target speaker without diverse speaking styles, cross-speaker style transfer [4,5,6,7,8] is a feasible solution.\n\nFor the style representations in style transfer scenarios, existing works mainly include two different methods, i.e. the predefined style id category index [9,10,11,12] and hidden variables extracted from the reference signal [13,14,15,16,17]. However, the id-based methods are limited to the styles of fixed discrete categories, which leads to less flexibility. Although the reference-based methods can produce various speech through different references, the extracted style representation is not interpretable. Moreover, in practical applications, it is difficult to accurately and conveniently select an appropriate reference for arbitrary textual content.\n\nWith the success of text and image generation from prompt descriptions [18,19], some prompt-based TTS methods are pro-posed to improve the expressiveness and naturalness of synthetic speech. Style-Tagging-TTS (ST-TTS) [20] produces expressive speech based on style tags, which are stylistic words or phrases labeled from audiobook datasets. But it is difficult to describe complex styles by a single word or phrase, and the style tags are hard to label in common audiobook datasets as most utterances may be recorded in a less-expressive reading style. PromptTTS [21] proposes to use a style prompt from five different factors (i.e. gender, pitch, speaking speed, volume, and emotion) to guide the style expression for the generated speech. The recent InstructTTS [22] can synthesize stylistic speech with the guidance of natural language descriptions without formal constraints as style prompts. It's a three-stage training approach to capture semantic information from natural language style prompts as conditioning to the TTS system. Intuitively, natural language description is a convenient and userfriendly way to describe the desired style since no prior acoustic knowledge is required.\n\nIn this study, we focus on style transfer in the audiobook generation, where the target speaker has little expressive data and no style description prompts. Through other expressive data and transferring diversified styles with natural language descriptions, expressive audiobook speech can be generated for the target speaker. Specifically, this paper proposes to leverage natural language description prompts to transfer style from the source speaker to the target speaker who has no expressive speech data. The proposed approach -PromptStyle -has a two-stage procedure utilizing text prompts to model the style appearance. In the first stage, based on an improved VITS [23], we use a style encoder to extract a style hidden representation from reference speech as the condition of the TTS system. Multi-speaker multistyle expressive data without style annotation is involved in this stage to achieve cross-speaker style transfer through diverse reference speech. In the second stage, we design a prompt encoder to model the style embedding from the style prompt. Expressive speech data with style annotations in natural language descriptions is involved to fine-tune the pre-trained language model and TTS acoustic model, capturing the relationship between prompt embedding and style embedding space. Due to the generalization capability of the language model, style transfer from unseen prompts is also feasible.\n\nWe summarize the contributions of PromptStyle as follows.\n\n\u2022 We propose a two-stage TTS approach for cross-speaker style transfer with natural language descriptions, which is more user-friendly and controllable than previous works. \u2022 The proposed two-stage approach first uses a large amount of data without annotations to train a reference-based style transfer TTS model, and then leverages only a small amount of labeled data with style prompts to fine-tune a prompt en-"}
{"tabids": ["3f8046c8-4fd5-40aa-a10f-b06c1fdfb796"], "corpus_id": 256416291, "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt", "paper_id": "8a328010eb04cebc675d6d0966cf858492429757", "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g.,\"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.", "intro": "T EXT-to-speech (TTS) aims to generate human-like speech from input text, which attracts broad interest in the audio and speech processing community. Nowadays, the state-of-the-art TTS systems [1]- [3] are able to produce natural and high-quality speech. However, there still exists a big gap between TTS-synthetic speech and human speech in terms of expressiveness, which limits the broad applications of current speech synthesis systems. Many researchers now focus on a more challenging task, i.e., expressive TTS, which aims to model and control the speaking style (e.g., emotion, speaking-rate and so on) in the generated speech according to human's demands. We note that there are generally two types of methods in the literature to learn speaking style information: Dongchao Yang and Helen Meng are with the Chinese University of Hong Kong. This work was done when Dongchao Yang was an intern at Tencent AI Lab. * denotes equal contribution with order determined by alphabetic order. Songxiang Liu and Chao Weng are with Tencent AI Lab. Rongjie Huang is with the Zhejiang University, China. Songxiang Liu is the corresponding author. 1 http://dongchaoyang.top/InstructTTS/ one type uses auxiliary categorical style labels as the condition of the framework [4], [5], the other imitates the speaking style of a reference speech [6]- [9]. However, there are limitations in the diversity of expressiveness when categorical style labels are used, as these models can only generate a few pre-defined styles from the training set. While TTS models that use a reference utterance to generate a particular speaking style can be trained in an unsupervised manner and are generalizable to out-of-domain speaking styles, the style information extracted from the reference speech may not be easily understandable or interpretable. Additionally, it can be challenging to select a reference speech sample that precisely matches a user's requirements.\n\nFor the first time, we study the modelling of expressive TTS with style prompt in natural language, where we meet with the following research problems: (1) how to train a language model that can capture semantic information from the natural language prompt and control the speaking style in the generated speech; (2) how to design an acoustic model to effectively model the challenging one-to-many learning problem of expressive TTS. In this paper, we will address these two challenges.\n\nThe main contributions of this study are summarized as follows:\n\n(1) For the first time, we study the modelling of expressive TTS with natural language prompts, which brings us a step closer to achieving user-controllable expressive TTS.\n\n(2) We introduce a novel three stage training strategy to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts.\n\n(3) We propose to model acoustic features in discrete latent space and cast speech synthesis as a sequence-to-sequence language modeling task. Specifically, we train a novel discrete diffusion model to generate vector-quantized (VQ) acoustic features rather than to predict the commonly-used hand-crafted intermediate acoustic features, such as the mel-spectrogram. (4) We explore to model two types of VQ acoustic features: mel-spectrogram based VQ features and waveform-based VQ features. We demonstrate that the two types of VQ features can be effectively modeled by our proposed novel discrete diffusion model. Our waveform-based modelling method only needs one-stage training, and it is a non-autoregressive model, which is far different from the concurrent work VALL-E [10] and MusicLM [11]. (5) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.  Where SALN denotes the style-adaptive layer normalization adaptor [12]. (b) shows the details of our proposed style encoder, which aims to extract style features from GT mel-spectrogram (training stage) or style prompt (inference stage). In Figure 1 (c), we give an example of discrete diffusion decoder to generate VQ mel-spectrogram acoustic features (we name it as Mel-VQ-Diffusion).\n\nThe rest of this paper is organized as follows: In Section II, we motivate our study by introducing the background and related work. In Section III, we present the details of the dataset. In Section IV, we introduce the details of our proposed methods. The experimental setting, evaluation metrics and results are presented from Section V to Section VII. The study is concluded in Section VIII."}
{"tabids": ["3f8046c8-4fd5-40aa-a10f-b06c1fdfb796"], "corpus_id": 253761189, "title": "PromptTTS: Controllable Text-to-Speech with Text Descriptions", "paper_id": "961dad067e4481ed4a3dbe9a7630bdb884952171", "abstract": "Using a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., ''A lady whispers to her friend slowly''). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available.", "intro": "Recent research has achieved great success in text and image generation guided with a text description as prompt [1,2,3,4] (e.g., GPT-3 [5] or DALLE-2 [6]). Beyond text and image generation, there is little research on prompt-based guidance for text-to-speech (TTS) synthesis [7,8] with different styles such as pitch, speaking speed, and emotion. Previous works on controllable TTS focus on controlling specific style factors: prosody control with word-level prosody tags [9], speaking speed control with sentence-level speaking-rate [10], and pitch control with pitch contours [11]. Style control can be achieved in an explicit manner by using the value of style factors such as pitch [12] or an implicit manner by learning a style token [13] from the reference speech. However, all previous works require users to provide the specific value of style factors with acoustic knowledge or choose the reference speech that meets the requirements, which are timeconsuming and not user-friendly. Therefore, it is a better choice if style control is achieved with a text description in natural language.\n\nIn this work, we explore the possibility of leveraging a text description (denoted as prompt) to guide speech synthesis.\n\nTo be more specific, as shown in Table 1, the input prompt consists of a style description (denoted as style prompt) and a content description containing the text to be converted to speech (denoted as content prompt) with a colon in between. For example, an input prompt, \"A lady whispers to her friend slowly: everything will go fine, right?\", means that the model needs to synthesize the speech with content of \"everything will go fine, right?\" in a female voice, a slow speaking speed, and a whispering manner. In this way, users are able to create speech from a prompt, resulting in style control without the requirements for acoustic knowledge or reference speech.\n\nAs the first exploration on TTS guided with prompts, there not exist datasets or systems for this task. Thus, we design a dataset, a system, and an evaluation metric for this task.\n\n(1) Dataset: we construct and release a dataset containing prompts with style and content information and the corresponding speech. The prompts describe the speech in 5 style factors including gender, pitch, speaking speed, volume, and emotion. (2) System: to synthesize speech according to a prompt, we propose PromptTTS to serve as baseline for future research in this task, which consists of a style encoder, a content encoder, and a speech decoder. The style and content encoders extract style and content representations from the prompt, respectively. The speech decoder utilizes both representations to synthesize speech accordingly. (3) Evaluation metric: we calculate the accuracy between the style factors from output speech and those from prompts as evaluation metric for style control.  The contributions of our work include: (1) we propose PromptTTS to synthesize the speech that is consistent with prompts in style and content, which is more user-friendly than previous works; (2) we collect and release a dataset consisting of prompts and the corresponding speech for this task; (3) the experiments show that PromptTTS can generate speech with precise style control and high speech quality."}
{"tabids": ["3f8046c8-4fd5-40aa-a10f-b06c1fdfb796"], "corpus_id": 261242529, "title": "TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models", "paper_id": "9f432d2500758cd1182fe47fb09c2065bf7b9123", "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/", "intro": "In recent years, significant advancements have been made in the field of speech synthesis [1,2,3], with an increasing focus on a more challenging task known as controllable Text-to-Speech (TTS). Previous studies in controllable TTS have predominantly employed either reference audio for style transfer [4] or employed different style factors such as speaking rate [5], pitch [6], and prosody [7] for speech control. However, these approaches necessitate users to provide specific \u2020 Corresponding Author: Zhou Zhao, zhaozhou@zju.edu.cn values for style factors which requiring professional acoustic knowledge [5,6,7], or select reference speech [4] that satisfies the desired criteria. These methods are time-consuming and lack user-friendliness. Moreover, the style information derived from the reference speech lacks intuitiveness and interpretability. The effect over these styles is often constrained to the training set of the reference audio, resulting in weak generalization capabilities for unseen styles.\n\nBased on the aforementioned limitations: PromptTTS [8] proposes that it is a preferable choice to achieve style control using a natural language text description. We believe that utilizing natural text descriptions for controlling style in speech is the direction for future development of controllable TTS systems, due to its user-friendliness, generalizability, and interpretability. However, to the best of our knowledge, there is currently no high-quality, large-scale open-source text style prompt speech dataset available for advanced textcontrollable TTS models. In this work, we introduce a novel 330-hour clean text style prompt speech emotion dataset called TextrolSpeech. Each style encompasses 5 style factors and 500 distinct natural language text descriptions. Given the increased demands of controllable TTS systems for speech diversity, we get inspiration from [2] and propose Salle, which employs discrete tokens [9] based Residual Vector Quantization (RVQ) instead of conventional mel spectrograms. The tokens in Salle exhibit a hierarchical structure, where tokens from previous quantizers capture acoustic properties such as speaker identity, while consecutive quantizers learn finegrained acoustic details. Building upon this characteristic, we directly utilize the text style tokens in an autoregressive manner to prompt the generation of the first layer of acoustic tokens. Our contributions can be summarized as follows:\n\n\u2022 We have released TextrolSpeech, an open-source speech emotion dataset that is large-scale, multi-speaker, and enriched with diverse and natural text descriptions. This dataset aims to drive the development of text controllable TTS systems.\n\n\u2022 We provide a detailed account of the creation process of TextrolSpeech, through our experiments, we have devised an efficient prompt programming methodology \u2022 We propose Salle, a multi-stage discrete style tokenguided control framework for TTS language models, which exhibits powerful in-context capabilities."}
{"tabids": ["8b29f176-220d-4c3a-aecd-3ec67a6fb864"], "corpus_id": 227209266, "title": "Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces", "paper_id": "089b79d40cbf3fc7ac3e282aee6fa3c53e1acb42", "abstract": "Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D geometry processing. Several recent state-of-the-art methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce \\textit{Neural-Pull}, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest points on the surface using the predicted signed distance values and the gradient at the query locations, both of which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods.", "intro": "Signed Distance Functions (SDFs) have been an important 3D shape representation for deep learning based 3D shape analysis (Park et al., 2019;Mescheder et al., 2019;Mildenhall et al., 2020;Michalkiewicz et al., 2019;Saito et al., 2019;Rematas et al., 2021;Sitzmann et al., 2020;Ost et al., Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). 2020; Takikawa et al., 2021;Martel et al., 2021;Oechsle et al., 2021;Azinovic et al., 2021;Dupont et al., 2021), due to their advantages over other representations in representing high resolution shapes with arbitrary topology. Given ground truth signed distance values, it is intuitive to learn an SDF by training a deep neural network to regress signed distance values for query 3D locations, where an image (Michalkiewicz et al., 2019;Park et al., 2019) or a point cloud (Jia & Kyan, 2020;Erler et al., 2020) representing the shape can serve as a condition which is an additional input of the network. It has also been shown how to learn SDFs from multiple 2D images rather than 3D information using differentiable renderers (Liu et al., 2020;Jiang et al., 2020b;Zakharov et al., 2020;Wu & Sun, 2020). In this paper, we address the problem of learning SDFs from raw point clouds and propose a new method that outperforms the state-of-the-art on widely used benchmarks.\n\nCurrent solutions (Gropp et al., 2020;Chibane et al., 2020b;Atzmon & Lipman, 2020a; aim to estimate unsigned distance fields by leveraging additional constraints. The rationale behind these solutions is that an unsigned distance field can be directly learned from the distances between a set of query 3D locations and their nearest neighbors on the 3D point clouds, while the signs of these distances require more information to infer, such as geometric regularization (Gropp et al., 2020), sign agnostic learning (Atzmon & Lipman, 2020a;, or analytical gradients (Chibane et al., 2020b).\n\nIn this paper, we propose a method to learn SDFs directly from raw point clouds without requiring ground truth signed distance values. Our method learns the SDF from a point cloud, or from multiple point clouds with conditions by training a neural network to learn to pull the surrounding 3D space onto the surface represented by the point cloud. Hence we call our method Neural-Pull. Specifically, given a 3D query location as input to the network, we ask the network to pull it to its closest point on the surface using the predicted signed distance value and the gradient at the query location, both of which are calculated by the network itself. The pulling operation is differentiable, and depending on the sign of the predicted distance, it moves the query location along or against the direction of the gradient with arXiv:2011.13495v2 [cs.CV] 23 May 2021 a stride given by the signed distance. Since our training objective involves both the signed distance and its gradient, it leads to highly effective learning. Our experiments using widely used benchmarks show that Neural-Pull can learn SDFs more accurately and flexibly when representing 3D shapes in different applications than previous state-of-theart methods. Our contributions are listed below. i) We introduce Neural-Pull, a novel approach to learn SDFs directly from raw 3D point clouds without ground truth signed distance values.\n\nii) We introduce the idea to effectively learn SDFs by updating the predicted signed distance values and the gradient simultaneously in order to pull surrounding 3D space onto the surface.\n\niii) We significantly improve the state-of-the-art accuracy in surface reconstruction and single image reconstruction under various benchmarks."}
{"tabids": ["8b29f176-220d-4c3a-aecd-3ec67a6fb864"], "corpus_id": 252735231, "title": "Learning Consistency-Aware Unsigned Distance Functions Progressively from Raw Point Clouds", "paper_id": "23e0e88c5abcda4b363f96e3d639ac43a85f5e7e", "abstract": "Surface reconstruction for point clouds is an important task in 3D computer vision. Most of the latest methods resolve this problem by learning signed distance functions (SDF) from point clouds, which are limited to reconstructing shapes or scenes with closed surfaces. Some other methods tried to represent shapes or scenes with open surfaces using unsigned distance functions (UDF) which are learned from large scale ground truth unsigned distances. However, the learned UDF is hard to provide smooth distance fields near the surface due to the noncontinuous character of point clouds. In this paper, we propose a novel method to learn consistency-aware unsigned distance functions directly from raw point clouds. We achieve this by learning to move 3D queries to reach the surface with a field consistency constraint, where we also enable to progressively estimate a more accurate surface. Specifically, we train a neural network to gradually infer the relationship between 3D queries and the approximated surface by searching for the moving target of queries in a dynamic way, which results in a consistent field around the surface. Meanwhile, we introduce a polygonization algorithm to extract surfaces directly from the gradient field of the learned UDF. The experimental results in surface reconstruction for synthetic and real scan data show significant improvements over the state-of-the-art under the widely used benchmarks.", "intro": "Reconstructing surfaces from 3D point clouds is vital in 3D vision, robotics and graphics. It bridges the gap between raw point clouds that can be captured by 3D sensors and the editable surfaces for various downstream applications. Recently, Neural Implicit Functions (NIFs) have achieved promising results by training deep networks to learn Signed Distance Functions (SDFs) [44,26,42,14] or occupancies [40,46,41,10], and then extract a polygon mesh of a continuous iso-surface from a discrete scalar field using the marching cubes algorithm [35]. However, the NIFs approaches based on learning internal and external relations can only reconstruct closed surfaces. The limitation prevents NIFs from representing most real-world objects such as cars with inner structures, clothes with unsealed ends or 3D scenes with open walls and holes.\n\nAs a remedy, state-of-the-art methods [12,62,49] learn Unsigned Distance Functions (UDFs) as a more general representation to reconstruct surfaces from point clouds. However, these methods can not learn UDFs with smooth distance fields near surfaces, due to the noncontinuous character of point clouds, even using ground truth distance values or large scale meshes during training. Moreover, most UDF approaches failed to extract surfaces directly from unsigned distance fields. Particularly, they rely on post-processing such as Ball-Pivoting-Algorithm (BPA) [3] to extract surfaces based on the dense point clouds generated from the learned UDF, which is very time-consuming and also leads to surfaces with discontinuity and poor quality.\n\nTo solve these issues, we propose a novel method to learn consistency-aware UDFs directly from raw point clouds. We learn to move 3D queries to reach the approximated surface aggressively with a field consistency constraint, and introduce a polygonization algorithm to extract surfaces from the learned unsigned distance functions in a new perspective. Our method can learn UDFs from a single point cloud without requiring ground truth distances, point normals, or a large scale training set. Specifically, given query locations sampled in 3D space as input, We learn to move them to the approximated surface according to the predicted unsigned distances and the gradient at the query locations. More appealing solutions [1,2,20,36] have been proposed to learn SDFs from raw point clouds by optimizing the relationship between the query point and its closest point in raw data as a surface prior. However, since the raw point cloud is a highly discrete approximation of the surface, the closest point to the query location is always inaccurate and ambiguous, which makes the network difficult to converge to an accurate UDF due to the inconsistent or even conflicting optimization directions in the distance field. Therefore, in order to encourage the network to learn a consistency-aware and accurate unsigned distance field, we propose to dynamically search the optimization target with a specially designed loss function containing field consistency to mimic the conflict optimizations. We also progressively infer the mapping between 3D queries and the approximated zero iso-surface by using well-moved queries as additional priors for promoting further convergence. To extract a surface in a direct way, we propose to use the gradient field of the learned UDFs to determine whether two queries are on the same side of the approximated surface or not. In contrast to NDF [12] which also learns UDFs but takes dense point clouds as output and depends on BPA [3] to generate meshes, our method shows great advantages in efficiency and accuracy due to the straightforward surface extraction.\n\nOur main contributions can be summarized as:\n\n\u2022 We propose a novel neural network that learns consistent-aware UDFs directly from raw point clouds. Our method gradually infers the relationship between 3D query locations and the approximated surface with a field consistent loss.\n\n\u2022 We introduce an algorithm for directly extracting high-fidelity iso-surfaces with arbitrary topology from the gradient field of the learned unsigned distance functions.\n\n\u2022 We obtain state-of-the-art results in surface reconstruction from synthetic and real scan point clouds under the widely used benchmarks."}
{"tabids": ["8b29f176-220d-4c3a-aecd-3ec67a6fb864"], "corpus_id": 261242415, "title": "SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction", "paper_id": "f0f8471873132d02c18d030aaf72cd21988e657e", "abstract": "Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code url is https://github.com/THHHomas/SuperUDF.", "intro": "Surface reconstruction from 3D point clouds has been a long-standing problem in graphics and vision.Since the seminal work of Poisson surface reconstruction [1], there have been a large body of literature [2].Albeit relying on normal as input, the idea of reconstruction based on implicit field has been inspiring many deep learning methods [3], [4], [5], [6], [7].Signed Distance Function (SDF) is a typical implicit representation of 3D shapes of arbitrary geometry and topology.Many deep learning models have been proposed to predict the SDF of a 3D point cloud for high-quality reconstruction [3], [4], [6], [8].However, a major drawback of SDF is that it can represent only closed and watertight surfaces due to its nature of inside/outside discrimination, so SDF-based methods find difficulty in handling open surfaces such as garments or incomplete scans.\n\nUnsigned Distance Function (UDF) is suited for representing open surfaces since it does not differentiate between inside and outside.Compared to SDF, UDF is easier to learn since it concerns only distance information and ignores the sign.The pioneering work of [9] proposes a direct rendering method based on UDFs.However, both this work and its followups [10], [11] require strong supervision of ground-truth UDFs.Moreover, surface extraction from UDFs is difficult due to the absence of signs which is critical to marching cube [12].[13] convert UDFs to meshes via elevating a UDF to an SDF which brings back the limitation of SDFs.\n\n1, National University of Defense Technology PSR NDF SuperUDF GT Input CAP Fig. 1.A scene from ScanNet [14] reconstructed with PSR [1], NDF [9] , CAP [15] and our SuperUDF.\n\nRecently, [15] introduces a self-supervised pipeline to learn smooth UDFs directly from raw point clouds.Their network is trained in a shape-specific manner, requiring \u223c20 minutes for overfitting one shape.Furthermore, the method is sensitive to the sampling density of the input point cloud; the prediction of field gradients is inaccurate for sparsely sampled point clouds.Inspired by recent works [5], [4] demonstrating that geometry priors learned from large datasets benefit efficient reconstruction, we propose SuperUDF, a self-supervised learning method for fast UDF estimation with learned priors.To obtain robustness under sparse sampling, we introduce a novel regularization loss to train SuperUDF.We also introduce a learning-based mesh extraction directly from the estimated UDFs.\n\nThe core idea of our self-supervised UDF estimation draws inspiration from the seminal surface approximation operator of Locally Optimal Projection (LOP) [16].In particular, we first upsample the input point cloud via per-point duplication and random perturbation.The key insight behind our design is that if the UDF is estimated correctly, the upsampled points should be locally projected onto the underlying (ground-truth) surface following the gradient of the UDF.Since the ground-truth surface is unknown a priori, we instead require the projected points to approximate the input point cloud.\n\nTo do so, we impose a direct constraint on the estimated UDF for a better data approximation by minimizing the mean UDF value of all points of the input point cloud.However, imposing the approximation constraints alone may lead to noisy UDFs where the monotonicity of UDF at one side of the zero-level-set may be violated.To this end, we devise a regularization loss to ensure UDF monotonicity.In particular, for each upsampled point, denoted by p, we compute its offset point q by moving p along UDF gradient for a distance of half of p's UDF value.Here, q is expected to reside at the same side as p about the surface.We impose that the UDF gradients at the two points are the same and the UDF value at q should be half of that of p.This design also makes the points projected more uniformly.\n\nA difficulty in the method, however, is that the computation of UDF gradients in the losses above is time-consuming, making the training intractable.We thus opt for the reverse.We instead estimate a projection vector for each upsampled point.This results in a projection flow as an approximation of the UDF gradients.The projection flow network can also be trained in a self-supervised manner using the constraints above.The resulting projection flow is a strong geometry prior learned from a shape dataset which greatly improves training efficiency.The UDF can be easily obtained from the projection flow based on their duality.\n\nTo learn mesh extraction from UDFs, we partition the 3D space into a regular grid and train a 3D CNN for each grid cell to estimate the signs of its 8 corners.Note that we do not care about the absolute signs being positive or negative, but only concern with their relative signs.In practice, we fix the sign of one corner and let the network predict the rest.Since the network is trained for local sign prediction, it is easy to train and generalizes well across different shapes.\n\nExtensive evaluations demonstrate that SuperUDF outperforms the state of the arts on ShapeNet, MGN and Scannet in terms of both quality and efficiency.The main contributions of our work are:\n\n\u2022 A self-supervised UDF estimation network inspired by the classical surface approximation operator of Locally Optimal Projection (LOP).\u2022 A number of inductive biases on UDF geometry and a pre-learned geometry prior for efficient learning.\u2022 A novel regularization loss to make SuperUDF robust to sparse sampling.\u2022 A learning-based mesh extraction method that generalizes well across shapes.\n\nII. RELATED WORK a) Traditional point cloud surface reconstruction: Point cloud reconstruction has been a long-standing task in graphics and vision.The most important traditional method is Poisson surface Reconstruction [1] and ball-pivoting reconstruction [17].The former method classifies the query point according to the Poisson indicator function.The latter constructs the continuous surface by making up a ball rolling on the points.Those two methods can reconstruct pretty good surfaces, however, the performance can be improved more.\n\nb) SDF-based implicit surface reconstruction: Deep methods based on SDF always classify the occupancy of query points or directly regress the SDF value, which can be divided into local methods, global methods, and a combination.The global method, as the name implies, when giving a query point, classifies the query according to the whole shape information.The local method classifies the query point according to its neighbor points of it.Representative global methods are DeepSDF [3], BSP-Net [18].The routine of those methods is extracting the feature code of the whole shape and then recovering the surface from the code.Representative local methods are ConvOccNet [4], SSR-Net [19], DeepMLS [3] and POCO [8].ConvOccNet [4] first converts the point cloud feature into voxels and then applies volume convolution to enhance the feature of every voxel.SSR-Net [19] firstly extracts point feature, then maps the neighborhood points feature to octants, and finally classifies the octants.DeepMLS [20] tries to predict the normal and radius of every point, then classify the query points according to the moving least-squares equation.Furthermore, except for local methods and global methods, there are some methods that try to combine global and local information.The most representative method is Points2Surf [6].It tries to regress the absolute SDF value according to local information and classifies the sign according to global information.Another implicit surface reconstruction methods based on SDF are SAL [21], SALD [22] and On-Surface Prior [23].This kind of method aims to convert explicit representation, such as point cloud and 3D triangle soup, to implicit SDF representation.Thus, every 3D model needs a unique training process and unique network parameters.SAL [21] uses MLP to predict the SDF of shapes but adopts the UDF-based metric to supervise the network training, SALD [22] follows the SAL and adds derivative regularization term.On-surface Prior [23] use a pre-trained UDF-based network to help the main network to predict better SDF.\n\nThe SDF methods mentioned above achieve excellent progress in point cloud reconstruction.However, SDF representation has its weakness.It is hard to represent an open surface or partial scan.Furthermore, all the deep methods mentioned above needs 3D ground truth as the training label, while 3D ground truth is expensive for closed shape and real scan.\n\nc) UDF-based implicit surface reconstruction: UDF can express more general surfaces, such as open surfaces and partial scans.Lots of works have focus on the UDF representation.NDF [9] uses UDF to represent the surface, then they propose a point cloud up-sample method while reconstructing the mesh not directly from UDF but up-sampled point cloud via Ball Pivoting [17] method.DUDE [24] represent the shape with a volumetric UDF encoded in a neural network.Those works bring UDF to deep implicit surface reconstruction area and achieve good results.However, they need the 3D ground truth as the training label.What's more, they remain an open problem on how to directly extract iso-surface from UDF. UWED [25] makes use of MLS ( Moving Least Squares ) to convert the UDF to dense point cloud.d) Self-supervised method for surface reconstruction: Besides the supervised methods mentioned above, there are a few self-supervised approaches.For example, Neural Pulling [7] proposes a pipeline to train a network such that the network can predict SDF in the whole space without any extra supervision.However, the method has several disadvantages.First, it needs a dense and complete point cloud as supervision, limiting its scalability in real-world scenarios.Second, the method is only capable of reconstructing objects with closed surfaces.Thus, CAP [15]  predicting UDF rather than SDF so it can represent open surface.But it also suffers from the requirements of dense point cloud and long inference time.e) Iso-surface extraction: For SDF, the most common method for iso-surface extraction is Marching Cube [12].It is a template matching method based on the sign of eight grid corners.For iso-surface extraction on UDF, there are only a few works.MeshUDF [13] uses a neural network to vote the sign of the grid corner while bringing some sign conflict.The two methods try to convert the UDF to SDF.But SDF has difficulty representing the open surface.They are meaningful exploitation of iso-surface extraction on UDF.While sharing the same drawback of SDF and weak generalization ability."}
{"tabids": ["5ac6483b-03ae-448e-a80a-b453d252023b"], "corpus_id": 257801039, "title": "CAP-Net: A Deep Learning-Based Angle Prediction Approach for ISAC-Enabled RIS-Assisted V2I Communications", "paper_id": "4a742d4ad3498628970fe2e95ff59132d4c0ffde", "abstract": "In the vehicle-to-infrastructure (V2I) scenarios, it is challenging to acquire accurate channel state information (CSI) due to the high mobility of the communication channels, which leads to considerable pilot overhead. To tackle this issue, we employ both integrated sensing and communication (ISAC) technique and reconfigurable intelligent surface (RIS) technique to reduce the pilot overhead significantly while maximizing the achievable rate. In particular, we consider a RIS-assisted ISAC system serving the vehicle and design a transmission protocol based on communication-sensing-computing integration architecture for the proposed system, where the ISAC base station (BS) and the dedicated sensors deployed at the RIS receive the reflected echo signals from the user equipment (UE) via the BS-UE-BS link and the BS-UE-sensors link, respectively. Then the CSI of the UE can be acquired from the received signals. Furthermore, to provide both high-quality and low-latency communication services, we propose a covariance-based angle prediction neural network (CAP-Net) to predict the angle parameters facilitating the joint transmit and reflective beamforming design for the next time slot. Simulation results show that the proposed RIS-assisted ISAC system with the CAP-Net achieves better communication performance compared with other baseline schemes and can approach the upper bound in terms of achievable rate.", "intro": "In the era of beyond 5G and 6G, it is expected that the sensing functionality is integrated into the current wireless communication systems as a new feature, serving the emerging application scenarios such as vehicle-to-everything (V2X) networks. Integrated sensing and communication (ISAC) is a promising technique that can provide integration or coordination gains for the sensing and communication services [1]. In addition, ISAC is deemed to achieve high spectral efficiency and low hardware cost via integrating two functionalities into a single system [2].\n\nIn particular, since the communication channels are highly dynamic in the vehicle-to-infrastructure (V2I) scenarios, it is challenging to acquire the real-time channel state information (CSI) of the vehicles. To address this issue, there have been several works studying ISAC-enabled V2I communications. The works in [3]- [5] employed the ISAC signal to realize V2I beam tracking and prediction. With the ISAC signal, various beam prediction methods were utilized, such as extended Kalman filtering in [3], factor graph in [4], and deep learning in [5]. The above works show the great superiority of the ISAC-based scheme over the conventional communicationonly scheme in terms of reducing the pilot overhead. However, They only considered the line-of-sight (LoS) link between the roadside units and the user equipment (UE), while the non-LoS (NLoS) paths that facilitate communication were not fully exploited.\n\nReconfigurable intelligent surface (RIS), consisting of a large number of reflecting elements, can independently adjust the amplitude and phase shift of each element to reconfigure the wireless environment [6], [7]. The RIS is typically deployed in the NLoS region to create a virtual LoS link between the transmitter and receiver, which is useful for both sensing and communication. There are already several works on RISassisted ISAC system. In [8], [9], the RIS was employed to minimize multi-user interference under the constraints of beampattern and Cram\u00e9r-Rao bound, respectively. Furthermore, the RIS-related CSI can be obtained with the aid of the ISAC by estimating the parameters of the UEs, which can be used for the reflective beamforming design. However, it should be noted that in the conventional mono-static sensing scenario, the base station (BS) acts as the mono-static radar and the acquisition of RIS-related CSI is based on the BS-RIStarget-RIS-BS link. Thus, the echo signals will suffer severe attenuation due to the high path loss. To address this problem, the authors in [10] designed a self-sensing RIS architecture to mitigate the impact of path loss by reducing the reflection hops, while they simply focused on the sensing performance without concern for the communication performance.\n\nIn this paper, motivated by the above works, we propose a novel RIS-assisted ISAC system serving the vehicle, where the CSI of the UE can be obtained by the received echo signals to reduce the pilot overhead and improve the communication rate. First, we design a a transmission protocol based on communication-sensing-computing integration architecture for the proposed system.  Fig. 1. The RIS-assisted ISAC system. next time slot, we propose a covariance-based angle prediction neural network (CAP-Net). The CAP-Net is composed of the convolutional neural network (CNN) and the long short-term memory (LSTM) network, which is exploited to predict angles of arrival (AoAs) based on historical covariance matrices of the received echo signals. Finally, the transmit and reflective beamforming can be jointly optimized to achieve higher communication rate.\n\nThe remainder of this paper is organized as follows. Section II provides the RIS-assisted ISAC system model and the transmission protocol. The proposed CAP-Net is described in Section III. Simulation results are shown in Section IV and the conclusion is given in Section V.\n\nNotations: a, a, A denote complex scalar value, vector, matrix, respectively; C represents the set of complex numbers; [\u00b7] * , [\u00b7] T , and [\u00b7] H denote the conjugate, transpose and conjugate-transpose operations, respectively; || and \u2225\u00b7\u2225 denotes the absolute value and the Euclidean norm, respectively; N \u00b5, \u03c3 2 and CN \u00b5, \u03c3 2 denotes the Gaussian and complex Gaussian distributions with mean \u00b5 and variance \u03c3 2 , respectively."}
{"tabids": ["5ac6483b-03ae-448e-a80a-b453d252023b"], "corpus_id": 255658077, "title": "A Self-Supervised Learning-Based Channel Estimation for IRS-Aided Communication Without Ground Truth", "paper_id": "612e801a4a325ee83ee22f9dc1eb179f0588217d", "abstract": "Deep learning (DL) is an emerging paradigm for accurate channel estimation for intelligent reflecting surface (IRS)-aided wireless communication systems. It has been proven to be a promising way to achieve better channel estimation performance for the IRS-aided wireless communication system than traditional methods (e.g., least-square algorithm). However, existing DL-based methods rely on ground truth (labels of the true channels) which is difficult to obtain in real networks. In this paper, we propose a self-supervised learning (SSL) method for the IRS channel estimation problem. No ground truth channel is needed in the training, while a simple and novel self-supervised denoising formula without a clean reference signal is presented. Particularly, in the training phase, the self-supervised signal and the input are the received signal vector and its noisy version, respectively. While in the inference phase the input is the estimated channel by using the least-square method and the output is the refined channel estimation. That is, our neural network-based channel estimation algorithm is not reciprocal for training and testing. We demonstrate that the proposed SSL solution has good convergence performance and generalization ability through numerical simulations. Interestingly, we find a \u201cdouble descent\u201d phenomenon in the learning curve during the test phase, i.e., when we gradually increase the number of training epochs, the performance first gets better, then becomes worse, and further gets better again. Besides, we propose to analyze SSL using the loss landscape and centered kernel alignment method. The results show that the self-supervised model has a similar loss landscape and representational similarity to the supervised model. We explored the effects of different signal-to-noise ratios (SNRs), different neural network sizes, and different training data volumes on our algorithm through numerical simulations. Extensive numerical simulation results show that our SSL algorithm is still competitive without ground truth. We also show that the developed scheme exhibits robustness to SNR ratio mismatch.", "intro": "I NTELLIGENT reflecting surface (IRS) [1], [2], [3], [4] is one of the promising techniques for the beyond fifthgeneration (B5G) networks. It can enhance the coverage and capacity of wireless communication systems with low hardware costs and energy consumption. Usually, an IRS is composed of numerous passive reflection elements, which can obtain customized reflection signals by adjusting the electromagnetic properties such as the phase of the incident electromagnetic signal. Thus, IRS can act as an intelligent relay to increase the flexibility of the wireless network. Compared with relay-based wireless communication technology, IRS does not need to consume huge energy and does not introduce additional noise. A more detailed comparison of IRS and other communication techniques can refer to [2]. Because of the excellent properties of IRS, it has attracted a lot of research interest [5] and has been proven a promising way to enhance various wireless communication networks, e.g., non-orthogonal multiple access (NOMA) systems [6], unmanned aerial vehicles-based wireless communications [7], etc. To realize a high-performance IRS communication system, high-precision channel estimation is essential.\n\nThe related work [8] points out that only when perfect channel state information (CSI) is obtained, can IRS fully exert its excellent characteristics. Actually, according to [9], the acquisition of perfect CSI is quite challenging in practice. With precise CSI, advanced methods, such as the machine learning method proposed in [10], can perform well optimizing transmit and reflecting beamforming in an IRS-assisted wireless system. Different from traditional wireless systems, IRS as a kind of passive component cannot send, receive and process pilot signals, making the IRS-involved channel difficult to estimate. Thus, directly estimating the cascaded user-IRS-base station channel estimation is a typical way [11]. In addition, compared to the traditional systems, there are often a large number of passive components in IRS systems, and the dimension of the cascaded channels is large, which results in a large pilot overhead. To address these obstacles, efficient channel estimations are studied by many related works. For example, a leastsquare (LS)-based estimation method is used in [12] and [13]. In [12], the LS estimator is aimed at obtaining the unbiased cascaded channel for single-user multiple-input single-output (SU-MISO) systems. Binary reflection-controlled least-square channel estimation is proposed in [13] for IRS-aided energy transfer from a multi-antenna power beacon to a single-antenna user. This new LS-based channel estimation protocol suggests that IRS can only switch on one reflecting element and switch off the remaining reflecting elements at each time slot. The minimum mean squared error (MMSE)-based estimator is another solution to the IRS channel estimation problem. In [14], channel estimation in a coherence block is divided into two sub-stages. By turning off the entire IRS in the first sub-stage, the base station estimates the channel between itself and the user. Following this, the passive reflection elements of the IRS are turned on in turn to allow the base station to perform the estimation. MMSE method is used after estimation results of all sub-phases are taken together. The literature [14] indicates that the MMSE estimates are both analytically and numerically shown to achieve a much lower estimation error than the conventional LS estimates. As presented in [15], the performance of the available LS-based estimators still has a large gap compared with the optimal Bayesian MMSE estimator. While the optimal MMSE estimator needs to know the probability distribution of the channel (which is difficult to achieve in practice) and involves a multidimensional integration that has high complexity. The optimal MMSE estimator is also difficult to achieve in the real IRS network. Sparse matrix factorization, ambiguity elimination, and matrix completion are used in [16] to achieve a three-stage IRS channel estimation mechanism. However, a lot of iterative steps need to be performed to achieve the optimization of each stage. This limits its efficient execution in real networks. Thankfully, the development of deep learning (DL) provides support for low-complexity algorithmic inference and it promises to be a good tradeoff between channel estimation performance and complexity.\n\nDifferent from traditional methods, DL can learn the mapping from the input data to the desired output and get at least a locally optimal solution. The main reasons why it attracts a large number of researchers carrying out cutting-edge research are: 1) The neural network used in deep learning only uses simple matrix multiplication in the inference process, and does not require complex iterative optimization. 2) Deep learning has the generalization ability, that is, a trained neural network can perform well on samples that have never been seen before. Regarding the channel estimation problem as a supervised learning problem, several DL-based methods [17], [18], [19] have shown their remarkable performance in the IRS channel estimation task. The recent work [20] which proposes a model-driven deep unfolding scheme also shows that DL can achieve low-complexity online inference with a trained model.\n\nRelated works [17], [18] explore the combination of wireless communication models and deep learning to achieve channel estimation for IRS networks. In [17], the traditional multi-stage channel estimation method is used to construct the dataset to train neural networks. In [18], the compressed sensing technique is used to improve estimation accuracy. Both of the above research adopted a convolutional neural network (CNN) to solve the channel estimation problem for IRS-assisted communication systems. However, the method proposed in [17] requires turning on individual IRS elements in a sequence, which needs to deploy a hybrid passive/active IRS, thus may incur additional training overhead or hardware costs. Different from them, an end-to-end supervised learning method is proposed in [15], where a convolutional deep residual network (CDRN) is proposed to reconstruct the channel from the noisy pilot-based observations. It improves the channel estimation accuracy without the requirements of additional system deployment. Similarly, [19] proposes to use residual dense network structure to obtain better channel estimation generalization and fitting capabilities by leveraging the low-rank structure of IRS channels. In [21], enhanced residual neural networks, which leverage the rank-deficient structure of RIS channels, are proposed to obtain accurate channel state information. In [22], data-driven nonlinear solutions based on deep learning are proposed, which are two convolutional neural network-based methods to perform denoising and approximate optimal MMSE channel estimation solutions. Although these studies have shown the performance superiority of deep learning as a solution for channel estimation, these schemes all use supervised learning to complete the training of neural networks. A natural question is how the labels needed for supervised learning are obtained in practice. Obviously, if the labels are obtained by traditional channel estimation algorithms (e.g., LS estimator or multi-stage channel estimation proposed in [14]), the performance of supervised learning methods will be limited by the performance of traditional algorithms. And end-to-end training of the neural network, like the method used in [15], needs to use the ground truth channel label to calculate the loss function to update the neural network. However, it is difficult to obtain the ground truth channel in the real world. Thus, an effective and practical method that takes advantage of the good properties (generalization ability, competitive online inference ability) of DL without relying on ground truth is expected.\n\nThe motivation of our work is to propose a DL method that does not require ground-truth labels to efficiently solve the IRS channel estimation problem. In this work, we propose a self-supervised learning algorithm 1 to achieve the above goal. Self-supervised learning (SSL) can be considered a kind of unsupervised learning which learns from unlabeled samples [23]. Usually, unsupervised learning works towards clustering and dimensionality reduction, whereas SSL performs conclusive tasks like classification and regression like any supervised model. The main idea of SSL is to self-generate labeled data by defining a pretext task that guides us to a supervised loss function and empowers us to exploit a variety 1 The code to reproduce this algorithm is open-sourced: https://github.com/jhcknzzm/SSL-Based-IRS-Channel-Estimation of labels that come with the data for free. In this context, our main contributions are the following:\n\n1. We propose a novel SSL method for IRS-aided wireless network channel estimation. Specifically, first, a new neural network architecture is designed, which is inspired by existing research on residual neural networks for channel estimation. Through empirical experiments, we demonstrate that this neural network outperforms existing neural networks CDRN [15] on the considered task. Then, we design a simple denoising pretext task to train this neural network. This pretext task lets the neural network reconstruct the input from a noisy version of the received signal. It is worth emphasizing that we do not assume that the perfect channel state information or the received signal not polluted by wireless transmission noise can be obtained as training labels. Besides, our solution is an unconventional learning method where training and testing are not reciprocal, i.e., in the test phase, the input is different from that in the training phase. Specifically, the input of the neural network in the test phase is the imprecise channel estimation calculated by the LS while the input of the training is the received signal.\n\n2. An interesting double descent phenomenon is observed by using the proposed channel estimation where training and testing are not reciprocal. Specifically, we observe \"epochwise double descent\" when we increase the training time, with test performance following a classical U-like curve in the underfitting stage (the same as the fundamental biasvariance trade-off concept) and then improving with training time to be sufficiently larger (contrary to traditional statistical learning theory but in line with advanced double descent theory). This phenomenon, which has not been observed in channel estimation work before, implies that we can train neural networks without early stopping [24] to achieve good performance.\n\n3. We analyze the similarity of SSL-trained models and supervised learning-trained models through loss landscape and centered kernel alignment (CKA). The loss landscape results show that the self-supervised trained model has similar generalization performance to supervised learning can be exhibited, and the CKA results show that the feature representation by self-supervised learning is also similar to supervised trained models. The extensive empirical evaluations show the proposed self-supervised solution has competitive performance. Besides, we show that the developed scheme exhibits robustness to signal-to-noise ratio (SNR) mismatch, i.e., our algorithm is robust when there is a certain difference in the distribution of test and training data.\n\nThe novelty of our work lies in the realization of channel estimation without ground-truth labels by establishing a simple and effective self-supervised learning scheme. Although its performance cannot exceed supervised learning, it offers the possibility to apply the good generalization ability of DL in real-world situations where labels are not available.\n\nThe rest of this paper is organized as follows. In Section II, we introduce our considered system model and the cascaded channel estimation problem of the intelligent reflecting surface-aided wireless communication network. In Section III, we propose a self-supervised learning method. In this section, the loss landscape and centered kernel alignment for analyzing the similarity between models are also introduced. Finally, numerical results and discussions are presented in Section IV, and a conclusion is drawn in Section V."}
{"tabids": ["5ac6483b-03ae-448e-a80a-b453d252023b"], "corpus_id": 255080909, "title": "Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and Communication System", "paper_id": "963f16197b5ce016e182bf2d5f7b733c9b673fb0", "abstract": "Integrated sensing and communication (ISAC), and intelligent reflecting surface (IRS) are envisioned as revolutionary technologies to enhance spectral and energy efficiencies for next wireless system generations. For the first time, this paper focuses on the channel estimation problem in an IRS-assisted ISAC system. This problem is challenging due to the lack of signal processing capacity in passive IRS, as well as the presence of mutual interference between sensing and communication (SAC) signals in ISAC systems. A three-stage approach is proposed to decouple the estimation problem into sub-ones, including the estimation of the direct SAC channels in the first stage, reflected communication channel in the second stage, and reflected sensing channel in the third stage. The proposed three-stage approach is based on a deep-learning framework, which involves two different convolutional neural network (CNN) architectures to estimate the channels at the full-duplex ISAC base station. Furthermore, two types of input-output pairs to train the CNNs are carefully designed, which affect the estimation performance under various signal-to-noise ratio conditions and system parameters. Simulation results validate the superiority of the proposed estimation approach compared to the least-squares baseline scheme, and its computational complexity is also analyzed.", "intro": "T HE integrated sensing and communication (ISAC) technology has been foreseen as a promising candidate to improve wireless resource utilization and hardware sharing efficiency in the next wireless system generations [1], [2], [3]. ISAC merges the sensing and communication (SAC) functionalities into a single system. The sensing functionality of ISAC collects and extracts the sensory information from noisy observations. Thus, it is envisioned as a critical enabler to measure or predict the surrounding wireless environment intelligently. On the other hand, the communication functionality processes and transfers the received noisy signals, with the ultimate goal of recovering the transmitted information accurately. Based on these concepts, to successfully integrate SAC into a single system and expand ISAC to a wide variety of wireless networks, the performance trade-off between SAC has been analyzed in the literature [4], [5], [6], [7], [8]. The authors in [4] and [5] focused on optimizing the sensing functionality, such as the target detection probability and target angle estimation accuracy, while guaranteeing an acceptable communication performance. On the other hand, the sensing-assisted communication scheme that promotes the communication performance was investigated in [6], [7], [8], including the sensing-assisted beam training, tracking, and prediction. The aforementioned works typically assumed that perfect channel state information (CSI) is available at the receiver side and rarely considered the channel estimation issue for the ISAC systems. Recently, another interesting study of sensing-assisted communication was performed in [9], which designed the precoder of the roadside units (RSU) and the received beamforming of the vehicle. With the help of the radar mounted on the RSU, the covariance matrices of both SAC channels were estimated by the echo signals to facilitate the transceiver beamforming design. However, the SAC channels in [9] were assumed to share the same dominant paths. Hence, its estimation approach is only limited to that specific case.\n\nIntelligent reflecting surface (IRS) has emerged as another promising technique to increase the coverage and capacity of next wireless system generations; it enables a programmable wireless propagation environment [10], [11], [12], [13]. In general, IRS is an artificial planar surface comprising a large number of low-cost passive reflecting elements. Each element independently configures its phase-shift according to the CSI of the surrounding environment and further controls the reflection of the incident signals [10]. By properly coordinating the phaseshifts of all the IRS elements, the signal transmission quality and system performance can be improved. This technology is referred to as passive beamforming [14], [15], [16]. Note that the above beamforming gain is based on an accurate CSI of the IRS-assisted wireless communication system. As such, estimating the channels is crucial in such a system. Since the reflected channel matrix (e.g., user equipment (UE)-IRS-base station (BS) link) has a large dimension and does not follow the traditional Rayleigh distribution, two challenges come up to the channel estimation [17], [18]. One challenge is the limited estimation accuracy, while the other one is the large training overhead. To overcome these challenges, model-driven channel estimation approaches have been widely researched recently, such as the reflection pattern controlled [19], [20] and element grouping schemes [21], [22]. Despite the vital contributions of these works, the above challenges have not been sufficiently addressed yet. Therefore, the data-driven deep-learning (DL) estimation approaches are further investigated in [23], [24], [25], [26]. These DL-based approaches reflect the potential to balance the estimation accuracy and training overhead in the IRS-assisted communication systems. The mapping between the received signals and channels is successfully characterized by adopting various DL networks, such as convolutional neural network (CNN) [23], [24], recurrent neural network [25], and deep residual learning [26].\n\nSince IRS has shown great prospects in enriching communication coverage and spectral/energy efficiency, it is expected to assist the ISAC system in providing better SAC performance. Recently, ISAC and IRS have been jointly explored, taking into account their cooperation merits [27], [28], [29], [30]. Due to the fact that the SAC signals coexist in the IRS-assisted ISAC systems, inherent interference occurs and affects the SAC performance. Then, the ISAC BS and IRS beamforming, as well as the ISAC waveform, are required to be properly designed for such systems. The authors in [27] and [28] concentrated on the beamforming designs for both the ISAC BS and IRS to provide a tradeoff between SAC performance, considering the effect of inherent interference. On the other hand, the joint ISAC waveform and IRS beamforming designs were investigated for the IRS-assisted ISAC systems in [29] and [30], aiming to mitigate the inherent interference while improving the communication performance under the constraints of sensing metrics. It is worth noting that an accurate CSI is necessary for all the aforementioned designs. The channel estimation problem in such IRS-assisted ISAC systems is challenging due to the inherent interference; to the best of the authors' knowledge, this problem has not been investigated yet. Considering the research gap in the existing literature, this paper proposes a novel three-stage channel estimation approach for an IRS-assisted ISAC multiple-input single-output (MISO) system. Each stage is devised at the full-duplex (FD) ISAC BS, and then a DL estimation framework is developed correspondingly, along with input-output pairs designs. In particular, the contributions of this paper are summarized as follows:\n\n1) A three-stage estimation approach is proposed to estimate the SAC channels of the IRS-assisted ISAC system. It decouples the overall estimation problem into sub-ones by controlling an on/off state of the IRS or BS transmission. The proposed estimator successively estimates the direct SAC, reflected UE-IRS-BS, and reflected BS-target-IRS-BS channels. The proposed approach is the first attempt to provide a practical channel estimation for such a system and successfully solves the estimation difficulty caused by the inherent interference. 2) The pilot transmission protocol for each stage is precisely designed to facilitate the SAC channels estimation. It involves the design of the pilot sequences adopted at the FD ISAC BS, pilot sequences employed at the UE, and IRS phase-shift vectors."}
{"tabids": ["488bdd5f-7ba5-4295-8895-aa0a0b67a1ab"], "corpus_id": 4001553, "title": "Early-warning signals for critical transitions", "paper_id": "c5e8862bfd224b8078a77655602c910963df75d3", "abstract": "Complex dynamical systems, ranging from ecosystems to financial markets and the climate, can have tipping points at which a sudden shift to a contrasting dynamical regime may occur. Although predicting such critical points before they are reached is extremely difficult, work in different scientific fields is now suggesting the existence of generic early-warning signals that may indicate for a wide class of systems if a critical threshold is approaching.", "intro": "It is becoming increasingly clear that many complex systems have critical thresholds\u2014so-called tipping points\u2014at which the system shifts abruptly from one state to another. In medicine, we have spontaneous systemic failures such as asthma attacks1 or epileptic seizures2,3; in global finance, there is concern about systemic market crashes4,5; in the Earth system, abrupt shifts in ocean circulation or climate may occur6; and catastrophic shifts in rangelands, fish populations or wildlife populations may threaten ecosystem services7,8.\n\nIt is notably hard to predict such critical transitions, because the state of the system may show little change before the tipping point is reached. Also, models of complex systems are usually not accurate enough to predict reliably where critical thresholds may occur. Interestingly, though, it now appears that certain generic symptoms may occur in a wide class of systems as they approach a critical point. At first sight, it may seem surprising that disparate phenomena such as the collapse of an overharvested population and ancient climatic transitions could be indicated by similar signals. However, as we will explain here, the dynamics of systems near a critical point have generic properties, regardless of differences in the details of each system9. Therefore, sharp transitions in a range of complex systems are in fact related. In models, critical thresholds for such transitions correspond to bifurcations10. Particularly relevant are \u2018catastrophic bifurcations\u2019 (see Box 1 for an example), where, once a threshold is exceeded, a positive feedback propels the system through a phase of directional change towards a contrasting state. Another important class of bifurcations are those that mark the transition from a stable equilibrium to a cyclic or chaotic attractor. Fundamental shifts that occur in systems when they pass bifurcations are collectively referred to as critical transitions11.\n\nWe will first highlight the theoretical background of leading indicators that may occur in non-equilibrium dynamics before critical transitions, and illustrate how such indicators can perform in model generated time-series. Subsequently, we will review emerging empirical work on different systems and discuss prospects and challenges."}
{"tabids": ["488bdd5f-7ba5-4295-8895-aa0a0b67a1ab"], "corpus_id": 237583501, "title": "Deep learning for early warning signals of tipping points", "paper_id": "91c6031240527a1cdba25aef61bdf5a3cf767498", "abstract": "Significance Early warning signals (EWS) of tipping points are vital to anticipate system collapse or other sudden shifts. However, existing generic early warning indicators designed to work across all systems do not provide information on the state that lies beyond the tipping point. Our results show how deep learning algorithms (artificial intelligence) can provide EWS of tipping points in real-world systems. The algorithm predicts certain qualitative aspects of the new state, and is also more sensitive and generates fewer false positives than generic indicators. We use theory about system behavior near tipping points so that the algorithm does not require data from the study system but instead learns from a universe of possible models.", "intro": "also their weakness, since these indicators do not tell us which type of bifurcation to expect (16).\n\nThe dominant eigenvalue is derived from a first-order approximation to dynamics near the equilibrium. Higher-order approximations can distinguish between different types of bifurcations. But they are not often used to develop early warning indicators because 1) the first-order approximation dominates dynamics sufficiently close to the equilibrium, causing critical slowing down to generate the strongest signal, and 2) the first-order approximation is more tractable to mathematical analysis of stochastic systems than the higher-order approximations (20). However, as a system gets closer to a bifurcation, it can drift farther from equilibrium due to critical slowing down. As a consequence, the higher-order terms become significant and may be large enough to provide clues about the type of transition that will occur. Statistical measures such as skew and kurtosis reflect the influence of these highest-order terms, for instance (20)(21)(22). Higher-order terms could be associated with features in time series data that are subtle but detectable, if we knew what to look for. Knowing qualitative information about the tipping point (such as whether it will be sudden or gradual) and the state that lies beyond it (such as whether it will oscillate or be stable) based on predicting the bifurcation type could be valuable in a range of applications."}
{"tabids": ["488bdd5f-7ba5-4295-8895-aa0a0b67a1ab"], "corpus_id": 250243662, "title": "Using Machine Learning to Anticipate Tipping Points and Extrapolate to Post-Tipping Dynamics of Non-Stationary Dynamical Systems", "paper_id": "ebe23db676b2b9a27f7eecb29ec0fe89a5dc0618", "abstract": "In this paper we consider the machine learning (ML) task of predicting tipping point transitions and long-term post-tipping-point behavior associated with the time evolution of an unknown (or partially unknown), non-stationary, potentially noisy and chaotic, dynamical system. We focus on the particularly challenging situation where the past dynamical state time series that is available for ML training predominantly lies in a restricted region of the state space, while the behavior to be predicted evolves on a larger state space set not fully observed by the ML model during training. In this situation, it is required that the ML prediction system have the ability to extrapolate to different dynamics past that which is observed during training. We investigate the extent to which ML methods are capable of accomplishing useful results for this task, as well as conditions under which they fail. In general, we found that the ML methods were surprisingly effective even in situations that were extremely challenging, but do (as one would expect) fail when ``too much\"extrapolation is required. For the latter case, we investigate the effectiveness of combining the ML approach with conventional modeling based on scientific knowledge, thus forming a hybrid prediction system which we find can enable useful prediction even when its ML-based and knowledge-based components fail when acting alone. We also found that achieving useful results may require using very carefully selected ML hyperparameters and we propose a hyperparameter optimization strategy to address this problem. The main conclusion of this paper is that ML-based approaches are promising tools for predicting the behavior of non-stationary dynamical systems even in the case where the future evolution (perhaps due to the crossing of a tipping point) includes dynamics on a set outside of that explored by the training data.", "intro": "Predicting the time evolution of a dynamical system is a problem at the heart of many fields. While some prediction problems focus on forecasting the evolution of the values of a set of system state observables over a timescale that is on the order of the characteristic time (\u03c4 s ) on which observables vary, in other situations one is concerned with predicting the statistical properties of observables over a timescale (\u03c4 ns ) that is much longer than the time over which the details of system state variation can be usefully forecasted (\u03c4 ns \u03c4 s ). Examples of the former include predicting daily rainfall, wind speed, and temperature (i.e., weather forecasting), while examples of the latter include predicting variations of average patterns associated with rainfall, wind and temperature over years.\n\n(Motivated by the terminology in atmospheric science, we use the term \"climate\" to refer to the long-term characteristics of typical orbits of any dynamical system.) Prediction is often particularly difficult since many systems of interest, such as the terrestrial climate system, can be highly complicated, and knowledge of some of their scientific principles, parameters, and boundary conditions may be incomplete, inaccurate, or unknown. In addition, such systems may have a wide range of spatial and temporal scales that cannot be resolved using conventional numerical methods. The long term statistics of the dynamics of such systems is often heuristically modeled using noisy, nonstationary dynamical systems which themselves may depend on a set of time-dependent parameters. It is well-known that for different fixed parameter settings, a noiseless stationary dynamical system (i.e., a system with no explicit time-dependence of the system itself) can, depending on its (timeindependent) parameters, exhibit a variety of behaviors ranging from periodic to chaotic. We refer to qualitative changes of the attractor orbits of stationary systems occurring with variation of (timeindependent) system parameters as bifurcations. In the case where the system is non-stationary, e.g., due to time-dependent parameters, the basic system dynamics may change with time. Moreover, if, for example, the time-dependent system temporally drifts through a critical parameter region, the state evolution of the non-stationary system can experience rapid change whereby the statistical behavior of its dynamics is qualitatively and quantitatively altered [1,2]. We refer to such changes in the dynamics of non-stationary systems as \"tipping points\".\n\nMachine learning has been widely applied to the problem of determining both the short-term future state evolution [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] and the long-term \"climate\" [18][19][20][21] of stationary dynamical systems. In this paper, we use the term \"climate\" to denote long-term statistical properties of the evolution of a dynamical system. For stationary systems with ergodic dynamics, this includes, for example, obtaining the distribution of states, Lyapunov exponents, temporal correlation functions, Fourier power spectra, etc. associated with typical long trajectories of the system. However, for nonstationary systems the estimation of local-in-time state distributions, Lyapunov exponents, Fourier power spectra, and temporal correlation functions can often be problematic. We note, however, that a more well-defined generalization of a time varying system state distribution for non-stationary systems is available through the concept of \"snapshot attractors\" (also called \"pullback attractors\"), and we shall make use of this. In particular, a snapshot attractor, at any given time t is obtained by considering an ensemble of states obtained from an ensemble of trajectories initiated from many randomly chosen initial conditions in the far past. See Refs. [22][23][24][25][26][27] for a more detailed discussion. While prediction of stationary systems by machine learning (ML) has received much recent attention, less progress has been made in applying ML to the problem of predicting the time evolution of nonstationary dynamical systems, particularly of their climate and of the tipping points they may experience. Refer to [12,[28][29][30] for recent works which apply machine learning to the problem of predicting the short-term state evolution of non-stationary dynamical systems and to [27,[31][32][33][34] for recent works which aim to address the problem of predicting changing statistical properties of non-stationary dynamical systems (including anticipating tipping points). In previous work [27] we demonstrated that ML provides a promising avenue for predicting the climate of a non-stationary dynamical system using the time series of its past states and knowledge of a non-stationarity-inducing system parameter time dependence. It was shown that a machine learning model can anticipate tipping points in a non-stationary dynamical system and, in some cases, predict post-tipping-point dynamics which are fundamentally different from those it was trained on.\n\nThe goal of this work is to further develop, devise, and test ML techniques for the prediction of non-stationary dynamical systems that undergo a tipping point transition. A main focus of our work is on situations where the observed pre-transition motion is constrained to a smaller restricted subset of the state space region than that on which the post-transition motion evolves.\n\nIn contrast, in previous work on predicting tipping points and the associated post-tipping-point dynamics [27,33,34] ML prediction was considered for cases in which the training data was obtained from orbits that typically explored large state space regions that included all or most of the state space visited by the predicted future orbits. Thus, in this prior work the ML predictor was directly aware of dynamical system information needed for the prediction of the future behavior, e.g., after a predicted tipping point. In some other previous works only the occurrence of a tipping point, but not the post-tipping-point behavior, was predicted. These latter works anticipate the occurrence of a tipping point based on observation of a pre-tipping-point orbit subject to dynamical noise, and use the fact that, as the tipping point is approached, the effect of the noise on the orbit increases. For example, in one work of this type [32] a deep learning technique was developed by training on a library of mathematical models to recognize dynamical response to noise that characterizes a system as it approaches a tipping point. Although quite useful, such techniques yield no information about the post-tipping-point dynamics of the system. In contrast to the above two different cases, in this paper we consider the situation where the ML predictor, although trained on a pre-tipping-point system trajectory which evolves on a smaller state space set contained within the larger set explored by the future post-tipping-point trajectory, is able to anticipate the tipping-point transition and extrapolate its learning from the neighborhood of the pre-tipping-point training data into the larger regions of the system state space not explored, or only sparsely explored, in the training data to predict the post-tipping-point dynamics.\n\nA further contribution of our paper relative to previous related papers using ML for prediction of non-stationary behavior [27,33,34] is that those previous works considered the case where the system non-stationarity was induced by time variation of a parameter of an otherwise unknown system, and this parameter time variation was assumed to be known and was used as an input to the ML prediction system. In this current paper, on the other hand, we consider the case where knowledge of the type described above may not be available.\n\nFurthermore, motivated by our finding that using the standard hyperparameter optimization validation scheme for this type of forecasting did not consistently yield useful results in our test cases, we accordingly introduce a new hyperparameter optimization strategy (which we use in the numerical experiments of this paper) for this purpose. We believe that this hyperparameter determination scheme may be generally beneficial for ML prediction of non-stationary systems.\n\nTipping point transitions between different dynamical states where the pre-transition state dynamics is constrained to a smaller set of the system state space, or to a different region of the system state space, than the post-transition orbits are observed in a wide array of natural systems [1,2]. Such transitions are commonly found in various terrestrial climate models [35][36][37], ecosystem models [38][39][40], epidemics [41,42], and physical and engineering applications (e.g., intermittency [43] and crisis [44] transitions in plasmas [45][46][47][48], lasers [49,50], electrical and power systems [51][52][53][54], thermoacoustic systems [55][56][57], hydrodynamical systems [58,59], and electrochemical systems [60]). Thus, developing methods to predict the climate evolution of non-stationary dynamical systems which may tip into a state characterized by motion of the system that may visit previously unex-plored, or only sparsely explored, regions of its state space is a problem of very general importance. We emphasize, however, that, as is the case in general for methods employing extrapolations, our method has limitations. In particular, extrapolations are more likely to fail as the \"amount\" of the attempted extrapolation from the known case increases. A case illustrating this point is given in Sec. 3.1 where, when considering a situation where the non-stationary drift of the system is too slow, predictive extrapolation of the system behavior fails, but is then found to succeed when dynamical noise is assumed to influence the system evolution. (Evidently, the beneficial role of the dynamical noise in this case is to increase the state space set that is sampled by the training data.) A related example is given in Sec. 3.4 where we begin by reporting a failed attempt to extrapolate and predict behavior through a tipping-point associated with a subcritical (hysteretic) Hopf bifurcation using a purely data-driven ML model. In order to enable prediction in such a case, we then consider a prediction system scheme in which an ML model is combined with a knowledge-based model component. Although the knowledge-based component in the example considered is so inaccurate that it cannot make useful predictions on its own, we show that using it in a combined ML-based/knowledge-based hybrid prediction system enables good predictions of a tipping point transition as well as of the post-tipping-point behavior. Based on the discussion of Sec. 3.4, we hypothesize that the ML prediction of post-tipping climate dynamics purely from pre-tipping training data will usually only be possible for tipping point processes mediated by non-hysteretic stationary system bifurcations.\n\nIn what follows, we use reservoir computing [61][62][63] as the ML platform. Reservoir computing has previously been successfully used to predict the time evolution of dynamical systems [3-5, 7, 11, 18-20, 27], and its training is computationally inexpensive since it can be accomplished via a simple linear regression. This allows us to rapidly test different methodologies and various test system scenarios. We expect other types of machine learning, such as deep learning, to also work well using the methods presented in this paper.\n\nThe rest of the paper is structured as follows. Section II presents a brief introduction to the reservoir computing setup and training, as well as to our hyperparameter optimization scheme for non-stationary systems. In Section III we numerically demonstrate the use of ML to anticipate tipping point transitions and post-tipping behavior from motion in a restricted state space region to motion that explores substantial state space regions not previously visited. The example test systems used in Section III for generating the training data are the three-dimensional Lorenz '"}
{"tabids": ["488bdd5f-7ba5-4295-8895-aa0a0b67a1ab"], "corpus_id": 261030871, "title": "Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces", "paper_id": "939f35d721c33187a71b43351a371a799ee409ab", "abstract": "Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points.", "intro": "Non-stationary chaotic dynamics are a prominent part of the world around us. In chaotic systems, small perturbations in the initial condition function significantly affect the long-term trajectory of the dynamics. Non-stationary chaotic systems possess further complexity due to their time-varying nature. For instance, the atmosphere and ocean dynamics that govern Earth's climate are modeled by highly nonlinear partial differential equations (PDEs). They exhibit non-stationary chaotic behavior due to changes in anthropogenic greenhouse gas emissions, insolation, and a myriad of complex internal feedbacks [1,2] (Figure 1a). One of the main areas in scientific computing is understanding such phenomena and providing computational methods to model their dynamics. Numerical methods, e.g., finite element and finite difference methods, have been widely used to solve PDEs. However, numerical methods have enormous computational requirements to capture the fine scales in complex processes. Moreover, they do not provide a manageable way to learn from data to reduce scientific modeling errors.\n\nLearning in non-stationary physical systems. These complexities in modeling complex physical systems make learning the dynamics of their evolution in function spaces notoriously difficult. Prior works proposed various neural networks, such as recurrent neural networks (RNNs) [3] and  Figure 1: (a) Tipping point in cloud fraction as a function of CO 2 concentration, from bulk model of the atmospheric boundary layer developed in [5,6]. 3d cloud cover renderings reproduced from [2].\n\n(b) RNO accurately forecasts tipping point 64 seconds ahead in non-stationary Lorenz-63 system. \"Predicted tipping point\" is the time at which our framework predicts a tipping point will occur, and \"forecast time\" is the time when our framework makes this prediction."}
{"tabids": ["8b1d0c59-2fa6-45b3-9b45-291056ee3725"], "corpus_id": 16944951, "title": "OpenMili: a 60 GHz software radio platform with a reconfigurable phased-array antenna", "paper_id": "1bc04cbbce54bc027b6147eb0a49189a2691a35c", "abstract": "The 60 GHz wireless technology holds great potential for multi-Gbps communications and high-precision radio sensing. But the lack of an accessible experimental platform has been impeding its progress. In this paper, we overcome the barrier with OpenMili, a reconfigurable 60 GHz radio architecture. OpenMili builds from off-the-shelf FPGA processor, data converters and 60 GHz RF front-end. It employs customized clocking, channelization and interfacing modules, to achieve Gsps sampling bandwidth, Gbps wireless bit-rate, and Gsps sample streaming from/to a PC host. It also incorporates the first programmable, electronically steerable 60 GHz phased-array antenna. OpenMili adopts programming models that ease development, through automatic parallelization inside signal processing blocks, and modular, rate-insensitive interfaces across blocks. It provides common reference designs to bootstrap the development of new network protocols and sensing applications. We verify the effectiveness of OpenMili through benchmark communication/sensing experiments, and showcase its usage by prototyping a pairwise phased-array localization scheme, and a learning-assisted real-time beam adaptation protocol.", "intro": "The unlicensed millimeter-wave (mmWave) spectrum around the 60 GHz frequency promises a blueprint of wireless networking at wire-speed. The vast amount of spectrum re-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. source, spanning 57 GHz to 64 GHz in many countries, enables multi-Gpbs data rate. The small wavelength (\u223c 5mm) enables miniature antenna-arrays that can form highly directional \"pencil beams\" to boost link quality and spatial reuse. mmWave is thus considered as an enabling technology for 5G wireless broadband [1]. Recent commercialization of 60 GHz devices (e.g., by Qualcomm [2] and Intel [3]) also triggers low-cost mmWave sensing applications, which used to be available only in dedicated environment for medical/security inspection. Together, short wavelength and high directionality translates into high sensitivity, enabling subtle object localization/tracking [4], vital-sign detection and mobile mmWave imaging [5].\n\nRealizing the vision of mmWave networking and sensing necessitates a reconfigurable experimental platform that allows prototyping before the protocols/applications are deployed. Ideally, one would need a mmWave software-radio platform that allows reconfiguration from the PHY layer up to applications, can transmit customized waveforms and acquire RSS/phase information for sensing applications [4,[6][7][8]. On the 2.4 GHz and 5 GHz microwave spectrum, counterpart devices such as USRP [9], WARP [10] and Sora [11] have reshaped the landscape of wireless experimentation in the past decade, speeding up the ratification of nextgeneration wireless standards (e.g., 802.11ax [12]) and sensing appliances [13]. However, to our knowledge, there exists no reconfigurable platform that can capture the unique features of 60 GHz wireless systems, particularly the Gsps sampling bandwidth and electronically steerable phased-array antennas.\n\nIn this paper, we propose OpenMili, an open-access 60 GHz software-radio, which fills the gap and opens up new directions for mmWave sensing and protocol development. OpenMili has a software-defined mmWave network stack spanning PHY layer signal processing to applications. It can also act as a programmable 60 GHz radio sensor with a custom-built phased-array. From the hardware architecture perspective, OpenMili integrates off-the-shelf baseband processing unit (BPU), ADC/DAC and 60 GHz RF front-end (frequency up/down-converters), and develops a signal chain to enable Gsps of sampling bandwidth. It allows flexible channelization and reclocks the 60 GHz front-end to overcome its inherent phase-noise problem. OpenMili's baseband processing unit (BPU) centers around a Kintex Ultra-Scale FPGA, and uses a customized PCIe module to realize 1 Gsps real-time sample streaming from/to a PC host. The PC host can reconfigure/monitor the RF front-end and signal processing modules in real-time. OpenMili's most out-standing feature is a reconfigurable phased-array antenna that can switch between 16 beam patterns at \u03bcs granularity, under the control of the BPU. We design the phased-array specifically to fit the WR-15 waveguide (a standard antenna interface on 60 GHz radios), ensuring it can retrofit both OpenMili's RF front-end and other commercial mmWave radios that are typically equipped with WR-15 horn antennas [14].\n\nFrom the software architecture perspective, OpenMili takes advantage of the massive parallelization on the FPGA to enable Gsps sample processing. It eases prototyping by using C++ to define signal processing blocks, and using AXI [15] as inter-block gluing mechanism, which facilitates modularity and eliminates the need for inter-block rate matchinga well-known headache in FPGA programming. We choose this programming model also because the complexity of intrablock parallelization can be hidden from application developers, although coarse-grained inter-block parallelization still need to be explicitly expressed.\n\nOpenMili provides three reference designs which we believe to capture the unique aspects of mmWave and can be instrumental for a wide range of 60 GHz network protocols and wireless sensing applications [4,[6][7][8]. (i) Gbps baseband communication module: allowing a wide range of network protocol development on the FPGA or PC host; (ii) Real-time RSS/phase sensing: using an 802.11ad-like preamble to sense the channel state information (CSI), with around 300K CSI readings per second across 1 GHz bandwidth, enabling many real-time sensing applications [8]. (iii) Real-time phased-array controller: allowing 16 beamforming patterns based on a discrete codebook. Codebook entry selection is made in real-time on a Microblaze processor, programmable in C. This module can also access the real-time CSI statistics, allowing beam adaptation based on channel conditions.\n\nTo showcase the use of OpenMili in prototyping 60 GHz systems, we propose and implement two new 60 GHz location sensing and beamforming adaptation mechanisms, which are of independent interest. (i) Pairwise relative localization of phased-arrays. Many recent systems have used 2.4 GHz phased-arrays [16,17] for angle-of-arrival (AoA) estimation, but they need multiple phased-arrays to triangulate a target radio, and they assume the phase-shift values are continuously adjustable, which is not applicable to practical 60 GHz phased-arrays that use hard-wired phase-shifters. We propose a simple algorithm that leverages the discrete phaseshifting to estimate the AoA as well as distance between a pair of phased-arrays, enabling pairwise localization instead of triangulation. (ii) Learning-assisted real-time beam adaptation. The short-wavelength at 60 GHz enables compact phased-array design, with many antenna elements but correspondingly a large number of beamforming codebook entries to choose from, which entails huge adaptation overhead at run time. We propose a new principle to make this tradeoff: we allow a pair of phased-array nodes to learn the correlation between beam patterns offline, and then prune the adaptation space at run-time, which substantially saves the beam searching overhead.\n\nContributions. To prepare for an open-source release, we have intentionally used off-the-shelf hardware modules to build OpenMili (except for the programmable phasedarray). Our main contribution lies in tasking these modules into a reconfigurable architecture, designing the soft- ware framework and common interfaces to enable the development of new mmWave communication and sensing applications. This contribution breaks down into the following aspects: (i) Hardware architecture (Sec. 2 and 4): We develop the first 60 GHz reconfigurable radio architecture that achieves Gsps sampling bandwidth, Gbps wireless bit-rate, and Gsps real-time sample streaming from/to a PC host. The hardware architecture builds on customized clocking, channelization and interfacing modules. Most remarkably, it incorporates the first programmable 60 GHz phased-array design.\n\n(ii) Software framework (Sec. 3): We explore FPGA programming models that ease development, through automatic parallelization inside signal processing blocks, and modular, rate-insensitive interfaces across blocks. We also provide reference designs to bootstrap the development of protocols and sensing applications, featuring the Gbps communications and beam steering capabilities of 60 GHz radios.\n\n(iii) New network protocol and sensing modality (Sec. 6): We design a new 60 GHz beam adaptation protocol and a simple pairwise location application. We further showcase the usefulness of the reconfigurable platform in prototyping and experimenting with such mechanisms.\n\nThe software code and hardware schematics of OpenMili will be available on our project website [18]."}
{"tabids": ["8b1d0c59-2fa6-45b3-9b45-291056ee3725"], "corpus_id": 211198441, "title": "M-Cube: a millimeter-wave massive MIMO software radio", "paper_id": "f75d48032b74d6f4426a0658dd99a878c101ff4a", "abstract": "Millimeter-wave (mmWave) technologies represent a cornerstone for emerging wireless network infrastructure, and for RF sensing systems in security, health, and automotive domains. Through a MIMO array of phased arrays with hundreds of antenna elements, mmWave can boost wireless bit-rates to 100+ Gbps, and potentially achieve near-vision sensing resolution. However, the lack of an experimental platform has been impeding research in this field. This paper fills the gap with M3 (M-Cube), the first mmWave massive MIMO software radio. M3 features a fully reconfigurable array of phased arrays, with up to 8 RF chains and 288 antenna elements. Despite the orders of magnitude larger antenna arrays, its cost is orders of magnitude lower, even when compared with state-of-the-art single RF chain mmWave software radios. The key design principle behind M3 is to hijack a low-cost commodity 802.11ad radio, separate the control path and data path inside, regenerate the phased array control signals, and recreate the data signals using a programmable baseband. Extensive experiments have demonstrated the effectiveness of the M3 design, and its usefulness for research in mmWave massive MIMO communication and sensing.", "intro": "Millimeter-wave (mmWave) networking technologies are widely recognized as the most promising solution to confront the mobile data explosion. However, commercially viable use cases, e.g., 60 GHz 802.11ad and 70 GHz backhaul, have been limited to short-range, static, point-to-point settings. The fundamental reason lies in the use of highly directional beams as the communication medium, which can be easily disturbed by obstacle blockage and device movement. These challenges become most severe when a large phased array is used, with a massive number of antenna elements (and hence a large number of directional beams to manage).\n\nIn addition, mmWave devices can serve as RF sensors to achieve high spatial resolution, owing to their intrinsically shorter wavelength, wider bandwidth, and larger antenna aperture [32]. Besides conventional use cases such as vehicular radar ranging and security/medical imaging, mmWave sensing is becoming available on pervasive mobile devices. For example, the 5G NR standard has incorporated mmWave location sensing [59]. Meanwhile, the emerging 802.11ay standard also introduces a WLAN radar mode which repurposes the mmWave radio as a MIMO radar [1].\n\nTo fully explore the challenges and opportunities in mmWave technologies, it is critical to have a programmable experimental platform with the following capabilities: (i) Equipped with low-cost and large-scale phased arrays which allow real-time beam switching, to accommodate high mobility vehicular networking/sensing scenarios; (ii) Supporting the mmWave MIMO architectures to be used in 5G NR and 802.11ay radios [22,37]; (iii) Allowing reconfiguration of beam patterns, communication/sensing algorithms and network stack. Existing mmWave experimental platforms are either too costly (around $200K per link [33,39]), or lack a reconfigurable phased array antenna with reasonable size [39,48,65]. Moreover, such devices are often bulky and can barely support mobile experiments. None of the existing platforms include support for both multiple RF chains and reconfigurable phased arrays, which are critical for research into mmWave MIMO.\n\nIn this paper, we describe the design and implementation of M 3 , the first mmWave massive MIMO experimental platform to meet the aforementioned requirements. M 3 is a low-cost softwaredefined radio/radar comprised of up to 256 antenna elements and up to 8 RF chains. The key research thrust in M 3 is to repurpose a commodity 802.11ad phased array as a programmable phased array, and to interface it with an existing baseband processing unit (BPU), such as an FPGA with data converters, or a low-frequency software radio. M 3 's software radio/radar design cuts the per-node cost significantly, e.g., down to $3.8K for a narrowband (  2 RF-chain 72-antenna mmWave MIMO, and below $15K for a wideband (4 GHz) 4 RF-chain 128-antenna version.\n\nThe key observation behind the M 3 design is that many modern mmWave radios [9,42] adopt a split-IF (intermediate frequency) architecture as shown in Fig. 1. The baseband-to-IF and IF-to-RFplus-antenna modules, henceforth referred to as baseband module (BM) and phased array module (PM), are realized in two separate chips, connected through a single coaxial cable that carries both data and control signals. By reverse engineering the control channel, and regenerating the control signals using an external FPGA, we gain full access to the phased array, including reconfiguring its codebook entries (beam patterns), triggering beam scanning, selecting and switching between the beam patterns in real-time, and tuning the individual antenna element gain. For the data channel, we replace the original 802.11ad BM with a customized BPU, along with a bridge board that interfaces the BPU and the PM. The bridge board is designed such that it can take as input/output either baseband I/Q signals or modulated RF signals below 4 GHz. With this board, the low-cost commodity phased array can be attached to any existing BPU, such as a USRP, WARP, or customized FPGAs.\n\nTo extend this architecture to a mmWave MIMO setup, we found that recently emerged multi-phased-array 802.11ad radios [3,30] provide the same data channel to multiple carrier-synchronized phased arrays, and can switch on one or more of them simultaneously. By interfacing each phased array to a separate bridge board and separate ADC/DAC channel on the BPU, we can construct a hybrid beamforming architecture with up to 8 RF chains, each attached to a 6\u00d76 phased array. Furthermore, by clock-sharing between the transmitter and receiver path, we can convert an M 3 node into a software-defined mmWave MIMO radar with a massive number of phased array elements. In addition, the phased arrays can be rearranged into a ring or cube layout, to expand the field-of-view to the entire 3D space.\n\nWe have conducted comprehensive measurement and testing to validate the feasibility and effectiveness of the M 3 design. Our key findings include: (i) Reconfigurability. After a one-time calibration, the multiple phased arrays on M 3 can be reconfigured separately to generate desired beam widths and directions. M 3 enables realtime mmWave MIMO communication, channel measurement, and radar sensing. (ii) Control path performance. M 3 can control the beam switching in real-time with a latency of 412 ns, which is commensurate with commodity 802.11ad radios. (iii) Data path performance. The bridge board design in M 3 does not degrade the phase noise and signal to noise ratio (SNR) level. Depending on the sampling frequency of the BPU, it can achieve an end-to-end SNR of 19 dB.  Figure 2: Measurement setup to anatomize a commodity split-IF 802.11ad radio.\n\nFurthermore, we conducted two case studies to demonstrate the application of M 3 in exploring mmWave MIMO networking and sensing systems. (i) Context-aware mmWave MIMO hybrid beamforming. We implement a mmWave MIMO OFDM framework to characterize the single-user MIMO and multi-user MIMO performance in indoor/outdoor environments. Our experiments reveal the need for multipath context-aware MIMO mode adaptation. (ii) mmWave MIMO radar with uniform and non-uniform array layout. We implement a multi-phased-array radar with non-uniform array layout, and demonstrate its higher angular resolution in comparison with single array or uniform arrays.\n\nThe key contributions of M 3 can be summarized as follows: (i) Designing the data path to bridge programmable baseband processors with low-cost commodity phased array modules. (ii) Designing the control path to reconfigure and control the phased array with sub-\u00b5s latency. (iii) Restructuring the commodity 802.11ad radio into a massive MIMO mmWave radio/radar. (iv) Experimental verification of the M 3 architecture and performance, along with new measurement insights for mmWave MIMO radio/radar systems. To our knowledge, M 3 represents the first-of-its-kind programmable mmWave MIMO platform. We will follow the WARP project model [54] to make M 3 available to the wireless research community, through open-source hardware and paid fabrication/assembly services. The code, documentation and further information will be released through the project website, http://m3.ucsd.edu/sdr/."}
{"tabids": ["8b1d0c59-2fa6-45b3-9b45-291056ee3725"], "corpus_id": 218569273, "title": "mm-FLEX: an open platform for millimeter-wave mobile full-bandwidth experimentation", "paper_id": "673b805e8da77fd9e1eb465a2c252d7b02d74c24", "abstract": "Millimeter-Wave (mm-wave) technology is increasingly being considered for mobile devices and use cases such as vehicular communication. This requires suitable experimentation platforms to support systems-oriented research to tackle the multitude of problems and challenges of mm-wave communications in such environments. To this end, we introduce mm-FLEX, a flexible and modular open platform with real-time signal processing capabilities that supports a bandwidth of 2 GHz and is compatible with mm-wave standard requirements. mm-FLEX integrates an FPGA-based baseband processor with full-duplex capabilities together with mm-wave RF front-ends and phased antenna arrays that are fully configurable from the processor in real-time. To demonstrate the capabilities of mm-FLEX, we implement a scalable, ultra-fast beam alignment mechanism for IEEE 802.11ad systems. It is based on compressive estimation of the signal's angle-of-arrival by means of switching through multiple receive beam patterns on a nano-second time-scale while receiving a packet preamble. Our implementation is open source and is made publicly available to the research community.", "intro": "Millimeter-Wave (mm-wave) network technology such as the IEEE 802.11ad [33] and the upcoming IEEE 802.11ay [34] standards for 60 GHz Wireless Local Area Networks (WLANs) can provide vastly higher data rates than networks operating below 6 GHz, due to the large amount of bandwidth available at mm-wave frequencies.\n\nMm-wave devices typically implement directional communication using phased arrays, so that the beam-forming gains of the antenna compensate for the higher path loss due to the small wavelength [22]. This not only ensures a good link range but also reduces interference and thus improves spatial reuse. Operating networks at mm-wave frequencies introduces new problems not only due to the RF front-end components but also due to the need for high gain directional communication and the associated beam training or channel estimation. Especially for mobile scenarios, further practical research is needed. For instance, fast device tracking is essential for continuous antenna beam alignment, but current beam training mechanisms introduce a high overhead and latency when used in highly mobile environments. At the same time, there is a lack of suitable experimentation platforms that allow for realtime packet processing and antenna reconfiguration, and that are flexible enough to support modifications of the physical and MAC layers. To address the challenges of mobile mm-wave communication networks, it is crucial to provide the research community with such an experimentation platform, that ideally should have the following features: i) giga-sampling rate capability, to be able to accurately study the characteristics of the wide-band channels and be compatible to current standards, ii) access to physical layer data, which brings invaluable information to researchers to measure the channel and develop new signal processing algorithms, iii) simplicity, flexibility and configurability, such that researchers can easily modify the behavior of the system, iv) fast antenna reconfiguration to support high-mobility scenarios, and v) affordability, to enable also smaller research groups to carry out mobile systems research. The latter is a problem specific to mm-wave systems and their large bandwidth, since the cost of the A/D converters and the FPGA are directly related to the maximum sampling frequency and the number of logic elements/speed grade, respectively. Different solutions in the literature aim to address the need for experimentation platforms. The MiRa platform [2] uses Universal Software Radio Peripheral (USRP) devices as baseband equipment attached to a custom 28 GHz RF front-end. It only supports narrowband channels due to the USRP's bandwidth limitations. The X60 testbed [23] is based on National Instruments (NI) hardware and a phased array antenna kit. While it meets the bandwidth requirements of the IEEE 802.11ad standard, it is expensive and the high complexity of the design makes low-level modifications more difficult. OpenMili [35] aims to increase performance compared to [2] at a more manageable cost compared to [23]. It does provide better performance than [2] but still does not meet the bandwidth requirements of mm-wave standards. Our system is complementary to [36] which provides a MIMO-capable RF front-end, whereas our focus is on a full-bandwidth baseband design. In addition, the solutions listed above lack support for high-mobility scenarios, which require fast control over the antenna array to enable real-time beam steering with very low antenna reconfiguration latency.\n\nIn this paper we propose a Millimeter Wave Mobile FulL-Bandwidth EX perimentation Platform (mm-FLEX). It is a flexible, modular and highly configurable testbed that exceeds the requirements of current mm-wave standards, supporting more than 2 GHz of bandwidth. mm-FLEX is an open source research platform that offers a unique set of features. At approximately twice the cost of a high-end narrow-band USRP (an order of magnitude less than [23]), it offers more than ten times the performance and does not sacrifice standard compliance as [2,35]. Furthermore, we add important features such as fast real-time antenna steering that are a key enablers for research targeting high mobility scenarios.\n\nThe testbed is composed of a powerful baseband system including a high performance FPGA and processor boards with the necessary components for real-time signal processing, as well as a 60 GHz RF front-end using a 16+16 element phased array antenna with high resolution phase shifters that can be fully controlled from the system. We design mm-FLEX to be modular and flexible with the necessary IP blocks that facilitate real-time experimentation with mobile mm-wave systems. Furthermore, the designed blocks have a flexible AXI interface that allows different Super Sample Rate (SSR) configurations such that they can even be used on different FPGA platforms, such as the narrow-band X310 USRP system. We implement the signal processing blocks for real-time capture and frame processing directly on the FPGA 1 , together with the fast beam sweeping of the phased-array antenna at nano-second time scale. Our testbed also supports a hardware-in-the-loop mode where part of the functionality is software modeled while the time-critical ones are hardware-implemented, facilitating the development and validation of signal processing applications for the mm-wave community.\n\nWhile our experimentation platform is currently designed for IEEE 802.11ad, it is flexible enough to be used with 28GHz equipment [27] for 5G New Radio (NR) and beyond research.\n\nTo showcase the capabilities of mm-FLEX, we use it to address the important problem of fast beam alignment in dense, mobile mm-wave scenarios. Conventional IEEE 802.11ad [33] beam training is done by transmitting frames using each of the available antenna configurations, while the target device listens with a quasiomnidirectional beam pattern. This avoids the complexity of training both ends of the link simultaneously. The target device then provides feedback which sector resulted in the highest Signal-to-Noise Ratio (SNR) [14]. Such brute force or simple hierarchical beam training (in case beam refinement is used) works well for relatively static deployments of moderate density. However, the overhead may become prohibitive when frequent beam training is required in more dynamic environments with device mobility or link interruptions through blocking obstacles.\n\nWe propose a design that eliminates dedicated station beam training to allow the network to handle very high network dynamics and client densities. Our mechanism works with unmodified standardcompliant IEEE 802.11ad Access Points (APs). While an AP sends multiple frames for the standard beam training, the stations perform quick signal strength measurements with different receive beam patterns during preamble reception of these frames after packet detection. We then use a fast sparse estimation algorithm to determine the Angle of Arrival (AoA) and select the best beam pattern to be used at the station. Our mechanism only requires minor changes in the firmware of the stations, since the fast beam-pattern sweeping capability is already included in 802.11ad standard compliant devices for the Beam Refinement Phase (BRP). We perform an extensive measurement campaign and show that our mechanism can switch through 10 receive beam patterns during the preamble without compromising detection accuracy for typical SNR values, whereas only 5 beam patterns are required to accurately estimate the AoA with an error of around 1 \u2022 .\n\nOverall, our paper provides the following contributions: \u2022 We design mm-FLEX, a flexible and configurable testbed that is able to perform real-time signal processing at multi-GHz bandwidths and is compatible with current mm-wave standards. At the same time, the hardware on which the system is based is affordable for such a high performance system. \u2022 We implement the baseband system on a powerful FPGA and processor board with dual AXI-stream data paths for separate transmitter and receiver chains. This brings modularity to the design and enables full-duplex applications. Our hardware processing blocks to detect and process the preamble of standard-compliant IEEE 802.11ad frames operates with the full bandwidth of 1.76 GHz. The code and specification of our testbed are publicly available [6].\n\n\u2022 We augment the testbed functionality to provide fast real-time antenna array reconfigurability, implementing a hardware block to directly switch beam patterns on a nano-second time scale, using a configurable input trigger. This is a crucial feature not only to support standard-compliant IEEE 802.11ad beam training, but more importantly to enable symbol-level beam reconfiguration for research targeting high-mobility scenarios.\n\n\u2022 We demonstrate the capabilities of the testbed platform by implementing a mechanism for passive station beam alignment for dense, mobile millimeter-wave networks, that significantly outperforms current standards and prior work."}
{"tabids": ["8b1d0c59-2fa6-45b3-9b45-291056ee3725"], "corpus_id": 53095465, "title": "E-Eye: Hidden Electronics Recognition through mmWave Nonlinear Effects", "paper_id": "a0c2ce28eb54b9a1904877861d9d53f3887a7e97", "abstract": "While malicious attacks on electronic devices (e-devices) have become commonplace, the use of e-devices themselves for malicious attacks has increased (e.g., explosives and eavesdropping). Modern e-devices (e.g., spy cameras, bugs or concealed weapons) can be sealed in parcels/boxes, hidden under clothing or disguised with cardboard to conceal their identities (named as hidden e-devices hereafter), which brings challenges in security screening. Inspection equipment (e.g., X-ray machines) is bulky and expensive. Moreover, screening reliability still rests on human performance, and the throughput in security screening of passengers and luggages is very limited. To this end, we propose to develop a low-cost and practical hidden e-device recognition technique to enable efficient screenings for threats of hidden electronic devices in daily life. First, we investigate and model the characteristics of nonlinear effects, a special passive response of electronic devices under millimeter-wave (mmWave) sensing. Based on this theory and our preliminary experiments, we design and implement, E-Eye, an end-to-end portable hidden electronics recognition system. E-Eye comprises a low-cost (i.e., under $100), portable (i.e., 11.8cm by 4.5cm by 1.8cm) and light-weight (i.e., 45.5g) 24GHz mmWave probe and a smartphone-based e-device recognizer. To validate the E-Eye performance, we conduct experiments with 46 commodity electronic devices under 39 distinct categories. Results show that E-Eye can recognize hidden electronic devices in parcels/boxes with an accuracy of more than 99% and has an equal error rate (EER) approaching 0.44% under a controlled lab setup. Moreover, we evaluate the reliability, robustness and performance variation of E-Eye under various real-world circumstances, and E-Eye can still achieve accuracy over 97%. Intensive evaluation indicates that E-Eye is a promising solution for hidden electronics recognition in daily life.", "intro": "Hidden electronic devices (hereafter, e-devices) bring both security and privacy threats in our daily life. For instance, explosion tragedies continuously occur due to the ineffective detection of remote-controlled disguised bombs [1,3,7,14], which can be triggered by electronic initiators. Apart from these life-threatening hazards, e-devices (e.g., smartphones and spy camera) can also be used for eavesdropping, cheating in private zones [13] or accessing other areas that restrict electronics [4,15,16]. The fact that these e-devices (hereafter, hidden e-devices) can be sealed in parcels or boxes, hidden insides in clothing and disguised in appearance increases the risk that they can easily pass undetected through security check points.\n\nEntry security check is the current method for defending against malicious, hidden e-devices requiring an X-ray machine [58] at safety-critical sites (e.g., airports and embassy offices). Unfortunately, their expensive cost and poor portability make it an infeasible solution against the proliferation and the deployment of portable e-devices [45]. Moreover, the radiation emitted from X-rays is harmful to workers and persons passing through the checkpoints. Other scanning methods based on metal scanners can only detect the existence of the e-device rather than recognize the specific type directly (see Sec. 9.1). Conventional computer vision methods cannot be applied because the camera can not see through containers or bodies. Thermal imaging also fails because it only can detect the temperature of the hidden e-device [39], which can be easily interfered with by other heat sources. As a result, how to recognize hidden e-devices in a cost-efficient, user-friendly and non-invasive manner remains an unsolved challenge for public security and privacy.\n\nRecently, there is a rising trend of applying radio-frequency signals, such as millimeter wave (mmWave [10]), in sensing and tracking applications, because mmWave can penetrate obstacles and \"image\" hidden objects due to its highly-directional beamforming and strong reflection properties on objects [64]. For example, Adib et al. studied the possibility of sensing human occurrence and vitals through WiFi signals [17]. Zhu et al. developed a 60GHz mmWave imager for object detection and classification [66]. Wei et al. designed mTrack, a mmWave instrument for precision object tracking [64]. However, existing works mainly target either human or non-electronic object detection and tracking. The capability of accurately recognizing hidden electronic devices through mmWave sensing is unknown.\n\nTo this end, we propose our system, E-Eye, to facilitate hidden e-device recognition in public security inspections. Its features are (1) cost-efficient: the cost of the solution is affordable in daily life for large scale deployment; (2) portable: it is easy to use in the inspection of different containers (e.g., delivery boxes, checkin luggages or even human body) and various environments (e.g., postal offices, airports or factories); (3) non-invasive: it can avoid the obtrusive (even illegal) opening of the container in real practice which sacrifices efficiency and may cause privacy issues.\n\nThe foundation of E-Eye rests on the nonlinear response effect from electronic circuits when probed by the mmWave. The intrinsic difference in circuits' hardware characteristics (e.g., a circuits' components and circuit layout) generates a distinct nonlinear response, which can serve as the identity of a certain e-device brand. This way, we enable a novel sensing modality for noninvasive and cost-effective hidden e-devices recognition based on the mmWave field. Specifically, we design and prototype a portable (11.8cm \u00d7 4.5cm \u00d7 1.8cm) and light-weight (45.4\u0434) 24GHz mmWave probe device which is enabled to probe the mmWave and capture the returned nonlinear responses. We address the challenges in noiseisolation and coherence to achieve high-quality signal with low complexity and cost (less than US $100). Afterwards, the signal is transferred to the smartphone and we propose the wavelet-based analysis module taking into consideration the unavoidable variance in the signal's scaling and magnitude in practical usage. Eventually, we develop a fine-tuned support vector machine (SVM) classifier for robust recognition under various conditions. In the experiment, we employ 46 e-devices and the comprehensive results show that E-Eye can accurately recognize each e-device brand under different scenarios.\n\nOur contributions are summarized as follows:\n\n\u2022 We propose a novel form of recognizing hidden e-devices by exploring the nonlinear response effect of mmWave of e-devices. We find that the circuit inside an e-device acts as a passive signal modulator which reflects back radio frequency (RF) signals with intrinsic identity information. \u2022 We develop E-Eye, an end-to-end system to facilitate the low-cost, non-invasive and robust hidden electronics recognition. We prototype the sensing hardware and implement the recognition algorithm for efficient and effective classification. There are usually two following forms of radio frequency (RF) response when probing a continuous wave (CW) with the transmit frequency f 0 towards a target.\n\nLinear Effects: The main carrier frequency of the received signal is the same as that of the transmitted signal. The phase change in the linearly demodulated signals is related to the geometrical information, such as object distance, shape and size [42]. However, these linear effects do not reflect the material properties and we need to seek other information in the application of e-device detection and recognition.\n\nNonlinear Effects: Besides the main carrier frequency, the received signal wave is also modulated with a set of the sub-carrier frequencies with more side lobes in the spectrum. These sub-carrier frequencies are generated due to the nonlinear properties of the target (e.g., material reflection efficiency) [48,49]. In the remaining part of this section, we provide an in-depth analysis of non-linear effects in recognizing electronics.\n\nNonlinear Effects from E-device: As shown in Figure 2, when the e-device enters the RF beam field, chips, connectors and metal traces of printed circuit board (PCB) on an e-device are viewed as an array of antennas in the resolution of mmWave, and these antenna with inductance (L), capacitance (C) and resistance (R) act as a passive processor and manipulates the transmit mmwave signals. More specifically, antennas can conduct and transform the mmWave signal to a high-frequency current along the conductors between the electronic components within the device [38]. The components (e.g., a diode) or parasitic parameters (e.g., a parasitic circuit) on the PCB modulate the response signal and generate the nonlinear distortion [33], formulated as Equation (1):\nr (t) = m(z(t),\u00e2(t)) \u2297 h f (t),(1)\nwhere z(t) is the response signal, m(\u00b7,\u00e2(t)) is the nonlinear modulation function of the PCB,\u00e2(t) is the complex power-series for the nonlinear system, \u2297 stands for convolution computing and h f (t) is the ideal bandpass filter function for the carrier bandwidth [26,27]. After the modulated signal radiate from the e-device, they would be captured by the probe receive (Rx) antenna. Thus, this nonlinear response of the e-device contains rich information of its physical characteristic and holds the potential to serve as the device's identity. Figure 2: The e-device generates a nonlinear response signature under the RF beam. The response is determined by its intrinsic physical characteristics."}
{"tabids": ["8b1d0c59-2fa6-45b3-9b45-291056ee3725"], "corpus_id": 6601027, "title": "Soli: Ubiquitous Gesture Sensing with Millimeter Wave Radar", "paper_id": "00f0a8788ed43c534a8888b2b2e4edbafafe8bf3", "abstract": "This paper presents Soli , a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solid-state radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.", "intro": "This paper presents Soli, a new, robust, high-resolution, lowpower, miniature gesture sensing technology for interactive computer graphics based on millimeter-wave radar. Radar operates on the principle of reflection and detection of radio frequency (RF) electromagnetic waves [Skolnik 1962]. The RF spectrum has several highly attractive properties as a sensing modality for interactive systems and applications: the sensors do not depend on lighting, * This research was supported by a Multi-University Research Agreement with Google while the author was a student at Stanford University. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. c 2016 Copyright held by the owner/author(s). SIGGRAPH 2016, July 24-28, 2016, Anaheim, CA ISBN: 978-1-4503-4279-7/16/07 DOI: http://doi.acm.org/10.1145/2897824.2925853 noise or atmospheric conditions; are extremely fast and highly precise; and can work through materials, which allows them to be easily embedded into devices and environments. When implemented at millimeter-wave RF frequencies, the entire sensor can be designed as a compact solid-state semiconductor device: a radar chip that is a miniature, low-power device having no moving parts and can be manufactured inexpensively at scale. The resulting Soli sensor delivers the promise of truly ubiquitous gesture interaction across a very broad range of applications, including but not limited to virtual reality (VR), wearables and smart garments, Internet of Things (IoT) and game controllers, as well as more traditional devices such as mobile phones, tablets and laptops.\n\nIt is important to point out that the first radar systems were developed as early as the 1930s [Watson-Watt 1945], and that RF sensing has since become a well established and mature field of engineering and applied science. The current radar hardware and computational methods, however, were primarily developed for mainstream radar applications, which usually involve detection and tracking of large moving objects at large distances, such as air and terrestrial traffic control, marine radar, aircraft anti-collision systems and outer space surveillance, and geophysical monitoring, among many others. The engineering requirements for such applications are not compatible with modern consumer applications in which sensors must fit into tiny mobile and wearable devices, run on limited computational resources, work at ultra-short distances (i.e. less than 5 mm), consume little power, and track the dynamic configuration of complex, highly deformable elastic objects, such as a human hand as opposed to a rigid airplane, at sub-millimeter accuracy. We are not aware of any existing radar system that could satisfy the above requirements. Our investigation suggests that developing a radar-based sensor optimized for human-computer interaction (HCI) requires re-thinking and re-building the entire sensor architecture from the ground up, starting with basic principles.\n\nIn this work, we present the first end-to-end radar sensing system specifically designed for tracking and recognizing fine hand gestures. Our work builds upon a large existing body of knowledge in the radar domain and, for the first time, explores the comprehensive design principles, implementation, and optimization of these tools for scalable gesture sensing within the constraints and requirements of modern HCI. We show that ubiquitous and intuitive gesture in-teraction is made possible through tailored, interdependent design of the entire sensor architecture, from the radar sensing paradigm (Sections 3 and 4) to hardware and solid-state radar chips (Section 5), interaction models (Section 6), gesture tracking techniques and vocabularies ( Figure 1 and Section 7), and software pipelines (Section 8). The complete end-to-end design, development, and evaluation of this new gesture sensing modality is the major achievement and contribution of this work, opening new research frontiers in non-imaging sensors for interaction."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 54475412, "title": "Fast Online Object Tracking and Segmentation: A Unifying Approach", "paper_id": "d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1", "abstract": "In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/~qwang/SiamMask.", "intro": "Tracking is a fundamental task in any video application requiring some degree of reasoning about objects of interest, as it allows to establish object correspondances between frames [38]. It finds use in a wide range of scenarios such as automatic surveillance, vehicle navigation, video labelling, human-computer interaction and activity recognition. Given the location of an arbitrary target of interest in the first frame of a video, the aim of visual object tracking is to estimate its position in all the subsequent frames [70,66,55].\n\nFor many applications, it is important that tracking can be performed online, while the video is streaming. In other words, the tracker should not make use of future frames to * Equal contribution. Work done while at University of Oxford.\n\nInit Estimates Figure 1. Our proposed method aims at distilling the best from the two tasks of object tracking and video object segmentation. Like conventional object trackers, it relies on a simple bounding box initialisation (blue) and operates online. Differently from state-ofthe-art trackers such as ECO [15] (red), SiamMask (green) is able to produce binary segmentation masks, which can more accurately describe the target object.\n\nreason about the current position of the object [30]. This is the scenario portrayed by visual object tracking benchmarks, which represent the target object with a simple axisaligned [63,34,42,60,43] or rotated [30] bounding box. Such a simple annotation helps to keep the cost of data labelling low; what is more, it allows a user to perform a quick and simple initialisation of the target.\n\nSimilar to object tracking, the task of semi-supervised video object segmentation (VOS) requires estimating the position of an arbitrary target specified in the first frame of a video. However, in this case the object representation consists of a binary segmentation mask which expresses whether each pixel belongs to the target or not [46]. Such a detailed representation is more desirable for applications that require pixel-level information, like video editing [44] and rotoscoping [41]. Understandably, producing pixel-level estimates requires more computational re-sources than a simple bounding box. As a consequence, VOS methods have been traditionally slow, often requiring several seconds per frame (e.g. [62,58,45,2]). Very recently, there has been a surge of interest in faster approaches [67,40,64,11,10,24,23]. However, even the fastest still cannot operate in real-time.\n\nIn this paper, we aim at narrowing the gap between arbitrary object tracking and VOS by proposing SiamMask, a simple multi-task learning approach that can be used to address both problems. Our method is motivated by the success of fast tracking approaches based on fullyconvolutional Siamese networks trained offline on millions of pairs of video frames [4,31] and by the very recent availability of a large video dataset with pixel-wise annotations such as YouTube-VOS [65]. We aim at retaining the offline trainability and online speed of these methods while at the same time significantly refining their representation of the target object, which is limited to a simple axis-aligned bounding box.\n\nTo achieve this goal, we simultaneously train a Siamese network on three tasks, each corresponding to a different strategy to establish correspondances between the target object and candidate regions in the new frames. As in the fully-convolutional approach of Bertinetto et al. [4], one task is to learn a measure of similarity between the target object and multiple candidates in a sliding window fashion. The output is a dense response map which only indicates the location of the object, without providing any information about its spatial extent. To refine this information, we simultaneously learn two further tasks: bounding box regression using a Region Proposal Network [52,31] and classagnostic binary segmentation [49]. Notably, binary labels are only required during offline training to compute the segmentation loss and not during tracking. In our proposed architecture, each task is represented by a different branch departing from a shared CNN and contributes towards a final loss, which sums the three outputs together.\n\nOnce trained, SiamMask solely relies on a single bounding box initialisation, operates online without updates and produces object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity and fast speed, SiamMask establishes a new state-of-the-art on VOT-2018 for the problem of real-time object tracking. Moreover, the same method is also very competitive against recent semi-supervised VOS approaches on DAVIS-2016 and DAVIS-2017, while being the fastest by a large margin. This result is achieved with a simple bounding box initialisation (as opposed to a mask) and without adopting costly techniques often used by VOS approaches such as fine-tuning [39,45,2,61], data augmentation [25,33] and optical flow [58,2,45,33,11].\n\nThe rest of this paper is organised as follows. Section 2 briefly outlines some of the most relevant prior work in vi-sual object tracking and semi-supervised VOS; Section 3 describes our proposal; Section 4 evaluates it on four benchmarks and illustrates several ablative studies; Section 5 concludes the paper."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 208175650, "title": "D3S -- A Discriminative Single Shot Segmentation Tracker", "paper_id": "45512d44f1205bc92775f2e880858b3f23c9f5fd", "abstract": "Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-art trackers on the TrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video object segmentation benchmark and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time.", "intro": "Visual object tracking is one of core computer vision problems. The most common formulation considers the task of reporting target location in each frame of the video given a single training image. Currently, the dominant tracking paradigm, performing best in evaluations [22,24], is correlation bounding box tracking [11,3,33,2,54,28] where the target represented by a multi-channel rectangular template is localized by cross-correlation between the template and a search region.\n\nState-of-the-art template-based trackers apply an efficient brute-force search for target localization. Such strategy is appropriate for low-dimensional transformations like translation and scale change, but becomes inefficient for more general situations e.g. such that induce an aspect ratio change and rotation. As a compromise, modern track- ers combine approximate exhaustive search with sampling and/or bounding box refinement/regression networks [10,27] for aspect ratio estimation. However, these approaches are restricted to axis-aligned rectangles. Estimation of high-dimensional template-based transformation is unreliable when a bounding box is a poor approximation of the target [31]. This is common -consider e.g. elongated, rotating, deformable objects, or a person with spread out hands. In these cases, the most accurate and well-defined target location model is a binary per-pixel segmentation mask. If such output is required, tracking becomes the video object segmentation task recently popularized by DAVIS [38,40] and YoutubeVOS [51] challenges.\n\nUnlike in tracking, video object segmentation challenges typically consider large targets observed for less than 100 frames with low background distractor presence. Top video object segmentation approaches thus fare poorly in shortterm tracking scenarios [24] where the target covers a fraction of the image, substantially changes its appearance over a longer period and moves over a cluttered background.\n\nBest trackers apply visual model adaptation, but in the case of segmentation errors it leads to an irrecoverable tracking failure [41]. Because of this, in the past, segmentation has played only an auxiliary role in template-based trackers [1], constrained DCF learning [33] and tracking by 3D model construction [20].\n\nRecently, the SiamRPN [28] tracker has been extended to produce high-quality segmentation masks in two stages [50] -the target bounding box is first localized by SiamRPN branches and then a segmentation mask is computed only within this region by another branch. The twostage processing misses the opportunity to treat localization and segmentation jointly to increase robustness. Another drawback is that a fixed template is used that cannot be discriminatively adapted to the changing scene.\n\nWe propose a new single-shot discriminative segmentation tracker, D3S, that addresses the above-mentioned limitations. The target is encoded by two discriminative visual models -one is adaptive and highly discriminative, but geometrically constrained to an Euclidean motion (GEM), while the other is invariant to broad range of transformation (GIM, geometrically invariant model), see Figure 1.\n\nGIM sacrifices spatial relations to allow target localization under significant deformation. On the other hand, GEM predicts only position, but discriminatively adapts to the target and acts as a selector between possibly multiple target segmentations inferred by GIM. In contrast to related trackers [50,27,10], the primary output of D3S is a segmentation map computed in a single pass through the network, which is trained end-to-end for segmentation only ( Figure 2). Some applications and most tracking benchmarks require reporting the target location as a bounding box. As a secondary contribution, we propose an effective method for interpreting the segmentation mask as a rotated rectangle. This avoids an error-prone greedy search and naturally addresses changes in location, scale, aspect ratio and rotation.\n\nD3S outperforms all state-of-the-art trackers on most of the major tracking benchmarks [23,24,19,35] despite not being trained for bounding box tracking. In video object segmentation benchmarks [38,40], D3S outperforms the leading segmentation tracker [50] and performs on par with top video object segmentation algorithms (often tuned to a specific domain), yet running orders of magnitude faster. Note that D3S is not re-trained for different benchmarks -a single pre-trained version shows remarkable generalization ability and versatility."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 208512936, "title": "Siam R-CNN: Visual Tracking by Re-Detection", "paper_id": "069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9", "abstract": "We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www.vision.rwth-aachen.de/page/siamrcnn.", "intro": "We approach Visual Object Tracking using the paradigm of Tracking by Re-Detection. We present a powerful novel re-detector, Siam R-CNN, an adaptation of Faster R-CNN [73] with a Siamese architecture, which re-detects a template object anywhere in an image by determining if a region proposal is the same object as a template region, and regressing the bounding box for this object. Siam R-CNN is robust against changes in object size and aspect ratio as the proposals are aligned to the same size, which is in contrast to the popular cross-correlation-based methods [49].\n\nTracking by re-detection has a long history, reaching back to the seminal work of Avidan [1] and Grabner et al. [28]. Re-detection is challenging due to the existence of distractor objects that are very similar to the template object. In the past, the problem of distractors has mainly been approached by strong spatial priors from previous predictions [4,49,48], or by online adaptation [1,28,2,75,30,76,42]. Both of these strategies are prone to drift.\n\nWe instead approach the problem of distractors by making two novel contributions beyond our Siam R-CNN re- \u2020 Work performed both while at the RWTH Aachen University and on a research visit at the University of Oxford. detector design. Firstly we introduce a novel hard example mining procedure which trains our re-detector specifically for difficult distractors. Secondly we propose a novel Tracklet Dynamic Programming Algorithm (TDPA) which simultaneously tracks all potential objects, including distractor objects, by re-detecting all object candidate boxes from the previous frame, and grouping boxes over time into tracklets (short object tracks). It then uses dynamic programming to select the best object in the current timestep based on the complete history of all target object and distractor object tracklets. By explicitly modeling the motion and interaction of all potential objects and pooling similarity information from detections grouped into tracklets, Siam R-CNN is able to effectively perform long-term tracking, while being resistant to tracker drift, and being able to immediately re-detect objects after disappearance. Our TDPA requires only a small set of new re-detections in each timestep, updating its tracking history iteratively online. This allows Siam R-CNN to run at 4.7 frames per second (FPS) and its speed-optimized variant to run at more than 15 FPS.\n\nWe present evaluation results on a large number of datasets. Siam R-CNN outperforms all previous methods on six short-term tracking benchmarks as well as on four long-term tracking benchmarks, where it achieves especially strong results, up to 10 percentage points higher than previous methods. By obtaining segmentation masks using an off-the-shelf box-to-segmentation network, Siam R-CNN also outperforms all previous Video Object Segmentation methods that only use the first-frame bounding box (without the mask) on four recent VOS benchmarks."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 235732286, "title": "Do Different Tracking Tasks Require Different Appearance Models?", "paper_id": "3c574538e1d37cc5f7428aeda5e106c932a48e12", "abstract": "Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple ``heads'' that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems.", "intro": "Unlike popular image-based computer vision tasks such as classification and object detection, which are (for the most part) unambiguous and clearly defined, the problem of object tracking has been considered under different setups and scenarios, each motivating the design of a separate set of benchmarks and methods. For instance, for the Single Object Tracking (SOT) and Video Object Segmentation (VOS) communities [93,40,65], tracking means estimating the location of an arbitrary user-annotated target object throughout a video, where the location of the object is represented by a bounding box in SOT and by a pixel-wise mask in VOS. Instead, in multiple object tracking settings (MOT [56], MOTS [80] and PoseTrack [2]), tracking means connecting sets of (often given) detections across video frames to address the problem of identity association and forming trajectories. Despite these tasks only differing in the number of objects per frame to consider and observation format (bounding boxes, keypoints or masks), the best practices developed by the methods tackling them vary significantly.\n\nThough the proliferation of setups, benchmarks and methods is positive in that it allows specific use cases to be thoroughly studied, we argue it makes increasingly harder to effectively study one of the fundamental problems that all these tasks have in common, i.e. what constitutes a good representation to track objects throughout a video? Recent advancements in large-scale models for language [20,8] and vision [32,13] have suggested that a strong representation can help addressing multiple downstream tasks. Similarly, we speculate that a good representation is likely to benefit many different tracking tasks, regardless of their specific setup. In order to validate our speculation, in this paper we present a framework that allows to adopt the same appearance model to address five different tracking tasks (Figure 2). In our taxonomy (Figure 4), we consider existing tracking tasks as problems that have either propagation or association at their core. When the core problem is propagation (as in SOT and VOS), one has to localise a target object in the current frame given its location in the previous one. Instead, in association problems (MOT, MOTS, and PoseTrack), target states in both previous and current frames are given, and the goal is to determine the correspondence between the two sets of observations. We show how most tracking tasks currently considered by the community can be simply expressed starting from the primitives of propagation or association. For propagation tasks, we employ existing box and mask propagation algorithms [7,84,81]. For association tasks, we propose a novel reconstruction-based metric that leverages fine-grained correspondence to measure similarities between observations. In the proposed framework, each individual task is assigned to a dedicated \"head\" that allows to represent the object(s) in the appropriate format to compare against prior arts on the relevant benchmarks.\n\nNote that, in our framework, only the appearance model contains parameters that can be learned via back-propagation, and that we do not experiment with appearance models that have been trained on specific tracking tasks. Instead, we adopt models trained via recent self-supervised learning (SSL) techniques and that have already demonstrated their effectiveness on a variety of image-based tasks. Our motivation is twofold. First, SSL models are particularly interesting for our use-case, as they are explicitly conceived to be of general purpose. As a byproduct, our work also serves the purpose of evaluating and comparing appearance models obtained from self-supervised learning approaches (see Figure 1). Second, we hope to facilitate the tracking community in directly benefiting from the rapid advancements of the self-supervised learning literature.\n\nTo summarise, the contributions of our work are as follows:\n\n\u2022 We propose UniTrack, a framework that supports five tracking tasks: SOT [93], VOS [65], MOT [56], MOTS [80], and PoseTrack [2]; and that can be easily extended to new ones.\n\n\u2022 We show how UniTrack can leverage many existing general-purpose appearance models to achieve a performance that is competitive with the state-of-the-art on several tracking tasks.\n\n\u2022 We propose a novel reconstruction-based similarity metric for association that preserves fine-grained visual features and supports multiple observation formats (box, mask and pose).\n\n\u2022 We perform an extensive evaluation of self-supervised models, significantly extending the empirical analysis of prior literature to video-based tasks."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 250526428, "title": "Towards Grand Unification of Object Tracking", "paper_id": "802e54d116cb34c060c6190b874842d209854e9e", "abstract": "We present a unified method, termed Unicorn, that can simultaneously solve four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the same model parameters. Due to the fragmented definitions of the object tracking problem itself, most existing trackers are developed to address a single or part of tasks and overspecialize on the characteristics of specific tasks. By contrast, Unicorn provides a unified solution, adopting the same input, backbone, embedding, and head across all tracking tasks. For the first time, we accomplish the great unification of the tracking network architecture and learning paradigm. Unicorn performs on-par or better than its task-specific counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17, BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will serve as a solid step towards the general vision model. Code is available at https://github.com/MasterBin-IIAU/Unicorn.", "intro": "Compared with weak AI designed for solving one specific task, artificial general intelligence (AGI) is expected to understand or learn any intellectual task that a human being can. Although there is still a large gap between this ambitious goal and the intellectual algorithms of today, some recent works [20,51,81,21] have begun to explore the possibility of building general vision models to address several vision tasks simultaneously.\n\nObject tracking is one of the fundamental tasks in computer vision, which aims to build pixel-level or instance-level correspondence between frames and to output trajectories typically in the forms of boxes or masks. Over the years, according to different application scenarios, the object tracking problem has been mainly divided into four separate sub-tasks: Single Object Tracking (SOT) [18,42], Multiple Object Tracking (MOT) [40,80], Video Object Segmentation (VOS) [46], and Multi-Object Tracking and Segmentation (MOTS) [61,80]. As a result, most tracking approaches are developed for only one of or part of the sub-tasks. Despite convenience for specific applications, this fragmented situation brings into the following drawbacks: (1) Trackers may over-specialize on the characteristic of specific sub-tasks, lacking in the generalization ability. (2) Independent model designs cause redundant parameters. For example, recent deep-learningbased trackers usually adopt similar backbones architectures, but the separate design philosophy hinders the potential reuse of parameters. It is natural to ask a question: Can all main-stream tracking tasks be solved by a unified model?\n\nAlthough some works [64,36,62,70,39] attempt to unify SOT&VOS or MOT& MOTS by adding a mask branch to the existing box-level tracking system, there is still little progress towards the unification of SOT and MOT. There are mainly three obstacles hindering this process. (1) The characteristics of tracked objects vary. MOT usually tracks tens even hundreds of instances of specific categories. In contrast, SOT needs to track one target given in the reference frame no matter what class it belongs to. (2) SOT and MOT require different types of correspondence. SOT requires distinguishing the target from the background. However, MOT needs to match the currently detected objects with previous trajectories. (3) Most SOT methods [3,29,15,5,9,77] only take a small search region as the input to save computation and filter potential distractors. However, MOT algorithms [2,74,8,67,85,90,39] usually take the high-resolution full image as the input for detecting instances as completely as possible.\n\nTo conquer these challenges, we propose two core designs: the target prior and the pixel-wise correspondence. To be specific, (1) the target prior is an additional input for the detection head and serves as the switch among four tasks. For SOT&VOS, the target prior is the propagated reference target map, enabling the head to focus on the tracked target. For MOT&MOTS, by setting the target prior as zero, the head degenerates into the usual class-specific detection head smoothly. (2) The pixel-wise correspondence is the similarity between all pairs of points from the reference frame and the current frame. Both the SOT correspondence (C SOT \u2208 R h \u2032 w \u2032 \u00d7hw ) and the MOT correspondence (C MOT \u2208 R M \u00d7N ) are subsets of the pixel-wise correspondence (C pix \u2208 R hw\u00d7hw ). (3) With the help of the informative target prior and the accurate pixel-wise correspondence, the design of the search region becomes unnecessary for SOT, leading to unified inputs as the full image for SOT and MOT.\n\nTowards the unification of object tracking, we propose Unicorn, a single network architecture to solve four tracking tasks. It takes the reference frame and the current frame as the inputs and produces their visual features by a weight-shared backbone. Then a feature interaction module is exploited to build pixel-wise correspondence between two frames. Based on the correspondence, a target prior is generated by propagating the reference target to the current frame. Finally, the target prior and the visual features are fused and sent to the detection head to get the tracked objects for all tasks.\n\nWith the unified network architecture, Unicorn can learn from various sources of tracking data and address four tracking tasks with the same model parameters.\n\nExtensive experiments show that Unicorn performs on-par or better than taskspecific counterparts on 8 challenging benchmarks from four tracking tasks.\n\nWe summarize that our work has the following contributions:\n\n-For the first time, Unicorn accomplishes the great unification of the network architecture and the learning paradigm for four tracking tasks. -Unicorn bridges the gap among methods of four tracking tasks by the target prior and the pixel-wise correspondence. -Unicorn puts forwards new state-of-the-art performance on 8 challenging tracking benchmarks with the same model parameters. This achievement will serve as a solid step towards the general vision model."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 247593925, "title": "Robust Visual Tracking by Segmentation", "paper_id": "9286efaa3dba58837b628f61f4940a09b3eeb85c", "abstract": "Estimating the target extent poses a fundamental challenge in visual object tracking. Typically, trackers are box-centric and fully rely on a bounding box to define the target in the scene. In practice, objects often have complex shapes and are not aligned with the image axis. In these cases, bounding boxes do not provide an accurate description of the target and often contain a majority of background pixels. We propose a segmentation-centric tracking pipeline that not only produces a highly accurate segmentation mask, but also internally works with segmentation masks instead of bounding boxes. Thus, our tracker is able to better learn a target representation that clearly differentiates the target in the scene from background content. In order to achieve the necessary robustness for the challenging tracking scenario, we propose a separate instance localization component that is used to condition the segmentation decoder when producing the output mask. We infer a bounding box from the segmentation mask, validate our tracker on challenging tracking datasets and achieve the new state of the art on LaSOT with a success AUC score of 69.7%. Since most tracking datasets do not contain mask annotations, we cannot use them to evaluate predicted segmentation masks. Instead, we validate our segmentation quality on two popular video object segmentation datasets.", "intro": "Visual object tracking is the task of estimating the state of a target object for each frame in a video sequence. The target is solely characterized by its initial state in the video. Current approaches predominately characterize the state itself with a bounding box. However, this only gives a very coarse representation of the target in the image. In practice, objects often have complex shapes, undergo substantial deformations. Often, targets do not align well with the image axes, while most benchmarks use axis-aligned bounding boxes. In such cases, the majority of the image content inside the target's bounding box often consists of background regions which provide limited information about the object itself. In contrast, a segmentation mask precisely indicates the object's extent in the image (see Fig. 1 frames #1600 and #3200). Such information is vital in a variety of applications, including video analysis, video editing, and robotics. In this work, we therefore develop an approach for accurate and robust target object segmentation, even in the highly challenging tracking datasets [16,36]. , the VOS method LWL [5] and our proposed method on two tracking sequences from the LaSOT [16] dataset. The ground-truth annotation ( ) is shown in each frame for reference. Our approach is more robust and predicts a more accurate target representation.\n\nWhile severely limiting the information about the target's state in the video, the aforementioned issues with the bounding box representation can itself lead to inaccurate bounding box predictions, or even tracking failure. To illustrate this, Fig. 1 shows two typical tracking sequences. The tracking method STARK [55] (first row) fails to regress bounding boxes that contain the entire object (#1600, #1400) or even starts tracking the wrong object (#0700). Conversely, segmentation masks are a better fit to differentiate pixels in the scene that belong to the background and the target. Therefore, a segmentation-centric tracking architecture designed to work internally with a segmentation mask of the target instead of a bounding box has the potential to learn better target representations, because it can clearly differentiate background from foreground regions in the scene.\n\nA few recent tracking methods [47,54] have recognized the advantage of producing segmentation masks instead of bounding boxes as final output. However, these trackers are typically bounding-box-centric and the final segmentation mask is obtained by a separate box-to-mask post-processing network. These methods do not leverage the accurate target definition of segmentation masks to learn a more accurate and robust internal representation of the target.\n\nIn contrast, most Video Object Segmentation (VOS) methods [38,5] follow a segmentation-centric paradigm. However, these methods are not designed for the challenging tracking scenarios. Typical VOS sequences consist only of a few hundred frames [41] whereas multiple sequences of more than ten thousand frames exist in tracking datasets [16]. Due to this setup, VOS methods focus on producing highly accurate segmentation masks but are sensitive to distractors, substantial deformations and occlusions of the target object. Fig. 1 shows two typical tracking sequences where the VOS method LWL [5] (second row) produces a fine-grained segmentation mask of the wrong object (#3200) or is unable to detect only the target within a crowd (#0700, #1400).\n\nWe propose Robust Visual Tracking by Segmentation (RTS), a unified tracking architecture capable of predicting accurate segmentation masks. To design a segmentation-centric approach, we take inspiration from the aforementioned LWL [5] method. However, to achieve robust and accurate segmentation on Visual Object Tracking (VOT) datasets, we introduce several new components. In particular, we propose an instance localization branch trained to predict a target appearance model, which allows occlusion detection and target identification even in cluttered scenes. The output of the instance localization branch is further used to condition the high-dimensional mask encoding. This allows the segmentation decoder to focus on the localized target, leading to a more robust mask prediction. Since our proposed method contains a segmentation and instance memory that need to be updated with previous tracking results, we design a memory management module. This module first assesses the prediction quality, decides whether the sample should enter the memory and, when necessary, triggers the model update. Contributions Our contributions are the following: (i) We propose a unified tracking architecture capable of predicting robust classification scores and accurate segmentation masks. We design separate feature spaces and memories to ensure optimal receptive fields and update rates for segmentation and instance localization. (ii) To produce a segmentation mask which agrees with the instance prediction, we design a fusion mechanism that conditions the segmentation decoder on the instance localization output and leads to more robust tracking performance. (iii) We introduce an effective inference procedure capable of fusing the instance localization output and mask encoding to ensure both robust and accurate tracking. (iv) We perform comprehensive evaluation and ablation studies of the proposed tracking pipeline on multiple popular tracking benchmarks. Our approach achieves the new state of the art on LaSOT with an area-under-the-curve (AUC) score of 69.7%."}
{"tabids": ["20317c3b-01c9-4813-9868-1b7139fc7c74"], "corpus_id": 261214723, "title": "Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation", "paper_id": "483a8014ac9c0048c4bcf10b404ab3f5b0f46e4f", "abstract": "Tracking any given object(s) spatially and temporally is a common purpose in Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint tracking and segmentation have been attempted in some studies but they often lack full compatibility of both box and mask in initialization and prediction, and mainly focus on single-object scenarios. To address these limitations, this paper proposes a Multi-object Mask-box Integrated framework for unified Tracking and Segmentation, dubbed MITS. Firstly, the unified identification module is proposed to support both box and mask reference for initialization, where detailed object information is inferred from boxes or directly retained from masks. Additionally, a novel pinpoint box predictor is proposed for accurate multi-object box prediction, facilitating target-oriented representation learning. All target objects are processed simultaneously from encoding to propagation and decoding, as a unified pipeline for VOT and VOS. Experimental results show MITS achieves state-of-the-art performance on both VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor by around 6% on the GOT-10k test set, and significantly improves the performance of box initialization on VOS benchmarks. The code is available at https://github.com/yoxu515/MITS.", "intro": "Visual object tracking (VOT) [57,23,32,33] and video object segmentation (VOS) [63,84,79,37] are two critical tasks in computer vision.Visual object tracking involves identifying and tracking specific object(s) in a video stream over time.Video object segmentation aims to segment given object(s) in a video sequence and separate it from the background.Both tasks are essential for applications such as video surveillance and autonomous driving.In VOT, object sizes and positions are indicated with boxes as box representation, while in VOS object shapes and contours are marked with pixel-level masks as mask representation.Despite their differences, VOT and VOS share similarities.Both tasks require the ability to identify and locate the target object in a video stream accurately in spatial dimension, and to be robust against challenges such as occlusion and fast motion in temporal dimension.\n\nIn view of the similarity, tracking and segmentation may have unified multi-knowledge representations [91] and have been explored jointly in some works.1) Unification.A straightforward solution is to perform conversion between boxes and masks to utilize VOT methods on VOS or VOS methods on VOT.A mask can be converted to a box easily, but generating a mask from a box is hard.Some methods were proposed to address the box-to-mask estimation problem [47,89,102].However, separate but not unified models hinder end-to-end training and are inconvenient to manage in practical applications.2) Compatibility.Several studies have attempted to unify these two tasks into a single frame-work.However, some of them [78,75,62] still lack compatibility and flexibility in box/mask input/output and resort to extra models.3) Multi-Object.Despite that some methods [48,81,86] possess strong compatibility across VOT and VOS, they mainly focus on the single object scenario and use an ensemble strategy to aggregate the separate result of each object in the multiple object scenario.\n\nTherefore, this paper aims to unify VOS and VOT and improve above shortcomings by integrating boxes and masks in a multi-object framework as Multi-object Integrated Tracking and Segmentation (MITS), as shown in Figure 1.For compatibility problem, a unified identification module is proposed to take both reference boxes in VOT and masks in VOS for initialization.The unified identification module encodes the reference boxes or masks into the unified identification embedding by assigning identities to objects.The coarse identification embedding from boxes is further refined to mitigate the gap between mask and box initialization.The unified identification module is more convenient than borrowing an extra model because it is trained with the whole model in an end-to-end manner.\n\nBesides, the novel pinpoint box predictor is proposed for joint training and prediction with the mask decoder.Previous corner head or center head estimates a box by corners or a center point, which are not have to be inside the object.Emphasizing exterior points may distract learning targetoriented features and affect the mask prediction.To address this problem, we estimate the box by localizing pinpoints, which are always on the edge of the object.However, directly supervise the learning of pinpoints is infeasible due to the lack of annotation.Therefore we perform decoupled aggregation on the pinpoint maps and determine the box only by side-aligned pinpoint coordinates.\n\nAll the modules in our framework are not only compatible with two tasks, but also able to process multiple objects simultaneously.The multi-object training and prediction make our framework efficient and robust under complex scenes with multiple objects.Extensive experiments are conducted to demonstrate the strong compatibility and capacity of our framework.Experimental results show that our framework achieves SOTA performance on VOT benchmarks including LaSOT [23], TrackingNet [57] and GOT-10k [32], and VOS benchmark YouTube-VOS [84].Our method improves 6% over previous SOTA VOT method on GOT-10k, and significantly improves the performance of box initialization on VOS benchmarks.In summary, our contributions are:\n\n\u2022 We present a multi-object framework integrating boxes and masks for unified tracking and segmentation.\n\n\u2022 The unified identification module is proposed to accept both masks and boxes for initialization.\n\n\u2022 A novel pinpoint box predictor is proposed for accurate box prediction together with the mask decoder."}
{"tabids": ["a9d669ae-0620-464b-addf-d5c677d93c1e"], "corpus_id": 219636007, "title": "Ensemble Distillation for Robust Model Fusion in Federated Learning", "paper_id": "053f4d6715a4dba6f8103456fc1bb5fd6a5266c4", "abstract": "Federated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios. In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.", "intro": "Federated Learning (FL) has emerged as an important machine learning paradigm in which a federation of clients participate in collaborative training of a centralized model (Shokri & Shmatikov, 2015;McMahan et al., 2016;Smith et al., 2017;Caldas et al., 2018;Bonawitz et al., 2019;Li et al., 2019;Kairouz et al., 2019). The clients send their model parameters to the server but never their private training datasets, thereby ensuring a basic level of privacy. Among the key challenges in federated training are communication overheads and delays (one would like to train the central model with as few communication rounds as possible), and client heterogeneity: the training data (non-i.i.d.-ness), as well as hardware and computing resources, can change drastically among clients, for instance when training on commodity mobile devices.\n\nClassic training algorithms in FL, such as federated averaging (FEDAVG) (McMahan et al., 2016) and its recent adaptations (Mohri et al., 2019;Li et al., 2020;Hsu et al., 2019;Karimireddy et al., 2019;Hsu et al., 2020;Reddi et al., 2020), are all based on directly averaging of the participating client's parameters and can hence only be applied if all client's models have the same size and structure. In contrast, ensemble learning methods (You et al., 2017;Furlanello et al., 2018;Anil et al., 2018;Dvornik et al., 2019;Park & Kwak, 2019;Liu et al., 2019;Wu et al., 2019) allow to combine multiple heterogeneous weak classifiers by averaging the predictions of the individual models instead. However, applying ensemble learning techniques directly in FL is infeasible in practice due to the large number of participating clients. Storing a different model per client on the server is not only impossible due to memory constraints, but also renders training and inference inefficient, and hinders knowledge transfer between clients.\n\nTo enable federated learning in more realistic settings, we propose to use ensemble distillation (Bucilu\u01ce et al., 2006;Hinton et al., 2015) for robust model fusion (FedDF). Our scheme leverages unlabeled data or artificially generated examples (e.g. by a GAN's generator (Goodfellow et al., 2014)) to aggregate knowledge from all received (heterogeneous) client models. We demonstrate with thorough empirical results that our ensemble distillation approach not only addresses the existing quality loss issue (Hsieh et al., 2019) of Batch Normalization (BN) (Ioffe & Szegedy, 2015) for networks in a homogeneous FL system, but can also break the knowledge barriers among heterogeneous client models. Our main contributions are:\n\n\u2022 We propose a distillation framework for robust federated model fusion, which allows for heterogeneous client models and data, and is robust to the choices of neural architectures. \u2022 We show in extensive numerical experiments on various CV/NLP datasets (CIFAR-10/100, Ima-geNet, AG News, SST2) and settings (heterogeneous models and/or data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique.\n\nWe further provide insights on when FedDF can outperform FEDAVG (see also Fig. 1 that highlights an intrinsic limitation of parameter averaging based approaches) and what factors influence FedDF.  (2019); Wang et al. (2020) propose to use optimal transport and other alignment schemes to first align or match individual neurons of the neural nets layer-wise before averaging the parameters. However, these layer-based alignment schemes necessitate client models with the same number of layers and structure, which is restrictive in heterogeneous systems in practice. Another line of work aims to improve local client training, i.e., client-drift problem caused by the heterogeneity of local data (Li et al., 2018;Karimireddy et al., 2019). For example, FEDPROX (Li et al., 2018) incorporates a proximal term for the local training. Other techniques like acceleration, recently appear in preprints (Hsu et al., 2019;2020;Reddi et al., 2020)."}
{"tabids": ["a9d669ae-0620-464b-addf-d5c677d93c1e"], "corpus_id": 263869520, "title": "Resource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion", "paper_id": "98fd8b8476bfa9da924c86543141a6fb920cfc4c", "abstract": "With increasing concern about user data privacy, federated learning (FL) has been developed as a unique training paradigm for training machine learning models on edge devices without access to sensitive data. Traditional FL and existing methods directly employ aggregation methods on all edges of the same models and training devices for a cloud server. Although these methods protect data privacy, they are not capable of model heterogeneity, even ignore the heterogeneous computing power, and incur steep communication costs. In this paper, we purpose a resource-aware FL to aggregate an ensemble of local knowledge extracted from edge models, instead of aggregating the weights of each local model, which is then distilled into a robust global knowledge as the server model through knowledge distillation. The local model and the global knowledge are extracted into a tiny size knowledge network by deep mutual learning. Such knowledge extraction allows the edge client to deploy a resource-aware model and perform multi-model knowledge fusion while maintaining communication ef\ufb01ciency and model heterogeneity. Empirical results show that our approach has sig-ni\ufb01cantly improved over existing FL algorithms in terms of communication cost and generalization performance in heterogeneous data and models. Our approach reduces the communication cost of VGG-11 by up to 102 \u00d7 and ResNet-32 by up to 30 \u00d7 when training ResNet-20 as the knowledge network.", "intro": "Federated learning (FL) has emerged as a novel machine learning paradigm for distributed clients to participate in the collaborative training of a centralized model.FL brings model asynchronous training on edge, where devices (e.g., mobile phones and IoT devices) extract the knowledge on the private-sensitive training data and then upload the learned models to the cloud for aggregation.FL stores user data locally and restricts direct access to it from cloud servers; thereby, this paradigm not only enhances privacypreserving but also introduces several inherent advantages, including model accuracy, cost efficiency, and diversity.With the massive demand for data in today's machine learning models and the social considerations of artificial intelligence (e.g., privacy and security (Yang et al. 2019;Curzon et al. 2021)), federated learning has great potential and role in counterpoising this trade-off.\n\nFederated learning has already shown its potential in practical applications, including health care (Sheller et al. 2020), environment protection (Hu et al. 2018), electrical vehicles (Saputra et al. 2019), and etc. Google, Apple, and NVIDIA are using FL for their applications nowadays (e.g., Google Keyboard (Yang et al. 2018;Chen et al. 2019), Apple Siri (Freudiger 2019;Paulik et al. 2021), NVIDIA medical imaging (Li et al. 2019)).In consequence, designing efficient FL models and deploying them effectively and fairly on edge devices is crucial for improving the performance of edge computing in the future.\n\nTraditional FL, represented by FedAvg (McMahan et al. 2017) in Figure 1 (b), broadcasts global model parameters to selected edge devices, performs a weighted average of the trained local model parameters based on their local data and updates the global model.This process is then iterated through many communication rounds to achieve good performance across all edge devices.From the framework of traditional FL, we observe the following limitations: First, iterative model weights sharing between network edges and the servers introduces excessive communication overhead.Second, the ever-increasing computational and memory requirements of AI models (e.g., deep neural networks -DNNs) and the heterogeneous computing power of edge devices make deploying the same models on resource-constrained edge devices problematic.For example, it is infeasible to deploy a large model on a resourcelimited edge device.In addition, local data from the realworld is typically imbalance/non-independent identically distributed (non-IID), which can easily lead to training failure in a decentralized situation (Michieli and Ozay 2021).Deep learning models are generally over-parameterized, and when local data are heterogeneous, aggregating local models tends to cause over-fitting, leading to high variance in learning and prediction (Jiang et al. 2019;Nishio and Yonetani 2019).Therefore, performing weight-average aggregating of local models, or even deploying the same model on devices with different computing power, as employed by most existing FL methods, may produce an unfair, ineffective global model and fail to deploy it efficiently.\n\nTo overcome the above limitations of previous FL methods, we came up with the idea of performing resource-aware federated learning using knowledge extraction and multimodel fusion (FedKEMF), as illustrated in Figure 1 (a).The benefits of our approach FedKEMF are: i) It distills the client's knowledge before aggregating it to the server, which prevents such large edge models from being over-parameterized while reducing the resource constraints of edge devices.ii) It effectively reduces communication costs by exchanging distilled tiny size neural networks between edges and clouds instead of the original large models.iii) Ensembling knowledge from edges efficiently robust the global model, reducing the risk of over-fitting and variance, and achieving better generalization performance in FL.Furthermore, our purposed approach is aware of resourceconstraint and applies multi-model fusion to break the limitations of model structure, deploy models fairly on edge devices, and enable federated learning in a more realistic setting.\n\nWe conduct extensive experiments on non-IID data settings and heterogeneous client models to validate the performance of the purposed approach and compare it with existing FL methods.Our results show that, when optimizing a small neural network as the knowledge network, FedKEMF significantly reduces communication cost and achieves better performance using fewer communication rounds.Another line in FL is personalized FL, which focuses on the problem of statistical heterogeneity.Personalized FL aims to personalize the global model for each client in FL and find how to develop improved personalized models that can benefit a large majority of clients (Kulkarni, Kulkarni, and Pant 2020).Although we have the same consideration of device heterogeneity (memory storage and computation power), data heterogeneity (non-IID data), and model heterogeneity (model structure and size), we focus on how to extract knowledge from different types of models and their corresponding training devices to build robust global knowledge."}
{"tabids": ["a9d669ae-0620-464b-addf-d5c677d93c1e"], "corpus_id": 250210682, "title": "Learn from Others and Be Yourself in Heterogeneous Federated Learning", "paper_id": "9787d29aa66966629350a5c0806e32a19882a2c9", "abstract": "Federated learning has emerged as an important distributed learning paradigm, which normally involves collaborative updating with others and local updating on private data. However, heterogeneity problem and catastrophic forgetting bring distinctive challenges. First, due to non-i.i.d (identically and independently distributed) data and heterogeneous architectures, models suffer performance degradation on other domains and communication barrier with participants models. Second, in local updating, model is separately optimized on private data, which is prone to overfit current data distribution and forgets previously acquired knowledge, resulting in catastrophic forgetting. In this work, we propose FCCL (Federated CrossCorrelation and Continual Learning). For heterogeneity problem, FCCL leverages unlabeled public data for communication and construct cross-correlation matrix to learn a generalizable representation under domain shift. Mean- while, for catastrophic forgetting, FCCL utilizes knowledge distillation in local updating, providing inter and intra domain information without leaking privacy. Empirical results on various image classification tasks demonstrate the effectiveness of our method and the efficiency of modules.", "intro": "Deep learning algorithms have achieved remarkable progress, owing to the availability of large-scale data [8,51,69]. However, in the real world, data are commonly dispersed over different participants (e.g., mobile devices, organizations). Due to growing privacy concerns and strict data protection regulations [84], participants cannot integrate data together to train a model. Driven by such realistic issues, federated learning [33,34,58,59,89] provides a privacy-preserving paradigm, where participants collabo- Figure 1. Problem illustration of heterogeneous federated learning. (a) In collaborative updating, how to handle communication problem of heterogeneous models and learn a generalizable representation under heterogeneous data (domain shift)? (b) In local updating, how to alleviate catastrophic forgetting to present stable and satisfactory performance in both inter and intra domains? ratively learn a model without leaking private data. It has been an active and challenging research topic and shows promising results in real-world setting [17,19,29,52,54].\n\nAlong with its pilot progress, researches on federated learning are baffled by some key challenges [30,42]. An inevitable and practical challenge is heterogeneity problem. On the one hand, distributed data might be non-i.i.d (identically and independently distributed), leading to data heterogeneity [30,39,95]. A myriad of methods [43,46,73,77] incorporate extra proximal terms to handle the data in label distribution skew (prior probability shift) [30], neglecting the fact that there exists domain shift (same label, different features) [60,64,66]. In particular, private model suffers severe performance degradation on other domains with no-ticeably different distribution. As a result, learning a generalizable representation under domain shift is technically challenging. On the other hand, due to different design criteria, distinct hardware capabilities [20,86] and intellectual property rights [56], participants require to customize models, which poses a practical challenge: model heterogeneity. Preceding methods are developed under the assumption that local models share parameters or gradients, which cannot work on heterogeneous models. In order to solve this problem, a main stream of subsequent effort leverages knowledge transfer through labeled data [38,74], shared model [48,72,92] or group operation [21,50]. But these methods have different limitations. Specifically, labeled data require server to collect data with similar distributions to private data, which causes costly human efforts and needs special domain expertise. For shared model, it raises computational cost and necessitates additional model structure in participant side. Group operation leverages unlabeled public data to measure distribution divergence. However, these methods mainly focus on label distribution skew and consider the performance on one domain. Simultaneously considering data and model heterogeneity, an essential issue has long been overlooked: (a) How to learn a generalizable representation in heterogeneous federated learning?\n\nBesides heterogeneity problem, another impediment for federated learning steams from its paradigm. Generally, federated learning could be viewed as a two-step cyclic process: collaborative updating and local updating [58,89]. In collaborative updating, participants learn from others. In local updating, model is optimized on private data, which is prone to overfit current knowledge and forget previous knowledge, resulting in catastrophic forgetting [57]. To tackle this challenge, one type of methods typically performs fine-tuning for several rounds [38,50,58,74,88]. However, carefully configuring hyper-parameters to achieve satisfactory performance is time-consuming and cannot tackle this problem systematically. Current popular solutions [41,43,73,77] focus on calculating parameter stiffness to regulate models, which can not explicitly depict the degree of effect from different participants. Consequently, a natural question arises: (b) How to balance multiple knowledge to reduce catastrophic forgetting? We further explain heterogeneity problem and catastrophic forgetting in Fig. 1.\n\nFor the heterogeneity problem, we take inspiration from the self-supervised learning [5,6,11,13,18,25,49,91,94]. In particular, self-supervised learning aims to learn a generalizable representation through rich and diverse data for downstream tasks and unseen classes. Intuitively, we expect that the models would present similar logits output for the same classes in different domains. This motivates us to leverage unlabeled public data for Federated Cross-Correlation Learning, which is diverse and easy to obtain. Specifically, we try to maximize the similarity between log-its output and minimize the redundancy within logits output on unlabeled public data. Through correlating same dimensions and decorrelating different dimensions on logits output, models would learn class invariance and encourage the diversity of different classes. Thus, our method handles the communication problem in heterogeneous models and learns a generalizable representation under domain shift.\n\nTo handle catastrophic forgetting, we develop Federated Continual Learning via knowledge distillation [2,24] in local updating to continually learn from inter and intra domains. To avoid forgetting inter domain information in local updating stage, we propose to distill the knowledge of intradomain (local) model learned in previous rounds, where it captured the inter domain information after communication with other participants. In addition, for intra domain forgetting problem, we leverage the initially pretrained local model (without knowledge learned from others) to constrain the later local updating for each participant. Therefore, balancing knowledge through distillation with these two models is reasonable to handle the catastrophic forgetting.\n\nIn this work, we propose a novel federated learning method, dubbed FCCL (Federated Cross-Correlation and Continual Learning). The overview of FCCL is illustrated in Fig. 2. In a nutshell, our contributions are three-fold:\n\n\u2022 We formulate a simple and effective method for heterogeneous federated learning. Through leveraging unlabeled public data and adopting self-supervised learning, heterogeneous models achieve communication and learn a generalizable representation.\n\n\u2022 We explore to alleviate catastrophic forgetting in federated learning. Through inter and intra domain knowledge distillation with updated and pretrained models, it balances knowledge from others and itself.\n\n\u2022 We conduct extensive experiments on two image classification tasks (e.g., Digits [27,37,62,68] and Office-Home [82]) with unlabeled public data [35,69,87]. FCCL achieves superior inter and intra domain performance over related methods. Ablation study on core module validates its efficacy and indispensability."}
{"tabids": ["a9d669ae-0620-464b-addf-d5c677d93c1e"], "corpus_id": 203951869, "title": "FedMD: Heterogenous Federated Learning via Model Distillation", "paper_id": "0a9945cc7ce7f98403358d0c74e9aa2da34e8089", "abstract": "Federated learning enables the creation of a powerful centralized model without compromising data privacy of multiple participants. While successful, it does not incorporate the case where each participant independently designs its own model. Due to intellectual property concerns and heterogeneous nature of tasks and data, this is a widespread requirement in applications of federated learning to areas such as health care and AI as a service. In this work, we use transfer learning and knowledge distillation to develop a universal framework that enables federated learning when each agent owns not only their private data, but also uniquely designed models. We test our framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and observe fast improvement across all participating models. With 10 distinct participants, the final test accuracy of each model on average receives a 20% gain on top of what's possible without collaboration and is only a few percent lower than the performance each model would have obtained if all private datasets were pooled and made directly available for all participants.", "intro": "Deep learning has provided a potentially powerful framework to automate perception and inference. However, large datasets are required to fully realize this potential. In areas like health care, it is often difficult and costly to curate large datasets. For instance, typical hospitals in the US may have only dozens of MRI images of a particular disease that needs to be annotated by human experts and must be protected from potential privacy breaches. Federated learning and similar ideas [1,2] rise to this challenge and effectively train a centralized model while keeping users' sensitive data on device. In particular, federated learning [1,3,4] is optimized for faster communication and is uniquely capable of handling a large number of users.\n\nFederated learning faces many challenges [5], among which, of particular importance is the heterogeneity that appear in all aspects of the learning process. There is system heterogeneity when each participant has a different amount of bandwidth and computational power; this was partly resolved by the native asynchronous scheme of federated learning, which was further refined e.g. to enable active sampling [6,7] and improve fault tolerance [8]. There is also statistical heterogneity (the non i.i.d. problem) where clients have a varying amount of data coming from distinct distributions [9,10,11,12,13,14].\n\nIn this work, we focus on a different type of heterogeneity: the differences of local models. In the original federated framework, all users have to agree on the particular architecture of a centralized model. This is a reasonable assumption when the participants are millions of low capacity devices such as cell phones. In this work, we instead explore extensions to the federated framework that is realistic in a business facing setting, where each participant has capacity and desire to design their 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. own unique model. This arise in areas like health care, finance, supply chain and AI services. For example, when several medical institutions collaborate without sharing private data, they may need to craft their own model to meet distinct specifications. They may not be willing to share details of their models due to privacy and intellectual property concerns. Another example is AI as a service. A typical AI vendor of, e.g. customer service chat bots, may have dozens of client companies. Each client's model is distinct and solves different tasks. The standard practice is to train a client's model with only its own data. It would be immensely beneficial if data from other clients can be utilized without compromising privacy or independency. How can one perform federated learning when each participant has a different model that is a blackbox to others? This is the central question that we will answer in this work.\n\nThis question is intimately related to the non-i.i.d. challenge of federated learning because a natural way to tackle statistical heterogeneity is to have individualized models for each user. Indeed, existing frameworks result in sightly different models. For example, [10] provides a framework for multitask learning if the problem is convex. Approaches based on frameworks such as Bayesian [11], meta-learning [12] and transfer learning [14] also achieve good performance on non-i.i.d. data while allowing a certain amount of model customization. However, to our knowledge, all existing frameworks require a centralized control over the design of local models. Full model independency, while related to the non-i.i.d. problem, is an important new research direction in its own right.\n\nThe key to full model heterogeneity is communication. In particular, there must be a translation protocol enabling a deep network to understand the knowledge of others without sharing data or model architecture. This question touches on fundamental issues in deep learning, such as interpretability and emergent communication protocols. In principle, machines should be able to learn the best communication protocol that is adaptive to any specific use case. As a first step in this direction, we employ a more transparent framework based on knowledge distillation that solves the problem.\n\nTransfer learning is another major framework addressing the scarcity of private data. In this work, our private datasets can be as small as a few samples per class. Therefore using transfer learning from a large public dataset is imperative in addition to federated learning. We leverage the power of transfer learning in two ways. First, before entering the collaboration, each model is fully trained first on the public data and then on its own private data. Second, and more importantly, the blackbox models communicate based on their output class scores on samples from the public dataset. This is realized through knowledge distillation [15], which has been capable of transmitting learned information in a model agnostic way."}
{"tabids": ["a9d669ae-0620-464b-addf-d5c677d93c1e"], "corpus_id": 257687494, "title": "FedGH: Heterogeneous Federated Learning with Generalized Global Header", "paper_id": "6d4a520935e17aca0763630c271293cac63edbf5", "abstract": "Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87% (for model-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53% of communication overhead.", "intro": "Federated learning (FL) [39] has become a widely adopted approach for collaborative model training involving multiple participants with decentralized data under the premise of privacy preservation. Horizontal FL methods, such as FedAvg [27], generally involve a central FL server coordinating multiple FL clients. In each round of distributed model training, the server broadcasts the global model to selected clients. The clients then train the received global model on their respective local datasets and send the updated local models back to the server. The server then updates the global model by aggregating the received local models. The above steps are iteratively executed until the global model converges. Since only the model parameters are transmitted between the server and clients without exposing the raw data, privacy protection is enhanced. Nevertheless, the above paradigm requires all clients to train models with the same structures (i.e., model homogeneity) in order to work.\n\nHowever, in practical cross-device FL scenarios, the clients participating in FL are mostly mobile edge devices with heterogeneous and constrained system resources (e.g., computing power, network bandwidth, memory, storage, and battery capacity) [35,36,40,42,[44][45][46]. This is also referred to as system heterogeneity in FL. Modelhomogeneous FL methods face three limitations in this scenario:\n\n\u2022 Device: when training a large global model, some low-end clients may never be able to join in FL since their limited arXiv:2303.13137v2 [cs.\n\nLG] 1 Aug 2023 system resources preclude them from training large models. As a result, the accuracy of the final global model may be degraded due to the lack of information from these clients. \u2022 Data: the data held by different devices are often not identically and independently distributed (Non-IID), also known as statistical heterogeneity in FL [24,34]. \u2022 Model: if all clients join FL, the capacity of the trained homogeneous models must match the weakest client's system configurations. Unfortunately, training models with a small capacity not only reduces their performance but also wastes high-end clients' system resources due to long idle time.\n\nAlthough model-heterogeneous FL approaches have emerged to address the aforementioned challenges facing model-homogeneous FL, they still have the following limitations. During learning, the high-level design intuition is to separate the training of the homogeneous portion and the heterogeneous portion of the FL model structure into unrelated processes. This not only results in limited performance improvement but also incurs high computation and communication costs [21,33,38]. In addition, some approaches even rely on the availability of suitable public datasets closely related to the learning task in order to leverage knowledge distillation to achieve model-heterogeneous FL [19,22]. However, this is not always viable in practice. Therefore, enabling FL clients to train heterogeneous FL models with the capacity adaptive to system resource limitations and diverse data distributions in an efficient manner remains open.\n\nTo bridge the aforementioned gaps in the model-heterogeneous FL literature, we propose the Federated Global prediction Header (FedGH) approach. It is a novel model-heterogeneous FL framework capable of achieving low communication and computation costs. Under FedGH, each client's local model consists of a heterogeneous feature extractor and a homogeneous prediction header. It leverages the representations extracted by clients' feature extractors to train a global generalized prediction header at the server for all clients to share. The updated global header captures all-class knowledge among multiple clients. The generalized global prediction header replaces each client's local prediction header to transfer global knowledge to clients. In this way, FedGH enables information interaction across heterogeneous clients' models through a shared generalized global prediction header.\n\nBy communicating only the representations and the global prediction header's parameters between clients and the server, FedGH reduces communication costs. By computing local class-averaged representations on FL clients, it reduces computational costs to a level tolerable for mobile edge devices. By not relying on a public dataset, its operation is not limited by the availability of such datasets. By only sending representations which are high-level abstractions of local data, it protects data privacy. We prove the nonconvex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87% (for model-homogeneous FL) and 1.83% (for modelheterogeneous FL) in terms of average test accuracy, while saving up to 85.53% of communication overhead."}
{"tabids": ["a9d669ae-0620-464b-addf-d5c677d93c1e"], "corpus_id": 261031130, "title": "Towards Personalized Federated Learning via Heterogeneous Model Reassembly", "paper_id": "99a0f0f6ada73216389307133e04e9468639fbb2", "abstract": "This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner.", "intro": "Federated learning (FL) aims to enable collaborative machine learning without the need to share clients' data with others, thereby upholding data privacy [1][2][3].However, traditional federated learning approaches [2,[4][5][6][7][8][9][10][11][12] typically enforce the use of an identical model structure for all clients during training.This constraint poses challenges in achieving personalized learning within the FL framework.In real-world scenarios, clients such as data centers, institutes, or companies often possess their own distinct models, which may have varying structures.Training on top of their original models should be a better solution than deploying new ones for collaborative purposes.Therefore, a practical solution lies in fostering heterogeneous model cooperation within FL, while preserving individual model structures.Only a few studies have attempted to address the challenging problem of heterogeneous model cooperation in FL [13][14][15][16][17], and most of them incorporate the use of a public dataset to facilitate both cooperation and personalization [14][15][16][17].However, these approaches still face several key issues:\n\n\u2022 Undermining personalization through consensus: Existing methods often generate consensual side information, such as class information [14], logits [15,18], and label-wise representations [19], using public data.This information is then exchanged and used to conduct average operations on the server, resulting in a consensus representation.However, this approach poses privacy and security concerns due to the exchange of side information [20].Furthermore, the averaging process \u2022 Excessive reliance on prior knowledge for distillation-based approaches: Distillation-based techniques, such as knowledge distillation (KD), are commonly employed for heterogeneous model aggregation in FL [16,17,21].However, these techniques necessitate the predefinition of a shared model structure based on prior knowledge [17].This shared model is then downloaded to clients to guide their training process.Consequently, handcrafted models can heavily influence local model personalization.Additionally, a fixed shared model structure may be insufficient for effectively guiding personalized learning when dealing with a large number of clients with non-IID data.Thus, it is crucial to explore methods that can automatically and dynamically generate client-specific personalized models as guidance.\n\n\u2022 Sensitivity to the choice of public datasets: Most existing approaches use public data to obtain guidance information, such as logits [15,18] or a shared model [17], for local model personalization.\n\nThe design of these approaches makes public data and model personalization tightly bound together.Thus, they usually choose the public data with the same distribution as the client data.Therefore, using public data with different distributions from client data will cause a significant performance drop in existing models.Figure 1 illustrates the performance variations of different models trained on the SVHN dataset with different public datasets (detailed experimental information can be found in Section 4.4).The figure demonstrates a significant performance drop when using alternative public datasets.Consequently, mitigating the adverse impact of employing diverse public data remains a critical yet practical research challenge in FL.\n\nMotivation & Challenges.In fact, both consensus-based and distillation-based approaches aim to learn aggregated and shared information used as guidance in personalized local model training, which is not an optimal way to achieve personalization.An ideal solution is to generate a personalized model for the corresponding client, which is significantly challenging since the assessable information on the server side can only include the uploaded client models and the public data.To avoid the issue of public data sensitivity, only client models can be used.These constraints motivate us to employ the model reassembly technique [22] to generate models first and then select the most matchable personalized model for a specific client from the generations.\n\nTo this end, we will face several new challenges.(C1) Applying the model reassembly technique will result in many candidates.Thus, the first challenge is how to get the optimal candidates.(C2) The layers of the generated candidates are usually from different client models, and the output dimension size of the first layer may not align with the input dimension size of the second layer, which leads to the necessity of network layer stitching.However, the parameters of the stitched layers are unknown.Therefore, the second challenge is how to learn those unknown parameters in the stitched models.(C3) Even with well-trained stitched models, digging out the best match between a client model and a stitched model remains a big challenge.\n\nOur Approach.To simultaneously tackle all the aforementioned challenges, we present a novel framework called pFedHR, which aims to achieve personalized federated learning and address the issue of heterogeneous model cooperation (as depicted in Figure 2).The pFedHR framework comprises two key updates: the server update and the client update.In particular, to tackle C3, we approach the issue of heterogeneous model personalization from a model-matching optimization perspective on the server side (see Section 3.1.1).To solve this problem, we introduce a novel heterogeneous model reassembly technique in Section 3.1.2to assemble models uploaded from clients, i.e., {w  and diverse model candidates using clustering results.Importantly, all layers in each candidate are derived from the uploaded client models.\n\nPrior to matching the client model with candidates, we perform network layer stitching while maximizing the retention of information from the original client models (Section 3.1.3).To tackle C2, we introduce public data D p to help the finetuning of the stitched candidates, i.e., {c 1 t , \u2022 \u2022 \u2022 , cM t }, where M is the number of generated candidates.Specifically, we employ labeled OR unlabeled public data to fine-tune the stitched and client models and then calculate similarities based on model outputs.Intuitively, if two models are highly related to each other, their outputs should also be similar.Therefore, we select the candidate with the highest similarity as the personalized model of the corresponding client, which results in matched pairs 2. In the client update (Section 3.2), we treat the matched personalized model as a guidance mechanism for client parameter learning using knowledge distillation 3 .\n{{w 1 t , ci t }, \u2022 \u2022 \u2022 , {w B t , cm t }} in Figure\nIt is worth noting that we minimally use public data during our model learning to reduce their adverse impact.In our model design, the public data are used for clustering layers, fine-tuning the stitched candidates, and guiding model matching.Clustering and matching stages use public data to obtain the feedforward outputs as guidance and do not involve model parameter updates.Only in the fine-tuning stage, the stitched models' parameters will be updated based on public data.To reduce its impact as much as possible, we limit the number of finetuning epochs during the model implementation.Although we cannot thoroughly break the tie between model training and public data, such a design at least greatly alleviates the problem of public data sensitivity in FL.\n\nContributions.Our work makes the following key contributions: (1) We introduce the first personalized federated learning framework based on model reassembly, specifically designed to address the challenges of heterogeneous model cooperation.(2) The proposed pFedHR framework demonstrates the ability to automatically and dynamically generate personalized candidates that are both informative and diverse, requiring minimal human intervention.(3) We present a novel heterogeneous model reassembly technique, which effectively mitigates the adverse impact caused by using public data with distributions different from client data.( 4) Experimental results show that the pFedHR framework achieves state-of-the-art performance on three datasets, exhibiting superior performance under both IID and Non-IID settings when compared to baselines employing labeled and unlabeled public datasets."}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 202540963, "title": "On Learning Disentangled Representations for Gait Recognition", "paper_id": "439aca3a3d4497f822b058ccabcbfc94a83c5ee9", "abstract": "Gait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and viewing angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as a dynamic gait feature while canonical features are averaged as a static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state-of-the-art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/lower resolutions, cross viewing angles.", "intro": "B IOMETRICS measures people's unique physical and behavioral characteristics to recognize the identity of an individual. Gait [1], the walking pattern of an individual, is one of biometrics modalities besides face, fingerprint, iris, etc. Gait recognition has the advantage that it can operate at a distance without users' cooperation. Also, it is difficult to camouflage. Due to these advantages, gait recognition is applicable to many applications such as person identification, criminal investigation, and healthcare.\n\nAs other recognition problems, gait data can usually be captured by five types of sensors [2], i.e., RGB camera, RGB-D camera [3], [4], accelerometer [5], floor sensor [6], and continuous-wave radar [7]. Among them, RGB camera is not only the most popular one due to the low sensor cost, but also the most challenging one since RGB pixels might not be effective in capturing the motion cues. This work studies gait recognition from RGB cameras.\n\nThe core of gait recognition lies in extracting gait features from the video frames of a walking person, where the prior work can be categorized into two types: appearance-based and model-based methods. The appearance-based methods, e.g., Gait Energy Image (GEI) [8], take the averaged silhouette image as the gait feature. While having a low computational cost and being able to handle low-resolution imagery, it can be sensitive to variations such as cloth change, carrying, viewing angles and walking speed [9]- [15]. The model-based methods use the articulated body skeleton from pose estimation as the gait feature. They show more robustness to aforementioned variations but at a price of a higher computational cost and dependency on pose estimation accuracy [16]- [18].\n\nIt is understandable that the challenge in designing a gait feature is the necessity of being invariant to the appearance variation due to clothing, viewing angle, carrying, etc. Therefore, our desire is to \u2022 Ziyuan Zhang, Luan Tran, Feng Liu, and  disentangle the gait feature from the non-gait-related appearance of the walking person. For both appearance-based or modelbased methods, such disentanglement is achieved by manually handcrafting the GEI-like [8], [10] or body skeleton-like [16]- [18] features, since neither has color or texture information. However, we argue that these manual disentanglements may be sensitive to changes in walking condition. In other words, they can lose certain or create redundant gait information. E.g., GEI-like features have distinct silhouettes for the same subject wearing different clothes. For skeleton-like features, when carrying accessories (e.g., bags, umbrella), certain body joints such as hands may have fixed positions, and hence are redundant information to gait.\n\nTo remedy the aforementioned issues in handcrafted features, as shown in Fig. 1 (a), this paper proposes a novel approach to learn gait representations from the RGB video directly. Specifically, we aim to automatically disentangle dynamic pose features (trajectory of gait) from pose-irrelevant features. To further distill identity information from pose-irrelevant features, we disentangle the poseirrelevant features into appearance (i.e., clothing) and canonical features. Here, the canonical feature refers to a standard and unique representation of human body, such as body ratio, width and limb lengths, etc. The pose features and canonical features are discriminative in identity and are used for gait recognition. Fig. 1 (b) visualizes the three disentangled features. This disentanglement is realized by designing an autoencoderbased Convolutional Neural Network (CNN), GaitNet, with novel loss functions. For each video frame, the encoder estimates three latent representations: pose, canonical and appearance features, by employing three loss functions: 1) cross reconstruction loss enforces that the canonical and appearance features of one frame, fused with the pose feature of another frame, can be decoded to the latter frame; 2) pose similarity loss forces a sequence of pose features extracted from a video sequence, of the same subject to be similar even under different conditions; 3) canonical  Fig. 3 show that, the appearance feature is video-specific capturing clothing information; the canonical feature is subject-specific capturing the overall body shape at a standard pose; the pose feature is frame-specific capturing body poses at individual frames.\n\nconsistency loss favors consistent canonical features among videos of the same subject under different conditions. Finally, the pose features of a sequence are fed into a multi-layer LSTM with our designed incremental identity loss to generate the sequence-based dynamic gait feature. The average of canonical features results in the sequence-based static gait feature. Given two gait videos, the cosine distances between their respective dynamic and static gait features are computed and their summation is the final video-tovideo gait similarity metric. In addition, most prior work [8], [10], [14], [16], [19]- [24] choose the walking video of the side view, which has the richest gait information, as the gallery sequence. However, in practices other viewing angles, such as the frontal view, can be very common when pedestrians walk toward or away from the surveillance camera. Also, the prior work [25]- [28] that focuses on frontal view are often based on RGB-D videos, which have additional depth information than RGB. Therefore, to encourage gait recognition from frontalview RGB videos that generally has the minimal amount of gait information, we collect a high-definition (HD, 1080p) Frontal-View Gait database, named FVG, with a wide range of variations. It has three frontal-view angles where the subject walks from left 45 \u2022 , 0 \u2022 , and right 45 \u2022 off the optical axes of the camera. For each of three angles, different variants are explicitly captured including walking speed, clothing, carrying, multiple people, etc.\n\nA preliminary version of this work was published in the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019 [29]. We extend the work from three aspects. 1) Instead of disentangling features in two components: pose and poseirrelevant [29], we further decouple the pose-irrelevant features into discriminative canonical feature and appearance feature. By devising an effective canonical consistency loss, the canonical feature helps to improve gait recognition accuracy. 2) We conduct more insightful ablation studies to analyze the relationship between our disentanglement losses and features, gait recognition over time, and contributions of dynamic and static gait features. 3) We perform side-by-side comparison between gait recognition and the state-of-the-art (SOTA) face recognition on the same dataset.\n\nIn summary, this paper makes the following contributions:\n\nOur proposed GaitNet directly learns disentangled representations from RGB videos, which is in sharp contrast to the conventional appearance-based or model-based methods.\n\nWe introduce a Frontal-View Gait database, including various variations of viewing angles, walking speeds, carrying, clothing changes, background and time gaps. This is the first HD gait database, with nearly twice the number of subjects compared to existing RGB gait databases.\n\nOur proposed method outperforms the state of the arts on three benchmarks, CASIA-B, USF, and FVG datasets.\n\nWe demonstrate the strength of gait recognition over face recognition in the task of person recognition from surveillancequality videos."}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 1815453, "title": "A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition", "paper_id": "0268d1744377ffbff48b014f513ea3e5e4a4dab1", "abstract": "Gait recognition has gained increasing interest from researchers, but there is still no standard evaluation method to compare the performance of different gait recognition algorithms. In this paper, a framework is proposed in an attempt to tackle this problem. The framework consists of a large gait database, a large set of well designed experiments and some evaluation metrics. There are 124 subjects in the database, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered in the database. The database is one of the largest database among the existing databases. Three sets of experiments, including a total of 363 experiments, are designed in the framework. Some metrics are proposed to evaluate gait recognition algorithms", "intro": "Gait is an attractive biometric feature for human identification at a distance, and recently has gained much interest from computer vision researchers. Compared with those traditional biometric features, such as face, iris and fingerprint, gait has many unique advantages such as non-contact, non-invasive and perceivable at a distance. Hence gait has been considered as a suitable biometric feature for human identification at a distance in visual surveillance.\n\nIn recent years many gait recognition algorithms have been developed. Some of them are model-based approaches [9,11], and some are appearance-based ones [7,12,13]. Even though many algorithms have been proposed, comparison of different algorithms and evaluation of robustness to some variations such as the variations of view angle, clothing, shoe types, surface types, carrying condition, illumination, and time are still hard and open problems. These variations should be fully studied to develop robust and ac-curate gait recognition algorithms.\n\nThe HumanID Gait Challenge Problem [1,8], which consists of a large database, a baseline algorithm and twelve experiments, tried to handle these problems. The data in the HumanID Gait Challenge Problem was collected in an outdoor environment with complex background, so it is a little hard to extract good quality human silhouettes, and this will affect the analysis of other factors. The twelve experiments were designed to evaluate an algorithm's robustness to view, shoe, surface, time, clothing and carrying condition changes. However, for these factors twelve experiments are not enough. Besides, the subjects walked in an elliptical path, and then the view angle kept changing while the subjects was walking, so the relationship between view angle and algorithm's performance can not be obtained. In conclusion, a database that is more suitable for evaluation and some well designed experiments are needed.\n\nA framework that consists of a large database, some experiments and metrics is proposed. In the database, data acquired from 11 views are included and also three most important factors, view angle, clothing and carrying condition changes, are separately considered. A total of 363 experiments were designed to thoroughly investigate these factors.\n\nThe organization of this paper is as follows. Section 2 describes the gait database. Experiment design is presented in Section 3, and metrics is in Section 4. Section 5 concludes this paper."}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 4633086, "title": "The OU-ISIR Gait Database Comprising the Treadmill Dataset", "paper_id": "55bc72e912c1db2c85dbbb6e4afe6eb7bbff89de", "abstract": "This paper describes a large-scale gait database comprising the Treadmill Dataset. The dataset focuses on variations in walking conditions and includes 200 subjects with 25 views, 34 subjects with 9 speed variations from 2 km/h to 10 km/h with a 1 km/h interval, and 68 subjects with at most 32 clothes variations. The range of variations in these three factors is significantly larger than that of previous gait databases, and therefore, the Treadmill Dataset can be used in research on invariant gait recognition. Moreover, the dataset contains more diverse gender and ages than the existing databases and hence it enables us to evaluate gait-based gender and age group classification in more statistically reliable way.", "intro": "In modern society, there is a growing need to identify individuals in many different situations, including for surveillance and access control. For personal identification, many biometric-based authentication methods have been proposed using a wide variety of cues, such as fingerprints, irises, faces, and gait. Of these, gait identification has attracted considerable attention because it provides surveillance systems with the ability to ascertain identity at a distance. In fact, automatic gait recognition from public CCTV images has been admitted as evidence in UK courts [36], and gait evidence has been used as a cue for criminal investigations in Japan.\n\nRecently, various approaches to gait identification have been proposed. These range from model-based approaches [4], [37], [40], [41], [46] to appearance-based approaches [3], [6], [10], [14], [16], [17], [25], [26], [39]. In addition, several common gait databases have been published [7], [29], [31], [33], [44] for fair comparison of gait recognition approaches. These databases are usually constructed taking the following into account: (1) the variation in walking conditions, and (2) the number and diversity of the subjects.\n\nThe first consideration is important to ensure the robustness of the gait recognition algorithms, since walking conditions often differ between enrollment and test stages. For example, observation views are often inconsistent due to the positions of the CCTV cameras on the street and/or walking directions possibly being different. In addition, walking speeds can change depending on whether the person is merely taking a walk in the park or 1 Osaka University, Ibaraki, Osaka 567-0047, Japan 2 University of Rajshahi, Rajshahi, 6205, Bangladesh a) makihara@am.sanken.osaka-u.ac.jp is walking to the station in a hurry, and clothing almost certainly changes depending on the season.\n\nThe second consideration is also important because the number of subjects determines the upper bound of the statistical reliability of the performance evaluation. In addition, if the database is used not only for person identification, but also gender and age estimation from gait, the diversity of subjects in terms of gender and age plays an important role in the performance evaluations of such applications.\n\nIn this paper, we describe a large-scale gait database composed of the Treadmill Dataset based on the two considerations. The Treadmill Dataset is a set of gait datasets with variations in walking conditions, comprising 25 surrounding views, 9 walking speeds from 2 km/h to 10 km/h with a 1 km/h interval, at most 32 clothes combinations, and gait fluctuation variations among gait periods. The proposed gait dataset thus enables us to evaluate view-invariant, speed-invariant, and clothing-invariant gait recognition algorithms in a more extensive range. Moreover, it comprises 200 subjects of both genders and including a wide range of ages. The proposed gait database thus enables us to evaluate gait-based gender classification and age group classification.\n\nThe outline of this paper is as follows. First, existing gait databases are briefly considered in Section 2. Next, the Treadmill Dataset is addressed with related performance evaluations of gait recognition algorithms in Sections 3. Section 4 contains our conclusions, discussions, and future work in the area."}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 247996921, "title": "Gait Recognition in the Wild with Dense 3D Representations and A Benchmark", "paper_id": "8a33556a6c89087904ff9ed53c3c6c6a08fcc2dd", "abstract": "Existing studies for gait recognition are dominated by 2D representations like the silhouette or skeleton of the human body in constrained scenes. However, humans live and walk in the unconstrained 3D space, so projecting the 3D human body onto the 2D plane will discard a lot of crucial information like the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper aims to explore dense 3D representations for gait recognition in the wild, which is a practical yet neglected problem. In particular, we propose a novel framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the human body for gait recognition, named SMPLGait. Our framework has two elaborately-designed branches of which one extracts appearance features from silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D SMPL model. In addition, due to the lack of suitable datasets, we build the first large-scale 3D representation-based gait recognition dataset, named Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL models recovered from video frames which can provide dense 3D information of body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively compare our method with existing gait recognition approaches, which reflects the superior performance of our framework and the potential of 3D representations for gait recognition in the wild. The code and dataset are available at https://gait3d.github.io.", "intro": "Visual gait recognition, which aims to identify a target person using her/his walking pattern in a video, has been studied for over two decades [30,42]. Existing approaches and datasets are dominated by 2D gait representations such as silhouette sequences [56], Gait Energy Images * This work was done when Jinkai Zheng was an intern at Explore Academy of JD.com. \u2020 Corresponding author.  [63], as shown in Figure 1. However, the human body is a 3D non-rigid object, so the 3D-to-2D projection discards a lot of useful information about shapes, viewpoints, and dynamics while presenting ambiguity for gait recognition. Therefore, this paper is focused on 3D gait recognition which is valuable yet neglected by the community.\n\nRecently, deep learning-based methods have dominated the state-of-the-art performance on the widely adopted 2D gait recognition benchmarks like CASIA-B [37] and OU-MVLP [36] by directly learning discriminative features from silhouette sequences [5,9,57] or GEIs [46]. Despite the excellent results on the in-the-lab datasets, these methods cannot work well in the wild scenarios which have more diverse 3D viewpoints of cameras and more complex environmental interference factors like occlusions [63]. Although several works exploit 3D cylin-ders [3] or 3D skeletons [41], these sparse 3D models also lose helpful information of human bodies like viewpoints and shapes. Fortunately, the development of parameterized human body models like the Skinned Multi-Person Linear (SMPL) model [28] and 3D human mesh recovery approaches [18,20,34] makes it possible to estimate precise 3D meshes and viewpoints of human bodies in video frames. The advantages of 3D meshes for gait recognition are two-fold: 1) the 3D mesh can provide not only the pose but also the shape of the human body in the 3D space, which is crucial for learning discriminative features of gait, and 2) the 3D viewpoint can be explored to normalize the orientations of human bodies during cross-view matching.\n\nTo this end, we design a novel 3D SMPL model-based Gait recognition framework, i.e., SMPLGait, to explore the 3D gait representations for human identification. Our SMPLGait framework has two branches based on deep neural networks. One branch takes the silhouette sequence of a person as the input to learn appearance features like clothing, hairstyle, and belongings. However, due to the extreme viewpoint changes in the wild, the shape of the human body can be distorted, which makes the appearance ambiguous, as shown in Figure 1. To overcome this challenge, we design a 3D Spatial-Transformation Network (3D-STN) as the other branch to learn 3D knowledge of viewpoint and shape from the 3D human mesh. The 3D-STN takes the 3D SMPL model of each frame as the input to learn a spatial transformation matrix. By applying the spatial transformation matrix to the appearance features, these features from different viewpoints are normalized in the latent space. By this means, the gait sequences of the same person will be closer in the feature space.\n\nNevertheless, there is no suitable dataset that provides 3D meshes of human bodies in the wild. Therefore, to facilitate the research, we build the first large-scale 3D mesh-based gait recognition dataset, named Gait3D, from high-resolution videos captured in the wild. Compared to existing datasets listed in Table 1, the Gait3D dataset has the following featured properties: 1) Gait3D contains 4,000 subjects with over 25,000 sequences captured by 39 cameras in an unconstrained indoor scene which makes it scalable for research and applications.\n\n2) It provides precise 3D human meshes recovered from video frames which can provide 3D pose and shape of human bodies as well as accurate viewpoint parameters. 3) It also provides conventional 2D silhouettes and keypoints which can be explored for gait recognition with multi-modal data.\n\nIn summary, the contributions of this paper are as follows:\n\n\u2022 We make one of the first attempts toward 3D gait recognition in the real-world scenario, which aims to explore dense 3D representations of the human body for gait recognition.\n\n\u2022 We propose a novel 3D gait recognition framework based on the SMPL model, named SMPLGait, to explore 3D human meshes for gait recognition.\n\n\u2022 We build the first large-scale 3D gait recognition dataset, named Gait3D, which provides the 3D human meshes of gait collected from unconstrained scenarios.\n\nThrough comprehensive experiments, we not only evaluate existing 2D silhouettes/skeleton-based approaches but also demonstrate the effectiveness of the proposed SMPLGait method, which reflects the potential of 3D representations for gait recognition. Moreover, the combination of 3D and 2D representations further improves the performance which shows the complementarity of multi-modal representations."}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 244906176, "title": "Gait Recognition in the Wild: A Benchmark", "paper_id": "5283abb18a063baa902be87bc2c3b5bc87c66069", "abstract": "Gait benchmarks empower the research community to train and evaluate high-performance gait recognition systems. Even though growing efforts have been devoted to cross-view recognition, academia is restricted by current existing databases captured in the controlled environment. In this paper, we contribute a new benchmark for Gait REcognition in the Wild (GREW). The GREW dataset is constructed from natural videos, which contains hundreds of cameras and thousands of hours streams in open systems. With tremendous manual annotations, the GREW consists of 26K identities and 128K sequences with rich attributes for unconstrained gait recognition. Moreover, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Compared with prevailing predefined cross-view datasets, the GREW has diverse and practical view variations, as well as more natural challenging factors. To the best of our knowledge, this is the first large-scale dataset for gait recognition in the wild. Equipped with this benchmark, we dissect the unconstrained gait recognition problem. Representative appearance-based and model-based methods are explored, and comprehensive baselines are established. Experimental results show (1) The proposed GREW benchmark is necessary for training and evaluating gait recognizer in the wild. (2) For state-of-the-art gait recognition approaches, there is a lot of room for improvement. (3) The GREW benchmark can be used as effective pre-training for controlled gait recognition. Benchmark website is https://www.grew-benchmark.org/.", "intro": "Gait recognition aims to identify a person according to his/her walking style in a video. Compared with face, fingerprint, iris and palmprint, gait is hard to disguise and can work at a long distance, giving it unique potential for crime prevention, forensic identification, and social security.  [74], OU-MVLP [51] and the proposed GREW. The first two are captured under constrained environments, while the GREW is constructed in the wild. Since OU-MVLP [51] does not release RGB data, visualization results from its original paper are adopted. Faces are masked in the GREW for privacy concern.\n\nRecognizing gait under a controlled environment has achieved significant progress due to the boom of deep learning. The essential engines of recent gait recognition consist of network architecture evolution [20,9,62,65,16,72,71,31,44,50,4,63,67,39], loss function design [78,17,75,79], and growing gait benchmarks [42,7,74,37,51,23]. Even though gait recognition has achieved impressive advance in past years and it possesses the unique advantage of long-distance recognition, this technique has not yet been widely deployed in real-world applications. A notable obstacle is that there is almost no public benchmark to train and evaluate gait recognizer in the wild.\n\nTo our knowledge, most gait datasets are captured in relatively fixed and constrained environments such as laboratory or static outdoors. CASIA-B [74] and OU-MVLP [51] are most popularly used datasets in recent gait recognition research as shown in Figure 1. CASIA-B contains 124 subjects and 13,640 sequences, which is constructed in 2006. OU-MVLP consists of 10,307 identities and 288,596 walking videos, making it a big gait dataset with respect to #subjects. The statistics of more datasets are shown in Table 1: Comparison of the GREW with existing gait recognition datasets regarding statistics, data type, captured environment, view variations and challenging factors. Datasets are sorted in publication time. #Id., #Seq. and #Cam. refer to numbers of identities, sequences and cameras. Sil., Inf., D. and A. mean silhouette, infrared, depth and audio. VI, DIS, BA, CA, DR, OCC, ILL, SU, SP, SH, and WD are abbreviations of view, distractor, background, carrying, dressing, occlusion, illumination, surface, speed, shoes, and walking directions.  [49,43,56,8,21,70,3,13,24,84] and person re-identification (ReID) [77,48,36,66,17,10,82,80,81,83,27,61], it is time to move to benchmark gait recognition in the wild.\n\nIn this paper, we present the Gait REcognition in the Wild (GREW) benchmark, which is the first work delving into this open problem to the best of our knowledge. The GREW dataset is constructed from natural streams with multiple cameras as shown in Figure 1. Identity information from raw videos is manually annotated, resulting in 26K subjects, 128K sequences and 14M boxes for unconstrained gait recognition. Besides, rich human attributes including gender, age group, carrying and dressing styles are labelled for fine-grained performance analysis. In practice, the gallery scale is a vital problem for recognition accuracy. To this end, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Since there are a series of gait recognition frameworks using different input data types, the GREW provides silhouettes, Gait Energy Images (GEIs) [14], optical flow, 2D and 3D poses by automatical processing. Compared with controlled gait dataset such as CASIA-B and OU-MVLP, our GREW is fully-unconstrained and has more diverse and practical view variations instead of predefined ones. Meanwhile, there are various challenging factors in the GREW such as distractor set, complex background, occlusion, carrying, dressing et al. as shown in Table 1 and Figure 2.\n\nEquipped with the proposed GREW, the unconstrained gait recognition problem is deeply investigated. Firstly, representative appearance-based and model-based baselines are performed on the GREW, which indicates a lot of room for improvement. For example, top-performed GaitSet [4] obtains 46.28% Rank-1 accuracy on the GREW test set, while it scores more than 80% on the CASIA-B and OU-MVLP. With the distractor set, gait recognition in the wild would become more challenging, while the best model scores only 41.97% Rank-1. Secondly, the influence of the data scale is explored, including the number of training identities and gallery size. Increasing training subjects consistently boosts the performance, while large-scale test set with distractor is still very difficult for CNN-based recognizer. Thirdly, performance on different attributes (gender, age group, carrying, and dressing) is reported, which gives in-depth analysis results. Lastly, we validate the effectiveness of the GREW for pre-training. Fine-tuning models pre-trained on the GREW shows superior performance for cross-dataset gait recognition.\n\nThe main contributions can be summarized as follows:\n\n\u2022 A large-scale benchmark is constructed for the research community towards gait recognition in the wild. The proposed GREW consists of 26K subjects and 128K sequences with rich attributes from flexible data streams, which makes it the first dataset for unconstrained gait recognition to the best of our knowledge. \u2022 To constitute the GREW benchmark, we collect thousands of hours of streams from multiple cameras in open systems. With automatical pre-processing and tremendous manual identity annotations, there are  more than 14M boxes that simultaneously provide silhouettes and human poses. Besides, we enrich the GREW by a distractor set with 233K sequences, making it more suitable for real-world applications. \u2022 Enabled by the new benchmark, we perform extensive gait recognition experiments and establish comprehensive baselines, including representative methods, scale influence, attributes analysis and pre-training. Results indicate that the GREW is necessary and effective for gait recognition in the wild. Besides, recognizing unconstrained gait is a very challenging task for current SOTA approaches. Lastly, the proposed dataset can be employed as effective pre-training data for controlled gait recognition to achieve higher performance."}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 252263212, "title": "Learning Gait Representations with Noisy Multi-Task Learning", "paper_id": "b697c84455e95d1c13fb3600ba99e32ef55e321f", "abstract": "Gait analysis is proven to be a reliable way to perform person identification without relying on subject cooperation. Walking is a biometric that does not significantly change in short periods of time and can be regarded as unique to each person. So far, the study of gait analysis focused mostly on identification and demographics estimation, without considering many of the pedestrian attributes that appearance-based methods rely on. In this work, alongside gait-based person identification, we explore pedestrian attribute identification solely from movement patterns. We propose DenseGait, the largest dataset for pretraining gait analysis systems containing 217 K anonymized tracklets, annotated automatically with 42 appearance attributes. DenseGait is constructed by automatically processing video streams and offers the full array of gait covariates present in the real world. We make the dataset available to the research community. Additionally, we propose GaitFormer, a transformer-based model that after pretraining in a multi-task fashion on DenseGait, achieves 92.5% accuracy on CASIA-B and 85.33% on FVG, without utilizing any manually annotated data. This corresponds to a +14.2% and +9.67% accuracy increase compared to similar methods. Moreover, GaitFormer is able to accurately identify gender information and a multitude of appearance attributes utilizing only movement patterns. The code to reproduce the experiments is made publicly.", "intro": "Technologies relying on facial and pedestrian analysis play a crucial role in intelligent video surveillance and security systems. Facial and pedestrian analysis systems have become the norm in video intelligence, such systems being deployed ubiquitously. However, appearance-based pedestrian re-identification [1] and facial recognition models [2] invariably suffer from extrinsic factors related to camera viewpoint and resolution, and to the change in a person's appearance such as different clothing, hairstyles and accessories. Moreover, due to the proliferation of privacy laws such as GDPR, it is increasingly difficult to deploy appearance-based solutions for video-intelligence. Human movement is highly correlated with many internal and external aspects of a particular individual including age, gender, body mass index, clothing, carrying conditions, emotions and personality [3]. The manner of walking is unique to each person, it does not significantly change in short periods of time [4] and cannot be easily faked to impersonate another person [5]. Gait analysis has gained significant attention in recent years [6,7], due to solving many of the problems of appearance-based technologies without relying on the direct cooperation of subjects. However, compared to appearance-based methods, gait analysis is intrinsically harder to perform with reliable accuracy, due to the influence of many confounding factors that affect the manner of walking. This problem is tackled in literature in two major ways, either by building specialized neural architectures that are invariant to walking variations [8][9][10], or by creating large-scale and diverse datasets for training [11][12][13][14][15].\n\nOne of the first attempts of building a large-scale gait recognition dataset is OU-ISIR [14], which is comprised of 10,307 identities that walk in a straight line for a short duration of time. Such a dataset is severely limited by its lack of walking variability, having only viewpoint change as a confounding factor. Building sufficiently large datasets that account for all the walking variations imply an immense annotation effort. For example, the GREW benchmark [12] for gait-based identification, reportedly took 3 months of continuous manual annotation by 20 workers. In contrast, automatic, weakly annotated datasets are much easier to gather by leveraging existing state-of-the-art models-UWG [11], a comparatively large dataset of individual walking tracklets proved to be a promising new direction in the field. Increasing the dataset size is indeed correlated with performance on downstream gait recognition benchmarks [11], even though no manual annotations are provided. One limitation of these datasets is that they are annotated with attributes per individual only sparsely, and not addressing the problem of pedestrian attribute identification (PAI), currently performed only through appearance-based methods [16][17][18]. Walking pedestrians are often annotated only with their gender, age, and camera viewpoint [8,12,14,15]. Even though gait-based demographic identification is a viable method for pedestrian analysis [19], it is also severely limited by the lack of data. Also, many attributes from PAI networks such as gender, age and body type have a definite impact on walking patterns [20][21][22], and we posit that they can be identified with a reasonable degree of accuracy using only movement patterns and not utilizing appearance information.\n\nWe propose DenseGait, the largest gait dataset for pretraining to date, containing 217 K anonymized tracklets in the form of skeleton sequences, automatically gathered by processing real-world surveillance streams through state-of-the-art models for pose estimation and pose tracking. An ensemble of PAI networks was used to densely annotate each skeleton sequence with 42 appearance attributes such as their gender, age group, body fat, camera viewpoint, clothing information and apparent action. The purpose of DenseGait is to be used for pretraining networks for gait recognition and attribute identification, it is not suitable for evaluation since it is annotated automatically and does not contain manual, ground-truth labels. DenseGait contains walking individuals in real scenarios, it is markerless, non-treadmill, and avoids unnatural and constrictive laboratory conditions, which have been shown to affect gait [23]. It practically contains the full array of factors that are present in real world gait patterns.\n\nThe dataset is fully anonymized, and any information pertaining to individual identities is removed, such as the time, location and source of the video stream, and the appearance and height information of the person. DenseGait is a gait analysis dataset primarily intended for pretraining neural models-using it to explicitly identify the individuals within it is highly unfeasible, requiring extensive external information about the individuals, such as personal identifying information (i.e., their name or ID) and a baseline gait pattern. According to GDPR (https://eur-lex.europa.eu/eli/reg/2016/679/oj, accessed on 1 July 2022) legislation, data used for research purposes can be used if anonymized. Moreover, anonymized data does not conform to the rigors of personal data and can be processed without explicit consent. Nevertheless, any attempt to use of DenseGait to explicitly identify individuals present in it is highly discouraged.\n\nWe chose to utilize only skeleton sequences for gait analysis, as current appearancebased methods that rely on silhouettes are not privacy preserving, potentially allowing for identification based only on the person's appearance, rather than their movement [24]. Skeleton sequences encode only the movement of the person, abstracting away any visual queues regarding identity and attributes. Moreover, skeleton-based solutions have the potential to generalize across tasks such as action recognition, allowing for a flexible and extensible computation.\n\nDenseGait, compared to other similar datasets [11], contains 10\u00d7 more sequences and is automatically annotated with 42 appearance attributes through a pretrained PAI ensemble (Table 1). In total, 60 h of video streams were processed, having a cumulative walking duration of pedestrians of 410 h. We release the dataset under open credentialized access, for research purposes only, under CC-BY-NC-ND-4.0 (https://creativecommons. org/licenses/by-nc-nd/4.0/legalcode, accessed on 1 July 2022) License. We also propose GaitFormer, a multi-task transformer-based architecture [25] that is pretrained on DenseGait in a self-supervised manner, being able to perform exceptionally well in zero-shot gait recognition scenarios on benchmark datasets, achieving 92.5% identification accuracy from direct transfer on the popular CASIA-B dataset, without using any manually annotated data. Moreover, it obtains good results on demographic and pedestrian attribute identification from walking patterns, with no manual annotations. GaitFormer represents the first use of a plain transformer encoder architecture in gait skeleton sequence processing, without relying on hand-crafted architectural modifications as in the case of graph neural networks [26,27]. This paper makes the following contributions:"}
{"tabids": ["6840a3c6-7ae9-412f-bed3-78e666859dcf"], "corpus_id": 261049754, "title": "PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait", "paper_id": "78b8f98c133c876d6f9ee151821d271100733cd5", "abstract": "Psychological trait estimation from external factors such as movement and appearance is a challenging and long-standing problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totalling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.", "intro": "How we move and behave in the physical space is intrinsically tied to our psychological workings.Besides each individual's muscle developments and the influence of extrinsic factors, walking (or gait) is influenced by gender [10], emotions [4], personality traits [59], and mental distress [23].Walking is an action performed ubiquitously by healthy individuals.So far, in the domain of AI-powered gait analysis, significant attention has been dedicated to gait recognition [2,13,17,22,28,40,69], but with little focus being dedicated to exploring the manifestations of psycholog-ical traits in gait [29].\n\nWhile there has been research in studying personality manifestations in video [11,24,43,54], walking is left largely unexplored.Nonetheless, some studies [46,59,64] separately confirm significant differences in gait between individuals with different personalities, levels of aggression, depression and self-esteem.Such works are based on the assumption that psychological experience manifests itself into behaviour, in line with theories of embodiment [51,74].Embodiment suggests a biofeedback influence loop between the psyche and physical gestures / actions.Actions, gestures and posture can have a clear impact on thought [27], memory and recall [50], and mood [51,53].In this context, currently, there is no open dataset for exploring the embodied manifestation of psychological traits in gait.\n\nWe propose PsyMo (Psychological traits from Motion), a multi-purpose gait database containing 312 subjects walking under multiple viewpoints and walking variations, annotated with self-assessed psychological traits from 6 psychological questionnaires: Big Five Index [32], Rosenberg Self-Esteem [57], Buss-Perry Aggression [9], Occupational Fatigue Exhaustion/Recovery Scale [70], Depression Anxiety Stress Scale [45], and the General Health Questionnaire [26].Across all subscales and factors, PsyMo contains 17 psychological traits.Alongside self-assessed psychological traits, participants also submitted their age, gender, height and weight.PsyMo is the first public dataset of its kind and pertains to the set of \"controlled\" gait datasets used for benchmarking gait analysis tasks under various walking variations, similar to CASIA-B [73] and FVG [75].\n\nPsyMo covers 7 walking variations: normal walking, changing clothes, slow walking, fast walking, walking with a bag, and two additional dual-tasks (i.e.walking while performing a cognitive task [19]), largely ignored in current datasets: walking while texting and walking while talking on the phone.The walks are captured using 3 synchronized consumer video surveillance cameras (Tapo C200) to mimic adverse conditions (i.e.low-fps, fish-eye distortion, lowresolution) present in real-world surveillance scenarios, for a total of 6 viewpoints (which include the round-trips).In total, we gathered 14,976 walking sequences.\n\nWe provide 2D / 3D human poses, silhouettes, and 3D human meshes for gait processing.We extracted appearance-based silhouettes using instance segmentation with a pretrained Hybrid Task Cascade (HTC) model [14], and 2D skeletons using AlphaPose [36], a state-of-the-art model for pose estimation.Further, we used CLIFF [39] to estimate 3D human pose and 3D meshes in the form of parametric SMPL predictions.The dataset is fully anonymized, and subjects gave explicit and informed consent for processing and distributing it.We do not release raw videos, as they can lead to identifying the subjects in our study.We release only processed gait information in the form of silhouettes, 2D / 3D skeletons and 3D human meshes.\n\nPsyMo is primarily intended as a rich resource for exploring psychological manifestations into walking patterns, allowing for an interdisciplinary study into human behaviour from both the artificial intelligence and psychology research communities.We propose several evaluation procedures for the estimation of psychological traits from gait.Furthermore, due to its size and number of walking variations, PsyMo can also be used as a benchmark dataset for standard gait recognition, in addition to / as a more diverse drop-in replacement to existing gait benchmark datasets [60,73,75].For this purpose, we also proposed several evaluation protocols for gait recognition."}
{"tabids": ["e816c01a-4fb4-4c23-ac5d-15b0242c82e4"], "corpus_id": 246411621, "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "paper_id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "intro": "The NLP landscape has recently been revolutionized by language models (Peters et al., 2018;Devlin et al., 2019;Brown et al., 2020, inter alia). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020;Brown et al., 2020, inter alia). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).\n\nThis work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015;Chiang and Chen, 2019;Amini et al., 2019;Chen et al., 2019). Second, large language models offer the exciting prospect of in-context few-shot learning via prompting. That is, instead of finetuning a separate language model checkpoint for each new task, one can simply \"prompt\" the model with a few input-output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).\n\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input-output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: input, chain of thought, output . A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting. An example prompt is shown in Figure 1.\n\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result-on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."}
{"tabids": ["e816c01a-4fb4-4c23-ac5d-15b0242c82e4"], "corpus_id": 252762395, "title": "LANGUAGE MODELS", "paper_id": "99832586d55f540f603637e458a292406a0ed75d", "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "intro": "A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky, 1987;Luria, 1965;Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specific actions, we may reason in language in order to track progress (\"now that everything is cut, I should heat up the pot of water\"), to handle exceptions or adjust the plan according to the situation (\"I don't have salt, so let me use soy sauce and pepper instead\"), and to realize when external information is needed (\"how do I prepare dough? Let me search on the Internet\"). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (\"What dish can I make right now?\"). This tight synergy between \"acting\" and \"reasoning\" allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.\n\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLMs) have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive  (Yang et al., 2018) question.\n\n(2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs). answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2022). However, this \"chain-of-thought\" reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022;Nakano et al., 2021;Yao et al., 2020;Huang et al., 2022a), with a focus on predicting actions via language priors. These approaches usually convert multi-modal observations into text, use a language model to generate domain-specific actions or plans, and then use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic benefits compared to reasoning or acting alone.\n\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018), fact verification (Fever, Thorne et al., 2018), text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact with, ReAct outperforms vanilla action generation models while being competitive with chain-ofthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct and CoT that allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with 10 3 \u223c 10 5 task instances, with an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate the importance of sparse, versatile reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost, the combination of reasoning and acting also contributes to model interpretability, trustworthiness, and diagnosability across all domains, as humans can readily distinguish information from model's internal knowledge versus external environments, as well as inspect reasoning traces to understand the decision basis of model actions.\n\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel promptbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a few-shot learning setup over prior approaches that perform either reasoning or action generation in isolation; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial finetuning experiments showing the potential of ReAct to improve with additional training data. Scaling up ReAct to train and operate on more tasks and combining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models. 2 REAC T : SYNERGIZING REASONING + AC T ING Consider a general setup of an agent interacting with an environment for task solving. At time step t, an agent receives an observation o t \u2208 O from the environment and takes an action a t \u2208 A following some policy \u03c0(a t |c t ), where c t = (o 1 , a 1 , \u00b7 \u00b7 \u00b7 , o t\u22121 , a t\u22121 , o t ) is the context to the agent. Learning a policy is challenging when the mapping c t \u2192 a t is highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct final action (Act 4) to finish the QA task as it requires complex reasoning over the trajectory context (Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\n\nThe idea of ReAct is simple: we augment the agent's action space to\u00c2 = A \u222a L, where L is the space of language. An action\u00e2 t \u2208 L in the language space, which we will refer to as a thought or a reasoning trace, does not affect the external environment, thus leading to no observation feedback. Instead, a thought\u00e2 t aims to compose useful information by reasoning over the current context c t , and update the context c t+1 = (c t ,\u00e2 t ) to support future reasoning or acting. As shown in Figure 1, there could be various types of useful thoughts, e.g. decomposing task goals and create action plans (2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1), extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans (2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\n\nHowever, as the language space L is unlimited, learning in this augmented action space is difficult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022) 1 , is prompted with few-shot in-context examples to generate both domain-specific actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself.\n\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail prompt design for each task in Sections 3 and 4. B) General and flexible: Due to the flexible thought space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but not limited to QA, fact verification, text game, and web navigation. C) Performant and robust: ReAct shows strong generalization to new task instances while learning solely from one to six in-context examples, consistently outperforming baselines with only reasoning or acting across different domains. We also show in Section 3 additional benefits when finetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections. D) Human aligned and controllable: ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover, humans can also control or correct the agent behavior on the go by thought editing, as shown in Figure 5 in Section 4."}
{"tabids": ["e816c01a-4fb4-4c23-ac5d-15b0242c82e4"], "corpus_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "paper_id": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "intro": "Although large language models (LLMs) can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may benefit from further iterative refinement-i.e., iteratively mapping a candidate output to an improved one-to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations (Madaan et al., 2021;Ouyang et al., 2022), which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision.\n\nIterative self -refinement is a fundamental characteristic of human problem-solving (Simon, 1962;Flower and Hayes, 1981;Amabile, 1983). Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, when"}
{"tabids": ["e816c01a-4fb4-4c23-ac5d-15b0242c82e4"], "corpus_id": 258865812, "title": "Reasoning with Language Model is Planning with World Model", "paper_id": "5dbffedcabe3fa43060ebbe2b1789500edfd871f", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.", "intro": "Large language models (LLMs) have exhibited emergent reasoning abilities in a wide range of tasks (Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2023).Recent approaches further boost their ability by prompting LLMs to generate intermediate reasoning steps, e.g., Chain-of-Thought, CoT (Wei et al., 2022) or answer a series of subquestions, e.g., least-to-most prompting (Zhou et al., 2022).However, LLMs still face difficulties with tasks that humans find easy.For example, in creating action plans to move blocks to a target state, GPT-3 (Brown et al., 2020) achieves a success rate of only 1%, compared to 78% for humans (Valmeekam et al., 2022); these models also struggle with complex tasks that require multiple steps of math, logical, or commonsense reasoning (Huang and Chang, 2022;Mialon et al., 2023).\n\nHumans possess an internal world model, a mental representation of the environment (Johnson-Laird, 1983, 2010;Gentner and Stevens, 2014), which enables humans to simulate actions and their effects on the world's state for deliberate planning for complex tasks of motor control, imagery, inference, and decision making (Tolman, 1948;Briscoe, 2011;Schulkin, 2012;LeCun, 2022).For example, to make an action plan towards a goal, planning with the world model involves exploring various alternative courses of actions, assessing the likely outcomes by rolling out possible future scenarios, and iteratively refining the plan based on the assessment (Huys et al., 2012;Gasparski and Orel, 2014;Ho et al., 2021).This is in stark contrast to the current LLM reasoning, which instinctively generates a reasoning trace in an autoregressive manner.In particular, we identify several key limitations of the current reasoning with LLMs, including (1) the lack of an internal world model to simulate the state of the world (e.g., the configuration of blocks, the values of intermediate variables), which is the foundation of human planning; (2) the absence of a reward mechanism to assess and guide the reasoning towards the desired state; and due to both limitations, (3) the incapability of balancing exploration vs. exploitation to efficiently explore vast"}
{"tabids": ["e816c01a-4fb4-4c23-ac5d-15b0242c82e4"], "corpus_id": 258833055, "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "paper_id": "0671fd553dd670a4e820553a974bc48040ba0819", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "intro": "Recent works such as ReAct [30], SayCan [1], Toolformer [22], HuggingGPT [23], generative agents [19], and WebGPT [17] have demonstrated the feasibility of autonomous decision-making agents that are built on top of a large language model (LLM) core.These methods use LLMs to generate text and 'actions' that can be used in API calls and executed in an environment.Since they rely on massive models with an enormous number of parameters, such approaches have been so far limited to using in-context examples as a way of teaching the agents, since more traditional optimization schemes like reinforcement learning with gradient descent require substantial amounts of compute and time.\n\nIn this paper, we propose an alternative approach called Reflexion that uses verbal reinforcement to help agents learn from prior failings.Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the LLM agent in the next episode.This self-reflective feedback acts as a 'semantic' gradient signal by providing the agent with a concrete direction to improve upon, helping it learn from prior mistakes to perform better on the task.This is akin to how humans iteratively learn to accomplish complex tasks in a few-shot manner -by reflecting on their previous failures in order to form an improved plan of attack for the next attempt.For example, in figure 1, a Reflexion agent learns to optimize its own behavior to solve decision-making, programming, and reasoning tasks through trial, error, and self-reflection.\n\nGenerating useful reflective feedback is challenging since it requires a good understanding of where the model made mistakes (i.e. the credit assignment problem [25]) as well as the ability to generate a summary containing actionable insights for improvement.We explore three ways for doing this -simple binary environment feedback, pre-defined heuristics for common failure cases, and self-evaluation such as binary classification using LLMs (decision-making) or self-written unit tests (programming).In all implementations, the evaluation signal is amplified to natural language experience summaries which can be stored in long-term memory.\n\nReflexion has several advantages compared to more traditional RL approaches like policy or valuebased learning: 1) it is lightweight and doesn't require finetuning the LLM, 2) it allows for more nuanced forms of feedback (e.g.targeted changes in actions), compared to scalar or vector rewards that are challenging to perform accurate credit assignment with, 3) it allows for a more explicit and interpretable form of episodic memory over prior experiences, and 4) it provides more explicit hints for actions in future episodes.At the same time, it does have the disadvantages of relying on the power of the LLM's self-evaluation capabilities (or heuristics) and not having a formal guarantee for success.However, as LLM capabilities improve, we only expect this paradigm to get better over time.\n\nWe perform experiments on (1) decision-making tasks to test sequential action choices over long trajectories, (2) reasoning tasks to test knowledge-intensive, single-step generation improvement, and (3) programming tasks to teach the agent to effectively use external tools such as compilers and interpreters.Across all three types of tasks, we observe Reflexion agents are better decisionmakers, reasoners, and programmers.More concretely, Reflexion agents improve on decision-making AlfWorld [24] tasks over strong baseline approaches by an absolute 22% in 12 iterative learning steps, and on reasoning questions in HotPotQA [28] by 20%, and Python programming tasks on HumanEval [6] by as much as 11%.\n\nTo summarize, our contributions are the following:\n\n\u2022 We propose Reflexion, a new paradigm for 'verbal' reinforcement that parameterizes a policy as an agent's memory encoding paired with a choice of LLM parameters.\n\n\u2022 We explore this emergent property of self-reflection in LLMs and empirically show that self-reflection is extremely useful to learn complex tasks over a handful of trials.\n\n\u2022 We introduce LeetcodeHardGym, a code-generation RL gym environment consisting of 40 challenging Leetcode questions ('hard-level') in 19 programming languages.\n\n\u2022 We show that Reflexion achieves improvements over strong baselines across several tasks, and achieves state-of-the-art results on various code generation benchmarks."}
{"tabids": ["e816c01a-4fb4-4c23-ac5d-15b0242c82e4"], "corpus_id": 260611249, "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "paper_id": "81b10e64133e775dab53153cc82277d276efe1f7", "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.", "intro": "Recently, we have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing tasks on their own, ultimately in the service of a goal, rather than simply responding to queries from human users. Prominent studies, including ReAct (Yao et al., 2023), Toolformer (Schick et al., 2023), Hug-gingGPT (Shen et al., 2023), generative agents (Park et al., 2023), WebGPT (Nakano et al., 2021), AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023), have successfully showcased the viability of creating autonomous decision-making agents by leveraging the capabilities of LLMs. These approaches use LLMs to generate text-based outputs and actions that can be further employed for making API calls and executing operations within a given environment.\n\nGiven the immense scale of LLMs with an extensive parameter count, the behaviors of most existing language agents, however, are not optimized or aligned with environment reward functions. An exception is a very recent language agent architecture, namely Reflexion (Shinn et al., 2023), and several other related work, e.g., Self-Refine (Madaan et al., 2023b) and Generative Agent (Park et al., 2023), which use verbal feedback, namely self-reflection, to help agents learn from prior failure. These reflective agents convert binary or scalar reward from the environment into verbal feedback in the form of a textual summary, which is then added as additional context to the prompt for the language agent. The self-reflection feedback acts as a semantic signal by providing the agent Figure 1: An example of uninformative self-reflections from a frozen LLM. The root cause of failure in prior trial is that the agent should have responded only the spinoff series \"Teen Titans Go\" not \"Teen Titans\" as the answer. The agent forgot its goal during a chain of thought, actions and lengthy observations. The verbal feedback from frozen LLM, however, only rephrases the prior actions sequences as the proposed new plan, resulting the same, incorrect actions in the next trial.\n\nWe conduct experiments on open-source simulation and real-world environments including Hot-PotQA (Yang et al., 2018), which consists of search-based question answering tasks to test the tool use abilities of a web agent which needs to call Wikipedia APIs in multiple steps for answering question. We observe Retroformer agents are faster learners compared with reflexion, which does not use gradient for reasoning and planning, and better decision-makers and reasoners. More concretely, Retroformer agents improve the success rate of search-based question-anwering tasks in HotPotQA (Yang et al., 2018) by 18% in 4 attempts, which demonstrates the effectiveness of gradient-based reasoning and planning for tool use under large state-action space.\n\nTo summarize, our contributions are the following:\n\n\u2022 The paper introduces Retroformer, which iteratively refines the prompts given to large language agents based on environmental feedback to improve learning speed and task completion.\n\nWe take a policy gradient approach with the Actor LLM being part of the environment, allowing learning from a wide range of reward signals for diverse tasks and environments. \u2022 The proposed method focuses on enhancing the retrospective model in the language agent architecture, without accessing the Actor LLM parameters or needing to propagate gradients through it. The agnostic nature of Retroformer makes it a flexible plug-in module for various types of cloud-based LLMs, such as GPT or Bard."}
{"tabids": ["254278b5-ee90-4d52-88e1-fadc9531e8a0"], "corpus_id": 222177159, "title": "Adversarial Patch Attacks on Monocular Depth Estimation Networks", "paper_id": "54065d21ad9f7b8597f14a39debcee383114b092", "abstract": "Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.", "intro": "Estimating pixel-wise depth from 2-D images has become increasingly important with the recent development of autonomous driving, augmented realities (AR), and robotics. A large body of previous work has been devoted to depth estimation from stereo or more than two images [1]- [4]. At the same time, monocular depth estimation [5]- [8], in which depth is estimated from a single image, 1 has attracted attention due to its less demanding hardware requirements. Monocular depth estimation has been greatly enhanced by the excellent learning capability of deep convolutional neural networks (CNN). As a result, current state-of-the-art results with monocular depth estimation are quite impressive, and seemingly comparable to those with stereo methods (see Fig. 1, where (a) is the input image and (b) is the depth estimated by Guo et al. [7]). However, monocular depth estimation is essentially an ill-posed problem because a monocular image alone does not contain sufficient physical cues\n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Syed Islam . 1 Generally, monocular depth estimation includes techniques that use a temporal sequence of images captured from a single camera [9]- [11]. However, in this article, we focus on methods that use only a single image from a single viewpoint for depth estimation.\n\nfor scene depth. Instead of using the physical cues, these methods seem to rely on implicit knowledge (e.g., the color, vertical position, or shadows) that are learned from the training dataset [12]. We argue that monocular depth estimation depends too much on non-depth features in the given image, which makes it quite vulnerable to attacks.\n\nTo reveal the limitation mentioned above, we propose a method of adversarial patch attack for CNN-based monocular depth estimation. Specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Figure 1(c) shows an example of our adversarial patches superimposed on the input image. As shown in (d), Guo et al.'s method [7] failed to estimate correct depth in the region where the patch was located; closer depth values were obtained than the original result in (b), as was intended with our design for this pattern. In this case, the attack was conducted in a digital manner; we digitally manipulated the pixel values of the input image to superimpose the patch. Our method can also be implemented in the real world, and we have achieved similar effects by physically placing the printed patterns in a real scene.\n\nMoreover, to further analyze the behavior of monocular depth estimation under attacks, we visualize the activation levels of the intermediate layers ( Fig. 1(e)) and the regions that are potentially affected by adversarial attacks (Fig. 1(f)). These visualizations lead to a deeper understanding of the mechanism by which adversarial patches affect the target CNN. Our source code, learned patches and demo video are available at https://www.fujii.nuee.nagoyau.ac.jp/Research/MonoDepth."}
{"tabids": ["254278b5-ee90-4d52-88e1-fadc9531e8a0"], "corpus_id": 250426022, "title": "Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches", "paper_id": "35efa06e8c55a209677bcb48a6790b654d8b322f", "abstract": "Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.", "intro": "Monocular Depth Estimation (MDE) is a technique for estimating the distance between an object and the camera from RGB image inputs. It is a critical vision task for autonomous driving (AD) because it bridges the gap between Lidar sensors and RGB cameras [57] and its measurement has an effect on a variety of downstream perception tasks (e.g., object detection [57], visual SLAM [60], and visual relocalization [50]). For its importance, Tesla has integrated MDE into its productiongrade Autopilot system [6,7], and other AD companies such as Toyota [23] and Huawei [9] are also actively investigating this technique. With the increasing popularity of MDE, ensuring its security becomes a prominent challenge.\n\nExisting adversarial attacks against MDE are implemented in digital- [72,61] or physical-world platforms [68]. Compared to digital-world attacks, attacks in the physical world are more challenging because they require robust perturbations to Fig. 1: Attack MDE and 3D object detection with a natural adversarial patch. The left is a benign scenario and the right is the corresponding adversarial scenario. 3D object detection takes the pseudo-Lidar (i.e., point cloud projected from 2D depth map) as input and outputs bounding boxes of recognized objects. Observe in the adversarial scenario (b) that our optimized adversarial patch can disturb the depth estimation of the target vehicle significantly and the effect propagates to an area larger than the patch itself. Pseudo-Lidar of the vehicle is thus distorted and it cannot be detected in the downstream task.\n\novercome various photometric and geometric changes [10], reducing their stealth. Prior efforts for physical-world adversarial attacks [68,49,26,11] generally employ an unnatural-looking adversarial patch and sacrifice stealth for attack effectiveness, leaving plenty of room for improvement. Additionally, with MDE's rapid development, many downstream tasks that previously require expensive Lidar sensors or depth cameras can now be performed entirely with MDE's measurement and achieve competitive performance. However, the investigation of the impact of compromised MDE on these downstream tasks remains largely unknown.\n\nTo address the aforementioned problems, in this paper, we investigate the stealth of physical-world attack against MDE and present a physical-objectoriented adversarial patch optimization framework to generate stealthy, effective and robust adversarial patches for target objects (e.g., vehicles and pedestrians). In particular, we are able to achieve the followings: we design a physical-objectoriented adversarial optimization, which binds the patch and the target object together regarding attack effects and physical-world transformations ( \u00a73.2); we optimize the patch region on the target object with a differentiable patch mask representation, which automatically locates the highly effective area for attack on the target object and improves attack performance with a small patch size ( \u00a73.3); we camouflage the adversarial pattern with natural styles (e.g., rusty and dirty) with deep photo style transfer [31], resulting in stealthier patch for the attack ( \u00a73.4); we investigate the impact of compromised MDE on a representative downstream task in AD -3D object detection ( \u00a74.4). Fig. 1 gives an example. In addition, we examine our attack with popular defence techniques ( \u00a74.5). Our key contributions are:\n\n1. We develop a physical-object-oriented adversarial patch attack against MDE that balances stealth and effectiveness. To the best of our knowledge, we are the first to investigate stealthy physical-world attacks against MDE considering both the patch size and naturalness. 2. We propose an optimization framework that considers physical object characteristics, has a differentiable patch region representation, and provides natural style based camouflage.\n\n3. We evaluate our attack on 3 representative MDE models and a downstream task with real-world driving scenarios in both digital and physical worlds. Our attack is effective on different types of target objects and state-of-theart models. It causes over 6 meters of mean depth estimation error for a real vehicle, with a patch only 1/9 of the vehicle's rear area, and achieves more than 90% attack success rate in 3D object detection. A video is available at https://youtu.be/L-SyoAsAM0Y."}
{"tabids": ["254278b5-ee90-4d52-88e1-fadc9531e8a0"], "corpus_id": 257279866, "title": "APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation", "paper_id": "1bd2ff4afa465b40340847335cb9b8e943b3f878", "abstract": "In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to erroneous depth predictions only within the overlapping region with the input image, without considering the characteristics of the target object, such as its size, shape, and position. In this paper, we introduce a novel adversarial patch named APARATE. This patch possesses the ability to selectively undermine MDE in two distinct ways: by distorting the estimated distances or by creating the illusion of an object disappearing from the perspective of the autonomous system. Notably, APARATE is designed to be sensitive to the shape and scale of the target object, and its influence extends beyond immediate proximity. APARATE, results in a mean depth estimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of the targeted region when applied to CNN-based MDE models. Furthermore, it yields a significant error of $0.34$ and exerts substantial influence over $94\\%$ of the target region in the context of Transformer-based MDE.", "intro": "MDE has found increasing utility across various practical applications such as robotics and autonomous driving (AD).MDE involves deriving depth insights from a single image, thereby enhancing scene comprehension.Its significance extends to several critical robotic functions, including obstacle avoidance [51], object detection [43], visual SLAM [37,46], and visual relocalization [42].\n\nSeveral methodologies for depth estimation rely on technologies like RGB-D cameras, Radar, LiDAR, or ultrasound devices to directly capture depth information within a scene.\n\nHowever, these alternatives exhibit notable shortcomings.RGB-D cameras possess a limited measurement range, LiDAR and Radar deliver sparse data, and both are costly sensing solutions that might not be viable for compact autonomous systems like low-cost, lightweight and small-sized mobile robots.Ultrasound devices, on the other hand, are marred by inherent measurement inaccuracies.\n\nMoreover, these technologies demand substantial energy consumption and feature large form factors, rendering them unsuitable for resource-restricted, small-scale systems that must adhere to stringent real-world design constraints.In contrast, RGB cameras stand out as lightweight and cost-effective options.Importantly, they have the capacity to furnish more comprehensive environmental data.Prominent players within the autonomous vehicle sector are actively pushing the envelope of self-driving technology by harnessing cost-effective camera solutions.\n\nNotably, MDE has been seamlessly integrated into Tesla's production-grade Autopilot system [1,2].Evidently, other major autonomous driving (AD) enterprises, including Toyota [19] and Huawei [3], are following Tesla's footsteps to propel self-driving advancements through this approach.\n\nIn recent years, the advancement of deep learning arXiv:2303.01351v2[cs.CV] 20 Nov 2023\n\nYamanaka et al."}
{"tabids": ["254278b5-ee90-4d52-88e1-fadc9531e8a0"], "corpus_id": 260682704, "title": "SAAM: Stealthy Adversarial Attack on Monoculor Depth Estimation", "paper_id": "79411a597f24ef32126c388b3813a8070f93c75d", "abstract": "In this paper, we investigate the vulnerability of MDE to adversarial patches. We propose a novel \\underline{S}tealthy \\underline{A}dversarial \\underline{A}ttacks on \\underline{M}DE (SAAM) that compromises MDE by either corrupting the estimated distance or causing an object to seamlessly blend into its surroundings. Our experiments, demonstrate that the designed stealthy patch successfully causes a DNN-based MDE to misestimate the depth of objects. In fact, our proposed adversarial patch achieves a significant 60\\% depth error with 99\\% ratio of the affected region. Importantly, despite its adversarial nature, the patch maintains a naturalistic appearance, making it inconspicuous to human observers. We believe that this work sheds light on the threat of adversarial attacks in the context of MDE on edge devices. We hope it raises awareness within the community about the potential real-life harm of such attacks and encourages further research into developing more robust and adaptive defense mechanisms.", "intro": "M ONOCULAR depth estimation (MDE) is increasingly being utilized in a wide array of real-world applications, ranging from autonomous driving to robotics. Its main purpose is to acquire depth data, enabling a deeper and more comprehensive comprehension of the surrounding scene. MDE plays a critical and indispensable role in various tasks, including obstacle avoidance [1], object detection [2], visual SLAM [3], [4], visual re-localization [5], and numerous others.\n\nSeveral methods for estimating depth rely on sensors like RGB-D cameras, radar, lidar, or ultrasound to collect the depth information directly from a scene. These latter, however, have serious shortcomings. In fact, ultrasound devices suffer from inherently imprecise measurements, LIDAR and radar produce sparse information, and RGB-D cameras have a narrow measuring range. The aforementioned devices are extremely large and power hungry for small-sized systems, especially those that must adhere to rigorous real-world design constraints. Contrarily, RGB cameras are lightweight and less expensive. More importantly, they can offer more detailed environmental information. Several  tasks can now be accomplished totally using MDE's measurement and attain competitive performance thanks to MDE's rapid developments. These remarkable improvements can be attributed to the successful integration of deep neural networks, which have significantly enhanced MDE's capabilities.\n\nHowever, the increasing reliance on deep neural networks also brings attention to their vulnerability to adversarial attacks. As shown in various studies, these networks can be susceptible to manipulations that intentionally deceive their predictions. Hence, it becomes crucial to prioritize the security of MDE models to ensure their reliability and trustworthiness in practical applications. Safeguarding MDE systems against adversarial attacks is vital for preserving their integrity and preventing potential misinterpretations and misjudgments in real-world scenarios.\n\nPatch-based adversarial attacks [6]- [8] are a type of adversarial attack in computer vision, where carefully crafted perturbations are applied to specific patches or regions of an input image to deceive a deep learning model. The goal of patch-based adversarial attacks is to cause the model to misclassify the entire image or produce incorrect predictions for targeted regions. In contrast to traditional global adversarial attacks that perturb the entire input image, patch-based attacks are more localized, focusing on specific regions of interest.\n\nPatch-based adversarial attacks have implications in various applications, including object detection, image segmentation, and scene understanding. They demonstrate the vulnerability of deep learning models to localized adversarial perturbations and highlight the importance of developing robust defense mechanisms to protect against such attacks.\n\nOnly a limited number of studies have explored the realm of patch-based adversarial attacks on depth estimation. This particular direction of research remains relatively unexplored compared to other adversarial attack methods in computer vision. Previous work for patch-based adversarial attacks on MDE [7], [9], [10] aiming at tricking the perception module of an autonomous vehicle. Their effectiveness is limited, as they only mislead the depth prediction of specific parts within the overlapping region between the input image and the patch, usually utilizing conspicuous and eye-catching patterns. In this work, we investigate stealthy adversarial patches that can either fully conceal a particular object or trick the target methods into estimating the depth of that object incorrectly at a target depth.\n\nThis paper introduces a technique for deceiving a CNN-based monocular depth estimation system by leveraging naturalistic adversarial patches (See Figure  3). These patches are strategically designed to manipulate the system's predictions, resulting in the generation of false distance estimates. The proposed approach allows the adversarial patch to seamlessly blend into its surroundings while for example resembling a painting on a wall or a poster, and can be applied to conceal specific objects or areas of interest effectively. In fact, we set out to achieve two key goals: depth manipulation and object concealment (i.e., object-background blending). Through our proposed techniques, we can intentionally alter the perceived depth of specific objects in a scene, leading to inaccurate depth estimations. Additionally, we have developed a method to completely conceal certain objects from the depth estimation process, making them effectively invisible to the system. Moreover, our approach enables selected objects to seamlessly blend with the background, creating a visual effect where they appear to be part of the scenery, thus reducing their conspicuousness and detection. An overview of our novel contributions is shown in Figure 1. In summary, the contributions of this work are:\n\n\u2022 We present a novel patch-based adversarial attack that targets DNN-based monocular depth estimation. \u2022 Our framework generates a stealthy adversarial patch (SAAM) that can seamlessly blend into its surroundings (e.g., resembling a painting on a wall or a poster). \u2022 Our patch (SAAM) has the ability to withstand diverse transformations and adapt to different scenarios. It demonstrated robustness against a range of deformations, including rotation, perspective change, and lighting variation. Additionally, the patch can be placed at arbitrary locations within the scene, even under occlusion. \u2022 Our proposed adversarial patch extends its applicability to multiple use cases such as navigation tasks, obstacle Ratio of affected region detection, localization, etc. With a patch size as small as 0.7% of the input image, we achieve an impressive 60% depth estimation error. Moreover, the adversarial patch nearly covers the entire target region, with an almost 100% ratio of the affected region. These findings highlight the effectiveness and potency of our attack in disrupting the depth estimation process.\n\nThe structure of the remaining article is organized as follows. Section II provides a comprehensive overview of related work in the field of patch-based adversarial attacks on monocular depth estimation. In Section III, we present our proposed methodology to generate the adversarial patch. This section outlines the step-by-step process of crafting the patch and explains the techniques employed to achieve effective depth manipulation and visual realism. Section IV details the experimental setup used to evaluate the performance of the proposed adversarial attack. In Section V, we delve into the evaluation of the proposed attack. We present the results obtained from various metrics, such as depth error, affected region ratio, and SSIM, to assess the attack's potency and visual similarity of the generated patch. In Section VI, we thoroughly discuss the findings and implications of our experiments. We analyze the strengths and limitations of the proposed attack and interpret the results in the context of real-world applications. Section VII provides a succinct summary and conclusion of our study."}
{"tabids": ["ba04afbc-1ef6-407a-93db-56d82d5671d7"], "corpus_id": 53751136, "title": "Noise2Void - Learning Denoising from Single Noisy Images", "paper_id": "be2cafced16bb8834bdd322a0a512142c8d05388", "abstract": "The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.", "intro": "Image denoising is the task of inspecting a noisy image x = s + n in order to separate it into two components: its signal s and the signal degrading noise n we would like to remove. Denoising methods typically rely on the assumption that pixel values in s are not statistically independent. In other words, observing the image context of an unobserved pixel might very well allow us to make sensible predictions on the pixel intensity.\n\nA large body of work (e.g. [16,19]) explicitly modeled these interdependencies via Markov Random Fields (MRFs). In recent years, convolutional neural networks (CNNs) have been trained in various ways to predict pixel values from surrounding image patches, i.e. from the recep-noisy clean Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional  Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional Traditional   Input Input Input Input Input Input Input Input Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input  Input Input Input Input Input Input Input Input Input   noisy  noisy   NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE  NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE NOISE2NOISE   noisy  void   NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID  NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID NOISE2VOID Figure 1: Training schemes for CNN-based denoising. Traditionally, training networks for denoising requires pairs of noisy and clean images. For many practical applications, however, clean target images are not available. NOISE2NOISE (N2N) [12] enables the training of CNNs from independent pairs of noisy images. Still, also noisy image pairs are not usually available. This motivated us to propose NOISE2VOID (N2V), a novel training procedure that does not require noisy image pairs, nor clean target images. By enabling CNNs to be trained directly on a body of noisy images, we open the door to a plethora of new applications, e.g. on biomedical data.\n\ntive field of that pixel [24,11,26,6,23,25,18,14]. Typically, such systems require training pairs (x j , s j ) of noisy input images x j and their respective clean target images s j (ground truth). Network parameters are then tuned to minimize an adequately formulated error metric (loss) between network predictions and known ground truth.\n\nWhenever ground truth images are not available, these methods cannot be trained and are therefore rendered useless for the denoising task at hand. Recent work by Lehtinen et al. [12] offers an elegant solution for this problem. Instead of training a CNN to map noisy inputs to clean ground truth images, their NOISE2NOISE (N2N) train-ing attempts to learn a mapping between pairs of independently degraded versions of the same training image, i.e. (s + n, s + n ), that incorporate the same signal s, but independently drawn noise n and n . Naturally, a neural network cannot learn to perfectly predict one noisy image from another one. However, networks trained on this impossible training task can produce results that converge to the same predictions as traditionally trained networks that do have access to ground truth images [12]. In cases where ground truth data is physically unobtainable, N2N can still enable the training of denoising networks. However, this requires that two images capturing the same content (s) with independent noises (n, n ) can be acquired [3].\n\nDespite these advantages of N2N training, there are at least two shortcomings to this approach: (i) N2N training requires the availability of pairs of noisy images, and (ii) the acquisition of such pairs with (quasi) constant s is only possible for (quasi) static scenes.\n\nHere we present NOISE2VOID (N2V), a novel training scheme that overcomes both limitations. Just as N2N, also N2V leverages on the observation that high quality denoising models can be trained without the availability of clean ground truth data. However, unlike N2N or traditional training, N2V can also be applied to data for which neither noisy image pairs nor clean target images are available, i.e. N2V is a self-supervised training method. In this work we make two simple statistical assumptions: (i) the signal s is not pixel-wise independent, (ii) the noise n is conditionally pixel-wise independent given the signal s.\n\nWe evaluate the performance of N2V on the BSD68 dataset [17] and simulated microscopy data 1 . We then compare our results to the ones obtained by a traditionally trained network [24], a N2N trained network and several self-supervised methods like BM3D [5], non-local means [2], and to mean-and median-filters. While it cannot be expected that our approach outperforms methods that have additional information available during training, we observe that the denoising performance of our results only drops moderately and is still outperforming BM3D.\n\nAdditionally, we apply N2V training and prediction to three biomedical datasets: cryo-TEM images from [3], and two datasets from the Cell Tracking Challenge 2 [20]. For all these examples, the traditional training scheme cannot be applied due to the lack of ground truth data and N2N training is only applicable on the cryo-TEM data. This demonstrates the tremendous practical utility of our method.\n\nIn summary, our main contributions are: \u2022 Introduction of NOISE2VOID, a novel approach for training denoising CNNs that requires only a body of single, noisy images. \u2022 Comparison of our N2V trained denoising results 1 For simulated microscopy data we know the perfect ground truth. 2 http://celltrackingchallenge.net/ to results obtained with existing CNN training schemes [24,12,25] and non-trained methods [18,2]. \u2022 A sound theoretical motivation for our approach as well as a detailed description of an efficient implementation. The remaining manuscript is structured as follows: Section 2 contains a brief overview of related work. In Section 3, we introduce the baseline methods we later compare our own results to. This is followed by a detailed description of our proposed method and its efficient implementation. All experiments and their results are described in Section 4, and our findings are finally discussed in Section 5."}
{"tabids": ["ba04afbc-1ef6-407a-93db-56d82d5671d7"], "corpus_id": 59523708, "title": "Noise2Self: Blind Denoising by Self-Supervision", "paper_id": "ea9cf47573638745c9992cf9c5ebdabadd3c6849", "abstract": "We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement. Moreover, our framework is not restricted to a particular denoising model. We show how it can be used to calibrate any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. Finally, we prove a theoretical lower bound on the performance of an optimal denoiser. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.", "intro": "We would often like to reconstruct a signal from highdimensional measurements that are corrupted, undersampled, or otherwise noisy. Devices like high-resolution cameras, electron microscopes, and DNA sequencers are capable of producing measurements in the thousands to millions of feature dimensions. But when these devices are pushed to their limits, taking videos with ultra-fast frame rates at very low-illumination, probing individual molecules with electron microscopes, or sequencing tens of thousands of cells simultaneously, each individual feature can become quite noisy. Nevertheless, the objects being studied are often very structured and the values of different features are highly correlated. Speaking loosely, if the \"latent dimension\" of the space of objects under study is much lower than * Equal contribution 1 Chan-Zuckerberg Biohub. Correspondence to: Joshua Batson <joshua.batson@czbiohub.org>, Loic Royer <loic.royer@czbiohub.org>. the dimension of the measurement, it may be possible to implicitly learn that structure, denoise the measurements, and recover the signal without any prior knowledge of the signal or the noise.\n\nTraditional denoising methods each exploit a property of the noise, such as Gaussianity, or structure in the signal, such as spatiotemporal smoothness, self-similarity, or having low-rank. The performance of these methods is limited by the accuracy of their assumptions. For example, if the data are genuinely not low rank, then a low rank model will fit it poorly. This requires prior knowledge of the signal structure, which limits application to new domains and modalities. These methods also require calibration, as hyperparameters such as the degree of smoothness, the scale of self-similarity, or the rank of a matrix have dramatic impacts on performance.\n\nIn contrast, a data-driven prior, such as pairs (x i , y i ) of noisy and clean measurements of the same target, can be used to set up a supervised learning problem. A neural net trained to predict y i from x i may be used to denoise new noisy measurements (Weigert et al., 2018). As long as the new data are drawn from the same distribution, one can expect performance similar to that observed during training. Lehtinen et al. demonstrated that clean targets are unnecessary (2018). A neural net trained on pairs (x i , x i ) of independent noisy measurements of the same target will, under certain distributional assumptions, learn to predict the clean signal. These supervised approaches extend to image denoising the success of convolutional neural nets, which currently give state-of-the-art performance for a vast range of image-to-image tasks. Both of these methods require an experimental setup in which each target may be measured multiple times, which can be difficult in practice.\n\nIn this paper, we propose a framework for blind denoising based on self-supervision. We use groups of features whose noise is independent conditional on the true signal to predict one another. This allows us to learn denoising functions from single noisy measurements of each target, with performance close to that of supervised methods. The same approach can also be used to calibrate traditional image denoising methods such as median filters and non-local means, and, using a different independence structure, denoise highly under-sampled single-cell gene expression data. d Figure 1. (a) The box represents the dimensions of the measurement x. J is a subset of the dimensions, and f is a J-invariant function: it has the property that the value of f (x) restricted to dimensions in J, f (x)J , does not depend on the value of x restricted to J, xJ . This enables self-supervision when the noise in the data is conditionally independent between sets of dimensions. Here are 3 examples of dimension partitioning: (b) two independent image acquisitions, (c) independent pixels of a single image, (d) independently detected RNA molecules from a single cell.\n\nWe model the signal y and the noisy measurement x as a pair of random variables in R m . If J \u2282 {1, . . . , m} is a subset of the dimensions, we write x J for x restricted to J.\n\nDefinition. Let J be a partition of the dimensions {1, . . . , m} and let J \u2208 J . A function f :\nR m \u2192 R m is J-invariant if f (x) J does not depend on the value of x J . It is J -invariant if it is J-invariant for each J \u2208 J .\nWe propose minimizing the loss\nL(f ) = E f (x) \u2212 x 2 ,(1)\nover J -invariant functions f . Since f has to use information from outside of each subset of dimensions J to predict the values inside of J, it cannot merely be the identity.\n\nSuppose x is an unbiased estimator of y, that is, E[x|y] = y, and the noise in each subset J \u2208 J is independent from the noise in its complement J c , conditional on y. Then we can decompose the loss as\nL(f ) = E f (x) \u2212 y 2 + E x \u2212 y 2 ,(2)\nthe sum of the ordinary supervised loss and the variance of the noise. By minimizing the self-supervised loss L(f ), which we can compute from noisy data alone, we are able to find the actual optimal denoiser among any class of Jinvariant functions.\n\nFor example, if the signal is an image with independent, mean-zero noise on each pixel, we may choose J = {{1}, . . . , {m}} to be the singletons of each coordinate. In \u00a73, we show how to adapt any traditional image denoising function into a J -invariant one, and then calibrate it using this loss. For example, \"donut\" median filters, with a hole in the center, form a class of J -invariant functions. By comparing the value of the loss for different filter radii, we are able to select the optimal radius for denoising the image at hand.\n\nThe donut median filter has just one parameter and therefore limited ability to adapt to the data. At the other extreme, we may search over all J -invariant functions for the global optimum:\nProposition 1. The J -invariant function f * J minimizing (1) satisfies f * J (x) J = E[y J |x J c ] for each subset J \u2208 J .\nThat is, the optimal J -invariant predictor for the dimensions of y in some J \u2208 J is their expected value conditional on observing the dimensions of x outside of J.\n\nIn \u00a74, we use analytical examples to illustrate how the optimal J -invariant denoising function approaches the optimal general denoising function as the amount of correlation between features in the data increases.\n\nIn practice, we may attempt to approximate the optimal denoiser by searching over a very large class of functions, such as deep neural networks with millions of parameters. In \u00a75, we show that a deep convolutional network, modified to become J -invariant using a masking procedure, can achieve state-of-the-art denoising performance on three diverse datasets.\n\nSample code is available on GitHub 1 and deferred proofs are contained in the Supplement."}
{"tabids": ["ba04afbc-1ef6-407a-93db-56d82d5671d7"], "corpus_id": 173990717, "title": "Probabilistic Noise2Void: Unsupervised Content-Aware Denoising", "paper_id": "7cca7a3754d6960618df3ee86c4fbb48a8dbc937", "abstract": "Today, Convolutional Neural Networks (CNNs) are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods such as Noise2Void~(N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present 'Probabilistic Noise2Void' (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods.", "intro": "Image restoration is the problem of reconstructing an image from a corrupted version of itself. Recent work shows how CNNs can be used to build powerful content-aware image restoration (CARE) pipelines [11,10,12,13,6,4,1,5]. However, for supervised CARE models, such as [10], pairs of clean and noisy images are required.\n\nFor many application areas, it is impractical or impossible to acquire clean ground-truth images [2]. In such cases, Noise2Noise (N2N) training [6] relaxes the problem, only requiring two noisy instances of the same data. Unfortunately, even the acquisition of two noisy realizations of the same image content is often difficult [2]. Self-supervised training methods, such as Noise2Void (N2V) [4], are a promising alternative, as they operate exclusively on single noisy images [4,1,5]. This is enabled by excluding/masking the center (blind-spot) of the network's receptive fields. Self-supervised training assumes that the noise is pixel-wise independent and that the true intensity of a pixel can be predicted from local image context, excluding before-mentioned blind-spots [4]. For many applications, especially in the context of microscopy images, the first assumption is fulfilled, but the second assumption offers room for improvements [5].\n\nHence, self-supervised models can often not compete with supervised training [4]. In concurrent work, by Laine et al. [5], this problem was elegantly addressed by assuming a Gaussian noise model and predicting Gaussian intensity distributions per pixel. The authors also showed that the same approach an be applied to other noise distributions, which can be approximated as Gaussian, or can be described analytically.\n\nHere, we introduce a new training approach called Probabilistic Noise2Void (PN2V). Similar to [5], PN2V proposes a way to leverage information of the network's blind-spots. However, PN2V is not restricted to Gaussian noise models or Gaussian intensity predictions. More precisely, to compute the posterior distribution of a pixel, we combine (i) a general noise model that can be represented as a histogram (observation likelihood), and (ii) a distribution of possible true pixel intensities (prior), represented by a set of predicted samples.\n\nHaving this complete probabilistic model for each pixel, we are now free to chose which statistical estimator to employ. In this work we use MMSE estimates for our final predictions and show that MMSE-PN2V consistently outperformes other self-supervised methods and, in many cases, leads to results that are competitive even with supervised state-of-the-art CARE networks (see below)."}
{"tabids": ["ba04afbc-1ef6-407a-93db-56d82d5671d7"], "corpus_id": 225062070, "title": "Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising", "paper_id": "9d9ad34f43d9968cc58d4f8b929021f95cbd0b05", "abstract": "Self-supervised frameworks that learn denoising models with merely individual noisy images have shown strong capability and promising performance in various image denoising tasks. Existing self-supervised denoising frameworks are mostly built upon the same theoretical foundation, where the denoising models are required to be J-invariant. However, our analyses indicate that the current theory and the J-invariance may lead to denoising models with reduced performance. In this work, we introduce Noise2Same, a novel self-supervised denoising framework. In Noise2Same, a new self-supervised loss is proposed by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J-invariance nor extra information about the noise model and can be used in a wider range of denoising applications. We analyze our proposed Noise2Same both theoretically and experimentally. The experimental results show that our Noise2Same remarkably outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency. Our code is available at https://github.com/divelab/Noise2Same.", "intro": "The quality of deep learning methods for signal reconstruction from noisy images, also known as deep image denoising, has benefited from the advanced neural network architectures such as ResNet [8], U-Net [19] and their variants [29,16,26,31,25,14]. While more powerful deep image denoising models are developed over time, the problem of data availability becomes more critical.\n\nMost deep image denoising algorithms are supervised methods that require matched pairs of noisy and clean images for training [27,29,2,7]. The problem of these supervised methods is that, in many denoising applications, the clean images are hard to obtain due to instrument or cost limitations. To overcome this problem, Noise2Noise [13] explores an alternative training framework, where pairs of noisy images are used for training. Here, each pair of noisy images should correspond to the same but unknown clean image. Note that Noise2Noise is basically still a supervised method, just with noisy supervision.\n\nDespite the success of Noise2Noise, its application scenarios are still limited as pairs of noisy images are not available in some cases and may have registration problems. Recently, various of denoising frameworks that can be trained on individual noisy images [23,17,28,10,1,12] have been developed. These studies can be divided into two categories according to the amount of extra information required. Methods in the first category requires the noise model to be known. For example, the simulation-based methods [17,28] use the noise model to generate simulated noises and make individual noisy images noisier. Then a framework similar to Noise2Noise can be applied to train the model with pairs of noisier image and the original noisy image. The limitation is obvious as the noise model may be too complicated or even not available. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\nOn the other hand, algorithms in the second category target at more general cases where only individual noisy images are available without any extra information [23,10,1,12]. In this category, self-supervised learning [30,6,24] has been widely explored, such as Noise2Void [10], Noise2Self [1], and the convolutional blind-spot neural network [12]. Note that these self-supervised models can be improved as well if information about the noise model is given. For example, Laine et al. [12] and Krull et al. [11] propose the Bayesian post-processing to utilize the noise model. However, with the proposed post-processing, these methods fall into the first category where applicability is limited.\n\nIn this work, we stick to the most general cases where only individual noisy images are provided and focus on the self-supervised framework itself without any post-processing step. We note that all of these existing self-supervised denoising frameworks are built upon the same theoretical background, where the denoising models are required to be J -invariant (Section 2). We perform in-depth analyses on the J -invariance property and argue that it may lead to denoising models with reduced performance. Based on this insight, we propose Noise2Same, a novel self-supervised denoising framework, with a new theoretical foundation. Noise2Same comes with a new self-supervised loss by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J -invariance nor extra information about the noise model. We analyze the effect of the new loss theoretically and conduct thorough experiments to evaluate Noise2Same. Result show that our Noise2Same consistently outperforms previous self-supervised denoising methods."}
{"tabids": ["ba04afbc-1ef6-407a-93db-56d82d5671d7"], "corpus_id": 219619080, "title": "Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image", "paper_id": "68a7b1a8dc78b4d9d7fca1b6da1dcb3d5d5285bc", "abstract": "In last few years, supervised deep learning has emerged as one powerful tool for image denoising, which trains a denoising network over an external dataset of noisy/clean image pairs. However, the requirement on a high-quality training dataset limits the broad applicability of the denoising networks. Recently, there have been a few works that allow training a denoising network on the set of external noisy images only. Taking one step further, this paper proposes a self-supervised learning method which only uses the input noisy image itself for training. In the proposed method, the network is trained with dropout on the pairs of Bernoulli-sampled instances of the input image, and the result is estimated by averaging the predictions generated from multiple instances of the trained model with dropout. The experiments show that the proposed method not only significantly outperforms existing single-image learning or non-learning methods, but also is competitive to the denoising networks trained on external datasets.", "intro": "Image denoising is the process to remove measurement noises from noisy images. It not only has great practical value, but also serves as a core module in many image recovery tasks. A noisy image y is usually modeled as\ny = x + n,(1)\nwhere x denotes the clean image (ground truth), and n denotes the measurement noise often assumed to be random. In recent years, deep learning has become a prominent approach for image denoising, which uses a set of training samples to train a deep neural network (NN), denoted by F \u03b8 (\u00b7) with the parameter vector \u03b8, that maps a noisy image to its clean counterpart. Most existing deep-learningbased denoising methods (e.g. [26,31,32]) use many pairs of clean/noisy images, denoted by {x (i) , y (i) } i , as the train-ing samples, and the training is done by solving\nmin \u03b8 i L(F \u03b8 (x (i) ), y (i) ),(2)\nwhere L(\u00b7, \u00b7) measures the distance between two images. The availability of a large number of training samples is one key factor contributing to the performance of these methods. Sometimes, it can be expensive and difficult to collect a large dataset of useful clean/noisy image pairs. Recently, there are some studies on training denoising NNs with only external noisy images. The Noise2Noise (N2N) method [19] showed that a denoising NN model can be trained using many pairs of two noisy images of the same scene. Using a self-prediction loss, together with a so-called blind-spot strategy to avoid learning an identity mapping, the Noise2Void (N2V) method [15] and the Noise2Self (N2S) method [3] showed the possibility to learn a denoising NN with good performance on a set of unorganized external noisy images. Yet, to achieve good performance, the external images used for training should be highly related to the noisy image being processed, in terms of image content and noise statistics. The collection of such external images can be costly or challenging in practice.\n\nIt is of great value to develop a powerful denoising NN that has no prerequisite on training samples. That is, the denoising NN is learned only on the input image itself. So far, there has been very little work along this line. Based on the deep image prior (DIP), Ulyanov et al. [25] proposed a single-image deep learning model for image recovery. The aforementioned dataset-based N2V and N2S methods can also be trained using only a noisy image. However, the performance of these methods is not competitive to existing non-local methods, e.g. BM3D [10]. To summarize, there is no satisfactory solution on how to train a denoising NN with good performance, given only the input noisy image."}
{"tabids": ["ba04afbc-1ef6-407a-93db-56d82d5671d7"], "corpus_id": 247447122, "title": "Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots", "paper_id": "3949f666dc13ee806a119e0e1e0ea7625e1b20c1", "abstract": "Real noisy-clean pairs on a large scale are costly and difficult to obtain. Meanwhile, supervised denoisers trained on synthetic data perform poorly in practice. Self-supervised denoisers, which learn only from single noisy images, solve the data collection problem. However, self-supervised denoising methods, especially blindspot-driven ones, suffer sizable information loss during input or network design. The absence of valuable information dramatically reduces the upper bound of denoising performance. In this paper, we propose a simple yet efficient approach called Blind2Unblind to overcome the information loss in blindspot-driven denoising methods. First, we introduce a global-aware mask mapper that enables global perception and accelerates training. The mask mapper samples all pixels at blind spots on denoised volumes and maps them to the same channel, allowing the loss function to optimize all blind spots at once. Second, we propose a re-visible loss to train the denoising network and make blind spots visible. The denoiser can learn directly from raw noise images without losing information or being trapped in identity mapping. We also theoretically analyze the convergence of the re-visible loss. Extensive experiments on synthetic and real-world datasets demonstrate the superior performance of our approach compared to previous work. Code is available at https://github.com/demonsjin/Blind2Unblind.", "intro": "Image denoising, an essential task of low-level image processing, aims to remove noise and restore a clean image. In vision applications, the quality of denoising significantly affects the performance of downstream tasks, such as superresolution [16], semantic segmentation [22], and object detection [31]. In addition, the denoiser can significantly improve the quality of images captured by mobile phones and other devices, reflecting a broad demand in imaging fields."}
{"tabids": ["d3a4840b-f497-4bfb-9c6a-7d2e5c7c7b0a"], "corpus_id": 247996703, "title": "Text2LIVE: Text-Driven Layered Image and Video Editing", "paper_id": "c0e8812789e96f5a7aa3ad940dba1c237aec822d", "abstract": "We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.", "intro": "Computational methods for manipulating the appearance and style of objects in natural images and videos have seen tremendous progress, facilitating a variety of editing effects to be achieved by novice users. Nevertheless, research in this area has been mostly focused in the Style-Transfer setting where the target appearance is given by a reference image (or domain of images), and the original image is edited in a global manner [16]. Controlling the localization of the edits typically involves additional input guidance such as segmentation masks. Thus, appearance transfer has been mostly restricted to global artistic stylization or to specific image domains or styles (e.g., faces, day-to-night, summer-to-winter). In this work, we seek to eliminate these requirements and enable more flexible and creative semantic appearance manipulation of real-world images and videos. Inspired by the unprecedented power of recent Vision-Language models, we use simple text prompts to express the target edit. This allows the user to easily and intuitively specify the target appearance and the object/region to be edited. Specifically, our method enables local, semantic editing that satisfies a given target text prompt (e.g., Fig. 1 and Fig. 2). For example, given the cake image in Fig. 1(b), and the target text: \"oreo cake\", our method automatically locates the cake region and synthesizes realistic, high-quality texture that combines naturally with the original image -the cream filling and the cookie crumbs \"paint\" the full cake and the sliced piece in a semantically-aware manner. As seen, these properties hold across a variety of different edits.\n\nOur framework leverages the representation learned by a Contrastive Language-Image Pretraining (CLIP) model, which has been pre-trained on 400 million text-image examples [35]. The richness of the enormous visual and textual space spanned by CLIP has been demonstrated by various recent image editing methods (e.g., [2,3,11,12,33]). However, the task of editing existing objects in arbi-trary, real-world images remains challenging. Most existing methods combine a pre-trained generator (e.g., a GAN or a Diffusion model) in conjunction with CLIP. With GANs, the domain of images is restricted and requires to invert the input image to the GAN's latent space -a challenging task by itself [49]. Diffusion models [13,45] overcome these barriers but face an inherent trade-off between satisfying the target edit and maintaining high-fidelity to the original content [2]. Furthermore, it is not straightforward to extend these methods to videos. In this work, we take a different route and propose to learn a generator from a single input-image or video and text prompts.\n\nIf no external generative prior is used, how can we steer the generation towards meaningful, high-quality edits? We achieve this via the following two key components: (i) we propose a novel text-guided layered editing, i.e., rather than directly generating the edited image, we represent the edit via an RGBA layer (color and opacity) that is composited over the input. This allows us to guide the content and localization of the generated edit via a novel objective function, including text-driven losses applied directly to the edit layer. For example, as seen in Fig. 2, we use text prompts to express not only the final edited image but also a target effect (e.g., fire) represented by the edit layer. (ii) We train our generator on an internal dataset of diverse image-text training examples by applying various augmentations to the input image and text. We demonstrate that our internal learning approach serves as a strong regularization, enabling high quality generation of complex textures and semi-transparent effects.\n\nWe further take our framework to the realm of text-guided video editing. Realworld videos often consist of complex object and camera motion, which provide abundant information about the scene. Nevertheless, achieving consistent video editing is difficult and cannot be accomplished na\u00efvely. We thus propose to decompose the video into a set of 2D atlases using [18]. Each atlas can be treated as a unified 2D image representing either a foreground object or the background throughout the video. This representation significantly simplifies the task of video editing: edits applied to a single 2D atlas are automatically mapped back to the entire video in a consistent manner. We demonstrate how to extend our framework to perform edits in the atlas space while harnessing the rich information readily available in videos.\n\nIn summary, we present the following contributions:\n\n-An end-to-end text-guided framework for performing localized, semantic edits of existing objects in real-world images. -A novel layered editing approach and objective function that automatically guides the content and localization of the generated edit. -We demonstrate the effectiveness of internal learning for training a generator on a single input in a zero-shot manner. -An extension to video which harnesses the richness of information across time, and can perform consistent text-guided editing. -We demonstrate various edits, ranging from changing objects' texture to generating complex semi-transparent effects, all achieved fully automatically across a wide-range of objects and scenes."}
{"tabids": ["d3a4840b-f497-4bfb-9c6a-7d2e5c7c7b0a"], "corpus_id": 254974187, "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation", "paper_id": "1367dcff4ccb927a5e95c452041288b3f0dd0eff", "abstract": "To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting$\\unicode{x2014}$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.", "intro": "The large-scale multimodal dataset [41], consisting of billions of text-image pairs crawled from the Internet, has enabled a breakthrough in Text-to-Image (T2I) generation [30,35,6,42,40]. To replicate this success in Text-to-Video (T2V) generation, recent works [42,15,18,53,47] have extended spatial-only T2I generation models to the spatio-temporal domain. These models generally adopt the standard paradigm of training on large-scale text-video datasets (e.g., WebVid-10M [2]). Although this paradigm produces promising results for T2V generation, it requires extensive training on large hardware accelerators, which is expensive and time-consuming.\n\nHumans possess the ability to create new concepts, ideas, or things by utilizing their existing knowledge and the information provided to them. For example, when presented a video with a textual description of \"a man skiing on snow\", we can imagine how a panda would ski on snow, drawing upon our knowledge of what a panda looks like. As T2I models pretrained with large-scale image-text data already capture knowledge of open-domain concepts, a intuitive question arises: can they infer other novel videos from a single video example, like humans? A new T2V generation setting is therefore introduced, namely, One-Shot Video Tuning, where only a single text-video pair is used to train a T2V generator. The generator is expected to capture essential motion information from the input video and synthesize novel videos with edited prompts.\n\nIntuitively, the key to successful video generation lies in preserving the continuous motion of consistent objects. So we make the following observations on state-of-the-art T2I diffusion models [37] that motivate our method accordingly.\n\n(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt \"a man is running on the beach\", the T2I models produce the snapshot where a man is running (not walking or jumping), albeit not necessarily in a continuous manner (the first row of Fig. 2). This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation. (2) Regarding consistent objects: Simply extending the spatial self-attention in the T2I model from one image to multiple images produces consistent content across frames. Taking \"A man is running on the beach\" ! spatial selfattention spatio-temporal attention Figure 2: Observations on pretrained T2I models: 1) They can generate still images that accurately represent the verb terms. 2) Extending spatial self-attention to spatio-temporal attention produces consistent content across frames.\n\" # $ ! \" # $ ! \" # $ ! \" # $ ! \" # $\nthe same example, when we generate consecutive frames in parallel with extended spatio-temporal attention, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions. We implement our findings into a simple yet effective method called Tune-A-Video. Our method is based on a simple inflation of state-of-the-art T2I models over spatiotemporal dimension. However, using full attention in spacetime inevitably leads to quadratic growth in computation. It is thus infeasible for generating videos with increasing frames. Additionally, employing a naive fine-tuning strategy that updates all the parameters can jeopardize the preexisting knowledge of T2I models and hinder the generation of videos with new concepts. To tackle these problems, we introduce a sparse spatio-temporal attention mechanism that only visits the first and the former video frame, as well as an efficient tuning strategy that only updates the projection matrices in attention blocks. Empirically, these designs maintain consistent objects across all frames but lack continuous motion. Therefore, at inference, we further seek structure guidance from input video through DDIM inversion, which is a reverse process of DDIM sampling [43]. With the inverted latent as initial noise, we produce temporallycoherent videos featuring smooth movement. Notably, our method is inherently compatible with exiting personalized and conditional pretrained T2I models, such as Dream-Booth [39] and T2I-Adapter [29], providing a personalized and controllable user interface.\n\nWe showcase remarkable results of Tune-A-Video across a wide range of applications for text-driven video generation (see Fig. 1). We compare our method against the stateof-the-art baselines through extensive qualitative and quantitative experiments, demonstrating its superiority. In summary, our key contributions are as follows:\n\n\u2022 We introduce a new setting of One-Shot Video Tuning for T2V generation, which eliminates the burden of training with large-scale video datasets.\n\n\u2022 We present Tune-A-Video, which is the first framework for T2V generation using pretrained T2I models.\n\n\u2022 We propose efficient attention tuning and structural inversion that significant improve temporal consistency.\n\n\u2022 We demonstrate remarkable results of our method through extensive experiments."}
{"tabids": ["d3a4840b-f497-4bfb-9c6a-7d2e5c7c7b0a"], "corpus_id": 261031087, "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing", "paper_id": "05cbac9a5101f47a6fabad72398616506572c9fa", "abstract": "Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at \\href{https://github.com/rese1f/StableVideo}{this https URL}.", "intro": "Recent years have witnessed significant progress in extensive computer vision tasks taken by deep learning. Nev-* The work was done when the author was with MSRA as an intern. ertheless, natural video editing, which aims at manipulating the appearance of target objects and scenes, still faces two essential challenges that are deterministic to the editing quality: the generator equipped with rich prior knowledge that consistently produces high-fidelity edited contents adhering faithfully to the original geometry of the target objects, and the propagator that disseminates the edited contents throughout the entire video while keeping highly temporal consistency.\n\nThe flourish of text-driven generative diffusion models pre-trained on large-scale image and language data [34,14,53,16,41,5] provides impressive generation quality. Several diffusion-based methods achieve good performance in image editing [2,31], but few methods have tried to apply diffusion models in video editing, since it is challenging to modify existing objects while preserving their appearance over the entire video [49,12,26]. Dreamix [27] proposes a solution to generate consistent video according to input image/video and prompts. However, it focuses more on generating smooth motions, e.g., pose and camera movements, rather than maintaining geometric consistency of the objects across time. Moreover, such video diffusion models often suffer from huge computing complexity which is not friendly for practical applications.\n\nNeural layered atlas (NLA) [24,23] tries to tackle the temporal continuity problem by decomposing the video into a set of atlas layers, each of which describes one target object to be edited. For each atlas layer, the positions of the video are mapped into the corresponding 2D positions in it, so that semantically correspondent pixels over the whole video can be represented by the same atlas position. Instead of frame-by-frame editing, NLA edits atlas layers to ensure that the modifications can be precisely mapped back to video frames for temporal smoothness. Text2LIVE [1] provides a text-driven appearance manipulation solution of adding additional edit layers on atlases, in which a specific generator for the edit layers is trained. Although it achieves good results with strict structure preserved, it is not able to apply thorough editing. Moreover, the specifically trained generator also limits the richness of the generated contents.\n\nThis brings up the question: Could text-driven diffusion video editing achieve high temporal consistency? Intuitively, employing text-driven diffusion models to edit the atlases corresponding to the target objects could reach such goal. However, this gives rise to drawbacks rather than benefits. Being the summary of the whole video, atlases always have distorted appearance due to the viewpoint and camera movement, which are required to be specifically pretrained and generated as in [1]. Diffusion models may fail in generating satisfied atlas pixels in many cases, so that the corresponding edited frames will also be contaminated. To answer the question, we present two concepts for utilizing diffusion models in video editing. Firstly, instead of editing the atlases directly, we propose to update the atlases via editing key video frames. Secondly, we introduce temporal dependency constraints for diffusion models to generate objects with consistent appearance across time.\n\nBased on analysis above, we present a novel diffusion video editing approach, StableVideo, to perform consistency-aware video editing. In specific, we propose two effective technologies for this purpose. Firstly, to edit the objects with consistent appearance, we design an interframe propagation mechanism on top of the existing diffusion model [55], which can generate new objects with coherent geometry across time. Secondly, to achieve temporal consistency by leveraging NLA, we design an aggregation network to generate the edited atlases from the key frames. We then build up a text-driven diffusion-based framework, which provides high-quality natural video editing. We conduct extensive qualitative and quantitative experiments to demonstrate the capability of our approach. Compared with state-of-the-art methods, our approach achieves superior results with much lower complexity.\n\nIn summary, we present the following contributions:\n\n\u2022 To our best knowledge, we are the first to solve the consistency problem of diffusion video editing by considering the concept of layered atlas approaches, which provides an efficient and effective way for this topic.\n\n\u2022 We present a new video editing framework which can manipulate the appearance of the objects with high ge-ometry and appearance consistency across time. Our method can be easily applied to other text-driven diffusion models.\n\n\u2022 We conduct extensive experiments on a variety of natural videos, which shows superior editing performance compared with state-of-the-art methods."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 7319196, "title": "Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images", "paper_id": "048d133e2ec513ce385c8e736df715d8ff496e17", "abstract": "In this paper, we introduce Recipe1M, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M dataset and food and cooking in general. Code, data and models are publicly available.", "intro": "T HERE are few things so fundamental to the human experience as food. Its consumption is intricately linked to our health, our feelings and our culture. Even migrants starting a new life in a foreign country often hold on to their ethnic food longer than to their native language. Vital as it is to our lives, food also offers new perspectives on topical challenges in computer vision like finding representations that are robust to occlusion and deformation (as occur during ingredient processing).\n\nThe profusion of online recipe collections with user-submitted photos presents the possibility of training machines to automatically understand food preparation by jointly analyzing ingredient lists, cooking instructions and food images. Far beyond applications solely in the realm of culinary arts, such a tool may also be applied to the plethora of food images shared on social media to achieve insight into the significance of food and its preparation on public health [1] and cultural heritage [2]. Developing a tool for automated analysis requires large and well-curated datasets.\n\nThe emergence of massive labeled datasets [3], [4] and deeplylearned representations [5], [6], [7] have redefined the state-ofthe-art in object recognition and scene classification. Moreover, the same techniques have enabled progress in new domains like dense labeling and image segmentation. Perhaps the introduction of a new large-scale food dataset-complete with its own intrinsic challenges-will yield a similar advancement of the field. For instance, categorizing an ingredient's state (e.g., sliced, diced, raw, baked, grilled, or boiled) provides a unique challenge in attribute recognition-one that is not well posed by existing datasets. Furthermore, the free-form nature of food suggests a departure from the concrete task of classification in favor of a more nuanced objective that integrates variation in a recipe's structure. *contributed equally. Learning cross-modal embeddings from recipe-image pairs collected from online resources. These embeddings enable us to achieve in-depth understanding of food from its ingredients to its preparation.\n\nHence, we argue that food images must be analyzed together with accompanying recipe ingredients and instructions in order to acquire a comprehensive understanding of \"behind-the-scene\" cooking process as illustrated in Fig. 1.\n\nExisting work, however, has focused largely on the use of medium-scale image datasets for performing food categorization. For instance, Bossard et al. [8] introduced the Food-101 visual classification dataset and set a baseline of 50.8% accuracy. Even with the impetus for food image categorization, subsequent work by [9], [10] and [11] could only improve this result to 77.4%, 79% and 80.9%, respectively, which indicates that the size of the dataset may be the limiting factor. Although Myers et al. [10] built upon Food-101 to tackle the novel challenge of estimating a meal's energy content, the segmentation and depth information used in their work are not made available for further exploration."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 234470115, "title": "A Large-Scale Benchmark for Food Image Segmentation", "paper_id": "99e6e9541166c1293d30e0ccf259f1d94f28e953", "abstract": "Food image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks -- the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images. In this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based, Feature Pyramid based, and Vision Transformer based) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works on fine-grained food image understanding. We make all these datasets and methods public at \\url{https://xiongweiwu.github.io/foodseg103.html}.", "intro": "Food computing has attracted increasing public attention in recent years, as it provides the core technologies for food and healthrelated research and applications. [2,9,31,43]. One of the important goals of food computing is to automatically recognize different types of food and profile their nutrition and calorie values. In computer vision, the related works include dish classification [11,50,52], recipe generation [14,39,46], and food image retrieval [6,42]. Most of them focus on representing and analysing the food image as a whole, and do not explicitly localize or classify its individual ingredients-the visible components in the cooked food. We call the former food image classification and the latter food image segmentation. Between the two, food image segmentation is more complex as it aims to recognize each ingredient category as well as its pixel-wise locations in the food image. As shown in Figure 1, given an \"hamburger\" example image, a good segmentation model  needs to recognize and mask out \"beef\", \"tomato\", \"lettuce\", \"onion\" and \"bread roll\" ingredients. Compared to semantic segmentation on general object images [3,17,22], food image segmentation is more challenging due to the large diversity in food appearances and the often imbalanced distribution of categories of ingredients. First, an ingredient cooked differently can vary a lot visually, e.g., \"pineapples\" cooked with meat in Figure 1 (a) versus the \"pineapples\" in a fruit platter in Figure 1 (b). Different ingredients may look very similar, e.g., \"pineapples\" cooked with meat cannot be easily distinguished from \"potatoes\" cooked with meat, as shown in Figures 1 (a) and (c) respectively. Second, food datasets usually suffer from imbalanced distributionboth food classes and ingredient classes often exist in long-tailed distributions. This is inevitable due to two reasons: 1) large number of food images are dominated by very few popular food classes while vast majority of food classes are unpopular; and 2) there is a selection bias in the construction of food image collection [44]. We will elaborate the detailed distribution analysis in Section 3.\n\nExisting food image datasets, such as ETH Food101 [1], Recipe1M [41], and Geo-Dish [52], mainly facilitate the research of dish classification or recipe generation. They do not have fine-grained ingredient masks or labels. UECFoodPix [13] and UECFoodPixComplete [35] are the only two public datasets for food image segmentation. However, their segmentation masks are annotated at dish level only. That is, each mask covers the region of an entire dish instead of that of food ingredients. We elaborate more dataset comparison in Section 3.3.\n\nDataset contribution: To facilitate fine-grained food image segmentation, we build a large-scale dataset called FoodSeg103, for which we have defined 103 ingredient classes and annotated 7,118 western food images using these labels together with the corresponding segmentation masks. Besides, we annotated an additional set of 2,372 images of Asian food which covers more diverse set of ingredients making these images more challenging than those in the main set (FoodSeg103). For this set, we defined 112 ingredient classes-55% overlap with the ingredient classes of the main set. In total, we annotated 154 classes of ingredients with around 60k masks (in the two datasets). We name the combined dataset as FoodSeg154. During the annotation, we carried out careful data selection, iterative refinement of labels and masks (to be further elaborated in Section 3.2), so as to guarantee high quality labels and masks in the dataset. Our annotation is thus expensive and time-consuming. In experiments, we use FoodSeg103 for in-domain training and testing, and use the additional set in FoodSeg154 for out-domain testing.\n\nModel contribution: The source images of FoodSeg103 are from another existing food dataset Recipe1M [41]-millions of images and cooking recipes, used for recipe generation. Each recipe contains not only \"how to cook\" but also \"what ingredient to use\". In our work, we leverage these recipe information as auxiliary information to train semantic segmentation models. We call this multi-modality knowledge transfer and name our training method ReLeM. Specifically, ReLeM integrates food recipe data, in the format of language embedding, with the visual representation of the food image. In this way, it forces the visual representation of an ingredient appearing in different dishes to have their appearances \"connected\" in the feature space through a common language embedding (extracted from the ingredient's label and its cooking instructions).\n\nExperiment contribution: We validate our proposed ReLeM model by plugging it into the state-of-the-art semantic segmentation models such as CCNet [17], Sem-FPN [22] and SeTR [54]. In experiments, we compare ReLeM-variants with these baseline models using both convolutional networks and transformer backbones. Our experiments show that ReLeM is generic to be applied into multiple segmentation frameworks, and it helps to achieve significant accuracy improvement when incorporated into the SOTA CNNbased model CCNet. This validates that our knowledge transfer approach works more efficient on stronger models-a characteristic preferred by the multimedia community.\n\nOur contributions are thus three-fold. i) We build a large-scale food image segmentation dataset called FoodSeg103 (and its extension FoodSeg154). It can facilitate a promising and challenging benchmark for the task of semantic segmentation in food images. ii) We propose a knowledge transfer approach ReLeM that utilizes the multi-modality information of recipe datasets. It can be incorporated into different semantic segmentation methods to boost the model performance. iii) We conduct extensive experiments that reveal the challenges of segmenting food on our FoodSeg103 dataset, and validate the efficiency of our ReLeM based on multiple baseline methods."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 206822288, "title": "Recipe recognition with large multimodal food dataset", "paper_id": "9170443fb0e6a343bdcb68b3f49ae8e3abadfb48", "abstract": "This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.", "intro": "Food category classification is a key technology for many food-related applications such as monitoring healthy diet, computational cooking, food recommendation system, etc. In [1], a novel smart phone application to record daily meal activities by image retrieval technique is developed. Based on this personal dietary data log system, they were able to conduct further usage preference experiments [2] and food nutrition balance estimation [3].\n\nOpen Food System 2 aims at inventing new smart cooking appliances, with the ability to monitor cooking settings automatically for optimal results and preserve the nutritional value and organoleptic qualities of cooked foods. The Technology Assisted Dietary Assessment (TADA) project of Purdue University [4] aims at developing a mobile food recorder which can translate dietary information to an accurate account of daily food and nutrient intake. Food category classification is an indispensable ingredient in all these applications.\n\nIn this paper, we focus on building automatic systems for image recipe recognition. For this purpose, we propose a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories collected from the web. Each item in this dataset is represented by one image and the HTML information including metadata, content etc. of the seed page from which the image originated. We detail our initiative to build our dataset in sections 2 and 3 explaining the specificities and the originality of our dataset. We perform experiments at a large scale to evaluate visual and textual features along with their fusion in section 4. We propose in section 5, further statistics to highlight dataset characteristics and comparison with another recent large scale dataset (ETHZ Food-101 [5]). Finally, in section 6, we demonstrate the interest of these recognition technologies coupled with web-based dataset in a mobile search application, which can receive food image as a query and return the most relevant classes and corresponding recipes."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 14915460, "title": "Automatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation", "paper_id": "e26a0e17c400cafa702cf944c72c0bd9bfca2e91", "abstract": ". In this paper, we propose a novel e\ufb00ective framework to expand an existing image dataset automatically leveraging existing categories and crowdsourcing. Especially, in this paper, we focus on expansion on food image data set. The number of food categories is uncountable, since foods are di\ufb00erent from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for di\ufb00erent food cultures have been built independently so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic \u201cfoodness\u201d classi\ufb01er and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the e\ufb00ectiveness of the proposed method over the baselines.", "intro": "Recently, needs for food image recognition become larger, since food habit recording services for smartphones are spreading widely for everyday health care. For food habit recording, conventional ways such as inputing food names by texts or selecting food items from menus are very tedious, which sometimes prevent users from using such systems regularly. Then, several works on food recognition have been proposed so far [1][2][3][4][5] to make it easy to use food habit recording. In these works, the number of food categories is 100 at most, which is not enough for practical use. In fact, all of the foods we eat in our everyday life cannot be covered with only one hundred food categories, and the number of foods which can be recognized should be increased much more.\n\nOn the other hand, in these years, large-scale image classification is paid attention, and many methods for that have been proposed recently [6][7][8][9]. Due to these works, the number of categories to be recognized have been increased up to 1000. For example, in ImageNet Large Scale Visual Recognition Challenge (ILSVRC), the number of categories to be classified is 1000. The data set for ImageNet Challenge is a subset of ImageNet [10], which is known as the largest visual database where the number of categories are more than 20,000. Largescale image data sets such as ImageNet cannot be created by researchers by themselves. Most of them use crowd-sourcing Web services such as Amazon Mechanical Turk to build them semi-automatically.\n\nIn this paper, we propose a novel framework to expand an existing image dataset automatically leveraging existing categories. Especially, in this paper, we focus on expansion on food image data set.\n\nWhile ImageNet covers comprehensive concepts, our target is restricted to foods. In ImageNet, annotation of each concept is gathered independently. On the other hand, since foods look more similar to each other, visual knowledge on foods of a certain country is expected to help collect annotations of food photos of the other countries. Then, in this paper, we propose a novel effective framework which utilizes knowledge on food of other countries by domain adaptation.\n\nBasically, we gather food image candidates on novel food categories from the Web, and select good photos and add bounding boxes by using crowd-sourcing. In general, raw Web images include many noise images which are irrelevant to a given keyword. Especially, in this work, non-food images can be regarded as noise images. To exclude them from the gather images, we filter and re-rank Web images related to a given food category by using visual knowledge extracted from the existing food dataset.\n\nFirstly, we built a generic \"foodness\" classifier from a Japanese food data set, UEC-Food100 [4]. We cluster all the food categories in the exist food image set into several food groups the member of which are similar to each other in terms of image feature vectors, and we train SVMs regarding each food group independently. Then, we evaluate unknown images using the trained SVMs on the food groups, and regards the maximum value of the output values of all the SVM as the \"foodness\" value of the given image. We can decide if a given image of a unknown category is a food photo or not based on the \"foodness\" value. In addition, because we select the maximum value from all the output valued of food groups, we estimate the most related food group to a given photo.\n\nAfter \"foodness\" filtering, we obtain a food photo set. However, it might include food photos irrelevant to the given food keyword. Secondly, we select and re-rank more relevant images from the images judged as food photos by using transfer learning with visually similar categories in the source food photo data set. As a method of transfer learning, we use Adaptive SVM (A-SVM) [11] which can learn a discriminative hyper-plane in the target domain taking into account source-domain training data. In this work, the labeled data of the source categories which are visually similar to the target food photos are used as sourcedomain training data. As an initial target-domain training data, we use upperranked photos by a unsupervised image ranking method, VisualRank (VR) [12]. Then, we select food candidate images to be submitted for the crowd-sourcing by applying a trained A-SVM. By the experiments, the precision of the food candidate photos by A-SVM has been proved to outperformed the results by only VisualRank and by normal standard SVM.\n\nThe contributions of this paper are as follows:\n\n(1) Propose a novel framework to extend an existing image dataset with a generic \"foodness\" classifier and domain transfer learning. (2) Three-step crowd-sourcing: selecting representative sample images, excluding noise photos, and drawing bounding boxes. (3) Evaluate and compare accuracy of built food datasets and costs regarding the proposed method and two baselines. (4) Apply the proposed framework in a large scale, and build a new 256-category food dataset based on the existing 100-category food dataset automatically."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 207240186, "title": "Deep-based Ingredient Recognition for Cooking Recipe Retrieval", "paper_id": "c67b16cc7e1d5893707e9d9b61e30a868d5262e4", "abstract": "Retrieving recipes corresponding to given dish pictures facilitates the estimation of nutrition facts, which is crucial to various health relevant applications. The current approaches mostly focus on recognition of food category based on global dish appearance without explicit analysis of ingredient composition. Such approaches are incapable for retrieval of recipes with unknown food categories, a problem referred to as zero-shot retrieval. On the other hand, content-based retrieval without knowledge of food categories is also difficult to attain satisfactory performance due to large visual variations in food appearance and ingredient composition. As the number of ingredients is far less than food categories, understanding ingredients underlying dishes in principle is more scalable than recognizing every food category and thus is suitable for zero-shot retrieval. Nevertheless, ingredient recognition is a task far harder than food categorization, and this seriously challenges the feasibility of relying on them for retrieval. This paper proposes deep architectures for simultaneous learning of ingredient recognition and food categorization, by exploiting the mutual but also fuzzy relationship between them. The learnt deep features and semantic labels of ingredients are then innovatively applied for zero-shot retrieval of recipes. By experimenting on a large Chinese food dataset with images of highly complex dish appearance, this paper demonstrates the feasibility of ingredient recognition and sheds light on this zero-shot problem peculiar to cooking recipe retrieval.", "intro": "While there is a large number of cooking recipes posted on the Internet, finding a right recipe given a picture of dish remains a challenge yet to be fully explored. The major problem underlying this challenge is the recognition of food categories as well as their ingredients. Indeed, the problem is commonly shared among health-related applications. For example, food-log management [1], which records daily food intake for dietary habit monitoring, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Figure 1: Variations in visual appearance and composition of ingredients show the challenges of predicting ingredients even for dishes within the same food category. The first row shows three examples of dishes for the category \"fried green peppers\", followed by \"yuba salad\" ad \"steam egg custard\" in second and third rows respectively. often requires manual input of food intake. In addition to timeconsuming, the process is error-prone. As investigated in [11], selfreporting data obtained from unfriendly acquired process tends to underestimate the actual food intake. These concerns motivate the use of mobile devices as a convenient means in capturing pictures of food intake for automatic recognition [24] [14] [25] [3] [16].\n\nThis paper studies the recognition of ingredients for recipe retrieval in the domain of Chinese dishes. Different from food categorization, which is to identify the name of a dish (e.g., fried green pepper shown in Figure 1), ingredient recognition is to uncover the ingredients inside a dish (e.g., green pepper, black bean, chopped garlic). In the literature, associating food categories to their respective recipes is regarded as a general pipeline that facilitates the estimation of calories and nutrition facts [35] [14]. The pipeline is effective for recognizing restaurant dishes and the food categories with standardized cooking method (e.g., fast food) that often have similar visual appearance with the same ingredients. However, most dishes in Chinese food have no standardized cooking method, food presentation and ingredient composition. Direct mapping between dishes and recipes, by using the names of food categories, is not likely to attain satisfactory retrieval rate, not mentioning the imperfect performance in food recognition. The difficulty of this task is probably alleviated, nevertheless, with the presence of GPS and restaurant menus as utilized by Im2Calories [24] and Menu-Match [2]. However, restaurant information are difficult to acquire as stated in [24] and such context-aware recognition is only limited to restaurant food. Therefore, this paper argues the need of ingredient recognition beyond food categorization for general recipe retrieval.\n\nIn the domain of Chinese food, two major obstacles in recognition are diverse appearances of dishes and wild composition of ingredients. Figure 1 shows some examples of Chinese dishes. Automatic recognition is challenged by the wildly different ways of mixing and placing ingredients even for the same food category. For the food category \"steamed egg custard\" (last row of Figure  1), there is even no overlap in ingredients except egg. Retrieving recipes without explicitly naming the underneath ingredients is expected to include false positives. Basically, ingredients can be treated as attributes of food categories. As the number of food categories is generally far larger than the number of ingredients, recognizing attributes is more feasible than food categories in terms of scale. Furthermore, ingredient recognition also gives light to the retrieval of recipes for unknown food categories during model training, a problem generally referred to as zero-shot recognition or retrieval [29].\n\nGenerally speaking, ingredient recognition is more difficult than food categorization. As observed in Figure 1, the size, shape and color of ingredients can exhibit large visual differences due to diverse ways of cutting and cooking, in addition to changes in viewpoints and lighting conditions. Recognizing ingredients alone without food category in mind is likely to result in unsatisfactory performance. This paper considers simultaneous recognition of food and ingredients, aiming to exploit the mutual relationship between them for enhancing the robustness of recognition. The key ingredients of a category remain similar despite composing with different auxiliary ingredients. Knowing food category basically eases the recognition of ingredients. On the other hand, the prediction of ingredients also helps food categorization, for example, the ingredient \"fungus\" has a higher chance than \"pork\" to appear in the food \"yuba salad\". Hence, learning food categories with the composition of ingredients in mind, and vice versa, in principle shall lead to better performance. Figure 2 gives an overview of the proposed framework, which is composed of two modules: ingredient recognition and zero-shot recipe retrieval. The first module formulates the recognition of ingredients as a problem of multi-task learning using deep convolution neural network (DCNN). Given a picture of dish, the module outputs the name of dish along with a histogram of ingredients. The developed DCNN can recognize 172 Chinese food categories and 353 ingredients. To the best of our knowledge, there is no result published yet for ingredient recognition on such a large scale. The second module performs zero-shot retrieval, by matching the predicted ingredients against a large corpus containing more than 60,000 recipes. The corpus includes some food categories as well as ingredients unknown to the multi-task DCNN. To boost retrieval performance, a graph encoding the contextual relationship among ingredients is learnt from the recipe corpus. Using this graph, conditional random field (CRF) is employed to probabilistically tune the probability distribution of ingredients to reduce potential recognition error due to unseen food category.\n\nTo summary, this paper contributes by developing multi-task learning technique for ingredient recognition and demonstrates its application for zero-shot recipe retrieval. Our work differs from the existing works, which mostly focus on recognition of food categories and operate in domains such as western and Japanese food [4] [22]. To our knowledge, zero-shot recipe retrieval, which requires knowledge of ingredients, has not yet been considered in the literature. Along with this paper, we will release the collected Chinese food dataset, VIREO Food-172, which contains 172 food and 353 ingredient labels. The dataset is larger than the publicly available datasets such as Food-101 [4], UEC Food-100 [22] and PFID [6], each with around 100 western or Japanese food categories but without ingredient labels."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 211117398, "title": "Mining Discriminative Food Regions for Accurate Food Recognition", "paper_id": "1054e7f22f050d8101e96c9a9b9d8abcd15b641b", "abstract": "Automatic food recognition is the very first step towards passive dietary monitoring. In this paper, we address the problem of food recognition by mining discriminative food regions. Taking inspiration from Adversarial Erasing, a strategy that progressively discovers discriminative object regions for weakly supervised semantic segmentation, we propose a novel network architecture in which a primary network maintains the base accuracy of classifying an input image, an auxiliary network adversarially mines discriminative food regions, and a region network classifies the resulting mined regions. The global (the original input image) and the local (the mined regions) representations are then integrated for the final prediction. The proposed architecture denoted as PAR-Net is end-to-end trainable, and highlights discriminative regions in an online fashion. In addition, we introduce a new fine-grained food dataset named as Sushi-50, which consists of 50 different sushi categories. Extensive experiments have been conducted to evaluate the proposed approach. On three food datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs consistently and achieves state-of-the-art results (top-1 testing accuracy of $90.4\\%$, $90.2\\%$, $92.0\\%$, respectively) compared with other existing approaches. Dataset and code are available at https://github.com/Jianing-Qiu/PARNet", "intro": "Diet-induced diseases are becoming increasingly prevalent among populations. One underlying factor is people's poor management of their daily dietary intake. The other factor is that there is currently no accurate measurement of dietary intake. Dietary measurement in nutritional epidemiology is heavily based on self-reported data that are highly inaccurate and subjective [15], which hinders nutritionists from designing effective dietary guidance. To mitigate the problem of existing dietary measurement techniques that require extensive user input and produce unsatisfactory results, the concept of passive dietary monitoring is proposed [12], which relies on sensors such as cameras to pervasively record eating episodes and automatically perform food recognition, volume estimation, and deduce dietary intake. In realising this concept of passive monitoring, food recognition is the first and a crucial step as any misrecognition will lead to inaccurate measurements afterwards. With recent advances in computer vision, recognising pictured dishes have achieved promising results but still remains as a challenging field of research given that there are enormous varieties of dishes and even the same type of food can have very different appearances. In this work, we aim to achieve accurate food recognition by mining discriminative regions of a food image. This is motivated by the previous work done by Bossard et al. [3] that utilises random forests to mine discriminative components from food images. Unlike [3], we develop a convolutional neural network (CNN) model and utilise a weakly supervised method to discover discriminative food regions. This weakly supervised method used in both network training and inference is based on Adversarial Erasing (AE) [19], a strategy developed for weakly supervised semantic segmentation. One prominent feature of AE is that it enables the discriminative region of an object of interest to be discovered progressively, which in our case enables better recognition of food items. Our implementation of AE however differs from [19] in that we integrate it into a new network architecture for object recognition (food recognition in particular), and all sub-networks involved are trained jointly, which is more convenient, compared to its original usage in which networks need to be trained independently. Although region mining is performed, the proposed approach still predicts the food class of an input image efficiently in an end-to-end manner, which will be detailed in Sections 3.1 and 3.2.\n\nThe contributions of our work are twofold: (i) we propose a new network architecture that is able to mine discriminative food regions in a weakly supervised fashion and be trained endto-end. The mining strategy is adopted and optimised for food recognition. Comprehensive experiments are performed to validate the proposed approach; (ii) we introduce a new finegrained food dataset which consists of 50 sub-categories of one common food class, i.e., sushi, in contrast to most existing datasets that only contain coarse food classes."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 37105431, "title": "ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition", "paper_id": "477ce151f22a546c86959f5763a802142f645de9", "abstract": "In this paper, we introduce a new and challenging large-scale food image dataset called\"ChineseFoodNet\", which aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In our dataset, images of each food category of our dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. We present our efforts to build this large-scale image dataset, including food category selection, data collection, and data clean and label, in particular how to use machine learning methods to reduce manual labeling work that is an expensive process. We share a detailed benchmark of several state-of-the-art deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose a novel two-step data fusion approach referred as\"TastyNet\", which combines prediction results from different CNNs with voting method. Our proposed approach achieves top-1 accuracies of 81.43% on the validation set and 81.55% on the test set, respectively. The latest dataset is public available for research and can be achieved at https://sites.google.com/view/chinesefoodnet.", "intro": "F OOD plays an essential role in everyone's lives, and the behaviour of diet and eating impacts everyone's health [1]. Underestimating food intake directly relates to diverse psychological implications [2]. In recent years, photographing foods and sharing them on social networks have become a part of daily life. Consequently, several applications have been developed to record daily meal activities in personal food log system [3] [4] [5], which are employed to computeraided dietary assessment [6], further usage preference experiments [7] [8], calorie measurement [9] and nutrition balance estimation [10] [11]. As one of user-friendly ways to input of the food log, automatic recognition of dish pictures gives rise of a research field of interest.\n\nDeep convolutional neural networks (CNNs) have achieved state-of-the-art in a variety of computer vision tasks [12] [13]. The visual dish recognition task is the same situation [14]. The quality of training datasets always plays an important role for \u2020 These authors contributed equally to this work.\n\n* means corresponding author. Xin Chen, Hua Zhou, Yu Zhu   training a deep neural network, where the high performance of the deep model is still data-driven to some extent [15] [16].\n\nHowever, to the best of our knowledge, there still exist no effective Chinese food recognition system matured enough to be used in real-world. The major reason is absence of largescale and high quality image datasets. In [17], the Chinese food dataset includes 50 categories, each of which has only 100 images. Obviously, the size of this dataset is not sufficient to satisfy deep learning training requirements.\n\nThe visual dish recognition problem has widely been considered as one of challenging computer vision and pattern recognition tasks [14] [18]. Compared to other types of food such as Italian food and Japanese food, it is more difficult to recognize the images of Chinese dish as the following reasons:\n\n1) The images of same category appear differently. ornament, etc. In order to give impetus to the progress of visual food classification and related computer vision tasks, we build a large-scale image dataset of Chinese dish, named by Chine-seFoodNet. This dataset contains 185,628 images of 208 food categories covering most of popular Chinese food, and these images include web images and photos taken in real world under unconstrained conditions. To the best of our knowledge, ChineseFoodNet is the largest and most comprehensive dataset for visual Chinese food recognition. Some of images of ChineseFoodNet are shown in Figure. 2.\n\nWe benchmark nine CNNs models of four state-of-the-art deep CNNs, SqueezeNet [19], VGG [20], ResNet [21], and DenseNet [22], on our dataset. Experimental results reveal that ChineseFoodNet is capable of learning complex models.\n\nIn this paper, we also propose a novel two-step data fusion approach with voting. Although simple, voting is an effective way to fuse results [23] [24]. Guided by our benchmarks, we try some combination of different CNNs models Based on results on ChineseFoodNet, we take ResNet152, DenseNet121, DeneseNet169, DenseNet201 and VGG19-batch normalization (BN) [25] as our predictive models. 1 Then we fusing these results with voting as a final result. This method is designated as\" TastyNet\". Our proposed method has achieved top-1 accuracy 81.43% in validation set and 81.55% in test set, respectively. Compared to best results of the approaches with a single network structure, the improvements of 2.38% in validation set and 2.33% in these sets have been achieved, respectively. This paper takes three major contributions as following: 1) We present a large-scale image dataset, ChineseFoodNet, for Chinese food recognition tasks. ChineseFoodNet is made up with 185,628 images of 208 categories, and most of the food image are from users' daily life. It is public available for research in related topics. 2 2) We provide a benchmark on our dataset. Totally nine different models of four state-of-the-art CNNs architectures are evaluated. We presents the details of the methodology used in the evaluation and the pre-trained models will be public available for further research. 3) We propose a novel two-step data fusion approach for visual food recognition, which combines predictive results of different CNNs with voting. Experimental results on ChineseFoodNet have shown that approach improves the performance compared to one deep CNNs model. It has shown that data fusion should be an alternative way to improve accuracy instead of only increasing numbers of layers in CNNs. The paper is organized as follows. Section II briefly reviews some public food datasets and the state-of-the-art visual food recognition methods. Section III describes the procedure of building and tagging the ChineseFoodNet dataset. In section IV, several state-of-the-art CNNs methods are benchmarked on ChineseFoodNet. Section V details our proposed data fusion approach and present our results on Chinese-FoofNet. This paper closes with a conclusion of our work and some future directions in section VI."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 3914807, "title": "You Are What You Eat: Exploring Rich Recipe Information for Cross-Region Food Analysis", "paper_id": "edbc233b4a8e9163c1b94dc3e0badb2c94ed700e", "abstract": "Cuisine is a style of cooking and usually associated with a specific geographic region. Recipes from different cuisines shared on the web are an indicator of culinary cultures in different countries. Therefore, analysis of these recipes can lead to deep understanding of food from the cultural perspective. In this paper, we perform the first cross-region recipe analysis by jointly using the recipe ingredients, food images, and attributes such as the cuisine and course (e.g., main dish and dessert). For that solution, we propose a culinary culture analysis framework to discover the topics of ingredient bases and visualize them to enable various applications. We first propose a probabilistic topic model to discover cuisine-course specific topics. The manifold ranking method is then utilized to incorporate deep visual features to retrieve food images for topic visualization. At last, we applied the topic modeling and visualization method for three applications: 1) multimodal cuisine summarization with both recipe ingredients and images, 2) cuisine-course pattern analysis including topic-specific cuisine distribution and cuisine-specific course distribution of topics, and 3) cuisine recommendation for both cuisine-oriented and ingredient-oriented queries. Through these three applications, we can analyze the culinary cultures at both macro and micro levels. We conduct the experiment on a recipe database Yummly-66K with 66,615 recipes from 10 cuisines in Yummly. Qualitative and quantitative evaluation results have validated the effectiveness of topic modeling and visualization, and demonstrated the advantage of the framework in utilizing rich recipe information to analyze and interpret the culinary cultures from different regions.", "intro": "U NDERSTANDING the cultural diversity has become imperative in almost every aspect of life. Cuisine has always been a significant aspect of cultures. The recipes from different cuisines are strong signals of the culinary habits of individuals from various parts of the world. For example, East Asian cuisines are dominated by some ingredients such as \"soy sauce\" and \"sesame oil\" from the recipes [2]. Some other ingredients uniquely represent a certain cuisine (e.g., mozzarella to the Italian cuisine) [51]. Therefore, the analysis of recipes can facilitate deep understanding of food from not only the health and marketing perspectives, but also the cultural one. Such analysis can further enable various applications, such as food preference learning [27], [62], cuisine classification [56] and health diet analysis [39]. Since the culinary habits have such importance for the culture, we address the topic of investigating and analyzing the culinary cultures of different countries through the recipes.\n\nExisting work mainly focuses on the analysis of recipes based on the textual descriptions (e.g., ingredients) [51] and checkin information from social websites [53]. For example, Sajadmanesh et al. [51] used the ingredients and attribute information, such as the taste and cuisine information to understand cuisines and culinary habits around the world. Silva et al. [53] analyzed the check-ins from Foursquare to identify cultural boundaries and similarities across populations at different scales. However, little work has investigated how to jointly utilize rich modality and attribute information to enable the analysis and comparison of culinary cultures comprehensively.\n\nWhile existing studies rely on text-oriented recipe analysis, we believe rich modality and attribute information are critical to comprehensively analyze the diversity of the culinary cultures. (1) Relying exclusively on text-based descriptions can impose high cognitive load in analyzing the culinary habits. Some works such as [14], [37], [62] have found the importance of visual information in eating habits-related applications. For example, Zepeda et al. [37] pointed out that photographic food diaries were more effective than written ones at prompting patients to truly understand their eating habits. Yang et al. [62] found that the analysis of image features can provide a valuable signal for food preference learning. (2) Different attributes (e.g., the course and cuisine information) reflect respective aspects of the recipes and jointly contribute to comprehensive recipe analysis. Through attribute-based analysis, we can reveal the culinary cultures from different perspectives. However, it is non-trivial to make the correlation between the content and various attributes. Because this requires us to design a more flexible model, which is able to incorporate arbitrary number of observed attribute information, yet inference remains relatively simple.\n\nTaking all the above-mentioned factors into consideration, we propose a culinary culture analysis framework (Fig. 1), which is capable of discovering topics conditioned on different attributes and visualizing them to enable various applications. In particular, we take Yummly, 1 one of the most popular recipesharing websites in our study. As shown in the left part of Fig. 1, each recipe includes the ingredients, food images and various attributes (e.g., the cuisine and course). Given the input of rich recipe information, we firstly propose a Bayesian Cuisine-Course Topic Model (BC 2 TM), which incorporates different attributes to discover the cuisine-course specific topics. Since topic models have been proved successfully in discovering meaningful and interpretable topics or patterns in the text domain, such discovered topics are very suitable for our cross-region food analysis and comparison. For further analysis and better visualization, based on the learned model parameters, namely topic-ingredient distribution, we then utilize the manifold ranking method to integrate both deep visual features and topic-ingredients to retrieve relevant food images for topic visualization. Finally, we exploit the topic modeling and visualization for three applications: (1) Multi-modal cuisine summarization, which summarizes cuisines with both recipe ingredients and food images. Through cuisine summarization, we can 1 http://www.yummly.com/ analyze and interpret the culinary cultures from the macro level.\n\n(2) Cuisine-course pattern analysis including topic-specific cuisine distribution and cuisine-specific course distribution of topics. Such comparative analysis can provide us with more details from the micro level. (3) Cuisine recommendation. It supports flexible queries including both cuisine-oriented and ingredientoriented queries. We conduct the experiment on one dataset, including about 66K recipes with the ingredients, food images, course and cuisine attributes from 10 cuisines from Yummly. Experiment results demonstrate the advantage of the framework in utilizing rich recipe information to discover and compare different eating habits from different regions.\n\nThe contributions of the proposed approach can be summarized as follows:\n\n1) To our knowledge, this is the first time to jointly utilize rich recipe information including multi-modal information and various attributes into a unified framework to enable comprehensive analysis and comparison of culinary cultures. 2) We propose a culinary cultures analysis framework, which is capable of discovering topics conditioned on different attributes and visualizing them for recipe-oriented applications. 3) We present a wide variety of applications, including 1) multi-modal cuisine summarization, 2) cuisine-course pattern analysis, and 3) cuisine recommendation. 4) We conduct the comprehensive evaluation on a real-world recipe dataset Yummly-66K and the experimental results have validated the effectiveness of our proposed method and framework. The rest of the paper is organized as follows. Section II reviews the related work. Section III presents the core components of the proposed culinary culture analysis, including the proposed Bayesian Cuisine-Course Topic Model (BC 2 TM) and manifold ranking based topic visualization. Section IV introduces three derived applications, including multi-modal cuisine summarization, cuisine-course pattern analysis and cuisine recommendation. Experimental results are reported in Section V. Finally, we conclude the paper and give the future work in Section VI."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 221112548, "title": "ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network", "paper_id": "d567e8c06d6688ead92186c88ab8d58d61f681cc", "abstract": "Food recognition has received more and more attention in the multimedia community for its various real-world applications, such as diet management and self-service restaurants. A large-scale ontology of food images is urgently needed for developing advanced large-scale food recognition algorithms, as well as for providing the benchmark dataset for such algorithms. To encourage further progress in food recognition, we introduce the dataset ISIA Food- 500 with 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume. Furthermore, we propose a stacked global-local attention network, which consists of two sub-networks for food recognition. One subnetwork first utilizes hybrid spatial-channel attention to extract more discriminative features, and then aggregates these multi-scale discriminative features from multiple layers into global-level representation (e.g., texture and shape information about food). The other one generates attentional regions (e.g., ingredient relevant regions) from different regions via cascaded spatial transformers, and further aggregates these multi-scale regional features from different layers into local-level representation. These two types of features are finally fused as comprehensive representation for food recognition. Extensive experiments on ISIA Food-500 and other two popular benchmark datasets demonstrate the effectiveness of our proposed method, and thus can be considered as one strong baseline. The dataset, code and models can be found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.", "intro": "Food computing [38] is emerging as a new field to ameliorate the issues from many food-relevant fields, such as nutrition, agriculture and medicine. As one significant task in food computing, food recognition has received more attention in multimedia and beyond [15,25,36,41] for its various applications, such as visual food diary [36], health-aware recommendation [42] and self-service restaurants [2].\n\nDespite its great potential applications, recognizing food from images is still a challenging task, and the challenge derives from three-fold:\n\n\u2022 There is a lack of large-scale food dataset for food recognition. Existing works mainly focus on utilizing smaller datasets for food recognition, such as ETH Food-101 [6] and Vireo Food-172 [7]. For example, Bossard et al. [6] released one food dataset ETH Food-101 from western cuisines with 101 food categories and 101,000 images. Chen et al. [7] introduced the Vireo Food-172 dataset from 172 Chinese food categories. These data-sets is lack of diversity and coverage in food categories and do not include a wide range of food images. Therefore, they are probably not sufficient to construct more complicated deep learning models for food recognition.\n\n\u2022 There are larger intra-class variations in the global appearance, shape and other configurations for food images. As shown in Fig. 1, there are different shapes for the butter pecan and different textures appear in the mie goreng dish. Although numerous methods have been developed for addressing the problem of food recognition, most of these methods mainly focus on extracting features with certain type or some types while ignoring other aspects. For example, works on [4] mainly extracted color features while Niki et al. [32] designed a network to capture certain vertical structure for food recognition.\n\n\u2022 There are subtle discriminative details from food images, which are harder to capture in many cases. Food recognition belongs to fine-grained recognition. Therefore, discriminative details are too subtle to be well-represented by existing CNNs in many cases. As shown in Fig. 1, global features are not discriminative enough to distinguish between corn stew and leek soup. Although local regional features are probably more useful, we should carefully design one network to capture and represent such subtle difference. In order to improve the recognition performance, additional context information, such as location and ingredients [4,41,51,59] is utilized. However, when these information is unavailable, these methods probably do not work. In this work, we address data limitations by introducing a new large-scale dataset ISIA Food-500 with 399,726 images and 500 categories. In contrast with existing popular benchmark datasets, it is a more comprehensive food dataset with larger category coverage, larger data volume and higher diversity. To solve another two challenges, we propose a Stacked Global-Local Attention Network (SGLANet) to jointly learn complementary global and local visual features for food recognition. This is achieved by two sub-networks, namely Global Feature Learning Subnetwork (GloFLS) and Local-Feature Learning Subnetwork (LocFLS). GloFLS first utilizes hybrid spatial-channel attention to obtain more discriminative features for each layer, and then aggregates these features from different layers with both coarse and fine-grained levels, such as shape and texture cues about food into global-level features. LocFLS adopts cascaded Spatial Transformers (STs) to localize different attentional regions (e.g., ingredient-relevant regions), and aggregates fused regional features from different layers into local-level representation. In addition, SGLANet is trained with different types of losses in an end-to-end fashion to maximize their complementary effect in terms of discriminative power.\n\nThe contributions of our paper can be summarized as follows: \u2022 We introduce a new large-scale and highly diverse food image dataset with 500 categories and about 400,000 images, which will be made publicly available to further the development of scalable food recognition.\n\n\u2022 We propose a stacked global-local attention network architecture to jointly learn food-oriented global and local features Dataset #Images #Categories #Coverage PFID [9] 4,545 101 Japanese UEC Food100 [34] 14,361 100 Japanese UEC Food256 [27] 25,088 256 Japanese ETHZ Food-101 [6] 101,000 101 Western UPMC Food-101 [48] 90,840 101 Western UNIMIB2015 [12] 2,000 15 Misc. UNIMIB2016 [13] 1,027 73 Misc. ChineseFoodNet [10] 192,000 208 Chinese Vireo Food-172 [7] 110,241 172 Chinese KenyanFood13 [23] 8,174 13 Kenyan Sushi-50 [44] 3,963 50 Japanese FoodX-251 [26] 158 via combining hybrid spatial-channel attention and multi-scale strategy for food recognition.\n\n\u2022 We conduct extensive evaluation on our proposed dataset and other two popular food benchmark datasets to verify the effectiveness of our approach. As one strong baseline, code and models will also be released upon publication to support future research."}
{"tabids": ["e0bdb237-b51f-4c8a-af12-aa5107284905"], "corpus_id": 260681484, "title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models", "paper_id": "36d23a17309e5e31a5b966c0158386ebe6ce719c", "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.", "intro": "Despite the remarkable success of vision-language models (VLMs) [18,33,34,38,45,47,52] in substantial uni-modal and multi-modal downstream tasks, they are still poorly understood as yet. The prevalent approach for evaluating VLMs is comparing their performance on downstream tasks after fine-tuning. However, evaluation solely based on the fine-tuning results renders poor interpretability [59], which hinders the further development of VLMs. Consequently, researchers have proposed a range of probing methods and benchmarks [9,13,29,30,39,48] in recent years to assess the capabilities of VLMs from various perspectives, providing a more comprehensive understanding of these models. However, these methodologies are still limited in the general domain. They typically construct evaluation benchmarks by employing images from widely used general-domain datasets and subsequently assigning hand-crafted textual annotations to these images. If VLMs perform well in a specific domain, we can directly employ the models in that domain without any modifications. However, the above situation is Figure 1: An example from our Food-500 Cap. The image is equipped with the label, geographic origin, and a detailed description. This description is annotated with a class label (red) and hand-curated various fine-grained visible content of the image such as ingredients (blue), food colors (green), and the food container (orange).\n\nunclear due to only few works studying the generalization of using VLMs directly in specific domains without fine-tuning.\n\nMotivated by this, we focus on evaluating the generalization capacity of VLMs in a specific domain, namely, the food domain. Since food computing [25] has been gaining widespread attention as it has the potential to support numerous food-related applications, such as healthy diets and food choices. To comprehensively evaluate the VLMs' performance on food-related tasks, we introduce a new benchmark named Food-500 Cap, which comprises 24,700 food images with 494 categories, each accompanied by a detailed caption. The Food-500 Cap dataset is created by selecting images from ISIA Food-500 [26] that covers a wide range of food categories. We select 50 images from each category and engage an annotation company to annotate fine-grained descriptions for all 24,700 images. Each description includes the original food category label and fine-grained attributes of the food, such as the color, shape, and ingredients. Such an in-house labeling process guarantees the high quality of our dataset. Besides, as food is always associated with a specific geographic region, we also provide a taxonomy that classifies food categories based on their original place, enabling a more comprehensive investigation of VLMs' performance across culinary cultures. We provide a sample of Food-500 Cap in Figure 1, which contains a Japanese food image labeled agedashi tofu from and a description with some related attributes. In contrast to the prevalent food datasets [3,23,54], Food-500 Cap are equipped with high-quality image captions containing richer visual information and geographic origin tags, which is more suitable for exploring the performance of VLM in the food domain.\n\nTo comprehensively evaluate VLMs' capacity in the food domain, we seriously pick up nine representative models from three popular architectures, including vision-language representation models (e.g. CLIP [33]), image-to-text generative models (e.g. OFA [52]), and text-to-image generative models (e.g. Stable Diffusion [38]). We probe these VLMs with various food-related tasks in a zero-shot setting. For vision-language representation models, we employ food classification and image-text retrieval to assess VLMs' multi-modal alignment capabilities. As for image-to-text generative models and text-to-image generative models, we utilize image captioning and image synthesis respectively to test their multi-modal generation capabilities. Both qualitative and quantitative analyses are performed on the experimental results, revealing that these models exhibit poor performance in the food domain, in contrast to their performance in the general domain. Moreover, we find that all the models display a significant bias in culinary culture, with their performance in Asian cuisine falling markedly behind that in European, North American, and Latin American cuisine. In summary, this paper makes the following contributions:\n\n\u2022 We equip a subset of the ISIA Food-500 dataset with (1)"}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 23714201, "title": "Improved Regularization of Convolutional Neural Networks with Cutout", "paper_id": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "abstract": "Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results with almost no additional computational cost. We also show improved performance in the low-data regime on the STL-10 dataset.", "intro": "In recent years deep learning has contributed to considerable advances in the field of computer vision, resulting in state-of-the-art performance in many challenging vision tasks such as object recognition [8], semantic segmentation [11], image captioning [19], and human pose estimation [17]. Much of these improvements can be attributed to the use of convolutional neural networks (CNNs) [9], which are capable of learning complex hierarchical feature representations of images. As the complexity of the task to be solved increases, the resource utilization of such models increases as well: memory footprint, parameters, operations count, inference time and power consumption [2]. Modern networks commonly contain on the order of tens to hundreds of millions of learned parameters which provide the necessary representational power for such tasks, but with the increased representational power also comes increased probability of overfitting, leading to poor generalization.\n\nIn order to combat the potential for overfitting, several different regularization techniques can be applied, such as data augmentation or the judicious addition of noise to activations, parameters, or data. In the domain of computer vision, data augmentation is almost ubiquitous due to its ease of implementation and effectiveness. Simple image transforms such as mirroring or cropping can be applied to create new training data which can be used to improve model robustness and increase accuracy [9]. Large models can also be regularized by adding noise during the training process, whether it be added to the input, weights, or gradients. One of the most common uses of noise for improving model accuracy is dropout [6], which stochastically drops neuron activations during training and as a result discourages the co-adaptation of feature detectors.\n\nIn this work we consider applying noise in a similar fashion to dropout, but with two important distinctions. The first difference is that units are dropped out only at the input layer of a CNN, rather than in the intermediate feature layers. The second difference is that we drop out contiguous sections of inputs rather than individual pixels, as demon-strated in Figure 1. In this fashion, dropped out regions are propagated through all subsequent feature maps, producing a final representation of the image which contains no trace of the removed input, other than what can be recovered by its context. This technique encourages the network to better utilize the full context of the image, rather than relying on the presence of a small set of specific visual features. This method, which we call cutout, can be interpreted as applying a spatial prior to dropout in input space, much in the same way that convolutional neural networks leverage information about spatial structure in order to improve performance over that of feed-forward networks.\n\nIn the remainder of this paper, we introduce cutout and demonstrate that masking out contiguous sections of the input to convolutional neural networks can improve model robustness and ultimately yield better model performance. We show that this simple method works in conjunction with other current state-of-the-art techniques such as residual networks and batch normalization, and can also be combined with most regularization techniques, including standard dropout and data augmentation. Additionally, cutout can be applied during data loading in parallel with the main training task, making it effectively computationally free. To evaluate this technique we conduct tests on several popular image recognition datasets, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN. We also achieve competitive results on STL-10, demonstrating the usefulness of cutout for low data and higher resolution problems."}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 3162051, "title": "mixup: Beyond Empirical Risk Minimization", "paper_id": "4feef0fd284feb1233399b400eb897f59ec92755", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.", "intro": "Large deep neural networks have enabled breakthroughs in fields such as computer vision (Krizhevsky et al., 2012), speech recognition , and reinforcement learning (Silver et al., 2016). In most successful applications, these neural networks share two commonalities. First, they are trained as to minimize their average error over the training data, a learning rule also known as the Empirical Risk Minimization (ERM) principle (Vapnik, 1998). Second, the size of these state-of-theart neural networks scales linearly with the number of training examples. For instance, the network of Springenberg et al. (2015) used 10 6 parameters to model the 5 \u00b7 10 4 images in the CIFAR-10 dataset, the network of (Simonyan & Zisserman, 2015) used 10 8 parameters to model the 10 6 images in the ImageNet-2012 dataset, and the network of Chelba et al. (2013) used 2 \u00b7 10 10 parameters to model the 10 9 words in the One Billion Word dataset.\n\nStrikingly, a classical result in learning theory (Vapnik & Chervonenkis, 1971) tells us that the convergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural network) does not increase with the number of training data. Here, the size of a learning machine is measured in terms of its number of parameters or, relatedly, its VC-complexity (Harvey et al., 2017).\n\nThis contradiction challenges the suitability of ERM to train our current neural network models, as highlighted in recent research. On the one hand, ERM allows large neural networks to memorize (instead of generalize from) the training data even in the presence of strong regularization, or in classification problems where the labels are assigned at random . On the other hand, neural networks trained with ERM change their predictions drastically when evaluated on examples just outside the training distribution (Szegedy et al., 2014), also known as adversarial examples. This evidence suggests that ERM is unable to explain or provide generalization on testing distributions that differ only slightly from the training data. However, what is the alternative to ERM?\n\nThe method of choice to train on similar but different examples to the training data is known as data augmentation (Simard et al., 1998), formalized by the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000). In VRM, human knowledge is required to describe a vicinity or neighborhood around each example in the training data. Then, additional virtual examples can be drawn from the vicinity distribution of the training examples to enlarge the support of the training distribution. For instance, when performing image classification, it is common to define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings. While data augmentation consistently leads to improved generalization (Simard et al., 1998), the procedure is dataset-dependent, and thus requires the use of expert knowledge. Furthermore, data augmentation assumes that the examples in the vicinity share the same class, and does not model the vicinity relation across examples of different classes.\n\nContribution Motivated by these issues, we introduce a simple and data-agnostic data augmentation routine, termed mixup (Section 2). In a nutshell, mixup constructs virtual training examples\nx = \u03bbx i + (1 \u2212 \u03bb)x j , y = \u03bby i + (1 \u2212 \u03bb)y j ,\nwhere (x i , y i ) and (x j , y j ) are two examples drawn at random from our training data, and \u03bb \u2208 [0, 1]. Therefore, mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets. mixup can be implemented in a few lines of code, and introduces minimal computation overhead.\n\nDespite its simplicity, mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR-100, and ImageNet-2012 image classification datasets (Sections 3.1 and 3.2). Furthermore, mixup increases the robustness of neural networks when learning from corrupt labels (Section 3.4), or facing adversarial examples (Section 3.5). Finally, mixup improves generalization on speech (Sections 3.3) and tabular (Section 3.6) data, and can be used to stabilize the training of GANs (Section 3.7). The source-code necessary to replicate our experiments is available at:\n\nhttps://coming.soon/mixup.\n\nWe conclude by exploring the connections to prior work (Section 4), as well as offering some points for discussion (Section 5)."}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 152282661, "title": "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features", "paper_id": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "abstract": "Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (\\eg leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. {Such removal is not desirable because it leads to information loss and inefficiency during training.} We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and \\mbox{retaining} the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.", "intro": "Deep convolutional neural networks (CNNs) have shown promising performances on various computer vision problems such as image classification [30,19,11], object de- * Correspondence to sangdoo.yun@navercorp.com  tection [29,23], semantic segmentation [1,24], and video analysis [27,31]. To further improve the training efficiency and performance, a number of training strategies have been proposed, including data augmentation [19] and regularization techniques [33,16,37]. In particular, to prevent a CNN from focusing too much on a small set of intermediate activations or on a small region on input images, random feature removal regularizations have been proposed. Examples include dropout [33] for randomly dropping hidden activations and regional dropout [2,49,32,7] for erasing random regions on the input. Researchers have shown that the feature removal strategies improve generalization and localization by letting a model attend not only to the most discriminative parts of objects, but rather to the entire object region [32,7]. While regional dropout strategies have shown improvements of classification and localization performances to a certain degree, deleted regions are usually zeroed-out [2,32] or filled with random noise [49], greatly reducing the proportion of informative pixels on training images. We recognize this as a severe conceptual limitation as CNNs are generally data hungry [26]. How can we maximally utilize the deleted regions, while taking advantage of better generalization and localization using regional dropout?\n\nWe address the above question by introducing an augmentation strategy CutMix. Instead of simply removing pixels, we replace the removed regions with a patch from another image (See Table 1). The ground truth labels are also mixed proportionally to the number of pixels of combined images. CutMix now enjoys the property that there is no uninformative pixel during training, making training efficient, while retaining the advantages of regional dropout to attend to non-discriminative parts of objects. The added patches further enhance localization ability by requiring the model to identify the object from a partial view. The training and inference budgets remain the same.\n\nCutMix shares similarity with Mixup [46] which mixes two samples by interpolating both the images and labels. Mixup has been found to improve classification, but the interpolated sample tends to be unnatural (See the mixed image in Table 1). On the other hand, CutMix overcomes the problem by replacing the image region with a image patch from another training image. Table 1 gives an overview of Mixup [46], Cutout [2], and CutMix on image classification, weakly supervised localization, and transfer learning to object detection methods. Although Mixup and Cutout enhance the ImageNet classification accuracies, they suffer from performance degradation on ImageNet localization and object detection tasks. On the other hand, CutMix consistently achieves significant enhancements across three tasks, proving its superior classification and localization ability beyond the baseline and other augmentation methods.\n\nWe present extensive evaluations of our CutMix on various CNN architectures and on various datasets and tasks. Summarizing the key results, CutMix has significantly improved the accuracy of a baseline classifier on CIFAR-100 and has obtained the state-of-the-art top-1 error 14.23%. On ImageNet [30], applying CutMix to ResNet-50 and ResNet-101 [11] has improved the classification accuracy by +2.08% and +1.70%, respectively. On the localization front, CutMix improves the performance of the weaklysupervised object localization (WSOL) task on CUB200-2011 [42] and ImageNet [30] by +5.4% and +0.9% gains, respectively. The superior localization capability is further evidenced by fine-tuning a detector and an image caption generator on CutMix-ImageNet-pretrained models; the Cut-Mix pretraining has improved the overall detection perfor-mances on Pascal VOC [5] by +1% mAP and image captioning performance on MS-COCO [22] by 2 BLEU score. CutMix also enhances the model robustness and dramatically alleviates the over-confident issue [12,21] of deep networks."}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe", "9a34d702-e100-4c93-b280-35b662fc1cbe", "9a34d702-e100-4c93-b280-35b662fc1cbe", "9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 256416324, "title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression", "paper_id": "9e3125c041e96be417ae53c7f9d02508234e4751", "abstract": "Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to $81.73\\%$, surpassing the previous best preprocessing-based countermeasures by $37.97\\%$ absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturbation. We further test stronger, adaptive poisoning, and show it falls short of being an ideal defense against ISS. Overall, our results demonstrate the importance of considering various (simple) countermeasures to ensure the meaningfulness of analysis carried out during the development of PAP methods.", "intro": "The ever-growing amount of data that is easily available online has driven the tremendous advances of deep neural networks (DNNs) (Schmidhuber, 2015;LeCun et al., 2015;He et al., 2016;Brown et al., 2020). However, online data may be proprietary or contain private information, raising concerns about unauthorized use. Perturbative availability poisons (PAPs) are recognized as a promising approach to data protection and recently a large number of PAP methods have been proposed that add perturbations to images which block training by acting as shortcuts Huang et al., 2021;Fowl et al., 2021b;a). As illustrated by Figure 1 (a)\u2192(b), the high test accuracy of a DNN model is substantially reduced by PAPs.\n\nExisting research has shown that PAPs can be compromised to a limited extent by preprocessing-based-countermeasures, such as data augmentations (Huang et al., 2021;Fowl et al., 2021b) and pre-filtering Chen et al., 2023). However, a widely adopted belief is that no approaches exist that are capable of effectively countering PAPs. Adversarial training (AT) has been proven to be a strong countermeasure Wen et al., 2023). However, it is not considered to be a practical one, since it requires a large amount of computation and also gives rise to a non-negligible trade-off in test accuracy of the clean (non-poisoned) model (Madry et al., 2018;Zhang et al., 2019). Further, AT trained with a specific L p norm is hard to generalize to other norms (Tramer & Boneh, 2019;Laidlaw et al., 2021).\n\nIn this paper, we challenge the belief that it is impossible to counter PAP methods both easily and effectively by demonstrating that they are vulnerable to simple compression. First, we categorize 12 PAP methods into three categories with respect to the surrogate models they use during poison generation: slightly-trained Huang et al., 2021;Yuan & Wu, 2021;van Vlijmen et al., 2022), fully-trained Fowl et al., 2021b;Chen et al., 2023), and surrogatefree Yu et al., 2022;. Then, we analyze perturbations/shortcuts that are learned with these methods and demonstrate that they are strongly dependent on features that are learned in different training stages of the model. Specifically, we find that the methods using a slightly-trained surrogate model prefer lowfrequency shortcuts, while those using a fully-trained model prefer high-frequency shortcuts.\n\nBuilding on this new understanding, we propose Image Shortcut Squeezing (ISS), a simple, compression-based approach to countering PAPs. As illustrated by Figure 1 (b)\u2192(c), the low test accuracy of the poisoned DNN model is restored by our ISS to be close to the original accuracy. In particular, grayscale compression is used to eliminate low-frequency shortcuts, and JPEG compression is used to eliminate high-frequency shortcuts. We also show that our understanding of high vs. low frequency can also help eliminate surrogate-free PAPs Yu et al., 2022;. Our ISS substantially outperforms previously studied data augmentation and prefiltering countermeasures. ISS also achieves comparable results to adversarial training and has three main advantages: 1) generalizability to multiple L p norms, 2) efficiency, and 3) low trade-off in clean model accuracy (see Section 4.2 for details).\n\nWe further test the performance of ISS against potentially stronger PAP methods that are aware of ISS and can be adapted to it. We show that they are not ideal against our ISS. Overall, we hope our study can inspire more meaningful analyses of PAP methods and encourage future research to evaluate various (simple) countermeasures when developing new PAP methods.\n\nIn sum, we make the following main contributions:\n\n\u2022 We identify the strong dependency of the perturbation frequency patterns on the nature of the surrogate model. Based on this new insight, we show that 12 existing perturbative availability poison (PAP) methods are indeed very vulnerable to simple image compression.\n\n\u2022 We propose Image Shortcut Squeezing (ISS), a simple yet effective approach to countering PAPs. ISS applies image compression operations, such as JPEG and grayscale, to poisoned images for restoring the model accuracy.\n\n\u2022 We demonstrate that ISS outperforms existing data augmentation and pre-filtering countermeasures by a large margin and is comparable to adversarial training but is more generalizable to multiple L p norms and more efficient.\n\n\u2022 We explore stronger, adaptive poisons against our ISS and provide interesting insights into understanding PAPs, e.g., about the model learning preference of different perturbations."}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 257532941, "title": "The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models", "paper_id": "253b8903c6dc437918edffd93e56f6b10d15a63e", "abstract": "Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data\"unexploitable.\"In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over.", "intro": "Neural networks have achieved great success in various areas of computer vision including object detection [22,12], semantic segmentation [72,34], and photo-realistic image/video generation [29,9,48]. While the efforts of the community in the development of such models cannot be undermined, this unparalleled success would have been impossible without the abundance of data resources available today [7,31,44,33]. In this regard, social media, and the internet in general, provides a platform that can be crawled easily to create massive datasets. This capability can act both as a blessing and a curse: while the collected data can facilitate learning larger, more accurate neural networks, the users might lose control over protecting their personal data from being exploited. This issue has raised increasing concerns about misuse of personal data [25,24,4].\n\nRecently, there has been an increasing number of studies on hindering the unauthorized use of personal data for neural network image classifiers [13,28,66,15,16,65,56,45]. These methods tend to add an imperceptible amount of noise to the clean images so that while the data has the same appearance as the ground-truth, it cannot provide any meaningful patterns for the neural networks to learn. As a result, such approaches, collectively known as availability attacks [3], claim that personal image data can be made unexploitable for the neural networks [28,65]. While there has been an abundance of research for designing better availability attacks, far too little attention has been paid to counter-attacks that might be employed by adversaries to break such precautionary measures.\n\nUnfortunately, the assumptions of existing availability attacks are far too weak to make the data unexploitable. Going back to our real-world example of users sharing their photos over their social media, one can clearly see that once the the data has been released, it can still be exploited for training a neural network. To clarify the real-world application of this regime, consider a corporate entity that aims to train face recognition models by crawling over social media without the consent of the users. While this unauthorized entity might not have unprotected versions of a particular person's image from his/her social media, they can have a large pre-trained model representing a facial image distribution. Given this threat model, shown in Figure 1, we aim to show that achieving this goal is indeed plausible.\n\nTo this end, we show that pre-trained density estimators are powerful tools that can be used to defuse the effects of the data-protecting perturbations, eventually enabling us to exploit protected data. We utilize the power of diffusion models in representing the image data distributions to show that reverse-engineering unexploitable data is easier than what is thought. In particular, given a training  Figure 1: The threat model considered in this paper. Availability attacks cannot guarantee to protect all the data that exists over the web. A data exploiter might use large density estimators to defuse the data-protecting perturbations and still be able to exploit the data. dataset, we first diffuse the images by adding a controlled amount of Gaussian noise following the forward process of a pre-trained diffusion model. Then, we denoise the noisy images using the reverse process of the aforementioned model, resulting in a dataset purified from data-protecting perturbations. Theoretically, using contraction properties of stochastic difference equations we prove that the number of diffusion steps required to cancel the data-protecting perturbations is directly influenced by its norm. We also empirically show that our approach is surprisingly powerful, being able to deliver the state-of-the-art (SOTA) performance against a wide variety of recent availability attacks. Our results demonstrate that in the era of diffusion models, protected personal data might only be an illusion. Thus, our findings indicate the fragility of unexploitable data, calling for more research to protect personal data.\n\nOur contributions can be summarized as follows:\n\n\u2022 We introduce AVATAR as a countermeasure against data availability attacks. To the best of our knowledge, this is the first work that explores the use of diffusion models to circumvent such attacks.\n\n\u2022 We show the power of AVATAR in breaking availability attacks over five datasets, four architectures, and seven of the most recent availability attacks. AVATAR achieves the SOTA performance against availability attacks, outperforming adversarial training.\n\n\u2022 Theoretically, we show that the amount of noise needed to diffuse the data-protecting perturbation is directly related to its norm. This result indicates that achieving both goals of availability attacks (data utility and protection) at the same time might be impossible."}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe", "9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 257766788, "title": "Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks", "paper_id": "9c3dd8af0a704b12d262432b49b6503d0d96f8a4", "abstract": "Unlearnable example attacks are data poisoning techniques that can be used to safeguard public data against unauthorized use for training deep learning models. These methods add stealthy perturbations to the original image, thereby making it difficult for deep learning models to learn from these training data effectively. Current research suggests that adversarial training can, to a certain degree, mitigate the impact of unlearnable example attacks, while common data augmentation methods are not effective against such poisons. Adversarial training, however, demands considerable computational resources and can result in non-trivial accuracy loss. In this paper, we introduce the UEraser method, which outperforms current defenses against different types of state-of-the-art unlearnable example attacks through a combination of effective data augmentation policies and loss-maximizing adversarial augmentations. In stark contrast to the current SOTA adversarial training methods, UEraser uses adversarial augmentations, which extends beyond the confines of $ \\ell_p $ perturbation budget assumed by current unlearning attacks and defenses. It also helps to improve the model's generalization ability, thus protecting against accuracy loss. UEraser wipes out the unlearning effect with error-maximizing data augmentations, thus restoring trained model accuracies. Interestingly, UEraser-Lite, a fast variant without adversarial augmentations, is also highly effective in preserving clean accuracies. On challenging unlearnable CIFAR-10, CIFAR-100, SVHN, and ImageNet-subset datasets produced with various attacks, it achieves results that are comparable to those obtained during clean training. We also demonstrate its efficacy against possible adaptive attacks. Our code is open source and available to the deep learning community: https://github.com/lafeat/ueraser.", "intro": "Deep learning has achieved great success in fields such as computer vision [13] and natural language processing [8], and the development of various fields now relies on large-scale datasets. While these datasets have undoubtedly contributed significantly to the progress of deep learning, the collection of unauthorized private data for training these models now presents an emerging concern. Recently, numerous poisoning methods [10,15,27,30,33] have been proposed to add imperceptible perturbations to images. These perturbations can form \"shortcuts\" [11,15] in the training data to prevent training and thus make the data unlearnable in order to preserve privacy. It is commonly perceived that the only effective defense against unlearnable examples are adversarial training algorithms [15,30,10]. Popular data augmentation methods such as CutOut [9], MixUp [36], and AutoAugment [5], however, have all been demonstrated to be ineffective defenses.\n\nCurrent methods of unlearnable attacks involves the specification of an p perturbation budget, where p \u2208 {2, \u221e} in general. Essentially, they constrain the added perturbation to a small -ball of p -distance from the source image, in order to ensure stealthiness of these attacks. Adversarial training defenses [20,10] Figure 1: A high-level overview of UEraser for countering unlearning poisons. Note that UEraser recovers the clean accuracy of unlearnable examples by data augmentations. The reported results are for EM [15] unlearnable CIFAR-10 with an \u221e perturbation budget of 8/255. seeks to counteract the bounded perturbations from such unlearnable attacks. However, large defensive perturbations comes with significant accuracy degradations. This prompts the inquiry of the existence of effective defense mechanisms that leverage threat models that are outside the purview of attackers. Specifically, can we devise effective adversarial policies for training models that extend beyond the confines of the p perturbation budgets?\n\nIn this paper, we thus propose UEraser, which performs error-maximizing data augmentation, to defense against unlearning poisons. UEraser challenges the preconception that data augmentation is not an effective defense against unlearning poisons. UEraser expands the perturbation distance far beyond traditional adversarial training, as data augmentation policies do not confine themselves to the p perturbation constraints. It can therefore effectively disrupt \"unlearning shortcuts\" formed by attacks within narrow p constraints. Yet, the augmentations employed by UEraser are natural and realistic transformations extensively utilized by existing works to improve the models' ability to generalize. This, in turn, helps in avoiding accuracy loss due to perturbations used by adversarial training that could potentially be out-of-distribution. Finally, traditional adversarial training is not effective in mitigating unlearning poisons produced by adaptive attacks [10], while UEraser is highly resiliant against adaptive attacks with significantly lower accuracy reduction.\n\nIn summary, our work has three main contributions:\n\n\u2022 It extends adversarial training beyond the confines of the p perturbation budgets commonly imposed by attackers into data augmentation policies.\n\n\u2022 We propose UEraser, which introduces an effective adversarial augmentation to wipe out unlearning perturbations. It defends against the unlearnable attacks by maximizing the error of the augmented samples.\n\n\u2022 UEraser is highly effective in wiping out the unlearning effect on five state-of-the-art (SOTA) unlearning attacks, outperforming existing SOTA defense methods.\n\n\u2022 We explore the adaptive attacks on UEraser and explored additional combinations of augmentation policies. It lays a fresh foundation for future competitions among unlearnable example attack and defense strategies.\n\nUnlearnable example attacks bear great significance, not just from the standpoint of privacy preservation, but also as a form of data poisoning attack. It is thus of great significance to highlight the shortcomings of current attack methods. Perhaps most surprisingly, even a well-known unlearnable attack such as EM [15] is unable to impede the effectiveness of UEraser. By training a ResNet-18 model from scratch using exclusively CIFAR-10 unlearnable data produced with EM (with an \u221e budget of 8/255), UEraser achieves exceptional accuracy of 95.24% on the clean test set, which closely matches the accuracy achievable by standard training on a clean training set. This suggests that existing unlearning perturbations are tragically inadequate in making data unlearnable, even with adaptive attacks that employs UEraser. By understanding their weaknesses, we can anticipate how malicious actors may attempt to exploit them, and prepare stronger safeguards against such threats. We hope UEraser can help facilitate the advancement of research in these attacks and defenses."}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 3488815, "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "paper_id": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386", "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.", "intro": "Recent breakthroughs in computer vision and speech recognition are bringing trained classifiers into the center of security-critical systems. Important examples include vision for autonomous cars, face recognition, and malware detection. These developments make security aspects of machine learning increasingly important. In particular, resistance to adversarially chosen inputs is becoming a crucial design goal. While trained models tend to be very effective in classifying benign inputs, recent work [22,7,14,20] shows that an adversary is often able to manipulate the input so that the model produces an incorrect output.\n\nThis phenomenon has received particular attention in the context of deep neural networks, and there is now a quickly growing body of work on this topic [6,12,16,18,23,21,25]. Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability [22,7,14,20,13]. This holds even when the benign example was classified correctly, and the change is imperceptible to a human. Apart from the security implications, this phenomenon also demonstrates that our current models are not learning the underlying concepts in a robust manner. All these findings raise a fundamental question:"}
{"tabids": ["9a34d702-e100-4c93-b280-35b662fc1cbe"], "corpus_id": 260680386, "title": "APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses", "paper_id": "c0c06de39c1fd89db3bdb52b56851bfbabba0b09", "abstract": "The efficacy of availability poisoning, a method of poisoning data by injecting imperceptible perturbations to prevent its use in model training, has been a hot subject of investigation. Previous research suggested that it was difficult to effectively counteract such poisoning attacks. However, the introduction of various defense methods has challenged this notion. Due to the rapid progress in this field, the performance of different novel methods cannot be accurately validated due to variations in experimental setups. To further evaluate the attack and defense capabilities of these poisoning methods, we have developed a benchmark -- APBench for assessing the efficacy of adversarial poisoning. APBench consists of 9 state-of-the-art availability poisoning attacks, 8 defense algorithms, and 4 conventional data augmentation techniques. We also have set up experiments with varying different poisoning ratios, and evaluated the attacks on multiple datasets and their transferability across model architectures. We further conducted a comprehensive evaluation of 2 additional attacks specifically targeting unsupervised models. Our results reveal the glaring inadequacy of existing attacks in safeguarding individual privacy. APBench is open source and available to the deep learning community: https://github.com/lafeat/apbench.", "intro": "Recent advancements of deep neural networks (DNNs) [14,20,34] heavily rely on the abundant availability of data resources [4,18,31]. However, the unauthorized collection of large-scale data through web scraping for model training has raised concerns regarding data security and privacy. In response to these concerns, a new paradigm of practical and effective data protection methods has emerged, known as availability poisoning attacks (APA) [7,8,9,13,13,16,30,30,33,36,39,40,41], or unlearnable example attacks. These poisoning methods inject small perturbations into images that are typically imperceptible to humans, creating \"shortcuts\" [10] in the training process that hinder the model's ability to learn the original features of the images. Recently, the field of deep learning has witnessed advancements in defense strategies [6,16,22,28] that hold the potential to challenge APAs, thereby undermining their claimed effectiveness and robustness. These defenses reveal the glaring inadequacy of existing APAs in safeguarding individual privacy in images. Consequently, we anticipate an impending arms race between attack and defense strategies in the near future.\n\nHowever, evaluating the performance of these new methods across diverse model architectures and datasets poses a significant challenge due to variations in experimental settings of recent literatures. In addition, researchers face the daunting task of staying abreast of the latest methods and assessing the effectiveness of various competing attack-defense combinations. This could greatly hamper the development and empirical exploration of novel attack and defense strategies.\n\nTo tackle this challenge, we propose the APBench, a benchmark specifically designed for availability poisoning attacks and defenses. It involves implementing poisoning attack and defense mechanisms under standardized perturbations and training hyperparameters, in order to ensure fair and reproducible comparative evaluations. APBench comprises a range of availability poisoning attacks and defense algorithms, and commonly-used data augmentation policies. This comprehensive suite allows us to evaluate the effectiveness of the poisoning attacks thoroughly.\n\nOur contributions can be summarized as follows:\n\n\u2022 An open source benchmark for state-of-the-art availability poisoning attacks and defenses, including 9 supervised and 2 unsupervised poisoning attack methods, 8 defense strategies and 4 common data augmentation methods. \u2022 We conduct a comprehensive evaluation of competitions between pairs of poisoning attacks and defenses. \u2022 We conducted experiments across 4 publicly available datasets, and also extensively examined scenarios of partial poisoning, increased perturbations, the transferability of attacks to 4 different DNN models under various defenses, and unsupervised learning. We provide visual evaluation tools such as t-SNE, Shapley value map and Grad-CAM to qualitatively analyze the impact of poisoning attacks.\n\nThe aim of APBench is to serve as a catalyst for facilitating and promoting future advancements in both availability poisoning attack and defense methods. By providing a platform for evaluation and comparison, we aspire to pave the way for the development of future availability poisoning attacks that can effectively preserve utility and protect privacy.\n\n2 Related Work"}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 195497272, "title": "Semi-Supervised Learning With Graph Learning-Convolutional Networks", "paper_id": "960a6293d068e479435b693df04b839b24ffb7d9", "abstract": "Graph Convolutional Neural Networks (graph CNNs) have been widely used for graph data representation and semi-supervised learning tasks. However, existing graph CNNs generally use a fixed graph which may not be optimal for semi-supervised learning tasks. In this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for graph data representation and semi-supervised learning. The aim of GLCN is to learn an optimal graph structure that best serves graph CNNs for semi-supervised learning by integrating both graph learning and graph convolution in a unified network architecture. The main advantage is that in GLCN both given labels and the estimated labels are incorporated and thus can provide useful \u2018weakly\u2019 supervised information to refine (or learn) the graph construction and also to facilitate the graph convolution operation for unknown label estimation. Experimental results on seven benchmarks demonstrate that GLCN significantly outperforms the state-of-the-art traditional fixed structure based graph CNNs.", "intro": "Deep neural networks have been widely used in many computer vision and pattern recognition tasks. Recently, many methods have been proposed to generalize the convolution operation on arbitrary graphs to address graph structure data [5,1,15,11,19,21]. Overall, these methods can be categorized into spatial convolution and spectral convolution methods [22]. For spatial methods, they generally define graph convolution operation directly by defining an operation on node groups of neighbors. For example, Duvenaud et al. [5] propose a convolutional neural network that operates directly on graphs and provide an end-to-end feature learning for graph data. Atwood and Towsley [1] propose Diffusion-Convolutional Neural Networks (DCNNs) by employing a graph diffusion process to incorporate the contextual information of node in graph node classification. * Corresponding author Monti et al. [15] present mixture model CNNs (MoNet) and provide a unified generalization of CNN architectures on graphs. By designing an attention layer, Veli\u010dkovi\u0107 et al. [21] present Graph Attention Networks (GAT) for semisupervised learning. For spectral methods, they generally define graph convolution operation based on spectral representation of graphs. For example, Bruna et al. [3] propose to define graph convolution in the Fourier domain based on eigen-decomposition of graph Laplacian matrix. Defferrard et al. [4] propose to approximate the spectral filters based on Chebyshev expansion of graph Laplacian to avoid the high computational complexity of eigen-decomposition. Kipf et al. [11] propose a more simple Graph Convolutional Network (GCN) for semi-supervised learning.\n\nThe above graph CNNs have been widely used for supervised or semi-supervised learning tasks. In this paper, we focus on semi-supervised learning. One important aspect of graph CNNs is the graph structure representation of data. In general, the data we provide to graph CNNs either has a known intrinsic graph structure, such as social networks, or we construct a human established graph for it, such as k-nearest neighbor graph with Gaussian kernel. However, it is difficult to evaluate whether the graphs obtained from domain knowledge (e.g., social network) or established by human are optimal for semi-supervised learning in graph C-NNs. Henaff et al. [7] propose to learn a supervised graph with a fully connected network. However, the learned graph is obtained from a separate network which is also not guaranteed to best serve the graph CNNs. Li et al. [19] propose optimal graph CNNs, in which the graph is learned adaptively by using a distance metric learning. However, it use an approximate algorithm to estimate graph Laplacian which may lead to weak local optimal solution.\n\nIn this paper, we propose a novel Graph Learning-Convolutional Network (GLCN) for semi-supervised learning problem. The main idea of GLCN is to learn an optimal graph representation that best serves graph CNNs for semisupervised learning by integrating both graph learning and graph convolution simultaneously in a unified network architecture. The main advantages of the proposed GLCN for semi-supervised learning are summarized as follows. \u2022 In GLCN, both given labels and the estimated labels are incorporated and thus can provide useful 'weakly' supervised information to refine (or learn) the graph construction and to facilitate the graph convolution operation in graph CNN for unknown label estimation.\n\n\u2022 GLCN can be trained via a single optimization manner, which can thus be implemented simply.\n\nTo the best of our knowledge, this is the first attempt to build a unified graph learning-convolutional network architecture for semi-supervised learning. Experimental results demonstrate that GLCN outperforms state-of-the-art graph CNNs on semi-supervised learning tasks."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 202558560, "title": "Joint Learning of Graph Representation and Node Features in Graph Convolutional Neural Networks", "paper_id": "6f8b327b9c950daecf1ed36d03ca7708a9aa06bf", "abstract": "Graph Convolutional Neural Networks (GCNNs) extend classical CNNs to graph data domain, such as brain networks, social networks and 3D point clouds. It is critical to identify an appropriate graph for the subsequent graph convolution. Existing methods manually construct or learn one fixed graph for all the layers of a GCNN. In order to adapt to the underlying structure of node features in different layers, we propose dynamic learning of graphs and node features jointly in GCNNs. In particular, we cast the graph optimization problem as distance metric learning to capture pairwise similarities of features in each layer. We deploy the Mahalanobis distance metric and further decompose the metric matrix into a low-dimensional matrix, which converts graph learning to the optimization of a low-dimensional matrix for efficient implementation. Extensive experiments on point clouds and citation network datasets demonstrate the superiority of the proposed method in terms of both accuracies and robustness.", "intro": "Graph Convolutional Neural Networks (GCNNs) have been receiving increasing attention as a powerful tool for irregularly structured data on graphs, such as citation networks, social networks and 3D point clouds.The construction of an appropriate graph topology plays a critical role in GCNNs for efficient feature learning.In settings where the graph is inaccurate or even not readily available, it is necessary to infer or learn a graph topology from data before it is used for guiding graph convolution in GCNNs.\n\nMost of the previous studies construct underlying graphs from data empirically, such as k-Nearest-Neighbor (k-NN) graphs [1,2], which may lead to sub-optimal solutions.Few methods exploit graph learning for optimized representation [3,4,5,6], which learns a fixed and shared graph for all instances, or an individual graph for each instance, or a combination of shared and individual graphs.However, only one fixed graph is learned and applied to all layers of the entire network, which may not well capture the underlying structure of node features in different layers dynamically.\n\nExtending on these previous studies, we propose a Joint Learning Graph Convolutional Network (JLGCN), which exploits dynamic learning of graphs and node features jointly in GCNNs.In particular, we optimize an underlying graph kernel from data features via distance metric learning that characterizes pairwise similarities of data.We deploy the Mahalanobis distance [7], which takes into account data correlations for intrinsic representation.Given a Kdimensional feature vector f i per node i, f i \u2208 R K , the Mahalanobis distance between the features f i and f j of nodes i and j is defined as: d M (f i , f j ) = (f i \u2212 f j ) M(f i \u2212 f j ), where M \u2208 R K\u00d7K is the Mahalanobis distance metric matrix which reflects feature correlations.Hence, we convert the problem of graph learning to the optimization of M. As M is positive semi-definite (PSD), it is often nontrivial to solve efficiently.Instead, we decompose it as M = RR and learn R for ease of optimization, where R \u2208 R K\u00d7S has a lower dimension S << K to reduce number of parameters for efficient implementation.Given features f , we seek to minimize the Graph Laplacian Regularizer (GLR) [8] f L(R)f by optimizing R, which measures the smoothness of features f with respect to the graph Laplacian L1 .This essentially enforces the graph encoded in L(R) to capture pairwise similarities of f .Hence, we formulate the joint learning of graphs L(R) and node features f as an optimization problem, which minimizes a weighted sum of the GLR and cross-entropy.We set this objective as the loss function of the proposed JLGCN to guide network model optimization, which employs a localized first-order approximation of spectral graph convolution as in [9].Further, the learned graph at the previous layer is added to that of the current layer for multi-level feature learning.The proposed JLGCN can be integrated into any GCNN architecture for applications such as node classification and graph classification.To validate the effectiveness of the proposed JLGCN, we apply it to semi-supervised learning for citation networks and 3D point cloud learning problems.Extensive experimental results demonstrate the superiority and robustness of JLGCN compared with state-of-the-art methods on four datasets even with a small model size.\n\nOur contributions can be summarized as follows:\n\n\u2022 We propose joint learning of underlying graphs and node features at each layer of a GCNN, which captures pairwise similarities of node features dynamically.\u2022 We cast the graph learning problem as distance metric learning with the Mahalanobis distance deployed.We further decompose the distance metric into a low-dimensional matrix for efficient implementation, which is optimized from both the GLR and cross entropy along with node features.\n\n\u2022 Extensive experiments on semi-supervised learning and point cloud classification demonstrate the superiority and robustness of our method."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 94822, "title": "Dynamic Graph CNN for Learning on Point Clouds", "paper_id": "e1799aaf23c12af6932dc0ef3dfb1638f01413d1", "abstract": "Point clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. Hence, the design of intelligent computational models that act directly on point clouds is critical, especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv is differentiable and can be plugged into existing architectures. Compared to existing modules operating largely in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked or recurrently applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. Beyond proposing this module, we provide extensive evaluation and analysis revealing that EdgeConv captures and exploits fine-grained geometric properties of point clouds. The proposed approach achieves state-of-the-art performance on standard benchmarks including ModelNet40 and S3DIS.", "intro": "Point clouds, or scattered collections of points in 2D or 3D, are arguably the simplest shape representation; they also comprise the output of 3D sensing technology including LiDAR scanners and stereo reconstruction. With the advent of fast 3D point cloud acquisition, recent pipelines for graphics and vision often process point clouds directly, bypassing expensive mesh reconstruction or denoising due to efficiency considerations or instability of these techniques in the presence of noise. A few of the many recent applications of point cloud processing and analysis include indoor navigation [57], self-driving vehicles [33], robotics [40], and shape synthesis and modeling [14]. Modern applications demand high-level processing of point clouds. Rather than identifying salient geometric features like corners and edges, recent algorithms search for semantic cues and affordances. These features do not fit cleanly into the frameworks of computational or differential geometry and typically require learning-based approaches that derive relevant information through statistical analysis of labeled or unlabeled datasets.\n\nIn this paper, we primarily consider point cloud classification and segmentation, two model tasks in the point cloud processing world. Traditional methods for solving these problems employ handcrafted features to capture geometric properties of point clouds [26,38,39]. More recently, the success of deep neural networks for image processing has motivated a data-driven approach to learning features on point clouds. Deep point cloud processing and analysis methods are developing rapidly and outperform traditional approaches in various tasks [10].\n\nAdaptation of deep learning to point cloud data, however, is far from straightforward. Most critically, standard deep neural network models take as input data with regular structure, while point clouds are fundamentally irregular: Point positions are continuously distributed in the space, and any permutation of their ordering does not change the Figure 1. Point cloud segmentation using the proposed neural network. Bottom: schematic neural network architecture. Top: Structure of the feature spaces produced at different layers of the network, visualized as the distance from the red point to all the rest of the points (shown left-to-right are the input and layers 1-3; rightmost figure shows the resulting segmentation). Observe how the feature space structure in deeper layers captures semantically similar structures such as wings, fuselage, or turbines, despite a large distance between them in the original input space. spatial distribution. One common approach to process point cloud data using deep learning models is to first convert raw point cloud data into a volumetric representation, namely a 3D grid [30,54]. This approach, however, usually introduces quantization artifacts and excessive memory usage, making it difficult to go to capture high-resolution or finegrained features.\n\nState-of-the-art deep neural networks are designed specifically to handle the irregularity of point clouds, directly manipulating raw point cloud data rather than passing to an intermediate regular representation. This approach was pioneered by PointNet [34], which achieves permutation invariance of points by operating on each point independently and subsequently applying a symmetric function to accumulate features. Various extensions of Point-Net consider neighborhoods of points rather than acting on each independently [36,43]; these allow the network to exploit local features, improving upon performance of the basic model. These techniques largely treat points independently at local scale to maintain permutation invariance. This independence, however, neglects the geometric relationships among points, presenting a fundamental limitation that leads to local features missing.\n\nTo address these drawbacks, we propose a novel simple operation, called EdgeConv, which captures local geometric structure while maintaining permutation invariance. Instead of generating points' features directly from their embeddings, EdgeConv generates edge features that describe the relationships between a point and its neighbors. EdgeConv is designed to be invariant to the ordering of neighbors, and thus permutation invariant.\n\nEdgeConv is easy to implement and integrate into existing deep learning models to improve their performance. In our experiments, we integrate EdgeConv into the basic version of PointNet without using any feature transformation. We show performance improvement by a large margin; the resulting network achieves state-of-the-art performance on several datasets, most notably ModelNet40 and S3DIS for classification and segmentation.\n\nKey Contributions. We summarize the key contributions of our work as follows:\n\n\u2022 We present a novel operation for point clouds, Edge-Conv, to better capture local geometric features of point clouds while still maintaining permutation invariance.\n\n\u2022 We show the model can learn to semantically group points by dynamically updating the graph.\n\n\u2022 We demonstrate that EdgeConv can be integrated into multiple existing pipelines for point cloud processing.\n\n\u2022 We present extensive analysis and testing of EdgeConv and show that it achieves state-of-the-art performance on benchmark datasets. Left: An example of computing an edge feature, eij, from a point pair, xi and xj. In this example, h \u0398 () is instantiated using a fully connected layer, and the learnable parameters are its associated weights and bias. Right: Visualize the EdgeConv operation. The output of EdgeConv is calculated by aggregating the edge features associated with all the edges emanating from each connected vertex."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 85543335, "title": "Learning Discrete Structures for Graph Neural Networks", "paper_id": "1e059de3858c90445f8dce8a48cdb16ce3fc2af0", "abstract": "Graph neural networks (GNNs) are a popular class of machine learning models whose major advantage is their ability to incorporate a sparse and discrete dependency structure between data points. Unfortunately, GNNs can only be used when such a graph-structure is available. In practice, however, real-world graphs are often noisy and incomplete or might not be available at all. With this work, we propose to jointly learn the graph structure and the parameters of graph convolutional networks (GCNs) by approximately solving a bilevel program that learns a discrete probability distribution on the edges of the graph. This allows one to apply GCNs not only in scenarios where the given graph is incomplete or corrupted but also in those where a graph is not available. We conduct a series of experiments that analyze the behavior of the proposed method and demonstrate that it outperforms related methods by a significant margin.", "intro": "Relational learning is concerned with methods that cannot only leverage the attributes of data points but also their relationships. Diagnosing a patient, for example, not only depends on the patient's vitals and demographic information but also on the same information about their relatives, the information about the hospitals they have visited, and so on. Relational learning, therefore, does not make the assumption of independence between data points but models their dependency explicitly. Graphs are a natural way to represent relational information and there is a large number of machine learning algorithms leveraging graph structure. Graph neural networks (GNNs) (Scarselli et al., 2009) are one such class of algorithms that are able to incorporate sparse and discrete dependency structures between data points.\n\nWhile a graph structure is available in some domains, in others it has to be inferred or constructed. A possible approach is to first create a k-nearest neighbor (kNN) graph based on some measure of similarity between data points. This is a common strategy used by several learning methods such as LLE (Roweis & Saul, 2000) and Isomap (Tenenbaum et al., 2000). A major shortcoming of this approach, however, is that the efficacy of the resulting models hinges on the choice of k and, more importantly, on the choice of a suitable similarity measure over the input features. In any case, the graph creation and parameter learning steps are independent and require heuristics and trial and error. Alternatively, one could simply use a kernel matrix to model the similarity of examples implicitly at the cost, however, of introducing a dense dependency structure which may be problematic from a computational point of view.\n\nWith this paper, we follow a different route with the aim of learning discrete and sparse dependencies between data points while simultaneously training the parameters of graph convolutional networks (GCN), a class of GNNs. Intuitively, GCNs learn node representations by passing and aggregating messages between neighboring nodes (Kipf & Welling, 2017;Monti et al., 2017;Gilmer et al., 2017;Hamilton et al., 2017;Duran & Niepert, 2017;Velickovic et al., 2018). We propose to learn a generative probabilistic model for graphs, samples from which are used both during training and at prediction time. Edges are modelled with random variables whose parameters are treated as hyperparameters in a bilevel learning framework (Franceschi et al., 2018). We iteratively sample the structure while minimizing an inner objective (a training error) and optimize the edge distribution parameters by minimizing an outer objective (a validation error).\n\nTo the best of our knowledge, this is the first method that simultaneously learns the graph and the parameters of a GNN for semi-supervised classification. Moreover, and this might be of independent interest, we adapt gradient-based hyperparameter optimization to work for a class of discrete hyperparameters (edges, in this work). The proposed approach makes GNNs applicable to problems where the graph is incomplete or entirely missing. We conduct a series of experiments and show that the proposed method is competitive with and often outperforms existing approaches. We also verify that the resulting graph generative models have meaningful edge probabilities."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 214003631, "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings", "paper_id": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf", "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.", "intro": "Recent years have seen a significantly growing amount of interest in graph neural networks (GNNs), especially on efforts devoted to developing more effective GNNs for node classification [27,34,16,48], graph classification [55,40] and graph generation [44,35,56]. Despite GNNs' powerful ability in learning expressive node embeddings, unfortunately, they can only be used when graph-structured data is available. Many real-world applications naturally admit network-structured data (e.g., social networks). However, these intrinsic graph-structures are not always optimal for the downstream tasks. This is partially because the raw graphs were constructed from the original feature space, which may not reflect the \"true\" graph topology after feature extraction and transformation. Another potential reason is that real-world graphs are often noisy or even incomplete due to the inevitably error-prone data measurement or collection. Furthermore, many applications such as those in natural language processing [7,53] may only have sequential data or even just the original feature matrix, requiring additional graph construction from the original data matrix.\n\nTo address these limitations, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning the graph structure and the GNN parameters that are optimized towards the downstream prediction task. The key rationale of our IDGL framework is to learn a better graph structure based on better node embeddings, and at the same time, to learn better node embeddings based on a better graph structure. In particular, IDGL is a novel iterative method that aims to search for a hidden graph structure that augments the initial graph structure (if not available we use a kNN graph) with the goal of optimizing the graph for supervised prediction tasks. The iterative method adjusts when to stop in each mini-batch when the learned graph structure approaches close enough to the graph optimized for the prediction task.\n\nFurthermore, we present a graph learning neural network that uses multi-head self-attention with epsilon-neighborhood sparsification for constructing a graph. Moreover, unlike the work in [23] that directly optimizes an adjacency matrix without considering the downstream task, we learn a graph metric learning function by optimizing a joint loss combining both task-specific prediction loss and graph regularization loss. Finally, we further propose a scalable version of our IDGL framework, namely IDGL-ANCH, by combining the anchor-based approximation technique, which reduces the time and memory complexity from quadratic to linear with respect to the numbers of graph nodes.\n\nIn short, we summarize the main contributions as follows:\n\n\u2022 We propose a novel end-to-end graph learning framework (IDGL) for jointly and iteratively learning the graph structure and graph embedding. IDGL dynamically stops when the learned graph structure approaches the optimized graph (for prediction). To the best of our knowledge, we are the first to introduce the iterative learning for graph structure learning. \u2022 Combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which achieves linear complexity in both computational time and memory consumption with respect to the number of graph nodes. \u2022 Experimental results show that our models consistently outperform or match state-of-the-art baselines on various downstream tasks. More importantly, IDGL can be more robust to adversarial graph examples and can cope with both transductive and inductive learning."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 210698881, "title": "Graph-Bert: Only Attention is Needed for Learning Graph Representations", "paper_id": "78542c2be9bb853a4e04642f2d315cfb0c6d94b3", "abstract": "The dominant graph neural networks (GNNs) over-rely on the graph links, several serious performance problems with which have been witnessed already, e.g., suspended animation problem and over-smoothing problem. What's more, the inherently inter-connected nature precludes parallelization within the graph, which becomes critical for large-sized graph, as memory constraints limit batching across the nodes. In this paper, we will introduce a new graph neural network, namely GRAPH-BERT (Graph based BERT), solely based on the attention mechanism without any graph convolution or aggregation operators. Instead of feeding GRAPH-BERT with the complete large input graph, we propose to train GRAPH-BERT with sampled linkless subgraphs within their local contexts. GRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a pre-trained GRAPH-BERT can also be transferred to other application tasks directly or with necessary fine-tuning if any supervised label information or certain application oriented objective is available. We have tested the effectiveness of GRAPH-BERT on several graph benchmark datasets. Based the pre-trained GRAPH-BERT with the node attribute reconstruction and structure recovery tasks, we further fine-tune GRAPH-BERT on node classification and graph clustering tasks specifically. The experimental results have demonstrated that GRAPH-BERT can out-perform the existing GNNs in both the learning effectiveness and efficiency.", "intro": "Graph provides a unified representation for many interconnected data in the real-world, which can model both the diverse attribute information of the node entities and the extensive connections among these nodes. For instance, the human brain imaging data, online social media and bio-medical molecules can all be represented as graphs, i.e., the brain graph , social graph [Ugander et al., 2011] and molecular graph [Jin et al., 2018], respectively. Traditional machine learning models can hardly be applied to the graph data directly, which usually take the feature vectors as the inputs. Viewed in such a perspective, learning the representations of the graph structured data is an important research task.\n\nIn recent years, great efforts have been devoted to designing new graph neural networks (GNNs) for effective graph representation learning. Besides the network embedding models, e.g., node2vec [Grover and Leskovec, 2016] and deepwalk [Perozzi et al., 2014a], the recent graph neural networks, e.g., GCN [Kipf and Welling, 2016], GAT [Veli\u010dkovi\u0107 et al., 2018] and LOOPYNET [Zhang, 2018], are also becoming much more important, which can further refine the learned representations for specific application tasks. Meanwhile, most of these existing graph representation learning models are still based on the graph structures, i.e., the links among the nodes. Via necessary neighborhood information aggregation or convolutional operators along the links, nodes' representations learned by such approaches can preserve the graph structure information.\n\nHowever, several serious learning performance problem, e.g., suspended animation problem  and over-smoothing problem [Li et al., 2018], with the existing GNN models have also been witnessed in recent years. According to , for the GNNs based on the approximated graph convolutional operators [Hammond et al., 2011], as the model architecture goes deeper and reaches certain limit, the model will not respond to the training data and suffers from the suspended animation problem. Meanwhile, the node representations obtained by such deep models tend to be over-smoothed and also become indistinguishable [Li et al., 2018]. Both of these two problems greatly hinder the applications of GNNs for deep graph representation learning tasks. What's more, the inherently interconnected nature precludes parallelization within the graph, which becomes critical for large-sized graph input, as memory constraints limit batching across the nodes.\n\nTo address the above problems, in this paper, we will propose a new graph neural network model, namely GRAPH-BERT (Graph based BERT). Inspired by , model GRAPH-BERT will be trained with sampled nodes together with their context (which are called linkless subgraphs in this paper) from the input large-sized graph data. Distinct from the existing GNN models, in the representation learning process, GRAPH-BERT utilizes no links in such sampled batches, which will be purely based on the attention mechanisms instead [Vaswani et al., 2017;Devlin et al., 2018]. Therefore, GRAPH-BERT can get rid of the aforementioned learning effectiveness and efficiency problems with existing GNN models promisingly.\n\nWhat's more, compared with computer vision [He et al., 2018] and natural language processing [Devlin et al., 2018], graph neural network pre-training and fine-tuning are still not common practice by this context so far. The main obstacles that prevent such operations can be due to the diverse input graph structures and the extensive connections among the nodes. Also the different learning task objectives also prevents the transfer of GNNs across different tasks. Since GRAPH-BERT doesn't really rely on the graph links at all, in this paper, we will investigate the transfer of pre-trained GRAPH-BERT on new learning tasks and other sequential models (with necessary fine-tuning), which will also help construct the functional pipeline of models in graph learning.\n\nWe summarize our contributions of this paper as follows:\n\n\u2022 New GNN Model: In this paper, we introduce a new GNN model GRAPH-BERT for graph data representation learning. GRAPH-BERT doesn't rely on the graph links for representation learning and can effectively address the suspended animation problems aforementioned. Also GRAPH-BERT is trainable with sampled linkless subgraphs (i.e., target node with context), which is more efficient than existing GNNs constructed for the complete input graph. To be more precise, the training cost of GRAPH-BERT is only decided by (1) training instance number, and (2) sampled subgraph size, which is uncorrelated with the input graph size at all.\n\n\u2022 Unsupervised Pre-Training: Given the input unlabeled graph, we will pre-train GRAPH-BERT based on to two common tasks in graph studies, i.e., node attribute reconstruction and graph structure recovery. Node attribute recovery ensures the learned node representations can capture the input attribute information; whereas graph structure recovery can further ensure GRAPH-BERT learned with linkless subgraphs can still maintain both the graph local and global structure properties.\n\n\u2022 Fine-Tuning and Transfer: Depending on the specific application task objectives, the GRAPH-BERT model can be further fine-tuned to adapt the learned representations to specific application requirements, e.g., node classification and graph clustering. Meanwhile, the pretrained GRAPH-BERT can also be transferred and applied to other sequential models, which allows the construction of functional pipelines for graph learning.\n\nThe remaining parts of this paper are organized as follows. We will introduce the related work in Section 2. Detailed information about the GRAPH-BERT model will be introduced in Section 3, whereas the pre-training and finetuning of GRAPH-BERT will be introduced in Section 4 in detail. The effectiveness of GRAPH-BERT will be tested in Section 5. Finally, we will conclude this paper in Section 6."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 208139383, "title": "Graph-Revised Convolutional Network", "paper_id": "69f9b9a454e02ec91a6a3f6dfbc51120dfd4f8bb", "abstract": "Graph Convolutional Networks (GCNs) have received increasing attention in the machine learning community for effectively leveraging both the content features of nodes and the linkage patterns across graphs in various applications. As real-world graphs are often incomplete and noisy, treating them as ground-truth information, which is a common practice in most GCNs, unavoidably leads to sub-optimal solutions. Existing efforts for addressing this problem either involve an over-parameterized model which is difficult to scale, or simply re-weight observed edges without dealing with the missing-edge issue. This paper proposes a novel framework called Graph-Revised Convolutional Network (GRCN), which avoids both extremes. Specifically, a GCN-based graph revision module is introduced for predicting missing edges and revising edge weights w.r.t. downstream tasks via joint optimization. A theoretical analysis reveals the connection between GRCN and previous work on multigraph belief propagation. Experiments on six benchmark datasets show that GRCN consistently outperforms strong baseline methods by a large margin, especially when the original graphs are severely incomplete or the labeled instances for model training are highly sparse.", "intro": "Graph Convolutional Networks (GCNs) have received increasing attention in recent years as they are highly effective in graph-based node feature induction and belief propagation, and widely applicable to many real-world problems, including computer vision (Wang et al. 2018;Landrieu and Simonovsky 2018), natural language processing (Kipf and Welling 2016;Marcheggiani and Titov 2017), recommender systems (Monti et al. 2017;Ying et al. 2018), epidemiological forecasting (Wu et al. 2018), and more.\n\nHowever, the power of GCNs has not been fully exploited as most of the models assume that the given graph perfectly depicts the ground-truth of the relationship between nodes. Such assumptions are bound to yield sub-optimal results as real-world graphs are usually highly noisy, incomplete (with many missing edges), and not necessarily ideal for different downstream tasks. Ignoring these issues is a fundamental weakness of many existing GCN methods.\n\nRecent methods that attempt to modify the original graph can be split into two major streams: 1) Edge reweighting: GAT (Veli\u010dkovi\u0107 et al. 2017) and GLCN (Jiang et al. 2019) use attention mechanism or feature similarity to reweight the existing edges of the given graph. Since the topological structure of the graph is not changed, the model is prone to be affected by noisy data when edges are sparse. 2) Full graph parameterization: LDS (Franceschi et al. 2019), on the other hand, allows every possible node pairs in a graph to be parameterized. Although this design is more flexible, the memory cost is intractable for large datasets, since the number of parameters increases quadratically with the number of nodes. Therefore, finding a balance between model expressiveness and memory consumption remains an open challenge.\n\nTo enable flexible edge editing while maintaining scalability, we develop a GCN-based graph revision module that performs edge addition and edge reweighting. In each iteration, we calculate an adjacency matrix via GCN-based node embeddings, and select the edges with high confidence to be added. Our method permits a gradient-based training of an end-to-end neural model that can predict unseen edges. Our theoretical analysis demonstrates the effectiveness of our model from the perspective of multigraph (Balakrishnan 1997), which allows more than one edges from different sources between a pair of vertices. To the best of our knowledge, we are the first to reveal the connection between graph convolutional networks and multigraph propagation. Our contributions can be summarized as follows:\n\n\u2022 We introduce a novel structure that simultaneously learns both graph revision and node classification through different GCN modules.\n\n\u2022 Through theoretical analysis, we show our model's advantages in the view of multigraph propagation.\n\n\u2022 Comprehensive experiments on six benchmark datasets from different domains show that our proposed model achieves the best or highly competitive results, especially under the scenarios of highly incomplete graphs or sparse training labels. arXiv:1911.07123v1 [cs.\n\nLG] 17 Nov 2019"}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 231855665, "title": "SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks", "paper_id": "4dc3c61426a3332238ea0feb23f2113a96aef0d4", "abstract": "Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.", "intro": "Graph representation learning has grown rapidly and found applications in domains where a natural graph of the data points is available [4,26]. Graph neural networks (GNNs) [45] have been a key component to the success of the research in this area. Specifically, GNNs have shown promising results for semi-supervised classification when the available graph structure exhibits a high degree of homophily (i.e. connected nodes often belong to the same class) [63].\n\nWe study the applicability of GNNs to (semi-supervised) classification problems where a graph structure is not readily available. The existing approaches for this problem either fix a similarity graph between the nodes or learn the GNN parameters and a graph structure simultaneously (see Related Work). In both cases, one main goal is to construct or learn a graph structure with a high degree of homophily with respect to the labels to aid the GNN classification. The latter approach is sometimes called latent graph learning and often results in higher predictive performance compared to the former approach (see, e.g., [13]).\n\nWe identify a supervision starvation problem in latent graph learning approaches in which the edges between pairs of nodes that are far from labeled nodes receive insufficient supervision; this results in learning poor structures away from labeled nodes and hence poor generalization. We propose a solution for this problem by adopting a multi-task learning framework in which we supplement the classification task with a self-supervised task. The self-supervised task is based on the hypothesis that a graph structure that is suitable for predicting the node features is also suitable for predicting the node labels. It works by masking some input features (or adding noise to them) and training a separate GNN aiming at updating the adjacency matrix in such a way that it can recover the masked (or noisy) features. The task is generic and can be combined with several existing latent graph learning approaches.\n\nWe develop a latent graph learning model, dubbed SLAPS, that adopts the proposed self-supervised task. We provide a comprehensive experimental study on nine datasets (thirteen variations) of various sizes and from various domains and perform thorough analyses to show the merit of SLAPS.\n\nOur main contributions include: 1) identifying a supervision starvation problem for latent graph learning, 2) proposing a solution for the identified problem through self-supervision, 3) developing SLAPS, a latent graph learning model that adopts the self-supervised solution, 4) providing comprehensive experimental results showing SLAPS substantially outperforms existing latent graph learning baselines from various categories on various benchmarks, and 5) providing an implementation for latent graph learning that scales to graphs with hundreds of thousands of nodes."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 246015780, "title": "Towards Unsupervised Deep Graph Structure Learning", "paper_id": "a8d0fdfaf2aa42f7a90ba90a865849cfad70778b", "abstract": "In recent years, graph neural networks (GNNs) have emerged as a successful tool in a variety of graph-related applications. However, the performance of GNNs can be deteriorated when noisy connections occur in the original graph structures; besides, the dependence on explicit structures prevents GNNs from being applied to general unstructured scenarios. To address these issues, recently emerged deep graph structure learning (GSL) methods propose to jointly optimize the graph structure along with GNN under the supervision of a node classification task. Nonetheless, these methods focus on a supervised learning scenario, which leads to several problems, i.e., the reliance on labels, the bias of edge distribution, and the limitation on application tasks. In this paper, we propose a more practical GSL paradigm, unsupervised graph structure learning, where the learned graph topology is optimized by data itself without any external guidance (i.e., labels). To solve the unsupervised GSL problem, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) with the aid of self-supervised contrastive learning. Specifically, we generate a learning target from the original data as an\"anchor graph\", and use a contrastive loss to maximize the agreement between the anchor graph and the learned graph. To provide persistent guidance, we design a novel bootstrapping mechanism that upgrades the anchor graph with learned structures during model learning. We also design a series of graph learners and post-processing schemes to model the structures to learn. Extensive experiments on eight benchmark datasets demonstrate the significant effectiveness of our proposed SUBLIME and high quality of the optimized graphs.", "intro": "Recent years have witnessed the prosperous development of graphbased applications in numerous domains, such as chemistry, bioinformatics and cybersecurity. As a powerful deep learning tool to model graph-structured data, graph neural networks (GNNs) have drawn increasing attention and achieved state-of-the-art performance in various graph analytical tasks, including node classification [22,40], link prediction [21,32], and node clustering [42,55]. GNNs usually follow a message-passing scheme, where node representations are learned by aggregating information from the neighbors on an observed topology (i.e., the original graph structure).\n\nMost GNNs rely on a fundamental assumption that the original structure is credible enough to be viewed as ground-truth information for model training. Such assumption, unfortunately, is usually violated in real-world scenarios, since graph structures are usually extracted from complex interaction systems which inevitably contain uncertain, redundant, wrong and missing connections [45]. Such noisy information in original topology can seriously damage the performance of GNNs. Besides, the reliance on explicit structures hinders GNNs' broad applicability. If GNNs are capable of uncovering the implicit relations between samples, e.g., two images containing the same object, they can be applied to more general domains like vision and language.\n\nTo tackle the aforementioned problems, deep graph structure learning (GSL) is a promising solution that constructs and improves the graph topology with GNNs [7,12,20,58]. Concretely, these methods parameterize the adjacency matrix with a probabilistic model [12,45], full parameterization [20] or metric learning model [7,11,53], and jointly optimize the parameters of the adjacency matrix and GNNs by solving a downstream task (i.e., node classification) [58]. However, existing methods learn graph structures in a supervised scenario, which brings the following issues: (1) The reliance on label information. In supervised GSL methods, humanannotated labels play an important role in providing supervision signal for structure improvement. Such reliance on labels limits the application of supervised GSL on more general cases where annotation is unavailable. (2) The bias of learned edge distribution. Node classification usually follows a semi-supervised setting, where only a small fraction of nodes (e.g., 140/2708 in Cora dataset) are under the supervision of labels. As a result, the connections among these nodes and their neighbors would receive more guidance in arXiv:2201.06367v1 [cs.LG] 17 Jan 2022  structure learning, while the relations between nodes far away from them are rarely discovered by GSL [11]. Such imbalance leads to the bias of edge distribution, affecting the quality of the learned structures.\n\n(3) The limitation on downstream tasks. In existing methods, the structure is specifically learned for node classification, so it may contain more task-specific information rather than general knowledge. Consequently, the refined topology may not benefit other downstream tasks like link prediction or node clustering, indicating the poor generalization ability of the learned structures.\n\nTo address these issues, in this paper, we investigate a novel unsupervised learning paradigm for GSL, namely unsupervised graph structure learning. As compared in Fig. 1, in our learning paradigm, structures are learned by data itself without any external guidance (i.e., labels), and the acquired universal, edge-unbiased topology can be freely applied to various downstream tasks. In this case, one natural question can be raised: how to provide sufficient supervision signal for unsupervised GSL? To answer this, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) to learn graph structures with the aid of self-supervised contrastive learning [25]. Concretely, our method constructs an \"anchor graph\" from the original data to guide structure optimization, with a contrastive loss to maximize the mutual information (MI) between anchor graph and the learned structure. Through maximizing their consistency, informative hidden connections can be discovered, which well respects the node proximity conveyed by the original features and structures. Meanwhile, as we optimize the contrastive loss on the representations of every node, all potential edge candidates will receive the essential supervision, which promotes a balanced edge distribution in the inferred topology. Furthermore, we design a bootstrapping mechanism to update anchor graph with the learned edges, which provides a self-enhanced supervision signal for GSL. Besides, we carefully design multiple graph learners and post-processing schemes to model graph topology for diverse data. In summary, our core contributions are three-fold:\n\n\u2022 Problem. We propose a novel unsupervised learning paradigm for graph structure learning, which is more practical and challenging than the existing supervised counterpart. To the best of our knowledge, this is the first attempt to learn graph structures with GNNs in an unsupervised setting. \u2022 Algorithm. We propose a novel unsupervised GSL method SUBLIME, which guides structure optimization by maximizing the agreement between the learned structure and a crafted self-enhanced learning target with contrastive learning.\n\n\u2022 Evaluations. We perform extensive experiments to corroborate the effectiveness and analyze the properties of SUBLIME via thorough comparisons with state-of-the-art methods on eight benchmark datasets."}
{"tabids": ["a4751f48-857a-4785-ad8a-e690dd719028"], "corpus_id": 245219275, "title": "Graph Structure Learning with Variational Information Bottleneck", "paper_id": "7fc37fbd2f808984cfc5c78410a993f89eb0ef49", "abstract": "Graph Neural Networks (GNNs) have shown promising results on a broad spectrum of applications. Most empirical studies of GNNs directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, graphs in the real world are inevitably noisy or incomplete, which could even exacerbate the quality of graph representations. In this work, we propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the perspective of information theory. VIB-GSL advances the Information Bottleneck (IB) principle for graph structure learning, providing a more elegant and universal framework for mining underlying task-relevant relations. VIB-GSL learns an informative and compressive graph structure to distill the actionable information for specific downstream tasks. VIB-GSL deduces a variational approximation for irregular graph data to form a tractable IB objective function, which facilitates training stability. Extensive experimental results demonstrate that the superior effectiveness and robustness of VIB-GSL.", "intro": "Recent years have seen a significant growing amount of interest in graph representation learning (Zhang et al. 2018;Tong et al. 2021), especially in efforts devoted to developing more effective graph neural networks (GNNs) (Zhou et al. 2020). Despite GNNs' powerful ability in learning graph representations, most of them directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, these raw graphs are naturally admitted from network-structure data (e.g., social network) or constructed from the original feature space by some pre-defined rules, which are usually independent of the downstream tasks and lead to the gap between the raw graph and the optimal graph for specific tasks. Moreover, most of graphs in the real-word are noisy or incomplete due to the error-prone data collection (Chen, Wu, and Zaki 2020), which could even exacerbate the quality of representations produced by GNNs (Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018;Sun et al. 2018). It's also found that the properties of a graph Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. are mainly determined by some critical structures rather than the whole graph (Sun et al. 2021;Peng et al. 2021). Furthermore, many graph enhanced applications (e.g., text classification  and vision navigation (Gao et al. 2021)) may only have data without graph-structure and require additional graph construction to perform representation learning. The above issues pose a great challenge for applying GNNs to real-world applications, especially in some risk-critical scenarios. Therefore, learning a task-relevant graph structure is a fundamental problem for graph representation learning.\n\nTo adaptively learn graph structures for GNNs, many graph structure learning methods (Zhu et al. 2021;Franceschi et al. 2019;Chen, Wu, and Zaki 2020) are proposed, most of which optimize the adjacency matrix along with the GNN parameters toward downstream tasks with assumptions (e.g., community) or certain constraints (e.g., sparsity, low-rank, and smoothness) on the graphs. However, these assumptions or explicit certain constraints may not be applicable to all datasets and tasks. There is still a lack of a general framework that can mine underlying relations from the essence of representation learning.\n\nRecalling the above problems, the key of structure learning problem is learning the underlying relations invariant to task-irrelevant information. Information Bottleneck (IB) principle (Tishby, Pereira, and Bialek 2000) provides a framework for constraining such task-irrelevant information retained at the output by trading off between prediction and compression. Specifically, the IB principle seeks for a representation Z that is maximally informative about target Y (i.e., maximize mutual information I(Y ; Z)) while being minimally informative about input data X (i.e., minimize mutual information I(X; Z)). Based on the IB principle, the learned representation is naturally more robust to data noise. IB has been applied to representation learning Jeon et al. 2021;Pan et al. 2020;Bao 2021;Dubois et al. 2020) and numerous deep learning tasks such as model ensemble (Sinha et al. 2020), fine-tuning (Mahabadi, Belinkov, and Henderson 2021), salient region discovery (Zhmoginov, Fischer, and Sandler 2020).\n\nIn this paper, we advance the IB principle for graph to solve the graph structure learning problem. We propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL. VIB-GSL employs the irrelevant feature masking and structure learning method to generate a new IB-Graph G IB as a bottleneck to distill the actionable information for the downstream task. VIB-GSL consists of three steps: (1) the IB-Graph generator module learns the IB-graph G IB by masking irrelevant node features and learning a new graph structure based on the masked feature; (2) the GNN module takes the IBgraph G IB as input and learns the distribution of graph representations; (3) the graph representation is sampled from the learned distribution with a reparameterization trick and then used for classification. The overall framework can be trained efficiently with the supervised classification loss and the distribution KL-divergence loss for the IB objective. The main contributions are summarized as follows:\n\n\u2022 VIB-GSL advances the Information Bottleneck principle for graph structure learning, providing an elegant and universal framework in the perspective of information theory. \u2022 VIB-GSL is model-agnostic and has a tractable variational optimization upper bound that is easy and stable to optimize. It is sufficient to plug existing GNNs into the VIB-GSL framework to enhance their performances. \u2022 Extensive experiment results in graph classification and graph denoising demonstrate that the proposed VIB-GSL enjoys superior effectiveness and robustness compared to other strong baselines.\n\n2 Background and Problem Formulation"}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 254926784, "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning", "paper_id": "0c0300f53c01ae609c97395c98de4c9d85d92876", "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric \u2013 Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.", "intro": "With the advances in large-scale pre-trained language models (PLMs), recent studies have explored various efficient learning paradigms (Brown et al., 2020;Liu et al., 2021;Wei et al., 2021;Xie et al., 2021) to generalize PLMs to new tasks without task-specific tuning. Among these, instruction * Zhiyang Xu and Ying Shen contributed equally to this work. 1 The dataset, source code, and model checkpoints are publicly available at https://github.com/VT-NLP/ MultiInstruct. tuning (Wei et al., 2021) has achieved significant success in zero-shot learning on natural language processing tasks. By fine-tuning a PLM on tasks described through instructions, instruction tuning allows the model to learn to understand and follow the instructions to perform predictions on unseen tasks. Recent advancement in multimodal pretraining (Wang et al., 2022a;Alayrac et al., 2022;Bao et al., 2022;Wang et al., 2022c) has shown the potential of jointly interpreting text and images in a shared semantic space, which further leads us to ask: can the instruction tuning be leveraged to improve the generalizability of Vision-Language pretrained models on multi-modal and vision tasks?\n\nIn this work, we propose MULTIINSTRUCT, the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories, including Visual Question Answering (Goyal et al., 2017;Suhr et al., 2017), Commonsense Reasoning (Zellers et al., 2019;Xie et al., 2019), Visual Relationship Understanding (Krishna et al., 2017) and so on. We equipped each task with 5 instructions that are written by two experts in natural language processing. As shown in Figure 1, we formulate all the tasks into a unified sequence-to-sequence format in which the input text, images, instructions, and bounding boxes are represented in the same token space.\n\nWe use OFA (Wang et al., 2022a) 2 , a unified model that is pre-trained on a diverse set of multimodal and unimodal tasks in a single Transformerbased sequence-to-sequence framework, as the base pre-trained multimodal language model, and fine-tune it on MULTIINSTRUCT. To utilize NATU-RAL INSTRUCTIONS (Mishra et al., 2022), a largescale text-only instruction tuning dataset, we further explore two transfer learning strategies, in-Grounded Caption"}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 258352455, "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "paper_id": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "intro": "to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. However, systematic collaboration approaches, including Visual ChatGPT [Wu et al., 2023], MM-REACT [Yang et al., 2023], and HuggingGPT [Shen et al., 2023], are designed to facilitate the coordination of various vision models or tools to express visual information with text descriptions. However, these approaches may not be able to comprehend specific multimodal instructions due to their lack of alignment with different modalities. Additionally, these approaches may encounter challenges related to inference efficiency and cost. End-to-end models, such as BLIP-2 , LLaVA [Liu et al., 2023], and MiniGPT-4 [Zhu et al., 2023a] aim to use unified models to support different modalities. However, these models have some limitations as they take frozen visual models, which may lead to inadequate alignment due to the limited number of parameters. Moreover, they cannot unlock various abilities due to missing unimodal and multimodal instruction.\n\nIn this paper, we present mPLUG-Owl with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently, drawing inspiration from the concept of modularization [Xu et al., 2023b, Xu et al., 2021, Ye et al., 2022. Our method harnesses the power of pre-trained LLM, visual knowledge module, and connected visual abstractor module to achieve effective alignment between images and text, and utilizes a twostage training scheme to stimulate impressive unimodal and multimodal abilities. Our approach even enhances the strong generation abilities of LLM by modality collaboration between modalities. In the first step, we align the image and text to acquire comprehensive visual knowledge using textimage pairs, which is accomplished by training the visual knowledge module and abstractor module with the frozen LLM module. Subsequently, we fine-tune mPLUG-Owl with language-only and multi-modal instructions to unlock a range of unimodal and multimodal abilities. We freeze the visual knowledge module and train low-rank adaption (LoRA) [Hu et al., 2022] on LLM and visual abstractor module jointly. This approach allows for the effective integration of textual and visual information, facilitating the development of versatile and robust cognitive abilities.\n\nOur experiments on a carefully-built visually related instruction evaluation set OwlEval shows that mPLUG-Owl outperforms existing models such as MiniGPT-4 [Zhu et al., 2023a] and LLaVA [Liu et al., 2023]. We separately verifies mPLUG-Owl's remarkable abilities in instruction understanding, visual understanding, knowledge transfer, and multi-turn dialogue. Abundant ablation study is performed to show the effectiveness of our training paradigm. Furthermore, we find some unexpected emerging ability such as multi-image correlation, multilingual conversation and scene text understanding.\n\nOur main contributions can be highlighted as follows:\n\n\u2022 We propose mPLUG-Owl, a novel training paradigm for large language models through modularization.\n\n\u2022 We carefully construct an instruction evaluation set, dubbed OwlEval, to assess the capabilities of different models in the context of visual-related tasks.\n\n\u2022 Experimental results demonstrate that mPLUG-Owl excels in multi-modal instruction understanding and multi-turn dialogue, surpassing the performance of existing models.\n\n2 Related Work"}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 258615266, "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "paper_id": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.", "intro": "A longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that can solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction tuning [46,7] proves to be a promising approach toward that goal. By finetuning a large language model (LLM) on a wide range of tasks described by natural language instructions, instruction tuning enables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been leveraged for vision-language tasks. For example, BLIP-2 [20] effectively adapts frozen instructiontuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in image-to-text generation.\n\nCompared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual inputs from various domains. This poses a greater challenge to a unified model that is supposed to generalize to diverse vision-language tasks, many unseen during training. Most previous work can be grouped into two approaches. The first approach, multitask learning [6,27], formulates various vision-language tasks into the same input-output format. However, we empirically find multitask learning without instructions (Table 4) does not generalize well to unseen datasets and tasks. The\n\nWhat could have happened based on the current scene?\n\nBased on the current scene in the image, it is possible that a hurricane or severe weather event caused significant damage to the buildings and infrastructure in the area. The person walking through the debris-covered area suggests that they are inspecting the damage caused by the disaster. The presence of palm trees indicates that the location may be in a tropical or sub-tropical region, which can be more susceptible to hurricanes and other natural disasters.\n\nIntroduce me this painting in detail.\n\nThe painting depicts a young girl with a pearl earring, also known as \"The Girl with the Pearl Earring\" or \"Girl with a Pearl Earring\". She is wearing a blue dress and has a pearl earring on her left ear. Her hair is pulled back into a bun, and she is looking directly at the viewer. This painting is a famous work of art by Dutch painter Johannes Vermeer, created in 1665. It is considered one of his masterpieces and is widely recognized for its intricate details and realistic portrayal of the young girl's facial expression.\n\nCan yon tell me about this image in detail?\n\nThe image depicts a collection of various vegetables including carrots, cucumbers, tomatoes, and nuts arranged on a stone surface. There are several jars filled with different types of ingredients, such as peanuts, cashews, sunflower seeds, and pumpkin seeds. These ingredients are likely to be part of a healthy meal or snack.\n\nCan you give me an example of such meal?\n\nAn example of a healthy meal or snack that incorporates some of the ingredients in the image could be a salad made with mixed greens, carrots, cucumbers, tomatoes, and nuts. The salad can be dressed with a homemade vinaigrette made with olive oil, lemon juice, salt, and pepper. Additionally, the salad can be topped with pumpkin seeds or sunflower seeds for added crunch and nutritional value.\n\nSounds good! Show me steps of making this salad.\n\nSure! Here are the steps to make this healthy salad: 1. Wash and chop the mixed greens, carrots, cucumbers, and tomatoes into bite-sized pieces. 2. In a small bowl, whisk together olive oil, lemon juice, salt, and pepper to create a homemade vinaigrette dressing. 3. Place the chopped vegetables in a large bowl and toss them with the vinaigrette dressing. 4. Sprinkle pumpkin seeds or sunflower seeds on top of the salad for added crunch and nutritional value. 5. Serve the salad immediately or store it in the refrigerator until ready to enjoy.\n\nCan you describe this image in detail? The image depicts a person standing in front of an open doorway, looking out into space. They are wearing a jacket and appears to be contemplating the vastness of the universe beyond the doorway. In the background, there are multiple stars visible, creating a stunning celestial scene. The person's gaze is directed towards the center of the image, where a bright star can be seen shining brightly. There are also smaller stars scattered throughout the scene, adding to the overall cosmic atmosphere.\n\nWhat if they jump over the doorway?\n\nIf the person were to jump over the doorway, they would be leaving the confines of their current location and venturing into the vastness of space. This could be a metaphorical representation of stepping out of one's comfort zone or embarking on a new adventure. However, it is important to note that such an action would be extremely dangerous and potentially lifethreatening due to the harsh conditions of space.\n\nWhat makes this dog special?\n\nThe dog in the image is wearing futuristic armor and has a glowing green eye, giving it a unique and futuristic appearance. This combination of elements makes the dog stand out from other dogs and adds an element of fantasy or sci-fi to the image. Additionally, the dog's posture and facial expression convey a sense of confidence and determination, further emphasizing its unique features. second approach [20,4] extends a pre-trained LLM with additional visual components, and trains the visual components with image caption data. Nevertheless, such data are too limited to allow broad generalization to vision-language tasks that require more than visual descriptions.\n\nTo address the aforementioned challenges, this paper presents InstructBLIP, a vision-language instruction tuning framework that enables general-purpose models to solve a wide range of visuallanguage tasks through a unified natural language interface. InstructBLIP uses a diverse set of instruction data to train a multimodal LLM. Specifically, we initialize training with a pre-trained BLIP-2 model consisting of an image encoder, an LLM, and a Query Transformer (Q-Former) to bridge the two. During instruction tuning, we finetune the Q-Former while keeping the image encoder and LLM frozen. Our paper makes the following key contributions:\n\n\u2022 We perform a comprehensive and systematic study on vision-language instruction tuning. We transform 26 datasets into the instruction tuning format and group them into 11 task categories. We use 13 held-in datasets for instruction tuning and 13 held-out datasets for zero-shot evaluation. Moreover, we withhold four entire task categories for zero-shot evaluation at the task level. Exhaustive quantitative and qualitative results demonstrate the effectiveness of InstructBLIP on vision-language zero-shot generalization.\n\n\u2022 We propose instruction-aware visual feature extraction, a novel mechanism that enables flexible and informative feature extraction according to the given instructions. Specifically, the textual instruction is given not only to the frozen LLM, but also to the Q-Former, so that it can extract instruction-aware visual features from the frozen image encoder. Also, we propose a balanced sampling strategy to synchronize learning progress across datasets.\n\n\u2022 We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1) FlanT5 [7], an encoder-decoder LLM finetuned from T5 [34]; 2) Vicuna [2], a decoder-only LLM finetuned from LLaMA [41]. The InstructBLIP models achieve state-of-the-art zero-shot performance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead to state-of-the-art finetuning performance when used as the model initialization on individual downstream tasks."}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 259095896, "title": "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning", "paper_id": "6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8", "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.", "intro": "There has been a continuously increasing trend to develop intelligent assistants that can follow human instructions [3,36,37]. In the natural language processing (NLP) field, instruction tuning [35,53] is a success paradigm that leverages large-scale well-formatted instances to align large language models (LLMs) to human instructions. By finetuning on instances with specific task descriptions, LLMs learn to follow the instruction to perform various tasks, and demonstrate strong generalization ability on unseen tasks [29]. Expanding beyond NLP, a general-purpose intelligent agent must encompass various modalities, such as vision, prompting recent efforts to investigate instruction tuning in vision-language domains [63,28,7]. To develop powerful vision-language models (VLMs), it is essential to have a well-constructed dataset that encompasses diverse vision-language tasks and aligns with human instructions. However, the instructional data supporting existing VLMs is either not publicly available (e.g.,  or offers limited task and language coverage (e.g., only tasks in English are considered). This scarcity of comprehensive datasets has impeded the progress of open vision-language models, highlighting the importance of multi-modal instruction tuning and the need for high-quality datasets.\n\nIn this paper, we aim to advance instruction tuning research in the multi-modal domain by introducing an open dataset M 3 IT, a Multi-Modal Multilingual Instruction Tuning dataset, as an essential step towards building a versatile general-purpose assistant. We build this dataset by converting existing datasets into a unified vision-to-text schema with four stages: (1) manual instruction writing, (2) dataset pre-processing, (3) careful quality check and (4) dataset translation for key tasks. Our dataset encompasses a wide range of tasks, including classic image-text tasks such as image classification, visual question answering, and image captioning. Video-related tasks, such as video questionanswering, are also incorporated to ensure comprehensive coverage across multiple modalities. We further integrate Chinese vision-language datasets with corresponding Chinese instructions. The resulting dataset compiles 40 diverse tasks and 400 instructions. Finally, key vision-language tasks are translated into 80 languages with a strong translation system, to support multilingual studies.\n\nTo evaluate the effectiveness of the proposed dataset, we develop a vision-language model, Ying-VLM, by integrating a strong vision encoder, BLIP-2 [23] with a large language model, Ziya-13B [61], derived from LLaMA [49]. Building on the successful approach of incorporating visual tokens as textual prompts in LLMs [7,63,28], we employ a two-stage training process: (1) the initial stage aligns vision features with text embeddings through image captioning on LAION400M [41], and (2) the second stage enhances the model by conducting instruction tuning on selected tasks of our dataset. Experimental results reveal that Ying-VLM surpasses strong baseline models in knowledgeable VQA tasks and exhibits improved generalization performance to unseen video and cross-lingual tasks. Further analysis indicates that the improved performance corresponds to increased tasks for instruction tuning, while the diversity of instructions also affects outcomes. This paper presents two key contributions: (1) We introduce the open-source, large-scale Multimodal, multilingual Instruction Tuning (M 3 IT) dataset, designed to enable the development of general-purpose multi-modal agents. (2) We develop Ying-VLM, a visual assistant that excels in knowledgeable VQA tasks, demonstrates strong generalization to unseen video QA and Chinese multi-modal tasks, and offers valuable insights for future research."}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 259165040, "title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", "paper_id": "a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9", "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena", "intro": "Large Language Models (LLMs), such as LLaMA [1], GPT-3 [2], and Vicuna [3], have demonstrated remarkable progress in Natural Language Processing (NLP). These models leverage large-scale pretraining data and huge networks to achieve impressive results in NLP benchmarks. Recently, GPT-4 [4] further expanded the impact to the multimodal community, stimulating the rapid development of Large Vision-Language Models (LVLMs) and revolutionizing the landscape of artificial intelligence.\n\nLarge Vision-Language Models (LVLM) have achieved remarkable progress in multimodal visionlanguage learning for various multimodal tasks such as visual question answering and multimodal conversation. Specifically, LVLMs capitalize on the knowledge from LLMs and effectively align visual features with the textual space. Flamingo [5], a pioneering LVLM, integrates visual features into LLMs through cross-attention layers. Later studies proposed more efficient vision-text interactions [6], more efficient training methods [7,8], and employing instruction tuning [9,7,[10][11][12][13]8].\n\nHowever, despite the great success, few efforts have been made to provide systematic evaluations of LVLMs. But evaluation plays a critical role in understanding the strengths and weaknesses of LVLMs, thereby guiding their future development. Recent work [14] presents a systematic investigation of object hallucination of LVLMs by proposing a polling-based object probing evaluation method. Moreover, ImageNetVC [15] studies how well LVLMs can master visual commonsense knowledge. Liu et al. [16] comprehensively evaluate the performance of LVLMs in visual recognition with text recognition, such as optical character recognition. GVT [17] evaluates LVLM's visual semantic understanding and fine-grained perception capabilities. Nevertheless, these studies only evaluate a portion of LVLMs on specific tasks, lacking an overall understanding of LVLM's capabilities.\n\nIn pursuit of a comprehensive evaluation of LVLMs, we build an LVLM Evaluation hub (LVLM-eHub) consolidating 8 representative LVLMs such as InstrucBLIP [13] and MiniGPT-4 [10]. The details about model configuration and training data are listed in Table 1. Our LVLM-eHub consists of a quantitative capability evaluation and an online arena platform, providing a thorough investigation of the selected LVLMs. Specifically, the quantitative capability evaluation extensively evaluates 6 categories of multimodal capabilities of LVLMs including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence (see Fig. 1 (a)), by collecting 47 standard text-related visual benchmarks. On the other hand, the online arena platform features anonymous randomized pairwise battles in a crowd-sourced manner, providing a user-level model ranking in the open-world question-answering scenario (see Fig. 1 (b & c)).\n\nOur LVLM-eHub comprehensively evaluates LVLMs, revealing several innovative findings. (1) Instruction-tuned LVLM with massive in-domain data suffers from overfitting problem and generalizes poorly in open-world scenarios , such as InstructBLIP (see Fig. 1 (a)). (2) With moderate instructionfollowing data, instruction-tuned LVLM may cause object hallucination issues, generating objects that are inconsistent with target images in the descriptions. This leads to incorrect answers or renders current evaluation metrics, such as CIDEr for image captioning, ineffective. (3) We find that a multi-turn reasoning evaluation pipeline can mitigate the issue of object hallucination, indicating that developing an effective pipeline for LVLM evaluation is urgent.\n\nThe contributions of our work are summarized follows. (1) We propose LVLM-eHub which is the first comprehensive evaluation benchmark for large vision-language models, to our best knowledge.   Table 1: Comparison between Different LVLMs. 'VE', 'Adapt', 'ToP', 'TuP', and '# Token' represent the visual encoder, adaption module, number of total parameters, tuning parameters, and visual tokens fed into the LLM, respectively. \u2020 indicates that the model is frozen. CC * consists of COCO [18], CC3M [19], and CC12M [20]. CC, VG, CY, L400, and LC indicate Conceptual Caption [19,20], Visual Genome [21], COYO-700M [22], LAION 400M [23] and LAION COCO [24], respectively. SBU [25] contains 1 million images with captions. LLaVA-I represents 158K multimodal instruction-following data in LLaVA [9]. QA * denotes 13 question-answering datasets in InstructBLIP [13]. We count all the data and tuning parameters needed to convert the pretrained vision model and LLM into a visual instruction model. evaluation results reveal several innovative findings, providing a foundational framework for the assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques."}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 263860779, "title": "Aligning Large Multi-Modal Model with Robust Instruction Tuning", "paper_id": "84dc889beff9d51fe429cff8c92735e7410ee3c2", "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction . Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation . To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE) , a novel approach to evaluate visual instruction tuning without the need for human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate that existing LMMs exhibit significant hallucination when presented with our negative instructions, particularly with Existent Element Manipulation instructions. Moreover, by finetuning MiniGPT4 on LRV-Instruction , we successfully mitigate hallucination while improving performance on public datasets using less training data compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Our project link is available at this link.", "intro": "Significant progress has been made in the field of natural language processing, leading to the development of models that can comprehend and follow instructions given natural language inputs [36; 10; 26; 5]. These models harness the power of large language models (LLM) and rely on high-quality instruction data. Similarly, efforts have been made to introduce similar capabilities to multi-modal models. GPT4 [27] has demonstrated impressive performance in multi-modal conversations with humans, yet the techniques contributing to its extraordinary capabilities remain opaque. As a result, several large multi-modal models (LMMs) have recently emerged [39; 23; 10; 8], such as MiniGPT4 [39] and LLaVA [23], both utilize the Vicuna [7] as the language generator but with different vision encoders [28; 16]. InstructBLIP [8] is initialized from a pre-trained BLIP-2 [15] while Multimodal-GPT (MMGPT) [10] is built on Flamingo [1; 3].\n\nA recent study [14] revealed that the hallucination issue of LLM, although not desired, is inherited by these LMMs [39; 23; 10; 8]. Hallucination, a major ethical concern associated with LLMs [4], can lead to harmful consequences, especially when users without adequate domain knowledge over-rely on these increasingly convincing language models. In the context of LMM hallucinations, the model There is no mention of a dog or a ball in the given image. Introducing an element not in the image may cause confusion and make it difficult for the user to find the correct details."}
{"tabids": ["199fe872-0610-48fc-a187-4394add0d91f"], "corpus_id": 260887670, "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", "paper_id": "d6c2523ab97416c2692cbbeab082ed1790e8e55e", "abstract": "We introduce VisIT-Bench ( Vis ual I ns T ruction Bench mark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 \u201cinstruction families\u201d that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model\u2019s response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/ .", "intro": "A long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For textual tasks, several recent works [2,3,4,5,6,7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8,9,10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, butperhaps more-so -for creative, useful, and real-world queries that downstream users of language technologies are likely to make.\n\nOn the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., \"how many apples are in this image?\" or \"What is this?\" or \"Write a poem in the style of Robert Frost about this scene.\") a textual response is provided. Recent works like OpenFlamingo [11,12], LLaVA [13] and others [14,15,16,17,18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20] are commonly used to assess performance, less is know about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.\n\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual InsTruction Benchmark), a dynamic benchmark consisting of 592 challenging visionlanguage instructions. Each instance contains an instruction, input image(s), a instructionconditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 1). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks \"how can I cook this in a healthy way?\". Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., \"write a poem...\" or \"what should I bring if I were to visit here?\").\n\nWe created VisIT-Bench to cover a wide array of \"instruction families\". Our starting point was a set of 70 \"wish-list\" tasks such as \"home renovation\" and \"gardening tips\" collected by the authors: 1 each requiring varied high-level skills from recognition to complex reasoning ( Figure 2). We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA) [21] and robust change captioning [22] into a chatbot-style format (this reformatting differs from prior work [14,17,13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\n\nWe started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions -unlike prior work [23,24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.\n\nWe conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench ( \u00a74). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals significant differences between models (e.g., that LLaVA-13b [13] is generally preferred to Panda [18]), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25,26],"}
{"tabids": ["01fcc1e8-bb38-4360-851a-3ba035d3b355"], "corpus_id": 8945673, "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases", "paper_id": "05e882679d61f4c64a68ebe21826251a39f87e98", "abstract": "The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely\"ChestX-ray8\", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based\"reading chest X-rays\"(i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC", "intro": "The rapid and tremendous progress has been evidenced in a range of computer vision problems via deep learning and large-scale annotated image datasets [26,38,13,28]. Drastically improved quantitative performances in object recognition, detection and segmentation are demonstrated in Figure 1. Eight common thoracic diseases observed in chest X-rays that validate a challenging task of fully-automated diagnosis.\n\ncomparison to previous shallow methodologies built upon hand-crafted image features. Deep neural network representations further make the joint language and vision learning tasks more feasible to solve, in image captioning [49,24,33,48,23], visual question answering [2,46,51,55] and knowledge-guided transfer learning [4,34], and so on. However, the intriguing and strongly observable performance gaps of the current state-of-the-art object detection and segmentation methods, evaluated between using PASCAL VOC [13] and employing Microsoft (MS) COCO [28], demonstrate that there is still significant room for performance improvement when underlying challenges (represented by different datasets) become greater. For example, MS COCO is composed of 80 object categories from 200k images, with 1.2M instances (350k are people) where every instance is segmented and many instances are small objects. Comparing to PASCAL VOC of only 20 classes and 11,530 images containing 27,450 annotated objects with bounding-boxes (BBox), the top competing object detection approaches achieve in 0.413 in MS COCO versus 0.884 in PASCAL VOC under mean Average Precision (mAP).\n\nDeep learning yields similar rises in performance in the medical image analysis domain for object (often human anatomical or pathological structures in radiology imaging) detection and segmentation tasks. Recent notable work includes (but do not limit to) an overview review on the future promise of deep learning [14] and a collection of important medical applications on lymph node and interstitial lung disease detection and classification [37,43]; cerebral microbleed detection [11]; pulmonary nodule detection in CT images [40]; automated pancreas segmentation [36]; cell image segmentation and tracking [35], predicting spinal radiological scores [21] and extensions of multi-modal imaging segmentation [30,16]. The main limitation is that all proposed methods are evaluated on some small-to-middle scale problems of (at most) several hundred patients. It remains unclear how well the current deep learning techniques will scale up to tens of thousands of patient studies.\n\nIn the era of deep learning in computer vision, research efforts on building various annotated image datasets [38,13,28,2,33,55,23,25] with different characteristics play indispensably important roles on the better definition of the forthcoming problems, challenges and subsequently possible technological progresses. Particularly, here we focus on the relationship and joint learning of image (chest Xrays) and text (X-ray reports). The previous representative image caption generation work [49,24] utilize Flickr8K, Flickr30K [53] and MS COCO [28] datasets that hold 8,000, 31,000 and 123,000 images respectively and every image is annotated by five sentences via Amazon Mechanical Turk (AMT). The text generally describes annotator's attention of objects and activity occurring on an image in a straightforward manner. Region-level ImageNet pre-trained convolutional neural networks (CNN) based detectors are used to parse an input image and output a list of attributes or \"visually-grounded high-level concepts\" (including objects, actions, scenes and so on) in [24,51]. Visual question answering (VQA) requires more detailed parsing and complex reasoning on the image contents to answer the paired natural language questions. A new dataset containing 250k natural images, 760k questions and 10M text answers [2] is provided to address this new challenge. Additionally, databases such as \"Flickr30k Entities\" [33], \"Visual7W\" [55] and \"Visual Genome\" [25,23] (as detailed as 94,000 images and 4,100,000 region-grounded captions) are introduced to construct and learn the spatially-dense and increasingly difficult semantic links between textual descriptions and image regions through the object-level grounding.\n\nThough one could argue that the high-level analogy exists between image caption generation, visual question answering and imaging based disease diagnosis [42,41], there are three factors making truly large-scale medical image based diagnosis (e.g., involving tens of thousands of patients) tremendously more formidable. 1, Generic, openended image-level anatomy and pathology labels cannot be obtained through crowd-sourcing, such as AMT, which is prohibitively implausible for non-medically trained annota-tors. Therefore we exploit to mine the per-image (possibly multiple) common thoracic pathology labels from the image-attached chest X-ray radiological reports using Natural Language Processing (NLP) techniques. Radiologists tend to write more abstract and complex logical reasoning sentences than the plain describing texts in [53,28]. 2, The spatial dimensions of an chest X-ray are usually 2000\u00d73000 pixels. Local pathological image regions can show hugely varying sizes or extents but often very small comparing to the full image scale. Fig. 1 shows eight illustrative examples and the actual pathological findings are often significantly smaller (thus harder to detect). Fully dense annotation of region-level bounding boxes (for grounding the pathological findings) would normally be needed in computer vision datasets [33,55,25] but may be completely nonviable for the time being. Consequently, we formulate and verify a weakly-supervised multi-label image classification and disease localization framework to address this difficulty. 3, So far, all image captioning and VQA techniques in computer vision strongly depend on the ImageNet pre-trained deep CNN models which already perform very well in a large number of object classes and serves a good baseline for further model fine-tuning. However, this situation does not apply to the medical image diagnosis domain. Thus we have to learn the deep image recognition and localization models while constructing the weakly-labeled medical image database.\n\nTo tackle these issues, we propose a new chest X-ray database, namely \"ChestX-ray8\", which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined eight common disease labels, mined from the text radiological reports via NLP techniques. In particular, we demonstrate that these commonly occurred thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization formulation. Our initial quantitative results are promising. However developing fully-automated deep learning based \"reading chest X-rays\" systems is still an arduous journey to be exploited. Details of accessing the ChestX-ray8 dataset can be found via the website 1 ."}
{"tabids": ["01fcc1e8-bb38-4360-851a-3ba035d3b355"], "corpus_id": 53759905, "title": "fastMRI: An Open Dataset and Benchmarks for Accelerated MRI", "paper_id": "65d3e7fa7b452611aa729ac789d4373d6ee505b0", "abstract": "Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.", "intro": "The excellent soft tissue contrast and flexibility of magnetic resonance imaging (MRI) makes it a very powerful diagnostic tool for a wide range of disorders, including neurological, musculoskeletal, and oncological diseases. However, the long acquisition time in MRI, which can easily exceed 30 minutes, leads to low patient throughput, problems with patient comfort and compliance, artifacts from patient motion, and high exam costs.\n\nAs a consequence, increasing imaging speed has been a major ongoing research goal since the advent of MRI in the 1970s. Increases in imaging speed have been achieved through both hardware developments (such as improved magnetic field gradients) and software advances (such as new pulse sequences). One noteworthy development in this context is parallel imaging, introduced in the 1990s, which allows multiple data points to be sampled simultaneously, rather than in a traditional sequential order [39,26,9].\n\nThe introduction of compressed sensing (CS) in 2006 [2,23] promised another breakthrough in the reduction of MR scan time. At their core, CS techniques speed up the MR acquisition by acquiring less measurement data than has previously been required to reconstruct diagnostic quality images. Since undersampling of this kind violates the Nyquist-Shannon sampling theorem, aliasing 2 Introduction to MR Image Acquisition and Reconstruction MR imaging is an indirect process, whereby cross-sectional images of the subject's anatomy are produced from frequency and phase measurements instead of direct, spatially-resolved measurements. A measuring instrument, known as a receiver coil, is placed in proximity to the area to be imaged ( Figure 1). During imaging, a sequence of spatially-and temporally-varying magnetic fields, called a \"pulse sequence,\" is applied by the MRI machine. This induces the body to emit resonant electromagnetic response fields which are measured by the receiver coil. The measurements typically correspond to points along a prescribed path through the multidimensional Fourier-space representation of an imaged body. This Fourier space is known as k-space in the medical imaging community. In the most basic usage of MR imaging, the full Fourier-space representation of a region is captured by a sequence of samples that tile the space up to a specified maximum frequency. The spatially-resolved image m can be estimated from the full k-space y by performing an inverse multidimensional Fourier transform:\nm = F \u22121 (y),(1)\nwherem is a noise-corrupted estimate of the true image m.\n\nThe number of samples captured in k-space is a limiting factor for the speed of MR imaging. Fewer samples can be captured by sampling up to a lower maximum frequency, however this produces images of lower spatial resolution. An alternative undersampling approach involves omitting some number of k-space samples within a given maximum frequency range, which then results in aliasing artifacts. In order to remove these artifacts and infer the true underlying spatial structure of the imaged subject, one may apply a number of possible reconstruction strategies."}
{"tabids": ["01fcc1e8-bb38-4360-851a-3ba035d3b355"], "corpus_id": 218865062, "title": "PTB-XL, a large publicly available electrocardiography dataset", "paper_id": "f4b5dfa199eecc0a60b82886b53472531fe6dfa9", "abstract": "Electrocardiography (ECG) is a key non-invasive diagnostic tool for cardiovascular diseases which is increasingly supported by algorithms based on machine learning. Major obstacles for the development of automatic ECG interpretation algorithms are both the lack of public datasets and well-defined benchmarking procedures to allow comparison s of different algorithms. To address these issues, we put forward PTB-XL, the to-date largest freely accessible clinical 12-lead ECG-waveform dataset comprising 21837 records from 18885 patients of 10\u2009seconds length. The ECG-waveform data was annotated by up to two cardiologists as a multi-label dataset, where diagnostic labels were further aggregated into super and subclasses. The dataset covers a broad range of diagnostic classes including, in particular, a large fraction of healthy records. The combination with additional metadata on demographics, additional diagnostic statements, diagnosis likelihoods, manually annotated signal properties as well as suggested folds for splitting training and test sets turns the dataset into a rich resource for the development and the evaluation of automatic ECG interpretation algorithms.", "intro": "Cardiovascular diseases are the leading cause of mortality worldwide, which is in high-income countries only surpassed by cancer 1 . Electrocardiography (ECG) provides a key non-invasive diagnostic tool for assessing the cardiac clinical status of a patient. Advanced decision support systems based on automatic ECG interpretation algorithms promise significant assistance for the medical personnel due to the large number of ECGs that are routinely taken. However, there are at least two major obstacles that restrict the progress in this field beyond the demonstration of exceptional performance of closed-source algorithms on custom datasets with restricted access 2,3 , (1) the lack of large publicly available datasets for training and validation 4 , and (2) the lack of well-defined evaluation procedures for these algorithms. We aim to address both issues and to close this gap in the research landscape by putting forward PTB-XL 5 , a clinical ECG dataset of unprecedented size along with proposed folds for the evaluation of machine learning algorithms.\n\nThe raw signal data underlying the PTB-XL dataset was recorded by devices from the Schiller AG between October 1989 and June 1996. The transfer of the raw data into a structured database, its curation along with the development of corresponding ECG analysis algorithms was a long term project at the Physikalisch Technische Bundesanstalt (PTB). These efforts resulted in a number of publications [6][7][8][9][10][11] , but the access to the dataset remained restricted until now. The dataset comprises 21837 clinical 12-lead ECG records of 10 seconds length from 18885 patients. The dataset is balanced with respect to sex (52% male and 48% female) and covers the whole range of ages from 0 to 95 years (median 62 and interquantile range of 22). The ECG records were annotated by up to two cardiologists with potentially multiple ECG statements out of a set of 71 different statements conforming to the SCP-ECG standard 12 . The statements cover form, rhythm and diagnostic statements in a unified, machine-readable form. For the diagnostic labels we provide a hierarchical organization in terms of 5 coarse superclasses and 24 subclasses for the diagnostic labels, see Fig. 1 for a graphical summary of the dataset, that allow for different levels of granularity. Besides annotations in the form of ECG statements along with likelihood information for diagnostic statements, additional metadata for example in the form of manually annotated signal quality statements are available.\n\nApart from the outstanding nominal size of PTB-XL, the dataset is distinguished by its diversity, both in terms of signal quality (with 77.01% of highest signal quality) but also in terms of a rich coverage of pathologies, many different co-occurring diseases but also a large proportion of healthy control samples that is rarely found in clinical datasets. It is in particular this diversity, which makes PTB-XL a rich source for the training and evaluation of algorithms in a real-world setting, where machine learning (ML) algorithms have to work reliably regardless of the recording conditions or potentially poor quality data.\n\nTo highlight the uniqueness of the PTB-XL dataset, we compare different commonly used ECG datasets in Table 1 based on sample statistics (number of ECG signals, number of recorded leads, number of patients, average recording length in seconds) and their respective annotations ((D)iagnostic, (F)orm, (R)hytm, (C)linical, (B)eat annotation and the respective number of classes). Most open datasets are provided by PhysioNet 13 , but typically cover only a few hundred patients. Most notably, this includes the PTB Diagnostic ECG Database 6 , which was collected during the course of the same long-term project at the PTB, which, however, shares no records with the PTB-XL dataset. The PTB Diagnostic ECG Database includes only 549 records from a single site and provides only a single label per record as opposed to multi-label, machine-readable annotations covering a much broader range of pathologies in PTB-XL. The only exceptions in terms of freely accessible datasets with larger samples sizes are the AF classification dataset 14 and the Chinese ICBEB Challenge 2018 dataset 15 , which contain, however, either just single-lead ECGs or cover only a very limited set of ECG statements. There are several larger datasets that are either commercial or where the access is restricted by certain conditions (top five rows in Table 1). This includes commercial datasets such as CSE 16 , which has traditionally been used to benchmark ECG interpretation algorithms."}
{"tabids": ["01fcc1e8-bb38-4360-851a-3ba035d3b355"], "corpus_id": 33285731, "title": "MIMIC-III, a freely accessible critical care database", "paper_id": "95cd83603a0d2b6918a8e34a5637a8f382da96f5", "abstract": "MIMIC-III (\u2018Medical Information Mart for Intensive Care\u2019) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.", "intro": "In recent years there has been a concerted move towards the adoption of digital health record systems in hospitals. In the US, for example, the number of non-federal acute care hospitals with basic digital systems increased from 9.4 to 75.5% over the 7 year period between 2008 and 2014 (ref. 1).\n\nDespite this advance, interoperability of digital systems remains an open issue, leading to challenges in data integration. As a result, the potential that hospital data offers in terms of understanding and improving care is yet to be fully realized. In parallel, the scientific research community is increasingly coming under criticism for the lack of reproducibility of studies 2 .\n\nHere we report the release of the MIMIC-III database, an update to the widely-used MIMIC-II database (Data Citation 1). MIMIC-III integrates deidentified, comprehensive clinical data of patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts, and makes it widely accessible to researchers internationally under a data use agreement (Fig. 1). The open nature of the data allows clinical studies to be reproduced and improved in ways that would not otherwise be possible.\n\nBased on our experience with the previous major release of MIMIC (MIMIC-II, released in 2010) we anticipate MIMIC-III to be widely used internationally in areas such as academic and industrial research, quality improvement initiatives, and higher education coursework.\n\nTo recognize the increasingly broad usage of MIMIC, we have renamed the full title of the database from 'Multiparameter Intelligent Monitoring in Intensive Care' to 'Medical Information Mart for Intensive Care'. The MIMIC-III critical care database is unique and notable for the following reasons:\n\n\u2022 it is the only freely accessible critical care database of its kind; \u2022 the dataset spans more than a decade, with detailed information about individual patient care; \u2022 analysis is unrestricted once a data use agreement is accepted, enabling clinical research and education around the world.  6) and the median length of a hospital stay is 6.9 days (Q1-Q3: 4.1-11.9). A mean of 4579 charted observations ('chartevents') and 380 laboratory measurements ('labevents') are available for each hospital admission. Table 1 provides a breakdown of the adult population by care unit."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 14093453, "title": "Multi-behavioral Sequential Prediction with Recurrent Log-bilinear Model", "paper_id": "2ac113d5cc36d2adaea0148c6a568f2951d5fc15", "abstract": "With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role in improving various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are based on a strong independence assumption. As two classical neural network methods for modeling sequences, recurrent neural networks cannot well model short-term contexts, and the log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent Log-BiLinear (RLBL) model. It can model multiple types of behaviors in historical sequences with behavior-specific transition matrices. RLBL applies a recurrent structure for modeling long-term contexts. It models several items in each hidden layer and employs position-specific transition matrices for modeling short-term contexts. Moreover, considering continuous time difference in behavioral history is a key factor for dynamic prediction, we further extend RLBL and replace position-specific transition matrices with time-specific transition matrices, and accordingly propose a Time-Aware Recurrent Log-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL model and TA-RLBL model yield significant improvements over the competitive compared methods on three datasets, i.e., Movielens-1M dataset, Global Terrorism Database and Tmall dataset with different numbers of behavior types.", "intro": "N OWADAYS, Collaborative Filtering (CF) [14] plays an important role in a large number of applications, e.g., recommender systems, information retrieval and social network analysis. Conventional CF methods focus on modeling users preference based on their historical choices of items and always ignore the sequential information. It is reasonable to assume that user preferences change with his or her behavioral sequence. Meanwhile, rather than with merely one type of behaviors, e.g., purchasing in e-commerce and clicking on websites, there are many sequential scenarios with multiple types of behaviors towards items, e.g., clicking, purchasing, adding to favorites in e-commerce and downloading, using, uninstalling in app usage. Accordingly, it is necessary to model multi-behavioral sequences and collaboratively predict what a user will prefer next under a specific behavior. For instance, multiple types of behaviors, i.e., posting, sharing and commenting, on social media has been separately modeled and studied recently, which makes great contribution to user interest detection [47]. Besides ecommerce and other Internet applications, multi-behavioral sequential prediction can be implemented for social good, such as predicting security events in a specific area [21] [41] or predicting air quality [48]. Nowadays, some efforts have been put into developing CF methods with sequential information [3] [21] [33] [40] [46]. To the best of our knowledge, none of existing methods are designed for modeling sequences with multiple types of behaviors. And if we directly treat different behaviors towards one item as different elements in sequences, or simply ignore the differences among behaviors, conventional methods will have difficulty in revealing the correlations among behaviors and items. As shown in the example of app usage in Figure 1, different behaviors reveal users' different attitudes towards apps. Downloading and using means you may like the app, while uninstalling means you do not like the app and similar ones should not be recommended. So, it is essential to find a proper way to reveal the correlations among behaviors and items.\n\nMoreover, existing methods still have their own limitations even for single-behavioral sequences. Markov Chain (MC) based models [44] [33] [30] have become the most popular methods for sequential prediction. MC based models aim to predict the users' next behavior based on the past behaviors. A transition matrix is estimated, which can give the probability of an action based on the previous ones. However, a major problem of MC based models is that all the components are independently combined, indicating that it makes strong independence assumption among multiple factors [40].\n\nRecently, Recurrent Neural Networks (RNN) have been successfully employed to model temporal dependency for different applications, such as sentence modeling tasks [23] [24] [25], video modeling [8], sequential click prediction [46] and location prediction [21]. When modeling the sequential data, RNN assumes that the temporal dependency changes monotonously along with positions in a sequence. This Taking app usage prediction as an example of multi-behavioral sequential prediction. This example shows a user's behaviors towards apps in an hour, including downloading, using and uninstalling. We can predict what app the user is going to download or use next. means that, one element, e.g., a word, a frame and a product, in a sequence usually has more significant effect than the previous one for prediction. Such rules may well model words in a sentence or frames in a video, since adjacent words or frames have significant correlation. The larger the distance between two words or two frames, the smaller the correlation. However, for behavior prediction tasks, this assumption does not confirm to complex real situations, especially for the most recent elements in historical sequences. Sometimes, several most recent elements have similar effects on users' next behavior. For instance, if you went to a gym, a restaurant and a shopping market yesterday morning, afternoon and evening respectively, these three behaviors may have similar effects on your behaviors today. Sometimes, most recent elements have more complex effects on the future. For instance, going to a gym yesterday has dominant effects on how you exercise today, and what you ate at a restaurant yesterday or what you bought at a shopping market yesterday can affect what you want to eat today a lot. There is no guarantee that one element has more or less significant effect than the previous one. The effects of most recent elements in modeling human behaviors are much more complicated than that in modeling sentences or videos. But RNN can only tell us that behaviors in yesterday morning have more significant effects than behaviors in yesterday afternoon, and behaviors in yesterday afternoon have more significant effects than behaviors in yesterday evening. Accordingly, we can say that, RNN cannot well model short-term contexts in a sequence.\n\nDifferent from the recurrent architecture in RNN based language models [23] [24] [25], the Log-BiLinear (LBL) model [27] represents each word in a sentence, i.e., each position in a sequence, with a specific matrix. It can better model the complex situations of local contexts in sequences. But when the sequence is too long, a maximal length is usually set. And in real behavior prediction scenarios, length of behavioral sequences is usually not fixed. So, LBL cannot well model long-term contexts in a sequence. Furthermore, time difference between input elements, e.g., continuous time difference between apps that the user has behaviors on in Figure 1, is another key factor in sequential modeling. However, to our best knowledge, none of existing models, including above MC based methods, RNN and LBL, can jointly model sequential information and time difference information in one framework.\n\nIn this paper, to overcome above shortcomings of conventional methods and model multi-behavioral sequences, we propose two novel sequential prediction methods, i.e., Recurrent Log-BiLinear (RLBL) model and Time-Aware Recurrent Log-BiLinear (TA-RLBL) model. First, to capture the properties of different types of behaviors in historical sequences, we employ behavior-specific transition matrices in our model. To the best of our knowledge, this is the first work which is designed for predicting multi-behavioral sequences Second, we design RLBL model as a recurrent architecture to capture long-term contexts in sequences. It models several elements in each hidden layer and uses position-specific transition matrices to capture short-term contexts of the historical sequence. Our RLBL not only can model the subtle characteristics of the most recent items in a sequence, but also can deal with long-term contexts with a recurrent structure. Third, we further extend the RLBL model based on time difference information, and propose the TA-RLBL model. Rather than specific matrices for each position in RLBL, we use specific matrices, i.e., time-specific transition matrices, for each time difference value between input elements in TA-RLBL. Since it is difficult to estimate matrices for all the continuous time difference values, we divide all the possible temporal values into discrete bins. For a specific time difference value in one time bin, we can calculate the corresponding transition matrix via a linear interpolation of transition matrices of the upper bound and lower bound. Incorporating continuous time difference information, TA-RLBL can further improve the performance of RLBL.\n\nThe main contributions of this work are listed as follows:\n\n\u2022 We firstly address the problem of multi-behavioral sequential prediction, which is a significant problem in sequential prediction. And we use behaviorspecific matrices to represent the effects of different types of behaviors."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 32370905, "title": "Micro Behaviors: A New Perspective in E-commerce Recommender Systems", "paper_id": "590e75fdbb7006a7918bab1dfc8d2da881aea899", "abstract": "The explosive popularity of e-commerce sites has reshaped users\u00bb shopping habits and an increasing number of users prefer to spend more time shopping online. This evolution allows e-commerce sites to observe rich data about users. The majority of traditional recommender systems have focused on the macro interactions between users and items, i.e., the purchase history of a customer. However, within each macro interaction between a user and an item, the user actually performs a sequence of micro behaviors, which indicate how the user locates the item, what activities the user conducts on the item (e.g., reading the comments, carting, and ordering) and how long the user stays with the item. Such micro behaviors offer fine-grained and deep understandings about users and provide tremendous opportunities to advance recommender systems in e-commerce. However, exploiting micro behaviors for recommendations is rather limited, which motivates us to investigate e-commerce recommendations from a micro-behavior perspective in this paper. Particularly, we uncover the effects of micro behaviors on recommendations and propose an interpretable Recommendation framework RIB, which models inherently the sequence of mIcro Behaviors and their effects. Experimental results on datasets from a real e-commence site demonstrate the effectiveness of the proposed framework and the importance of micro behaviors for recommendations.", "intro": "The modern e-commerce sites such as Amazon 1 and eBay 2 offer hundreds of millions of products for sale. For example, as on June 20th, 2017, Amazon has more than 372 million products 3 . It has become increasingly challenging for consumers to find their interested items. Recommender systems play a crucial role in mitigating this information overload problem by suggesting products that have potentials to fit consumers' needs. They have been proven to not only help increase customer satisfaction and create customer loyalty [34] but also boost many aspects of e-commerce services such as revenue and growth [24].\n\nMeanwhile, the popularity of e-commerce sites has reshaped users' shopping habits and users prefer to spend more time shopping online. For example, on average, American parents spend 7 hours per week on e-commerce sites 4 . This evolution enables ecommerce sites to observe rich data about their users. Figure 1 illustrates a real example of observed data on a user from an ecommerce site in a short period. The user first enters a page of iPhone 7 from searching result page. She reads the detailed description, as well as others' comments and adds it to the cart. Then she shifts to a page of iPhone 6 from the searching result page and reads the comments. After that, she browses a page of iPhone 7 cases from the sale page and orders the case. Finally, she jumps to a page of Samsung Galaxy from the home page of the e-commerce site. From a macro perspective as shown in the top subfigure in Figure 1, the user interacted with iPhone 7, iPhone 6, iPhone 7 cases and Samsung Galaxy. While from a micro perspective as shown in the bottom subfigure, each macro interaction includes a sequence of behaviors that can indicate how the user located the product page (e.g., the search engine or the sale promotion), whether the user clicks detailed information about a product (e.g., comments, or specifications), whether a user carts or orders a product, and how long the user dwells on a product. In this work, we refer these behaviors in macro interactions as micro-behaviors. These micro behaviors can provide fine-grained understandings about users. For example, locating a product from searching could indicate stronger intents than from the e-commerce homepage; and longer dwell time on a product suggests more interests in the product than shorter dwell time. Hence, exploiting micro behaviors has immense potential to advance recommender systems. However, such research is rather limited in the literature.\n\nIn this paper, we investigate e-commerce recommendations from the micro-behavior perspective where (a) data of a user is inherently viewed as a sequence of macro interactions between users and items; and (b) each macro interaction between a user and an item includes a sequence of micro behaviors as shown in Figure 1. One major advantage of the new perspective is -it provides a unified setting that makes various settings of existing recommender systems become its special cases:\n\n\u2022 if we completely ignore the sequential information of macro interactions and their micro-behaviors, macro interactions can be denoted as an user-item matrix, which is the typically setting for traditional collaborative filtering [29]; \u2022 when we completely ignore the sequential information of macro interactions but consider certain micro behaviors in macro interactions, our setting is boiled down to traditional collaborative filtering with implicit feedback from certain micro behaviors such clicks [12] and dwell time [35]; and \u2022 when we only consider the sequential information of macro interactions in a session and ignore micro-behaviors within macro interactions, the studied problem become sessionbased recommendations [14,15,31].\n\nOn the other hand, it also poses tremendous challenges including (1) how to model sequential information and (2) how to capture effects from a variety of micro behaviors. Solutions to these two challenges lead to a novel recommendation framework. Our major contributions are summarizas follows:\n\n\u2022 We uncover the effects of micro behaviors on e-commerce recommendations; \u2022 We provide a principled approach to capture the sequence of various micro behaviors mathematically; \u2022 We propose an interpretable Recommendation framework from the mIcro Behavior perspective RIB, which incorporates the sequence of micro behaviors and their corresponding effects into a coherent model; and\n\n\u2022 We demonstrate the effectiveness of the proposed framework and the importance of micro behaviors on data from a real e-commerce site. The rest of this paper is organized as follows. In Section 2, we formally define the problem. We perform preliminary data analysis on micro behaviors in Section 3. In Section 4, we detail the proposed framework RIB. Experimental results with discussions are presented in Section 5. In Section 6, we briefly review related work. Finally we conclude our work and discuss the future work in Section 7."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 50768534, "title": "Learning from History and Present: Next-item Recommendation via Discriminatively Exploiting User Behaviors", "paper_id": "34d1a65bdef695380900d92da531fa10b6fbf0de", "abstract": "In the modern e-commerce, the behaviors of customers contain rich information, e.g., consumption habits, the dynamics of preferences. Recently, session-based recommendations are becoming popular to explore the temporal characteristics of customers' interactive behaviors. However, existing works mainly exploit the short-term behaviors without fully taking the customers' long-term stable preferences and evolutions into account. In this paper, we propose a novel Behavior-Intensive Neural Network (BINN) for next-item recommendation by incorporating both users' historical stable preferences and present consumption motivations. Specifically, BINN contains two main components, i.e., Neural Item Embedding, and Discriminative Behaviors Learning. Firstly, a novel item embedding method based on user interactions is developed for obtaining an unified representation for each item. Then, with the embedded items and the interactive behaviors over item sequences, BINN discriminatively learns the historical preferences and present motivations of the target users. Thus, BINN could better perform recommendations of the next items for the target users. Finally, for evaluating the performances of BINN, we conduct extensive experiments on two real-world datasets, i.e., Tianchi and JD. The experimental results clearly demonstrate the effectiveness of BINN compared with several state-of-the-art methods.", "intro": "Recommender system, as an essential component of modern ecommerce websites, tries to predict what the most suitable products or services are of users, based on the users' preferences [30]. With the mechanism development of e-commerce, a massive amount of customer interactions (e.g., browse, click, collect, cart, purchase) have been logged, which imply luxuriant consumption patterns. These information-rich logs provide opportunities for understanding customers' historical stable preferences and also their present consumption motivations, which may further contribute to smarter recommendations.\n\nAlong this line, there is a particular interest in understanding interactive behaviors of customers. Existing works can be concluded into two main paradigms. The first paradigm is the general recommenders. These works focus on mining the static relevancy between users and items from interactions, which are represented by the traditional collaborative filtering models [18,22,40]. For example, Zhang et al. made recommendations through a factorization model with different item semantic representations from the knowledge base [40]. However, most of these works have taken user-item specific relationships into consideration from the static views but neglect the dynamics and evolutions of users' preferences implied in sequential interactions. The other paradigm is recommending next items based on sequential pattern mining [34,39] or transition modeling [29,42]. Along this line, researchers recently show more interest in an e-commerce scenario where user profiles are invisible so that recommender systems are developed based on the user interactions in short sessions [15,19,27]. These session-based models have provided the comprehension about users' decision-making process in a short term, but the dynamics of preferences [38] and how to perfectly integrate both the historical stable preferences with present consumption motivations are still largely unexplored.\n\nActually, as a user's interactive behaviors naturally form a behavioral sequence over time, the user's historical preferences from the long-term view and present motivations or demands from the short-term view can be dynamically revealed. For instance, Figure 1 illustrates a typical online shopping scenario. The user's historical interactions imply that this user might be a \"Star Wars\" fan since that the user has bought or collected various spin-off products of the \"Star Wars\". Moreover, we infer that this user would like to buy dark T-shirts because a black shirt is included in the personal cart and the user has browsed many short sleeve shirts. However, following the general collaborative filtering approaches, another spin-off product may be recommended since all the preference behaviors of the user's entire history are exploited in the static manners as shown in the blue chart of Figure 1. By contraries, if we only consider the current session behaviors of this user in accordance with what the session-based models do, another similar or popular shirt would be recommended as shown in the green chart of Figure 1. Actually, by exploiting both the historical stable preferences and present consumption motivations of this user, more attention should be paid to short sleeve shirts and the \"Star Wars\" graphic T-shirts perfectly match user's tastes. Therefore, we can conclude that a smarter recommender system should not only consider users' historical stable preferences but also take into account the present consumption motivations by discriminatively exploiting different terms or types of user behaviors.\n\nBased on the intuition and observations, we propose a novel solution framework called Behavior-Intensive Neural Network (BINN) to address the next-item recommendation problem. Our BINN framework contains two main components: Neural Item Embedding and Discriminative Behaviors Learning. Specially, we propose a novel neural item embedding method to obtain a unified item representation space for learning latent vectors which could capture the sequential similarities between items. Different from the traditional item embedding methods which are based on inherent features such as item images or text descriptions, our neural item embedding method generates item representations by means of exploiting users' collaborative sequential interactions over items directly. Then, with the item embedded, we design two different behavior alignments, i.e., Session Behaviors Learning and Preference Behaviors Learning to respectively model users' present consumption motivations and historical stable preferences by discriminatively exploring interactive behaviors of users. Specific to the alignments, we respectively develop two deep neural network architectures to jointly learn the session behaviors and preference behaviors. Finally, by matching the potentially preferred items in the latent space, BINN generates recommendations for the target users. For evaluating the performances of BINN, we conduct extensive experiments on two real-world datasets. The experimental results clearly demonstrate the effectiveness of BINN compared with several state-of-the-art methods. In summary, the main contributions of this study can be summarized as follows.\n\n\u2022 We propose to make item-recommendation by integrating both the historical preferences and present motivations of users, which are all learned from the users' interactive behaviors.\n\n\u2022 We propose a novel Behavior-Intensive Neural Network (BINN) which includes embedding items by users' interactions and discriminative behavior alignments accompanied by two applicable neural network architectures.\n\n\u2022 We conduct extensive experiments on two real-world datasets.\n\nThe results show that BINN model outperforms other state-ofthe-art methods from various aspects."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 51609715, "title": "Institutional Knowledge at Singapore Management University Modeling contemporaneous basket sequences with twin networks for next-item recommendation", "paper_id": "b214539c77dd0d233f0bcd8d7f5ba40117c0e60d", "abstract": "Our interactions with an application frequently leave a heterogeneous and contemporaneous trail of actions and adoptions (e.g., clicks, bookmarks, pur-chases). Given a sequence of a particular type (e.g., purchases)\u2013 referred to as the target sequence, we seek to predict the next item expected to appear beyond this sequence. This task is known as next-item recommendation. We hypothesize two means for improvement. First, within each time step, a user may interact with multiple items (a basket), with potential latent associations among them. Second, predicting the next item in the target sequence may be helped by also learning from another support-ing sequence (e.g., clicks). We develop three twin network structures modeling the generation of both target and support basket sequences. One based on \u201cSiamese networks\u201d facilitates full sharing of parameters between the two sequence types. The other two based on \u201cfraternal networks\u201d facilitate partial sharing of parameters. Experiments on real-world datasets show signi\ufb01cant improvements upon baselines relying on one sequence type.", "intro": "In this era of digitization, most of our needs and wants are but a screen away. We shop at marketplaces such as Amazon or Alibaba; order meals from Uber Eats, stream music over Spotify or Pandora; get our screen time fix from Netflix or YouTube; etc. Consequently, some of us knowingly, while others unwittingly, are leaving our digital footprints, tracing the pages or activities where we have been. Some services, such as Foursquare, might in some cases even be able to approximate literal (walking) footprints based on check-ins.\n\nImportantly, these traces from the past may well contain prescient signals of where we are headed in the future, in terms of our adoptions or consumptions. Hence, an important problem of wide interest and implication in both industry and academia is that of next-item recommendation. Based on historical data of consumers' activities, we would like to predict a new item that a consumer will likely adopt next.\n\nBroadly, there are several main directions in the literature. One direction is collaborative filtering, whereby recommen-dations are driven by users' personalized preferences [Koren et al., 2009]. Another direction is content-based recommendation, whereby recommendations are driven by similarity in content among items [Pazzani and Billsus, 2007]. Yet another direction is sequential preference, whereby recommendations are driven by latent dependencies between the next item and other items that a user has adopted at previous occasions.\n\nProblem. We focus on sequential preference. There are scenarios where future actions are influenced by past actions. For one example, music streaming services are interested in generating coherent playlists, which requires paying attention to sequential transitions between songs [Chen et al., 2012]. For another, the topics that Tweeters post tend to exhibit a sequential nature [Li et al., 2016]. So is recommending courses, where there are progression over time and precedence relationships [Parameswaran et al., 2010]. As each user is commonly associated with a sequence, the essence of this paradigm is learning sequentiality among items across users' sequences, rather than personalization per se.\n\nRecent works on sequential recommendation are based on Recurrent Neural Networks (RNN) [Lipton et al., 2015] (see Section 2). However, direct application of RNN to sequential recommendation suffers from two major limitations in modeling choices. First, it models a sequence of one type of actions (e.g., only purchases). Second, it assumes that at each time step, there is only one action (e.g., one item purchased). However, these assumptions may not bear out in some scenarios. For one, there are multiple types of actions resulting from user interaction with a system. In an online marketplace, a user may click on various items under consideration, abandon most, add some to a shopping cart, and puts others on a wish list, before an eventual purchase takes place. In a video streaming service, a user may watch some trailers, follow through to watch some shows fully, and later on may rate or review some movies, of which a few might be rated highly. In each case, we are dealing with multiple sequence types (e.g., sequence of clicks and sequence of purchases). Importantly, these sequence types are contemporaneous, occurring within a common period of time, and may well be capturing some related underlying behaviors. For instance, to predict what one would purchase, it may be instructive to pay attention to not only what a user has purchased previously, but also what she has clicked in the past. Therefore, we postulate the need for modeling these contemporaneous sequences jointly.\n\nFor another, we are not always dealing with a strict ordering of individual items. More frequently, we deal with groups or sessions, whereby there may be sequentiality from one session to another, but the ordering within a session may not be informative. For example, when planning travel, one day we may be searching for airfare, while on another day we may be booking accommodations. When grocery shopping, we may buy for different meal plans on different days. Though not necessarily sequentially ordered, items within a session are probably correlated to some degree, e.g., items of the same meal plan. We refer to such a group or session as \"basket\".\n\nApproach. To address those limitations, we propose to model contemporaneous basket sequences. In this work, we focus on a pair 1 of sequence types: target and support. The target sequence refers to high-quality, high-value, and possibly sparser interactions (e.g., purchases) for which we wish to predict the next interaction (e.g., next purchase). The support sequence refers to more frequent and informative interactions (e.g., clicks) that would be relevant for predicting the next target item. For example, if purchasing is the target, and clicking is the support, then we are predicting the next purchase by modeling sequence of purchases and sequence of clicks.\n\nWe explore dual-RNN structure to represent the two sequence types. Having been generated contemporaneously from the same ecosystem of interactions, the sequence types likely model related phenomena. Instead of two completely different RNN's, we base our Contemporaneous Basket Sequences or CBS framework on the concept of twin networks. Analogously to biological twins, they share some commonalities, but to different degrees in different cases. We develop three CBS architectures along the spectrum of commonalities. In all, the two sequence types share a basket encoder to capture in-basket associations among items. They vary in how much sharing occurs at the recurrent units. For CBS-SN (Siamese Networks), the sequence types share a recurrent encoder. For CBS-CFN (Concordant Fraternal Networks), they each have a different recurrent encoder with the same recurrent units. For CBS-DFN (Discordant Fraternal Networks), one sequence type has a recurrent encoder and the other does not, to model different scopes of sequential effects.\n\nContributions. As our first contribution, we hypothesize that modeling contemporaneous basket sequences could be beneficial for next-item recommendation due to synergies between the target and support sequences. As our second contribution, we develop three neural network architectures: CBS-SN, CBS-CFN and CBS-DFN, describe their design in Section 3, and note some learning details in Section 4.1. Our third contribution is to investigate research questions on the effectiveness of modeling contemporaneous basket sequences jointly on public datasets (see Section 4)."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 196171043, "title": "Buying or Browsing?: Predicting Real-time Purchasing Intent using Attention-based Deep Network with Multiple Behavior", "paper_id": "fb9ddc74bb9371bb0d6b31fe859ecd69e0fdb41d", "abstract": "E-commerce platforms are becoming a primary place for people to find, compare and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to predict user purchasing intent, which is an important part of user understanding and allows for providing better services for both sellers and customers. However, previous work cannot predict real-time user purchasing intent with a high accuracy, limited by the representation capability of traditional browse-interactive behavior adopted. In this paper, we propose a novel end-to-end deep network, named Deep Intent Prediction Network (DIPN), to predict real-time user purchasing intent. In particular, besides the traditional browse-interactive behavior, we collect a new type of user interactive behavior, called touch-interactive behavior, which can capture more fine-grained real-time user features. To combine these behavior effectively, we propose a hierarchical attention mechanism, where the bottom attention layer focuses on the inner parts of each behavior sequence while the top attention layer learns the inter-view relations between different behavior sequences. In addition, we propose to train DIPN with multi-task learning to better distinguish user behavior patterns. In the experiments conducted on a large-scale industrial dataset, DIPN significantly outperforms the baseline solutions. Notably, DIPN gains about 18.96% improvement on AUC than the state-of-the-art solution only using traditional browse-interactive behavior sequences. Moreover, DIPN has been deployed in the operational system of Taobao. Online A/B testing results with more than 12.9 millions of users reveal the potential of knowing users' real-time purchasing intent.", "intro": "In the internet era, large e-commerce platforms such as Taobao and Amazon are becoming a primary place for people to find, compare and ultimately purchase products. As an important part of user understanding, it is crucial to know whether a customer is buying or just browsing on an e-commerce application, as it allows for providing better services for both sellers and customers. From the perspective of the sellers, knowing users' current purchasing intent can increase their sales volume and profit margin. When the e-commerce platform has increased confidence that a subset of users are more likely to purchase, it can perform some proactive actions to maximize conversion based on this information. The platform may offer time-limited coupons or create bundles of complementary products to push the users to complete their purchases. From the perspective of the customers, recognizing users' current buying or browsing intent is vital for the e-commerce platform to set appropriate strategies for the recommendation system and search engine to improve user experience.\n\nPrevious studies focus on leveraging traditional user behavior, which we call browse-interactive behavior, to predict users' purchasing intent [15,18,19,23]. However, limited by the representation capability and frequency of occurrence of the browse-interactive behavior, e.g., browse, search or collect a product, it is hard to predict users' real-time purchasing intent depending solely on these actions. In other words, these actions contain insufficient information about user behavior patterns that would lead to a purchase in a short time. The purchase intent of a customer may slowly build over time and may not instantaneously lead to a purchase. As a result, it is challenging to identify the moment when the customer finally places the order. To this end, we need some more fine-grained behavior data to model user purchasing behavior patterns. In order to capture users' real-time purchasing intent, we collect a new type of interactive behavior data, which we call touchinteractive behavior. With the rapid development of hardware and software on mobile devices, we take advantage of the sensors and accelerometers of the mobile devices to automatically glean the real-time context of user interactions, such as the swipe and tap actions. Compared with the browse-interactive actions, the touchinteractive actions occur more frequently. As shown in Table 1, the number of swipe actions and tap actions generated per user per day are 37.7 times and 9.3 times more than that of the browseinteractive actions, respectively. As a result, the touch-interactive behavior contains more rich information about user behavior patterns. For example, we find that some customers would browse the product comments for a long time before they place the order. Such typical patterns can be easily captured by using the touchinteractive behavior. By combing the traditional browse-interactive behavior and the new touch-interactive behavior, we are able to model the user behavior patterns more comprehensively.\n\nHowever, there exist several challenges in predicting users' realtime purchasing intent. First, the touch-interactive behavior contains less semantic information than the browse-interactive behavior. Therefore, it is challenging to extract useful features from these data to improve the prediction performance. Second, it is necessary to figure out an effective fusion mechanism to combine the browseinteractive behavior and the touch-interactive behavior in order to bring their advantages into full play. Third, due to the complexity of the browsing behavior where the customers with different purchasing intent can appear to be very similar, it is essential to capture common features that can well depict the customers and unique features that would lead to different purchasing behavior.\n\nIn this paper, we propose a novel end-to-end deep network, named Deep Intent Prediction Network (DIPN), for the real-time purchasing intent prediction. In DIPN, the user behavior features are automatically learned from the raw data without the need of extensive feature engineering. In particular, we propose a hierarchical attention mechanism to fuse the views extracted from different interactive behavior sources. In the bottom attention layer, we design an intra-view attention mechanism which focuses on the inner parts of the behavior sequence. In the top attention layer, we propose an inter-view attention mechanism that learns the inter-view relations between different behavior sequences. In addition, we propose to train the real-time and long-term purchasing intent simultaneously with the same model. With the multi-task learning, DIPN can capture common features that well depict the customers and unique features that would lead to different purchasing behavior. The contribution of the paper can be summarized as follows:\n\n\u2022 We collect a new type of user behavior, the touch-interactive behavior, which contains rich information about user behavior patterns. Together with the traditional browse-interactive behavior, we are able to depict a user from different views for better performance of purchasing intent prediction. \u2022 We propose a deep network DIPN for real-time purchasing intent prediction. A novel hierarchical attention mechanism is proposed to fuse multiple views extracted from different interactive behavior sources. In addition, multi-task learning is introduced to better distinguish user behavior patterns. \u2022 We conduct extensive experiments to evaluate the performance of DIPN in both offline and online settings. Experimental results on a large-scale industrial dataset shows the superiority of DIPN in predicting purchasing intent. In particular, DIPN has been deployed in the operational system of Taobao and adopted in the coupon allocation task at a shopping festival. Online A/B testing shows the benefits of knowing users' real-time purchasing intent.\n\nThe rest of the paper is organized as follows. We discuss related work in Section 2, followed by the data description in Section 3. We describe the design of DIPN model in Section 4 and give an overview of the deployment of DIPN in Section 5. We present experiments in Section 6 and conclude the paper in Section 7."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 210883769, "title": "Hierarchical User Profiling for E-commerce Recommender Systems", "paper_id": "9ad8857c4f37bfe5f8b2e8517212b05b3e76eb82", "abstract": "Hierarchical user profiling that aims to model users' real-time interests in different granularity is an essential issue for personalized recommendations in E-commerce. On one hand, items (i.e. products) are usually organized hierarchically in categories, and correspondingly users' interests are naturally hierarchical on different granularity of items and categories. On the other hand, multiple granularity oriented recommendations become very popular in E-commerce sites, which require hierarchical user profiling in different granularity as well. In this paper, we propose HUP, a Hierarchical User Profiling framework to solve the hierarchical user profiling problem in E-commerce recommender systems. In HUP, we provide a Pyramid Recurrent Neural Networks, equipped with Behavior-LSTM to formulate users' hierarchical real-time interests at multiple scales. Furthermore, instead of simply utilizing users' item-level behaviors (e.g., ratings or clicks) in conventional methods, HUP harvests the sequential information of users' temporal finely-granular interactions (micro-behaviors, e.g., clicks on components of items like pictures or comments, browses with navigation of the search engines or recommendations) for modeling. Extensive experiments on two real-world E-commerce datasets demonstrate the significant performance gains of the HUP against state-of-the-art methods for the hierarchical user profiling and recommendation problems. We release the codes and datasets at https://github.com/guyulongcs/WSDM2020_HUP.", "intro": "In the era of Internet, recommender systems are playing crucial roles in various applications such as E-commerce portals (e.g. Amazon, JD.com, Alibaba), social networking websites like Facebook, video-sharing sites like Youtube, visual discovery sites like Pinterest and so on. In practice, User Profiling [5,11,18,24,33,38] is one of the most important phases in recommender systems. It yields profile vectors, which formally represent users' interests by deeply understanding their historical interactions, can be used for candidate generation [31,42], click-through rate prediction [4,39,40], conversion rate prediction [3,16] and long-term user engagement optimization [34][35][36][37][44][45][46].\n\nRecently, modeling users' hierarchical real-time interests is emerging to be a crucial issue in E-commerce recommender systems. Firstly, items (i.e. products) in E-commerce sites are typically organized in hierarchical catalogue. Correspondingly, users' interests naturally lie hierarchically on multiple granularity of items and categories. Secondly, different granularity of recommendations (e.g. item, topic and category) become very popular in E-commerce sites, and such scenarios require hierarchical user profiling in different  Figure 2: Hierarchical views of Micro behaviors of a user in JD.com granularity as well. For instance, Figure 1 illustrates a real example of hierarchical recommendations in Amazon. The left side of the figure recommends some items (mobile phones) to a user, while the right side shows a list of recommendations on the categories of \"phone accessories\", \"chargers\" and so on. Category recommendation can help the recommender systems quickly figure out the main interest of the user and make better recommendations.\n\nExisting user profiling methods mainly focus on item recommendations, usually based on users' item-level responses like ratings [20] or clicks [14]. Among existing methods, latent factor modeling is a popular branch, including matrix factorization [13,20,38], neural embedding [8,10], etc. Generally they learn a unified embedding for the target user to represent her interests on the items based on her historical behaviors. Recently, recurrent neural networks (RNN) have achieved state-of-the-art performance in session-based recommendations [14,29].\n\nExisting methods have the following limitations. First, when facing different granularity of recommendation tasks, most of them usually need to run a similar algorithm multiple times on different granularity of item organizations, where each run builds users' certain level profile vectors for the corresponding recommendation task, i.e., item-level profiles for item recommendations and categorylevel profiles for category recommendations. Correspondingly, the training process of each level's profile vectors is completely independent from the others. However, users' multiple-level interests are closely correlated. Figure 2 illustrates a user's hierarchical interests, including an item level and two category levels, with her historical behaviors. Resulting from the correlations between items and categories, improvement on one recommendation task might benefit others. However, to the best of our knowledge, such privilege has not been explored in existing methods.\n\nSecond, only harvesting the signals of users' item-level interactions like ratings and clicks is insufficient. In most of the Ecommerce portals, users provide finely-granular responses such as clicking and browsing different modules (e.g., comments and pictures) of items, adding to shopping carts and purchases, which are referred to as \"micro-behaviors\" [30,41]. For example, the bottom layer of Figure 2 presents a user's historical micro-behaviors in JD.com (one of the largest e-commerce site in the world), including browsing a pair of Nike shoes from the homepage, searching and reading specifications of iPhone 8, browsing Google Pixels 2 from the promoting page, searching iPhone X, reading comments and adding it into the shopping cart for purchasing, etc. Obviously, in comparison with users' item-level responses, micro-behaviors provide more detailed information, and preliminary studies [30,41] have demonstrated the advantage of modeling such detailed behaviors. However, to our best knowledge, none of existing methods has leveraged such advantages to improve the performance of multiplelevel user profiling.\n\nThird, generally users' interests are dynamic and continuously shifting. Some state-of-the-art methods like Time-LSTM [43] usually incorporate time intervals to track the interests shifting. However, we argue that besides the time intervals, the types of behaviors and their dwell time are also extremely important. As shown in Figure 2, we know that iPhone X is preferable to others, since various micro-behaviors are performed on iPhone X with long dwell time. We also observe that triggered by making an order on iPhone X, the user's interests on mobile phones drop sharply. Neglecting to model behavior types and dwell times, Time-LSTM would be in trouble to capture users' detailed preferences and interests shifting.\n\nTo cope with these challenges, we present HUP, a hierarchical user profiling framework to precisely formulate users' real-time interests on multiple organizations of items, targeting significant performance gains in recommendation accuracy. In particular, it models users' multiple-level interests with a Pyramid Recurrent Neural Networks, which typically consist of a micro layer, an item layer, and multiple category recurrent neural network layers. The micro layer harvests the detailed behavioral information and passes it to the higher layers, which could abstract users' hierarchical interests on the corresponding levels of the item organizations simultaneously. Furthermore, to sensitively track users' real-time interests, we introduce Behavior-LSTM in each layer, where a behavior gate is designed to model the types and dwell time of behaviors. Extensive experiments for item recommendation and category recommendation tasks have been conducted on two large-scale real e-commerce datasets to demonstrate the effectiveness of our proposed approach.\n\nTo sum up, our major contributions are listed as follows:\n\n\u2022 We formulate a novel hierarchical user profiling problem, which aims to precisely model users' multiple level interests simultaneously in E-commerce recommender systems. \u2022 We present HUP, which exploits a Pyramid Recurrent Neural Networks for hierarchical user profiling based on users' historical micro-behaviors. \u2022 We propose Behavior-LSTM, which utilizes a behavior gate to model the types and dwell time of behaviors for effectively formulating users' real-time interests. \u2022 We conduct extensive experiments and prove that our method outperforms state-of-the-art baselines greatly on both item recommendation and category recommendation tasks."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 235571722, "title": "Modeling Multiple Coexisting Category-Level Intentions for Next Item Recommendation", "paper_id": "f9bfe124c6512d2196e42298ca4459ec640c01b2", "abstract": "Purchase intentions have a great impact on future purchases and thus can be exploited for making recommendations. However, purchase intentions are typically complex and may change from time to time. Through empirical study with two e-commerce datasets, we observe that behaviors of multiple types can indicate user intentions and a user may have multiple coexisting category-level intentions that evolve over time. In this article, we propose a novel Intention-Aware Recommender System (IARS) which consists of four components for mining such complex intentions from user behaviors of multiple types. In the first component, we utilize several Recurrent Neural Networks (RNNs) and an attention layer to model diverse user intentions simultaneously and design two kinds of Multi-behavior GRU (MGRU) cells to deal with heterogeneous behaviors. To reveal user intentions, we carefully design three tasks that share representations from MGRUs. The next-item recommendation is the main task and leverages attention to select user intentions according to candidate items. The remaining two (item prediction and sequence comparison) are auxiliary tasks and can reveal user intentions. Extensive experiments on the two real-world datasets demonstrate the effectiveness of our models compared with several state-of-the-art recommendation methods in terms of hit ratio and NDCG.", "intro": "Recommender systems have become an essential component of online retail websites and aim to recommend items that a user will most likely to purchase in the future. With the fast development of e-commerce, these websites recorded a massive amount of heterogeneous interaction behaviors 23:2 Y. Xu et al. (e.g., view, cart, and purchase) between users and items. These interaction records provide opportunities for understanding user purchase intentions.\n\nAlong this line, recommendation with data of behaviors of multiple types (multi-behavior data for short) has attracted wide concerns. Existing studies leveraging multi-behavior data can be divided into two main categories. The first category is extending matrix factorization models. These studies aim to learn the relevance between users and items based on multiple interaction matrices [7,9,18,34]. However, these methods take user-item relationships into consideration from static views and neglect the evolution of users' intentions and the context of purchase behavior. The second kind of work recommends next items by mining sequential patterns or modeling dependency between items [17,35]. In these studies, researchers show more interest in mining the user intentions in short sessions for recommendation. But they model behaviors of different types independently and convert a sequence to a single latent vector which may fail to handle complex intentions.\n\nIn this article, we aim to exploit the multi-behavior data to mine user purchase intentions for the next-item recommendation. Behaviors are the ways (e.g., click, add-to-cart, purchase) that users interact with items on websites. Users usually have multiple kinds of behaviors (e.g., click many related items) before they finally buy a product. We find that behaviors of multiple types can indicate user intentions and users may have multiple coexisting purchase intentions on items of several categories through an empirical study (see details in Section 3). Intentions denote the purpose or goal of one purchase interaction. For example, a user wants to buy both a T-shirt and a pair of shorts (i.e., two kinds of intentions) when summer comes (see Figure 1). He may have click behaviors on products of the two categories alternately before he buys them. Therefore, users can have more than one intention simultaneously. If we can learn such latent complex user intentions from multi-behavior data with item category information and dynamically determine which intention is the most important for the purchase of next item, we will improve the recommendation performance greatly. For better modeling such multiple coexisting intentions based on interaction behaviors, our proposed model should satisfy the following several characteristics:\n\n\u2022 Exploiting Different Behaviors Discriminately. Behaviors of various types have different importance for indicating user intentions. Casual viewing behaviors may be generated with curiosity while purchasing behaviors usually confirm users' preference on items. Therefore, they should be exploited discriminately.\n\n\u2022 Reserving the Dependency among Behaviors of Diverse Types. On the one hand, the interaction data should be seen as sequences for retaining the dependency among items and evolving trends of user intentions. On the other hand, behaviors of different types should be seen as the context of adjacent behaviors rather than be utilized independently. \u2022 Modeling Multiples User Purchase Intentions Simultaneously. Previous research presents user intentions with a single latent vector which lacks the ability for modeling multiple intentions. Our approach should learn several intentions on item category levels simultaneously. Previous research usually mines user hidden intentions from interaction sequences of item IDs. But categories of items in interaction records can explicitly reflect user intentions which need to be further studied.\n\nTo achieve the above goals, we propose a novel neural network model named IARS to model user purchase intentions with the sequences of multiple kinds of behaviors. The model consists of four components, i.e., one RNN-based module and three task modules. The RNN-based module is used as one encoder to learn latent representations encoding user intentions from sequences of behaviors. It discriminately processes different behaviors using two proposed Multi-behavior Gated Recurrent Unit (MGRU) cells. In addition, for modeling users' multiple intentions, we utilize several RNNs to process behaviors simultaneously and adopt attention in recommendation to enable each RNN to learn different intentions. The other three modules are designed for solving diverse tasks based on the learned representations. The first task is to predict next item and category. The second task is to judge whether two sequences are of the same user and whether they are adjacent sequences. These two tasks are aimed to reveal user intentions from record levels and sequence levels, respectively. As far as we know, we are the first to propose sequence comparison tasks to improve recommendation performance. The third task is to estimate user preference over items which is the main task of our model. In this task, we utilize an attention layer to select user intentions according to candidate items.\n\nTo summarize, the main contributions of this article are as follows:\n\n\u2022 We conduct an empirical study on two real-world e-commerce datasets and observe that users have multiple coexisting category-level intentions in interaction sequences of diverse behaviors. \u2022 We propose a novel neural model named IARS to mine user complex intentions for recommendation. It includes several recurrent neural networks for learning latent multiple user intentions from behaviors and three diverse tasks. The various auxiliary behaviors are utilized to reveal user intentions before purchase behavior. Item category prediction task and sequence comparison task are proposed to further reveal user intentions. \u2022 We conduct experiments on the two real-world datasets. The results show that our model significantly outperforms other state-of-the-art methods from various aspects.\n\nThe remainder of this article is organized as follows: In Section 2, we first introduce related work. In Section 3, we analyze two real-world datasets and show several key observations. Next, we introduce our proposed models in Section 4. Then, we evaluate our models based on the two e-commerce datasets in Section 5. Finally, we conclude this article."}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 235324699, "title": "DeepRec: On-device Deep Learning for Privacy-Preserving Sequential Recommendation in Mobile Commerce", "paper_id": "4723b22b77cc1aa754645256f9779c944dfee842", "abstract": "Sequential recommendation techniques are considered to be a promising way of providing better user experience in mobile commerce by learning sequential interests within user historical interaction behaviors. However, the recently increasing focus on privacy concerns, such as the General Data Protection Regulation (GDPR), can significantly affect the deployment of state-of-the-art sequential recommendation techniques, because user behavior data are no longer allowed to be arbitrarily used without the user\u2019s explicit permission. To address the issue, this paper proposes DeepRec, an on-device deep learning framework of mining interaction behaviors for sequential recommendation without sending any raw data or intermediate results out of the device, preserving user privacy maximally. DeepRec constructs a global model using data collected before GDPR and fine-tunes a personal model continuously on individual mobile devices using data collected after GDPR. DeepRec employs the model pruning and embedding sparsity techniques to reduce the computation and network overhead, making the model training process practical on computation-constraint mobile devices. Evaluation results show that DeepRec can achieve comparable recommendation accuracy to existing centralized recommendation approaches with small computation overhead and up to 10x reduction in network overhead.", "intro": "With the rapid growth and widespread adoption of smartphones and tablet computers, mobile commerce [3] is becoming the dominant form of e-commerce during recent years. It is forecasted that mobile commerce will outpace non-mobile commerce in 2021, accounting for more than half of total e-commerce sales [2]. The special characteristic of user interactions on mobile devices, i.e., accessing the Internet at any time and any location, advances mobile commerce to provide even more personalized experience than traditional e-commerce by leveraging the temporal or spatial information of user interaction behaviors to precisely recommend items. Among these efforts, sequential recommendation [11,25,26,33,61] is one of the most promising approaches because the user interactions on mobile devices are usually fragmented and momentary [30] and sequential recommendation can effectively work on a small scale of fragmented data. Just like many tasks where deep neural networks significantly improve the performance [9,23,32], state-ofthe-art sequential recommenders are mostly based on deep learning techniques [14,57].\n\nHowever, the enforcement of policies and legislation formulations to regulate the usage of privacy data, such as the General Data Protection Regulation (GDPR) 1"}
{"tabids": ["ca93e9fc-b206-43af-957e-5f635558fde0"], "corpus_id": 247398476, "title": "MBN: Towards Multi-Behavior Sequence Modeling for Next Basket Recommendation", "paper_id": "44893ac563e04d1f58ff4d74bd4ab1fffecf732e", "abstract": "Next basket recommendation aims at predicting the next set of items that a user would likely purchase together, which plays an important role in e-commerce platforms. Unlike conventional item recommendation, the next basket recommendation focuses on capturing item correlations among baskets and learning the user\u2019s temporal interest from the past purchasing basket sequence. In practice, most users interact with items in various kinds of behaviors. The multi-behavior data sheds light on user\u2019s potential purchasing intention and resolves noisy signals from accidentally purchased items. In this article, we conduct an empirical study on real datasets to exploit the characteristics of multi-behavior data and confirm its positive effects on next basket recommendation. We develop a novel Multi-Behavior Network (MBN) model that captures item correlations and acquires meta-knowledge from multi-behavior basket sequences effectively. MBN employs the meta multi-behavior sequence encoder to model temporal dependencies of each individual behavior and extract meta-knowledge across different behaviors. Furthermore, we design the recurring-item-aware predictor in MBN to realize the high degree of the repeated occurrences of items, leading to better recommendation performance. We conduct extensive experiments to evaluate the performance of our proposed MBN model using real-world multi-behavior data. The results demonstrate the superior recommendation performance of MBN compared with various state-of-the-art methods.", "intro": "Recommender systems play an increasingly important role in filtering overwhelmed item information and providing personalized offers for users. In most real-world scenarios, users often purchase a basket of items during a visit to an e-commerce platform. Next basket recommendation (NBR), 81:2 Y. Shen et al.\n\nwhich aims at accurately predicting the set of items that a user would like to purchase in the next basket, is an essential task in recommender systems [39,46]. The NBR problem distinguishes itself from the conventional item recommendation in two aspects. First, NBR requires capturing item correlations among baskets. For example, items in the same basket can be correlated to each other (e.g., bread and milk) or dependent on the items purchased in the previous baskets. Second, NBR has to learn a user's sequential purchasing interest from previously purchasing basket sequence. However, item recommendation has focused on modeling static user and item characteristics and exploring their interaction mode [18]. To this end, the widely adopted collaborative filtering methods such as latent factor models [1,2,7,8,27,38] are thus inappropriate to recommend the next basket of items.\n\nA variety of NBR researches have emerged with success, which mainly include Markov Chain (MC)-based methods and Recurrent Neural Network (RNN) based methods. MC-based models [39,43], model user preference by the last K interaction(s) in a K-order MC, but ignore the other previous interactions. Recently, RNN has become a better choice in dealing with basket sequences, due to its impressive capability in modeling natural language sequences. RNN-based methods [4,16,29,30,34,46] leverage all the interactions in a long sequence of an anonymous user, capturing the user's temporal interaction patterns. These approaches have been verified to be effective in predicting the next purchasing basket, but they do not consider different behaviors.\n\nIn practice, users often interact with items in a variety of behaviors. In addition to purchasing items, a user may interact with items by clicking, adding to cart, tagging with favor, and so on. In fact, the temporal item interactions of a user with multiple behaviors can be naturally organized into separate basket sequences, as illustrated in Figure 1. It is important to notice that exploring the information from multi-behavior sequences is beneficial to the NBR for two reasons. First, a user would likely purchase an item after showing interest in it via clicking the item link or adding it to the cart. Hence, non-purchasing sequences shed light on user's potential purchasing intention, especially for the new items that have not been purchased before. Second, learning collective knowledge from multi-behavior sequences improves the understanding of user's true latent interest, reducing noises from accidentally purchased items. In this article, we focus on the following problem: given multiple basket sequences under distinct behaviors, how can we exploit multi-behavior knowledge to improve the accuracy of recommending next purchasing basket?\n\nIdeally, when modeling multi-behavior sequences, we would like to capture the latent and dynamic item correlations within and across sequences to disclose a user's purchasing habit, her general interest and the evolving pattern. While a line of researches [11-13, 23, 26, 35, 40, 42, 45, 47, 48] have studied the multi-behavior data, they focus on the typical recommendation task and treat multi-behavior data as auxiliary information to address the well-recognized data sparsity challenge. They are thus insufficient to solve the NBR problem due to the ignorance of temporal information and item correlations encoded in multi-behavior sequences. Another line of researches [15,28,32,33,37,49] developed RNN-based models over multi-behavior sequential data for next item or session-based recommendation. For example, BINN [32] uses a single RNN to deal with different behaviors mixedly. These approaches do not establish a direct connection among the past items interacted with different behaviors or explicitly modeling their relationship to the item set to be purchased next.\n\nTo tackle the problem, we conduct a comprehensive empirical study on real datasets to disclose the characteristics of multi-behavior sequences and comprehend the strengths and challenges of exploiting multi-behavior data for NBR. Based on our observations, we propose a novel Multi-Behavior Network (MBN) model that learns collective knowledge from multi-behavior sequences towards better recommendation results. Our model consists of three major components. The basket encoder is developed to capture item correlations within any basket in a sequence. The meta multi-behavior sequence encoder is used to learn a user's general interest and its evolving trend by taking into consideration the collective meta-knowledge extracted from multi-behavior sequences. We introduce two schemes (i.e., gathering and gathering-scattering) that model the dependencies across sequences in different manners. Furthermore, to realize the repeated occurrences of items in historical user-item interactions, we present the recurring-item-aware predictor based on the copy mechanism [14] where a mixed probabilistic function is established to predict the probability of each item in the next basket given the multi-behavior knowledge.\n\nThe major contributions of this article are summarized as follows:\n\n-We propose to exploit multi-behavior sequences for the NBR. An end-to-end neural approach is developed to learn collective behavioral knowledge from sequences towards better recommendation performance. -We conduct an empirical study to examine the characteristics of multi-behavior sequence data and provide several insights on how to model multi-behavior sequences for NBR. -We propose a novel neural network model named MBN following our insights. Our model consists of three components to compute basket representations, learn temporal meta-knowledge across multi-behavior sequences, and realize the recurring-item pattern, respectively. -We conduct extensive experiments on three real-world datasets. The results demonstrate the effectiveness of exploiting multi-behavior knowledge for NBR and the superior recommendation performance of our proposed MBN compared with various existing methods.\n\nThe remainder of this article is organized as follows. Section 3 provides the definitions and problem statement. Section 4 analyzes the characteristics of multi-behavior sequences. Section 5 presents our MBN model for the NBR. Section 6 demonstrates the experimental results. We conclude this article in Section 7.1."}
{"tabids": ["d5efb5c3-ff9c-4944-b60d-c70574c8ed7c"], "corpus_id": 59523656, "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "paper_id": "162cad5df347bdac469331df540440b320b5aa21", "abstract": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.", "intro": "Text classification is a fundamental task in natural language processing (NLP). Machine learning and deep learning have achieved high accuracy on tasks ranging from sentiment analysis (Tang et al., 2015) to topic classification (Tong and Koller, 2002), but high performance often depends on the size and quality of training data, which is often tedious to collect. Automatic data augmentation is commonly used in computer vision (Simard et al., 1998;Szegedy et al., 2014;Krizhevsky et al., 2017) and speech (Cui et al., 2015;Ko et al., 2015) and can help train more robust models, particularly when using smaller datasets. However, because it is challenging to come up with generalized rules for language transformation, universal data augmentation techniques in NLP have not been thoroughly explored.\n\nPrevious work has proposed some techniques for data augmentation in NLP. One popular study generated new data by translating sentences into French and back into English (Yu et al., 2018). Other work has used data noising as smoothing   (Xie et al., 2017) and predictive language models for synonym replacement (Kobayashi, 2018). Although these techniques are valid, they are not often used in practice because they have a high cost of implementation relative to performance gain.\n\nIn this paper, we present a simple set of universal data augmentation techniques for NLP called EDA (easy data augmentation). To the best of our knowledge, we are the first to comprehensively explore text editing techniques for data augmentation. We systematically evaluate EDA on five benchmark classification tasks, showing that EDA provides substantial improvements on all five tasks and is particularly helpful for smaller datasets. Code is publicly available at http://github. com/jasonwei20/eda_nlp."}
{"tabids": ["d5efb5c3-ff9c-4944-b60d-c70574c8ed7c"], "corpus_id": 15600925, "title": "Improving Neural Machine Translation Models with Monolingual Data", "paper_id": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.", "intro": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training.Target-side monolingual data plays an important role in boosting fluency for phrase-based statisti-\n\nThe research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp.z o.o.-Samsung R&D Institute Poland.cal machine translation, and we investigate the use of monolingual data for NMT.\n\nLanguage models trained on monolingual data have played a central role in statistical machine translation since the first IBM models (Brown et al., 1990).There are two major reasons for their importance.Firstly, word-based and phrase-based translation models make strong independence assumptions, with the probability of translation units estimated independently from context, and language models, by making different independence assumptions, can model how well these translation units fit together.Secondly, the amount of available monolingual data in the target language typically far exceeds the amount of parallel data, and models typically improve when trained on more data, or data more similar to the translation task.\n\nIn (attentional) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014;Bahdanau et al., 2015), the decoder is essentially an RNN language model that is also conditioned on source context, so the first rationale, adding a language model to compensate for the independence assumptions of the translation model, does not apply.However, the data argument is still valid in NMT, and we expect monolingual data to be especially helpful if parallel data is sparse, or a poor fit for the translation task, for instance because of a domain mismatch.\n\nIn contrast to previous work, which integrates a separately trained RNN language model into the NMT model (G\u00fcl\u00e7ehre et al., 2015), we explore strategies to include monolingual training data in the training process without changing the neural network architecture.This makes our approach applicable to different NMT architectures.\n\nThe main contributions of this paper are as follows:\n\n\u2022 we show that we can improve the machine translation quality of NMT systems by mixing monolingual target sentences into the training set.\n\n\u2022 we investigate two different methods to fill the source side of monolingual training instances: using a dummy source sentence, and using a source sentence obtained via backtranslation, which we call synthetic.We find that the latter is more effective.\n\n\u2022 we successfully adapt NMT models to a new domain by fine-tuning with either monolingual or parallel in-domain data."}
{"tabids": ["d5efb5c3-ff9c-4944-b60d-c70574c8ed7c"], "corpus_id": 249062776, "title": "Towards More Realistic Generation of Information-Seeking Conversations", "paper_id": "9f7f67870b6625084166aa7ed58200c75f5947d0", "abstract": "In this paper, we introduce a novel framework SimSeek ( sim ulating information- seek ing conversation from unlabeled documents) and compare two variants of it to provide a deeper perspective into the information-seeking behavior. We \ufb01rst introduce a strong simulator for information- sym metric conversation, SimSeek-sym, where questioner and answerer share all knowledge when conversing with one another. Although it simulates reasonable conversations, we take a further step toward more realistic information-seeking conversation. Hence, we propose SimSeek-asym that assumes information asym metry between two agents, which encourages the questioner to seek new information from an inaccessible document. In our experiments, we demonstrate that SimSeek-asym successfully generates information-seeking conversations for two downstream tasks, CQA and conversational search. In particular, SimSeek-asym improves baseline models by 1.1-1.9 F1 score in QuAC (Choi et al., 2018), and by 1.1 of MRR in OR-QuAC (Qu et al., 2020). Moreover, we thoroughly analyze our synthetic datasets to identify crucial factors for realistic information-seeking conversation.", "intro": "Conversational question answering (CQA) involves modeling the information-seeking process of humans' dialogue. In the task, systems are asked to answer context-dependent questions that need to be understood in conversational flow. It makes CQA complex since even the same word could be interpreted differently depending on the context, and almost infinite cases of conversational context can be given with the question. To build robust * Equal contribution. \u2020 Corresponding author Who was their coach? q 2 (a) Information-symmetric Conversation (b) Information-asymmetric Conversation Figure 1: Examples of two conversation scenarios. In the former, the questioner can access the evidence document, allowing them to ask less related information to the conversation (q 1 , q 2 in (a)). In the latter, questioners are encouraged to seek new information from the hidden document. Hence, information-seeking behaviors are frequently observed; open-ended, unanswerable, and \"Anything else?\" questions (q 1 , q 2 , q 3 in (b)) system that can handle innumerable cases, largescale CQA datasets Reddy et al., 2019;Saeidi et al., 2018;Penha et al., 2019;Campos et al., 2020;Feng et al., 2020) have recently been developed. Still, it is practically infeasible to cover most of the interactions in real-world scenarios, which motivates automated methods for generating realistic CQA datasets.\n\nHowever, generating realistic CQA is a more challenging task, which requires synthesizing multiple interdependent ingredients, e.g., conversation history, appropriate follow-up question, and accurate answer from grounding document. Most of the literature has discussed only sub-parts of the overall process. A line of research in conversational question generation (CQG) aims to generate humanlike follow-up questions upon conversational history Pan et al., 2019;Qi et al., 2020;Gu et al., 2021). Another line of research has greatly improved answer accuracy Qu et al., 2019b;Kim et al., 2021;Zhao et al., 2021). In other words, they are limited in assuming that all other ingredients (i.e., held-out conversations by humans and their gold answer) are provided. Thus, the prior approaches cannot construct whole conversations upon the unlabeled corpus and therefore have never shown a practical use of synthetic conversations.\n\nIn this paper, we delve into the problem of generating a realistic CQA dataset from unlabeled documents. We focus on information-asymmetric conversations where reference information is unequally distributed to two agents, encouraging more realistic conversation. As illustrated in Figure  1 (a), when questioners have excessive information, they often assume and ask for external knowledge less relevant to the conversation. On the other hand, information asymmetry drives them to seek new information in conversational style or sometimes fail to do so (q 1 , q 2 , and q 3 in Figure 1 (b)). We claim that simulating these information-seeking behaviors is an important step towards a more realistic generation of CQA.\n\nTo take a further step towards generating realistic CQA, we propose and contrast two novel frameworks, SimSeek (Simulating Information-Seeking conversation) that can generate synthetic conversation upon the unlabeled corpus, replicating each scenario. We first introduce a strong simulator for information-symmetric conversation (1) SimSeek-sym where CQG model generates context-dependent questions based on the answer candidates, which are automatically provided in advance by an extractive model. Although it succeeds in generating reasonable conversations, we propose a more realistic approach that designs informationasymmetry, (2) SimSeek-asym. In SimSeek-asym, the CQG component first asks questions without accessing any answer-containing document and target answer. Then, an answerer model predicts corresponding answers to the questions.\n\nTo demonstrate the effectiveness of SimSeek in a semi-supervised setup, we conduct experiments in one of the challenging CQA benchmarks, QuAC . To the best of our knowledge, it is the first successful adaptation of the synthetic datasets for the semi-supervised CQA. In the experiment, SimSeek-asym consistently improves backbone CQA models by 1.1-1.9 F1 score, outperforming other CQA generation baselines. Besides, our resulting dataset could also enhance dense retrieval models for the conversational search task. Our framework improves the baseline dense retriever, DPR (Karpukhin et al., 2020) on the conversational search benchmark, OR-QuAC (Qu et al., 2020), by 1.1 of MRR and 1.3 of R@5.\n\nTo provide a deeper perspective into the information-seeking behavior, we thoroughly analyze how the two frameworks synthesize the results differently. Following Qi et al. (2020), we quantify various properties, specificity, answer relevance, and informativeness of the synthetic datasets. We compare them with a human-annotated dataset on the metrics and find that information asymmetry makes conversations closer to the human's information-seeking behavior.\n\nOur main contributions are summarized as:\n\n\u2022 We propose a novel framework, SimSeek, which can generate synthetic informationseeking conversations from unlabeled documents.\n\n\u2022 To the best of our knowledge, we are the first to demonstrate the effectiveness of synthetic datasets in the semi-supervised CQA, achieving competitive performance with humans.\n\n\u2022 We provide insight into realistic informationseeking conversations by contrasting two proposed approaches that simulate each CQA scenario."}
{"tabids": ["d5efb5c3-ff9c-4944-b60d-c70574c8ed7c"], "corpus_id": 248863311, "title": "Dialog Inpainting: Turning Documents into Dialogs", "paper_id": "3bbf28bac4c4a150bc271bf17a03e2094def5f65", "abstract": "Many important questions (e.g.\"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains on standard evaluation metrics.", "intro": "Modern information-seeking tools such as web search and question answering (Karpukhin et al., 2020;Zhu et al., 2021) excel at questions that have well-defined answers (e.g., \"Where was Barack Obama born?\"). But many important questions are more open-ended-e.g., \"How to eat healthier?\"-and require conversation to elicit context and explore in depth: \"How do I eat more protein?\", \"What about vegetarians?\". Conversational question answering systems (ConvQA) (Stede & Schlangen, 2004;Radlinski & Craswell, 2017;Culpepper et al., 2018), would empower users to answer these questions as if they could discuss with an expert at any time.\n\nDespite this promising vision, progress has been stymied by scarce training data. While conversational data is abundant in online forums, much of it focuses on personal anecdotes and subjective opinions, and is thus unsuitable for an information-seeking system: we desire responses that minimize personal biases and cite reliable sources. Directly crowdsourcing dialogs is also hard: crowdworkers are rarely experts in the domain of interest and tend to overlook important questions or provide shallow answers (Li et al., 2021). It is also expensive: the largest extant datasets contain only about 10,000 conversations each (Choi et al., 2018;Reddy arXiv:2205.09073v2 [cs.CL] 31 May 2022et al., 2019Dinan et al., 2018;Saeidi et al., 2018;Campos et al., 2020;Feng et al., 2020;Anantha et al., 2021).\n\nOn the other hand, high-quality documents, such as those in Wikipedia or PubMed, are abundant. These documents are often edited or written by experts who have invested significant time streamlining their discourse and anticipating a reader's questions. What if we could rewrite these documents into dialogs between the writer and their imagined reader? This would yield an enormous corpus of information-seeking dialogs with attributable, expert answers that could then be used to train a ConvQA system. We aim to achieve this with dialog inpainting.\n\nTo transform any document into a dialog, our first observation is that we already know what the writer wants to discuss-that is reflected in the original text of the document. So, we pretend that the original document is the transcript of what the writer said to the reader in an imagined dialog. But we are still missing what the reader asked. This is like overhearing someone else's phone call: you hear one side, but not the other. Oftentimes, one can still guess what the other side was saying -we call this prediction task dialog inpainting, because we are \"inpainting\" the missing parts of the dialog that we did not hear (inspired by the term's usage in computer vision (Iizuka et al., 2017;Liu et al., 2018;Yu et al., 2018)). Drawing on this intuition, we train an inpainter model to predict missing utterances in a dialog, and use it to predict the unobserved questions in a document. By interleaving the generated questions and sentences from the document, we form a dialog ( Figure 1).\n\nWe apply our inpainter to passages from Wikipedia and the web, yielding WikiDialog and WebDialog, 1 two datasets totalling 19M+ dialogs -1,000x larger than the largest existing ConvQA dataset. When evaluated for conversationality and answer adequacy, we surprisingly find that our synthetically generated data is as good or better than previous crowd-sourced datasets (Section 3). Our generated dialogs inherit the good qualities of the professionally written documents we inpaint (topical diversity, coherent discourse, evidence-backed claims, etc.) without needing to train on dialog data of the same quality.\n\nImportantly, we find that our inpainted datasets are powerful sources of training data for ConvQA systems (Section 4). When used to pre-train standard retriever and reranker architectures, they advance state-of-the-art across three different ConvQA retrieval benchmarks (QRECC, OR-QUAC, TREC-CAST), delivering up to 40% relative gains on standard evaluation metrics (Section 5). Remarkably, we find that just pre-training on WikiDialog enables strong zero-shot retrieval performance-up to 95% of a finetuned retriever's performance-without using any in-domain ConvQA data."}
{"tabids": ["d5efb5c3-ff9c-4944-b60d-c70574c8ed7c"], "corpus_id": 259370708, "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models", "paper_id": "8d36390a430845849b62646a8c8be4c79f2b3d62", "abstract": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.", "intro": "In information-seeking conversations, users repeatedly ask questions based on their interests, and the dialogue system provides answers to fulfill their information needs (Stede and Schlangen, 2004;Choi et al., 2018;Reddy et al., 2019). This scenario is important for addressing real-world open-ended questions, which requires discussions to explore in depth (Dai et al., 2022), e.g., How to learn more efficiently? Though great progress has been achieved in recent years, most existing researches depend on abundant human annotation, which can be highly costly and limited in knowledge coverage.\n\nA promising way to alleviate this problem is data augmentation . Traditional methods, including token-level manipulation (Kobayashi, 2018;Wei and Zou, 2019) Method DG Data Needs EDA (Wei and Zou, 2019) \u2717  and sentence-level paraphrasing (Sennrich et al., 2016), improve the linguistic diversity of training data. However, they cannot create conversations grounded on new documents, which are indispensable for dealing with out-of-domain scenarios. Another line of research focuses on simulation-based methods (Wu et al., 2021;Kim et al., 2022). Specifically, they can iteratively generate conversations grounded on new documents based on a span extractor and an utterance generator. Nevertheless, both the training of the extractor and the generator still require abundant human dialogues. Besides the above ways, Dai et al. (2022) propose Dialog Inpainting, which creates information-seeking dialogues by inserting utterances between neighboring sentences in documents. One potential risk is the gap between the structure of documents and that of conversations. Documents are tighter, while realworld conversations are more open-ended. To alleviate the above issues, we propose a simple yet effective method AutoConv for Automatically generating information-seeking Conversations, which takes advantage of the fewshot learning ability and generation capacity of large language models (LLM) (Brown et al., 2020). Specifically, we formulate conversation generation as a language modeling task and utilize an LLM for generating synthetic conversations grounded on external documents. Surprisingly, finetuning with a few human dialogues can help LLM capture the characteristics of the information-seeking process"}
{"tabids": ["7efdbf19-1e0e-4e51-830a-df5830384bc7"], "corpus_id": 211171550, "title": "Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based Target Behavior Prediction", "paper_id": "6ee369ac1bff352b885b94e6d7e2e6ce690165ed", "abstract": "Session-based target behavior prediction aims to predict the next item to be interacted with specific behavior types (e.g., clicking). Although existing methods for session-based behavior prediction leverage powerful representation learning approaches to encode items' sequential relevance in a low-dimensional space, they suffer from several limitations. Firstly, they focus on only utilizing the same type of user behavior for prediction, but ignore the potential of taking other behavior data as auxiliary information. This is particularly crucial when the target behavior is sparse but important (e.g., buying or sharing an item). Secondly, item-to-item relations are modeled separately and locally in one behavior sequence, and they lack a principled way to globally encode these relations more effectively. To overcome these limitations, we propose a novel Multi-relational Graph Neural Network model for Session-based target behavior Prediction, namely MGNN-SPred for short. Specifically, we build a Multi-Relational Item Graph (MRIG) based on all behavior sequences from all sessions, involving target and auxiliary behavior types. Based on MRIG, MGNN-SPred learns global item-to-item relations and further obtains user preferences w.r.t. current target and auxiliary behavior sequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to adaptively fuse user representations for predicting next item interacted with target behavior. The extensive experiments on two real-world datasets demonstrate the superiority of MGNN-SPred by comparing with state-of-the-art session-based prediction methods, validating the benefits of leveraging auxiliary behavior and learning item-to-item relations over MRIG.", "intro": "Unlike conventional recommendation algorithms which get accustomed to modeling each user-item interaction separately [11], recent sequential recommendation approaches meet more realistic requirements for its ability of modeling user dynamic interest. Session-based target behavior prediction [8] is the one of the main studied problem in this regard, aiming to predict the next item to be interacted with a user under a specific type of behavior (e.g., clicking an item). Based on the predictions, information providers can effectively deliver items to appropriate users and at the same time, and users can quickly find the items what they actually want. Note that we use session-based prediction and session-based recommendation interchangeably throughout this paper.\n\nEarly studies for this problem assume that the appearance of the next item depends only on its previous item [23,33] in the same sequence. With such a strong assumption, they could only model the last item in each sequence and ignore other information from the sequence. To relieve this assumption, various methods adopt sequential models for session-based recommendation system to learn behavior sequences. Recurrent Neural Networks (RNN) [9] is commonly leveraged to obtain promising performance. The relevant methods could roughly be attributed into two categories: singlesession based recommendation models [8,30] and multi-session based recommendation models [21,31]. As the latter category requires the user ID of each behavior sequence should be known in advance to link multiple sequences of the same user together, it is not so universal than the first category due to privacy issues and user scalability problem (e.g., a billion of active users each day in WeChat). As such, we study session-based target behavior prediction from the perspective of single-session based modeling.\n\nIn the domain of single-session based behavior prediction, some studies [14,22,25] adopt attention mechanism [1,28] and outperform the pioneering RNN based methods [8]. Recent advances in graph neural networks (GNN) [3,7] further boost the performance of session-based behavior prediction by modeling each sessionbased behavior sequence as a graph to achieve the state-of-the-art performance [29,30]. However, existing studies in this regard still suffer from several limitations. Firstly, they focus on only using the same type of user behavior as input for the next item prediction, but ignore the potential of leveraging other type of behavior as auxiliary information. This is particularly crucial when the target behavior is sparse but important (e.g., buying or sharing an item). Secondly, item-to-item relations are modeled separately and locally, since both RNN based and GNN based recommendation models only utilize one behavior sequence each time. It is intuitive that abundant item-to-item relations are hidden in various behavior sequences. For example, if many other users who have bought item B after buying item A, the relation between item A and B is especially vital if a target user just bought item A.\n\nTo overcome these limitations, we propose a novel Multi-relational Graph Neural Network model for Session-based target behavior Prediction, namely MGNN-SPred for short. The target behavior we focused on is the aforementioned sparse behavior beyond the dense click behavior. MGNN-SPred jointly considers target behavior and auxiliary behavior sequences and explores global item-toitem relations for accurate prediction. Specifically, for the purpose of considering the global item-to-item relations, we build a Multi-Relational Item Graph (MRIG) based on the past behavior sequences of all sessions. There might exist multiple relations between two graph nodes, denoting target and auxiliary behavior types. Based on MRIG, MGNN-SPred encodes global item-to-item relations into node representations and further obtains local representations for current target and auxiliary behavior sequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to adaptively fuse the representations from target behavior sequence and auxiliary behavior sequence to produce current user interest representation.\n\nThe main contributions of this work is summarized as follows: 1. We address the two limitations of existing methods by breaking the restriction of only using one type of behavior sequence in session-based recommendation and exploring another type of behavior as auxiliary information. We further construct the multirelational item graph for learning global item-to-item relations.\n\n2. To effectively model MRIG w.r.t. target and auxiliary behavior sequences, we develop the novel graph model MGNN-SPred which learns global item-to-item relations through graph neural network and integrates representations of target and auxiliary of current sequences by the gating mechanism.\n\n3. We carry out extensive experiments and demonstrate MGNN-SPred achieves the best performance among strong competitors, showing the benefits of overcoming the two limitations."}
{"tabids": ["7efdbf19-1e0e-4e51-830a-df5830384bc7"], "corpus_id": 235358755, "title": "DMBGN: Deep Multi-Behavior Graph Networks for Voucher Redemption Rate Prediction", "paper_id": "7171d359a97a9808043a594cec58f432b6e155bb", "abstract": "In E-commerce, vouchers are important marketing tools to enhance users' engagement and boost sales and revenue. The likelihood that a user redeems a voucher is a key factor in voucher distribution decision. User-item Click-Through-Rate (CTR) models are often applied to predict the user-voucher redemption rate. However, the voucher scenario involves more complicated relations among users, items and vouchers. The users' historical behavior in a voucher collection activity reflects users' voucher usage patterns, which is nevertheless overlooked by the CTR-based solutions. In this paper, we propose a Deep Multi-behavior Graph Networks (DMBGN) to shed light on this field for the voucher redemption rate prediction. The complex structural user-voucher-item relationships are captured by a User-Behavior Voucher Graph (UVG). User behavior happening both before and after voucher collection is taken into consideration, and a high-level representation is extracted by Higher-order Graph Neural Networks. On top of a sequence of UVGs, an attention network is built which can help to learn users' long-term voucher redemption preference. Extensive experiments on three large-scale production datasets demonstrate the proposed DMBGN model is effective, with 10% to 16% relative AUC improvement over Deep Neural Networks (DNN), and 2% to 4% AUC improvement over Deep Interest Network (DIN). Source code and a sample dataset are made publicly available to facilitate future research.", "intro": "In E-commerce, vouchers have already become significant tools which not only drive sales, but also help user growth and enhances customer loyalty. An online voucher (or coupon in some literature) is a digital ticket that can be redeemed with discount or rebate when a product is purchased 2 . A typical voucher is characterized by two components: condition and discount. The former specifies requirements of a voucher to be applicable, e.g, the minimum number of items purchased or the minimum amount of money spent in an order, and the latter refers to the amount or percentage deductible when the voucher is applied. This work focuses on minimum-spend and discount-amount based vouchers. Fig. 1 shows a typical user-voucher interaction scenario on an E-commerce platform: a user may do some online shopping before being assigned a voucher (i.e., pre-collection phase). After collecting the voucher (either manually or automatically), the user may continue to search, click or add-to-cart the items that he/she wants to buy with the voucher (i.e., post-collection phase). Finally, the user may redeem the voucher by purchasing some items (i.e., redemption phase), or simply leave the voucher expired without redemption. The capability of predicting the user-voucher redemption rate (VRR henceforth in this paper), the likelihood that a user may redeem a voucher, would contribute to a successful voucher marketing in multiple ways:\n\n\u2022 Performance Forecasting: Retailers are able to estimate the returns of a voucher (e.g. overall redemption rate, redeemed voucher count) before a campaign starts. \u2022 Budget Control: Retailers are able to know when to stop distributing vouchers so that the marketing budget would not be over-utilized or under-utilized. \u2022 Personalized Distribution: Retailers are able to distribute different voucher schema (e.g. condition and discount) to different customers to maximize the overall returns, by applying an optimization algorithm to the VRR results.\n\nThere are some preliminary studies [15,20,28] focusing on VRR prediction. Those studies solve voucher redemption rate prediction tasks by directly borrowing the ideas from user-item Click-Through-Rate (CTR) prediction models [6,38]. A common practice is to treat a given voucher in the same way as treating an item, simply replacing item features by voucher features in model training and prediction. Nonetheless, the VRR task is more challenging in several ways. Firstly, in item-wise CTR models, items in historical behavior and the item to be evaluated are homogeneous, while in VRR, the behavior involves heterogeneous relations between vouchers and items. Secondly, in CTR models, users' behavior on items has no obvious inter-dependent relationships. In voucher scenarios, by contrast, the user behavior is often triggered by the collected voucher and is strongly affected by the voucher condition and discount. As shown in Table 1, the behavior sequence in the post-collection phase becomes relatively longer than in pre-collection. Lastly, a user may have a multiple voucher redemption history, the long-term and short-term voucher usage preferences of the user need to be considered and balanced to better predict the VRR of the current voucher to be distributed (namely target voucher).\n\nTo tackle the above issues and shed light on the VRR prediction task, we propose a graph-based model Deep Multi-Behavior Graph Networks (DMBGN). To evaluate a user's VRR of the target voucher, DMBGN takes into consideration the user's historical interactions with vouchers. To model the structural user-voucher-item relationships, a User-Behavior Voucher Graph (UVG) is constructed for each user-voucher interaction activity. A UVG is a satellite-like heterogeneous graph containing nodes for both voucher and items in user behavior happening before and after the voucher collection. The techniques of Higher-order Graph Neural Networks are adopted to extract the high-level structural representation of a voucher activity, so that relationships like user-item preference, voucher-item dependency and user-voucher preference can be better distilled. To learn the user's long-term interests in voucher redemption, an attention network is applied on top of the historical UVG sequence. Finally, all the implicit representations of user, item, voucher and other side information are forwarded to a DNN network for the VRR prediction.\n\nThe main contributions of this paper are as follows:\n\n\u2022 To our best knowledge, this is the first work to propose a model specialized for the VRR prediction task. The model, DMBGN, follows the user's mindset in a real voucher activity"}
{"tabids": ["7efdbf19-1e0e-4e51-830a-df5830384bc7"], "corpus_id": 252216587, "title": "Global and Personalized Graphs for Heterogeneous Sequential Recommendation by Learning Behavior Transitions and User Intentions", "paper_id": "f18d768b62198e6c7421b4422f69374b354f3e0b", "abstract": "Heterogeneous sequential recommendation (HSR) is a very important recommendation problem, which aims to predict a user\u2019s next interacted item under a target behavior type (e.g., purchase in e-commerce sites) based on his/her historical interactions with different behaviors. Though existing sequential methods have achieved advanced performance by considering the varied impacts of interactions with sequential information, a large body of them still have two major shortcomings. Firstly, they usually model different behaviors separately without considering the correlations between them. The transitions from item to item under diverse behaviors indicate some users\u2019 potential behavior manner. Secondly, though the behavior information contains a user\u2019s fine-grained interests, the insufficient consideration of the local context information limits them from well understanding user intentions. Utilizing the adjacent interactions to better understand a user\u2019s behavior could improve the certainty of prediction. To address these two issues, we propose a novel solution utilizing global and personalized graphs for HSR (GPG4HSR) to learn behavior transitions and user intentions. Specifically, our GPG4HSR consists of two graphs, i.e., a global graph to capture the transitions between different behaviors, and a personalized graph to model items with behaviors by further considering the distinct user intentions of the adjacent contextually relevant nodes. Extensive experiments on four public datasets with the state-of-the-art baselines demonstrate the effectiveness and general applicability of our method GPG4HSR.", "intro": "In lots of scenes including e-commerce sites, recommender systems have played a crucial booster role. Through learning from the historical (user, item) interactions, it is able to generate the digital scores or a ranked list for the uninteracted items, and recommend some appropriate items for each user to alleviate the information overload problem. As a user's preferences widely exist in the form of sequence and the way he/she interacted with items, how to model the user's sequential and heterogeneous behaviors becomes a research hotspot recently.\n\nAlthough existing methods model the sequential and behavior information in an effective way, they usually suffer from the following two issues. Firstly, they often ignore the correlations of different behaviors. Specifically, the different transitions according to their behavior types indicate importance to a different extent, but modeling behaviors separately, i.e., splitting an interaction sequence into multiple subsequences according to the behavior types, cannot learn the potential correlations well as the connection between different types of behaviors is broken. Secondly, they usually model behaviors without utilizing the local context information. Even though the behavior information could capture different levels of user intentions, the context information is able to further enhance the certainty and decrease the randomness. For example, if a user Alice performed a sequence of browsing item A, adding item A to the cart, and purchasing item A, and another user Bob also performed an interaction sequence but with a slight difference, i.e., browsing item A, adding item A to the cart, and browsing item B. The behavior of adding item A to the cart exists in both sequences, which indicates a high probability of purchasing item A. However, the subsequent interactions are quite different, and are also very useful to obtain an enhanced understanding of \"adding item A to the cart\" for each user, i.e., Alice would purchase items in cart with higher certainty, but there is more randomness for Bob to do so.\n\nMost of traditional recommendation algorithms [9,18] fill a rating matrix of (user, item) pairs to deal with general recommendation, without utilizing the sequential and behavior information. Compared with general recommendation, early works for sequential recommendation [8,10,21,29] obtain better performance since they capture more accurate preferences by learning the sequential patterns. For instance, GRU4Rec [8] utilizes gated recurrent units (GRUs), a variant of recurrent neural networks (RNNs), to transmit the previous information to subsequent units for learning the sequential patterns inherently. With the increasing amount of e-commerce users and items, the information overload problem becomes crucial, while the increasing behavior data offers more detailed information to gain a deeper understanding of a user's intentions. This actually raises an emerging and valuable topic, i.e., heterogeneous sequential recommendation (HSR). For example, TransRec++ [31] adapts TransRec [5] to strengthen its ability of capturing the behavior transitions of users, which considers the correlations between different behaviors but lacks consideration of distinguishing those behaviors. RIB [32] concatenates the item and behavior embeddings as the input embedding to consider the influences of different behaviors, and adopts an RNN layer to learn the sequential patterns. However, concatenation of two embeddings could only learn the distinct importance of behaviors but not the relations between them. MGNN-SPred [23] constructs two itemto-item graphs by incorporating the context information between items under the same behavior in order to learn the transitions, but ignores the transitions under different behaviors.\n\nTo overcome these issues, we propose a novel solution called global and personalized graphs for heterogeneous sequential recommendation (GPG4HSR). Firstly, we introduce a global graph layer, equipped with a global graph (GG) of items and behavior transitions to capture the transitions between all behaviors, which addresses the aforementioned first issue. Secondly, to learn user intentions by distinguishing the varied effects of the same interaction with different context information and address the second issue, we utilize a personalized graph layer with a personalized graph (PG) of interaction sequence to model heterogeneous feedback with the influence of adjacent interactions. In addition, to balance these two modules (i.e., the global and personalized graph layers) and take a further step in learning the sequential patterns, we apply an effective fusion layer and an attention layer. Finally, we introduce a prediction layer in order to predict the next likely-to-purchase item. Note that we group all behaviors into examinations and purchases as their two representative behavior types, and the exploratory study in Section 6.2 indicates that our GPG4HSR is applicable to scenes with more types of behaviors.\n\nWe summarize our main contributions as follows: (i) We propose a novel model called GPG4HSR to address two fundamental issues in HSR by considering the correlations of all behaviors via a global graph (GG) and enhancing the interaction representation with the context information via a personalized graph (PG). (ii) We develop a global graph layer and a personalized graph layer to model GG and PG, respectively, in which the former weighs the transitions between different behaviors and the latter incorporates the representations of a local behavior and its adjacent contextually relevant interactions. (iii) We study the recommendation performance of our GPG4HSR and eight competitive baselines on four datasets, and find that our GPG4HSR performs the best in most cases. Moreover, further studies, including exploratory studies with more than two different types of behaviors, ablation studies of the major components, framework studies with different backbone models and sensitivity analysis of the hyperparameter, show the effectiveness of our solution."}
{"tabids": ["7efdbf19-1e0e-4e51-830a-df5830384bc7"], "corpus_id": 255894127, "title": "BGNN: Behavior-aware graph neural network for heterogeneous session-based recommendation", "paper_id": "7f0069d805956abc7659ecbd68bc93dbdf189b83", "abstract": "Session-based recommendation (SBR) and multi-behavior recommendation (MBR) are both important problems and have attracted the attention of many researchers and practitioners. Different from SBR that solely uses one single type of behavior sequences and MBR that neglects sequential dynamics, heterogeneous SBR (HSBR) that exploits different types of behavioral information (e.g., examinations like clicks or browses, purchases, adds-to-carts and adds-to-favorites) in sequences is more consistent with real-world recommendation scenarios, but it is rarely studied. Early efforts towards HSBR focus on distinguishing different types of behaviors or exploiting homogeneous behavior transitions in a sequence with the same type of behaviors. However, all the existing solutions for HSBR do not exploit the rich heterogeneous behavior transitions in an explicit way and thus may fail to capture the semantic relations between different types of behaviors. However, all the existing solutions for HSBR do not model the rich heterogeneous behavior transitions in the form of graphs and thus may fail to capture the semantic relations between different types of behaviors. The limitation hinders the development of HSBR and results in unsatisfactory performance. As a response, we propose a novel behavior-aware graph neural network (BGNN) for HSBR. Our BGNN adopts a dual-channel learning strategy for differentiated modeling of two different types of behavior sequences in a session. Moreover, our BGNN integrates the information of both homogeneous behavior transitions and heterogeneous behavior transitions in a unified way. We then conduct extensive empirical studies on three real-world datasets, and find that our BGNN outperforms the best baseline by 21.87%, 18.49%, and 37.16% on average correspondingly. A series of further experiments and visualization studies demonstrate the rationality and effectiveness of our BGNN. An exploratory study on extending our BGNN to handle more than two types of behaviors show that our BGNN can easily and effectively be extended to multi-behavior scenarios.", "intro": "Intelligent recommendation is a technology to alleviate the problem of information overload in this era of information explosion [1,2]. Session-based recommendation aims to recommend the next possible interacted item to a user based on the user's behavior sequence in the session, which has recently been a research hotspot [3,4].\n\nHowever, session-based recommendation solely uses a single type of behavior sequence of the user and often faces the problem of data sparsity, especially when the behavior is naturally sparse [5]. For example, in an e-commerce scenario, the company's ultimate goal is to guide users to purchase items for profit while the data of purchases is very rare. Using only purchase sequences for session-based recommendation would lead to unsatisfactory recommendation performance since users' interests are diverse while the data is sparse [5,6]. For instance, a user may purchase certain items for certain occasions (such as birthday and festivals), or purchase items for his/her family members, which are both not related to their recent purchases [7].\n\nFortunately, the behavior sequence of a session in a realworld e-commerce platform is often heterogeneous and composed of more than one type of behaviors, e.g., purchases and examinations, in which examinations can be clicks or browses. Therefore, we can exploit the more abundant information in the heterogeneous behavior sequence to improve the recommendation performance of predicting the target behaviors, i.e., purchases. We name this type of research problem as heterogeneous session-based recommendation (HSBR) and name the traditional session-based recommendation as homogeneous session-based recommendation in this paper. Although the researches on HSBR are relatively few, we argue that HSBR is more consistent with the setting of real-world scenarios and deserves more attention from the community.\n\nExisting works for HSBR can be classified into two categories, i.e., methods based on recurrent neural network (RNN) [8][9][10] and methods based on graph neural network (GNN) [5,6]. In addition, some researchers have used Transformer-based methods in modeling of multi-behavior sequences in CTR and CVR tasks [11,12]. For example, the RNN-based method RIB [10] uses the behavior embedding to distinguish the different types of behaviors in a heterogeneous session, and takes the concatenation of the item embedding and the behavior embedding as the input of a gate recurrent unit (GRU) layer. The GNN-based method M-SR [5] proposes to treat a heterogeneous behavior sequence as an item sequence and an operation (behavior type) sequence for separate modeling. Specifically, M-SR uses a GNN method to model the complex item transitions in the item sequence and uses a GRU layer to model the operation sequence, supposing that there are sequential dependencies between different behavior types. The other GNN-based method MGNN-SPred [6] constructs a global multi-relational item graph based on two types of homogeneous behavior sequences, i.e., examination sequences and purchase sequences, over all sessions, and uses a GNN method to enhance the learning of item representations. Although MGNN-SPred strengthens the learning of item representations by exploiting the homogeneous behavior transition information, its modeling of different types of behavior sequences is the same, resulting in examining and purchasing an item will get the same representation in the feature space (see the details in Section 5.2.4), which may limit the capability of the model.\n\nMore importantly, most of the existing methods for HSBR focus on either distinguishing different types of behaviors or exploiting homogeneous behavior transitions intra each type of behavior sequences, while ignoring the useful information of heterogeneous behavior transitions inter different types of behavior sequences. For example, a user usually examines items for comparison before making a purchase decision, where the transition between examinations like clicks or browses indicates similarity and the transition from examination to purchase may indicate a progressive relationship [3]. Therefore, exploiting the heterogeneous behavior transitions is essential for learning the semantic connections between different types of behaviors and achieving accurate recommendation.\n\nTo fill this gap in HSBR, we propose to construct two different global graphs, i.e., homogeneous behavior transition graph (HoBTG) and heterogeneous behavior transition graph (HeBTG), based on all sessions to exploit the behavior transition information from two aspects, i.e., homogeneous and heterogeneous behavior transitions, respectively. We further propose a novel method named behavior-aware graph neural network (BGNN) to fully leverage the knowledge in the two graphs, in which a dual-channel learning strategy is designed to focus more on the modeling of the auxiliary behavior sequences, i.e., the examination sequences in this paper. To be more specific, we regard HoBTG as a common graph for learning high-quality item representations for both the purchases and the examinations, and utilize HeBTG for modeling of the examinations, aiming to explicitly utilize the purchase-oriented semantic connections implicit in HeBTG. In our BGNN, we introduce a personalized item neighbor aggregator (PING), which personalizes the messages passing in HeBTG by considering the users' purchase preference when aggregating neighboring nodes. And we also design a representation gating (RNG) module to balance the two views of item representations of the examination sequence learned from the two graphs. Moreover, BGNN learns the two types of behavior sequence representations through a parallel softattention mechanism, and finally fuses them into the session representation.\n\nThe main contributions of our work are summarized as follows.\n\n\u2022 We propose to construct two kinds of graphs, i.e., HoBTG and HeBTG, to capture the information of homogeneous behavior transitions and heterogeneous behavior transitions, respectively. \u2022 We propose a novel solution named behavior-aware graph neural network (BGNN) based on a dual-channel learning strategy, in which the use of HeBTG in the auxiliary channel makes the contribution of the auxiliary sequences to the target sequences more clear. \u2022 We design a personalized item neighbor aggregator (PING) to personalize the messages passing in HeBTG when aggregating neighboring nodes. And we also introduce a representation gating (RNG) module to endow our BGNN with the capability of balancing the two views of item representations well. \u2022 The experimental results on three real-world datasets show that our BGNN outperforms the best baseline by a large margin, demonstrating the superiority of our method. The visualization studies further verify the rationality and significance of exploiting heterogeneous behavior transitions in our BGNN. We also conduct an exploratory study of extending our BGNN to deal with more than two types of behaviors, where the results indicate that our BGNN can be easily and effectively extended to multi-behavior scenarios."}
{"tabids": ["7efdbf19-1e0e-4e51-830a-df5830384bc7"], "corpus_id": 256832144, "title": "BA-GNN: Behavior-aware graph neural network for session-based recommendation", "paper_id": "786125bb641c8d6d45afc0deaaf7fe08e02a485b", "abstract": "Session-based recommendation is a popular research topic that aims to predict users\u2019 next possible interactive item by exploiting anonymous sessions. The existing studies mainly focus on making predictions by considering users\u2019 single interactive behavior. Some recent efforts have been made to exploit multiple interactive behaviors, but they generally ignore the influences of different interactive behaviors and the noise in interactive sequences. To address these problems, we propose a behavior-aware graph neural network for session-based recommendation. First, different interactive sequences are modeled as directed graphs. Thus, the item representations are learned via graph neural networks. Then, a sparse self-attention module is designed to remove the noise in behavior sequences. Finally, the representations of different behavior sequences are aggregated with the gating mechanism to obtain the session representations. Experimental results on two public datasets show that our proposed method outperforms all competitive baselines. The source code is available at the website of GitHub.", "intro": "With the rapid development of Internet technology, the explosive generation of data has brought about a severe information overload problem. Recommendation system can quickly provide us with the information that we are interested in by mining the historical interaction data and modeling users' profiles or preferences. However, it is usually difficult to obtain users' profiles because of privacy protection. In addition, users' preferences are instantaneous and variable, making it difficult to mine long-term user interests. To solve the above problems, researchers propose session-based recommendation (SR for short). SR refers to predicting the next interactive item that users may be interested in by mining the hidden preference in an anonymous session, where the session consists of multiple items that a user has clicked or bought in the past time slot [1]. Thus, SR is able to capture users' short-term and variable preferences, providing more timely and accurate recommendations for users. SR has become a popular research direction in recent years.\n\nThe majority of popular SR methods are mainly designed based on deep neural networks. For example, GRU4Rec [2] and NARM [3] are based on Recurrent Neural Network (RNN). They capture the dynamic users' preferences. The methods based on Graph Neural Network(GNN) model sessions as graphs and capture complex item transitions [4\u22127]. Although the above methods perform well and promote this area quickly, they only exploit single interactive behavior during the recommendation.\n\nIn fact, the session contains different types of behaviors. For example, a user may first click on an item and then perform a series of actions such as favorites, purchases. These actions are beneficial to mine users' dynamic preferences, and then improve the accuracy of recommendation.\n\nTherefore, recent studies are carried out to take multiple interactive behaviors into consideration. Wang et al. [8] proposed MGNN-SPred which constructs a multi-relational behavior graph based on two kinds of action sequences. With the help of all interactive behaviors, the users' preference representations are well learned, significantly improving the accuracy of the recommendation. However, the following limitations remain to be overcome.\n\n(1) Various types of interactive actions have different effects on recommendation. As is shown in Fig. 1(a), a user clicks foundation, lipstick, perfume, and eye shadow, and finally buy a bottle of foundation. It implies that the user is interested in make-up. The previous efforts on multiple actions focus on treating a certain action as a target, and the other actions are considered as auxiliaries. To model users' real preferences and improve the performance of recommendation, both the actions of click and buy should be considered simultaneously.\n\n(2) Noise is widely existed in historical interactive data. It is common that we may click a certain item or advertisement accidentally, when we browse what we are interested in. As is shown in Fig. 1(b), a user clicks foundation, lipstick, mouse, perfume, shoes and buys a lipstick. Obviously, the mouse and shoes are noise data that should be removed.\n\nTo solve these problems, we propose a behavior-aware graph neural network for session-based recommendation, BA-GNN. Specifically, we first construct directed graphs for click and purchase sequences to obtain the item representations. Then, a sparse self-attention module is designed to remove the noise in click sequences. The session representations are composed of the behavior sequences representations. Finally, we predict the users' next interactive items by session representations and item representations.\n\nOur main contributions are summarized as follows:\n\n\u2022 We propose a behavior-aware graph neural network for session-based recommendation. We fully consider the influences of different interactive behaviors, and construct directed graphs for the sequences with two kinds of interactive actions, respectively. \u2022 To remove the noise in interactive behavior sequences, we design a sparse self-attention module that obtains better sequence representations. \u2022 We conduct extensive experiments on two public datasets and compare with multiple baselines. The experimental results show that our proposed method achieves the best performance."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 202782600, "title": "An Information-Theoretic Approach to Transferability in Task Transfer Learning", "paper_id": "5b6a12a94993e8c60133db1bbe9449669b461957", "abstract": "Task transfer learning is a popular technique in image processing applications that uses pre-trained models to reduce the supervision cost of related tasks. An important question is to determine task transferability, i.e. given a common input domain, estimating to what extent representations learned from a source task can help in learning a target task. Typically, transferability is either measured experimentally or inferred through task relatedness, which is often defined without a clear operational meaning. In this paper, we present a novel metric, H-score, an easily-computable evaluation function that estimates the performance of transferred representations from one task to another in classification problems using statistical and information theoretic principles. Experiments on real image data show that our metric is not only consistent with the empirical transferability measurement, but also useful to practitioners in applications such as source model selection and task transfer curriculum learning.", "intro": "Transfer learning is a learning paradigm that exploits the relatedness between different learning tasks in order to gain certain benefits, e.g. reducing the demand for supervision ([1]). In task transfer learning, we assume that the input domain of the different tasks are the same. Then for a target task T T , instead of learning a model from scratch, we can initialize the parameters from a previously trained model for some related source task T S (Figure 1). For example, deep convolutional neural networks trained for the ImageNet classification task have been used as the source network in transfer learning for target tasks with fewer labeled data [2], such as medical image analysis [3] and structural damage recognition in buildings [4]. An imperative question in task transfer learning is transferability, i.e. when a transfer may work and to what extent. Given a metric capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning, to a large extent, is simplified to search procedures over potential transfer sources and targets as quantified by the metric. Traditionally, transferability is measured purely empirically using model loss or accuracy on the validation set ([5, 6, 7]). There have been theoretical studies that focus on task relatedness ([8, 9, 10, 11]). However, they either cannot be computed explicitly from data or do not directly explain task transfer performance. In this study, we aim to estimate transferability analytically, directly from the training data.\n\nWe quantify the transferability of feature representations across tasks via an approach grounded in statistics and information theory. The key idea of our method is to show that the expected log-loss of using a feature of the input data to predict the label of a given task under the probabilistic model can be characterized by an analytically expression, which we refer as the H-score of the feature. H-score is particularly useful to quantify feature transferability among tasks. Using this idea, we define task transferability as the normalized H-score of the optimal source task feature with respect to the target task.\n\nAs we demonstrate in this paper, the advantage of our transferability metric is threefold. (i) it is theoretically driven and has a strong operational meaning rooted in statistics and information theory; (ii) it can be computed directly and efficiently from the input data, with fewer samples than those needed for empirical learning; (iii) it can be shown to be strongly consistent with empirical transferability measurements."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 201303557, "title": "Transferability and Hardness of Supervised Classification Tasks", "paper_id": "41db7308930ea8bf2fbefcab41294fac6f929721", "abstract": "We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets -- CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks) -- together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for tasks estimated to be highly transferable.", "intro": "How easy is it to transfer a representation learned for one task to another? How can we tell which of several tasks is hardest to solve? Answers to these questions are vital in planning model transfer and reuse, and can help reveal fundamental properties of tasks and their relationships in the process of developing universal perception engines [3]. The importance of these questions is therefore driving research efforts, with several answers proposed in recent years.\n\nSome of the answers to these questions established task relationship indices, as in the Taskonomy [71] and Task2Vec [1,2] projects. Others analyzed task relationships in the context of multi-task learning [31, 37,61,68,73]. Importantly, however, these and other efforts are computa- * Work at Amazon Web Services, prior to joining current affiliation. tional in nature, and so build on specific machine learning solutions as proxy task representations.\n\nBy relying on such proxy task representations, these approaches are naturally limited in their application: Rather than insights on the tasks themselves, they may reflect relationships between the specific solutions chosen to represent them, as noted by previous work [71]. Some, moreover, establish task relationships by maintaining model zoos, with existing trained models already available. They may therefore also be computationally expensive [1,71]. Finally, in some scenarios, establishing task relationships requires multi-task learning of the models, to measure the influence different tasks have on each other [31, 37,61,68,73].\n\nWe propose a radically different, solution agnostic approach: We seek underlying relationships, irrespective of the particular models trained to solve these tasks or whether these models even exist. We begin by noting that supervised learning problems are defined not by the models trained to solve them, but rather by the data sets of labeled examples and a choice of loss functions. We therefore go to the source and explore tasks directly, by examining their data sets rather than the models they were used to train.\n\nTo this end, we consider supervised classification tasks defined over the same input domain. As a loss, we assume the cross entropy function, thereby including most commonly used loss functions. We offer the following surprising result: By assuming an optimal loss on two tasks, the conditional entropy (CE) between the label sequences of their training sets provides a bound on the transferability of the two tasks-that is, the log-likelihood on a target task for a trained representation transferred from a source task. We then use this result to obtain a-priori estimates of task transferability and hardness.\n\nImportantly, we obtain effective transferability and hardness estimates by evaluating only training labels; we do not consider the solutions trained for each task or the input domain. This result is surprising considering that it greatly simplifies estimating task hardness and task relationships, yet, as far as we know, was overlooked by previous work.\n\nWe verify our claims with rigorous tests on a total of 437 tasks from the CelebA [35], Animals with Attributes 2 (AwA2) [67], and Caltech-UCSD Birds 200 (CUB) [66] sets. We show that our approach reliably predicts task transferability and hardness. As a case study, we evaluate transferability from face recognition to facial attribute classification. On attributes estimated to be highly transferable from recognition, our results outperform the state of the art despite using a simple approach, involving training a linear support vector machine per attribute."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 211572839, "title": "LEEP: A New Measure to Evaluate Transferability of Learned Representations", "paper_id": "43f5124d50b5b148c71cb2e56970e4a386be055a", "abstract": "We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.", "intro": "Transferability estimation (Eaton et al., 2008;Ammar et al., 2014;Sinapov et al., 2015) is the problem of quantitatively estimating how easy it is to transfer knowledge learned from one classification task to another. Specifically, given a source task, represented by a labeled data set or a pre-trained model, and a target task, represented by a labeled data set, transferability estimation aims to develop a measure (or a score) that can tell us, ideally without training on the target task, how effectively transfer learning algorithms can transfer knowledge from the source task to the target task.\n\nAnswering this question is important, since good estimations of transferability can help understand the relationships between tasks (Tran et al., 2019), select groups of highly transferable tasks for joint training (Zamir et al., 2018), or choose good source models for a given target task Bao et al., 2019;Bhattacharjee et al., 2019). Previous approaches to transferability estimation often require running a transfer learning algorithm that involves expensive parameter optimization (Zamir et al., 2018;Achille et al., 2019), do not have a simple interpretation (Bao et al., 2019), or make strong assumptions about the data sets that limit their applicability (Zamir et al., 2018;Tran et al., 2019).\n\nWe propose a novel measure called the Log Expected Empirical Prediction (LEEP) for transferability estimation of deep networks that overcomes all the shortcomings above. In contrast to previous approaches, LEEP scores are obtained without training on the target task, thus avoiding the expensive parameter optimization step. Additionally, they have a simple interpretation and can be applied in general settings to a wide range of modern deep networks.\n\nIn particular, LEEP scores are obtained from a source model and a target data set by making a single forward pass of the model through the target data. This is a simpler process than previous methods, such as Taskonomy (Zamir et al., 2018) and Task2Vec , where one has to re-train at least part of the source model on the target data set. Furthermore, LEEP has a simple interpretation: it is the average log-likelihood of the expected empirical predictor, a simple classifier that makes prediction based on the expected empirical conditional distribution between source and target labels. Finally, LEEP does not make any assumption on the source and target input samples, except that they have the same size. This is more general and applicable than previous work (Zamir et al., 2018;Tran et al., 2019) where source and target data sets were assumed to share the same input samples.\n\nContributions. We formally define LEEP and rigorously analyze it, both theoretically and empirically. We show two theoretical properties of the measure: (1) LEEP is upper bounded by the average log-likelihood of the optimal model, obtained by re-training the head classifier while freezing the feature extractor; (2) LEEP is related to the negative conditional entropy measure proposed by Tran et al. (2019).\n\nWe conduct extensive experiments to evaluate our LEEP measure in several scenarios. We show that the measure is useful for predicting the performance of two commonly used transfer learning algorithms -head classifier re-training Razavian et al., 2014) and model fine-tuning (Agrawal et al., 2014;Girshick et al., arXiv:2002.12462v1 [cs.LG] 27 Feb 2020 2014) -not only for large target data sets, but also for small or imbalanced target data sets that are difficult to use for re-training. We also show that LEEP can predict the convergence speed of the fine-tuning method for transfer learning.\n\nWe further demonstrate that LEEP can predict the performance of a recently developed meta-transfer learning method, the Conditional Neural Adaptive Processes (Requeima et al., 2019). Meta-transfer learning (Wei et al., 2018;Sun et al., 2019;Requeima et al., 2019) is a framework for learning to transfer using several meta-training tasks. Importantly, to our knowledge, our work is the first to develop a transferability measure for meta-transfer learning.\n\nWe empirically compare our method with the very recent negative conditional entropy measure (Tran et al., 2019) and H scores (Bao et al., 2019). Our comparisons show that LEEP better correlates with the actual transfer accuracy than these methods. Finally, we demonstrate the effectiveness of LEEP for the source model selection problem in comparison with the negative conditional entropy and H scores."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 227126796, "title": "Ranking Neural Checkpoints", "paper_id": "31d737977efb98471dc2ff32a619b2b1b25b3d12", "abstract": "This paper is concerned with ranking many pre-trained deep neural networks (DNNs), called checkpoints, for the transfer learning to a downstream task. Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints from various sources. Which of them transfers the best to our downstream task of interest? Striving to answer this question thoroughly, we establish a neural checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking measures. These measures are generic, applying to the checkpoints of different output types without knowing how the checkpoints are pre-trained on which dataset. They also incur low computation cost, making them practically meaningful. Our results suggest that the linear separability of the features extracted by the checkpoints is a strong indicator of transferability. We also arrive at a new ranking measure, NLEEP, which gives rise to the best performance in the experiments.", "intro": "There is an increasing number of pre-trained deep neural networks (DNNs), which we call checkpoints. We may produce hundreds of intermediate checkpoints when we sweep through various learning rates, optimizers, and losses to train a DNN. Furthermore, semi-supervised [10,4,49,36,58,39,37,8] and self-supervised [15,26,11,62,43] learning make it feasible to harvest DNN checkpoints with scarce or no labels. Fine-tuning [65,44] has become a de facto standard to adapt the pre-trained checkpoints to target tasks. It leads to faster convergence [16,27,51] and better performance [35] on the downstream tasks.\n\nHowever, not all checkpoints are equally useful for a target task, and some could even under-perform a randomly initialized checkpoint (cf. Section 2.2). This paper is con-* This work was done while the first author was an intern at Google. cerned with ranking neural checkpoints, which aims to measure how effectively fine-tuning can transfer knowledge from the pre-trained checkpoints to the target task. The measurement should be generic enough for all the neural checkpoints, meaning that it works without knowing any pre-training details (e.g., pre-training examples, hyperparameters, losses, early stopping stages, etc.) of the checkpoints. It also should be lightweight, ideally without training on the downstream task, to make it practically useful. We may use the measurement to choose the top few checkpoints before running fine-tuning, which is computationally more expensive than calculating the measurements.\n\nRanking neural checkpoints is crucial. Some domains or applications lack large-scale human-curated data, like medical images [48], raising a pressing need for high-quality pre-trained checkpoints as a warm start for fine-tuning. Fortunately, there exist hundreds of thousands of checkpoints of popular neural network architectures. For instance, many computer vision models are built upon ResNet [28], Inception-ResNet [56], and VGG [52]. As a result, we can construct a candidate pool by collecting the checkpoints released by different groups, for various tasks, and over distinct datasets.\n\nIt is nontrivial to rank the checkpoints for a downstream task. We explain this point by drawing insights from the related, yet arguably easier, task transferability problem [1,20,66,41], which aims to provide high-level guidance about how well a neural network pre-trained in one task might transfer to another. However, not all checkpoints pre-trained in the same source task transfer equally well to the target task [70,35]. The pre-training strategy also matters. Zhai et al. [68] find that combining supervision with self-supervision improves a network's transfer results on downstream tasks. He et al. [26] also show that selfsupervised pre-training benefits object detection more than its supervised counterpart under the same fine-tuning setup.\n\nWe may also appreciate the challenge in ranking neural checkpoints by comparing it with another related line of work: predicting DNNs' generalization gaps [40,31,5]. Jiang et al. [30] use a linear regressor to predict a DNN's generalization gap, i.e., the discrepancy between its training and test accuracies, by exploring the training data's margin distributions. Other signals studied in the literature include network complexity and noise stability. Ranking neural checkpoints is more challenging than predicting a DNN's generalization gap. Unlike the training and test sets that share the same underlying distribution, the downstream task may be arbitrarily distant from the source task over which a checkpoint is pre-trained. Moreover, we do not have access to the pre-training data at all. Finally, instead of keeping the networks static, fine-tuning dramatically changes all weights of the checkpoints.\n\nWe establish a neural checkpoint ranking benchmark (NeuCRaB) to study the problem systematically. Neu-CRaB covers various checkpoints pre-trained on widely used, large-scale datasets by different training strategies and architectures at a range of early stopping stages. It also contains diverse downstream tasks, whose training sets are medium-sized, making it practically meaningful to rank and fine-tune existing checkpoints. Pairing up all the checkpoints and downstream tasks, we conduct careful finetuning with thorough hyper-parameter sweeping to obtain the best transfer accuracy for each checkpoint-downstreamtask pair. Hence, we know the groundtruth ranking of the checkpoints for each downstream task according to the final accuracies (over the test/validation sets).\n\nA functional checkpoint ranking measurement should be highly correlated with the groundtruth ranking and, equally importantly, incurs as low computation cost as possible. We study several intuitive methods for ranking the neural checkpoints. One is to freeze the checkpoints as feature extractors and use a linear classifier to evaluate the features' separability on the target task. Another is to run fine-tuning for only a few epochs (to avoid heavy computation) and then evaluate the resulting networks on the target task's validation set. We also estimate the mutual information between labels and the features extracted from a checkpoint.\n\nFinally, we propose a lightweight measure, named Gaussian LEEP (N LEEP), to rank checkpoints based on the recently proposed log expected empirical prediction (LEEP) [41]. LEEP was originally designed to measure between-task transferabilities. It cannot handle the checkpoints pre-trained by unsupervised or self-supervised learning since it requires all checkpoints to have a classification head. Its computation cost could blow up when the classification head corresponds to a large output space. Moreover, it depends on the classification head's probabilistic output, which, unfortunately, is often overly confident [25].\n\nTo tackle the above problems, we replace the checkpoints' output layer with a Gaussian mixture model (GMM). This simple change kills two birds with one stone. On the one hand, GMM's soft assignment of input to clusters seamlessly applies to LEEP, resulting in the lightweight, effective N LEEP measure that works regardless of the checkpoints' output types. On the other hand, since we fit GMM to the target task's data, instead of the pre-training data of a different source task, the cluster assignment probabilities are likely more calibrated than the classification probabilities for the target task, if there exist classification heads."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 231985863, "title": "LogME: Practical Assessment of Pre-trained Models for Transfer Learning", "paper_id": "54a0d678fa5cacc4fe033d70b86ca8b89977142e", "abstract": "This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo \\emph{without fine-tuning}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is \\emph{immune to over-fitting}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is \\emph{fast, accurate, and general}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most $3000\\times$ speedup in wall-clock time and requires only $1\\%$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: \\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.", "intro": "Human performance on many recognition tasks has been surpassed by deep neural networks (He et al., 2015;2016) Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021.Copyright 2021 by the author(s).\n\ntrained with large-scale supervised data (Deng et al., 2009;Russakovsky et al., 2015) and specialized computational devices (Jouppi et al., 2017).These trained neural networks, also known as pre-trained models, not only work well on tasks they are intended for but also produce generic representations (Donahue et al., 2014) that benefit downstream tasks such as object detection (Girshick et al., 2014).\n\nApart from serving as fixed feature extractors, pre-trained models can be fine-tuned (Yosinski et al., 2014;He et al., 2019) to serve downstream tasks better.The transfer learning paradigm \"pre-training \u2192 fine-tuning\" enjoys tremendous success in both vision (Kornblith et al., 2019) and language (Devlin et al., 2019) communities, and continues to expand to communities like geometric learning (Hu et al., 2020).Transfer of pre-trained models has become one of the cornerstones of deep learning.\n\nNowadays, there are numerous public pre-trained models offered by PyTorch (Benoit et al., 2019), TensorFlow (Abadi et al., 2016) and third-party libraries like HuggingFace Transformers (Wolf et al., 2020).When a practitioner wants to employ transfer learning to solve a specific task, the first problem is to select a good pre-trained model to start from.The problem is non-trivial and task adaptive, considering that different tasks favor different pre-trained models.The problem challenges researchers to develop a practical assessment method that is fast, accurate and general.It should be fast enough compared to brute-force fine-tuning all available pre-trained models (Zamir et al., 2018), should be accurate enough so that potentially best models can be identified, and should be general enough to tackle a wide variety of common learning scenarios.\n\nDespite its practical significance, there is limited guidance arXiv: 2102.11005v3 [cs.LG] 23 Jun 2021 on task adaptive pre-trained model selection.Based on NCE (Tran et al., 2019), Nguyen et al. (2020) recently studied the problem when both the pre-train task and the downstream task are classification.They construct an empirical predictor by estimating the joint distribution over the pretrained and target label spaces and take the performance of the empirical predictor (LEEP) to assess pre-trained models.Though being fast, prior methods are not accurate and are specialized for transferring supervised pre-trained models to classification.They cannot apply to either contrastive pre-trained models (He et al., 2020;Chen et al., 2020a), unsupervised pre-trained language models (Devlin et al., 2019;Liu et al., 2019), or regression tasks.\n\nTable 1 shows the applicability of pre-trained model selection methods.Prior to this paper, for most (4 out of 5) transfer learning settings, task adaptive pre-trained model selection does not have a decent solution.\n\nTo provide a general method for pre-trained model selection in various settings, we consider the features extracted by pre-trained models, thus being agnostic to how models are pre-trained.The maximum value of label evidence (marginalized likelihood) given extracted features is calculated, providing a general probabilistic approach that is applicable to both classification and regression tasks.Finally, the logarithm of maximum evidence (LogME) is used to assess pre-trained models for transfer learning.The maximum evidence is less prone to over-fitting (Bishop, 2006), and its humongous computational cost is dramatically reduced by our carefully designed algorithm.\n\nThe contributions of this paper are two-fold:\n\n\u2022 We propose LogME for task adaptive pre-trained model selection, and develop a fast algorithm to accelerate the computation.LogME is easy to interpret and is extremely efficient.It brings at most 3000\u00d7 speedup in wall-clock time and requires just 1% memory footprint, characterizing itself as the first practical method for assessing pre-trained models in various transfer learning settings.\n\n\u2022 We extensively validate the generality and superior performance of LogME on 22 pre-trained models and 17 downstream tasks, covering various pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language)."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 238744475, "title": "Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance", "paper_id": "0bf1e3ee521c5c5ea3a91819273b7574561f246b", "abstract": "Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\\times$-10$\\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalance ratios etc. for some recent metrics e.g., NCE, LEEP that resulted in them being misrepresented as leading measures. We propose a correction and recommend measuring correlation performance against relative accuracy in such settings. We support our findings with ~164,000 (fine-tuning trials) experiments on both vision models and graph neural networks.", "intro": "Transfer learning is a set of techniques of using abundant somewhat related source data p(X (s) , Y (s) ) to ensure that a model can generalize well to the target domain, defined as either little amount of labelled data p(X (t) , Y (t) ) (supervised), and/or a lot of unlabelled data p(X (t) ) (unsupervised transfer learning). Transfer learning is most commonly achieved either via fine-tuning or co-training. Finetuning is a process of adapting a model trained on source data by using target \u22c6 This work was completed as an Intern and Student Researcher at Google. arXiv:2110.06893v3 [cs.LG] 26 May 2023 data to do several optimization steps (for example, stochastic gradient descent) that update the model parameters. Co-training on source and target data usually involves reweighting the instances in some way or enforcing domain irrelevance on feature representation layer, such that the model trained on such combined data works well on target data. Fine-tuning is becoming increasing popular because large models like ResNet50 [11], BERT [6] etc. are released by companies and are easily modifiable. Training such large models from scratch is often prohibitively expensive for the end user.\n\nIn this paper, we are primarily interested in effectively measuring transferability before training of the final model begins. Given a source data/model, a transferability measure quantifies how much knowledge of source domain/model is transferable to the target model. Transferability measures are important for various reasons: they allow understanding of relationships between different learning tasks, selection of highly transferable tasks for joint training on source and target domains, selection of optimal pre-trained source models for the relevant target task, prevention of trial procedures attempting to transfer from each source domain and optimal policy learning in reinforcement learning scenarios (e.g. optimal selection of next task to learn by a robot). If a measure is capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning is greatly simplified by using the measure to search over candidate transfer sources and targets. Contributions Our contributions are three-fold:\n\n1. We show that H-score, commonly used as a baseline for newer transferability measures, suffers from instability due to poor estimation of covariance matrices. We propose shrinkage-based estimation of H-score with regularized covariance estimation techniques from statistical literature. We show 80% absolute increase over the original H-score and show superior performance in majority cases against all newer transferability measures across various fine-tuning scenarios. 2. We present a fast implementation of our estimator that is 3 \u2212 10 times faster than state-of-the-art LogME measure. 3. We identify problems with 3 other transferability measures (NCE, LEEP and N LEEP) in target task selection when either the number of target classes or the class imbalance varies across candidate target tasks. We propose measuring correlation against relative target accuracy (instead of vanilla accuracy) in such scenarios.\n\nOur large set of \u223c 164, 000 fine-tuning experiments with vision models and graph convolutional networks on real-world datasets shows usefulness of our proposals. This paper is organized as follows. Section 2 describes general fine-tuning regimes and transfer learning tasks. Section 3 discusses transferability measures. Section 4 addresses shortcomings of the pioneer transferability measure (H-Score) that arise due to unreliable estimation and proposes a new shrinkagebased estimator for the H-Score. In Section 5, we demonstrate problems with recent NCE, LEEP and N LEEP metrics and propose a way to address them. Finally, Section 6 presents a meta study of all metrics."}
{"tabids": ["99ae82e7-1829-4360-825b-1c668277bc86"], "corpus_id": 244709516, "title": "Transferability Estimation using Bhattacharyya Class Separability", "paper_id": "4fa282f35dacd5f390c5001af964adea9f44bb8b", "abstract": "Transfer learning has become a popular method for leveraging pre-trained models in computer vision. However, without performing computationally expensive fine-tuning, it is difficult to quantify which pre-trained source models are suitable for a specific target task, or, conversely, to which tasks a pre-trained source model can be easily adapted to. In this work, we propose Gaussian Bhattacharyya Coefficient (GBC), a novel method for quantifying transferability between a source model and a target dataset. In a first step we embed all target images in the feature space defined by the source model, and represent them with per-class Gaussians. Then, we estimate their pairwise class separability using the Bhattacharyya coefficient, yielding a simple and effective measure of how well the source model transfers to the target task. We evaluate GBC on image classification tasks in the context of dataset and architecture selection. Further, we also perform experiments on the more complex semantic segmentation transferability estimation task. We demonstrate that GBC outperforms state-of-the-art transferability metrics on most evaluation criteria in the semantic segmentation settings, matches the performance of top methods for dataset transferability in image classification, and performs best on architecture selection problems for image classification.", "intro": "The goal of transfer learning is to reuse knowledge learned on a source task to help train a model for a target task. Currently, the most common form of transfer learning in computer vision is to pre-train a source model on the ILSVRC'12 dataset [55] and then fine-tune it on the target dataset [3,14,23,24,30,35,57,75]. However, each target task may benefit from a different source model architecture [12,25,45,53] or different source dataset [42,46,71]. The challenge then becomes to determine which (pre-* Currently at Waymo."}
{"tabids": ["8d85c51b-a012-4dfc-b2c7-c7930c473701"], "corpus_id": 202775981, "title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "paper_id": "3e70bbe6c4cd98d66599db709e32b748f184a2d4", "abstract": "Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.", "intro": "(a) CondConv: (\u03b11W1 + . . . + \u03b1nWn) * x (b) Mixture of Experts: \u03b11(W1 * x)+. . .+\u03b1n(Wn * x) Figure 1: (a) Our CondConv layer architecture with n = 3 kernels vs. (b) a mixture of experts approach. By parameterizing the convolutional kernel conditionally on the input, CondConv is mathematically equivalent to the mixture of experts approach, but requires only 1 convolution.\n\nWe propose conditionally parameterized convolutions (CondConv), which challenge the paradigm of static convolutional kernels by computing convolutional kernels as a function of the input. In particular, we parameterize the convolutional kernels in a CondConv layer as a linear combination of n experts (\u03b1 1 W 1 + . . . + \u03b1 n W n ) * x, where \u03b1 1 , . . . , \u03b1 n are functions of the input learned through gradient descent. To efficiently increase the capacity of a CondConv layer, model developers can increase the number of experts. This is much more computationally efficient than increasing the size of the convolutional kernel itself, because the convolutional kernel is applied at many different positions within the input, while the experts are combined only once per input. This allows model developers to increase model capacity and performance while maintaining efficient inference.\n\nCondConv can be used as a drop-in replacement for existing convolutional layers in CNN architectures. We demonstrate that replacing convolutional layers with CondConv improves model capacity and performance on several CNN architectures on ImageNet classification and COCO object detection, while maintaining efficient inference. In our analysis, we find that CondConv layers learn semantically meaningful relationships across examples to compute the conditional convolutional kernels."}
{"tabids": ["8d85c51b-a012-4dfc-b2c7-c7930c473701"], "corpus_id": 2097418, "title": "Dynamic Filter Networks", "paper_id": "aba48504f4f9563eafa44e0cfb22e1345d767c80", "abstract": "In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.", "intro": "Humans are good at predicting another view from related views. For example, humans can use their everyday experience to predict how the next frame in a video will differ; or after seeing a person's profile face have an idea of her frontal view. This capability is extremely useful to get early warnings about impinging dangers, to be prepared for necessary actions, etc. The vision community has realized that endowing machines with similar capabilities would be rewarding.\n\nSeveral papers have already addressed the generation of an image conditioned on given image(s). Yim et al. [25] and Yang et al. [24] learn to rotate a given face to another pose. The authors of [18,21,20,17,14] train a deep neural network to predict subsequent video frames. Flynn et al. [5] use a deep network to interpolate between views separated by a wide baseline. Yet all these methods apply the exact same set of filtering operations on each and every input image. This seems suboptimal for the tasks at hand. For example, for video prediction, there are different motion patterns within different video clips. The main idea behind our work is to generate the future frames with parameters adapted to the motion pattern within a particular video. Therefore, we propose a learnable parameter layer that provides custom parameters for different samples.\n\nOur dynamic filter module consists of two parts: a filter-generating network and a dynamic filtering layer (see Figure 1). The filter-generating network dynamically generates sample-specific filter parameters conditioned on the network's input. Note that these are not fixed after training, like regular model parameters. The dynamic filtering layer then applies those sample-specific filters to the input. Both components of the dynamic filter module are differentiable with respect to the model parameters such that gradients can be backpropagated throughout the network. The filters can be convolutional, but other options are possible. In particular, we propose a special kind of dynamic filtering layer which we coin dynamic local filtering layer, which is not only sample-specific but also position-specific. The filters in that case vary from position to position and from sample to sample, allowing for more sophisticated operations on the input. Our framework can learn both spatial and photometric changes, as pixels are not simply displaced, but the filters possibly operate on entire neighbourhoods.\n\nWe demonstrate the effectiveness of the proposed dynamic filter module on several tasks, including video prediction and stereo prediction. We also show that, because the computed dynamic filters are explicitly calculated -can be visualised as an image similar to an optical flow or stereo map. Moreover, they are learned in a totally unsupervised way, i.e. without groundtruth maps.\n\nThe rest of paper is organised as follows. In section 2 we discuss related work. Section 3 describes the proposed method. We show the evaluation in section 4 and conclude the paper in section 5."}
{"tabids": ["8d85c51b-a012-4dfc-b2c7-c7930c473701"], "corpus_id": 233444201, "title": "Decoupled Dynamic Filter Networks", "paper_id": "3afbc6386026fe2e694337d0316d3979df7911e5", "abstract": "Convolution is one of the basic building blocks of CNN architectures. Despite its common use, standard convolution has two main shortcomings: Content-agnostic and Computation-heavy. Dynamic filters are content-adaptive, while further increasing the computational overhead. Depth-wise convolution is a lightweight variant, but it usually leads to a drop in CNN performance or requires a larger number of channels. In this work, we propose the Decoupled Dynamic Filter (DDF) that can simultaneously tackle both of these shortcomings. Inspired by recent advances in attention, DDF decouples a depth-wise dynamic filter into spatial and channel dynamic filters. This decomposition considerably reduces the number of parameters and limits computational costs to the same level as depth-wise convolution. Meanwhile, we observe a significant boost in performance when replacing standard convolution with DDF in classification networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the top-1 accuracy, while their computational costs are reduced by nearly half. Experiments on the detection and joint upsampling networks also demonstrate the superior performance of the DDF upsampling variant (DDF-Up) in comparison with standard convolution and specialized content-adaptive layers.", "intro": "Convolution is a fundamental building block of convolutional neural networks (CNNs) that have seen tremendous success in several computer vision tasks, such as image classification, semantic segmentation, pose estimation, to name a few. Thanks to its simple formulation and optimized implementations, convolution has become a de facto standard to propagate and integrate features across image pixels. In this work, we aim to alleviate two of its main shortcomings: Content-agnostic and Computation-heavy. Content-agnostic. Spatial-invariance is one of the prominent properties of a standard convolution. That is, convolu- Figure 1. Comparison between convolution, the dynamic filter, and DDF. Top: Convolution shares a static filter among pixels and samples. Medium: The dynamic filter generates one complete filter for each pixel via a separate branch. Bottom: DDF decouples the dynamic filter into spatial and channel ones. tion filters are shared across all the pixels in an image. Consider the sample road scene shown in Figure 1 (top). The convolution filters are shared across different regions such as buildings, cars, roads, etc. Given the varied nature of contents in a scene, a spatially shared filter may not be optimal to capture features across different image regions [52,42]. In addition, once a CNN is trained, the same convolution filters are used across different images (for instance images taken in daylight and at night). In short, standard convolution filters are content-agnostic and are shared across images and pixels, leading to sub-optimal feature learning. Several existing works [23,48,42,57,49,45,22,11] propose different types of content-adaptive (dynamic) filters for CNNs. However, these dynamic filters are either computeintensive [57,23], memory-intensive [42,22], or specialized processing units [11,48,49,45]. As a result, most of the existing dynamic filters can not completely replace standard convolution in CNNs and are usually used as a few layers of a CNN [49,45,42,22], or in tiny architecture [57,23], or in specific scenarios, like upsampling [48]. Computation-heavy. Despite the existence of highlyoptimized implementations, the computation complexity of standard convolution still increases considerably with the enlarge in the filter size or channel number. This poses a significant problem as convolution layers in modern CNNs have a large number of channels in the orders of hundreds or even thousands. Grouped or depth-wise convolutions are commonly used to reduce the computation complexity. However, these alternatives usually result in CNN performance drops when directly used as a drop-in replacement to standard convolution. To retain similar performance with depth-wise or grouped convolutions, we need to considerably increase the number of feature channels, leading to more memory consumption and access times.\n\nIn this work, we propose the Decoupled Dynamic Filter (DDF) that simultaneously addresses both the abovementioned shortcomings of the standard convolution layer. The full dynamic filter [57,23,49,45] uses a separate network branch to predict a complete convolution filter at each pixel. See Figure 1 (middle) for an illustration. We observe that this dynamic filtering is equivalent to applying attention on unfolded input features, as illustrated in Figure 3. Inspired by the recent advances in attention mechanisms that apply spatial and channel-wise attention [36,50], we propose a new variant of the dynamic filter where we decouple spatial and channel filters. In particular, we adopt separate attention-style branches that individually predict spatial and channel dynamic filters, which are then combined to form a filter at each pixel. See Figure 1 (bottom) for an illustration of DDF. We observe that this decoupling of the dynamic filter is efficient yet effective, making DDF to have similar computational costs as depth-wise convolution while achieving better performance against existing dynamic filters. This lightweight nature enables DDF to be directly inserted as a replacement of the standard convolution layer. Unlike several existing dynamic filtering layers, we can replace all k \u00d7 k (k > 1) convolutions in a CNN with DDF. We also propose a variant of DDF, called DDF-Up, that can be used as a specialized upsampling or jointupsampling layer.\n\nWe empirically validate the performance of DDF by drop-in replacing convolution layers in several classification networks with DDF. Experiments indicate that applying DDF consistently boosts the performance while reducing computational costs. In addition, we also demonstrate the superior upsampling performance of DDF-Up in object detection and joint upsampling networks. In summary, DDF and DDF-Up have the following favorable properties:\n\n\u2022 Content-adaptive. DDF provides spatially-varying filtering that makes filters adaptive to image contents.\n\n\u2022 Fast runtime. DDF has similar computational costs as depth-wise convolution, so its inference speed is faster than both standard convolution and dynamic filters. \u2022 Smaller memory footprint. DDF significantly reduces memory consumption of dynamic filters, making it possible to replace all standard convolution layers with DDF. \u2022 Consistent performance improvements. Replacing a standard convolution with DDF / DDF-Up results in consistent improvements and achieves the state-of-the-art performance across various networks and tasks."}
{"tabids": ["8d85c51b-a012-4dfc-b2c7-c7930c473701"], "corpus_id": 218630285, "title": "TAM: Temporal Adaptive Module for Video Recognition", "paper_id": "3ac6ad718bbdeda6b5b00b61983f8b520d8a6bcb", "abstract": "Video data is with complex temporal dynamics due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adaptive module ({\\bf TAM}) to generate video-specific temporal kernels based on its own feature map. TAM proposes a unique two-level adaptive modeling scheme by decoupling the dynamic kernel into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture short-term information, while the aggregation weight is generated from a global view with a focus on long-term structure. TAM is a modular block and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The extensive experiments on Kinetics-400 and Something-Something datasets demonstrate that our TAM outperforms other temporal modeling methods consistently, and achieves the state-of-the-art performance under the similar complexity. The code is available at \\url{ https://github.com/liu-zhy/temporal-adaptive-module}.", "intro": "Deep learning has brought great progress for various recognition tasks in image domain, such as image classification [21,12], object detection [28], and instance segmentation [11]. The key to these successes is to devise flexible and efficient architectures that are capable of learning powerful visual representations from large-scale image datasets [4]. However, deep learning research progress in video understanding is relatively slower, partially due to the high complexity of video data. The core technical problem in video understanding is to design an effective temporal module, that is expected to be able to capture complex temporal structure with high flexibility, while yet to be of : Corresponding author. low computational consumption for processing high dimensional video data efficiently.\n\n3D Convolutional Neural Networks (3D CNNs) [15,34] have turned out to be mainstream architectures for video modeling [1,8,36,27]. The 3D convolution is a direct extension over its 2D counterparts and provides a learnable operator for video recognition. However, this simple extension lacks specific consideration about the temporal properties in video data and might as well lead to high computational cost. Therefore, recent methods aim to model video sequences in two different aspects by combining a lightweight temporal module with 2D CNNs to improve efficiency (e.g., TSN [40], TSM [23]), or designing a dedicated temporal module to better capture temporal relation (e.g., Nonlocal Net [41], ARTNet [38], STM [17], TDN [39]). However, how to devise a temporal module with both high efficiency and strong flexibility still remains to be an unsolved problem. Consequently, we aim at advancing the current video architectures along this direction.\n\nIn this paper, we focus on devising an adaptive module to capture temporal information in a more flexible way. Intuitvely, we observe that video data is with extremely complex dynamics along the temporal dimension due to factors such as camera motion and various speeds. Thus 3D convolutions (temporal convolutions) might lack enough representation power to describe motion diversity by simply employing a fixed number of video invariant kernels. To deal with such complex temporal variations in videos, we argue that adaptive temporal kernels for each video are effective and as well necessary to describe motion patterns. To this end, as shown in Figure 1, we present a two-level adaptive modeling scheme to decompose the video specific temporal kernel into a location sensitive importance map and a location invariant (also video adaptive) aggregation kernel. This unique design allows the location sensitive importance map to focus on enhancing discriminative temporal information from a local view, and enables the video adaptive aggregation to capture temporal dependencies with a global view of the input video sequence.\n\nSpecifically, the design of temporal adaptive module  Figure 1. Temporal module comparisons: The standard temporal convolution shares weights among videos and may lack the flexibility to handle video variations due to the diversity of videos. The temporal attention learns position sensitive weights by assigning varied importance for different time without any temporal interaction, and may ignore the long-range temporal dependencies. Our proposed temporal adaptive module (TAM) presents a two-level adaptive scheme by learning the local importance weights for location adaptive enhancement and the global kernel weights for video adaptive aggregation. \u2299 is attention operation, and \u2297 is convolution operation.\n\n(TAM) strictly follows two principles: high efficiency and strong flexibility. To ensure our TAM with a low computational cost, we first squeeze the feature map by employing a global spatial pooling, and then establish our TAM in a channel-wise manner to keep the efficiency. Our TAM is composed of two branches: a local branch (L) and a global branch (G). As shown in Fig. 2, TAM is implemented in an efficient way. The local branch employs temporal convolutions to produce the location sensitive importance maps to enhance the local features, while the global branch uses fully connected layers to produce the location invariant kernel for temporal aggregation. The importance map generated by a local temporal window focuses on short-term motion modeling and the aggregation kernel using a global view pays more attention to the long-term temporal information. Furthermore, our TAM could be flexibly plugged into the existing 2D CNNs to yield an efficient video recognition architecture, termed as TANet.\n\nWe verify the proposed TANet on the task of action classification in videos. In particular, we first study the performance of the TANet on the Kinetics-400 dataset, and demonstrate that our TAM is better at capturing temporal information than other several counterparts, such as temporal pooling, temporal convolution, TSM [23], TEINet [24], and Non-local block [41]. Our TANet is able to yield a very competitive accuracy with the FLOPs similar to 2D CNNs. We further test our TANet on the motion dominated dataset of Something-Something, where the state-of-the-art performance is achieved."}
{"tabids": ["8d85c51b-a012-4dfc-b2c7-c7930c473701"], "corpus_id": 260866000, "title": "Temporally-Adaptive Models for Efficient Video Understanding", "paper_id": "fe779d52ef85c0387ed9a68cd90ca11033689bfa", "abstract": "Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.", "intro": "Convolutions are an indispensable operation in modern deep vision models [1], [2], [3], [4], whose different variants have driven the state-of-the-art performances of convolutional neural networks (CNNs) in many visual tasks [5], [6], [7], [8], [9] and application scenarios [10], [11]. In the video paradigm, compared to the 3D convolutions [12], the combination of 2D spatial convolutions and 1D temporal convolutions is more widely preferred owing to its efficiency [13], [14]. Nevertheless, 1D temporal convolutions introduce non-negligible computation overhead on top of the spatial convolutions. Therefore, we seek to directly equip spatial convolutions with temporal modeling abilities.\n\nOne essential property of convolutions is the translation invariance [15], [16], resulting from its local connectivity and shared weights. However, recent works in dynamic filtering have shown that strictly shard weights for all pixels may be sub-optimal for modeling various spatial contents [17], [18].\n\nGiven the diverse nature of the temporal dynamics in videos, we hypothesize that temporal modeling could benefit from relaxed invariance along the temporal dimension. This means that convolution weights for different time steps are no longer strictly shared. Existing dynamic filter networks \u2022 * Correspondence to Shiwei Zhang (zhangjin.zsw@alibaba-inc.com) and\n\nMarcelo H. Ang Jr (mpeangh@nus.edu.sg). could achieve this but with two drawbacks. (i) it is difficult for most of them [11], [17] to leverage pre-trained weights, which is critical in video applications since training video models from scratch is highly resource demanding [19], [20] and prone to over-fitting on small datasets. (ii) for most dynamic filters, the weights are generated with respect to its spatial context [17], [21] or the global descriptor [11], [22], which is incapable of capturing the fine-grained temporal variations between frames. Motivated by this, we present Temporally-Adaptive Convolution (TAdaConv) for video understanding, where the convolution weights are no longer fixed across different frames. Specifically, the convolution kernel for the t-th frame W t is factorized to the multiplication of the base weight and a calibration weight: W t = \u03b1 t \u00b7 W b , where the base weight W b is learnable and the calibration weight \u03b1 t is adaptively generated from the input data in the base weight W b . For each frame, we generate the calibration weight based on the frame descriptors of its adjacent time steps as well as the global descriptor, which effectively encodes both local and global temporal dynamics in videos. The difference between TAdaConv and standard convolutions is visualized in Fig. 1.\n\nThe main advantages of this factorization are threefold: (i) TAdaConv can be easily plugged into any existing models to enhance temporal modeling, and their pretrained weights can still be exploited; (ii) the temporal modeling ability can be highly improved with the help of the temporally-adaptive weight; (iii) in comparison with temporal convolutions that often operate on the learned 2D feature maps, TAdaConv is more efficient by directly operating on the convolution kernels.\n\nTAdaConv is proposed as a drop-in replacement for the convolutions in existing models. A preliminary version of this work [23] is published in ICLR 2022, where TAda- Conv has demonstrated a strong capability of temporal modeling, introducing notable performance gains to both image-based models as well as existing video models. In this work, we follow the conceptual idea of TAdaConv and present improvements to the preliminary version on both structural designs as well as model and data scaling. In terms of structural designs, we optimize TAdaConv in the following aspects: (i) At the operation level, the calibration factor generation process of TAdaConv is optimized, where multi-head self-attention [24] is introduced for modeling the global information of the videos. (ii) At the block level, we construct stronger TAdaBlocks by introducing efficient temporal feature aggregation, which we use to construct our convolutional model TAdaConvNeXtV2 and transformer TAdaFormer. Our empirical results show a notable improvement brought by our modifications on both scene-and motion-centric benchmarks. Based on the TAdaConvNeXtV2 and TAdaFormer, we further scale up both the model and data scale, which lead to a competitive performance to existing state-of-the-art approaches."}
