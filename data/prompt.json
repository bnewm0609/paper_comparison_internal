{
    "extract_presupposition": {
        "prompt": "Identify a presupposition of the given question as many as possible.\n\nQuestion: What is the problem with instructional videos online for online learners this paper tackles?\nPresupposition: This paper tackles the problem with instructional videos online for online learners.\n\nQuestion: What kind of human-AI collaboration is involved in this paper?\nPresupposition: Human-AI collaboration is involved in this paper.\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "evaluate_presupposition": {
        "prompt": "Identify whether a given paper satisfies a given presupposition based on a given title and abstract.\n\nPresupposition: This paper tackles the problem with instructional videos online for online learners.\nPaper title: Promptiverse: Scalable Generation of Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation\nPaper abstract: Online learners are hugely diverse with varying prior knowledge, but most instructional videos online are created to be one-size-fits-all. Thus, learners may struggle to understand the content by only watching the videos. Providing scaffolding prompts can help learners overcome these struggles through questions and hints that relate different concepts in the videos and elicit meaningful learning. However, serving diverse learners would require a spectrum of scaffolding prompts, which incurs high authoring effort. In this work, we introduce Promptiverse, an approach for generating diverse, multi-turn scaffolding prompts at scale, powered by numerous traversal paths over knowledge graphs. To facilitate the construction of the knowledge graphs, we propose a hybrid human-AI annotation tool, Grannotate. In our study (N=24), participants produced 40 times more on-par quality prompts with higher diversity, through Promptiverse and Grannotate, compared to hand-designed prompts. Promptiverse presents a model for creating diverse and adaptive learning experiences online.\nSatisfied: True\n\nPresupposition: Human-AI collaboration is involved in this paper.\nPaper title: ConceptScape: Collaborative Concept Mapping for Video Learning\nPaper abstract: While video has become a widely adopted medium for online learning, existing video players provide limited support for navigation and learning. It is difficult to locate parts of the video that are linked to specific concepts. Also, most video players afford passive watching, thus making it difficult for learners with limited metacognitive skills to deeply engage with the content and reflect on their understanding. To support concept-driven navigation and comprehension of lecture videos, we present ConceptScape, a system that generates and presents a concept map for lecture videos. ConceptScape engages crowd workers to collaboratively generate a concept map by prompting them to externalize reflections on the video. We present two studies to show that (1) interactive concept maps can be useful tools for concept-based video navigation and comprehension, and (2) with ConceptScape, novice crowd workers can collaboratively generate complex concept maps that match the quality of those by experts.\nSatisfied: False\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "generate_answer": {
        "prompt": "Answer a given question based on a given paper's title and abstract a short phrase (within fifteen words).\n\nQuestion: What is the problem with instructional videos online for online learners this paper tackles?\nPaper title: ConceptScape: Collaborative Concept Mapping for Video Learning\nPaper abstract: While video has become a widely adopted medium for online learning, existing video players provide limited support for navigation and learning. It is difficult to locate parts of the video that are linked to specific concepts. Also, most video players afford passive watching, thus making it difficult for learners with limited metacognitive skills to deeply engage with the content and reflect on their understanding. To support concept-driven navigation and comprehension of lecture videos, we present ConceptScape, a system that generates and presents a concept map for lecture videos. ConceptScape engages crowd workers to collaboratively generate a concept map by prompting them to externalize reflections on the video. We present two studies to show that (1) interactive concept maps can be useful tools for concept-based video navigation and comprehension, and (2) with ConceptScape, novice crowd workers can collaboratively generate complex concept maps that match the quality of those by experts.\nAnswer: The paper tackles the limited support for navigation, comprehension, and active engagement in online instructional videos.\n\nQuestion: What kind of human-AI collaboration is involved in this paper?\nPaper title: Promptiverse: Scalable Generation of Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation\nPaper abstract: Online learners are hugely diverse with varying prior knowledge, but most instructional videos online are created to be one-size-fits-all. Thus, learners may struggle to understand the content by only watching the videos. Providing scaffolding prompts can help learners overcome these struggles through questions and hints that relate different concepts in the videos and elicit meaningful learning. However, serving diverse learners would require a spectrum of scaffolding prompts, which incurs high authoring effort. In this work, we introduce Promptiverse, an approach for generating diverse, multi-turn scaffolding prompts at scale, powered by numerous traversal paths over knowledge graphs. To facilitate the construction of the knowledge graphs, we propose a hybrid human-AI annotation tool, Grannotate. First, Grannotate has two representations of an annotated graph: overlaid on the transcript and a graph visualization. Our tool also shows how prompts would be created out of the annotation. Finally, to alleviate the burden of having many options for entities and relation classes, our tool provides following AI-driven recommendations: 1) entity recommendation, 2) relation existence recommendation, and 3) relation class recommendation. In our study (N=24), participants produced 40 times more on-par quality prompts with higher diversity, through Promptiverse and Grannotate, compared to hand-designed prompts. Promptiverse presents a model for creating diverse and adaptive learning experiences online.\nAnswer: The paper involves human-AI collaboration in knowledge graph annotation for diverse scaffolding prompt generation.\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "is_repetitive": {
        "prompt": "Determine whether a given question is repetitive to a given question list.\n\nQuestion: What is the problem with instructional videos online for online learners this paper tackles?\nQuestion list: [“What learners’ problem does the paper address regarding online instructional videos?”, “What are the issues that online learners face when navigating online instructional videos as per the study?“, “What comprehension and engagement problems are discussed in this paper with regard to online instructional videos?“]\nRepetitive: True\n\nQuestion: What kind of human-AI collaboration is involved in this paper?\nQuestion list: [“What is the role of AI in this collaboration?”, “What are the benefits and challenges of using human-AI collaboration in knowledge graph annotation?“]\nRepetitive: False\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "generate_followup": {
        "prompt": "Generate a follow-up question that asks further information based on a given question and answer. Return a list of questions as follows: \"[\"<followup_question>\", ...]\"\n\nQuestion: What is the problem with instructional videos online for online learners this paper tackles?\nAnswer: The paper tackles the limited support for navigation, comprehension, and active engagement in online instructional videos.\nFollow-up question: [\"What solutions does the paper propose to improve navigation, comprehension, and active engagement in online instructional videos?\", \"How does the lack of support for navigation, comprehension, and active engagement affect online learners according to the paper?\"]\n\nQuestion: What kind of human-AI collaboration is involved in this paper?\nAnswer: The paper involves human-AI collaboration in knowledge graph annotation for diverse scaffolding prompt generation.\nFollow-up question: [\"Can you elaborate on how human-AI collaboration is utilized for diverse scaffolding prompt generation in knowledge graph annotation?\", \"What are the benefits and challenges of using human-AI collaboration in knowledge graph annotation?\", \"What is the role of AI in this collaboration?\"]\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "generate_lowlevel": {
        "prompt": "Generate low-level questions that ask for information already in the given answer to the main question. Prevent generating repetitive questions. Return a list of questions as follows: \"[\"<lowlevel_question>\", ...]\"\n\nMain question: What is the problem with instructional videos online for online learners this paper tackles?\nAnswer: The paper tackles the limited support for navigation, comprehension, and active engagement in online instructional videos.\nLow-level question: [\"What problem does this paper address?\", \"What is the issue with instructional videos online?\", \"Which videos does this paper address?\"]\n\nMain question: What kind of human-AI collaboration is involved in this paper?\nAnswer: The paper involves human-AI collaboration in knowledge graph annotation for diverse scaffolding prompt generation.\nLow-level question: [\"What type of collaboration does this paper involve?\", \"What kind of annotation is the human-AI collaboration involved in?\", \"What's the purpose of the knowledge graph annotation?\"]\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "generate_generic": {
        "prompt": "Generate questions by abstracting specific details from the original question and replacing them with general terms. Prevent generating repetitive questions. Return a list of questions as follows: \"[\"<generic_question>\", ...]\"\n\nOriginal Question: What kind of human-AI collaboration is involved in this paper used for prompt generation?\nGeneric questions: [\"What kind of human-AI collaboration is involved in this paper?\"]\n\nOriginal Question: What were the metrics used to measure the quality and diversity of the prompts in this paper?\nGeneric questions: [\"What were the metrics used to measure the quality and diversity of the artifacts?\", \"What were the metrics used to measure the artifacts?\"]\n\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "zero_shot_paper_to_table": {
        "prompt": "[System]\nWe would like you to build a table that has each paper as a row and each important dimension to be compared as a column. You will be given multiple papers labeled A, B, and C. You will be provided with the title of each paper and abstract.\n\n{paper}[System]\nPlease create a table that compares and contrasts given papers. Please identify relevant questions for which comparisons/contrasts can be made among the answers to the question acquired from each paper. Return the list of pairs of question and the dictionary of each paper's answer as a Python JSON object of the following format: \"{{\"<question that asks the content of given papers> \": {{\"paper_1\": [\"<answer to the identified question grounded on Paper 1>\"], \"paper_2\":[\" <answer to the identified question grounded on Paper 2>\"], \"paper_3\": [\"<answer to the identified question grounded on Paper 3>\"]}}, ..}}\n\n[Table]",
        "prompt_max": "[System]\nWe would like you to build a table that has each paper as a row and each important dimension to be compared as a column. You will be given multiple papers labeled A, B, and C. You will be provided with the title of each paper and abstract.\n\n{paper}[System]\nPlease create a table that compares and contrasts given papers. Please identify questions for which comparisons/contrasts can be made with the answers to the questions acquired from each paper. Make questions as many as possible. Return the list of pairs of questions and the dictionary of each paper's answer as a Python JSON object of the following format: \"{{\"<question that asks the content of given papers> \": {{\"paper_1\": [\"<answer to the identified question grounded on Paper 1>\"], \"paper_2\":[\" <answer to the identified question grounded on Paper 2>\"], \"paper_3\": [\"<answer to the identified question grounded on Paper 3>\"]}}, ..}}\n\n[Table]",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "zero_shot_paper_to_cc": {
        "prompt": "[System]\nWe would like you to compare and contrast multiple research papers. You will be provided with the title of each paper and their abstracts. Make compare and contrast statements with the bullet points. One statement with one bullet point. \n\n{paper}[Response]",
        "prompt_max": "[System]\nWe would like you to compare and contrast multiple research papers. You will be provided with the title of each paper and their abstracts. Make compare and contrast statements with the bullet points as many as possible. One statement with one bullet point. \n\n{paper}[Response]",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "zero_shot_cc_to_table": {
        "prompt": "[System]\nWe would like you to build a table that has each paper as a row and each important dimension to be compared as a column. You will be given multiple papers labeled A, B, and C. You will be provided with the title of each paper and abstract. You will also be given comparison and contrast statements to help you to build a table. {paper_cc}[System]\nPlease create a table that compares and contrasts given papers grounded on given statements. Please extract dimensions that compare those papers from each statement. For each of the extracted dimension, find the value relevant to the dimension from each paper. Return the list of pairs of dimension and the dictionary of each paper's value as a Python JSON object of the following format: \"{{\"<dimension of comparison and contrast from the given statements> \": {{\"paper_1\": [\"<relevant value to the identified dimension grounded on Paper 1>\"], \"paper_2\":[\" <relevant value to the identified dimension grounded on Paper 2>\"], \"paper_3\": [\"<relevant value to the identified dimension grounded on Paper 3>\"]}}, ..}}\"\n\n[Table]",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "zero_shot_paper_to_scheme": {
        "prompt": "[System]\nWe would like you to generate as many questions that can be answered with at least one paper as possible. You will be provided with the title of each paper and their abstracts. Return a list of questions as follows: \"[\"<question that can be answered with at least one paper>\", ...]\"\n\n{paper}[Response]",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "zero_shot_pap_to_tab_iteration": {
        "prompt": "[System]\nWe have a table that compares and contrasts given papers. The table is given as the list of pairs of questions and the dictionary of each paper's answer as a Python JSON object of the following format: \"{\"<question that asks the content of given papers> \": {\"paper_1\": [\"<answer to the identified question grounded on Paper 1>\"], \"paper_2\":[\" <answer to the identified question grounded on Paper 2>\"], \"paper_3\": [\"<answer to the identified question grounded on Paper 3>\"]}, ..}. \n\nPaper: {}\n\nTable: {}\n\n[System]\nWe would like you to add more questions for which comparisons/contrasts can be made with the answers to the questions acquired from each paper. Make questions as many as possible but the questions to be added should be different from existing questions in the table we provided. Return the list of pairs of questions and the dictionary of each paper's answer as a Python JSON object as we provided as the input.\n\n[Table]",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    },
    "zero_shot_onepap_to_scheme": {
        "prompt": "[System]\nWe would like you to generate as many questions that can be answered with the given paper as possible. You will be provided with the title and abstract of the paper. Return a list of questions as follows: \"[\"<question that can be answered with the given paper>\", ...]\"\n\n[Paper]\n{paper}\n\n[Questions]\n",
        "system_instruction": "You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers."
    }
}