[{"paperid": "paper0", "title": "IPatch: a remote adversarial patch", "abstract": "Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image. In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples `remote adversarial patches' (RAP). We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.", "introduction": "\n\nDeep learning has become the go-to method for automating image-based tasks.This is because, deep neural networks (DNNs) are excellent at learning and identifying spatial patterns and abstract concepts. With advances in both hardware and neural architectures, deep learning has become both a practical and reliable solution. Companies now use image-based deep learning to automate tasks in life critical operations such as autonomous driving [1], [2], surveillance [3], and medical image screening [4].\n\nIn tasks such as these, multiple objects must be identified per image. One way to accomplish this is to predict a class probability for each pixel in the input image . This approach is called image segmentation and companies such as Telsa use it to guide their autonomous vehicles safely through an environment [2]. Another approach is called object detection where is split into a grid of cells or regions and the model predicts both a class probability and a bounding box for each of them [5], [6]. In both cases, these models rely on image semantics to successfully parse and interpret a scene.\n\nJust like other deep learning models, these semantic models are also susceptible to adversarial attacks. In 2017, researchers demonstrated how a small 'adversarial' patch can be placed in a real world scene and override an image-classifier's prediction, regardless of the patch's location or orientation [7]. This gave rise to a number of works which demonstrated the concept of adversarial patches against image segmentation and object detection models [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19]. However, current adversarial patches are limited in the following ways: Location Only predictions around the patch itself are explicitly affected. This limits where objects can be made to 'appear'\n\nThe author is with Ben-Gurion University (e-mail: yisroel@post.bgu.ac.il see https://ymirsky.github.io/).\n\n\n"}, {"paperid": "paper1", "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks", "abstract": "Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.", "introduction": "\n\nThe rise of deep learning unlocked unprecedented performance in several scientific areas [24]. Convolutional (a) (b) (c) (d) (e) (f) Figure 1: Proposed adversarial patches on Cityscapes [5] (b) and CARLA Simulator [6] (e); (c/f) show the corresponding SS predicted by BiSeNet [40]; (a/d) show the corresponding predictions obtained using random patches instead of adversarial ones.\n\nneural networks [16] (CNNs) yielded super-human performance for many different computer vision tasks, such as image recognition [9], object detection [27] [26], and image segmentation [20]. Image segmentation, and semantic segmentation (SS) in particular, is used in autonomous driving perception pipelines [30], mainly for object detection [20]. Despite their high performance, CNNs are prone to adversarial attacks [31]. Most of the literature on adversarial attacks focuses on directly manipulating the pixels of the whole image, hence making the assumption that the attacker has control over the digital representation of the environment obtained by the on-board cameras. This kind of unsafe inputs are called digital adversarial examples.\n\nAlthough such digital attacks do not transfer well into the real world, they continue to be used to evaluate the robustness of models in safety-critical systems [12,3,19]. Realworld adversarial examples (RWAEs), on the other hand, are physical objects that can be placed in the field of view of a camera, such that the resulting image acts as an adversarial example for the neural network under attack [17]. Thus, RWAEs can induce errors in neural networks without requiring the attacker to access the digital representation of the image, thereby making them a more realistic and dangerous threat to safety-critical systems. This paper. This work focuses on RWAEs, as they repre-sent a potential threat to tasks in autonomous driving today. Although the effects of RWAEs have been studied extensively in the literature for classification and object detection, those on SS remain relatively unexplored. However, SS is an integral part of autonomous driving pipelines [30]. Thus, this paper examines various state-of-the-art models for real-time SS aiming at benchmarking their robustness to RWAEs in autonomous driving scenarios.\n\nOf the several types of RWAEs proposed in the literature [32], the form of attack used in this paper is adversarial patches [4]. This is because attacks that perturb the whole image are not practically feasible in the real world. Conversely, such patches can be easily printed and attached to any visible 2D surface in the driving environment, such as billboards and road signs, thus making them a simple, yet effective attack strategy.\n\nThe paper starts by recognizing the shortcomings of the standard cross-entropy loss for optimizing adversarial patches for SS. Thus, an extension to the cross-entropy loss is proposed and integrated in all the performed attacks. This extension forces the optimization to focus on pixels that are not yet misclassified, thus obtaining patches that are more powerful compared to those generated with the standard cross-entropy-based setting [21].\n\nFollowing this rationale, the robustness of real-time SS models to RWAEs attacks is benchmarked. The paper starts by first examining the case of driving images, crafting adversarial patches on the Cityscapes dataset [5], a popular benchmark of high-resolution images of urban driving. Robust real-world patches are crafted by following the Expectation Over Transformation (EOT) [2] paradigm, which has been extended in this work to attack SS models. Furthermore, a comparison against non-robust patches (without EOT) is presented to question their effectiveness on driving scenes.\n\nAnother set of experiments targeted a virtual 3D scenario, for which a stronger adversarial attack is presented and tested. The proposed scene-specific attack, defined in Section 3.4, is a more practical tool for crafting adversarial patches in a realistic autonomous driving scenario. It assumes that the attacker is interested in targeting an autonomous driving scene at a particular corner of a specific town, where information about the position of the attackable 2D surface (in our case, a billboard) is available. To satisfy such requirements we developed and tested this attack using the CARLA simulator, which provides all the needed geometric information. These experiments include a comparison with the EOT-based and non-robust patches, performed by importing them into the CARLA world and placing them on billboards to simulate a realistic study. Figure 1 provides some examples of the effect of our patches on Cityscapes and CARLA.\n\nThe last set of experiments were conducted on a real-world driving scenario, which required collecting a dataset within the city, optimizing a patch on it, physically printing said patch on a billboard, and finally evaluating SS models on images containing the printed patch.\n\nTo the best of our knowledge, this work represents the first exhaustive evaluation of the robustness of SS models against RWAEs for autonomous driving systems. The results of the experiments state important observations that should be taken into consideration while evaluating the trustworthiness of SS models in autonomous driving. First, they demonstrate that non-robust patches are not good candidates for assessing the practical robustness of an SS model to adversarial examples. Indeed, while they proved to be effective in attacking images related to driving scenes (from Cityscapes), they do not induce any real-world adversarial effect when crafted and tested in a virtual 3D world (based on CARLA). Conversely, robust patches, crafted with EOT or the proposed scene-specific approach, resulted to be less effective than non-robust ones on Cityscapes images, but were capable to accomplish the attack in both virtual 3D world and the real world. Nevertheless, their effectiveness in the latter two cases still resulted to be quite limited, hence questioning the practical relevance of RWAEs.\n\nIn summary, the paper makes the following contributions:\n\n\u2022 It proposes an extension to the pixel-wise crossentropy loss to enable crafting strong patches for the semantic segmentation setting. \u2022 It proposes a novel technique for crafting adversarial patches for autonomous driving scenarios that utilize geometric information of the 3D world. \u2022 It finally reports an extensive evaluation of RWAEbased attacks on a set of real-time semantic segmentation models using data from the Cityscapes dataset, CARLA, and the real world. The remainder of this paper is organized as follows: Section 2 provides a brief overview of related work existing in the literature, Section 3 formalizes the proposed loss function, pipeline, and attack strategy, Section 4 reports the experimental results, and Section 5 states the conclusions and proposes ideas for future work.\n\n\n"}]