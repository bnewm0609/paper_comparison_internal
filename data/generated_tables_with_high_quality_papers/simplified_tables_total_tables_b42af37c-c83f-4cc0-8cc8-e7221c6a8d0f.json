{"pap_to_tab": {"Main focus of the study": {"paper_1": ["Creating a subjectively annotated video quality assessment (VQA) database called KoNViD-1k with authentic distortions and a wide variety of content."], "paper_2": ["Constructing a large-scale video quality assessment database (LIVE-VQC) to advance no-reference (NR) video quality prediction on real-world videos."], "paper_3": ["Introducing a large scale UGC dataset for understanding compression and quality assessment challenges on non-pristine originals."], "paper_4": ["Developing the largest subjective video quality dataset and new no-reference VQA models (PVQ and PVQ Mapper) to handle video quality problems."], "paper_5": ["Introducing a new large and diverse in-the-wild VQA dataset called KonVid-150k and proposing new efficient VQA approaches (MLSP-VQA)."], "paper_6": ["Extensive empirical evaluation of CNNs on large-scale video classification with a focus on spatio-temporal information in the videos."], "paper_7": ["Describing the DeepMind Kinetics human action video dataset and providing baseline performance figures and analysis for human action classification."]}, "Types of data presented in the study": {"paper_1": ["1,200 public-domain video sequences with subjective mean opinion scores (MOS)."], "paper_2": ["585 videos with a diverse range of content and authentic distortions, along with over 205,000 opinion scores from crowdsourced subjective video quality assessments."], "paper_3": ["Large scale dataset of 1,500 20-second video clips sampled from YouTube covering various categories and features."], "paper_4": ["39,000 real-world distorted videos, 117,000 space-time localized video patches ('v-patches'), and 5.5 million human perceptual quality annotations."], "paper_5": ["A dataset consisting of 153,841 videos with five quality ratings each, and 1,596 videos with a minimum of 89 ratings each."], "paper_6": ["A dataset of 1 million YouTube videos belonging to 487 classes, for studying CNNs in video classification tasks."], "paper_7": ["A dataset containing 400 human action classes with at least 400 video clips for each action, sourced from YouTube videos."]}, "Methodology used for quality assessment": {"paper_1": ["Subjective video quality assessment through mean opinion scores (MOS)."], "paper_2": ["Crowdsourced subjective video quality assessments to collect a large number of opinion scores, benchmarking of NR video quality predictors."], "paper_3": ["No-reference objective quality metrics such as Noise, Banding, and SLEEQ for evaluating UGC video quality."], "paper_4": ["Subjective annotations, development of two unique NR-VQA models and space-time video quality mapping (PVQ Mapper)."], "paper_5": ["Multi-level spatially pooled deep-features (MLSP) for no-reference video quality assessment (VQA), Spearman rank-order correlation coefficient (SRCC) performance metric."], "paper_6": ["Empirical evaluation of CNNs for video classification without explicit focus on quality assessment."], "paper_7": ["No explicit methodology for quality assessment described; focus is on human action classification performance baselines."]}, "Proposed contributions": {"paper_1": ["KoNViD-1k database intended to assist the development and evaluation of objective VQA methods."], "paper_2": ["The LIVE Video Quality Challenge Database (LIVE-VQC) and a study comparing leading NR video quality predictors."], "paper_3": ["A novel sampling method for UGC dataset and discussion of challenges for UGC compression and quality evaluation."], "paper_4": ["The largest subjective video quality dataset, two novel NR-VQA models (PVQ and PVQ Mapper), and future database and model availability."], "paper_5": ["The KonVid-150k dataset for in-the-wild VQA, new MLSP-VQA approaches, and demonstration of improved generalization and state-of-the-art performance."], "paper_6": ["Comprehensive study of CNNs on video classification using a large dataset and exploration of spatio-temporal network architectures."], "paper_7": ["Kinetics human action video dataset and insights into how imbalance in the dataset influences classifier bias."]}}, "cc_to_tab": {"dataset_size": {"paper_1": ["1,200 video sequences"], "paper_2": ["585 videos"], "paper_3": ["1,500 20-sec video clips"], "paper_4": ["39,000 videos and 117,000 v-patches"], "paper_5": ["153,841 videos, 1,596 densely annotated"], "paper_6": ["1 million YouTube videos"], "paper_7": ["At least 400 clips for each of the 400 actions"]}, "dataset_focus": {"paper_1": ["No-reference video quality assessment"], "paper_2": ["No-reference video quality assessment"], "paper_3": ["Compression and quality assessment of UGC"], "paper_4": ["No-reference perceptual video quality assessment"], "paper_5": ["No-reference video quality assessment of in-the-wild videos"], "paper_6": ["Video classification with CNNs"], "paper_7": ["Human action video dataset"]}, "content_diversity": {"paper_1": ["Variety of content and authentic distortions"], "paper_2": ["Unique contents, capture devices, distortion types"], "paper_3": ["Popular categories like Gaming, Sports, HDR features"], "paper_4": ["Real-world distorted videos and local video patches"], "paper_5": ["Substantially larger and diverse, in-the-wild videos"], "paper_6": ["Large-scale video classification across 487 classes"], "paper_7": ["400 human action classes, broad range"]}, "data_collection_methodology": {"paper_1": ["Subjectively annotated"], "paper_2": ["Crowdsourcing with 4776 participants, 205000 opinion scores"], "paper_3": ["Sampling based on features from encoding"], "paper_4": ["5.5M human perceptual quality annotations"], "paper_5": ["Coarsely annotated with five quality ratings each, some with at least 89 ratings"], "paper_6": ["Empirical evaluation on a new dataset"], "paper_7": ["Detailed collection description and baseline figures"]}, "UGC_focus": {"paper_1": ["No specific focus on UGC"], "paper_2": ["No specific focus on UGC"], "paper_3": ["UGC compression and quality evaluation"], "paper_4": ["Emphasis on UGC video data"], "paper_5": ["Focus on in-the-wild UGC"], "paper_6": ["Videos from YouTube, UGC mentioned but not the focus"], "paper_7": ["YouTube videos are source, but focus is on human actions rather than UGC as a category"]}, "reference_to_real_world_conditions": {"paper_1": ["'In the wild' authentic distortions"], "paper_2": ["Real-world video data with authentic distortions"], "paper_3": ["Non-pristine originals from YouTube"], "paper_4": ["Real-world UGC video data"], "paper_5": ["In-the-wild videos"], "paper_6": ["Use of YouTube videos implies real-world relevance"], "paper_7": ["YouTube videos, but real-world conditions not explicitly stated"]}}, "multi_scheme": {"number of human perceptual quality annotations": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["5.5M"], "paper_5": [], "paper_6": [], "paper_7": []}, "traditional metrics mentioned as less accurate for non-pristine originals": {"paper_1": [], "paper_2": [], "paper_3": ["BD-Rate", "PSNR"], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "Spearman rank-order correlation coefficient (SRCC) performance": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": ["improves to 0.82"], "paper_6": [], "paper_7": []}, "SRCC values on KoNViD-1k and LIVE-Qualcomm datasets": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": ["0.83", "0.64"], "paper_6": [], "paper_7": []}, "experiments under different levels of label noise and dataset size": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": ["investigated"], "paper_6": [], "paper_7": []}, "release timing of new database and prediction models": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["immediately following the review process"], "paper_5": [], "paper_6": [], "paper_7": []}, "video sampling method in UGC dataset": {"paper_1": [], "paper_2": [], "paper_3": ["novel sampling method based on features extracted from encoding"], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "purpose of PVQ Mapper": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["helps localize and visualize perceptual distortions in space and time"], "paper_5": [], "paper_6": [], "paper_7": []}, "shortcomings of traditional reference-based metrics on UGC": {"paper_1": [], "paper_2": [], "paper_3": ["addressed"], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "performance difference between spatio-temporal networks and single-frame models": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": [], "paper_6": ["4.6% improvement"], "paper_7": []}, "study dimensions where this is the largest video quality assessment study": {"paper_1": [], "paper_2": ["number of unique contents, capture devices, distortion types, study participants, subjective scores"], "paper_3": [], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "extent LIVE-VQC database represents real world videos": {"paper_1": [], "paper_2": ["representing a wide range of levels of complex, authentic distortions"], "paper_3": [], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "public video dataset from which KoNViD-1k video sequences were sampled": {"paper_1": ["YFCC100m"], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "performance analysis based on potential bias due to dataset imbalance": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": ["preliminary analysis conducted"]}, "kind of extracted features used in novel sampling method": {"paper_1": [], "paper_2": [], "paper_3": ["features extracted from encoding"], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "performance of current NR video quality models on real-world video data": {"paper_1": [], "paper_2": ["often perform poorly"], "paper_3": [], "paper_4": ["current NR models are limited in their prediction capabilities"], "paper_5": [], "paper_6": [], "paper_7": []}, "local-to-global region-based NR VQA architecture proposed": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["PVQ"], "paper_5": [], "paper_6": [], "paper_7": []}, "minimum number of video clips per action class in the Kinetics dataset": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": ["at least 400 clips"]}, "focus of the VQA methods discussed in the paper": {"paper_1": ["authentic distortions, wide variety of content"], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": ["videos in-the-wild"], "paper_6": [], "paper_7": []}, "duration of the video clips in the UGC dataset": {"paper_1": [], "paper_2": [], "paper_3": ["1500 20 sec video clips"], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}}, "ours_final_table": {"Problem Tackled": {"paper_1": ["Creation of a subjectively annotated VQA database consisting of 1,200 public-domain video sequences to help in the development and evaluation of objective quality assessment methods for real-world video sequences with authentic distortions."], "paper_2": ["Construction of a large-scale video quality assessment database with 585 videos and over 205000 opinion scores to advance NR video quality prediction handling a diversity of distortions."], "paper_3": ["Introduction of a large scale UGC dataset to help understand difficulties for compression and quality assessment in the scenario of UGC, addressing the shortcomings of traditional metrics."], "paper_4": ["Addressing the problem of NR perceptual video quality assessment for UGC by creating a substantial subjective video quality dataset and developing region-based NR VQA architecture and a video quality mapping engine."], "paper_5": ["Introduction of KonVid-150k, a new in-the-wild VQA dataset, and proposing new VQA approaches, to enhance the performance of deep-learning models and hand-crafted feature-based methods for in-the-wild video quality assessment."], "paper_6": ["Extensive empirical evaluation of CNNs on large-scale video classification to take advantage of local spatio-temporal information and improve performance over strong feature-based baselines."], "paper_7": ["Description and analysis of the Kinetics human action video dataset for neural network training and testing, involving classification of human actions, and assessment of dataset imbalance impact on classifier bias."]}, "Approach": {"paper_1": ["KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences for 'in the wild' video quality assessment"], "paper_2": ["LIVE Video Quality Challenge Database (LIVE-VQC), a large-scale VQA database with 585 videos and over 205,000 opinion scores to advance NR video quality prediction"], "paper_3": ["Introduces a large scale UGC dataset for video compression research and discusses challenges for UGC compression and quality evaluation"], "paper_4": ["Creation of a large subjective video quality dataset and two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (PVQ) and (b) a space-time video quality mapping engine (PVQ Mapper)"], "paper_5": ["Introduction of KonVid-150k for no-reference video quality assessment and a new efficient VQA approach based on multi-level spatially pooled deep-features (MLSP)"], "paper_6": ["Empirical evaluation of CNNs on large-scale video classification using a new dataset and suggesting a multiresolution, foveated architecture for spatio-temporal networks"], "paper_7": ["Description of the Kinetics human action video dataset for action classification and baseline performances for network architectures on this dataset"]}, "VQA definition": {"paper_1": ["Video Quality Assessment (VQA) deals with the evaluation of video quality based on various factors such as semantics, context, and visual distortions."], "paper_2": ["VQA is implicitly defined as the assessment of video quality that deals with a variety of video impairments caused by different factors including capture, compression, processing, and display."], "paper_3": ["VQA in the context of this paper refers to the assessment of video quality specifically for user-generated content which is popular on video-sharing platforms."], "paper_4": ["VQA refers to the assessment of video quality without reference, particularly dealing with imperfect, user-generated content (UGC)."], "paper_5": ["VQA is an evaluation method that has traditionally focused on the assessment of specific types of degradation in video quality, usually on a small set of reference videos."], "paper_6": ["While not explicitly defined in this paper, VQA is part of the larger domain of video classification and recognition which CNNs are being evaluated for."], "paper_7": ["Not specifically dealing with VQA, but the dataset provided could be used in VQA for human action classification."]}, "Necessity of large database with real-world sequences and subjective MOS": {"paper_1": ["A large dataset is necessary for training and validating general-purpose VQA methods as well as to represent a wide spectrum of video content and all types of distortions."], "paper_2": ["LIVE-VQC provides a large-scale database with diverse, authentic distortions and a high number of subjective scores for advanced NR video quality prediction."], "paper_3": ["A large and diverse dataset of YouTube UGC is important for understanding challenges in compression and quality assessment, and non-pristine originals are not well assessed by traditional metrics."], "paper_4": ["A large-scale database is necessary for the development of no-reference VQA models that can predict global video quality and map perceptual distortions in real-world UGC videos."], "paper_5": ["KonVid-150k is introduced with a large and diverse set of videos, advocating that a coarsely annotated database is crucial for developing efficient VQA methods suited for in-the-wild content."], "paper_6": ["The emphasis is on the evaluation of CNN models for video classification, suggesting that large-scale databases contribute to the generalizability and performance of video recognition models not strictly limited to quality assessment."], "paper_7": ["The Kinetics dataset contains a variety of human actions and could indirectly stress the importance of large, varied datasets for improving the performance of computational models including VQA."]}, "Creation of larger VQA databases": {"paper_1": ["Introduced KoNViD-1k, a VQA database with 1,200 public-domain video sequences, collected from YFCC100m to represent a wide variety of content."], "paper_2": ["Constructed the LIVE Video Quality Challenge Database (LIVE-VQC) with 585 videos, over 205000 opinion scores from crowdsourcing, captured by a variety of users and devices."], "paper_3": ["Provided a large scale UGC dataset with 1500 20 sec video clips sampled from YouTube, covered various categories and challenges for UGC compression and quality evaluation."], "paper_4": ["Created the largest subjective video quality dataset with 39,000 videos and 5.5M human perceptual quality annotations, which also included 117,000 video patches for localized assessment."], "paper_5": ["Assembled KonVid-150k, an in-the-wild VQA dataset with 153,841 videos, proposing efficient VQA models."], "paper_6": ["Explored large-scale video classification by creating a dataset of 1 million YouTube videos, examining CNN extensions for video data and training on local spatio-temporal information."], "paper_7": ["Developed the Kinetics human action video dataset containing 400 classes with at least 400 videos each, focused on human actions from YouTube clips."]}, "Addressing diverse and authentic distortions": {"paper_1": ["Focused on 'in the wild' authentic distortions within the collected videos."], "paper_2": ["Videos in the LIVE-VQC database contain a wide range of complex, authentic distortions."], "paper_3": ["Discussed challenges in UGC compression and quality evaluation, addressing shortcomings of traditional metrics when applied to non-pristine originals."], "paper_4": ["Collected real-world distorted videos and video patches to facilitate space-time localization of perceptual distortions in UGC."], "paper_5": ["Provided a dataset with authentically distorted videos and introduced MLSP-VQA approaches for handling in-the-wild video quality assessment."], "paper_6": ["Focused on video classification rather than VQA, therefore offers no direct solution for authentic distortions in VQA databases."], "paper_7": ["Dataset centers on human actions and does not specifically address the issue of diverse and authentic distortions in the context of VQA."]}, "VQA database construction": {"paper_1": ["KoNViD-1k database, 1,200 video sequences sampled from YFCC100m, aimed at \u2018in the wild\u2019 authentic distortions"], "paper_2": ["LIVE Video Quality Challenge Database (LIVE-VQC), 585 videos, captured by many users, crowdsourced opinion scores"], "paper_3": ["YouTube UGC Dataset, 1500 20 sec video clips sampled from millions of YouTube videos"], "paper_4": ["Subjective video quality dataset, 39,000 videos with 117,000 localized patches ('v-patches'), 5.5M human perceptual annotations"], "paper_5": ["KonVid-150k dataset, 153,841 videos with five ratings each, and 1,596 videos with a minimum of 89 ratings"], "paper_6": ["Dataset of 1 million YouTube videos belonging to 487 classes for large-scale video classification"], "paper_7": ["Kinetics human action video dataset, 400 classes with at least 400 clips each from different YouTube videos"]}, "VQA database validation": {"paper_1": ["Subjective mean opinion scores (MOS) for VQA methods, focus on deep learning purposes"], "paper_2": ["Collected subjective video quality scores via crowdsourcing, comparison of leading NR video quality predictors"], "paper_3": ["Evaluation with three no-reference metrics (Noise, Banding, SLEEQ)"], "paper_4": ["Two new NR-VQA models PVQ and PVQ Mapper, aims to localize and visualize perceptual distortions"], "paper_5": ["Proposed new efficient VQA approaches (MLSP-VQA), SP-VQA-FF model improving on standard benchmarks"], "paper_6": ["Empirical evaluation of CNNs for large-scale video classification, generalization performance of models"], "paper_7": ["Baseline performance figures for neural network architectures, preliminary analysis of bias due to dataset imbalance"]}, "Potential applications of VQA databases": {"paper_1": ["Development and evaluation of objective quality assessment methods, particularly for deep learning, validating VQA methods intended to be 'general purpose'."], "paper_2": ["Advancing no-reference video quality prediction, enabling better handling of diverse video distortions, and establishing a benchmark for comparing leading NR video quality predictors."], "paper_3": ["Providing understanding and challenges for compression and quality assessment in UGC scenarios, and evaluating UGC quality with no-reference objective quality metrics."], "paper_4": ["Improving NR-VQA models to monitor and guide processing of user-generated content, and localizing perceptual distortions in space and time."], "paper_5": ["Facilitating efficient VQA approaches that are trained at scale, improving cross-test performance, and providing a robust dataset for training deep learning models."], "paper_6": ["Video classification and understanding local spatio-temporal information in videos by extending CNNs in the time domain."], "paper_7": ["Training neural network architectures for human action classification and analysis of biases in classifiers due to dataset imbalances."]}, "issue focus of the paper": {"paper_1": ["Development and evaluation of objective video quality assessment (VQA) methods; lack of large datasets with real-world video sequences and subjective mean opinion scores (MOS) for 'in the wild' authentic distortions and diverse content."], "paper_2": ["Inadequacy of current no-reference (NR) video quality models to handle a diversity of video impairments due to limited content and fixed resolutions in existing datasets; advancing NR video quality prediction with a new large-scale video quality assessment database."], "paper_3": ["Challenges of compression and quality assessment for non-professional user-generated content (UGC), inadequacy of traditional metrics for non-pristine originals, and the proposal of a new UGC dataset for research."], "paper_4": ["No-reference (NR) perceptual video quality assessment (VQA) for user-generated content (UGC) is limited; need for efficient and accurate quality predictors for real-world 'in the wild' UGC data and creation of a large subjective video quality dataset."], "paper_5": ["Underperformance of traditional video quality assessment (VQA) methods for in-the-wild videos; small size and diversity of existing VQA datasets; presentation of a new, larger, and diverse in-the-wild VQA dataset."], "paper_6": ["Video classification with convolutional neural networks (CNNs); challenges in extending CNN connectivity in the temporal domain and evaluating the effectiveness of different architectures for large-scale video classification."], "paper_7": ["Creation and analysis of the Kinetics human action video dataset; ensuring the dataset covers a broad range of human actions without leading to bias in the classifiers due to imbalance."]}, "large and diverse datasets": {"paper_1": ["existing VQA databases include only a small number of video sequences with artificial distortions"], "paper_2": ["current video quality assessment databases fail to adequately represent real world videos"], "paper_3": ["there are few public UGC datasets available for research"], "paper_4": ["current NR models are limited in their prediction capabilities on real-world, 'in-the-wild' UGC video data"], "paper_5": ["most traditional VQA methods under-perform 'in-the-wild'; existing datasets either artificial or authentically distorted are small and lack diversity"], "paper_6": [], "paper_7": []}, "real-world distortions": {"paper_1": ["existing VQA databases lack sequences with 'in the wild' authentic distortions"], "paper_2": ["video datasets do not contain real-world videos subject to authentic, often commingled distortions"], "paper_3": ["non-pristine originals (majority of UGC) are not well-addressed"], "paper_4": ["in the wild' user-generated content (UGC) often contains imperfect and authentic distortions"], "paper_5": ["in-the-wild videos present challenges due to their authentic distortions"], "paper_6": [], "paper_7": []}, "subjective mean opinion scores (MOS)": {"paper_1": ["larger datasets of real-world video sequences with corresponding subjective MOS needed for deep learning"], "paper_2": ["collected a large number of subjective video quality scores via crowdsourcing"], "paper_3": [], "paper_4": ["created the largest subjective video quality dataset"], "paper_5": ["dataset contains videos having subjective quality ratings"], "paper_6": [], "paper_7": []}, "general-purpose VQA methods": {"paper_1": ["training and validation of any VQA method intended to be \u2018general purpose\u2019 requires a large dataset"], "paper_2": [], "paper_3": ["challenges for UGC compression and quality evaluation emphasize the need for versatile metrics and methods"], "paper_4": [], "paper_5": ["proposes new efficient VQA approaches suited for in-the-wild content"], "paper_6": [], "paper_7": []}, "UGC-specific features": {"paper_1": [], "paper_2": [], "paper_3": ["dataset covers unique features like Gaming, Sports, and HDR"], "paper_4": [], "paper_5": [], "paper_6": [], "paper_7": []}, "Methodologies for improving no-reference video quality models": {"paper_1": ["The KoNViD-1k database was created to help with the development and evaluation of objective quality assessment methods, especially for deep learning purposes. It provides a large set of real-world video sequences with varied content and authentic distortions annotated with subjective mean opinion scores (MOS)."], "paper_2": ["The LIVE-VQC database was constructed containing unique content, a wide range of capture devices and distortion types, and a large number of subjective video quality scores collected via crowdsourcing to facilitate the advancement of no-reference video quality prediction."], "paper_3": ["Introduced a large-scale YouTube UGC dataset covering a variety of content and features, offering an evaluation of UGC quality with no-reference objective quality metrics (Noise, Banding, and SLEEQ) to challenge traditional reference-based metrics which may not perform well on non-pristine originals."], "paper_4": ["Developed a local-to-global region-based NR VQA architecture (PVQ) and a space-time video quality mapping engine (PVQ Mapper) that predicts global video quality and helps visualize distortions, using the largest subjective video quality dataset with real-world distorted videos and annotations to date."], "paper_5": ["Introduced the KonVid-150k VQA dataset and new efficient VQA approaches such as MLSP-VQA with multi-level spatially pooled deep-features to improve performance on in-the-wild videos by leveraging a large and diverse dataset for training."], "paper_6": ["Discussed approaches for extending CNN connectivity in the time domain for video classification, also suggesting a multiresolution, foveated architecture, but did not specifically focus on no-reference video quality models."], "paper_7": ["The Kinetics dataset focuses on human action video classification using deep learning, but does not directly contribute methodologies for improving no-reference video quality models."]}, "Performance measurement metrics or methods used": {"paper_1": ["Mean Opinion Scores (MOS) for subjective video quality assessment"], "paper_2": ["Crowdsourcing subjective video quality scores, leading NR video quality predictors comparison"], "paper_3": ["No-reference objective quality metrics (Noise, Banding, and SLEEQ) evaluation"], "paper_4": ["Subjective video quality dataset, local-to-global region-based NR VQA architecture (PVQ), space-time video quality mapping (PVQ Mapper)"], "paper_5": ["Spearman rank-order correlation coefficient (SRCC) using MLSP-VQA methods with deep-features (MLSP)"], "paper_6": ["Performance improvements over feature-based baselines and single-frame models, retraining top layers on UCF-101 dataset"], "paper_7": ["Baseline performance figures for neural network architectures trained and tested for human action classification"]}, "Real-world data characteristics and source": {"paper_1": ["1,200 public-domain video sequences, YFCC100m dataset"], "paper_2": ["585 unique videos, captured by various users, wide range of complex authentic distortions"], "paper_3": ["Large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos"], "paper_4": ["39,000 real-world distorted videos and 117,000 space-time localized video patches ('v-patches')"], "paper_5": ["153,841 videos with five quality ratings each and 1,596 videos with at least 89 ratings from KonVid-150k dataset"], "paper_6": ["Evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos"], "paper_7": ["400 human action classes with at least 400 video clips for each class from different YouTube videos"]}, "Performance improvement results or comparisons": {"paper_1": ["Aimed at developing and evaluating objective quality assessment methods for videos 'in the wild'"], "paper_2": ["Demonstrates value of new database by comparing leading NR video quality predictors"], "paper_3": ["Evaluated with three no-reference metrics and discussed the challenges for UGC compression and quality evaluation"], "paper_4": ["PVQ achieves state-of-the-art performance on 3 UGC datasets, PVQ Mapper helps localize and visualize distortions"], "paper_5": ["MLSP-VQA-FF improves SRCC on KoNViD-1k to 0.82, setting new state-of-the-art for cross-test performance"], "paper_6": ["Best spatio-temporal networks show performance improvements over baselines and single-frame models"], "paper_7": ["Provides baseline performance figures and preliminary analysis of bias due to class imbalance in classifiers"]}, "datasets or benchmarks used for no-reference video quality assessment": {"paper_1": ["KoNViD-1k"], "paper_2": ["LIVE Video Quality Challenge Database (LIVE-VQC)"], "paper_3": ["YouTube UGC dataset"], "paper_4": ["Patch-VQ database"], "paper_5": ["KonVid-150k"], "paper_6": ["New dataset of 1 million YouTube videos"], "paper_7": ["Kinetics Human Action Video dataset"]}, "Performance issue concerning video quality models": {"paper_1": ["Limited dataset of video sequences with artificial distortions; lacks large datasets with real-world sequences and subjective MOS for deep learning"], "paper_2": ["Current no-reference models cannot handle the diversity of distortions due to limited content, fixed resolutions, and modest number of distortions in existing databases"], "paper_3": ["Traditional metrics like BD-Rate and PSNR are not accurate for non-pristine originals like UGC; challenges in UGC compression and quality assessment"], "paper_4": ["Current NR models limited in prediction capabilities on real-world, 'in-the-wild' UGC video data"], "paper_5": ["Traditional VQA methods under-perform in-the-wild due to focusing on specific degradation types; deep learning approaches limited by dataset size and diversity"], "paper_6": ["CNN models show only modest improvement for video classification over single-frame models, suggesting spatio-temporal information utilization is challenging"], "paper_7": ["Potential bias in classifiers due to imbalance in the dataset; this could affect performance when trained and tested for human action classification"]}, "type of video data no-reference video quality models struggle with": {"paper_1": ["real-world video sequences with 'in the wild' authentic distortions"], "paper_2": ["videos with great variations due to videographic skills, camera designs, compression, processing protocols, and displays, leading to a diversity of distortions"], "paper_3": ["User Generated Content (UGC) with non-pristine originals, covers various categories like Gaming, Sports, and features like High Dynamic Range (HDR)"], "paper_4": ["real-world 'in-the-wild' user-generated content (UGC) with a variety of space-time localized video patches ('v-patches') and perceptual distortions"], "paper_5": ["in-the-wild videos with diverse content and authentic distortions, large and diverse coarsely annotated videos"], "paper_6": ["large-scale video classification without specific mention to quality models struggling, but involves large-scale video data with spatio-temporal information"], "paper_7": ["human action video dataset without specific details on video quality struggles, mainly focuses on human-centric actions"]}, "Inadequate traditional metrics for UGC video compression and quality assessment": {"paper_1": ["Not specifically addressed; focuses on semantic, context, and visual distortions in subjective VQA"], "paper_2": ["Not specifically addressed; discusses limitations of current NR video quality models and databases"], "paper_3": ["BD-Rate and PSNR are designed for pristine originals, not suitable for non-pristine UGC"], "paper_4": ["Not specifically addressed; paper focuses on NR-VQA models for UGC"], "paper_5": ["Not explicitly mentioned; focuses on introducing MLSP-VQA approaches for in-the-wild content"], "paper_6": ["Not explicitly mentioned; paper focuses on CNNs for video classification"], "paper_7": ["Not specifically addressed; focuses on human action video dataset for classification"]}, "Proposal for overcoming UGC video quality assessment shortcomings": {"paper_1": ["KoNViD-1k subjectively annotated database includes 1,200 public-domain video sequences with 'in the wild' authentic distortions, which helps in developing better VQA methods"], "paper_2": ["LIVE-VQC database contains 585 videos with a large number of subjective scores, advancing NR video quality prediction with diverse, authentic distortions and high variability in content and imaging conditions"], "paper_3": ["A large scale UGC dataset (1500 clips) with a novel sampling method; evaluation with three no-reference metrics addresses the inadequacy of traditional metrics on UGC"], "paper_4": ["The Patch-VQ dataset with 39,000 distorted videos and 5.5M annotations allows the creation of region-based NR VQA models and a video quality mapping engine to predict and localize video quality issues"], "paper_5": ["KonVid-150k, a large in-the-wild VQA dataset with MLSP-VQA approaches using multi-level spatially pooled features intended to train at scale and handle label noise while providing superior performance"], "paper_6": ["Exploration of CNNs for video classification with suggestions for spatio-temporal connectivity and a proposed multiresolution architecture for efficient training"], "paper_7": ["DeepMind Kinetics dataset emphasizing human actions, aiming to reduce bias by ensuring a broad range of classes and addressing the imbalance in the dataset"]}, "Proposal for overcoming UGC video compression shortcomings": {"paper_1": ["Not specifically addressed in abstract"], "paper_2": ["Not specifically addressed in abstract"], "paper_3": ["Introduces challenges in UGC compression, highlights inadequacy of traditional metrics, and suggests evaluating with no-reference objective quality metrics"], "paper_4": ["Not specifically addressed in abstract"], "paper_5": ["Not specifically addressed in abstract, but new efficient VQA approaches may imply improvements in compression indirectly"], "paper_6": ["Not specifically addressed in abstract, but improvements in video classification could lead to better content-aware compression strategies"], "paper_7": ["Not specifically addressed in abstract, but a broad dataset might influence compression algorithm development to cater to varied content"]}, "new metrics for evaluating UGC video compression and quality": {"paper_1": ["Not explicitly mentioned, but the work suggests the use of subjective mean opinion scores (MOS) on a diverse set of real-world videos from the YFCC100m dataset."], "paper_2": ["Introduction of a large-scale video quality assessment database (LIVE-VQC) containing 585 videos with a wide range of complex, authentic distortions, and collection of a large number of subjective video quality scores via crowdsourcing."], "paper_3": ["Discussion of challenges for UGC compression and quality evaluation, advocating for the use of no-reference objective quality metrics (Noise, Banding, SLEEQ) for UGC."], "paper_4": ["Presented two unique no-reference video quality assessment (NR-VQA) models: a local-to-global region-based NR VQA architecture (PVQ) and a space-time video quality mapping engine (PVQ Mapper) to help localize and visualize perceptual distortions."], "paper_5": ["Proposal of multi-level spatially pooled deep-features (MLSP) for efficient VQA approaches and introduction of a new dataset (KonVid-150k) for training these models."], "paper_6": ["Not a focus on UGC video compression or quality metric innovations, but an extensive empirical evaluation of CNNs for large-scale video classification."], "paper_7": ["No new metrics or methodologies introduced for UGC video compression and quality, instead, the paper focuses on the creation of the Kinetics human action video dataset for classification tasks."]}, "Issues with the accuracy of traditional metrics": {"paper_1": ["Not specified explicitly, but hints at inadequacy with 'in the wild' distortions"], "paper_2": ["No-reference video quality models perform poorly on real-world data with diverse distortions"], "paper_3": ["Significant accuracy drop when applied on non-pristine originals (UGC)"], "paper_4": ["None specified directly, but implies limitations of existing models on 'in-the-wild' UGC"], "paper_5": ["Traditional metrics under-perform in-the-wild; Deep learning approaches limited due to dataset size and diversity"], "paper_6": ["Only modest improvement compared to single-frame models for spatio-temporal networks"], "paper_7": ["Not specified explicitly; the paper focuses on action classification rather than video quality assessment"]}, "Types of content not adequately represented by traditional metrics": {"paper_1": ["Real-world, 'in the wild' content with authentic distortions"], "paper_2": ["Varied content obtained under highly diverse imaging conditions with authentic, often commingled distortions"], "paper_3": ["User Generated Content (UGC) featuring varied categories and new features like HDR"], "paper_4": ["Real-world distorted videos and 'v-patches' highlighting spatial-temporal localization of distortions"], "paper_5": ["In-the-wild content significantly larger and diverse in video quality assessment"], "paper_6": ["Large-scale video classification with a vast variety of YouTube videos"], "paper_7": ["Different YouTube videos with human actions, but not directly related to VQA metrics"]}, "Limitations in capturing a diverse range of distortions": {"paper_1": ["Existing databases fail to cover 'general purpose' spectrum, implying limited range of distortions"], "paper_2": ["Limited representation in current databases; authentic, commingled distortions difficult to simulate"], "paper_3": ["Shortcomings of traditional metrics highlighted but not in relation to distortion representation"], "paper_4": ["Focus on space-time perceptual distortions visualization rather than range of distortions"], "paper_5": ["Current approaches mainly focus on particular types of degradation"], "paper_6": ["Not addressed directly; focuses on spatio-temporal connectivity for classification, not quality assessment"], "paper_7": ["Not addressed directly; focuses on human action classification, not video quality and distortion types"]}, "Suitability of traditional metrics for non-pristine originals/UGC": {"paper_1": ["Implies a need for databases representing 'in the wild' content for VQA methods"], "paper_2": ["Current no-reference quality predictors tested on real-world data often perform poorly"], "paper_3": ["Traditional metrics' accuracy drops for non-pristine originals, majority of UGC"], "paper_4": ["Creates models to predict global video quality, addressing issues with existing NR models on UGC data"], "paper_5": ["Introduces new dataset and methods for 'in-the-wild' VQA, aiming to improve on traditional metrics' poor performance"], "paper_6": ["Not specified; the focus is on video classification"], "paper_7": ["Not specified; the focus is on human action dataset"]}, "Problem-solving focus": {"paper_1": ["Creating a database for subjective video quality assessment with authentic distortions and a variety of content for VQA methods"], "paper_2": ["Constructing a large-scale video quality assessment database with authentic distortions and diverse content to improve NR video quality predictors"], "paper_3": ["Introducing a UGC dataset for video compression research, addressing the challenges in UGC compression and evaluation with current metrics"], "paper_4": ["Developing new no-reference VQA models and a comprehensive video quality dataset to improve the prediction of video quality in real-world UGC"], "paper_5": ["Introducing a large and diverse in-the-wild VQA dataset, proposing efficient VQA approaches, and improving performance metrics on benchmark datasets"], "paper_6": ["Empirical evaluation of CNNs on large-scale video classification and suggesting architectures to take advantage of local spatio-temporal information"], "paper_7": ["Describing the Kinetics human action video dataset for neural network training and testing in human action classification"]}, "shortcomings addressed": {"paper_1": ["Creating a natural video quality database with 'in the wild' authentic distortions representing a wide variety of content to help in the development and evaluation of VQA methods, especially for general-purpose use."], "paper_2": ["Addressing the diversity of distortions not well-represented in existing VQA databases through a large-scale VQA database to advance NR video quality prediction, containing diverse contents, capture devices, distortion types, and subjective scores."], "paper_3": ["Highlighting the inadequacy of traditional metrics for UGC video compression and quality assessment, introducing a large-scale UGC dataset, and evaluating it with no-reference objective quality metrics."], "paper_4": ["Improving no-reference perceptual video quality assessment on 'in-the-wild' UGC data by creating a large dataset of real-world distorted videos and proposing new NR-VQA models for better prediction capability."], "paper_5": ["Introducing a substantially larger and diverse in-the-wild VQA dataset, proposing new efficient VQA approaches, and challenging the performance of traditional VQA methods and deep learning approaches with in-the-wild videos."], "paper_6": ["Extending CNN models to large-scale video classification and analyzing their performance improvements over both feature-based baselines and single-frame models in the context of local spatio-temporal information."], "paper_7": ["Detailed description and baseline performance for a large human action video dataset catering to machine learning models for classification, also discussing potential class imbalance and the associated bias in classifiers."]}, "main problem addressed": {"paper_1": ["Creation of a large video quality assessment dataset with 'in the wild' authentic distortions to aid in the development and evaluation of objective VQA methods, especially for deep learning"], "paper_2": ["Inability of current no-reference video quality models to handle the diversity of video impairments caused by variations in videographic skills, camera designs, compression protocols, and displays"], "paper_3": ["Lack of public UGC datasets for research and the inadequacy of traditional metrics used in compression and quality assessment for non-pristine originals commonly found in UGC"], "paper_4": ["Limitations of current no-reference models in their prediction capabilities on real-world 'in-the-wild' UGC video data, requiring more efficient and accurate video quality predictors"], "paper_5": ["Under-performance of traditional VQA methods and limited success of deep learning approaches due to the small size and diversity of existing VQA datasets"], "paper_6": ["Finding effective ways for video classification using CNNs on a large-scale dataset and improving upon the performance of the models by using local spatio-temporal information"], "paper_7": ["Need for a large human action video dataset with a substantial number of classes and clips to improve the training and evaluation of neural network architectures for action classification"]}}}