[{"paperid": "paper0", "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "abstract": "Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.", "introduction": "\n\nLike many natural language generation tasks, machine translation (MT) is difficult to evaluate because the set of correct answers for each input is large and usually unknown. This limits the accuracy of automatic metrics, and necessitates costly human evaluation to provide a reliable gold standard for measuring MT quality and progress. Yet even human evaluation is problematic. For instance, we often wish to decide which of two translations is better, and by how much, but what should this take into account? If one translation sounds somewhat more natural than another, but contains a slight inaccuracy, what is the best way to quantify this? To what extent will different raters agree on their assessments?\n\nThe complexities of evaluating translationsboth machine and human-have been extensively studied, and there are many recommended best practices. However, due to expedience, human evaluation of MT is frequently carried out on isolated sentences by inexperienced raters with the aim of assigning a single score or ranking. When MT quality is poor, this can provide a useful signal; but as quality improves, there is a risk that the signal will become lost in rater noise or bias. Recent papers have argued that poor human evaluation practices have led to misleading results, including erroneous claims that MT has achieved human parity (Toral, 2020;L\u00e4ubli et al., 2018).\n\nThis paper aims to contribute to the evolution of standard practices for human evaluation of highquality MT. Our key insight is that any scoring or ranking of translations is implicitly based on an identification of errors and other imperfections. Making such an identification explicit by enumerating errors provides a \"platinum standard\" from which various gold-standard scorings can be derived, depending on the importance placed on different categories of errors for different downstream tasks. This is not a new insight: it is the conceptual basis for the Multidimensional Quality Metrics (MQM) framework developed in the EU QTLaunchPad and QT21 projects (www.qt21.eu), which we endorse and adopt for our experiments.\n\nMQM is a generic framework that provides a hierarchy of translation errors which can be tailored to specific applications. We identified a hierarchy appropriate for broad-coverage MT, and annotated outputs from 10 top-performing \"systems\" (including human references) for both the English\u2192German (EnDe) and Chinese\u2192English (ZhEn) language directions in the WMT 2020 news translation task (Barrault et al., 2020), using arXiv:2104.14478v1 [cs.CL] 29 Apr 2021 professional translators with access to full document context. For comparison purposes, we also collected scalar ratings on a 7-point scale from both professionals and crowd workers.\n\nWe analyze the resulting data along many different dimensions: comparing the system rankings resulting from different rating methods, including the original WMT scores; characterizing the error patterns of modern neural MT systems, including profiles of difficulty across documents, and comparing them to human translations (HT); measuring MQM inter-annotator agreement; and re-evaluating the performance of automatic metrics submitted to the WMT 2020 metrics task. Our most striking finding is that MQM ratings sharply revise the original WMT ranking of translations, exhibiting a clear preference for HT over MT, and promoting some low-ranked MT systems to much higher positions. This in turn changes the conclusions about the relative performance of different automatic metrics; interestingly, we find that most metrics correlate better with MQM rankings than WMT human scores do. We hope these results will underscore and help publicize the need for more careful human evaluation, particularly in shared tasks intended to assess MT or metric performance. We release our corpus to encourage further research. 1 Our main contributions are:\n\n\u2022 A proposal for a standard MQM scoring scheme appropriate for broad-coverage MT.\n\n\u2022 Release of a large-scale MQM corpus with annotations for over 100k HT and highquality-MT segments in two language pairs (EnDe and ZhEn) from WMT 2020. This is by far the largest study of human evaluation results released to the public.\n\n\u2022 Re-evaluation of the performance of MT systems and automatic metrics on our corpus, showing clear distinctions between HT and MT based on MQM ratings, adding to the evidence against claims of human parity.\n\n\u2022 Demonstration that crowd-worker evaluation has low correlation with our MQM-based evaluation, calling into question conclusions drawn on the basis of previous crowdsourced evaluations.\n\n1 https://github.com/google/ wmt-mqm-human-evaluation \u2022 Demonstration that automatic metrics based on pre-trained embeddings can outperform human crowd workers.\n\n\u2022 Characterization of current error types in HT and MT, identifying specific MT weaknesses.\n\n\u2022 Recommendations for the number of ratings needed to establish a reliable human benchmark, and for the most efficient way of distributing them across documents.\n\n\n"}, {"paperid": "paper1", "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics", "abstract": "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.", "introduction": "\n\nFactuality is defined as a measure of \"whether eventualities are characterized as corresponding to facts, possibilities, or situations that do not hold in the world\" (Sauri, 2008;Saur\u00ed and Pustejovsky, 2012). In summarization, this \"world\" is the article, which is taken as ground-truth, and the output summary must be faithful to the article's facts. Despite advancements in neural abstractive summarization (Narayan et al., 2018;Liu and Lapata, 2019;Lewis et al., 2020), \u223c30% of summaries have factual inconsistencies (Cao et al., 2018). With summarization being an integral component of information consumption, this highlights a need for ensuring summarization systems are factually consistent and developing methods for evaluating them.\n\nCommon evaluation metrics for summarization based on n-gram overlap -BLEU, ROUGE, and METEOR (Papineni et al., 2002;Lin, 2004;Lavie and Agarwal, 2007) -are insufficient to measure the factual correctness of summaries and fail to correlate with the human judgements of factuality (Falke et al., 2019;Kryscinski et al., 2019). More recent metrics proposed to improve the evaluation of summarization factuality (Kryscinski et al., 2020;Durmus et al., 2020;Wang et al., 2020;Maynez et al., 2020) cannot be compared due to the lack of common benchmarks. More critically, while these approaches differ in the way they model factuality, they all consider factuality as a binary concept, labeling summaries of any length as factual or non-factual. They do not provide any finegrained understanding of the factual errors made by different systems that could serve as an actionable feedback on a system's limitations.\n\nThe binary factuality of a text can be difficult to determine. Falke et al. (2019) show relatively low crowd-expert agreement, indicating the presence of subjectivity in the annotation process. Moreover, not all factual errors are equally important and the number of errors can have a significant impact on the perceived factuality of a text. This suggests that non-factuality should be modeled as a multidimensional construct and not a label.\n\nIn this work, we propose a linguistically motivated typology of factual errors for fine-grained analysis of factuality in summarization systems ( \u00a72). Our typology is theoretically grounded in frame semantics (Fillmore et al., 1976;Palmer et al., 2005) and linguistic discourse theory (Brown and Yule, 1983). It provides several benefits. First, we find that decomposing the concept of factuality in (relatively) well-defined and grounded categories makes the final binary decision more objective leading to near perfect agreement between crowd and expert annotators (\u03ba = 0.86). Second, this approach provides some measure of the degree of non-factuality both in terms of the quantity and the category of factual violations that appear Figure 1: We propose a linguistically grounded typology of factual errors. We select crowd workers to annotate summaries from two datasets according to this typology achieving near perfect agreement with experts. We collect FRANK, the resulting dataset, to benchmark factuality metrics and state-of-art summarization systems.\n\nin the text. This typology also provides us with the means to categorize the types of errors made by summarization systems, helping us gain deeper insights than simply categorizing content as factual or hallucinated.\n\nWe define an annotation protocol of factuality based on our typology and collect a dataset of human judgements over a diverse set of model generated summaries on the CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) datasets ( \u00a73). Through this dataset, we aim to both assess the factuality of summarization systems and benchmark recently proposed factuality metrics. In \u00a74 we discuss various state-of-art models and show a detailed analysis of the factual errors they make. Finally, in \u00a75 we evaluate multiple summarization metrics against our benchmark and show their strengths and weaknesses in detecting specific types of factual errors. Figure 1 shows an overview of this work.\n\n\n"}, {"paperid": "paper2", "title": "SNaC: Coherence Error Detection for Narrative Summarization", "abstract": "Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework for fine-grained annotations of long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowdworkers. Furthermore, we show that the collected annotations allow us to benchmark past work in coherence modeling and train a strong classifier for automatically localizing coherence errors in generated summaries. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction.", "introduction": "\n\nAs pre-trained models for news summarization (Lewis et al., 2020;Zhang et al., 2020;Brown et al., 2020) have improved drastically, researchers have begun tackling increasingly challenging settings, particularly long document summarization and generation of longer summaries (Kry\u015bci\u0144ski et al., 2021;Huang et al., 2021;Zhang et al., 2022;Wu et al., 2021). Summaries in these settings differ considerably from the newswire summaries of past research efforts (Nallapati et al., 2016;Narayan et al., 2018): models now need to extract salient information from different parts of a significantly Figure 1: Excerpt from a generated book summary by OpenAI's 175B model (Wu et al., 2021). Individual segments do not follow a coherent structure and extra information is often needed to understand the narrative.\n\nlonger document, and na\u00efvely combining these in a much longer output is less likely to yield a summary with coherent discourse structure.\n\nThis shift in the scope of the summarization task calls for a reexamination of the summarization evaluation framework. Even for short newswire summaries, Fabbri et al. (2021) showed that automated metrics are inadequate, and consequently, reporting results from a human evaluation study has become the standard practice. However, human evaluation is rarely done for longer summaries possibly due to the associated labor costs of reading and evaluating long text. It is also unclear whether A/B testing or Likert-scale based annotation frameworks transfer to long summary settings. Establishing human evaluation protocols is critical for comparing different modeling approaches and measuring progress.\n\nRecently, Wu et al. (2021) proposed a strong book summarization model but showed that although generated summaries covered important information from the books, they read like a list of events stapled together without any coherent narrative structure (see Figure 1). We found similar \n\n\n"}, {"paperid": "paper3", "title": "Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text", "abstract": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow\u2014such as redundancy, commonsense errors, and incoherence\u2014are identified through several rounds of crowd annotation experiments without a predefined ontology.We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.", "introduction": "None"}, {"paperid": "paper4", "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA", "abstract": "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.", "introduction": "\n\nText simplification aims to improve a text's readability or content accessibility while preserving its fundamental meaning (Stajner, 2021;Chandrasekar et al., 1996).Traditional human evaluation for text simplification often relies on individual, shallow sentence-level ratings (Sulem et al., 2018c;Alva-Manchego et al., 2021), easily affected by the annotator's preference or bias.Maddela et al. (2023) recently proposes a more reliable and consistent human evaluation method by ranking and rating multiple simplifications altogether.However, as text simplification involves performing a series of transformations, or edits, such as paraphrasing, removing irrelevant details, or splitting a long sen-  tence into multiple shorter ones (Xu et al., 2012), sentence-level scoring remains difficult to interpret since it is not reflective of detailed information about the types of edits being performed.\n\nFine-grained human evaluation through span selection has been explored for machine translation (Lommel et al., 2014) and open-ended text generation (Dou et al., 2022).Yet, these evaluation methods are error-driven -i.e., focusing solely on evaluating failure -which punishes creative and diverse generations with minor errors in favor of generic ones.Additionally, machine translation and open-ended generation tasks usually retain none of the input words, while text simplification must balance the editing and preservation of words in the original input (Xu et al., 2016).We thus evaluate simplification quality as the aggregation of edit successes and failures, as depicted in Figure 1.\n\nWe introduce SALSA -Success and FAiluredriven Linguistic Simplification Annotation -an arXiv:2305.14458v2[cs.CL] 22 Oct 2023 edit-level human evaluation framework capturing a broad range of simplification transformations.SALSA is built on a comprehensive typology ( \u00a72) containing 21 quality and error edit types.Using SALSA, we develop an interactive interface and collect 19K edit annotations of 840 simplifications written by eleven state-of-the-art language models and two humans.With these annotations, we conduct a large-scale analysis of model and automatic metric performance, and further introduce the automatic word-level quality estimation task for text simplification.Our main findings are as follows:\n\n\u2022 Few-shot GPT-3.5 far surpasses existing models, particularly in making syntax and content edits.However, its simplifications are not aligned to the types of operations performed by human.( \u00a74) \u2022 Some fine-tuned models such as the MUSS (Martin et al., 2022) produce more diverse edits than GPT-3.5, yet suffer from incredibly high errors, while others (T5, Raffel et al., 2020) learn to minimize loss by making very few changes.( \u00a74) \u2022 Open-source instruction fine-tuned models such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) perform a similar number of edits as GPT-3.5, but at a cost of more conceptual errors due to the inherent limits of model imitation.( \u00a74) \u2022 Fine-tuned on SALSA annotations, our referencefree metric, LENS-SALSA, captures the subtleties of specific simplification approaches beyond existing automatic evaluation metrics.( \u00a75) \u2022 Leveraging our data, we present the automatic word-level quality estimation task for text simplification and establish several baseline approaches for future modeling efforts.( \u00a76)\n\nOur results demonstrate that SALSA provides an interpretable and exhaustive evaluation of text simplification.\n\n\n"}, {"paperid": "paper5", "title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction", "abstract": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as \u201cGood\u201d or \u201cAcceptable\u201d in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.", "introduction": "\n\nGrammatical Error Correction (GEC) systems are often only evaluated in terms of overall performance because system hypotheses are not annotated. This can be misleading however, and a system that performs poorly overall may in fact outperform others at specific error types. This is significant because a robust specialised system is actually more desirable than a mediocre general system. Without an error type analysis however, this information is completely unknown.\n\nThe main aim of this paper is hence to rectify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information. This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback to non-native learners. Given that different corpora are also annotated according to different standards, we also attempted to standardise existing datasets under a common error type framework.\n\nOur approach consists of two main steps. First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016) and second, we classify them according to a new, rule-based framework that relies solely on dataset-agnostic information such as lemma and part-of-speech. We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT) 1 , by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical error correction (Ng et al., 2014).\n\nIt is worth mentioning that despite an increased interest in GEC evaluation in recent years (Dahlmeier and Ng, 2012;Felice and Briscoe, 2015;Bryant and Ng, 2015;Napoles et al., 2015;Grundkiewicz et al., 2015;Sakaguchi et al., 2016), ERRANT is the only toolkit currently capable of producing error types scores.\n\n\n"}, {"paperid": "paper6", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.", "introduction": "\n\nState-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans [29,28,9], with a combination of supervised learning and reinforcement learning.Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29,3,2,33,10].Many of these studies adopt reinforcement learning from human feedback (RLHF) [46], a framework that converts human feedback into an effective LM training signal to reach these goals.Specifically, humans are presented with two or more outputs and asked to select one or rank them, and this signal is then used to train a reward model, which computes a single scalar reward for each LM-generated the layer of gases, generally known as air\u2026 > = >\n\nFigure 1: Comparison of (a) RL with human preference and (b) our FINE-GRAINED RLHF on long-form QA.Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors.We train a fine-grained reward model for each type of error and optimize LM against these reward models.\n\nIn this example, we provide a relevance reward and a factuality reward after each sentence is generated.There is also a holistic information completeness reward after the whole text is generated.\n\nIn this paper, we propose that humans give fine-grained feedback to LM output, associating categories of undesired behavior (e.g., false or irrelevant generations) and a text span at a density (e.g., sentence or sub-sentence-level).To enable LMs to learn from such fine-grained feedback, we introduce the FINE-GRAINED RLHF framework.As shown in Figure 1, we first use collected human feedback to train fine-grained reward models such that each of them focuses on one category and provides rewards at the density associated with that category.We then integrate these reward models into Proximal Policy Optimization (PPO) [37], a commonly used RL algorithm for training LMs with preference-based human feedback ( \u00a72).\n\nWe conduct experiments on two language generation tasks-detoxification [12] ( \u00a73) and long-form question answering (QA) [39] ( \u00a74).For detoxification, toxicity is the only error category and we explore learning with a dense reward.We adopt PERSPECTIVE [1], a widely used language toxicity detection model trained on millions of human annotations, as our reward model.We use it to calculate a fine-grained reward after the generation of every sentence.Our experimental results show the efficacy and data efficiency of training models with dense reward compared to a holistic sequence-level reward, supported by automatic evaluation results.\n\nWith experiments on long-form QA, we aim to examine training models with fine-grained rewards at the two granularity dimensions (density and error category), for which we construct a long-form QA dataset, QA-FEEDBACK, along with our collected human feedback.We carefully develop a pipeline to collect fine-grained human feedback on three error categories at different density levels: i) irrelevance, repetition, or incoherence (sub-sentence), ii) incorrect or unverifiable facts (sentence), and iii) incomplete information (whole sequence; see Figure 1).Our experimental results show improved results in each error category by learning with such fine-grained feedback, supported by both automatic and human evaluation results.In a scenario with multiple reward models representing different error types, we also show FINE-GRAINED RLHF allows us to combine reward models with different weights and thus control the model training process towards a customized combination of desired behaviors.\n\n\n"}, {"paperid": "paper7", "title": "Improving Large-scale Paraphrase Acquisition and Generation", "abstract": "This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of 130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert (MultiPIT_expert) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MultiPIT_NMR) and a large automatically constructed training set (MultiPIT_Auto) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MultiPIT_Auto generate more diverse and high-quality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT.", "introduction": "\n\nParaphrases are alternative expressions that convey a similar meaning (Bhagat and Hovy, 2013). Studying paraphrase facilitates research in both natural language understanding and generation. For instance, identifying paraphrases on social media is important for tracking the spread of misinformation (Bakshy et al., 2011) and capturing emerging events (Vosoughi and Roy, 2016). On the other hand, paraphrase generation improves the linguistic diversity in conventional agents (Li et al., 2016) and machine translation (Thompson and Post, 2020). It has also been successfully applied in data argumentation to improve information extraction (Zhang et al., 2015;Ferguson et al., 2018) and question answering systems (Gan and Ng, 2019). 7. In Tibet, climate change causes bigger, faster avalanches.  Many researchers have been leveraging Twitter data to study paraphrase given its lexical and style diversity as well as coverage of up-to-date events. However, existing Twitter-based paraphrase datasets, namely PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017), suffer from quality issues such as topic unbalance and annotation noise, 1 which limit the performance of the models trained using them. Moreover, past efforts on creating paraphrase corpora only consider one paraphrase criteria without taking into account the fact that the desired \"strictness\" of semantic equivalence in paraphrases varies from task to task (Bhagat and Hovy, 2013;Liu and Soh, 2022). For example, for the purpose of tracking unfolding events, \"A tsunami hit Haiti.\" and \"303 people died because of the tsunami in Haiti\" are sufficiently close to be considered as paraphrases; whereas for paraphrase generation, the extra information \"303 people dead\" in the latter sentence may lead models to learn to  Table 1: Statistics of MULTIPIT CROWD and MULTIPIT EXPERT datasets. The sentence/tweet lengths are calculated based on the number of tokens per unique sentence/tweet. %Multi-Ref denotes the percentage of source sentences with more than one paraphrase. Compared with prior work, our MULTIPIT CROWD dataset has a significantly larger size, a higher portion of paraphrases, and a more balanced topic distribution.\n\nhallucinate and generate more unfaithful content.\n\nIn this paper, we present an effective data collection and annotation method to address these issues. We curate the Multi-Topic Paraphrase in Twitter (MULTIPIT) corpus, which includes MUL-TIPIT CROWD , a large crowdsourced set of 125K sentence pairs that is useful for tracking information on Twitter, and MULTIPIT EXPERT , an expert annotated set of 5.5K sentence pairs using a stricter definition that is more suitable for acquiring paraphrases for generation purpose. Compared to PIT-2015 and Twitter-URL, our corpus contains more than twice as much data with more balanced topic distribution and better annotation quality. Two sets of examples from MULTIPIT are shown in Figure 1.\n\nWe extensively evaluate several state-of-the-art neural language models on our datasets to demonstrate the importance of having task-specific paraphrase definition. Our best model achieves 84.2 F 1 for automatic paraphrase identification. In addition, we construct a continually growing paraphrase dataset, MULTIPIT AUTO , by applying the automatic identification model to unlabelled Twitter data. Empirical results and analysis show that generation models fine-tuned on MULTIPIT AUTO generate more diverse and high-quality paraphrases compared to models trained on other corpora, such as MSCOCO (Lin et al., 2014), ParaNMT (Wieting andGimpel, 2018), and Quora. 2 We hope our MULTIPIT corpus will facilitate future innovation in paraphrase research.\n\n\n"}, {"paperid": "paper8", "title": "Towards a Spell Checker for Zamboanga Chavacano Orthography", "abstract": "Zamboanga Chabacano (ZC) is the most vibrant variety of Philippine Creole Spanish, with over 400,000 native speakers in the Philippines (as of 2010). Following its introduction as a subject and a medium of instruction in the public schools of Zamboanga City from Grade 1 to 3 in 2012, an official orthography for this variety - the so-called \u201cZamboanga Chavacano Orthography\u201d - has been approved in 2014. Its complexity, however, is a barrier to most speakers, since it does not necessarily reflect the particular phonetic evolution in ZC, but favours etymology instead. The distance between the correct spelling and the different spelling variations is often so great that delivering acceptable performance with the current de facto spell checking technologies may be challenging. The goals of this research have been to propose i) a spelling error taxonomy for ZC, formalised as an ontology and ii) an adaptive spell checking approach using Character-Based Statistical Machine Translation to correct spelling errors in ZC. Our results show that this approach is suitable for the goals mentioned and that it could be combined with other current spell checking technologies to achieve even higher performance.", "introduction": "\n\nAccording to a recent report from Komisyon sa Wikang Filipino (KWF, 2015), the Philippines is home to 135 living languages, among which Chabacano is the only creole language. In the 2010 Census of Population and Housing (CPH), 405,798 people claimed Zamboanga Chabacano (ZC, locally known simply as \"Chavacano\" or \"Zamboangue\u00f1o\") as their mother tongue, thus making it the most widely spoken variety of Philippine Creole Spanish in the country, and the only one still experiencing natural growth (National Statistics Office (NSO), 2003a, 2014a; Philippine Statistics Authority (PSA), 2014). With the implementation of the K-12 curriculum and the Mother-Tongue Based Multilingual Education (MTB-MLE) program nationwide in 2012, ZC is nowadays taught as a subject and is used as a medium of instruction in the public schools of Zamboanga City from Grade 1 to 3 (Government of the Philippines, 2011). This has motivated the local government to invest in the standardisation of ZC, resulting in the approval of an official orthography (Zamboanga Chavacano Orthography) in 2014, followed by the publication of a basic grammar, as well as some collections of texts and children's books; even a normative dictionary was published in late 2018. Despite all the official initiatives in the last few years, the testimonials of many Zamboangue\u00f1os suggest that there might be no room for optimism. ZC has been facing a multiglossic situation for decades and there are evidences that it is no longer the preferred language for socialisation among the younger generations, which raises some concerns regarding its future (Himoro, 2019). Besides (see Table 1), although the absolute number of native ZC speakers is still increasing, the census figures show that this increase is not proportional to the population growth, since the ratio of native ZC speakers in the Zamboanga City population is continuously decreasing. The on-going standardisation process is thus vital to enable the language to be used efficiently in higher written registers and counterbalance the  1970, 1980, 1990, 2000and 2010CPH. (National Census and Statistics Office (NCSO), 1974, 1983National Statistics Office (NSO), 1992, 2003b, 2014b hegemony of the official languages, namely English and Tagalog. The approval of the orthography, nevertheless, has raised some other issues. The first of them regards its applicability: the main principle of the orthography is that words should be written according to their original form in the language of origin (DepEd Zamboanga City Division, 2016), which means that the spelling deemed as correct does not always reflect the phonetic evolution of the language, requiring prior study and memorisation work from the writers. The second one is basically technical: the gap between the different spelling systems currently in use in ZC and the orthography is often so big that the current de facto spell checking technologies are unable to yield acceptable results. This research aims at addressing these issues and contributing to the standardisation of ZC by proposing i) a spelling error taxonomy for ZC formalised as an ontology and ii) an adaptive spell checking approach using Character-Based Statistical Machine Translation to correct spelling errors in ZC. We have also implemented a hunspell spell checker to be used as a baseline to evaluate our approach. We argue that this solution presents the following advantages over hunspell: (i) it can correct previously unseen words by recognising some patterns, and (ii) it has a better performance when not dealing with simple spelling errors, but rather with different writing systems, as is the case of ZC.\n\n\n"}, {"paperid": "paper9", "title": "Fine-Grained Analysis of Propaganda in News Article", "abstract": "Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.", "introduction": "\n\nResearch on detecting propaganda has focused primarily on articles (Barr\u00f3n-Cedeno et al., 2019;Rashkin et al., 2017). In many cases, there are no labeled data for individual articles, but there are such labels for entire news outlets. Thus, often all articles from the same news outlet get labeled the way that this outlet is labeled. Yet, it has been observed that propagandistic sources could post objective non-propagandistic articles periodically to increase their credibility (Horne et al., 2018). Similarly, media generally recognized as objective might occasionally post articles that promote a particular editorial agenda and are thus propagandistic. Thus, it is clear that transferring the label of the news outlet to each of its articles, could introduce noise. Such labels can still be useful for training robust systems, but they cannot be used to get a fair assessment of a system at testing time.\n\nOne option to deal with the lack of labels for articles is to crowdsource the annotation. However, in preliminary experiments we observed that the average annotator cannot detach her personal mindset from the judgment of propaganda and bias, i.e., if a clearly propagandistic text expresses ideas aligned with the annotator's beliefs, it is unlikely that she would judge it as such.\n\nWe argue that in order to study propaganda in a sound and reliable way, we need to rely on highquality trusted professional annotations and it is best to do so at the fragment level, targeting specific techniques rather than using a label for an entire document or an entire news outlet.\n\nOurs is the first work that goes at a fine-grained level: identifying specific instances of propaganda techniques used within an article. In particular, we create a corresponding corpus. For this purpose, we asked six experts to annotate articles from news outlets recognized as propagandistic and non-propagandistic, marking specific text spans with eighteen propaganda techniques. We also designed appropriate evaluation measures. Taken together, the annotated corpus and the evaluation measures represent the first manually-curated evaluation framework for the analysis of finegrained propaganda. We release the corpus (350K tokens) as well as our code in order to enable future research. 1 Our contributions are as follows:\n\n\u2022 We formulate a new problem: detect the use of specific propaganda techniques in text.\n\n\u2022 We build a new large corpus for this problem.\n\n\u2022 We propose a suitable evaluation measure.\n\n\u2022 We design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.\n\nOur corpus could enable research in propagandistic and non-objective news, including the development of explainable AI systems. A system that can detect instances of use of specific propagandistic techniques would be able to make it explicit to the users why a given article was predicted to be propagandistic. It could also help train the users to spot the use of such techniques in the news. The remainder of this paper is organized as follows: Section 2 presents the propagandistic techniques we focus on. Section 3 describes our corpus. Section 4 discusses an evaluation measures for comparing labeled fragments. Section 5 presents the formulation of the task and our proposed models. Section 6 describes our experiments and the evaluation results. Section 7 presents some relevant related work. Finally, Section 8 concludes and discusses future work.\n\n\n"}, {"paperid": "paper10", "title": "arXivEdits: Understanding the Human Revision Process in Scientific Writing", "abstract": "Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.", "introduction": "\n\nWriting is essential for sharing scientific findings. Researchers devote a huge amount of effort to revising their papers by improving the writing quality or updating new discoveries. Valuable knowledge is encoded in this revision process. Up to January 1st, 2022, arXiv (https://arxiv.org/), an open access eprint service, has archived over 1.9 million papers, among which more than 600k papers have multiple versions available. This provides an amazing data * Work done as an undergraduate student. source for studying text revision in scientific writing. Specifically, revisions between different versions of papers contain valuable information about logical and structural improvements at documentlevel, as well as stylistic and grammatical refinements at sentence-and word-levels. It also can support various natural language processing (NLP) applications, including writing quality assessment and error correction (Louis and Nenkova, 2013;Xue and Hwa, 2014;Daudaravicius et al., 2016;Bryant et al., 2019), text simplification and compression (Xu et al., 2015;Filippova et al., 2015), style transfer (Xu et al., 2012;Krishna et al., 2020), hedge detection (Medlock and Briscoe, 2007), and paraphrase generation (Dou et al., 2022).\n\nIn this paper, we present a complete solution for studying the human revision process in the scientific writing domain, including annotated data, analysis, and system. We first construct ARXIVEDITS, which consists of 751 full arXiv papers with gold sentence alignment across their multiple versions of revisions, as shown in Figure 1. Our corpus spans 6 research areas, including physics, mathematics, computer science, quantitative biology, quantitative finance, and statistics, published in 23 years (from 1996 to 2019). To the best of our knowledge, this is the first text revision corpus that covers full multi-page research papers. To study sentence-level revision, we manually annotated fine-grained edits and their underlying intentions that reflect why the edits are being made for 1,000 sentence pairs, based on a taxonomy that we developed consisting of 7 categories.\n\nOur dataset addresses two major limitations in prior work. First, previous researchers mainly focus on the abstract (G\u00e1bor et al., 2018;Kang et al., 2018;Du et al., 2022) and introduction (Tan and Lee, 2014;Mita et al., 2022) sections, limiting the generalizability of their conclusions. In addition, a sentence-level revision may consist of multiple fine-grained edits made for different purposes (see\n\n\n"}]