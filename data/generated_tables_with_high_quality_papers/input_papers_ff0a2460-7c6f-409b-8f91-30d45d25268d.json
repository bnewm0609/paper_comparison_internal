[{"paperid": "paper0", "title": "Text and Code Embeddings by Contrastive Pre-Training", "abstract": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.", "introduction": "\n\nDeep unsupervised learning with generative and embedding models has seen dramatic success in the past few years. Generative models (Peters et al., 2018;Raffel et al., 2019;van den Oord et al., 2016;Ramesh et al., 2021;Brown et al., 2020;Chen et al., 2021)  imize the likelihood of observed data while embedding models are trained to distinguish observed data from noise (Sohn, 2016;van den Oord et al., 2018;Radford et al., 2021;Jia et al., 2021;Gao et al., 2021;Izacard et al., 2021). Generative models have been shown to produce realistic content and benefit many downstream applications, reducing the need for labeled training datasets. In generative models, the information about the input is typically distributed over multiple hidden states of the model. While some generative models (Kingma & Welling, 2014;Kiros et al., 2015) can learn a single representation of the input, most autoregressive Transformer (Vaswani et al., 2017) models do not (Raffel et al., 2019;Brown et al., 2020;Chen et al., 2021;Ramesh et al., 2021). However, learning such a representation (or embedding) is necessary for many tasks. Systems that search over millions or billions of items require each entry to be embedded as a dense representation and build an index in advance to save computational costs at query time. These embeddings are useful features for classification tasks and can also enable data visualization applications via techniques such as clustering. Embedding models are explicitly optimized to learn a low dimensional representation that captures the semantic meaning of the input Jia et al., 2021;Giorgi et al., 2020;Gao et al., 2021;Izacard et al., 2021).\n\nIn this work, we train embedding models using a contrastive learning objective with in-batch negatives (Sohn, 2016;Yih et al., 2011) on unlabeled data. The input is encoded with a Transformer encoder (Vaswani et al., 2017) and we leverage naturally occurring paired data to construct training data with no explicit labels. Text embedding models are trained on paired text data where we consider neighboring pieces of text on the Internet as positive pairs. Code embedding models treat the top-level docstring in a function along with its implementation as a (text, code) pair. The training signal of the contrastive objective on its own is not sufficient to learn useful representations and we overcome this by initializing our model with other pretrained models (Brown et al., 2020;Chen et al., 2021). Finally, we find that it is critical to use a sufficiently large batch to achieve the optimal performance. We show that this simple recipe combining pre-trained model initialization, large-batch contrastive learning and training at scale, can produce text and code embeddings that possess a broad range of capabilities.\n\nWe train a series of unsupervised text embedding models (cpt-text) of different sizes, ranging from 300M to 175B parameters, and observe a consistent performance improvement with increasing model sizes ( Figure  1). On classification accuracy averaging across 7 linearprobe classification tasks in SentEval (Conneau & Kiela, 2018), our largest unsupervised model achieves new stateof-the-art results with a relative improvement of 4% and 1.8% over the previous best unsupervised (Giorgi et al., 2020) and supervised (Gao et al., 2021) text embedding models, respectively.\n\nText embedding in previous work was studied under different domains, varying in data, training objective and model architecture. Precisely, sentence embedding (Reimers & Gurevych, 2019;Gao et al., 2021;Giorgi et al., 2020) and neural information retrieval Guu et al., 2020;Karpukhin et al., 2020a;Sachan et al., 2021;Izacard et al., 2021) have remained different research topics evaluated on distinct benchmarks, even though both aim to learn high-quality text representation. However, we find the same model that achieves good performance on sentence embedding benchmarks, as discussed above, is also able to obtain impressive results on large-scale information retrieval. When evaluated on the MSMARCO passage ranking task (Nguyen et al., 2016) to search over 4M passages, cpt-text gets a relative improvement of 23.4% over previous best unsupervised methods (Robertson, 2009). On the task of searching on 21M documents from Wikipedia, cpt-text obtains a relative improvement of 14.7%, and 10.6% over previous unsupervised methods (Izacard et al., 2021) for Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), respectively. On Triv-iaQA, our unsupervised method is even competitive with fine-tuned models.\n\nNext, we train code embedding models (cpt-code) using the same recipe. Our models learn via (text, code) pairs, extracted from open source code. We evaluate our model on CodeSearchNet (Husain et al., 2020), a commonly used code search benchmark, where the task is to find the most relevant code snippet given a natural language query. Our models achieve new state-of-the-art results with a 20.8% relative improvement over the previous best result (Guo et al., 2021). Unlike text embedding models, we observe no performance improvement on code search when increasing the number of parameters of cpt-code from 300M to 1.2B.\n\nFinally, we experiment with fine-tuning our models on several supervised datasets and study the transfer learning performance. When fine-tuned on NLI (Natural Language Inference) datasets, we see a further boost in linearprobe classification, outperforming the previous best transfer method (Gao et al., 2021) by 2.2%. On SST-2 sentiment classification (Socher et al., 2013), we find that our representations are sufficiently descriptive that even a simple k-NN classifier achieves results comparable to a linearprobe classifier. Interestingly, zero-shot performance with our embeddings outperforms the supervised neural network models introduced along with the release of the SST-2 dataset. We also fine-tune the unsupervised model on MS-MARCO and evaluate it on a suite of zero-shot search tasks in the BEIR benchmark (Thakur et al., 2021). In the transfer setting, our models achieve a 5.2% relative improvement over previous methods (Izacard et al., 2021) and is comparable even with methods (Santhanam et al., 2021;Formal et al., 2021;Wang et al., 2020) that demand substantially more computation at test time.\n\n\n"}, {"paperid": "paper1", "title": "Large Dual Encoders Are Generalizable Retrievers", "abstract": "It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.", "introduction": "\n\nTypical neural retrieval models follow a dual encoder paradigm (Gillick et al., 2018;Karpukhin et al., 2020). In this setup, queries and documents are encoded separately into a shared fixed-dimensional embedding space, where relevant queries and documents are represented in each other's proximity. Then, approximated nearest neighbor search (Vanderkam et al., 2013;Johnson et al., 2021) is applied to efficiently retrieve relevant documents given an encoded input query. * Correspondence to jianmon@google.com \u2020 Work done while at Google Research While dual encoders are popular neural retrievers, the expressiveness of the model is limited by a bottleneck layer consisting of only a simple dotproduct between query and passage embeddings. Lu et al. (2021); Khattab and Zaharia (2020) argued that the dot-product (or cosine similarity) between the embeddings might not be powerful enough to capture the semantic relevance. Similarly, Thakur et al. (2021) suggested that dual encoder models have \"issues for out-of-distribution data\" and models with more interactions between queries and documents have better generalization ability.\n\nIn this paper, we challenge this belief by scaling up the dual encoder model size while keeping the bottleneck as a single dot-product with a fixed size. Note that scaling up a dual encoder is different from scaling up pretrained language models, such as BERT  and T5 (Raffel et al., 2020), because of the presence of the bottleneck layer. While increasing the model size can greatly increase model capacity, for dual encoders with a fixed bottleneck embedding size, the interactions between queries and documents are still limited by a simple dot-product.\n\nTo test this hypothesis, we take advantage of the T5 architecture and checkpoints to build encoders of up to 5 billion parameters while keeping the  Figure 2: Architecture of Generalizable T5-based dense Retrievers. The research question we ask is: can scaling up dual encoder model size improve the retrieval performance while keeping the bottleneck layers as a single dot-product with a fixed size? Only the encoder is taken from the pre-trained T5 models, and the two towers of the dual encoder share parameters.\n\nbottleneck embedding dimension of 768 in all configurations, as illustrated in Figure 2. Following , we build dual encoders by taking the encoder part of T5. To effectively leverage the power of large models, we collect two billion web question-answer pairs as generic pre-training data. By combining pre-training with generic data and fine-tuning with MS Marco (Nguyen et al., 2016), we are able to train large-scale dual encoder retrieval models. We call the resulting models Generalizable T5-based dense Retrievers (GTR). We assess the zero-shot performance of GTR on the BEIR benchmark (Thakur et al., 2021), which has 18 information retrieval tasks across 9 domains. We showed that scaling up leads to better generalization despite the fixed single-dot product bottleneck. Second, pre-training on community questionanswer pairs and fine-tuning on human curated data are both important to fully utilize the power of the scaled up model. In addition, with scaling and pre-training, we found GTR to be highly data efficient in terms of human annotated queries, as it only needs to use 10% of MS Marco to match the overall out-of-domain performance.\n\n\n"}, {"paperid": "paper2", "title": "Task-aware Retrieval with Instructions", "abstract": "We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.", "introduction": "\n\nInformation retrieval (IR) is the task of finding relevant documents from a large collection of texts to fulfill a user's information need, typically expressed in the form of a textual query (Singhal et al., 2001). The notion of relevance from the user's perspective (i.e., intent) can be amorphous (Mizzaro, 1998), and a query alone may not fully capture user information needs (Ruthven and Lalmas, 2003;Taylor, 1962). As illustrated in Figure 1 (top), given the same query, \"implementing batch normalization in Python,\" a user may want to retrieve a passage that describes how to do the task or to identify a similar query, or even to directly locate a code snippet.  Figure 1: User intents are not fully captured in query q only (top). Conventional approaches (bottom left) take a query and retrieve documents from a closed corpus using a task-specific retriever. Retrieval with instructions (bottom right) takes a query and explicit intent and retrieves documents aligning with the user's expectations.\n\nMost existing work tries to learn those implicit intents from labeled data (e.g., pairs of queries and relevant documents), yielding separate models for different intents as shown in the bottom left of Figure 1. This approach has several limitations. First, a vast number of annotated examples may be required to train a model to capture the task-specific notion of relevance, while they could benefit from the abundance of data available from related tasks. Second, a model trained on one task may not easily transfer to new tasks that are not closely related.\n\nIn this work we advocate for a new task formulation, retrieval with instructions, to explicitly model a user's search intent by providing a natural language description of the search task (i.e., an instruction). Here, the goal of retrieval systems is to retrieve documents that are both relevant to the query and well-suited to the instructions.\n\nDespite active research in other settings, instruction-following has not been systematically explored in retrieval, partly due to the lack of annotated resources. To facilitate research in retrieval with instructions, we introduce BERRI (Bank of Explicit RetRieval Instructions), a collection of approximately 40 retrieval datasets with diverse instructions in a unified format, covering 10 diverse domains. Each task has on average 3.5 diverse instructions annotated by experts, following our novel instruction schema for retrieval tasks.\n\nWe use BERRI to train TART (Task-aware ReTriever), a single multi-task retrieval system that follows instructions to perform diverse tasks with no parameter updates on each task. We employ two widely explored architectures: TART-dual is a dense dual-encoder architecture, retrieving documents based on the similarity of independently encoded query and document embeddings; TARTfull calculates probabilities of a document being relevant to the query according to the instruction using a cross-encoder. TART is trained with carefully designed negative samples, including our novel instruction-unfollowing negatives samples.\n\nThe TART models, particularly TART-full yields state-of-the-art results on two popular zeroshot retrieval benchmarks, BEIR (Thakur et al., 2021) and LOTTE-pooled (Santhanam et al., 2022), outperforming systems using three times more parameters (Nogueira et al. 2020;Ni et al. 2021; Muennighoff 2022) as well as task-specific retrievers trained on millions of automatically generated examples (Dai et al., 2022;Wang et al., 2022a).\n\nWe further introduce a new evaluation setup, X 2 -Retrieval (Cross-task Cross-domain Retrieval), where a system needs to handle queries with diverse intents to find relevant documents from a large-scale, cross-domain pooled corpus, simulating challenges in real-world retrieval applications. In this under-explored setting, TART outperforms other state-of-the-art methods, demonstrating its ability to find documents in a large-scale opendomain corpus by leveraging explicit textual intents. Our analysis shows that training a model on diverse tasks with instructions, our new negative samples leveraging instructions and giving informative instructions are crucial.\n\nIn summary, our contributions are as follows:\n\n\u2022 Retrieval with instructions, a new formulation to model users' intent explicitly (Section 3).\n\n\u2022 BERRI, a new large-scale collection of approximately 40 retrieval datasets in diverse domains with instructions (Section 4).\n\n\u2022 TART, a task-aware retriever trained on BERRI that advances state of the art on zeroshot and cross-task retrieval (Section 5).\n\n\n"}, {"paperid": "paper3", "title": "Transformer Memory as a Differentiable Search Index", "abstract": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.", "introduction": "\n\nInformation retrieval (IR) systems map a user query q \u2208 Q to a ranked list of relevant documents {d 1 , . . . , d n } \u2286 D, typically represented by integers or short strings called document identifiers (docids). The most widely used IR approaches are based on pipelined retrieve-then-rank strategies. For retrieval, approaches based on inverted indexes or nearest neighbor search are common where contrastive learning based dual encoders (DEs) (Gillick et al., 2018;Karpukhin et al., 2020;Ni et al., 2021) are the present state-of-the-art. This paper proposes an alternative architecture, wherein a sequence-to-sequence (seq2seq) learning system (Sutskever et al., 2014) is used to directly map a query q to a relevant docid j \u2208 Y. This proposal is shown in the bottom half of Figure 1, for a sequence-to-sequence encoder-decoder architecture.\n\nWe call this proposed architecture a differentiable search index (DSI), and implement it with a large pre-trained Transformer (Vaswani et al., 2017) model, building on the recent success of large generative language models (LMs) Raffel et al., 2019;Devlin et al., 2018;Thoppilan et al., 2022;Du et al., 2021). In this proposed architecture, all information of the corpus is encoded within the parameters of the Transformer language model. At inference time, the trained model takes as input a text query q and outputs a docid j. If desired, beam search can be used to produce a ranked list of potentially-relevant docids. As we show, this process can work surprisingly well when trained properly. In our experiments it can consistently outperform DE baselines, sometimes drastically: for a base-sized T5 model, Hits@1 on the smallest corpus is improved by more than 20 points, from 12.4% for a DE to 33.9% for DSI; and on a corpus   \neach term t \u2192 {dj 1 , . . . , dj k } each docvec v d j \u2192 j to map dj \u2192 j retrieval approximate sparse matmul approximate MIPS run trained model (top-1) to find argmax j v T q v d j to find argmax j v T q v d j to find argmax j Pr(j|q)\n30\u00d7 larger, performance is improved by nearly 7 points. These gains increase when larger models are used: for an 11B-parameter T5 model, Hits@1 performance improves by more than 25 points over DE on the small corpus, and more than 15 points on the large corpus. DSI also performs extremely well in a zero-shot setting, e.g., improving Hits@1 by 14 points over BM25.\n\nIn addition to these quantitative gains, the DSI architecture is much simpler than a DE (see Table 1).\n\nA DE system fixes a search procedure (MIPS) and learns internal representations that optimize performance for that search procedure; in contrast, a DSI system contains no special-purpose fixed search procedure, instead using standard model inference to map from encodings to docids.\n\nOf particular interest to the machine learning community, as Table 1 shows, in DSI all aspects of retrieval are mapped into well-understood ML tasks. This may lead to new potential approaches to solving long-standing IR problems. As one example, since indexing is now a special case of model training, incrementally updating an index becomes a special case of model updating .\n\nIn this paper, DSI is applied to moderate-sized corpora (from 10k to 320k documents), all of which are derived from one challenging retrieval task, and we leave the important question of the scaling DSI to larger corpora to future work. The task considered is retrieving supporting passages given questions from the Natural Questions (NQ) dataset, a challenging task for lexical models.\n\nWhile the idea of DSI is simple, there are a number of ways it can be realized, some of which work surprisingly well, and some of which work surprisingly poorly. Below we explore a number of variations of the DSI architecture.\n\nDocument representation. We explore several approaches to representing documents, including a \"naive\" approach of using the document's full text, as well as variants of the bag-of-words representation used by traditional IR engines.\n\nDocid representation. We look at several ways to represent docids. In addition to naively representing integers as text strings, we also consider unstructured atomic docids, where each document is assigned a unique token, and some simple baselines for constructing structured semantic docids that describe how to navigate to a document through a hierarchical clustering of the corpus. Structured docidseither semantically structured via clustering, or naively structured as tokenized integers-scale better to large corpora, since the size of the vocabulary used in the decoder is made larger.\n\nIndexing. A trainable IR system traditionally has two phases: indexing a corpus (i.e., memorizing information about each document), and learning how to effectively retrieve from the index. In DSI, the index is stored in the model parameters, and indexing is simply another kind of model training. Figure 1 suggests one approach to indexing a corpus: namely, to train on (1) (2) alone do not provide enough information for a system to generalize to novel retrievals, there are many alternatives to examples of type (1) that might plausibly \"teach\" a model about the associations between documents and docids. We explore a number of these below, and show that some plausible-seeming techniques perform very poorly. We also explore a number of alternative multi-task optimization and curriculum learning schemes for combining these types of examples.\n\nEffects of model and corpus size. Since recent results suggest that some properties of large LMs emerge only for very large model sizes , we explore the performance of DSI for a range of model sizes and corpus sizes of 10k, 100k, and 320k documents.\n\nSummary. We show that even naive representations for documents and docids, coupled with appropriate training procedures to fine-tune modern large LMs, can perform surprisingly well; we present two improved docid representations, unstructured docids and semantically-structured docids, which improve the naive representation choice. We show that there is substantial variation in performance among indexing/training strategies and we show that performance of DSI significantly and consistently improves with model scale. To our knowledge this is the first case of generative indexing improving performance over strong baselines for a well-studied document retrieval task.\n\n\n"}, {"paperid": "paper4", "title": "Large Language Models are Built-in Autoregressive Search Engines", "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.", "introduction": "\n\nAlong with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;. For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022). To reduce the high training cost of autoregressive search engine, a smaller model size is preferred. However, the results of our pilot study in Figure 1 show smaller language models are significantly worse at mapping passages to document identifiers than larger ones. Moreover, different retrieval tasks can have unique retrieval requirements. One task may require a model to retrieve factual evidence to support or refute a claim (i.e., fact checking) (Onoe et al., 2021) while another may require a model to retrieve specific trivia information about an entity (i.e., entity linking) (Petroni et al., 2021;. It would be better if the retriever was capable of generalizing to new retrieval tasks with only a few examples.\n\nIn this work, we explore the use of in-context demonstrations to prompt LLMs to directly generate web URLs for document retrieval, namely LLM-URL. Surprisingly, we find that by providing a few (query, URL) pairs as contextual demonstrations, large language models (e.g. GPT-3) generate Web URLs where nearly 90% of the corresponding documents contain answers to opendomain questions. In this way, LLMs can be thought of as built-in search engines, as they have not been explicitly trained to map questions or documents to identifiers. Instead of using newlycreated document identifiers, LLM-URL leverages existing and widely used document identifiers directly, i.e., URLs. We compare our approach to existing document retrieval methods on three different open-domain question answering (QA) datasets: WebQ (Berant et al., 2013), NQ (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017). Further, to avoid exceeding the limit on the number of input tokens of LLMs, we employ an unsupervised passage filtering module to remove irrelevant portions of supporting documents. To summarize, our main contributions are as follows:\n\n1. We reveal that LLMs are built-in autoregressive search engines capable of document re-trieval by directly generating Web page URLs under both zero and few-shot settings.\n\n2. We show retrieving documents by generating URLs with LLMs significantly outperforms existing methods for document retrieval, as measured by Recall@K. Further, we show that breaking the retrieved documents into passages then using a ranker to filter the passages significantly reduces the number of supporting passages while maintaining high recall.\n\n3. We show the retrieved documents improve downstream QA performance as measured by EM when compared to baseline methods.\n\n2 Related Work\n\n\n"}]