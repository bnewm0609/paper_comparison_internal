{"pap_to_tab": {"Primary focus of the paper": {"paper_1": ["Mapping from language instructions and egocentric vision to action sequences for household tasks"], "paper_2": ["Robot performing manipulation actions to change the environment to answer questions"], "paper_3": ["Long-horizon language-conditioned tasks for general-purpose robots"], "paper_4": ["Multi-agent interactive question answering in dynamic environments"], "paper_5": ["Rearranging deep features for robotic manipulation using visual input"], "paper_6": ["Language-guided robotic manipulation tasks"], "paper_7": ["Cooperative, heterogeneous multi-agent learning in multimodal tasks"], "paper_8": ["Collaborative visual task completion with explicit and implicit communication"], "paper_9": ["Multi-agent task planning from ambiguous instructions with environmental perception and external knowledge"], "paper_10": ["Multi-robot manipulation conditioned on language instructions for collaborative tasks"]}, "Key methodological approaches used": {"paper_1": ["Embodied vision-and-language tasks with expert demonstrations in visual environments"], "paper_2": ["QA module with VQA method, DQN model for manipulation actions"], "paper_3": ["Open-source benchmark for embodied agents with onboard sensors"], "paper_4": ["Multi-agent 3D Reconstruction, shared multi-layer memories, and a 3D-CNN based QA model"], "paper_5": ["Transporter Network model to infer spatial displacements for manipulation"], "paper_6": ["AMSolver system, modular rule-based templates, and 6D-CLIPort model for manipulation"], "paper_7": ["Multimodal dataset, integrated learning framework, and state-of-the-art MARL techniques"], "paper_8": ["Learning to collaborate using AI2-THOR, explicit and implicit communication strategies"], "paper_9": ["Embodied task planning framework with use of semantic information and dynamic task allocation"], "paper_10": ["Procedural task generation, expert demonstrations, modular hierarchical planning approach"]}, "Involvement of a new dataset or benchmark": {"paper_1": ["ALFRED benchmark"], "paper_2": ["Novel dataset for MQA"], "paper_3": ["CALVIN benchmark"], "paper_4": ["Use of IQuADv1 dataset"], "paper_5": ["Transporter Nets experiments on simulated tasks"], "paper_6": ["VLMbench Benchmark"], "paper_7": ["Multimodal benchmark dataset"], "paper_8": ["Experiments in AI2-THOR"], "paper_9": ["Benchmark dataset for task planning"], "paper_10": ["LEMMA benchmark for multi-robot manipulation"]}, "Challenges or room for improvement": {"paper_1": ["Significant room for visual language understanding model development"], "paper_2": ["Challenges in robot's continuous manipulation until the question is answered"], "paper_3": ["Significant space for innovative agents to learn human language relation to world models"], "paper_4": ["Improvements needed for efficient multi-agent cooperation and knowledge sharing"], "paper_5": ["Sample efficiency and generalization to multi-step tasks as potential improvements"], "paper_6": ["Encourages research on language-guided robotic manipulation"], "paper_7": ["Advancing multi-agent RL methods in multimodal settings"], "paper_8": ["Benefits and challenges of explicit and implicit modes of communication"], "paper_9": ["Bridging simulation and physical environments, and sim2real issues"], "paper_10": ["Potential for developing future language-conditioned multi-robot systems"]}}, "cc_to_tab": {"Task Focus": {"paper_1": ["Mapping instructions and vision to actions for household tasks"], "paper_2": ["Robotic manipulation to answer questions"], "paper_3": ["Language-conditioned policy learning for long-horizon manipulation tasks"], "paper_4": ["Multi-agent interactive question answering"], "paper_5": ["Rearranging visual world for robotic manipulation"], "paper_6": ["Language-guided robotic manipulation"], "paper_7": ["Cooperative multi-agent reinforcement learning"], "paper_8": ["Collaborative visual task completion"], "paper_9": ["Multi-agent task planning from ambiguous instructions"], "paper_10": ["Multi-robot manipulation with language instruction"]}, "Approach": {"paper_1": ["Egocentric vision, natural language instructions"], "paper_2": ["QA module, DQN-based manipulation module"], "paper_3": ["Open-source simulated benchmark for long-horizon tasks"], "paper_4": ["Shared structural and semantic memories, 3D-CNN"], "paper_5": ["Infer spatial displacements from visual input"], "paper_6": ["Modular rule-based task templates, keypoint-based model"], "paper_7": ["Multimodality impact on learning, simple message passing"], "paper_8": ["Explicit and implicit communication modes"], "paper_9": ["Utilize external knowledge and semantic information"], "paper_10": ["Hierarchical planning, language-conditioned collaboration"]}, "Challenge": {"paper_1": ["Complex and compositional tasks"], "paper_2": ["Manipulation to change the environment for QA"], "paper_3": ["Long-horizon tasks, zero-shot to novel language instructions"], "paper_4": ["Balanced work division, knowledge sharing"], "paper_5": ["Complex multi-modal policy distributions"], "paper_6": ["Diverse object shapes, action types, and motion constraints"], "paper_7": ["Unique challenges of multimodality"], "paper_8": ["Learning to collaborate from pixels"], "paper_9": ["Ambiguous instruction interpretation, dynamic task planning"], "paper_10": ["Task allocation, strong temporal dependencies"]}, "Collaboration": {"paper_1": ["Single-agent performing household tasks"], "paper_2": ["Single-agent manipulation to reveal information"], "paper_3": ["Single-agent long-horizon tasks"], "paper_4": ["Multi-agent cooperation for QA"], "paper_5": ["Single-agent manipulation tasks"], "paper_6": ["Single-agent following language instructions"], "paper_7": ["Cooperative multi-agent learning"], "paper_8": ["Dual-agent collaboration"], "paper_9": ["Multi-agent collaborative task planning"], "paper_10": ["Multi-robot manipulation collaboration"]}, "Novelty and Language Aspect": {"paper_1": ["Interpreting natural language for task execution"], "paper_2": ["QA model adopted from Visual Question Answering"], "paper_3": ["Language-conditioned tasks, zero-shot learning"], "paper_4": ["Agents share knowledge for efficient QA"], "paper_5": ["Spatial reasoning without objectness assumptions"], "paper_6": ["Language-guided manipulation with motion constraints"], "paper_7": ["Multimodal (vision-and-language) cooperative learning"], "paper_8": ["Learning explicit/implicit communication in visual tasks"], "paper_9": ["Ambiguous language instruction resolution"], "paper_10": ["Language-conditioned task allocation and manipulation"]}}, "multi_scheme": {"types of tasks in LEMMA benchmark": {"paper_10": ["8 types of procedurally generated tasks with varying degree of complexity, including tool use and passing tools between robots"]}, "evaluation of multi-agent collaborative tasks on simulation platform": {"paper_9": ["Evaluation experiments on the built benchmark dataset, includes three types of high-level instructions, simulation and physical scenario assessments"]}, "generalization aspect of the Transporter Network": {"paper_5": ["Generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place"]}, "expert demonstrations in ALFRED": {"paper_1": ["Includes expert demonstrations in interactive visual environments"]}, "components of proposed MQA framework": {"paper_2": ["QA module and a manipulation module"]}, "agents developed with CALVIN understanding novel instructions": {"paper_3": ["Evaluated in zero-shot to novel language instructions"]}, "generalization of MQA framework to different environments": {"paper_2": ["No value provided"]}, "source for Transporter Network materials": {"paper_5": ["Experiment videos and code available at https://transporternets.github.io"]}, "applying proposed model in physical scenarios results": {"paper_9": ["Achieved promising results for multi-agent collaborative tasks in physical scenarios"]}, "challenges presented by MQA task for robotic manipulation": {"paper_2": ["Manipulation actions to change the environment, continuous interaction until the answer is found"]}, "ALFRED benchmark's suggestion on grounded visual language models": {"paper_1": ["Recent models perform poorly on ALFRED, indicating significant room for development"]}, "state changes in ALFRED tasks": {"paper_1": ["Include long, compositional tasks with non-reversible state changes"]}, "task allocation by multiple agents in the framework": {"paper_9": ["Joint reasoning of operation details, dynamic task planning and allocation amongst agents"]}, "CALVIN's relation of language to robot perceptions/actions": {"paper_3": ["Learning long-horizon language-conditioned tasks, agents relate human language to their world models"]}, "role of multimodality in CH-MARL": {"paper_7": ["Multimodality introduces unique challenges for cooperative multi-agent learning"]}, "main objective of ALFRED benchmark": {"paper_1": ["Learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks"]}, "agents using CALVIN tested in novel environments/objects": {"paper_3": ["Evaluated in novel environments and with novel objects"]}, "resolution of high-level ambiguous instructions in task planning": {"paper_9": ["Utilizes external knowledge and dynamic perceived visual information to resolve high-level instructions"]}, "simulated robots in CH-MARL multimodal tasks": {"paper_7": ["Multiple simulated heterogeneous robots in a multi-room home environment"]}, "robot's determination to stop manipulating objects in MQA task": {"paper_2": ["The robot keeps manipulating objects until the answer to the question is found"]}}, "ours_final_table": {"problem tackled": {"paper_1": ["Interpreting Grounded Instructions for Everyday Tasks in a benchmark for mapping natural language instructions and egocentric vision to action sequences for household tasks"], "paper_2": ["Answering questions through robotic manipulation where a robot interacts with the environment to find answers"], "paper_3": ["Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks and relating human language to perceptions and actions for daily tasks"], "paper_4": ["Multi-Agent Interactive Question Answering where agents explore a scene to answer questions and need to share knowledge to answer accurately"], "paper_5": ["Inferring spatial displacements for robotic manipulation from visual input with Transporter Networks, avoiding assumptions of objectness"], "paper_6": ["Language-guided embodied agents for object manipulation benchmarking with an Automatic Manipulation Solver system (AMSolver)"], "paper_7": ["Multimodal (vision-and-language) collaborative multi-agent reinforcement learning in a rich multi-room home environment"], "paper_8": ["Collaborative visual task completion requiring learning from pixels, to communicate, and to perceive in visually rich environments"], "paper_9": ["Embodied Multi-Agent Task Planning from Ambiguous Instructions for human-robot collaboration where implicit high-level instructions need to be interpreted and executed"], "paper_10": ["Language-Conditioned Multi-robot Manipulation for task allocation and long-horizon object manipulation based on human language instructions"]}, "proposed approach": {"paper_1": ["ALFRED does not propose a new approach but introduces a benchmark for learning mappings from natural language instructions and egocentric vision to action sequences for household tasks."], "paper_2": ["MQA proposes a framework consisting of a QA module adopting the Visual Question Answering method and a manipulation module with a DQN model to generate manipulation actions."], "paper_3": ["CALVIN presents an open-source simulated benchmark to learn long-horizon language-conditioned tasks and does not focus on a new approach, but suggests the need for innovative agents."], "paper_4": ["Multi-agent Embodied QA involves a proposed framework with multi-layer structural and semantic memories and a QA model built on a 3D-CNN for encoding scene memories."], "paper_5": ["Transporter Networks suggests a model architecture that rearranges deep features to infer spatial displacements from visual input, parameterizing robot actions."], "paper_6": ["VLMbench introduces the AMSolver system and establishes a benchmark with modular rule-based task templates for language-guided robotic manipulation."], "paper_7": ["CH-MARL proposes a multimodal benchmark for cooperative, heterogeneous multi-agent learning and a simple message passing method between agents for collaborative tasks."], "paper_8": ["The approach in 'Two Body Problem' entails learning to perform visual tasks through cooperation leveraging explicit and implicit communication, studied directly from pixels."], "paper_9": ["The proposed framework in 'Embodied Multi-Agent Task Planning from Ambiguous Instruction' utilizes external knowledge and perceived information to resolve instructions and allocate tasks."], "paper_10": ["LEMMA introduces a benchmark for multi-robot manipulation conditioned on language, focusing on task allocation and manipulation, and proposes a modular hierarchical planning approach."]}, "Main Problem Addressed": {"paper_1": ["Learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks"], "paper_2": ["Enabling a robot to perform manipulation actions to change the environment to answer a given question"], "paper_3": ["Developing agents that relate human language to perceptions and actions for composing long-horizon language-conditioned tasks"], "paper_4": ["Multi-Agent Interactive Question Answering with agents exploring interactively to answer questions"], "paper_5": ["Learning vision-based manipulation tasks by rearranging deep features to infer spatial displacements for robotic manipulation"], "paper_6": ["Language-guided robotic manipulation based on human commands for complex tasks"], "paper_7": ["A multimodal benchmark for cooperative and heterogeneous multi-agent learning"], "paper_8": ["Learning to collaborate from pixels in visually rich environments and solving the problem of task completion"], "paper_9": ["Embodied Multi-Agent Task Planning from Ambiguous Instruction to perform operations and plan tasks dynamically"], "paper_10": ["Benchmark for language-conditioned multi-robot manipulation focused on task allocation and manipulation based on human language instructions"]}, "challenge focal point": {"paper_1": ["Developing innovative grounded visual language understanding models due to poor baseline model performance on complex, long, compositional tasks with non-reversible state changes"], "paper_2": ["Integrating a QA module with a Manipulation module for a robot to interact with the environment and change it in order to answer questions"], "paper_3": ["Creating agents that can solve many robotic manipulation tasks over a long horizon, following unconstrained language instructions, with poor performance of baseline models in zero-shot novel language instructions and novel environments"], "paper_4": ["Multi-Agent Interactive Question Answering where agents cooperate in a shared environment and task division and knowledge sharing are essential for efficient collaboration"], "paper_5": ["Learning efficient spatial displacements for robotic manipulation in a sample efficient way and generalizing across different manipulation tasks"], "paper_6": ["Developing language-guided robotic manipulation capabilities with a focus on diverse object manipulations that follow human-generated language instructions"], "paper_7": ["Addressing cooperative, heterogeneous multi-agent learning in a multimodal vision-and-language context with significant room for advancement in multi-agent reinforcement learning methods"], "paper_8": ["Learning to collaborate in visually rich environments and balancing explicit and implicit communication between agents to complete visual tasks"], "paper_9": ["Resolving ambiguous instructions for multi-agent task planning and bridging the gap between simulation and real-world (sim2real) environments"], "paper_10": ["Facilitating language-conditioned multi-robot collaboration for object manipulation, task allocation, and dealing with temporal dependencies in complex, procedurally generated tasks"]}, "Integration of robotic manipulation with question-answering systems": {"paper_1": ["ALFRED does not directly integrate question-answering with manipulation but focuses on mapping instructions and vision to action sequences for household tasks, which could indirectly aid question-answering via manipulation"], "paper_2": ["MQA proposes to change the environment through manipulation actions in order to answer questions, integrating a QA module with a DQN-based manipulation module"], "paper_3": ["While not explicitly question-answering, CALVIN involves following language instructions for robot manipulation, which is language-conditioned policy learning related to the task"], "paper_4": ["Multi-Agent Interactive Question Answering requires agents to explore and interact with the environment, implicitly suggesting manipulation actions to answer questions"], "paper_5": ["Transporter Networks rearrange visual world features for robotic manipulation; not explicitly linking to question-answering but manipulation knowledge could be used in such systems"], "paper_6": ["VLMbench focuses on language-guided robotic manipulation without a direct question-answering component, but manipulation is related to language instructions"], "paper_7": ["CH-MARL includes a multimodal benchmark for multi-agent collaboration, which could imply the use of manipulation in conjunction with language tasks, yet no direct QA integration is discussed"], "paper_8": ["The study explores collaborative visual task completion without mentioning an explicit integration of QA systems, but collaboration could involve implied QA tasks"], "paper_9": ["The embodied multi-agent task planning framework uses language and visual perception to resolve high-level instructions, which could tie in with question-answering via manipulation actions"], "paper_10": ["LEMMA involves multi-robot manipulation based on language instructions, which may support integration with question-answering systems, though not the primary focus"]}, "Challenges in robotic manipulation for QA tasks": {"paper_1": ["Baseline models perform poorly, indicating complexity in language understanding, sequence length, and action space."], "paper_2": ["The need for a robot to interact with the environment based on the QA module's output and the challenge of continuous manipulation to find answers."], "paper_3": ["Difficulty of developing agents that relate human language to their world models due to task complexity and long horizon nature."], "paper_4": ["Agents require organized work division and shared knowledge for collaboration, presenting challenges in multi-agent exploration and coordination."], "paper_5": ["Challenges include learning spatial displacements without object assumptions and generalizing to multi-step tasks and different manipulation scenarios."], "paper_6": ["Complexity in generating language instructions, variety in object shapes and motions, and creating 6DoF action sequences."], "paper_7": ["Unique challenges introduced by multimodality for learning, the need of message passing for coordination, and the adjustment to heterogeneous agents."], "paper_8": ["Learning to collaborate from pixels, deciding when and what to communicate, and the challenges of combining communication with visual perception for task completion."], "paper_9": ["Difficulties in understanding ambiguous instructions, integrating external knowledge sources with dynamic planning, and bridging the simulation-to-reality gap."], "paper_10": ["Challenges in task allocation based on language, temporal dependencies in collaboration, and the requirement for robots to understand and use tools interactively."]}, "environment interactions for question-answering": {"paper_1": ["Household task execution with natural language directives and egocentric vision for action sequence mapping"], "paper_2": ["Robot performing manipulation actions in a bin to change environment and answer questions"], "paper_3": ["Composing long-horizon tasks following unconstrained language instructions using onboard sensors"], "paper_4": ["Multi-agent exploration jointly in interactive environments to answer questions, with agents organizing work and sharing object knowledge"], "paper_5": ["Manipulation tasks requiring spatial displacements inferred from visual input, with experiments on simulated tasks and real-world validation"], "paper_6": ["Language-guided robotic manipulation with diverse object shapes, motion constraints, and action types, including multi-view observation handling"], "paper_7": ["Collaboration between heterogenous robots in multi-room home environment, featuring vision-and-language dataset and multi-modal learning"], "paper_8": ["Collaborative visual task completion in AI2-THOR, focusing on explicit and implicit communication and perception"], "paper_9": ["Embodied multi-agent task planning from ambiguous instruction, using external knowledge and perception for task allocation"], "paper_10": ["Multi-robot manipulation based on language instructions, with task allocation, tool usage, and expert demonstrations for training"]}, "Issue addressed by the paper": {"paper_1": ["Aligning natural language instructions and egocentric vision with action sequences for everyday household tasks"], "paper_2": ["How a robot can perform manipulation actions to change the environment in response to a given question"], "paper_3": ["Learning long-horizon, language-conditioned tasks by robots through human language for a variety of daily tasks"], "paper_4": ["Multi-Agent Interactive Question Answering through cooperative exploration and knowledge sharing in interactive environments"], "paper_5": ["Inferring spatial displacements from visual input for robotic manipulation without assuming objectness"], "paper_6": ["Combining language flexibility and compositional benefits for language-guided robotic manipulation tasks"], "paper_7": ["Introducing a multimodal dataset for cooperative heterogeneous multi-agent learning in a home environment"], "paper_8": ["Collaborative Visual Task Completion that involves communication and interaction between agents in visually rich environments"], "paper_9": ["Multi-Agent Task Planning from ambiguous instructions requiring joint reasoning and collaborative operation"], "paper_10": ["Language-Conditioned Multi-robot Manipulation for task allocation and long-horizon object manipulation"]}, "interaction objects": {"paper_1": ["household objects"], "paper_2": ["variety of object models in a simulation bin"], "paper_3": ["robotic manipulation tasks on long horizon language-conditioned tasks"], "paper_4": ["3D scene reconstruction objects and interactive environment objects"], "paper_5": ["blocks, ropes, piles of small objects, and kits with unseen objects"], "paper_6": ["diverse object shapes and appearances in categorized robotic manipulation tasks"], "paper_7": ["objects in a multi-room home environment for cooperative tasks"], "paper_8": ["visually rich environments, AI2-THOR objects for collaborative tasks"], "paper_9": ["ambiguous high-level instruction resolved objects for multi-agent task planning"], "paper_10": ["procedurally generated tasks involving object and tool manipulations"]}, "Robotic Manipulation Tasks": {"paper_1": ["interpreting grounded instructions for everyday household tasks"], "paper_2": ["manipulation actions to change the environment to answer questions"], "paper_3": ["long-horizon robot manipulation tasks following unconstrained language instructions"], "paper_4": ["exploration and 3D reconstruction in interactive environments for question answering"], "paper_5": ["vision-based manipulation tasks such as stacking blocks, assembling kits, manipulating ropes, and pushing piles of small objects"], "paper_6": ["language-guided robotic manipulation tasks with diverse object shapes, action types, and motion constraints"], "paper_7": ["collaborative tasks involving multiple heterogeneous robots in home environments"], "paper_8": ["collaborative visual task completion in visually rich environments"], "paper_9": ["embodied multi-agent task planning from ambiguous instruction for collaborative tasks"], "paper_10": ["task allocation and long-horizon object manipulation in tabletop setting requiring tool use and passing"]}, "central challenge explored in the study": {"paper_1": ["learning a mapping from natural language instructions and egocentric vision to sequences of actions for everyday household tasks"], "paper_2": ["enabling a robot to perform manipulation actions to change the environment as an answer to a given question"], "paper_3": ["developing agents that can solve robotic manipulation tasks specified only via human language over long horizons"], "paper_4": ["agents exploring jointly to answer questions in multi-agent interactive environments and the cooperation for task completion"], "paper_5": ["learning vision-based manipulation tasks using spatial displacements with sample efficiency and generalization to complex tasks"], "paper_6": ["guiding embodied agents through complex manipulation tasks with language instruction, focusing on diverse shapes, actions, and motion constraints"], "paper_7": ["cooperative multi-agent learning in a multimodal (vision-and-language) setting and the integration of communication between heterogeneous agents"], "paper_8": ["learning to collaborate directly from pixels, including aspects of performing tasks, communication, and action based on visual perception"], "paper_9": ["multi-agent task planning from ambiguous instructions, integrating external knowledge and visual perception for dynamic task allocation"], "paper_10": ["language-conditioned multi-robot manipulation, focusing on task allocation and long-horizon manipulation with human instructions"]}, "Knowledge Sharing Strategies": {"paper_1": ["Not explicitly mentioned in the abstract"], "paper_2": ["Not explicitly mentioned in the abstract"], "paper_3": ["Not explicitly mentioned in the abstract"], "paper_4": ["Multi-layer structural and semantic memories", "Shared by all agents"], "paper_5": ["Not explicitly mentioned in the abstract"], "paper_6": ["Not explicitly mentioned in the abstract"], "paper_7": ["Multimodal implementations of multi-agent reinforcement learning", "Consistent evaluation protocol", "Message passing method between agents"], "paper_8": ["Explicit communication through messages", "Implicit communication through perception of the agents and visual world"], "paper_9": ["Utilization of external knowledge sources", "Semantic information for environment perception"], "paper_10": ["Not explicitly mentioned in the abstract"]}, "Work Division Strategies": {"paper_1": ["Not explicitly mentioned in the abstract"], "paper_2": ["Manipulation actions are generated by a DQN model"], "paper_3": ["Not explicitly mentioned in the abstract"], "paper_4": ["Balanced work division organized by next viewpoints planning"], "paper_5": ["Not explicitly mentioned in the abstract"], "paper_6": ["Not explicitly mentioned in the abstract"], "paper_7": ["Heterogeneous multi-agent learning, implying potentially diverse roles"], "paper_8": ["Learning when and what to communicate for task performance", "Acts based on communications and visual perceptions"], "paper_9": ["Joint reasoning of operation details and dynamic task planning", "Dynamic allocation of decomposed tasks to multiple agents"], "paper_10": ["Task allocation based on human language instructions", "Hierarchical planning approach for assigning sub-tasks"]}, "knowledge sharing": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["features multi-layer structural and semantic memories shared by all agents"], "paper_5": [], "paper_6": [], "paper_7": ["introduces a simple message passing method between agents"], "paper_8": ["demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks"], "paper_9": ["utilize external knowledge sources and dynamically perceived visual information"], "paper_10": []}, "work division": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["agents simultaneously explore and scan the scene with a clear division of work, organized by next viewpoints planning"], "paper_5": [], "paper_6": [], "paper_7": [], "paper_8": ["Learning to collaborate entails learning when and what to communicate, and how to act based on these communications"], "paper_9": ["jointly reason the operation details and perform the embodied multi-agent task planning"], "paper_10": ["focused on task allocation and long-horizon object manipulation based on human language instructions"]}, "interactive question answering performance": {"paper_1": ["Performance is not directly linked to interactive question answering but to action sequences following instructions."], "paper_2": ["Performance is measured by the robot's ability to manipulate objects to answer a question, not explicitly linked to interactivity."], "paper_3": ["CALVIN measures performance on zero-shot language instructions and novel environments, but not interactive question answering performance."], "paper_4": ["Performance is improved in multi-agent scenarios for interactive question answering with knowledge sharing and work division."], "paper_5": ["While interactive question answering is not the focus, Transporter Networks are sample efficient in learning manipulation tasks."], "paper_6": ["Interactive question answering is not directly addressed; the focus is on language-guided manipulation performance."], "paper_7": ["Performance relates to cooperative tasks in multi-agent settings rather than interactive question answering."], "paper_8": ["Performance in collaborative visual task completion is improved with explicit and implicit communication, which may correlate to interactive question answering efficiency."], "paper_9": ["Performance is evaluated about multi-agent task planning from ambiguous instruction, which involves interactive planning rather than question answering."], "paper_10": ["Not focused on question answering, but on language-conditioned manipulation tasks where performance is based on collaboration and task execution."]}, "framework or model": {"paper_1": ["ALFRED does not directly address multi-agent systems but provides a benchmark for learning from natural language instructions and egocentric vision to action sequences."], "paper_2": ["Manipulation Question Answering (MQA) framework with a QA module based on Visual Question Answering (VQA) and a manipulation module using a Deep Q Network (DQN) model."], "paper_3": ["CALVIN provides an open-source simulated benchmark for long-horizon, language-conditioned tasks, with focus on single-agent systems."], "paper_4": ["Multi-Agent Interactive Question Answering framework with multi-layer structural and semantic memories shared by all agents, 3D-CNN for encoding the scene, and organized by next viewpoints planning."], "paper_5": ["Transporter Network to infer spatial displacements from visual input, not specific to multi-agent systems."], "paper_6": ["Automatic Manipulation Solver (AMSolver) system and Vision-and-Language Manipulation benchmark (VLMbench), with keypoint-based model 6D-CLIPort for multi-view observations and language instructions."], "paper_7": ["Multimodal benchmark with integrated learning framework, including implementations of state-of-the-art multi-agent reinforcement learning techniques and a simple message passing method."], "paper_8": ["Framework for learning to collaborate directly from pixels in AI2-THOR, with explicit and implicit communication modes studied."], "paper_9": ["Embodied multi-agent task planning framework that utilizes external knowledge and dynamic perception to interpret ambiguous instructions and allocate tasks."], "paper_10": ["Benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) with a modular hierarchical planning approach to address task allocation and manipulation."]}, "issues addressed regarding multi-agent systems": {"paper_1": ["Single-agent embodied AI task benchmark"], "paper_2": ["Single-agent system for manipulating objects to answer questions"], "paper_3": ["Single-agent language-conditioned policy learning"], "paper_4": ["Multi-agent coordination and knowledge sharing for answering questions"], "paper_5": ["Single-agent system for robotic manipulation"], "paper_6": ["Single-agent system for language-guided robotic manipulation"], "paper_7": ["Multimodal cooperative learning for heterogeneous multi-agent systems"], "paper_8": ["Collaboration between multi-agents in visually rich environments, with a focus on communication"], "paper_9": ["Embodied multi-agent planning with ambiguous instructions, utilizing semantic and visual perception"], "paper_10": ["Multi-robot manipulation and task allocation with language instructions"]}, "interactive question answering": {"paper_1": ["ALFRED dataset focuses on mapping natural language instructions to actions, not directly on question answering but rather on performing tasks based on directives."], "paper_2": ["MQA involves a robot manipulating the environment to answer questions, thus combining question answering with interactive manipulation."], "paper_3": ["CALVIN is not explicitly focused on question answering but on policy learning from language instructions for long-horizon tasks which could include question answering components."], "paper_4": ["The focus is on multi-agent interactive question answering, where agents must coordinate to answer questions about the environment."], "paper_5": ["Not focused on question answering, but on robotic manipulation through spatial displacements inferred from visual input."], "paper_6": ["VLMbench involves language instructions for object manipulation, indirect reference to interactive question answering through the 'Automatic Manipulation Solver' system."], "paper_7": ["CH-MARL is a benchmark for multi-agent learning with multimodal (vision-and-language) data, which could involve interactive question answering in a cooperative context."], "paper_8": ["Focused on collaborative visual task completion which may involve communication and could implicitly link to interactive question answering."], "paper_9": ["Focuses on multi-agent task planning from ambiguous instruction which includes understanding instruction and could potentially be expanded to include question answering."], "paper_10": ["LEMMA focuses on multi-robot manipulation based on language, which is not directly linked to question answering but to task execution from instructions."]}, "robotic manipulation": {"paper_1": ["Robotic manipulation through action sequences based on natural language directives."], "paper_2": ["Robotic manipulation to alter the environment in service of answering questions."], "paper_3": ["Robotic manipulation based on language instructions for long-horizon tasks."], "paper_4": ["Not directly focused on robotic manipulation, but it's implied as part of multi-agent exploration and environment interaction."], "paper_5": ["Direct focus on learning robotic manipulation tasks from visual input."], "paper_6": ["Implementation of robotic manipulations guided by language instructions for varied tasks."], "paper_7": ["Multi-agent reinforcement learning which could involve robotic manipulation as part of task performance."], "paper_8": ["Collaborative task completion that may involve robotic manipulation as part of the collaborative effort."], "paper_9": ["Task planning for robots indicates robotic manipulation, though the focus is on planning and language understanding."], "paper_10": ["Robotic manipulation across multiple robots, each with different capabilities and tasks."]}, "language-guided tasks": {"paper_1": ["Language-guided tasks are the core of ALFRED through high and low-level natural language directives."], "paper_2": ["Language serves as input to determine the robot's actions for manipulation tasks; thus, language guides the task completion."], "paper_3": ["CALVIN is designed for language-conditioned policy learning, implying tasks are guided by language instructions."], "paper_4": ["Implicitly involves language guidance as part of the multi-agent question answering process."], "paper_5": ["Language is not the primary focus but is potentially part of the interface for task specification."], "paper_6": ["Language-guided robotic manipulation via categorized instructions is a principal component of VLMbench."], "paper_7": ["Multimodality with vision-and-language implies tasks could be directed by language combined with vision."], "paper_8": ["Learning to collaborate effectively may involve understanding and following language-based instructions."], "paper_9": ["Robots use external knowledge and visual perception to understand and execute language-guided tasks."], "paper_10": ["Language-Conditioned Multi-robot Manipulation benchmark implies language guides the manipulation tasks."]}, "Inferring spatial displacements from visual input": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": ["The Transporter Network uses deep features to infer spatial displacements from visual input."], "paper_6": [], "paper_7": [], "paper_8": [], "paper_9": [], "paper_10": []}, "Proposed method to enable this capability in robots": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": ["The Transporter Network rearranges deep features to parameterize robot actions for manipulation tasks."], "paper_6": [], "paper_7": [], "paper_8": [], "paper_9": [], "paper_10": []}, "Methodologies for teaching robots": {"paper_1": ["ALFRED uses expert demonstrations in interactive visual environments to map from language instructions to action sequences. It assesses a baseline model based on embodied vision-and-language tasks."], "paper_2": ["MQA employs a QA module adapted from VQA and a manipulation module using a DQN model for generating manipulation actions based on visual input."], "paper_3": ["CALVIN is a benchmark for language-conditioned policy learning using long-horizon tasks, evaluating agents on zero-shot language instructions with an imitation learning baseline model."], "paper_4": ["A framework with multi-layer structural and semantic memories and a 3D-CNN based QA model, supplemented by multi-agent 3D Reconstruction for scene exploration and knowledge sharing."], "paper_5": ["The Transporter Network architecture rearranges deep features to infer spatial displacements from visual input and parameterizes robot actions for manipulation."], "paper_6": ["VLMbench involves an Automatic Manipulation Solver (AMSolver) system with modular templates for generating demonstrations, using a keypoint-based model 6D-CLIPort for multi-view language conditioned manipulation."], "paper_7": ["CH-MARL presents a multimodal dataset with tasks for cooperative multi-agent learning, employing multimodal multi-agent reinforcement learning techniques and a simple message passing method."], "paper_8": ["Two Body Problem studies the role of explicit and implicit communication for task completion in visually rich environments, learning from visual input and communication data."], "paper_9": ["An embodied multi-agent task planning framework that uses external knowledge and dynamic perception to interpret ambiguous instructions and allocate tasks among agents."], "paper_10": ["LEMMA introduces a benchmark for language-conditioned multi-robot manipulation in a tabletop setting, using a modular hierarchical planning approach for task allocation based on expert demonstrations and human language instructions."]}, "Manipulation tasks based on visual input": {"paper_1": ["Egocentric vision mapping to action sequences with non-reversible state changes."], "paper_2": ["Robotic manipulation actions to answer questions with a constant environmental interaction."], "paper_3": ["Language-conditioned tasks with an open-source simulated benchmark, agents learn from onboard sensors and language instructions."], "paper_4": ["Agents explore and interact with scenes in a shared 3D space to collaboratively answer questions through multi-agent exploration and scene reconstruction."], "paper_5": ["Inducing sequences of spatial displacements through vision-based manipulation tasks, including stacking, assembly, and object pushing with visual input."], "paper_6": ["Robot demonstrations following language instructions for categorized manipulation tasks with object shapes, appearances, and motion constraints."], "paper_7": ["Tasks involving collaboration among heterogeneous robots for multi-agent learning in a home environment using visually rich inputs."], "paper_8": ["Task learning, communication, and action performance in a visual environment with AI2-THOR, focusing on collaborative visual task completion."], "paper_9": ["High-level instruction interpretation for dynamic task planning with joint reasoning and embodied multi-agent planning in both simulated and physical scenarios."], "paper_10": ["Procedurally generated tasks for multi-robot manipulation with tool use and passing based on language instructions and expert demonstrations."]}, "implications for robotic manipulation": {"paper_1": ["ALFRED benchmark requires interpreting language for action sequences, highlighting the challenges in mapping instructions to physical actions, and suggests need for advanced models."], "paper_2": ["MQA framework's manipulation actions based on QA and DQN suggests robots adapting environment interactions for specific information retrieval, which impacts how robots perceive and manipulate their surroundings for tasks."], "paper_3": ["CALVIN benchmark emphasizes learning long-horizon, language-conditioned tasks, pointing towards the need for robots to understand complex instructions and perform sequences of manipulations."], "paper_4": ["Multi-agent collaboration for interactive QA relies on structural and semantic memories and balanced workload, indicating advancements in shared knowledge bases and coordinated manipulations."], "paper_5": ["Transporter Networks focus on rearranging visual world using spatial displacements implies significant efficiency and generalization in robotic manipulation without explicit object models."], "paper_6": ["VLMbench and AMSolver demonstrate object manipulation by robot following language instructions, stressing communication of spatial constraints and manipulation nuances."], "paper_7": ["CH-MARL introduces cooperative learning with heterogeneous agents indicating multimodal input and communication can enhance collaborative manipulation tasks."], "paper_8": ["Study of collaborative visual task completion emphasizes importance of communication, perception, and actions based on visual inputs, suggesting complex multi-agent spatial coordination."], "paper_9": ["Framework for multi-agent task planning from ambiguous instruction underlines the need for robots to interpret and dynamically plan manipulative actions in coordination with other agents."], "paper_10": ["LEMMA benchmarks robots performing language-conditioned manipulation demonstrating importance of task allocation, collaborative execution, and temporal dependencies in manipulative actions."]}}}