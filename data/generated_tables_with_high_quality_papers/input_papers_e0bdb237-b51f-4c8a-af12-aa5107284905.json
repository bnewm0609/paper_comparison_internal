[{"paperid": "paper0", "title": "Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images", "abstract": "In this paper, we introduce Recipe1M, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M dataset and food and cooking in general. Code, data and models are publicly available.", "introduction": "\n\nT HERE are few things so fundamental to the human experience as food. Its consumption is intricately linked to our health, our feelings and our culture. Even migrants starting a new life in a foreign country often hold on to their ethnic food longer than to their native language. Vital as it is to our lives, food also offers new perspectives on topical challenges in computer vision like finding representations that are robust to occlusion and deformation (as occur during ingredient processing).\n\nThe profusion of online recipe collections with user-submitted photos presents the possibility of training machines to automatically understand food preparation by jointly analyzing ingredient lists, cooking instructions and food images. Far beyond applications solely in the realm of culinary arts, such a tool may also be applied to the plethora of food images shared on social media to achieve insight into the significance of food and its preparation on public health [1] and cultural heritage [2]. Developing a tool for automated analysis requires large and well-curated datasets.\n\nThe emergence of massive labeled datasets [3], [4] and deeplylearned representations [5], [6], [7] have redefined the state-ofthe-art in object recognition and scene classification. Moreover, the same techniques have enabled progress in new domains like dense labeling and image segmentation. Perhaps the introduction of a new large-scale food dataset-complete with its own intrinsic challenges-will yield a similar advancement of the field. For instance, categorizing an ingredient's state (e.g., sliced, diced, raw, baked, grilled, or boiled) provides a unique challenge in attribute recognition-one that is not well posed by existing datasets. Furthermore, the free-form nature of food suggests a departure from the concrete task of classification in favor of a more nuanced objective that integrates variation in a recipe's structure. *contributed equally. Learning cross-modal embeddings from recipe-image pairs collected from online resources. These embeddings enable us to achieve in-depth understanding of food from its ingredients to its preparation.\n\nHence, we argue that food images must be analyzed together with accompanying recipe ingredients and instructions in order to acquire a comprehensive understanding of \"behind-the-scene\" cooking process as illustrated in Fig. 1.\n\nExisting work, however, has focused largely on the use of medium-scale image datasets for performing food categorization. For instance, Bossard et al. [8] introduced the Food-101 visual classification dataset and set a baseline of 50.8% accuracy. Even with the impetus for food image categorization, subsequent work by [9], [10] and [11] could only improve this result to 77.4%, 79% and 80.9%, respectively, which indicates that the size of the dataset may be the limiting factor. Although Myers et al. [10] built upon Food-101 to tackle the novel challenge of estimating a meal's energy content, the segmentation and depth information used in their work are not made available for further exploration.\n\n\n"}, {"paperid": "paper1", "title": "A Large-Scale Benchmark for Food Image Segmentation", "abstract": "Food image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks -- the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images. In this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based, Feature Pyramid based, and Vision Transformer based) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works on fine-grained food image understanding. We make all these datasets and methods public at \\url{https://xiongweiwu.github.io/foodseg103.html}.", "introduction": "\n\nFood computing has attracted increasing public attention in recent years, as it provides the core technologies for food and healthrelated research and applications. [2,9,31,43]. One of the important goals of food computing is to automatically recognize different types of food and profile their nutrition and calorie values. In computer vision, the related works include dish classification [11,50,52], recipe generation [14,39,46], and food image retrieval [6,42]. Most of them focus on representing and analysing the food image as a whole, and do not explicitly localize or classify its individual ingredients-the visible components in the cooked food. We call the former food image classification and the latter food image segmentation. Between the two, food image segmentation is more complex as it aims to recognize each ingredient category as well as its pixel-wise locations in the food image. As shown in Figure 1, given an \"hamburger\" example image, a good segmentation model  needs to recognize and mask out \"beef\", \"tomato\", \"lettuce\", \"onion\" and \"bread roll\" ingredients. Compared to semantic segmentation on general object images [3,17,22], food image segmentation is more challenging due to the large diversity in food appearances and the often imbalanced distribution of categories of ingredients. First, an ingredient cooked differently can vary a lot visually, e.g., \"pineapples\" cooked with meat in Figure 1 (a) versus the \"pineapples\" in a fruit platter in Figure 1 (b). Different ingredients may look very similar, e.g., \"pineapples\" cooked with meat cannot be easily distinguished from \"potatoes\" cooked with meat, as shown in Figures 1 (a) and (c) respectively. Second, food datasets usually suffer from imbalanced distributionboth food classes and ingredient classes often exist in long-tailed distributions. This is inevitable due to two reasons: 1) large number of food images are dominated by very few popular food classes while vast majority of food classes are unpopular; and 2) there is a selection bias in the construction of food image collection [44]. We will elaborate the detailed distribution analysis in Section 3.\n\nExisting food image datasets, such as ETH Food101 [1], Recipe1M [41], and Geo-Dish [52], mainly facilitate the research of dish classification or recipe generation. They do not have fine-grained ingredient masks or labels. UECFoodPix [13] and UECFoodPixComplete [35] are the only two public datasets for food image segmentation. However, their segmentation masks are annotated at dish level only. That is, each mask covers the region of an entire dish instead of that of food ingredients. We elaborate more dataset comparison in Section 3.3.\n\nDataset contribution: To facilitate fine-grained food image segmentation, we build a large-scale dataset called FoodSeg103, for which we have defined 103 ingredient classes and annotated 7,118 western food images using these labels together with the corresponding segmentation masks. Besides, we annotated an additional set of 2,372 images of Asian food which covers more diverse set of ingredients making these images more challenging than those in the main set (FoodSeg103). For this set, we defined 112 ingredient classes-55% overlap with the ingredient classes of the main set. In total, we annotated 154 classes of ingredients with around 60k masks (in the two datasets). We name the combined dataset as FoodSeg154. During the annotation, we carried out careful data selection, iterative refinement of labels and masks (to be further elaborated in Section 3.2), so as to guarantee high quality labels and masks in the dataset. Our annotation is thus expensive and time-consuming. In experiments, we use FoodSeg103 for in-domain training and testing, and use the additional set in FoodSeg154 for out-domain testing.\n\nModel contribution: The source images of FoodSeg103 are from another existing food dataset Recipe1M [41]-millions of images and cooking recipes, used for recipe generation. Each recipe contains not only \"how to cook\" but also \"what ingredient to use\". In our work, we leverage these recipe information as auxiliary information to train semantic segmentation models. We call this multi-modality knowledge transfer and name our training method ReLeM. Specifically, ReLeM integrates food recipe data, in the format of language embedding, with the visual representation of the food image. In this way, it forces the visual representation of an ingredient appearing in different dishes to have their appearances \"connected\" in the feature space through a common language embedding (extracted from the ingredient's label and its cooking instructions).\n\nExperiment contribution: We validate our proposed ReLeM model by plugging it into the state-of-the-art semantic segmentation models such as CCNet [17], Sem-FPN [22] and SeTR [54]. In experiments, we compare ReLeM-variants with these baseline models using both convolutional networks and transformer backbones. Our experiments show that ReLeM is generic to be applied into multiple segmentation frameworks, and it helps to achieve significant accuracy improvement when incorporated into the SOTA CNNbased model CCNet. This validates that our knowledge transfer approach works more efficient on stronger models-a characteristic preferred by the multimedia community.\n\nOur contributions are thus three-fold. i) We build a large-scale food image segmentation dataset called FoodSeg103 (and its extension FoodSeg154). It can facilitate a promising and challenging benchmark for the task of semantic segmentation in food images. ii) We propose a knowledge transfer approach ReLeM that utilizes the multi-modality information of recipe datasets. It can be incorporated into different semantic segmentation methods to boost the model performance. iii) We conduct extensive experiments that reveal the challenges of segmenting food on our FoodSeg103 dataset, and validate the efficiency of our ReLeM based on multiple baseline methods.\n\n\n"}, {"paperid": "paper2", "title": "Recipe recognition with large multimodal food dataset", "abstract": "This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.", "introduction": "\n\nFood category classification is a key technology for many food-related applications such as monitoring healthy diet, computational cooking, food recommendation system, etc. In [1], a novel smart phone application to record daily meal activities by image retrieval technique is developed. Based on this personal dietary data log system, they were able to conduct further usage preference experiments [2] and food nutrition balance estimation [3].\n\nOpen Food System 2 aims at inventing new smart cooking appliances, with the ability to monitor cooking settings automatically for optimal results and preserve the nutritional value and organoleptic qualities of cooked foods. The Technology Assisted Dietary Assessment (TADA) project of Purdue University [4] aims at developing a mobile food recorder which can translate dietary information to an accurate account of daily food and nutrient intake. Food category classification is an indispensable ingredient in all these applications.\n\nIn this paper, we focus on building automatic systems for image recipe recognition. For this purpose, we propose a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories collected from the web. Each item in this dataset is represented by one image and the HTML information including metadata, content etc. of the seed page from which the image originated. We detail our initiative to build our dataset in sections 2 and 3 explaining the specificities and the originality of our dataset. We perform experiments at a large scale to evaluate visual and textual features along with their fusion in section 4. We propose in section 5, further statistics to highlight dataset characteristics and comparison with another recent large scale dataset (ETHZ Food-101 [5]). Finally, in section 6, we demonstrate the interest of these recognition technologies coupled with web-based dataset in a mobile search application, which can receive food image as a query and return the most relevant classes and corresponding recipes.\n\n\n"}, {"paperid": "paper3", "title": "Automatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation", "abstract": ". In this paper, we propose a novel e\ufb00ective framework to expand an existing image dataset automatically leveraging existing categories and crowdsourcing. Especially, in this paper, we focus on expansion on food image data set. The number of food categories is uncountable, since foods are di\ufb00erent from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for di\ufb00erent food cultures have been built independently so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic \u201cfoodness\u201d classi\ufb01er and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the e\ufb00ectiveness of the proposed method over the baselines.", "introduction": "\n\nRecently, needs for food image recognition become larger, since food habit recording services for smartphones are spreading widely for everyday health care. For food habit recording, conventional ways such as inputing food names by texts or selecting food items from menus are very tedious, which sometimes prevent users from using such systems regularly. Then, several works on food recognition have been proposed so far [1][2][3][4][5] to make it easy to use food habit recording. In these works, the number of food categories is 100 at most, which is not enough for practical use. In fact, all of the foods we eat in our everyday life cannot be covered with only one hundred food categories, and the number of foods which can be recognized should be increased much more.\n\nOn the other hand, in these years, large-scale image classification is paid attention, and many methods for that have been proposed recently [6][7][8][9]. Due to these works, the number of categories to be recognized have been increased up to 1000. For example, in ImageNet Large Scale Visual Recognition Challenge (ILSVRC), the number of categories to be classified is 1000. The data set for ImageNet Challenge is a subset of ImageNet [10], which is known as the largest visual database where the number of categories are more than 20,000. Largescale image data sets such as ImageNet cannot be created by researchers by themselves. Most of them use crowd-sourcing Web services such as Amazon Mechanical Turk to build them semi-automatically.\n\nIn this paper, we propose a novel framework to expand an existing image dataset automatically leveraging existing categories. Especially, in this paper, we focus on expansion on food image data set.\n\nWhile ImageNet covers comprehensive concepts, our target is restricted to foods. In ImageNet, annotation of each concept is gathered independently. On the other hand, since foods look more similar to each other, visual knowledge on foods of a certain country is expected to help collect annotations of food photos of the other countries. Then, in this paper, we propose a novel effective framework which utilizes knowledge on food of other countries by domain adaptation.\n\nBasically, we gather food image candidates on novel food categories from the Web, and select good photos and add bounding boxes by using crowd-sourcing. In general, raw Web images include many noise images which are irrelevant to a given keyword. Especially, in this work, non-food images can be regarded as noise images. To exclude them from the gather images, we filter and re-rank Web images related to a given food category by using visual knowledge extracted from the existing food dataset.\n\nFirstly, we built a generic \"foodness\" classifier from a Japanese food data set, UEC-Food100 [4]. We cluster all the food categories in the exist food image set into several food groups the member of which are similar to each other in terms of image feature vectors, and we train SVMs regarding each food group independently. Then, we evaluate unknown images using the trained SVMs on the food groups, and regards the maximum value of the output values of all the SVM as the \"foodness\" value of the given image. We can decide if a given image of a unknown category is a food photo or not based on the \"foodness\" value. In addition, because we select the maximum value from all the output valued of food groups, we estimate the most related food group to a given photo.\n\nAfter \"foodness\" filtering, we obtain a food photo set. However, it might include food photos irrelevant to the given food keyword. Secondly, we select and re-rank more relevant images from the images judged as food photos by using transfer learning with visually similar categories in the source food photo data set. As a method of transfer learning, we use Adaptive SVM (A-SVM) [11] which can learn a discriminative hyper-plane in the target domain taking into account source-domain training data. In this work, the labeled data of the source categories which are visually similar to the target food photos are used as sourcedomain training data. As an initial target-domain training data, we use upperranked photos by a unsupervised image ranking method, VisualRank (VR) [12]. Then, we select food candidate images to be submitted for the crowd-sourcing by applying a trained A-SVM. By the experiments, the precision of the food candidate photos by A-SVM has been proved to outperformed the results by only VisualRank and by normal standard SVM.\n\nThe contributions of this paper are as follows:\n\n(1) Propose a novel framework to extend an existing image dataset with a generic \"foodness\" classifier and domain transfer learning. (2) Three-step crowd-sourcing: selecting representative sample images, excluding noise photos, and drawing bounding boxes. (3) Evaluate and compare accuracy of built food datasets and costs regarding the proposed method and two baselines. (4) Apply the proposed framework in a large scale, and build a new 256-category food dataset based on the existing 100-category food dataset automatically.\n\n\n"}, {"paperid": "paper4", "title": "Deep-based Ingredient Recognition for Cooking Recipe Retrieval", "abstract": "Retrieving recipes corresponding to given dish pictures facilitates the estimation of nutrition facts, which is crucial to various health relevant applications. The current approaches mostly focus on recognition of food category based on global dish appearance without explicit analysis of ingredient composition. Such approaches are incapable for retrieval of recipes with unknown food categories, a problem referred to as zero-shot retrieval. On the other hand, content-based retrieval without knowledge of food categories is also difficult to attain satisfactory performance due to large visual variations in food appearance and ingredient composition. As the number of ingredients is far less than food categories, understanding ingredients underlying dishes in principle is more scalable than recognizing every food category and thus is suitable for zero-shot retrieval. Nevertheless, ingredient recognition is a task far harder than food categorization, and this seriously challenges the feasibility of relying on them for retrieval. This paper proposes deep architectures for simultaneous learning of ingredient recognition and food categorization, by exploiting the mutual but also fuzzy relationship between them. The learnt deep features and semantic labels of ingredients are then innovatively applied for zero-shot retrieval of recipes. By experimenting on a large Chinese food dataset with images of highly complex dish appearance, this paper demonstrates the feasibility of ingredient recognition and sheds light on this zero-shot problem peculiar to cooking recipe retrieval.", "introduction": "\n\nWhile there is a large number of cooking recipes posted on the Internet, finding a right recipe given a picture of dish remains a challenge yet to be fully explored. The major problem underlying this challenge is the recognition of food categories as well as their ingredients. Indeed, the problem is commonly shared among health-related applications. For example, food-log management [1], which records daily food intake for dietary habit monitoring, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Figure 1: Variations in visual appearance and composition of ingredients show the challenges of predicting ingredients even for dishes within the same food category. The first row shows three examples of dishes for the category \"fried green peppers\", followed by \"yuba salad\" ad \"steam egg custard\" in second and third rows respectively. often requires manual input of food intake. In addition to timeconsuming, the process is error-prone. As investigated in [11], selfreporting data obtained from unfriendly acquired process tends to underestimate the actual food intake. These concerns motivate the use of mobile devices as a convenient means in capturing pictures of food intake for automatic recognition [24] [14] [25] [3] [16].\n\nThis paper studies the recognition of ingredients for recipe retrieval in the domain of Chinese dishes. Different from food categorization, which is to identify the name of a dish (e.g., fried green pepper shown in Figure 1), ingredient recognition is to uncover the ingredients inside a dish (e.g., green pepper, black bean, chopped garlic). In the literature, associating food categories to their respective recipes is regarded as a general pipeline that facilitates the estimation of calories and nutrition facts [35] [14]. The pipeline is effective for recognizing restaurant dishes and the food categories with standardized cooking method (e.g., fast food) that often have similar visual appearance with the same ingredients. However, most dishes in Chinese food have no standardized cooking method, food presentation and ingredient composition. Direct mapping between dishes and recipes, by using the names of food categories, is not likely to attain satisfactory retrieval rate, not mentioning the imperfect performance in food recognition. The difficulty of this task is probably alleviated, nevertheless, with the presence of GPS and restaurant menus as utilized by Im2Calories [24] and Menu-Match [2]. However, restaurant information are difficult to acquire as stated in [24] and such context-aware recognition is only limited to restaurant food. Therefore, this paper argues the need of ingredient recognition beyond food categorization for general recipe retrieval.\n\nIn the domain of Chinese food, two major obstacles in recognition are diverse appearances of dishes and wild composition of ingredients. Figure 1 shows some examples of Chinese dishes. Automatic recognition is challenged by the wildly different ways of mixing and placing ingredients even for the same food category. For the food category \"steamed egg custard\" (last row of Figure  1), there is even no overlap in ingredients except egg. Retrieving recipes without explicitly naming the underneath ingredients is expected to include false positives. Basically, ingredients can be treated as attributes of food categories. As the number of food categories is generally far larger than the number of ingredients, recognizing attributes is more feasible than food categories in terms of scale. Furthermore, ingredient recognition also gives light to the retrieval of recipes for unknown food categories during model training, a problem generally referred to as zero-shot recognition or retrieval [29].\n\nGenerally speaking, ingredient recognition is more difficult than food categorization. As observed in Figure 1, the size, shape and color of ingredients can exhibit large visual differences due to diverse ways of cutting and cooking, in addition to changes in viewpoints and lighting conditions. Recognizing ingredients alone without food category in mind is likely to result in unsatisfactory performance. This paper considers simultaneous recognition of food and ingredients, aiming to exploit the mutual relationship between them for enhancing the robustness of recognition. The key ingredients of a category remain similar despite composing with different auxiliary ingredients. Knowing food category basically eases the recognition of ingredients. On the other hand, the prediction of ingredients also helps food categorization, for example, the ingredient \"fungus\" has a higher chance than \"pork\" to appear in the food \"yuba salad\". Hence, learning food categories with the composition of ingredients in mind, and vice versa, in principle shall lead to better performance. Figure 2 gives an overview of the proposed framework, which is composed of two modules: ingredient recognition and zero-shot recipe retrieval. The first module formulates the recognition of ingredients as a problem of multi-task learning using deep convolution neural network (DCNN). Given a picture of dish, the module outputs the name of dish along with a histogram of ingredients. The developed DCNN can recognize 172 Chinese food categories and 353 ingredients. To the best of our knowledge, there is no result published yet for ingredient recognition on such a large scale. The second module performs zero-shot retrieval, by matching the predicted ingredients against a large corpus containing more than 60,000 recipes. The corpus includes some food categories as well as ingredients unknown to the multi-task DCNN. To boost retrieval performance, a graph encoding the contextual relationship among ingredients is learnt from the recipe corpus. Using this graph, conditional random field (CRF) is employed to probabilistically tune the probability distribution of ingredients to reduce potential recognition error due to unseen food category.\n\nTo summary, this paper contributes by developing multi-task learning technique for ingredient recognition and demonstrates its application for zero-shot recipe retrieval. Our work differs from the existing works, which mostly focus on recognition of food categories and operate in domains such as western and Japanese food [4] [22]. To our knowledge, zero-shot recipe retrieval, which requires knowledge of ingredients, has not yet been considered in the literature. Along with this paper, we will release the collected Chinese food dataset, VIREO Food-172, which contains 172 food and 353 ingredient labels. The dataset is larger than the publicly available datasets such as Food-101 [4], UEC Food-100 [22] and PFID [6], each with around 100 western or Japanese food categories but without ingredient labels.\n\n\n"}, {"paperid": "paper5", "title": "Mining Discriminative Food Regions for Accurate Food Recognition", "abstract": "Automatic food recognition is the very first step towards passive dietary monitoring. In this paper, we address the problem of food recognition by mining discriminative food regions. Taking inspiration from Adversarial Erasing, a strategy that progressively discovers discriminative object regions for weakly supervised semantic segmentation, we propose a novel network architecture in which a primary network maintains the base accuracy of classifying an input image, an auxiliary network adversarially mines discriminative food regions, and a region network classifies the resulting mined regions. The global (the original input image) and the local (the mined regions) representations are then integrated for the final prediction. The proposed architecture denoted as PAR-Net is end-to-end trainable, and highlights discriminative regions in an online fashion. In addition, we introduce a new fine-grained food dataset named as Sushi-50, which consists of 50 different sushi categories. Extensive experiments have been conducted to evaluate the proposed approach. On three food datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs consistently and achieves state-of-the-art results (top-1 testing accuracy of $90.4\\%$, $90.2\\%$, $92.0\\%$, respectively) compared with other existing approaches. Dataset and code are available at https://github.com/Jianing-Qiu/PARNet", "introduction": "\n\nDiet-induced diseases are becoming increasingly prevalent among populations. One underlying factor is people's poor management of their daily dietary intake. The other factor is that there is currently no accurate measurement of dietary intake. Dietary measurement in nutritional epidemiology is heavily based on self-reported data that are highly inaccurate and subjective [15], which hinders nutritionists from designing effective dietary guidance. To mitigate the problem of existing dietary measurement techniques that require extensive user input and produce unsatisfactory results, the concept of passive dietary monitoring is proposed [12], which relies on sensors such as cameras to pervasively record eating episodes and automatically perform food recognition, volume estimation, and deduce dietary intake. In realising this concept of passive monitoring, food recognition is the first and a crucial step as any misrecognition will lead to inaccurate measurements afterwards. With recent advances in computer vision, recognising pictured dishes have achieved promising results but still remains as a challenging field of research given that there are enormous varieties of dishes and even the same type of food can have very different appearances. In this work, we aim to achieve accurate food recognition by mining discriminative regions of a food image. This is motivated by the previous work done by Bossard et al. [3] that utilises random forests to mine discriminative components from food images. Unlike [3], we develop a convolutional neural network (CNN) model and utilise a weakly supervised method to discover discriminative food regions. This weakly supervised method used in both network training and inference is based on Adversarial Erasing (AE) [19], a strategy developed for weakly supervised semantic segmentation. One prominent feature of AE is that it enables the discriminative region of an object of interest to be discovered progressively, which in our case enables better recognition of food items. Our implementation of AE however differs from [19] in that we integrate it into a new network architecture for object recognition (food recognition in particular), and all sub-networks involved are trained jointly, which is more convenient, compared to its original usage in which networks need to be trained independently. Although region mining is performed, the proposed approach still predicts the food class of an input image efficiently in an end-to-end manner, which will be detailed in Sections 3.1 and 3.2.\n\nThe contributions of our work are twofold: (i) we propose a new network architecture that is able to mine discriminative food regions in a weakly supervised fashion and be trained endto-end. The mining strategy is adopted and optimised for food recognition. Comprehensive experiments are performed to validate the proposed approach; (ii) we introduce a new finegrained food dataset which consists of 50 sub-categories of one common food class, i.e., sushi, in contrast to most existing datasets that only contain coarse food classes.\n\n\n"}, {"paperid": "paper6", "title": "ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition", "abstract": "In this paper, we introduce a new and challenging large-scale food image dataset called\"ChineseFoodNet\", which aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In our dataset, images of each food category of our dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. We present our efforts to build this large-scale image dataset, including food category selection, data collection, and data clean and label, in particular how to use machine learning methods to reduce manual labeling work that is an expensive process. We share a detailed benchmark of several state-of-the-art deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose a novel two-step data fusion approach referred as\"TastyNet\", which combines prediction results from different CNNs with voting method. Our proposed approach achieves top-1 accuracies of 81.43% on the validation set and 81.55% on the test set, respectively. The latest dataset is public available for research and can be achieved at https://sites.google.com/view/chinesefoodnet.", "introduction": "\n\nF OOD plays an essential role in everyone's lives, and the behaviour of diet and eating impacts everyone's health [1]. Underestimating food intake directly relates to diverse psychological implications [2]. In recent years, photographing foods and sharing them on social networks have become a part of daily life. Consequently, several applications have been developed to record daily meal activities in personal food log system [3] [4] [5], which are employed to computeraided dietary assessment [6], further usage preference experiments [7] [8], calorie measurement [9] and nutrition balance estimation [10] [11]. As one of user-friendly ways to input of the food log, automatic recognition of dish pictures gives rise of a research field of interest.\n\nDeep convolutional neural networks (CNNs) have achieved state-of-the-art in a variety of computer vision tasks [12] [13]. The visual dish recognition task is the same situation [14]. The quality of training datasets always plays an important role for \u2020 These authors contributed equally to this work.\n\n* means corresponding author. Xin Chen, Hua Zhou, Yu Zhu   training a deep neural network, where the high performance of the deep model is still data-driven to some extent [15] [16].\n\nHowever, to the best of our knowledge, there still exist no effective Chinese food recognition system matured enough to be used in real-world. The major reason is absence of largescale and high quality image datasets. In [17], the Chinese food dataset includes 50 categories, each of which has only 100 images. Obviously, the size of this dataset is not sufficient to satisfy deep learning training requirements.\n\nThe visual dish recognition problem has widely been considered as one of challenging computer vision and pattern recognition tasks [14] [18]. Compared to other types of food such as Italian food and Japanese food, it is more difficult to recognize the images of Chinese dish as the following reasons:\n\n1) The images of same category appear differently. ornament, etc. In order to give impetus to the progress of visual food classification and related computer vision tasks, we build a large-scale image dataset of Chinese dish, named by Chine-seFoodNet. This dataset contains 185,628 images of 208 food categories covering most of popular Chinese food, and these images include web images and photos taken in real world under unconstrained conditions. To the best of our knowledge, ChineseFoodNet is the largest and most comprehensive dataset for visual Chinese food recognition. Some of images of ChineseFoodNet are shown in Figure. 2.\n\nWe benchmark nine CNNs models of four state-of-the-art deep CNNs, SqueezeNet [19], VGG [20], ResNet [21], and DenseNet [22], on our dataset. Experimental results reveal that ChineseFoodNet is capable of learning complex models.\n\nIn this paper, we also propose a novel two-step data fusion approach with voting. Although simple, voting is an effective way to fuse results [23] [24]. Guided by our benchmarks, we try some combination of different CNNs models Based on results on ChineseFoodNet, we take ResNet152, DenseNet121, DeneseNet169, DenseNet201 and VGG19-batch normalization (BN) [25] as our predictive models. 1 Then we fusing these results with voting as a final result. This method is designated as\" TastyNet\". Our proposed method has achieved top-1 accuracy 81.43% in validation set and 81.55% in test set, respectively. Compared to best results of the approaches with a single network structure, the improvements of 2.38% in validation set and 2.33% in these sets have been achieved, respectively. This paper takes three major contributions as following: 1) We present a large-scale image dataset, ChineseFoodNet, for Chinese food recognition tasks. ChineseFoodNet is made up with 185,628 images of 208 categories, and most of the food image are from users' daily life. It is public available for research in related topics. 2 2) We provide a benchmark on our dataset. Totally nine different models of four state-of-the-art CNNs architectures are evaluated. We presents the details of the methodology used in the evaluation and the pre-trained models will be public available for further research. 3) We propose a novel two-step data fusion approach for visual food recognition, which combines predictive results of different CNNs with voting. Experimental results on ChineseFoodNet have shown that approach improves the performance compared to one deep CNNs model. It has shown that data fusion should be an alternative way to improve accuracy instead of only increasing numbers of layers in CNNs. The paper is organized as follows. Section II briefly reviews some public food datasets and the state-of-the-art visual food recognition methods. Section III describes the procedure of building and tagging the ChineseFoodNet dataset. In section IV, several state-of-the-art CNNs methods are benchmarked on ChineseFoodNet. Section V details our proposed data fusion approach and present our results on Chinese-FoofNet. This paper closes with a conclusion of our work and some future directions in section VI.\n\n\n"}, {"paperid": "paper7", "title": "You Are What You Eat: Exploring Rich Recipe Information for Cross-Region Food Analysis", "abstract": "Cuisine is a style of cooking and usually associated with a specific geographic region. Recipes from different cuisines shared on the web are an indicator of culinary cultures in different countries. Therefore, analysis of these recipes can lead to deep understanding of food from the cultural perspective. In this paper, we perform the first cross-region recipe analysis by jointly using the recipe ingredients, food images, and attributes such as the cuisine and course (e.g., main dish and dessert). For that solution, we propose a culinary culture analysis framework to discover the topics of ingredient bases and visualize them to enable various applications. We first propose a probabilistic topic model to discover cuisine-course specific topics. The manifold ranking method is then utilized to incorporate deep visual features to retrieve food images for topic visualization. At last, we applied the topic modeling and visualization method for three applications: 1) multimodal cuisine summarization with both recipe ingredients and images, 2) cuisine-course pattern analysis including topic-specific cuisine distribution and cuisine-specific course distribution of topics, and 3) cuisine recommendation for both cuisine-oriented and ingredient-oriented queries. Through these three applications, we can analyze the culinary cultures at both macro and micro levels. We conduct the experiment on a recipe database Yummly-66K with 66,615 recipes from 10 cuisines in Yummly. Qualitative and quantitative evaluation results have validated the effectiveness of topic modeling and visualization, and demonstrated the advantage of the framework in utilizing rich recipe information to analyze and interpret the culinary cultures from different regions.", "introduction": "\n\nU NDERSTANDING the cultural diversity has become imperative in almost every aspect of life. Cuisine has always been a significant aspect of cultures. The recipes from different cuisines are strong signals of the culinary habits of individuals from various parts of the world. For example, East Asian cuisines are dominated by some ingredients such as \"soy sauce\" and \"sesame oil\" from the recipes [2]. Some other ingredients uniquely represent a certain cuisine (e.g., mozzarella to the Italian cuisine) [51]. Therefore, the analysis of recipes can facilitate deep understanding of food from not only the health and marketing perspectives, but also the cultural one. Such analysis can further enable various applications, such as food preference learning [27], [62], cuisine classification [56] and health diet analysis [39]. Since the culinary habits have such importance for the culture, we address the topic of investigating and analyzing the culinary cultures of different countries through the recipes.\n\nExisting work mainly focuses on the analysis of recipes based on the textual descriptions (e.g., ingredients) [51] and checkin information from social websites [53]. For example, Sajadmanesh et al. [51] used the ingredients and attribute information, such as the taste and cuisine information to understand cuisines and culinary habits around the world. Silva et al. [53] analyzed the check-ins from Foursquare to identify cultural boundaries and similarities across populations at different scales. However, little work has investigated how to jointly utilize rich modality and attribute information to enable the analysis and comparison of culinary cultures comprehensively.\n\nWhile existing studies rely on text-oriented recipe analysis, we believe rich modality and attribute information are critical to comprehensively analyze the diversity of the culinary cultures. (1) Relying exclusively on text-based descriptions can impose high cognitive load in analyzing the culinary habits. Some works such as [14], [37], [62] have found the importance of visual information in eating habits-related applications. For example, Zepeda et al. [37] pointed out that photographic food diaries were more effective than written ones at prompting patients to truly understand their eating habits. Yang et al. [62] found that the analysis of image features can provide a valuable signal for food preference learning. (2) Different attributes (e.g., the course and cuisine information) reflect respective aspects of the recipes and jointly contribute to comprehensive recipe analysis. Through attribute-based analysis, we can reveal the culinary cultures from different perspectives. However, it is non-trivial to make the correlation between the content and various attributes. Because this requires us to design a more flexible model, which is able to incorporate arbitrary number of observed attribute information, yet inference remains relatively simple.\n\nTaking all the above-mentioned factors into consideration, we propose a culinary culture analysis framework (Fig. 1), which is capable of discovering topics conditioned on different attributes and visualizing them to enable various applications. In particular, we take Yummly, 1 one of the most popular recipesharing websites in our study. As shown in the left part of Fig. 1, each recipe includes the ingredients, food images and various attributes (e.g., the cuisine and course). Given the input of rich recipe information, we firstly propose a Bayesian Cuisine-Course Topic Model (BC 2 TM), which incorporates different attributes to discover the cuisine-course specific topics. Since topic models have been proved successfully in discovering meaningful and interpretable topics or patterns in the text domain, such discovered topics are very suitable for our cross-region food analysis and comparison. For further analysis and better visualization, based on the learned model parameters, namely topic-ingredient distribution, we then utilize the manifold ranking method to integrate both deep visual features and topic-ingredients to retrieve relevant food images for topic visualization. Finally, we exploit the topic modeling and visualization for three applications: (1) Multi-modal cuisine summarization, which summarizes cuisines with both recipe ingredients and food images. Through cuisine summarization, we can 1 http://www.yummly.com/ analyze and interpret the culinary cultures from the macro level.\n\n(2) Cuisine-course pattern analysis including topic-specific cuisine distribution and cuisine-specific course distribution of topics. Such comparative analysis can provide us with more details from the micro level. (3) Cuisine recommendation. It supports flexible queries including both cuisine-oriented and ingredientoriented queries. We conduct the experiment on one dataset, including about 66K recipes with the ingredients, food images, course and cuisine attributes from 10 cuisines from Yummly. Experiment results demonstrate the advantage of the framework in utilizing rich recipe information to discover and compare different eating habits from different regions.\n\nThe contributions of the proposed approach can be summarized as follows:\n\n1) To our knowledge, this is the first time to jointly utilize rich recipe information including multi-modal information and various attributes into a unified framework to enable comprehensive analysis and comparison of culinary cultures. 2) We propose a culinary cultures analysis framework, which is capable of discovering topics conditioned on different attributes and visualizing them for recipe-oriented applications. 3) We present a wide variety of applications, including 1) multi-modal cuisine summarization, 2) cuisine-course pattern analysis, and 3) cuisine recommendation. 4) We conduct the comprehensive evaluation on a real-world recipe dataset Yummly-66K and the experimental results have validated the effectiveness of our proposed method and framework. The rest of the paper is organized as follows. Section II reviews the related work. Section III presents the core components of the proposed culinary culture analysis, including the proposed Bayesian Cuisine-Course Topic Model (BC 2 TM) and manifold ranking based topic visualization. Section IV introduces three derived applications, including multi-modal cuisine summarization, cuisine-course pattern analysis and cuisine recommendation. Experimental results are reported in Section V. Finally, we conclude the paper and give the future work in Section VI.\n\n\n"}, {"paperid": "paper8", "title": "ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network", "abstract": "Food recognition has received more and more attention in the multimedia community for its various real-world applications, such as diet management and self-service restaurants. A large-scale ontology of food images is urgently needed for developing advanced large-scale food recognition algorithms, as well as for providing the benchmark dataset for such algorithms. To encourage further progress in food recognition, we introduce the dataset ISIA Food- 500 with 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume. Furthermore, we propose a stacked global-local attention network, which consists of two sub-networks for food recognition. One subnetwork first utilizes hybrid spatial-channel attention to extract more discriminative features, and then aggregates these multi-scale discriminative features from multiple layers into global-level representation (e.g., texture and shape information about food). The other one generates attentional regions (e.g., ingredient relevant regions) from different regions via cascaded spatial transformers, and further aggregates these multi-scale regional features from different layers into local-level representation. These two types of features are finally fused as comprehensive representation for food recognition. Extensive experiments on ISIA Food-500 and other two popular benchmark datasets demonstrate the effectiveness of our proposed method, and thus can be considered as one strong baseline. The dataset, code and models can be found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.", "introduction": "\n\nFood computing [38] is emerging as a new field to ameliorate the issues from many food-relevant fields, such as nutrition, agriculture and medicine. As one significant task in food computing, food recognition has received more attention in multimedia and beyond [15,25,36,41] for its various applications, such as visual food diary [36], health-aware recommendation [42] and self-service restaurants [2].\n\nDespite its great potential applications, recognizing food from images is still a challenging task, and the challenge derives from three-fold:\n\n\u2022 There is a lack of large-scale food dataset for food recognition. Existing works mainly focus on utilizing smaller datasets for food recognition, such as ETH Food-101 [6] and Vireo Food-172 [7]. For example, Bossard et al. [6] released one food dataset ETH Food-101 from western cuisines with 101 food categories and 101,000 images. Chen et al. [7] introduced the Vireo Food-172 dataset from 172 Chinese food categories. These data-sets is lack of diversity and coverage in food categories and do not include a wide range of food images. Therefore, they are probably not sufficient to construct more complicated deep learning models for food recognition.\n\n\u2022 There are larger intra-class variations in the global appearance, shape and other configurations for food images. As shown in Fig. 1, there are different shapes for the butter pecan and different textures appear in the mie goreng dish. Although numerous methods have been developed for addressing the problem of food recognition, most of these methods mainly focus on extracting features with certain type or some types while ignoring other aspects. For example, works on [4] mainly extracted color features while Niki et al. [32] designed a network to capture certain vertical structure for food recognition.\n\n\u2022 There are subtle discriminative details from food images, which are harder to capture in many cases. Food recognition belongs to fine-grained recognition. Therefore, discriminative details are too subtle to be well-represented by existing CNNs in many cases. As shown in Fig. 1, global features are not discriminative enough to distinguish between corn stew and leek soup. Although local regional features are probably more useful, we should carefully design one network to capture and represent such subtle difference. In order to improve the recognition performance, additional context information, such as location and ingredients [4,41,51,59] is utilized. However, when these information is unavailable, these methods probably do not work. In this work, we address data limitations by introducing a new large-scale dataset ISIA Food-500 with 399,726 images and 500 categories. In contrast with existing popular benchmark datasets, it is a more comprehensive food dataset with larger category coverage, larger data volume and higher diversity. To solve another two challenges, we propose a Stacked Global-Local Attention Network (SGLANet) to jointly learn complementary global and local visual features for food recognition. This is achieved by two sub-networks, namely Global Feature Learning Subnetwork (GloFLS) and Local-Feature Learning Subnetwork (LocFLS). GloFLS first utilizes hybrid spatial-channel attention to obtain more discriminative features for each layer, and then aggregates these features from different layers with both coarse and fine-grained levels, such as shape and texture cues about food into global-level features. LocFLS adopts cascaded Spatial Transformers (STs) to localize different attentional regions (e.g., ingredient-relevant regions), and aggregates fused regional features from different layers into local-level representation. In addition, SGLANet is trained with different types of losses in an end-to-end fashion to maximize their complementary effect in terms of discriminative power.\n\nThe contributions of our paper can be summarized as follows: \u2022 We introduce a new large-scale and highly diverse food image dataset with 500 categories and about 400,000 images, which will be made publicly available to further the development of scalable food recognition.\n\n\u2022 We propose a stacked global-local attention network architecture to jointly learn food-oriented global and local features Dataset #Images #Categories #Coverage PFID [9] 4,545 101 Japanese UEC Food100 [34] 14,361 100 Japanese UEC Food256 [27] 25,088 256 Japanese ETHZ Food-101 [6] 101,000 101 Western UPMC Food-101 [48] 90,840 101 Western UNIMIB2015 [12] 2,000 15 Misc. UNIMIB2016 [13] 1,027 73 Misc. ChineseFoodNet [10] 192,000 208 Chinese Vireo Food-172 [7] 110,241 172 Chinese KenyanFood13 [23] 8,174 13 Kenyan Sushi-50 [44] 3,963 50 Japanese FoodX-251 [26] 158 via combining hybrid spatial-channel attention and multi-scale strategy for food recognition.\n\n\u2022 We conduct extensive evaluation on our proposed dataset and other two popular food benchmark datasets to verify the effectiveness of our approach. As one strong baseline, code and models will also be released upon publication to support future research.\n\n\n"}, {"paperid": "paper9", "title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models", "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.", "introduction": "\n\nDespite the remarkable success of vision-language models (VLMs) [18,33,34,38,45,47,52] in substantial uni-modal and multi-modal downstream tasks, they are still poorly understood as yet. The prevalent approach for evaluating VLMs is comparing their performance on downstream tasks after fine-tuning. However, evaluation solely based on the fine-tuning results renders poor interpretability [59], which hinders the further development of VLMs. Consequently, researchers have proposed a range of probing methods and benchmarks [9,13,29,30,39,48] in recent years to assess the capabilities of VLMs from various perspectives, providing a more comprehensive understanding of these models. However, these methodologies are still limited in the general domain. They typically construct evaluation benchmarks by employing images from widely used general-domain datasets and subsequently assigning hand-crafted textual annotations to these images. If VLMs perform well in a specific domain, we can directly employ the models in that domain without any modifications. However, the above situation is Figure 1: An example from our Food-500 Cap. The image is equipped with the label, geographic origin, and a detailed description. This description is annotated with a class label (red) and hand-curated various fine-grained visible content of the image such as ingredients (blue), food colors (green), and the food container (orange).\n\nunclear due to only few works studying the generalization of using VLMs directly in specific domains without fine-tuning.\n\nMotivated by this, we focus on evaluating the generalization capacity of VLMs in a specific domain, namely, the food domain. Since food computing [25] has been gaining widespread attention as it has the potential to support numerous food-related applications, such as healthy diets and food choices. To comprehensively evaluate the VLMs' performance on food-related tasks, we introduce a new benchmark named Food-500 Cap, which comprises 24,700 food images with 494 categories, each accompanied by a detailed caption. The Food-500 Cap dataset is created by selecting images from ISIA Food-500 [26] that covers a wide range of food categories. We select 50 images from each category and engage an annotation company to annotate fine-grained descriptions for all 24,700 images. Each description includes the original food category label and fine-grained attributes of the food, such as the color, shape, and ingredients. Such an in-house labeling process guarantees the high quality of our dataset. Besides, as food is always associated with a specific geographic region, we also provide a taxonomy that classifies food categories based on their original place, enabling a more comprehensive investigation of VLMs' performance across culinary cultures. We provide a sample of Food-500 Cap in Figure 1, which contains a Japanese food image labeled agedashi tofu from and a description with some related attributes. In contrast to the prevalent food datasets [3,23,54], Food-500 Cap are equipped with high-quality image captions containing richer visual information and geographic origin tags, which is more suitable for exploring the performance of VLM in the food domain.\n\nTo comprehensively evaluate VLMs' capacity in the food domain, we seriously pick up nine representative models from three popular architectures, including vision-language representation models (e.g. CLIP [33]), image-to-text generative models (e.g. OFA [52]), and text-to-image generative models (e.g. Stable Diffusion [38]). We probe these VLMs with various food-related tasks in a zero-shot setting. For vision-language representation models, we employ food classification and image-text retrieval to assess VLMs' multi-modal alignment capabilities. As for image-to-text generative models and text-to-image generative models, we utilize image captioning and image synthesis respectively to test their multi-modal generation capabilities. Both qualitative and quantitative analyses are performed on the experimental results, revealing that these models exhibit poor performance in the food domain, in contrast to their performance in the general domain. Moreover, we find that all the models display a significant bias in culinary culture, with their performance in Asian cuisine falling markedly behind that in European, North American, and Latin American cuisine. In summary, this paper makes the following contributions:\n\n\u2022 We equip a subset of the ISIA Food-500 dataset with (1) \n\n\n"}]