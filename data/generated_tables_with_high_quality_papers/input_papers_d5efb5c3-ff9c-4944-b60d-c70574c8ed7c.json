[{"paperid": "paper0", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "abstract": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.", "introduction": "\n\nText classification is a fundamental task in natural language processing (NLP). Machine learning and deep learning have achieved high accuracy on tasks ranging from sentiment analysis (Tang et al., 2015) to topic classification (Tong and Koller, 2002), but high performance often depends on the size and quality of training data, which is often tedious to collect. Automatic data augmentation is commonly used in computer vision (Simard et al., 1998;Szegedy et al., 2014;Krizhevsky et al., 2017) and speech (Cui et al., 2015;Ko et al., 2015) and can help train more robust models, particularly when using smaller datasets. However, because it is challenging to come up with generalized rules for language transformation, universal data augmentation techniques in NLP have not been thoroughly explored.\n\nPrevious work has proposed some techniques for data augmentation in NLP. One popular study generated new data by translating sentences into French and back into English (Yu et al., 2018). Other work has used data noising as smoothing   (Xie et al., 2017) and predictive language models for synonym replacement (Kobayashi, 2018). Although these techniques are valid, they are not often used in practice because they have a high cost of implementation relative to performance gain.\n\nIn this paper, we present a simple set of universal data augmentation techniques for NLP called EDA (easy data augmentation). To the best of our knowledge, we are the first to comprehensively explore text editing techniques for data augmentation. We systematically evaluate EDA on five benchmark classification tasks, showing that EDA provides substantial improvements on all five tasks and is particularly helpful for smaller datasets. Code is publicly available at http://github. com/jasonwei20/eda_nlp.\n\n\n"}, {"paperid": "paper1", "title": "Improving Neural Machine Translation Models with Monolingual Data", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.", "introduction": "\n\nNeural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training.Target-side monolingual data plays an important role in boosting fluency for phrase-based statisti-\n\nThe research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp.z o.o.-Samsung R&D Institute Poland.cal machine translation, and we investigate the use of monolingual data for NMT.\n\nLanguage models trained on monolingual data have played a central role in statistical machine translation since the first IBM models (Brown et al., 1990).There are two major reasons for their importance.Firstly, word-based and phrase-based translation models make strong independence assumptions, with the probability of translation units estimated independently from context, and language models, by making different independence assumptions, can model how well these translation units fit together.Secondly, the amount of available monolingual data in the target language typically far exceeds the amount of parallel data, and models typically improve when trained on more data, or data more similar to the translation task.\n\nIn (attentional) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014;Bahdanau et al., 2015), the decoder is essentially an RNN language model that is also conditioned on source context, so the first rationale, adding a language model to compensate for the independence assumptions of the translation model, does not apply.However, the data argument is still valid in NMT, and we expect monolingual data to be especially helpful if parallel data is sparse, or a poor fit for the translation task, for instance because of a domain mismatch.\n\nIn contrast to previous work, which integrates a separately trained RNN language model into the NMT model (G\u00fcl\u00e7ehre et al., 2015), we explore strategies to include monolingual training data in the training process without changing the neural network architecture.This makes our approach applicable to different NMT architectures.\n\nThe main contributions of this paper are as follows:\n\n\u2022 we show that we can improve the machine translation quality of NMT systems by mixing monolingual target sentences into the training set.\n\n\u2022 we investigate two different methods to fill the source side of monolingual training instances: using a dummy source sentence, and using a source sentence obtained via backtranslation, which we call synthetic.We find that the latter is more effective.\n\n\u2022 we successfully adapt NMT models to a new domain by fine-tuning with either monolingual or parallel in-domain data.\n\n\n"}, {"paperid": "paper2", "title": "Towards More Realistic Generation of Information-Seeking Conversations", "abstract": "In this paper, we introduce a novel framework SimSeek ( sim ulating information- seek ing conversation from unlabeled documents) and compare two variants of it to provide a deeper perspective into the information-seeking behavior. We \ufb01rst introduce a strong simulator for information- sym metric conversation, SimSeek-sym, where questioner and answerer share all knowledge when conversing with one another. Although it simulates reasonable conversations, we take a further step toward more realistic information-seeking conversation. Hence, we propose SimSeek-asym that assumes information asym metry between two agents, which encourages the questioner to seek new information from an inaccessible document. In our experiments, we demonstrate that SimSeek-asym successfully generates information-seeking conversations for two downstream tasks, CQA and conversational search. In particular, SimSeek-asym improves baseline models by 1.1-1.9 F1 score in QuAC (Choi et al., 2018), and by 1.1 of MRR in OR-QuAC (Qu et al., 2020). Moreover, we thoroughly analyze our synthetic datasets to identify crucial factors for realistic information-seeking conversation.", "introduction": "\n\nConversational question answering (CQA) involves modeling the information-seeking process of humans' dialogue. In the task, systems are asked to answer context-dependent questions that need to be understood in conversational flow. It makes CQA complex since even the same word could be interpreted differently depending on the context, and almost infinite cases of conversational context can be given with the question. To build robust * Equal contribution. \u2020 Corresponding author Who was their coach? q 2 (a) Information-symmetric Conversation (b) Information-asymmetric Conversation Figure 1: Examples of two conversation scenarios. In the former, the questioner can access the evidence document, allowing them to ask less related information to the conversation (q 1 , q 2 in (a)). In the latter, questioners are encouraged to seek new information from the hidden document. Hence, information-seeking behaviors are frequently observed; open-ended, unanswerable, and \"Anything else?\" questions (q 1 , q 2 , q 3 in (b)) system that can handle innumerable cases, largescale CQA datasets Reddy et al., 2019;Saeidi et al., 2018;Penha et al., 2019;Campos et al., 2020;Feng et al., 2020) have recently been developed. Still, it is practically infeasible to cover most of the interactions in real-world scenarios, which motivates automated methods for generating realistic CQA datasets.\n\nHowever, generating realistic CQA is a more challenging task, which requires synthesizing multiple interdependent ingredients, e.g., conversation history, appropriate follow-up question, and accurate answer from grounding document. Most of the literature has discussed only sub-parts of the overall process. A line of research in conversational question generation (CQG) aims to generate humanlike follow-up questions upon conversational history Pan et al., 2019;Qi et al., 2020;Gu et al., 2021). Another line of research has greatly improved answer accuracy Qu et al., 2019b;Kim et al., 2021;Zhao et al., 2021). In other words, they are limited in assuming that all other ingredients (i.e., held-out conversations by humans and their gold answer) are provided. Thus, the prior approaches cannot construct whole conversations upon the unlabeled corpus and therefore have never shown a practical use of synthetic conversations.\n\nIn this paper, we delve into the problem of generating a realistic CQA dataset from unlabeled documents. We focus on information-asymmetric conversations where reference information is unequally distributed to two agents, encouraging more realistic conversation. As illustrated in Figure  1 (a), when questioners have excessive information, they often assume and ask for external knowledge less relevant to the conversation. On the other hand, information asymmetry drives them to seek new information in conversational style or sometimes fail to do so (q 1 , q 2 , and q 3 in Figure 1 (b)). We claim that simulating these information-seeking behaviors is an important step towards a more realistic generation of CQA.\n\nTo take a further step towards generating realistic CQA, we propose and contrast two novel frameworks, SimSeek (Simulating Information-Seeking conversation) that can generate synthetic conversation upon the unlabeled corpus, replicating each scenario. We first introduce a strong simulator for information-symmetric conversation (1) SimSeek-sym where CQG model generates context-dependent questions based on the answer candidates, which are automatically provided in advance by an extractive model. Although it succeeds in generating reasonable conversations, we propose a more realistic approach that designs informationasymmetry, (2) SimSeek-asym. In SimSeek-asym, the CQG component first asks questions without accessing any answer-containing document and target answer. Then, an answerer model predicts corresponding answers to the questions.\n\nTo demonstrate the effectiveness of SimSeek in a semi-supervised setup, we conduct experiments in one of the challenging CQA benchmarks, QuAC . To the best of our knowledge, it is the first successful adaptation of the synthetic datasets for the semi-supervised CQA. In the experiment, SimSeek-asym consistently improves backbone CQA models by 1.1-1.9 F1 score, outperforming other CQA generation baselines. Besides, our resulting dataset could also enhance dense retrieval models for the conversational search task. Our framework improves the baseline dense retriever, DPR (Karpukhin et al., 2020) on the conversational search benchmark, OR-QuAC (Qu et al., 2020), by 1.1 of MRR and 1.3 of R@5.\n\nTo provide a deeper perspective into the information-seeking behavior, we thoroughly analyze how the two frameworks synthesize the results differently. Following Qi et al. (2020), we quantify various properties, specificity, answer relevance, and informativeness of the synthetic datasets. We compare them with a human-annotated dataset on the metrics and find that information asymmetry makes conversations closer to the human's information-seeking behavior.\n\nOur main contributions are summarized as:\n\n\u2022 We propose a novel framework, SimSeek, which can generate synthetic informationseeking conversations from unlabeled documents.\n\n\u2022 To the best of our knowledge, we are the first to demonstrate the effectiveness of synthetic datasets in the semi-supervised CQA, achieving competitive performance with humans.\n\n\u2022 We provide insight into realistic informationseeking conversations by contrasting two proposed approaches that simulate each CQA scenario.\n\n\n"}, {"paperid": "paper3", "title": "Dialog Inpainting: Turning Documents into Dialogs", "abstract": "Many important questions (e.g.\"How to eat healthier?\") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains on standard evaluation metrics.", "introduction": "\n\nModern information-seeking tools such as web search and question answering (Karpukhin et al., 2020;Zhu et al., 2021) excel at questions that have well-defined answers (e.g., \"Where was Barack Obama born?\"). But many important questions are more open-ended-e.g., \"How to eat healthier?\"-and require conversation to elicit context and explore in depth: \"How do I eat more protein?\", \"What about vegetarians?\". Conversational question answering systems (ConvQA) (Stede & Schlangen, 2004;Radlinski & Craswell, 2017;Culpepper et al., 2018), would empower users to answer these questions as if they could discuss with an expert at any time.\n\nDespite this promising vision, progress has been stymied by scarce training data. While conversational data is abundant in online forums, much of it focuses on personal anecdotes and subjective opinions, and is thus unsuitable for an information-seeking system: we desire responses that minimize personal biases and cite reliable sources. Directly crowdsourcing dialogs is also hard: crowdworkers are rarely experts in the domain of interest and tend to overlook important questions or provide shallow answers (Li et al., 2021). It is also expensive: the largest extant datasets contain only about 10,000 conversations each (Choi et al., 2018;Reddy arXiv:2205.09073v2 [cs.CL] 31 May 2022et al., 2019Dinan et al., 2018;Saeidi et al., 2018;Campos et al., 2020;Feng et al., 2020;Anantha et al., 2021).\n\nOn the other hand, high-quality documents, such as those in Wikipedia or PubMed, are abundant. These documents are often edited or written by experts who have invested significant time streamlining their discourse and anticipating a reader's questions. What if we could rewrite these documents into dialogs between the writer and their imagined reader? This would yield an enormous corpus of information-seeking dialogs with attributable, expert answers that could then be used to train a ConvQA system. We aim to achieve this with dialog inpainting.\n\nTo transform any document into a dialog, our first observation is that we already know what the writer wants to discuss-that is reflected in the original text of the document. So, we pretend that the original document is the transcript of what the writer said to the reader in an imagined dialog. But we are still missing what the reader asked. This is like overhearing someone else's phone call: you hear one side, but not the other. Oftentimes, one can still guess what the other side was saying -we call this prediction task dialog inpainting, because we are \"inpainting\" the missing parts of the dialog that we did not hear (inspired by the term's usage in computer vision (Iizuka et al., 2017;Liu et al., 2018;Yu et al., 2018)). Drawing on this intuition, we train an inpainter model to predict missing utterances in a dialog, and use it to predict the unobserved questions in a document. By interleaving the generated questions and sentences from the document, we form a dialog ( Figure 1).\n\nWe apply our inpainter to passages from Wikipedia and the web, yielding WikiDialog and WebDialog, 1 two datasets totalling 19M+ dialogs -1,000x larger than the largest existing ConvQA dataset. When evaluated for conversationality and answer adequacy, we surprisingly find that our synthetically generated data is as good or better than previous crowd-sourced datasets (Section 3). Our generated dialogs inherit the good qualities of the professionally written documents we inpaint (topical diversity, coherent discourse, evidence-backed claims, etc.) without needing to train on dialog data of the same quality.\n\nImportantly, we find that our inpainted datasets are powerful sources of training data for ConvQA systems (Section 4). When used to pre-train standard retriever and reranker architectures, they advance state-of-the-art across three different ConvQA retrieval benchmarks (QRECC, OR-QUAC, TREC-CAST), delivering up to 40% relative gains on standard evaluation metrics (Section 5). Remarkably, we find that just pre-training on WikiDialog enables strong zero-shot retrieval performance-up to 95% of a finetuned retriever's performance-without using any in-domain ConvQA data.\n\n\n"}, {"paperid": "paper4", "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models", "abstract": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.", "introduction": "\n\nIn information-seeking conversations, users repeatedly ask questions based on their interests, and the dialogue system provides answers to fulfill their information needs (Stede and Schlangen, 2004;Choi et al., 2018;Reddy et al., 2019). This scenario is important for addressing real-world open-ended questions, which requires discussions to explore in depth (Dai et al., 2022), e.g., How to learn more efficiently? Though great progress has been achieved in recent years, most existing researches depend on abundant human annotation, which can be highly costly and limited in knowledge coverage.\n\nA promising way to alleviate this problem is data augmentation . Traditional methods, including token-level manipulation (Kobayashi, 2018;Wei and Zou, 2019) Method DG Data Needs EDA (Wei and Zou, 2019) \u2717  and sentence-level paraphrasing (Sennrich et al., 2016), improve the linguistic diversity of training data. However, they cannot create conversations grounded on new documents, which are indispensable for dealing with out-of-domain scenarios. Another line of research focuses on simulation-based methods (Wu et al., 2021;Kim et al., 2022). Specifically, they can iteratively generate conversations grounded on new documents based on a span extractor and an utterance generator. Nevertheless, both the training of the extractor and the generator still require abundant human dialogues. Besides the above ways, Dai et al. (2022) propose Dialog Inpainting, which creates information-seeking dialogues by inserting utterances between neighboring sentences in documents. One potential risk is the gap between the structure of documents and that of conversations. Documents are tighter, while realworld conversations are more open-ended. To alleviate the above issues, we propose a simple yet effective method AutoConv for Automatically generating information-seeking Conversations, which takes advantage of the fewshot learning ability and generation capacity of large language models (LLM) (Brown et al., 2020). Specifically, we formulate conversation generation as a language modeling task and utilize an LLM for generating synthetic conversations grounded on external documents. Surprisingly, finetuning with a few human dialogues can help LLM capture the characteristics of the information-seeking process \n\n\n"}]