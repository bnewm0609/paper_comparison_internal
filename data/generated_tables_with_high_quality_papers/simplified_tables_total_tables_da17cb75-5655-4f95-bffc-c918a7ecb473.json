{"pap_to_tab": {"Adversarial example type introduced": {"paper_1": ["remote adversarial patches (RAP)"], "paper_2": ["real-world adversarial examples (RWAEs), physical objects like billboards and printable patches optimized to be adversarial"]}, "Main field of application": {"paper_1": ["autonomous vehicles and medical screening"], "paper_2": ["autonomous driving"]}, "Effect of adversarial attacks on deep learning models": {"paper_1": ["changes the classification or semantics of locations far from the patch"], "paper_2": ["crafted adversarial patches affect the robustness of popular SS models"]}, "Datasets used for analysis": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset, CARLA driving simulator"]}, "Success rate of the attacks": {"paper_1": ["up to 93% on average"], "paper_2": ["attacks are notably less effective in the real world"]}, "Novel approach or method proposed or investigated": {"paper_1": ["implementation of IPatch for RAP attacks on image segmentation and object recognition models"], "paper_2": ["scene-specific attack leveraging the CARLA driving simulator, extending of EOT to SS"]}, "Conclusion on practical relevance of adversarial attacks": {"paper_1": ["Not explicitly mentioned, but high success rate suggests significant relevance"], "paper_2": ["attacks are notably less effective in the real world, questioning practical relevance"]}, "Targeted machine learning models": {"paper_1": ["five state-of-the-art architectures with eight different encoders, YOLOv3 model"], "paper_2": ["popular semantic segmentation (SS) models"]}}, "cc_to_tab": {"Adversarial patch type": {"paper_1": ["remote adversarial patches (RAP) that can alter image semantics from any location"], "paper_2": ["robustness against digital and real-world adversarial patch attacks"]}, "Datasets used for evaluation": {"paper_1": ["CamVid street view dataset", "YOLOv3 model"], "paper_2": ["Cityscapes dataset", "CARLA driving simulator", "outdoor driving tests"]}, "Success rate and effectiveness": {"paper_1": ["Change classification of remote target regions with up to 93% success rate"], "paper_2": ["Less effective in real-world settings"]}, "Primary focus": {"paper_1": ["Creation and validation of a novel adversarial patch type"], "paper_2": ["Comprehensive robustness study, novel loss function, practical relevance"]}, "Attack mechanism and practical relevance": {"paper_1": ["High success rate on average of RAP"], "paper_2": ["Investigation into practical relevance and transferability of attacks"]}, "Novel attack formulations": {"paper_1": ["Implementation of IPatch for RAP"], "paper_2": ["Extension of EOT paradigm, scene-specific attack optimization"]}, "Security focus": {"paper_1": ["Advancements in adversarial example development"], "paper_2": ["Real-world implications and defense against adversarial attacks"]}, "Real-world applicability": {"paper_1": ["Theoretical and dataset-focused analysis"], "paper_2": ["Printed billboard tested in outdoor driving scenario"]}}, "multi_scheme": {"IPatch": {"paper_1": ["A new class of adversarial example called 'remote adversarial patch' (RAP) that alters a model's perception of an image's semantics and can be placed anywhere within an image to change the classification or semantics of locations far from the patch."], "paper_2": []}, "scene-specific attack": {"paper_1": [], "paper_2": ["An attack optimization leveraging the CARLA driving simulator to improve the transferability of EOT-based attacks to a real 3D environment."]}, "Expectation Over Transformation (EOT) paradigm": {"paper_1": [], "paper_2": ["Extended for the Cityscapes dataset to cope with semantic segmentation and used to inform scene-specific attack optimizations."]}, "alteration by IPatch": {"paper_1": ["The IPatch changes the classification of a remote target region with a success rate of up to 93% on average."], "paper_2": []}, "main computer vision tasks": {"paper_1": [], "paper_2": ["Object detection and semantic segmentation (SS)."]}, "environmental tests on SS models": {"paper_1": [], "paper_2": ["Revealed that the proposed attack formulations outperform previous work to craft digital and real-world adversarial patches for SS, but are notably less effective in the real world."]}, "generalization beyond tested models": {"paper_1": ["Not directly addressed, but preliminary results on YOLOv3 model for object recognition are mentioned."], "paper_2": []}, "practical relevance of adversarial attacks": {"paper_1": [], "paper_2": ["Experimental results showed adversarial attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving."]}, "focus of robustness evaluation": {"paper_1": [], "paper_2": ["To assess the robustness of popular semantic segmentation models against real-world adversarial patch attacks."]}, "robustness in SS models": {"paper_1": [], "paper_2": ["Current state of robustness is questioned due to the notable ineffectiveness of the attacks in real-world tests compared to digital scenarios."]}, "adversarial perturbations": {"paper_1": [], "paper_2": ["Physical objects or digital inputs optimized to be adversarial to the model's entire perception pipeline, affecting tasks like object detection and semantic segmentation."]}, "state-of-the-art architectures tested": {"paper_1": ["Five state-of-the-art architectures for image segmentation RAP attacks."], "paper_2": []}, "encoders used": {"paper_1": ["Eight different encoders used in testing image segmentation RAP attacks."], "paper_2": []}, "IPatch tested on object recognition models": {"paper_1": ["Yes, preliminary results on the YOLOv3 model are mentioned."], "paper_2": []}, "comparison to previous work": {"paper_1": [], "paper_2": ["Proposed attack formulations outperform previous work in crafting adversarial patches for SS."]}, "novel loss function": {"paper_1": [], "paper_2": ["A novel loss function is mentioned as part of the powerful attacks enriched but not detailed in the abstract."]}, "purpose of a remote adversarial patch (RAP)": {"paper_1": ["To alter a model's perception of an image's semantics from a distance, changing the classification or semantics of locations far from the patch."], "paper_2": []}, "implications for autonomous vehicles and medical screening": {"paper_1": ["These models could be fooled by RAPs such as IPatch altering image semantics, potentially causing incorrect localizations and identifications."], "paper_2": []}, "applications affected by adversarial patches": {"paper_1": ["Applications like autonomous vehicles and medical screening that rely on object localization and identification."], "paper_2": []}, "effect of IPatch on image semantics": {"paper_1": ["IPatch can change the model's perception of an image's semantics, influencing classification or semantics of areas far from the patch itself."], "paper_2": []}}, "ours_final_table": {"Problem tackled by the paper": {"paper_1": ["The creation of 'remote adversarial patches' (RAP) which can be placed anywhere within an image to change the classification or semantics of locations far from the patch, specifically within applications such as autonomous vehicles and medical screening that use deep learning models for localization and identification."], "paper_2": ["The robustness of popular semantic segmentation (SS) models in the context of autonomous driving against real-world adversarial patch attacks, including an in-depth evaluation of both digital and real-world adversarial patches, as well as the formulation of a novel attack optimization strategy."]}, "approach": {"paper_1": ["IPatch introduces a new type of adversarial patch named 'remote adversarial patches' (RAP), which can be placed anywhere within an image to change the classification or semantics of locations far from the patch. The paper presents an in-depth analysis on image segmentation attack effectiveness using state-of-the-art architectures on the CamVid dataset and preliminary results on YOLOv3 for object recognition."], "paper_2": ["An in-depth evaluation of the robustness of semantic segmentation models against both digital and real-world adversarial patches is proposed. To do this, powerful attacks are crafted with a novel loss function, and an extension of the Expectation Over Transformation (EOT) approach is developed to cope with semantic segmentation. Additionally, a scene-specific attack leveraging the CARLA simulator to improve real-world transferability is introduced. Real-world feasibility is tested with a printed billboard containing an adversarial patch in outdoor driving scenarios."]}, "Type of adversarial patch introduced": {"paper_1": ["Remote adversarial patches (RAP) that can alter an image's semantics and be placed anywhere within an image"], "paper_2": ["Real-world adversarial patches (RWAEs) optimized to be adversarial to the entire perception pipeline"]}, "Success rate of the adversarial patch": {"paper_1": ["Up to 93% on average for changing the classification of a remote target region"], "paper_2": ["Not explicitly mentioned, but proposed attacks outperform previous work and are notably less effective in the real world"]}, "Deep learning architectures analyzed": {"paper_1": ["Five state-of-the-art architectures with eight different encoders on the CamVid dataset"], "paper_2": ["Popular semantic segmentation models, detailed evaluation on the Cityscapes dataset"]}, "Methods for generating adversarial patches": {"paper_1": ["Implementation of IPatch using an in-depth analysis on image segmentation attacks"], "paper_2": ["Extension of Expectation Over Transformation (EOT) paradigm, plus a novel loss function, and a scene-specific attack leveraging the CARLA driving simulator"]}, "Real-world applicability and testing of the adversarial patch": {"paper_1": ["Preliminary results on the YOLOv3 model for object recognition"], "paper_2": ["Printed physical billboard containing an adversarial patch was tested in outdoor driving scenario"]}, "Datasets used for testing and analysis": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset, CARLA driving simulator for real 3D environment testing"]}, "methods to generate adversarial patches": {"paper_1": ["IPatch introduces a new type of adversarial patch termed 'remote adversarial patches' (RAP) which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. In-depth analysis was performed on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Preliminary results on the popular YOLOv3 model for object recognition are also provided."], "paper_2": ["The paper presents an in-depth evaluation of the robustness of semantic segmentation models using both digital and real-world adversarial patches. The attacks are crafted with powerful attacks enriched with a novel loss function. Extends the Expectation Over Transformation (EOT) paradigm to cope with semantic segmentation. It introduces a novel attack optimization called scene-specific attack which uses the CARLA driving simulator to improve the transferability of EOT-based attacks to a real 3D environment. Furthermore, a printed physical billboard with an adversarial patch was tested in an outdoor driving scenario."]}, "Implications for security of image recognition systems": {"paper_1": ["IPatch introduces 'remote adversarial patches' that can change image classifications or semantics from a distance, significantly impacting the accuracy of deep learning models in critical applications like autonomous vehicles and medical screening. There's up to a 93% success rate on average in changing remote target region classification, indicating a severe threat to model reliability."], "paper_2": ["Real-world adversarial patch attacks demonstrate the vulnerability of semantic segmentation models used in autonomous driving, although these attacks are notably less effective in real-world scenarios compared to digital simulations. This reveals a potential gap in the practical relevance of such adversarial attacks, but still poses a notable security risk for systems reliant on these models."]}, "defense mechanisms against such adversarial attacks": {"paper_1": [], "paper_2": []}, "Issue with deep learning models": {"paper_1": ["Adversarial patches can be used to fool models by altering a model's perception of an image's semantics, affecting the classification or semantics of locations far from the patch with a high success rate."], "paper_2": ["Deep learning models are vulnerable to both digital and real-world adversarial patch attacks, which can deceive object detection and semantic segmentation used in autonomous driving."]}, "developed method for altering model perception": {"paper_1": ["IPatch: a type of remote adversarial patch (RAP) which alters a model's perception of an image's semantics by placing the patch anywhere within an image to change the classification or semantics of locations far from the patch"], "paper_2": ["A novel loss function enriched with Expectation Over Transformation (EOT) paradigm, and a scene-specific attack optimization for crafting digital and real-world adversarial patches to evaluate the robustness of popular semantic segmentation (SS) models"]}, "adversarial patch type": {"paper_1": ["remote adversarial patches (RAP)"], "paper_2": ["real-world adversarial examples (RWAEs)"]}, "adversarial patch effect": {"paper_1": ["alters a model's perception of an image's semantics"], "paper_2": ["optimizes physical objects to be adversarial to the entire perception pipeline"]}, "target models": {"paper_1": ["image segmentation models", "object recognition models (YOLOv3)"], "paper_2": ["semantic segmentation (SS) models"]}, "dataset used for evaluation": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset", "CARLA driving simulator"]}, "attack method": {"paper_1": ["implementation of IPatch", "in-depth analysis of image segmentation RAP attacks"], "paper_2": ["crafting of digital and real-world adversarial patches", "novel loss function", "Expectation Over Transformation (EOT) paradigm", "scene-specific attack"]}, "outcome": {"paper_1": ["up to 93% success rate in changing classification of a remote target region"], "paper_2": ["attacks are notably less effective in the real world"]}, "methods to enhance the robustness of semantic segmentation models": {"paper_1": [], "paper_2": []}, "Proposed solutions to mitigate real-world adversarial patch attacks": {"paper_1": ["The introduction of 'remote adversarial patches' (RAP) called IPatch changes the model's perception of an image's semantics. Extensive analysis was performed using state-of-the-art architectures, and while not explicitly stated, mitigation strategies might involve improving model robustness against these remote adversarial patches."], "paper_2": ["The robustness of popular semantic segmentation (SS) models against adversarial patches was evaluated, including a novel loss function and attack optimization methods like the scene-specific attack and an extended Expectation Over Transformation (EOT) paradigm. Additionally, the paper tested real-world scenarios using a printed physical billboard, highlighting that attacks are less effective in the real world, implying that practical evaluations are critical for mitigation."]}, "Effectiveness of proposed solutions in real-world environments": {"paper_1": ["IPatch demonstrated a high success rate of up to 93% on average in changing the classification of a remote target region against models, which suggests the importance of developing countermeasures against such high-efficacy attacks in real-world environments."], "paper_2": ["Real-world testing revealed that the attacks, while effective in a controlled digital environment, were notably less effective in real scenarios, suggesting that current adversarial attack models may overstate the practical threat and that the actual environment needs to be considered in mitigation strategies."]}, "Types of Adversarial Patch Attacks": {"paper_1": ["Remote adversarial patches (RAP) that change the classification or semantics of locations far from the patch"], "paper_2": ["Real-world adversarial examples (RWAEs), digital adversarial patches, EOT-based attacks, scene-specific attacks optimized for 3D environments"]}, "Target Models": {"paper_1": ["State-of-the-art image segmentation architectures with different encoders, YOLOv3 object recognition model"], "paper_2": ["Popular semantic segmentation (SS) models"]}, "Datasets Used": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset, CARLA driving simulator"]}, "Effectiveness in Real World": {"paper_1": ["Not specifically addressed within the abstract"], "paper_2": ["Attacks were less effective in the real world, questioning the practical relevance"]}, "problem addressed concerning semantic segmentation models": {"paper_1": ["Adversarial patches that can be placed anywhere in an image to impact the perception of image semantics and change the classification or semantics of remote regions, with high success rates affecting segmentation models."], "paper_2": ["Robustness of semantic segmentation models in autonomous driving against adversarial perturbations, specifically real-world adversarial examples like physical objects or billboards, which may impact the entire perception pipeline."]}, "Application Area of Focus for Robustness": {"paper_1": ["Adversarial attacks in image semantics and classification, particularly in autonomous vehicles and object recognition systems like YOLOv3."], "paper_2": ["Semantic Segmentation (SS) robustness for autonomous driving in the face of digital and real-world adversarial patch attacks."]}, "Differentiation in adversarial patch design": {"paper_1": ["IPatch introduces a new type of adversarial patch named 'remote adversarial patches' (RAP) that alters a model's perception of an image's semantics by being placed anywhere within the image to change the classification or semantics of locations far from the patch."], "paper_2": ["The paper focuses on evaluating the robustness of semantic segmentation models against real-world adversarial patch attacks rather than introducing a new adversarial patch design."]}, "Effectiveness of adversarial patch": {"paper_1": ["IPatch changes the classification of a remote target region with a success rate of up to 93% on average on the CamVid street view dataset and preliminary results on the YOLOv3 model."], "paper_2": ["The paper proposes a novel loss function for crafting both digital and real-world adversarial patches, and while the attacks are more optimized and outperform previous work, their effectiveness is notably less in real-world scenarios questioning the practical relevance."]}, "Attack methodology": {"paper_1": ["Uses state-of-the-art architectures with different encoders to analyze image segmentation RAP attacks, extending the attack to object recognition models."], "paper_2": ["Evaluates robustness by testing the effects of digital and real-world adversarial patches crafted with powerful attacks enriched with a novel loss function, using EOT paradigm extended for SS and a scene-specific attack leveraging a driving simulator."]}, "Implications for robustness of models": {"paper_1": ["Remote adversarial patches (RAP) like IPatch can significantly alter a model's perception of an image's semantics anywhere within the image, changing classification or semantics of distant locations. The implementation of IPatch demonstrated a high success rate of up to 93% on average in changing classifications on the CamVid dataset and showed preliminary success on object recognition models like YOLOv3."], "paper_2": ["Real-world adversarial examples (RWAEs), such as physical adversarial patches, present a challenge to SS models' robustness in autonomous driving scenarios. The research found these attacks to be less effective in the real world compared to digital simulations, yet they still represent a potential risk to the reliability of perception systems in autonomous/assisted driving that require further study and mitigation."]}, "Countermeasures for IPatch": {"paper_1": ["The paper does not provide specific countermeasures for IPatch, but suggests that understanding the attack vector, which involves remote adversarial patches, helps in determining how to defend against it."], "paper_2": ["The paper discusses evaluating the robustness of semantic segmentation models and suggests the use of adversarial training with digital and real-world adversarial patches, as well as improving model architectures and defense methods."]}, "Impact on image recognition systems": {"paper_1": ["IPatch can significantly alter the classification of remote target regions in an image with a high success rate, affecting the reliability of image recognition systems."], "paper_2": ["Adversarial patches can seriously degrade the performance of semantic segmentation models that are crucial for autonomous driving; however, their effect is notably less in real-world scenarios."]}, "Datasets used for testing": {"paper_1": ["CamVid street view dataset and preliminary results on the YOLOv3 model"], "paper_2": ["Cityscapes dataset and CARLA driving simulator for real-world testing"]}, "Adversarial patch application": {"paper_1": ["The remote adversarial patch can be placed anywhere within an image and affects locations far from the patch itself."], "paper_2": ["The paper examines the use of both digital and physical real-world adversarial patches and their effects on semantic segmentation."]}, "Proposed approach or solution": {"paper_1": ["A new type of adversarial patch called IPatch, which creates 'remote adversarial patches' (RAP) that can be placed anywhere within an image to change the classification or semantics of locations far from the patch. An in-depth analysis of image segmentation attacks using various architectures and preliminary results on object recognition models, specifically YOLOv3, are also presented."], "paper_2": ["An in-depth evaluation of the robustness of popular semantic segmentation models against both digital and real-world adversarial patches. The study introduces powerful attack methods enriched with a novel loss function and extends the Expectation Over Transformation paradigm to SS. Additionally, it proposes a scene-specific attack optimization for improving attack transferability to real 3D environments and tests the effectiveness of these attacks in real-world outdoor driving scenarios."]}, "IPatch": {"paper_1": ["a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch, called 'remote adversarial patches' (RAP). IPatch is an implementation of RAP and has shown a high success rate in altering image segmentation and object recognition models."], "paper_2": ["The paper does not specifically mention IPatch, it focuses on evaluating the robustness of semantic segmentation in the context of real-world adversarial patch attacks, which might include attacks like IPatch."]}, "Purpose of adversarial patch": {"paper_1": ["Introduce a new type of adversarial patch (IPatch), referred to as 'remote adversarial patches' (RAP), which changes the classification or semantics of locations far from the patch within an image and performs in-depth analysis on image segmentation RAP attacks, as well as showing preliminary success on object recognition models like YOLOv3."], "paper_2": ["Evaluate the robustness of semantic segmentation models used in autonomous driving against real-world adversarial patch attacks (RWAEs), test both digital and real-world adversarial patches, and propose a novel attack optimization for improving the transferability of attacks from digital to real 3D environments through experiments in the CARLA driving simulator and an outdoor driving scenario."]}}}