[{"paperid": "paper0", "title": "Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning", "abstract": "In this work, we introduce a bootstrapped, iterative NER model that integrates a PU learning algorithm for recognizing named entities in a low-resource setting. Our approach combines dictionary-based labeling with syntactically-informed label expansion to efficiently enrich the seed dictionaries. Experimental results on a dataset of manually annotated e-commerce product descriptions demonstrate the effectiveness of the proposed framework.", "introduction": "\n\nThe vast majority of existing named entity recognition (NER) methods focus on a small set of prominent entity types, such as persons, organizations, diseases, and genes, for which labeled datasets are readily available (Tjong Kim Sang and De Meulder, 2003;Smith et al., 2008;Weischedel et al., 2011;Li et al., 2016). There is a marked lack of studies in many other domains, such as e-commerce, and for novel entity types, e.g. products and components.\n\nThe lack of annotated datasets in the ecommerce domain makes it hard to apply supervised NER methods. An alternative approach is to use dictionaries (Nadeau et al., 2006;Yang et al., 2018), but freely available knowledge resources, e.g. Wikidata (Vrande\u010dic and Kr\u00f6tzsch, 2014) or YAGO (Suchanek et al., 2007), contain only very limited information about e-commerce entities. Manually creating a dictionary of sufficient quality and coverage would be prohibitively expensive. This is amplified by the fact that in the e-commerce domain, entities are frequently ex-pressed as complex noun phrases instead of proper names. Product and component category terms are often combined with brand names, model numbers, and attributes (\"hard drive\" \u2192 \"SSD hard drive\" \u2192 \"WD Blue 500 GB SSD hard drive\"), which are almost impossible to enumerate exhaustively. In such a low-coverage setting, employing a simple dictionary-based approach would result in very low recall, and yield very noisy labels when used as a source of labels for a supervised machine learning algorithm. To address the drawbacks of dictionary-based labeling, Peng et al. (2019) propose a positive-unlabeled (PU) NER approach that labels positive instances using a seed dictionary, but makes no label assumptions for the remaining tokens (Bekker and Davis, 2018). The authors validate their approach on the CoNLL, MUC and Twitter datasets for standard entity types, but it is unclear how their approach transfers to the ecommerce domain and its entity types.\n\nContributions We adopt the PU algorithm of Peng et al. (2019) to the domain of consumer electronic product descriptions, and evaluate its effectiveness on four entity types: Product, Component, Brand and Attribute. Our algorithm bootstraps NER with a seed dictionary, iteratively labels more data and expands the dictionary, while accounting for accumulated errors from model predictions. During labeling, we utilize dependency parsing to efficiently expand dictionary matches in text. Our experiments on a novel dataset of product descriptions show that this labeling mechanism, combined with a PU learning strategy, consistently improves F1 scores over a standard BiLSTM classifier. Iterative learning quickly expands the dictionary, and further improves model performance. The proposed approach exhibits much better recall than the baseline model, and generalizes better to unseen entities.\n\nAlgorithm 1: Iterative Bootstrapping NER Input: Dictionary D seed , Corpus C, threshold K, max iterations I Result: Dictionary D + , Classifier L D + \u2190 D seed ; C dep \u2190 dependency parse(C); i \u2190 0; while not converged(D + ) and i < I do C lab \u2190 label(C, D + ); C exp \u2190 expand labels(C lab , C dep ); L \u2190 train classif ier(C exp ); C pred \u2190 predict(C exp , L); for e \u2190 C pred do if e / \u2208 D + and freq(e) > K then D + \u2190 add entity(D + , e); end end i \u2190 i + 1; end\n\n\n"}, {"paperid": "paper1", "title": "The Effect of Natural Distribution Shift on Question Answering Models", "abstract": "We build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. Our first test set is from the original Wikipedia domain and measures the extent to which existing systems overfit the original test set. Despite several years of heavy test set re-use, we find no evidence of adaptive overfitting. The remaining three test sets are constructed from New York Times articles, Reddit posts, and Amazon product reviews and measure robustness to natural distribution shifts. Across a broad range of models, we observe average performance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.", "introduction": "\n\nSince its release in 2016, the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has generated intense interest from the natural language processing community. At first glance, this intense interest has lead to impressive results. The best performing models in 2020  have F1 scores more than 40 points higher than the baseline presented by Rajpurkar et al. (2016). At the same time, it remains unclear to what extent progress on these benchmark numbers is a reliable indicator of progress more broadly.\n\nThe goal of building question answering systems is not merely to obtain high scores on the SQuAD leaderboard, but rather to generalize to new examples beyond the SQuAD test set. However, the competition format of SQuAD puts pressure on the validity of leaderboard scores. It is well-known that repeatedly evaluating models on a held-out test set can give overly optimistic estimates of model performance, a phenomenon known as adaptive overfitting Dwork et al. (2015). Moreover, the standard SQuAD evaluation only measures model performance on new examples from a single distribution, i.e., paragraphs derived from Wikipedia articles. Nevertheless, we often use models in settings different from the one in which they were trained. While Jia and Liang (2017) demonstrated that SQuAD models are not robust to adversarial distribution shifts, one might still hope that the models are more robust to natural distribution shifts, for instance changing from Wikipedia to newspaper articles.\n\nThis state of affairs raises two important questions:\n\nAre SQuAD models overfit to the SQuAD test set?\n\nAre SQuAD models robust to natural distribution shifts? In this work, we address both questions by replicating the SQuAD dataset creation process and generating four new SQuAD test sets on both the original Wikipedia domain, as well as three new domains: New York Times articles, Reddit posts, and Amazon product reviews.\n\nWe first show that there is no evidence of adaptive overfitting on SQuAD. Across a large collection of SQuAD models, there is little to no difference between the F1 scores from the original SQuAD test set and our replication. This even holds when comparing scores from the SQuAD development set (which was publicly released with answers) to our new test set. The lack of adaptive overfitting is consistent with recent replication studies in the context of image classification ; Yadav and Bottou (2019). These studies leave open the possibility that this phenomenon is specific to the data or models typical in computer vision research. Our result demonstrates this same phenomenon also holds for natural language processing.\n\nBeyond adaptive overfitting, we also demonstrate that SQuAD models exhibit robustness to some of our natural distribution shifts, though they still suffer substantial performance degradation on others. On the New York Times dataset, models in our testbed on average drop 3.8 F1 points. On the Reddit and Amazon datasets, the drop is on average 14.0 and 17.4 F1 points, respectively. All of our datasets were collected using the same data generation pipeline, so this degradation can be attributed purely to changes in the source text rather than differences in the annotation procedures across datasets.\n\nWe complement each of these experiments with a strong human baseline comprised of the authors of this paper. On the original SQuAD data, our human accuracy numbers are on par with the best SQuAD models  and significantly better than the Mechanical Turk baseline reported by Rajpurkar et al. (2016). On our new test sets, average human F1 scores decrease by 0.1 F1 on New York Times, 2.9 on Reddit, and 3.0 on Amazon. All of the resulting F1 scores are substantially higher than the best SQuAD models on the respective test sets. Figure 1 summarizes the main results of our experiments. Humans show consistent behavior on all four test sets, while models are substantially less robust against two of the distribution shifts. Although there has been steady progress on the SQuAD leaderboard, there has been markedly less progress in this robustness dimension.\n\nTo enable future research, all of our new tests sets are freely available online. 1 \n\n\n"}, {"paperid": "paper2", "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "abstract": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval-2014 Task 4 aimed to foster research in the \ufb01eld of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.", "introduction": "\n\nWith the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span (Pang et al., 2002;Turney, 2002). However, the need for a more fine-grained approach, such as aspect-based (or 'feature-based') sentiment analysis (ABSA), soon became apparent (Liu, 2012). For example, laptop reviews not only express the overall sentiment about a specific model (e.g., \"This is a great This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ laptop\"), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc. Subsequently, a review may convey opposing sentiments (e.g., \"Its performance is ideal, I wish I could say the same about the price\") or objective information (e.g., \"This one still has the CD slot\") for different aspects of an entity.\n\nABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005;Titov and McDonald, 2008;Hu and Liu, 2004a;Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009;Brody and Elhadad, 2010).\n\nPrevious publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks. The restaurant reviews dataset of Ganu et al. (2009) uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral). Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, \"The restaurant was expensive, but the menu was great.\" would be assigned the aspect PRICE with negative polarity and FOOD with positive polarity. In the product reviews dataset of Hu and Liu (2004a;2004b), aspect terms, i.e., terms naming aspects (e.g., 'radio', 'voice dialing') together with strength scores (e.g., 'radio': +2, 'voice dialing': \u22123) are pro-vided. No predefined inventory of aspects is provided, unlike the dataset of Ganu et al.\n\nThe SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see Section 2). Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice.\n\n\n"}, {"paperid": "paper3", "title": "Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product", "abstract": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset will be released to the public.", "introduction": "\n\nProduct attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015;Yu et al., 2017), product recommendations (Gong,  2009; Cao et al., 2018), and product retrieval (Liao et al., 2018;Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers' shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1.\n\nThough plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011;More, 2016;Shinzato and Sekine, 2013;Zheng et al., 2018;Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task.\n\n\n"}, {"paperid": "paper4", "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval", "abstract": "Passage retrieval is a fundamental task in information retrieval (IR) research, which has drawn much attention recently. In the English field, the availability of large-scale annotated dataset (e.g, MS MARCO) and the emergence of deep pre-trained language models (e.g, BERT) has resulted in a substantial improvement of existing passage retrieval systems. However, in the Chinese field, especially for specific domains, passage retrieval systems are still immature due to quality-annotated dataset being limited by scale. Therefore, in this paper, we present a novel multi-domain Chinese dataset for passage retrieval (Multi-CPR). The dataset is collected from three different domains, including E-commerce, Entertainment video and Medical. Each dataset contains millions of passages and a certain amount of human annotated query-passage related pairs. We implement various representative passage retrieval methods as baselines. We find that the performance of retrieval models trained on dataset from general domain will inevitably decrease on specific domain. Nevertheless, a passage retrieval system built on in-domain annotated dataset can achieve significant improvement, which indeed demonstrates the necessity of domain labeled data for further optimization. We hope the release of the Multi-CPR dataset could benchmark Chinese passage retrieval task in specific domain and also make advances for future studies.", "introduction": "\n\nLarge scale passage retrieval is an important problem in information retrieval research field. Passage retrieval is often regarded Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '22, July 11-15, 2022 [26,30], machine reading comprehension [36,43] and web search systems [4], etc. Recent advances in deep learning have allowed state of the art performance on passage retrieval task compared to conventional statistical models [15-17, 26, 42]. However, these deep neural models usually contain millions of parameters that necessitate a large amount of training data. As such, high-quality public available benchmark dataset is critical for research progress with a deep-model fashion for the passage retrieval task.\n\nIn the English field, we observed that large, high-quality dataset enables the community rapidly develop new models for passage retrieval task, and at the same time, the research on model architecture also obtains a more deep understanding. As mentioned above, passage retrieval mainly serves downstream tasks such as question answering and machine reading comprehension. Therefore, existing datasets are also constructed based on the above two tasks. In term of question answering, there are several benchmark datasets like TREC QA [48], WikiPassageQA [6] and InsuranceQA [14]. For machine reading comprehension task, representative datasets including SQuAD [44], MS MARCO [5], CNN /Daily News [23] provide good benchmarks. In summary, dataset in the English field is relatively mature in terms of data scale and domain richness. On the other hand, in the field of Chinese, although some information retrieval datasets and machine reading comprehension datasets have been released in recent years like Sogou-QCL [55], Dureader [22] and SC-MRC [8], these datasets are mainly concentrated in the general domain, and dataset that can be adopted for specific domain passage retrieval research is still in shortage.\n\nTo push forward the quality and variety of Chinese passage retrieval dataset, we present Multi-CPR. There are three main properties of Multi-CPR: a) Multi-CPR is the first dataset that covers multiple specific domains for Chinese passage retrieval, including E-commerce, Entertainment video and Medical. There is a high degree of differentiation within the three domains. Furthermore, Only one (Medical) of these domains has been studied in previous research [54]. b) Multi-CPR is the largest domain specific Chinese passage retrieval dataset. For each domain, Multi-CPR contains millions of passages (e,g. 1,002,822 passages for the E-commerce domain) and sufficient human annotated query-passage related pairs. More detailed statistics of Multi-CPR and annotated examples can be found in Table 4 and Section 3.2. c) All Queries and passages in Multi-CPR are collected from real search engine systems within Alibaba Group. The authenticity of the samples allows Multi-CPR to meet the needs of both academia and industry fields.\n\nAs an attempt to tackle Multi-CPR and provide strong baselines, we implement various representative passage retrieval methods including both sparse and dense models. For the sparse models, except for the basic BM25 method, we also verified that previously proposed optimization methods based on the sparse strategy can indeed achieve significant improvement compared to the BM25 baseline (e,g, doc2query method). For the dense models, we mainly implemented methods based on the DPR model. Similarly, we also made some optimizations based on the DPR model. Compared to the sparse models, we found that the retrieval performance of the dense models trained on labeled dataset can be greatly improved. This observation empirically confirms the value of annotated data. In further, we verified that the retrieval-then-reranking two-stage framework based on the BERT model can further improve the overall retrieval performance on all three datasets in Multi-CPR, which once again corroborates the quality of Multi-CPR.\n\nIn summary, the major contributions of this paper are threefold:\n\n\u2022 We present Multi-CPR, the largest-scale Chinese multi domain passage retrieval dataset collected from practical search engine systems, and it covers E-commence, Entaitement vedio and Medical domain. \u2022 We conduct an in-depth analysis on Multi-CPR. Based on Multi-CPR, we have analyzed the characteristics of different passage retrieval methods along with their optimization strategies associated, which enables us to have a deeper understanding of Chinese passage retrieval task in specific domain. \u2022 We implement various representative methods as baselines and show the performance of existing methods on Multi-CPR, which provides an outlook for future research.\n\n\n"}, {"paperid": "paper5", "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce", "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.", "introduction": "\n\nIn the field of E-commerce, the progress made in natural language processing (NLP) and deep learning (DL) has significantly contributed to the advancement of E-commerce technology. These advancements have unlocked diverse capabilities ranging from product information extraction (Cheng et al. 2021;Wang et al. 2021) to user query understanding (Zhao, Chen, and Yin 2019;Ahmadvand et al. 2020). Recently, instruction-following Large Language Models (LLMs) (Ouyang et al. 2022;Taori et al. 2023;Chiang et al. 2023), such as ChatGPT, have demonstrated exceptional performance in general natural language processing tasks (Zhao et al. 2023). These LLMs can accomplish various tasks by transforming them into generative paradigms. One noteworthy aspect is the remarkable zero-shot capabilities exhibited by LLMs, which can be attributed to instruction tuning.\n\nHowever, despite their numerous merits, general LLMs are not specifically designed for the E-commerce sector. This can lead to suboptimal performance for various E-commerce tasks. Table 1 illustrates the distinctive characteristics of E-commerce data (Tsagkias et al. 2021;Jiang et al. 2022) compared to general domains. Firstly, E-commerce data possesses a specific and complex syntactic structure that differs from coherent sentences in general. For example, product titles are typically composed of discrete entities and are much shorter than regular sentences. Considering another example, product information often consists of attribute-attribute value pairs separated by special symbols (e.g., \"##\"), which also poses challenges for general LLMs to comprehend. Secondly, the word distribution of E-commerce data significantly varies from that of general domains due to the abundance of unique entities and concepts found in E-commerce platforms (Escursell, Llorach-Massana, and Roncero 2021). Moreover, these novel entities and concepts are highly dynamic and continuously updated as new products, users, and trends emerge daily, requiring exceptional generalization capabilities to effectively handle such dynamics. Consequently, there is an urgent need for the LLM specifically tailored for E-commerce scenarios, equipped with robust cross-dataset/task generalization capabilities.\n\nIn the BERT era, numerous efforts (Zhang et al. 2021;Qiu et al. 2022;Xu et al. 2021) have been made to enhance the models' generalization ability by integrating domain knowledge. For instance, E-BERT (Poerner, Waltinger, and Sch\u00fctze 2020) further pre-trains BERT on the Amazon dataset to incorporate semantic knowledge of the E-commerce domain into BERT. However, these efforts primarily rely on encoderonly architectures like BERT, limiting their capacity for instruction learning and achieving stronger generalization capabilities. Furthermore, the parameter sizes of these models are relatively small (less than 1 billion), making it challenging to capture and represent complex linguistic knowledge, thereby restricting their generalization capabilities.\n\nTo enhance models' generalization ability cross dataset/tasks, this work presents the first E-commerce instruction dataset, EcomInstruct, comprising a total of 2.5 million instruction data and 134 tasks. EcomInstruct are built from two main sources. Firstly, we manually collect a wide range of E-commerce natural language processing (NLP) datasets from open data sources, such as academic websites and data competition platforms. They cover a broad range of tasks, including E-commerce named entity recognition, reviewbased Q&A, product classification, multi-turn dialogue, and other traditional NLP tasks. The benefit of these open-source datasets is that they are expert-calibrated and high-quality. Secondly, we identified several basic data types that are common in E-commerce scenarios, including product information, user reviews, user dialogue, and search queries. Around these basic data types, we build a large number of atomic tasks. Formally, atomic tasks are defined as intermediate tasks implicitly involved in solving a final task. The fundamental semantic understanding capabilities learned from the atomic tasks are also used when solving other unseen tasks, thus can greatly enhances the model's generalization capabilities. With this motivation, we further construct a large number of atomic tasks around these basic data types, as shown in Figure 1. Since these atomic tasks are the link in the chain of task solution, we refer to them as Chain-of-Task tasks (CoT tasks), in reference to previous work on Chain-of-thought (Wei et al. 2022;Wang et al. 2022a). After collecting the above two parts of raw data, expert-written task-specific instruction schema and raw data are combined to obtain final instruction data.\n\nBy training the backbone model BLOOMZ with EcomInstruct, we developed the instruction-following LLM EcomGPT for E-commerce. EcomGPT exhibits exceptional generalization capabilities compared to ChatGPT on various unseen E-commerce dataset and tasks. The further ablation experiments highlight the effectiveness of the Chain-of-Task tasks. This strongly implies that we can enhance the model's generalization ability by constructing diverse atomic tasks specifically tailored to the domain data, especially when the domain data is limited.\n\nIn summary, the contributions of this work are threefold:\n\nLang. Task Para. # task # train inst. # test inst. CLS  15  130,596  34,189  Ext  15  82,397  47,284  Gen  22  353,486  96,585  Other  10  61,756  36,481   ZH   CLS  18  324,062  362,845  Ext  9  131,814  54,725  Gen  37  444,503  353,486  Other  8  111,814  36,481   ALL  134  1,533,300  1,023,076   Table 2: Statistics for EcomInstruct.\n\n\n"}]