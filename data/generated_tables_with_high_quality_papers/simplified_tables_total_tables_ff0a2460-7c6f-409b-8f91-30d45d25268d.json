{"pap_to_tab": {"Main focus of the paper": {"paper_1": ["Developing high-quality vector representations of text and code through contrastive pre-training on unsupervised data at scale."], "paper_2": ["Investigating the scalability of dual encoder models in the size while keeping the bottleneck layer fixed, to improve multi-domain retrieval tasks performance."], "paper_3": ["Creating a general-purpose task-aware retrieval system using multi-task instruction tuning that follows human instructions for better document retrieval."], "paper_4": ["Introducing the Differentiable Search Index (DSI) where information retrieval is accomplished with a single Transformer that encodes all information into its parameters."], "paper_5": ["Finding that large language models can act as built-in autoregressive search engines to directly generate URLs for document retrieval, without explicit training for mapping questions to document identifiers."]}, "Novel approach or model": {"paper_1": ["Contrastive pre-training model for both text and code embeddings."], "paper_2": ["Generalizable T5-based dense Retrievers (GTR) with a scaled-up dual encoder model."], "paper_3": ["Task-aware retrieval system (TART) trained on BERRI with multi-task instruction tuning."], "paper_4": ["Differentiable Search Index (DSI), a text-to-text model that serves as a search index."], "paper_5": ["Method of leveraging large language models as autoregressive search engines by generating Web URLs conditioned on queries."]}, "Datasets or benchmarks for evaluation": {"paper_1": ["MSMARCO, Natural Questions, TriviaQA benchmarks."], "paper_2": ["BEIR dataset."], "paper_3": ["BEIR and LOTTE benchmarks, and a new evaluation setup, X^2-Retrieval."], "paper_4": ["Datasets are not specified; however, the model is compared with dual encoder models and a BM25 baseline."], "paper_5": ["Open-domain question answering benchmarks; specific datasets are not mentioned."]}, "Main results or findings": {"paper_1": ["The unsupervised model achieves a relative improvement over previous models in linear-probe classification accuracy and large-scale semantic search tasks."], "paper_2": ["GTR significantly outperforms previous sparse and dense retrievers, especially in out-of-domain generalization."], "paper_3": ["TART shows strong zero-shot retrieval capabilities and outperforms larger models in new evaluation setups."], "paper_4": ["DSI significantly outperforms strong baselines like dual encoder models and has strong generalization capabilities."], "paper_5": ["LLMs as search engines achieve better retrieval performance than existing approaches on several benchmarks in both zero and few-shot settings."]}, "Contribution to the field of information retrieval": {"paper_1": ["Shows that unsupervised learning can outperform fine-tuned models in text embedding tasks and establishes new benchmarks."], "paper_2": ["Demonstrates that model scaling can lead to better generalization in retrieval tasks without complex interaction layers."], "paper_3": ["Advances the concept of instruction-based retrieval and presents a new large-scale dataset collection for training and evaluation."], "paper_4": ["Presents the novel concept of a search index within the parameters of a Transformer model, simplifying retrieval processes."], "paper_5": ["Challenges the status quo by showing that pre-trained LLMs inherently have capabilities to perform retrieval tasks without domain-specific training."]}}, "cc_to_tab": {"Focus of Research": {"paper_1": ["Improvement of text and code embeddings using contrastive pre-training."], "paper_2": ["Impact of scaling up model size for better retrieval performance, particularly for out-of-domain generalization."], "paper_3": ["Development of a task-aware retrieval system that utilizes multi-task instruction tuning."], "paper_4": ["Encoding corpus information into a Transformer model's parameters for retrieval."], "paper_5": ["Large language models as inherent autoregressive search engines, generating URLs for document retrieval."]}, "Approach for Embedding or Retrieval": {"paper_1": ["Unsupervised contrastive pre-training on data at scale for vector representations."], "paper_2": ["Multi-stage training with dual encoders and a focus on embedding size and its effect on retrieval tasks."], "paper_3": ["Multi-task instruction tuning for a retrieval system that adapts to tasks via instructions."], "paper_4": ["Differentiable Search Index (DSI) where the retrieval is done using a Transformer model without traditional query-document interactions."], "paper_5": ["Direct generation of document URLs through in-context demonstrations using large language models."]}, "Model's Generalization Capabilities": {"paper_1": ["Unsupervised embeddings performing competitively with fine-tuned models on multiple benchmarks."], "paper_2": ["Generalizable T5-based dense Retrievers (GTR) demonstrating out-of-domain generality with less data."], "paper_3": ["Task-aware retrieval system (TART) showing strong generalization in zero-shot retrieval benchmarks."], "paper_4": ["DSI outperforming strong baselines in a zero-shot setup."], "paper_5": ["LLMs demonstrate better retrieval performance under both zero and few-shot settings."]}, "Improvements Over Previous Studies": {"paper_1": ["Relative improvement in linear-probe classification and semantic search over previous unsupervised methods."], "paper_2": ["Significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization."], "paper_3": ["Advancements in state of the art on zero-shot retrieval benchmarks."], "paper_4": ["Significant outperformance over dual encoder models and a BM25 baseline in zero-shot setup."], "paper_5": ["Consistently achieves better retrieval performance than existing retrieval approaches on benchmarks."]}, "Training Strategies": {"paper_1": ["Trains on unsupervised data at scale."], "paper_2": ["Scaling up the model size with multi-stage training, analyzing bottleneck layer."], "paper_3": ["Using multi-task instruction tuning for adaptation to new retrieval tasks."], "paper_4": ["Training procedure variations, representing documents and identifiers, and model-corpus size interplay."], "paper_5": ["Models generate document identifiers without being explicitly trained for mapping questions."]}, "Adaptability and Flexibility in Retrieval Systems": {"paper_1": ["Not explicitly discussed."], "paper_2": ["Generality across domains but not focused on adaptability to new tasks."], "paper_3": ["Strong adaptability to new retrieval tasks via human-provided instructions."], "paper_4": ["Retrieval simplification with corpus data integrated in model parameters, indirect adaptability."], "paper_5": ["Retrieval capabilities inherent to LLMs without the need for direct task-specific adaptability."]}, "Utilization of Bottleneck Layers in Dual Encoders": {"paper_1": ["Not discussed."], "paper_2": ["Evaluates the significance of the bottleneck layer and its impact on retrieval performance."], "paper_3": ["Not discussed."], "paper_4": ["Does not use traditional dual-encoder approaches, hence bypassing the use of bottleneck layers."], "paper_5": ["Challenges the dual-encoder architecture with an autoregressive approach, not relying on bottleneck layers."]}}, "multi_scheme": {"TART's performance in X^2-Retrieval": {"paper_3": ["TART significantly outperforms competitive baselines"]}, "TART's adaptability to new retrieval tasks": {"paper_3": ["TART shows strong capabilities to adapt to a new retrieval task via instructions"]}, "Assistance of human-written instructions in TART": {"paper_3": ["Human-written instructions help in guiding TART to find the best documents for a given query"]}, "Method for developing a task-aware retrieval system": {"paper_3": ["Multi-task instruction tuning"]}, "Success rate of LLMs generating correct Web URLs": {"paper_5": ["LLMs can generate Web URLs where nearly 90% of the corresponding documents contain correct answers to open-domain questions"]}, "Claim about dual encoders and out-of-domain generalization": {"paper_2": ["Scaling up the size of the dual encoder model improves out-of-domain generalization"]}, "Information retrieval using DSI": {"paper_4": ["DSI learns a text-to-text model mapping string queries directly to relevant docids using only its parameters"]}, "Training strategy for dual encoder model": {"paper_2": ["Multi-stage training"]}, "Use of the same model architecture for different use cases": {"paper_1": ["Contrastive pre-training on unsupervised data at scale leads to high quality embeddings for different use cases"]}, "Limitation belief of dual encoders in retrieval tasks": {"paper_2": ["The bottleneck layer is too limited as it uses the dot-product between query vector and passage vector"]}, "Improvement over previous models in linear-probe classification": {"paper_1": ["Best unsupervised model achieves a relative improvement of 4% over previous best unsupervised models and 1.8% over supervised models"]}, "Dataset for evaluating out-of-domain generalizability": {"paper_2": ["BEIR dataset"], "paper_3": ["BEIR dataset and LOTTE"]}, "Differentiable Search Index (DSI)": {"paper_4": ["DSI is a Transformer-based model embedding all information about the corpus in its parameters"]}, "Benefits of guiding retrieval with instructions": {"paper_3": ["Instructions help TART to better adapt and perform in diverse domains and tasks"]}, "Single unsupervised model achieving state-of-the-art results": {"paper_1": ["Unsupervised text embeddings achieve new state-of-the-art results in classification and semantic search"]}, "Performance improvement by scaling up dual encoder models": {"paper_2": ["Scaling up the model size significantly improves performance on a variety of retrieval tasks"]}, "Benefit of contrastive pre-training for code embeddings": {"paper_1": ["20.8% relative improvement in code search over prior best work"]}, "Open-domain question answering benchmarks for evaluation": {"paper_5": ["Evaluated on three open-domain question answering benchmarks"]}, "Comparison of unsupervised text embeddings to fine-tuned models": {"paper_1": ["Unsupervised embeddings sometimes perform competitively with fine-tuned models"]}, "Impact of increasing embedding size in dual encoders": {"paper_2": ["Demonstrate diminishing improvement when scaling up the embedding size"]}}, "ours_final_table": {"problem tackled by each paper": {"paper_1": ["Improving vector representations of text and code through contrastive pre-training on a large scale of unsupervised data, leading to enhancements in tasks such as semantic search and text similarity."], "paper_2": ["The challenge of dual encoders' limited generalizability in domain transfer for retrieval tasks and examining the impact of scaling model size and bottleneck layer dimension."], "paper_3": ["Developing a general-purpose, task-aware retrieval system which can understand user intent expressed in instructions and perform retrieval across multiple domains and tasks."], "paper_4": ["Introducing a retrieval paradigm where all information about the corpus is encoded in the parameters of a single Transformer, creating a Differentiable Search Index (DSI) that maps string queries to document IDs."], "paper_5": ["Investigating the inherent ability of large language models (LLMs) to act as autoregressive search engines to retrieve documents directly based on questions and in-context demonstrations without explicit training."]}, "approach proposed": {"paper_1": ["Contrastive pre-training on unsupervised data to create high quality vector representations of text and code."], "paper_2": ["Scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product, multi-stage training for generalizable retrieval."], "paper_3": ["Multi-task instruction tuning for task-aware retrieval system that uses human-written instructions to find the best documents for a given query."], "paper_4": ["Differentiable Search Index (DSI), a text-to-text model that maps string queries directly to relevant document identifiers using only its parameters."], "paper_5": ["Using large language models as built-in autoregressive search engines to generate URLs for document retrieval based on {Query-URL} pairs given as in-context demonstrations."]}, "Contrastive pre-training": {"paper_1": ["Trains models on unsupervised data at scale, resulting in high-quality vector representations of text and code, achieving state-of-the-art results in linear-probe classification and semantic search."], "paper_2": ["Does not directly discuss contrastive pre-training, but introduces large dual encoders with a single dot-product bottleneck that demonstrates improved retrieval tasks performance and out-of-domain generalization."], "paper_3": ["Focuses on task-aware retrieval using multi-task instruction tuning rather than contrastive pre-training."], "paper_4": ["Introduces Differentiable Search Index (DSI) which uses a single Transformer model to achieve information retrieval, without specific mention of contrastive pre-training."], "paper_5": ["Explores how large language models can generate document retrieval URLs without being explicitly trained for this task, but does not specifically address contrastive pre-training for embeddings."]}, "Code embeddings": {"paper_1": ["Embeddings trained on (text, code) pairs with contrastive methods achieve a relative improvement over previous methods on code search benchmarks."], "paper_2": ["Does not specifically discuss code embeddings in the context of contrastive pre-training."], "paper_3": ["Does not specifically discuss code embeddings in the context of contrastive pre-training."], "paper_4": ["Does not specifically discuss code embeddings, but introduces a full-text model approach for retrieval which could potentially be adapted for code."], "paper_5": ["Does not specifically discuss code embeddings in the context of contrastive pre-training."]}, "High-quality vector representations": {"paper_1": ["Obtained via contrastive pre-training on a large scale unsupervised dataset."], "paper_2": ["Discusses the generalization of large dual encoders as retrievers but does not detail the method for achieving high-quality vector representations for contrastive pre-training."], "paper_3": ["Achieved through a multi-task instruction-tuned retrieval system rather than contrastive pre-training."], "paper_4": ["Achieved through a Transformer-based Differentiable Search Index."], "paper_5": ["Suggests that large language models inherently contain high-quality vector representations enabling retrieval tasks without traditional contrastive pre-training."]}, "Quality of vector representations": {"paper_1": ["High quality vector representations of text and code"], "paper_2": [], "paper_3": [], "paper_4": [], "paper_5": []}, "Performance in benchmarks and tasks": {"paper_1": ["Relative improvement on linear-probe classification", "Impressive semantic search capabilities"], "paper_2": ["Significant improvement on a variety of retrieval tasks", "Effective out-of-domain generalization"], "paper_3": ["Advances the state of the art on two zero-shot retrieval benchmarks", "Strong capabilities to adapt to new retrieval tasks via instructions"], "paper_4": ["DSI significantly outperforms strong baselines such as dual encoder models"], "paper_5": ["Better retrieval performance than existing retrieval approaches on three open-domain question answering benchmarks"]}, "Improvements over previous models": {"paper_1": ["4% and 1.8% relative improvement over unsupervised and supervised models respectively"], "paper_2": ["GTR outperforms previous sparse and dense retrievers significantly"], "paper_3": ["Outperform models up to three times larger"], "paper_4": ["Outperforming a BM25 baseline in a zero-shot setup"], "paper_5": ["Achieve better retrieval performance than existing retrieval approaches by a significant margin"]}, "Efficiency or data requirements for training": {"paper_1": [], "paper_2": ["Very data efficient, only needs 10% of MS Marco supervised data"], "paper_3": ["Developed using multi-task instruction tuning"], "paper_4": [], "paper_5": []}, "Generalization to other domains or tasks": {"paper_1": [], "paper_2": ["Especially for out-of-domain generalization"], "paper_3": ["Strong capabilities to adapt to a new retrieval task via instructions"], "paper_4": ["Strong generalization capabilities"], "paper_5": ["Consistently achieve better retrieval performance under both zero and few-shot settings"]}, "Simplification of the retrieval process": {"paper_1": [], "paper_2": [], "paper_3": [], "paper_4": ["A DSI model answers queries directly using only its parameters, simplifying the whole retrieval process"], "paper_5": ["LLMs can be thought of as built-in search engines"]}, "datasets": {"paper_1": ["unspecified large-scale unsupervised data", "MSMARCO", "Natural Questions", "TriviaQA"], "paper_2": ["BEIR dataset", "MS Marco"], "paper_3": ["BERRI", "BEIR", "LOTTE"], "paper_4": ["unspecified variations in corpus sizes"], "paper_5": ["Open-domain question answering benchmarks"]}, "evaluation metrics": {"paper_1": ["linear-probe classification accuracy", "semantic search performance"], "paper_2": ["retrieval task performance", "out-of-domain generalization"], "paper_3": ["zero-shot retrieval benchmarks", "X^2-Retrieval setup"], "paper_4": ["text-to-text model query mapping accuracy", "zero-shot performance comparison with BM25"], "paper_5": ["retrieval performance", "zero and few-shot performance settings"]}, "focus on creating": {"Paper_1": ["high quality vector representations of text and code using contrastive pre-training"], "Paper_2": ["large-scale, generalized dual encoder models for diverse retrieval tasks"], "Paper_3": ["a general-purpose task-aware retrieval system using multi-task instruction tuning"], "Paper_4": ["a Differentiable Search Index as a text-to-text model mapping queries to relevant docids"], "Paper_5": ["using large language models as built-in search engines to generate URLs for document retrieval"]}, "method for creating embeddings": {"paper_1": ["Contrastive pre-training on unsupervised data at scale"], "paper_2": ["Scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product"], "paper_3": ["Multi-task instruction tuning for a task-aware retrieval system"], "paper_4": ["Transformer model encoding information as parameters for a Differentiable Search Index (DSI)"], "paper_5": ["Autoregressive generation of document identifiers using large language models with Query-URL pairs as in-context demonstrations"]}, "embeddings": {"paper_1": ["contrastive pre-trained text and code embeddings"], "paper_2": ["dual encoder embeddings with a fixed-size bottleneck layer (GTR)"], "paper_3": ["task-aware retrieval system embeddings (TART) with multi-task instruction tuning"], "paper_4": ["Transformer Memory embeddings encoded in the parameters for retrieval (Differentiable Search Index)"], "paper_5": ["autoregressive large language model embeddings for document retrieval"]}, "main challenge addressed in this research": {"paper_1": ["Contrastive pre-training on unsupervised data for high quality vector representations of text and code, to enhance semantic search and classification tasks."], "paper_2": ["Generalization of dual encoders to other domains for retrieval tasks, despite using a bottleneck layer with a fixed dot-product size."], "paper_3": ["Retrieval with explicit user instructions to adapt to diverse domains and tasks in a multi-task instruction-tuned system."], "paper_4": ["Achieving information retrieval solely with a Transformer model's parameters, proposing a Differentiable Search Index to simplify the retrieval process."], "paper_5": ["Replacing dual-encoder retrievers with autoregressive mechanisms in large language models to generate document identifiers for effective retrieval."]}, "Issue Focus": {"paper_1": ["Contrastive pre-training leads to high-quality vector representations of text and code, and the model's performance on linear-probe classification and semantic search tasks."], "paper_2": ["Scaling up the size of dual encoder models leads to significant improvement in retrieval tasks and out-of-domain generalization, questioning the efficacy of the bottleneck layer."], "paper_3": ["Developing a general-purpose task-aware retrieval system that uses multi-task instruction tuning to adapt to new retrieval tasks via instructions and evaluates it on a novel collection, BERRI."], "paper_4": ["Introducing a Differentiable Search Index (DSI) that enables information retrieval using a single Transformer where all information is encoded in the model's parameters."], "paper_5": ["Large Language Models (LLMs) can function as autoregressive search engines, generating document URLs for retrieval without explicit training for mapping questions to document identifiers."]}, "core problem": {"paper_1": ["how to achieve high quality vector representations of text and code using contrastive pre-training on unsupervised data at scale"], "paper_2": ["the belief that dual encoders fail to generalize to out-of-domain retrieval tasks due to a bottleneck layer and challenging this by scaling up model size"], "paper_3": ["developing a retrieval system that adapts to user intent via multi-task instruction tuning for various retrieval tasks with instructions"], "paper_4": ["achieving information retrieval using a single Transformer with a Differentiable Search Index (DSI) that encodes information directly in the model parameters"], "paper_5": ["overcoming limitations of shallow interactions in standard dense retrievers by demonstrating large language models as built-in autoregressive search engines capable of directly generating URLs for document retrieval"]}, "Proposed Approach/Model": {"paper_3": ["TART, a multi-task retrieval system trained on BERRI with instructions"]}, "Differentiating Features/Methods": {"paper_1": ["Contrastive pre-training on unsupervised data at scale"], "paper_2": ["Dual encoders, scaled-up model size, multi-stage training"], "paper_3": ["Multi-task instruction tuning, guidance with human-written instructions for retrieval tasks"], "paper_4": ["Differentiable Search Index (DSI), transformer model encodes corpus info into parameters"], "paper_5": ["Autoregressive search capability within large language models (LLMs), using in-context demonstrations"]}, "Performance Metrics/Results": {"paper_1": ["Relative improvements on semantic search and linear-probe classification tasks"], "paper_2": ["Significant improvement for out-of-domain generalization on retrieval tasks"], "paper_3": ["Advances the state of the art on zero-shot retrieval benchmarks BEIR and LOTTE"], "paper_4": ["DSI outperforms strong baselines like dual encoder models and a BM25 baseline in zero-shot setup"], "paper_5": ["Better retrieval performance than existing approaches on open-domain QA benchmarks in zero and few-shot settings"]}, "Unique Training/Data Collection Methods": {"paper_1": ["Large-scale unsupervised data for pre-training"], "paper_2": ["Use of 10% of MS Marco supervised data for out-of-domain performance"], "paper_3": ["Introduction of BERRI, a large-scale collection of retrieval datasets with instructions"], "paper_4": ["Variations in document representation and training procedures for DSI model"], "paper_5": ["In-context demonstrations with few {Query-URL} pairs for autoregressive retrieval"]}, "Methods to enable the system to understand instructions": {"paper_1": ["Contrastive pre-training on unsupervised data"], "paper_2": ["Scaling up the size of dual encoder model with multi-stage training"], "paper_3": ["Multi-task instruction tuning"], "paper_4": ["Transformer-based Differentiable Search Index (DSI)"], "paper_5": ["Large Language Models (LLMs) following human instructions to generate URLs"]}, "Methods to use user-provided instructions": {"paper_1": ["The method is not explicitly stated for using user instructions."], "paper_2": ["Generalization of the model using the scaled bottlenecks to handle a variety of retrieval tasks"], "paper_3": ["TART, a multi-task retrieval system trained with instructions that can adapt to new retrieval tasks"], "paper_4": ["A text-to-text model maps string queries directly to relevant docids using model parameters"], "paper_5": ["In-context demonstration with {Query-URL} pairs for LLMs to generate Web URLs"]}, "datasets and benchmarks": {"paper_1": ["linear-probe classification over 7 tasks", "MSMARCO, Natural Questions, TriviaQA"], "paper_2": ["BEIR dataset"], "paper_3": ["BERRI", "BEIR", "LOTTE", "X^2-Retrieval"], "paper_4": ["Experiments with different model and corpus sizes; specific datasets not cited in abstract"], "paper_5": ["Experiments conducted on three open-domain question answering benchmarks; specific names not cited in abstract"]}, "development/contribution": {"paper_1": ["High quality vector representations of text and code via contrastive pre-training on unsupervised data, achieving state-of-the-art results in linear-probe classification and large-scale semantic search"], "paper_2": ["Scaled up dual encoder model with significant improvement on retrieval tasks and out-of-domain generalization, presenting Generalizable T5-based dense Retrievers (GTR)"], "paper_3": ["A general-purpose task-aware retrieval system (TART) using multi-task instruction tuning trained on BERRI with instructions, advancing state of the art on zero-shot retrieval benchmarks"], "paper_4": ["Differentiable Search Index (DSI), a text-to-text model that maps string queries directly to relevant docids with the retrieval information encoded in the Transformer's parameters"], "paper_5": ["Methodology for large language models to act as built-in autoregressive search engines, directly generating URLs for document retrieval without explicit training on question-document mapping"]}, "System type": {"paper_1": ["Contrastive Pre-Training for Text and Code Embeddings"], "paper_2": ["Generalizable T5-based dense Retrievers (GTR)"], "paper_3": ["Task-aware Retrieval with Instructions (TART)"], "paper_4": ["Differentiable Search Index (DSI)"], "paper_5": ["Built-in Autoregressive Search Engines in Large Language Models (LLMs)"]}, "Capability Aim": {"paper_1": ["Contrastive pre-training to produce high-quality vector representations of text and code, achieving state-of-the-art results in linear-probe classification, semantic search capabilities, and performance on code search tasks"], "paper_2": ["Scaling up dual encoders to improve retrieval task performance and out-of-domain generalization, with focus on the impact of the bottleneck layer size"], "paper_3": ["Multi-task instruction tuning to develop a task-aware retrieval system capable of understanding users' intent described in their queries and following instructions for document retrieval"], "paper_4": ["Using a single Transformer model with Differentiable Search Index (DSI) to map string queries to docids encoded in the model's parameters, and achieve information retrieval without external indices"], "paper_5": ["Utilizing large language models as built-in autoregressive search engines that generate document URLs for queries provided with in-context Query-URL demonstrations"]}, "Transformer model corpus encoding method": {"paper_1": ["Contrastive pre-training on unsupervised data at scale for generating high quality vector representations of text and code."], "paper_2": ["Scaling up the size of a dual-encoder Transformer model and multi-stage training to improve on a variety of retrieval tasks without specific mention about how the corpus is encoded in parameters."], "paper_3": ["Using multi-task instruction tuning to train a retrieval system (not focusing on Transformer parameter encoding)."], "paper_4": ["Introduces the Differentiable Search Index (DSI), where the model learns to map string queries directly to docids, encoding information about the corpus in the parameters of a single Transformer model."], "paper_5": ["Large language models (potentially Transformer-based) act as built-in autoregressive search engines, generating URLs for document retrieval without explicit training on mapping questions to document identifiers."]}, "Transformers for information retrieval": {"paper_1": ["Contrastive pre-training on unsupervised data produces high-quality vector representations of text and code, demonstrating state-of-the-art results in linear-probe classification, semantic search capabilities, and competitive performance with fine-tuned models on code search tasks."], "paper_2": ["Scaling up dual encoder model size improves retrieval tasks significantly, particularly for out-of-domain generalization, challenging the belief that a dot-product bottleneck layer is too limited."], "paper_3": ["Task-aware retrieval system (TART) trained with multi-task instruction tuning on BERRI adapts to new retrieval tasks via instructions and outscores models on zero-shot retrieval benchmarks, demonstrating the effectiveness of retrieval with instructions."], "paper_4": ["Differentiable Search Index (DSI) uses a Transformer that directly maps string queries to docIDs using its parameters alone, simplifying the retrieval process and outperforming dual encoder models and a BM25 baseline."], "paper_5": ["Large language models can function as built-in autoregressive search engines by directly generating URLs for document retrieval following instructions, achieving better retrieval performance than existing approaches on open-domain question answering benchmarks."]}, "Transformer-based approach": {"paper_1": ["Contrastive pre-training for text and code embeddings, achieves state-of-the-art results."], "paper_2": ["Scaled-up dual encoder model with multi-stage training, while keeping a bottleneck layer as a single dot-product."], "paper_3": ["TART, a multi-task retrieval system trained with instructions, adapts to new retrieval tasks."], "paper_4": ["Differentiable Search Index (DSI), a text-to-text model that maps string queries directly to relevant docids using Transformer parameters."], "paper_5": ["Large language models (LLMs) as built-in autoregressive search engines, directly generating URLs for document retrieval."]}, "Comparison to traditional information retrieval methods": {"paper_1": ["Compares unsupervised text embeddings with fine-tuned models, showing sometimes competitive performance."], "paper_2": ["Outperforms previous sparse and dense retrievers, especially in out-of-domain generalization."], "paper_3": ["Advances the state of the art on two zero-shot retrieval benchmarks, implies comparison to existing methods."], "paper_4": ["Significantly outperforms strong dual encoder models and a BM25 baseline in zero-shot setup."], "paper_5": ["Achieves better retrieval performance than existing approaches on three open-domain QA benchmarks under both zero and few-shot settings."]}}}