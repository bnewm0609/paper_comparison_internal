[{"paperid": "paper0", "title": "Improved Regularization of Convolutional Neural Networks with Cutout", "abstract": "Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results with almost no additional computational cost. We also show improved performance in the low-data regime on the STL-10 dataset.", "introduction": "\n\nIn recent years deep learning has contributed to considerable advances in the field of computer vision, resulting in state-of-the-art performance in many challenging vision tasks such as object recognition [8], semantic segmentation [11], image captioning [19], and human pose estimation [17]. Much of these improvements can be attributed to the use of convolutional neural networks (CNNs) [9], which are capable of learning complex hierarchical feature representations of images. As the complexity of the task to be solved increases, the resource utilization of such models increases as well: memory footprint, parameters, operations count, inference time and power consumption [2]. Modern networks commonly contain on the order of tens to hundreds of millions of learned parameters which provide the necessary representational power for such tasks, but with the increased representational power also comes increased probability of overfitting, leading to poor generalization.\n\nIn order to combat the potential for overfitting, several different regularization techniques can be applied, such as data augmentation or the judicious addition of noise to activations, parameters, or data. In the domain of computer vision, data augmentation is almost ubiquitous due to its ease of implementation and effectiveness. Simple image transforms such as mirroring or cropping can be applied to create new training data which can be used to improve model robustness and increase accuracy [9]. Large models can also be regularized by adding noise during the training process, whether it be added to the input, weights, or gradients. One of the most common uses of noise for improving model accuracy is dropout [6], which stochastically drops neuron activations during training and as a result discourages the co-adaptation of feature detectors.\n\nIn this work we consider applying noise in a similar fashion to dropout, but with two important distinctions. The first difference is that units are dropped out only at the input layer of a CNN, rather than in the intermediate feature layers. The second difference is that we drop out contiguous sections of inputs rather than individual pixels, as demon-strated in Figure 1. In this fashion, dropped out regions are propagated through all subsequent feature maps, producing a final representation of the image which contains no trace of the removed input, other than what can be recovered by its context. This technique encourages the network to better utilize the full context of the image, rather than relying on the presence of a small set of specific visual features. This method, which we call cutout, can be interpreted as applying a spatial prior to dropout in input space, much in the same way that convolutional neural networks leverage information about spatial structure in order to improve performance over that of feed-forward networks.\n\nIn the remainder of this paper, we introduce cutout and demonstrate that masking out contiguous sections of the input to convolutional neural networks can improve model robustness and ultimately yield better model performance. We show that this simple method works in conjunction with other current state-of-the-art techniques such as residual networks and batch normalization, and can also be combined with most regularization techniques, including standard dropout and data augmentation. Additionally, cutout can be applied during data loading in parallel with the main training task, making it effectively computationally free. To evaluate this technique we conduct tests on several popular image recognition datasets, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN. We also achieve competitive results on STL-10, demonstrating the usefulness of cutout for low data and higher resolution problems.\n\n\n"}, {"paperid": "paper1", "title": "mixup: Beyond Empirical Risk Minimization", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.", "introduction": "\n\nLarge deep neural networks have enabled breakthroughs in fields such as computer vision (Krizhevsky et al., 2012), speech recognition , and reinforcement learning (Silver et al., 2016). In most successful applications, these neural networks share two commonalities. First, they are trained as to minimize their average error over the training data, a learning rule also known as the Empirical Risk Minimization (ERM) principle (Vapnik, 1998). Second, the size of these state-of-theart neural networks scales linearly with the number of training examples. For instance, the network of Springenberg et al. (2015) used 10 6 parameters to model the 5 \u00b7 10 4 images in the CIFAR-10 dataset, the network of (Simonyan & Zisserman, 2015) used 10 8 parameters to model the 10 6 images in the ImageNet-2012 dataset, and the network of Chelba et al. (2013) used 2 \u00b7 10 10 parameters to model the 10 9 words in the One Billion Word dataset.\n\nStrikingly, a classical result in learning theory (Vapnik & Chervonenkis, 1971) tells us that the convergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural network) does not increase with the number of training data. Here, the size of a learning machine is measured in terms of its number of parameters or, relatedly, its VC-complexity (Harvey et al., 2017).\n\nThis contradiction challenges the suitability of ERM to train our current neural network models, as highlighted in recent research. On the one hand, ERM allows large neural networks to memorize (instead of generalize from) the training data even in the presence of strong regularization, or in classification problems where the labels are assigned at random . On the other hand, neural networks trained with ERM change their predictions drastically when evaluated on examples just outside the training distribution (Szegedy et al., 2014), also known as adversarial examples. This evidence suggests that ERM is unable to explain or provide generalization on testing distributions that differ only slightly from the training data. However, what is the alternative to ERM?\n\nThe method of choice to train on similar but different examples to the training data is known as data augmentation (Simard et al., 1998), formalized by the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000). In VRM, human knowledge is required to describe a vicinity or neighborhood around each example in the training data. Then, additional virtual examples can be drawn from the vicinity distribution of the training examples to enlarge the support of the training distribution. For instance, when performing image classification, it is common to define the vicinity of one image as the set of its horizontal reflections, slight rotations, and mild scalings. While data augmentation consistently leads to improved generalization (Simard et al., 1998), the procedure is dataset-dependent, and thus requires the use of expert knowledge. Furthermore, data augmentation assumes that the examples in the vicinity share the same class, and does not model the vicinity relation across examples of different classes.\n\nContribution Motivated by these issues, we introduce a simple and data-agnostic data augmentation routine, termed mixup (Section 2). In a nutshell, mixup constructs virtual training examples\nx = \u03bbx i + (1 \u2212 \u03bb)x j , y = \u03bby i + (1 \u2212 \u03bb)y j ,\nwhere (x i , y i ) and (x j , y j ) are two examples drawn at random from our training data, and \u03bb \u2208 [0, 1]. Therefore, mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets. mixup can be implemented in a few lines of code, and introduces minimal computation overhead.\n\nDespite its simplicity, mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR-100, and ImageNet-2012 image classification datasets (Sections 3.1 and 3.2). Furthermore, mixup increases the robustness of neural networks when learning from corrupt labels (Section 3.4), or facing adversarial examples (Section 3.5). Finally, mixup improves generalization on speech (Sections 3.3) and tabular (Section 3.6) data, and can be used to stabilize the training of GANs (Section 3.7). The source-code necessary to replicate our experiments is available at:\n\nhttps://coming.soon/mixup.\n\nWe conclude by exploring the connections to prior work (Section 4), as well as offering some points for discussion (Section 5).\n\n\n"}, {"paperid": "paper2", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features", "abstract": "Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (\\eg leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. {Such removal is not desirable because it leads to information loss and inefficiency during training.} We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and \\mbox{retaining} the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.", "introduction": "\n\nDeep convolutional neural networks (CNNs) have shown promising performances on various computer vision problems such as image classification [30,19,11], object de- * Correspondence to sangdoo.yun@navercorp.com  tection [29,23], semantic segmentation [1,24], and video analysis [27,31]. To further improve the training efficiency and performance, a number of training strategies have been proposed, including data augmentation [19] and regularization techniques [33,16,37]. In particular, to prevent a CNN from focusing too much on a small set of intermediate activations or on a small region on input images, random feature removal regularizations have been proposed. Examples include dropout [33] for randomly dropping hidden activations and regional dropout [2,49,32,7] for erasing random regions on the input. Researchers have shown that the feature removal strategies improve generalization and localization by letting a model attend not only to the most discriminative parts of objects, but rather to the entire object region [32,7]. While regional dropout strategies have shown improvements of classification and localization performances to a certain degree, deleted regions are usually zeroed-out [2,32] or filled with random noise [49], greatly reducing the proportion of informative pixels on training images. We recognize this as a severe conceptual limitation as CNNs are generally data hungry [26]. How can we maximally utilize the deleted regions, while taking advantage of better generalization and localization using regional dropout?\n\nWe address the above question by introducing an augmentation strategy CutMix. Instead of simply removing pixels, we replace the removed regions with a patch from another image (See Table 1). The ground truth labels are also mixed proportionally to the number of pixels of combined images. CutMix now enjoys the property that there is no uninformative pixel during training, making training efficient, while retaining the advantages of regional dropout to attend to non-discriminative parts of objects. The added patches further enhance localization ability by requiring the model to identify the object from a partial view. The training and inference budgets remain the same.\n\nCutMix shares similarity with Mixup [46] which mixes two samples by interpolating both the images and labels. Mixup has been found to improve classification, but the interpolated sample tends to be unnatural (See the mixed image in Table 1). On the other hand, CutMix overcomes the problem by replacing the image region with a image patch from another training image. Table 1 gives an overview of Mixup [46], Cutout [2], and CutMix on image classification, weakly supervised localization, and transfer learning to object detection methods. Although Mixup and Cutout enhance the ImageNet classification accuracies, they suffer from performance degradation on ImageNet localization and object detection tasks. On the other hand, CutMix consistently achieves significant enhancements across three tasks, proving its superior classification and localization ability beyond the baseline and other augmentation methods.\n\nWe present extensive evaluations of our CutMix on various CNN architectures and on various datasets and tasks. Summarizing the key results, CutMix has significantly improved the accuracy of a baseline classifier on CIFAR-100 and has obtained the state-of-the-art top-1 error 14.23%. On ImageNet [30], applying CutMix to ResNet-50 and ResNet-101 [11] has improved the classification accuracy by +2.08% and +1.70%, respectively. On the localization front, CutMix improves the performance of the weaklysupervised object localization (WSOL) task on CUB200-2011 [42] and ImageNet [30] by +5.4% and +0.9% gains, respectively. The superior localization capability is further evidenced by fine-tuning a detector and an image caption generator on CutMix-ImageNet-pretrained models; the Cut-Mix pretraining has improved the overall detection perfor-mances on Pascal VOC [5] by +1% mAP and image captioning performance on MS-COCO [22] by 2 BLEU score. CutMix also enhances the model robustness and dramatically alleviates the over-confident issue [12,21] of deep networks.\n\n\n"}, {"paperid": "paper3", "title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression", "abstract": "Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to $81.73\\%$, surpassing the previous best preprocessing-based countermeasures by $37.97\\%$ absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturbation. We further test stronger, adaptive poisoning, and show it falls short of being an ideal defense against ISS. Overall, our results demonstrate the importance of considering various (simple) countermeasures to ensure the meaningfulness of analysis carried out during the development of PAP methods.", "introduction": "\n\nThe ever-growing amount of data that is easily available online has driven the tremendous advances of deep neural networks (DNNs) (Schmidhuber, 2015;LeCun et al., 2015;He et al., 2016;Brown et al., 2020). However, online data may be proprietary or contain private information, raising concerns about unauthorized use. Perturbative availability poisons (PAPs) are recognized as a promising approach to data protection and recently a large number of PAP methods have been proposed that add perturbations to images which block training by acting as shortcuts Huang et al., 2021;Fowl et al., 2021b;a). As illustrated by Figure 1 (a)\u2192(b), the high test accuracy of a DNN model is substantially reduced by PAPs.\n\nExisting research has shown that PAPs can be compromised to a limited extent by preprocessing-based-countermeasures, such as data augmentations (Huang et al., 2021;Fowl et al., 2021b) and pre-filtering Chen et al., 2023). However, a widely adopted belief is that no approaches exist that are capable of effectively countering PAPs. Adversarial training (AT) has been proven to be a strong countermeasure Wen et al., 2023). However, it is not considered to be a practical one, since it requires a large amount of computation and also gives rise to a non-negligible trade-off in test accuracy of the clean (non-poisoned) model (Madry et al., 2018;Zhang et al., 2019). Further, AT trained with a specific L p norm is hard to generalize to other norms (Tramer & Boneh, 2019;Laidlaw et al., 2021).\n\nIn this paper, we challenge the belief that it is impossible to counter PAP methods both easily and effectively by demonstrating that they are vulnerable to simple compression. First, we categorize 12 PAP methods into three categories with respect to the surrogate models they use during poison generation: slightly-trained Huang et al., 2021;Yuan & Wu, 2021;van Vlijmen et al., 2022), fully-trained Fowl et al., 2021b;Chen et al., 2023), and surrogatefree Yu et al., 2022;. Then, we analyze perturbations/shortcuts that are learned with these methods and demonstrate that they are strongly dependent on features that are learned in different training stages of the model. Specifically, we find that the methods using a slightly-trained surrogate model prefer lowfrequency shortcuts, while those using a fully-trained model prefer high-frequency shortcuts.\n\nBuilding on this new understanding, we propose Image Shortcut Squeezing (ISS), a simple, compression-based approach to countering PAPs. As illustrated by Figure 1 (b)\u2192(c), the low test accuracy of the poisoned DNN model is restored by our ISS to be close to the original accuracy. In particular, grayscale compression is used to eliminate low-frequency shortcuts, and JPEG compression is used to eliminate high-frequency shortcuts. We also show that our understanding of high vs. low frequency can also help eliminate surrogate-free PAPs Yu et al., 2022;. Our ISS substantially outperforms previously studied data augmentation and prefiltering countermeasures. ISS also achieves comparable results to adversarial training and has three main advantages: 1) generalizability to multiple L p norms, 2) efficiency, and 3) low trade-off in clean model accuracy (see Section 4.2 for details).\n\nWe further test the performance of ISS against potentially stronger PAP methods that are aware of ISS and can be adapted to it. We show that they are not ideal against our ISS. Overall, we hope our study can inspire more meaningful analyses of PAP methods and encourage future research to evaluate various (simple) countermeasures when developing new PAP methods.\n\nIn sum, we make the following main contributions:\n\n\u2022 We identify the strong dependency of the perturbation frequency patterns on the nature of the surrogate model. Based on this new insight, we show that 12 existing perturbative availability poison (PAP) methods are indeed very vulnerable to simple image compression.\n\n\u2022 We propose Image Shortcut Squeezing (ISS), a simple yet effective approach to countering PAPs. ISS applies image compression operations, such as JPEG and grayscale, to poisoned images for restoring the model accuracy.\n\n\u2022 We demonstrate that ISS outperforms existing data augmentation and pre-filtering countermeasures by a large margin and is comparable to adversarial training but is more generalizable to multiple L p norms and more efficient.\n\n\u2022 We explore stronger, adaptive poisons against our ISS and provide interesting insights into understanding PAPs, e.g., about the model learning preference of different perturbations.\n\n\n"}, {"paperid": "paper4", "title": "The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models", "abstract": "Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data\"unexploitable.\"In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over.", "introduction": "\n\nNeural networks have achieved great success in various areas of computer vision including object detection [22,12], semantic segmentation [72,34], and photo-realistic image/video generation [29,9,48]. While the efforts of the community in the development of such models cannot be undermined, this unparalleled success would have been impossible without the abundance of data resources available today [7,31,44,33]. In this regard, social media, and the internet in general, provides a platform that can be crawled easily to create massive datasets. This capability can act both as a blessing and a curse: while the collected data can facilitate learning larger, more accurate neural networks, the users might lose control over protecting their personal data from being exploited. This issue has raised increasing concerns about misuse of personal data [25,24,4].\n\nRecently, there has been an increasing number of studies on hindering the unauthorized use of personal data for neural network image classifiers [13,28,66,15,16,65,56,45]. These methods tend to add an imperceptible amount of noise to the clean images so that while the data has the same appearance as the ground-truth, it cannot provide any meaningful patterns for the neural networks to learn. As a result, such approaches, collectively known as availability attacks [3], claim that personal image data can be made unexploitable for the neural networks [28,65]. While there has been an abundance of research for designing better availability attacks, far too little attention has been paid to counter-attacks that might be employed by adversaries to break such precautionary measures.\n\nUnfortunately, the assumptions of existing availability attacks are far too weak to make the data unexploitable. Going back to our real-world example of users sharing their photos over their social media, one can clearly see that once the the data has been released, it can still be exploited for training a neural network. To clarify the real-world application of this regime, consider a corporate entity that aims to train face recognition models by crawling over social media without the consent of the users. While this unauthorized entity might not have unprotected versions of a particular person's image from his/her social media, they can have a large pre-trained model representing a facial image distribution. Given this threat model, shown in Figure 1, we aim to show that achieving this goal is indeed plausible.\n\nTo this end, we show that pre-trained density estimators are powerful tools that can be used to defuse the effects of the data-protecting perturbations, eventually enabling us to exploit protected data. We utilize the power of diffusion models in representing the image data distributions to show that reverse-engineering unexploitable data is easier than what is thought. In particular, given a training  Figure 1: The threat model considered in this paper. Availability attacks cannot guarantee to protect all the data that exists over the web. A data exploiter might use large density estimators to defuse the data-protecting perturbations and still be able to exploit the data. dataset, we first diffuse the images by adding a controlled amount of Gaussian noise following the forward process of a pre-trained diffusion model. Then, we denoise the noisy images using the reverse process of the aforementioned model, resulting in a dataset purified from data-protecting perturbations. Theoretically, using contraction properties of stochastic difference equations we prove that the number of diffusion steps required to cancel the data-protecting perturbations is directly influenced by its norm. We also empirically show that our approach is surprisingly powerful, being able to deliver the state-of-the-art (SOTA) performance against a wide variety of recent availability attacks. Our results demonstrate that in the era of diffusion models, protected personal data might only be an illusion. Thus, our findings indicate the fragility of unexploitable data, calling for more research to protect personal data.\n\nOur contributions can be summarized as follows:\n\n\u2022 We introduce AVATAR as a countermeasure against data availability attacks. To the best of our knowledge, this is the first work that explores the use of diffusion models to circumvent such attacks.\n\n\u2022 We show the power of AVATAR in breaking availability attacks over five datasets, four architectures, and seven of the most recent availability attacks. AVATAR achieves the SOTA performance against availability attacks, outperforming adversarial training.\n\n\u2022 Theoretically, we show that the amount of noise needed to diffuse the data-protecting perturbation is directly related to its norm. This result indicates that achieving both goals of availability attacks (data utility and protection) at the same time might be impossible.\n\n\n"}, {"paperid": "paper5", "title": "Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks", "abstract": "Unlearnable example attacks are data poisoning techniques that can be used to safeguard public data against unauthorized use for training deep learning models. These methods add stealthy perturbations to the original image, thereby making it difficult for deep learning models to learn from these training data effectively. Current research suggests that adversarial training can, to a certain degree, mitigate the impact of unlearnable example attacks, while common data augmentation methods are not effective against such poisons. Adversarial training, however, demands considerable computational resources and can result in non-trivial accuracy loss. In this paper, we introduce the UEraser method, which outperforms current defenses against different types of state-of-the-art unlearnable example attacks through a combination of effective data augmentation policies and loss-maximizing adversarial augmentations. In stark contrast to the current SOTA adversarial training methods, UEraser uses adversarial augmentations, which extends beyond the confines of $ \\ell_p $ perturbation budget assumed by current unlearning attacks and defenses. It also helps to improve the model's generalization ability, thus protecting against accuracy loss. UEraser wipes out the unlearning effect with error-maximizing data augmentations, thus restoring trained model accuracies. Interestingly, UEraser-Lite, a fast variant without adversarial augmentations, is also highly effective in preserving clean accuracies. On challenging unlearnable CIFAR-10, CIFAR-100, SVHN, and ImageNet-subset datasets produced with various attacks, it achieves results that are comparable to those obtained during clean training. We also demonstrate its efficacy against possible adaptive attacks. Our code is open source and available to the deep learning community: https://github.com/lafeat/ueraser.", "introduction": "\n\nDeep learning has achieved great success in fields such as computer vision [13] and natural language processing [8], and the development of various fields now relies on large-scale datasets. While these datasets have undoubtedly contributed significantly to the progress of deep learning, the collection of unauthorized private data for training these models now presents an emerging concern. Recently, numerous poisoning methods [10,15,27,30,33] have been proposed to add imperceptible perturbations to images. These perturbations can form \"shortcuts\" [11,15] in the training data to prevent training and thus make the data unlearnable in order to preserve privacy. It is commonly perceived that the only effective defense against unlearnable examples are adversarial training algorithms [15,30,10]. Popular data augmentation methods such as CutOut [9], MixUp [36], and AutoAugment [5], however, have all been demonstrated to be ineffective defenses.\n\nCurrent methods of unlearnable attacks involves the specification of an p perturbation budget, where p \u2208 {2, \u221e} in general. Essentially, they constrain the added perturbation to a small -ball of p -distance from the source image, in order to ensure stealthiness of these attacks. Adversarial training defenses [20,10] Figure 1: A high-level overview of UEraser for countering unlearning poisons. Note that UEraser recovers the clean accuracy of unlearnable examples by data augmentations. The reported results are for EM [15] unlearnable CIFAR-10 with an \u221e perturbation budget of 8/255. seeks to counteract the bounded perturbations from such unlearnable attacks. However, large defensive perturbations comes with significant accuracy degradations. This prompts the inquiry of the existence of effective defense mechanisms that leverage threat models that are outside the purview of attackers. Specifically, can we devise effective adversarial policies for training models that extend beyond the confines of the p perturbation budgets?\n\nIn this paper, we thus propose UEraser, which performs error-maximizing data augmentation, to defense against unlearning poisons. UEraser challenges the preconception that data augmentation is not an effective defense against unlearning poisons. UEraser expands the perturbation distance far beyond traditional adversarial training, as data augmentation policies do not confine themselves to the p perturbation constraints. It can therefore effectively disrupt \"unlearning shortcuts\" formed by attacks within narrow p constraints. Yet, the augmentations employed by UEraser are natural and realistic transformations extensively utilized by existing works to improve the models' ability to generalize. This, in turn, helps in avoiding accuracy loss due to perturbations used by adversarial training that could potentially be out-of-distribution. Finally, traditional adversarial training is not effective in mitigating unlearning poisons produced by adaptive attacks [10], while UEraser is highly resiliant against adaptive attacks with significantly lower accuracy reduction.\n\nIn summary, our work has three main contributions:\n\n\u2022 It extends adversarial training beyond the confines of the p perturbation budgets commonly imposed by attackers into data augmentation policies.\n\n\u2022 We propose UEraser, which introduces an effective adversarial augmentation to wipe out unlearning perturbations. It defends against the unlearnable attacks by maximizing the error of the augmented samples.\n\n\u2022 UEraser is highly effective in wiping out the unlearning effect on five state-of-the-art (SOTA) unlearning attacks, outperforming existing SOTA defense methods.\n\n\u2022 We explore the adaptive attacks on UEraser and explored additional combinations of augmentation policies. It lays a fresh foundation for future competitions among unlearnable example attack and defense strategies.\n\nUnlearnable example attacks bear great significance, not just from the standpoint of privacy preservation, but also as a form of data poisoning attack. It is thus of great significance to highlight the shortcomings of current attack methods. Perhaps most surprisingly, even a well-known unlearnable attack such as EM [15] is unable to impede the effectiveness of UEraser. By training a ResNet-18 model from scratch using exclusively CIFAR-10 unlearnable data produced with EM (with an \u221e budget of 8/255), UEraser achieves exceptional accuracy of 95.24% on the clean test set, which closely matches the accuracy achievable by standard training on a clean training set. This suggests that existing unlearning perturbations are tragically inadequate in making data unlearnable, even with adaptive attacks that employs UEraser. By understanding their weaknesses, we can anticipate how malicious actors may attempt to exploit them, and prepare stronger safeguards against such threats. We hope UEraser can help facilitate the advancement of research in these attacks and defenses.\n\n\n"}, {"paperid": "paper6", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.", "introduction": "\n\nRecent breakthroughs in computer vision and speech recognition are bringing trained classifiers into the center of security-critical systems. Important examples include vision for autonomous cars, face recognition, and malware detection. These developments make security aspects of machine learning increasingly important. In particular, resistance to adversarially chosen inputs is becoming a crucial design goal. While trained models tend to be very effective in classifying benign inputs, recent work [22,7,14,20] shows that an adversary is often able to manipulate the input so that the model produces an incorrect output.\n\nThis phenomenon has received particular attention in the context of deep neural networks, and there is now a quickly growing body of work on this topic [6,12,16,18,23,21,25]. Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability [22,7,14,20,13]. This holds even when the benign example was classified correctly, and the change is imperceptible to a human. Apart from the security implications, this phenomenon also demonstrates that our current models are not learning the underlying concepts in a robust manner. All these findings raise a fundamental question:\n\n\n"}, {"paperid": "paper7", "title": "APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses", "abstract": "The efficacy of availability poisoning, a method of poisoning data by injecting imperceptible perturbations to prevent its use in model training, has been a hot subject of investigation. Previous research suggested that it was difficult to effectively counteract such poisoning attacks. However, the introduction of various defense methods has challenged this notion. Due to the rapid progress in this field, the performance of different novel methods cannot be accurately validated due to variations in experimental setups. To further evaluate the attack and defense capabilities of these poisoning methods, we have developed a benchmark -- APBench for assessing the efficacy of adversarial poisoning. APBench consists of 9 state-of-the-art availability poisoning attacks, 8 defense algorithms, and 4 conventional data augmentation techniques. We also have set up experiments with varying different poisoning ratios, and evaluated the attacks on multiple datasets and their transferability across model architectures. We further conducted a comprehensive evaluation of 2 additional attacks specifically targeting unsupervised models. Our results reveal the glaring inadequacy of existing attacks in safeguarding individual privacy. APBench is open source and available to the deep learning community: https://github.com/lafeat/apbench.", "introduction": "\n\nRecent advancements of deep neural networks (DNNs) [14,20,34] heavily rely on the abundant availability of data resources [4,18,31]. However, the unauthorized collection of large-scale data through web scraping for model training has raised concerns regarding data security and privacy. In response to these concerns, a new paradigm of practical and effective data protection methods has emerged, known as availability poisoning attacks (APA) [7,8,9,13,13,16,30,30,33,36,39,40,41], or unlearnable example attacks. These poisoning methods inject small perturbations into images that are typically imperceptible to humans, creating \"shortcuts\" [10] in the training process that hinder the model's ability to learn the original features of the images. Recently, the field of deep learning has witnessed advancements in defense strategies [6,16,22,28] that hold the potential to challenge APAs, thereby undermining their claimed effectiveness and robustness. These defenses reveal the glaring inadequacy of existing APAs in safeguarding individual privacy in images. Consequently, we anticipate an impending arms race between attack and defense strategies in the near future.\n\nHowever, evaluating the performance of these new methods across diverse model architectures and datasets poses a significant challenge due to variations in experimental settings of recent literatures. In addition, researchers face the daunting task of staying abreast of the latest methods and assessing the effectiveness of various competing attack-defense combinations. This could greatly hamper the development and empirical exploration of novel attack and defense strategies.\n\nTo tackle this challenge, we propose the APBench, a benchmark specifically designed for availability poisoning attacks and defenses. It involves implementing poisoning attack and defense mechanisms under standardized perturbations and training hyperparameters, in order to ensure fair and reproducible comparative evaluations. APBench comprises a range of availability poisoning attacks and defense algorithms, and commonly-used data augmentation policies. This comprehensive suite allows us to evaluate the effectiveness of the poisoning attacks thoroughly.\n\nOur contributions can be summarized as follows:\n\n\u2022 An open source benchmark for state-of-the-art availability poisoning attacks and defenses, including 9 supervised and 2 unsupervised poisoning attack methods, 8 defense strategies and 4 common data augmentation methods. \u2022 We conduct a comprehensive evaluation of competitions between pairs of poisoning attacks and defenses. \u2022 We conducted experiments across 4 publicly available datasets, and also extensively examined scenarios of partial poisoning, increased perturbations, the transferability of attacks to 4 different DNN models under various defenses, and unsupervised learning. We provide visual evaluation tools such as t-SNE, Shapley value map and Grad-CAM to qualitatively analyze the impact of poisoning attacks.\n\nThe aim of APBench is to serve as a catalyst for facilitating and promoting future advancements in both availability poisoning attack and defense methods. By providing a platform for evaluation and comparison, we aspire to pave the way for the development of future availability poisoning attacks that can effectively preserve utility and protect privacy.\n\n2 Related Work\n\n\n"}]