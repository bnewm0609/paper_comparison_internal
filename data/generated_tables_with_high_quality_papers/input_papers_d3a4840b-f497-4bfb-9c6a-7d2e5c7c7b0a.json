[{"paperid": "paper0", "title": "Text2LIVE: Text-Driven Layered Image and Video Editing", "abstract": "We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.", "introduction": "\n\nComputational methods for manipulating the appearance and style of objects in natural images and videos have seen tremendous progress, facilitating a variety of editing effects to be achieved by novice users. Nevertheless, research in this area has been mostly focused in the Style-Transfer setting where the target appearance is given by a reference image (or domain of images), and the original image is edited in a global manner [16]. Controlling the localization of the edits typically involves additional input guidance such as segmentation masks. Thus, appearance transfer has been mostly restricted to global artistic stylization or to specific image domains or styles (e.g., faces, day-to-night, summer-to-winter). In this work, we seek to eliminate these requirements and enable more flexible and creative semantic appearance manipulation of real-world images and videos. Inspired by the unprecedented power of recent Vision-Language models, we use simple text prompts to express the target edit. This allows the user to easily and intuitively specify the target appearance and the object/region to be edited. Specifically, our method enables local, semantic editing that satisfies a given target text prompt (e.g., Fig. 1 and Fig. 2). For example, given the cake image in Fig. 1(b), and the target text: \"oreo cake\", our method automatically locates the cake region and synthesizes realistic, high-quality texture that combines naturally with the original image -the cream filling and the cookie crumbs \"paint\" the full cake and the sliced piece in a semantically-aware manner. As seen, these properties hold across a variety of different edits.\n\nOur framework leverages the representation learned by a Contrastive Language-Image Pretraining (CLIP) model, which has been pre-trained on 400 million text-image examples [35]. The richness of the enormous visual and textual space spanned by CLIP has been demonstrated by various recent image editing methods (e.g., [2,3,11,12,33]). However, the task of editing existing objects in arbi-trary, real-world images remains challenging. Most existing methods combine a pre-trained generator (e.g., a GAN or a Diffusion model) in conjunction with CLIP. With GANs, the domain of images is restricted and requires to invert the input image to the GAN's latent space -a challenging task by itself [49]. Diffusion models [13,45] overcome these barriers but face an inherent trade-off between satisfying the target edit and maintaining high-fidelity to the original content [2]. Furthermore, it is not straightforward to extend these methods to videos. In this work, we take a different route and propose to learn a generator from a single input-image or video and text prompts.\n\nIf no external generative prior is used, how can we steer the generation towards meaningful, high-quality edits? We achieve this via the following two key components: (i) we propose a novel text-guided layered editing, i.e., rather than directly generating the edited image, we represent the edit via an RGBA layer (color and opacity) that is composited over the input. This allows us to guide the content and localization of the generated edit via a novel objective function, including text-driven losses applied directly to the edit layer. For example, as seen in Fig. 2, we use text prompts to express not only the final edited image but also a target effect (e.g., fire) represented by the edit layer. (ii) We train our generator on an internal dataset of diverse image-text training examples by applying various augmentations to the input image and text. We demonstrate that our internal learning approach serves as a strong regularization, enabling high quality generation of complex textures and semi-transparent effects.\n\nWe further take our framework to the realm of text-guided video editing. Realworld videos often consist of complex object and camera motion, which provide abundant information about the scene. Nevertheless, achieving consistent video editing is difficult and cannot be accomplished na\u00efvely. We thus propose to decompose the video into a set of 2D atlases using [18]. Each atlas can be treated as a unified 2D image representing either a foreground object or the background throughout the video. This representation significantly simplifies the task of video editing: edits applied to a single 2D atlas are automatically mapped back to the entire video in a consistent manner. We demonstrate how to extend our framework to perform edits in the atlas space while harnessing the rich information readily available in videos.\n\nIn summary, we present the following contributions:\n\n-An end-to-end text-guided framework for performing localized, semantic edits of existing objects in real-world images. -A novel layered editing approach and objective function that automatically guides the content and localization of the generated edit. -We demonstrate the effectiveness of internal learning for training a generator on a single input in a zero-shot manner. -An extension to video which harnesses the richness of information across time, and can perform consistent text-guided editing. -We demonstrate various edits, ranging from changing objects' texture to generating complex semi-transparent effects, all achieved fully automatically across a wide-range of objects and scenes.\n\n\n"}, {"paperid": "paper1", "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation", "abstract": "To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting$\\unicode{x2014}$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.", "introduction": "\n\nThe large-scale multimodal dataset [41], consisting of billions of text-image pairs crawled from the Internet, has enabled a breakthrough in Text-to-Image (T2I) generation [30,35,6,42,40]. To replicate this success in Text-to-Video (T2V) generation, recent works [42,15,18,53,47] have extended spatial-only T2I generation models to the spatio-temporal domain. These models generally adopt the standard paradigm of training on large-scale text-video datasets (e.g., WebVid-10M [2]). Although this paradigm produces promising results for T2V generation, it requires extensive training on large hardware accelerators, which is expensive and time-consuming.\n\nHumans possess the ability to create new concepts, ideas, or things by utilizing their existing knowledge and the information provided to them. For example, when presented a video with a textual description of \"a man skiing on snow\", we can imagine how a panda would ski on snow, drawing upon our knowledge of what a panda looks like. As T2I models pretrained with large-scale image-text data already capture knowledge of open-domain concepts, a intuitive question arises: can they infer other novel videos from a single video example, like humans? A new T2V generation setting is therefore introduced, namely, One-Shot Video Tuning, where only a single text-video pair is used to train a T2V generator. The generator is expected to capture essential motion information from the input video and synthesize novel videos with edited prompts.\n\nIntuitively, the key to successful video generation lies in preserving the continuous motion of consistent objects. So we make the following observations on state-of-the-art T2I diffusion models [37] that motivate our method accordingly.\n\n(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt \"a man is running on the beach\", the T2I models produce the snapshot where a man is running (not walking or jumping), albeit not necessarily in a continuous manner (the first row of Fig. 2). This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation. (2) Regarding consistent objects: Simply extending the spatial self-attention in the T2I model from one image to multiple images produces consistent content across frames. Taking \"A man is running on the beach\" ! spatial selfattention spatio-temporal attention Figure 2: Observations on pretrained T2I models: 1) They can generate still images that accurately represent the verb terms. 2) Extending spatial self-attention to spatio-temporal attention produces consistent content across frames.\n\" # $ ! \" # $ ! \" # $ ! \" # $ ! \" # $\nthe same example, when we generate consecutive frames in parallel with extended spatio-temporal attention, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions. We implement our findings into a simple yet effective method called Tune-A-Video. Our method is based on a simple inflation of state-of-the-art T2I models over spatiotemporal dimension. However, using full attention in spacetime inevitably leads to quadratic growth in computation. It is thus infeasible for generating videos with increasing frames. Additionally, employing a naive fine-tuning strategy that updates all the parameters can jeopardize the preexisting knowledge of T2I models and hinder the generation of videos with new concepts. To tackle these problems, we introduce a sparse spatio-temporal attention mechanism that only visits the first and the former video frame, as well as an efficient tuning strategy that only updates the projection matrices in attention blocks. Empirically, these designs maintain consistent objects across all frames but lack continuous motion. Therefore, at inference, we further seek structure guidance from input video through DDIM inversion, which is a reverse process of DDIM sampling [43]. With the inverted latent as initial noise, we produce temporallycoherent videos featuring smooth movement. Notably, our method is inherently compatible with exiting personalized and conditional pretrained T2I models, such as Dream-Booth [39] and T2I-Adapter [29], providing a personalized and controllable user interface.\n\nWe showcase remarkable results of Tune-A-Video across a wide range of applications for text-driven video generation (see Fig. 1). We compare our method against the stateof-the-art baselines through extensive qualitative and quantitative experiments, demonstrating its superiority. In summary, our key contributions are as follows:\n\n\u2022 We introduce a new setting of One-Shot Video Tuning for T2V generation, which eliminates the burden of training with large-scale video datasets.\n\n\u2022 We present Tune-A-Video, which is the first framework for T2V generation using pretrained T2I models.\n\n\u2022 We propose efficient attention tuning and structural inversion that significant improve temporal consistency.\n\n\u2022 We demonstrate remarkable results of our method through extensive experiments.\n\n\n"}, {"paperid": "paper2", "title": "StableVideo: Text-driven Consistency-aware Diffusion Video Editing", "abstract": "Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at \\href{https://github.com/rese1f/StableVideo}{this https URL}.", "introduction": "\n\nRecent years have witnessed significant progress in extensive computer vision tasks taken by deep learning. Nev-* The work was done when the author was with MSRA as an intern. ertheless, natural video editing, which aims at manipulating the appearance of target objects and scenes, still faces two essential challenges that are deterministic to the editing quality: the generator equipped with rich prior knowledge that consistently produces high-fidelity edited contents adhering faithfully to the original geometry of the target objects, and the propagator that disseminates the edited contents throughout the entire video while keeping highly temporal consistency.\n\nThe flourish of text-driven generative diffusion models pre-trained on large-scale image and language data [34,14,53,16,41,5] provides impressive generation quality. Several diffusion-based methods achieve good performance in image editing [2,31], but few methods have tried to apply diffusion models in video editing, since it is challenging to modify existing objects while preserving their appearance over the entire video [49,12,26]. Dreamix [27] proposes a solution to generate consistent video according to input image/video and prompts. However, it focuses more on generating smooth motions, e.g., pose and camera movements, rather than maintaining geometric consistency of the objects across time. Moreover, such video diffusion models often suffer from huge computing complexity which is not friendly for practical applications.\n\nNeural layered atlas (NLA) [24,23] tries to tackle the temporal continuity problem by decomposing the video into a set of atlas layers, each of which describes one target object to be edited. For each atlas layer, the positions of the video are mapped into the corresponding 2D positions in it, so that semantically correspondent pixels over the whole video can be represented by the same atlas position. Instead of frame-by-frame editing, NLA edits atlas layers to ensure that the modifications can be precisely mapped back to video frames for temporal smoothness. Text2LIVE [1] provides a text-driven appearance manipulation solution of adding additional edit layers on atlases, in which a specific generator for the edit layers is trained. Although it achieves good results with strict structure preserved, it is not able to apply thorough editing. Moreover, the specifically trained generator also limits the richness of the generated contents.\n\nThis brings up the question: Could text-driven diffusion video editing achieve high temporal consistency? Intuitively, employing text-driven diffusion models to edit the atlases corresponding to the target objects could reach such goal. However, this gives rise to drawbacks rather than benefits. Being the summary of the whole video, atlases always have distorted appearance due to the viewpoint and camera movement, which are required to be specifically pretrained and generated as in [1]. Diffusion models may fail in generating satisfied atlas pixels in many cases, so that the corresponding edited frames will also be contaminated. To answer the question, we present two concepts for utilizing diffusion models in video editing. Firstly, instead of editing the atlases directly, we propose to update the atlases via editing key video frames. Secondly, we introduce temporal dependency constraints for diffusion models to generate objects with consistent appearance across time.\n\nBased on analysis above, we present a novel diffusion video editing approach, StableVideo, to perform consistency-aware video editing. In specific, we propose two effective technologies for this purpose. Firstly, to edit the objects with consistent appearance, we design an interframe propagation mechanism on top of the existing diffusion model [55], which can generate new objects with coherent geometry across time. Secondly, to achieve temporal consistency by leveraging NLA, we design an aggregation network to generate the edited atlases from the key frames. We then build up a text-driven diffusion-based framework, which provides high-quality natural video editing. We conduct extensive qualitative and quantitative experiments to demonstrate the capability of our approach. Compared with state-of-the-art methods, our approach achieves superior results with much lower complexity.\n\nIn summary, we present the following contributions:\n\n\u2022 To our best knowledge, we are the first to solve the consistency problem of diffusion video editing by considering the concept of layered atlas approaches, which provides an efficient and effective way for this topic.\n\n\u2022 We present a new video editing framework which can manipulate the appearance of the objects with high ge-ometry and appearance consistency across time. Our method can be easily applied to other text-driven diffusion models.\n\n\u2022 We conduct extensive experiments on a variety of natural videos, which shows superior editing performance compared with state-of-the-art methods.\n\n\n"}]