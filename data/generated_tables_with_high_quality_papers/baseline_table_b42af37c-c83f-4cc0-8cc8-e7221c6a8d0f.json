{"input_paper": [{"paperid": "paper0", "title": "The Konstanz natural video database (KoNViD-1k)", "abstract": "Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be \u2018general purpose\u2019 requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at \u2018in the wild\u2019 authentic distortions, depicting a wide variety of content.", "introduction": "None"}, {"paperid": "paper1", "title": "Large-Scale Study of Perceptual Video Quality", "abstract": "The great variations of videographic skills, camera designs, compression and processing protocols, and displays lead to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores, resulting in an average of 240 recorded human opinions per video. We demonstrate the value of the new resource, which we call the LIVE Video Quality Challenge Database (LIVE-VQC), by conducting a comparison of leading NR video quality predictors on it. This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores. The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html .", "introduction": "None"}, {"paperid": "paper2", "title": "YouTube UGC Dataset for Video Compression Research", "abstract": "Non-professional video, commonly known as User Generated Content (UGC) has become very popular in today's video sharing applications. However, traditional metrics used in compression and quality assessment, like BD-Rate and PSNR, are designed for pristine originals. Thus, their accuracy drops significantly when being applied on non-pristine originals (the majority of UGC). Understanding difficulties for compression and quality assessment in the scenario of UGC is important, but there are few public UGC datasets available for research. This paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel sampling method based on features extracted from encoding, challenges for UGC compression and quality evaluation are also discussed. Shortcomings of traditional reference-based metrics on UGC are addressed. We demonstrate a promising way to evaluate UGC quality by no-reference objective quality metrics, and evaluate the current dataset with three no-reference metrics (Noise, Banding, and SLEEQ).", "introduction": "None"}, {"paperid": "paper3", "title": "Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem", "abstract": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world,\"in-the-wild\"UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.", "introduction": "User-generated content (UGC) and video streaming has exploded on social media platforms such as Facebook, Instagram, YouTube, and TikTok, each supporting millions and billions of users [1]. It has been estimated that each day, about 4 billion video views occur on Facebook [2] and 1 billion hours are viewed on YouTube [3]. Given the tremendous prevalence of Internet video, it would be of great value to measure and control the quality of UGC videos, both in capture devices and at social media sites where they are uploaded, encoded, processed, and analyzed.\n\nFull-reference (FR) video quality assessment (VQA) models perceptually compare quality against pristine videos, while no-reference (NR) models involve no such comparison. Thus, NR video quality monitoring could transform the processing and interpretation of videos on * \u2020 Equal contribution \u2021 The entity that conducted all of the data collection/experimentation. smartphones, social media, telemedicine, surveillance, and vision-guided robotics, in ways that FR models are unable to. Unfortunately, measuring video quality without a pristine reference is very hard. Hence, though FR models are successfully deployed at the largest scales [4], NR video quality prediction on UGC content remains largely unsolved, for several reasons. First, UGC video distortions arise from highly diverse capture conditions, unsteady hands of content creators, imperfect camera devices, processing and editing artifacts, frame rates, compression and transmission artifacts, and the way they are perceived by viewers. Inter-mixing of distortions is common, creating complex, composite distortions that are harder to model in videos. Moreover, it is wellknown that the technical degree of distortion (e.g. amount of blur, blocking, or noise) does not correlate well with perceptual quality [5], because of neurophysiological processes that induce masking [6]. Indeed, equal amounts of distortions may very differently affect the quality of two different videos [7].\n\nSecond, most existing video quality resources are too small and unrepresentative of the complex real-world distortions [8,9,10,11,12,13,14]. While three publicly avail-able databases of authentically distorted UGC videos are available [15,16,17], they are far too small to train modern, data-hungry deep neural networks. What is needed are very large databases of videos corrupted by real-world distortions, subjectively rated by large numbers of human viewers. However, conducting large-scale psychometric studies is much harder and time-consuming (per video) than standard object or action classification tasks.\n\nFinally, although a few NR algorithms achieve reasonable performance on small databases [18,19,20,21,22,23,24], most of them fail to account for the complex spacetime distortions common to UGC videos. UGC distortions are often transient (e.g., frame drops, focus changes, and transmission glitches) and yet may significantly impact the overall perceived quality of a video [25]. Most existing models are frame-based, or use sample frame differences, and cannot capture diverse temporal impairments.\n\nWe have made recent progress towards addressing these challenges, by learning to model the relationships that exist between local and global spatio-temporal distortions and perceptual quality. We built a large-scale public UGC video dataset of unprecedented size, comprising full videos and three kinds of spatio-temporal video patches (Fig. 1), and we conducted an online visual psychometric study to gather large numbers of human subjective quality scores on them. This unique data collection allowed us to successfully learn to exploit interactions between local and global video quality perception and to create algorithms that accurately predict video quality and space-time quality maps. We summarize our contributions below:\n\n\u2022 We built the largest video quality database in existence. We sampled hundreds of thousands of open source Internet UGC digital videos to match the feature distributions of social media UGC videos. Our final collection includes 39, 000 real-world videos of diverse sizes, contents, and distortions, 26 times larger than the most recent UGC dataset [17]. We also extracted three types of v-patches from each video, yielding 117, 000 space-time video patches (\"v-patches\") in total (Sec. 3.1). \u2022 We conducted the largest subjective video quality study to date. Our final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6, 300 subjects, more than 9 times larger than any prior UGC video quality study (Sec. 3.2). \u2022 We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D video features using PaQ2PiQ [29], in parallel with 3D features using ResNet3D [30]. The 2D and 3D features feed a time series regressor [31] that learns to accurately predict both global video, as well as local spacetime v-patch quality, by exploiting the relations between them. This new model, which we call Patch VQ (PVQ) achieves top performance on the new database as well as on smaller \"in-the-wild\" databases [16,15], without finetuning (Secs. 4.1 and 5.3). \u2022 We also create another unique prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships. This second model, called the PVQ Mapper, helps localize, visualize, and act on video distortions (Sec. 5.2)."}, {"paperid": "paper4", "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild", "abstract": "Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.", "introduction": "None"}, {"paperid": "paper5", "title": "Large-Scale Video Classification with Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "introduction": "Images and videos have become ubiquitous on the internet, which has encouraged the development of algorithms that can analyze their semantic content for various applications, including search and summarization. Recently, Convolutional Neural Networks (CNNs) [15] have been demonstrated as an effective class of models for understanding image content, giving state-of-the-art results on image recognition, segmentation, detection and retrieval [11,3,2,20,9,18]. The key enabling factors behind these results were techniques for scaling up the networks to tens of millions of parameters and massive labeled datasets that can support the learning process. Under these conditions, CNNs have been shown to learn powerful and interpretable image features [28]. Encouraged by positive results in domain of images, we study the performance of CNNs in large-scale video classification, where the networks have access to not only the appearance information present in single, static images, but also their complex temporal evolution. There are several challenges to extending and applying CNNs in this setting.\n\nFrom a practical standpoint, there are currently no video classification benchmarks that match the scale and variety of existing image datasets because videos are significantly more difficult to collect, annotate and store. To obtain sufficient amount of data needed to train our CNN architectures, we collected a new Sports-1M dataset, which consists of 1 million YouTube videos belonging to a taxonomy of 487 classes of sports. We make Sports-1M available to the research community to support future work in this area.\n\nFrom a modeling perspective, we are interested in answering the following questions: what temporal connectivity pattern in a CNN architecture is best at taking advantage of local motion information present in the video? How does the additional motion information influence the predictions of a CNN and how much does it improve performance overall? We examine these questions empirically by evaluating multiple CNN architectures that each take a different approach to combining information across the time domain.\n\nFrom a computational perspective, CNNs require extensively long periods of training time to effectively optimize the millions of parameters that parametrize the model. This difficulty is further compounded when extending the connectivity of the architecture in time because the network must process not just one image but several frames of video at a time. To mitigate this issue, we show that an effective approach to speeding up the runtime performance of CNNs is to modify the architecture to contain two separate streams of processing: a context stream that learns features on low-resolution frames and a high-resolution fovea stream that only operates on the middle portion of the frame. We observe a 2-4x increase in runtime performance of the network due to the reduced dimensionality of the input, while retaining the classification accuracy.\n\nFinally, a natural question that arises is whether features learned on the Sports-1M dataset are generic enough to generalize to a different, smaller dataset. We investigate the transfer learning problem empirically, achieving significantly better performance (65.4%, up from 41.3%) on UCF-101 by re-purposing low-level features learned on the Sports-1M dataset than by training the entire network on UCF-101 alone. Furthermore, since only some classes in UCF-101 are related to sports, we can quantify the relative improvements of the transfer learning in both settings.\n\nOur contributions can be summarized as follows:\n\n\u2022 We provide extensive experimental evaluation of multiple approaches for extending CNNs into video classification on a large-scale dataset of 1 million videos with 487 categories (which we release as Sports-1M dataset) and report significant gains in performance over strong feature-based baselines.\n\n\u2022 We highlight an architecture that processes input at two spatial resolutions -a low-resolution context stream and a high-resolution fovea stream -as a promising way of improving the runtime performance of CNNs at no cost in accuracy.\n\n\u2022 We apply our networks to the UCF-101 dataset and report significant improvement over feature-based stateof-the-art results and baselines established by training networks on UCF-101 alone."}, {"paperid": "paper6", "title": "The Kinetics Human Action Video Dataset", "abstract": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.", "introduction": "In this paper we introduce a new, large, video dataset for human action classification. We developed this dataset principally because there is a lack of such datasets for human action classification, and we believe that having one will facilitate research in this area -both because the dataset is large enough to train deep networks from scratch, and also because the dataset is challenging enough to act as a performance benchmark where the advantages of different architectures can be teased apart.\n\nOur aim is to provide a large scale high quality dataset, covering a diverse range of human actions, that can be used for human action classification, rather than temporal localization. Since the use case is classification, only short clips of around 10s containing the action are included, and there are no untrimmed videos. However, the clips also contain sound so the dataset can potentially be used for many purposes, including multi-modal analysis. Our inspiration in providing a dataset for classification is ImageNet [18], where the significant benefits of first training deep networks on this dataset for classification, and then using the trained network for other purposes (detection, image segmentation, non-visual modalities (e.g. sound, depth), etc) are well known.\n\nThe Kinetics dataset can be seen as the successor to the two human action video datasets that have emerged as the standard benchmarks for this area: HMDB-51 [15] and UCF-101 [20]. These datasets have served the community very well, but their usefulness is now expiring. This is because they are simply not large enough or have sufficient variation to train and test the current generation of human action classification models based on deep learning. Coincidentally, one of the motivations for introducing the HMDB dataset was that the then current generation of action datasets was too small. The increase then was from 10 to 51 classes, and we in turn increase this to 400 classes. Table 1 compares the size of Kinetics to a number of recent human action datasets. In terms of variation, although the UCF-101 dataset contains 101 actions with 100+ clips for each action, all the clips are taken from only 2.5k distinct videos. For example there are 7 clips from one video of the same person brushing their hair. This means that there is far less variation than if the action in each clip was performed by a different person (and different viewpoint, lighting, etc). This problem is avoided in Kinetics as each clip is taken from a different video.\n\nThe clips are sourced from YouTube videos. Consequently, for the most part, they are not professionally videoed and edited material (as in TV and film videos). There can be considerable camera motion/shake, illumination variations, shadows, background clutter, etc. More im-"}], "pap_to_tab": {"What is the main focus of the research?": {"paper_1": ["Creating a large VQA database of real-world video sequences with subjective mean opinion scores to benefit the development and evaluation of VQA methods."], "paper_2": ["Conducting a large-scale study of perceptual video quality and constructing a large video quality assessment database to advance NR video quality prediction."], "paper_3": ["Introducing a large scale UGC dataset for video compression and quality assessment research, discussing its challenges, and evaluating UGC quality by no-reference metrics."], "paper_4": ["Creating the largest subjective video quality dataset and developing region-based NR VQA models to improve the prediction of video quality on UGC."], "paper_5": ["Introducing a new VQA dataset 'KonVid-150k' for videos in-the-wild and proposing new efficient VQA approaches with multi-level spatially pooled deep-features."], "paper_6": ["Providing an extensive empirical evaluation of CNNs on a new large-scale video classification dataset of YouTube videos and exploring different architectures."], "paper_7": ["Describing the DeepMind Kinetics human action video dataset and analyzing the classification performance and potential dataset biases."]}, "What type of dataset is introduced or used?": {"paper_1": ["The KoNViD-1k database consisting of 1,200 public-domain video sequences with subjective annotations."], "paper_2": ["The LIVE Video Quality Challenge Database (LIVE-VQC) containing 585 videos with a large number of subjective scores collected via crowdsourcing."], "paper_3": ["A large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos, covering popular categories and features like HDR."], "paper_4": ["A subjective video quality dataset containing 39,000 real-world distorted videos and 117,000 'v-patches', along with 5.5M human quality annotations."], "paper_5": ["KonVid-150k, a dataset of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each."], "paper_6": ["A new dataset of 1 million YouTube videos belonging to 487 classes for large-scale video classification."], "paper_7": ["The Kinetics human action video dataset containing 400 action classes, with over 400 video clips per action sourced from YouTube."]}, "How were the video sequences obtained for the dataset?": {"paper_1": ["Fairly sampled from a large public video dataset, YFCC100m."], "paper_2": ["Captured by a large number of users, with wide ranges of levels of complex, authentic distortions."], "paper_3": ["Sampled from millions of YouTube videos through a novel sampling method based on features extracted from encoding."], "paper_4": ["Collected real-world UGC video data and space-time localized video patches."], "paper_5": ["Automatically gathered from the web and coarsely annotated."], "paper_6": ["Automatically extracted from YouTube using video metadata."], "paper_7": ["Manually selected from YouTube, ensuring a variety of human actions and contexts."]}, "What are the applications of the proposed method or dataset?": {"paper_1": ["Training and validation of general-purpose VQA methods, especially useful for deep learning approaches."], "paper_2": ["Advancing no-reference video quality prediction by providing a resource for evaluating video quality predictors."], "paper_3": ["Improving video compression and quality assessment in the context of UGC by providing a dataset representative of real-world conditions."], "paper_4": ["Improving NR perceptual VQA algorithms for social and streaming media applications, localization, and visualization of perceptual distortions."], "paper_5": ["Providing a basis for developing and benchmarking VQA methods suited for videos in-the-wild."], "paper_6": ["Evaluating the performance of CNN architectures for large-scale video classification tasks and the extraction of spatio-temporal features."], "paper_7": ["Training and testing neural network architectures for human action classification and analyzing dataset imbalance and its effects."]}, "What is the scale of the dataset in terms of video count and variety?": {"paper_1": ["1,200 video sequences depicting a wide variety of content and authentic distortions."], "paper_2": ["585 videos of unique content, captured by numerous users and a wide range of authentically distorted videos."], "paper_3": ["1500 video clips representing popular categories and various features such as HDR."], "paper_4": ["A dataset containing 39,000 videos and 117,000 video patches for localized quality assessment."], "paper_5": ["153,841 videos for coarsely annotated set, and 1,596 videos for densely annotated set."], "paper_6": ["1 million YouTube videos belonging to 487 classes, providing significant variety in terms of content."], "paper_7": ["Approximately 400 video clips for each of the 400 human action classes, totaling over 160,000 clips."]}}, "cc_to_tab": {"Focus of VQA Database": {"paper_1": ["'in the wild' videos, a wide variety of content and authentic distortions"], "paper_2": ["Unique content and wide range of video impairments including complex, authentic distortions"], "paper_3": ["Video compression research for UGC, various categories like Gaming, Sports, and HDR features"], "paper_4": ["Space-time localized video patches ('v-patches'), real-world distorted videos"], "paper_5": ["Large and diverse VQA dataset with in-the-wild videos, coarsely and finely annotated"], "paper_6": ["Large-scale video classification, not focused on video quality assessment"], "paper_7": ["Human action classification, not focused on video quality assessment"]}, "Methodology for VQA": {"paper_1": ["Subjective VQA database creation"], "paper_2": ["Subjective VQA database with crowdsourced opinion scores and a study of leading NR video quality predictors"], "paper_3": ["UGC dataset sampling based on features extracted from encoding, and UGC quality evaluation"], "paper_4": ["Local-to-global NR VQA architecture, space-time video quality mapping engine"], "paper_5": ["Introduction of MLSP-VQA approaches for in-the-wild videos training at scale"], "paper_6": ["Empirical evaluation of CNNs for video classification"], "paper_7": ["Dataset for neural networks trained on human action classification"]}, "Dataset Contribution": {"paper_1": ["1,200 public-domain video sequences with subjective mean opinion scores (MOS)"], "paper_2": ["Large-scale VQA database with 585 videos and over 205,000 opinion scores"], "paper_3": ["Large scale UGC dataset (1500 20 sec clips) sampled from YouTube videos"], "paper_4": ["Largest subjective video quality dataset with 39,000 videos, 117,000 v-patches, and 5.5M quality annotations"], "paper_5": ["KonVid-150k dataset with 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each"], "paper_6": ["Dataset of 1 million YouTube videos across 487 classes for video classification"], "paper_7": ["400 human action classes with at least 400 video clips for each action"]}, "Content and Distortion Types": {"paper_1": ["Authentic distortions"], "paper_2": ["Complex, authentic distortions"], "paper_3": ["Variety of content like Gaming, Sports, HDR"], "paper_4": ["Real-world distorted videos"], "paper_5": ["VQA for authentically distorted videos in-the-wild"], "paper_6": ["Diverse YouTube video content for classification"], "paper_7": ["Human action videos"]}, "Model or Metric Proposed": {"paper_1": ["None, database for development of future models"], "paper_2": ["NR video quality predictors tested and comparison conducted"], "paper_3": ["No-reference objective quality metrics evaluation for UGC"], "paper_4": ["Local-to-global region-based NR VQA architecture (PVQ), space-time video quality mapping engine (PVQ Mapper)"], "paper_5": ["MLSP-VQA approaches with multi-level spatially pooled deep-features"], "paper_6": ["Exploration of CNN architectures for local spatio-temporal information"], "paper_7": ["Neural network architectures for human action classification"]}, "Use of Authentic or User-Generated Content": {"paper_1": ["Yes"], "paper_2": ["Yes"], "paper_3": ["Yes, with focus on UGC for compression and quality assessment"], "paper_4": ["Yes, focusing on real-world UGC imperfections"], "paper_5": ["Yes, in-the-wild VQA dataset"], "paper_6": ["Yes, 1 million YouTube videos"], "paper_7": ["Yes, YouTube video clips for human actions"]}}, "multi_scheme": {"What is the number of human perceptual quality annotations provided in the dataset mentioned in the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper?": {"paper_0": "", "paper_1": "The 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper does not provide details about the number of human perceptual quality annotations within the dataset it discusses.", "paper_2": "", "paper_3": "The dataset in the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6,300 subjects.", "paper_4": "5.5M", "paper_5": "The 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper states that the final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches.", "paper_6": "A total of 5.5 million perceptual quality judgments on videos and v-patches from almost 6,300 subjects"}, "What traditional metrics are mentioned as less accurate for non-pristine originals like UGC?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "BD-Rate and PSNR are mentioned as less accurate for non-pristine originals like UGC in the 'YouTube UGC Dataset for Video Compression Research' paper.", "paper_4": "", "paper_5": "The 'YouTube UGC Dataset for Video Compression Research' paper mentions BD-Rate and PSNR as traditional metrics that are less accurate for non-pristine originals like UGC.", "paper_6": "BD-Rate and PSNR are mentioned as less accurate for non-pristine originals like UGC."}, "What does the Spearman rank-order correlation coefficient (SRCC) indicate about the MLSP-VQA models' performance?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "The Spearman rank-order correlation coefficient (SRCC) is a statistical measure used to evaluate how well the relationship between two variables can be described using a monotonic function. In the context of the 'KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild' paper, it is used to indicate the MLSP-VQA models' performance by measuring how well the model predictions correlate with human subjective scores.", "paper_6": "The Spearman rank-order correlation coefficient (SRCC) is used to measure the predictability and performance of the MLSP-VQA models, with a higher SRCC value indicating better prediction of human perception of video quality. However, specific SRCC values and their implications for the MLSP-VQA models' performance are not provided in the given papers."}, "What SRCC values were achieved by the MLSP-VQA models on the KoNViD-1k and LIVE-Qualcomm datasets?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "The 'KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild' paper claims that the MLSP-VQA models trained on KonVid-150k set a new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively.", "paper_6": "SRCC values achieved by the MLSP-VQA models on the KoNViD-1k and LIVE-Qualcomm datasets are 0.82 and 0.64, respectively."}, "What experiments were conducted to investigate the performance under different levels of label noise and dataset size?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "The 'KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild' paper discusses investigating performance under different levels of label noise, and dataset size by examining how alternative approaches perform under these varying conditions to show that MLSP-VQA-FF is the best method for videos in-the-wild.", "paper_6": "Experiments were conducted to investigate the MLSP-VQA models' performance under different levels of label noise and dataset size, proving that MLSP-VQA-FF is the overall best method for videos in-the-wild."}, "When will the new database and prediction models from the 'Patch-VQ' paper be released according to the authors?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "", "paper_6": "The release date for the new database and prediction models from the 'Patch-VQ' paper is not specified in the provided text."}, "How are the videos in the UGC dataset sampled?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "For the 'YouTube UGC Dataset for Video Compression Research', videos in the UGC dataset are sampled using a novel sampling method based on features extracted from encoding.", "paper_4": "", "paper_5": "In the 'YouTube UGC Dataset for Video Compression Research' paper, the videos in the dataset are sampled using a novel method based on features extracted from encoding.", "paper_6": "The method of video sampling in the UGC dataset is not detailed in the provided paper introductions or abstracts."}, "What is the purpose of the PVQ Mapper mentioned in the paper?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "The purpose of the PVQ Mapper mentioned in the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper is to predict first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships, which helps localize, visualize, and act on video distortions.", "paper_4": "The PVQ Mapper is a prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships.", "paper_5": "The purpose of the PVQ Mapper, as mentioned in the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper, is to predict first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships, helping to localize, visualize, and act on video distortions.", "paper_6": "The purpose of the PVQ Mapper is to predict space-time maps of video quality by learning global-to-local quality relationships, helping to localize, visualize, and act on video distortions."}, "What are the shortcomings of traditional reference-based metrics on UGC according to the paper?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "The shortcomings of traditional reference-based metrics on UGC according to the 'YouTube UGC Dataset for Video Compression Research' paper include their significant drop in accuracy when applied to non-pristine originals which make up the majority of UGC.", "paper_4": "", "paper_5": "The 'YouTube UGC Dataset for Video Compression Research' paper and 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper both address the shortcomings of traditional reference-based metrics on UGC, which include their decreased accuracy when applied to non-pristine originals that are typical of UGC, as they were designed for pristine originals.", "paper_6": "Traditional reference-based metrics on UGC, such as BD-Rate and PSNR, are said to be less accurate for non-pristine originals and their shortcomings include not accounting for the diverse and authentic distortions found in UGC."}, "What is the performance difference between spatio-temporal networks and single-frame models reported in the paper?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "The 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper suggests that their new model achieves top performance on the new database as well as on smaller 'in-the-wild' databases without fine-tuning. However, specific performance difference values between spatio-temporal networks and single-frame models are not provided in the given introduction.", "paper_4": "", "paper_5": "The 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper seems to imply that spatio-temporal networks have better performance due to their ability to capture both spatial and temporal impairments, unlike single-frame models which may not capture diverse temporal impairments effectively.", "paper_6": "The performance difference between spatio-temporal networks and single-frame models is not provided in the given paper content."}, "On what measures is this study the largest video quality assessment study ever conducted?": {"paper_0": "", "paper_1": "This study is the largest video quality assessment study ever conducted in terms of the number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.", "paper_2": "", "paper_3": "The 'Large-Scale Study of Perceptual Video Quality\u2019 study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.", "paper_4": "Number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.", "paper_5": "According to the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper, this study is the largest ever conducted based on the number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.", "paper_6": "This study is the largest video quality assessment study ever conducted in terms of number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores."}, "To what extent does the LIVE-VQC database represent real world videos?": {"paper_0": "", "paper_1": "The LIVE-VQC database is designed to represent real world videos that contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate.", "paper_2": "", "paper_3": "The LIVE-VQC database represents real-world videos to a considerable extent as it contains 585 videos of unique content, captured by a large number of users, with a wide range of levels of complex, authentic distortions.", "paper_4": "", "paper_5": "The 'Large-Scale Study of Perceptual Video Quality' paper claims that the LIVE-VQC database contains a large variety of videos captured by numerous users under a wide range of levels of complex, authentic distortions, thereby adequately representing real world videos.", "paper_6": "The extent to which the LIVE-VQC database represents real world videos is characterized by its large and diverse collection of videos with complex, authentic distortions, but specific representational details are not provided in the given text."}, "From which public video dataset were the KoNViD-1k video sequences sampled?": {"paper_0": "YFCC100m", "paper_1": "The KoNViD-1k video sequences were sampled from the large public video dataset YFCC100m.", "paper_2": "YFCC100m", "paper_3": "The KoNViD-1k video sequences were sampled from the YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset.", "paper_4": "", "paper_5": "The 'The Konstanz natural video database (KoNViD-1k)' paper indicates that the video sequences in KoNViD-1k were sampled from the YFCC100m dataset.", "paper_6": "The KoNViD-1k video sequences were sampled from a large public video dataset, YFCC100m."}, "Was there a performance analysis based on the potential bias due to dataset imbalance?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "", "paper_6": "There is no mention of a performance analysis based on the potential bias due to dataset imbalance in the provided texts."}, "What kind of extracted features were used in the novel sampling method?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "In the 'YouTube UGC Dataset for Video Compression Research' paper, the novel sampling method used features extracted from encoding.", "paper_4": "", "paper_5": "The 'YouTube UGC Dataset for Video Compression Research' paper states that the novel sampling method used features extracted from encoding to sample videos.", "paper_6": "In the 'YouTube UGC Dataset for Video Compression Research' paper, extracted features from encoding are mentioned as part of the novel sampling method, but no specific details are provided on the types of features in the given texts."}, "What conclusions can be drawn about the performance of current NR video quality models on real-world video data?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "The 'Large-Scale Study of Perceptual Video Quality' paper infers that current NR video quality models often perform poorly when tested on real-world video data because traditional databases do not adequately represent the variations in content, imaging conditions, and authentic distortions seen in such videos.", "paper_6": "The conclusions about the performance of current NR video quality models on real-world video data are not specified in the given paper content."}, "What is the local-to-global region-based NR VQA architecture proposed in the paper?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "The local-to-global region-based NR VQA architecture proposed in the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper refers to a deep neural architecture that computes 2D video features in parallel with 3D features, which then feed a time series regressor that learns to predict both global video and local space-time v-patch quality.", "paper_4": "", "paper_5": "The 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper proposes a NR VQA architecture that computes 2D and 3D video features in parallel, feeding into a time series regressor that predicts global video quality and local space-time v-patch quality by learning relationships between local and global distortions.", "paper_6": "The local-to-global region-based NR VQA architecture is proposed to accurately predict video quality and space-time quality maps by exploiting the relations between local and global spatio-temporal distortions and perceptual quality."}, "What is the minimum number of video clips per action class in the Kinetics dataset?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "", "paper_4": "", "paper_5": "", "paper_6": "The minimum number of video clips per action class in the Kinetics dataset is not specified in the provided introduction."}, "What is the focus of the VQA methods discussed in the paper?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "The focus of the VQA methods discussed in the 'Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem' paper is on no-reference (NR) video quality prediction, particularly for user-generated content (UGC) which is highly diverse and lacks pristine reference videos for comparison.", "paper_4": "", "paper_5": "The VQA methods discussed in the papers primarily focus on assessing the quality of videos that contain a variety of real-world, authentically distorted content, specifically with no-reference methodologies in several cases.", "paper_6": "The focus of the VQA methods discussed in the paper is not explicitly stated in the provided content."}, "What is the duration of the video clips in the UGC dataset?": {"paper_0": "", "paper_1": "", "paper_2": "", "paper_3": "In the 'YouTube UGC Dataset for Video Compression Research' paper, the duration of the video clips in the UGC dataset is mentioned as 20 seconds.", "paper_4": "", "paper_5": "The 'YouTube UGC Dataset for Video Compression Research' paper mentions that the UGC dataset consists of 1500 20-second video clips sampled from YouTube videos.", "paper_6": "The duration of the video clips in the UGC dataset is mentioned as 20 seconds in the 'YouTube UGC Dataset for Video Compression Research' abstract."}}}