[{"paperid": "paper0", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks", "abstract": "We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like \u201cRinse off a mug and place it in the coffee maker.\u201d and low-level language instructions like \u201cWalk to the coffee maker on the right.\u201d ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision- and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.", "introduction": "\n\nA robot operating in a human spaces needs to connect natural language to the world. This symbol grounding [21] problem has largely focused on connecting language to static images. However, robots need to understand taskoriented language, for example \"Rinse off a mug and place it in the coffee maker\" illustrated in Figure 1.\n\nPlatforms for translating language to action have become increasingly popular, spawning new test-beds [12,3,14,41]. These benchmarks include language-driven navigation and embodied question answering, which have seen dramatic improvements in modeling thanks to environments like Matterport 3D [11,3], AI2-THOR [25], and AI Habi- 1 Paul G. Allen School of Computer Sci & Eng, Univ of Washington 2 Carnegie Mellon University LTI & Microsoft Research AI 3 Allen Institute for AI 4 NVIDIA \"walk to the coffee maker on the right\" \"wash the mug in the sink\" \"put the clean mug in the coffee maker\"\n\n\"pick up the mug and go back to the coffee maker\"\n\n\"pick up the dirty mug from the coffee maker\" \"turn and walk to the sink\"  Figure 1: ALFRED consists of 25k+ language directives corresponding to expert demonstrations on household tasks. Above, we highlight several action sequence frames corresponding to portions of the accompanying language instruction. Unlike related datasets that focus only on navigation, ALFRED requires interactions with objects, keeping track of state changes, and callbacks to previous instructions.\n\ntat [43]. However, these datasets ignore complexities arising from describing task-oriented behaviors with objects.\n\nWe introduce ALFRED, a new benchmark for connecting human language to actions, behaviors, and objects in an interactive visual environment. Expert task demonstrations are accompanied by both high-and low-level human language instructions in 120 indoor scenes in the new AI2-THOR 2.0 [25]. These demonstrations involve partial observability, long action horizons, underspecified natural language, and irreversible actions.\n\nALFRED includes 25,743 English language directives describing 8,055 expert demonstrations averaging 50 steps each, resulting in 428,322 image-action pairs. Motivated by work in robotics on segmentation-based grasping [36], agents in ALFRED interact with objects visually, specifying a pixelwise interaction mask of the target object. This TACoS [42] 17k+ High&Low Photos ---R2R [3]; Touchdown [14] 21k+; 9.3k+ Low Photos Ego Graph EQA [15] High Low Ego Discrete Matterport EQA [53] High Photos Ego Discrete IQA [20] High High Ego Discrete Discrete VirtualHome [41] 2.7k+ High&Low High 3 rd Person Discrete VSP [56] High High Ego Discrete ALFRED 25k+ High&Low High Ego Discrete Discrete + Mask Table 1: Dataset comparison. ALFRED is the first interactive visual dataset to include high-and low-level natural language instructions for object and environment interactions. TACoS [42] provides detailed high-and low-level text descriptions of cooking videos, but does not facilitate task execution. For navigation, ALFRED enables discretized, grid-based movement, while other datasets use topological graph navigation or avoid navigation altogether. ALFRED requires an agent to generate spatially located interaction masks for action commands. By contrast, other datasets only require choosing from a discrete set of available interactions and object classes or offer no interactive capability.\n\ninference is more realistic than simple object class prediction, where localization is treated as a solved problem.\n\nExisting beam-search [17,51,46] and backtracking solutions [23,28] are infeasible due to the larger action and state space, long horizon, and inability to undo certain actions.\n\nTo establish baseline performance levels, we evaluate a sequence-to-sequence model shown to be successful on vision-and-language navigation tasks [27]. This model is not effective on the complex tasks in ALFRED, achieving less than 5% success rates. For analysis, we also evaluate individual sub-goals like the routine of cooking something in a microwave. While performance is better for isolated sub-goals, the model lacks the reasoning capacity for longhorizon and compositional task planning.\n\nIn summary, ALFRED facilitates learning models that translate from language to sequences of actions and predictions of visual interaction masks for object interactions. This benchmark captures many reasoning challenges present in real-world settings for translating human language to robot actions for accomplishing household tasks. Models that can overcome these challenges will begin to close the gap towards real-world, language-driven robotics. Table 1 summarizes the benefits of ALFRED relative to other visual action datasets with language annotations. Vision & Language Navigation. In vision-and-language navigation tasks, either natural or templated language describes a route to a goal location through egocentric visual observations [30,13,12,3,14]. Since the proposal of R2R [3], researchers have dramatically improved the navigation performance of models [52,17,51,23,28] with techniques like progress monitoring [27], as well as introduced task variants with additional, on-route instructions [38,37,49]. Much of this research is limited to static environments. By contrast, ALFRED tasks include navigation, object interactions, and state changes. Vision & Language Task Completion. There are several existing benchmarks based on simple block worlds and fully observable scenes [9,33]. ALFRED provides more difficult tasks in richer, visually complex scenes, and uses partially observable environments. The CHAI benchmark [32] evaluates agents performing household instructions, but includes only a single interact action outside navigation. ALFRED has seven manipulation actions, such as pick up, turn on, and open, state changes like clean versus dirty, and variation in language and visual complexity.\n\n\n"}, {"paperid": "paper1", "title": "MQA: Answering the Question via Robotic Manipulation", "abstract": "In this paper, we propose a novel task, Manipulation Question Answering (MQA), where the robot performs manipulation actions to change the environment in order to answer a given question. To solve this problem, a framework consisting of a QA module and a manipulation module is proposed. For the QA module, we adopt the method for the Visual Question Answering (VQA) task. For the manipulation module, a Deep Q Network (DQN) model is designed to generate manipulation actions for the robot to interact with the environment. We consider the situation where the robot continuously manipulating objects inside a bin until the answer to the question is found. Besides, a novel dataset that contains a variety of object models, scenarios and corresponding question-answer pairs is established in a simulation environment. Extensive experiments have been conducted to validate the effectiveness of the proposed framework.", "introduction": "\n\nPeople have long anticipated the day when humans can ask questions to an intelligent robot directly with natural language and the robot knows to interact with the environment to respond. Imagine there is a bin in your kitchen which contains a variety of items, and you would like to know how many cans are left in it so that you can decide whether some replenishment should be done. Then you call your assistant robot in the kitchen and ask \"How many cans are there in the bin?\" Having the question well understood, the robot starts to explore the bin, where all kinds of objects may be mixed together. As some cans may be occluded by some other objects and can not be seen directly, the robot has to generate a sequence of manipulation actions to change the current scenario in order to explore the bin thoroughly. As shown in Fig.1, the robot keeps exploring the bin until all possible areas are explored and then it is able to report the answer to the user.\n\nRecently, the task of Question Answering (QA) has attracted increasing attention from many researchers worldwide. In the big family of QA research, the popular QA chatbot tries to communicate with humans by scraping the Internet or database to get the answer to the question [1]. One of the representative tasks among it is the Visual Question Answering (VQA) task [2][3] [16] [17], where the robot is required to have the ability to reason about the visual content in order to answer a question \u2020 indicates the authors with equal contributions. The authors are also with Beijing National Research Center for Information Science and Technology. Y. Deng is also with the Center of Intelligent Control and Telescience, Tsinghua Shenzhen International Graduate School, Tsinghua University. This work was jointly supported by the National Natural Science Fund for Distinguished Young Scholars (62025304), and in part by the Seed Fund of Tsinghua University (Department of Computer Science and Technology)-Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things. * Corresponding author: hpliu@tsinghua.edu.cn Fig. 1. Given a question, the manipulator explores the bin with a series of manipulation actions to find the answer. In this example, two cans are directly visible in the initial scene, while there may also be some cans occluded by other objects. Therefore, the manipulator tries to push a food box to the side and another can is revealed. After exploring all possible areas, three cans are found at last. And the robot report the answer \"Three\" to the user. about the given visual input. As a step forward to realize the natural human-robot interaction, a much more challenging task, Visual Dialog, is proposed, where the robot needs to answer a coherent series of questions to the visual content [9] [22]. However, they only try to answer the question passively from the visual input and the robot's ability to move in the environment is ignored.\n\nIn the real-world environment, the perception should never be passive but an active process [4] [7]. Considering the embodiment of intelligent agents, next emerges a body of work on Embodied Question Answering (EQA), where the mobile robot is able to actively explore the environment to find the answer to the question [10] [11]. In the EQA task, the robot needs to understand the acquired visual information and perform a series of actions accordingly to actively explore the environment to answer the question. A most important characteristic of the EQA task is that the perception and action ability of the agent are combined together. Additionally, under the large scope of EQA, Gordon et al. propose an Interactive Question Answering (IQA) task [14], which points out that besides merely navigating the environment, the robot should also be able to execute some interactive actions based on the object's affordance, such as opening the door of the refrigerator to better find the answer to the question. But they are limited EQA [10] IQA [14] MQA (Ours)  Understanding  Exploration  -Interaction  --Manipulation  ---to some simple standard actions, and lack of manipulation. In the real world environment, it is far more complex and highly unstructured. For example, in the cluttered scene, a target object may be occluded by other objects, which results in an even higher requirement on the robotic manipulation ability. To tackle this problem, we propose a novel task of Manipulation Question Answering (MQA), where the robot is required to find the answer by performing manipulation actions to actively explore the environment, rather than simply doing some predefined actions for the interaction. A comprehensive comparison of VQA, EQA, IQA, and the proposed MQA tasks is illustrated in TABLE I. It can be seen that the VQA task only requires the agent to have the ability to understand the environment. Comparing to VQA task, the EQA task makes a big improvement by further leveraging the embodiment ability of the agent. The agent needs to explore the environment to find the answer. IQA task is an extension of the EQA task which also allows the interaction with the environment. And the proposed MQA contains all the characteristics of aforementioned tasks. Additionally, in the MQA task, the agent can perform manipulation actions to change the environment in order to answer the given question. Meanwhile, the MQA task we proposed poses several new challenges. First, the robot is expected to perform manipulation actions to change the environment in order to find the answer, instead of merely referencing the static environment. And then, a new set of metrics is required to evaluate this new task as currently available research lacks quantitative accuracy metrics and benchmarks for the proposed task. Besides, there is no existing dataset suitable for our MQA task.\n\nIn response to these challenges, we proposed a framework that integrates a QA module and a manipulation module to accomplish the newly defined MQA task. The contributions of this paper can be summarized as follows:\n\n\u2022 We formulate a novel Manipulation Question Answering (MQA) task and a solution framework is built to solve it. \u2022 We design a Deep Q Network (DQN) for the robot to effectively generate manipulations for the MQA task. \u2022 We build a novel MQA dataset including a variety of object models, bin scenarios and question-answer pairs. A corresponding benchmark is also established. The organization of this paper is as follows. The related work is introduced in Section II. We describe the MQA task in Section III. Section IV includes the establishment of the MQA dataset and its analysis. The proposed MQA framework is presented in Section V. Experimental results and analysis are demonstrated in Section VI. Finally, we come to the conclusion of the paper.\n\n\n"}, {"paperid": "paper2", "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks", "abstract": "General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments and objects. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.", "introduction": "\n\nA LONG-STANDING goal for robotics and embodied agents is to build systems that can perform tasks specified in natural language. Concepts expressed in natural language provide humans with an intuitive way to represent, summarize, and abstract diverse knowledge skills. By means of abstraction, concepts such as \"open the drawer and push the middle object into the drawer\" can be extended to a potentially infinite set of new and unseen entities. Additionally, humans leverage concepts to describe complex tasks as sequences of natural language instructions. This stands in contrast to current robots, which typically lack this generalization ability and learn individual tasks one at a time. Moreover, multi-task learning approaches traditionally assume that tasks are specified to the agent at test time via mechanisms such as goal images [1] and one-hot skill selectors [2], [3] that are not practical for non-expert users to instruct robots in everyday real-world settings. As robots become ubiquitous across human-centered environments the need for intuitive task specification grows: how can we scale robot learning systems to autonomously acquire general-purpose knowledge that allows them to compose long-horizon tasks by following unconstrained language instructions?\n\nTo address this problem we present CALVIN, a new opensource simulated benchmark that links human language to robot motor skills, behaviors, and objects in interactive visual environments. In this setting, a single agent must solve complex manipulation tasks by understanding a series of unconstrained language expressions in a row, e.g., \"open the drawer . . . pick up the blue block . . . push the block into the drawer . . . open the sliding door\". Furthermore, to evaluate the agents' ability for long-horizon planning, agents in this scenario are expected to be able to perform any combination of subtasks in any order. CALVIN has been developed from the ground up to support training, prototyping, and validation of language-conditioned continuous control policies over a range of four indoor manipulation environments, visualized in Figure  1. CALVIN includes \u223c24 hours teleoperated unstructured play data together with 20K language directives. Unscripted playful interactions have the advantage of being task-agnostic, diverse, and relatively cheap to obtain [1], [4]. The simulation platform supports a range of sensors commonly utilized for visuomotor control: RGB-D images from both a static and a gripper camera, proprioceptive information, and vision-based tactile sensing [5]. We believe that this flexible sensor suite will allow researchers to develop improved multimodal agents that can solve many tasks in real-world settings. This is the first public benchmark of instruction following, to our knowledge, that combines: natural language conditioning, multimodal highdimensional inputs, 7-DOF continuous control, and longhorizon robotic object manipulation. We provide an evaluation protocol with evaluation modes of varying difficulty by choosing different combinations of sensor suites and amounts of training environments. This effort joins the recent efforts to standardize robotics research for better benchmarks and more reproducible results. To open the door for future development of agents that can generalize abstract concepts to unseen entities the same way humans do, we include a challenging zeroshot evaluation by training on large play corpora covering three environments and testing on an unseen scene. The language instructions used for testing are not included in the training set and represent novel ways of describing the manipulation tasks seen during training.\n\nTo establish baseline performance levels, we evaluate the multi-context imitation learning (MCIL) approach that uses relabeled imitation learning to distill many reusable behaviors into a goal-directed policy [6]. This model is not effective on the complex long horizon robot manipulation tasks in CALVIN. While it achieves up to 53.9% success rate in short horizon tasks, it performs poorly in the long-horizon setting. We note that there is no constraint to use imitation learning approaches to solve CALVIN tasks, as approaches that use reinforcement learning to learn language-conditioned policies can also be applied [7].\n\nIn summary, CALVIN facilitates learning models that translate from language to sequences of motor skills in a realistic simulation environment. This benchmark captures many challenges present in real-world settings for relating human language to robot actions and perception for accomplishing long-horizon manipulation tasks. Models that can overcome these challenges will begin to close the gap towards scalable, general-purpose, language-driven robotics.\n\n\n"}, {"paperid": "paper3", "title": "Multi-agent Embodied Question Answering in Interactive Environments", "abstract": "We investigate a new AI task \u2014 Multi-Agent Interactive Question Answering \u2014 where several agents explore the scene jointly in interactive environments to answer a question. To cooperate efficiently and answer accurately, agents must be well-organized to have balanced work division and share knowledge about the objects involved. We address this new problem in two stages: Multi-Agent 3D Reconstruction in Interactive Environments and Question Answering. Our proposed framework features multi-layer structural and semantic memories shared by all agents, as well as a question answering model built upon a 3D-CNN network to encode the scene memories. During the reconstruction, agents simultaneously explore and scan the scene with a clear division of work, organized by next viewpoints planning. We evaluate our framework on the IQuADv1 dataset and outperform the IQA baseline in a single-agent scenario. In multi-agent scenarios, our framework shows favorable speedups while remaining high accuracy.", "introduction": "\n\nFor decades, one of our best wishes has been to develop robots that can assist humans with the ability to understand the scene, to interact with environments, and to communicate with humans. For instance, a domestic robot might be asked: How many apples are in the house? To answer it, the agent must explore the house, open fridges & cabinets for possibly hidden apples, check the occurrence of apples, and answer the question by natural language.\n\nThis sort of problem refers to Embodied Question Answering (EQA) [4] : Being asked What color is the car?, an agent navigates to the car and observes it before it answers the question. Since the car may be out of sight initially, the agent must have common sense about possible locations of the car and a way to get there. However, point-to-point navigation is not enough -what if we want the agent to search for a missing fork which may be anywhere in the kitchen?\n\nTo be more practical, Interactive Question Answering (IQA) [7] takes both interactive actions (e.g., open a cabinet) and more generic questions (e.g., existence and counting) into consideration. To answer Is there a fork in the kitchen?, the agent must have comprehensive cognition to the kitchen, without missing any place where the target may exist, including interactive objects like containers. However, this process could be time-costing.\n\nParallelism has always been a fundamental but effective idea. Since several agents can search for an object simultaneously, the question will soon be answered if multiple robots can explore collaboratively. Therefore, we introduce Multi-Agent Interactive Question Answering, which presents additional challenges to AI systems. First, the multi-agent system must be well-organized to avoid duplicate work and unbalanced work. Second, the multi-agent QA system must integrate information from all agents and answer the question accurately without a repeat or a miss. Third, the multi-agent system should achieve as high speedup as possible while keeping the high accuracy.\n\nVery few studies have looked into multi-agent embodied question answering tasks. However, active 3D reconstruction [5] [24] is not a novel problem. Here we propose a two-stage framework for Multi-Agent IQA, which firstly executes a multi-agent (embodied) 3D reconstruction to construct 3D global structural and semantic memories and secondly encodes the scene via 3D memories to answer the question. To support interactive objects, we propose a multi-layer data structure as an extension to traditional voxel-based reconstructions.\n\nWe train and evaluate our proposed two-stage framework on the IQuADv1 IQA dataset [7] in both single-agent and multi-agent scenarios and observe promising results of highly effective and efficient in both cases.\n\nContributions. In summary, our main contributions include:\n\n-Problem. We introduce the Multi-Agent IQA, the task of organizing collaborative Interactive Question Answering for several agents. -Method. We propose a two-stage framework for Multi-Agent IQA, a method to efficiently construct 3D global memories via multi-agent 3D reconstruction and to answer the question by encoding the scene memories with 3D-CNN. -Results. Our 3D-memory-based framework surpasses the original IQA method in both answering accuracy and episode length, with a single agent on the IQuADv1 dataset. With 2, 3, and 4 agents, we show consistent high-level parallelism and affordable speedups in average episode length.\n\n\n"}, {"paperid": "paper4", "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation", "abstract": "Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io", "introduction": "\n\nEnd-to-end models that map directly from pixels to actions hold the capacity to learn complex manipulation skills, but are known to require copious amounts of data [1,2]. Integrating object-centric assumptions -e.g., object keypoints [3,4,5,6], embeddings [7,8], or dense descriptors [9,10,11] -has been shown to improve sample efficiency [10]. However, these representations often impose data collection burdens (i.e., configuring scenes with specific singulated objects) and still struggle to address challenging scenarios with unseen classes of objects, occluded objects, highly deformable objects, or piles of small objects [12]. This naturally leads us to ask: is there structure that we can incorporate into our end-to-end models to improve their learning efficiency, without imposing any of the limitations or burdens of explicit object representations?\n\nIn this work, we propose the Transporter Network, a simple end-to-end model architecture that preserves spatial structure for vision-based manipulation, without object-centric assumptions:\n\n\u2022 Manipulation involves rearranging things, which can be thought of as executing a sequence of spatial displacements: where the space being moved (i.e., transported) can encompass an object(s), part of an object, or end effector. We formulate vision for manipulation as estimating these displacements. Transporter Networks directly optimize for this by learning to 1) attend to a local region, and 2) predict its target spatial displacement via deep feature template matching -which then parameterizes robot actions for manipulation. This formulation enables high-level perceptual reasoning about which visual cues are important, and how they should be rearranged in a scene -the distributions of which can be learned from demonstrations. \u2022 Transporter Networks preserve the 3D spatial structure of the visual input. Prior end-to-end models [1,2] often use convolutional architectures with raw images, in which valuable spatial information can be lost to perspective distortions. Our method uses 3D reconstruction to project visual data onto a spatiallyconsistent representation as input, with which it is able to better exploit equivariance [13,14] for inductive biases that are present within the geometric symmetries [15] of the data for more efficient learning.\n\nIn our experiments, Transporter Networks exhibit superior sample efficiency on a number of tabletop manipulation tasks that involve changing the state of the robot's environment in a purposeful manner: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Transporter Networks excel in modeling 4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA. A Transporter Network is a simple model architecture that attends to a local region and predicts its spatial displacement (b) from visual input -which can parameterize robot actions. It is sample efficient in learning complex vision-based manipulation tasks: inserting blocks into fixtures (a), sequential pick-and-place in Towers of Hanoi (c), assembling kits with unseen objects (d), palletizing boxes (e), stacking a pyramid of blocks (f), manipulating rope (g), and pushing piles of small objects with closed-loop feedback (h) -and is practical to deploy on real production robots (k, m).\n\nmulti-modal spatial action distributions, and by construction generalize across rotations and translations of objects. They do not require any prior knowledge of the objects to be manipulated -they rely only on information contained within partial RGB-D data from demonstrations, and are capable of generalizing to new objects and configurations, and for some tasks, one-shot learning from a single demonstration.\n\nOur main contribution is a new perspective on the role of spatial structure and its capacity to improve end-toend learning for vision-based manipulation. We propose a simple model architecture that learns to attend to a local region and predict its spatial displacement, while retaining the spatial structure of the visual input. On 10 unique tabletop manipulation tasks, Transporter Networks trained from scratch are capable of achieving greater than 90% success on most tasks with objects in new configurations using 100 expert demonstrations, while other end-to-end alternatives struggle to generalize with the same amount of data. We also develop an extension to 6DoF tasks by combining 3DoF Transporter Networks with continuous regression to handle the remaining degrees of freedom. To facilitate further research in vision-based manipulation, we plan release code and open-source Ravens, our new simulated benchmark with all tasks. Ravens features a Gym-like API [16] with a built-in stochastic oracle to evaluate the sample efficiency of imitation learning methods.\n\n\n"}, {"paperid": "paper5", "title": "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation", "abstract": "Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents -- object manipulation by following human guidance, e.g.,\"move the red mug next to the box while keeping it upright.\"To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.", "introduction": "\n\n\"Can you help me to clean the disks in the sink?\" -humans communicate with each other using language to issue tasks and specify the requirements. Although recent progress in embodied AI pushes intelligent robotic systems to reality closer than any other time before, it is still an open question how the agent learn to manipulate objects following language instructions. Therefore, we introduce the task of Vision-and-Language Manipulation (VLM), where the agent is required to follow language instructions to do robotic manipulation. There are recent benchmarks developed to evaluate robotic manipulation tasks with language guidance and visual input [10,1,33]. However, the collected task demonstrations are not modular and can hardly scale because they lack (1) adaptation to novel objects (2) categorization for modular and flexible composition to complex tasks. Additionally, the lack of variations in language also lead to biases for visual reasoning learning. To deal with these problems, we expect an inclusive, modular, and scalable benchmark to evaluate embodied agents for various language-guided manipulation tasks.\n\nAn ideal VLM benchmark should have at least three characteristics: The first one is scalability. Such a benchmark should automatically generate various physics-realistic 6 degrees of freedom (DoF) interactions with affordable objects and expand new tasks effortlessly. The second one is task categorization, which exploits commonality concerning robot motion between different semantic tasks and is almost ignored in existing works. The third one is reasonable language generation, Figure 1: Given the language instructions and observations, the VLMbench requires the agent to generate an executable manipulation trajectory for specific task goals. On the left, we show that the complex tasks can be divided into the unit tasks according to the constraints of the end-effector, like \"Open the door of the dishwasher\" and \"Open the door of the fridge\" should both follow the rotation constraints of the revolute joint. On the right, we show examples of object-centric representations, where all graspable objects or parts will generate local grasping poses as their attributes. Depending on the modular design, we can generate reasonable VLM data automatically.\n\nwhich requires the benchmark can generate language instructions for testing diverse visual reasoning abilities without biases. However, existing benchmarks [30,10,33,1] lack at least one characteristic for VLM tasks. Motivated by these attributes, we present VLMbench, a highly categorical robotic manipulation benchmark with compositional language for visual reasoning. To build and scale VLMbench, we propose AMSolver, an automatic unit task builder that can compose unit tasks to create complex multi-step tasks and seamlessly adapt to novel objects. Compared to previous benchmarks, VLMbench categorizes manipulation tasks into various meta manipulation actions according to the constraints of robot trajectories for the first time. Meanwhile, the combinations of compositional language templates and object-centric representations provide numerous variations for visual reasoning in VLMbench, as shown in Figure 1.\n\nTo investigate the difficulty of the benchmark, we test them with several partially modal methods and a keypoint-based method, 6D-CLIPort, modified from the state-of-the-art language-guided manipulation method CLIPort [25]. The results show that there is still a massive room for improvement in the robust manipulation action generations and accurate language-guided visual understanding. To sum up, our contributions in this work include:\n\n\u2022 AMSolver, an automatic demonstration generator for various task semantics, motion constraints, object types and states defined in a novel task template formulation.\n\n\u2022 VLMbench, a robot manipulation benchmark on 3D tasks with visual observation and compositional language instructions, where we categorize the manipulation tasks by constraints and provide variations with minimal biases in the first time.\n\n\u2022 6D-CLIPort, a general vision-and-language manipulation baseline model evaluated on all kinds of VLMbench tasks.\n\n\n"}, {"paperid": "paper6", "title": "CH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous Multi-Agent Reinforcement Learning", "abstract": "We propose a multimodal (vision-and-language) benchmark for cooperative and heterogeneous multi-agent learning. We introduce a benchmark multimodal dataset with tasks involving collaboration between multiple simulated heterogeneous robots in a rich multi-room home environment. We provide an integrated learning framework, multimodal implementations of state-of-the-art multi-agent reinforcement learning techniques, and a consistent evaluation protocol. Our experiments investigate the impact of different modalities on multi-agent learning performance. We also introduce a simple message passing method between agents. The results suggest that multimodality introduces unique challenges for cooperative multi-agent learning and there is significant room for advancing multi-agent reinforcement learning methods in such settings.", "introduction": "\n\nWe posit that progress in multi-agent learning and its application to multi-robot problems could be sped up with the introduction of standard, sophisticated environments for training and evaluation. Prior work on cooperative multi-agent learning has focused on simplified environments [16]. Visually rich environments that support multi-agent, cooperative tasks have not been explored until very recently [24,8,9,18,23]. We propose the first multimodal benchmark on Cooperative Heterogeneous Multi-Agent Reinforcement Learning (CH-MARL) wherein two simulated robots must collaboratively find an object and place it at a target location.\n\nCH-MARL is built using visually rich scenes from VirtualHome [18], and includes language. We implement a language generator that procedurally provides feedback to guide embodied agents to achieve tasks. In addition to providing a novel large-scale vision and language dataset for collaborative task completion in simulated household environments, we conduct a comprehensive evaluation of several state of the art MARL algorithms under various setting for our collaborative robot benchmark task. We investigate and analyze the impact of various aspects of the collaborative MARL algorithms, including heterogeneity and multi-modality. We also propose and implement a message passing interface between agents to enable effective information sharing, especially in decentralized model setups where they would otherwise not have the ability to collaborate with each other. The results reveal interesting insights: 2. Vision and language grounding helps the learning process 3. Even simple multi-agent communication protocols substantially improve task performance by allowing effective collaboration.\n\nTo our knowledge, this is the first dataset to support multiple heterogeneous agents in a virtual environment collaboratively completing a specified task. A comparative study of state of the art embodied AI datasets is in Table 1. We expect this work to contribute towards a standard multi-modal testbed for MARL and foster research in this area. \u00d7 \u00d7 \u00d7 House3D [25] \u00d7 \u00d7 \u00d7 iGibson [13] \u00d7 \u00d7 Watch and Help [19] \u00d7 \u00d7 CH-MARL (Ours)\n\n\n"}, {"paperid": "paper7", "title": "Two Body Problem: Collaborative Visual Task Completion", "abstract": "Collaboration is a necessary skill to perform tasks that are beyond one agent's capabilities. Addressed extensively in both conventional and modern AI, multi-agent collaboration has often been studied in the context of simple grid worlds. We argue that there are inherently visual aspects to collaboration which should be studied in visually rich environments. A key element in collaboration is communication that can be either explicit, through messages, or implicit, through perception of the other agents and the visual world. Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks. Refer to our project page for more details: https://prior.allenai.org/projects/two-body-problem", "introduction": "\n\nDeveloping collaborative skills is known to be more cognitively demanding than learning to perform tasks independently. In AI, multi-agent collaboration has been studied in more conventional [32,43,9,58] and modern settings [53,28,79,35,56,61]. These studies have mainly been performed on grid-worlds and have factored out the role of perception in collaboration.\n\nIn this paper we argue that there are aspects of collaboration that are inherently visual. Studying collaboration in simplistic environments does not permit to observe the interplay between perception and communication, which is necessary for effective collaboration. Imagine moving a piece of furniture with a friend. Part of the collaboration is rooted in explicit communication through exchanging messages, and some part of it is done through implicit communication through interpreting perceivable cues about the other agents behavior. If you see your friend going around the furniture to grab it, you would naturally stay on the opposite side to avoid toppling it over. Additionally, communication and collaboration should be considered jointly with the task itself. The way you communicate, either explicitly or implicitly, in a soccer game is very different from when you move furniture. This suggests that factoring out per-ception and studying collaboration in isolation (grid-world) might not result in an ideal outcome.\n\nIn short, learning to perform tasks collaboratively in a visual environment entails joint learning of (1) how to perform tasks in that environment, (2) when and what to communicate, and (3) how to act based on implicit and explicit communication. In this work, we develop one of the first frameworks that enables the study of explicitly and implicitly communicating agents collaborating together in a photo-realistic environment.\n\nTo this end we consider the problem of finding and lifting bulky items, ones which cannot be lifted by a single agent. While conceptually simple, attaining proficiency in this task requires multiple stages of communication. The agents must search for the object of interest in the environment (possibly communicating their findings to each other), position themselves appropriately (for instance, opposing each other), and then lift the object simultaneously. If the agents position themselves incorrectly, lifting the object will cause it to topple over. Similarly, if the agents pick up the object at different time steps, they will not succeed.\n\nTo study this task, we use the AI2-THOR virtual environment [48], a photo-realistic, physics-enabled environment of indoor scenes used in past work to study single agent behavior. We extend AI2-THOR to enable multiple agents to communicate and interact.\n\nWe explore collaboration along several modes: (1) The benefits of communication for spatially constrained tasks (e.g., requiring agents to stand across one another while lifting an object) vs. unconstrained tasks. (2) The ability of agents to implicitly and explicitly communicate to solve these tasks. (3) The effect of the expressivity of the communication channel on the success of these tasks. (4) The efficacy of these developed communication protocols on known environments and their generalizability to new ones. (5) The challenges of egocentric visual environments vs. grid-world settings.\n\nWe propose a Two Body Network, or TBONE, for modeling the policies of agents in our environments. TBONE operates on a visual egocentric observation of the 3D world, a history of past observations and actions of the agent, as well as messages received from other agents in the scene. At each time step, agents go through two rounds of communication, akin to sending a message each and then replying to messages that are received in the first round. TBONE is trained with a warm start using a variant of DAgger [70], followed by a minimization of a sum of an A3C loss and a cross entropy loss between the agents actions and the actions of an expert policy.\n\nWe perform a detailed experimental analysis of the impact of communication using metrics including accuracy, number of failed pickup actions, and episode lengths. Following our above research questions, our findings show that: (1) Communication clearly benefits both constrained \nt \u2212 1 [ ] , a \u02c6 ( 1 ) t a \u02c6 ( 2 ) t Environment o ( 1 ) t o ( 2 ) t Comm. channel o ( 1 ) t + 1 o ( 2 ) t + 1\ns a m p le s a m p le Figure 2: A schematic depicting the inputs to the policy network. An agent's policy operates on a partial observation of the scene's state and a history of previous observations, actions, and messages received.\n\nand unconstrained tasks but is more advantageous for constrained tasks.\n\n(2) Both explicit and implicit communication are exploited by our agents and both are beneficial, individually and jointly. (3) For our tasks, large vocabulary sizes are beneficial. (4) Our agents generalize well to unseen environments. (5) Abstracting our environments towards a grid-world setting improves accuracy, confirming our notion that photo-realistic visual environments are more challenging than grid-world like settings. This is consistent with findings by past works for single agent scenarios. Finally we interpret the explicit mode of communication between agents by fitting logistic regression models to the messages to predict the values such as oracle distance to target, next action, etc., and find strong evidence matching our intuitions about the usage of messages between agents.\n\n\n"}, {"paperid": "paper8", "title": "Embodied Multi-Agent Task Planning from Ambiguous Instruction", "abstract": "\u2014In human-robots collaboration scenarios, a human would give robots an instruction that is intuitive for the human himself to accomplish. However, the instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction. This problem exhibits significant challenges in both language understanding and dynamic task planning with the perception information. In this work, an embodied multi-agent task planning framework is proposed to utilize external knowledge sources and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocate the decomposed tasks to multiple agents. Furthermore, we utilize the semantic information to perform environment perception and generate sub-goals to achieve the navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notori- ous sim2real problem. Finally, we build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit in instructions. We perform the evaluation experiments on the simulation platform and in physical scenarios, demonstrating that the proposed model can achieve promising results for multi-agent collaborative tasks.", "introduction": "\n\nIn real life, a group leader may release an ambiguous instruction, which contains his intention but lacks the implementation details. Nevertheless, intelligent group members may analyze the instruction to extract the intention and utilize their knowledge or shared-mental-mind with the leader to execute the necessary operational details to accomplish the task. Such a collaboration mechanism is also highly expected for humanrobot collaboration. For example, a human would give robots an instruction in which the process of completing the task is obvious to the human himself. However, the overall instruction given to robots is likely ambiguous for them to understand as some information is implicit in the instruction, such as \"Put the book and newspaper away\". Although human knows that the book and newspaper are most likely to be put on the bookshelf, or the drawer if there is no bookshelf found, robots may not know where to put the book and newspaper directly from the instruction, let alone collaborating to complete this task. Therefore, it is necessary for the robots to jointly reason the operation details and perform the embodied multi-agent task planning given the ambiguous instruction (Fig. 1). Fig. 1. An overview of the embodied multi-agent task planning from ambiguous instruction. Given a high-level instruction, several sub-tasks are generated and allocated to a group of agents. The agents explore the environment and implement the sub-tasks. With the change of the visual observation during the exploration process, the task decomposition and task allocation processes are also adjusted dynamically.\n\nIn the multi-agent task planning scenario, a complex task can be decomposed in multiple possible ways, and the decomposed sub-tasks are allocated to multiple agents for the execution [24]. Therefore, the task planning includes task decomposition, which focuses on the problem of what to do [21], task allocation which focuses on the problem of who does what [3], and task scheduling which focuses on the problem of how to arrange tasks in time [43]. Among them, the task decomposition is the problem of decomposing a complex task into simpler ones, down to the level of actionable tasks [21,28]; task allocation is the problem of determining which robot should execute which task in order to achieve the overall system goal [3], and task scheduling is the problem of sequencing tasks for execution [17]. The above problems have been extensively investigated in diversified works of literature, most of which transform the given task into a well-defined optimization problem that requires a clear, structured, and complete instruction [2,16]. In practical scenarios, task planning is highly coupled with the human-robot interaction and the perception of the environment. It should be dynamically adjusted due to the vagueness of the interaction and dynamics of the environment. In this work, we formulate such a problem as embodied multi-agent task planning from ambiguous instruction, which exhibits the following key challenges: 1) Ambiguous Instruction Due to the incompleteness and ambiguity of the given instruction, it is necessary to use external knowledge sources (such as domain knowledge in a specific field, knowledge graph, and industry rules) to reason and clarify the instruction. Based on the clarified instruction, combined with the characteristics of the robot, the given task is required to be decomposed into specific sub-tasks that the robot can perform. For example, the task \"Put the book and the newspaper away\" should be clarified to be \"Put the book and the newspaper on the bookshelf\" with the visual perceptions of robots and the knowledge that books and newspapers are always placed on the bookshelf. Then it should be initially decomposed into the sub-tasks of \"Find the book\", \"Find the newspaper\" and \"Find the bookshelf\".\n\n2) Dynamic Task Decomposition Since the initial visual perceptions of multiple robots are limited, the reasoning information based on the initial state may not be correct. Therefore, it is necessary to dynamically adjust the instruction reasoning and task decomposition with updated visual perceptions during the continuous execution process of agents. For example, the task \"Put the book and the newspaper away\" is clarified to be \"Put the book and the newspaper on the bookshelf\" with the initial visual perceptions. After several steps of exploration, agents find that there is no bookshelf but a drawer in the current scene based on their newly obtained visual perceptions. The book and newspaper can also be put in the drawer. Then agents need to re-reason the implicit information in the given instruction and obtain the new clarified instruction \"Put the book and the newspaper in the drawer\". Afterward, the subsequent decomposition and allocation processes are performed based on the clarified results.\n\n3) Dynamic Task Allocation Based on the specific decomposed sub-tasks, the sub-tasks need to be allocated to multiple robots considering the robots' perception and motion abilities so that each robot is assigned to a corresponding sub-task. More importantly, in the specific execution process, because of the ambiguity of instructions and the dynamic nature of the environment, robots are required to dynamically adjust their allocated sub-tasks according to the environment perception information. For example, the three decomposed tasks \"Find the book\", \"Find the newspaper\" and \"Find the bookshelf\" are allocated to Agent 1, Agent 2 and Agent 3 respectively based on their initial visual environment information. After several steps, if Agent 1 finds that it is actually closer to the bookshelf based on its obtained observations, they need to reallocate the sub-tasks and Agent 1 would change to perform \"Find the bookshelf\", and Agent 2 would change to \"Find the book\" accordingly.\n\nTo tackle the above issues, we propose an embodied multi-agent task planning framework demonstrated in Fig.1 which utilizes external knowledge sources, and dynamically perceives environment information to parse the high-level ambiguous instructions, dynamically allocates the decomposed sub-tasks and completes the distributed navigation tasks. In this framework, multiple agents are able to leverage the advantages of their embodiment attribute to dynamically and automatically adjust the instruction parsing results and efficiently complete the task. The main contributions are summarized as follows:\n\n1) Multi-agent embodied task planning framework: A multi-agent task planning framework is proposed to solve the multi-agent collaborative mission, which utilizes external knowledge sources, and dynamically perceived visual information to resolve the high-level instructions, and dynamically allocates the decomposed tasks.\n\n2) Sim&Real learning method for embodied task planning: A dynamic task allocation model is developed based on multi-agent collaboration. We utilize the semantic information to perform the environment perception and generate sub-goals to achieve navigation motion. This model effectively bridges the difference between the simulation environment and the physical environment, thus it can be simultaneously applied in both simulation and physical scenarios and avoid the notorious sim2real problem.\n\n3) Evaluation and validation: We build a benchmark dataset to validate the embodied multi-agent task planning problem, which includes three types of high-level instructions in which some target objects are implicit. We perform the evaluation experiments both in the AI2-THOR [22] platform and physical scenarios including Easy and Hard settings, which demonstrate that the proposed model can achieve promising results for multi-agent collaborative tasks.\n\n\n"}, {"paperid": "paper9", "title": "LEMMA: Learning Language-Conditioned Multi-Robot Manipulation", "abstract": "Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.", "introduction": "\n\nT HERE is growing interest in connecting human language to robot actions, particularly in single-agent systems [1], [2], [3], [4], [5]. However, there remains a research gap in enabling multi-robot systems to work together in response to language input.\n\nRecent vision and language tasks have primarily focused on navigation and object interactions [4], [6], [7]. However, the lack of physical manipulation in these works makes the settings oversimplified. Although some recent studies, such as [1], [5], address vision and language object manipulation in single-robot settings, the language instructions provided specify only short-term goals, neglecting long-term objectives. [8] attempt to address these limitations by exploring longhorizon planning with manipulation for individual robots. Nevertheless, there remains a need to investigate multi-robot systems capable of accomplishing a broader range of longhorizon tasks while following language instructions.\n\nLearning policies for multi-robot systems introduces distinct challenges, including diverse capabilities arising from physical Manuscript received: April, 18, 2023; Revised August, 1, 2023; Accepted August, 21, 2023. This paper was recommended for publication by Editor Aleksandra Faust upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by Amazon Alexa AI. Corresponding author: Xiaofeng Gao Project website: https://lemma-benchmark.github.io constraints such as the location and reach of different robots. Moreover, task planning heavily depends on the spatial and physical relations between the objects and robots, in addition to the geometries of the objects. To ensure suitable task assignments, an awareness of each robot's specific physical capabilities is needed.\n\nTo tackle the language-conditioned vision-based multi-robot object manipulation problem, we have developed LEMMA, a benchmark that contains 8 types of collaborative object manipulation tasks with varying degrees of complexity. Some tasks require the robot to use tools for object-object interactions. For each task, the object poses, appearances, and robot types are randomized, requiring object affordance estimation and robot capability understanding. To enable multi-task learning, each task is paired with an expert demonstration and several language instructions specifying the task at different granularities. As a result, LEMMA introduces a diverse range of challenges in multi-robot collaboration, including physics-based object manipulation, long-horizon task planning, scheduling and allocation, robot capability and object affordance estimation, tool use, and language grounding. Each aspect poses distinct challenges and is crucial for a multi-robot system that follows human instructions to complete tasks. To evaluate existing techniques on LEMMA, we further provide several baseline methods and compare their performance to each other. We assess task performance by utilizing the latest languageconditioned policy learning models. Our results indicate that current models for language-conditioned manipulation and task planning face significant challenges in LEMMA, especially when dealing with complex human instructions.\n\nWe make the following contributions: \u2022 We design eight novel collaborative object manipulation tasks involving robots with different physical configurations implemented in Nvidia Omniverse -Isaac Sim. \u2022 We provide an open-source dataset comprising 6,400 expert demonstrations and natural language instructions, including human and high-level instructions. \u2022 We implement a modular hierarchical planning approach as a baseline, which integrates language understanding, task planning, task allocation, and object manipulation.\n\n\n"}]