[{"paperid": "paper0", "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning", "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric \u2013 Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.", "introduction": "\n\nWith the advances in large-scale pre-trained language models (PLMs), recent studies have explored various efficient learning paradigms (Brown et al., 2020;Liu et al., 2021;Wei et al., 2021;Xie et al., 2021) to generalize PLMs to new tasks without task-specific tuning. Among these, instruction * Zhiyang Xu and Ying Shen contributed equally to this work. 1 The dataset, source code, and model checkpoints are publicly available at https://github.com/VT-NLP/ MultiInstruct. tuning (Wei et al., 2021) has achieved significant success in zero-shot learning on natural language processing tasks. By fine-tuning a PLM on tasks described through instructions, instruction tuning allows the model to learn to understand and follow the instructions to perform predictions on unseen tasks. Recent advancement in multimodal pretraining (Wang et al., 2022a;Alayrac et al., 2022;Bao et al., 2022;Wang et al., 2022c) has shown the potential of jointly interpreting text and images in a shared semantic space, which further leads us to ask: can the instruction tuning be leveraged to improve the generalizability of Vision-Language pretrained models on multi-modal and vision tasks?\n\nIn this work, we propose MULTIINSTRUCT, the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories, including Visual Question Answering (Goyal et al., 2017;Suhr et al., 2017), Commonsense Reasoning (Zellers et al., 2019;Xie et al., 2019), Visual Relationship Understanding (Krishna et al., 2017) and so on. We equipped each task with 5 instructions that are written by two experts in natural language processing. As shown in Figure 1, we formulate all the tasks into a unified sequence-to-sequence format in which the input text, images, instructions, and bounding boxes are represented in the same token space.\n\nWe use OFA (Wang et al., 2022a) 2 , a unified model that is pre-trained on a diverse set of multimodal and unimodal tasks in a single Transformerbased sequence-to-sequence framework, as the base pre-trained multimodal language model, and fine-tune it on MULTIINSTRUCT. To utilize NATU-RAL INSTRUCTIONS (Mishra et al., 2022), a largescale text-only instruction tuning dataset, we further explore two transfer learning strategies, in-Grounded Caption\n\n\n"}, {"paperid": "paper1", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "introduction": "to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. However, systematic collaboration approaches, including Visual ChatGPT [Wu et al., 2023], MM-REACT [Yang et al., 2023], and HuggingGPT [Shen et al., 2023], are designed to facilitate the coordination of various vision models or tools to express visual information with text descriptions. However, these approaches may not be able to comprehend specific multimodal instructions due to their lack of alignment with different modalities. Additionally, these approaches may encounter challenges related to inference efficiency and cost. End-to-end models, such as BLIP-2 , LLaVA [Liu et al., 2023], and MiniGPT-4 [Zhu et al., 2023a] aim to use unified models to support different modalities. However, these models have some limitations as they take frozen visual models, which may lead to inadequate alignment due to the limited number of parameters. Moreover, they cannot unlock various abilities due to missing unimodal and multimodal instruction.\n\nIn this paper, we present mPLUG-Owl with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently, drawing inspiration from the concept of modularization [Xu et al., 2023b, Xu et al., 2021, Ye et al., 2022. Our method harnesses the power of pre-trained LLM, visual knowledge module, and connected visual abstractor module to achieve effective alignment between images and text, and utilizes a twostage training scheme to stimulate impressive unimodal and multimodal abilities. Our approach even enhances the strong generation abilities of LLM by modality collaboration between modalities. In the first step, we align the image and text to acquire comprehensive visual knowledge using textimage pairs, which is accomplished by training the visual knowledge module and abstractor module with the frozen LLM module. Subsequently, we fine-tune mPLUG-Owl with language-only and multi-modal instructions to unlock a range of unimodal and multimodal abilities. We freeze the visual knowledge module and train low-rank adaption (LoRA) [Hu et al., 2022] on LLM and visual abstractor module jointly. This approach allows for the effective integration of textual and visual information, facilitating the development of versatile and robust cognitive abilities.\n\nOur experiments on a carefully-built visually related instruction evaluation set OwlEval shows that mPLUG-Owl outperforms existing models such as MiniGPT-4 [Zhu et al., 2023a] and LLaVA [Liu et al., 2023]. We separately verifies mPLUG-Owl's remarkable abilities in instruction understanding, visual understanding, knowledge transfer, and multi-turn dialogue. Abundant ablation study is performed to show the effectiveness of our training paradigm. Furthermore, we find some unexpected emerging ability such as multi-image correlation, multilingual conversation and scene text understanding.\n\nOur main contributions can be highlighted as follows:\n\n\u2022 We propose mPLUG-Owl, a novel training paradigm for large language models through modularization.\n\n\u2022 We carefully construct an instruction evaluation set, dubbed OwlEval, to assess the capabilities of different models in the context of visual-related tasks.\n\n\u2022 Experimental results demonstrate that mPLUG-Owl excels in multi-modal instruction understanding and multi-turn dialogue, surpassing the performance of existing models.\n\n2 Related Work"}, {"paperid": "paper2", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.", "introduction": "\n\nA longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that can solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction tuning [46,7] proves to be a promising approach toward that goal. By finetuning a large language model (LLM) on a wide range of tasks described by natural language instructions, instruction tuning enables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been leveraged for vision-language tasks. For example, BLIP-2 [20] effectively adapts frozen instructiontuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in image-to-text generation.\n\nCompared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual inputs from various domains. This poses a greater challenge to a unified model that is supposed to generalize to diverse vision-language tasks, many unseen during training. Most previous work can be grouped into two approaches. The first approach, multitask learning [6,27], formulates various vision-language tasks into the same input-output format. However, we empirically find multitask learning without instructions (Table 4) does not generalize well to unseen datasets and tasks. The\n\nWhat could have happened based on the current scene?\n\nBased on the current scene in the image, it is possible that a hurricane or severe weather event caused significant damage to the buildings and infrastructure in the area. The person walking through the debris-covered area suggests that they are inspecting the damage caused by the disaster. The presence of palm trees indicates that the location may be in a tropical or sub-tropical region, which can be more susceptible to hurricanes and other natural disasters.\n\nIntroduce me this painting in detail.\n\nThe painting depicts a young girl with a pearl earring, also known as \"The Girl with the Pearl Earring\" or \"Girl with a Pearl Earring\". She is wearing a blue dress and has a pearl earring on her left ear. Her hair is pulled back into a bun, and she is looking directly at the viewer. This painting is a famous work of art by Dutch painter Johannes Vermeer, created in 1665. It is considered one of his masterpieces and is widely recognized for its intricate details and realistic portrayal of the young girl's facial expression.\n\nCan yon tell me about this image in detail?\n\nThe image depicts a collection of various vegetables including carrots, cucumbers, tomatoes, and nuts arranged on a stone surface. There are several jars filled with different types of ingredients, such as peanuts, cashews, sunflower seeds, and pumpkin seeds. These ingredients are likely to be part of a healthy meal or snack.\n\nCan you give me an example of such meal?\n\nAn example of a healthy meal or snack that incorporates some of the ingredients in the image could be a salad made with mixed greens, carrots, cucumbers, tomatoes, and nuts. The salad can be dressed with a homemade vinaigrette made with olive oil, lemon juice, salt, and pepper. Additionally, the salad can be topped with pumpkin seeds or sunflower seeds for added crunch and nutritional value.\n\nSounds good! Show me steps of making this salad.\n\nSure! Here are the steps to make this healthy salad: 1. Wash and chop the mixed greens, carrots, cucumbers, and tomatoes into bite-sized pieces. 2. In a small bowl, whisk together olive oil, lemon juice, salt, and pepper to create a homemade vinaigrette dressing. 3. Place the chopped vegetables in a large bowl and toss them with the vinaigrette dressing. 4. Sprinkle pumpkin seeds or sunflower seeds on top of the salad for added crunch and nutritional value. 5. Serve the salad immediately or store it in the refrigerator until ready to enjoy.\n\nCan you describe this image in detail? The image depicts a person standing in front of an open doorway, looking out into space. They are wearing a jacket and appears to be contemplating the vastness of the universe beyond the doorway. In the background, there are multiple stars visible, creating a stunning celestial scene. The person's gaze is directed towards the center of the image, where a bright star can be seen shining brightly. There are also smaller stars scattered throughout the scene, adding to the overall cosmic atmosphere.\n\nWhat if they jump over the doorway?\n\nIf the person were to jump over the doorway, they would be leaving the confines of their current location and venturing into the vastness of space. This could be a metaphorical representation of stepping out of one's comfort zone or embarking on a new adventure. However, it is important to note that such an action would be extremely dangerous and potentially lifethreatening due to the harsh conditions of space.\n\nWhat makes this dog special?\n\nThe dog in the image is wearing futuristic armor and has a glowing green eye, giving it a unique and futuristic appearance. This combination of elements makes the dog stand out from other dogs and adds an element of fantasy or sci-fi to the image. Additionally, the dog's posture and facial expression convey a sense of confidence and determination, further emphasizing its unique features. second approach [20,4] extends a pre-trained LLM with additional visual components, and trains the visual components with image caption data. Nevertheless, such data are too limited to allow broad generalization to vision-language tasks that require more than visual descriptions.\n\nTo address the aforementioned challenges, this paper presents InstructBLIP, a vision-language instruction tuning framework that enables general-purpose models to solve a wide range of visuallanguage tasks through a unified natural language interface. InstructBLIP uses a diverse set of instruction data to train a multimodal LLM. Specifically, we initialize training with a pre-trained BLIP-2 model consisting of an image encoder, an LLM, and a Query Transformer (Q-Former) to bridge the two. During instruction tuning, we finetune the Q-Former while keeping the image encoder and LLM frozen. Our paper makes the following key contributions:\n\n\u2022 We perform a comprehensive and systematic study on vision-language instruction tuning. We transform 26 datasets into the instruction tuning format and group them into 11 task categories. We use 13 held-in datasets for instruction tuning and 13 held-out datasets for zero-shot evaluation. Moreover, we withhold four entire task categories for zero-shot evaluation at the task level. Exhaustive quantitative and qualitative results demonstrate the effectiveness of InstructBLIP on vision-language zero-shot generalization.\n\n\u2022 We propose instruction-aware visual feature extraction, a novel mechanism that enables flexible and informative feature extraction according to the given instructions. Specifically, the textual instruction is given not only to the frozen LLM, but also to the Q-Former, so that it can extract instruction-aware visual features from the frozen image encoder. Also, we propose a balanced sampling strategy to synchronize learning progress across datasets.\n\n\u2022 We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1) FlanT5 [7], an encoder-decoder LLM finetuned from T5 [34]; 2) Vicuna [2], a decoder-only LLM finetuned from LLaMA [41]. The InstructBLIP models achieve state-of-the-art zero-shot performance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead to state-of-the-art finetuning performance when used as the model initialization on individual downstream tasks.\n\n\n"}, {"paperid": "paper3", "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning", "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.", "introduction": "\n\nThere has been a continuously increasing trend to develop intelligent assistants that can follow human instructions [3,36,37]. In the natural language processing (NLP) field, instruction tuning [35,53] is a success paradigm that leverages large-scale well-formatted instances to align large language models (LLMs) to human instructions. By finetuning on instances with specific task descriptions, LLMs learn to follow the instruction to perform various tasks, and demonstrate strong generalization ability on unseen tasks [29]. Expanding beyond NLP, a general-purpose intelligent agent must encompass various modalities, such as vision, prompting recent efforts to investigate instruction tuning in vision-language domains [63,28,7]. To develop powerful vision-language models (VLMs), it is essential to have a well-constructed dataset that encompasses diverse vision-language tasks and aligns with human instructions. However, the instructional data supporting existing VLMs is either not publicly available (e.g.,  or offers limited task and language coverage (e.g., only tasks in English are considered). This scarcity of comprehensive datasets has impeded the progress of open vision-language models, highlighting the importance of multi-modal instruction tuning and the need for high-quality datasets.\n\nIn this paper, we aim to advance instruction tuning research in the multi-modal domain by introducing an open dataset M 3 IT, a Multi-Modal Multilingual Instruction Tuning dataset, as an essential step towards building a versatile general-purpose assistant. We build this dataset by converting existing datasets into a unified vision-to-text schema with four stages: (1) manual instruction writing, (2) dataset pre-processing, (3) careful quality check and (4) dataset translation for key tasks. Our dataset encompasses a wide range of tasks, including classic image-text tasks such as image classification, visual question answering, and image captioning. Video-related tasks, such as video questionanswering, are also incorporated to ensure comprehensive coverage across multiple modalities. We further integrate Chinese vision-language datasets with corresponding Chinese instructions. The resulting dataset compiles 40 diverse tasks and 400 instructions. Finally, key vision-language tasks are translated into 80 languages with a strong translation system, to support multilingual studies.\n\nTo evaluate the effectiveness of the proposed dataset, we develop a vision-language model, Ying-VLM, by integrating a strong vision encoder, BLIP-2 [23] with a large language model, Ziya-13B [61], derived from LLaMA [49]. Building on the successful approach of incorporating visual tokens as textual prompts in LLMs [7,63,28], we employ a two-stage training process: (1) the initial stage aligns vision features with text embeddings through image captioning on LAION400M [41], and (2) the second stage enhances the model by conducting instruction tuning on selected tasks of our dataset. Experimental results reveal that Ying-VLM surpasses strong baseline models in knowledgeable VQA tasks and exhibits improved generalization performance to unseen video and cross-lingual tasks. Further analysis indicates that the improved performance corresponds to increased tasks for instruction tuning, while the diversity of instructions also affects outcomes. This paper presents two key contributions: (1) We introduce the open-source, large-scale Multimodal, multilingual Instruction Tuning (M 3 IT) dataset, designed to enable the development of general-purpose multi-modal agents. (2) We develop Ying-VLM, a visual assistant that excels in knowledgeable VQA tasks, demonstrates strong generalization to unseen video QA and Chinese multi-modal tasks, and offers valuable insights for future research. \n\n\n"}, {"paperid": "paper4", "title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena", "introduction": "\n\nLarge Language Models (LLMs), such as LLaMA [1], GPT-3 [2], and Vicuna [3], have demonstrated remarkable progress in Natural Language Processing (NLP). These models leverage large-scale pretraining data and huge networks to achieve impressive results in NLP benchmarks. Recently, GPT-4 [4] further expanded the impact to the multimodal community, stimulating the rapid development of Large Vision-Language Models (LVLMs) and revolutionizing the landscape of artificial intelligence.\n\nLarge Vision-Language Models (LVLM) have achieved remarkable progress in multimodal visionlanguage learning for various multimodal tasks such as visual question answering and multimodal conversation. Specifically, LVLMs capitalize on the knowledge from LLMs and effectively align visual features with the textual space. Flamingo [5], a pioneering LVLM, integrates visual features into LLMs through cross-attention layers. Later studies proposed more efficient vision-text interactions [6], more efficient training methods [7,8], and employing instruction tuning [9,7,[10][11][12][13]8].\n\nHowever, despite the great success, few efforts have been made to provide systematic evaluations of LVLMs. But evaluation plays a critical role in understanding the strengths and weaknesses of LVLMs, thereby guiding their future development. Recent work [14] presents a systematic investigation of object hallucination of LVLMs by proposing a polling-based object probing evaluation method. Moreover, ImageNetVC [15] studies how well LVLMs can master visual commonsense knowledge. Liu et al. [16] comprehensively evaluate the performance of LVLMs in visual recognition with text recognition, such as optical character recognition. GVT [17] evaluates LVLM's visual semantic understanding and fine-grained perception capabilities. Nevertheless, these studies only evaluate a portion of LVLMs on specific tasks, lacking an overall understanding of LVLM's capabilities.\n\nIn pursuit of a comprehensive evaluation of LVLMs, we build an LVLM Evaluation hub (LVLM-eHub) consolidating 8 representative LVLMs such as InstrucBLIP [13] and MiniGPT-4 [10]. The details about model configuration and training data are listed in Table 1. Our LVLM-eHub consists of a quantitative capability evaluation and an online arena platform, providing a thorough investigation of the selected LVLMs. Specifically, the quantitative capability evaluation extensively evaluates 6 categories of multimodal capabilities of LVLMs including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence (see Fig. 1 (a)), by collecting 47 standard text-related visual benchmarks. On the other hand, the online arena platform features anonymous randomized pairwise battles in a crowd-sourced manner, providing a user-level model ranking in the open-world question-answering scenario (see Fig. 1 (b & c)).\n\nOur LVLM-eHub comprehensively evaluates LVLMs, revealing several innovative findings. (1) Instruction-tuned LVLM with massive in-domain data suffers from overfitting problem and generalizes poorly in open-world scenarios , such as InstructBLIP (see Fig. 1 (a)). (2) With moderate instructionfollowing data, instruction-tuned LVLM may cause object hallucination issues, generating objects that are inconsistent with target images in the descriptions. This leads to incorrect answers or renders current evaluation metrics, such as CIDEr for image captioning, ineffective. (3) We find that a multi-turn reasoning evaluation pipeline can mitigate the issue of object hallucination, indicating that developing an effective pipeline for LVLM evaluation is urgent.\n\nThe contributions of our work are summarized follows. (1) We propose LVLM-eHub which is the first comprehensive evaluation benchmark for large vision-language models, to our best knowledge.   Table 1: Comparison between Different LVLMs. 'VE', 'Adapt', 'ToP', 'TuP', and '# Token' represent the visual encoder, adaption module, number of total parameters, tuning parameters, and visual tokens fed into the LLM, respectively. \u2020 indicates that the model is frozen. CC * consists of COCO [18], CC3M [19], and CC12M [20]. CC, VG, CY, L400, and LC indicate Conceptual Caption [19,20], Visual Genome [21], COYO-700M [22], LAION 400M [23] and LAION COCO [24], respectively. SBU [25] contains 1 million images with captions. LLaVA-I represents 158K multimodal instruction-following data in LLaVA [9]. QA * denotes 13 question-answering datasets in InstructBLIP [13]. We count all the data and tuning parameters needed to convert the pretrained vision model and LLM into a visual instruction model. evaluation results reveal several innovative findings, providing a foundational framework for the assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques.\n\n\n"}, {"paperid": "paper5", "title": "Aligning Large Multi-Modal Model with Robust Instruction Tuning", "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction . Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation . To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE) , a novel approach to evaluate visual instruction tuning without the need for human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate that existing LMMs exhibit significant hallucination when presented with our negative instructions, particularly with Existent Element Manipulation instructions. Moreover, by finetuning MiniGPT4 on LRV-Instruction , we successfully mitigate hallucination while improving performance on public datasets using less training data compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Our project link is available at this link.", "introduction": "\n\nSignificant progress has been made in the field of natural language processing, leading to the development of models that can comprehend and follow instructions given natural language inputs [36; 10; 26; 5]. These models harness the power of large language models (LLM) and rely on high-quality instruction data. Similarly, efforts have been made to introduce similar capabilities to multi-modal models. GPT4 [27] has demonstrated impressive performance in multi-modal conversations with humans, yet the techniques contributing to its extraordinary capabilities remain opaque. As a result, several large multi-modal models (LMMs) have recently emerged [39; 23; 10; 8], such as MiniGPT4 [39] and LLaVA [23], both utilize the Vicuna [7] as the language generator but with different vision encoders [28; 16]. InstructBLIP [8] is initialized from a pre-trained BLIP-2 [15] while Multimodal-GPT (MMGPT) [10] is built on Flamingo [1; 3].\n\nA recent study [14] revealed that the hallucination issue of LLM, although not desired, is inherited by these LMMs [39; 23; 10; 8]. Hallucination, a major ethical concern associated with LLMs [4], can lead to harmful consequences, especially when users without adequate domain knowledge over-rely on these increasingly convincing language models. In the context of LMM hallucinations, the model There is no mention of a dog or a ball in the given image. Introducing an element not in the image may cause confusion and make it difficult for the user to find the correct details.\n\n\n"}, {"paperid": "paper6", "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", "abstract": "We introduce VisIT-Bench ( Vis ual I ns T ruction Bench mark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 \u201cinstruction families\u201d that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model\u2019s response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/ .", "introduction": "\n\nA long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For textual tasks, several recent works [2,3,4,5,6,7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8,9,10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, butperhaps more-so -for creative, useful, and real-world queries that downstream users of language technologies are likely to make.\n\nOn the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., \"how many apples are in this image?\" or \"What is this?\" or \"Write a poem in the style of Robert Frost about this scene.\") a textual response is provided. Recent works like OpenFlamingo [11,12], LLaVA [13] and others [14,15,16,17,18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20] are commonly used to assess performance, less is know about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.\n\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual InsTruction Benchmark), a dynamic benchmark consisting of 592 challenging visionlanguage instructions. Each instance contains an instruction, input image(s), a instructionconditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 1). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks \"how can I cook this in a healthy way?\". Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., \"write a poem...\" or \"what should I bring if I were to visit here?\").\n\nWe created VisIT-Bench to cover a wide array of \"instruction families\". Our starting point was a set of 70 \"wish-list\" tasks such as \"home renovation\" and \"gardening tips\" collected by the authors: 1 each requiring varied high-level skills from recognition to complex reasoning ( Figure 2). We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA) [21] and robust change captioning [22] into a chatbot-style format (this reformatting differs from prior work [14,17,13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\n\nWe started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions -unlike prior work [23,24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.\n\nWe conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench ( \u00a74). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals significant differences between models (e.g., that LLaVA-13b [13] is generally preferred to Panda [18]), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25,26],\n\n\n"}]