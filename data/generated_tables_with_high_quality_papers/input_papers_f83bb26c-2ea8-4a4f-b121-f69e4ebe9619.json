[{"paperid": "paper0", "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.", "introduction": "\n\nRecent advances in language model pre-training have shown that models such as BERT (Devlin et al., 2018), RoBERTa  and T5 (Raffel et al., 2019) store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on (Petroni et al., 2019). For example, BERT is able to * Equal contribution 1 Google Research. Correspondence to: Kelvin Guu <kguu@google.com>, Kenton Lee <ken-tonl@google.com>, Zora Tung <gatoatigrado@google.com>, Panupong Pasupat <ppasupat@google.com>, Ming-Wei Chang <mingweichang@google.com>. Figure 1. REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus, Z (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in Z-a significant computational challenge that we address. correctly predict the missing word in the following sentence: \"The is the currency of the United\n\nKingdom\" (answer: \"pound\").\n\nIn these language models, the learned world knowledge is stored implicitly in the parameters of the underlying neural network. This makes it difficult to determine what knowledge is stored in the network and where. Furthermore, storage space is limited by the size of the network-to capture more world knowledge, one must train ever-larger networks, which can be prohibitively slow or expensive.\n\nTo capture knowledge in a more interpretable and modular way, we propose a novel framework, Retrieval-Augmented Language Model (REALM) pre-training, which augments language model pre-training algorithms with a learned textual knowledge retriever. In contrast to models that store knowledge in their parameters, this approach explicitly exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during inference. Before making each prediction, the language model uses the retriever to retrieve documents 1 from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction. Learning this model end-toend requires backpropagating through a retrieval step that considers an entire corpus of textual knowledge, as shown in Figure 1.\n\nThe key intuition of REALM is to train the retriever using a performance-based signal from unsupervised text: a retrieval that improves the language model's perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized. For example, in Figure 1, if the model needs to fill the blank in \"the at the top of the pyramid\", the retriever should be rewarded for selecting a document containing \"The pyramidion on top allows for less material higher up the pyramid\". We achieve this behavior by modeling our retrieve-then-predict approach as a latent variable language model and optimizing the marginal likelihood.\n\nIncorporating a large-scale neural retrieval module during pre-training constitutes a significant computational challenge, since the retriever must consider millions of candidate documents for each pre-training step, and we must backpropagate through its decisions. To address this, we structure the retriever such that the computation performed for each document can be cached and asynchronously updated, and selection of the best documents can be formulated as Maximum Inner Product Search (MIPS).\n\nNumerous prior works have demonstrated the benefit of adding a discrete retrieval step to neural networks (Miller et al., 2016;Chen et al., 2017), but did not apply the framework to language model pre-training and employed non-learned retrievers to handle large-scale document collections. In the language modeling literature, the k-Nearest Neighbor Language Model (Khandelwal et al., 2019) (kNN-LM) retrieves similar LM examples to improve memorization. However, kNN-LM was not finetuned for downstream tasks, perhaps because it is unclear how to adapt the retrieval mechanism: a kNN can only use examples labeled for the target task-during fine-tuning, this precludes LM examples, which contain the desired world knowledge. In contrast, REALM's retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.\n\nWe evaluate our approach by fine-tuning the models pre-trained with REALM on the task of Opendomain Question Answering (Open-QA), one of the most knowledge-intensive tasks in natural language processing. We evaluate on three popular Open-QA benchmarks (NATURALQUESTIONS-OPEN, WEBQUESTIONS, and CURATEDTREC) and compare to state-of-the-art Open-QA models, including both extremely large models that store knowledge implicitly (such as T5) as well as previous approaches that also use a knowledge retriever to access external knowledge, but implement retrieval in a more heuristic fashion Min et al., 2019a;Asai et al., 2019). REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy. We also demonstrate qualitative benefits of REALM, including interpretability and modularity.\n\n\n"}, {"paperid": "paper1", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.", "introduction": "\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [41]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [45,46]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can't straightforwardly provide insight into their predictions, and may produce \"hallucinations\" [34]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [18,22,42] can address some of these issues because knowledge can be directly revised and expanded, and its access can be inspected and interpreted. REALM [18] and ORQA [27], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results, Figure 1: An overview of retrieval-augmented generation (RAG). We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained encoder-decoder (Generator) and fine-tune end-to-end. For some query x, we use Maximum Inner Product Search (MIPS) to find the top-K most relevant documents of all documents z i . To make the final prediction y, we treat z as a latent variable and marginalize over the encoder-decoder predictions given different documents.\n\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \"workhorse of NLP,\" i.e. sequence-to-sequence (seq2seq) models.\n\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained generative seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed using a pre-trained neural retriever. We combine these components in an end-to-end probabilistic model; the document retriever (Dense Passage Retriever [22], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [28]) then conditions on both these latent documents and the input to generate the output. We marginalize the latent variables through a top-K approximation, either on a per answer basis (assuming the same document is responsible for all tokens) or a per answer token basis (assuming different documents can be responsible for different tokens). Just like T5 [45] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the sequence generator and retriever are jointly learned.\n\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks-e.g. in memory networks [58,49], stackaugmented networks [21] and memory layers for transformers [26]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained knowledge-access mechanisms, the ability to access knowledge is present without additional training.\n\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks. Our RAG models achieve state-of-the-art results on open Natural Questions [25], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [20]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For the FEVER [50] fact verification task, we achieve results within 4% of sophisticated, state-of-the-art pipeline models which use strong supervision. Finally, we show that the non-parametric memory can be replaced in order to control generation, demonstrating a simple mechanism to update the knowledge that the model uses as facts about the world change.\n\n\n"}, {"paperid": "paper2", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.", "introduction": "\n\nLarge language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022b;Yasunaga et al., 2022), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model Figure 1. Different from previous retrieval-augmented approaches  that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served via APIs. Izacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.\n\nIn this work, we introduce REPLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy. As shown in arXiv:2301.12652v4 [cs.CL] 24 May 2023 Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.\n\nWe also introduce REPLUG LSR (REPLUG with LM-Supervised Retrieval), a training scheme that can further improve the initial retrieval model in REPLUG with supervision signals from a black-box language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work  that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\n\nOur experiments show that REPLUG can improve the performance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019;Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) leads to additional improvements, including up to 6.3% increase in GPT-3 175B language modeling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs (>100B model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:\n\n\u2022 We introduce REPLUG ( \u00a73), the first retrievalaugmented language modeling framework for enhancing large black-box language models with retrieval.\n\n\u2022 We propose a training scheme ( \u00a74) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.\n\n\u2022 Evaluations on language modeling ( \u00a76), open-domain QA and MMLU demonstrate that REPLUG can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters.\n\n\n"}, {"paperid": "paper3", "title": "Few-shot Learning with Retrieval Augmented Language Models", "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.", "introduction": "\n\nLarge language models (LLMs) are impressive few-shot learners Rae et al., 2021;Hoffmann et al., 2022;Chowdhery et al., 2022). They are able to learn new tasks with very few examples or even from instructions alone. For this generalisation ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the larger training data. While it is intuitive to assume that increased reasoning abilities lead to better generalisation, and hence few-shot learning, the same is not true for in-parameter memorisation. Specifically, it is unclear to what extent effective few-shot learning requires vast knowledge in the parameters of the model.\n\nIn this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from generalisation. To do so, we leverage the fact that memory can be outsourced and replaced by an external non-parametric knowledge source by employing a retrieval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, interpretability and efficiency (Guu et al., 2020;Borgeaud et al., 2021, inter alia). However, retrieval-augmented models have yet to \u2026 \u2026 Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot performance on knowledge tasks, and uses retrieval during both pre-training and fine-tuning.\n\ndemonstrate compelling few-shot learning capabilities. In this work we address this gap, and present Atlas, a retrieval-augmented language model capable of strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners.\n\nAtlas retrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder architecture, based on the Contriever . The retrieved documents are processed, along with the current context, by a sequence-to-sequence model using the Fusion-in-Decoder architecture ) that generates the corresponding output. We study the impact of different techniques to train Atlas on its few-shot performance on a range of downstream tasks, including question answering and fact checking. We find that jointly pre-training the components is crucial for few-shot performance, and we carefully evaluate a number of existing and novel pre-training tasks and schemes for this purpose. Atlas achieves strong downstream performance in both few-shot and resource-rich settings. For example, with only 11B parameters, Atlas achieves an accuracy of 42.4% on NaturalQuestions using 64 training examples (45.1% with a Wikipedia-only index), outperforming PaLM (Chowdhery et al., 2022), a 540B parameter model by almost 3 points, and 64.0% in a full-dataset setting with a Wikipedia index, establishing a new state of the art by 8 points.\n\nIn summary we make the following contributions:\n\n\u2022 A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample efficiency.\n\n\u2022 The findings of this study lead to a retrieval-augmented language model, called Atlas, that exhibits few-shot abilities that emerge at lower scale than standard LLM.\n\n\u2022 We provide an exploration of fine-tuning strategies to efficiently adapt both the retriever and the language model to the task at hand.\n\n\u2022 Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15\u00d7 more parameters on MMLU.\n\n\u2022 Experiments investigating full-dataset finetuning, setting new state-of-the-art results in NaturalQuestions (+8.1%), TriviaQA (+9.3%) and 5 KILT Tasks.\n\n\u2022 Experiments demonstrating the updatability and interpretability characteristics of Atlas.\n\n\u2022 Experiments demonstrating that a compressed index using product quantisation achieves comparable performance as an uncompressed index while resulting in a 5x memory reduction.\n\nOur code, pretrained Atlas checkpoints, and various supporting data are available at https://github.com/ facebookresearch/atlas\n\n\n"}, {"paperid": "paper4", "title": "Internet-augmented language models through few-shot prompting for open-domain question answering", "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.", "introduction": "\n\nUndoubtedly, large-scale language models (LSLMs) present a research breakthrough for language research, particularly for their state-of-the-art language modeling results [1,2] and impressive generative capabilities. Above all, increasing scale has made few-shot learning a defining new paradigm for language models (LMs). Due to the versatility of prompting, these models can now be quickly adapted using only a handful of examples to perform tasks ranging from question answering and numeric reasoning to creative writing [3]. All these considerations place few-shot LSLMs at an excellent position to be used as building blocks for open-ended and \"in the wild\" user interactions.\n\nDespite these successes, few-shot LSLMs still lack a key ingredient; they are susceptible to hallucinations [4] caused by incorrect retrieval of knowledge stored in their weights or due to the model having incomplete or outdated knowledge. As for many user interactions we expect factuality to play an important role, it is imperative to find ways to keep LSLMs up-to-date and grounded to factual and new information as it becomes available. As the current trend sees the size of these models to continually grow, mitigating those issues should rely on flexible and robust approaches that can be easily transferred to different domains and tasks.\n\nHere, we aim to capitalize on the unique benefits offered by pre-trained LSLMs and propose to overcome some of their limitations by drawing ideas from semi-parametric models [5][6][7][8] that ground their decisions in external retrieved evidence to reduce hallucinations and improve factuality [9]. Specifically, we use the Internet as a source of up-to-date knowledge, and rely on the powerful few-shot capabilities of these LSLMs to learn how to use it effectively for answering questions. Taking open-domain question answering as a task where factual correctness is vital, we design a system that given a question uses a retrieval model to retrieve relevant documents from the Internet. Then, using few-shot learning we prompt the model to answer the question via conditioning on the retrieved documents, without the need to fine-tune or learn extra parameters. As a retrieval system we use a search engine -in particular Google Search -allowing us to treat the whole web as a knowledge source. While Wikipedia has been the dominant knowledge source driving progress on a multitude of tasks, given the current progress and the quest towards more complex interactions, there has never been a better time to widen their scope, embracing the opportunities working with the whole web, such as considering a wider range of topics and views, as well as the many challenges, such as working with more noisy and potentially uncurated and unsafe text in the wild. Indeed, there is momentum building up in breaking away from Wikipedia-only research [10][11][12][13].\n\nTo test the effectiveness of equipping LSLMs with Internet search on open-domain question answering, we use a mix of single-hop and multi-hop, language generation and classification tasks. We find that our biggest LSLMs benefit from conditioning on the web through few-shot prompting. For the language generation tasks, we see a relative performance increase of 15%-30% over the commonly used closed-book few-shot approach. Surprisingly, we find that our method achieves gains, albeit smaller, even on complex multi-hop questions, despite the fact that these questions suffer from higher retrieval errors. Moreover, we see that in certain cases conditioning models on the Internet makes up performance-wise for their smaller size. While perhaps the mainstream view places scaling models' parameters as the primary way to increase their few-shot performance, our results add to the stream of work that emphasizes instead better use of the models' powerful prompting abilities [14,15]. As such, our approach presents a lightweight method applicable to virtually any pre-trained LM without the need for fine-tuning or adding extra learnable parameters. Finally, increasing the inferencetime compute of models via sampling multiple answers and reranking using scores computed from the same LSLMs not only adds further performance gains, but also alleviates generally decreased performance of smaller few-shot LMs, partly closing their performance gap with larger models.\n\nAll in all, our findings hint at the possibility of slowing down the race towards the biggest model and instead shifting the attention to more targeted and effective use of models' few-shot capabilities in combination with increasing inference-time compute, a generally more scalable approach.\n\n\n"}, {"paperid": "paper5", "title": "Rethinking with Retrieval: Faithful Large Language Model Inference", "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.", "introduction": "\n\nLarge language models (LLMs) have shown exceptional performance across various tasks through in-context learning without task-specific training or fine-tuning (Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;Ouyang et al., 2022). Recent progress in prompting Kojima et al., 2022) and decoding  has made it feasible for LLMs to tackle tasks that demand complex reasoning. * Part of this work was done while the author was at the University of Pennsylvania. 1 Our code is publicly available at https://github. com/HornHehhf/RR.  Figure 1: An overview of three approaches for using LLMs: (a) Standard prompting for generating a prediction in response to a query. (b) Chain-of-thought prompting for generating both an explanation and a prediction in response to a query. (c) Rethinking with retrieval, our proposed approach for using the decomposed reasoning steps obtained from chain-of-thought prompting to retrieve relevant external knowledge for LLMs, leading to more faithful explanations and improved predictions in response to a query.\n\nHowever, the knowledge stored in LLMs might inevitably be incomplete, out-of-date, or incorrect. As a result, external sources of knowledge, such as Wikipedia, may be essential for the successful deployment of LLMs for real-world applications. Previously, people tried to utilize knowledge for smaller language models (LMs), such as T5 (Raffel et al., 2020), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019). However, these methods often require additional training or fine-tuning, which can be costly and thus impractical for LLMs.\n\nIn this paper, we present a post-processing approach called rethinking with retrieval (RR) for utilizing external knowledge in LLMs. Our method begins by using the chain-of-thought (CoT) prompting method  to generate a diverse set of reasoning paths, as described in . We then use each reasoning step in those paths to retrieve relevant external knowledge, which enables RR to provide more faithful explanations and more accurate predictions, as illustrated in Figure 1.\n\nWe evaluate the effectiveness of our proposed method, RR, on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning, using GPT-3 175B (Brown et al., 2020) and different external knowledge sources: Wikipedia, Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), WordNet (Miller, 1995), and Conceptnet (Speer et al., 2017). The results demonstrate that RR consistently outperforms all baselines on all three tasks without requiring additional training or fine-tuning, indicating the superiority of our approach in leveraging external knowledge to enhance the performance of LLMs.\n\n2 Related Work Enhancing LMs through retrieval. Retrievalenhanced LMs have received significant attention as a means of improving performance through the incorporation of external knowledge. For example, the k-most similar training contexts can be retrieved to improve the estimation of the next word distribution in both the training stage (Borgeaud et al., 2021) and the inference stage (Khandelwal et al., 2020). Furthermore, search query generators have been adopted to generate search queries for search engines to retrieve relevant documents (Komeili et al., 2022;Shuster et al., 2022;Thoppilan et al., 2022). Other approaches have utilized retrieved documents as the additional context in generation tasks (Joshi et al., 2020;Guu et al., 2020;Lewis et al., 2020). Nakano et al. (2021) instead use human feedback in a text-based web-browsing environment. Among these previous works, Khandelwal et al. (2020) is most closely related to our approach. However, they focus on improving local inference by using the nearest neighbor datastore constructed from training data, whereas we focus on conducting faithful inference using external knowledge. In contrast to other aforementioned approaches, which require training or fine-tuning to incorporate retrieved knowledge, we propose a post-processing method for leveraging retrieved knowledge without additional training or fine-tuning.\n\nIncorporating external knowledge into LMs. Significant effort has been devoted to leveraging external knowledge to improve the reasoning ability of LMs. Previous work has incorporated external knowledge sources such as WordNet (Miller, 1995) and ConceptNet (Speer et al., 2017) to enhance LMs for tabular reasoning tasks (Neeraja et al., 2021;Varun et al., 2022). Explicit rules have also been added to inputs to improve reasoning ability over implicit knowledge (Talmor et al., 2020). In addition, explicit knowledge from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and implicit knowledge in LLMs have been integrated into a transformer (Vaswani et al., 2017) for visual question answering (Gui et al., 2021). Nye et al. (2021) instead introduces a symbolic reasoning module to improve coherence and consistency in LLMs. Among these previous works, Nye et al. (2021) is the most relevant to our approach. Still, they focus on incorporating logical constraints to improve coherence and consistency, whereas we aim to improve the faithfulness of explanations through the use of external knowledge. In contrast to other aforementioned approaches that incorporate external knowledge before generation and require additional training or fine-tuning, our proposal leverages external knowledge in a postprocessing manner to enhance LMs without additional training or fine-tuning.\n\nUncovering latent Knowledge in LLMs. There has been a line of work exploring the knowledge hidden within LLMs for reasoning. This has included the use of careful prompting to encourage LLMs to generate explanations in the reasoning process, such as through chain of thought prompting in few-shot  or zero-shot (Kojima et al., 2022) learning, or through the use of scratchpads for intermediate computation (Nye et al., 2022). In addition, various methods based on sampling a diverse set of reasoning paths in LLMs have been proposed, including training verifiers to judge the correctness of model completions (Cobbe et al., 2021), calibrating model predictions based on the reliability of the explanations (Ye and Durrett, 2022), and promoting selfconsistency over diverse reasoning paths . Zelikman et al. (2022) instead iteratively bootstrap the ability of LLMs to generate high-quality rationales from a few initial examples. Liu et al. (2022) further propose generating knowledge from LLMs, which is then used as additional input to improve commonsense reasoning. In contrast to this line of work, our proposal focuses on leveraging external knowledge to enhance LLMs, while they aim to explore the knowledge hidden within LLMs.\n\nLLMs have been shown to generate incorrect supporting facts from time to time, even when they accurately capture the perspective needed to answer a question. This phenomenon highlights intrinsic issues in the way LLMs store and retrieve knowledge, including (1) the presence of out-of-date, incorrect, or missing relevant knowledge in the pre-training corpus; (2) incorrect memorization of relevant knowledge during pre-training; and (3) incorrect retrieval of relevant knowledge during the inference stage. To address these issues, we propose the use of RR, which leverages external knowledge through the retrieval of relevant information based on decomposed reasoning steps.\n\nOverview. Given a query Q, we utilize chain-ofthought prompting to generate a diverse set of reasoning paths R 1 , R 2 , \u00b7 \u00b7 \u00b7 R N , where each reasoning path R i consists of an explanation E i followed by a prediction P i . After that, we retrieve relevant knowledge K 1 , \u00b7 \u00b7 \u00b7 K M from a suitable knowledge base KB to support the explanation in each reasoning path, and select the predictionP that is most faithful to this knowledge. To better illustrate our proposal, we use \"Did Aristotle use a laptop?\" as a running example in this work.\n\nChain-of-thought prompting. In contrast to standard prompting, CoT prompting  includes demonstrations of step-by-step reasoning examples in the prompt to produce a series of short sentences that capture the reasoning process. For instance, given the question \"Did Aristotle use a laptop?\", CoT prompting aims to generate the complete reasoning path \"Aristotle died in 322 BC. The first laptop was invented in 1980. Thus, Aristotle did not use a laptop. So the answer is no.\" rather than simply outputs \"No.\" Empirical results show that CoT prompting significantly improves the performance of LLMs on many multistep reasoning tasks. Therefore, we adopt CoT prompting to obtain both explanation E and prediction P for the query Q.\n\nSampling diverse reasoning paths. Similar to , we sample a diverse set of reasoning paths R 1 , R 2 , \u00b7 \u00b7 \u00b7 R N rather than only considering the greedy path as in . For the question \"Did Aristotle use a laptop?\", the potential reasoning paths can be as follows:\n\n(R 1 ) Aristotle died in 2000. The first laptop was invented in 1980. Thus, Aristotle used a laptop. So the answer is yes.\n\n(R 2 ) Aristotle died in 322BC. The first laptop was invented in 2000. Thus, Aristotle did not use a laptop. So the answer is no.\n\n(R 3 ) Aristotle died in 322BC. The first laptop was invented in 1980. Thus, Aristotle did not use a laptop. So the answer is no.\n\nKnowledge retrieval. Different knowledge bases can be used to address different tasks. For example, to address the question \"Did Aristotle use a laptop?\", we can use Wikipedia as the external knowledge base KB. Information retrieval techniques can be applied to retrieve the relevant knowledge K 1 , \u00b7 \u00b7 \u00b7 K M from Wikipedia based on the decomposed reasoning steps. Ideally, we would obtain the following two paragraphs from Wikipedia for this question:\n\n(K 1 ) Aristotle (384-322 BC) was a Greek philosopher and polymath during the Classical period in Ancient Greece. ...\n\n(K 2 ) The Epson HX-20, the first laptop computer, was invented in 1980. ...\n\n\n"}, {"paperid": "paper6", "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit", "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.", "introduction": "\n\nLarge language models (LLMs) have attracted increasing attention from both research community and industry (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023;Chowdhery et al., 2022;Zeng et al., 2022). With tremendous world knowledge stored in parameters (Petroni et al., 2019;Roberts et al., 2020;Jiang et al., 2020) and the Reinforcement Learning * Corresponding author.\n\nfrom Human Feedback (RLHF) techniques (Christiano et al., 2017;Ziegler et al., 2019), LLMs can generate helpful, detailed, and polite texts in response to user inputs. Many studies have demonstrated LLMs' extraordinary abilities in various areas, including nature language processing (Moslem et al., 2023), information retrieval (Sun et al., 2023;Wang et al., 2023;Mao et al., 2023), and recommendation .\n\nHowever, LLMs still tend to hallucinate and sometimes generate texts opposite to facts (Zhou et al., 2021;. To tackle these problems, researchers have proposed a new paradigm to strengthen LLMs with information retrieval systems (retrieval-augmented LLMs) (Shi et al., 2023;Jiang et al., 2023;Nakano et al., 2022), which enables LLMs to retrieve relevant contents from an external repository (knowledge corpus) to generate texts based on them. It has been verified that retrieval-augmented LLMs can generate texts in response to user input with fewer hallucinations (Nakano et al., 2022). Furthermore, by incorporating customized private data resources, retrieval-augmented LLMs can respond to in-domain queries that cannot be answered by LLMs trained with public data.\n\nTo support research in this area and help users build their own in-domain LLM-based systems, we devise RETA-LLM, a RETreival-Augmented LLM toolkit. Different from previous general LLMenhanced toolkits such as LangChain, 1 RETA-LLM focuses on the retrieval-augmented LLMs and provides more plug-in modules. Typically, retrieval-augmented LLMs use a retrieve-andgenerate strategy with two modules: First, they retrieve documents or passages based on user request (document retrieval module); then, they generate answers utilizing these relevant documents as references (answer generation module). In addi-\n\n\n"}, {"paperid": "paper7", "title": "In-Context Retrieval-Augmented Language Models", "abstract": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1", "introduction": "\n\nRecent advances in language models (LMs) have dramatically increased the usefulness of machine-generated text across a wide range of use-cases and domains (Brown et al., 2020).However, the mainstream paradigm of generating text with LMs bears inherent limitations in access to external knowledge.First, LMs are not coupled with any source attribution, and must be trained in order to incorporate up-to-date information that was not seen during training.More importantly, they tend to produce factual inaccuracies and errors (Lin et al., 2022;Maynez et al., 2020;Huang et al., 2020).This problem is present in any LM generation scenario, and is exacerbated when generation is made in uncommon domains or private data.A promising approach for addressing the above is Retrieval-Augmented Language Modeling (RALM), grounding the LM during generation by conditioning on relevant documents retrieved from an external knowledge source.RALM systems include two high level components: (i) document selection, selecting the set of documents upon which to condition; and (ii) document reading, determining how to incorporate the selected documents into the LM generation process.\n\nLeading RALM systems introduced recently tend to be focused on altering the language model architecture (Khandelwal et al., 2020;Borgeaud et al., 2022;Zhong et al., 2022;Levine et al., 2022c;Li et al., 2022).Notably, Borgeaud et al. (2022) introduced RETRO, featuring document reading via nontrivial modifications that require further training to the LM architecture, while using an off-the-shelf frozen BERT retriever for document selection.Although the paper's experimental findings showed impressive performance gains, the need for changes in architecture and dedicated retraining has hindered the wide adoption of such models.\n\nIn this paper, we show that a very simple document reading mechanism can have a large impact, and that substantial gains can also be made by adapting the document selection mechanism to the task of language modeling.Thus, we show that many of the benefits of RALM can be achieved while working with off-the-shelf LMs, even via API access.Specifically, we consider a simple but powerful RALM framework, dubbed In-Context RALM (presented in Section 3), which employs a zero-effort document reading mechanism: We simply prepend the selected documents to the LM's input text (Figure 1).\n\nSection 4 describes our experimental setup.To show the wide applicability of our framework, we performed LM experiments on a suite of five diverse corpora: WikiText-103 (Merity et al., 2016), RealNews (Zellers et al., 2019), and three datasets from The Pile (Gao et al., 2021): ArXiv, Stack Exchange, and FreeLaw.We use open-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT, and LLaMA model families).\n\nIn Section 5 we evaluate the application of off-the-shelf retrievers to our framework.In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM's number of parameters by 2-3\u00d7 across all of the text corpora we examined.In Section 6 we investigate methods for adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom.Our adaptation methods range from using a small LM to perform zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision from the LM signal.These methods lead to further gains in the LM task corresponding to an additional size increase of 2\u00d7 in the LM architecture.As a concrete example of the gains, a 345M parameter GPT-2 enhanced by In-Context RALM outperforms a 762M parameter GPT-2 when employing an off-the-shelf BM25 retriever (Robertson and Zaragoza, 2009), and outperforms a 1.5B parameter GPT-2 when employing our trained LM-oriented reranker (see Figure 2).For large model sizes, our method is even more effective: In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter parameter OPT model (see Figure 4).(Robertson and Zaragoza, 2009) to the LM task ( \u00a75) yields significant gains, and choosing the grounding documents via our new class of Predictive Rerankers ( \u00a76) provides a further boost.See Table 1 for the full results on five diverse corpora.\n\nIn Section 7 we demonstrate the applicability of In-Context RALM to downstream open-domain questions answering (ODQA) tasks.\n\nIn a concurrent work, Shi et al. (2023) also suggest to augment off-the-shelf LMs with retrieved texts by prepending them to the input.Their results are based on training a dedicated retriever for language modeling.In contrast, we focus on the gains achievable in using off-the-shelf retrievers for this task.We show strong gains of this simpler setting by investigating: (1) which off-the-shelf retriever is best suited for language modeling, (2) the frequency of retrieval operations, and (3) the optimal query length.In addition, we boost the off-the-shelf retrieval performance by introducing two reranking methods that demonstrate further gains in perplexity.\n\nWe believe that In-Context RALM can play two important roles in making RALM systems more powerful and more prevalent.First, given its simple reading mechanism, In-Context RALM can serve as a clean probe for developing document retrieval methods that are specialized for the LM task.These in turn can be used to improve both In-Context RALM and other more elaborate RALM methods that currently leverage general purpose retrievers.Second, due to its compatibility with off-the-shelf LMs, In-Context RALM can help drive wider deployment of RALM systems.\n\n\n"}, {"paperid": "paper8", "title": "Improving language models by retrieving from trillions of tokens", "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.", "introduction": "\n\nLanguage modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions ( 1 , . . . , ) = ( | < ). Neural networks have proven to be powerful language models, first in the form of recurrent architectures (Graves, 2013;Jozefowicz et al., 2016;Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters (Brown et al., 2020;Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020). The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data.\n\nIn this work, we endeavor to decouple these, by exploring efficient means of augmenting language models with a massive-scale memory without significantly increasing computations. Specifically, we suggest retrieval from a large text database as a complementary path to scaling language models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions-a semi-parametric approach. At a high level, our Retrieval Transformer (R ) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing retrieval for language modelling work only considers small transformers (100 millions parameters) and databases of limited size (up to billions of tokens) (Guu et al., 2020;Khandelwal et al., 2020;Lewis et al., 2020;Yogatama et al., 2021). To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by \u223c 10\u00d7. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours. Past this, performance begins to degrade, perhaps due to the reduced quality. At evaluation R can be used without retrieval data (R [OFF]), bringing limited performance degradation compared to baseline transformers.\n\ncontributions are the following.\n\n\u2022 We introduce R , a retrieval-enhanced autoregressive language model ( \u00a72.2). We use a chunked cross-attention module to incorporate the retrieved text ( \u00a72.4), with time complexity linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen B model ( \u00a72.3) works at scale, removing the need for training and updating a retriever network.\n\n\u2022 We show that our method scales well with model size and database size ( Fig. 1): R provides a constant gain for models ranging from 150M to 7B parameters, and R can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) ( \u00a74). We show that R can be fine-tuned to achieve competitive performance on downstream tasks such as question answering ( \u00a74.3).\n\n\u2022 We propose an evaluation aware of proximity of test documents with the training set ( \u00a72.6), addressing the problem of test set leakage (Lee et al., 2021). This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation. Using this methodology, we show that the performance of R comes from both explicit neighbour copying and general knowledge extraction ( \u00a74.4).\n\n\n"}, {"paperid": "paper9", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.", "introduction": "\n\nLarge language models are capable of answering complex questions by generating step-bystep natural language reasoning steps-so called chains of thoughts (CoT)-when prompted appropriately (Wei et al., 2022). This approach has been successful when all information needed to answer the question is either provided as context (e.g., algebra questions) or assumed to be present in the model's parameters (e.g., commonsense reasoning). 1 Code, data, and prompts are available at https:// github.com/stonybrooknlp/ircot\n\nIn what country was Lost Gravity manufactured?\n\nThe Lost Gravity was manufactured by Mack Rides.\n\nMack Rides is a company from Germany.\n\nThe answer is Germany.\n\ncumulate docs cumulate docs cumulate docs Figure 1: IRCoT interleaves chain-of-thought (CoT) generation and knowledge retrieval steps in order to guide the retrieval by CoT and vice-versa. This interleaving allows retrieving more relevant information for later reasoning steps, compared to standard retrieval using solely the question as the query.\n\nHowever, for many open-domain questions, all required knowledge is not always available or up-todate in models' parameters and it's beneficial to retrieve knowledge from external sources (Lazaridou et al., 2022;Kasai et al., 2022). How can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks that require complex, multi-step reasoning?\n\nWhile a one-shot retrieval from a knowledge source based solely on the question can successfully augment LMs with relevant knowledge for many factoid-based tasks (Lewis et al., 2020;Guu et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022), this strategy has clear limitations for more complex multi-step reasoning questions. For such questions, one often must retrieve partial knowledge, perform partial reasoning, retrieve additional information based on the outcome of the partial reasoning done so far, and iterate. As an example, consider the question illustrated in Fig. 1, \"In what country was Lost Gravity manufactured?\". The Wikipedia document retrieved using the question (in particular, the roller coaster Lost Gravity) as the query does not mention where Lost Gravity was manufactured. Instead, one must first infer that it was manufactured by a company called Mack Rides, and then perform further retrieval, guided by the inferred company name, to obtain evidence pointing to the manufacturing country.\n\nThus, the retrieval and reasoning steps must inform each other. Without retrieval, a model is likely to generate an incorrect reasoning step due to hallucination. Additionally, without generating the first reasoning step, the text supporting the second step can't be identified easily given the lack of lexical or even semantic overlap with the question. In other words, we need retrieved facts in order to generate factually correct reasoning steps and the reasoning steps to retrieve relevant facts.\n\nBased on this intuition, we propose an interleaving approach to this problem, where the idea is to use retrieval to guide the chain-of-thought (CoT) reasoning steps and use CoT reasoning to guide the retrieval. Fig. 1 shows an overview of our retrieval method, which we call IRCoT. 2 We begin by retrieving a base set of paragraphs using the question as a query. Subsequently, we alternate between the following two steps: (i) extend CoT: use the question, the paragraphs collected thus far, and the CoT sentences generated thus far to generate the next CoT sentence; (ii) expand retrieved information: use the last CoT sentence as a query to retrieve additional paragraphs to add to the collected set. We repeat these steps till the CoT reports an answer or we reach the maximum allowed number of reasoning steps. Upon termination, all collected paragraphs are returned as the retrieval outcome. Finally, we use these as the context for answering the question via direct QA prompting (Brown et al., 2020) or CoT prompting (Wei et al., 2022).\n\nWe evaluate the efficacy of our system on 4 multi-step reasoning datasets under an open-domain setting: HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and IIRC (Ferguson et al., 2020). Our experiments using OpenAI GPT3 (code-davinci-002) (Brown et al., 2020;Ouyang et al., 2022;Chen et al., 2021) demon-2 Interleaved Retrieval guided by Chain-of-Thought. strate that retrieval using IRCoT is substantially more effective than the baseline, one-step, questionbased retrieval by 11-21 recall points under a fixedbudget optimal recall setup. 3 When IRCoT is used in conjunction with a prompting-based reader, it also leads to substantial improvement (up to 15 F1 points) in downstream few-shot QA performance and reduces factual errors in generated CoT by up to 50%. Our approach also works on much smaller Flan-T5 models (11B, 3B, and 0.7B) showing similar trends. In particular, we find QA using Flan-T5-XL (3B) with IRCoT even outperforms the 58X larger GPT3 with a one-step questionbased retrieval. Furthermore, these improvements also hold up in an out-of-distribution (OOD) setting where the demonstrations from one dataset are used when testing on another dataset. Lastly, we note that our QA scores exceed those reported by recent works on few-shot prompting for open-domain QA (ODQA) (Khot et al., 2023;Press et al., 2022;Yao et al., 2022), although a fair apples-to-apples comparison with them isn't possible (cf. Appendix C).\n\nIn summary, our main contribution is a novel retrieval method, IRCoT, that leverages LMs' chainof-thought generation capabilities to guide retrieval and uses retrieval in turn to improve CoT reasoning. We demonstrate that IRCoT:\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain QA datasets, in both IID and OOD settings; 2. reduces factual errors in generated CoTs; and 3. improves performance with both large-scale (175B models) as well as smaller-scale models (Flan-T5-*, \u226411B) without any training.\n\n\n"}, {"paperid": "paper10", "title": "Active Retrieval Augmented Generation", "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.", "introduction": "\n\nGenerative language models (LMs) (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023;Chowdhery et al., 2022;Zhang et al., 2022;Touvron et al., 2023;Zhao et al., 2023) have become a foundational component in natural language processing (NLP) systems with their remarkable abilities.Although LMs have memorized some world knowledge during training (Petroni et al., 2019;Roberts et al., 2020;Jiang et al., 2020), they still tend to hallucinate and create imaginary content (Maynez et al., 2020;Zhou et al., 2021).Augmenting LMs with retrieval components that look up relevant information from external knowledge resources is a promising direction to address hallucination (Khandelwal et al., 2020;Izacard et al., 2022).\n\nRetrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve documents based on the user's input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017;Guu et al., 2020;Lewis et al., 2020;Izacard and Grave, 2021;Sachan et al., 2021;Lee et al., 2021;Jiang et al., 2022;Izacard et al., 2022;Nakano et al., 2021;Qian et al., 2023;Lazaridou et al., 2022;Shi et al., 2023).These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for shortform knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019;Joshi et al., 2017), where the information needs are clear in the user's input, and it is sufficient to retrieve relevant knowledge once solely based on the input.\n\nIncreasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as longform QA (Fan et al., 2019;Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021;Hayashi et al., 2021;Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022;Ho et al., 2020;Geva et al., 2021;Hendrycks et al., 2020).In contrast to short-form generation, long-form generation presents complex information needs that are not always evident from the input alone.Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowledge throughout the generation process.For example, to generate a summary about a particular topic, the initial retrieval based on the topic name Joe Biden (born November 20, 1942) is the 46th president of the United States.\n\nHe graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.\n\nJoe Biden attended the University of Pennsylvania, where he earned a law degree.\n\n\n"}]