[{"paperid": "paper0", "title": "Fast Online Object Tracking and Segmentation: A Unifying Approach", "abstract": "In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/~qwang/SiamMask.", "introduction": "\n\nTracking is a fundamental task in any video application requiring some degree of reasoning about objects of interest, as it allows to establish object correspondances between frames [38]. It finds use in a wide range of scenarios such as automatic surveillance, vehicle navigation, video labelling, human-computer interaction and activity recognition. Given the location of an arbitrary target of interest in the first frame of a video, the aim of visual object tracking is to estimate its position in all the subsequent frames [70,66,55].\n\nFor many applications, it is important that tracking can be performed online, while the video is streaming. In other words, the tracker should not make use of future frames to * Equal contribution. Work done while at University of Oxford.\n\nInit Estimates Figure 1. Our proposed method aims at distilling the best from the two tasks of object tracking and video object segmentation. Like conventional object trackers, it relies on a simple bounding box initialisation (blue) and operates online. Differently from state-ofthe-art trackers such as ECO [15] (red), SiamMask (green) is able to produce binary segmentation masks, which can more accurately describe the target object.\n\nreason about the current position of the object [30]. This is the scenario portrayed by visual object tracking benchmarks, which represent the target object with a simple axisaligned [63,34,42,60,43] or rotated [30] bounding box. Such a simple annotation helps to keep the cost of data labelling low; what is more, it allows a user to perform a quick and simple initialisation of the target.\n\nSimilar to object tracking, the task of semi-supervised video object segmentation (VOS) requires estimating the position of an arbitrary target specified in the first frame of a video. However, in this case the object representation consists of a binary segmentation mask which expresses whether each pixel belongs to the target or not [46]. Such a detailed representation is more desirable for applications that require pixel-level information, like video editing [44] and rotoscoping [41]. Understandably, producing pixel-level estimates requires more computational re-sources than a simple bounding box. As a consequence, VOS methods have been traditionally slow, often requiring several seconds per frame (e.g. [62,58,45,2]). Very recently, there has been a surge of interest in faster approaches [67,40,64,11,10,24,23]. However, even the fastest still cannot operate in real-time.\n\nIn this paper, we aim at narrowing the gap between arbitrary object tracking and VOS by proposing SiamMask, a simple multi-task learning approach that can be used to address both problems. Our method is motivated by the success of fast tracking approaches based on fullyconvolutional Siamese networks trained offline on millions of pairs of video frames [4,31] and by the very recent availability of a large video dataset with pixel-wise annotations such as YouTube-VOS [65]. We aim at retaining the offline trainability and online speed of these methods while at the same time significantly refining their representation of the target object, which is limited to a simple axis-aligned bounding box.\n\nTo achieve this goal, we simultaneously train a Siamese network on three tasks, each corresponding to a different strategy to establish correspondances between the target object and candidate regions in the new frames. As in the fully-convolutional approach of Bertinetto et al. [4], one task is to learn a measure of similarity between the target object and multiple candidates in a sliding window fashion. The output is a dense response map which only indicates the location of the object, without providing any information about its spatial extent. To refine this information, we simultaneously learn two further tasks: bounding box regression using a Region Proposal Network [52,31] and classagnostic binary segmentation [49]. Notably, binary labels are only required during offline training to compute the segmentation loss and not during tracking. In our proposed architecture, each task is represented by a different branch departing from a shared CNN and contributes towards a final loss, which sums the three outputs together.\n\nOnce trained, SiamMask solely relies on a single bounding box initialisation, operates online without updates and produces object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity and fast speed, SiamMask establishes a new state-of-the-art on VOT-2018 for the problem of real-time object tracking. Moreover, the same method is also very competitive against recent semi-supervised VOS approaches on DAVIS-2016 and DAVIS-2017, while being the fastest by a large margin. This result is achieved with a simple bounding box initialisation (as opposed to a mask) and without adopting costly techniques often used by VOS approaches such as fine-tuning [39,45,2,61], data augmentation [25,33] and optical flow [58,2,45,33,11].\n\nThe rest of this paper is organised as follows. Section 2 briefly outlines some of the most relevant prior work in vi-sual object tracking and semi-supervised VOS; Section 3 describes our proposal; Section 4 evaluates it on four benchmarks and illustrates several ablative studies; Section 5 concludes the paper.\n\n\n"}, {"paperid": "paper1", "title": "D3S \u2013 A Discriminative Single Shot Segmentation Tracker", "abstract": "Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-art trackers on the TrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video object segmentation benchmark and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time.", "introduction": "\n\nVisual object tracking is one of core computer vision problems. The most common formulation considers the task of reporting target location in each frame of the video given a single training image. Currently, the dominant tracking paradigm, performing best in evaluations [22,24], is correlation bounding box tracking [11,3,33,2,54,28] where the target represented by a multi-channel rectangular template is localized by cross-correlation between the template and a search region.\n\nState-of-the-art template-based trackers apply an efficient brute-force search for target localization. Such strategy is appropriate for low-dimensional transformations like translation and scale change, but becomes inefficient for more general situations e.g. such that induce an aspect ratio change and rotation. As a compromise, modern track- ers combine approximate exhaustive search with sampling and/or bounding box refinement/regression networks [10,27] for aspect ratio estimation. However, these approaches are restricted to axis-aligned rectangles. Estimation of high-dimensional template-based transformation is unreliable when a bounding box is a poor approximation of the target [31]. This is common -consider e.g. elongated, rotating, deformable objects, or a person with spread out hands. In these cases, the most accurate and well-defined target location model is a binary per-pixel segmentation mask. If such output is required, tracking becomes the video object segmentation task recently popularized by DAVIS [38,40] and YoutubeVOS [51] challenges.\n\nUnlike in tracking, video object segmentation challenges typically consider large targets observed for less than 100 frames with low background distractor presence. Top video object segmentation approaches thus fare poorly in shortterm tracking scenarios [24] where the target covers a fraction of the image, substantially changes its appearance over a longer period and moves over a cluttered background.\n\nBest trackers apply visual model adaptation, but in the case of segmentation errors it leads to an irrecoverable tracking failure [41]. Because of this, in the past, segmentation has played only an auxiliary role in template-based trackers [1], constrained DCF learning [33] and tracking by 3D model construction [20].\n\nRecently, the SiamRPN [28] tracker has been extended to produce high-quality segmentation masks in two stages [50] -the target bounding box is first localized by SiamRPN branches and then a segmentation mask is computed only within this region by another branch. The twostage processing misses the opportunity to treat localization and segmentation jointly to increase robustness. Another drawback is that a fixed template is used that cannot be discriminatively adapted to the changing scene.\n\nWe propose a new single-shot discriminative segmentation tracker, D3S, that addresses the above-mentioned limitations. The target is encoded by two discriminative visual models -one is adaptive and highly discriminative, but geometrically constrained to an Euclidean motion (GEM), while the other is invariant to broad range of transformation (GIM, geometrically invariant model), see Figure 1.\n\nGIM sacrifices spatial relations to allow target localization under significant deformation. On the other hand, GEM predicts only position, but discriminatively adapts to the target and acts as a selector between possibly multiple target segmentations inferred by GIM. In contrast to related trackers [50,27,10], the primary output of D3S is a segmentation map computed in a single pass through the network, which is trained end-to-end for segmentation only ( Figure 2). Some applications and most tracking benchmarks require reporting the target location as a bounding box. As a secondary contribution, we propose an effective method for interpreting the segmentation mask as a rotated rectangle. This avoids an error-prone greedy search and naturally addresses changes in location, scale, aspect ratio and rotation.\n\nD3S outperforms all state-of-the-art trackers on most of the major tracking benchmarks [23,24,19,35] despite not being trained for bounding box tracking. In video object segmentation benchmarks [38,40], D3S outperforms the leading segmentation tracker [50] and performs on par with top video object segmentation algorithms (often tuned to a specific domain), yet running orders of magnitude faster. Note that D3S is not re-trained for different benchmarks -a single pre-trained version shows remarkable generalization ability and versatility.\n\n\n"}, {"paperid": "paper2", "title": "Siam R-CNN: Visual Tracking by Re-Detection", "abstract": "We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www.vision.rwth-aachen.de/page/siamrcnn.", "introduction": "\n\nWe approach Visual Object Tracking using the paradigm of Tracking by Re-Detection. We present a powerful novel re-detector, Siam R-CNN, an adaptation of Faster R-CNN [73] with a Siamese architecture, which re-detects a template object anywhere in an image by determining if a region proposal is the same object as a template region, and regressing the bounding box for this object. Siam R-CNN is robust against changes in object size and aspect ratio as the proposals are aligned to the same size, which is in contrast to the popular cross-correlation-based methods [49].\n\nTracking by re-detection has a long history, reaching back to the seminal work of Avidan [1] and Grabner et al. [28]. Re-detection is challenging due to the existence of distractor objects that are very similar to the template object. In the past, the problem of distractors has mainly been approached by strong spatial priors from previous predictions [4,49,48], or by online adaptation [1,28,2,75,30,76,42]. Both of these strategies are prone to drift.\n\nWe instead approach the problem of distractors by making two novel contributions beyond our Siam R-CNN re- \u2020 Work performed both while at the RWTH Aachen University and on a research visit at the University of Oxford. detector design. Firstly we introduce a novel hard example mining procedure which trains our re-detector specifically for difficult distractors. Secondly we propose a novel Tracklet Dynamic Programming Algorithm (TDPA) which simultaneously tracks all potential objects, including distractor objects, by re-detecting all object candidate boxes from the previous frame, and grouping boxes over time into tracklets (short object tracks). It then uses dynamic programming to select the best object in the current timestep based on the complete history of all target object and distractor object tracklets. By explicitly modeling the motion and interaction of all potential objects and pooling similarity information from detections grouped into tracklets, Siam R-CNN is able to effectively perform long-term tracking, while being resistant to tracker drift, and being able to immediately re-detect objects after disappearance. Our TDPA requires only a small set of new re-detections in each timestep, updating its tracking history iteratively online. This allows Siam R-CNN to run at 4.7 frames per second (FPS) and its speed-optimized variant to run at more than 15 FPS.\n\nWe present evaluation results on a large number of datasets. Siam R-CNN outperforms all previous methods on six short-term tracking benchmarks as well as on four long-term tracking benchmarks, where it achieves especially strong results, up to 10 percentage points higher than previous methods. By obtaining segmentation masks using an off-the-shelf box-to-segmentation network, Siam R-CNN also outperforms all previous Video Object Segmentation methods that only use the first-frame bounding box (without the mask) on four recent VOS benchmarks.\n\n\n"}, {"paperid": "paper3", "title": "Do Different Tracking Tasks Require Different Appearance Models?", "abstract": "Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple ``heads'' that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems.", "introduction": "\n\nUnlike popular image-based computer vision tasks such as classification and object detection, which are (for the most part) unambiguous and clearly defined, the problem of object tracking has been considered under different setups and scenarios, each motivating the design of a separate set of benchmarks and methods. For instance, for the Single Object Tracking (SOT) and Video Object Segmentation (VOS) communities [93,40,65], tracking means estimating the location of an arbitrary user-annotated target object throughout a video, where the location of the object is represented by a bounding box in SOT and by a pixel-wise mask in VOS. Instead, in multiple object tracking settings (MOT [56], MOTS [80] and PoseTrack [2]), tracking means connecting sets of (often given) detections across video frames to address the problem of identity association and forming trajectories. Despite these tasks only differing in the number of objects per frame to consider and observation format (bounding boxes, keypoints or masks), the best practices developed by the methods tackling them vary significantly.\n\nThough the proliferation of setups, benchmarks and methods is positive in that it allows specific use cases to be thoroughly studied, we argue it makes increasingly harder to effectively study one of the fundamental problems that all these tasks have in common, i.e. what constitutes a good representation to track objects throughout a video? Recent advancements in large-scale models for language [20,8] and vision [32,13] have suggested that a strong representation can help addressing multiple downstream tasks. Similarly, we speculate that a good representation is likely to benefit many different tracking tasks, regardless of their specific setup. In order to validate our speculation, in this paper we present a framework that allows to adopt the same appearance model to address five different tracking tasks (Figure 2). In our taxonomy (Figure 4), we consider existing tracking tasks as problems that have either propagation or association at their core. When the core problem is propagation (as in SOT and VOS), one has to localise a target object in the current frame given its location in the previous one. Instead, in association problems (MOT, MOTS, and PoseTrack), target states in both previous and current frames are given, and the goal is to determine the correspondence between the two sets of observations. We show how most tracking tasks currently considered by the community can be simply expressed starting from the primitives of propagation or association. For propagation tasks, we employ existing box and mask propagation algorithms [7,84,81]. For association tasks, we propose a novel reconstruction-based metric that leverages fine-grained correspondence to measure similarities between observations. In the proposed framework, each individual task is assigned to a dedicated \"head\" that allows to represent the object(s) in the appropriate format to compare against prior arts on the relevant benchmarks.\n\nNote that, in our framework, only the appearance model contains parameters that can be learned via back-propagation, and that we do not experiment with appearance models that have been trained on specific tracking tasks. Instead, we adopt models trained via recent self-supervised learning (SSL) techniques and that have already demonstrated their effectiveness on a variety of image-based tasks. Our motivation is twofold. First, SSL models are particularly interesting for our use-case, as they are explicitly conceived to be of general purpose. As a byproduct, our work also serves the purpose of evaluating and comparing appearance models obtained from self-supervised learning approaches (see Figure 1). Second, we hope to facilitate the tracking community in directly benefiting from the rapid advancements of the self-supervised learning literature.\n\nTo summarise, the contributions of our work are as follows:\n\n\u2022 We propose UniTrack, a framework that supports five tracking tasks: SOT [93], VOS [65], MOT [56], MOTS [80], and PoseTrack [2]; and that can be easily extended to new ones.\n\n\u2022 We show how UniTrack can leverage many existing general-purpose appearance models to achieve a performance that is competitive with the state-of-the-art on several tracking tasks.\n\n\u2022 We propose a novel reconstruction-based similarity metric for association that preserves fine-grained visual features and supports multiple observation formats (box, mask and pose).\n\n\u2022 We perform an extensive evaluation of self-supervised models, significantly extending the empirical analysis of prior literature to video-based tasks.\n\n\n"}, {"paperid": "paper4", "title": "Towards Grand Unification of Object Tracking", "abstract": "We present a unified method, termed Unicorn, that can simultaneously solve four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the same model parameters. Due to the fragmented definitions of the object tracking problem itself, most existing trackers are developed to address a single or part of tasks and overspecialize on the characteristics of specific tasks. By contrast, Unicorn provides a unified solution, adopting the same input, backbone, embedding, and head across all tracking tasks. For the first time, we accomplish the great unification of the tracking network architecture and learning paradigm. Unicorn performs on-par or better than its task-specific counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17, BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will serve as a solid step towards the general vision model. Code is available at https://github.com/MasterBin-IIAU/Unicorn.", "introduction": "\n\nCompared with weak AI designed for solving one specific task, artificial general intelligence (AGI) is expected to understand or learn any intellectual task that a human being can. Although there is still a large gap between this ambitious goal and the intellectual algorithms of today, some recent works [20,51,81,21] have begun to explore the possibility of building general vision models to address several vision tasks simultaneously.\n\nObject tracking is one of the fundamental tasks in computer vision, which aims to build pixel-level or instance-level correspondence between frames and to output trajectories typically in the forms of boxes or masks. Over the years, according to different application scenarios, the object tracking problem has been mainly divided into four separate sub-tasks: Single Object Tracking (SOT) [18,42], Multiple Object Tracking (MOT) [40,80], Video Object Segmentation (VOS) [46], and Multi-Object Tracking and Segmentation (MOTS) [61,80]. As a result, most tracking approaches are developed for only one of or part of the sub-tasks. Despite convenience for specific applications, this fragmented situation brings into the following drawbacks: (1) Trackers may over-specialize on the characteristic of specific sub-tasks, lacking in the generalization ability. (2) Independent model designs cause redundant parameters. For example, recent deep-learningbased trackers usually adopt similar backbones architectures, but the separate design philosophy hinders the potential reuse of parameters. It is natural to ask a question: Can all main-stream tracking tasks be solved by a unified model?\n\nAlthough some works [64,36,62,70,39] attempt to unify SOT&VOS or MOT& MOTS by adding a mask branch to the existing box-level tracking system, there is still little progress towards the unification of SOT and MOT. There are mainly three obstacles hindering this process. (1) The characteristics of tracked objects vary. MOT usually tracks tens even hundreds of instances of specific categories. In contrast, SOT needs to track one target given in the reference frame no matter what class it belongs to. (2) SOT and MOT require different types of correspondence. SOT requires distinguishing the target from the background. However, MOT needs to match the currently detected objects with previous trajectories. (3) Most SOT methods [3,29,15,5,9,77] only take a small search region as the input to save computation and filter potential distractors. However, MOT algorithms [2,74,8,67,85,90,39] usually take the high-resolution full image as the input for detecting instances as completely as possible.\n\nTo conquer these challenges, we propose two core designs: the target prior and the pixel-wise correspondence. To be specific, (1) the target prior is an additional input for the detection head and serves as the switch among four tasks. For SOT&VOS, the target prior is the propagated reference target map, enabling the head to focus on the tracked target. For MOT&MOTS, by setting the target prior as zero, the head degenerates into the usual class-specific detection head smoothly. (2) The pixel-wise correspondence is the similarity between all pairs of points from the reference frame and the current frame. Both the SOT correspondence (C SOT \u2208 R h \u2032 w \u2032 \u00d7hw ) and the MOT correspondence (C MOT \u2208 R M \u00d7N ) are subsets of the pixel-wise correspondence (C pix \u2208 R hw\u00d7hw ). (3) With the help of the informative target prior and the accurate pixel-wise correspondence, the design of the search region becomes unnecessary for SOT, leading to unified inputs as the full image for SOT and MOT.\n\nTowards the unification of object tracking, we propose Unicorn, a single network architecture to solve four tracking tasks. It takes the reference frame and the current frame as the inputs and produces their visual features by a weight-shared backbone. Then a feature interaction module is exploited to build pixel-wise correspondence between two frames. Based on the correspondence, a target prior is generated by propagating the reference target to the current frame. Finally, the target prior and the visual features are fused and sent to the detection head to get the tracked objects for all tasks.\n\nWith the unified network architecture, Unicorn can learn from various sources of tracking data and address four tracking tasks with the same model parameters.\n\nExtensive experiments show that Unicorn performs on-par or better than taskspecific counterparts on 8 challenging benchmarks from four tracking tasks.\n\nWe summarize that our work has the following contributions:\n\n-For the first time, Unicorn accomplishes the great unification of the network architecture and the learning paradigm for four tracking tasks. -Unicorn bridges the gap among methods of four tracking tasks by the target prior and the pixel-wise correspondence. -Unicorn puts forwards new state-of-the-art performance on 8 challenging tracking benchmarks with the same model parameters. This achievement will serve as a solid step towards the general vision model.\n\n\n"}, {"paperid": "paper5", "title": "Robust Visual Tracking by Segmentation", "abstract": "Estimating the target extent poses a fundamental challenge in visual object tracking. Typically, trackers are box-centric and fully rely on a bounding box to define the target in the scene. In practice, objects often have complex shapes and are not aligned with the image axis. In these cases, bounding boxes do not provide an accurate description of the target and often contain a majority of background pixels. We propose a segmentation-centric tracking pipeline that not only produces a highly accurate segmentation mask, but also internally works with segmentation masks instead of bounding boxes. Thus, our tracker is able to better learn a target representation that clearly differentiates the target in the scene from background content. In order to achieve the necessary robustness for the challenging tracking scenario, we propose a separate instance localization component that is used to condition the segmentation decoder when producing the output mask. We infer a bounding box from the segmentation mask, validate our tracker on challenging tracking datasets and achieve the new state of the art on LaSOT with a success AUC score of 69.7%. Since most tracking datasets do not contain mask annotations, we cannot use them to evaluate predicted segmentation masks. Instead, we validate our segmentation quality on two popular video object segmentation datasets.", "introduction": "\n\nVisual object tracking is the task of estimating the state of a target object for each frame in a video sequence. The target is solely characterized by its initial state in the video. Current approaches predominately characterize the state itself with a bounding box. However, this only gives a very coarse representation of the target in the image. In practice, objects often have complex shapes, undergo substantial deformations. Often, targets do not align well with the image axes, while most benchmarks use axis-aligned bounding boxes. In such cases, the majority of the image content inside the target's bounding box often consists of background regions which provide limited information about the object itself. In contrast, a segmentation mask precisely indicates the object's extent in the image (see Fig. 1 frames #1600 and #3200). Such information is vital in a variety of applications, including video analysis, video editing, and robotics. In this work, we therefore develop an approach for accurate and robust target object segmentation, even in the highly challenging tracking datasets [16,36]. , the VOS method LWL [5] and our proposed method on two tracking sequences from the LaSOT [16] dataset. The ground-truth annotation ( ) is shown in each frame for reference. Our approach is more robust and predicts a more accurate target representation.\n\nWhile severely limiting the information about the target's state in the video, the aforementioned issues with the bounding box representation can itself lead to inaccurate bounding box predictions, or even tracking failure. To illustrate this, Fig. 1 shows two typical tracking sequences. The tracking method STARK [55] (first row) fails to regress bounding boxes that contain the entire object (#1600, #1400) or even starts tracking the wrong object (#0700). Conversely, segmentation masks are a better fit to differentiate pixels in the scene that belong to the background and the target. Therefore, a segmentation-centric tracking architecture designed to work internally with a segmentation mask of the target instead of a bounding box has the potential to learn better target representations, because it can clearly differentiate background from foreground regions in the scene.\n\nA few recent tracking methods [47,54] have recognized the advantage of producing segmentation masks instead of bounding boxes as final output. However, these trackers are typically bounding-box-centric and the final segmentation mask is obtained by a separate box-to-mask post-processing network. These methods do not leverage the accurate target definition of segmentation masks to learn a more accurate and robust internal representation of the target.\n\nIn contrast, most Video Object Segmentation (VOS) methods [38,5] follow a segmentation-centric paradigm. However, these methods are not designed for the challenging tracking scenarios. Typical VOS sequences consist only of a few hundred frames [41] whereas multiple sequences of more than ten thousand frames exist in tracking datasets [16]. Due to this setup, VOS methods focus on producing highly accurate segmentation masks but are sensitive to distractors, substantial deformations and occlusions of the target object. Fig. 1 shows two typical tracking sequences where the VOS method LWL [5] (second row) produces a fine-grained segmentation mask of the wrong object (#3200) or is unable to detect only the target within a crowd (#0700, #1400).\n\nWe propose Robust Visual Tracking by Segmentation (RTS), a unified tracking architecture capable of predicting accurate segmentation masks. To design a segmentation-centric approach, we take inspiration from the aforementioned LWL [5] method. However, to achieve robust and accurate segmentation on Visual Object Tracking (VOT) datasets, we introduce several new components. In particular, we propose an instance localization branch trained to predict a target appearance model, which allows occlusion detection and target identification even in cluttered scenes. The output of the instance localization branch is further used to condition the high-dimensional mask encoding. This allows the segmentation decoder to focus on the localized target, leading to a more robust mask prediction. Since our proposed method contains a segmentation and instance memory that need to be updated with previous tracking results, we design a memory management module. This module first assesses the prediction quality, decides whether the sample should enter the memory and, when necessary, triggers the model update. Contributions Our contributions are the following: (i) We propose a unified tracking architecture capable of predicting robust classification scores and accurate segmentation masks. We design separate feature spaces and memories to ensure optimal receptive fields and update rates for segmentation and instance localization. (ii) To produce a segmentation mask which agrees with the instance prediction, we design a fusion mechanism that conditions the segmentation decoder on the instance localization output and leads to more robust tracking performance. (iii) We introduce an effective inference procedure capable of fusing the instance localization output and mask encoding to ensure both robust and accurate tracking. (iv) We perform comprehensive evaluation and ablation studies of the proposed tracking pipeline on multiple popular tracking benchmarks. Our approach achieves the new state of the art on LaSOT with an area-under-the-curve (AUC) score of 69.7%.\n\n\n"}, {"paperid": "paper6", "title": "Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation", "abstract": "Tracking any given object(s) spatially and temporally is a common purpose in Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint tracking and segmentation have been attempted in some studies but they often lack full compatibility of both box and mask in initialization and prediction, and mainly focus on single-object scenarios. To address these limitations, this paper proposes a Multi-object Mask-box Integrated framework for unified Tracking and Segmentation, dubbed MITS. Firstly, the unified identification module is proposed to support both box and mask reference for initialization, where detailed object information is inferred from boxes or directly retained from masks. Additionally, a novel pinpoint box predictor is proposed for accurate multi-object box prediction, facilitating target-oriented representation learning. All target objects are processed simultaneously from encoding to propagation and decoding, as a unified pipeline for VOT and VOS. Experimental results show MITS achieves state-of-the-art performance on both VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor by around 6% on the GOT-10k test set, and significantly improves the performance of box initialization on VOS benchmarks. The code is available at https://github.com/yoxu515/MITS.", "introduction": "\n\nVisual object tracking (VOT) [57,23,32,33] and video object segmentation (VOS) [63,84,79,37] are two critical tasks in computer vision.Visual object tracking involves identifying and tracking specific object(s) in a video stream over time.Video object segmentation aims to segment given object(s) in a video sequence and separate it from the background.Both tasks are essential for applications such as video surveillance and autonomous driving.In VOT, object sizes and positions are indicated with boxes as box representation, while in VOS object shapes and contours are marked with pixel-level masks as mask representation.Despite their differences, VOT and VOS share similarities.Both tasks require the ability to identify and locate the target object in a video stream accurately in spatial dimension, and to be robust against challenges such as occlusion and fast motion in temporal dimension.\n\nIn view of the similarity, tracking and segmentation may have unified multi-knowledge representations [91] and have been explored jointly in some works.1) Unification.A straightforward solution is to perform conversion between boxes and masks to utilize VOT methods on VOS or VOS methods on VOT.A mask can be converted to a box easily, but generating a mask from a box is hard.Some methods were proposed to address the box-to-mask estimation problem [47,89,102].However, separate but not unified models hinder end-to-end training and are inconvenient to manage in practical applications.2) Compatibility.Several studies have attempted to unify these two tasks into a single frame-work.However, some of them [78,75,62] still lack compatibility and flexibility in box/mask input/output and resort to extra models.3) Multi-Object.Despite that some methods [48,81,86] possess strong compatibility across VOT and VOS, they mainly focus on the single object scenario and use an ensemble strategy to aggregate the separate result of each object in the multiple object scenario.\n\nTherefore, this paper aims to unify VOS and VOT and improve above shortcomings by integrating boxes and masks in a multi-object framework as Multi-object Integrated Tracking and Segmentation (MITS), as shown in Figure 1.For compatibility problem, a unified identification module is proposed to take both reference boxes in VOT and masks in VOS for initialization.The unified identification module encodes the reference boxes or masks into the unified identification embedding by assigning identities to objects.The coarse identification embedding from boxes is further refined to mitigate the gap between mask and box initialization.The unified identification module is more convenient than borrowing an extra model because it is trained with the whole model in an end-to-end manner.\n\nBesides, the novel pinpoint box predictor is proposed for joint training and prediction with the mask decoder.Previous corner head or center head estimates a box by corners or a center point, which are not have to be inside the object.Emphasizing exterior points may distract learning targetoriented features and affect the mask prediction.To address this problem, we estimate the box by localizing pinpoints, which are always on the edge of the object.However, directly supervise the learning of pinpoints is infeasible due to the lack of annotation.Therefore we perform decoupled aggregation on the pinpoint maps and determine the box only by side-aligned pinpoint coordinates.\n\nAll the modules in our framework are not only compatible with two tasks, but also able to process multiple objects simultaneously.The multi-object training and prediction make our framework efficient and robust under complex scenes with multiple objects.Extensive experiments are conducted to demonstrate the strong compatibility and capacity of our framework.Experimental results show that our framework achieves SOTA performance on VOT benchmarks including LaSOT [23], TrackingNet [57] and GOT-10k [32], and VOS benchmark YouTube-VOS [84].Our method improves 6% over previous SOTA VOT method on GOT-10k, and significantly improves the performance of box initialization on VOS benchmarks.In summary, our contributions are:\n\n\u2022 We present a multi-object framework integrating boxes and masks for unified tracking and segmentation.\n\n\u2022 The unified identification module is proposed to accept both masks and boxes for initialization.\n\n\u2022 A novel pinpoint box predictor is proposed for accurate box prediction together with the mask decoder.\n\n\n"}]