[{"paperid": "paper0", "title": "An Information-Theoretic Approach to Transferability in Task Transfer Learning", "abstract": "Task transfer learning is a popular technique in image processing applications that uses pre-trained models to reduce the supervision cost of related tasks. An important question is to determine task transferability, i.e. given a common input domain, estimating to what extent representations learned from a source task can help in learning a target task. Typically, transferability is either measured experimentally or inferred through task relatedness, which is often defined without a clear operational meaning. In this paper, we present a novel metric, H-score, an easily-computable evaluation function that estimates the performance of transferred representations from one task to another in classification problems using statistical and information theoretic principles. Experiments on real image data show that our metric is not only consistent with the empirical transferability measurement, but also useful to practitioners in applications such as source model selection and task transfer curriculum learning.", "introduction": "\n\nTransfer learning is a learning paradigm that exploits the relatedness between different learning tasks in order to gain certain benefits, e.g. reducing the demand for supervision ([1]). In task transfer learning, we assume that the input domain of the different tasks are the same. Then for a target task T T , instead of learning a model from scratch, we can initialize the parameters from a previously trained model for some related source task T S (Figure 1). For example, deep convolutional neural networks trained for the ImageNet classification task have been used as the source network in transfer learning for target tasks with fewer labeled data [2], such as medical image analysis [3] and structural damage recognition in buildings [4]. An imperative question in task transfer learning is transferability, i.e. when a transfer may work and to what extent. Given a metric capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning, to a large extent, is simplified to search procedures over potential transfer sources and targets as quantified by the metric. Traditionally, transferability is measured purely empirically using model loss or accuracy on the validation set ([5, 6, 7]). There have been theoretical studies that focus on task relatedness ([8, 9, 10, 11]). However, they either cannot be computed explicitly from data or do not directly explain task transfer performance. In this study, we aim to estimate transferability analytically, directly from the training data.\n\nWe quantify the transferability of feature representations across tasks via an approach grounded in statistics and information theory. The key idea of our method is to show that the expected log-loss of using a feature of the input data to predict the label of a given task under the probabilistic model can be characterized by an analytically expression, which we refer as the H-score of the feature. H-score is particularly useful to quantify feature transferability among tasks. Using this idea, we define task transferability as the normalized H-score of the optimal source task feature with respect to the target task.\n\nAs we demonstrate in this paper, the advantage of our transferability metric is threefold. (i) it is theoretically driven and has a strong operational meaning rooted in statistics and information theory; (ii) it can be computed directly and efficiently from the input data, with fewer samples than those needed for empirical learning; (iii) it can be shown to be strongly consistent with empirical transferability measurements.\n\n\n"}, {"paperid": "paper1", "title": "Transferability and Hardness of Supervised Classification Tasks", "abstract": "We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets -- CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks) -- together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for tasks estimated to be highly transferable.", "introduction": "\n\nHow easy is it to transfer a representation learned for one task to another? How can we tell which of several tasks is hardest to solve? Answers to these questions are vital in planning model transfer and reuse, and can help reveal fundamental properties of tasks and their relationships in the process of developing universal perception engines [3]. The importance of these questions is therefore driving research efforts, with several answers proposed in recent years.\n\nSome of the answers to these questions established task relationship indices, as in the Taskonomy [71] and Task2Vec [1,2] projects. Others analyzed task relationships in the context of multi-task learning [31, 37,61,68,73]. Importantly, however, these and other efforts are computa- * Work at Amazon Web Services, prior to joining current affiliation. tional in nature, and so build on specific machine learning solutions as proxy task representations.\n\nBy relying on such proxy task representations, these approaches are naturally limited in their application: Rather than insights on the tasks themselves, they may reflect relationships between the specific solutions chosen to represent them, as noted by previous work [71]. Some, moreover, establish task relationships by maintaining model zoos, with existing trained models already available. They may therefore also be computationally expensive [1,71]. Finally, in some scenarios, establishing task relationships requires multi-task learning of the models, to measure the influence different tasks have on each other [31, 37,61,68,73].\n\nWe propose a radically different, solution agnostic approach: We seek underlying relationships, irrespective of the particular models trained to solve these tasks or whether these models even exist. We begin by noting that supervised learning problems are defined not by the models trained to solve them, but rather by the data sets of labeled examples and a choice of loss functions. We therefore go to the source and explore tasks directly, by examining their data sets rather than the models they were used to train.\n\nTo this end, we consider supervised classification tasks defined over the same input domain. As a loss, we assume the cross entropy function, thereby including most commonly used loss functions. We offer the following surprising result: By assuming an optimal loss on two tasks, the conditional entropy (CE) between the label sequences of their training sets provides a bound on the transferability of the two tasks-that is, the log-likelihood on a target task for a trained representation transferred from a source task. We then use this result to obtain a-priori estimates of task transferability and hardness.\n\nImportantly, we obtain effective transferability and hardness estimates by evaluating only training labels; we do not consider the solutions trained for each task or the input domain. This result is surprising considering that it greatly simplifies estimating task hardness and task relationships, yet, as far as we know, was overlooked by previous work.\n\nWe verify our claims with rigorous tests on a total of 437 tasks from the CelebA [35], Animals with Attributes 2 (AwA2) [67], and Caltech-UCSD Birds 200 (CUB) [66] sets. We show that our approach reliably predicts task transferability and hardness. As a case study, we evaluate transferability from face recognition to facial attribute classification. On attributes estimated to be highly transferable from recognition, our results outperform the state of the art despite using a simple approach, involving training a linear support vector machine per attribute.\n\n\n"}, {"paperid": "paper2", "title": "LEEP: A New Measure to Evaluate Transferability of Learned Representations", "abstract": "We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.", "introduction": "\n\nTransferability estimation (Eaton et al., 2008;Ammar et al., 2014;Sinapov et al., 2015) is the problem of quantitatively estimating how easy it is to transfer knowledge learned from one classification task to another. Specifically, given a source task, represented by a labeled data set or a pre-trained model, and a target task, represented by a labeled data set, transferability estimation aims to develop a measure (or a score) that can tell us, ideally without training on the target task, how effectively transfer learning algorithms can transfer knowledge from the source task to the target task.\n\nAnswering this question is important, since good estimations of transferability can help understand the relationships between tasks (Tran et al., 2019), select groups of highly transferable tasks for joint training (Zamir et al., 2018), or choose good source models for a given target task Bao et al., 2019;Bhattacharjee et al., 2019). Previous approaches to transferability estimation often require running a transfer learning algorithm that involves expensive parameter optimization (Zamir et al., 2018;Achille et al., 2019), do not have a simple interpretation (Bao et al., 2019), or make strong assumptions about the data sets that limit their applicability (Zamir et al., 2018;Tran et al., 2019).\n\nWe propose a novel measure called the Log Expected Empirical Prediction (LEEP) for transferability estimation of deep networks that overcomes all the shortcomings above. In contrast to previous approaches, LEEP scores are obtained without training on the target task, thus avoiding the expensive parameter optimization step. Additionally, they have a simple interpretation and can be applied in general settings to a wide range of modern deep networks.\n\nIn particular, LEEP scores are obtained from a source model and a target data set by making a single forward pass of the model through the target data. This is a simpler process than previous methods, such as Taskonomy (Zamir et al., 2018) and Task2Vec , where one has to re-train at least part of the source model on the target data set. Furthermore, LEEP has a simple interpretation: it is the average log-likelihood of the expected empirical predictor, a simple classifier that makes prediction based on the expected empirical conditional distribution between source and target labels. Finally, LEEP does not make any assumption on the source and target input samples, except that they have the same size. This is more general and applicable than previous work (Zamir et al., 2018;Tran et al., 2019) where source and target data sets were assumed to share the same input samples.\n\nContributions. We formally define LEEP and rigorously analyze it, both theoretically and empirically. We show two theoretical properties of the measure: (1) LEEP is upper bounded by the average log-likelihood of the optimal model, obtained by re-training the head classifier while freezing the feature extractor; (2) LEEP is related to the negative conditional entropy measure proposed by Tran et al. (2019).\n\nWe conduct extensive experiments to evaluate our LEEP measure in several scenarios. We show that the measure is useful for predicting the performance of two commonly used transfer learning algorithms -head classifier re-training Razavian et al., 2014) and model fine-tuning (Agrawal et al., 2014;Girshick et al., arXiv:2002.12462v1 [cs.LG] 27 Feb 2020 2014) -not only for large target data sets, but also for small or imbalanced target data sets that are difficult to use for re-training. We also show that LEEP can predict the convergence speed of the fine-tuning method for transfer learning.\n\nWe further demonstrate that LEEP can predict the performance of a recently developed meta-transfer learning method, the Conditional Neural Adaptive Processes (Requeima et al., 2019). Meta-transfer learning (Wei et al., 2018;Sun et al., 2019;Requeima et al., 2019) is a framework for learning to transfer using several meta-training tasks. Importantly, to our knowledge, our work is the first to develop a transferability measure for meta-transfer learning.\n\nWe empirically compare our method with the very recent negative conditional entropy measure (Tran et al., 2019) and H scores (Bao et al., 2019). Our comparisons show that LEEP better correlates with the actual transfer accuracy than these methods. Finally, we demonstrate the effectiveness of LEEP for the source model selection problem in comparison with the negative conditional entropy and H scores.\n\n\n"}, {"paperid": "paper3", "title": "Ranking Neural Checkpoints", "abstract": "This paper is concerned with ranking many pre-trained deep neural networks (DNNs), called checkpoints, for the transfer learning to a downstream task. Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints from various sources. Which of them transfers the best to our downstream task of interest? Striving to answer this question thoroughly, we establish a neural checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking measures. These measures are generic, applying to the checkpoints of different output types without knowing how the checkpoints are pre-trained on which dataset. They also incur low computation cost, making them practically meaningful. Our results suggest that the linear separability of the features extracted by the checkpoints is a strong indicator of transferability. We also arrive at a new ranking measure, NLEEP, which gives rise to the best performance in the experiments.", "introduction": "\n\nThere is an increasing number of pre-trained deep neural networks (DNNs), which we call checkpoints. We may produce hundreds of intermediate checkpoints when we sweep through various learning rates, optimizers, and losses to train a DNN. Furthermore, semi-supervised [10,4,49,36,58,39,37,8] and self-supervised [15,26,11,62,43] learning make it feasible to harvest DNN checkpoints with scarce or no labels. Fine-tuning [65,44] has become a de facto standard to adapt the pre-trained checkpoints to target tasks. It leads to faster convergence [16,27,51] and better performance [35] on the downstream tasks.\n\nHowever, not all checkpoints are equally useful for a target task, and some could even under-perform a randomly initialized checkpoint (cf. Section 2.2). This paper is con-* This work was done while the first author was an intern at Google. cerned with ranking neural checkpoints, which aims to measure how effectively fine-tuning can transfer knowledge from the pre-trained checkpoints to the target task. The measurement should be generic enough for all the neural checkpoints, meaning that it works without knowing any pre-training details (e.g., pre-training examples, hyperparameters, losses, early stopping stages, etc.) of the checkpoints. It also should be lightweight, ideally without training on the downstream task, to make it practically useful. We may use the measurement to choose the top few checkpoints before running fine-tuning, which is computationally more expensive than calculating the measurements.\n\nRanking neural checkpoints is crucial. Some domains or applications lack large-scale human-curated data, like medical images [48], raising a pressing need for high-quality pre-trained checkpoints as a warm start for fine-tuning. Fortunately, there exist hundreds of thousands of checkpoints of popular neural network architectures. For instance, many computer vision models are built upon ResNet [28], Inception-ResNet [56], and VGG [52]. As a result, we can construct a candidate pool by collecting the checkpoints released by different groups, for various tasks, and over distinct datasets.\n\nIt is nontrivial to rank the checkpoints for a downstream task. We explain this point by drawing insights from the related, yet arguably easier, task transferability problem [1,20,66,41], which aims to provide high-level guidance about how well a neural network pre-trained in one task might transfer to another. However, not all checkpoints pre-trained in the same source task transfer equally well to the target task [70,35]. The pre-training strategy also matters. Zhai et al. [68] find that combining supervision with self-supervision improves a network's transfer results on downstream tasks. He et al. [26] also show that selfsupervised pre-training benefits object detection more than its supervised counterpart under the same fine-tuning setup.\n\nWe may also appreciate the challenge in ranking neural checkpoints by comparing it with another related line of work: predicting DNNs' generalization gaps [40,31,5]. Jiang et al. [30] use a linear regressor to predict a DNN's generalization gap, i.e., the discrepancy between its training and test accuracies, by exploring the training data's margin distributions. Other signals studied in the literature include network complexity and noise stability. Ranking neural checkpoints is more challenging than predicting a DNN's generalization gap. Unlike the training and test sets that share the same underlying distribution, the downstream task may be arbitrarily distant from the source task over which a checkpoint is pre-trained. Moreover, we do not have access to the pre-training data at all. Finally, instead of keeping the networks static, fine-tuning dramatically changes all weights of the checkpoints.\n\nWe establish a neural checkpoint ranking benchmark (NeuCRaB) to study the problem systematically. Neu-CRaB covers various checkpoints pre-trained on widely used, large-scale datasets by different training strategies and architectures at a range of early stopping stages. It also contains diverse downstream tasks, whose training sets are medium-sized, making it practically meaningful to rank and fine-tune existing checkpoints. Pairing up all the checkpoints and downstream tasks, we conduct careful finetuning with thorough hyper-parameter sweeping to obtain the best transfer accuracy for each checkpoint-downstreamtask pair. Hence, we know the groundtruth ranking of the checkpoints for each downstream task according to the final accuracies (over the test/validation sets).\n\nA functional checkpoint ranking measurement should be highly correlated with the groundtruth ranking and, equally importantly, incurs as low computation cost as possible. We study several intuitive methods for ranking the neural checkpoints. One is to freeze the checkpoints as feature extractors and use a linear classifier to evaluate the features' separability on the target task. Another is to run fine-tuning for only a few epochs (to avoid heavy computation) and then evaluate the resulting networks on the target task's validation set. We also estimate the mutual information between labels and the features extracted from a checkpoint.\n\nFinally, we propose a lightweight measure, named Gaussian LEEP (N LEEP), to rank checkpoints based on the recently proposed log expected empirical prediction (LEEP) [41]. LEEP was originally designed to measure between-task transferabilities. It cannot handle the checkpoints pre-trained by unsupervised or self-supervised learning since it requires all checkpoints to have a classification head. Its computation cost could blow up when the classification head corresponds to a large output space. Moreover, it depends on the classification head's probabilistic output, which, unfortunately, is often overly confident [25].\n\nTo tackle the above problems, we replace the checkpoints' output layer with a Gaussian mixture model (GMM). This simple change kills two birds with one stone. On the one hand, GMM's soft assignment of input to clusters seamlessly applies to LEEP, resulting in the lightweight, effective N LEEP measure that works regardless of the checkpoints' output types. On the other hand, since we fit GMM to the target task's data, instead of the pre-training data of a different source task, the cluster assignment probabilities are likely more calibrated than the classification probabilities for the target task, if there exist classification heads.\n\n\n"}, {"paperid": "paper4", "title": "LogME: Practical Assessment of Pre-trained Models for Transfer Learning", "abstract": "This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo \\emph{without fine-tuning}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is \\emph{immune to over-fitting}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is \\emph{fast, accurate, and general}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most $3000\\times$ speedup in wall-clock time and requires only $1\\%$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: \\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.", "introduction": "\n\nHuman performance on many recognition tasks has been surpassed by deep neural networks (He et al., 2015;2016) Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021.Copyright 2021 by the author(s).\n\ntrained with large-scale supervised data (Deng et al., 2009;Russakovsky et al., 2015) and specialized computational devices (Jouppi et al., 2017).These trained neural networks, also known as pre-trained models, not only work well on tasks they are intended for but also produce generic representations (Donahue et al., 2014) that benefit downstream tasks such as object detection (Girshick et al., 2014).\n\nApart from serving as fixed feature extractors, pre-trained models can be fine-tuned (Yosinski et al., 2014;He et al., 2019) to serve downstream tasks better.The transfer learning paradigm \"pre-training \u2192 fine-tuning\" enjoys tremendous success in both vision (Kornblith et al., 2019) and language (Devlin et al., 2019) communities, and continues to expand to communities like geometric learning (Hu et al., 2020).Transfer of pre-trained models has become one of the cornerstones of deep learning.\n\nNowadays, there are numerous public pre-trained models offered by PyTorch (Benoit et al., 2019), TensorFlow (Abadi et al., 2016) and third-party libraries like HuggingFace Transformers (Wolf et al., 2020).When a practitioner wants to employ transfer learning to solve a specific task, the first problem is to select a good pre-trained model to start from.The problem is non-trivial and task adaptive, considering that different tasks favor different pre-trained models.The problem challenges researchers to develop a practical assessment method that is fast, accurate and general.It should be fast enough compared to brute-force fine-tuning all available pre-trained models (Zamir et al., 2018), should be accurate enough so that potentially best models can be identified, and should be general enough to tackle a wide variety of common learning scenarios.\n\nDespite its practical significance, there is limited guidance arXiv: 2102.11005v3 [cs.LG] 23 Jun 2021 on task adaptive pre-trained model selection.Based on NCE (Tran et al., 2019), Nguyen et al. (2020) recently studied the problem when both the pre-train task and the downstream task are classification.They construct an empirical predictor by estimating the joint distribution over the pretrained and target label spaces and take the performance of the empirical predictor (LEEP) to assess pre-trained models.Though being fast, prior methods are not accurate and are specialized for transferring supervised pre-trained models to classification.They cannot apply to either contrastive pre-trained models (He et al., 2020;Chen et al., 2020a), unsupervised pre-trained language models (Devlin et al., 2019;Liu et al., 2019), or regression tasks.\n\nTable 1 shows the applicability of pre-trained model selection methods.Prior to this paper, for most (4 out of 5) transfer learning settings, task adaptive pre-trained model selection does not have a decent solution.\n\nTo provide a general method for pre-trained model selection in various settings, we consider the features extracted by pre-trained models, thus being agnostic to how models are pre-trained.The maximum value of label evidence (marginalized likelihood) given extracted features is calculated, providing a general probabilistic approach that is applicable to both classification and regression tasks.Finally, the logarithm of maximum evidence (LogME) is used to assess pre-trained models for transfer learning.The maximum evidence is less prone to over-fitting (Bishop, 2006), and its humongous computational cost is dramatically reduced by our carefully designed algorithm.\n\nThe contributions of this paper are two-fold:\n\n\u2022 We propose LogME for task adaptive pre-trained model selection, and develop a fast algorithm to accelerate the computation.LogME is easy to interpret and is extremely efficient.It brings at most 3000\u00d7 speedup in wall-clock time and requires just 1% memory footprint, characterizing itself as the first practical method for assessing pre-trained models in various transfer learning settings.\n\n\u2022 We extensively validate the generality and superior performance of LogME on 22 pre-trained models and 17 downstream tasks, covering various pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language).\n\n\n"}, {"paperid": "paper5", "title": "Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance", "abstract": "Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\\times$-10$\\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalance ratios etc. for some recent metrics e.g., NCE, LEEP that resulted in them being misrepresented as leading measures. We propose a correction and recommend measuring correlation performance against relative accuracy in such settings. We support our findings with ~164,000 (fine-tuning trials) experiments on both vision models and graph neural networks.", "introduction": "\n\nTransfer learning is a set of techniques of using abundant somewhat related source data p(X (s) , Y (s) ) to ensure that a model can generalize well to the target domain, defined as either little amount of labelled data p(X (t) , Y (t) ) (supervised), and/or a lot of unlabelled data p(X (t) ) (unsupervised transfer learning). Transfer learning is most commonly achieved either via fine-tuning or co-training. Finetuning is a process of adapting a model trained on source data by using target \u22c6 This work was completed as an Intern and Student Researcher at Google. arXiv:2110.06893v3 [cs.LG] 26 May 2023 data to do several optimization steps (for example, stochastic gradient descent) that update the model parameters. Co-training on source and target data usually involves reweighting the instances in some way or enforcing domain irrelevance on feature representation layer, such that the model trained on such combined data works well on target data. Fine-tuning is becoming increasing popular because large models like ResNet50 [11], BERT [6] etc. are released by companies and are easily modifiable. Training such large models from scratch is often prohibitively expensive for the end user.\n\nIn this paper, we are primarily interested in effectively measuring transferability before training of the final model begins. Given a source data/model, a transferability measure quantifies how much knowledge of source domain/model is transferable to the target model. Transferability measures are important for various reasons: they allow understanding of relationships between different learning tasks, selection of highly transferable tasks for joint training on source and target domains, selection of optimal pre-trained source models for the relevant target task, prevention of trial procedures attempting to transfer from each source domain and optimal policy learning in reinforcement learning scenarios (e.g. optimal selection of next task to learn by a robot). If a measure is capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning is greatly simplified by using the measure to search over candidate transfer sources and targets. Contributions Our contributions are three-fold:\n\n1. We show that H-score, commonly used as a baseline for newer transferability measures, suffers from instability due to poor estimation of covariance matrices. We propose shrinkage-based estimation of H-score with regularized covariance estimation techniques from statistical literature. We show 80% absolute increase over the original H-score and show superior performance in majority cases against all newer transferability measures across various fine-tuning scenarios. 2. We present a fast implementation of our estimator that is 3 \u2212 10 times faster than state-of-the-art LogME measure. 3. We identify problems with 3 other transferability measures (NCE, LEEP and N LEEP) in target task selection when either the number of target classes or the class imbalance varies across candidate target tasks. We propose measuring correlation against relative target accuracy (instead of vanilla accuracy) in such scenarios.\n\nOur large set of \u223c 164, 000 fine-tuning experiments with vision models and graph convolutional networks on real-world datasets shows usefulness of our proposals. This paper is organized as follows. Section 2 describes general fine-tuning regimes and transfer learning tasks. Section 3 discusses transferability measures. Section 4 addresses shortcomings of the pioneer transferability measure (H-Score) that arise due to unreliable estimation and proposes a new shrinkagebased estimator for the H-Score. In Section 5, we demonstrate problems with recent NCE, LEEP and N LEEP metrics and propose a way to address them. Finally, Section 6 presents a meta study of all metrics.\n\n\n"}, {"paperid": "paper6", "title": "Transferability Estimation using Bhattacharyya Class Separability", "abstract": "Transfer learning has become a popular method for leveraging pre-trained models in computer vision. However, without performing computationally expensive fine-tuning, it is difficult to quantify which pre-trained source models are suitable for a specific target task, or, conversely, to which tasks a pre-trained source model can be easily adapted to. In this work, we propose Gaussian Bhattacharyya Coefficient (GBC), a novel method for quantifying transferability between a source model and a target dataset. In a first step we embed all target images in the feature space defined by the source model, and represent them with per-class Gaussians. Then, we estimate their pairwise class separability using the Bhattacharyya coefficient, yielding a simple and effective measure of how well the source model transfers to the target task. We evaluate GBC on image classification tasks in the context of dataset and architecture selection. Further, we also perform experiments on the more complex semantic segmentation transferability estimation task. We demonstrate that GBC outperforms state-of-the-art transferability metrics on most evaluation criteria in the semantic segmentation settings, matches the performance of top methods for dataset transferability in image classification, and performs best on architecture selection problems for image classification.", "introduction": "\n\nThe goal of transfer learning is to reuse knowledge learned on a source task to help train a model for a target task. Currently, the most common form of transfer learning in computer vision is to pre-train a source model on the ILSVRC'12 dataset [55] and then fine-tune it on the target dataset [3,14,23,24,30,35,57,75]. However, each target task may benefit from a different source model architecture [12,25,45,53] or different source dataset [42,46,71]. The challenge then becomes to determine which (pre-* Currently at Waymo.\n\n\n"}]