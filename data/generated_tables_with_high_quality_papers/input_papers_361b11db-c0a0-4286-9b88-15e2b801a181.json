[{"paperid": "paper0", "title": "InPars: Data Augmentation for Information Retrieval using Large Language Models", "abstract": "The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars .", "introduction": "\n\nLanguage models (LMs) such as GPT-3 (Brown et al., 2020), FLAN (Wei et al., 2022), Gopher (Rae et al., 2021), and T0++ (Sanh et al., 2021) have demonstrated impressive performance on many NLP tasks. Additionally, when sufficient supervised information is not available for a task, they have been shown to be effective and at times yield compelling results (Winata et al., 2021;Schick and Sch\u00fctze, 2021b).\n\nDespite the appealing capabilities of large LMs, multi-billion parameter models are rarely used in information retrieval (IR), with p q p q What are the effects of caffeine during pregnancy? Document: We don't know a lot about the effects of caffeine during pregnancy on you and your baby. So it's best to limit the amount you get each day. Question:\n\n\n"}, {"paperid": "paper1", "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.", "introduction": "\n\nRecently, major progress has been made on neural retrieval models such as dual encoders, which can retrieve knowledge from a large collection of documents containing millions to billions of passages (Yih et al., 2011;Karpukhin et al., 2020). However, Thakur et al. (2021) recently proposed the BEIR heterogeneous retrieval benchmark, and showed that it is still difficult for neural retrievers to perform well on a wide variety of retrieval tasks that lack dedicated training data. Thus, previous approaches focus on transferring knowledge from question answering (QA) datasets such as MS MARCO (Nguyen et al., 2016). To best transfer from QA datasets, expressive retrievers are developed that allow fine-grained token-level interaction such as ColBERT (Khattab & Zaharia, 2020;Santhanam et al., 2022) and SPLADE (Formal et al., 2021) but with higher inference cost. Data augmentation via synthetic question generation has previously been explored Shakeri et al., 2020), but these question generators are typically only trained on popular QA datasets.\n\nWe argue that it is hard to expect models based on one or two QA datasets to perform well across different retrieval tasks. First, different retrieval tasks have very different search intents; in other words, different definitions of \"relevance\". For example, as illustrated in Figure 1(a), both Dbpedia-Entity (Hasibi et al., 2017) and FEVER (Thorne et al., 2018) are tasks to retrieve documents from Wikipedia. Dbpedia-Entity is a task to retrieve entities that are mentioned in the query, while FEVER is a task to find evidence that either supports or refutes a given statement. Which document is relevant to the query can be very different from one task to another task even if they share the same (2) which trains model on a large QA retrieval datasets and transfer to other retrieval tasks. Right (c): Few-shot PROMPTAGATOR performance. Average nDCG@10 on 11 datasets from BEIR from our PROMPTAGATOR models and previously MS MARCO-supervised models (SPLADE v2).\n\ndomain. Moreover, different tasks have distinct distributions of queries even when their search intents are similar. For example, in the BEIR benchmark, queries in HotpotQA (Yang et al., 2018) are long compositional questions, while queries in FiQA (Maia et al., 2018) are short financial questions.\n\nIn this paper, we advocate to work on the setting of Few-shot Retrieval for diverse retrieval ( \u00a72), where each task comes with a short description and a few annotated examples to clearly illustrate the search intents. Given that only a few examples are available, we propose Prompt-base Query Generation for Retriever (PROMPTAGATOR) ( \u00a73) which aims to resolve the data scarcity issue while retaining the efficiency of a small dual encoder, by harnessing the power of large language models (LLM) such as FLAN (Wei et al., 2022a). PROMPTAGATOR combines prompting with LLMs as a query generator without fine-tuning ( \u00a73.1), and can generate good queries with minimal supervision -shown in Figure 1(b), it solely relies on a few supervised examples from the target task without using annotated query-document pairs from Natural Questions (Kwiatkowski et al., 2019) or MS MARCO (Nguyen et al., 2016) to train the retriever directly. The key insight of PROMPTAGATOR is to amplify the power of few-shot examples by creating task-specific prompting, which in turn enables generating a large set of synthetic queries for training retrievers suited for the task. To ensure the generated data quality, we develop a filtering technique that ensures round-trip consistency using generated data only ( \u00a73.2). Our filter is tailored to retrieval, which removes ambiguous, generic, and low-quality questions, and significantly improves retrieval performance.\n\nWhile PROMPTAGATOR is not the first application of LLM for retrieval, prior attempts of using LLMs often come with higher serving cost. Neelakantan et al. (2022) proposes to use the GPT-3 (Brown et al., 2020) embeddings in dual encoder models. However, the embedding size is 12k and hence makes the search index footprint and inference cost high. Sachan et al. (2022) and Bonifacio et al. (2022) have applied prompting and LLMs for reranking, while leaving the retriever untouched. With PROMPTAGATOR, we show that LLMs can be used to generate efficient end-to-end retriever with high accuracy. The contributions of the paper are as follows:\n\n\u2022 We analyze the previously overlooked differences across retrieval tasks in their search intents and query distributions, and propose a Few-Shot Retrieval setting for the BEIR dataset. Our prompt and fewshot examples will be released to facilitate future research.\n\n\u2022 We propose PROMPTAGATOR, a simple recipe for few-shot retrieval by prompting with a LLM to generate synthetic task-specific training data. For the first time, end-to-end retrievers solely based on a few supervised examples can be strong and efficient to serve with PROMPTAGATOR.\n\n\u2022 Our experimental results show that, surprisingly, PROMPTAGATOR with two-to-eight examples produced significantly better retrievers compared to recent models trained on MS MARCO or NQ that have over 500K human annotated examples (Figure 1(c)). PROMPTA-GATOR outperforms ColBERT v2 and SPLADE v2 on 11 retrieval tasks we tested, while reranking boosts results by another 5 points on standard retrieval evaluation metric.\n\n\n"}, {"paperid": "paper2", "title": "AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation", "abstract": "Dense retrievers have made significant strides in text retrieval and open-domain question answering, even though most achievements were made possible only with large amounts of human supervision. In this work, we aim to develop unsupervised methods by proposing two methods that create pseudo query-document pairs and train dense retrieval models in an annotation-free and scalable manner: query extraction and transferred query generation. The former method produces pseudo queries by selecting salient spans from the original document. The latter utilizes generation models trained for other NLP tasks (e.g., summarization) to produce pseudo queries. Extensive experiments show that models trained with the proposed augmentation methods can perform comparably well (or better) to multiple strong baselines. Combining those strategies leads to further improvements, achieving the state-of-the-art performance of unsupervised dense retrieval on both BEIR and ODQA datasets.", "introduction": "\n\nText retrieval is one of the most impactful artificial intelligence applications nowadays. Billions of users access massive amounts of data on the Internet through common internet services powered by information retrieval techniques, such as web search and product search. Despite the fact that traditional lexical retrieval remains a simple yet effective solution, neural network based models, namely dense retrievers, have made significant progress in recent years and demonstrated advantages in scenarios concerning semantic matching.\n\nNevertheless, most dense retrievers heavily rely on training with a large amount of annotated data. For example, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are the two most widely used datasets, and models trained with them have obtained outstanding performance. But each of these datasets has hundreds of thousands of human annotated pairs, which can be prohibitively costly to collect, and the models trained with them may not perform well in unseen domains (Thakur et al., 2021). Training dense retrieval models without human-annotated data remains an unsolved challenge. Nevertheless, most dense retrievers heavily rely on training with a large amount of annotated data. For example, MS MARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are the two most widely used datasets, and models trained with them have obtained outstanding performance. But each of these datasets has hundreds of thousands of human annotated query-document pairs, which can be prohibitively costly to collect, and the models trained with them may not perform well in unseen domains (Thakur et al., 2021). Training dense retrieval models without human-annotated data remains an unsolved challenge.\n\nRecently, there have been some efforts showing the feasibility of training dense retrievers in an annotation-free way (Izacard et al., 2021;Ram et al., 2021;Neelakantan et al., 2022). Following the regular paradigm of self-supervised learning, a pretext task is designed by taking two different views of a single document (a positive pair). Then a dual-encoder model is trained using contrastive learning, mapping the two views of data to hidden representations as similar as possible. When directly applied on downstream retrieval tasks, the scores of those unsupervised models lag behind the classic lexical method BM25 by large margins, but a boost in performance is observed after tuning with annotated query-doc pairs. This motivates us to focus on the gap between the pretext task and downstream retrieval tasks. Existing strategies for constructing positive pairs are very heuristic, e.g. Contriever (Izacard et al., 2021) samples two random text spans from a document to form a positive pair. One can see that the quality of the positive pairs is poorly controlled, and the resulting pseudo queries bear little resemblance to real-world ones. As a result, the models are negatively impacted by the noisy pseudo pairs, and they achieve inferior performance on down-stream tasks.\n\nIn this study, we propose two new categories of strategies for constructing pseudo query-doc pairs, namely query extraction and transferred query generation. Given a document of an arbitrary domain, both methods can produce pseudo queries in an unsupervised manner, and the resulting queries are paired with the original document to train dense retrievers using contrastive learning. The contributions of this study can be summarized as follows:\n\n1. We propose query extraction (QEXT), a novel data augmentation method for training dense retrievers. Given a document, a list of random spans is sampled from it. We utilize various methods to gauge their salience to the document and select the spans with the highest scores as pseudo queries.\n\n2. We propose transferred query generation (TQGEN). Pseudo queries are produced by language generation models for other NLP tasks, e.g., summarization. Even though query generation (QGen) has been studied for long, existing studies require annotated data for training query generation models. Instead, we utilize out-of-distribution generation models for this end, and the result demonstrates that the inductive bias of specific NLP tasks can be effective for training dense retrievers.\n\n3. We introduce two datasets AUGQ-WIKI and AUGQ-CC, consisting of 22.6M and 52.4M pseudo query-document pairs, by applying the proposed augmentation methods on two large corpora.\n\n4. Extensive experiments show that the retrievers trained with AUGQ, named AUGTRIEVER, outperform strong baselines and achieve state-ofthe-art results on both BEIR and open-domain QA benchmarks, greatly bridging the gap between unsupervised dense methods and BM25.  \n\n\n"}, {"paperid": "paper3", "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers", "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.", "introduction": "\n\nThe advent of neural information retrieval (IR) has led to notable performance improvements on document and passage retrieval tasks (Nogueira and Cho, 2019;Khattab and Zaharia, 2020;Formal et al., 2021) as well as downstream knowledgeintensive NLP tasks such as open-domain questionanswering and fact verification (Guu et al., 2020;Lewis et al., 2020;Khattab et al., 2021;Izacard et al., 2022).Neural retrievers for these tasks often benefit from fine-tuning on large labeled datasets such as SQuAD (Rajpurkar et al., 2018), Natural Questions (NQ) (Kwiatkowski et al., 2019), and KILT (Petroni et al., 2021).However, IR models can experience significant drops in accuracy due to distribution shifts from the training to the target domain (Thakur et al., 2021;Santhanam et al., 2022b).For example, dense retrieval models trained on MS MARCO (Nguyen et al., 2016) might not generalize well to queries about COVID-19 scientific publications (Voorhees et al., 2021;Wang et al., Figure 1: Overview of UDAPDR.An expensive LLM like GPT-3 is used to create an initial set of synthetic queries.These are incorporated into a set of prompts for a less expensive LLM that can generate large numbers of synthetic queries cheaply.The queries stemming from each prompt are used to train separate rerankers, and these are distilled into a single ColBERTv2 retriever for use in the target domain.2020), considering for instance that MS MARCO predates COVID-19 and thus lacks related topics.\n\nRecent work has sought to adapt IR models to new domains by using large language models (LLMs) to create synthetic target-domain datasets for fine-tuning retrievers (Bonifacio et al., 2022;Meng et al., 2022;Dua et al., 2022).For example, using synthetic queries, Thakur et al. (2021) and Dai et al. (2022) fine-tune the retriever itself and train a cross-encoder to serve as a passage reranker for improving retrieval accuracy.This significantly improves retriever performance in novel domains, but it comes at a high computational cost stemming from extensive use of LLMs.This has limited the applicability of these methods for researchers and practitioners, particularly in high-demand, user-facing settings.\n\nIn this paper, we develop Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers (UDAPDR), 1 an efficient strategy for using LLMs to facilitate unsupervised domain adaptation of neural retriever models.We show that UDAPDR leads to large gains in zero-shot settings on a diverse range of domains.\n\nThe approach is outlined in Figure 1.We begin with a collection of passages from a target domain (no in-domain queries or labels are required) as well as a prompting strategy incorporating these passages with the goal of query generation.A powerful (and perhaps expensive) language model like GPT-3 is used to create a modest number of synthetic queries.These queries form the basis for corpus-adapted prompts that provide examples of passages with good and bad queries, with the goal of generating good queries for new target domain passages.These prompts are fed to a smaller (and presumably less expensive) LM that can generate a very large number of queries for fine-tuning neural rerankers.We train a separate reranker on the queries from each of these corpus-adapted prompts, and these rerankers are distilled into a single student ColBERTv2 retriever (Khattab and Zaharia, 2020;Santhanam et al., 2022b,a), which is evaluated on the target domain.\n\nBy distilling from multiple passage rerankers instead of a single one, we improve the utility of ColBERTv2, preserving more retrieval accuracy gains while reducing latency at inference.Our core contributions are as follows:\n\n\u2022 We propose UDAPDR, a novel unsupervised domain adaptation method for neural IR that strategically leverages expensive LLMs like GPT-3 (Brown et al., 2020) and less expensive ones like Flan-T5 XXL (Chung et al., 2022), as well as multiple passage rerankers.Our approach improves retrieval accuracy in zeroshot settings for LoTTE (Santhanam et al., 2022b), SQuAD, and NQ.\n\n\u2022 We preserve the accuracy gains of these rerankers while maintaining the competitive latency of ColBERTv2.This leads to substantial reductions in query latency.\n\n\u2022 Unlike a number of previous domain adaptation approaches that utilize millions of synthetic queries, our technique only requires 1 pronounced: Yoo-Dap-ter 1000s of synthetic queries to prove effective and is compatible with various LLMs designed for handling instruction-based tasks like creating synthetic queries (e.g., GPT-3, T5, Flan-T5).\n\n\u2022 We generate synthetic queries using multiple prompting strategies that leverage GPT-3 and Flan-T5 XXL.This bolsters the effectiveness of our unsupervised domain adaptation approach.The broader set of synthetic queries allows us to fine-tune multiple passage rerankers and distill them more effectively.\n\n2 Related Work\n\n\n"}, {"paperid": "paper4", "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models", "abstract": "Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.", "introduction": "\n\nTraditional informational retrieval (IR) methods like TF-IDF and BM25 [25] are based on token-level similarity matching and then suffer from lexical gap [1]. Inspired by the progress in deep learning, researchers have proposed to utilize neural networks to overcome the lexical gap. DR is such a kind of method based on neural networks. DR models like DPR [10] and Col-BERT [11] [27] encode each query or document to a dense vector, the dimensionality of which is determined by the neural networks. In practice, dense retrievers pre-compute the embeddings of documents and then on which build an approximate nearest neighbor (ANN) index for fast search. When a new query comes in, only its embedding is computed and fed into the following ANN search system. Unlike TF-IDF and BM25, DR cares more about the similarity of the overall semantic meaning.\n\nEven though neural retrieval mitigates the lexical gap, it still suffers from the challenge of lacking domain-specific training data. Some researchers have proposed to leverage transfer learning to tackle this challenge. To tackle this problem. Evidence [32] [6] shows that not all DR models and domains can benefit from transfer learning equally. Recently, LLMs like CPT-3 [4], LLaMA [33], and Vicuna [5] show the strong ability of zero-shot and few-shot learning. Instead of fine-tuning the LLMs on task-specific data, prompting concatenates the instructions for certain tasks (e.g., TL;DR translate to English) and a few corresponding examples as input and obtains the answers from the output of large language model (LLM). These kinds of human-written prompts are also called hard prompts. Researchers [28] recently have estimated that a good language classifier prompt is worth hundreds to thousands of extra data points. InPars [2] and PROMPTAGATOR [6] both utilize hard prompts to prompt the LLMs to tag the unlabeled documents with weak queries and then train task-specific retrievers. However, hard prompts have some drawbacks: a) It is not easy to find good hard prompts. Hard prompts must be hand-crafted by humans through trial and error, and sometimes intuition and luck are needed; b) Even with handcrafted prompts, the downstream tasks still underperform tuned models. For instance, compared with the performance of fine-tuned T5-XXL [24] on SuperGLUE [35], GPT-3 175B few-shot gets a 17.5 points smaller score despite using 16 times more parameters [12].\n\nInstead of utilizing humanly readable words as a hard prompt [22], soft prompt [12] [13] is a list of embeddings, unrecognizable to the human eye, appended to the input of the neural network. During the soft prompt tuning, the parameters of the LLM are frozen, and only the parameters associated with soft prompt are updated. Even though [12] and [13] both demonstrate that soft prompt outperforms the hard prompt, there is no work utilizing soft prompt tuning to augment DR. In this paper, we propose soft prompt tuning for augmenting DR (SPTAR). Specifically, for each task, we leverage soft prompt tuning to optimize the parameters associated with soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific retrievers. Moreover, we find that even with the optimized soft prompt, the quality of generated weak queries is sensitive to the example document-query pairs in the prompt. So, we design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries as well as the DR tasks. Our main contributions can be summarized as follows:\n\n\u2022 To the best of our knowledge, this is the first work that utilizes LLMs with soft prompt tuning for augmenting DR tasks. \u2022 We introduce a novel soft prompt filter to select high-quality document-query pairs in the prompt to further improve the quality of generated weak data. \u2022 Comprehensive experiments are conducted to demonstrate our approach outperforming BM25 and InPars [2]. \u2022 Experiments are based on the recent open-source LLMs, and we will make the code publicly available upon paper acceptance.  [27] share the same BERT but utilize a different special token after \"[CLS]\" to distinguish query and document. Unlike DPR directly measures the similarity between query embedding and document embeddings, Col-BERT introduces a late interaction mechanism. Specifically, for each token in the query, ColBERT computes its similarity with all the tokens in the document and applies a maximum pooling on these similarity scores. The similarity score of a pair of query and document is the summarization of all the scores after the maximum pooling. Given a query with one positive document and one negative document, ColBERT is optimized by the pairwise softmax cross-entropy loss over the computed scores of the positive and negative documents. ANCE [39] is a bi-encoder trained on (query, positive document, negative document) tuples where the negative document is retrieved from an ANN built on the checkpoint of the last step. BM25CE [36] is a re-ranking-based DR. BM25CE first applies BM25 to retrieve documents and then employs the trained crossed-encoder to re-rank the retrieved documents. Our contribution is not to propose new dense retrievers but to propose a novel method to augment the existing dense retrievers.\n\n\n"}, {"paperid": "paper5", "title": "Questions Are All You Need to Train a Dense Passage Retriever", "abstract": "We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g., questions and potential answer passages). It uses a new passage-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence passages, and (2) the passages are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both passage and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.1 Our code and model checkpoints are available at: https://github.com/DevSinghSachan/art.", "introduction": "\n\nDense passage retrieval methods (Karpukhin et al., 2020;Xiong et al., 2021), initialized with encoders such as BERT  and trained using supervised contrastive losses (Oord et al., 2018), have surpassed the performance achieved by previously popular keyword-based approaches like BM25 (Robertson and Zaragoza, 2009). Such retrievers are core components in models for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hardnegative mining and denoising of positive examples. In this paper, we introduce the first unsupervised method, based on a new corpus-level autoencoding approach, that can match or surpass strong supervised performance levels with no labeled training data or task-specific losses.\n\nWe propose ART, Autoencoding-based Retriever Training, which only assumes access to sets of unpaired questions and passages. Given an input question, ART first retrieves a small set of possible evidence passages. It then reconstructs the original question by attending to these passages (see Figure 1 for an overview). The key idea in ART is to consider the retrieved passages as a noisy representation of the original question and question reconstruction probability as a way of denoising that provides soft-labels for how likely each passage is to have been the correct result.\n\nTo bootstrap the training of a strong model, it is important to both have a strong initial retrieval model and to be able to compute reliable initial estimates of question reconstruction probability when conditioned on a (retrieved) passage. Although passage representations from BERTstyle models are known to be reasonable retrieval baselines, it is less clear how to do zero-shot question generation. We use a generative pre-trained language model (PLM) and prompt it with the passage as input to generate the question tokens using teacher-forcing. As finetuning of the questiongeneration PLM is not needed, only the retrieval model, ART can use large PLMs and obtain accurate soft-label estimates of which passages are likely to be the highest quality.\n\nThe retriever is trained to penalize the divergence of a passage likelihood from its soft-label score. For example, if the question is ''Where is the bowling hall of fame located?'' as shown in Figure 1, then the training process will boost the retrieval likelihood of the passage ''Bowling Hall of Fame is located in Arlington,'' as it is relevant and would lead to a higher question reconstruction likelihood, while the likelihood of the passage ''Hall of Fame is a song by ...'' would be penalized as it is irrelevant. In this manner, the training process encourages correct retrieval results and vice-versa, leading to an iterative improvement in passage retrieval.\n\nComprehensive experiments on five benchmark QA datasets demonstrate the usefulness of our proposed training approach. By simply using questions from the training set, ART outperforms models like DPR by an average of 5 points absolute in top-20 and 4 points absolute in top-100 accuracy. We also train using all the questions contained in the Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) and find that even with a mix of answerable and unanswerable questions, ART achieves strong generalization on outof-distribution datasets due to relying on PLM. Our analysis further reveals that ART is highly sample-efficient, outperforming BM25 and DPR with just 100 and 1000 questions, respectively, on the NQ-Open dataset, and that scaling up to larger retriever models consistently improves performance.\n\n\n"}]