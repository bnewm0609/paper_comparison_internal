[{"paperid": "paper0", "title": "On Learning Disentangled Representations for Gait Recognition", "abstract": "Gait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and viewing angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as a dynamic gait feature while canonical features are averaged as a static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state-of-the-art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/lower resolutions, cross viewing angles.", "introduction": "\n\nB IOMETRICS measures people's unique physical and behavioral characteristics to recognize the identity of an individual. Gait [1], the walking pattern of an individual, is one of biometrics modalities besides face, fingerprint, iris, etc. Gait recognition has the advantage that it can operate at a distance without users' cooperation. Also, it is difficult to camouflage. Due to these advantages, gait recognition is applicable to many applications such as person identification, criminal investigation, and healthcare.\n\nAs other recognition problems, gait data can usually be captured by five types of sensors [2], i.e., RGB camera, RGB-D camera [3], [4], accelerometer [5], floor sensor [6], and continuous-wave radar [7]. Among them, RGB camera is not only the most popular one due to the low sensor cost, but also the most challenging one since RGB pixels might not be effective in capturing the motion cues. This work studies gait recognition from RGB cameras.\n\nThe core of gait recognition lies in extracting gait features from the video frames of a walking person, where the prior work can be categorized into two types: appearance-based and model-based methods. The appearance-based methods, e.g., Gait Energy Image (GEI) [8], take the averaged silhouette image as the gait feature. While having a low computational cost and being able to handle low-resolution imagery, it can be sensitive to variations such as cloth change, carrying, viewing angles and walking speed [9]- [15]. The model-based methods use the articulated body skeleton from pose estimation as the gait feature. They show more robustness to aforementioned variations but at a price of a higher computational cost and dependency on pose estimation accuracy [16]- [18].\n\nIt is understandable that the challenge in designing a gait feature is the necessity of being invariant to the appearance variation due to clothing, viewing angle, carrying, etc. Therefore, our desire is to \u2022 Ziyuan Zhang, Luan Tran, Feng Liu, and  disentangle the gait feature from the non-gait-related appearance of the walking person. For both appearance-based or modelbased methods, such disentanglement is achieved by manually handcrafting the GEI-like [8], [10] or body skeleton-like [16]- [18] features, since neither has color or texture information. However, we argue that these manual disentanglements may be sensitive to changes in walking condition. In other words, they can lose certain or create redundant gait information. E.g., GEI-like features have distinct silhouettes for the same subject wearing different clothes. For skeleton-like features, when carrying accessories (e.g., bags, umbrella), certain body joints such as hands may have fixed positions, and hence are redundant information to gait.\n\nTo remedy the aforementioned issues in handcrafted features, as shown in Fig. 1 (a), this paper proposes a novel approach to learn gait representations from the RGB video directly. Specifically, we aim to automatically disentangle dynamic pose features (trajectory of gait) from pose-irrelevant features. To further distill identity information from pose-irrelevant features, we disentangle the poseirrelevant features into appearance (i.e., clothing) and canonical features. Here, the canonical feature refers to a standard and unique representation of human body, such as body ratio, width and limb lengths, etc. The pose features and canonical features are discriminative in identity and are used for gait recognition. Fig. 1 (b) visualizes the three disentangled features. This disentanglement is realized by designing an autoencoderbased Convolutional Neural Network (CNN), GaitNet, with novel loss functions. For each video frame, the encoder estimates three latent representations: pose, canonical and appearance features, by employing three loss functions: 1) cross reconstruction loss enforces that the canonical and appearance features of one frame, fused with the pose feature of another frame, can be decoded to the latter frame; 2) pose similarity loss forces a sequence of pose features extracted from a video sequence, of the same subject to be similar even under different conditions; 3) canonical  Fig. 3 show that, the appearance feature is video-specific capturing clothing information; the canonical feature is subject-specific capturing the overall body shape at a standard pose; the pose feature is frame-specific capturing body poses at individual frames.\n\nconsistency loss favors consistent canonical features among videos of the same subject under different conditions. Finally, the pose features of a sequence are fed into a multi-layer LSTM with our designed incremental identity loss to generate the sequence-based dynamic gait feature. The average of canonical features results in the sequence-based static gait feature. Given two gait videos, the cosine distances between their respective dynamic and static gait features are computed and their summation is the final video-tovideo gait similarity metric. In addition, most prior work [8], [10], [14], [16], [19]- [24] choose the walking video of the side view, which has the richest gait information, as the gallery sequence. However, in practices other viewing angles, such as the frontal view, can be very common when pedestrians walk toward or away from the surveillance camera. Also, the prior work [25]- [28] that focuses on frontal view are often based on RGB-D videos, which have additional depth information than RGB. Therefore, to encourage gait recognition from frontalview RGB videos that generally has the minimal amount of gait information, we collect a high-definition (HD, 1080p) Frontal-View Gait database, named FVG, with a wide range of variations. It has three frontal-view angles where the subject walks from left 45 \u2022 , 0 \u2022 , and right 45 \u2022 off the optical axes of the camera. For each of three angles, different variants are explicitly captured including walking speed, clothing, carrying, multiple people, etc.\n\nA preliminary version of this work was published in the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019 [29]. We extend the work from three aspects. 1) Instead of disentangling features in two components: pose and poseirrelevant [29], we further decouple the pose-irrelevant features into discriminative canonical feature and appearance feature. By devising an effective canonical consistency loss, the canonical feature helps to improve gait recognition accuracy. 2) We conduct more insightful ablation studies to analyze the relationship between our disentanglement losses and features, gait recognition over time, and contributions of dynamic and static gait features. 3) We perform side-by-side comparison between gait recognition and the state-of-the-art (SOTA) face recognition on the same dataset.\n\nIn summary, this paper makes the following contributions:\n\nOur proposed GaitNet directly learns disentangled representations from RGB videos, which is in sharp contrast to the conventional appearance-based or model-based methods.\n\nWe introduce a Frontal-View Gait database, including various variations of viewing angles, walking speeds, carrying, clothing changes, background and time gaps. This is the first HD gait database, with nearly twice the number of subjects compared to existing RGB gait databases.\n\nOur proposed method outperforms the state of the arts on three benchmarks, CASIA-B, USF, and FVG datasets.\n\nWe demonstrate the strength of gait recognition over face recognition in the task of person recognition from surveillancequality videos.\n\n\n"}, {"paperid": "paper1", "title": "A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition", "abstract": "Gait recognition has gained increasing interest from researchers, but there is still no standard evaluation method to compare the performance of different gait recognition algorithms. In this paper, a framework is proposed in an attempt to tackle this problem. The framework consists of a large gait database, a large set of well designed experiments and some evaluation metrics. There are 124 subjects in the database, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered in the database. The database is one of the largest database among the existing databases. Three sets of experiments, including a total of 363 experiments, are designed in the framework. Some metrics are proposed to evaluate gait recognition algorithms", "introduction": "\n\nGait is an attractive biometric feature for human identification at a distance, and recently has gained much interest from computer vision researchers. Compared with those traditional biometric features, such as face, iris and fingerprint, gait has many unique advantages such as non-contact, non-invasive and perceivable at a distance. Hence gait has been considered as a suitable biometric feature for human identification at a distance in visual surveillance.\n\nIn recent years many gait recognition algorithms have been developed. Some of them are model-based approaches [9,11], and some are appearance-based ones [7,12,13]. Even though many algorithms have been proposed, comparison of different algorithms and evaluation of robustness to some variations such as the variations of view angle, clothing, shoe types, surface types, carrying condition, illumination, and time are still hard and open problems. These variations should be fully studied to develop robust and ac-curate gait recognition algorithms.\n\nThe HumanID Gait Challenge Problem [1,8], which consists of a large database, a baseline algorithm and twelve experiments, tried to handle these problems. The data in the HumanID Gait Challenge Problem was collected in an outdoor environment with complex background, so it is a little hard to extract good quality human silhouettes, and this will affect the analysis of other factors. The twelve experiments were designed to evaluate an algorithm's robustness to view, shoe, surface, time, clothing and carrying condition changes. However, for these factors twelve experiments are not enough. Besides, the subjects walked in an elliptical path, and then the view angle kept changing while the subjects was walking, so the relationship between view angle and algorithm's performance can not be obtained. In conclusion, a database that is more suitable for evaluation and some well designed experiments are needed.\n\nA framework that consists of a large database, some experiments and metrics is proposed. In the database, data acquired from 11 views are included and also three most important factors, view angle, clothing and carrying condition changes, are separately considered. A total of 363 experiments were designed to thoroughly investigate these factors.\n\nThe organization of this paper is as follows. Section 2 describes the gait database. Experiment design is presented in Section 3, and metrics is in Section 4. Section 5 concludes this paper.\n\n\n"}, {"paperid": "paper2", "title": "The OU-ISIR Gait Database Comprising the Treadmill Dataset", "abstract": "This paper describes a large-scale gait database comprising the Treadmill Dataset. The dataset focuses on variations in walking conditions and includes 200 subjects with 25 views, 34 subjects with 9 speed variations from 2 km/h to 10 km/h with a 1 km/h interval, and 68 subjects with at most 32 clothes variations. The range of variations in these three factors is significantly larger than that of previous gait databases, and therefore, the Treadmill Dataset can be used in research on invariant gait recognition. Moreover, the dataset contains more diverse gender and ages than the existing databases and hence it enables us to evaluate gait-based gender and age group classification in more statistically reliable way.", "introduction": "\n\nIn modern society, there is a growing need to identify individuals in many different situations, including for surveillance and access control. For personal identification, many biometric-based authentication methods have been proposed using a wide variety of cues, such as fingerprints, irises, faces, and gait. Of these, gait identification has attracted considerable attention because it provides surveillance systems with the ability to ascertain identity at a distance. In fact, automatic gait recognition from public CCTV images has been admitted as evidence in UK courts [36], and gait evidence has been used as a cue for criminal investigations in Japan.\n\nRecently, various approaches to gait identification have been proposed. These range from model-based approaches [4], [37], [40], [41], [46] to appearance-based approaches [3], [6], [10], [14], [16], [17], [25], [26], [39]. In addition, several common gait databases have been published [7], [29], [31], [33], [44] for fair comparison of gait recognition approaches. These databases are usually constructed taking the following into account: (1) the variation in walking conditions, and (2) the number and diversity of the subjects.\n\nThe first consideration is important to ensure the robustness of the gait recognition algorithms, since walking conditions often differ between enrollment and test stages. For example, observation views are often inconsistent due to the positions of the CCTV cameras on the street and/or walking directions possibly being different. In addition, walking speeds can change depending on whether the person is merely taking a walk in the park or 1 Osaka University, Ibaraki, Osaka 567-0047, Japan 2 University of Rajshahi, Rajshahi, 6205, Bangladesh a) makihara@am.sanken.osaka-u.ac.jp is walking to the station in a hurry, and clothing almost certainly changes depending on the season.\n\nThe second consideration is also important because the number of subjects determines the upper bound of the statistical reliability of the performance evaluation. In addition, if the database is used not only for person identification, but also gender and age estimation from gait, the diversity of subjects in terms of gender and age plays an important role in the performance evaluations of such applications.\n\nIn this paper, we describe a large-scale gait database composed of the Treadmill Dataset based on the two considerations. The Treadmill Dataset is a set of gait datasets with variations in walking conditions, comprising 25 surrounding views, 9 walking speeds from 2 km/h to 10 km/h with a 1 km/h interval, at most 32 clothes combinations, and gait fluctuation variations among gait periods. The proposed gait dataset thus enables us to evaluate view-invariant, speed-invariant, and clothing-invariant gait recognition algorithms in a more extensive range. Moreover, it comprises 200 subjects of both genders and including a wide range of ages. The proposed gait database thus enables us to evaluate gait-based gender classification and age group classification.\n\nThe outline of this paper is as follows. First, existing gait databases are briefly considered in Section 2. Next, the Treadmill Dataset is addressed with related performance evaluations of gait recognition algorithms in Sections 3. Section 4 contains our conclusions, discussions, and future work in the area.\n\n\n"}, {"paperid": "paper3", "title": "Gait Recognition in the Wild with Dense 3D Representations and A Benchmark", "abstract": "Existing studies for gait recognition are dominated by 2D representations like the silhouette or skeleton of the human body in constrained scenes. However, humans live and walk in the unconstrained 3D space, so projecting the 3D human body onto the 2D plane will discard a lot of crucial information like the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper aims to explore dense 3D representations for gait recognition in the wild, which is a practical yet neglected problem. In particular, we propose a novel framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the human body for gait recognition, named SMPLGait. Our framework has two elaborately-designed branches of which one extracts appearance features from silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D SMPL model. In addition, due to the lack of suitable datasets, we build the first large-scale 3D representation-based gait recognition dataset, named Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL models recovered from video frames which can provide dense 3D information of body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively compare our method with existing gait recognition approaches, which reflects the superior performance of our framework and the potential of 3D representations for gait recognition in the wild. The code and dataset are available at https://gait3d.github.io.", "introduction": "\n\nVisual gait recognition, which aims to identify a target person using her/his walking pattern in a video, has been studied for over two decades [30,42]. Existing approaches and datasets are dominated by 2D gait representations such as silhouette sequences [56], Gait Energy Images * This work was done when Jinkai Zheng was an intern at Explore Academy of JD.com. \u2020 Corresponding author.  [63], as shown in Figure 1. However, the human body is a 3D non-rigid object, so the 3D-to-2D projection discards a lot of useful information about shapes, viewpoints, and dynamics while presenting ambiguity for gait recognition. Therefore, this paper is focused on 3D gait recognition which is valuable yet neglected by the community.\n\nRecently, deep learning-based methods have dominated the state-of-the-art performance on the widely adopted 2D gait recognition benchmarks like CASIA-B [37] and OU-MVLP [36] by directly learning discriminative features from silhouette sequences [5,9,57] or GEIs [46]. Despite the excellent results on the in-the-lab datasets, these methods cannot work well in the wild scenarios which have more diverse 3D viewpoints of cameras and more complex environmental interference factors like occlusions [63]. Although several works exploit 3D cylin-ders [3] or 3D skeletons [41], these sparse 3D models also lose helpful information of human bodies like viewpoints and shapes. Fortunately, the development of parameterized human body models like the Skinned Multi-Person Linear (SMPL) model [28] and 3D human mesh recovery approaches [18,20,34] makes it possible to estimate precise 3D meshes and viewpoints of human bodies in video frames. The advantages of 3D meshes for gait recognition are two-fold: 1) the 3D mesh can provide not only the pose but also the shape of the human body in the 3D space, which is crucial for learning discriminative features of gait, and 2) the 3D viewpoint can be explored to normalize the orientations of human bodies during cross-view matching.\n\nTo this end, we design a novel 3D SMPL model-based Gait recognition framework, i.e., SMPLGait, to explore the 3D gait representations for human identification. Our SMPLGait framework has two branches based on deep neural networks. One branch takes the silhouette sequence of a person as the input to learn appearance features like clothing, hairstyle, and belongings. However, due to the extreme viewpoint changes in the wild, the shape of the human body can be distorted, which makes the appearance ambiguous, as shown in Figure 1. To overcome this challenge, we design a 3D Spatial-Transformation Network (3D-STN) as the other branch to learn 3D knowledge of viewpoint and shape from the 3D human mesh. The 3D-STN takes the 3D SMPL model of each frame as the input to learn a spatial transformation matrix. By applying the spatial transformation matrix to the appearance features, these features from different viewpoints are normalized in the latent space. By this means, the gait sequences of the same person will be closer in the feature space.\n\nNevertheless, there is no suitable dataset that provides 3D meshes of human bodies in the wild. Therefore, to facilitate the research, we build the first large-scale 3D mesh-based gait recognition dataset, named Gait3D, from high-resolution videos captured in the wild. Compared to existing datasets listed in Table 1, the Gait3D dataset has the following featured properties: 1) Gait3D contains 4,000 subjects with over 25,000 sequences captured by 39 cameras in an unconstrained indoor scene which makes it scalable for research and applications.\n\n2) It provides precise 3D human meshes recovered from video frames which can provide 3D pose and shape of human bodies as well as accurate viewpoint parameters. 3) It also provides conventional 2D silhouettes and keypoints which can be explored for gait recognition with multi-modal data.\n\nIn summary, the contributions of this paper are as follows:\n\n\u2022 We make one of the first attempts toward 3D gait recognition in the real-world scenario, which aims to explore dense 3D representations of the human body for gait recognition.\n\n\u2022 We propose a novel 3D gait recognition framework based on the SMPL model, named SMPLGait, to explore 3D human meshes for gait recognition.\n\n\u2022 We build the first large-scale 3D gait recognition dataset, named Gait3D, which provides the 3D human meshes of gait collected from unconstrained scenarios.\n\nThrough comprehensive experiments, we not only evaluate existing 2D silhouettes/skeleton-based approaches but also demonstrate the effectiveness of the proposed SMPLGait method, which reflects the potential of 3D representations for gait recognition. Moreover, the combination of 3D and 2D representations further improves the performance which shows the complementarity of multi-modal representations.\n\n\n"}, {"paperid": "paper4", "title": "Gait Recognition in the Wild: A Benchmark", "abstract": "Gait benchmarks empower the research community to train and evaluate high-performance gait recognition systems. Even though growing efforts have been devoted to cross-view recognition, academia is restricted by current existing databases captured in the controlled environment. In this paper, we contribute a new benchmark for Gait REcognition in the Wild (GREW). The GREW dataset is constructed from natural videos, which contains hundreds of cameras and thousands of hours streams in open systems. With tremendous manual annotations, the GREW consists of 26K identities and 128K sequences with rich attributes for unconstrained gait recognition. Moreover, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Compared with prevailing predefined cross-view datasets, the GREW has diverse and practical view variations, as well as more natural challenging factors. To the best of our knowledge, this is the first large-scale dataset for gait recognition in the wild. Equipped with this benchmark, we dissect the unconstrained gait recognition problem. Representative appearance-based and model-based methods are explored, and comprehensive baselines are established. Experimental results show (1) The proposed GREW benchmark is necessary for training and evaluating gait recognizer in the wild. (2) For state-of-the-art gait recognition approaches, there is a lot of room for improvement. (3) The GREW benchmark can be used as effective pre-training for controlled gait recognition. Benchmark website is https://www.grew-benchmark.org/.", "introduction": "\n\nGait recognition aims to identify a person according to his/her walking style in a video. Compared with face, fingerprint, iris and palmprint, gait is hard to disguise and can work at a long distance, giving it unique potential for crime prevention, forensic identification, and social security.  [74], OU-MVLP [51] and the proposed GREW. The first two are captured under constrained environments, while the GREW is constructed in the wild. Since OU-MVLP [51] does not release RGB data, visualization results from its original paper are adopted. Faces are masked in the GREW for privacy concern.\n\nRecognizing gait under a controlled environment has achieved significant progress due to the boom of deep learning. The essential engines of recent gait recognition consist of network architecture evolution [20,9,62,65,16,72,71,31,44,50,4,63,67,39], loss function design [78,17,75,79], and growing gait benchmarks [42,7,74,37,51,23]. Even though gait recognition has achieved impressive advance in past years and it possesses the unique advantage of long-distance recognition, this technique has not yet been widely deployed in real-world applications. A notable obstacle is that there is almost no public benchmark to train and evaluate gait recognizer in the wild.\n\nTo our knowledge, most gait datasets are captured in relatively fixed and constrained environments such as laboratory or static outdoors. CASIA-B [74] and OU-MVLP [51] are most popularly used datasets in recent gait recognition research as shown in Figure 1. CASIA-B contains 124 subjects and 13,640 sequences, which is constructed in 2006. OU-MVLP consists of 10,307 identities and 288,596 walking videos, making it a big gait dataset with respect to #subjects. The statistics of more datasets are shown in Table 1: Comparison of the GREW with existing gait recognition datasets regarding statistics, data type, captured environment, view variations and challenging factors. Datasets are sorted in publication time. #Id., #Seq. and #Cam. refer to numbers of identities, sequences and cameras. Sil., Inf., D. and A. mean silhouette, infrared, depth and audio. VI, DIS, BA, CA, DR, OCC, ILL, SU, SP, SH, and WD are abbreviations of view, distractor, background, carrying, dressing, occlusion, illumination, surface, speed, shoes, and walking directions.  [49,43,56,8,21,70,3,13,24,84] and person re-identification (ReID) [77,48,36,66,17,10,82,80,81,83,27,61], it is time to move to benchmark gait recognition in the wild.\n\nIn this paper, we present the Gait REcognition in the Wild (GREW) benchmark, which is the first work delving into this open problem to the best of our knowledge. The GREW dataset is constructed from natural streams with multiple cameras as shown in Figure 1. Identity information from raw videos is manually annotated, resulting in 26K subjects, 128K sequences and 14M boxes for unconstrained gait recognition. Besides, rich human attributes including gender, age group, carrying and dressing styles are labelled for fine-grained performance analysis. In practice, the gallery scale is a vital problem for recognition accuracy. To this end, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Since there are a series of gait recognition frameworks using different input data types, the GREW provides silhouettes, Gait Energy Images (GEIs) [14], optical flow, 2D and 3D poses by automatical processing. Compared with controlled gait dataset such as CASIA-B and OU-MVLP, our GREW is fully-unconstrained and has more diverse and practical view variations instead of predefined ones. Meanwhile, there are various challenging factors in the GREW such as distractor set, complex background, occlusion, carrying, dressing et al. as shown in Table 1 and Figure 2.\n\nEquipped with the proposed GREW, the unconstrained gait recognition problem is deeply investigated. Firstly, representative appearance-based and model-based baselines are performed on the GREW, which indicates a lot of room for improvement. For example, top-performed GaitSet [4] obtains 46.28% Rank-1 accuracy on the GREW test set, while it scores more than 80% on the CASIA-B and OU-MVLP. With the distractor set, gait recognition in the wild would become more challenging, while the best model scores only 41.97% Rank-1. Secondly, the influence of the data scale is explored, including the number of training identities and gallery size. Increasing training subjects consistently boosts the performance, while large-scale test set with distractor is still very difficult for CNN-based recognizer. Thirdly, performance on different attributes (gender, age group, carrying, and dressing) is reported, which gives in-depth analysis results. Lastly, we validate the effectiveness of the GREW for pre-training. Fine-tuning models pre-trained on the GREW shows superior performance for cross-dataset gait recognition.\n\nThe main contributions can be summarized as follows:\n\n\u2022 A large-scale benchmark is constructed for the research community towards gait recognition in the wild. The proposed GREW consists of 26K subjects and 128K sequences with rich attributes from flexible data streams, which makes it the first dataset for unconstrained gait recognition to the best of our knowledge. \u2022 To constitute the GREW benchmark, we collect thousands of hours of streams from multiple cameras in open systems. With automatical pre-processing and tremendous manual identity annotations, there are  more than 14M boxes that simultaneously provide silhouettes and human poses. Besides, we enrich the GREW by a distractor set with 233K sequences, making it more suitable for real-world applications. \u2022 Enabled by the new benchmark, we perform extensive gait recognition experiments and establish comprehensive baselines, including representative methods, scale influence, attributes analysis and pre-training. Results indicate that the GREW is necessary and effective for gait recognition in the wild. Besides, recognizing unconstrained gait is a very challenging task for current SOTA approaches. Lastly, the proposed dataset can be employed as effective pre-training data for controlled gait recognition to achieve higher performance.\n\n\n"}, {"paperid": "paper5", "title": "Learning Gait Representations with Noisy Multi-Task Learning", "abstract": "Gait analysis is proven to be a reliable way to perform person identification without relying on subject cooperation. Walking is a biometric that does not significantly change in short periods of time and can be regarded as unique to each person. So far, the study of gait analysis focused mostly on identification and demographics estimation, without considering many of the pedestrian attributes that appearance-based methods rely on. In this work, alongside gait-based person identification, we explore pedestrian attribute identification solely from movement patterns. We propose DenseGait, the largest dataset for pretraining gait analysis systems containing 217 K anonymized tracklets, annotated automatically with 42 appearance attributes. DenseGait is constructed by automatically processing video streams and offers the full array of gait covariates present in the real world. We make the dataset available to the research community. Additionally, we propose GaitFormer, a transformer-based model that after pretraining in a multi-task fashion on DenseGait, achieves 92.5% accuracy on CASIA-B and 85.33% on FVG, without utilizing any manually annotated data. This corresponds to a +14.2% and +9.67% accuracy increase compared to similar methods. Moreover, GaitFormer is able to accurately identify gender information and a multitude of appearance attributes utilizing only movement patterns. The code to reproduce the experiments is made publicly.", "introduction": "\n\nTechnologies relying on facial and pedestrian analysis play a crucial role in intelligent video surveillance and security systems. Facial and pedestrian analysis systems have become the norm in video intelligence, such systems being deployed ubiquitously. However, appearance-based pedestrian re-identification [1] and facial recognition models [2] invariably suffer from extrinsic factors related to camera viewpoint and resolution, and to the change in a person's appearance such as different clothing, hairstyles and accessories. Moreover, due to the proliferation of privacy laws such as GDPR, it is increasingly difficult to deploy appearance-based solutions for video-intelligence. Human movement is highly correlated with many internal and external aspects of a particular individual including age, gender, body mass index, clothing, carrying conditions, emotions and personality [3]. The manner of walking is unique to each person, it does not significantly change in short periods of time [4] and cannot be easily faked to impersonate another person [5]. Gait analysis has gained significant attention in recent years [6,7], due to solving many of the problems of appearance-based technologies without relying on the direct cooperation of subjects. However, compared to appearance-based methods, gait analysis is intrinsically harder to perform with reliable accuracy, due to the influence of many confounding factors that affect the manner of walking. This problem is tackled in literature in two major ways, either by building specialized neural architectures that are invariant to walking variations [8][9][10], or by creating large-scale and diverse datasets for training [11][12][13][14][15].\n\nOne of the first attempts of building a large-scale gait recognition dataset is OU-ISIR [14], which is comprised of 10,307 identities that walk in a straight line for a short duration of time. Such a dataset is severely limited by its lack of walking variability, having only viewpoint change as a confounding factor. Building sufficiently large datasets that account for all the walking variations imply an immense annotation effort. For example, the GREW benchmark [12] for gait-based identification, reportedly took 3 months of continuous manual annotation by 20 workers. In contrast, automatic, weakly annotated datasets are much easier to gather by leveraging existing state-of-the-art models-UWG [11], a comparatively large dataset of individual walking tracklets proved to be a promising new direction in the field. Increasing the dataset size is indeed correlated with performance on downstream gait recognition benchmarks [11], even though no manual annotations are provided. One limitation of these datasets is that they are annotated with attributes per individual only sparsely, and not addressing the problem of pedestrian attribute identification (PAI), currently performed only through appearance-based methods [16][17][18]. Walking pedestrians are often annotated only with their gender, age, and camera viewpoint [8,12,14,15]. Even though gait-based demographic identification is a viable method for pedestrian analysis [19], it is also severely limited by the lack of data. Also, many attributes from PAI networks such as gender, age and body type have a definite impact on walking patterns [20][21][22], and we posit that they can be identified with a reasonable degree of accuracy using only movement patterns and not utilizing appearance information.\n\nWe propose DenseGait, the largest gait dataset for pretraining to date, containing 217 K anonymized tracklets in the form of skeleton sequences, automatically gathered by processing real-world surveillance streams through state-of-the-art models for pose estimation and pose tracking. An ensemble of PAI networks was used to densely annotate each skeleton sequence with 42 appearance attributes such as their gender, age group, body fat, camera viewpoint, clothing information and apparent action. The purpose of DenseGait is to be used for pretraining networks for gait recognition and attribute identification, it is not suitable for evaluation since it is annotated automatically and does not contain manual, ground-truth labels. DenseGait contains walking individuals in real scenarios, it is markerless, non-treadmill, and avoids unnatural and constrictive laboratory conditions, which have been shown to affect gait [23]. It practically contains the full array of factors that are present in real world gait patterns.\n\nThe dataset is fully anonymized, and any information pertaining to individual identities is removed, such as the time, location and source of the video stream, and the appearance and height information of the person. DenseGait is a gait analysis dataset primarily intended for pretraining neural models-using it to explicitly identify the individuals within it is highly unfeasible, requiring extensive external information about the individuals, such as personal identifying information (i.e., their name or ID) and a baseline gait pattern. According to GDPR (https://eur-lex.europa.eu/eli/reg/2016/679/oj, accessed on 1 July 2022) legislation, data used for research purposes can be used if anonymized. Moreover, anonymized data does not conform to the rigors of personal data and can be processed without explicit consent. Nevertheless, any attempt to use of DenseGait to explicitly identify individuals present in it is highly discouraged.\n\nWe chose to utilize only skeleton sequences for gait analysis, as current appearancebased methods that rely on silhouettes are not privacy preserving, potentially allowing for identification based only on the person's appearance, rather than their movement [24]. Skeleton sequences encode only the movement of the person, abstracting away any visual queues regarding identity and attributes. Moreover, skeleton-based solutions have the potential to generalize across tasks such as action recognition, allowing for a flexible and extensible computation.\n\nDenseGait, compared to other similar datasets [11], contains 10\u00d7 more sequences and is automatically annotated with 42 appearance attributes through a pretrained PAI ensemble (Table 1). In total, 60 h of video streams were processed, having a cumulative walking duration of pedestrians of 410 h. We release the dataset under open credentialized access, for research purposes only, under CC-BY-NC-ND-4.0 (https://creativecommons. org/licenses/by-nc-nd/4.0/legalcode, accessed on 1 July 2022) License. We also propose GaitFormer, a multi-task transformer-based architecture [25] that is pretrained on DenseGait in a self-supervised manner, being able to perform exceptionally well in zero-shot gait recognition scenarios on benchmark datasets, achieving 92.5% identification accuracy from direct transfer on the popular CASIA-B dataset, without using any manually annotated data. Moreover, it obtains good results on demographic and pedestrian attribute identification from walking patterns, with no manual annotations. GaitFormer represents the first use of a plain transformer encoder architecture in gait skeleton sequence processing, without relying on hand-crafted architectural modifications as in the case of graph neural networks [26,27]. This paper makes the following contributions:\n\n\n"}, {"paperid": "paper6", "title": "PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait", "abstract": "Psychological trait estimation from external factors such as movement and appearance is a challenging and long-standing problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totalling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.", "introduction": "\n\nHow we move and behave in the physical space is intrinsically tied to our psychological workings.Besides each individual's muscle developments and the influence of extrinsic factors, walking (or gait) is influenced by gender [10], emotions [4], personality traits [59], and mental distress [23].Walking is an action performed ubiquitously by healthy individuals.So far, in the domain of AI-powered gait analysis, significant attention has been dedicated to gait recognition [2,13,17,22,28,40,69], but with little focus being dedicated to exploring the manifestations of psycholog-ical traits in gait [29].\n\nWhile there has been research in studying personality manifestations in video [11,24,43,54], walking is left largely unexplored.Nonetheless, some studies [46,59,64] separately confirm significant differences in gait between individuals with different personalities, levels of aggression, depression and self-esteem.Such works are based on the assumption that psychological experience manifests itself into behaviour, in line with theories of embodiment [51,74].Embodiment suggests a biofeedback influence loop between the psyche and physical gestures / actions.Actions, gestures and posture can have a clear impact on thought [27], memory and recall [50], and mood [51,53].In this context, currently, there is no open dataset for exploring the embodied manifestation of psychological traits in gait.\n\nWe propose PsyMo (Psychological traits from Motion), a multi-purpose gait database containing 312 subjects walking under multiple viewpoints and walking variations, annotated with self-assessed psychological traits from 6 psychological questionnaires: Big Five Index [32], Rosenberg Self-Esteem [57], Buss-Perry Aggression [9], Occupational Fatigue Exhaustion/Recovery Scale [70], Depression Anxiety Stress Scale [45], and the General Health Questionnaire [26].Across all subscales and factors, PsyMo contains 17 psychological traits.Alongside self-assessed psychological traits, participants also submitted their age, gender, height and weight.PsyMo is the first public dataset of its kind and pertains to the set of \"controlled\" gait datasets used for benchmarking gait analysis tasks under various walking variations, similar to CASIA-B [73] and FVG [75].\n\nPsyMo covers 7 walking variations: normal walking, changing clothes, slow walking, fast walking, walking with a bag, and two additional dual-tasks (i.e.walking while performing a cognitive task [19]), largely ignored in current datasets: walking while texting and walking while talking on the phone.The walks are captured using 3 synchronized consumer video surveillance cameras (Tapo C200) to mimic adverse conditions (i.e.low-fps, fish-eye distortion, lowresolution) present in real-world surveillance scenarios, for a total of 6 viewpoints (which include the round-trips).In total, we gathered 14,976 walking sequences.\n\nWe provide 2D / 3D human poses, silhouettes, and 3D human meshes for gait processing.We extracted appearance-based silhouettes using instance segmentation with a pretrained Hybrid Task Cascade (HTC) model [14], and 2D skeletons using AlphaPose [36], a state-of-the-art model for pose estimation.Further, we used CLIFF [39] to estimate 3D human pose and 3D meshes in the form of parametric SMPL predictions.The dataset is fully anonymized, and subjects gave explicit and informed consent for processing and distributing it.We do not release raw videos, as they can lead to identifying the subjects in our study.We release only processed gait information in the form of silhouettes, 2D / 3D skeletons and 3D human meshes.\n\nPsyMo is primarily intended as a rich resource for exploring psychological manifestations into walking patterns, allowing for an interdisciplinary study into human behaviour from both the artificial intelligence and psychology research communities.We propose several evaluation procedures for the estimation of psychological traits from gait.Furthermore, due to its size and number of walking variations, PsyMo can also be used as a benchmark dataset for standard gait recognition, in addition to / as a more diverse drop-in replacement to existing gait benchmark datasets [60,73,75].For this purpose, we also proposed several evaluation protocols for gait recognition.\n\n\n"}]