[{"paperid": "paper0", "title": "Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces", "abstract": "Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D geometry processing. Several recent state-of-the-art methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce \\textit{Neural-Pull}, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest points on the surface using the predicted signed distance values and the gradient at the query locations, both of which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods.", "introduction": "\n\nSigned Distance Functions (SDFs) have been an important 3D shape representation for deep learning based 3D shape analysis (Park et al., 2019;Mescheder et al., 2019;Mildenhall et al., 2020;Michalkiewicz et al., 2019;Saito et al., 2019;Rematas et al., 2021;Sitzmann et al., 2020;Ost et al., Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). 2020; Takikawa et al., 2021;Martel et al., 2021;Oechsle et al., 2021;Azinovic et al., 2021;Dupont et al., 2021), due to their advantages over other representations in representing high resolution shapes with arbitrary topology. Given ground truth signed distance values, it is intuitive to learn an SDF by training a deep neural network to regress signed distance values for query 3D locations, where an image (Michalkiewicz et al., 2019;Park et al., 2019) or a point cloud (Jia & Kyan, 2020;Erler et al., 2020) representing the shape can serve as a condition which is an additional input of the network. It has also been shown how to learn SDFs from multiple 2D images rather than 3D information using differentiable renderers (Liu et al., 2020;Jiang et al., 2020b;Zakharov et al., 2020;Wu & Sun, 2020). In this paper, we address the problem of learning SDFs from raw point clouds and propose a new method that outperforms the state-of-the-art on widely used benchmarks.\n\nCurrent solutions (Gropp et al., 2020;Chibane et al., 2020b;Atzmon & Lipman, 2020a; aim to estimate unsigned distance fields by leveraging additional constraints. The rationale behind these solutions is that an unsigned distance field can be directly learned from the distances between a set of query 3D locations and their nearest neighbors on the 3D point clouds, while the signs of these distances require more information to infer, such as geometric regularization (Gropp et al., 2020), sign agnostic learning (Atzmon & Lipman, 2020a;, or analytical gradients (Chibane et al., 2020b).\n\nIn this paper, we propose a method to learn SDFs directly from raw point clouds without requiring ground truth signed distance values. Our method learns the SDF from a point cloud, or from multiple point clouds with conditions by training a neural network to learn to pull the surrounding 3D space onto the surface represented by the point cloud. Hence we call our method Neural-Pull. Specifically, given a 3D query location as input to the network, we ask the network to pull it to its closest point on the surface using the predicted signed distance value and the gradient at the query location, both of which are calculated by the network itself. The pulling operation is differentiable, and depending on the sign of the predicted distance, it moves the query location along or against the direction of the gradient with arXiv:2011.13495v2 [cs.CV] 23 May 2021 a stride given by the signed distance. Since our training objective involves both the signed distance and its gradient, it leads to highly effective learning. Our experiments using widely used benchmarks show that Neural-Pull can learn SDFs more accurately and flexibly when representing 3D shapes in different applications than previous state-of-theart methods. Our contributions are listed below. i) We introduce Neural-Pull, a novel approach to learn SDFs directly from raw 3D point clouds without ground truth signed distance values.\n\nii) We introduce the idea to effectively learn SDFs by updating the predicted signed distance values and the gradient simultaneously in order to pull surrounding 3D space onto the surface.\n\niii) We significantly improve the state-of-the-art accuracy in surface reconstruction and single image reconstruction under various benchmarks.\n\n\n"}, {"paperid": "paper1", "title": "Learning Consistency-Aware Unsigned Distance Functions Progressively from Raw Point Clouds", "abstract": "Surface reconstruction for point clouds is an important task in 3D computer vision. Most of the latest methods resolve this problem by learning signed distance functions (SDF) from point clouds, which are limited to reconstructing shapes or scenes with closed surfaces. Some other methods tried to represent shapes or scenes with open surfaces using unsigned distance functions (UDF) which are learned from large scale ground truth unsigned distances. However, the learned UDF is hard to provide smooth distance fields near the surface due to the noncontinuous character of point clouds. In this paper, we propose a novel method to learn consistency-aware unsigned distance functions directly from raw point clouds. We achieve this by learning to move 3D queries to reach the surface with a field consistency constraint, where we also enable to progressively estimate a more accurate surface. Specifically, we train a neural network to gradually infer the relationship between 3D queries and the approximated surface by searching for the moving target of queries in a dynamic way, which results in a consistent field around the surface. Meanwhile, we introduce a polygonization algorithm to extract surfaces directly from the gradient field of the learned UDF. The experimental results in surface reconstruction for synthetic and real scan data show significant improvements over the state-of-the-art under the widely used benchmarks.", "introduction": "\n\nReconstructing surfaces from 3D point clouds is vital in 3D vision, robotics and graphics. It bridges the gap between raw point clouds that can be captured by 3D sensors and the editable surfaces for various downstream applications. Recently, Neural Implicit Functions (NIFs) have achieved promising results by training deep networks to learn Signed Distance Functions (SDFs) [44,26,42,14] or occupancies [40,46,41,10], and then extract a polygon mesh of a continuous iso-surface from a discrete scalar field using the marching cubes algorithm [35]. However, the NIFs approaches based on learning internal and external relations can only reconstruct closed surfaces. The limitation prevents NIFs from representing most real-world objects such as cars with inner structures, clothes with unsealed ends or 3D scenes with open walls and holes.\n\nAs a remedy, state-of-the-art methods [12,62,49] learn Unsigned Distance Functions (UDFs) as a more general representation to reconstruct surfaces from point clouds. However, these methods can not learn UDFs with smooth distance fields near surfaces, due to the noncontinuous character of point clouds, even using ground truth distance values or large scale meshes during training. Moreover, most UDF approaches failed to extract surfaces directly from unsigned distance fields. Particularly, they rely on post-processing such as Ball-Pivoting-Algorithm (BPA) [3] to extract surfaces based on the dense point clouds generated from the learned UDF, which is very time-consuming and also leads to surfaces with discontinuity and poor quality.\n\nTo solve these issues, we propose a novel method to learn consistency-aware UDFs directly from raw point clouds. We learn to move 3D queries to reach the approximated surface aggressively with a field consistency constraint, and introduce a polygonization algorithm to extract surfaces from the learned unsigned distance functions in a new perspective. Our method can learn UDFs from a single point cloud without requiring ground truth distances, point normals, or a large scale training set. Specifically, given query locations sampled in 3D space as input, We learn to move them to the approximated surface according to the predicted unsigned distances and the gradient at the query locations. More appealing solutions [1,2,20,36] have been proposed to learn SDFs from raw point clouds by optimizing the relationship between the query point and its closest point in raw data as a surface prior. However, since the raw point cloud is a highly discrete approximation of the surface, the closest point to the query location is always inaccurate and ambiguous, which makes the network difficult to converge to an accurate UDF due to the inconsistent or even conflicting optimization directions in the distance field. Therefore, in order to encourage the network to learn a consistency-aware and accurate unsigned distance field, we propose to dynamically search the optimization target with a specially designed loss function containing field consistency to mimic the conflict optimizations. We also progressively infer the mapping between 3D queries and the approximated zero iso-surface by using well-moved queries as additional priors for promoting further convergence. To extract a surface in a direct way, we propose to use the gradient field of the learned UDFs to determine whether two queries are on the same side of the approximated surface or not. In contrast to NDF [12] which also learns UDFs but takes dense point clouds as output and depends on BPA [3] to generate meshes, our method shows great advantages in efficiency and accuracy due to the straightforward surface extraction.\n\nOur main contributions can be summarized as:\n\n\u2022 We propose a novel neural network that learns consistent-aware UDFs directly from raw point clouds. Our method gradually infers the relationship between 3D query locations and the approximated surface with a field consistent loss.\n\n\u2022 We introduce an algorithm for directly extracting high-fidelity iso-surfaces with arbitrary topology from the gradient field of the learned unsigned distance functions.\n\n\u2022 We obtain state-of-the-art results in surface reconstruction from synthetic and real scan point clouds under the widely used benchmarks.\n\n\n"}, {"paperid": "paper2", "title": "SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction", "abstract": "Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code url is https://github.com/THHHomas/SuperUDF.", "introduction": "\n\nSurface reconstruction from 3D point clouds has been a long-standing problem in graphics and vision.Since the seminal work of Poisson surface reconstruction [1], there have been a large body of literature [2].Albeit relying on normal as input, the idea of reconstruction based on implicit field has been inspiring many deep learning methods [3], [4], [5], [6], [7].Signed Distance Function (SDF) is a typical implicit representation of 3D shapes of arbitrary geometry and topology.Many deep learning models have been proposed to predict the SDF of a 3D point cloud for high-quality reconstruction [3], [4], [6], [8].However, a major drawback of SDF is that it can represent only closed and watertight surfaces due to its nature of inside/outside discrimination, so SDF-based methods find difficulty in handling open surfaces such as garments or incomplete scans.\n\nUnsigned Distance Function (UDF) is suited for representing open surfaces since it does not differentiate between inside and outside.Compared to SDF, UDF is easier to learn since it concerns only distance information and ignores the sign.The pioneering work of [9] proposes a direct rendering method based on UDFs.However, both this work and its followups [10], [11] require strong supervision of ground-truth UDFs.Moreover, surface extraction from UDFs is difficult due to the absence of signs which is critical to marching cube [12].[13] convert UDFs to meshes via elevating a UDF to an SDF which brings back the limitation of SDFs.\n\n1, National University of Defense Technology PSR NDF SuperUDF GT Input CAP Fig. 1.A scene from ScanNet [14] reconstructed with PSR [1], NDF [9] , CAP [15] and our SuperUDF.\n\nRecently, [15] introduces a self-supervised pipeline to learn smooth UDFs directly from raw point clouds.Their network is trained in a shape-specific manner, requiring \u223c20 minutes for overfitting one shape.Furthermore, the method is sensitive to the sampling density of the input point cloud; the prediction of field gradients is inaccurate for sparsely sampled point clouds.Inspired by recent works [5], [4] demonstrating that geometry priors learned from large datasets benefit efficient reconstruction, we propose SuperUDF, a self-supervised learning method for fast UDF estimation with learned priors.To obtain robustness under sparse sampling, we introduce a novel regularization loss to train SuperUDF.We also introduce a learning-based mesh extraction directly from the estimated UDFs.\n\nThe core idea of our self-supervised UDF estimation draws inspiration from the seminal surface approximation operator of Locally Optimal Projection (LOP) [16].In particular, we first upsample the input point cloud via per-point duplication and random perturbation.The key insight behind our design is that if the UDF is estimated correctly, the upsampled points should be locally projected onto the underlying (ground-truth) surface following the gradient of the UDF.Since the ground-truth surface is unknown a priori, we instead require the projected points to approximate the input point cloud.\n\nTo do so, we impose a direct constraint on the estimated UDF for a better data approximation by minimizing the mean UDF value of all points of the input point cloud.However, imposing the approximation constraints alone may lead to noisy UDFs where the monotonicity of UDF at one side of the zero-level-set may be violated.To this end, we devise a regularization loss to ensure UDF monotonicity.In particular, for each upsampled point, denoted by p, we compute its offset point q by moving p along UDF gradient for a distance of half of p's UDF value.Here, q is expected to reside at the same side as p about the surface.We impose that the UDF gradients at the two points are the same and the UDF value at q should be half of that of p.This design also makes the points projected more uniformly.\n\nA difficulty in the method, however, is that the computation of UDF gradients in the losses above is time-consuming, making the training intractable.We thus opt for the reverse.We instead estimate a projection vector for each upsampled point.This results in a projection flow as an approximation of the UDF gradients.The projection flow network can also be trained in a self-supervised manner using the constraints above.The resulting projection flow is a strong geometry prior learned from a shape dataset which greatly improves training efficiency.The UDF can be easily obtained from the projection flow based on their duality.\n\nTo learn mesh extraction from UDFs, we partition the 3D space into a regular grid and train a 3D CNN for each grid cell to estimate the signs of its 8 corners.Note that we do not care about the absolute signs being positive or negative, but only concern with their relative signs.In practice, we fix the sign of one corner and let the network predict the rest.Since the network is trained for local sign prediction, it is easy to train and generalizes well across different shapes.\n\nExtensive evaluations demonstrate that SuperUDF outperforms the state of the arts on ShapeNet, MGN and Scannet in terms of both quality and efficiency.The main contributions of our work are:\n\n\u2022 A self-supervised UDF estimation network inspired by the classical surface approximation operator of Locally Optimal Projection (LOP).\u2022 A number of inductive biases on UDF geometry and a pre-learned geometry prior for efficient learning.\u2022 A novel regularization loss to make SuperUDF robust to sparse sampling.\u2022 A learning-based mesh extraction method that generalizes well across shapes.\n\nII. RELATED WORK a) Traditional point cloud surface reconstruction: Point cloud reconstruction has been a long-standing task in graphics and vision.The most important traditional method is Poisson surface Reconstruction [1] and ball-pivoting reconstruction [17].The former method classifies the query point according to the Poisson indicator function.The latter constructs the continuous surface by making up a ball rolling on the points.Those two methods can reconstruct pretty good surfaces, however, the performance can be improved more.\n\nb) SDF-based implicit surface reconstruction: Deep methods based on SDF always classify the occupancy of query points or directly regress the SDF value, which can be divided into local methods, global methods, and a combination.The global method, as the name implies, when giving a query point, classifies the query according to the whole shape information.The local method classifies the query point according to its neighbor points of it.Representative global methods are DeepSDF [3], BSP-Net [18].The routine of those methods is extracting the feature code of the whole shape and then recovering the surface from the code.Representative local methods are ConvOccNet [4], SSR-Net [19], DeepMLS [3] and POCO [8].ConvOccNet [4] first converts the point cloud feature into voxels and then applies volume convolution to enhance the feature of every voxel.SSR-Net [19] firstly extracts point feature, then maps the neighborhood points feature to octants, and finally classifies the octants.DeepMLS [20] tries to predict the normal and radius of every point, then classify the query points according to the moving least-squares equation.Furthermore, except for local methods and global methods, there are some methods that try to combine global and local information.The most representative method is Points2Surf [6].It tries to regress the absolute SDF value according to local information and classifies the sign according to global information.Another implicit surface reconstruction methods based on SDF are SAL [21], SALD [22] and On-Surface Prior [23].This kind of method aims to convert explicit representation, such as point cloud and 3D triangle soup, to implicit SDF representation.Thus, every 3D model needs a unique training process and unique network parameters.SAL [21] uses MLP to predict the SDF of shapes but adopts the UDF-based metric to supervise the network training, SALD [22] follows the SAL and adds derivative regularization term.On-surface Prior [23] use a pre-trained UDF-based network to help the main network to predict better SDF.\n\nThe SDF methods mentioned above achieve excellent progress in point cloud reconstruction.However, SDF representation has its weakness.It is hard to represent an open surface or partial scan.Furthermore, all the deep methods mentioned above needs 3D ground truth as the training label, while 3D ground truth is expensive for closed shape and real scan.\n\nc) UDF-based implicit surface reconstruction: UDF can express more general surfaces, such as open surfaces and partial scans.Lots of works have focus on the UDF representation.NDF [9] uses UDF to represent the surface, then they propose a point cloud up-sample method while reconstructing the mesh not directly from UDF but up-sampled point cloud via Ball Pivoting [17] method.DUDE [24] represent the shape with a volumetric UDF encoded in a neural network.Those works bring UDF to deep implicit surface reconstruction area and achieve good results.However, they need the 3D ground truth as the training label.What's more, they remain an open problem on how to directly extract iso-surface from UDF. UWED [25] makes use of MLS ( Moving Least Squares ) to convert the UDF to dense point cloud.d) Self-supervised method for surface reconstruction: Besides the supervised methods mentioned above, there are a few self-supervised approaches.For example, Neural Pulling [7] proposes a pipeline to train a network such that the network can predict SDF in the whole space without any extra supervision.However, the method has several disadvantages.First, it needs a dense and complete point cloud as supervision, limiting its scalability in real-world scenarios.Second, the method is only capable of reconstructing objects with closed surfaces.Thus, CAP [15]  predicting UDF rather than SDF so it can represent open surface.But it also suffers from the requirements of dense point cloud and long inference time.e) Iso-surface extraction: For SDF, the most common method for iso-surface extraction is Marching Cube [12].It is a template matching method based on the sign of eight grid corners.For iso-surface extraction on UDF, there are only a few works.MeshUDF [13] uses a neural network to vote the sign of the grid corner while bringing some sign conflict.The two methods try to convert the UDF to SDF.But SDF has difficulty representing the open surface.They are meaningful exploitation of iso-surface extraction on UDF.While sharing the same drawback of SDF and weak generalization ability.\n\n\n"}]