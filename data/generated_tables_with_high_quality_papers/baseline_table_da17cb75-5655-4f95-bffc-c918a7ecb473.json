{"input_paper": [{"paperid": "paper0", "title": "IPatch: a remote adversarial patch", "abstract": "Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image. In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples `remote adversarial patches' (RAP). We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.", "introduction": "None"}, {"paperid": "paper1", "title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks", "abstract": "Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.", "introduction": "The rise of deep learning unlocked unprecedented performance in several scientific areas [24]. Convolutional (a) (b) (c) (d) (e) (f) Figure 1: Proposed adversarial patches on Cityscapes [5] (b) and CARLA Simulator [6] (e); (c/f) show the corresponding SS predicted by BiSeNet [40]; (a/d) show the corresponding predictions obtained using random patches instead of adversarial ones.\n\nneural networks [16] (CNNs) yielded super-human performance for many different computer vision tasks, such as image recognition [9], object detection [27] [26], and image segmentation [20]. Image segmentation, and semantic segmentation (SS) in particular, is used in autonomous driving perception pipelines [30], mainly for object detection [20]. Despite their high performance, CNNs are prone to adversarial attacks [31]. Most of the literature on adversarial attacks focuses on directly manipulating the pixels of the whole image, hence making the assumption that the attacker has control over the digital representation of the environment obtained by the on-board cameras. This kind of unsafe inputs are called digital adversarial examples.\n\nAlthough such digital attacks do not transfer well into the real world, they continue to be used to evaluate the robustness of models in safety-critical systems [12,3,19]. Realworld adversarial examples (RWAEs), on the other hand, are physical objects that can be placed in the field of view of a camera, such that the resulting image acts as an adversarial example for the neural network under attack [17]. Thus, RWAEs can induce errors in neural networks without requiring the attacker to access the digital representation of the image, thereby making them a more realistic and dangerous threat to safety-critical systems. This paper. This work focuses on RWAEs, as they repre-sent a potential threat to tasks in autonomous driving today. Although the effects of RWAEs have been studied extensively in the literature for classification and object detection, those on SS remain relatively unexplored. However, SS is an integral part of autonomous driving pipelines [30]. Thus, this paper examines various state-of-the-art models for real-time SS aiming at benchmarking their robustness to RWAEs in autonomous driving scenarios.\n\nOf the several types of RWAEs proposed in the literature [32], the form of attack used in this paper is adversarial patches [4]. This is because attacks that perturb the whole image are not practically feasible in the real world. Conversely, such patches can be easily printed and attached to any visible 2D surface in the driving environment, such as billboards and road signs, thus making them a simple, yet effective attack strategy.\n\nThe paper starts by recognizing the shortcomings of the standard cross-entropy loss for optimizing adversarial patches for SS. Thus, an extension to the cross-entropy loss is proposed and integrated in all the performed attacks. This extension forces the optimization to focus on pixels that are not yet misclassified, thus obtaining patches that are more powerful compared to those generated with the standard cross-entropy-based setting [21].\n\nFollowing this rationale, the robustness of real-time SS models to RWAEs attacks is benchmarked. The paper starts by first examining the case of driving images, crafting adversarial patches on the Cityscapes dataset [5], a popular benchmark of high-resolution images of urban driving. Robust real-world patches are crafted by following the Expectation Over Transformation (EOT) [2] paradigm, which has been extended in this work to attack SS models. Furthermore, a comparison against non-robust patches (without EOT) is presented to question their effectiveness on driving scenes.\n\nAnother set of experiments targeted a virtual 3D scenario, for which a stronger adversarial attack is presented and tested. The proposed scene-specific attack, defined in Section 3.4, is a more practical tool for crafting adversarial patches in a realistic autonomous driving scenario. It assumes that the attacker is interested in targeting an autonomous driving scene at a particular corner of a specific town, where information about the position of the attackable 2D surface (in our case, a billboard) is available. To satisfy such requirements we developed and tested this attack using the CARLA simulator, which provides all the needed geometric information. These experiments include a comparison with the EOT-based and non-robust patches, performed by importing them into the CARLA world and placing them on billboards to simulate a realistic study. Figure 1 provides some examples of the effect of our patches on Cityscapes and CARLA.\n\nThe last set of experiments were conducted on a real-world driving scenario, which required collecting a dataset within the city, optimizing a patch on it, physically printing said patch on a billboard, and finally evaluating SS models on images containing the printed patch.\n\nTo the best of our knowledge, this work represents the first exhaustive evaluation of the robustness of SS models against RWAEs for autonomous driving systems. The results of the experiments state important observations that should be taken into consideration while evaluating the trustworthiness of SS models in autonomous driving. First, they demonstrate that non-robust patches are not good candidates for assessing the practical robustness of an SS model to adversarial examples. Indeed, while they proved to be effective in attacking images related to driving scenes (from Cityscapes), they do not induce any real-world adversarial effect when crafted and tested in a virtual 3D world (based on CARLA). Conversely, robust patches, crafted with EOT or the proposed scene-specific approach, resulted to be less effective than non-robust ones on Cityscapes images, but were capable to accomplish the attack in both virtual 3D world and the real world. Nevertheless, their effectiveness in the latter two cases still resulted to be quite limited, hence questioning the practical relevance of RWAEs.\n\nIn summary, the paper makes the following contributions:\n\n\u2022 It proposes an extension to the pixel-wise crossentropy loss to enable crafting strong patches for the semantic segmentation setting. \u2022 It proposes a novel technique for crafting adversarial patches for autonomous driving scenarios that utilize geometric information of the 3D world. \u2022 It finally reports an extensive evaluation of RWAEbased attacks on a set of real-time semantic segmentation models using data from the Cityscapes dataset, CARLA, and the real world. The remainder of this paper is organized as follows: Section 2 provides a brief overview of related work existing in the literature, Section 3 formalizes the proposed loss function, pipeline, and attack strategy, Section 4 reports the experimental results, and Section 5 states the conclusions and proposes ideas for future work."}], "pap_to_tab": {"What is the main focus of the research?": {"paper_1": ["Introduction of a new type of adversarial patch called 'remote adversarial patches' (RAP) that alters model perception of image semantics from a distance and testing its effect on image segmentation and object recognition models."], "paper_2": ["Evaluating the robustness of Semantic Segmentation (SS) models for autonomous driving against both digital and real-world adversarial patch attacks including a novel loss function for crafting attacks."]}, "What applications are considered in the research?": {"paper_1": ["Autonomous vehicles, medical screening, image segmentation, and object recognition."], "paper_2": ["Autonomous driving specifically focusing on the Semantic Segmentation (SS) aspect."]}, "What type of adversarial examples are introduced or investigated?": {"paper_1": ["Remote adversarial patches that can be placed anywhere within an image and change the classification or semantics of locations far from the patch."], "paper_2": ["Real-world adversarial examples in the form of physical objects like billboards and printable patches as well as digital adversarial patches."]}, "What datasets are used in the research?": {"paper_1": ["CamVid street view dataset for image segmentation RAP attacks and preliminary results on the YOLOv3 model for object recognition."], "paper_2": ["Cityscapes dataset for digital attacks and the CARLA driving simulator for real-world scenarios."]}, "What is the proposed method or approach?": {"paper_1": ["A new adversarial example called IPatch, which is a remote adversarial patch for altering semantics, along with an in-depth analysis on its effects using different state-of-the-art architectures."], "paper_2": ["An in-depth evaluation using a novel loss function, extension of the Expectation Over Transformation (EOT) paradigm for SS, and a scene-specific attack leveraging the CARLA driving simulator."]}, "What are the findings or results of the paper?": {"paper_1": ["The IPatch can change the classification of a remote target region with a success rate of up to 93% on average on tested models."], "paper_2": ["Experiments revealed that proposed attack formulations outperform previous work for crafting adversarial patches but are notably less effective in the real world, questioning the practical relevance of such attacks."]}, "What architectures are tested?": {"paper_1": ["Five state-of-the-art architectures with eight different encoders for image segmentation and preliminary tests on the YOLOv3 model for object recognition."], "paper_2": ["Popular Semantic Segmentation models, though specific architectures are not listed in the abstract."], "paper_3": ["Not applicable"]}, "What are the implications for autonomous driving?": {"paper_1": ["The study suggests vulnerabilities in autonomous vehicles' perception systems due to remotely placed adversarial patches that can mislead such systems."], "paper_2": ["The study assesses the feasibility of adversarial attacks on SS models in real-world autonomous driving scenarios and shows reduced effectiveness compared to digital scenarios."]}}, "cc_to_tab": {"Topic Focus": {"paper_1": ["Robustness of machine learning models, object identification, image semantics alteration"], "paper_2": ["Robustness of machine learning models, object detection, semantic segmentation"]}, "Adversarial Attacks": {"paper_1": ["Introduction of a new form of adversarial attack: remote adversarial patches (IPatch)"], "paper_2": ["Evaluation of digital and real-world adversarial patches, effectiveness in real-world scenarios"]}, "Methodologies": {"paper_1": ["In-depth analysis using CamVid dataset, effects on various architectures and encoders"], "paper_2": ["Cityscapes dataset and CARLA driving simulator used for testing susceptibility to physical adversarial objects"]}, "Novel Contributions": {"paper_1": ["Introduction of the remote adversarial patch concept"], "paper_2": ["New type of attack optimization: scene-specific attack, extension of EOT paradigm to semantic segmentation"]}, "Real-world Application and Testing": {"paper_1": ["Preliminary results on YOLOv3 model, implying practical implications"], "paper_2": ["Testing of a physical adversarial billboard in an outdoor scenario, assessing real-world impact"]}, "Findings on Efficacy": {"paper_1": ["High success rates in changing target classifications"], "paper_2": ["Digital attacks are effective, real-world attacks notably less effective"]}, "Potential for Real-world Exploitation": {"paper_1": ["High success rate, indicating significant vulnerability"], "paper_2": ["Questions practical relevance with decreased effectiveness in real-world scenarios"]}, "Target Models": {"paper_1": ["Image segmentation models"], "paper_2": ["Semantic segmentation models and their robustness"]}}, "multi_scheme": {"What is an IPatch as described in the paper?": {"paper_0": "IPatch is a new type of adversarial patch introduced in the paper which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch.", "paper_1": "IPatch is a new type of adversarial patch introduced in the paper that alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch."}, "What is the scene-specific attack proposed in the paper, and how does it relate to the CARLA driving simulator?": {"paper_0": "", "paper_1": "The scene-specific attack proposed in the paper is a practical tool for crafting adversarial patches in a realistic autonomous driving scenario. It assumes that the attacker is interested in targeting a specific autonomous driving scene and uses geometric information about the position of the attackable 2D surface, such as a billboard. This attack was developed and tested using the CARLA simulator, which provides the needed geometric information."}, "How was the Expectation Over Transformation (EOT) paradigm extended in the paper?": {"paper_0": "", "paper_1": "The paper extended the Expectation Over Transformation (EOT) paradigm to attack SS models. The paper compares the robustness of real-time SS models to RWAEs using EOT-based patches and demonstrates their capacity to accomplish an attack in both virtual 3D world and the real world."}, "How does the IPatch alter the classification or semantics of locations far from the patch?": {"paper_0": "The IPatch alters the classification or semantics of locations far from the patch by being placed anywhere within an image, leading to changes in how the model perceives different parts of the image not just where the patch is located.", "paper_1": "The paper abstract mentions that the IPatch can be placed anywhere within an image to change the classification or semantics of locations far from the patch. It implies that the IPatch is designed to remotely manipulate the model's perception of different regions of the image, not just the area where the patch is located."}, "What are the main computer vision tasks mentioned in the paper where deep learning and convolutional neural networks have achieved impressive performance?": {"paper_0": "", "paper_1": "The main computer vision tasks mentioned in the paper where deep learning and CNNs have achieved impressive performance include image recognition, object detection, and image segmentation."}, "What were the results of the environmental tests on SS models when subjected to digital and real-world adversarial patches?": {"paper_0": "", "paper_1": ""}, "Can the findings on image segmentation RAP attacks be generalized to other models beyond those tested in the study?": {"paper_0": "", "paper_1": ""}, "Did the experimental results confirm the practical relevance of adversarial attacks to SS models for autonomous/assisted driving?": {"paper_0": "", "paper_1": "The experimental results question the practical relevance of RWAEs, as robust patches crafted using EOT or the proposed scene-specific approach were less effective than non-robust ones on Cityscapes images and showed quite limited effectiveness in both virtual 3D world and the real world."}, "What is the main focus of the evaluation conducted in the paper on the robustness of semantic segmentation (SS) models?": {"paper_0": "", "paper_1": "The main focus of the evaluation on the robustness of SS models is benchmarking these models' robustness to real-world adversarial patch attacks in autonomous driving scenarios."}, "What does the paper reveal about the current state of robustness in SS models for autonomous driving against adversarial attacks?": {"paper_0": "", "paper_1": "The paper reveals that the current state of robustness in SS models for autonomous driving against adversarial attacks needs to be improved, as demonstrated by the effectiveness of robust patches in virtual 3D and real-world driving scenarios."}, "What are adversarial perturbations and how do they affect deep learning models?": {"paper_0": "", "paper_1": "Adversarial perturbations are considered unsafe inputs that take the form of subtle modifications made to the input of a deep learning model with the intent to confuse and mislead the model into making incorrect decisions or classifications."}, "How many state-of-the-art architectures were tested for image segmentation RAP attacks in the study?": {"paper_0": "Five state-of-the-art architectures with eight different encoders were tested for image segmentation RAP attacks in the study.", "paper_1": ""}, "What encoders were used in the study's analysis on image segmentation RAP attacks?": {"paper_0": "The paper does not specify which encoders were used in the study's analysis on image segmentation RAP attacks in the abstract.", "paper_1": ""}, "Was the IPatch tested on object recognition models in the study, and if so, which model?": {"paper_0": "Yes, the IPatch was tested on object recognition models, with preliminary results on the popular YOLOv3 model.", "paper_1": ""}, "How do the proposed attack formulations compare to previous work in crafting adversarial patches for SS?": {"paper_0": "", "paper_1": "The paper recognizes the shortcomings of the standard cross-entropy loss for optimizing adversarial patches for SS and proposes an extension to the loss that focuses the optimization on pixels not yet misclassified. This creates more powerful patches compared to those generated with the standard cross-entropy-based setting. The paper proposes a novel technique utilizing geometric information of the 3D world for crafting adversarial patches specific to autonomous driving scenarios."}, "What novel loss function is introduced in the paper for crafting adversarial patches?": {"paper_0": "", "paper_1": "An extension to the pixel-wise cross-entropy loss is introduced in the paper to enable crafting strong patches for the semantic segmentation setting. This extension focuses the optimization on pixels that are not yet misclassified."}, "What is the main purpose of a remote adversarial patch (RAP)?": {"paper_0": "The main purpose of a remote adversarial patch (RAP) appears to be to alter the perception of a deep learning model in image classification or semantics across different regions of an image, not just where the patch is located, thus impacting the model's overall interpretation of an image's content.", "paper_1": ""}, "What implications might the use of RAPs like IPatch have for models used in autonomous vehicles and medical screening?": {"paper_0": "The use of RAPs like IPatch could be detrimental to the performance of models used in autonomous vehicles and medical screening, by fooling them into misclassifying or misinterpreting images, which could lead to serious safety and health consequences.", "paper_1": ""}, "What are the applications that might be affected by adversarial patches according to the paper?": {"paper_0": "Applications such as autonomous vehicles and medical screening are mentioned as potentially affected by adversarial patches.", "paper_1": "The applications that might be affected by adversarial patches according to the paper could include autonomous vehicles and medical screening systems, as they both use deep learning models to localize and identify objects or relevant features."}, "How does the IPatch affect the model's perception of an image's semantics?": {"paper_0": "The IPatch affects the model's perception of an image's semantics by being able to change the classification or semantics of locations within the image that are remote from the actual location of the patch, thus interfering with the model's overall semantic interpretation.", "paper_1": "The IPatch affects the model's perception of an image's semantics by placing the patch anywhere within an image to change the classification or semantics of locations far from the patch, essentially fooling the model by altering how it interprets various parts of the scene."}}}