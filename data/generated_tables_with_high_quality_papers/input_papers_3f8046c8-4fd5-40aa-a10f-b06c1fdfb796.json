[{"paperid": "paper0", "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions", "abstract": "Style transfer TTS has shown impressive performance in recent years. However, style control is often restricted to systems built on expressive speech recordings with discrete style categories. In practical situations, users may be interested in transferring style by typing text descriptions of desired styles, without the reference speech in the target style. The text-guided content generation techniques have drawn wide attention recently. In this work, we explore the possibility of controllable style transfer with natural language descriptions. To this end, we propose PromptStyle, a text prompt-guided cross-speaker style transfer system. Specifically, PromptStyle consists of an improved VITS and a cross-modal style encoder. The cross-modal style encoder constructs a shared space of stylistic and semantic representation through a two-stage training process. Experiments show that PromptStyle can achieve proper style transfer with text prompts while maintaining relatively high stability and speaker similarity. Audio samples are available in our demo page.", "introduction": "\n\nText-to-speech (TTS) [1,2] aims to produce human-like speech from input text. Recent progress in deep learning approaches has greatly improved the naturalness of speech [3]. With the wide applications of TTS in real-world human-computer interaction, expressive TTS with diverse styles attracts more attention. Generating stylistic speech for a specific speaker intuitively needs the same speaker's high-quality expressive speech recordings, which incurs a high cost for data collection. To solve the problem of synthesizing expressive speech for the target speaker without diverse speaking styles, cross-speaker style transfer [4,5,6,7,8] is a feasible solution.\n\nFor the style representations in style transfer scenarios, existing works mainly include two different methods, i.e. the predefined style id category index [9,10,11,12] and hidden variables extracted from the reference signal [13,14,15,16,17]. However, the id-based methods are limited to the styles of fixed discrete categories, which leads to less flexibility. Although the reference-based methods can produce various speech through different references, the extracted style representation is not interpretable. Moreover, in practical applications, it is difficult to accurately and conveniently select an appropriate reference for arbitrary textual content.\n\nWith the success of text and image generation from prompt descriptions [18,19], some prompt-based TTS methods are pro-posed to improve the expressiveness and naturalness of synthetic speech. Style-Tagging-TTS (ST-TTS) [20] produces expressive speech based on style tags, which are stylistic words or phrases labeled from audiobook datasets. But it is difficult to describe complex styles by a single word or phrase, and the style tags are hard to label in common audiobook datasets as most utterances may be recorded in a less-expressive reading style. PromptTTS [21] proposes to use a style prompt from five different factors (i.e. gender, pitch, speaking speed, volume, and emotion) to guide the style expression for the generated speech. The recent InstructTTS [22] can synthesize stylistic speech with the guidance of natural language descriptions without formal constraints as style prompts. It's a three-stage training approach to capture semantic information from natural language style prompts as conditioning to the TTS system. Intuitively, natural language description is a convenient and userfriendly way to describe the desired style since no prior acoustic knowledge is required.\n\nIn this study, we focus on style transfer in the audiobook generation, where the target speaker has little expressive data and no style description prompts. Through other expressive data and transferring diversified styles with natural language descriptions, expressive audiobook speech can be generated for the target speaker. Specifically, this paper proposes to leverage natural language description prompts to transfer style from the source speaker to the target speaker who has no expressive speech data. The proposed approach -PromptStyle -has a two-stage procedure utilizing text prompts to model the style appearance. In the first stage, based on an improved VITS [23], we use a style encoder to extract a style hidden representation from reference speech as the condition of the TTS system. Multi-speaker multistyle expressive data without style annotation is involved in this stage to achieve cross-speaker style transfer through diverse reference speech. In the second stage, we design a prompt encoder to model the style embedding from the style prompt. Expressive speech data with style annotations in natural language descriptions is involved to fine-tune the pre-trained language model and TTS acoustic model, capturing the relationship between prompt embedding and style embedding space. Due to the generalization capability of the language model, style transfer from unseen prompts is also feasible.\n\nWe summarize the contributions of PromptStyle as follows.\n\n\u2022 We propose a two-stage TTS approach for cross-speaker style transfer with natural language descriptions, which is more user-friendly and controllable than previous works. \u2022 The proposed two-stage approach first uses a large amount of data without annotations to train a reference-based style transfer TTS model, and then leverages only a small amount of labeled data with style prompts to fine-tune a prompt en- \n\n\n"}, {"paperid": "paper1", "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt", "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g.,\"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.", "introduction": "\n\nT EXT-to-speech (TTS) aims to generate human-like speech from input text, which attracts broad interest in the audio and speech processing community. Nowadays, the state-of-the-art TTS systems [1]- [3] are able to produce natural and high-quality speech. However, there still exists a big gap between TTS-synthetic speech and human speech in terms of expressiveness, which limits the broad applications of current speech synthesis systems. Many researchers now focus on a more challenging task, i.e., expressive TTS, which aims to model and control the speaking style (e.g., emotion, speaking-rate and so on) in the generated speech according to human's demands. We note that there are generally two types of methods in the literature to learn speaking style information: Dongchao Yang and Helen Meng are with the Chinese University of Hong Kong. This work was done when Dongchao Yang was an intern at Tencent AI Lab. * denotes equal contribution with order determined by alphabetic order. Songxiang Liu and Chao Weng are with Tencent AI Lab. Rongjie Huang is with the Zhejiang University, China. Songxiang Liu is the corresponding author. 1 http://dongchaoyang.top/InstructTTS/ one type uses auxiliary categorical style labels as the condition of the framework [4], [5], the other imitates the speaking style of a reference speech [6]- [9]. However, there are limitations in the diversity of expressiveness when categorical style labels are used, as these models can only generate a few pre-defined styles from the training set. While TTS models that use a reference utterance to generate a particular speaking style can be trained in an unsupervised manner and are generalizable to out-of-domain speaking styles, the style information extracted from the reference speech may not be easily understandable or interpretable. Additionally, it can be challenging to select a reference speech sample that precisely matches a user's requirements.\n\nFor the first time, we study the modelling of expressive TTS with style prompt in natural language, where we meet with the following research problems: (1) how to train a language model that can capture semantic information from the natural language prompt and control the speaking style in the generated speech; (2) how to design an acoustic model to effectively model the challenging one-to-many learning problem of expressive TTS. In this paper, we will address these two challenges.\n\nThe main contributions of this study are summarized as follows:\n\n(1) For the first time, we study the modelling of expressive TTS with natural language prompts, which brings us a step closer to achieving user-controllable expressive TTS.\n\n(2) We introduce a novel three stage training strategy to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts.\n\n(3) We propose to model acoustic features in discrete latent space and cast speech synthesis as a sequence-to-sequence language modeling task. Specifically, we train a novel discrete diffusion model to generate vector-quantized (VQ) acoustic features rather than to predict the commonly-used hand-crafted intermediate acoustic features, such as the mel-spectrogram. (4) We explore to model two types of VQ acoustic features: mel-spectrogram based VQ features and waveform-based VQ features. We demonstrate that the two types of VQ features can be effectively modeled by our proposed novel discrete diffusion model. Our waveform-based modelling method only needs one-stage training, and it is a non-autoregressive model, which is far different from the concurrent work VALL-E [10] and MusicLM [11]. (5) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.  Where SALN denotes the style-adaptive layer normalization adaptor [12]. (b) shows the details of our proposed style encoder, which aims to extract style features from GT mel-spectrogram (training stage) or style prompt (inference stage). In Figure 1 (c), we give an example of discrete diffusion decoder to generate VQ mel-spectrogram acoustic features (we name it as Mel-VQ-Diffusion).\n\nThe rest of this paper is organized as follows: In Section II, we motivate our study by introducing the background and related work. In Section III, we present the details of the dataset. In Section IV, we introduce the details of our proposed methods. The experimental setting, evaluation metrics and results are presented from Section V to Section VII. The study is concluded in Section VIII.\n\n\n"}, {"paperid": "paper2", "title": "Prompttts: Controllable Text-To-Speech With Text Descriptions", "abstract": "Using a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., ''A lady whispers to her friend slowly''). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available.", "introduction": "\n\nRecent research has achieved great success in text and image generation guided with a text description as prompt [1,2,3,4] (e.g., GPT-3 [5] or DALLE-2 [6]). Beyond text and image generation, there is little research on prompt-based guidance for text-to-speech (TTS) synthesis [7,8] with different styles such as pitch, speaking speed, and emotion. Previous works on controllable TTS focus on controlling specific style factors: prosody control with word-level prosody tags [9], speaking speed control with sentence-level speaking-rate [10], and pitch control with pitch contours [11]. Style control can be achieved in an explicit manner by using the value of style factors such as pitch [12] or an implicit manner by learning a style token [13] from the reference speech. However, all previous works require users to provide the specific value of style factors with acoustic knowledge or choose the reference speech that meets the requirements, which are timeconsuming and not user-friendly. Therefore, it is a better choice if style control is achieved with a text description in natural language.\n\nIn this work, we explore the possibility of leveraging a text description (denoted as prompt) to guide speech synthesis.\n\nTo be more specific, as shown in Table 1, the input prompt consists of a style description (denoted as style prompt) and a content description containing the text to be converted to speech (denoted as content prompt) with a colon in between. For example, an input prompt, \"A lady whispers to her friend slowly: everything will go fine, right?\", means that the model needs to synthesize the speech with content of \"everything will go fine, right?\" in a female voice, a slow speaking speed, and a whispering manner. In this way, users are able to create speech from a prompt, resulting in style control without the requirements for acoustic knowledge or reference speech.\n\nAs the first exploration on TTS guided with prompts, there not exist datasets or systems for this task. Thus, we design a dataset, a system, and an evaluation metric for this task.\n\n(1) Dataset: we construct and release a dataset containing prompts with style and content information and the corresponding speech. The prompts describe the speech in 5 style factors including gender, pitch, speaking speed, volume, and emotion. (2) System: to synthesize speech according to a prompt, we propose PromptTTS to serve as baseline for future research in this task, which consists of a style encoder, a content encoder, and a speech decoder. The style and content encoders extract style and content representations from the prompt, respectively. The speech decoder utilizes both representations to synthesize speech accordingly. (3) Evaluation metric: we calculate the accuracy between the style factors from output speech and those from prompts as evaluation metric for style control.  The contributions of our work include: (1) we propose PromptTTS to synthesize the speech that is consistent with prompts in style and content, which is more user-friendly than previous works; (2) we collect and release a dataset consisting of prompts and the corresponding speech for this task; (3) the experiments show that PromptTTS can generate speech with precise style control and high speech quality.\n\n\n"}, {"paperid": "paper3", "title": "TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models", "abstract": "Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,220 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available at https://sall-e.github.io/", "introduction": "\n\nIn recent years, significant advancements have been made in the field of speech synthesis [1,2,3], with an increasing focus on a more challenging task known as controllable Text-to-Speech (TTS). Previous studies in controllable TTS have predominantly employed either reference audio for style transfer [4] or employed different style factors such as speaking rate [5], pitch [6], and prosody [7] for speech control. However, these approaches necessitate users to provide specific \u2020 Corresponding Author: Zhou Zhao, zhaozhou@zju.edu.cn values for style factors which requiring professional acoustic knowledge [5,6,7], or select reference speech [4] that satisfies the desired criteria. These methods are time-consuming and lack user-friendliness. Moreover, the style information derived from the reference speech lacks intuitiveness and interpretability. The effect over these styles is often constrained to the training set of the reference audio, resulting in weak generalization capabilities for unseen styles.\n\nBased on the aforementioned limitations: PromptTTS [8] proposes that it is a preferable choice to achieve style control using a natural language text description. We believe that utilizing natural text descriptions for controlling style in speech is the direction for future development of controllable TTS systems, due to its user-friendliness, generalizability, and interpretability. However, to the best of our knowledge, there is currently no high-quality, large-scale open-source text style prompt speech dataset available for advanced textcontrollable TTS models. In this work, we introduce a novel 330-hour clean text style prompt speech emotion dataset called TextrolSpeech. Each style encompasses 5 style factors and 500 distinct natural language text descriptions. Given the increased demands of controllable TTS systems for speech diversity, we get inspiration from [2] and propose Salle, which employs discrete tokens [9] based Residual Vector Quantization (RVQ) instead of conventional mel spectrograms. The tokens in Salle exhibit a hierarchical structure, where tokens from previous quantizers capture acoustic properties such as speaker identity, while consecutive quantizers learn finegrained acoustic details. Building upon this characteristic, we directly utilize the text style tokens in an autoregressive manner to prompt the generation of the first layer of acoustic tokens. Our contributions can be summarized as follows:\n\n\u2022 We have released TextrolSpeech, an open-source speech emotion dataset that is large-scale, multi-speaker, and enriched with diverse and natural text descriptions. This dataset aims to drive the development of text controllable TTS systems.\n\n\u2022 We provide a detailed account of the creation process of TextrolSpeech, through our experiments, we have devised an efficient prompt programming methodology \u2022 We propose Salle, a multi-stage discrete style tokenguided control framework for TTS language models, which exhibits powerful in-context capabilities.\n\n\n"}]