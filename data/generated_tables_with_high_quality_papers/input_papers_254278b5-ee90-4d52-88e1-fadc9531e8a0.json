[{"paperid": "paper0", "title": "Adversarial Patch Attacks on Monocular Depth Estimation Networks", "abstract": "Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.", "introduction": "\n\nEstimating pixel-wise depth from 2-D images has become increasingly important with the recent development of autonomous driving, augmented realities (AR), and robotics. A large body of previous work has been devoted to depth estimation from stereo or more than two images [1]- [4]. At the same time, monocular depth estimation [5]- [8], in which depth is estimated from a single image, 1 has attracted attention due to its less demanding hardware requirements. Monocular depth estimation has been greatly enhanced by the excellent learning capability of deep convolutional neural networks (CNN). As a result, current state-of-the-art results with monocular depth estimation are quite impressive, and seemingly comparable to those with stereo methods (see Fig. 1, where (a) is the input image and (b) is the depth estimated by Guo et al. [7]). However, monocular depth estimation is essentially an ill-posed problem because a monocular image alone does not contain sufficient physical cues\n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Syed Islam . 1 Generally, monocular depth estimation includes techniques that use a temporal sequence of images captured from a single camera [9]- [11]. However, in this article, we focus on methods that use only a single image from a single viewpoint for depth estimation.\n\nfor scene depth. Instead of using the physical cues, these methods seem to rely on implicit knowledge (e.g., the color, vertical position, or shadows) that are learned from the training dataset [12]. We argue that monocular depth estimation depends too much on non-depth features in the given image, which makes it quite vulnerable to attacks.\n\nTo reveal the limitation mentioned above, we propose a method of adversarial patch attack for CNN-based monocular depth estimation. Specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Figure 1(c) shows an example of our adversarial patches superimposed on the input image. As shown in (d), Guo et al.'s method [7] failed to estimate correct depth in the region where the patch was located; closer depth values were obtained than the original result in (b), as was intended with our design for this pattern. In this case, the attack was conducted in a digital manner; we digitally manipulated the pixel values of the input image to superimpose the patch. Our method can also be implemented in the real world, and we have achieved similar effects by physically placing the printed patterns in a real scene.\n\nMoreover, to further analyze the behavior of monocular depth estimation under attacks, we visualize the activation levels of the intermediate layers ( Fig. 1(e)) and the regions that are potentially affected by adversarial attacks (Fig. 1(f)). These visualizations lead to a deeper understanding of the mechanism by which adversarial patches affect the target CNN. Our source code, learned patches and demo video are available at https://www.fujii.nuee.nagoyau.ac.jp/Research/MonoDepth.\n\n\n"}, {"paperid": "paper1", "title": "Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches", "abstract": "Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.", "introduction": "\n\nMonocular Depth Estimation (MDE) is a technique for estimating the distance between an object and the camera from RGB image inputs. It is a critical vision task for autonomous driving (AD) because it bridges the gap between Lidar sensors and RGB cameras [57] and its measurement has an effect on a variety of downstream perception tasks (e.g., object detection [57], visual SLAM [60], and visual relocalization [50]). For its importance, Tesla has integrated MDE into its productiongrade Autopilot system [6,7], and other AD companies such as Toyota [23] and Huawei [9] are also actively investigating this technique. With the increasing popularity of MDE, ensuring its security becomes a prominent challenge.\n\nExisting adversarial attacks against MDE are implemented in digital- [72,61] or physical-world platforms [68]. Compared to digital-world attacks, attacks in the physical world are more challenging because they require robust perturbations to Fig. 1: Attack MDE and 3D object detection with a natural adversarial patch. The left is a benign scenario and the right is the corresponding adversarial scenario. 3D object detection takes the pseudo-Lidar (i.e., point cloud projected from 2D depth map) as input and outputs bounding boxes of recognized objects. Observe in the adversarial scenario (b) that our optimized adversarial patch can disturb the depth estimation of the target vehicle significantly and the effect propagates to an area larger than the patch itself. Pseudo-Lidar of the vehicle is thus distorted and it cannot be detected in the downstream task.\n\novercome various photometric and geometric changes [10], reducing their stealth. Prior efforts for physical-world adversarial attacks [68,49,26,11] generally employ an unnatural-looking adversarial patch and sacrifice stealth for attack effectiveness, leaving plenty of room for improvement. Additionally, with MDE's rapid development, many downstream tasks that previously require expensive Lidar sensors or depth cameras can now be performed entirely with MDE's measurement and achieve competitive performance. However, the investigation of the impact of compromised MDE on these downstream tasks remains largely unknown.\n\nTo address the aforementioned problems, in this paper, we investigate the stealth of physical-world attack against MDE and present a physical-objectoriented adversarial patch optimization framework to generate stealthy, effective and robust adversarial patches for target objects (e.g., vehicles and pedestrians). In particular, we are able to achieve the followings: we design a physical-objectoriented adversarial optimization, which binds the patch and the target object together regarding attack effects and physical-world transformations ( \u00a73.2); we optimize the patch region on the target object with a differentiable patch mask representation, which automatically locates the highly effective area for attack on the target object and improves attack performance with a small patch size ( \u00a73.3); we camouflage the adversarial pattern with natural styles (e.g., rusty and dirty) with deep photo style transfer [31], resulting in stealthier patch for the attack ( \u00a73.4); we investigate the impact of compromised MDE on a representative downstream task in AD -3D object detection ( \u00a74.4). Fig. 1 gives an example. In addition, we examine our attack with popular defence techniques ( \u00a74.5). Our key contributions are:\n\n1. We develop a physical-object-oriented adversarial patch attack against MDE that balances stealth and effectiveness. To the best of our knowledge, we are the first to investigate stealthy physical-world attacks against MDE considering both the patch size and naturalness. 2. We propose an optimization framework that considers physical object characteristics, has a differentiable patch region representation, and provides natural style based camouflage.\n\n3. We evaluate our attack on 3 representative MDE models and a downstream task with real-world driving scenarios in both digital and physical worlds. Our attack is effective on different types of target objects and state-of-theart models. It causes over 6 meters of mean depth estimation error for a real vehicle, with a patch only 1/9 of the vehicle's rear area, and achieves more than 90% attack success rate in 3D object detection. A video is available at https://youtu.be/L-SyoAsAM0Y.\n\n\n"}, {"paperid": "paper2", "title": "APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation", "abstract": "In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to erroneous depth predictions only within the overlapping region with the input image, without considering the characteristics of the target object, such as its size, shape, and position. In this paper, we introduce a novel adversarial patch named APARATE. This patch possesses the ability to selectively undermine MDE in two distinct ways: by distorting the estimated distances or by creating the illusion of an object disappearing from the perspective of the autonomous system. Notably, APARATE is designed to be sensitive to the shape and scale of the target object, and its influence extends beyond immediate proximity. APARATE, results in a mean depth estimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of the targeted region when applied to CNN-based MDE models. Furthermore, it yields a significant error of $0.34$ and exerts substantial influence over $94\\%$ of the target region in the context of Transformer-based MDE.", "introduction": "\n\nMDE has found increasing utility across various practical applications such as robotics and autonomous driving (AD).MDE involves deriving depth insights from a single image, thereby enhancing scene comprehension.Its significance extends to several critical robotic functions, including obstacle avoidance [51], object detection [43], visual SLAM [37,46], and visual relocalization [42].\n\nSeveral methodologies for depth estimation rely on technologies like RGB-D cameras, Radar, LiDAR, or ultrasound devices to directly capture depth information within a scene.\n\nHowever, these alternatives exhibit notable shortcomings.RGB-D cameras possess a limited measurement range, LiDAR and Radar deliver sparse data, and both are costly sensing solutions that might not be viable for compact autonomous systems like low-cost, lightweight and small-sized mobile robots.Ultrasound devices, on the other hand, are marred by inherent measurement inaccuracies.\n\nMoreover, these technologies demand substantial energy consumption and feature large form factors, rendering them unsuitable for resource-restricted, small-scale systems that must adhere to stringent real-world design constraints.In contrast, RGB cameras stand out as lightweight and cost-effective options.Importantly, they have the capacity to furnish more comprehensive environmental data.Prominent players within the autonomous vehicle sector are actively pushing the envelope of self-driving technology by harnessing cost-effective camera solutions.\n\nNotably, MDE has been seamlessly integrated into Tesla's production-grade Autopilot system [1,2].Evidently, other major autonomous driving (AD) enterprises, including Toyota [19] and Huawei [3], are following Tesla's footsteps to propel self-driving advancements through this approach.\n\nIn recent years, the advancement of deep learning arXiv:2303.01351v2[cs.CV] 20 Nov 2023\n\nYamanaka et al.\n\n\n"}, {"paperid": "paper3", "title": "SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation", "abstract": "In this paper, we investigate the vulnerability of MDE to adversarial patches. We propose a novel \\underline{S}tealthy \\underline{A}dversarial \\underline{A}ttacks on \\underline{M}DE (SAAM) that compromises MDE by either corrupting the estimated distance or causing an object to seamlessly blend into its surroundings. Our experiments, demonstrate that the designed stealthy patch successfully causes a DNN-based MDE to misestimate the depth of objects. In fact, our proposed adversarial patch achieves a significant 60\\% depth error with 99\\% ratio of the affected region. Importantly, despite its adversarial nature, the patch maintains a naturalistic appearance, making it inconspicuous to human observers. We believe that this work sheds light on the threat of adversarial attacks in the context of MDE on edge devices. We hope it raises awareness within the community about the potential real-life harm of such attacks and encourages further research into developing more robust and adaptive defense mechanisms.", "introduction": "\n\nM ONOCULAR depth estimation (MDE) is increasingly being utilized in a wide array of real-world applications, ranging from autonomous driving to robotics. Its main purpose is to acquire depth data, enabling a deeper and more comprehensive comprehension of the surrounding scene. MDE plays a critical and indispensable role in various tasks, including obstacle avoidance [1], object detection [2], visual SLAM [3], [4], visual re-localization [5], and numerous others.\n\nSeveral methods for estimating depth rely on sensors like RGB-D cameras, radar, lidar, or ultrasound to collect the depth information directly from a scene. These latter, however, have serious shortcomings. In fact, ultrasound devices suffer from inherently imprecise measurements, LIDAR and radar produce sparse information, and RGB-D cameras have a narrow measuring range. The aforementioned devices are extremely large and power hungry for small-sized systems, especially those that must adhere to rigorous real-world design constraints. Contrarily, RGB cameras are lightweight and less expensive. More importantly, they can offer more detailed environmental information. Several  tasks can now be accomplished totally using MDE's measurement and attain competitive performance thanks to MDE's rapid developments. These remarkable improvements can be attributed to the successful integration of deep neural networks, which have significantly enhanced MDE's capabilities.\n\nHowever, the increasing reliance on deep neural networks also brings attention to their vulnerability to adversarial attacks. As shown in various studies, these networks can be susceptible to manipulations that intentionally deceive their predictions. Hence, it becomes crucial to prioritize the security of MDE models to ensure their reliability and trustworthiness in practical applications. Safeguarding MDE systems against adversarial attacks is vital for preserving their integrity and preventing potential misinterpretations and misjudgments in real-world scenarios.\n\nPatch-based adversarial attacks [6]- [8] are a type of adversarial attack in computer vision, where carefully crafted perturbations are applied to specific patches or regions of an input image to deceive a deep learning model. The goal of patch-based adversarial attacks is to cause the model to misclassify the entire image or produce incorrect predictions for targeted regions. In contrast to traditional global adversarial attacks that perturb the entire input image, patch-based attacks are more localized, focusing on specific regions of interest.\n\nPatch-based adversarial attacks have implications in various applications, including object detection, image segmentation, and scene understanding. They demonstrate the vulnerability of deep learning models to localized adversarial perturbations and highlight the importance of developing robust defense mechanisms to protect against such attacks.\n\nOnly a limited number of studies have explored the realm of patch-based adversarial attacks on depth estimation. This particular direction of research remains relatively unexplored compared to other adversarial attack methods in computer vision. Previous work for patch-based adversarial attacks on MDE [7], [9], [10] aiming at tricking the perception module of an autonomous vehicle. Their effectiveness is limited, as they only mislead the depth prediction of specific parts within the overlapping region between the input image and the patch, usually utilizing conspicuous and eye-catching patterns. In this work, we investigate stealthy adversarial patches that can either fully conceal a particular object or trick the target methods into estimating the depth of that object incorrectly at a target depth.\n\nThis paper introduces a technique for deceiving a CNN-based monocular depth estimation system by leveraging naturalistic adversarial patches (See Figure  3). These patches are strategically designed to manipulate the system's predictions, resulting in the generation of false distance estimates. The proposed approach allows the adversarial patch to seamlessly blend into its surroundings while for example resembling a painting on a wall or a poster, and can be applied to conceal specific objects or areas of interest effectively. In fact, we set out to achieve two key goals: depth manipulation and object concealment (i.e., object-background blending). Through our proposed techniques, we can intentionally alter the perceived depth of specific objects in a scene, leading to inaccurate depth estimations. Additionally, we have developed a method to completely conceal certain objects from the depth estimation process, making them effectively invisible to the system. Moreover, our approach enables selected objects to seamlessly blend with the background, creating a visual effect where they appear to be part of the scenery, thus reducing their conspicuousness and detection. An overview of our novel contributions is shown in Figure 1. In summary, the contributions of this work are:\n\n\u2022 We present a novel patch-based adversarial attack that targets DNN-based monocular depth estimation. \u2022 Our framework generates a stealthy adversarial patch (SAAM) that can seamlessly blend into its surroundings (e.g., resembling a painting on a wall or a poster). \u2022 Our patch (SAAM) has the ability to withstand diverse transformations and adapt to different scenarios. It demonstrated robustness against a range of deformations, including rotation, perspective change, and lighting variation. Additionally, the patch can be placed at arbitrary locations within the scene, even under occlusion. \u2022 Our proposed adversarial patch extends its applicability to multiple use cases such as navigation tasks, obstacle Ratio of affected region detection, localization, etc. With a patch size as small as 0.7% of the input image, we achieve an impressive 60% depth estimation error. Moreover, the adversarial patch nearly covers the entire target region, with an almost 100% ratio of the affected region. These findings highlight the effectiveness and potency of our attack in disrupting the depth estimation process.\n\nThe structure of the remaining article is organized as follows. Section II provides a comprehensive overview of related work in the field of patch-based adversarial attacks on monocular depth estimation. In Section III, we present our proposed methodology to generate the adversarial patch. This section outlines the step-by-step process of crafting the patch and explains the techniques employed to achieve effective depth manipulation and visual realism. Section IV details the experimental setup used to evaluate the performance of the proposed adversarial attack. In Section V, we delve into the evaluation of the proposed attack. We present the results obtained from various metrics, such as depth error, affected region ratio, and SSIM, to assess the attack's potency and visual similarity of the generated patch. In Section VI, we thoroughly discuss the findings and implications of our experiments. We analyze the strengths and limitations of the proposed attack and interpret the results in the context of real-world applications. Section VII provides a succinct summary and conclusion of our study.\n\n\n"}]