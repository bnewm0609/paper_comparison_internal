[{"paperid": "paper0", "title": "The Konstanz natural video database (KoNViD-1k)", "abstract": "Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. Currently, all existing VQA databases include only a small number of video sequences with artificial distortions. The development and evaluation of objective quality assessment methods would benefit from having larger datasets of real-world video sequences with corresponding subjective mean opinion scores (MOS), in particular for deep learning purposes. In addition, the training and validation of any VQA method intended to be \u2018general purpose\u2019 requires a large dataset of video sequences that are representative of the whole spectrum of available video content and all types of distortions. We report our work on KoNViD-1k, a subjectively annotated VQA database consisting of 1,200 public-domain video sequences, fairly sampled from a large public video dataset, YFCC100m. We present the challenges and choices we have made in creating such a database aimed at \u2018in the wild\u2019 authentic distortions, depicting a wide variety of content.", "introduction": "\n\nMost of the Internet traffic today stems from user-generated videos on sharing web-sites and social networks. Video sequences pass through several stages of processing before they reach consumers, which often deteriorate visual quality. Moreover, the vast amount of user-generated video content and the increased diversity of end user devices (ranging from smaller and power-constrained mobile devices to large displays such as 4K Ultra HDTVs and TV walls) calls for a broad range of video quality to be supported. Adapting video quality to different use cases has become an important topic for researchers, content providers and distributors [1].\n\nAutomatic and accurate prediction of video quality is a basic operation for many video processing applications such as video quality monitoring in transmission protocols, video quality filtering in sharing services, automatic and recommended camera parameter settings during video capturing, and video enhancement. Specifically, no-reference methods attempt to judge the quality of a video sequence without any additional information about the original recorded scene. Such blind methods may apply machine learning techniques to learn from large amounts of annotated data. However, current video quality assessment (VQA) databases contain only a small number of video sequences with little content diversity, thus offering limited support for designing and evaluating noreference VQA methods effectively and fairly.\n\nAdditionally, these databases were mostly designed to include only artificially distorted video sequences to simulate quality loss in compression, transmission, and other parts of the video processing and distribution pipeline. Some databases capture imagery with a variety of cameras to encompass authentic video acquisition distortions, however, with content restricted to a small number of physical scenes.\n\nWinkler [2] proposed several criteria for quantitative comparisons of source content, test conditions, and subjective ratings, applying them to 27 image and video databases. Most collections have not been found satisfactory in terms of content range and uniformity. Only few databases showed good uniformity for test conditions (image/video quality), but not over the whole quality range. Also the distortion variety was found lacking in most databases covering mainly compression and transmission, but not the many other types of natural distortions found \"in the wild\" [3].\n\nTo overcome these limitations we introduce KoNViD-1k, a large publicly available database of video sequences based on YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset [4] with a diverse set of video content. In this paper we report the filtering mechanisms and sampling procedures necessary to construct high-quality VQA databases of this kind, focusing on their usefulness in a variety of applications.\n\nIn the next section, we describe the database creation procedure and the set of attributes we have considered to maximise its diversity. Additional information regarding the removal of non-natural video sequences and sampling techniques are provided as well. Next, in Sec. III, we review our crowdsourcing-based process of collecting subjective mean opinion scores (MOS) and detail our results as well as crowd worker statistics. In Sec. IV we relate our database characteristics and creation methodology with other existing works and outline the differences, before discussing conclusions of our work and considering possible future work.\n\n\n"}, {"paperid": "paper1", "title": "Large-Scale Study of Perceptual Video Quality", "abstract": "The great variations of videographic skills, camera designs, compression and processing protocols, and displays lead to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores, resulting in an average of 240 recorded human opinions per video. We demonstrate the value of the new resource, which we call the LIVE Video Quality Challenge Database (LIVE-VQC), by conducting a comparison of leading NR video quality predictors on it. This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores. The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html .", "introduction": "\n\nThe goal of Video quality assesement (VQA) research is to develop video quality models that produce predictions that are in close agreement with human judgments, regardless of the video contents or the type and severities of the distortions (or the mixtures of distortions) that have corrupted the videos. Over the past decade, we have experienced a surge in the number of videos recorded, shared and watched. Sharing \"inthe-moment\" experiences in the form of video has become quite popular using applications such as Instagram, Facebook, Twitter via Periscope, Snapchat, and so on. Online videos have also revolutionized modern journalism as they enable online news stories to unfold live, and allow the viewing audience to comment on or otherwise interact with it. Over the past year, Facebook alone generated more than 100 million of Zeina Sinno and Alan C. Bovik are with the Department of Electrical and Computer Engineering at The University of Texas at Austin, Austin, TX, 78712, USA (e-mails: zeina@utexas.edu -bovik@ece.utexas.edu). video watch hours each day [1]. On YouTube, the overall durations of the videos that are uploaded daily exceeds 65 years, and more than 1 billion hours of their video content is watched each day [2]. These numbers are continuing to rise and to reshape digital marketing, entertainment, journalism, and amateur videography. The volume of streaming video viewed online has become so large that more than 130 million people are now Netflix subscribers [3]. Streaming videos now comprises the majority of Internet traffic today. It is no surprise that videos account for the largest portion of Internet traffic, which is expected to eclipse 82% of all transmitted bits by 2021 [4]. These videos are captured using a very wide variety of camera devices by users having very diverse goals and expertise.\n\n\n"}, {"paperid": "paper2", "title": "YouTube UGC Dataset for Video Compression Research", "abstract": "Non-professional video, commonly known as User Generated Content (UGC) has become very popular in today's video sharing applications. However, traditional metrics used in compression and quality assessment, like BD-Rate and PSNR, are designed for pristine originals. Thus, their accuracy drops significantly when being applied on non-pristine originals (the majority of UGC). Understanding difficulties for compression and quality assessment in the scenario of UGC is important, but there are few public UGC datasets available for research. This paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel sampling method based on features extracted from encoding, challenges for UGC compression and quality evaluation are also discussed. Shortcomings of traditional reference-based metrics on UGC are addressed. We demonstrate a promising way to evaluate UGC quality by no-reference objective quality metrics, and evaluate the current dataset with three no-reference metrics (Noise, Banding, and SLEEQ).", "introduction": "\n\nVideo makes up the majority of todays Internet traffic. Consequently, this motivates video service companies (e.g. YouTube) to spend substantial effort to control bandwidth usage [1]. The main remedy deployed is typically video bitrate reduction. However, aggressive bitrate reduction may hurt perceptual visual quality at the same time as both creators and viewers have increasing expectations for streaming media quality. The evolution of new codec technology (HEVC, VP9, AV1) continues to address this bitrate/quality tradeoff.\n\nTo measure the quality degradation, numerous quality metrics have been proposed in the last few decades. Some reference-based metrics (like PSNR, SSIM [2] and VMAF [3]) have been widely used in the industry. A common assumption held by most video quality and compression research is that the original video is perfect, and any operation on the original (processing, compression etc.) makes it worse. Most research measures how good the resulting video is by comparing it to the original. However, such an assumption does not hold for most of User Generated Content (UGC) due to the following reasons:\n\n\u2022 Non-pristine original When there are visual artifacts present in the original, it is not clear whether an encoder should be putting in efforts to accurately represent those artifacts. It is necessary to consider the effect that the encoding has on those undesired artifacts, but it is also necessary to consider the effect that the artifacts have on the ability to encode the video effectively. \u2022 Mismatched absolute and reference quality Using the original as a reference does not always make sense when the original isnt perfect. Quality improvement may be affected by pre/post processing before/after transcoding, but reference-based metrics (e.g. PSNR, SSIM) cannot fairly evaluate the impact of these tools in a compression chain. We created this large scale UGC dataset in order to encourage and facilitate research that considers the practical and realistic needs of video compression and quality assessment in video processing infrastructure.\n\nA major contribution of this work is the analysis of the enormous content in YouTube in a way that illustrates the breadth of visual quality in media worldwide. That analysis leads to the creation of a statistically representative set that is more amenable to academic research and computational resources. We built a large scale UGC dataset(Section III), and propose (Section IV) a novel sampling scheme based on features extracted from encoding logs, which achieves high coverage over millions of YouTube videos. Shortcomings of traditional reference-based metrics on UGC are discussed (Section V), and we evaluate the dataset with three noreference metrics: Noise [4], Banding [5], and Self-reference based LEarning-free Evaluator of Quality (SLEEQ) [6].\n\nThe dataset can be previewed and downloaded from https://media.withyoutube.com/ugc-dataset.\n\n\n"}, {"paperid": "paper3", "title": "Patch-VQ: \u2018Patching Up\u2019 the Video Quality Problem", "abstract": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world,\"in-the-wild\"UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.", "introduction": "\n\nUser-generated content (UGC) and video streaming has exploded on social media platforms such as Facebook, Instagram, YouTube, and TikTok, each supporting millions and billions of users [1]. It has been estimated that each day, about 4 billion video views occur on Facebook [2] and 1 billion hours are viewed on YouTube [3]. Given the tremendous prevalence of Internet video, it would be of great value to measure and control the quality of UGC videos, both in capture devices and at social media sites where they are uploaded, encoded, processed, and analyzed.\n\nFull-reference (FR) video quality assessment (VQA) models perceptually compare quality against pristine videos, while no-reference (NR) models involve no such comparison. Thus, NR video quality monitoring could transform the processing and interpretation of videos on * \u2020 Equal contribution \u2021 The entity that conducted all of the data collection/experimentation. smartphones, social media, telemedicine, surveillance, and vision-guided robotics, in ways that FR models are unable to. Unfortunately, measuring video quality without a pristine reference is very hard. Hence, though FR models are successfully deployed at the largest scales [4], NR video quality prediction on UGC content remains largely unsolved, for several reasons. First, UGC video distortions arise from highly diverse capture conditions, unsteady hands of content creators, imperfect camera devices, processing and editing artifacts, frame rates, compression and transmission artifacts, and the way they are perceived by viewers. Inter-mixing of distortions is common, creating complex, composite distortions that are harder to model in videos. Moreover, it is wellknown that the technical degree of distortion (e.g. amount of blur, blocking, or noise) does not correlate well with perceptual quality [5], because of neurophysiological processes that induce masking [6]. Indeed, equal amounts of distortions may very differently affect the quality of two different videos [7].\n\nSecond, most existing video quality resources are too small and unrepresentative of the complex real-world distortions [8,9,10,11,12,13,14]. While three publicly avail-able databases of authentically distorted UGC videos are available [15,16,17], they are far too small to train modern, data-hungry deep neural networks. What is needed are very large databases of videos corrupted by real-world distortions, subjectively rated by large numbers of human viewers. However, conducting large-scale psychometric studies is much harder and time-consuming (per video) than standard object or action classification tasks.\n\nFinally, although a few NR algorithms achieve reasonable performance on small databases [18,19,20,21,22,23,24], most of them fail to account for the complex spacetime distortions common to UGC videos. UGC distortions are often transient (e.g., frame drops, focus changes, and transmission glitches) and yet may significantly impact the overall perceived quality of a video [25]. Most existing models are frame-based, or use sample frame differences, and cannot capture diverse temporal impairments.\n\nWe have made recent progress towards addressing these challenges, by learning to model the relationships that exist between local and global spatio-temporal distortions and perceptual quality. We built a large-scale public UGC video dataset of unprecedented size, comprising full videos and three kinds of spatio-temporal video patches (Fig. 1), and we conducted an online visual psychometric study to gather large numbers of human subjective quality scores on them. This unique data collection allowed us to successfully learn to exploit interactions between local and global video quality perception and to create algorithms that accurately predict video quality and space-time quality maps. We summarize our contributions below:\n\n\u2022 We built the largest video quality database in existence. We sampled hundreds of thousands of open source Internet UGC digital videos to match the feature distributions of social media UGC videos. Our final collection includes 39, 000 real-world videos of diverse sizes, contents, and distortions, 26 times larger than the most recent UGC dataset [17]. We also extracted three types of v-patches from each video, yielding 117, 000 space-time video patches (\"v-patches\") in total (Sec. 3.1). \u2022 We conducted the largest subjective video quality study to date. Our final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6, 300 subjects, more than 9 times larger than any prior UGC video quality study (Sec. 3.2). \u2022 We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D video features using PaQ2PiQ [29], in parallel with 3D features using ResNet3D [30]. The 2D and 3D features feed a time series regressor [31] that learns to accurately predict both global video, as well as local spacetime v-patch quality, by exploiting the relations between them. This new model, which we call Patch VQ (PVQ) achieves top performance on the new database as well as on smaller \"in-the-wild\" databases [16,15], without finetuning (Secs. 4.1 and 5.3). \u2022 We also create another unique prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships. This second model, called the PVQ Mapper, helps localize, visualize, and act on video distortions (Sec. 5.2).\n\n\n"}, {"paperid": "paper4", "title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild", "abstract": "Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.", "introduction": "\n\nVideos have become a central medium for business marketing [1], with over 81% of businesses using video as a marketing tool. Additionally, over 40% of businesses have adopted live video formats such as Facebook Live for marketing and user connection purposes [2]. For consumers, video is the primary source of media entertainment; for example the average US consumer spends 38 hours per week watching video content [3] and it is projected that online videos will make up more than 82% of all consumer internet traffic by 2022 [4]. Streaming platforms such as YouTube report that more than a billion hours of video are watched every day [5]. The success of online videos is due in part to the consumer belief that traditional TV offers an inferior quality [3]. Additionally, increased accessibility to video content The associate editor coordinating the review of this manuscript and approving it for publication was Zhang Lu. acquisition hardware, as well as improvements in overall image quality, are a central aspect in smartphone technology advancement. Similarly, user-generated content is produced at an increasing rate, but the resulting videos often suffer from quality defects.\n\nTherefore, a wide range of video producers and consumers should be able to get automated feedback on video quality. For example, user-generated video distribution platforms like YouTube or Vimeo may want to analyze new videos according to quality to separate professional from the amateur video content, instead of only indexing by video playback resolution. Additionally, with an automated video quality assessment (VQA) system, video streaming services can adjust video encoding parameters to minimize bandwidth requirements while ensuring the delivery of satisfactory video quality.\n\nA critical emerging challenge for VQA is to handle ecologically valid in-the-wild videos. In environmental psychology, VOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ ecological validity is defined as ''the applicability of the results of laboratory analogues to non-laboratory, real life settings'' [6]. In our case the term can be understood as a measure for the extent to which the data represented in a dataset can be generalized to data that would be naturally encountered in the use of a technology. Concretely, this would refer to the types and degree of distortions in visual media contents of internet videos, such as those consumed on YouTube, Flickr, or Vimeo. The term in-the-wild refers to datasets that are ''not constructed and designed with research questions in mind'' [7]. In the case of VQA this would mean datasets that are not recorded or altered with a specific research purpose in mind, such as artificially distorting videos at variable degrees. It comes as no surprise that no-reference VQA (NR-VQA), in particular, has been a field of intensive research in the past few years achieving significant performance gains [8]- [19]. However, state-of-the-art NR-VQA algorithms perform worse on in-the-wild videos than on synthetically distorted ones. These methods aggregate individual video frame quality characteristics that are engineered for specific purposes, such as detecting particular compression artifacts. Often, these features are a balance between precision and computational efficiency. Furthermore, since there is a lack of large-scale in-the-wild video quality datasets with authentic distortions, a thorough evaluation of NR-VQA methods is difficult. Most existing databases are intended as benchmarks for the detection of those specific artificial distortions that NR-VQA algorithms have classically been designed to detect.\n\nGiven the previous challenges, our first contribution is the creation of a large ecologically valid dataset, KonVid-150k. Similar to the dataset KoNViD-1k [20], the ecological validity of KonVid-150k stems from its size, content diversity, as well as naturally occurring, and thus representative degradations. However, being two orders of magnitude larger than existing datasets, it poses new challenges to VQA methods, requiring to train across a vast amount of content and a wide span of authentic distortions. Moreover, since a fixed budget usually constrains the development of a dataset, we needed to ensure a minimum level of annotation quality. Therefore, a part of KonVid-150k consists of 153,841 five seconds long videos that are annotated by five subjective opinions each. This set, from here on called KonVid-150k-A, is over 125 times larger than existing VQA datasets in terms of number of videos and with close to one million subjective ratings over eight times larger in number of annotations [20]- [23]. The dataset is accompanied by a benchmark set of nearly 1,600 videos (KonVid-150k-B) from the same source with a minimum of 89 opinion scores each. This presents a unique opportunity to analyze the trade-off between the number of training videos and the annotation noise/precision, in terms of the performance on the KonVid-150k-B benchmark dataset.\n\nThis new dataset exacerbates two problems of classical NR-VQA methods. First, the computational costs of hand-crafted feature-based approaches are increased through the sheer number of videos. Second, since hand-crafted features handle in-the-wild videos worse than conventional databases, this dataset is very challenging for classical NR-VQA methods. An alternative to hand-crafted features comes with the rise of deep convolutional neural networks (DCNNs), where stacked layers of increasingly complex feature detectors are learned directly from observations of input images. These features are often relatively generic and have been proven to transfer well to similar tasks that are not too different from the source domain [24], [25]. This suggests considering a DCNN as a feature extractor with a benefit over hand-crafted features in that the features are entirely learned from data.\n\nAs a second contribution, we propose to use a new way of extracting video features by aggregating activations of all layers of DCNNs, pre-trained for classification. We adopt a strategy similar to Hosu et al. [26] and extract narrow multi-level spatially pooled (MLSP) features of video frames from an InceptionResNet-v2 [27] architecture to learn VQA. By global average pooling the outputs of inception module activation blocks, we obtain fixed sized feature representations of the frames. We showcase the scalability of this approach by comparing it to the baseline of freezing the weights of the feature extraction network and training a new head, which is a technique that is commonly used in transfer learning.\n\nThe third contribution of this paper consists of two network variants trained on the frame feature vectors that surpass state-of-the-art NR-VQA methods on in-the-wild datasets and train at a rate that is able to scale to hundreds of thousands of videos. In a short ablation study we investigate the impact of architectural and hyperparameter choices of both models. Both approaches are then evaluated on existing VQA datasets consisting of authentic videos as well as those containing artificially degraded videos and show that on in-the-wild videos the proposed method outperforms classical methods based on hand-crafted features. In particular, training and testing on KoNViD-1k improves the state-of-the-art 0.80 to 0.82 SRCC. Finally, we show that training our proposed model on the new dataset we achieve a 0.83 SRCC when cross-testing on KoNViD-1k. This outperforms state-of-the-art intra-dataset test scenarios, where training and testing is performed on the same dataset. It is surprising, as intra-dataset tests have the benefit of not being affected by any domain shift [28].\n\nIn summary, our main contributions are:\n\n\u2022 KonVid-150k, an ecologically valid in-the-wild video quality assessment database, two orders of magnitude larger than existing ones.\n\n\u2022 The successful application of deep multi-layer spatially pooled features for video quality assessment, which allows training of state-of-the-art models at scale on conventional hardware.\n\n\u2022 Three deep neural network models (MLSP-VQA-FF, -RN, and -HYB). They surpass the intra-dataset stateof-the-art performance on KoNViD-1k with 0.82 SRCC versus the best existing 0.80 SRCC, and show excellent generalization in inter-dataset tests when trained on KonVid-150k, surpassing even the intra-dataset tests with 0.83 SRCC.\n\n\n"}, {"paperid": "paper5", "title": "Large-Scale Video Classification with Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "introduction": "\n\nImages and videos have become ubiquitous on the internet, which has encouraged the development of algorithms that can analyze their semantic content for various applications, including search and summarization. Recently, Convolutional Neural Networks (CNNs) [15] have been demonstrated as an effective class of models for understanding image content, giving state-of-the-art results on image recognition, segmentation, detection and retrieval [11,3,2,20,9,18]. The key enabling factors behind these results were techniques for scaling up the networks to tens of millions of parameters and massive labeled datasets that can support the learning process. Under these conditions, CNNs have been shown to learn powerful and interpretable image features [28]. Encouraged by positive results in domain of images, we study the performance of CNNs in large-scale video classification, where the networks have access to not only the appearance information present in single, static images, but also their complex temporal evolution. There are several challenges to extending and applying CNNs in this setting.\n\nFrom a practical standpoint, there are currently no video classification benchmarks that match the scale and variety of existing image datasets because videos are significantly more difficult to collect, annotate and store. To obtain sufficient amount of data needed to train our CNN architectures, we collected a new Sports-1M dataset, which consists of 1 million YouTube videos belonging to a taxonomy of 487 classes of sports. We make Sports-1M available to the research community to support future work in this area.\n\nFrom a modeling perspective, we are interested in answering the following questions: what temporal connectivity pattern in a CNN architecture is best at taking advantage of local motion information present in the video? How does the additional motion information influence the predictions of a CNN and how much does it improve performance overall? We examine these questions empirically by evaluating multiple CNN architectures that each take a different approach to combining information across the time domain.\n\nFrom a computational perspective, CNNs require extensively long periods of training time to effectively optimize the millions of parameters that parametrize the model. This difficulty is further compounded when extending the connectivity of the architecture in time because the network must process not just one image but several frames of video at a time. To mitigate this issue, we show that an effective approach to speeding up the runtime performance of CNNs is to modify the architecture to contain two separate streams of processing: a context stream that learns features on low-resolution frames and a high-resolution fovea stream that only operates on the middle portion of the frame. We observe a 2-4x increase in runtime performance of the network due to the reduced dimensionality of the input, while retaining the classification accuracy.\n\nFinally, a natural question that arises is whether features learned on the Sports-1M dataset are generic enough to generalize to a different, smaller dataset. We investigate the transfer learning problem empirically, achieving significantly better performance (65.4%, up from 41.3%) on UCF-101 by re-purposing low-level features learned on the Sports-1M dataset than by training the entire network on UCF-101 alone. Furthermore, since only some classes in UCF-101 are related to sports, we can quantify the relative improvements of the transfer learning in both settings.\n\nOur contributions can be summarized as follows:\n\n\u2022 We provide extensive experimental evaluation of multiple approaches for extending CNNs into video classification on a large-scale dataset of 1 million videos with 487 categories (which we release as Sports-1M dataset) and report significant gains in performance over strong feature-based baselines.\n\n\u2022 We highlight an architecture that processes input at two spatial resolutions -a low-resolution context stream and a high-resolution fovea stream -as a promising way of improving the runtime performance of CNNs at no cost in accuracy.\n\n\u2022 We apply our networks to the UCF-101 dataset and report significant improvement over feature-based stateof-the-art results and baselines established by training networks on UCF-101 alone.\n\n\n"}, {"paperid": "paper6", "title": "The Kinetics Human Action Video Dataset", "abstract": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.", "introduction": "\n\nIn this paper we introduce a new, large, video dataset for human action classification. We developed this dataset principally because there is a lack of such datasets for human action classification, and we believe that having one will facilitate research in this area -both because the dataset is large enough to train deep networks from scratch, and also because the dataset is challenging enough to act as a performance benchmark where the advantages of different architectures can be teased apart.\n\nOur aim is to provide a large scale high quality dataset, covering a diverse range of human actions, that can be used for human action classification, rather than temporal localization. Since the use case is classification, only short clips of around 10s containing the action are included, and there are no untrimmed videos. However, the clips also contain sound so the dataset can potentially be used for many purposes, including multi-modal analysis. Our inspiration in providing a dataset for classification is ImageNet [18], where the significant benefits of first training deep networks on this dataset for classification, and then using the trained network for other purposes (detection, image segmentation, non-visual modalities (e.g. sound, depth), etc) are well known.\n\nThe Kinetics dataset can be seen as the successor to the two human action video datasets that have emerged as the standard benchmarks for this area: HMDB-51 [15] and UCF-101 [20]. These datasets have served the community very well, but their usefulness is now expiring. This is because they are simply not large enough or have sufficient variation to train and test the current generation of human action classification models based on deep learning. Coincidentally, one of the motivations for introducing the HMDB dataset was that the then current generation of action datasets was too small. The increase then was from 10 to 51 classes, and we in turn increase this to 400 classes. Table 1 compares the size of Kinetics to a number of recent human action datasets. In terms of variation, although the UCF-101 dataset contains 101 actions with 100+ clips for each action, all the clips are taken from only 2.5k distinct videos. For example there are 7 clips from one video of the same person brushing their hair. This means that there is far less variation than if the action in each clip was performed by a different person (and different viewpoint, lighting, etc). This problem is avoided in Kinetics as each clip is taken from a different video.\n\nThe clips are sourced from YouTube videos. Consequently, for the most part, they are not professionally videoed and edited material (as in TV and film videos). There can be considerable camera motion/shake, illumination variations, shadows, background clutter, etc. More im-\n\n\n"}]