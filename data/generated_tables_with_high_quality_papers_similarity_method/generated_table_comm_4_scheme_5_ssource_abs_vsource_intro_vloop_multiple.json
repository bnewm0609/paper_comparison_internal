{"experiment_type": {"scheme_source": "abs", "commonality_num": 4, "attribute_num": 5, "value_source": "intro", "value_paper_loop": "multiple"}, "data": {"set_0": {"gold_data": {"caption": "Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.", "mention_paragraphs": ["DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. 1, public VQA datasets are significantly smaller in size when compared to video classification datasets (Karpathy et al., 2014; Kay et al., 2017). The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality (Chen et al., 2010; Ho\u00dffeld et al., 2014; Shahid et al., 2014; Chen et al., 2015), which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset (Hosu et al., 2017) requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets (Ying et al., 2021; G\u00f6tz-Hahn et al., 2021). However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset (G\u00f6tz-Hahn et al., 2021) revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data."], "glossary": {"dataset": "The dataset the paper uses for to evaluate its task.", "task": "The task the dataset was designed for.", "VQA": "An acronym that stands for video quality assessment.", "classification": "The video classification task.", "Annotations": "The number of annotations per video.", "Size": "The number of annotated videos in the dataset"}}, "generations": {"commonalities": ["Focus on video quality assessment (VQA)", "Discussion on diversity and realistic distortions in video content", "Use of subjective mean opinion scores (MOS) or quality ratings", "Application of deep learning or neural networks in VQA"], "tables": {"Focus on video quality assessment (VQA)": {"Dataset size": {"paper_1": ["1,000 video sequences"], "paper_2": ["N/A"], "paper_3": ["large scale UGC dataset"], "paper_4": ["39,000 real-world videos, 117,000 space-time video patches"], "paper_5": ["153,841 five-second long videos; benchmark set of nearly 1,600 videos"], "paper_6": ["1 million YouTube videos"], "paper_7": ["N/A"]}, "Dataset source": {"paper_1": ["YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset"], "paper_2": ["N/A"], "paper_3": ["YouTube"], "paper_4": ["open source Internet UGC digital videos"], "paper_5": ["KonVid-150k sourced from the internet for in-the-wild videos"], "paper_6": ["Sports-1M dataset, YouTube videos"], "paper_7": ["YouTube videos"]}, "Quality assessment methodology": {"paper_1": ["crowdsourcing-based process of collecting subjective mean opinion scores (MOS)"], "paper_2": ["N/A"], "paper_3": ["evaluation with three no-reference metrics: Noise, Banding, and SLEEQ"], "paper_4": ["online visual psychometric study to gather human subjective quality scores"], "paper_5": ["five subjective opinions each; nearly 1 million subjective ratings; deep multi-layer spatially pooled features for VQA"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "Types of distortions included": {"paper_1": ["compression, transmission, and other parts of the video processing and distribution pipeline; diverse set of authentic video acquisition distortions"], "paper_2": ["N/A"], "paper_3": ["visual artifacts present in the original; considers the practical and realistic needs of video compression and quality assessment"], "paper_4": ["UGC video distortions, including composite distortions, unsteady capture conditions, processing and editing artifacts"], "paper_5": ["authentic distortions, not artificially distorted; relevant to consumer internet videos"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "Resolution and format of videos": {"paper_1": ["N/A"], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["N/A"], "paper_5": ["N/A"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "schemes": ["Dataset size", "Dataset source", "Quality assessment methodology", "Types of distortions included", "Resolution and format of videos"]}, "Discussion on diversity and realistic distortions in video content": {"Diversity of video content": {"paper_1": ["high diversity, user-generated content, multiple processing stages and use cases"], "paper_2": ["N/A"], "paper_3": ["high diversity, user-generated content, content from YouTube UGC"], "paper_4": ["real-world user-generated content (UGC), social media platforms"], "paper_5": ["highly diverse, user-generated content, videos in-the-wild"], "paper_6": ["content from a taxonomy of 487 classes of sports, large scale"], "paper_7": ["diverse range of human actions"]}, "Realism of video distortions": {"paper_1": ["natural distortions in user-generated videos"], "paper_2": ["type and severities of the distortions, mixtures of distortions"], "paper_3": ["real-world visual quality of media, visual artifacts present in original UGC content"], "paper_4": ["authentic UGC video distortions, processing, editing, compression and transmission artifacts"], "paper_5": ["authentic distortions in in-the-wild videos"], "paper_6": ["N/A"], "paper_7": ["real-world circumstances like camera motion or shake, illumination variations"]}, "Size of the dataset": {"paper_1": ["1,000 video sequences"], "paper_2": ["N/A"], "paper_3": ["large scale, high coverage over millions of YouTube videos"], "paper_4": ["39,000 real-world videos, 117,000 video patches"], "paper_5": ["153,841 five-second long videos, nearly 1 million subjective ratings"], "paper_6": ["1 million YouTube videos"], "paper_7": ["large dataset, significantly larger than HMDB-51 or UCF-101"]}, "Types of distortions analyzed": {"paper_1": ["compression, transmission, varying conditions of video capture and processing"], "paper_2": ["distortions or mixtures of distortions that corrupt the videos"], "paper_3": ["non-pristine original, effect of encoding on visual artifacts, traditional reference-based metrics limitations"], "paper_4": ["highly diverse capture conditions, processing and editing artifacts, compression and transmission"], "paper_5": ["natural occurring distortions, authentic real-life degradations"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "Sources of the video content": {"paper_1": ["YFCC100m (Yahoo Flickr Creative Commons 100 Million) dataset"], "paper_2": ["various applications such as Instagram, Facebook, Twitter, Snapchat"], "paper_3": ["YouTube"], "paper_4": ["online open source UGC, social media platforms"], "paper_5": ["user-generated content on internet platforms, not recorded or altered for research"], "paper_6": ["YouTube videos related to sports"], "paper_7": ["YouTube videos, not professionally videoed content"]}, "schemes": ["Diversity of video content", "Realism of video distortions", "Size of the dataset", "Types of distortions analyzed", "Sources of the video content"]}, "Use of subjective mean opinion scores (MOS) or quality ratings": {"assessment methodology": {"paper_1": ["crowdsourcing-based process for collecting subjective mean opinion scores (MOS)"], "paper_2": ["details not provided within the introduction"], "paper_3": ["evaluation using no-reference metrics: Noise, Banding, and SLEEQ"], "paper_4": ["online visual psychometric study to gather subjective quality scores"], "paper_5": ["crowdsourcing for subjective opinions"], "paper_6": ["features learned from large scale datasets, transferability to other datasets"], "paper_7": ["N/A"]}, "dataset size and diversity": {"paper_1": ["large publicly available database called KoNViD-1k, diverse set of video content"], "paper_2": ["large-scale research, though explicit dataset size not detailed in introduction"], "paper_3": ["large scale UGC dataset from YouTube videos"], "paper_4": ["largest video quality database with 39,000 videos and 117,000 v-patches"], "paper_5": ["KonVid-150k, a dataset two orders of magnitude larger than previous datasets"], "paper_6": ["Sports-1M, a dataset of 1 million videos in 487 categories"], "paper_7": ["Kinetics, intended for human action classification with hundreds of classes"]}, "video resolution and quality variation": {"paper_1": ["supports a broad range of video quality to be supported"], "paper_2": ["details not provided within the introduction"], "paper_3": ["high coverage over millions of YouTube videos"], "paper_4": ["videos of diverse sizes, contents, and distortions, includes spatio-temporal video patches"], "paper_5": ["diverse content and naturally occurring, representative degradations"], "paper_6": ["video from various sources, implying variability in resolution and quality"], "paper_7": ["N/A"]}, "subjective rating acquisition protocol": {"paper_1": ["described as a crowdsourcing-based process"], "paper_2": ["details not provided within the introduction"], "paper_3": ["N/A"], "paper_4": ["online visual psychometric study with large numbers of human subjects"], "paper_5": ["large scale crowdsourcing, with minimum of five subjective opinions per video"], "paper_6": ["use of a separate dataset to transfer learning and assess feature generalizability"], "paper_7": ["N/A"]}, "MOS scoring scale": {"paper_1": ["N/A"], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["5.5M perceptual quality judgments, scoring scale not specified"], "paper_5": ["close to one million subjective ratings, specific scoring scale not mentioned"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "schemes": ["assessment methodology", "dataset size and diversity", "video resolution and quality variation", "subjective rating acquisition protocol", "MOS scoring scale"]}, "Application of deep learning or neural networks in VQA": {"Dataset size and diversity": {"paper_1": ["A large public database of video sequences with diverse set of video content."], "paper_2": ["N/A"], "paper_3": ["Analysis of YouTube content to create a representative set for video compression research."], "paper_4": ["A large-scale public UGC video dataset comprising full videos and spatio-temporal video patches (v-patches)."], "paper_5": ["KonVid-150k: an ecologically valid in-the-wild video quality assessment database, two orders of magnitude larger than existing ones."], "paper_6": ["A large-scale dataset of 1 million videos with 487 categories of sports called Sports-1M."], "paper_7": ["The Kinetics dataset, covering a diverse range of human actions, with clips sourced from YouTube."]}, "Deep learning architecture used": {"paper_1": ["N/A"], "paper_2": ["N/A"], "paper_3": ["Use of no-reference metrics such as Noise, Banding, and SLEEQ."], "paper_4": ["A deep neural architecture that computes 2D video features using PaQ2PiQ, in parallel with 3D features using ResNet3D."], "paper_5": ["Use of deep convolutional neural networks (DCNNs), particularly InceptionResNet-v2, as feature extractors."], "paper_6": ["Multiple Convolutional Neural Networks (CNNs) architectures for video classification, including models with dual streams processing at different resolutions."], "paper_7": ["N/A"]}, "Training methodology": {"paper_1": ["Crowdsourcing-based process of collecting subjective mean opinion scores (MOS)."], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["Large-scale subjective video quality study to gather human subjective quality scores for training deep models."], "paper_5": ["Training on KonVid-150k dataset with nearly 1 million subjective ratings."], "paper_6": ["Utilization of a new dataset, Sports-1M, for extensive training of CNNs."], "paper_7": ["N/A"]}, "Video quality assessment methodology": {"paper_1": ["No-reference VQA methods focusing on content diversity to support a broad range of video quality."], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["Deep blind video quality prediction and the creation of space-time maps of video quality."], "paper_5": ["Application of deep multi-layer spatially pooled features for video quality assessment."], "paper_6": ["Evaluation of CNNs in large-scale video classification with context and fovea stream processing."], "paper_7": ["N/A"]}, "Performance metrics": {"paper_1": ["N/A"], "paper_2": ["Large-scale study of VQA models to achieve close agreement with human judgments."], "paper_3": ["Large-scale UGC dataset for facilitating video compression and quality assessment research."], "paper_4": ["Predicting video quality using a deep learning model capable of capturing local and global spatio-temporal distortions."], "paper_5": ["N/A"], "paper_6": ["Empirical evaluation of CNNs on the Sports-1M dataset and report of classification performance."], "paper_7": ["Introduction of a large, high quality dataset for human action classification as a performance benchmark."]}, "schemes": ["Dataset size and diversity", "Deep learning architecture used", "Training methodology", "Video quality assessment methodology", "Performance metrics"]}}}}, "set_1": {"gold_data": {"caption": "Physical adversarial attacks against Semantic Segmentation tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.", "mention_paragraphs": ["Table XII presents a comprehensive comparison of various adversarial attack methods in the semantic segmentation task. It provides an overview of their attack goals, patch placement strategies, consideration of changing viewpoints, testing in the physical domain, and transferability to other models. Table XIII offers detailed information on adversarial attacks, including the attacker\u2019s knowledge level, robustness techniques, stealthiness techniques, physical test types, and space of operation. Table XIV provides information on the datasets used, the evaluated networks, and the links to open-source code for the experiments conducted in the semantic segmentation task."], "glossary": {"IPatch": "An adversarial attack that targets a particular region in an image by inserting a adversarially optimized patch.", "Attacker's Knowledge": "The knowledge required by the attacker. Can be one of White-box, Black-box or Grey-box", "White-box": "refer to adversarial attacks where the attacker possesses complete knowledge of the training and testing data used to train the victim model, as well as the architecture and parameters of the target model.", "Robustness Technique": "Techniques for maintaining robustness to attacks.", "EOT": "Expectation of Transformation. for adversarial attacks, which takes potential transformations in the real world into account during the optimization, resulting in better robustness. EOT adds random distortions in the optimization to make the perturbation more robust.", "Stealthiness Technique": "Techniques for evading detection that the attack was used.", "Static": "Attacks that do not change once inserted into the scene.", "Space": "Whether the attack takes place in two dimensions (eg like a sticker) or three (eg an object)", "2D": "The attack takes place in two dimensions"}}, "generations": {"commonalities": ["Use of deep learning models in applications like autonomous vehicles and medical screening", "Presence of adversarial patches which fool deep learning models", "Analysis of the effects these adversarial patches have on image classification or semantics", "Discussion of the vulnerability of convolutional neural networks to adversarial perturbations"], "tables": {"Use of deep learning models in applications like autonomous vehicles and medical screening": {"Methodology (in the context of use in autonomous vehicles and medical screening)": {"paper_1": ["Adversarial patch attacks against image segmentation and object detection models in life-critical operations like autonomous driving."], "paper_2": ["Investigation of the robustness of semantic segmentation models in autonomous driving against real-world adversarial patch attacks, focusing on real-time models and driving scenes."]}, "Model architecture (in the context of deep learning applications in safety-critical areas)": {"paper_1": ["N/A"], "paper_2": ["Convolutional Neural Networks (CNNs), reference to BiSeNet for semantic segmentation."]}, "Dataset used (relevant to autonomous driving and medical image screening)": {"paper_1": ["N/A"], "paper_2": ["Cityscapes dataset for urban driving scenes, CARLA simulator for virtual 3D environment, real-world driving scenario dataset."]}, "Evaluation metrics (in the context of assessing deep learning models in life-critical systems)": {"paper_1": ["N/A"], "paper_2": ["Effectiveness of adversarial patches in different scenarios including Cityscapes, CARLA virtual environment, and the real world."]}, "Robustness (relating to reliability in autonomous vehicles and other critical applications)": {"paper_1": ["Limitations of location-based adversarial attacks that affect predictions only around the patch area."], "paper_2": ["Evaluation of state-of-the-art models for real-time semantic segmentation to RWAEs, comparison between non-robust patches and robust patches crafted with EOT and other techniques."]}, "schemes": ["Methodology", "Model architecture", "Dataset used", "Evaluation metrics", "Robustness"]}, "Presence of adversarial patches which fool deep learning models": {"Patch generation method": {"paper_1": ["Not explicitly described, but the introduction suggests that the patches override classifiers' predictions and are a limited form of adversarial attacks because they only affect predictions around their location"], "paper_2": ["Extension to the pixel-wise cross-entropy loss to create stronger patches for semantic segmentation; Use of EOT for robust patch crafting; Scene-specific attack for crafting patches in a realistic autonomous driving scenario"]}, "Target model architectures": {"paper_1": ["Image segmentation models, object detection models"], "paper_2": ["Real-time Semantic Segmentation (SS) models, BiSeNet"]}, "Evaluation metrics": {"paper_1": ["N/A"], "paper_2": ["Effectiveness of patches on Cityscapes dataset, CARLA, and real-world driving scenarios; Comparison of robust patches with non-robust ones"]}, "Robustness assessment methods": {"paper_1": ["N/A"], "paper_2": ["Benchmarking robustness to Real-World Adversarial Examples; EOT-based patches and scene-specific patches compared to non-robust patches"]}, "Patch application technique": {"paper_1": ["Placing adversarial patches in real-world scenes"], "paper_2": ["Printing and attaching adversarial patches to 2D surfaces in the driving environment; Integrating patches into virtual 3D scenarios using CARLA"]}, "schemes": ["Patch generation method", "Target model architectures", "Evaluation metrics", "Robustness assessment methods", "Patch application technique"]}, "Analysis of the effects these adversarial patches have on image classification or semantics": {"Type of adversarial attack": {"paper_1": ["Remote adversarial patch"], "paper_2": ["Real-World Adversarial Patch Attacks"]}, "Target model architecture": {"paper_1": ["Image segmentation and object detection models"], "paper_2": ["Real-time semantic segmentation models"]}, "Dataset used for evaluation": {"paper_1": ["Not explicitly mentioned in the introduction section"], "paper_2": ["Cityscapes dataset, CARLA simulator, Real-world driving scenario"]}, "Robustness evaluation metrics": {"paper_1": ["N/A"], "paper_2": ["Effectiveness of adversarial patches in virtual 3D world and the real world"]}, "Real-world applicability": {"paper_1": ["Limited to the effect around the location of the patch"], "paper_2": ["Assesses the practical robustness of SS models to adversarial examples in both virtual and real-world settings"]}, "schemes": ["Type of adversarial attack", "Target model architecture", "Dataset used for evaluation", "Robustness evaluation metrics", "Real-world applicability"]}, "Discussion of the vulnerability of convolutional neural networks to adversarial perturbations": {"Attack success rate": {"paper_1": ["N/A"], "paper_2": ["Non-robust patches effective on Cityscapes but not in virtual 3D world or real world, robust patches crafted with EOT or scene-specific approach less effective on Cityscapes images, but can accomplish attack in virtual 3D and real world with limited effectiveness"]}, "Detection evasion capability": {"paper_1": ["Adversarial patch can be placed in a real-world scene and override an image-classifier's prediction"], "paper_2": ["RWAEs represent a more realistic threat as they do not require attacker to access the digital representation, patches crafted with extension to cross-entropy loss aim for high evasion capability"]}, "Perturbation invisibility": {"paper_1": ["N/A"], "paper_2": ["Physical adversarial patches, while visible, are designed to be inserted inconspicuously in the environment"]}, "Robustness of defenses": {"paper_1": ["N/A"], "paper_2": ["Raises questions on the practical relevance of RWAEs, demonstrating limited effectiveness of attacks in real-world scenarios"]}, "Impact on semantic segmentation": {"paper_1": ["Introduction of adversarial patches can disrupt image segmentation and object detection semantic models"], "paper_2": ["Exhaustive evaluation of the robustness of real-time SS models against RWAEs, demonstrating that certain adversarial patches could affect SS model performance in autonomous driving systems"]}, "schemes": ["Attack success rate", "Detection evasion capability", "Perturbation invisibility", "Robustness of defenses", "Impact on semantic segmentation"]}}}}, "set_2": {"gold_data": {"caption": "The comparison of existing data augmentation methods powered by LLMs for training retrieval models.", "mention_paragraphs": ["Additionally, to highlight the similarities and differences among the corresponding methods, we present a comparative result in Table III. It compares the aforementioned methods from various perspectives, including the number of examples, the generator employed, the type of synthetic data produced, the method applied to filter synthetic data, and whether LLMs are fine-tuned. This table serves to facilitate a clearer understanding of the landscape of these methods."], "glossary": {"Methods": "The name of the data augmentation method used by the paper", "# Examples": {"text": "The number of in-context examples used in the prompt the paper's method proposes.", "notes": "guess"}, "Generator": "The generation model used for data augmentation", "Synthetic data": "The type of synthetic data produced by the method in the paper.", "Relevant query": "The data augmentation mehtod produces synthetic queries that are relevant to a set of documents.", "Soft relevance labels": {"text": "In some downstream tasks of retrieval, such as question-answering, the collection of questions is also sufficient. However, the relevance labels connecting these questions with the passages of supporting evidence are very limited. In this context, leveraging the capability of LLMs for relevance label generation is a promising approach that can augment the training corpus for retrievers. LLMs produce the generation probabilities of the question conditioned on these top passages. After a normalization process, these probabilities serve as soft relevance labels for the training of the retriever.", "notes": "quote"}, "Filter Method": "How the synthetic data produced by the method in the paper is filtered to produce high quality data.", "Generation probability": {"text": "The probability of the synthetic query given the documents.", "notes": "guess"}, "Round-trip filtering": "First a retriever is fine-tuned on generated samples and then it is used to filter the samples.", "LLMs' tuning": "Whether the LLMs are finetuned, and what method is used if they are.", "Fixed": {"text": "the LLMs are not fintetuned", "notes": "guess"}, "Soft Prompt tuning": "only the prompts' embedding layer is optimized during the training process."}}, "generations": {"commonalities": ["use of large pretrained language models (LLMs)", "focus on information retrieval (IR) tasks", "challenge of domain-specific data availability", "utilization of synthetic or pseudo data for model training"], "tables": {"use of large pretrained language models (LLMs)": {"model architecture and size": {"paper_1": ["Uses various language models e.g., GPT-3, FLAN, Gopher, T0++"], "paper_2": ["Employs large language models (LLMs) such as FLAN"], "paper_3": ["No specific mention of LLM architecture and size"], "paper_4": ["Leverages GPT-3 and smaller language models like Flan-T5 XXL"], "paper_5": ["Utilizes LLMs such as CPT-3, LLaMA, Vicuna"], "paper_6": ["Employs a generative pre-trained language model (PLM)"]}, "pretraining dataset and size": {"paper_1": ["LLMs pretrained on diverse datasets (size not specified)"], "paper_2": ["No specific mention of pretraining dataset and size"], "paper_3": ["No specific mention of pretraining dataset and size"], "paper_4": ["No specific mention of pretraining dataset and size"], "paper_5": ["No specific mention of pretraining dataset and size"], "paper_6": ["No specific mention of pretraining dataset and size"]}, "fine-tuning approach": {"paper_1": ["Presumably no fine-tuning, discusses data augmentation via LLMs"], "paper_2": ["No fine-tuning, introduces Prompt-base Query Generation"], "paper_3": ["No fine-tuning, discusses contrastive learning with unsupervised strategies"], "paper_4": ["Unsupervised domain adaptation method, does not fine-tune LLMs"], "paper_5": ["Soft prompt tuning where only parameters associated with the soft prompt are updated, no fine-tuning of LLMs"], "paper_6": ["Unsupervised approach without fine-tuning PLMs, uses autoencoding-based retriever training"]}, "evaluation metrics and benchmarks": {"paper_1": ["Data augmentation effectiveness for IR"], "paper_2": ["nDCG@10 on BEIR with PROMPTAGATOR models"], "paper_3": ["Performance on BEIR and open-domain QA benchmarks"], "paper_4": ["Zero-shot settings on domains like LoTTE, SQuAD, NQ"], "paper_5": ["Comparison to models like BM25 and InPars"], "paper_6": ["Top-20 and top-100 accuracy on benchmark QA datasets"]}, "domain of application": {"paper_1": ["Information Retrieval"], "paper_2": ["Dense Retrieval across various retrieval tasks"], "paper_3": ["Unsupervised Dense Retrieval"], "paper_4": ["Unsupervised domain adaptation for neural IR"], "paper_5": ["Augmenting Dense Retrieval"], "paper_6": ["Dense Passage Retrieval for open-domain tasks"]}, "schemes": ["model architecture and size", "pretraining dataset and size", "fine-tuning approach", "evaluation metrics and benchmarks", "domain of application"]}, "focus on information retrieval (IR) tasks": {"data augmentation methods for IR tasks": {"paper_1": ["Incorporation of multi-billion parameter LMs into IR via data augmentation"], "paper_2": ["N/A"], "paper_3": ["Unsupervised dense retrieval via query extraction, transferred query generation"], "paper_4": ["N/A"], "paper_5": ["Soft prompt tuning for tagging documents with weak queries"], "paper_6": ["Autoencoding-based Retriever Training (ART) with corpus-level autoencoding"]}, "usage of large language models for IR tasks": {"paper_1": ["Large LMs for effective data augmentation"], "paper_2": ["Promoting large LMs to generate task-specific prompts for few-shot retrieval"], "paper_3": ["N/A"], "paper_4": ["Using LLMs for unsupervised domain adaptation of neural retrievers"], "paper_5": ["LLMs with soft prompt tuning to enhance dense retrieval"], "paper_6": ["Utilizing generative pre-trained LMs for zero-shot question generation"]}, "few-shot learning capabilities for IR tasks": {"paper_1": ["N/A"], "paper_2": ["Few-shot retrieval by prompting with LLMs to generate synthetic task-specific training data"], "paper_3": ["N/A"], "paper_4": ["Retrieval accuracy gains in zero-shot settings"], "paper_5": ["N/A"], "paper_6": ["Highly sample-efficient, outperforming with just a few questions"]}, "unsupervised learning techniques for IR tasks": {"paper_1": ["N/A"], "paper_2": ["N/A"], "paper_3": ["Dense retrieval with data augmentation without human-annotated data"], "paper_4": ["Unsupervised domain adaptation method for neural IR"], "paper_5": ["N/A"], "paper_6": ["Unsupervised method with no labeled training data or task-specific losses"]}, "domain adaptation strategies for IR tasks": {"paper_1": ["N/A"], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["Strategic use of LLMs for domain adaptation"], "paper_5": ["N/A"], "paper_6": ["Strong generalization on out-of-distribution datasets"]}, "schemes": ["data augmentation methods", "usage of large language models", "few-shot learning capabilities", "unsupervised learning techniques", "domain adaptation strategies"]}, "challenge of domain-specific data availability": {"Data augmentation techniques": {"paper_1": ["InPars uses large language models to generate synthetic data for information retrieval tasks when there's a lack of supervised information."], "paper_2": ["PROMPTAGATOR creates synthetic queries for few-shot retrieval using large language models without fine-tuning, applying task-specific prompting and filtering techniques to ensure data quality."], "paper_3": ["AugTriever uses query extraction and transferred query generation for constructing pseudo query-doc pairs and introduces two datasets consisting of millions of pseudo query-document pairs."], "paper_4": ["UDAPDR uses a combination of strategy involving LLMs to create synthetic queries for domain adaptation and training multiple retrievers that distill into one."], "paper_5": ["Soft Prompt Tuning (SPTAR) augments dense retrieval by prompting LLMs with tuned soft prompts to generate weak queries, which includes a filtering process for improving quality."], "paper_6": ["ART uses an autoencoding-based approach for training using unpaired questions and passages, relying on the reconstruction of questions and soft-label scores."]}, "Usage of large language models for data generation": {"paper_1": ["InPars employs large language models for data augmentation in the context of information retrieval."], "paper_2": ["PROMPTAGATOR utilizes large language models such as FLAN for query generation in few-shot retrieval settings."], "paper_3": ["AugTriever indirectly uses large language models via transferred query generation to produce pseudo queries."], "paper_4": ["UDAPDR incorporates large language models like GPT-3 and Flan-T5 to generate a diverse set of synthetic queries for domain adaptation."], "paper_5": ["SPTAR uses soft prompt tuning of large language models to generate weak queries for dense retrieval."], "paper_6": ["ART uses generative pre-trained language models for question generation in an unsupervised manner."]}, "Few-shot learning capability": {"paper_1": ["N/A"], "paper_2": ["PROMPTAGATOR is designed for few-shot dense retrieval, generating data with minimal supervision (as few as 2-8 examples)."], "paper_3": ["N/A"], "paper_4": ["The domain adaptation process in UDAPDR can work under few-shot conditions, leveraging LLM-generated synthetic queries for training rerankers."], "paper_5": ["While not explicitly stated, SPTAR's approach of prompting LLMs with soft prompts suggests potential for few-shot learning applications."], "paper_6": ["ART is not described as few-shot but demonstrates high sample efficiency, which may suggest capabilities in few-shot contexts."]}, "Scalability of data augmentation methods": {"paper_1": ["Not explicitly stated, but the use of large language models implies potential scalability."], "paper_2": ["PROMPTAGATOR's approach is scalable, relying on large language models for generating a large set of synthetic queries from a few examples."], "paper_3": ["Highly scalable, AugTriever introduces two large datasets consisting of millions of pseudo query-document pairs for training."], "paper_4": ["UDAPDR shows scalability in its approach by distilling multiple trained rerankers into a single model."], "paper_5": ["Scalability may be limited due to the requirement of fine-tuning soft prompts for each specific task, despite generating weak data at scale."], "paper_6": ["ART is scalable as it does not require labeled training data or task-specific losses, and it leverages unpaired questions and passages available at scale."]}, "Unsupervised domain adaptation strategies": {"paper_1": ["InPars does not emphasize unsupervised domain adaptation but focuses on data augmentation using LLMs."], "paper_2": ["PROMPTAGATOR does not directly address unsupervised domain adaptation, instead focusing on few-shot retrieval."], "paper_3": ["AugTriever is focused on unsupervised training of dense retrieval models with pseudo query-document pairs, indicating an unsupervised domain adaptation approach."], "paper_4": ["UDAPDR specializes in unsupervised domain adaptation of retrieval models through LLM prompting strategies and distillation of multiple rerankers."], "paper_5": ["SPTAR's approach of using LLMs with soft prompt tuning suggests applicability to unsupervised domain adaptation, even though it's not the main focus."], "paper_6": ["Unsupervised in nature, ART focuses on domain agnostic training without requiring labeled data or task-specific tuning, aligning with unsupervised domain adaptation."]}, "schemes": ["Data augmentation techniques", "Usage of large language models for data generation", "Few-shot learning capability", "Scalability of data augmentation methods", "Unsupervised domain adaptation strategies"]}, "utilization of synthetic or pseudo data for model training": {"Data augmentation technique utilizing synthetic or pseudo data for model training": {"paper_1": ["Data augmentation utilizing prompts"], "paper_2": ["Data augmentation via synthetic question generation using prompting"], "paper_3": ["Data augmentation through unsupervised methods, creating pseudo query-doc pairs"], "paper_4": ["Domain adaptation via synthetic query generation"], "paper_5": ["Data augmentation using soft prompt tuning"], "paper_6": ["Data augmentation through unsupervised autoencoding-based method"]}, "Application domain for synthetic or pseudo data utilization in model training": {"paper_1": ["Information retrieval"], "paper_2": ["Neural retrieval tasks"], "paper_3": ["Dense retrieval, BEIR and open-domain QA benchmarks"], "paper_4": ["Neural information retrieval"], "paper_5": ["Dense retrieval"], "paper_6": ["Dense passage retrieval for QA tasks"]}, "Synthetic data generation method for model training": {"paper_1": ["Large language models"], "paper_2": ["Large language models, few-shot prompting"], "paper_3": ["Query extraction and transferred query generation"], "paper_4": ["Language model prompting and passage reranker distillation"], "paper_5": ["Soft prompt tuning with large language models"], "paper_6": ["Generative pre-trained language models with teacher-forcing for question reconstruction"]}, "Use of large language models for generation in model training": {"paper_1": ["Yes, using large language models like GPT-3 and FLAN"], "paper_2": ["Yes, using large language models like FLAN"], "paper_3": ["No, unsupervised generation models for query generation"], "paper_4": ["Yes, GPT-3 and Flan-T5 XXL"], "paper_5": ["Yes, using soft prompt tuning without fine-tuning LLMs"], "paper_6": ["Yes, leveraging large language models for question generation"]}, "Training methodology involving synthetic or pseudo data": {"paper_1": ["Not specified in provided text"], "paper_2": ["Training with task-specific prompting and a filtering technique"], "paper_3": ["Contrastive learning using unsupervised pseudo pairs"], "paper_4": ["Distillation of multiple passage rerankers into a single retriever"], "paper_5": ["Tuning only soft prompt parameters of LLMs, not the LLMs themselves"], "paper_6": ["Autoencoding-based retriever training"]}, "schemes": ["Data augmentation technique", "Application domain", "Synthetic data generation method", "Use of large language models for generation", "Training methodology"]}}}}, "set_3": {"gold_data": {"caption": "The comparison of retrievers that leverage LLMs as the foundation.", "mention_paragraphs": ["To provide a comprehensive understanding of this topic, Table 4 summarizes the common and unique characteristics of the LLM-based retrievers discussed above."], "glossary": {"Methods": "The method introduced by the paper", "Backbone": {"text": "The model that the LLM-based retriever is built off of", "notes": "guess"}, "Architecture": "The architecture of the model used for the LLM-based retriever.", "Encoder-based": "LLMs, like the T5-family of models, are used to embed passages which are then placed in an index.", "Generative": "LLMs generate document identifiers or URLs directly rather than using the 'index-retrieval-rank' paradigm.", "LLM's tuning": "How LLM's need to be tuned for the paper's method.", "Training from scratch": {"text": "The model has to be trained from scratch", "notes": "guess"}, "Fine-tuning": {"text": "The model has to be fine-tuned", "notes": "guess"}, "Fine-tuning & Prompting": {"text": "The model has to be fine-tuned and prompted in a specific way to perform retrieval", "notes": "guess"}, "Prompting": {"text": "The model does not have to be fine-tuned and can instead just be prompted", "notes": "guess"}}}, "generations": {"commonalities": ["Text and code embeddings for retrieval tasks", "Improvements over previous models in specific benchmarks", "Generalization capabilities for out-of-domain data", "Scaling up model size"], "tables": {"Text and code embeddings for retrieval tasks": {"Model architecture": {"paper_1": ["Transformer encoder for embedding"], "paper_2": ["T5 encoder-based dual encoders"], "paper_3": ["Dense dual-encoder and cross-encoder architectures"], "paper_4": ["Transformers as differentiable search index"], "paper_5": ["Autoregressive model with token-level cross-attention"]}, "Pre-training tasks": {"paper_1": ["In-batch negatives contrastive learning on unlabeled data"], "paper_2": ["Pre-training on two billion web question-answer pairs"], "paper_3": ["Not explicitly stated"], "paper_4": ["Not explicitly stated"], "paper_5": ["Not explicitly stated but leverages large pre-trained models like GPT-3"]}, "Fine-tuning tasks": {"paper_1": ["Supervised datasets for NLI, SST-2, and MS-MARCO"], "paper_2": ["Fine-tuning on MS MARCO"], "paper_3": ["Zero-shot and cross-task retrieval"], "paper_4": ["Fine-tuning on NQ dataset retrieval tasks"], "paper_5": ["Document retrieval with generated Web URLs"]}, "Data sources for training": {"paper_1": ["Text and code pair data, pretrained models"], "paper_2": ["Generic web question-answer pairs, human curated data like MS MARCO"], "paper_3": ["BERRI datasets across diverse domains"], "paper_4": ["Corpora derived from the Natural Questions dataset"], "paper_5": ["Open-domain question answering datasets"]}, "Embedding dimensions": {"paper_1": ["N/A"], "paper_2": ["Fixed embedding dimension of 768"], "paper_3": ["N/A"], "paper_4": ["N/A"], "paper_5": ["N/A"]}, "schemes": ["Model architecture", "Pre-training tasks", "Fine-tuning tasks", "Data sources for training", "Embedding dimensions"]}, "Improvements over previous models in specific benchmarks": {"Model architecture": {"paper_1": ["Contrastive learning of text and code embeddings using Transformer encoder"], "paper_2": ["Scaled up dual encoder model using T5 architecture"], "paper_3": ["Task-aware ReTriever (TART) with dual and cross-encoder architectures"], "paper_4": ["Differentiable search index (DSI) with Transformer seq2seq LM"], "paper_5": ["Built-in autoregressive search engine within Large Language Models (LLMs)"]}, "Pre-training tasks": {"paper_1": ["In-batch negatives on unlabeled data leveraging naturally occurring paired data"], "paper_2": ["Pre-training on two billion web question-answer pairs"], "paper_3": ["Trained on BERRI dataset with multiple task instructions"], "paper_4": ["N/A"], "paper_5": ["In-context demonstrations to prompt LLMs"]}, "Embedding dimensions": {"paper_1": ["300M to 175B parameters (not direct embedding size)"], "paper_2": ["Pre-trained T5 models up to 5 billion parameters; bottleneck embedding of 768"], "paper_3": ["N/A"], "paper_4": ["N/A"], "paper_5": ["N/A"]}, "Benchmarks evaluated": {"paper_1": ["SentEval, MSMARCO, Natural Questions, TriviaQA"], "paper_2": ["BEIR benchmark"], "paper_3": ["BEIR, LOTTE-pooled, X2-Retrieval"], "paper_4": ["Natural Questions"], "paper_5": ["WebQ, NQ, TriviaQA"]}, "Retrieval performance": {"paper_1": ["Consistent performance improvement with increasing model sizes"], "paper_2": ["Scaling up leads to better generalization despite fixed single-dot product bottleneck"], "paper_3": ["State-of-the-art results on zero-shot and cross-task retrieval"], "paper_4": ["Improved Hits@1 performance over dual encoders; over 20 points in base-sized T5 model"], "paper_5": ["URL generation outperforms existing methods; high recall for supporting documents"]}, "schemes": ["Model architecture", "Pre-training tasks", "Embedding dimensions", "Benchmarks evaluated", "Retrieval performance"]}, "Generalization capabilities for out-of-domain data": {"Pre-training objective types": {"paper_1": ["Contrastive learning objective with in-batch negatives on unlabeled data"], "paper_2": ["Scaling up dual encoder model size, approximated nearest neighbor search"], "paper_3": ["Explicit modeling of user intent with retrieval instructions"], "paper_4": ["Generative indexing and retrieval using seq2seq Transformer models"], "paper_5": ["In-context demonstrations to prompt LLMs to generate web URLs for document retrieval"]}, "Embedding dimensions": {"paper_1": ["Unspecified, but with models ranging from 300M to 175B parameters which suggests variability in embedding dimensions"], "paper_2": ["Fixed embedding dimension of 768 for bottleneck"], "paper_3": ["Unspecified"], "paper_4": ["Unspecified, but indicative of full-text or bag-of-words document representation approaches"], "paper_5": ["LLMs such as GPT-3, suggesting large embedding dimensions"]}, "Benchmark datasets for out-of-domain evaluation": {"paper_1": ["MSMARCO", "SentEval", "Natural Questions", "TriviaQA"], "paper_2": ["MS Marco", "BEIR benchmark"], "paper_3": ["BEIR", "LOTTE-pooled", "X2-Retrieval"], "paper_4": ["Natural Questions"], "paper_5": ["WebQ", "NQ (Natural Questions)", "TriviaQA"]}, "Evaluation metrics for generalization": {"paper_1": ["Classification accuracy", "Improvement percentage over previous methods"], "paper_2": ["Zero-shot performance", "Data efficiency", "Overall out-of-domain performance"], "paper_3": ["Model's ability to follow instructions", "State-of-the-art comparison"], "paper_4": ["Hits@1", "Zero-shot performance"], "paper_5": ["Recall@K", "Exact Match (EM)"]}, "Model architectures": {"paper_1": ["Transformers"], "paper_2": ["Generalizable T5-based dense Retrievers (GTR)"], "paper_3": ["Dense dual-encoder and cross-encoder architectures"], "paper_4": ["Sequence-to-sequence Transformer"], "paper_5": ["Autoregressive search engines, Large Language Models (LLMs) like GPT-3"]}, "schemes": ["Pre-training objective types", "Embedding dimensions", "Benchmark datasets for out-of-domain evaluation", "Evaluation metrics for generalization", "Model architectures"]}, "Scaling up model size": {"Model architecture": {"paper_1": ["Unsupervised text and code embeddings with Transformer models"], "paper_2": ["T5-based dual encoder"], "paper_3": ["Task-aware Retriever (TART); Dual-encoder and cross-encoder architectures"], "paper_4": ["Sequence-to-sequence Transformer"], "paper_5": ["Autoregressive model (e.g., GPT-3)"]}, "Parameter count": {"paper_1": ["300M to 175B"], "paper_2": ["Up to 5 billion"], "paper_3": ["N/A"], "paper_4": ["Base size to 11B"], "paper_5": ["N/A, but implies potentially using models as large as GPT-3"]}, "Pre-training tasks": {"paper_1": ["Paired text and code data, pre-trained model initialization"], "paper_2": ["Community question-answer pairs, fine-tuning on MS Marco"], "paper_3": ["BERRI datasets with diverse instructions"], "paper_4": ["Training with NQ dataset for supporting passage retrieval"], "paper_5": ["In-context demonstrations with (query, URL) pairs"]}, "Embedding dimensions": {"paper_1": ["N/A"], "paper_2": ["Bottleneck embedding dimension of 768"], "paper_3": ["N/A"], "paper_4": ["N/A"], "paper_5": ["N/A"]}, "Contrastive learning techniques": {"paper_1": ["In-batch negatives with contrastive learning on unlabeled data"], "paper_2": ["Single dot-product bottleneck with fixed size"], "paper_3": ["Designed negative samples including instruction-unfollowing negative samples"], "paper_4": ["Contrastive learning based dual encoders"], "paper_5": ["N/A"]}, "schemes": ["Model architecture", "Parameter count", "Pre-training tasks", "Embedding dimensions", "Contrastive learning techniques"]}}}}}}