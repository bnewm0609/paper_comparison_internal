{"experiment_type": {"scheme_source": "abs", "commonality_num": 4, "attribute_num": 5, "value_source": "abs", "value_paper_loop": "multiple"}, "data": {"set_0": {"gold_data": {"caption": "Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.", "mention_paragraphs": ["DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. 1, public VQA datasets are significantly smaller in size when compared to video classification datasets (Karpathy et al., 2014; Kay et al., 2017). The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality (Chen et al., 2010; Ho\u00dffeld et al., 2014; Shahid et al., 2014; Chen et al., 2015), which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset (Hosu et al., 2017) requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets (Ying et al., 2021; G\u00f6tz-Hahn et al., 2021). However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset (G\u00f6tz-Hahn et al., 2021) revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data."], "glossary": {"dataset": "The dataset the paper uses for to evaluate its task.", "task": "The task the dataset was designed for.", "VQA": "An acronym that stands for video quality assessment.", "classification": "The video classification task.", "Annotations": "The number of annotations per video.", "Size": "The number of annotated videos in the dataset"}}, "generations": {"commonalities": ["Focus on video quality assessment (VQA)", "Utilization of user-generated content (UGC)", "Challenges in current VQA methods due to diverse real-world distortions", "Inclusion of subjective mean opinion scores (MOS) or quality ratings from human viewers"], "tables": {"Focus on video quality assessment (VQA)": {"Dataset size": {"paper_1": ["1,200 video sequences"], "paper_2": ["585 videos"], "paper_3": ["1500 20 sec video clips"], "paper_4": ["39,000 videos and 117,000 video patches ('v-patches')"], "paper_5": ["153,841 videos and 1,596 videos with detailed ratings"], "paper_6": ["1 million YouTube videos"], "paper_7": ["At least 400 video clips for each of 400 actions"]}, "Content diversity": {"paper_1": ["wide variety of content"], "paper_2": ["unique contents captured by a large number of users"], "paper_3": ["popular categories like Gaming, Sports, and High Dynamic Range (HDR)"], "paper_4": ["real-world distorted videos"], "paper_5": ["substantially larger and diverse video content"], "paper_6": ["large-scale video classification with 487 classes"], "paper_7": ["400 human action classes, human-object and human-human interactions"]}, "Resolution": {"paper_1": ["N/A"], "paper_2": ["wide ranges of resolutions"], "paper_3": ["N/A"], "paper_4": ["N/A"], "paper_5": ["N/A"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "Subjective assessment methodology": {"paper_1": ["subjectively annotated"], "paper_2": ["subjective video quality scores via crowdsourcing with 4776 unique participants"], "paper_3": ["no-reference objective quality metrics used for evaluation"], "paper_4": ["5.5M human perceptual quality annotations"], "paper_5": ["five quality ratings each, up to 89 ratings for some videos"], "paper_6": ["empirical evaluation, but not specific on methodology"], "paper_7": ["baseline performance figures for neural network architectures, but not specific on subjective assessment methodology"]}, "Objective assessment methods": {"paper_1": ["advantages for objective VQA method development"], "paper_2": ["comparison of leading NR video quality predictors"], "paper_3": ["evaluate current dataset with three no-reference metrics (Noise, Banding, and SLEEQ)"], "paper_4": ["two unique NR-VQA models (PVQ and PVQ Mapper)"], "paper_5": ["new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features"], "paper_6": ["N/A"], "paper_7": ["Preliminary analysis of neural network performance"]}, "schemes": ["Dataset size", "Content diversity", "Resolution", "Subjective assessment methodology", "Objective assessment methods"]}, "Utilization of user-generated content (UGC)": {"Dataset size": {"paper_1": ["1,200 video sequences"], "paper_2": ["585 videos"], "paper_3": ["1500 video clips"], "paper_4": ["39,000 videos, 117,000 video patches, 5.5M annotations"], "paper_5": ["153,841 videos"], "paper_6": ["1 million YouTube videos"], "paper_7": ["At least 400 clips for each of 400 actions"]}, "Content type": {"paper_1": ["Public-domain, 'in the wild', varied content"], "paper_2": ["Unique content captured by numerous users, authentic distortions"], "paper_3": ["User Generated Content (UGC) from YouTube"], "paper_4": ["Real-world user-generated content (UGC)"], "paper_5": ["In-the-wild UGC, diverse"], "paper_6": ["Large-scale video classification"], "paper_7": ["Human action classes from YouTube"]}, "Annotation method": {"paper_1": ["Subjective mean opinion scores (MOS)"], "paper_2": ["Crowdsourced subjective quality scores"], "paper_3": ["No-reference objective quality metrics"], "paper_4": ["Human perceptual quality annotations"], "paper_5": ["Coarsely and extensively annotated quality ratings"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "Intended application": {"paper_1": ["Objective quality assessment methods, deep learning"], "paper_2": ["Advancing NR video quality prediction"], "paper_3": ["UGC compression and quality evaluation"], "paper_4": ["Perceptual video quality assessment"], "paper_5": ["Deep learning VQA methods, training at scale"], "paper_6": ["Large-scale video classification"], "paper_7": ["Neural networks for human action classification, avoiding dataset bias in classifiers"]}, "Quality assessment methodology": {"paper_1": ["Subjective VQA database"], "paper_2": ["Crowdsourced VQA study, comparison of NR VQA models"], "paper_3": ["Evaluation by no-reference metrics"], "paper_4": ["NR-VQA models, spatial-temporal mapping"], "paper_5": ["Multi-level spatially pooled deep-features (MLSP) for VQA"], "paper_6": ["Empirical evaluation of CNN models"], "paper_7": ["Baseline performance figures for neural network architectures"]}, "schemes": ["Dataset size", "Content type", "Annotation method", "Intended application", "Quality assessment methodology"]}, "Challenges in current VQA methods due to diverse real-world distortions": {"Dataset size and diversity": {"paper_1": ["1,200 public-domain video sequences sampled from YFCC100m"], "paper_2": ["585 videos, over 205000 opinion scores"], "paper_3": ["Large scale, 1500 20-sec video clips sampled from YouTube"], "paper_4": ["39,000 real-world distorted videos and 117,000 video patches"], "paper_5": ["153,841 videos with five quality ratings each, 1,596 videos with a minimum of 89 ratings each"], "paper_6": ["New dataset of 1 million YouTube videos belonging to 487 classes"], "paper_7": ["400 human action classes, with at least 400 video clips for each action"]}, "Video content and source variability": {"paper_1": ["Videos fairly sampled to cover the whole spectrum of video content and distortions"], "paper_2": ["Unique content, captured by a large number of users with diverse conditions"], "paper_3": ["Popular categories like Gaming, Sports, new features like High Dynamic Range (HDR)"], "paper_4": ["Real-world, 'in-the-wild' UGC video data"], "paper_5": ["KonVid-150k, large diversity and in-the-wild videos"], "paper_6": ["1 million YouTube videos across multiple categories"], "paper_7": ["Different YouTube videos showing wide variety of human actions"]}, "Types of distortions and artifacts addressed": {"paper_1": ["'In the wild' authentic distortions"], "paper_2": ["Authentic, often commingled distortions impossible to simulate"], "paper_3": ["Challenges for UGC compression and quality evaluation, traditional metrics shortcomings"], "paper_4": ["Advancement on 'in-the-wild' UGC data, spatial-temporal distortions"], "paper_5": ["Deep learning for authentic in-the-wild video distortions"], "paper_6": ["Local spatio-temporal information in video classification"], "paper_7": ["Focuses on human action classification, not specific distortions"]}, "Availability of ground truth or human judgments for video quality": {"paper_1": ["Subjective mean opinion scores (MOS) for all sequences"], "paper_2": ["Over 205,000 subjective video quality scores collected via crowdsourcing"], "paper_3": ["No-reference objective quality metrics evaluation"], "paper_4": ["5.5 million human perceptual quality annotations"], "paper_5": ["Subjective ratings with varying quantities per video"], "paper_6": ["Performance evaluation on large scale video classification, no explicit mention of human judgments"], "paper_7": ["Baseline performance figures for trained neural networks, but no mention of human judgments specifically for video quality"]}, "Methodology for video quality assessment": {"paper_1": ["General purpose subjective VQA intended to be representative of all types of content and distortions"], "paper_2": ["No-reference video quality prediction"], "paper_3": ["No-reference objective quality metrics for UGC evaluation"], "paper_4": ["Two NR-VQA models: local-to-global region-based and a space-time video quality mapping engine"], "paper_5": ["Proposed new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features"], "paper_6": ["Empirical evaluation of CNNs for video classification, extending connectivity in time domain"], "paper_7": ["Human action classification using neural network architectures, not specified for video quality assessment"]}, "schemes": ["Dataset size and diversity", "Video content and source variability", "Types of distortions and artifacts addressed", "Availability of ground truth or human judgments for video quality", "Methodology for video quality assessment (e.g., reference-based, no-reference, subjective, objective)"]}, "Inclusion of subjective mean opinion scores (MOS) or quality ratings from human viewers": {"Dataset size": {"paper_1": ["1,200 video sequences"], "paper_2": ["585 videos"], "paper_3": ["1500 20 sec video clips"], "paper_4": ["39,000 videos and 117,000 v-patches"], "paper_5": ["153,841 videos (KonVid-150k), 1,596 videos (other subset)"], "paper_6": ["1 million YouTube videos"], "paper_7": ["At least 400 clips for each of 400 actions"]}, "Video source": {"paper_1": ["public-domain videos, YFCC100m"], "paper_2": ["user-generated content"], "paper_3": ["YouTube videos"], "paper_4": ["real-world user-generated content (UGC)"], "paper_5": ["user-generated content (UGC)"], "paper_6": ["YouTube videos"], "paper_7": ["YouTube videos"]}, "Annotation method": {"paper_1": ["subjective annotation"], "paper_2": ["subjective video quality scores via crowdsourcing"], "paper_3": ["no-reference objective quality metrics"], "paper_4": ["5.5M human perceptual quality annotations"], "paper_5": ["quality ratings"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "MOS or quality rating scale": {"paper_1": ["MOS"], "paper_2": ["opinion scores (240 recorded opinions per video)"], "paper_3": ["objective quality metrics (Noise, Banding, SLEEQ)"], "paper_4": ["5.5M human perceptual quality annotations"], "paper_5": ["five quality ratings each, minimum of 89 ratings for a subset"], "paper_6": ["N/A"], "paper_7": ["N/A"]}, "Diversity of content": {"paper_1": ["wide variety of content"], "paper_2": ["unique content, capture devices, distortion types"], "paper_3": ["popular categories like Gaming, Sports, and HDR features"], "paper_4": ["real-world distorted videos"], "paper_5": ["large and diverse set of in-the-wild videos"], "paper_6": ["videos belonging to 487 classes"], "paper_7": ["400 human action classes"]}, "schemes": ["Dataset size", "Video source", "Annotation method", "MOS or quality rating scale", "Diversity of content"]}}}}, "set_1": {"gold_data": {"caption": "Physical adversarial attacks against Semantic Segmentation tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.", "mention_paragraphs": ["Table XII presents a comprehensive comparison of various adversarial attack methods in the semantic segmentation task. It provides an overview of their attack goals, patch placement strategies, consideration of changing viewpoints, testing in the physical domain, and transferability to other models. Table XIII offers detailed information on adversarial attacks, including the attacker\u2019s knowledge level, robustness techniques, stealthiness techniques, physical test types, and space of operation. Table XIV provides information on the datasets used, the evaluated networks, and the links to open-source code for the experiments conducted in the semantic segmentation task."], "glossary": {"IPatch": "An adversarial attack that targets a particular region in an image by inserting a adversarially optimized patch.", "Attacker's Knowledge": "The knowledge required by the attacker. Can be one of White-box, Black-box or Grey-box", "White-box": "refer to adversarial attacks where the attacker possesses complete knowledge of the training and testing data used to train the victim model, as well as the architecture and parameters of the target model.", "Robustness Technique": "Techniques for maintaining robustness to attacks.", "EOT": "Expectation of Transformation. for adversarial attacks, which takes potential transformations in the real world into account during the optimization, resulting in better robustness. EOT adds random distortions in the optimization to make the perturbation more robust.", "Stealthiness Technique": "Techniques for evading detection that the attack was used.", "Static": "Attacks that do not change once inserted into the scene.", "Space": "Whether the attack takes place in two dimensions (eg like a sticker) or three (eg an object)", "2D": "The attack takes place in two dimensions"}}, "generations": {"commonalities": ["use of deep learning models in computer vision tasks such as object detection and semantic segmentation", "vulnerability of deep learning models to adversarial perturbations", "creation and analysis of adversarial patches that mislead deep learning models", "discussion of the attack's effectiveness on state-of-the-art architectures"], "tables": {"use of deep learning models in computer vision tasks such as object detection and semantic segmentation": {"Model architecture used": {"paper_1": ["five state-of-the-art architectures with eight different encoders"], "paper_2": ["popular SS models without specifying architecture"]}, "Dataset and benchmarks": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset, CARLA driving simulator"]}, "Task performance metrics": {"paper_1": ["attack success rate of up to 93% on average"], "paper_2": ["robustness evaluation through attack performance, less effective in real-world application"]}, "Robustness to adversarial attacks": {"paper_1": ["RAP attacks affecting image semantics and classification"], "paper_2": ["digital and real-world adversarial patches, reduced effectiveness in real-world scenarios"]}, "Application domain focus": {"paper_1": ["object recognition models, emphasizing on YOLOv3"], "paper_2": ["autonomous/assisted driving applications"]}, "schemes": ["Model architecture used", "Dataset and benchmarks", "Task performance metrics", "Robustness to adversarial attacks", "Application domain focus"]}, "vulnerability of deep learning models to adversarial perturbations": {"Attack type": {"paper_1": ["remote adversarial patches (RAP)"], "paper_2": ["digital and real-world adversarial patches"]}, "Attack target": {"paper_1": ["classification, semantics of locations far from the patch"], "paper_2": ["semantic segmentation (SS)"]}, "Application domain": {"paper_1": ["autonomous vehicles, object localization and identification"], "paper_2": ["autonomous driving, semantic segmentation"]}, "Evaluation metrics": {"paper_1": ["success rate up to 93% on average"], "paper_2": ["success rate, effectiveness in real-world scenario"]}, "Dataset used for evaluation": {"paper_1": ["CamVid street view dataset, YOLOv3 model"], "paper_2": ["Cityscapes dataset, real-world testing in an outdoor driving scenario"]}, "schemes": ["Attack type (e.g., patch, adversarial example)", "Attack target (e.g., classification, semantic segmentation)", "Application domain (e.g., general, autonomous driving)", "Evaluation metrics (e.g., success rate, IoU)", "Dataset used for evaluation"]}, "creation and analysis of adversarial patches that mislead deep learning models": {"Patch creation method": {"paper_1": ["Introduced a new type of adversarial patch called `remote adversarial patches' (RAP), specifically the IPatch, which alters a model's perception of an image's semantics"], "paper_2": ["Used a novel loss function to craft powerful attacks, extended the Expectation Over Transformation (EOT) paradigm for semantic segmentation, and proposed a scene-specific attack that improves transferability to real 3D environments"]}, "Targeted model architectures": {"paper_1": ["Five state-of-the-art architectures with eight different encoders for image segmentation, and preliminary results on YOLOv3 for object recognition"], "paper_2": ["Popular semantic segmentation models, robustness tested against both digital and real-world adversarial patches"]}, "Attack success rate": {"paper_1": ["Up to 93% on average for changing the classification of a remote target region"], "paper_2": ["Not specifically quantified in the abstract but stated that the proposed attack formulations outperform previous work"]}, "Detection and defense mechanisms": {"paper_1": ["N/A"], "paper_2": ["N/A"]}, "Dataset and benchmarks used": {"paper_1": ["CamVid street view dataset for image segmentation RAP attacks, YOLOv3 model for object recognition"], "paper_2": ["Cityscapes dataset, CARLA driving simulator for scene-specific attack optimization, and outdoor driving scenario for real-world testing"]}, "schemes": ["Patch creation method", "Targeted model architectures", "Attack success rate", "Detection and defense mechanisms", "Dataset and benchmarks used"]}, "discussion of the attack's effectiveness on state-of-the-art architectures": {"Attack success rate": {"paper_1": ["up to 93% on average"], "paper_2": ["less effective in the real world"]}, "Robustness of targeted architectures": {"paper_1": ["in-depth analysis performed on five state-of-the-art architectures with eight different encoders"], "paper_2": ["popular SS models tested; robustness questioned in real-world scenarios"]}, "Scalability of the attack": {"paper_1": ["N/A"], "paper_2": ["extended EOT paradigm to improve transferability to real 3D environments"]}, "Type of architectures tested": {"paper_1": ["image segmentation and object recognition models, including YOLOv3"], "paper_2": ["SS models; used the CARLA driving simulator for testing"]}, "Type of adversarial patch": {"paper_1": ["remote adversarial patches (RAP)"], "paper_2": ["digital and real-world adversarial patches; a physical billboard tested"]}, "schemes": ["Attack success rate", "Robustness of targeted architectures", "Scalability of the attack", "Type of architectures tested", "Type of adversarial patch"]}}}}, "set_2": {"gold_data": {"caption": "The comparison of existing data augmentation methods powered by LLMs for training retrieval models.", "mention_paragraphs": ["Additionally, to highlight the similarities and differences among the corresponding methods, we present a comparative result in Table III. It compares the aforementioned methods from various perspectives, including the number of examples, the generator employed, the type of synthetic data produced, the method applied to filter synthetic data, and whether LLMs are fine-tuned. This table serves to facilitate a clearer understanding of the landscape of these methods."], "glossary": {"Methods": "The name of the data augmentation method used by the paper", "# Examples": {"text": "The number of in-context examples used in the prompt the paper's method proposes.", "notes": "guess"}, "Generator": "The generation model used for data augmentation", "Synthetic data": "The type of synthetic data produced by the method in the paper.", "Relevant query": "The data augmentation mehtod produces synthetic queries that are relevant to a set of documents.", "Soft relevance labels": {"text": "In some downstream tasks of retrieval, such as question-answering, the collection of questions is also sufficient. However, the relevance labels connecting these questions with the passages of supporting evidence are very limited. In this context, leveraging the capability of LLMs for relevance label generation is a promising approach that can augment the training corpus for retrievers. LLMs produce the generation probabilities of the question conditioned on these top passages. After a normalization process, these probabilities serve as soft relevance labels for the training of the retriever.", "notes": "quote"}, "Filter Method": "How the synthetic data produced by the method in the paper is filtered to produce high quality data.", "Generation probability": {"text": "The probability of the synthetic query given the documents.", "notes": "guess"}, "Round-trip filtering": "First a retriever is fine-tuned on generated samples and then it is used to filter the samples.", "LLMs' tuning": "Whether the LLMs are finetuned, and what method is used if they are.", "Fixed": {"text": "the LLMs are not fintetuned", "notes": "guess"}, "Soft Prompt tuning": "only the prompts' embedding layer is optimized during the training process."}}, "generations": {"commonalities": ["use of large pretrained language models (LLMs) in information retrieval tasks", "focus on few-shot or zero-shot learning capabilities", "development of methods to generate data (queries or query-document pairs) for information retrieval", "aim to improve performance of retrieval systems without relying on large labeled datasets"], "tables": {"use of large pretrained language models (LLMs) in information retrieval tasks": {"Use of LLMs in information retrieval tasks - Model architecture and size": {"Paper 1": ["Large pretrained transformer models"], "Paper 2": ["Large language models"], "Paper 3": ["Dense retrieval models"], "Paper 4": ["Large language models, family of reranker models"], "Paper 5": ["Soft prompt-tuned large language models"], "Paper 6": ["Dense retrieval models, pre-trained language model"]}, "Use of LLMs in information retrieval tasks - Pretraining objectives and datasets": {"Paper 1": ["MS MARCO dataset"], "Paper 2": ["Large language models pretraining, no mention of specific dataset"], "Paper 3": ["Leverage models trained for other NLP tasks like summarization"], "Paper 4": ["Generation of synthetic queries using an LLM"], "Paper 5": ["Use LLMs trained on large-scale datasets like MS MARCO"], "Paper 6": ["Generic initialization from a pre-trained language model"]}, "Use of LLMs in information retrieval tasks - Fine-tuning methodologies": {"Paper 1": ["Finetuning on unsupervised and synthetic datasets"], "Paper 2": ["Few-shot guidance, finetuning on generated data from Promptagator"], "Paper 3": ["Training with pseudo query-document pairs generated through query extraction and transferred query generation"], "Paper 4": ["Large number of synthetic queries are used to finetune reranker models"], "Paper 5": ["Soft prompt-tuning for task-specific dynamic prompts"], "Paper 6": ["Autoencoding approach with passage-retrieval autoencoding scheme"]}, "Use of LLMs in information retrieval tasks - Data augmentation techniques": {"Paper 1": ["Generates synthetic data for IR tasks using large pretrained language models"], "Paper 2": ["Prompt-based query generation using LLMs"], "Paper 3": ["Creation of pseudo query-document pairs"], "Paper 4": ["Synthetic query generation with large and then less expensive LLMs"], "Paper 5": ["Prompting LLMs with soft prompts to generate weak queries"], "Paper 6": ["Autoencoding approach for unsupervised learning of query and passage encoders"]}, "Use of LLMs in information retrieval tasks - Information retrieval tasks evaluated": {"Paper 1": ["Zero-shot transfer learning for various IR tasks"], "Paper 2": ["Few-shot Dense Retrieval"], "Paper 3": ["Unsupervised dense retrieval on BEIR and ODQA datasets"], "Paper 4": ["Zero-shot accuracy on long-tail domains for information retrieval tasks"], "Paper 5": ["Improving zero-shot and few-shot dense retrieval models"], "Paper 6": ["QA retrieval benchmarks within Open QA systems"]}, "schemes": ["Model architecture and size", "Pretraining objectives and datasets", "Fine-tuning methodologies", "Data augmentation techniques", "Information retrieval tasks evaluated"]}, "focus on few-shot or zero-shot learning capabilities": {"Learning approach (focus on few-shot or zero-shot learning capabilities)": {"paper_1": ["few-shot and zero-shot"], "paper_2": ["few-shot"], "paper_3": ["zero-shot"], "paper_4": ["zero-shot"], "paper_5": ["few-shot and zero-shot"], "paper_6": ["zero-shot"]}, "Domain of application": {"paper_1": ["information retrieval (IR)"], "paper_2": ["information retrieval (IR)"], "paper_3": ["text retrieval and open-domain question answering"], "paper_4": ["information retrieval tasks in long-tail domains"], "paper_5": ["dense retrieval (DR)"], "paper_6": ["open-domain tasks, Open QA"]}, "Data augmentation techniques": {"paper_1": ["synthetic data generation"], "paper_2": ["prompt-based query generation"], "paper_3": ["pseudo query-document pairs"], "paper_4": ["generation of synthetic queries"], "paper_5": ["soft prompt tuning for augmenting DR"], "paper_6": ["corpus-level autoencoding"]}, "Use of large language models": {"paper_1": ["yes"], "paper_2": ["yes"], "paper_3": ["yes"], "paper_4": ["yes"], "paper_5": ["yes"], "paper_6": ["yes"]}, "Approach to prompt engineering/tuning": {"paper_1": ["N/A"], "paper_2": ["prompt-based query generation"], "paper_3": ["transferred query generation"], "paper_4": ["initial generation of a small number of synthetic queries"], "paper_5": ["soft prompt tuning"], "paper_6": ["N/A"]}, "schemes": ["Learning approach (few-shot vs zero-shot)", "Domain of application", "Data augmentation techniques", "Use of large language models", "Approach to prompt engineering/tuning"]}, "development of methods to generate data (queries or query-document pairs) for information retrieval": {"Data augmentation techniques for generating data for information retrieval": {"paper_1": ["Unsupervised dataset generation for training IR models"], "paper_2": ["Prompt-base Query Generation for Retriever (Promptagator)"], "paper_3": ["Query extraction and transferred query generation"], "paper_4": ["LLM to generate synthetic queries for reranker models"], "paper_5": ["Soft prompt tuning for augmenting DR (SPTAR) with weak queries"], "paper_6": ["Passage-retrieval autoencoding (ART)"]}, "Usage of large language models (LLMs) for generating data for information retrieval": {"paper_1": ["Large pretrained language models as synthetic data generators"], "paper_2": ["Use of LLMs for a few-shot query generator"], "paper_3": ["Generation models trained for other NLP tasks"], "paper_4": ["Large and smaller LLMs to generate synthetic queries"], "paper_5": ["Soft prompt-tuning of LLMs to tag documents with weak queries"], "paper_6": ["Generic initialization from a pre-trained language model"]}, "Level of supervision required for generating data for information retrieval": {"paper_1": ["Few-shot"], "paper_2": ["Few-shot"], "paper_3": ["Unsupervised"], "paper_4": ["Starts with few-shot, scales to unsupervised"], "paper_5": ["Few-shot"], "paper_6": ["Unsupervised"]}, "Applicability of few-shot learning in generating data for information retrieval": {"paper_1": ["Capable of few-shot capabilities"], "paper_2": ["Few examples for task-specific retriever creation"], "paper_3": ["N/A"], "paper_4": ["Applicable for initial synthetic queries generation"], "paper_5": ["Applies few-shot learning for soft prompt tuning"], "paper_6": ["N/A"]}, "Scalability of the method for generating data for information retrieval": {"paper_1": ["Scalable to various IR tasks"], "paper_2": ["Scalable; does not require training on large datasets like MS MARCO"], "paper_3": ["Scalable; suitable for large-scale unsupervised training"], "paper_4": ["Scalable; uses less expensive LLM for large-scale query generation"], "paper_5": ["Scalable through the use of soft prompts"], "paper_6": ["Scalable; only requires unpaired inputs and outputs for training"]}, "schemes": ["Data augmentation techniques", "Usage of large language models", "Supervision level", "Few-shot learning applicability", "Scalability"]}, "aim to improve performance of retrieval systems without relying on large labeled datasets": {"Method of Data Augmentation aimed to improve performance of retrieval systems without relying on large labeled datasets": {"paper_1": ["Generating synthetic data for IR tasks using few-shot capabilities of LLMs"], "paper_2": ["Creating task-specific retrievers with Prompt-based Query Generation using LLMs"], "paper_3": ["Producing pseudo query-document pairs through query extraction and transferred query generation"], "paper_4": ["Generating synthetic queries with LLMs for the fine-tuning of reranker models"], "paper_5": ["Soft prompt tuning to generate weak queries with LLMs for task-specific DR models"], "paper_6": ["Passage-retrieval autoencoding scheme for unsupervised learning"]}, "Use of Large Language Models (LLMs) aimed to improve performance of retrieval systems without relying on large labeled datasets": {"paper_1": ["Yes, for unsupervised data generation"], "paper_2": ["Yes, as a few-shot query generator in Promptagator"], "paper_3": ["Yes, for creating pseudo queries through transferred query generation"], "paper_4": ["Yes, for initial generation of synthetic queries"], "paper_5": ["Yes, for generating weak queries and soft prompt tuning"], "paper_6": ["N/A, does not explicitly mention LLMs but uses a pre-trained language model for initialization"]}, "Few-shot Learning Capabilities aimed to improve performance of retrieval systems without relying on large labeled datasets": {"paper_1": ["Yes, finetuning models on minimal synthetic data for better performance"], "paper_2": ["Yes, enabling end-to-end retrieval with minimal LLM prompting"], "paper_3": ["N/A, focuses on unsupervised methods without explicit few-shot setting"], "paper_4": ["N/A, focuses on using synthetic queries for fine-tuning without specific mention of few-shot capabilities"], "paper_5": ["Yes, working with limited ground truth data"], "paper_6": ["N/A, focuses on unsupervised corpus-level autoencoding without explicit few-shot setting"]}, "Scalability of Augmentation aimed to improve performance of retrieval systems without relying on large labeled datasets": {"paper_1": ["High, through automated synthetic data generation"], "paper_2": ["High, LLMs can generate large data from few examples"], "paper_3": ["High, scalable methods for creating pseudo queries"], "paper_4": ["High, iteratively using cheaper LLMs to scale up query generation"], "paper_5": ["High, through soft prompt tuning and LLM tagging for weak queries"], "paper_6": ["High, only requires unpaired inputs and outputs for autoencoding"]}, "Unsupervised Learning Approaches aimed to improve performance of retrieval systems without relying on large labeled datasets": {"paper_1": ["Yes, using unsupervised synthetic dataset for training"], "paper_2": ["N/A, few-shot learning but not explicitly unsupervised"], "paper_3": ["Yes, both methods are annotation-free"], "paper_4": ["N/A, method itself is not unsupervised although it generates data for unsupervised fine-tuning"], "paper_5": ["N/A, soft prompt tuning requires limited ground truth data"], "paper_6": ["Yes, corpus-level autoencoding with unsupervised learning"]}, "schemes": ["Method of Data Augmentation", "Use of Large Language Models (LLMs)", "Few-shot Learning Capabilities", "Scalability of Augmentation", "Unsupervised Learning Approaches"]}}}}, "set_3": {"gold_data": {"caption": "The comparison of retrievers that leverage LLMs as the foundation.", "mention_paragraphs": ["To provide a comprehensive understanding of this topic, Table 4 summarizes the common and unique characteristics of the LLM-based retrievers discussed above."], "glossary": {"Methods": "The method introduced by the paper", "Backbone": {"text": "The model that the LLM-based retriever is built off of", "notes": "guess"}, "Architecture": "The architecture of the model used for the LLM-based retriever.", "Encoder-based": "LLMs, like the T5-family of models, are used to embed passages which are then placed in an index.", "Generative": "LLMs generate document identifiers or URLs directly rather than using the 'index-retrieval-rank' paradigm.", "LLM's tuning": "How LLM's need to be tuned for the paper's method.", "Training from scratch": {"text": "The model has to be trained from scratch", "notes": "guess"}, "Fine-tuning": {"text": "The model has to be fine-tuned", "notes": "guess"}, "Fine-tuning & Prompting": {"text": "The model has to be fine-tuned and prompted in a specific way to perform retrieval", "notes": "guess"}, "Prompting": {"text": "The model does not have to be fine-tuned and can instead just be prompted", "notes": "guess"}}}, "generations": {"commonalities": ["Text embeddings are useful features in computing text similarity and semantic search", "Training models on unsupervised data can lead to high-quality vector representations", "The research includes methods to improve retrieval tasks for documents or information", "Models trained for retrieval tasks can achieve state-of-the-art performance"], "tables": {"Text embeddings are useful features in computing text similarity and semantic search": {"Embedding generation method": {"Paper_1": ["Contrastive pre-training on unsupervised data"], "Paper_2": ["Scaling up the size of the dual encoder model with multi-stage training"], "Paper_3": ["Multi-task instruction tuning"], "Paper_4": ["Transformer-based Differentiable Search Index (DSI)"], "Paper_5": ["Large language models (LLMs) with in-context demonstrations"]}, "Embedding dimensions": {"Paper_1": ["N/A"], "Paper_2": ["Diminishing returns with scaling up"], "Paper_3": ["N/A"], "Paper_4": ["N/A"], "Paper_5": ["N/A"]}, "Pre-training tasks": {"Paper_1": ["Unsupervised data pre-training"], "Paper_2": ["BEIR dataset training"], "Paper_3": ["Training on the BERRI dataset with instructions"], "Paper_4": ["Variations in document representation and training procedures"], "Paper_5": ["N/A"]}, "Downstream tasks evaluated": {"Paper_1": ["Semantic search, Linear-probe classification"], "Paper_2": ["Variety of retrieval tasks"], "Paper_3": ["Zero-shot retrieval"], "Paper_4": ["Generalization in zero-shot setup"], "Paper_5": ["Open-domain question answering"]}, "Evaluation metrics": {"Paper_1": ["Classification accuracy, Semantic search benchmarks (MSMARCO, Natural Questions, TriviaQA)"], "Paper_2": ["Out-of-domain retrieval performance"], "Paper_3": ["Adaptation to instructions, X^2-Retrieval"], "Paper_4": ["Performance against baselines and zero-shot generalization"], "Paper_5": ["Retrieval performance on open-domain QA benchmarks"]}, "schemes": ["Embedding generation method", "Embedding dimensions", "Pre-training tasks", "Downstream tasks evaluated", "Evaluation metrics"]}, "Training models on unsupervised data can lead to high-quality vector representations": {"Model architecture informed by unsupervised data for high-quality vector representations": {"paper_1": ["Contrastive pre-training model"], "paper_2": ["Scaled-up dual encoder model"], "paper_3": ["Multi-task retrieval system with instructions (TART)"], "paper_4": ["Transformer with Differentiable Search Index (DSI)"], "paper_5": ["Large language models (LLMs)"]}, "Pre-training tasks leveraging unsupervised data for high-quality vector representations": {"paper_1": ["Unsupervised contrastive pre-training"], "paper_2": ["Multi-stage training on retrieval tasks"], "paper_3": ["Multi-task instruction tuning on retrieval datasets"], "paper_4": ["Text-to-text model mapping queries to docids"], "paper_5": ["Instructions-guided retrieval leveraging in-context learning"]}, "Data domains used to achieve high-quality vector representations": {"paper_1": ["Text and code datasets"], "paper_2": ["Retrieval tasks from various domains"], "paper_3": ["Approximately 40 retrieval datasets with instructions (BERRI)"], "paper_4": ["Variations in documents and corpus sizes"], "paper_5": ["Web URLs for open-domain question answering"]}, "Contrastive learning components for high-quality vector representations": {"paper_1": ["Text and code embeddings"], "paper_2": ["Bottleneck layer with dot-product on embeddings"], "paper_3": ["Instruction-based retrieval tasks"], "paper_4": ["N/A"], "paper_5": ["N/A"]}, "Evaluation tasks used to assess the unsupervised-light models": {"paper_1": ["Linear-probe classification, semantic search"], "paper_2": ["BEIR benchmark for retrieval tasks"], "paper_3": ["Zero-shot retrieval benchmarks, BEIR and LOTTE"], "paper_4": ["Retrieval performance against strong baselines, zero-shot setup"], "paper_5": ["Open-domain question answering benchmarks"]}, "schemes": ["Model architecture", "Pre-training tasks", "Data domains", "Contrastive learning components", "Evaluation tasks"]}, "The research includes methods to improve retrieval tasks for documents or information": {"Retrieval model architecture": {"paper_1": ["Unsupervised contrastive pre-training of text and code embeddings"], "paper_2": ["Scaled dual encoder model with a fixed-size bottleneck layer"], "paper_3": ["Multi-task instruction tuning for task-aware retrieval"], "paper_4": ["Differentiable Search Index (DSI) with Transformer model"], "paper_5": ["In-context learning with large language models (LLMs) for direct URL generation"]}, "Contrastive pre-training techniques": {"paper_1": ["Contrastive pre-training on unsupervised data at scale"], "paper_2": ["Multi-stage training including contrastive learning"], "paper_3": ["N/A"], "paper_4": ["Contrastive learning variations in training procedures"], "paper_5": ["N/A"]}, "Encoding strategies for text and code": {"paper_1": ["Vector representations for both text and code"], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["Encoding information about the corpus in model parameters"], "paper_5": ["Using LLMs to generate document identifiers"]}, "Retrieval performance metrics": {"paper_1": ["Linear-probe classification accuracy, large-scale semantic search"], "paper_2": ["BEIR dataset retrieval performance"], "paper_3": ["Zero-shot retrieval benchmarks like BEIR and LOTTE, X^2-Retrieval evaluation"], "paper_4": ["Performance against dual encoder models and BM25 baseline in zero-shot setup"], "paper_5": ["Zero and few-shot settings on open-domain question answering benchmarks"]}, "Generalizability across different domains": {"paper_1": ["Generalizability highlighted by performance on various benchmarks"], "paper_2": ["Significant out-of-domain generalization with less supervised data"], "paper_3": ["Strong adaptation to new retrieval tasks via instructions"], "paper_4": ["Strong generalization capabilities demonstrated"], "paper_5": ["Generalizability in zero and few-shot settings for open-domain QA"]}, "schemes": ["Retrieval model architecture", "Contrastive pre-training techniques", "Encoding strategies for text and code", "Retrieval performance metrics", "Generalizability across different domains"]}, "Models trained for retrieval tasks can achieve state-of-the-art performance": {"Model architecture": {"paper_1": ["Unsupervised text and code embeddings with contrastive pre-training"], "paper_2": ["Scaled-up dual encoder model with fixed-size bottleneck layer"], "paper_3": ["TART: Task-aware retrieval system with multi-task instruction tuning"], "paper_4": ["Differentiable Search Index (DSI) with Transformer model"], "paper_5": ["Large language models (LLMs) with in-context learning for retrieval"]}, "Pre-training objectives": {"paper_1": ["Contrastive pre-training on unsupervised data"], "paper_2": ["Multi-stage training, with a single dot-product bottleneck layer"], "paper_3": ["Multi-task instruction tuning on BERRI dataset"], "paper_4": ["Encoding corpus information in model parameters for retrieval"], "paper_5": ["No explicit mapping pre-training; retrieval performed by LLMs with in-context demonstrations"]}, "Datasets used for training and evaluation": {"paper_1": ["Unspecified large-scale unsupervised datasets; MSMARCO, Natural Questions, and TriviaQA for evaluation"], "paper_2": ["MSMARCO for training; BEIR dataset for evaluation"], "paper_3": ["BERRI dataset for training; BEIR and LOTTE for zero-shot evaluation"], "paper_4": ["Unspecified datasets for training and evaluation; comparison with dual encoder models and BM25 baseline"], "paper_5": ["Utilizes in-context Query-URL pairs; evaluated on three open-domain question answering benchmarks"]}, "Retrieval mechanisms": {"paper_1": ["Linear-probe classification and large-scale semantic search capabilities"], "paper_2": ["Generalizable T5-based dense Retrievers (GTR)"], "paper_3": ["Guided retrieval with human-written instructions"], "paper_4": ["Text-to-text model mapping queries directly to docids, using model parameters"], "paper_5": ["Generation of Web URLs for document retrieval through in-context learning by LLMs"]}, "Performance metrics": {"paper_1": ["Relative improvement over previous models in unsupervised and supervised settings; 23.4%, 14.7%, and 10.6% relative improvement on benchmarks"], "paper_2": ["Outperforms previous sparse and dense retrievers; data efficiency with out-of-domain performance"], "paper_3": ["Zero-shot retrieval performance outperforming models up to three times larger"], "paper_4": ["Outperforms dual encoder models and BM25 baseline in zero-shot setup"], "paper_5": ["Significant margin improvement on three open-domain QA benchmarks under zero and few-shot settings; nearly 90% correct answer retrieval"]}, "schemes": ["Model architecture", "Pre-training objectives", "Datasets used for training and evaluation", "Retrieval mechanisms", "Performance metrics"]}}}}}}