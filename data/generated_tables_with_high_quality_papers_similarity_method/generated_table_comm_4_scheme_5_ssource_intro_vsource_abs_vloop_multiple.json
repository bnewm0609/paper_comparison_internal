{"experiment_type": {"scheme_source": "intro", "commonality_num": 4, "attribute_num": 5, "value_source": "abs", "value_paper_loop": "multiple"}, "data": {"set_0": {"gold_data": {"caption": "Comparisons of public VQA and video classification datasets. Note that obtaining reliable quality annotations requires massive subjective annotations per video, which limits the size of VQA datasets.", "mention_paragraphs": ["DNN-based VQA methods are hindered by the limited scale of existing VQA datasets. As demonstrated in Tab. 1, public VQA datasets are significantly smaller in size when compared to video classification datasets (Karpathy et al., 2014; Kay et al., 2017). The primary reason for this discrepancy is the prevalent use of crowdsourcing to annotate subjective video quality (Chen et al., 2010; Ho\u00dffeld et al., 2014; Shahid et al., 2014; Chen et al., 2015), which is a time-consuming yet indispensable process in eliminating randomness and enhancing consistency. For example, the KoNViD-1k dataset (Hosu et al., 2017) requires an average of 114 subjective scores to produce a valid label. Recently, there have been attempts to scale up VQA datasets (Ying et al., 2021; G\u00f6tz-Hahn et al., 2021). However, these efforts often sacrifice the annotation quality, as they reduce the average number of annotations per video in order to improve efficiency. For instance, an experiment conducted on the KoNViD-150k dataset (G\u00f6tz-Hahn et al., 2021) revealed that the correlation coefficient of SRCC between randomly sampled five annotations is 0.8, while it is 0.9 for fifty annotations. Thus, in this paper, we primarily focus on enhancing the performance of VQA under the circumstance of limited availability of high-quality annotated data."], "glossary": {"dataset": "The dataset the paper uses for to evaluate its task.", "task": "The task the dataset was designed for.", "VQA": "An acronym that stands for video quality assessment.", "classification": "The video classification task.", "Annotations": "The number of annotations per video.", "Size": "The number of annotated videos in the dataset"}}, "generations": {"commonalities": ["focus on video content analysis", "use of video quality assessment (VQA)", "inclusion of user-generated content (UGC)", "application of deep learning and convolutional neural networks"], "tables": {"focus on video content analysis": {"Video Database Size": {"paper_1": ["1,200 video sequences"], "paper_2": ["585 videos"], "paper_3": ["1500 20 sec video clips"], "paper_4": ["39,000 videos, 117,000 v-patches"], "paper_5": ["153,841 videos coarsely rated, 1,596 videos with a minimum of 89 ratings each"], "paper_6": ["1 million YouTube videos"], "paper_7": ["At least 400 clips for each of 400 actions"]}, "Quality Assessment Methodology": {"paper_1": ["Subjective mean opinion scores (MOS)"], "paper_2": ["Subjective video quality scores via crowdsourcing"], "paper_3": ["No-reference objective quality metrics (Noise, Banding, and SLEEQ)"], "paper_4": ["Subjective video quality dataset, two unique NR-VQA models (PVQ, PVQ Mapper)"], "paper_5": ["Coarse and detailed video quality ratings, new VQA approaches (MLSP-VQA)"], "paper_6": ["Spatio-temporal information in CNNs, large-scale video classification"], "paper_7": ["Neural network architectures trained and tested for human action classification"]}, "Dataset Diversity": {"paper_1": ["Wide variety of 'in the wild' content and authentic distortions"], "paper_2": ["Unique content, capture devices, distortion types and combinations"], "paper_3": ["Gaming, Sports, High Dynamic Range (HDR) categories"], "paper_4": ["Real-world distorted videos and localized 'v-patches'"], "paper_5": ["A large and diverse set of 'in-the-wild' videos"], "paper_6": ["Videos belonging to 487 classes"], "paper_7": ["400 human action classes, human-object and human-human interactions"]}, "Types of Video Content Analyzed": {"paper_1": ["Public-domain video sequences"], "paper_2": ["Authentic distortions in real-world videos"], "paper_3": ["User Generated Content (UGC) from YouTube"], "paper_4": ["User-generated content (UGC)"], "paper_5": ["A variety of 'in-the-wild' videos"], "paper_6": ["Large-scale video classification on YouTube videos"], "paper_7": ["Human action video dataset, human-object and human-human interaction"]}, "Subjective vs. Objective Quality Measures": {"paper_1": ["Subjective"], "paper_2": ["Subjective"], "paper_3": ["Objective"], "paper_4": ["Subjective evaluations with NR-VQA models"], "paper_5": ["Subjective and objective (proposed MLSP-VQA models)"], "paper_6": ["Objective with CNN evaluation"], "paper_7": ["Objective (baseline performance for neural network architectures)"]}, "schemes": ["Video Database Size", "Quality Assessment Methodology", "Dataset Diversity", "Types of Video Content Analyzed", "Subjective vs. Objective Quality Measures"]}, "use of video quality assessment (VQA)": {"Database or dataset used": {"paper_1": ["KoNViD-1k"], "paper_2": ["LIVE Video Quality Challenge Database (LIVE-VQC)"], "paper_3": ["Large scale UGC dataset from YouTube videos"], "paper_4": ["Subjective video quality dataset with 39,000 videos and 117,000 v-patches"], "paper_5": ["KonVid-150k"], "paper_6": ["New dataset of 1 million YouTube videos"], "paper_7": ["DeepMind Kinetics human action video dataset"]}, "Type of video quality assessment": {"paper_1": ["Subjective"], "paper_2": ["Subjective"], "paper_3": ["No-reference"], "paper_4": ["No-reference, Subjective"], "paper_5": ["No-reference, Subjective"], "paper_6": ["Not explicitly stated (likely objective given the context of CNN classification)"], "paper_7": ["Not explicitly stated (likely objective given the context of human action classification)"]}, "Methodology or approach": {"paper_1": ["Development of a large VQA database for training and validating \u2018general purpose\u2019 VQA methods"], "paper_2": ["Large-scale subjective studies and comparison of NR video quality predictors"], "paper_3": ["A novel sampling method based on features extracted from encoding, evaluation with no-reference quality metrics"], "paper_4": ["Creation of two unique NR-VQA models: PVQ and PVQ Mapper"], "paper_5": ["Introducing MLSP-VQA models using multi-level spatially pooled deep-features"], "paper_6": ["Empirical evaluation of CNNs for video classification, suggestion of a multiresolution foveated architecture"], "paper_7": ["Collection and baseline performance figures for neural network architectures, analysis of bias in classifiers"]}, "Evaluation metrics": {"paper_1": ["Mean opinion scores (MOS)"], "paper_2": ["Subjective video quality scores via crowdsourcing"], "paper_3": ["No-reference metrics (Noise, Banding, SLEEQ)"], "paper_4": ["Human perceptual quality annotations"], "paper_5": ["Spearman rank-order correlation coefficient (SRCC)"], "paper_6": ["Performance improvements over the UCF-101 baseline model"], "paper_7": ["Baseline performance figures for trained neural network architectures"]}, "Scale of the study": {"paper_1": ["Large-scale (1,200 video sequences)"], "paper_2": ["Large-scale (585 videos, 4776 participants, over 205,000 opinion scores)"], "paper_3": ["Large-scale (1500 20 sec video clips)"], "paper_4": ["Large-scale (39,000 videos, 117,000 v-patches, 5.5M annotations)"], "paper_5": ["Large-scale (153,841 videos with five ratings each, and 1,596 videos with a minimum of 89 ratings)"], "paper_6": ["Large-scale (1 million videos)"], "paper_7": ["Large-scale (at least 400 clips for each of 400 actions)"]}, "schemes": ["Database or dataset used", "Type of video quality assessment (e.g., subjective, objective, no-reference)", "Methodology or approach", "Evaluation metrics", "Scale of the study (e.g., small-scale, large-scale)"]}, "inclusion of user-generated content (UGC)": {"source_of_UGC": {"paper_1": ["YFCC100m dataset"], "paper_2": ["Captured by numerous users, link provided"], "paper_3": ["Sampled from YouTube videos"], "paper_4": ["Real-world distorted videos"], "paper_5": ["Coarsely annotated set from a large collection"], "paper_6": ["1 million YouTube videos"], "paper_7": ["YouTube videos"]}, "dataset_size": {"paper_1": ["1,200 video sequences"], "paper_2": ["585 videos"], "paper_3": ["1,500 20 sec video clips"], "paper_4": ["39,000 videos and 117,000 v-patches"], "paper_5": ["153,841 videos, 1,596 with detailed ratings"], "paper_6": ["1 million videos"], "paper_7": ["At least 400 video clips for each of 400 actions"]}, "video_quality_metrics_used": {"paper_1": ["Subjective mean opinion scores (MOS)"], "paper_2": ["Subjective video quality scores"], "paper_3": ["BD-Rate, PSNR, and no-reference objective quality metrics (Noise, Banding, SLEEQ)"], "paper_4": ["Human perceptual quality annotations"], "paper_5": ["Five quality ratings each video, Spearman rank-order correlation coefficient"], "paper_6": ["Performance improvements compared to feature-based baselines"], "paper_7": ["Baseline performance figures for neural network architectures"]}, "domain_of_application": {"paper_1": ["Video quality assessment (VQA) for deep learning"], "paper_2": ["No-reference (NR) video quality prediction"], "paper_3": ["UGC compression and quality evaluation"], "paper_4": ["No-reference (NR) perceptual video quality assessment"], "paper_5": ["In-the-wild video quality assessment"], "paper_6": ["Large-scale video classification"], "paper_7": ["Human action classification"]}, "annotation_methodology": {"paper_1": ["Subjective annotation", "Crowdsourcing"], "paper_2": ["Crowdsourcing with 4776 participants providing over 205000 opinion scores"], "paper_3": ["Novel sampling method based on features extracted from encoding"], "paper_4": ["5.5M human perceptual quality annotations collected"], "paper_5": ["Annotation based on coarse number of ratings and detailed ratings in a subset"], "paper_6": ["Not explicitly mentioned"], "paper_7": ["Not explicitly mentioned"]}, "schemes": ["source_of_UGC", "dataset_size", "video_quality_metrics_used", "domain_of_application", "annotation_methodology"]}, "application of deep learning and convolutional neural networks": {"Dataset": {"paper_1": ["KoNViD-1k - 1,200 public-domain video sequences with real-world distortions"], "paper_2": ["LIVE Video Quality Challenge Database (LIVE-VQC) - 585 videos with authentic distortions"], "paper_3": ["Large scale UGC dataset - 1500 20 sec video clips from YouTube"], "paper_4": ["Subjective video quality dataset - 39,000 real-world distorted videos and 117,000 video patches ('v-patches')"], "paper_5": ["KonVid-150k - 153,841 videos with coarse quality ratings, 1,596 with detailed ratings"], "paper_6": ["New dataset - 1 million YouTube videos belonging to 487 classes"], "paper_7": ["DeepMind Kinetics - 400 human action classes, at least 400 clips each"]}, "Task": {"paper_1": ["Development and evaluation of 'general purpose' VQA methods"], "paper_2": ["Advancing NR video quality prediction"], "paper_3": ["Compression and quality assessment challenges for UGC"], "paper_4": ["NR perceptual VQA, local-to-global region-based VQA architecture (PVQ), space-time video quality mapping (PVQ Mapper)"], "paper_5": ["Introducing efficient VQA approaches (MLSP-VQA), evaluation of in-the-wild video quality"], "paper_6": ["Large-scale video classification, spatio-temporal connectivity of CNNs"], "paper_7": ["Human action classification, analyzing dataset imbalance effects"]}, "Deep Learning Model Types": {"paper_1": ["N/A"], "paper_2": ["N/A"], "paper_3": ["N/A"], "paper_4": ["Local-to-global region-based NR VQA architecture (PVQ), Space-time video quality mapping engine (PVQ Mapper)"], "paper_5": ["MLSP-VQA - Deep learning models employing multi-level spatially pooled features"], "paper_6": ["CNNs, spatio-temporal network architecture, multiresolution foveated architecture"], "paper_7": ["Neural network architectures for human action classification"]}, "Evaluation Metrics": {"paper_1": ["Subjective mean opinion scores (MOS)"], "paper_2": ["Subjective video quality scores via crowdsourcing"], "paper_3": ["No-reference objective quality metrics (Noise, Banding, SLEEQ)"], "paper_4": ["Human perceptual quality annotations"], "paper_5": ["Spearman rank-order correlation coefficient (SRCC)"], "paper_6": ["Performance improvements comparison to baselines"], "paper_7": ["Baseline performance figures for training and testing neural network architectures"]}, "Input Modality": {"paper_1": ["Video"], "paper_2": ["Video"], "paper_3": ["Video"], "paper_4": ["Real-world distorted videos, space-time localized video patches ('v-patches')"], "paper_5": ["In-the-wild video data"], "paper_6": ["Large-scale video data"], "paper_7": ["Video clips from YouTube"]}, "schemes": ["Dataset", "Task", "Deep Learning Model Types", "Evaluation Metrics", "Input Modality"]}}}}, "set_1": {"gold_data": {"caption": "Physical adversarial attacks against Semantic Segmentation tasks. Attacker's knowledge, Robustness technique, Stealthiness technique, Physical test type, and Space.", "mention_paragraphs": ["Table XII presents a comprehensive comparison of various adversarial attack methods in the semantic segmentation task. It provides an overview of their attack goals, patch placement strategies, consideration of changing viewpoints, testing in the physical domain, and transferability to other models. Table XIII offers detailed information on adversarial attacks, including the attacker\u2019s knowledge level, robustness techniques, stealthiness techniques, physical test types, and space of operation. Table XIV provides information on the datasets used, the evaluated networks, and the links to open-source code for the experiments conducted in the semantic segmentation task."], "glossary": {"IPatch": "An adversarial attack that targets a particular region in an image by inserting a adversarially optimized patch.", "Attacker's Knowledge": "The knowledge required by the attacker. Can be one of White-box, Black-box or Grey-box", "White-box": "refer to adversarial attacks where the attacker possesses complete knowledge of the training and testing data used to train the victim model, as well as the architecture and parameters of the target model.", "Robustness Technique": "Techniques for maintaining robustness to attacks.", "EOT": "Expectation of Transformation. for adversarial attacks, which takes potential transformations in the real world into account during the optimization, resulting in better robustness. EOT adds random distortions in the optimization to make the perturbation more robust.", "Stealthiness Technique": "Techniques for evading detection that the attack was used.", "Static": "Attacks that do not change once inserted into the scene.", "Space": "Whether the attack takes place in two dimensions (eg like a sticker) or three (eg an object)", "2D": "The attack takes place in two dimensions"}}, "generations": {"commonalities": ["Deep learning and convolutional neural networks are widely used in image-based tasks", "Semantic segmentation and object detection are key components in autonomous driving systems", "Adversarial attacks pose a threat to the reliability of deep learning models in safety-critical applications like autonomous driving", "Adversarial patches are a form of real-world adversarial examples that can be physically placed in an environment to fool neural networks"], "tables": {"Deep learning and convolutional neural networks are widely used in image-based tasks": {"Task performed": {"paper_1": ["Evaluating the impact of remote adversarial patches (RAP) on deep learning models for object localization and identification"], "paper_2": ["Assessing the robustness of semantic segmentation (SS) models against digital and real-world adversarial patches"]}, "Dataset used": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset, CARLA driving simulator for real 3D environment"]}, "Network architecture": {"paper_1": ["Five state-of-the-art architectures with eight different encoders, YOLOv3 model"], "paper_2": ["Popular SS models (specific models not mentioned in the abstract)"]}, "Evaluation metrics": {"paper_1": ["Success rate of changing the classification of a remote target region"], "paper_2": ["Effectiveness of attack formulations, Transferability of the EOT-based attack, Real-world effectiveness"]}, "Adversarial attack method": {"paper_1": ["IPatch implementing remote adversarial patches"], "paper_2": ["EOT-based attack extended for SS, novel loss function, scene-specific attack leveraging CARLA driving simulator"]}, "schemes": ["Task performed", "Dataset used", "Network architecture", "Evaluation metrics", "Adversarial attack method"]}, "Semantic segmentation and object detection are key components in autonomous driving systems": {"Adversarial robustness": {"paper_1": ["Introduced remote adversarial patches (RAP) that alter model perception of image semantics, successful attacks with up to 93% success rate on average"], "paper_2": ["Proposed a novel attack optimization leveraging the CARLA driving simulator, with active exploration of both digital and real-world adversarial patches; attacks are notably less effective in real-world scenarios"]}, "Segmentation accuracy": {"paper_1": ["Performed an in-depth analysis on image segmentation attacks using state-of-the-art architectures"], "paper_2": ["Conducted an investigation on the Cityscapes dataset, including an evaluation of segmentation model robustness against digital and real-world patches"]}, "Object detection precision": {"paper_1": ["Demonstrated preliminary results on YOLOv3 model for adversarial attacks"], "paper_2": ["N/A"]}, "Real-world applicability for semantic segmentation and object detection in autonomous driving systems": {"paper_1": ["N/A"], "paper_2": ["Tested a printed physical billboard with an adversarial patch in an outdoor driving scenario, questioning the practical relevance of adversarial attacks"]}, "Methodology": {"paper_1": ["Introduced IPatch for RAPs, evaluated on CamVid dataset with various architectures and encoders"], "paper_2": ["Extended Expectation Over Transformation (EOT) paradigm, scene-specific attack optimization; tested with Cityscapes dataset and CARLA driving simulator"]}, "schemes": ["Adversarial robustness", "Segmentation accuracy", "Object detection precision", "Real-world applicability", "Methodology"]}, "Adversarial attacks pose a threat to the reliability of deep learning models in safety-critical applications like autonomous driving": {"Attack vector": {"paper_1": ["Remote adversarial patches (RAP), specifically IPatch, altering image semantics and classification"], "paper_2": ["Digital and real-world adversarial patches, scene-specific attacks, EOT paradigm extension"]}, "Evaluation metrics": {"paper_1": ["Success rate of changing the classification of a remote target region"], "paper_2": ["Performance of attack formulations against SS models, effectiveness in real-world conditions"]}, "Model architectures affected": {"paper_1": ["Five state-of-the-art image segmentation architectures with eight different encoders, YOLOv3 object recognition model"], "paper_2": ["Popular semantic segmentation (SS) models"]}, "Datasets used": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset, CARLA driving simulator for attack transferability evaluation"]}, "Robustness techniques evaluated": {"paper_1": ["N/A"], "paper_2": ["Feasibility of attacks in real-world outdoor driving scenario"]}, "schemes": ["Attack vector", "Evaluation metrics", "Model architectures affected", "Datasets used", "Robustness techniques evaluated"]}, "Adversarial patches are a form of real-world adversarial examples that can be physically placed in an environment to fool neural networks": {"Target model architectures": {"paper_1": ["five state-of-the-art architectures with eight different encoders", "YOLOv3"], "paper_2": ["popular semantic segmentation (SS) models, no specific architecture mentioned"]}, "Patch generation methodologies": {"paper_1": ["remote adversarial patches (RAP) implementing IPatch"], "paper_2": ["digital and real-world adversarial patches", "Expectation Over Transformation (EOT) paradigm", "scene-specific attack"]}, "Robustness evaluation metrics": {"paper_1": ["success rate of up to 93% on average"], "paper_2": ["in-depth evaluation", "robustness", "experiments and results", "questioning the practical relevance"]}, "Real-world applicability": {"paper_1": ["image segmentation RAP attacks", "object recognition models", "extended to YOLOv3"], "paper_2": ["real-world adversarial examples (RWAEs), physical objects like billboards", "tested with a printed physical billboard in an outdoor driving scenario"]}, "Datasets used": {"paper_1": ["CamVid street view dataset"], "paper_2": ["Cityscapes dataset", "CARLA driving simulator"]}, "schemes": ["Target model architectures", "Patch generation methodologies", "Robustness evaluation metrics", "Real-world applicability ", "Datasets used"]}}}}}}