{"id": 12552176, "updated": "2023-09-27 23:00:53.036", "metadata": {"title": "FlowNet: Learning Optical Flow with Convolutional Networks", "authors": "[{\"first\":\"Philipp\",\"last\":\"Fischer\",\"middle\":[]},{\"first\":\"Alexey\",\"last\":\"Dosovitskiy\",\"middle\":[]},{\"first\":\"Eddy\",\"last\":\"Ilg\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Hausser\",\"middle\":[]},{\"first\":\"Caner\",\"last\":\"Hazirbacs\",\"middle\":[]},{\"first\":\"Vladimir\",\"last\":\"Golkov\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Smagt\",\"middle\":[\"van\",\"der\"]},{\"first\":\"Daniel\",\"last\":\"Cremers\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Brox\",\"middle\":[]}]", "venue": "2015 IEEE International Conference on Computer Vision (ICCV)", "journal": "2015 IEEE International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2015, "month": 4, "day": 26}, "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1504.06852", "mag": "2951309005", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/DosovitskiyFIHH15", "doi": "10.1109/iccv.2015.316"}}, "content": {"source": {"pdf_hash": "2e80ce889fa47bae8583f89d501a41e283c1551b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1504.06852v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1504.06852", "status": "GREEN"}}, "grobid": {"id": "ea8eac4a38803b82a24a2f194d64b6066ea12295", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2e80ce889fa47bae8583f89d501a41e283c1551b.txt", "contents": "\nFlowNet: Learning Optical Flow with Convolutional Networks\n\n\nPhilipp Fischer \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nAlexey Dosovitskiy \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nEddy Ilg ilg@cs.uni-freiburg.de \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nPhilip H\u00e4usser \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nCaner Haz\u0131rba\u015f \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nVladimir Golkov golkov@cs.tum.edu \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nPatrick Van Der Smagt \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nDaniel Cremers cremers@tum.de \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nThomas Brox brox@cs.uni-freiburg.de \nUniversity of Freiburg Technical University of Munich\nTechnical University of Munich\nTechnical University of Munich\nUniversity of Freiburg\n\n\nFlowNet: Learning Optical Flow with Convolutional Networks\n\nConvolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations.Since existing ground truth datasets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.\n\nIntroduction\n\nConvolutional neural networks have become the method of choice in many fields of computer vision. They are classically applied to classification [25,24], but recently presented architectures also allow for per-pixel predictions like semantic segmentation [28] or depth estimation from single images [10]. In this paper, we propose training CNNs endto-end to learn predicting the optical flow field from a pair of images.\n\nWhile optical flow estimation needs precise per-pixel localization, it also requires finding correspondences between two input images. This involves not only learning image feature representations, but also learning to match them at different locations in the two images. In this respect, optical * Supported by the Deutsche Telekom Stiftung \u2021 These authors contributed equally Figure 1. We present neural networks which learn to estimate optical flow, being trained end-to-end. The information is first spatially compressed in a contractive part of the network and then refined in an expanding part.\n\nflow estimation fundamentally differs from previous applications of CNNs.\n\nSince it was not clear whether this task could be solved with a standard CNN architecture, we additionally developed an architecture with a correlation layer that explicitly provides matching capabilities. This architecture is trained end-to-end. The idea is to exploit the ability of convolutional networks to learn strong features at multiple levels of scale and abstraction and to help it with finding the actual correspondences based on these features. The layers on top of the correlation layer learn how to predict flow from these matches. Surprisingly, helping the network this way is not necessary and even the raw network can learn to predict optical flow with competitive accuracy.\n\nTraining such a network to predict generic optical flow requires a sufficiently large training set. Although data augmentation does help, the existing optical flow datasets are still too small to train a network on par with state of the art.\n\nGetting optical flow ground truth for realistic video material is known to be extremely difficult [7]. Trading in realism for quantity, we generate a synthetic Flying Chairs dataset which consists of random background images from Flickr on which we overlay segmented images of chairs from [1]. These data have little in common with the real world, but we can generate arbitrary amounts of samples with custom properties. CNNs trained on just these data generalize surprisingly well to realistic datasets, even without fine-tuning.\n\nLeveraging an efficient GPU implementation of CNNs, our method is faster than most competitors. Our networks predict optical flow at up to 10 image pairs per second on the full resolution of the Sintel dataset, achieving state-ofthe-art accuracy among real-time methods.\n\n\nRelated Work\n\nOptical Flow. Variational approaches have dominated optical flow estimation since the work of Horn and Schunck [19]. Many improvements have been introduced [29,5,34]. The recent focus was on large displacements, and combinatorial matching has been integrated into the variational approach [6,35]. The work of [35] termed Deep-Matching and DeepFlow is related to our work in that feature information is aggregated from fine to coarse using sparse convolutions and max-pooling. However, it does not perform any learning and all parameters are set manually. The successive work of [30] termed EpicFlow has put even more emphasis on the quality of sparse matching as the matches from [35] are merely interpolated to dense flow fields while respecting image boundaries. We only use a variational approach for optional refinement of the flow field predicted by the convolutional net and do not require any handcrafted methods for aggregation, matching and interpolation.\n\nSeveral authors have applied machine learning techniques to optical flow before. Sun et al. [32] study statistics of optical flow and learn regularizers using Gaussian scale mixtures; Rosenbaum et al. [31] model local statistics of optical flow with Gaussian mixture models. Black et al. [4] compute principal components of a training set of flow fields. To predict optical flow they then estimate coefficients of a linear combination of these 'basis flows'. Other methods train classifiers to select among different inertial estimates [21] or to obtain occlusion probabilities [27].\n\nThere has been work on unsupervised learning of disparity or motion between frames of videos using neural network models. These methods typically use multiplicative interactions to model relations between a pair of images. Disparities and optical flow can then be inferred from the latent variables. Taylor et al. [33] approach the task with factored gated restricted Boltzmann machines. Konda and Memisevic [23] use a special autoencoder called 'synchrony autoencoder'. While these approaches work well in a controlled setup and learn features useful for activity recognition in videos, they are not competitive with classical methods on realistic videos.\n\n\nNetwork Architectures\n\nConvolutional Networks. Convolutional neural networks trained with backpropagation [25] have recently been shown to perform well on large-scale image classification by Krizhevsky et al. [24]. This gave the beginning to a surge of works on applying CNNs to various computer vision tasks.\n\nWhile there has been no work on estimating optical flow with CNNs, there has been research on matching with neural networks. Fischer et al. [12] extract feature representations from CNNs trained in supervised or unsupervised manner and match these features based on Euclidean distance. Zbontar and LeCun [36] train a CNN with a Siamese architecture to predict similarity of image patches. A drastic difference of these methods to our approach is that they are patch based and leave the spatial aggregation to postprocessing, whereas the networks in this paper directly predict complete flow fields.\n\nRecent applications of CNNs include semantic segmentation [11,15,17,28], depth prediction [10], keypoint prediction [17] and edge detection [13]. These tasks are similar to optical flow estimation in that they involve per-pixel predictions. Since our architectures are largely inspired by the recent progress in these per-pixel prediction tasks, we briefly review different approaches.\n\nThe simplest solution is to apply a conventional CNN in a 'sliding window' fashion, hence computing a single prediction (e.g. class label) for each input image patch [8,11]. This works well in many situations, but has drawbacks: high computational costs (even with optimized implementations involving re-usage of intermediate feature maps) and per-patch nature, disallowing to account for global output properties, for example sharp edges. Another simple approach [17] is to upsample all feature maps to the desired full resolution and stack them together, resulting in a concatenated per-pixel feature vector that can be used to predict the value of interest.\n\nEigen et al. [10] refine a coarse depth map by training an additional network which gets as inputs the coarse prediction and the input image. Long et al. [28] and Dosovitskiy et al. [9] iteratively refine the coarse feature maps with the use of 'upconvolutional' layers 1 . Our approach integrates ideas from both works. Unlike Long et al., we 'upconvolve' not just the coarse prediction, but the whole coarse feature maps, allowing to transfer more high-level information to the fine prediction. Unlike Dosovitskiy et al., we concate- nate the 'upconvolution' results with the features from the 'contractive' part of the network.\n\nConvolutional neural networks are known to be very good at learning input-output relations given enough labeled data. We therefore take an end-to-end learning approach to predicting optical flow: given a dataset consisting of image pairs and ground truth flows, we train a network to predict the x-y flow fields directly from the images. But what is a good architecture for this purpose?\n\nA simple choice is to stack both input images together and feed them through a rather generic network, allowing the network to decide itself how to process the image pair to extract the motion information. This is illustrated in Fig. 2 (top). We call this architecture consisting only of convolutional layers 'FlowNetSimple'.\n\nIn principle, if this network is large enough, it could learn to predict optical flow. However, we can never be sure that a local gradient optimization like stochastic gradient descent can get the network to this point. Therefore, it could be beneficial to hand-design an architecture which is less generic, but may perform better with the given data and optimization techniques.\n\nA straightforward step is to create two separate, yet identical processing streams for the two images and to combine them at a later stage as shown in Fig. 2 (bottom). With this architecture the network is constrained to first produce meaningful representations of the two images separately and then combine them on a higher level. This roughly resembles the standard matching approach when one first extracts features from patches of both images and then compares those feature vectors. However, given feature representations of two images, how would the network find correspondences?\n\nTo aid the network in this matching process, we introduce a 'correlation layer' that performs multiplicative patch comparisons between two feature maps. An illustration of the network architecture 'FlowNetCorr' containing this layer is shown in Fig. 2 (bottom). Given two multi-channel feature maps f 1 , f 2 : R 2 \u2192 R c , with w, h, and c being their width, height and number of channels, our correlation layer lets the network compare each patch from f 1 with each path from f 2 .\n\nFor now we consider only a single comparison of two patches. The 'correlation' of two patches centered at x 1 in the first map and x 2 in the second map is then defined as\nc(x 1 , x 2 ) = o\u2208[\u2212k,k]\u00d7[\u2212k,k] f 1 (x 1 + o), f 2 (x 2 + o)(1)\nfor a square patch of size K := 2k + 1. Note that Eq. 1 is identical to one step of a convolution in neural networks, but instead of convolving data with a filter, it convolves data with other data. For this reason, it has no trainable weights.\n\nComputing c(x 1 , x 2 ) involves c \u00b7 K 2 multiplications. Comparing all patch combinations involves w 2 \u00b7 h 2 such computations, yields a large result and makes efficient forward and backward passes intractable. Thus, for computa- Figure 3. Refinement of the coarse feature maps to the high resolution prediction. tional reasons we limit the maximum displacement for comparisons and also introduce striding in both feature maps.\n\nGiven a maximum displacement d, for each location x 1 we compute correlations c(x 1 , x 2 ) only in a neighborhood of size D := 2d + 1, by limiting the range of x 2 . We use strides s 1 and s 2 , to quantize x 1 globally and to quantize x 2 within the neighborhood centered around x 1 .\n\nIn theory, the result produced by the correlation is fourdimensional: for every combination of two 2D positions we obtain a correlation value, i.e. the scalar product of the two vectors which contain the values of the cropped patches respectively. In practice we organize the relative displacements in channels. This means we obtain an output of size (w \u00d7 h \u00d7 D 2 ). For the backward pass we implemented the derivatives with respect to each bottom blob accordingly.\n\nRefinement. CNNs are good at extracting high-level abstract features of images, by interleaving convolutional layers and pooling, i.e. spatially shrinking the feature maps. Pooling is necessary to make network training computationally feasible and, more fundamentally, to allow aggregation of information over large areas of the input images. However, pooling results in reduced resolution, so in order to provide dense per-pixel predictions we need a way to refine the coarse pooled representation.\n\nOur approach to this refinement is depicted in Figure 3. The main ingredient are 'upconvolutional' layers, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. Such layers have been used previously [38,37,16,28,9]. To perform the refinement, we apply the 'upconvolution' to feature maps, and concatenate it with corresponding feature maps from the 'contractive' part of the network and an upsampled coarser flow prediction (if available). This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps. Each step increases the resolution twice. We repeat this 4 times, resulting in a predicted flow for which the resolution is still 4 times smaller than the input.\n\nWe discover that further refinement from this resolution does not significantly improve the results, compared to a Ground truth FlowNetS FlowNetS+v computationally less expensive bilinear upsampling to full image resolution. The result of this bilinear upsampling is the final flow predicted by the network.\n\nIn an alternative scheme, instead of bilinear upsampling we use the variational approach from [6] without the matching term: we start at the 4 times downsampled resolution and then use the coarse to fine scheme with 20 iterations to bring the flow field to the full resolution. Finally, we run 5 more iterations at the full image resolution. We additionally compute image boundaries with the approach from [26] and respect the detected boundaries by replacing the smoothness coefficient by \u03b1 = exp(\u2212\u03bbb(x, y) \u03ba ), where b(x, y) denotes the thin boundary strength resampled at the respective scale and between pixels. This upscaling method is more computationally expensive than simple bilinear upsampling, but adds the benefits of variational methods to obtain smooth and subpixel-accurate flow fields. In the following, we denote the results obtained by this variational refinement with a '+v' suffix. An example of variational refinement can be seen in Fig. 4.\n\n\nTraining Data\n\nUnlike traditional approaches, neural networks require data with ground truth not only for optimizing several parameters, but to learn to perform the task from scratch. In general, obtaining such ground truth is hard, because true pixel correspondences for real world scenes cannot easily be determined. An overview of the available datasets is given in Table 1 \n\n\nExisting Datasets\n\nThe Middlebury dataset [2] contains only 8 image pairs for training, with ground truth flows generated using four different techniques. Displacements are very small, typically below 10 pixels.\n\nThe KITTI dataset [14] is larger (194 training image pairs) and includes large displacements, but contains only a very special motion type. The ground truth is obtained from real world scenes by simultaneously recording the scenes with a camera and a 3D laser scanner. This assumes that the scene is rigid and that the motion stems from a moving observer. Moreover, motion of distant objects, such as the sky, cannot be captured, resulting in sparse optical flow ground truth.\n\nThe MPI Sintel [7] dataset obtains ground truth from rendered artificial scenes with special attention to realistic image properties. Two versions are provided: the Final version contains motion blur and atmospheric effects, such as fog, while the Clean version does not include these effects. Sintel is the largest dataset available (1,041 training image pairs for each version) and provides dense ground truth for small and large displacement magnitudes.\n\n\nFlying Chairs\n\nThe Sintel dataset is still too small to train large CNNs. To provide enough training data, we create a simple synthetic dataset, which we name Flying Chairs, by applying affine transformations to images collected from Flickr and a publicly available rendered set of 3D chair models [1]. We retrieve 964 images from Flickr 2 with a resolution of 1, 024 \u00d7 768 from the categories 'city' (321), 'landscape' (129) and 'mountain' (514). We cut the images into 4 quadrants and use the resulting 512 \u00d7 384 image crops as background. As foreground objects we add images of multiple chairs from [1] to the background. From the original dataset we remove very similar chairs, resulting in 809 chair types and 62 views per chair available. Examples are shown in Figure 5.\n\nTo generate motion, we randomly sample affine transformation parameters for the background and the chairs. The chairs' transformations are relative to the background transformation, which can be interpreted as both the camera and the objects moving. Using the transformation parameters we render the second image, the optical flow and occlusion regions.\n\nAll parameters for each image pair (number, types, sizes and initial positions of the chairs; transformation parameters) are randomly sampled. We adjust the random distributions of these parameters in such a way that the resulting displacement histogram is similar to the one from Sintel (details can be found in the supplementary material). Using this procedure, we generate a dataset with 22,872 image pairs and flow fields (we re-use each background image multiple times). Note that this size is chosen arbitrarily and could be larger in principle.\n\n\nData Augmentation\n\nA widely used strategy to improve generalization of neural networks is data augmentation [24,10]. Even though the Flying Chairs dataset is fairly large, we find that using augmentations is crucial to avoid overfitting. We perform augmentation online during network training. The augmentations we use include geometric transformations: translation, rotation and scaling, as well as additive Gaussian noise and changes in brightness, contrast, gamma, and color. To be reasonably quick, all these operations are processed on the GPU. Some examples of augmentation are given in Fig. 5.\n\nAs we want to increase not only the variety of images but also the variety of flow fields, we apply the same strong geometric transformation to both images of a pair, but additionally a smaller relative transformation between the two images. We adapt the flow field accordingly by applying the per-image augmentations to the flow field from either side.\n\nSpecifically we sample translation from a the range \n\n\nExperiments\n\nWe report the results of our networks on the Sintel, KITTI and Middlebury datasets, as well as on our synthetic Flying Chairs dataset. We also experiment with fine-tuning of the networks on Sintel data and variational refinement of the predicted flow fields. Additionally, we report runtimes of our networks, in comparison to other methods.\n\n\nNetwork and Training Details\n\nThe exact architectures of the networks we train are shown in Fig. 2. Overall, we try to keep the architectures of different networks consistent: they have nine convolutional layers with stride of 2 (the simplest form of pooling) in six of them and a ReLU nonlinearity after each layer. We do not have any fully connected layers, which allows the networks to take images of arbitrary size as input. Convolutional filter sizes decrease towards deeper layers of networks: 7 \u00d7 7 for the first layer, 5 \u00d7 5 for the following two layers and 3 \u00d7 3 starting from the fourth layer. The number of feature maps increases in the deeper layers, roughly doubling after each layer with a stride of 2. For the correlation layer in FlowNetC we chose the parameters k = 0, d = 20, s 1 = 1, s 2 = 2. As training loss we use the endpoint error (EPE), which is the standard error measure for optical flow estimation. It is the Euclidean distance between the predicted flow vector and the ground truth, averaged over all pixels.\n\nFor training CNNs we use a modified version of the caffe [20] framework. We choose Adam [22] as optimization method because for our task it shows faster convergence than standard stochastic gradient descent with momentum. We fix the parameters of Adam as recommended in [22]: \u03b2 1 = 0.9 and \u03b2 2 = 0.999. Since, in a sense, every pixel is a training sample, we use fairly small mini-batches of 8 image pairs. We start with learning rate \u03bb = 1e\u22124 and then divide it by 2 every 100k iterations after the first 300k. With FlowNetCorr we observe exploding gradients with \u03bb = 1e\u22124. To tackle this problem, we start by training with a very low learning rate \u03bb = 1e\u22126, slowly increase it to reach \u03bb = 1e\u22124 after 10k iterations and then follow the schedule just described.\n\nTo monitor overfitting during training and fine-tuning, we split the Flying Chairs dataset into 22, 232 training and 640 test samples and split the Sintel training set into 908 training and 133 validation pairs.\n\nWe found that upscaling the input images during testing may improve the performance. Although the optimal scale depends on the specific dataset, we fixed the scale once for each network for all tasks. For FlowNetS we do not upscale, for FlowNetC we chose a factor of 1.25.\n\nFine-tuning. The used datasets are very different in terms of object types and motions they include. A standard solution is to fine-tune the networks on the target datasets. The KITTI dataset is small and only has sparse flow ground truth. Therefore, we choose to fine-tune on the Sintel training set. We use images from the Clean and Final versions of Sintel together and fine-tune using a low learning rate \u03bb = 1e\u22126 for several thousand iterations. For best performance, after defining the optimal number of iterations using a validation set, we then fine-tune on the whole training set for the same number of iterations. In tables we denote finetuned networks with a '+ft' suffix. Table 2 shows the endpoint error (EPE) of our networks and several well-performing methods on public datasets (Sintel, KITTI, Middlebury), as well as on our Flying Chairs dataset. Additionally we show runtimes of different methods on Sintel.\n\n\nResults\n\nThe networks trained just on the non-realistic Flying Chairs perform very well on real optical flow datasets, beating for example the well-known LDOF [6] method. After fine-tuning on Sintel our networks can outperform the competing real-time method EPPM [3] on Sintel Final and KITTI while being twice as fast.\n\nSintel. From Table 2 one can see that FlowNetC is better than FlowNetS on Sintel Clean, while on Sintel Final the situation changes. On this difficult dataset, FlowNetS+ft+v is even on par with DeepFlow. Since the average endpoint error often favors over-smoothed solutions, it is interesting to see qualitative results of our method. Figure 7 shows examples of the raw optical flow predicted by the two FlowNets (without fine-tuning), compared to ground truth and EpicFlow. The figure shows how the nets often produce visually appealing results, but are still worse in terms of endpoint error. Taking a closer look reveals that one reason for this may be the noisy non-smooth output of the nets especially in large smooth background regions. This we can partially compensate with variational refinement.  Table 2. Average endpoint errors (in pixels) of our networks compared to several well-performing methods on different datasets. The numbers in parentheses are the results of the networks on data they were trained on, and hence are not directly comparable to other results.\n\nImages Ground truth EpicFlow FlowNetS FlowNetC Figure 6. Examples of optical flow prediction on the Flying Chairs dataset. The images include fine details and small objects with large displacements which EpicFlow often fails to find. The networks are much more successful.\n\nKITTI, probably because the images and motions in Sintel are more natural than in Flying Chairs. The FlowNetS outperforms FlowNetC on this dataset.\n\nFlying Chairs. Our networks are trained on the Flying Chairs, and hence are expected to perform best on those. When training, we leave aside a test set consisting of 640 images. Table 2 shows the results of various methods on this test set, some example predictions are shown in Fig. 6. One can see that FlowNetC outperforms FlowNetS and that the nets outperform all state-of-the-art methods. Another interesting finding is that this is the only dataset where the variational refinement does not improve performance but makes things worse. Apparently the networks can do better than variational refinement already. This indicates that with a more realistic training set, the networks might also perform even better on other data.\n\nTimings. In Table 2 we show the per-frame runtimes of different methods in seconds. Unfortunately, many methods only provide the runtime on a single CPU, whereas our FlowNet uses layers only implemented on GPU. While the error rates of the networks are below the state of the art, they are the best among real-time methods. For both train-ing and testing of the networks we use an NVIDIA GTX Titan GPU. The CPU timings of DeepFlow and EpicFlow are taken from [30], while the timing of LDOF was computed on a single 2.66GHz core.\n\n\nAnalysis\n\nTraining data. To check if we benefit from using the Flying Chairs dataset instead of Sintel, we trained a network just on Sintel, leaving aside a validation set to control the performance. Thanks to aggressive data augmentation, even Sintel alone is enough to learn optical flow fairly well. When testing on Sintel, the network trained exclusively on Sintel has EPE roughly 1 pixel higher than the net trained on Flying Chairs and fine-tuned on Sintel. The Flying Chairs dataset is fairly large, so is data augmentation still necessary? The answer is positive: training a network without data augmentation on the Flying Chairs results in an EPE increase of roughly 2 pixels when testing on Sintel.\n\nComparing the architectures. The results in Table 2 allow to draw conclusions about strengths and weaknesses of the two architectures we tested.\n\nFirst, FlowNetS generalizes to Sintel Final better than FlowNetC. On the other hand, FlowNetC outperforms FlowNetS on Flying chairs and Sintel Clean. Note that Flying Chairs do not include motion blur or fog, as in Sintel Final. These results together suggest that even though the number of parameters of the two networks is virtually the same, the FlowNetC slightly more overfits to the training data. This does not mean the network remembers the training samples by heart, but it adapts to the kind of data it is presented during training. Though in our current setup this can be seen as a weakness, if better training data were available it could become an advantage.\n\nSecond, FlowNetC seems to have more problems with large displacements. This can be seen from the results\n\n\nImages\n\nGround truth EpicFlow FlowNetS FlowNetC Figure 7. Examples of optical flow prediction on the Sintel dataset. In each row left to right: overlaid image pair, ground truth flow and 3 predictions: EpicFlow, FlowNetS and FlowNetC. Endpoint error is shown for every frame. Note that even though the EPE of FlowNets is usually worse than that of EpicFlow, the networks often better preserve fine details. on KITTI discussed above, and also from detailed performance analysis on Sintel Final (not shown in the tables).\n\nFlowNetS+ft achieves an s40+ error (EPE on pixels with displacements of at least 40 pixels) of 43.3px, and for FlowNetC+ft this value is 48px. One explanation is that the maximum displacement of the correlation does not allow to predict very large motions. This range can be increased at the cost of computational efficiency.\n\n\nConclusion\n\nBuilding on recent progress in design of convolutional network architectures, we have shown that it is possible to train a network to directly predict optical flow from two in-put images. Intriguingly, the training data need not be realistic. The artificial Flying Chairs dataset including just affine motions of synthetic rigid objects is sufficient to predict optical flow in natural scenes with competitive accuracy. This proves the generalization capabilities of the presented networks. On the test set of the Flying Chairs the CNNs even outperform state-of-the-art methods like Deep-Flow and EpicFlow. It will be interesting to see how future networks perform as more realistic training data becomes available.\n\n\nSupplementary Material for 'FlowNet: Learning Optical Flow with\n\nConvolutional Networks'\n\n\nFlow field color coding\n\nTo visualize the flow fields, we use the tool provided with Sintel [7]. Flow direction is encoded with color and magnitude with color intensity. White corresponds to no motion. Figure 1 illustrates flow color coding: the flow vector at each pixel is a vector from the center of the square to this pixel. Since the magnitudes of flows in different image pairs shown in the main paper are very different, we independently normalize the maximum color intensity for each image pair, but in the same way for different methods applied to one image pair.\n\n\nDetails of generating Flying Chairs\n\nWe explain in detail the process of generating the Flying Chairs dataset. As background we use 964 images of resolution 1024 \u00d7 768 pixels, downloaded from Flickr. As foreground objects we use 809 chair models from the dataset of Aubry et al. [1], each rendered from 62 views: 31 azimuth angles and 2 elevation angles. To generate the first image in an image pair, we take a background image and randomly position a random set of chairs ontop. The number of the chairs is sampled uniformly from [16; 24], the types and viewpoints of the chairs are sampled uniformly and the locations of the chairs are sampled uniformly from the whole image. The sizes of the chairs (in pixels) are sampled from a Gaussian with mean 200 and standard deviation 200, and then clamped between 50 and 640.\n\nTo generate the second image in a pair and the flow field, we apply random transformations to the chairs and the background. Each of these transformations is a composition of zooming, rotation and translation. The parameters to sample are the zoom coefficient, the rotation angle and the translation vector. We aim to roughly match the displacement distribution of Sintel, shown in Fig. 2 (left). Simply sampling the transformation parameters from Gaussians results in too few small displacements, we hence make the distributions of the transformation parameters to be more peaked around zero than Gaussians.\n\nThe family of distributions from which we sample the parameters contains mixtures of two distributions: a constant \u00b5 with probability 1 \u2212 p and a power of a Gaussian  with probability p. More precisely, let \u03b3 \u223c N (\u00b5, \u03c3) be a univariate Gaussian. We raise its absolute value to a power k (keeping the sign) and clamp to the interval [a, b]. We then set the value to \u00b5 with probability 1 \u2212 p. Overall, the result is given by: \u03be = \u03b2 \u00b7 max(min(sign (\u03b3) \u00b7 |\u03b3| k , b), a) + (1 \u2212 \u03b2) \u00b7 \u00b5, where \u03b2 is a Bernoulli random variable equaling 1 with probability p and 0 with probability 1 \u2212 p. We denote the distribution of \u03be by G(k, \u00b5, \u03c3, a, b, p). All transformation parameters are sampled from distributions from this family, with parameters shown in Table 1.\n\nGiven the transformation parameters, it is straightforward to generate the second image in the pair, as well as the flow field and the occlusion map. We then cut each image into 4 quarters, resulting in 4 image pairs of size 512 \u00d7 384 pixels each. The displacement histogram of the Flying Chairs dataset is shown in Fig. 2 (right).\n\nWe did not study in detail the effect of the dataset parameters on the FlowNet results. However, we observed, that with much simpler strategy of sampling all transformation parameters from Gaussians the networks still work, but are less accurate than networks trained on the data described above.\n\n\nConvolutional Filters\n\nWhen taking a closer look at the filters of the FlowNets, one can see that lower layer filters have few structure and higher layer filters are more structured. Fig. 3 shows how the first layer filters have not completely converged, however coarse gradients are visible. In contrast, the filters that are applied to the output of the correlation layer have very visible structure, as shown in Fig. 5. Different filters are  \n\n\nVideo\n\nIn the supplementary video we demonstrate the real-time operation of the FlowNets using a notebook with a GeForce GTX 980M GPU. Resolution of images captured with a webcam is 640 \u00d7 480 pixels. We show example flow fields produced from real-life videos by both FlowNetSimple and FlowNetCorr for indoor and outdoor scenes. The video can be found on http://goo.gl/YmMOkR. A sample frame from the video can be seen in Fig. 4. \n\nFigure 2 .\n2The two network architectures: FlowNetSimple (top) and FlowNetCorr (bottom).\n\nFigure 4 .\n4The effect of variational refinement. In case of small motions (first row) the predicted flow is changed dramatically. For larger motions (second row), big errors are not corrected, but the flow field is smoothed, resulting in lower EPE.\n\n\n[\u221220%, 20%] of the image width for x and y; rotation from [\u221217 \u2022 , 17 \u2022 ]; scaling from [0.9, 2.0]. The Gaussian noise has a sigma uniformly sampled from [0, 0.04]; contrast is sampled within [\u22120.8, 0.4]; multiplicative color changes to the RGB channels per image from [0.5, 2]; gamma values from [0.7, 1.5] and additive brightness changes using Gaussian with a sigma of 0.2.\n\nFigure 5 .\n5Two examples from the Flying Chairs dataset. Generated image pair and color coded flow field (first three columns), augmented image pair and corresponding color coded flow field respectively (last three columns).\n\nFigure 1 .\n1Flow field color coding. The central pixel does not move, and the displacement of every other pixel is the vector from the center to this pixel.\n\nFigure 2 .\n2Histogram of displacement distribution in Sintel (left) and Flying Chairs (right) with linear (top) and logarithmic (bottom) y-axis. The distribution was cut off at the displacement of 150 pixels, the maximum flow in Sintel is actually around 450 pixels.\n\nFigure 3 .\n3First layer filters of FlowNetCorr. The filters are noisy, but some structure is still visible.\n\nFigure 4 .\n4Demo Video: Application of FlowNets to a live video stream. Watch on YouTube: http://goo.gl/YmMOkR selective for different flow directions and magnitudes.\n\nFigure 5 .\n5Visualization of filters applied on top of the correlation layer in FlowNetCorr. There are 256 filters and for each of them we show the weights shaped as a 21 \u00d7 21 pixels patch, where each pixel corresponds to a displacement vector. The center pixel of each patch corresponds to zero displacement. The filters favor interesting unique displacement patterns.\n\n\n.Table 1. Size of already available datasets and the proposed Flying Chairs dataset.Frame Frames with \nGround truth \npairs ground truth density per frame \nMiddlebury \n72 \n8 \n100% \nKITTI \n194 \n194 \n50% \nSintel \n1,041 \n1,041 \n100% \nFlying Chairs 22,872 \n22,872 \n100% \n\n\n\n\nKITTI. The KITTI dataset contains strong projective transformations which are very different from what the networks encountered during training on Flying Chairs. Still, the raw network output is already fairly good, and additional fine-tuning and variational refinement give a further boost. Interestingly, fine-tuning on Sintel improves the results onMethod \n\nSintel Clean \nSintel Final \nKITTI \nMiddlebury train Middlebury test Chairs \nTime (sec) \ntrain \ntest \ntrain \ntest \ntrain \ntest AEE \nAAE \nAEE \nAAE \ntest \nCPU GPU \nEpicFlow [30] \n2.40 \n4.12 \n3.70 \n6.29 3.47 \n3.8 \n0.31 \n3.24 \n0.39 \n3.55 \n2.94 \n16 \n-\nDeepFlow [35] \n3.31 \n5.38 \n4.56 \n7.21 4.58 \n5.8 \n0.21 \n3.04 \n0.42 \n4.22 \n3.53 \n17 \n-\nEPPM [3] \n-\n6.49 \n-\n8.38 \n-\n9.2 \n-\n-\n0.33 \n3.36 \n-\n-\n0.2 \nLDOF [6] \n4.29 \n7.56 \n6.42 \n9.12 13.73 12.4 0.45 \n4.97 \n0.56 \n4.55 \n3.47 \n65 \n2.5 \nFlowNetS \n4.50 \n7.42 \n5.45 \n8.43 8.26 \n-\n1.09 \n13.28 \n-\n-\n2.71 \n-\n0.08 \nFlowNetS+v \n3.66 \n6.45 \n4.76 \n7.67 6.50 \n-\n0.33 \n3.87 \n-\n-\n2.86 \n-\n1.05 \nFlowNetS+ft \n(3.66) 6.96 (4.44) 7.76 7.52 \n9.1 \n0.98 \n15.20 \n-\n-\n3.04 \n-\n0.08 \nFlowNetS+ft+v \n(2.97) 6.16 (4.07) 7.22 6.07 \n7.6 \n0.32 \n3.84 \n0.47 \n4.58 \n3.03 \n-\n1.05 \nFlowNetC \n4.31 \n7.28 \n5.87 \n8.81 9.35 \n-\n1.15 \n15.64 \n-\n-\n2.19 \n-\n0.15 \nFlowNetC+v \n3.57 \n6.27 \n5.25 \n8.01 7.45 \n-\n0.34 \n3.92 \n-\n-\n2.61 \n-\n1.12 \nFlowNetC+ft \n(3.78) 6.85 (5.28) 8.51 8.79 \n-\n0.93 \n12.33 \n-\n-\n2.27 \n-\n0.15 \nFlowNetC+ft+v (3.20) 6.08 (4.83) 7.88 7.31 \n-\n0.33 \n3.81 \n0.50 \n4.52 \n2.67 \n-\n1.12 \n\n\n\n\nTable 1. Parameters of the distribution of the transformation parameters.Parameter \n\nk \u00b5 \n\u03c3 \na \nb \np \nTranslation BG 4 0 \n1.3 \n\u221240 \n40 \n1 \nRotation BG \n2 0 \n1.3 \n\u221210 \n10 0.3 \nZoom BG \n2 1 \n0.1 \n0.93 1.07 0.6 \nTranslation CH 3 0 \n2.3 \u2212120 120 \n1 \nRotation CH \n2 0 \n2.3 \n\u221230 \n30 0.7 \nZoom CH \n2 1 0.18 \n0.8 \n1.2 0.7 \n\n\nThese layers are often named 'deconvolutional', although the operation they perform is technically convolution, not deconvolution\nNon-commercial public license. We use the code framework by Hays and Efros[18] \nAcknowledgmentsThe work was partially funded by the ERC Starting Grants VideoLearn and ConvexVision, by the DFG Grants BR-3815/7-1 and CR 250/13-1, and by the EC FP7 project 610967 (TACMAN).\nSeeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. M Aubry, D Maturana, A Efros, B Russell, J Sivic, CVPR. 11M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment us- ing a large dataset of cad models. In CVPR, 2014. 2, 5, 11\n\nA database and evaluation methodology for optical flow. S Baker, D Scharstein, J Lewis, S Roth, M J Black, R Szeliski, MSR-TR-2009-179Technical ReportS. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and R. Szeliski. A database and evaluation methodology for op- tical flow. Technical Report MSR-TR-2009-179, December 2009. 5\n\nFast edge-preserving patchmatch for large displacement optical flow. L Bao, Q Yang, H Jin, CVPR. 67L. Bao, Q. Yang, and H. Jin. Fast edge-preserving patch- match for large displacement optical flow. In CVPR, 2014. 6, 7\n\nLearning parameterized models of image motion. M J Black, Y Yacoob, A D Jepson, D J Fleet, CVPR. M. J. Black, Y. Yacoob, A. D. Jepson, and D. J. Fleet. Learn- ing parameterized models of image motion. In CVPR, 1997. 2\n\nHigh accuracy optical flow estimation based on a theory for warping. T Brox, A Bruhn, N Papenberg, J Weickert, ECCV. Springer3024T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac- curacy optical flow estimation based on a theory for warping. In ECCV, volume 3024, pages 25-36. Springer, 2004. 2\n\nLarge displacement optical flow: descriptor matching in variational motion estimation. T Brox, J Malik, PAMI337T. Brox and J. Malik. Large displacement optical flow: de- scriptor matching in variational motion estimation. PAMI, 33(3):500-513, 2011. 2, 4, 6, 7\n\nA naturalistic open source movie for optical flow evaluation. D J Butler, J Wulff, G B Stanley, M J Black, ECCV, Part IV. A. Fitzgibbon et al.Springer-Verlag757711D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In A. Fitzgibbon et al. (Eds.), editor, ECCV, Part IV, LNCS 7577, pages 611-625. Springer-Verlag, Oct. 2012. 2, 5, 11\n\nDeep neural networks segment neuronal membranes in electron microscopy images. D C Ciresan, L M Gambardella, A Giusti, J Schmidhuber, NIPS. D. C. Ciresan, L. M. Gambardella, A. Giusti, and J. Schmid- huber. Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852-2860, 2012. 2\n\nLearning to generate chairs with convolutional neural networks. A Dosovitskiy, J T Springenberg, T Brox, CVPR. 24A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015. 2, 4\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, NIPS. 15D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. NIPS, 2014. 1, 2, 5\n\nLearning hierarchical features for scene labeling. C Farabet, C Couprie, L Najman, Y Lecun, PAMI35C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. PAMI, 35(8):1915- 1929, 2013. 2\n\nDescriptor matching with convolutional neural networks: a comparison to SIFT. P Fischer, A Dosovitskiy, T Brox, arXiv:1405.5769v1[cs.CV].2pre-printP. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching with convolutional neural networks: a comparison to SIFT. 2014. pre-print, arXiv:1405.5769v1 [cs.CV]. 2\n\nN\u02c64 -fields: Neural network nearest neighbor fields for image transforms. Y Ganin, V S Lempitsky, ACCV. Y. Ganin and V. S. Lempitsky. N\u02c64 -fields: Neural net- work nearest neighbor fields for image transforms. In ACCV, pages 536-551, 2014. 2\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research. 5A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. 5\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, CVPR. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 2\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In NIPS, 2014. 4\n\nHypercolumns for object segmentation and fine-grained localization. B Hariharan, P Arbel\u00e1ez, R Girshick, J Malik, CVPR. 2B. Hariharan, P. Arbel\u00e1ez, R. Girshick, and J. Malik. Hyper- columns for object segmentation and fine-grained localiza- tion. CVPR, 2015. 2\n\nEfros. im2gps: estimating geographic information from a single image. J Hays, A A , CVPR. J. Hays and A. A. Efros. im2gps: estimating geographic information from a single image. In CVPR, 2008. 5\n\nDetermining optical flow. B K P Horn, B G Schunck, Artificial Intelligence. 172B. K. P. Horn and B. G. Schunck. Determining optical flow. Artificial Intelligence, 17:185-203, 1981. 2\n\nY Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, arXiv:1408.5093Caffe: Convolutional architecture for fast feature embedding. arXiv preprintY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolu- tional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014. 6\n\nOptical flow with geometric occlusion estimation and fusion of multiple frames. R Kennedy, C Taylor, EMM-CVPR. R. Kennedy and C. Taylor. Optical flow with geometric oc- clusion estimation and fusion of multiple frames. In EMM- CVPR. 2015. 2\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6\n\nUnsupervised learning of depth and motion. CoRR, abs/1312. K R Konda, R Memisevic, 3429K. R. Konda and R. Memisevic. Unsupervised learning of depth and motion. CoRR, abs/1312.3429, 2013. 2\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. 15A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1106-1114, 2012. 1, 2, 5\n\nBackpropagation applied to handwritten zip code recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , Neural computation. 14Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural compu- tation, 1(4):541-551, 1989. 1, 2\n\nEfficient closed-form solution to generalized boundary detection. M Leordeanu, R Sukthankar, C Sminchisescu, Proceedings of the 12th European Conference on Computer Vision -Volume Part IV, ECCV'12. the 12th European Conference on Computer Vision -Volume Part IV, ECCV'12Berlin, HeidelbergSpringer-VerlagM. Leordeanu, R. Sukthankar, and C. Sminchisescu. Effi- cient closed-form solution to generalized boundary detec- tion. In Proceedings of the 12th European Conference on Computer Vision -Volume Part IV, ECCV'12, pages 516- 529, Berlin, Heidelberg, 2012. Springer-Verlag. 4\n\nLocally affine sparse-to-dense matching for motion and occlusion estimation. M Leordeanu, A Zanfir, C Sminchisescu, M. Leordeanu, A. Zanfir, and C. Sminchisescu. Locally affine sparse-to-dense matching for motion and occlusion es- timation. ICCV, 0:1721-1728, 2013. 2\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1, 2, 4\n\nDense estimation and object-based segmentation of the optical flow with robust techniques. E M\u00e9min, P P\u00e9rez, 7E. M\u00e9min and P. P\u00e9rez. Dense estimation and object-based segmentation of the optical flow with robust techniques. 7(5):703-719, May 1998. 2\n\nEpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow. J Revaud, P Weinzaepfel, Z Harchaoui, C Schmid, CVPR. Boston, United States27J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid. EpicFlow: Edge-Preserving Interpolation of Correspon- dences for Optical Flow. In CVPR, Boston, United States, June 2015. 2, 7\n\nLearning the local statistics of optical flow. D Rosenbaum, D Zoran, Y Weiss, NIPS. D. Rosenbaum, D. Zoran, and Y. Weiss. Learning the local statistics of optical flow. In NIPS, 2013. 2\n\nLearning optical flow. D Sun, S Roth, J Lewis, M J Black, ECCV. D. Sun, S. Roth, J. Lewis, and M. J. Black. Learning optical flow. In ECCV, 2008. 2\n\nConvolutional learning of spatio-temporal features. G W Taylor, R Fergus, Y Lecun, C Bregler, ECCV. G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Convolu- tional learning of spatio-temporal features. In ECCV, pages 140-153, 2010. 2\n\nStructureand motion-adaptive regularization for high accuracy optic flow. A Wedel, D Cremers, T Pock, H Bischof, ICCV. Kyoto, JapanA. Wedel, D. Cremers, T. Pock, and H. Bischof. Structure- and motion-adaptive regularization for high accuracy optic flow. In ICCV, Kyoto, Japan, 2009. 2\n\nDeepFlow: Large displacement optical flow with deep matching. P Weinzaepfel, J Revaud, Z Harchaoui, C Schmid, ICCV. Sydney, AustraliaP. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. DeepFlow: Large displacement optical flow with deep matching. In ICCV, Sydney, Australia, Dec. 2013. 2, 7\n\nComputing the stereo matching cost with a convolutional neural network. CoRR, abs/1409. J Zbontar, Y Lecun, 4326J. Zbontar and Y. LeCun. Computing the stereo match- ing cost with a convolutional neural network. CoRR, abs/1409.4326, 2014. 2\n\nVisualizing and understanding convolutional networks. M D Zeiler, R Fergus, ECCV. M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014. 4\n\nAdaptive deconvolutional networks for mid and high level feature learning. M D Zeiler, G W Taylor, R Fergus, ICCV. M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive decon- volutional networks for mid and high level feature learning. In ICCV, pages 2018-2025, 2011. 4\n", "annotations": {"author": "[{\"end\":219,\"start\":62},{\"end\":380,\"start\":220},{\"end\":554,\"start\":381},{\"end\":711,\"start\":555},{\"end\":868,\"start\":712},{\"end\":1044,\"start\":869},{\"end\":1208,\"start\":1045},{\"end\":1380,\"start\":1209},{\"end\":1558,\"start\":1381},{\"end\":219,\"start\":62},{\"end\":380,\"start\":220},{\"end\":554,\"start\":381},{\"end\":711,\"start\":555},{\"end\":868,\"start\":712},{\"end\":1044,\"start\":869},{\"end\":1208,\"start\":1045},{\"end\":1380,\"start\":1209},{\"end\":1558,\"start\":1381}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":70},{\"end\":238,\"start\":227},{\"end\":389,\"start\":386},{\"end\":569,\"start\":562},{\"end\":726,\"start\":718},{\"end\":884,\"start\":878},{\"end\":1066,\"start\":1053},{\"end\":1223,\"start\":1216},{\"end\":1392,\"start\":1388},{\"end\":77,\"start\":70},{\"end\":238,\"start\":227},{\"end\":389,\"start\":386},{\"end\":569,\"start\":562},{\"end\":726,\"start\":718},{\"end\":884,\"start\":878},{\"end\":1066,\"start\":1053},{\"end\":1223,\"start\":1216},{\"end\":1392,\"start\":1388}]", "author_first_name": "[{\"end\":69,\"start\":62},{\"end\":226,\"start\":220},{\"end\":385,\"start\":381},{\"end\":561,\"start\":555},{\"end\":717,\"start\":712},{\"end\":877,\"start\":869},{\"end\":1052,\"start\":1045},{\"end\":1215,\"start\":1209},{\"end\":1387,\"start\":1381},{\"end\":69,\"start\":62},{\"end\":226,\"start\":220},{\"end\":385,\"start\":381},{\"end\":561,\"start\":555},{\"end\":717,\"start\":712},{\"end\":877,\"start\":869},{\"end\":1052,\"start\":1045},{\"end\":1215,\"start\":1209},{\"end\":1387,\"start\":1381}]", "author_affiliation": "[{\"end\":218,\"start\":79},{\"end\":379,\"start\":240},{\"end\":553,\"start\":414},{\"end\":710,\"start\":571},{\"end\":867,\"start\":728},{\"end\":1043,\"start\":904},{\"end\":1207,\"start\":1068},{\"end\":1379,\"start\":1240},{\"end\":1557,\"start\":1418},{\"end\":218,\"start\":79},{\"end\":379,\"start\":240},{\"end\":553,\"start\":414},{\"end\":710,\"start\":571},{\"end\":867,\"start\":728},{\"end\":1043,\"start\":904},{\"end\":1207,\"start\":1068},{\"end\":1379,\"start\":1240},{\"end\":1557,\"start\":1418}]", "title": "[{\"end\":59,\"start\":1},{\"end\":1617,\"start\":1559},{\"end\":59,\"start\":1},{\"end\":1617,\"start\":1559}]", "venue": null, "abstract": "[{\"end\":2471,\"start\":1619},{\"end\":2471,\"start\":1619}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2636,\"start\":2632},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2639,\"start\":2636},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2746,\"start\":2742},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2790,\"start\":2786},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4623,\"start\":4620},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4814,\"start\":4811},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5456,\"start\":5452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5501,\"start\":5497},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5503,\"start\":5501},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5506,\"start\":5503},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5633,\"start\":5630},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5636,\"start\":5633},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5654,\"start\":5650},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5923,\"start\":5919},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6025,\"start\":6021},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6403,\"start\":6399},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6512,\"start\":6508},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6598,\"start\":6595},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6847,\"start\":6843},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6889,\"start\":6885},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7210,\"start\":7206},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7304,\"start\":7300},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7661,\"start\":7657},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7764,\"start\":7760},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8006,\"start\":8002},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8170,\"start\":8166},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8524,\"start\":8520},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8527,\"start\":8524},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8530,\"start\":8527},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8533,\"start\":8530},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8556,\"start\":8552},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8582,\"start\":8578},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8606,\"start\":8602},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9018,\"start\":9015},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9021,\"start\":9018},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9317,\"start\":9313},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9528,\"start\":9524},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9669,\"start\":9665},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9696,\"start\":9693},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14723,\"start\":14719},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14726,\"start\":14723},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14729,\"start\":14726},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14732,\"start\":14729},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14734,\"start\":14732},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15680,\"start\":15677},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15993,\"start\":15989},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16972,\"start\":16969},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17162,\"start\":17158},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17636,\"start\":17633},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18378,\"start\":18375},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18682,\"start\":18679},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19876,\"start\":19872},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19879,\"start\":19876},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22232,\"start\":22228},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22263,\"start\":22259},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22445,\"start\":22441},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24512,\"start\":24509},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24616,\"start\":24613},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27368,\"start\":27364},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30836,\"start\":30833},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31598,\"start\":31595},{\"end\":31855,\"start\":31847},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39279,\"start\":39275},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2636,\"start\":2632},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2639,\"start\":2636},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2746,\"start\":2742},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2790,\"start\":2786},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4623,\"start\":4620},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4814,\"start\":4811},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5456,\"start\":5452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5501,\"start\":5497},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5503,\"start\":5501},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5506,\"start\":5503},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5633,\"start\":5630},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5636,\"start\":5633},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5654,\"start\":5650},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5923,\"start\":5919},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6025,\"start\":6021},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6403,\"start\":6399},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6512,\"start\":6508},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6598,\"start\":6595},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6847,\"start\":6843},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6889,\"start\":6885},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7210,\"start\":7206},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7304,\"start\":7300},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7661,\"start\":7657},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7764,\"start\":7760},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8006,\"start\":8002},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8170,\"start\":8166},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8524,\"start\":8520},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8527,\"start\":8524},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8530,\"start\":8527},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8533,\"start\":8530},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8556,\"start\":8552},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8582,\"start\":8578},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8606,\"start\":8602},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9018,\"start\":9015},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9021,\"start\":9018},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9317,\"start\":9313},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9528,\"start\":9524},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9669,\"start\":9665},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9696,\"start\":9693},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14723,\"start\":14719},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14726,\"start\":14723},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14729,\"start\":14726},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14732,\"start\":14729},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14734,\"start\":14732},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15680,\"start\":15677},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15993,\"start\":15989},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16972,\"start\":16969},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17162,\"start\":17158},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17636,\"start\":17633},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18378,\"start\":18375},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18682,\"start\":18679},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19876,\"start\":19872},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19879,\"start\":19876},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22232,\"start\":22228},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22263,\"start\":22259},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22445,\"start\":22441},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24512,\"start\":24509},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24616,\"start\":24613},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27368,\"start\":27364},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30836,\"start\":30833},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31598,\"start\":31595},{\"end\":31855,\"start\":31847},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39279,\"start\":39275}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35098,\"start\":35009},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35349,\"start\":35099},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35727,\"start\":35350},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35953,\"start\":35728},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36111,\"start\":35954},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36379,\"start\":36112},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36488,\"start\":36380},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36656,\"start\":36489},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37027,\"start\":36657},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37297,\"start\":37028},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38751,\"start\":37298},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39070,\"start\":38752},{\"attributes\":{\"id\":\"fig_0\"},\"end\":35098,\"start\":35009},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35349,\"start\":35099},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35727,\"start\":35350},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35953,\"start\":35728},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36111,\"start\":35954},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36379,\"start\":36112},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36488,\"start\":36380},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36656,\"start\":36489},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37027,\"start\":36657},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37297,\"start\":37028},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38751,\"start\":37298},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39070,\"start\":38752}]", "paragraph": "[{\"end\":2907,\"start\":2487},{\"end\":3509,\"start\":2909},{\"end\":3584,\"start\":3511},{\"end\":4277,\"start\":3586},{\"end\":4520,\"start\":4279},{\"end\":5052,\"start\":4522},{\"end\":5324,\"start\":5054},{\"end\":6305,\"start\":5341},{\"end\":6890,\"start\":6307},{\"end\":7548,\"start\":6892},{\"end\":7860,\"start\":7574},{\"end\":8460,\"start\":7862},{\"end\":8847,\"start\":8462},{\"end\":9509,\"start\":8849},{\"end\":10141,\"start\":9511},{\"end\":10530,\"start\":10143},{\"end\":10857,\"start\":10532},{\"end\":11238,\"start\":10859},{\"end\":11825,\"start\":11240},{\"end\":12309,\"start\":11827},{\"end\":12482,\"start\":12311},{\"end\":12791,\"start\":12547},{\"end\":13221,\"start\":12793},{\"end\":13509,\"start\":13223},{\"end\":13976,\"start\":13511},{\"end\":14477,\"start\":13978},{\"end\":15272,\"start\":14479},{\"end\":15581,\"start\":15274},{\"end\":16544,\"start\":15583},{\"end\":16924,\"start\":16562},{\"end\":17138,\"start\":16946},{\"end\":17616,\"start\":17140},{\"end\":18074,\"start\":17618},{\"end\":18853,\"start\":18092},{\"end\":19208,\"start\":18855},{\"end\":19761,\"start\":19210},{\"end\":20364,\"start\":19783},{\"end\":20719,\"start\":20366},{\"end\":20773,\"start\":20721},{\"end\":21129,\"start\":20789},{\"end\":22169,\"start\":21162},{\"end\":22933,\"start\":22171},{\"end\":23146,\"start\":22935},{\"end\":23420,\"start\":23148},{\"end\":24347,\"start\":23422},{\"end\":24669,\"start\":24359},{\"end\":25749,\"start\":24671},{\"end\":26023,\"start\":25751},{\"end\":26172,\"start\":26025},{\"end\":26903,\"start\":26174},{\"end\":27433,\"start\":26905},{\"end\":28144,\"start\":27446},{\"end\":28290,\"start\":28146},{\"end\":28962,\"start\":28292},{\"end\":29068,\"start\":28964},{\"end\":29590,\"start\":29079},{\"end\":29917,\"start\":29592},{\"end\":30647,\"start\":29932},{\"end\":30738,\"start\":30715},{\"end\":31313,\"start\":30766},{\"end\":32136,\"start\":31353},{\"end\":32746,\"start\":32138},{\"end\":33496,\"start\":32748},{\"end\":33829,\"start\":33498},{\"end\":34127,\"start\":33831},{\"end\":34576,\"start\":34153},{\"end\":35008,\"start\":34586},{\"end\":2907,\"start\":2487},{\"end\":3509,\"start\":2909},{\"end\":3584,\"start\":3511},{\"end\":4277,\"start\":3586},{\"end\":4520,\"start\":4279},{\"end\":5052,\"start\":4522},{\"end\":5324,\"start\":5054},{\"end\":6305,\"start\":5341},{\"end\":6890,\"start\":6307},{\"end\":7548,\"start\":6892},{\"end\":7860,\"start\":7574},{\"end\":8460,\"start\":7862},{\"end\":8847,\"start\":8462},{\"end\":9509,\"start\":8849},{\"end\":10141,\"start\":9511},{\"end\":10530,\"start\":10143},{\"end\":10857,\"start\":10532},{\"end\":11238,\"start\":10859},{\"end\":11825,\"start\":11240},{\"end\":12309,\"start\":11827},{\"end\":12482,\"start\":12311},{\"end\":12791,\"start\":12547},{\"end\":13221,\"start\":12793},{\"end\":13509,\"start\":13223},{\"end\":13976,\"start\":13511},{\"end\":14477,\"start\":13978},{\"end\":15272,\"start\":14479},{\"end\":15581,\"start\":15274},{\"end\":16544,\"start\":15583},{\"end\":16924,\"start\":16562},{\"end\":17138,\"start\":16946},{\"end\":17616,\"start\":17140},{\"end\":18074,\"start\":17618},{\"end\":18853,\"start\":18092},{\"end\":19208,\"start\":18855},{\"end\":19761,\"start\":19210},{\"end\":20364,\"start\":19783},{\"end\":20719,\"start\":20366},{\"end\":20773,\"start\":20721},{\"end\":21129,\"start\":20789},{\"end\":22169,\"start\":21162},{\"end\":22933,\"start\":22171},{\"end\":23146,\"start\":22935},{\"end\":23420,\"start\":23148},{\"end\":24347,\"start\":23422},{\"end\":24669,\"start\":24359},{\"end\":25749,\"start\":24671},{\"end\":26023,\"start\":25751},{\"end\":26172,\"start\":26025},{\"end\":26903,\"start\":26174},{\"end\":27433,\"start\":26905},{\"end\":28144,\"start\":27446},{\"end\":28290,\"start\":28146},{\"end\":28962,\"start\":28292},{\"end\":29068,\"start\":28964},{\"end\":29590,\"start\":29079},{\"end\":29917,\"start\":29592},{\"end\":30647,\"start\":29932},{\"end\":30738,\"start\":30715},{\"end\":31313,\"start\":30766},{\"end\":32136,\"start\":31353},{\"end\":32746,\"start\":32138},{\"end\":33496,\"start\":32748},{\"end\":33829,\"start\":33498},{\"end\":34127,\"start\":33831},{\"end\":34576,\"start\":34153},{\"end\":35008,\"start\":34586}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12546,\"start\":12483},{\"attributes\":{\"id\":\"formula_0\"},\"end\":12546,\"start\":12483}]", "table_ref": "[{\"end\":16923,\"start\":16916},{\"end\":24113,\"start\":24106},{\"end\":24691,\"start\":24684},{\"end\":25484,\"start\":25477},{\"end\":26359,\"start\":26352},{\"end\":26924,\"start\":26917},{\"end\":28197,\"start\":28190},{\"end\":33495,\"start\":33488},{\"end\":16923,\"start\":16916},{\"end\":24113,\"start\":24106},{\"end\":24691,\"start\":24684},{\"end\":25484,\"start\":25477},{\"end\":26359,\"start\":26352},{\"end\":26924,\"start\":26917},{\"end\":28197,\"start\":28190},{\"end\":33495,\"start\":33488}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2485,\"start\":2473},{\"attributes\":{\"n\":\"2.\"},\"end\":5339,\"start\":5327},{\"attributes\":{\"n\":\"3.\"},\"end\":7572,\"start\":7551},{\"attributes\":{\"n\":\"4.\"},\"end\":16560,\"start\":16547},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16944,\"start\":16927},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18090,\"start\":18077},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19781,\"start\":19764},{\"attributes\":{\"n\":\"5.\"},\"end\":20787,\"start\":20776},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21160,\"start\":21132},{\"attributes\":{\"n\":\"5.2.\"},\"end\":24357,\"start\":24350},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27444,\"start\":27436},{\"end\":29077,\"start\":29071},{\"attributes\":{\"n\":\"6.\"},\"end\":29930,\"start\":29920},{\"end\":30713,\"start\":30650},{\"attributes\":{\"n\":\"1.\"},\"end\":30764,\"start\":30741},{\"attributes\":{\"n\":\"2.\"},\"end\":31351,\"start\":31316},{\"attributes\":{\"n\":\"3.\"},\"end\":34151,\"start\":34130},{\"attributes\":{\"n\":\"4.\"},\"end\":34584,\"start\":34579},{\"end\":35020,\"start\":35010},{\"end\":35110,\"start\":35100},{\"end\":35739,\"start\":35729},{\"end\":35965,\"start\":35955},{\"end\":36123,\"start\":36113},{\"end\":36391,\"start\":36381},{\"end\":36500,\"start\":36490},{\"end\":36668,\"start\":36658},{\"attributes\":{\"n\":\"1.\"},\"end\":2485,\"start\":2473},{\"attributes\":{\"n\":\"2.\"},\"end\":5339,\"start\":5327},{\"attributes\":{\"n\":\"3.\"},\"end\":7572,\"start\":7551},{\"attributes\":{\"n\":\"4.\"},\"end\":16560,\"start\":16547},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16944,\"start\":16927},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18090,\"start\":18077},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19781,\"start\":19764},{\"attributes\":{\"n\":\"5.\"},\"end\":20787,\"start\":20776},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21160,\"start\":21132},{\"attributes\":{\"n\":\"5.2.\"},\"end\":24357,\"start\":24350},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27444,\"start\":27436},{\"end\":29077,\"start\":29071},{\"attributes\":{\"n\":\"6.\"},\"end\":29930,\"start\":29920},{\"end\":30713,\"start\":30650},{\"attributes\":{\"n\":\"1.\"},\"end\":30764,\"start\":30741},{\"attributes\":{\"n\":\"2.\"},\"end\":31351,\"start\":31316},{\"attributes\":{\"n\":\"3.\"},\"end\":34151,\"start\":34130},{\"attributes\":{\"n\":\"4.\"},\"end\":34584,\"start\":34579},{\"end\":35020,\"start\":35010},{\"end\":35110,\"start\":35100},{\"end\":35739,\"start\":35729},{\"end\":35965,\"start\":35955},{\"end\":36123,\"start\":36113},{\"end\":36391,\"start\":36381},{\"end\":36500,\"start\":36490},{\"end\":36668,\"start\":36658}]", "table": "[{\"end\":37297,\"start\":37114},{\"end\":38751,\"start\":37652},{\"end\":39070,\"start\":38827},{\"end\":37297,\"start\":37114},{\"end\":38751,\"start\":37652},{\"end\":39070,\"start\":38827}]", "figure_caption": "[{\"end\":35098,\"start\":35022},{\"end\":35349,\"start\":35112},{\"end\":35727,\"start\":35352},{\"end\":35953,\"start\":35741},{\"end\":36111,\"start\":35967},{\"end\":36379,\"start\":36125},{\"end\":36488,\"start\":36393},{\"end\":36656,\"start\":36502},{\"end\":37027,\"start\":36670},{\"end\":37114,\"start\":37030},{\"end\":37652,\"start\":37300},{\"end\":38827,\"start\":38754},{\"end\":35098,\"start\":35022},{\"end\":35349,\"start\":35112},{\"end\":35727,\"start\":35352},{\"end\":35953,\"start\":35741},{\"end\":36111,\"start\":35967},{\"end\":36379,\"start\":36125},{\"end\":36488,\"start\":36393},{\"end\":36656,\"start\":36502},{\"end\":37027,\"start\":36670},{\"end\":37114,\"start\":37030},{\"end\":37652,\"start\":37300},{\"end\":38827,\"start\":38754}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":3295,\"start\":3287},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10767,\"start\":10761},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11397,\"start\":11391},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12078,\"start\":12072},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":13032,\"start\":13024},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14534,\"start\":14526},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16543,\"start\":16537},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18852,\"start\":18844},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20363,\"start\":20357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21230,\"start\":21224},{\"end\":25014,\"start\":25006},{\"end\":25806,\"start\":25798},{\"end\":26459,\"start\":26453},{\"end\":29127,\"start\":29119},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30951,\"start\":30943},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32533,\"start\":32520},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33828,\"start\":33814},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34319,\"start\":34313},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34551,\"start\":34545},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35006,\"start\":35000},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":3295,\"start\":3287},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10767,\"start\":10761},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11397,\"start\":11391},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12078,\"start\":12072},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":13032,\"start\":13024},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14534,\"start\":14526},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16543,\"start\":16537},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18852,\"start\":18844},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20363,\"start\":20357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21230,\"start\":21224},{\"end\":25014,\"start\":25006},{\"end\":25806,\"start\":25798},{\"end\":26459,\"start\":26453},{\"end\":29127,\"start\":29119},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30951,\"start\":30943},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32533,\"start\":32520},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33828,\"start\":33814},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34319,\"start\":34313},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34551,\"start\":34545},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35006,\"start\":35000}]", "bib_author_first_name": "[{\"end\":39564,\"start\":39563},{\"end\":39573,\"start\":39572},{\"end\":39585,\"start\":39584},{\"end\":39594,\"start\":39593},{\"end\":39605,\"start\":39604},{\"end\":39855,\"start\":39854},{\"end\":39864,\"start\":39863},{\"end\":39878,\"start\":39877},{\"end\":39887,\"start\":39886},{\"end\":39895,\"start\":39894},{\"end\":39897,\"start\":39896},{\"end\":39906,\"start\":39905},{\"end\":40202,\"start\":40201},{\"end\":40209,\"start\":40208},{\"end\":40217,\"start\":40216},{\"end\":40400,\"start\":40399},{\"end\":40402,\"start\":40401},{\"end\":40411,\"start\":40410},{\"end\":40421,\"start\":40420},{\"end\":40423,\"start\":40422},{\"end\":40433,\"start\":40432},{\"end\":40435,\"start\":40434},{\"end\":40641,\"start\":40640},{\"end\":40649,\"start\":40648},{\"end\":40658,\"start\":40657},{\"end\":40671,\"start\":40670},{\"end\":40963,\"start\":40962},{\"end\":40971,\"start\":40970},{\"end\":41199,\"start\":41198},{\"end\":41201,\"start\":41200},{\"end\":41211,\"start\":41210},{\"end\":41220,\"start\":41219},{\"end\":41222,\"start\":41221},{\"end\":41233,\"start\":41232},{\"end\":41235,\"start\":41234},{\"end\":41616,\"start\":41615},{\"end\":41618,\"start\":41617},{\"end\":41629,\"start\":41628},{\"end\":41631,\"start\":41630},{\"end\":41646,\"start\":41645},{\"end\":41656,\"start\":41655},{\"end\":41922,\"start\":41921},{\"end\":41937,\"start\":41936},{\"end\":41939,\"start\":41938},{\"end\":41955,\"start\":41954},{\"end\":42180,\"start\":42179},{\"end\":42189,\"start\":42188},{\"end\":42200,\"start\":42199},{\"end\":42402,\"start\":42401},{\"end\":42413,\"start\":42412},{\"end\":42424,\"start\":42423},{\"end\":42434,\"start\":42433},{\"end\":42660,\"start\":42659},{\"end\":42671,\"start\":42670},{\"end\":42686,\"start\":42685},{\"end\":42969,\"start\":42968},{\"end\":42978,\"start\":42977},{\"end\":42980,\"start\":42979},{\"end\":43180,\"start\":43179},{\"end\":43190,\"start\":43189},{\"end\":43198,\"start\":43197},{\"end\":43209,\"start\":43208},{\"end\":43497,\"start\":43496},{\"end\":43509,\"start\":43508},{\"end\":43520,\"start\":43519},{\"end\":43531,\"start\":43530},{\"end\":43728,\"start\":43727},{\"end\":43742,\"start\":43741},{\"end\":43759,\"start\":43758},{\"end\":43768,\"start\":43767},{\"end\":43774,\"start\":43773},{\"end\":43790,\"start\":43789},{\"end\":43799,\"start\":43798},{\"end\":43812,\"start\":43811},{\"end\":44051,\"start\":44050},{\"end\":44064,\"start\":44063},{\"end\":44076,\"start\":44075},{\"end\":44088,\"start\":44087},{\"end\":44315,\"start\":44314},{\"end\":44323,\"start\":44322},{\"end\":44325,\"start\":44324},{\"end\":44467,\"start\":44466},{\"end\":44471,\"start\":44468},{\"end\":44479,\"start\":44478},{\"end\":44481,\"start\":44480},{\"end\":44625,\"start\":44624},{\"end\":44632,\"start\":44631},{\"end\":44645,\"start\":44644},{\"end\":44656,\"start\":44655},{\"end\":44667,\"start\":44666},{\"end\":44675,\"start\":44674},{\"end\":44687,\"start\":44686},{\"end\":44701,\"start\":44700},{\"end\":45089,\"start\":45088},{\"end\":45100,\"start\":45099},{\"end\":45295,\"start\":45294},{\"end\":45297,\"start\":45296},{\"end\":45307,\"start\":45306},{\"end\":45464,\"start\":45463},{\"end\":45466,\"start\":45465},{\"end\":45475,\"start\":45474},{\"end\":45660,\"start\":45659},{\"end\":45674,\"start\":45673},{\"end\":45687,\"start\":45686},{\"end\":45689,\"start\":45688},{\"end\":45921,\"start\":45920},{\"end\":45930,\"start\":45929},{\"end\":45939,\"start\":45938},{\"end\":45941,\"start\":45940},{\"end\":45951,\"start\":45950},{\"end\":45964,\"start\":45963},{\"end\":45966,\"start\":45965},{\"end\":45976,\"start\":45975},{\"end\":45987,\"start\":45986},{\"end\":45989,\"start\":45988},{\"end\":46282,\"start\":46281},{\"end\":46295,\"start\":46294},{\"end\":46309,\"start\":46308},{\"end\":46870,\"start\":46869},{\"end\":46883,\"start\":46882},{\"end\":46893,\"start\":46892},{\"end\":47118,\"start\":47117},{\"end\":47126,\"start\":47125},{\"end\":47139,\"start\":47138},{\"end\":47366,\"start\":47365},{\"end\":47375,\"start\":47374},{\"end\":47603,\"start\":47602},{\"end\":47613,\"start\":47612},{\"end\":47628,\"start\":47627},{\"end\":47641,\"start\":47640},{\"end\":47911,\"start\":47910},{\"end\":47924,\"start\":47923},{\"end\":47933,\"start\":47932},{\"end\":48074,\"start\":48073},{\"end\":48081,\"start\":48080},{\"end\":48089,\"start\":48088},{\"end\":48098,\"start\":48097},{\"end\":48100,\"start\":48099},{\"end\":48252,\"start\":48251},{\"end\":48254,\"start\":48253},{\"end\":48264,\"start\":48263},{\"end\":48274,\"start\":48273},{\"end\":48283,\"start\":48282},{\"end\":48512,\"start\":48511},{\"end\":48521,\"start\":48520},{\"end\":48532,\"start\":48531},{\"end\":48540,\"start\":48539},{\"end\":48786,\"start\":48785},{\"end\":48801,\"start\":48800},{\"end\":48811,\"start\":48810},{\"end\":48824,\"start\":48823},{\"end\":49108,\"start\":49107},{\"end\":49119,\"start\":49118},{\"end\":49315,\"start\":49314},{\"end\":49317,\"start\":49316},{\"end\":49327,\"start\":49326},{\"end\":49518,\"start\":49517},{\"end\":49520,\"start\":49519},{\"end\":49530,\"start\":49529},{\"end\":49532,\"start\":49531},{\"end\":49542,\"start\":49541},{\"end\":39564,\"start\":39563},{\"end\":39573,\"start\":39572},{\"end\":39585,\"start\":39584},{\"end\":39594,\"start\":39593},{\"end\":39605,\"start\":39604},{\"end\":39855,\"start\":39854},{\"end\":39864,\"start\":39863},{\"end\":39878,\"start\":39877},{\"end\":39887,\"start\":39886},{\"end\":39895,\"start\":39894},{\"end\":39897,\"start\":39896},{\"end\":39906,\"start\":39905},{\"end\":40202,\"start\":40201},{\"end\":40209,\"start\":40208},{\"end\":40217,\"start\":40216},{\"end\":40400,\"start\":40399},{\"end\":40402,\"start\":40401},{\"end\":40411,\"start\":40410},{\"end\":40421,\"start\":40420},{\"end\":40423,\"start\":40422},{\"end\":40433,\"start\":40432},{\"end\":40435,\"start\":40434},{\"end\":40641,\"start\":40640},{\"end\":40649,\"start\":40648},{\"end\":40658,\"start\":40657},{\"end\":40671,\"start\":40670},{\"end\":40963,\"start\":40962},{\"end\":40971,\"start\":40970},{\"end\":41199,\"start\":41198},{\"end\":41201,\"start\":41200},{\"end\":41211,\"start\":41210},{\"end\":41220,\"start\":41219},{\"end\":41222,\"start\":41221},{\"end\":41233,\"start\":41232},{\"end\":41235,\"start\":41234},{\"end\":41616,\"start\":41615},{\"end\":41618,\"start\":41617},{\"end\":41629,\"start\":41628},{\"end\":41631,\"start\":41630},{\"end\":41646,\"start\":41645},{\"end\":41656,\"start\":41655},{\"end\":41922,\"start\":41921},{\"end\":41937,\"start\":41936},{\"end\":41939,\"start\":41938},{\"end\":41955,\"start\":41954},{\"end\":42180,\"start\":42179},{\"end\":42189,\"start\":42188},{\"end\":42200,\"start\":42199},{\"end\":42402,\"start\":42401},{\"end\":42413,\"start\":42412},{\"end\":42424,\"start\":42423},{\"end\":42434,\"start\":42433},{\"end\":42660,\"start\":42659},{\"end\":42671,\"start\":42670},{\"end\":42686,\"start\":42685},{\"end\":42969,\"start\":42968},{\"end\":42978,\"start\":42977},{\"end\":42980,\"start\":42979},{\"end\":43180,\"start\":43179},{\"end\":43190,\"start\":43189},{\"end\":43198,\"start\":43197},{\"end\":43209,\"start\":43208},{\"end\":43497,\"start\":43496},{\"end\":43509,\"start\":43508},{\"end\":43520,\"start\":43519},{\"end\":43531,\"start\":43530},{\"end\":43728,\"start\":43727},{\"end\":43742,\"start\":43741},{\"end\":43759,\"start\":43758},{\"end\":43768,\"start\":43767},{\"end\":43774,\"start\":43773},{\"end\":43790,\"start\":43789},{\"end\":43799,\"start\":43798},{\"end\":43812,\"start\":43811},{\"end\":44051,\"start\":44050},{\"end\":44064,\"start\":44063},{\"end\":44076,\"start\":44075},{\"end\":44088,\"start\":44087},{\"end\":44315,\"start\":44314},{\"end\":44323,\"start\":44322},{\"end\":44325,\"start\":44324},{\"end\":44467,\"start\":44466},{\"end\":44471,\"start\":44468},{\"end\":44479,\"start\":44478},{\"end\":44481,\"start\":44480},{\"end\":44625,\"start\":44624},{\"end\":44632,\"start\":44631},{\"end\":44645,\"start\":44644},{\"end\":44656,\"start\":44655},{\"end\":44667,\"start\":44666},{\"end\":44675,\"start\":44674},{\"end\":44687,\"start\":44686},{\"end\":44701,\"start\":44700},{\"end\":45089,\"start\":45088},{\"end\":45100,\"start\":45099},{\"end\":45295,\"start\":45294},{\"end\":45297,\"start\":45296},{\"end\":45307,\"start\":45306},{\"end\":45464,\"start\":45463},{\"end\":45466,\"start\":45465},{\"end\":45475,\"start\":45474},{\"end\":45660,\"start\":45659},{\"end\":45674,\"start\":45673},{\"end\":45687,\"start\":45686},{\"end\":45689,\"start\":45688},{\"end\":45921,\"start\":45920},{\"end\":45930,\"start\":45929},{\"end\":45939,\"start\":45938},{\"end\":45941,\"start\":45940},{\"end\":45951,\"start\":45950},{\"end\":45964,\"start\":45963},{\"end\":45966,\"start\":45965},{\"end\":45976,\"start\":45975},{\"end\":45987,\"start\":45986},{\"end\":45989,\"start\":45988},{\"end\":46282,\"start\":46281},{\"end\":46295,\"start\":46294},{\"end\":46309,\"start\":46308},{\"end\":46870,\"start\":46869},{\"end\":46883,\"start\":46882},{\"end\":46893,\"start\":46892},{\"end\":47118,\"start\":47117},{\"end\":47126,\"start\":47125},{\"end\":47139,\"start\":47138},{\"end\":47366,\"start\":47365},{\"end\":47375,\"start\":47374},{\"end\":47603,\"start\":47602},{\"end\":47613,\"start\":47612},{\"end\":47628,\"start\":47627},{\"end\":47641,\"start\":47640},{\"end\":47911,\"start\":47910},{\"end\":47924,\"start\":47923},{\"end\":47933,\"start\":47932},{\"end\":48074,\"start\":48073},{\"end\":48081,\"start\":48080},{\"end\":48089,\"start\":48088},{\"end\":48098,\"start\":48097},{\"end\":48100,\"start\":48099},{\"end\":48252,\"start\":48251},{\"end\":48254,\"start\":48253},{\"end\":48264,\"start\":48263},{\"end\":48274,\"start\":48273},{\"end\":48283,\"start\":48282},{\"end\":48512,\"start\":48511},{\"end\":48521,\"start\":48520},{\"end\":48532,\"start\":48531},{\"end\":48540,\"start\":48539},{\"end\":48786,\"start\":48785},{\"end\":48801,\"start\":48800},{\"end\":48811,\"start\":48810},{\"end\":48824,\"start\":48823},{\"end\":49108,\"start\":49107},{\"end\":49119,\"start\":49118},{\"end\":49315,\"start\":49314},{\"end\":49317,\"start\":49316},{\"end\":49327,\"start\":49326},{\"end\":49518,\"start\":49517},{\"end\":49520,\"start\":49519},{\"end\":49530,\"start\":49529},{\"end\":49532,\"start\":49531},{\"end\":49542,\"start\":49541}]", "bib_author_last_name": "[{\"end\":39570,\"start\":39565},{\"end\":39582,\"start\":39574},{\"end\":39591,\"start\":39586},{\"end\":39602,\"start\":39595},{\"end\":39611,\"start\":39606},{\"end\":39861,\"start\":39856},{\"end\":39875,\"start\":39865},{\"end\":39884,\"start\":39879},{\"end\":39892,\"start\":39888},{\"end\":39903,\"start\":39898},{\"end\":39915,\"start\":39907},{\"end\":40206,\"start\":40203},{\"end\":40214,\"start\":40210},{\"end\":40221,\"start\":40218},{\"end\":40408,\"start\":40403},{\"end\":40418,\"start\":40412},{\"end\":40430,\"start\":40424},{\"end\":40441,\"start\":40436},{\"end\":40646,\"start\":40642},{\"end\":40655,\"start\":40650},{\"end\":40668,\"start\":40659},{\"end\":40680,\"start\":40672},{\"end\":40968,\"start\":40964},{\"end\":40977,\"start\":40972},{\"end\":41208,\"start\":41202},{\"end\":41217,\"start\":41212},{\"end\":41230,\"start\":41223},{\"end\":41241,\"start\":41236},{\"end\":41626,\"start\":41619},{\"end\":41643,\"start\":41632},{\"end\":41653,\"start\":41647},{\"end\":41668,\"start\":41657},{\"end\":41934,\"start\":41923},{\"end\":41952,\"start\":41940},{\"end\":41960,\"start\":41956},{\"end\":42186,\"start\":42181},{\"end\":42197,\"start\":42190},{\"end\":42207,\"start\":42201},{\"end\":42410,\"start\":42403},{\"end\":42421,\"start\":42414},{\"end\":42431,\"start\":42425},{\"end\":42440,\"start\":42435},{\"end\":42668,\"start\":42661},{\"end\":42683,\"start\":42672},{\"end\":42691,\"start\":42687},{\"end\":42975,\"start\":42970},{\"end\":42990,\"start\":42981},{\"end\":43187,\"start\":43181},{\"end\":43195,\"start\":43191},{\"end\":43206,\"start\":43199},{\"end\":43217,\"start\":43210},{\"end\":43506,\"start\":43498},{\"end\":43517,\"start\":43510},{\"end\":43528,\"start\":43521},{\"end\":43537,\"start\":43532},{\"end\":43739,\"start\":43729},{\"end\":43756,\"start\":43743},{\"end\":43765,\"start\":43760},{\"end\":43771,\"start\":43769},{\"end\":43787,\"start\":43775},{\"end\":43796,\"start\":43791},{\"end\":43809,\"start\":43800},{\"end\":43819,\"start\":43813},{\"end\":44061,\"start\":44052},{\"end\":44073,\"start\":44065},{\"end\":44085,\"start\":44077},{\"end\":44094,\"start\":44089},{\"end\":44320,\"start\":44316},{\"end\":44476,\"start\":44472},{\"end\":44489,\"start\":44482},{\"end\":44629,\"start\":44626},{\"end\":44642,\"start\":44633},{\"end\":44653,\"start\":44646},{\"end\":44664,\"start\":44657},{\"end\":44672,\"start\":44668},{\"end\":44684,\"start\":44676},{\"end\":44698,\"start\":44688},{\"end\":44709,\"start\":44702},{\"end\":45097,\"start\":45090},{\"end\":45107,\"start\":45101},{\"end\":45304,\"start\":45298},{\"end\":45310,\"start\":45308},{\"end\":45472,\"start\":45467},{\"end\":45485,\"start\":45476},{\"end\":45671,\"start\":45661},{\"end\":45684,\"start\":45675},{\"end\":45696,\"start\":45690},{\"end\":45927,\"start\":45922},{\"end\":45936,\"start\":45931},{\"end\":45948,\"start\":45942},{\"end\":45961,\"start\":45952},{\"end\":45973,\"start\":45967},{\"end\":45984,\"start\":45977},{\"end\":46292,\"start\":46283},{\"end\":46306,\"start\":46296},{\"end\":46322,\"start\":46310},{\"end\":46880,\"start\":46871},{\"end\":46890,\"start\":46884},{\"end\":46906,\"start\":46894},{\"end\":47123,\"start\":47119},{\"end\":47136,\"start\":47127},{\"end\":47147,\"start\":47140},{\"end\":47372,\"start\":47367},{\"end\":47381,\"start\":47376},{\"end\":47610,\"start\":47604},{\"end\":47625,\"start\":47614},{\"end\":47638,\"start\":47629},{\"end\":47648,\"start\":47642},{\"end\":47921,\"start\":47912},{\"end\":47930,\"start\":47925},{\"end\":47939,\"start\":47934},{\"end\":48078,\"start\":48075},{\"end\":48086,\"start\":48082},{\"end\":48095,\"start\":48090},{\"end\":48106,\"start\":48101},{\"end\":48261,\"start\":48255},{\"end\":48271,\"start\":48265},{\"end\":48280,\"start\":48275},{\"end\":48291,\"start\":48284},{\"end\":48518,\"start\":48513},{\"end\":48529,\"start\":48522},{\"end\":48537,\"start\":48533},{\"end\":48548,\"start\":48541},{\"end\":48798,\"start\":48787},{\"end\":48808,\"start\":48802},{\"end\":48821,\"start\":48812},{\"end\":48831,\"start\":48825},{\"end\":49116,\"start\":49109},{\"end\":49125,\"start\":49120},{\"end\":49324,\"start\":49318},{\"end\":49334,\"start\":49328},{\"end\":49527,\"start\":49521},{\"end\":49539,\"start\":49533},{\"end\":49549,\"start\":49543},{\"end\":39570,\"start\":39565},{\"end\":39582,\"start\":39574},{\"end\":39591,\"start\":39586},{\"end\":39602,\"start\":39595},{\"end\":39611,\"start\":39606},{\"end\":39861,\"start\":39856},{\"end\":39875,\"start\":39865},{\"end\":39884,\"start\":39879},{\"end\":39892,\"start\":39888},{\"end\":39903,\"start\":39898},{\"end\":39915,\"start\":39907},{\"end\":40206,\"start\":40203},{\"end\":40214,\"start\":40210},{\"end\":40221,\"start\":40218},{\"end\":40408,\"start\":40403},{\"end\":40418,\"start\":40412},{\"end\":40430,\"start\":40424},{\"end\":40441,\"start\":40436},{\"end\":40646,\"start\":40642},{\"end\":40655,\"start\":40650},{\"end\":40668,\"start\":40659},{\"end\":40680,\"start\":40672},{\"end\":40968,\"start\":40964},{\"end\":40977,\"start\":40972},{\"end\":41208,\"start\":41202},{\"end\":41217,\"start\":41212},{\"end\":41230,\"start\":41223},{\"end\":41241,\"start\":41236},{\"end\":41626,\"start\":41619},{\"end\":41643,\"start\":41632},{\"end\":41653,\"start\":41647},{\"end\":41668,\"start\":41657},{\"end\":41934,\"start\":41923},{\"end\":41952,\"start\":41940},{\"end\":41960,\"start\":41956},{\"end\":42186,\"start\":42181},{\"end\":42197,\"start\":42190},{\"end\":42207,\"start\":42201},{\"end\":42410,\"start\":42403},{\"end\":42421,\"start\":42414},{\"end\":42431,\"start\":42425},{\"end\":42440,\"start\":42435},{\"end\":42668,\"start\":42661},{\"end\":42683,\"start\":42672},{\"end\":42691,\"start\":42687},{\"end\":42975,\"start\":42970},{\"end\":42990,\"start\":42981},{\"end\":43187,\"start\":43181},{\"end\":43195,\"start\":43191},{\"end\":43206,\"start\":43199},{\"end\":43217,\"start\":43210},{\"end\":43506,\"start\":43498},{\"end\":43517,\"start\":43510},{\"end\":43528,\"start\":43521},{\"end\":43537,\"start\":43532},{\"end\":43739,\"start\":43729},{\"end\":43756,\"start\":43743},{\"end\":43765,\"start\":43760},{\"end\":43771,\"start\":43769},{\"end\":43787,\"start\":43775},{\"end\":43796,\"start\":43791},{\"end\":43809,\"start\":43800},{\"end\":43819,\"start\":43813},{\"end\":44061,\"start\":44052},{\"end\":44073,\"start\":44065},{\"end\":44085,\"start\":44077},{\"end\":44094,\"start\":44089},{\"end\":44320,\"start\":44316},{\"end\":44476,\"start\":44472},{\"end\":44489,\"start\":44482},{\"end\":44629,\"start\":44626},{\"end\":44642,\"start\":44633},{\"end\":44653,\"start\":44646},{\"end\":44664,\"start\":44657},{\"end\":44672,\"start\":44668},{\"end\":44684,\"start\":44676},{\"end\":44698,\"start\":44688},{\"end\":44709,\"start\":44702},{\"end\":45097,\"start\":45090},{\"end\":45107,\"start\":45101},{\"end\":45304,\"start\":45298},{\"end\":45310,\"start\":45308},{\"end\":45472,\"start\":45467},{\"end\":45485,\"start\":45476},{\"end\":45671,\"start\":45661},{\"end\":45684,\"start\":45675},{\"end\":45696,\"start\":45690},{\"end\":45927,\"start\":45922},{\"end\":45936,\"start\":45931},{\"end\":45948,\"start\":45942},{\"end\":45961,\"start\":45952},{\"end\":45973,\"start\":45967},{\"end\":45984,\"start\":45977},{\"end\":46292,\"start\":46283},{\"end\":46306,\"start\":46296},{\"end\":46322,\"start\":46310},{\"end\":46880,\"start\":46871},{\"end\":46890,\"start\":46884},{\"end\":46906,\"start\":46894},{\"end\":47123,\"start\":47119},{\"end\":47136,\"start\":47127},{\"end\":47147,\"start\":47140},{\"end\":47372,\"start\":47367},{\"end\":47381,\"start\":47376},{\"end\":47610,\"start\":47604},{\"end\":47625,\"start\":47614},{\"end\":47638,\"start\":47629},{\"end\":47648,\"start\":47642},{\"end\":47921,\"start\":47912},{\"end\":47930,\"start\":47925},{\"end\":47939,\"start\":47934},{\"end\":48078,\"start\":48075},{\"end\":48086,\"start\":48082},{\"end\":48095,\"start\":48090},{\"end\":48106,\"start\":48101},{\"end\":48261,\"start\":48255},{\"end\":48271,\"start\":48265},{\"end\":48280,\"start\":48275},{\"end\":48291,\"start\":48284},{\"end\":48518,\"start\":48513},{\"end\":48529,\"start\":48522},{\"end\":48537,\"start\":48533},{\"end\":48548,\"start\":48541},{\"end\":48798,\"start\":48787},{\"end\":48808,\"start\":48802},{\"end\":48821,\"start\":48812},{\"end\":48831,\"start\":48825},{\"end\":49116,\"start\":49109},{\"end\":49125,\"start\":49120},{\"end\":49324,\"start\":49318},{\"end\":49334,\"start\":49328},{\"end\":49527,\"start\":49521},{\"end\":49539,\"start\":49533},{\"end\":49549,\"start\":49543}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13989517},\"end\":39796,\"start\":39472},{\"attributes\":{\"doi\":\"MSR-TR-2009-179\",\"id\":\"b1\"},\"end\":40130,\"start\":39798},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5963738},\"end\":40350,\"start\":40132},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":403007},\"end\":40569,\"start\":40352},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":76390},\"end\":40873,\"start\":40571},{\"attributes\":{\"id\":\"b5\"},\"end\":41134,\"start\":40875},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4637111},\"end\":41534,\"start\":41136},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7725346},\"end\":41855,\"start\":41536},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15960930},\"end\":42102,\"start\":41857},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2255738},\"end\":42348,\"start\":42104},{\"attributes\":{\"id\":\"b10\"},\"end\":42579,\"start\":42350},{\"attributes\":{\"doi\":\"arXiv:1405.5769v1[cs.CV].2\",\"id\":\"b11\"},\"end\":42892,\"start\":42581},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9615581},\"end\":43135,\"start\":42894},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9455111},\"end\":43412,\"start\":43137},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215827080},\"end\":43696,\"start\":43414},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1033682},\"end\":43980,\"start\":43698},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12225766},\"end\":44242,\"start\":43982},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2061602},\"end\":44438,\"start\":44244},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1371968},\"end\":44622,\"start\":44440},{\"attributes\":{\"doi\":\"arXiv:1408.5093\",\"id\":\"b19\"},\"end\":45006,\"start\":44624},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9470945},\"end\":45248,\"start\":45008},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6628106},\"end\":45402,\"start\":45250},{\"attributes\":{\"id\":\"b22\"},\"end\":45592,\"start\":45404},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195908774},\"end\":45857,\"start\":45594},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":41312633},\"end\":46213,\"start\":45859},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1851952},\"end\":46790,\"start\":46215},{\"attributes\":{\"id\":\"b26\"},\"end\":47059,\"start\":46792},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1629541},\"end\":47272,\"start\":47061},{\"attributes\":{\"id\":\"b28\"},\"end\":47523,\"start\":47274},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12184146},\"end\":47861,\"start\":47525},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14417884},\"end\":48048,\"start\":47863},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":969406},\"end\":48197,\"start\":48050},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16347832},\"end\":48435,\"start\":48199},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14652562},\"end\":48721,\"start\":48437},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206769904},\"end\":49017,\"start\":48723},{\"attributes\":{\"id\":\"b35\"},\"end\":49258,\"start\":49019},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3960646},\"end\":49440,\"start\":49260},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":975170},\"end\":49710,\"start\":49442},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13989517},\"end\":39796,\"start\":39472},{\"attributes\":{\"doi\":\"MSR-TR-2009-179\",\"id\":\"b1\"},\"end\":40130,\"start\":39798},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5963738},\"end\":40350,\"start\":40132},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":403007},\"end\":40569,\"start\":40352},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":76390},\"end\":40873,\"start\":40571},{\"attributes\":{\"id\":\"b5\"},\"end\":41134,\"start\":40875},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4637111},\"end\":41534,\"start\":41136},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7725346},\"end\":41855,\"start\":41536},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15960930},\"end\":42102,\"start\":41857},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2255738},\"end\":42348,\"start\":42104},{\"attributes\":{\"id\":\"b10\"},\"end\":42579,\"start\":42350},{\"attributes\":{\"doi\":\"arXiv:1405.5769v1[cs.CV].2\",\"id\":\"b11\"},\"end\":42892,\"start\":42581},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9615581},\"end\":43135,\"start\":42894},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9455111},\"end\":43412,\"start\":43137},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215827080},\"end\":43696,\"start\":43414},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1033682},\"end\":43980,\"start\":43698},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12225766},\"end\":44242,\"start\":43982},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2061602},\"end\":44438,\"start\":44244},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1371968},\"end\":44622,\"start\":44440},{\"attributes\":{\"doi\":\"arXiv:1408.5093\",\"id\":\"b19\"},\"end\":45006,\"start\":44624},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9470945},\"end\":45248,\"start\":45008},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6628106},\"end\":45402,\"start\":45250},{\"attributes\":{\"id\":\"b22\"},\"end\":45592,\"start\":45404},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195908774},\"end\":45857,\"start\":45594},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":41312633},\"end\":46213,\"start\":45859},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1851952},\"end\":46790,\"start\":46215},{\"attributes\":{\"id\":\"b26\"},\"end\":47059,\"start\":46792},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1629541},\"end\":47272,\"start\":47061},{\"attributes\":{\"id\":\"b28\"},\"end\":47523,\"start\":47274},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12184146},\"end\":47861,\"start\":47525},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14417884},\"end\":48048,\"start\":47863},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":969406},\"end\":48197,\"start\":48050},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16347832},\"end\":48435,\"start\":48199},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14652562},\"end\":48721,\"start\":48437},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206769904},\"end\":49017,\"start\":48723},{\"attributes\":{\"id\":\"b35\"},\"end\":49258,\"start\":49019},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3960646},\"end\":49440,\"start\":49260},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":975170},\"end\":49710,\"start\":49442}]", "bib_title": "[{\"end\":39561,\"start\":39472},{\"end\":40199,\"start\":40132},{\"end\":40397,\"start\":40352},{\"end\":40638,\"start\":40571},{\"end\":41196,\"start\":41136},{\"end\":41613,\"start\":41536},{\"end\":41919,\"start\":41857},{\"end\":42177,\"start\":42104},{\"end\":42966,\"start\":42894},{\"end\":43177,\"start\":43137},{\"end\":43494,\"start\":43414},{\"end\":43725,\"start\":43698},{\"end\":44048,\"start\":43982},{\"end\":44312,\"start\":44244},{\"end\":44464,\"start\":44440},{\"end\":45086,\"start\":45008},{\"end\":45292,\"start\":45250},{\"end\":45657,\"start\":45594},{\"end\":45918,\"start\":45859},{\"end\":46279,\"start\":46215},{\"end\":47115,\"start\":47061},{\"end\":47600,\"start\":47525},{\"end\":47908,\"start\":47863},{\"end\":48071,\"start\":48050},{\"end\":48249,\"start\":48199},{\"end\":48509,\"start\":48437},{\"end\":48783,\"start\":48723},{\"end\":49312,\"start\":49260},{\"end\":49515,\"start\":49442},{\"end\":39561,\"start\":39472},{\"end\":40199,\"start\":40132},{\"end\":40397,\"start\":40352},{\"end\":40638,\"start\":40571},{\"end\":41196,\"start\":41136},{\"end\":41613,\"start\":41536},{\"end\":41919,\"start\":41857},{\"end\":42177,\"start\":42104},{\"end\":42966,\"start\":42894},{\"end\":43177,\"start\":43137},{\"end\":43494,\"start\":43414},{\"end\":43725,\"start\":43698},{\"end\":44048,\"start\":43982},{\"end\":44312,\"start\":44244},{\"end\":44464,\"start\":44440},{\"end\":45086,\"start\":45008},{\"end\":45292,\"start\":45250},{\"end\":45657,\"start\":45594},{\"end\":45918,\"start\":45859},{\"end\":46279,\"start\":46215},{\"end\":47115,\"start\":47061},{\"end\":47600,\"start\":47525},{\"end\":47908,\"start\":47863},{\"end\":48071,\"start\":48050},{\"end\":48249,\"start\":48199},{\"end\":48509,\"start\":48437},{\"end\":48783,\"start\":48723},{\"end\":49312,\"start\":49260},{\"end\":49515,\"start\":49442}]", "bib_author": "[{\"end\":39572,\"start\":39563},{\"end\":39584,\"start\":39572},{\"end\":39593,\"start\":39584},{\"end\":39604,\"start\":39593},{\"end\":39613,\"start\":39604},{\"end\":39863,\"start\":39854},{\"end\":39877,\"start\":39863},{\"end\":39886,\"start\":39877},{\"end\":39894,\"start\":39886},{\"end\":39905,\"start\":39894},{\"end\":39917,\"start\":39905},{\"end\":40208,\"start\":40201},{\"end\":40216,\"start\":40208},{\"end\":40223,\"start\":40216},{\"end\":40410,\"start\":40399},{\"end\":40420,\"start\":40410},{\"end\":40432,\"start\":40420},{\"end\":40443,\"start\":40432},{\"end\":40648,\"start\":40640},{\"end\":40657,\"start\":40648},{\"end\":40670,\"start\":40657},{\"end\":40682,\"start\":40670},{\"end\":40970,\"start\":40962},{\"end\":40979,\"start\":40970},{\"end\":41210,\"start\":41198},{\"end\":41219,\"start\":41210},{\"end\":41232,\"start\":41219},{\"end\":41243,\"start\":41232},{\"end\":41628,\"start\":41615},{\"end\":41645,\"start\":41628},{\"end\":41655,\"start\":41645},{\"end\":41670,\"start\":41655},{\"end\":41936,\"start\":41921},{\"end\":41954,\"start\":41936},{\"end\":41962,\"start\":41954},{\"end\":42188,\"start\":42179},{\"end\":42199,\"start\":42188},{\"end\":42209,\"start\":42199},{\"end\":42412,\"start\":42401},{\"end\":42423,\"start\":42412},{\"end\":42433,\"start\":42423},{\"end\":42442,\"start\":42433},{\"end\":42670,\"start\":42659},{\"end\":42685,\"start\":42670},{\"end\":42693,\"start\":42685},{\"end\":42977,\"start\":42968},{\"end\":42992,\"start\":42977},{\"end\":43189,\"start\":43179},{\"end\":43197,\"start\":43189},{\"end\":43208,\"start\":43197},{\"end\":43219,\"start\":43208},{\"end\":43508,\"start\":43496},{\"end\":43519,\"start\":43508},{\"end\":43530,\"start\":43519},{\"end\":43539,\"start\":43530},{\"end\":43741,\"start\":43727},{\"end\":43758,\"start\":43741},{\"end\":43767,\"start\":43758},{\"end\":43773,\"start\":43767},{\"end\":43789,\"start\":43773},{\"end\":43798,\"start\":43789},{\"end\":43811,\"start\":43798},{\"end\":43821,\"start\":43811},{\"end\":44063,\"start\":44050},{\"end\":44075,\"start\":44063},{\"end\":44087,\"start\":44075},{\"end\":44096,\"start\":44087},{\"end\":44322,\"start\":44314},{\"end\":44328,\"start\":44322},{\"end\":44478,\"start\":44466},{\"end\":44491,\"start\":44478},{\"end\":44631,\"start\":44624},{\"end\":44644,\"start\":44631},{\"end\":44655,\"start\":44644},{\"end\":44666,\"start\":44655},{\"end\":44674,\"start\":44666},{\"end\":44686,\"start\":44674},{\"end\":44700,\"start\":44686},{\"end\":44711,\"start\":44700},{\"end\":45099,\"start\":45088},{\"end\":45109,\"start\":45099},{\"end\":45306,\"start\":45294},{\"end\":45312,\"start\":45306},{\"end\":45474,\"start\":45463},{\"end\":45487,\"start\":45474},{\"end\":45673,\"start\":45659},{\"end\":45686,\"start\":45673},{\"end\":45698,\"start\":45686},{\"end\":45929,\"start\":45920},{\"end\":45938,\"start\":45929},{\"end\":45950,\"start\":45938},{\"end\":45963,\"start\":45950},{\"end\":45975,\"start\":45963},{\"end\":45986,\"start\":45975},{\"end\":45992,\"start\":45986},{\"end\":46294,\"start\":46281},{\"end\":46308,\"start\":46294},{\"end\":46324,\"start\":46308},{\"end\":46882,\"start\":46869},{\"end\":46892,\"start\":46882},{\"end\":46908,\"start\":46892},{\"end\":47125,\"start\":47117},{\"end\":47138,\"start\":47125},{\"end\":47149,\"start\":47138},{\"end\":47374,\"start\":47365},{\"end\":47383,\"start\":47374},{\"end\":47612,\"start\":47602},{\"end\":47627,\"start\":47612},{\"end\":47640,\"start\":47627},{\"end\":47650,\"start\":47640},{\"end\":47923,\"start\":47910},{\"end\":47932,\"start\":47923},{\"end\":47941,\"start\":47932},{\"end\":48080,\"start\":48073},{\"end\":48088,\"start\":48080},{\"end\":48097,\"start\":48088},{\"end\":48108,\"start\":48097},{\"end\":48263,\"start\":48251},{\"end\":48273,\"start\":48263},{\"end\":48282,\"start\":48273},{\"end\":48293,\"start\":48282},{\"end\":48520,\"start\":48511},{\"end\":48531,\"start\":48520},{\"end\":48539,\"start\":48531},{\"end\":48550,\"start\":48539},{\"end\":48800,\"start\":48785},{\"end\":48810,\"start\":48800},{\"end\":48823,\"start\":48810},{\"end\":48833,\"start\":48823},{\"end\":49118,\"start\":49107},{\"end\":49127,\"start\":49118},{\"end\":49326,\"start\":49314},{\"end\":49336,\"start\":49326},{\"end\":49529,\"start\":49517},{\"end\":49541,\"start\":49529},{\"end\":49551,\"start\":49541},{\"end\":39572,\"start\":39563},{\"end\":39584,\"start\":39572},{\"end\":39593,\"start\":39584},{\"end\":39604,\"start\":39593},{\"end\":39613,\"start\":39604},{\"end\":39863,\"start\":39854},{\"end\":39877,\"start\":39863},{\"end\":39886,\"start\":39877},{\"end\":39894,\"start\":39886},{\"end\":39905,\"start\":39894},{\"end\":39917,\"start\":39905},{\"end\":40208,\"start\":40201},{\"end\":40216,\"start\":40208},{\"end\":40223,\"start\":40216},{\"end\":40410,\"start\":40399},{\"end\":40420,\"start\":40410},{\"end\":40432,\"start\":40420},{\"end\":40443,\"start\":40432},{\"end\":40648,\"start\":40640},{\"end\":40657,\"start\":40648},{\"end\":40670,\"start\":40657},{\"end\":40682,\"start\":40670},{\"end\":40970,\"start\":40962},{\"end\":40979,\"start\":40970},{\"end\":41210,\"start\":41198},{\"end\":41219,\"start\":41210},{\"end\":41232,\"start\":41219},{\"end\":41243,\"start\":41232},{\"end\":41628,\"start\":41615},{\"end\":41645,\"start\":41628},{\"end\":41655,\"start\":41645},{\"end\":41670,\"start\":41655},{\"end\":41936,\"start\":41921},{\"end\":41954,\"start\":41936},{\"end\":41962,\"start\":41954},{\"end\":42188,\"start\":42179},{\"end\":42199,\"start\":42188},{\"end\":42209,\"start\":42199},{\"end\":42412,\"start\":42401},{\"end\":42423,\"start\":42412},{\"end\":42433,\"start\":42423},{\"end\":42442,\"start\":42433},{\"end\":42670,\"start\":42659},{\"end\":42685,\"start\":42670},{\"end\":42693,\"start\":42685},{\"end\":42977,\"start\":42968},{\"end\":42992,\"start\":42977},{\"end\":43189,\"start\":43179},{\"end\":43197,\"start\":43189},{\"end\":43208,\"start\":43197},{\"end\":43219,\"start\":43208},{\"end\":43508,\"start\":43496},{\"end\":43519,\"start\":43508},{\"end\":43530,\"start\":43519},{\"end\":43539,\"start\":43530},{\"end\":43741,\"start\":43727},{\"end\":43758,\"start\":43741},{\"end\":43767,\"start\":43758},{\"end\":43773,\"start\":43767},{\"end\":43789,\"start\":43773},{\"end\":43798,\"start\":43789},{\"end\":43811,\"start\":43798},{\"end\":43821,\"start\":43811},{\"end\":44063,\"start\":44050},{\"end\":44075,\"start\":44063},{\"end\":44087,\"start\":44075},{\"end\":44096,\"start\":44087},{\"end\":44322,\"start\":44314},{\"end\":44328,\"start\":44322},{\"end\":44478,\"start\":44466},{\"end\":44491,\"start\":44478},{\"end\":44631,\"start\":44624},{\"end\":44644,\"start\":44631},{\"end\":44655,\"start\":44644},{\"end\":44666,\"start\":44655},{\"end\":44674,\"start\":44666},{\"end\":44686,\"start\":44674},{\"end\":44700,\"start\":44686},{\"end\":44711,\"start\":44700},{\"end\":45099,\"start\":45088},{\"end\":45109,\"start\":45099},{\"end\":45306,\"start\":45294},{\"end\":45312,\"start\":45306},{\"end\":45474,\"start\":45463},{\"end\":45487,\"start\":45474},{\"end\":45673,\"start\":45659},{\"end\":45686,\"start\":45673},{\"end\":45698,\"start\":45686},{\"end\":45929,\"start\":45920},{\"end\":45938,\"start\":45929},{\"end\":45950,\"start\":45938},{\"end\":45963,\"start\":45950},{\"end\":45975,\"start\":45963},{\"end\":45986,\"start\":45975},{\"end\":45992,\"start\":45986},{\"end\":46294,\"start\":46281},{\"end\":46308,\"start\":46294},{\"end\":46324,\"start\":46308},{\"end\":46882,\"start\":46869},{\"end\":46892,\"start\":46882},{\"end\":46908,\"start\":46892},{\"end\":47125,\"start\":47117},{\"end\":47138,\"start\":47125},{\"end\":47149,\"start\":47138},{\"end\":47374,\"start\":47365},{\"end\":47383,\"start\":47374},{\"end\":47612,\"start\":47602},{\"end\":47627,\"start\":47612},{\"end\":47640,\"start\":47627},{\"end\":47650,\"start\":47640},{\"end\":47923,\"start\":47910},{\"end\":47932,\"start\":47923},{\"end\":47941,\"start\":47932},{\"end\":48080,\"start\":48073},{\"end\":48088,\"start\":48080},{\"end\":48097,\"start\":48088},{\"end\":48108,\"start\":48097},{\"end\":48263,\"start\":48251},{\"end\":48273,\"start\":48263},{\"end\":48282,\"start\":48273},{\"end\":48293,\"start\":48282},{\"end\":48520,\"start\":48511},{\"end\":48531,\"start\":48520},{\"end\":48539,\"start\":48531},{\"end\":48550,\"start\":48539},{\"end\":48800,\"start\":48785},{\"end\":48810,\"start\":48800},{\"end\":48823,\"start\":48810},{\"end\":48833,\"start\":48823},{\"end\":49118,\"start\":49107},{\"end\":49127,\"start\":49118},{\"end\":49326,\"start\":49314},{\"end\":49336,\"start\":49326},{\"end\":49529,\"start\":49517},{\"end\":49541,\"start\":49529},{\"end\":49551,\"start\":49541}]", "bib_venue": "[{\"end\":46503,\"start\":46413},{\"end\":47677,\"start\":47656},{\"end\":48568,\"start\":48556},{\"end\":48856,\"start\":48839},{\"end\":46503,\"start\":46413},{\"end\":47677,\"start\":47656},{\"end\":48568,\"start\":48556},{\"end\":48856,\"start\":48839},{\"end\":39617,\"start\":39613},{\"end\":39852,\"start\":39798},{\"end\":40227,\"start\":40223},{\"end\":40447,\"start\":40443},{\"end\":40686,\"start\":40682},{\"end\":40960,\"start\":40875},{\"end\":41256,\"start\":41243},{\"end\":41674,\"start\":41670},{\"end\":41966,\"start\":41962},{\"end\":42213,\"start\":42209},{\"end\":42399,\"start\":42350},{\"end\":42657,\"start\":42581},{\"end\":42996,\"start\":42992},{\"end\":43261,\"start\":43219},{\"end\":43543,\"start\":43539},{\"end\":43825,\"start\":43821},{\"end\":44100,\"start\":44096},{\"end\":44332,\"start\":44328},{\"end\":44514,\"start\":44491},{\"end\":44786,\"start\":44726},{\"end\":45117,\"start\":45109},{\"end\":45316,\"start\":45312},{\"end\":45461,\"start\":45404},{\"end\":45702,\"start\":45698},{\"end\":46010,\"start\":45992},{\"end\":46411,\"start\":46324},{\"end\":46867,\"start\":46792},{\"end\":47153,\"start\":47149},{\"end\":47363,\"start\":47274},{\"end\":47654,\"start\":47650},{\"end\":47945,\"start\":47941},{\"end\":48112,\"start\":48108},{\"end\":48297,\"start\":48293},{\"end\":48554,\"start\":48550},{\"end\":48837,\"start\":48833},{\"end\":49105,\"start\":49019},{\"end\":49340,\"start\":49336},{\"end\":49555,\"start\":49551},{\"end\":39617,\"start\":39613},{\"end\":39852,\"start\":39798},{\"end\":40227,\"start\":40223},{\"end\":40447,\"start\":40443},{\"end\":40686,\"start\":40682},{\"end\":40960,\"start\":40875},{\"end\":41256,\"start\":41243},{\"end\":41674,\"start\":41670},{\"end\":41966,\"start\":41962},{\"end\":42213,\"start\":42209},{\"end\":42399,\"start\":42350},{\"end\":42657,\"start\":42581},{\"end\":42996,\"start\":42992},{\"end\":43261,\"start\":43219},{\"end\":43543,\"start\":43539},{\"end\":43825,\"start\":43821},{\"end\":44100,\"start\":44096},{\"end\":44332,\"start\":44328},{\"end\":44514,\"start\":44491},{\"end\":44786,\"start\":44726},{\"end\":45117,\"start\":45109},{\"end\":45316,\"start\":45312},{\"end\":45461,\"start\":45404},{\"end\":45702,\"start\":45698},{\"end\":46010,\"start\":45992},{\"end\":46411,\"start\":46324},{\"end\":46867,\"start\":46792},{\"end\":47153,\"start\":47149},{\"end\":47363,\"start\":47274},{\"end\":47654,\"start\":47650},{\"end\":47945,\"start\":47941},{\"end\":48112,\"start\":48108},{\"end\":48297,\"start\":48293},{\"end\":48554,\"start\":48550},{\"end\":48837,\"start\":48833},{\"end\":49105,\"start\":49019},{\"end\":49340,\"start\":49336},{\"end\":49555,\"start\":49551}]"}}}, "year": 2023, "month": 12, "day": 17}