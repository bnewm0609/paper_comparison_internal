{"id": 247922681, "updated": "2023-10-05 15:51:34.98", "metadata": {"title": "Evaluating the Text-to-SQL Capabilities of Large Language Models", "authors": "[{\"first\":\"Nitarshan\",\"last\":\"Rajkumar\",\"middle\":[]},{\"first\":\"Raymond\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dzmitry\",\"last\":\"Bahdanau\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.00498", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2204-00498", "doi": "10.48550/arxiv.2204.00498"}}, "content": {"source": {"pdf_hash": "51000d9f79be0eefd7972fe94e3c71dddc90d2c6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.00498v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6ad96fd384b9402cc5f9a31ec3bc7564d529ccc3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/51000d9f79be0eefd7972fe94e3c71dddc90d2c6.txt", "contents": "\nEvaluating the Text-to-SQL Capabilities of Large Language Models\n\n\nNitarshan Rajkumar \nUniversity of Cambridge\n\n\nRaymond Li \nServiceNow\n3 Mila\n\nDzmitry Bahdanau dzmitry.bahdanau@servicenow.com \n\nMcGill University\n\n\n\nCIFAR AI Chair\nCanada\n\nEvaluating the Text-to-SQL Capabilities of Large Language Models\n\nWe perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples. Radev. 2019. Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task.John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In . 2020. Semantic evaluation for text-to-sql with distilled test suites.Albert Ziegler. 2021. Research recitation.\n\nIntroduction\n\nTranslating natural language questions to SQL queries (Text-to-SQL) is an important business problem which has seen significant research interest. A common approach to this task involves training a model to produce a SQL query when given a question, a database schema, and possibly database content as inputs. A clear trend in this area is to finetune models pretrained on natural language; notably, performance significantly improves as larger pretrained models are used (Shaw et al., 2021;Scholak et al., 2021).\n\nRecent results from the broader field demonstrate that simply scaling training data and model size for generative language models brings advanced capabilities, such as few-shot learning without finetuning (GPT-3, Brown et al., 2020) and code generation (Codex, Chen et al., 2021). In this work we study if such models are already competitive Text-to-SQL solutions without any further finetuning on task-specific training data, evaluating Codex and GPT-3 models of different sizes with varied prompts on Text-to-SQL benchmarks.\n\nWe find that Codex achieves a competitive performance of up to 67% execution accuracy on the Spider development set. We analyze the predicted queries that automatic evaluation judged as wrong * Work partially done at Mila and the Universit\u00e9 de Montr\u00e9al. and find that many of them would be judged correct by humans, whereas others could likely be fixed within the no-finetuning paradigm. Lastly, using GeoQuery and Scholar benchmarks we show that adapting Codex to a specific domain by prompting it with few examples can be more effective than fine-tuning a smaller language model on the same examples.\n\n\nExperimental Setup\n\nModels Our evaluation focuses on the models accessible via the OpenAI API: GPT-3 (in the ascending ada, babbage, curie and davinci sizes) and Codex (in the ascending cushman-codex and davinci-codex sizes) 1 . These are generative language models which perform next-token prediction during training and inference; GPT-3 is trained on a diverse set of sources from the internet, and Codex is further finetuned on code from GitHub. We compare GPT-3 and Codex against methods from Shaw et al. (2021) using the T5 encoder-decoder model. Starting from public checkpoints pretrained on Common Crawl, the T5 model is finetuned on Spider to predict the output SQL, conditioned on the question and schema. The 3B parameter T5 model is currently the state-of-the-art on Spider when combined with constrained inference using the PICARD algorithm (Scholak et al., 2021). We also compare to BRIDGE v2 (Lin et al., 2020), a sequence-to-sequence model based on BERT.\n\nZero-Shot Experiments We use the Spider benchmark (Yu et al., 2019) for cross-domain Textto-SQL. We report performance using percentage of development set predictions which are valid (executable) SQLite SQL, execution accuracy, and test-suite execution accuracy. The latter metric was proposed by Zhong et al. (2020) to measure semantic equivalence of SQL queries written in different styles, which is essential when comparing Codex to models trained on Spider. We address concerns around possible memorization of Spider data by Codex in Appendix A.5.\n\nFew-Shot Experiments We re-purpose the question-splits of the GeoQuery and Scholar datasets (Zelle and Mooney, 1996;Iyer et al., 2017;Finegan-Dollak et al., 2018) to perform experiments in a few-shot setting. The examples in these datasets are grouped by query templates. Examples corresponding to the same template have the same SQL query structure, but may have different English questions and SQL literals. To define the few-shot task, we first sort the templates by their frequency in the training set. In the n-shot setting we then use one random example for each of the n most frequent templates.\n\nPrompts We use six prompt structures in our experiments (examples provided in Appendix C). Question provides no database information and just includes the question as a SQL comment. API Docs follows the style of the Text-to-SQL example in Codex documentation and includes a schema in a comment style which does not conform to SQLite standards. Select X includes in comments the results of executing a SELECT * FROM T LIMIT X query on each table, including schemas via column headers. Create Table includes the  CREATE TABLE commands for each table,   preceding two prompt formats. Finally, Fewshot additionally includes question-query pairs.\n\n\nZero-Shot Results\n\nWe present results for different model sizes in Table 1 and for different prompt styles in Table 2.\n\nFull results are available in Table 4 in Appendix B.\n\nCodex provides a strong baseline for Text-to-SQL tasks In Table 1 the best performing model (davinci-codex, Create Table + Select 3) achieves 67% execution accuracy and 56.5% test suite execution accuracy on Spider. This is comparable to the performance of the BRIDGE v2 (Lin et al., 2020) model which achieved a (then) state-of-the-art 68% execution accuracy in December 2020.\n\nPrompt design is critical for performance As seen in Table 2, providing the question alone results in a low 8.3% execution accuracy. There is a progressive improvement to 56.8% as schema information is introduced in API Docs, to 59.9% when valid SQL and foreign key information is used in Create Table, and to 67.0% when database content is introduced with Create Table + Select 3.\n\nMore database content can harm performance In Table 2 we observe that for the Select Limit X prompts there is a negligible change in performance when adding more rows. By contrast, Create Table + Select Limit X prompt accuracy peaks with 3 rows before significantly decreasing in performance as more rows are added.\n\nDiminishing returns for Codex model size While GPT-3 performance significantly benefits from increased model size, the davinci-codex model does not perform drastically better than cess whitespace tokens less efficiently than Codex models, and therefore cannot evaluate Create  cushman-codex. Full results in Table 4 in Appendix B show cushman-codex generally being within 1 percentage point of davinci-codex for the same prompt style; it even performs 3 percentage points better for the Create Table prompt. These results suggest that davinci-codex's longer context window may be a greater contributor to its peak performance than increased parameter count.\n\n\nError Analysis\n\nWe focus our error analysis on the davinci-codex model with Create Table + Select 3 prompt, and present a breakdown of prediction types in Table 3 and examples of errors in Figure 1. Our error categories were chosen to surface the most interesting Codex-specific behaviours we observed amongst the errors made. We randomly selected and annotated 100 predictions which were valid SQL yet were judged incorrect by test-suite evaluation. We first consider Semantic Incorrect behaviours, which Spider evaluation and the human annotator both view as incorrect predictions. Shortcut errors are where Codex made use of either specific table values or \"world knowledge\" from GPT-3 pretraining, while the ground-truth query contained the exact literals from the question. GROUP BY Convention errors are where Codex incorrectly groups on a non-primary-key column (such as a name or title column).\n\nWe also consider Ambiguous Correct behaviours which are semantically different from the gold query and are therefore judged as incorrect by Spider evaluation, but which the human annotator viewed as being an acceptable SQL translation of  We investigate whether Codex can perform fewshot Text-to-SQL. As described in Section 2, we re-purpose the GeoQuery and Sholar datasets in a few-shot setting. It is well known that models trained on Spider transfer poorly to other singledatabase Text-to-SQL datasets (Suhr et al., 2020) in a zero-shot setting. Studying few-shot Text-to-SQL on GeoQuery and Scholar should show to what extent models are able to leverage a small amount of examples to effectively adapt to a new domain.\n\nBaseline The baseline is a T5-3B model that was finetuned on Spider, reaching 71% exact-match accuracy on Spider validation set. The model is then further finetuned on the new domain -Geo-Query or Scholar. The learning rate for domainspecific-finetuning was selected in the 20-shot setting among [0.1, 0.2, 0.5, 1, 2] \u00b7 10 \u22125 , based on the best validation set performance after 300 steps. We use batch-size 1024, such that all the few-shot examples fit in the same batch.\n\nCodex Building on the Create Table + Select X prompt, we append n question-query examples to the input in an n-shot setting. An example of this prompt is provided in Figure 11. All samples are generated using greedy decoding, with temperature 0. Note that for a given n-shot setting, the baseline and Codex use the same set of support examples. These examples are in the prompt for Codex, and used to finetune the baseline on the new domain. Given the limited window-size of API models, on GeoQuery we can feed up to 40 support examples to davinci-codex, and up to 10 examples to cushman-codex and GPT-3 models. On Scholar the queries are longer and the schema more complexwe fit only 10 examples in the prompt of davincicodex, 5 for cushman-codex, and none at all for GPT-3 models. Figure 2 shows test-suite accuracies on the Scholar and GeoQuery datasets. The baseline reaches 85.7% test-set performance when trained on the complete GeoQuery training set (549 examples). Respectively, it reaches 87.2% test accuracy when trained on the whole Scholar training set (499 examples). This simple baseline is a very competitive model when considering the entire datasets. However Figure 2 shows that it is largely beaten by Codex in few-shot settings. In a zero-shot setting, both davinci-codex and cushman-codex al-  ready beat the baseline on GeoQuery. We speculate that Codex performs well here because it uses the same argmax convention as the GeoQuery dataset, which is different than the convention used in Spider. With up to 40 examples in the prompt, davinci-codex outperforms a T5-3B model finetuned on these same examples by a large margin, whereas GPT-3 davinci performs quite poorly on this task. On the other hand, the T5 model outperforms Codex in a zero-shot setting on Scholar. In 5 and 10-shot settings, Codex shows better adaptation from these few samples and beats the T5 baseline.\n\n\nResults\n\n\nConclusion\n\nWe demonstrated that generative language models trained on code provide a strong baseline for Text-to-SQL. We also provided analysis of failure modes for these models, which we hope guides further prompt design (whether few-shot or through natural language instructions) in this setting. Finally, we showed that prompt-based few-shot learning with these models performs competitively with finetuning-based few-shot learning of smaller models. A clear direction for future work is to evaluate the benefits of finetuning with Codex models. \n\n\nA API Details\n\nAt time of writing, the OpenAI API was accessible at https://openai.com/api/. The example from which our API Docs prompt draws from can be found at https://beta.openai.com/examples/ default-sql-translate.\n\n\nA.1 Hyperparameters\n\nWe sample 200 tokens from GPT-3 and Codex with temperature 0, with the following strings used as stop tokens to halt generation: \"--\", \"\\n\\n\", \";\", \"#\".\n\n\nA.2 Parameter Counts\n\nParameter counts for OpenAI API models are not openly available. Gao (2021) evaluated API GPT-3 models across a variety of language modelling tasks to compare to published results in Brown et al. (2020), finding that \"Ada, Babbage, Curie and Davinci line up closely with 350M, 1.3B, 6.7B, and 175B respectively\". We presume that the davincicodex model is the same size as the GPT-3 davinci model; cushman-codex is a new model name so we can only guess that it is of a similar (but not the same) size to GPT-3 curie. Nevertheless these remain guesses which should not be relied on.\n\n\nA.3 Model Versioning\n\nThe exact models served through the OpenAI API may vary over time. We verified that for each model type, only a single model version was used to generate results. These versions are ada:2020-05-03, babbage:2020-05-03, curie:2020-05-03, davinci:2020-05-03, cushman-codex:2021-08-03, davinci-codex:2021-08-03.\n\n\nA.4 Finetuning\n\nIn Table 4 we include preliminary results from finetuning GPT-3 models on the Spider training set. We used the full training set, and the default finetuning settings of 4 epochs, a batch size of 8, and a learning rate multiplier of 0.1. We did not perform a hyperparameter sweep due to the significant cost this would incur.\n\n\nA.5 Memorization\n\nThe Spider development set is available on GitHub, and is therefore possibly in the training set of Codex. We believe that this does not manifest as memorization for our results however, for the following reasons. Evaluation data on Spider's repo is formatted differently to our prompts. Most related is the dev.sql file, which contains evaluation questionquery pairs in the following format:\n\nQuestion 1: ... SQL: ... ... This resembles but isn't identical to our \"Question\" prompt. We prompted Codex with verbatim fragments of this file and generations failed to replicate any file contents. Our \"Question\" prompt has very poor performance -hardly an indication of memorization from dev.sql. Furthermore, most of Codex's performance is due to including in the prompt the schemas (see Table 2), which are not present in dev.sql.\n\nAs well, Codex prediction style is very different to evaluation gold queries. Gold queries make use of a consistent table aliasing strategy (using T1, T2, etc.) which we never see with Codex (see Figure 3 for example comparisons).\n\nFurthermore, in Table 4 we reported performance for all models on spider-realistic (Deng et al., 2021), a modification of the spider evaluation set that removes column name references in questions. We observe a similar trend in performance across models as on spider (the consistent performance drop on spider-realistic is expected due to the difficulty of the updated dataset). Memorization cannot account for the performance observed, as spider-realistic is not publicly available on GitHub.\n\nFinally, Ziegler (2021) studied memorization in Copilot, a derivative of the Codex models, and found that \"Copilot can quote a body of code verbatim, but that it rarely does so, and when it does, it mostly quotes code that everybody quotes, and mostly at the beginning of a file\". Spider evaluation data is rare on GitHub, and we use long contexts in our prompts that significantly differ from the files on GitHub.\n\n\nA.6 Choice of Spider Evaluation Set\n\nWe chose not to evaluate on the held-out test set of Spider, as this could not be done offline -it would instead require sending these held-out examples through the API to OpenAI, which risks inadvertently leaking them for retraining of Codex. Engine  Prompt  VA  EX  TS  GPT-3 Figure 4: Cherry-picked examples of Codex improvements from 0-shot to 10-shot text-to-SQL on GeoQuery validation set. The style of the generated SQL changes a lot and is much closer to that of the gold SQL when few-shot examples are in the prompt. The few-shot examples were also useful to adapt the generated SQL to the conventions of the dataset, like the way argmax is done, or the selected columns.\n\n\nB Additional Tables and Figures\n\n\nC Example Prompts\n\nWhat is Kyle's id? | network_1 | highschooler : id, name ( Kyle ), grade | friend : student_id, friend_id | likes : student_id, liked_id Figure 5: Example input for baseline T5 models.\n\n--Using valid SQLite, answer the following questions.\n\n--What is Kyle's id? SELECT Figure 6: Example prompt for Question.\n\n### SQLite SQL tables, with their properties: # # Highschooler(ID, name, grade) # Friend(student_id, friend_id) # Likes(student_id, liked_id) # ### What is Kyle's id? SELECT\n\nFigure 1 :\n1Examples of error types, as made by the davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type. Further examples are provided inFigure 3in Appendix B.\n\n\n. When trained on the whole GeoQuery training set (549 examples), the finetuned T5 reaches 85.7% accuracy. (b) Scholar. When trained on the whole Scholar training set (499 examples), the finetuned T5 reaches 87.2% accuracy.\n\nFigure 2 :\n2Test-suite accuracy with varying number of support examples. The x-axis shows the number of fewshot examples used.\n\n\nincluding column type and foreign key declarations. Create Table + Select X 2 is a combination of the 2 Only the davinci-codex model can evaluate Create Table + Select X prompts with more than 1 row, due to its expanded 4096-token prompt window compared to the 2048-token window of all other models. In addition, GPT-3 models prepro-Prompt \nVA \nEX \nTS \nQuestion \n14.0 \n8.3 \n8.2 \nAPI Docs \n83.8 56.8 47.5 \nSelect 1 \n86.3 60.9 52.0 \nSelect 3 \n85.8 60.3 52.2 \nSelect 5 \n85.2 60.5 51.5 \nSelect 10 \n86.0 60.8 51.2 \nCreate Table 89.8 59.9 50.0 \n+ Select 1 \n92.5 64.8 53.7 \n+ Select 3 \n91.6 67.0 55.1 \n+ Select 5 \n91.0 65.3 53.9 \n+ Select 10 \n91.2 63.3 52.4 \n\n\n\nTable 2 :\n2Spider development set performance across \nprompt styles on the davinci-codex model, as measured \nby percentage of predictions which are valid SQL (VA), \nexecution accuracy (EX), test-suite accuracy (TS). \n\n\n\nTable +\n+Select X prompts at all. 14% Shortcuts NL What is the number of car models created by the car maker American Motor Company? Gold SELECT count( * ) FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker WHERE T1.FullName = 'American Motor Company'; Pred SELECT COUNT(Model) FROM model_list WHERE Maker = 1; NL Give the city and country for the Alton airport. Gold SELECT City , Country FROM AIRPORTS WHERE AirportName = \"Alton\" Pred SELECT City, Country FROM airports WHERE AirportCode = 'ALN'; 8% SELECT Extra Columns NL List names of conductors in descending order of years of work. Gold SELECT Name FROM conductor ORDER BY Year_of_Work DESC Pred SELECT Name, Year_of_Work FROM conductor ORDER BY Year_of_Work DESC; Gold SELECT Maker , Model FROM MODEL_LIST; Pred SELECT DISTINCT car_makers.Maker, model_list.Model FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker;5% \nSELECT Convention \nNL \nWhat are all the makers and models? \n\n\nTable 3 :\n3Breakdown of prediction annotations over Spider development set for the davinci-codex model with Create Table + Select 3 prompt. % is percentage of all predictions, E% is percentage of manually annotated erroneous queries (see Section Section 3.1 for details). the given question. SELECT Convention errors are where Codex selects a different column than the per-database convention of the gold queries (such as name instead of ID). SELECT Extra Columns errors are where Codex includes additional useful columns in its query beyond what the gold query includes. Argmax errors are where Codex differs from the gold query in how a min/max resolution (such as \"youngest singer\") is handled for ties.We observe inTable 3that a significant 31% of valid yet erroneous predictions are penalized by Spider evaluation as being incorrect though a human annotator viewed them as acceptable solutions. Future work could be to investigate to what extent one can control the behaviour of Codex. This could allow to fix these ambiguous errors, either by prompt design or using a few examples.\n\nTable 4 :\n4Give the name, year of independence, and surface area of the country that has the lowest population. Gold SELECT Name, SurfaceArea, IndepYear FROM country ORDER BY Population LIMIT 1 Pred SELECT Name, IndepYear, SurfaceArea FROM country WHERE Population = (SELECT MIN(Population) FROM country); 51% Other -Semantic Incorrect NL Which countries have either English or Dutch as an official language? Gold SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = Pred SELECT * FROM countrylanguage WHERE IsOfficial = 'T' OR IsOfficial = 'F'; NL Show the name and the release year of the song by the youngest singer. Gold SELECT song_name , song_release_year FROM singer ORDER BY age LIMIT 1 Pred SELECT Name, Song_release_year FROM singer WHERE Age = (SELECT MIN(Age) FROM singer); NL What is the maximum miles per gallon of the car with 8 cylinders or produced before 1980 ? Gold select max(mpg) from cars_data where cylinders = 8 or year < 1980 Pred SSELECT MAX(MPG) FROM cars_data WHERE Cylinders = 8 AND Year < 1980; 14% Other -Ambiguous Correct NL Find the first name and age of students who have a pet. Gold SELECT DISTINCT T1.fname , T1.age FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid = T2.stuid Pred SELECT Fname, Age FROM Student WHERE StuID IN (SELECT StuID FROM Has_Pet); NL How many countries speak both English and Dutch? Gold SELECT COUNT( * ) FROM (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = \"Dutch\") Pred SELECT COUNT( * ) FROM countrylanguage WHERE Language = 'Dutch' AND CountryCode IN (SELECT CountryCode FROM countrylanguage WHERE Language = 'English'); NL What are the Asian countries which have a population larger than that of any country in Africa? Gold SELECT Name FROM country WHERE Continent = \"Asia\" AND population > (SELECT min(population) FROM country WHERE Continent = \"Africa\") Pred SELECT Name FROM country WHERE Continent = 'Asia' AND Population > (SELECT MAX(Population) FROM country WHERE Continent = 'Africa'); Figure 3: Additional examples of error types, as made by davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type. 10-shot examples what states does the missouri river run through SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"missouri\" ; -what is the size of texas SELECT STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"texas\" ; -what are the major cities in texas SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"texas\" ; -what is the capital of pennsylvania SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"pennsylvania\" ; -what is the biggest city in nebraska SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"nebraska\" ) AND CITYalias0.STATE_NAME = \"nebraska\" ; -what is the population of austin SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"austin\" ; -which state is kalamazoo in SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"kalamazoo\" ; -name all the rivers in colorado SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"colorado\" ; -what states border missouri SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"missouri\" ; -how many people live in new mexico SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"new mexico\" ; Very similar query in the few-shot prompt fixes the example Question which states border iowa Gold SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"iowa\" ; 0-shot pred SELECT state_name FROM border WHERE border = 'iowa' 10-shot pred SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"iowa\" Argmax convetion fixed Question what state has the smallest population Gold SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE AS STATEalias1) ; 0-shot pred SELECT state_name FROM state ORDER BY population LIMIT 1 10-shot pred SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE AS STATEalias1) SELECT extra columns fixed Question what is the population of the state with the largest area Gold SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS STATEalias1) ; 0-shot pred SELECT state_name, population FROM state WHERE area = (SELECT MAX(area) FROM state) 10-shot pred SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS STATEalias1)Performance on Spider across all evaluated models and prompts, as measured by percentage of predictions \nwhich are valid/executable SQL (VA), execution accuracy (EX), test-suite accuracy (TS). Main results are on the \nSpider development set, results in parantheses are on Spider-Realistic (Deng et al., 2021), a modified subset of the \nSpider development set with explicit references to column names removed from questions. \n\n\nAcknowledgementsNitarshan performed all zero-shot and finetuning experiments as well as error-analysis, and wrote most of the paper. Raymond performed all few-shot experiments and the associated writing. Dzmitry supervised, and contributed to paper editing.We thank D\u00f3ra J\u00e1mbor for insightful discussions, Laurent Charlin for providing funding for Nitarshan and for providing feedback on this work, Fraser Kelton and Dave Cummings for support with the OpenAI API, and Ruiqi Zhong for assistance with Spider test suites. We also thank anonymous ARR reviewers for their feedback and criticism in the review process.Nitarshan additionally thanks the city of Montr\u00e9al and its caf\u00e9s for providing inspirational settings in which to conduct this work.Figure 7: Example prompt for API Docs.  foreign key (student_id) references Highschooler(ID) ) --Using valid SQLite, answer the following questions for the tables provided above.--What is Kyle's id? SELECTTable.CREATE  \"highlow\" (\"state_name\" text, \"highest_elevation\" text, \"lowest_point\" text, \"highest_point\" text, \"lowest_elevation\" text) / * state_name highest_elevation lowest_point highest_point lowest_elevation alabama 734 gulf of mexico cheaha mountain 0 alaska 6194 pacific ocean mount mckinley 0 arizona 3851 colorado river humphreys peak 21 * / CREATE TABLE \"lake\" (\"lake_name\" text, \"area\" double DEFAULT NULL, \"country_name\" varchar(3)", "annotations": {"author": "[{\"end\":113,\"start\":68},{\"end\":144,\"start\":114},{\"end\":194,\"start\":145},{\"end\":215,\"start\":195},{\"end\":239,\"start\":216}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":78},{\"end\":124,\"start\":122},{\"end\":161,\"start\":153}]", "author_first_name": "[{\"end\":77,\"start\":68},{\"end\":121,\"start\":114},{\"end\":152,\"start\":145}]", "author_affiliation": "[{\"end\":112,\"start\":88},{\"end\":143,\"start\":126},{\"end\":214,\"start\":196},{\"end\":238,\"start\":217}]", "title": "[{\"end\":65,\"start\":1},{\"end\":304,\"start\":240}]", "venue": null, "abstract": "[{\"end\":1129,\"start\":306}]", "bib_ref": "[{\"end\":1636,\"start\":1617},{\"end\":1657,\"start\":1636},{\"end\":1892,\"start\":1865},{\"end\":1939,\"start\":1913},{\"end\":3308,\"start\":3290},{\"end\":3669,\"start\":3647},{\"end\":3832,\"start\":3815},{\"end\":4081,\"start\":4062},{\"end\":4434,\"start\":4410},{\"end\":4452,\"start\":4434},{\"end\":4480,\"start\":4452},{\"end\":14905,\"start\":14886}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17251,\"start\":16971},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17477,\"start\":17252},{\"attributes\":{\"id\":\"fig_2\"},\"end\":17605,\"start\":17478},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":18261,\"start\":17606},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18481,\"start\":18262},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":19449,\"start\":18482},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":20538,\"start\":19450},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":26233,\"start\":20539}]", "paragraph": "[{\"end\":1658,\"start\":1145},{\"end\":2186,\"start\":1660},{\"end\":2790,\"start\":2188},{\"end\":3763,\"start\":2813},{\"end\":4316,\"start\":3765},{\"end\":4920,\"start\":4318},{\"end\":5563,\"start\":4922},{\"end\":5684,\"start\":5585},{\"end\":5738,\"start\":5686},{\"end\":6117,\"start\":5740},{\"end\":6500,\"start\":6119},{\"end\":6817,\"start\":6502},{\"end\":7476,\"start\":6819},{\"end\":8381,\"start\":7495},{\"end\":9106,\"start\":8383},{\"end\":9580,\"start\":9108},{\"end\":11478,\"start\":9582},{\"end\":12041,\"start\":11503},{\"end\":12263,\"start\":12059},{\"end\":12439,\"start\":12287},{\"end\":13044,\"start\":12464},{\"end\":13376,\"start\":13069},{\"end\":13719,\"start\":13395},{\"end\":14132,\"start\":13740},{\"end\":14569,\"start\":14134},{\"end\":14801,\"start\":14571},{\"end\":15296,\"start\":14803},{\"end\":15712,\"start\":15298},{\"end\":16432,\"start\":15752},{\"end\":16672,\"start\":16488},{\"end\":16727,\"start\":16674},{\"end\":16795,\"start\":16729},{\"end\":16970,\"start\":16797}]", "formula": null, "table_ref": "[{\"end\":5470,\"start\":5413},{\"end\":5640,\"start\":5633},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":5683,\"start\":5676},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":5723,\"start\":5716},{\"end\":5805,\"start\":5798},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":6179,\"start\":6172},{\"end\":6421,\"start\":6415},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":6555,\"start\":6548},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":7134,\"start\":7127},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":7569,\"start\":7562},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":7641,\"start\":7634},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":13405,\"start\":13398},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14533,\"start\":14526},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":14826,\"start\":14819},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16029,\"start\":15996}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1143,\"start\":1131},{\"attributes\":{\"n\":\"2\"},\"end\":2811,\"start\":2793},{\"attributes\":{\"n\":\"3\"},\"end\":5583,\"start\":5566},{\"attributes\":{\"n\":\"3.1\"},\"end\":7493,\"start\":7479},{\"attributes\":{\"n\":\"4.1\"},\"end\":11488,\"start\":11481},{\"attributes\":{\"n\":\"5\"},\"end\":11501,\"start\":11491},{\"end\":12057,\"start\":12044},{\"end\":12285,\"start\":12266},{\"end\":12462,\"start\":12442},{\"end\":13067,\"start\":13047},{\"end\":13393,\"start\":13379},{\"end\":13738,\"start\":13722},{\"end\":15750,\"start\":15715},{\"end\":16466,\"start\":16435},{\"end\":16486,\"start\":16469},{\"end\":16982,\"start\":16972},{\"end\":17489,\"start\":17479},{\"end\":18272,\"start\":18263},{\"end\":18490,\"start\":18483},{\"end\":19460,\"start\":19451},{\"end\":20549,\"start\":20540}]", "table": "[{\"end\":18261,\"start\":17941},{\"end\":18481,\"start\":18274},{\"end\":19449,\"start\":19385},{\"end\":26233,\"start\":25807}]", "figure_caption": "[{\"end\":17251,\"start\":16984},{\"end\":17477,\"start\":17254},{\"end\":17605,\"start\":17491},{\"end\":17941,\"start\":17608},{\"end\":19385,\"start\":18492},{\"end\":20538,\"start\":19462},{\"end\":25807,\"start\":20551}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7676,\"start\":7668},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9757,\"start\":9748},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10373,\"start\":10365},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10766,\"start\":10758},{\"end\":14775,\"start\":14767},{\"end\":16038,\"start\":16030},{\"end\":16633,\"start\":16625},{\"end\":16765,\"start\":16757}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}