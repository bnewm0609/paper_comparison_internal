{"id": 259252094, "updated": "2023-10-04 23:32:07.264", "metadata": {"title": "Exploring the Robustness of Large Language Models for Solving Programming Problems", "authors": "[{\"first\":\"Atsushi\",\"last\":\"Shirafuji\",\"middle\":[]},{\"first\":\"Yutaka\",\"last\":\"Watanobe\",\"middle\":[]},{\"first\":\"Takumi\",\"last\":\"Ito\",\"middle\":[]},{\"first\":\"Makoto\",\"last\":\"Morishita\",\"middle\":[]},{\"first\":\"Yuki\",\"last\":\"Nakamura\",\"middle\":[]},{\"first\":\"Yusuke\",\"last\":\"Oda\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Suzuki\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Using large language models (LLMs) for source code has recently gained attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems. However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet. To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems. Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance. Furthermore, we observe that Codex relies on variable names, as randomized variables decrease the solved rate significantly. However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems. This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.14583", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-14583", "doi": "10.48550/arxiv.2306.14583"}}, "content": {"source": {"pdf_hash": "4c8fb68ef6a37cdf713685c17963e6ff6c585081", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.14583v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8f75a56ce37783b8971e0b6e2d7a7acb03a0bed9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4c8fb68ef6a37cdf713685c17963e6ff6c585081.txt", "contents": "\nExploring the Robustness of Large Language Models for Solving Programming Problems\n26 Jun 2023\n\nAtsushi Shirafuji \nYutaka Watanobe \nTakumi Ito \nNTT Communication Science Laboratories \u00a7\n\n\nMakoto Morishita \nYuki Nakamura \nNTT Communication Science Laboratories \u00a7\n\n\nYusuke Oda \nNTT Communication Science Laboratories \u00a7\n\n\nJun Suzuki \nNTT Communication Science Laboratories \u00a7\n\n\n\nUniversity of Aizu\nTohoku University\n\n\nExploring the Robustness of Large Language Models for Solving Programming Problems\n26 Jun 2023\nUsing large language models (LLMs) for source code has recently gained attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems. However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet. To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems. Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance. Furthermore, we observe that Codex relies on variable names, as randomized variables decrease the solved rate significantly. However, the state-ofthe-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems. This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations. 2\n\nIntroduction\n\nRecent advances in natural language processing (NLP) have led to the development of large language models (LLMs or large LMs 3 ). Such LLMs have often been trained on datasets containing the source code (programs), and thus they can also generate human-like source code. These models can be applied in various code-related tasks to assist developers, such as generating source code from natural language (code generation) [2][3][4][5][6][7][8][9][10][11][12][13], generating natural language from source code (code summarization) [2,3,5,[12][13][14], translating source code from one programming language into another (code translation) [14][15][16][17], generating source code to complete a partially written program (code completion) [2, 4-6, 14, 18, 19], detecting and correcting errors in source code (e.g., vulnerability detection and program repair) [14,[20][21][22][23][24][25], and classifying approaches or algorithms used in source code (e.g., program classification) [26][27][28].\n\nHowever, the performance of these models depends on the quality of the prompts they receive, as they are originally trained to predict the next word or token given a sequence of previous ones. In practice, LLMs' users need to carefully construct prompts to effectively leverage the LLMs; the phase investigating the prompts to obtain better generation results is called prompt engineering. As the job title of a prompt engineer has been opened recently with a high range of salary, designing a better prompt template is considered essential for better generation in current LLMs. Therefore, a comprehensive grasp of the robustness of these models to the given prompts is crucial to effectively leverage their capabilities.\n\nThe development in this field is extremely fast, and the interpretations of LLMs are not straightforward, which means that the specific accomplishments of current state-of-the-art (SOTA) LLMs are not thoroughly explained. More specifically, it has not been sufficiently clarified whether LLMs understand queries and source code they receive and generate. One possible explanation for their exceptional performance could be that, because of the high capacity of LLMs to memorize training data, they can perfectly respond to queries when identical queries appear in the training data and are being memorized by the LLMs but not otherwise.\n\nIn the context of natural language generation, memorization of training data can be a critical issue regarding leaking personal information [29]. In addition, in the context of code generation, copying some amount of source code from open-source projects can lead to the open-source license infringement issue [30]. Several works have investigated the copying and memorization issues of the texts generated by GPT-based models [31,32]. As related work, Karmakar et al. [33] demonstrated the potential of memorization issues of Codex against programming problems from HackerRank 4 ; Mastropaolo et al. [34] investigated code generation using different but semantically equivalent natural language descriptions; and Wang et al. [35] proposed ReCode, a benchmark for evaluating the robustness of code generation LMs. Although several prior works investigated the capability and limitation of current GPT-based models from the perspectives of memorization, copying, and robustness, to the best of our knowledge, there has been no systematic study on the impact of prompts on the performance of LLMs in solving programming problems.\n\nTo remove doubts and gain a better understanding of the current progress of SOTA LLMs, we explore their ability to generate Python programs. We focus on the task of automatically solving programming problems. As a case study, four popular recent SOTA code generation LLMs, CodeGen [4], Codex [2], InstructGPT [36], and ChatGPT 5 , are selected as automatic programming problem solvers. From various perspectives, we assess the LLMs' capabilities and limitations in understanding problem descriptions and the corresponding generated programs.\n\nWe primarily perform two experiments: (1) formatting problem descriptions and (2) modifying problem descriptions. In the first experiment, we format problem descriptions using several rules. We refer to this as superficial modification since formatting does not alter the problem specification. In contrast, the second experiment alters problem descriptions, which may change the difficulty or solution approach, including both superficial and semantic modifications. The investigation is based on comparing the generated programs by partially modifying the prompts given to the LLMs.\n\nFor experiments, we use the problem descriptions of introductory programming problems provided on Aizu Online Judge (AOJ) [37,38]. We also use hidden test cases to validate the functional correctness of the generated programs.\n\nThe experimental results show that the earlier models, CodeGen and Codex, may not understand the queries and source code in some cases, because the performance is influenced by the formatting and modifications of problem descriptions, even if the models perfectly handle some of the programming problems. On the other hand, the latest models, InstructGPT and ChatGPT, which incorporate reinforcement learning from human feedback (RLHF) [36], show almost the same performance for several modifications in problem descriptions, exhibiting robustness to the modifications.\n\nThe main contributions of our work are to fill the gap and provide insights for developers and researchers on how LLMs are influenced by various prompts (perturbations) for solving programming problems, as well as warning about the pitfalls of using LLMs in practice. a, b = map(int, input().split()) print(a * b, 2 * (a + b)) Generated Program Input : 1 1 Output: 1 4 Input : 23 Figure 1: Task of automatically solving programming problems by an LM evaluated by a judge system based on hidden test cases, an example of a problem named ITP1_1_C from AOJ.\n\nThe rest of this paper is organized as follows. Section 2 introduces the target task of this work, solving programming problems. Section 3 defines the primary research question (RQ) that we investigate throughout this work. Section 4 describes the methodology of how we organize the work to answer the RQ. Section 5 presents and discusses the obtained results. Section 6 reviews related work and provides an overview of LMs for code generation. Finally, Section 7 concludes this paper.\n\nThe original problem descriptions can be accessed via the API 6 . The problem descriptions used in the experiments are available here 7 .\n\n\nSolving Programming Problems\n\nThe task of solving programming problems is to generate complete solution programs from scratch in a specific programming language to solve a given problem using a problem description (Figure 1). Programming problems vary in type and difficulty, from introductory problems, such as input/output processing, simple calculations, if-branches, and for-loops, to advanced problems, such as algorithms and data structures requiring computer science knowledge.\n\nThe correctness of the submitted program is usually evaluated by actually executing the program using hidden test cases on judge systems [39], similar to unit testing in software development. The program is judged correct if it passes all hidden test cases and incorrect otherwise (i.e., if one or more test cases fail). The difference from unit testing is that not only the correctness of the output result of the program but also its execution time and memory usage are considered in the evaluation. Each problem imposes different limitations according to the algorithms and constraints, and the limitations are loosened in each programming language for fairness. For example, the time limit is 4 seconds and the memory limit is 655 MB when using Python 3, but 1 second and 131 MB when using C++ in ITP1_1_A on AOJ 8910 .\n\nSolvers must consider the constraints to select the suitable algorithm and data structure to satisfy the execution time and memory usage. A problem description contains the necessary information 6 http://developers.u-aizu.ac.jp/. 7 https://github.com/ashirafj/aoj-formatted-problems. 8 In ITP1_1_A, the time limit is set to 1 second and the memory limit to 131,072 KB by default 9 . When using Python 3, these limits are loosened by \u00d74.0 for time and \u00d75.0 for memory, whereas neither of them is loosened in C++ 10   to solve the problem: a problem statement, input and output formats, variable constraints (e.g., value types and ranges), and example test cases. However, since the problems for this task are usually based on competitive programming contests, problem descriptions rarely contain hints or instructions that clearly and directly explain how to solve the problem. Solvers are required to decide on the solution approach to the given problem and then implement it as an executable program.\n\nThe results (i.e., the output of the submitted program) must match the specified output format, not only the correctness of the implemented approach. For example, even a missing space or an empty line will be treated as an incorrect answer. In addition, if the implemented algorithm is improper from the perspective of time or space complexity, the program will be judged incorrect because of exceeding the time or memory limit even if the results are correct. For example, if a problem that must be solved using a binary search (time complexity is O(log n)) is solved using a linear search (time complexity is O(n)), the program will be judged incorrect because of exceeding the time limit.\n\nThese specifications, constraints, and evaluation criteria are critical difficulties in solving programming problems.\n\n\nResearch Question\n\nRecent LLMs for source code are considered highly capable of solving introductory programming problems. This may imply that they have the ability to understand problem descriptions. However, instead of interpreting the provided problem description, we suspect that the LLMs are just picking the most pertinent source code from training data based on small fractions of key terms. Therefore, we assume that even minor changes to the given inputs have a great impact on the code generation performance.\n\nOur primary RQ is, Do LLMs interpret given programming problems and generate programs with their thought or just retrieve source code from the most relevant problem in training data on the basis of superficial cues? We conduct an empirical investigation to address this RQ in the following sections.\n\n\nExperiments\n\nWe carry out two types of experiments to address our RQ. First, we format the problem descriptions using a set of predefined rules to examine the differences in generated programs. Second, we modify the problem specifications to observe the effect of the modifications on the generated programs and solved rate. The experimental framework is illustrated in Figure 2.\n\n\nDataset\n\nWe collect a set of programming problems for the experiments, which contains problem descriptions and test cases of the problems. We use introductory problems provided on AOJ [37], an online judge system where users solve given programming problems, and the judge system automatically evaluates the submitted programs using test cases [39]. AOJ provides approximately 3,000 program-ming problems ranging from introductory to advanced, along with example test cases and hidden test cases used for evaluation [38]. Moreover, AOJ provides APIs to access the data and resources 11 .\n\nWe target a course called Introduction to Programming I (ITP1) 12 , which involves a set of 44 introductory programming problems designed for programming beginners. AOJ problems are supported in English, Japanese, or both of them. We use the problem descriptions written in English because ITP1 is supported in both English and Japanese, and LLMs are usually more fluent in English. We exclude four unsuitable problems at the end of the set because they require interpreting an image given in the problem description. Therefore, there are 40 programming problems in the final target dataset.\n\n\nModels\n\nMany Transformer-based [40] autoregressive LLMs trained on source code have been released in the past year, such as Codex [2], AlphaCode [6], PolyCoder [11], CodeGen [4], and InCoder [5].\n\nMost recently, GPT-3.5 series models 13 , such as InstructGPT [36] and ChatGPT, and GPT-4 [41], which are based on Codex, have attracted great attention in not only academia but also industry.\n\nFor the experiments, we primarily use Codex and CodeGen models since both are available and are highly capable of code generation in Python. For comparison, we also use two latest and great capable models from GPT-3.5 series models, InstructGPT and ChatGPT, in the problem formatting experiment. We consider the other models, such as PolyCoder and InCoder, unsuitable in this work since their sizes (2.7B and 6.7B, respectively) are relatively small compared with Codex and Code-Gen (12B and 16B, respectively). Furthermore, although there have been announcements of other highly capable models, such as AlphaCode and GPT-4, unfortunately, we are unable to access them.\n\n\nCodex\n\nCodex 12B [2] is a GPT-3-based [42] LLM fine-tuned on massive source code publicly available on GitHub and further fine-tuned on 10,000 programming problems. Since the model is trained on a large number of programming problems, it has quite high capability of solving programming problems, according to the result of the benchmark against APPS [43], which is designed to evaluate the performance and capability of code generation tasks.\n\nCodex has been used not only for solving programming problems but also in other various tasks, such as solving mathematical problems [44][45][46], fixing bugs [23,24], and generating programming exercises [47].\n\nCodex had offered two models (engines) as API for the limited beta at the time: code-davinci-002 and code-cushman-002 14 .\n\nAlthough code-cushman-002 is much faster, we choose code-davinci-002 for the experiments because it is more capable.\n\n\nCodeGen\n\nCodeGen [4] is a Transformer-based [40] autoregressive LLM, incorporating a conversational paradigm, a system that responds to the natural language provided by a user at each turn, called a multi-turn conversation.\n\nCodeGen was firstly pre-trained on the Pile [48], a large natural language text corpus, and fine-tuned on two self-collected source code datasets. CodeGen-NL is the pre-trained model; CodeGen-Multi is the fine-tuned model trained on a multilingual source code dataset consisting of six programming languages such as C, Python, and Java; and CodeGen-Mono is the fine-tuned model further trained on a monolingual source code dataset consisting of only Python. However, CodeGen has not been fine-tuned on any programming problem datasets, unlike Codex. 11 http://developers.u-aizu.ac.jp/. 12 https://onlinejudge.u-aizu.ac.jp/courses/lesson/2/ITP1/1. 13 https://platform.openai.com/docs/models/gpt-3-5. 14 https://platform.openai.com/docs/models/codex. The family of CodeGen models varies in the size of parameters with 350M, 2.7B (2B), 6.1B (6B), and 16.1B (16B) 15 , and the models are publicly available on the Hugging Face Hub 16 . Since the experiments target Python 3, we choose CodeGen-16B-Mono 17 , which has the best capability for generating Python programs among the family models.\n\n\nGPT-3.5 Models\n\nGPT-3.5 18 models are a family of LLMs that are based on the GPT-3 architecture, especially Codex. From the GPT-3.5 models, we use InstructGPT and ChatGPT.\n\nInstructGPT InstructGPT [36] is trained to follow user instructions and provide detailed responses using an RLHF technique to align with the user's intentions. This ability has many potential applications, such as in customer service, virtual assistants, and education. Moreover, its ability to learn from corrective feedback can help reduce errors, thus improving the accuracy of its responses and enhancing the correctness and readability of the generated programs.\n\nOpenAI offers several InstructGPT-based models (engines) through pay-as-you-go APIs, especially the engines text-davinci-002 and text-davinci-003. The APIs of InstructGPT cost $0.02 for each 1,000 tokens of prompts and completions (i.e., the given problem description and the corresponding generated programs). In this work, we use the most recent and capable engine, text-davinci-003, which is an improved version of text-davinci-002.\n\nChatGPT ChatGPT 19 is a dialogue-based LLM which interacts with users conversationally. It is a GPT-3.5 model optimized for conversation and, thus, a sibling model to InstructGPT. [36]. Chat-GPT has improved its ability to answer follow-up questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.\n\nOpenAI has opened a service to use ChatGPT on GUI, ChatGPT UI 20 , which supports GPT-3.5 and GPT-4. It was initially launched as a free preview, but then OpenAI introduced a paid plan, ChatGPT Plus 21 .\n\nThe ChatGPT-based engines are offered as pay-as-you-go APIs named gpt-3.5-turbo. Although the engine is automatically updated with the latest model iteration, OpenAI also offers snapshot engines that will not receive any updates. The APIs of ChatGPT cost $0.002 for each 1,000 tokens, which is more than 10 times cheaper than InstructGPT-based engines. In this work, we use gpt-3.5-turbo-0301 22 , which is a snapshot version of gpt-3.5-turbo from March 1st, 2023, for reproducibility.\n\n\nGeneration Configuration\n\nThroughout the experiments, we ask the LLMs to generate programs in zero-shot setting, for which we do not provide any examples to demonstrate the expected inputs and outputs.\n\nWe target Python 3 programming language in this work because all the target LLMs are most capable in Python.\n\nWe set the temperature parameter, which determines creativity, to 0.8 for all code generation in this work, following the Codex experiments [2] Although a temperature of zero can produce a mostly deterministic program, a certain amount of creativity is required for evaluating the sensitivity (robustness) to prompts. Also, the maximum number of tokens to generate is set to 256.\n\nFor only ChatGPT, we set the system instruction to \"Solve the following programming problems in Python. You only need to output solution source code in a code block without any explanations in natural language.\" before each code generation. Since ChatGPT is a dialogue-based model unlike the completion-based models such as Codex and InstructGPT, it often generates excessive detailed explanations about the solution approach in natural language and sometimes show pseudo-code before generating a solution code block, when simply given a problem description. To suppress the generation of natural language explanation and to generate only solution source code in Python instead, we use the system instruction that ChatGPT officially supports in API to control the model's behavior throughout the conversation.\n\nFurthermore, for only ChatGPT, we append post-processing to remove backticks if they exist in the first and last lines in the generated programs. Since we ask the model to generate in a code block, the output of ChatGPT is generally constructed as a code block in Markdown format, which encloses source code with three backticks.\n\n\nEvaluation\n\n\nEvaluation Criteria\n\nWe use two quantitative evaluation metrics to evaluate code generation for the experiments of solving programming problems: solved rate and average solved rate.\n\nThe solved rate refers to the proportion of generated programs that successfully solve the given problem, and it focuses on evaluating how well the model can generate correct programs for each problem. The solved rate is defined as Formula 1, where P i is the number of correct programs and F i is the number of incorrect programs.\nSolved Rate = P i P i + F i(1)\nThe average solved rate is the average of the solved rate for all programming problems in our dataset. The average solved rate is defined as Formula 2, where n is the number of programming problems (n = 40 in this work).\nAverage Solved Rate = 1 n n i P i P i + F i(2)\nThere are several choices for evaluation metrics in code generation tasks: Exact Match, BLEU [49], CodeBLEU [50], test case average and strict accuracy [43], success rate [51], pass@k [2], n@k [6], etc. In particular, pass@k and n@k are popular metrics in recent works for evaluating datasets that target tasks of solving programming problems. However, the commonly used metrics focus on evaluating whether the model can solve a problem, rather than evaluating how many generated samples solve each given problem. Since our goal is to measure the models' robustness to different prompts, these metrics are not the most appropriate for this work.\n\n\nExecution Environment\n\nLLMs learned from various types of source code on GitHub have the risk of generating malicious or vulnerable programs, and so users are strongly discouraged from automatically executing the generated programs on the host computer without understanding them [2,52]. Therefore, we prepare an isolated sandbox environment that does not harm the host computer to execute the generated programs safely.\n\n\nFormatting Problem Descriptions\n\nEven with the same problem specifications, making slight modifications to the problem descriptions might significantly impact the generated programs. Therefore, we define several formatting rules for the problem descriptions; these rules are inspired by various public datasets, such as HumanEval [2], CodeContests [6], and APPS [43], aiming to benchmark text-to-code generation tasks. The formatting can be divided into two consecutive phases: problem formatting and prompt formatting.\n\n\nProblem Formatting\n\nIn the problem formatting phase, we format the problem descriptions to explicitly represent the problem specifications. First, we parse the raw problem description written in HTML into plain texts, and second, we manually modify the plain texts into various defined formats as follows. The modifications only change the superficial formats, and do not change any problem specifications. Therefore, we refer to these modifications as superficial modifications.\n\n\u2022 Raw HTML.\n\n\u2022 Parsed plain.\n\n\u2022 AlphaCode-inspired.\n\n\u2022 APPS-inspired.\n\n\u2022 Fully Markdown-formatted.\n\nWe list the example problem descriptions of the problem ITP1_1_C for each formatting in Appendix A.\n\nRaw HTML This format is the original problem description written in HTML, downloaded from AOJ. Figure 4 shows an example of this format.\n\nParsed Plain This format is obtained by automatically removing HTML tags and converting symbols from the Raw HTML format's text to unformatted plain text. We use the Beautiful Soup 23 library in Python to automatically parse the HTML texts. Figure 5 shows an example of this format.\n\n\nAlphaCode-Inspired\n\nThis format is manually modified by us, referring to the prompts used in AlphaCode [6]. Figure 6 shows an example of this format.\n\nIn this format, the programming language (i.e., Python 3) is specified first, and then an instruction is provided to generate a solution to the given problem. Originally, in the AlphaCode's prompts, the ratings and tags for a given problem were specified. However, we omit them in this work since the programming problems we use declare neither ratings nor tags. Furthermore, numbers, variable names, and mathematical symbols are surrounded by the $ symbol and represented in the T E X format, and each line is separated by an empty line.\n\n\nAPPS-Inspired\n\nSimilar to the AlphaCode-inspired format, this format is manually modified by us, referring to the problem descriptions provided by APPS [43]. Figure 7 shows an example of this format.\n\nThe major differences from the AlphaCode-inspired format are that (1) a section name is emphasized by surrounding it with five hyphens, and (2) no instruction sentence is provided. However, same as the AlphaCode-inspired format, numbers, variable names, and mathematical symbols are represented in the T E X format.\n\nFully Markdown-Formatted This format is a hybrid inspired by the AlphaCode, APPS, and Markdown formats. Figure 8 shows an example of this format.\n\nFollowing the Markdown format, a section name is prefixed with the # symbol to indicate the hierarchy of the section. Main sections are prefixed with one # symbol, and subsections are prefixed with two # symbols. Moreover, the examples of input and output cases are represented as code blocks, placing triple backticks before and after the source code. As usual, numbers, variable names, and mathematical symbols are represented in the T E X format.\n\n\nPrompt Formatting\n\nIn the prompt formatting phase, we further format the formatted problem description to construct a prompt. A prompt is an input text for the LLMs. Generally, completion-based LLMs like Codex predict and generate the next tokens for the given prompts, whereas dialogue-based LLMs like ChatGPT are a bit different.\n\nWe define the following formatting rules to construct a prompt, referring to the officially proposed best practices to improve the code generation of Codex 24 . Figure 9 shows an example of a formatted prompt.\n\n\u2022 Specifying the programming language (i.e., Python 3). \u2022 Providing as a docstring or comments. \u2022 Appending instruction commands, an instruction sentence, or not. \u2022 Defining a solve() function or not. \u2022 Providing single import, multiple imports, or not. \u2022 Inserting, removing, or keeping empty lines.\n\nSpecifying Programming Language Codex and CodeGen support not only Python but also many other programming languages. Especially for Codex, the model can generate any programs in a particular programming language by specifying the name of the target programming language. Although the target model of CodeGen is fine-tuned on the Python dataset and specialized for Python, we suspect that explicitly specifying the target programming language can help the model generate more accurate programs since the model is pre-trained on many programming languages. Therefore, throughout the experiments in this work, in the first line of the prompt, we append a comment specifying the Python 3 programming language (i.e., # Python 3).\n\n\nProviding as a Docstring or Comments\n\nThe problem description should be formed as a docstring or comments since the prompt needs to be a part of the Python program. One recommended best practice, which is announced by Codex's official document, is to use docstrings instead of comments because this can produce higher-quality results. Therefore, we investigate why docstrings are better in Codex and which option is suited to CodeGen.\n\nAppending Instruction Commands or a Sentence The prompts used in AlphaCode have an instruction sentence to solve the given problem, so just giving the problem description may cause the LLMs to struggle to decide what to do next. Therefore, we append two types of instructions to ask the LLMs to start solving the given problem: (1) instruction commands and (2) an instruction sentence.\n\nFor instruction commands, to indicate the range of the problem description, we append <START PROBLEM DESCRIPTION> at the beginning and <END PROBLEM DESCRIPTION> at the end of the prompt. For an instruction sentence, to indicate the end of the problem description, we append \"Write code to solve the above problem.\" at the end of the prompt.\n\nDefining a solve() Function Another best practice mentioned in Codex is to place a description of the function inside the function as a docstring. This can help Codex understand what we want the function to accomplish. Therefore, we provide the problem description as a docstring by defining the function solve().\n\nHowever, different from the case illustrated in the best practice, this format may not be effective in our experiments for the following reasons: (1) We ask the LLMs to generate an entire program to solve the given problem, rather than a function body with a specific purpose. (2) We can provide the LLMs with very little additional information since the defined function name (i.e., solve) is too common.\n\nProviding a Single or Multiple Imports According to the best practices, providing the necessary libraries can be beneficial. In this experiment, we examine two types of imports, namely, (1) a single import and (2) multiple imports.\n\nFor a single import, only the math library is provided in the prompt, which is surely required in some problems. For multiple imports, several commonly used libraries are provided in the prompt, including os, re, sys, and math. Although the libraries can be used in some solution approaches, they are not essential libraries except the math library. Because problems vary in their solution approaches, even if unnecessary libraries are imported in the prompts, we expect the LLMs to ignore libraries when they are not necessary for the solution of a given problem.\n\nInserting or Removing Empty Lines We also investigate the sensitivity of the LLMs to the empty lines of the problem descriptions because suitable breaks between sentences enhance text readability. Accordingly, we conduct two format types: (1) inserting an empty line between each sentence and (2) removing all empty lines from the problem description.\n\n\nModifying Problem Descriptions\n\nThere is a concern that the LLMs memorize typical programming problems and generate the corresponding most relevant source code, which is contained in the training data. We suspect that the performance for the same problem can be worsen if the LLMs simply memorize problems. Therefore, we conduct four experiments where we alter the problem description to assess whether the LLMs understand the details of the given problem description. We modify not only the superficial formatting but also the information related to problem specification.\n\nIn the following experiments, generating correct programs is not mandatory because the experiments aim to determine whether the LLMs can generate different programs from the original one to follow the modification. Furthermore, even if the modified problem specification is uncommon and difficult, we expect the LLMs to generate programs that attempt to solve the modified problem.\n\n\nRandomized Variable Names\n\nAccording to the qualitative evaluation of our early experiments, we observed that the generated programs often use the same variable names defined in the problem description. This suggests that the LLMs may be able to catch the variable names from the problem description. However, almost only typical variable names are used in the problem description, such as x, y, W , and H. Therefore, we suspect that the variable names used in the problem description might be the key to retrieving the most relevant solution programs from the training data.\n\nThis experiment aims to investigate (1) how an LLM catches the variable names defined in the problem description and (2) whether the generated program uses the defined variable names or defines new ones. We conduct three types of randomization: UUID variables, shuffled variables, and ABC variables.\n\nUUID Variables To ensure that the defined variable names do not appear in the training data and the LLM has never seen them, we anonymize all variable names in the problem descriptions with universally unique identifiers (UUID) [53]. For practical purposes, UUIDs are unique, with an extremely low probability of duplication. Moreover, to ensure that the LLM can use the anonymized variable names as actual ones in the Python program, we only use the UUIDs that start with an alphabet, and hyphens are removed. Figure 3 shows a partial example of a problem description, anonymizing variables a and b with 6cf 0633469244258255a9e1c0a973a and a58be6f 6e5234aedb093df 3eab95cf , respectively.\n\n...\n\n\n## Input\n\nThe length $c6cf0633469244258255a9e1c0a973a1$ and breadth $a58be6f6e5234aedb093df3eab95cf29$ of the rectangle are given in a line separated by a single space.\n\n...\n\n\n## Constraints\n\n$1 \\leq c6cf0633469244258255a9e1c0a973a1, a58be6f6e5234aedb093df3eab95cf29 \\leq 100$ ...  Shuffled Variables Since the UUIDs are unique and uncommon, the experiment setting might be much more difficult than we assumed. Furthermore, since the UUIDs are long, 32 characters excluding hyphens, an anonymized variable name can be tokenized into too many tokens and thus can be challenging to treat as one variable name. Using the official tokenizer of Codex 25 , we confirmed that UUID variables are tokenized into 20 tokens on average.\n\nTo simplify the experimental setting, in this randomization, the original variables defined in the problem description are shuffled. For example, if H originally indicates the height and W the width, they will indicate the width and height, respectively. This randomization especially aims to determine whether the common variable names affect the LLM in situations where the shuffled variable names are guaranteed to be common, being originally used, but the usage is different and unusual.\n\nABC Variables In this randomization, the original variables are replaced with variables named a, b, c, \u00b7 \u00b7 \u00b7 . If the original variables are already a, b, c, then the variables will be replaced with x, y, z. We expect that even if the solved rate decreases, the decreasing rate is better than the shuffled variables. This randomization removes the superficial information; but does not intend to confuse the LLM about the meaning of variable names.\n\n\nAnonymized Output Strings\n\nSimilar to Section 4.5.1, we doubt whether the LLM generates the specified output strings by interpreting the problem description. We suspect that the LLM uses the same output strings present in the training data because the specified output strings in the problem descriptions are also common. Therefore, this experiment aims to investigate whether the LLM catches the output strings specified in the problem description.\n\nWe anonymize all output strings specified in the problem description using UUIDs. Unlike the specification of UUIDs used in Section 4.5.1, for this experiment, the anonymized output strings may start with a number, as they are assumed to be defined as string literals rather than variable names.\n\n\nRewording Synonyms\n\nWe suspect that the LLM depends on some specific words appearing in the problem description. For example, we suspect that the solved rate can be decreased by simply rewording synonyms, owing to the bias of the words that often appears in the training data, or because the LLM cannot find the meanings of certain words that humans would consider almost the same with no difference in meaning.\n\nThis experiment investigates whether synonyms change the solved rate. We replace some words with synonyms to an extent that does not adversely affect the meaning of the problem specification. For example, \"ascending order\" is replaced with \"increasing order\", and \"print\" is replaced with \"output\".\n\n\nInverse Problem Specification\n\nThe LLMs can usually solve a problem with a high solved rate if the problem specification is typical and the problem description is clearly stated. We consider that the LLM memorizes the solution approach or the algorithm from some phrases in the problem description. Therefore, we conduct an experiment of semantic modification to determine (1) whether the LLM can follow the modification of problem specification even if it is uncommon or difficult and (2) how sensitive the LLM is to the modification.\n\nWe perform common and uncommon modifications to make the problem specification opposite but still valid. For example, modifying the sentence from \"output values in ascending order\" to \"output values in descending order\" is a common modification because both settings are typical for introductory programming with almost the same difficulty. On the other hand, modifying the sentence from \"determining whether a circle is arranged inside a rectangle\" to \"determining whether a rectangle is arranged inside a circle\" is an uncommon modification because the new setting is a bit more difficult than the original, but it is still valid and can be solved.\n\n\nResults and Discussion\n\n\nFormatting Problem Descriptions\n\nIn the experiments in this section, each LLM generates 100 programs for each of the 40 problems, and the judge system automatically validates the results. On average, Codex shows three times better performance than the CodeGen. Codex's successor, InstructGPT, doubles the average solved rate, and ChatGPT shows an even greater improvement. This large performance difference reflects the effect of fine-tuning for programming problems, as Codex is fine-tuned whereas CodeGen is not.\n\n\nProblem Formatting\n\nFor Codex, the average solved rate is improved by simply parsing the Raw HTML problems into plain text, and the fully Markdown-formatted problems show the highest average solved rate with a 9.0% performance improvement compared with the Raw HTML format. In Codex, the more explicitly formatted problem descriptions enhance the average solved rate of code generation. Accordingly, the effective formatting rules in Codex are (1) inserting an empty line between each sentence, (2) separating section names, and (3) representing example input and output cases as code blocks. However, we did not find a clear improvement in the other models.\n\nConversely, the improved performance in problem formatting in Codex suggests that the model is highly influenced by the superficial modification of the problem description and does not understand the problem description in detail, since problem formatting affects only the problem description while keeping the problem specification the same.\n\nFor InstructGPT and ChatGPT, the Raw HTML format yields the best performance, but it is not the best format in both CodeGen and Codex. Furthermore, the variance in the average solved rate is decreasing as the models evolve, except in CodeGen since its performance is extremely low. Although there still remains a high possibility of data leakage, robustness to superficial modifications may increase, as observed in Codex.\n\nFor the characteristics of the generated programs, Table 2 shows the average number of lines of generated programs, along with the average of users' solution programs for reference. Lines indicates the pure number of lines, also known as lines of code (LOC), which is simply calculated as the number of lines in the generated program. CommentLines indicates the number of comment lines, including lines that form docstrings (i.e., multi-lined comments). EmptyLines indicates the number of empty lines, including lines that only contain white spaces. ProcessLines indicates the number of actual processing lines, that is, lines that are neither comment or empty lines. However, empty lines of comments or docstrings are counted as comment lines, not empty lines.\n\nAs the model evolves from Codex to ChatGPT, the number of lines becomes smaller for all metrics, indicating that the generated program is becoming more concise. Although the ProcessLines is In ChatGPT, almost no comments are generated. As the number of comment lines in users' solutions is also very small, we consider that ChatGPT might be designed not to generate excessive comments in the programs. However, this might be influenced by the system instruction that we instructed ChatGPT to generate only a code block and to suppress generating natural language explanations, as introduced in Section 4.2.4. Although we intended to suppress generating explanations outside of a code block, not comments in the programs, this may cause the suppression of comments in natural language.\n\nSince moderate comments and empty lines are essential for readability, we cannot directly conclude that these decreases in the number of lines in the generated programs indicate the capability of the LLMs. However, an interesting observation is that the SOTA LLMs attempt to make the programs concise.  Table 3 shows the average solved rate for each type of prompt formatting, conducted on Codex and CodeGen. Providing the problem description as a docstring (Docstring) is the baseline because the problem formatting experiments are conducted using this format.\n\n\nPrompt Formatting\n\n\nProviding as a Docstring or Comments\n\nAlthough providing the problem descriptions as comments do not modify the problem descriptions at all, it significantly decreases the average solved rate in both models.\n\nOur qualitative evaluation revealed that, in this format, most of the incorrect programs do not try to solve the problem, and they continue to write comments instead. This qualitative observation agrees with our quantitative finding that 41.4% of lines in the generated programs, excluding the given prompts, are comments when providing in the comment format, whereas only 10.9% are comments when providing in the docstring format in Codex. The same issue of increasing comment proportion from the comment format to the docstring format is observed in CodeGen (37.9% \u2192 79.4%).\n\nThis results suggest that the worsened performance in the comment format is due to the unclear scope of the problem description. On the other hand, docstring format can explicitly indicate the beginning and end of the problem description by \"\"\" symbols to define the docstring.\n\n\nAppending Instruction Commands or a Sentence\n\nThe effect of appending instruction commands or an instruction sentence is different in Codex and CodeGen: a significant decrease in Codex and not significant in CodeGen.\n\nAs for the comment format in both models, the average comment proportion is reduced by appending instruction commands ( ). Accordingly, the docstring format is sufficient to indicate the end of the problem description. Moreover, our qualitative evaluation showed that many generated programs are affected by the instruction and generate similar comments, such as <START YOUR CODE> and <END YOUR CODE>.\n\nDefining a solve() Function Defining a solve() function shows quite the opposite effects in CodeGen and Codex. With appending invocation as post-processing, CodeGen shows a significant improvement and performs best, whereas Codex shows a significant decrease in performance.\n\nIn Codex, defining a solve() function significantly decreases the average solved rate. Too much emphasis is placed on implementing the defined function, and many generated programs do not invoke the function. When we append post-processing to invoke the defined function if the generated program does not invoke it, the average solved rate increases by 11.3% (14.8% \u2192 26.1%). However, the one with appending invocation is still less accurate than the docstring format. Although this format can be better to implement a single function definition, it is considered inappropriate if flexibility is required, such as solving programming problems. This format enforces the model to solve using only one function.\n\nIn CodeGen, as opposed to Codex, the average solved rate is significantly improved and shows the best performance when appending function invocation. This result strongly reflects that CodeGen excels at implementing function definitions rather than a whole program. However, similar to those in Codex, many generated programs in CodeGen only implement the function body and do not invoke the defined function. Furthermore, since CodeGen is based on conversational program synthesis, it is considered unsuitable for automatic problem-solving where only the problem description is given.\n\nProviding a Single or Multiple Imports We observed that the more the imports provided, the worse the models' performance. Therefore, we suggest not to provide imports and to leave the entire code generation to the model.\n\nBoth CodeGen and Codex are affected by the provided imports, and to use the libraries, they are forced to generate more LOC that are unnecessary for solving the given problem. Although some problems require the provided imports and others do not, the average solved rate is decreased in both.\n\nWe also found that appending imports in front of the generated program as post-processing (Docstring + appending imports and Comments + appending imports) does not improve the average solved rate. This shows that no generated programs miss necessary imports, and the models can import the libraries when they are required for the solution.\n\nWe conclude that providing imports does not help solve the given problem more accurately even if the problem requires them.\n\nInserting or Removing Empty Lines For inserting and removing empty lines, both formats in both models slightly decrease the solved rate, which demonstrates that LLMs are sensitive even to the empty lines. Moreover, the results do not support our expectation that inserting empty lines can make the problem description more readable. However, inserting is better than removing empty lines. Accordingly, we recommend using an appropriate number of empty lines (i.e., not inserting too much or removing too much) for better code generation.\n\n\nModifying Problem Descriptions\n\nIn this section, we focus on using Codex in our experiments, as the results of problem and prompt formatting have shown that Codex performs better than CodeGen in solving programming problems. We manually evaluate the 20 programs generated by Codex for each problem.  Table 4 illustrates that there is a statistically significant difference between the original and randomized variables for all types of randomization, using the Wilcoxon signed-rank test where the significance level is set to 0.05. This finding suggests that the model heavily relies on the information of the variable names defined in the problem description, although the problem specification is not changed.\n\n\nRandomized Variable Names\n\nBoth UUID and shuffled variables show a significant drop in the average solved rate, and the drop caused by UUID variables is larger. This result indicates that the UUID variables complicate the problem, as we considered in Section 4.5.1. However, these decreases in both variables suggest that the larger drop in the UUID variables is not only due to its unique format and the large number of tokens to be tokenized.\n\nMoreover, it is seen that ABC variables also decrease the average solved rate significantly. Unlike the UUID and shuffled variables, ABC variables are assumed not to distract the model by variable naming and to anonymize the meaning of the variables. The decrease in ABC variables further suggests that the model relies on the information of the variable names.\n\nHowever, the use of confusing variable names should be avoided, and the variable names that play some typical roles, such as H for height, W for width, and r for radius, should be used only as such for better code generation. For future work, we suggest that having many variations of variable names used in the training data may mitigate this issue.\n\nAdditionally, Table 5 shows that the programs generated for the modified problems rarely use the anonymized variable names, although many generated programs use the defined variable names in the original problems, and the UUID variables are ensured to be used as actual variable names. This implies that the knowledge of the LLM about variable naming refuses to use the anonymized variable names since they are uncommon. A similar observation is reported in the past study [54], Table 6: Results of anonymized output strings, with 20 programs generated for each problem.\n\n\nAnonymized Output Strings\n\nUsing Unrelated Strings indicates the proportion of generated programs that attempt to output unrelated strings, which are not specified in the problem description.  Table 6 shows the target output strings to be anonymized and the change in their solved rates. Compared with that of the original output strings, the average solved rate is significantly decreased when anonymized.\n\nOur qualitative evaluation indicates that the reason for the decreasing solved rate is the unique and uncommon format of the anonymized strings. We observed a few cases where the incorrectly generated programs regarded the anonymized output strings as encoded strings and attempted to output by encoding some strings. For instance, we observed a case of encoding an integer from 0 to 99 using a hash function, SHA-1, using the hashlib library and outputting its hexadecimal letters. However, we also found that they rarely attempt to output completely unrelated strings that do not appear in the problem description.\n\nThese results indicate that the LLM can understand which string is the output string although it often fails to find meaning in the anonymized output strings. Table 7 shows the result of rewording synonyms. This result does not meet our expectation that the code generation depends on a particular word in the problem description and rewording it would change the solved rate. Changing the format would change the solved rate significantly, but not in rewording. The results of the inverse problem specification and modifications details (the original and modified sentences) are shown in Table 8. Overall, the average solved rate is decreased in modified problems, although many generated programs could follow the solution approach for the modified version.\n\n\nRewording Synonyms\n\n\nInverse Problem Specification\n\nFor ITP1_2_C, which is a problem requiring output values in descending order, the solved rate for the modified problem is significantly decreased from 71% to 40%. The only additional processing required for the modified problem is to set the reverse order argument for the sorting function or to reverse the sorted list. Since the modified problem is also quite common, its difficulty is considered almost equivalent to the original version. Assuming the LLM comprehended the original problem, the modified problem is expected to have a similar solved rate.\n\nIn fact, we ascertain that the LLM can adapt to the modifications and strive to address the modified problem accordingly. Most of the generated programs employ correct solution approaches, such as attempting to reverse the given list of values, but some non-critical errors cause the wrong answer. The primary errors in the generated programs include receiving unnecessary inputs, outputting unnecessary strings, executing similar processes multiple times, and using incorrect output format (e.g., missing newlines and spaces).\n\nA similar issue arises in the modified problem requiring the determination of whether the values are in descending order (Inversion of ITP1_2_B). In this case, we find 3 out of 20 programs where the generated programs ignore the modification and solve the original problem. In other words, the LLM can generate programs implementing exactly the opposite solution approach for the given problem.\n\nAs for the uncommon modification (Inversion of ITP1_2_D), which is the problem requiring the determination of whether a rectangle is arranged inside a circle, none of the generated programs successfully solve the modified problem. For the modified problem, the expected solution is to calculate the distance from the circle's center to each of the rectangle's four corners and verify that the distances are less than or equal to the circle's radius. It is noteworthy that 8 out of 20 generated programs attempt to adapt to the modification and calculate the distances required to solve. However, although it is expected that the increased difficulty of the modified problem can lead to a lower solved rate, we expected that the modified problem should also be solved. Furthermore, like the case with the inversion of ITP1_2_D, two generated programs ignored the modification and solved the original problem instead.\n\nThese results suggest that code generation is affected by statistical biases, such as typical and common programming problems. We also conclude that, despite their ability to solve problems, LLMs can generate programs that target the exact opposite specification or output unnecessary source code.\n\n\nRelated Work\n\nVarious code-related tasks, such as code generation and code understanding, are handled by LMs. Examining their capabilities and limitations have attracted research interest in recent years.\n\n\nLMs for Code\n\nHere, we introduce the overview of LMs trained on source code aiming to perform code-related tasks. There are three main types of Transformer-based [40] LMs: encoder models, decoder models, and encoder-decoder models.\n\nEncoder Models Encoder models such as BERT [55] and RoBERTa [56] are models that use only the encoder part of the Transformer. Several BERT-based models for code, such as CodeBERT [57], CuBERT [58], and SynCoBERT [59], were proposed. They excel at downstream tasks requiring an understandings of source code, such as clone detection, defect detection, code summarization, and code search. TreeBERT [60] uses not only the source code and natural language but also the syntactic structure of source code, based on abstract syntax tree (AST). GraphCodeBERT [61] also uses graph data flow between variables, further transformed from the AST.\n\nDecoder Models Decoder models such as GPT-3 [42] and PaLM [8] are auto-regressive models that use only the decoder part of the Transformer. Many auto-regressive LLMs for code, such as Codex [2], PolyCoder [11], CodeGen [4], InCoder [5], and PanGu-Coder [9], were proposed in the past year. While the number of parameters of ChatGPT and GPT-4 [41], which are also trained on source code but private, is considered huge, PaLM-Coder [8], which has 540B parameters, is currently the largest LM trained on source code.\n\nEncoder-Decoder Models Encoder-decoder models, also known as sequence-to-sequence (seq2seq) models, such as BART [62] and T5 [63], use both encoder and decoder parts of the Transformer. These models excel in text-to-text conversion tasks such as text-to-code generation, code-totext summarization, code-to-code translation, and discriminative tasks, such as program repair, clone detection, and defect detection [64]. CoTexT [65], PLBART [15] based on BART, and CodeT5 [12] based on T5 are representatives of encoder-decoder models for code. AlphaCode [6] focused on solving competitive programming problems by adopting an encoder-decoder Transformer architecture to tackle code generation as a seq2seq translation task from a problem description to a solution source code because bidirectional representation can be helpful in understanding the problem description. TransCoder [16] has demonstrated high performance in code translation by applying the back-translation technique used in machine translation [66].\n\nOther Models In addition to simply using pre-trained LMs, various techniques are applied to enhance code generation. Parvez et al. [3] attempted to improve code generation and summarization using natural language retrieval techniques, and Washio and Miyao [67] and Zhou et al. [68] improved code generation for unknown libraries by retrieving the relevant documentation inspired by the behavior of human programmers referring to the API documentation while writing code. Recent studies have incorporated reinforcement learning techniques to reward compilability of generated programs to ensure syntactic correctness [10,69]. Executing example test cases is a way to select a correct program from generated samples [6,70]. TransCoder-ST [17] applied an automated unit test generation tool to filter out the invalid generated programs, and CodeT [7] used an LLM to generate test cases along with programs to select the most suitable generated program. UniXcoder [71] can change its architecture by switching between three modes, encoder-only, decoder-only, and encoderdecoder, to efficiently support various tasks.\n\n\nSolving Programming Problems Using LMs for Code\n\nLLMs pre-trained on a large amount of source code can perform several code-related tasks despite not being fine-tuned for specific tasks. Solving programming problems can be treated as a text-tocode generation task, converting a problem description written in natural language into a solution program written in a programming language.\n\nBalog et al. [72] conducted early research on solving competitive programming problems using deep learning. They proposed a framework, DeepCoder, that generates a program written in a domainspecific language from a problem description and several input/output examples. AlphaCode [6] is a model focused on solving competitive programming problems, trained on GitHub source code and fine-tuned on a competitive programming dataset, CodeContests [6], which includes CodeNet [73]. It achieved a middle or above-middle ranking on average in programming competitions, and it is equivalent to an intermediate-level programmer. Codex outscored most students [74] in solving introductory programming problems for first-year university students.\n\n\nCode Understanding of LMs for Code\n\nThe understanding of code by LMs is an important factor in their reliability across a range of applications, their generation results, and their robustness to prompts for practical use. There are studies to investigate what information LMs retain, pay attention, or depend on.\n\nProbing Probing is a diagnostic task aiming to investigate whether a pre-trained model contains specific information [75]. Troshin and Chirkova [54] probed the code understanding of several widely used pre-trained models for code. The results showed that they contain information about code syntactic structure and correctness, variable naming, etc.\n\nImprovement Techniques Kuznia et al. [76] summarized problem descriptions to improve the quality of generated programs. Codex outperformed the baseline (original problem descriptions) by 8.13% by removing information such as human characters and background stories from the problem description. However, the results indicated that the generation is affected by such information.\n\n\nMemorization Issues\n\nThere is concern about copying or memorization issues of the texts generated using GPT-based models [31,32], as gaining interest in, such as leaking personal information [29] and license infringement [30].\n\nFrom the perspective of the infringement of open-source software licenses, Ciniselli et al. [30] reported that longer generated code is unlikely to be copied from the training data although this depends on the length of the generated code. Similarly, in the NLP field, McCoy et al. [77] showed that text generated using smaller n-gram models is substantially less novel than human-generated text; however, text generated using larger n-gram models is as novel as or even more novel than human-generated text.\n\nHowever, as the most related work, Karmakar et al. [33] stated that Codex could not solve the modified version of problems that was initially solved perfectly. They also reported that some generated programs solve problems with completely different specifications, which is similar to our report on the inverse problem specification in Section 5.2.4. For example, although they modified the problem statement from \"print the sum of the elements in a set A\" to \"print the product of the elements in set A\", the model still generated the solution for the original problem statement, i.e., generated a program to print the sum. They stated that the Codex showed clear signs of memorization because it can also generate complete and correct programs even if significant information is missing. Also, Sontakke et al. [78] found that Transformer-based LMs, which aim to summarize source code, rely heavily on comments, function names, and variable names, and masking such information negatively impacts the generated summaries.\n\nTo defend against the memorization issue, Lai et al. [79] performed two perturbations (i.e., variations) of the collected problems as pre-processing to construct a benchmark dataset: surface perturbations (i.e., superficial modifications) and semantic perturbations (e.g., inverse problem specification).\n\nRobustness ReCode [35] is a benchmark dataset for evaluating robustness of code generation LMs. Various perturbations (variations) were performed on the prompts given to the LMs to evaluate the robustness of the LMs to superficial modifications. Mouselinos et al. [80] proposed a new framework to identify biases (i.e., defects of robustness) in several LLMs against several modifications for each block of a function: name block, description block, and example block.\n\nMastropaolo et al. [34] investigated code generation in GitHub Copilot using different but semantically equivalent (i.e., superficially modified) natural language descriptions. They observed that the modifications impacted the correctness of the generated code by \u00b128%. Similarly, in arithmetic reasoning, Shi et al. [81] observed that the performance of LLMs is dramatically decreased by adding irrelevant sentences to the problem descriptions.\n\n\nConclusion\n\nIn this paper, we evaluated the robustness of popular LLMs for source code, Codex, CodeGen, InstructGPT, and ChatGPT, in solving programming problems by varying the prompt formats and modifying the problem descriptions.\n\nOur results showed that the newer models which incorporated the RLHF technique have stronger robustness to the formatting of problem descriptions. On the other hand, Codex showed a 9.0% performance improvement, representing the difference between the worst and best formats. The prompt formatting, conducted on Codex and CodeGen, significantly affected the performance in both models, with a 25.9% improvement in Codex and a 14.7% improvement in CodeGen.\n\nWe also found that Codex heavily relies on the information of the variable names defined in the problems, as using unique variable names such as UUID variables caused the largest drop in the average solved rate, and the shuffled variable names had a similar impact. This suggests that variable names play an important role in code generation performance. Conversely, it is effective to use appropriate variable names in prompts, and we suggest not to use typical variable names in a wrong context. Furthermore, we observed that the model rejected using the anonymized variable names in their generated programs while using the original variable names.\n\nThe results indicated that the solved rate is greatly affected by superficial modifications of problem descriptions in earlier models, which suggests that the models may not have a deep understanding of the problem description. Nevertheless, we observed that Codex could adapt its solution approach in response to semantic modifications such as inverse problem specification. However, we also observed that the model rarely generates programs solving more general problems that are the exact opposite of the given problem, which may be due to statistical biases in the training data.\n\nAlthough an LLM can solve certain problems at a certain level, this does not necessarily imply that the model understands the problem and can solve other problems at the same level. Our findings indicate that slight modifications to problem descriptions can significantly impact code generation performance; nonetheless, the newer models are gaining robustness, and the influence is being mitigated models evolve.  \n\n\nA Problem Formatting Examples\n\n\nRectangle\n\nWrite a program which calculates the area and perimeter of a given rectangle.\n\n\nInput\n\nThe length a and breadth b of the rectangle are given in a line separated by a single space.\n\n\nOutput\n\nPrint the area and perimeter of the rectangle in a line. The two integers should be separated by a single space.    \n\n\n# Problem\n\nWrite a program which calculates the area and perimeter of a given rectangle.\n\n\n## Input\n\nThe length $a$ and breadth $b$ of the rectangle are given in a line separated by a single space.\n\n\n## Output\n\nPrint the area and perimeter of the rectangle in a line. The two integers should be separated by a single space.     \n\u00a1 \u00a2 \u00a3 \u00a4 \u00a5 \u00a6 \u00a7 \u00a8 \u00a9 h ! 5 \" # $ % & ' ( ) 0 1 2 3 4 6 7 8 9 @ A B C D E F G H I P Q R S T U V W X Y a b c d e f g i p q r s t u v w x y g ## Input d e f h i j k l m n o p q r s t u v w x y z { | } ~ \u00a1 \u00a2 \u00a3 \u00a4 \u00a5 \u00a6 \u00a7 \u00a8 \u00a9 \u00aa \u00ab \u00ac \u00ae \u00af # # Output\u00b0\u00b1 \u00b2 \u00b3 \u00b4 t he area \u00b5 \u00b6 \u00b7 \u00b8 \u00b9 \u00ba \u00bb \u00bc \u00bd \u00be \u00bf \u00c0 \u00c1 \u00c2 the rectangle \u00c3 \u00c4 a \u00c5 AE \u00c7 \u00c8 \u00c9 The \u00ca \u00cb \u00cc \u00cd \u00ce \u00cf \u00d0 \u00d1 \u00d2 \u00d3 \u00d4 \u00d5 \u00d6 \u00d7 \u00d8 \u00d9 \u00da be \u00db \u00dc \u00dd \u00de \u00df \u00e0 \u00e1 \u00e2\nFigure 2 :\n2Illustration of the experiments consisting of problem formatting, prompt formatting, code generation, and evaluation.\n\nFigure 3 :\n3A partial example of a fully Markdown-formatted problem (ITP1_1_C), with anonymized variables a and b.\n\nFigure 4 :\n4Example of Raw HTML problem (ITP1_1_C).\n\nFigure 5 :\n5Example of parsed plain problem (ITP1_1_C).LANGUAGE IS python3CORRECT SOLUTION Write a program which calculates the area and perimeter of a given rectangle.InputThe length $a$ and breadth $b$ of the rectangle are given in a line separated by a single space. area and perimeter of the rectangle in a line. The two integers should be separated by a single space.\n\nFigure 6 :\n6Example of AlphaCode-inspired problem (ITP1_1_C). area and perimeter of the rectangle in a line. The two integers should be separated by a single space.\n\nFigure 7 :\n7Example of APPS-inspired problem (ITP1_1_C).\n\nFigure 8 :\n8Example of fully Markdown-formatted problem (ITP1_1_C).\n\nFigure 9 :\n9Example of formatted prompt of fully Markdown-formatted problem (ITP1_1_C).\n\nTable 1 :\n1Average solved rate (%) for each type of problem formatting, generating 100 programs for each of the 40 problems. The best formatting in each model is shown in bold.Table 1shows the average solved rate for each type of problem formatting, conducted on four models, CodeGen, Codex, InstructGPT, and ChatGPT.Formatting \nCodeGen Codex InstructGPT ChatGPT \n\nRaw HTML \n10.9 \n30.9 \n75.9 \n90.1 \nParsed plain \n9.2 \n34.7 \n73.5 \n89.0 \nAlphaCode-inspired \n9.7 \n35.2 \n72.0 \n88.7 \nAPPS-inspired \n11.7 \n39.3 \n73.7 \n88.6 \nFully Markdown-formatted \n9.9 \n39.9 \n74.5 \n89.0 \n\nAverage \n10.3 \n36.0 \n73.9 \n89.1 \nVariance \n1.01 \n13.61 \n2.04 \n0.36 \n\n\n\nTable 2 :\n2Average number of lines, comment lines, empty lines, and actual processing lines in programs generated for fully Markdown-formatted problems. Values in parentheses indicate the proportion. Lines = CommentLines + EmptyLines + ProcessLines.Model \nLines CommentLines EmptyLines ProcessLines \n\nCodeGen \n30.8 \n11.7 (38.0%) 8.2 (26.6%) 10.9 (35.4%) \nCodex \n19.1 \n2.1 (11.0%) 6.0 (31.4%) 11.0 (57.6%) \nInstructGPT \n12.4 \n1.1 (8.9%) 2.8 (22.6%) \n8.5 (68.6%) \nChatGPT \n10.2 \n0.1 (1.0%) 1.9 (18.6%) \n8.1 (79.4%) \n\nUsers' Solutions \n15.7 \n0.4 (2.5%) 2.9 (18.5%) 12.4 (79.0%) \n\nequivalent in CodeGen and Codex, CodeGen generates much longer comment lines and empty lines \nthan Codex. CommentLines and EmptyLines can be decreased by simply removing the comment \nand empty lines, whereas the decrease in ProcessLines suggests that the program itself becomes \nmore concise. \n\n\n\nTable 3 :\n3Results for each type of prompt formatting, generating 100 programs for each of the 40 problems. ASR (%) indicates the average solved rate, ACP (%) indicates the average comment proportion in the generated programs, and AL indicates the average number of lines of generated programs including empty lines. The best ASR in each model is shown in bold.Codex \nCodeGen \n\n\n\n\n41.4% \u2192 40.4% in Codex and 79.4% \u2192 47.2% in CodeGen) or sentences (41.4% \u2192 36.3% in Codex and 79.4% \u2192 59.6% in CodeGen). Accordingly, they help indicate the end of the problem description. In contrast, in the docstring format, the average comment proportion is increased in both models with instruction command (10.9% \u2192 23.9% in Codex and 37.9% \u2192 40.6% in CodeGen) and with instruction sentence (10.9% \u2192 14.3% in Codex and 37.9% \u2192 42.3% in CodeGen\n\nTable 4 :\n4Results of randomized variable names, with 20 programs generated for \neach problem. \nAverage Solved Rate (%) \n\nRandomization \nn Original \nRandomized Change Rate (%) P-value \n\nUUID \n10 \n54.9 \n34.0 \n-38.1 < 0.05 \nShuffled \n9 \n52.0 \n32.8 \n-37.0 < 0.05 \nABC \n10 \n54.6 \n42.0 \n-23.1 < 0.05 \n\n\n\nTable 5 :\n5Proportion of generated programs using UUID variables \nwith the same variable names defined in the problem description for \neach problem, with 20 programs generated for each problem. \nUsing Defined Variables (%) \n\nProblem \nOriginal Variables Original \nAnonymized \n\nITP1_1_C a, b \n80 \n30 \nITP1_2_B a, b, c \n95 \n20 \nITP1_2_D H, W, x, y, r \n100 \n5 \nITP1_3_B x, i \n60 \n5 \nITP1_3_C x, y \n80 \n0 \nITP1_3_D a, b, c \n85 \n15 \nITP1_4_B r \n75 \n0 \nITP1_5_A H, W \n60 \n5 \nITP1_5_B H, W \n30 \n15 \nITP1_5_C H, W \n60 \n5 \n\nAverage \n73 \n10 \n\n\n\nTable 7 :\n7Results of rewording synonyms, generating 100 programs for \neach of n problems. No statistically significant differences were found \nbetween any synonyms using the Wilcoxon signed-rank test. \nRewording \nAverage Solved Rate (%) \n\nOriginal \nReplaced \nOriginal \nReplaced \nn P-value \n\nprint \noutput \n74.1 \n74.6 12 0.4750 \ncalculate \ncompute \n56.4 \n57.6 \n8 0.7422 \ninteger \nnumber \n69.7 \n70.6 \n7 0.9156 \nread \nreceive \n67.8 \n66.3 \n6 0.6875 \nascending increasing \n70.5 \n73.0 \n2 0.5000 \nconvert \nparse \n59.0 \n58.5 \n2 1.0000 \n\n\n\nTable 8 :\n8Results of inverse problem specification, with 20 programs generated for each problem. Fol-\nlows is the proportion of generated programs that follow the modification using a correct approach. \nThe modified parts are indicated in bold. \nSpecification \nSolved Rate (%) \n\nProblem \nOriginal \nModified \nOriginal Modified Follows (%) \n\nITP1_2_B Output Yes if the val-\nues are in ascending \norder, No otherwise. \n\nOutput Yes if the val-\nues are in descend-\ning order, No other-\nwise. \n\n71 \n40 \n80 \n\nITP1_2_C Output the values in \nascending order. \n\nOutput the values in \ndescending order. \n\n54 \n35 \n95 \n\nITP1_2_D Output Yes if the cir-\ncle is placed inside \nthe rectangle, No \notherwise. \n\nOutput Yes if the \nrectangle is placed \ninside the circle, No \notherwise. \n\n22 \n0 \n40 \n\nAverage \n49 \n25 \n72 \n\n\n\n\n<p> The length <var>a</var> and breadth <var>b</var> of the rectangle are given in a line separated by a single space. </p> <H2>Output</H2> <p> Print the area and perimeter of the rectangle in a line. The two integers should be separated by a single space. </p> <h2>Constraints</h2> <ul> <li> 1 &le; <var>a</var>, <var>b</var> &le; 100</li> </ul><H1>Rectangle</H1> \n\n<p> \nWrite a program which calculates the area and perimeter of a \ngiven rectangle. \n</p> \n\n<H2>Input</H2> \n\n<H2>Sample Input 1</H2> \n<pre> \n3 5 \n</pre> \n<H2>Sample Output 1</H2> \n<pre> \n15 16 \n</pre> \n\n1 \n2 \n3 \n4 \n\n5 \n6 \n7 \n8 \n9 \n10 \n11 \n\n12 \n13 \n14 \n15 \n16 \n17 \n\n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n\n\n\n\n## Constraints $1 \\leq a, b \\leq 100$## Examples \n\n### Input`3 \n\n5`# \n\n## Output`1 \n\n5 16`1 \n\n2 \n3 \n\n4 \n5 \n6 \n7 \n\n8 \n9 \n10 \n11 \n\n12 \n13 \n14 \n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n\n\n\n\n\u00e3by a \n\n\u00e4 \n\u00e5 \nae \n\u00e7 \n\u00e8 \n\u00e9 \n\u00ea \n\u00eb \n\u00ec \n\u00ed \n\u00ee \n\u00ef \n\n\u00f0 \n\u00f1 \n\u00f2 \n\u00f3 \n\u00f4 \n\u00f5 \n\u00f6 \n\u00f7 \n\u00f8 \n\u00f9 \n\u00fa \n\u00fb \n\u00fc \n\n\u00fd \n\u00fe \n\u00ff \n\n\u00a1 \n\n\u00a2 \n\u00a3 \n\u00a4 \nl \n\u00a5 \n\u00a6 \nI \n \u00a7 \n\u00a8 \n\u00a9 \n\n## \n\ni \n\n\n\n\n\n\n\n\n### Input`3 \n\n5`# \n\n## Output`1 \n\n5 16` \n\n\n\n\n! \n\" \n# \n$ \n% \nh \n& \n' \n( \n) \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n\n\"\"\" \n\n1 \n2 \n3 \n4 \n5 \n6 \n\n7 \n8 \n9 \n10 \n\n11 \n12 \n13 \n14 \n\n15 \n16 \n17 \n18 \n19 \n20 \n21 \n22 \n23 \n24 \n25 \n26 \n27 \n28 \n29 \n30 \n31 \n32 \n33 \n34 \n\n\nThis paper is an extended work of an earlier version presented at SOMET 2022[1].3 In this paper, we basically use the abbreviation LLM because we cover large-scale language models. However, when discussing non-large-scale language models, such as when discussing prior research, the abbreviation LM is used instead.\nhttps://www.hackerrank.com/. 5 https://openai.com/blog/chatgpt/.\nModel names are truncated to the decimal point of the actual size of parameters: 2.7B is 2B, 6.1B is 6B, and 16.1B is 16B.\nhttps://www.crummy.com/software/BeautifulSoup/bs4/doc/. 24 https://platform.openai.com/docs/guides/code/best-practices\nhttps://beta.openai.com/tokenizer.\nAcknowledgmentsThis work was supported by the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant Number JP23H03508.\nPrompt sensitivity of language model for solving programming problems. Atsushi Shirafuji, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, Jun Suzuki, Yutaka Watanobe, Proceedings of the 21st International Conference on Intelligent Software Methodologies, Tools and Techniques (SOMET). the 21st International Conference on Intelligent Software Methodologies, Tools and Techniques (SOMET)Atsushi Shirafuji, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, Jun Suzuki, and Yutaka Watanobe. Prompt sensitivity of language model for solving programming prob- lems. In Proceedings of the 21st International Conference on Intelligent Software Methodolo- gies, Tools and Techniques (SOMET), pages 346-359, 2022.\n\n. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Dario, Sam Amodei, Ilya Mccandlish, Wojciech Sutskever, Zaremba, arXiv:2107.03374Felipe Petroski Such. Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec RadfordEvaluating large language models trained on code. arXiv preprintMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo- hammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan- tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint, arXiv:2107.03374, 2021.\n\nRetrieval augmented code generation and summarization. Wasi Md Rizwan Parvez, Saikat Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Chang, Findings of the Association for Computational Linguistics: EMNLP. Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. In Findings of the Association for Computational Linguistics: EMNLP, pages 2719-2734, 2021.\n\nCodeGen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474arXiv preprintErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: An open large language model for code with multi-turn pro- gram synthesis. arXiv preprint, arXiv:2203.13474, 2022.\n\nInCoder: A generative model for code infilling and synthesis. Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, Mike Lewis, International Conference on Learning Representations (ICLR. 2023Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. InCoder: A generative model for code infilling and synthesis. In International Conference on Learning Representations (ICLR), 2023.\n\nEsme Sutherland Robson. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D&apos;autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Koray Kavukcuoglu, and Oriol Vinyals. Competitionlevel code generation with AlphaCode. Science. 3786624Nando de FreitasPushmeet KohliYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cy- prien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Rob- son, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition- level code generation with AlphaCode. Science, 378(6624):1092-1097, 2022.\n\nCodeT: Code generation with generated tests. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen, International Conference on Learning Representations (ICLR. 2023Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code generation with generated tests. In International Conference on Learning Representations (ICLR), 2023.\n\n. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Parker Gehrmann, Kensen Schuh, Sasha Shi, Joshua Tsvyashchenko, Abhishek Maynez, Parker Rao, Yi Barnes, Noam Tay, Vinodkumar Shazeer, Emily Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, arXiv:2204.02311Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-HellsternDouglas Eck, Jeff Dean, Slav Petrovand Noah Fiedel. PaLM: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shiv- ani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. arXiv preprint, arXiv:2204.02311, 2022.\n\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang, Qun Liu, Pangu-Coder, arXiv:2207.11280Program synthesis with function-level language modeling. arXiv preprint. Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. PanGu-Coder: Program synthesis with function-level language modeling. arXiv preprint, arXiv:2207.11280, 2022.\n\nCodeRL: Mastering code generation through pretrained models and deep reinforcement learning. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven Chu Hong Hoi, Advances in Neural Information Processing Systems (NeurIPS). 35Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learn- ing. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 21314-21328, 2022.\n\nA systematic evaluation of large language models of code. F Frank, Uri Xu, Graham Alon, Vincent Josua Neubig, Hellendoorn, Deep Learning for Code Workshop. Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evalua- tion of large language models of code. In Deep Learning for Code Workshop, 2022.\n\nCodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq Joty, Steven C H Hoi, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8696-8708, 2021.\n\nCode generation as a dual task of code summarization. Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, Zhi Jin, Advances in Neural Information Processing Systems (NeurIPS). 32Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summarization. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019.\n\nCodeXGLUE: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shengyu Shao Kun Deng, Shujie Fu, Liu, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks1Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.\n\nUnified pre-training for program understanding and generation. Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program understanding and generation. In Proceedings of the Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 2655-2668, 2021.\n\nUnsupervised translation of programming languages. Marie-Anne Baptiste Roziere, Lowik Lachaux, Guillaume Chanussot, Lample, Advances in Neural Information Processing Systems (NeurIPS). 33Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsuper- vised translation of programming languages. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 20601-20611, 2020.\n\nLeveraging automated unit tests for unsupervised code translation. Jie Baptiste Roziere, Francois Zhang, Mark Charton, Gabriel Harman, Guillaume Synnaeve, Lample, International Conference on Learning Representations (ICLR. 2022Baptiste Roziere, Jie Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guil- laume Lample. Leveraging automated unit tests for unsupervised code translation. In Interna- tional Conference on Learning Representations (ICLR), 2022.\n\nIntelliCode Compose: Code generation using transformer. Alexey Svyatkovskiy, Shengyu Shao Kun Deng, Neel Fu, Sundaresan, Proceedings of the ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). the ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. IntelliCode Com- pose: Code generation using transformer. In Proceedings of the ACM Joint Meeting on Euro- pean Software Engineering Conference and Symposium on the Foundations of Software Engi- neering (ESEC/FSE), pages 1433--1443, 2020.\n\nCode completion for programming education based on deep learning. Kenta Terada, Yutaka Watanobe, International Journal of Computational Intelligence Studies. 102-3Kenta Terada and Yutaka Watanobe. Code completion for programming education based on deep learning. International Journal of Computational Intelligence Studies, 10(2-3):78-98, 2021.\n\nTfix: Learning to fix coding errors with a text-to-text transformer. Berkay Berabi, Jingxuan He, Veselin Raychev, Martin Vechev, Proceedings of the 38th International Conference on Machine Learning (ICML). the 38th International Conference on Machine Learning (ICML)139Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. Tfix: Learning to fix coding errors with a text-to-text transformer. In Proceedings of the 38th International Conference on Machine Learning (ICML), volume 139, pages 780-791, 2021.\n\nA bidirectional lstm language model for code evaluation and repair. Md, Yutaka Mostafizer Rahman, Keita Watanobe, Nakamura, Symmetry. 1322021Md. Mostafizer Rahman, Yutaka Watanobe, and Keita Nakamura. A bidirectional lstm lan- guage model for code evaluation and repair. Symmetry, 13(2), 2021.\n\nA model with iterative trials for correcting logic errors in source code. Taku Matsumoto, Yutaka Watanobe, Keita Nakamura, Applied Sciences. 11112021Taku Matsumoto, Yutaka Watanobe, and Keita Nakamura. A model with iterative trials for correcting logic errors in source code. Applied Sciences, 11(11), 2021.\n\nCan openai's codex fix bugs? an evaluation on quixbugs. Hlib Julian Aron Prenner, Romain Babii, Robbes, Proceedings of the Third International Workshop on Automated Program Repair (APR). the Third International Workshop on Automated Program Repair (APR)Julian Aron Prenner, Hlib Babii, and Romain Robbes. Can openai's codex fix bugs? an evalua- tion on quixbugs. In Proceedings of the Third International Workshop on Automated Program Repair (APR), pages 69--75, 2022.\n\nExamining zero-shot vulnerability repair with large language models. H Pearce, B Tan, B Ahmad, R Karri, B Dolan-Gavitt, Proceedings of the 2023 IEEE Symposium on Security and Privacy (SP). the 2023 IEEE Symposium on Security and Privacy (SP)H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt. Examining zero-shot vulnerability repair with large language models. In Proceedings of the 2023 IEEE Symposium on Security and Privacy (SP), pages 2339-2356, 2023.\n\nRepair is nearly generation: Multilingual program repair with llms. Harshit Joshi, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, Gust Verbruggen, arXiv:2208.11640arXiv preprintHarshit Joshi, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, and Gust Ver- bruggen. Repair is nearly generation: Multilingual program repair with llms. arXiv preprint, arXiv:2208.11640, 2022.\n\nAutomatic algorithm recognition of source-code using machine learning. Maged Shalaby, Tarek Mehrez, Amr El Mougy, Khalid Abdulnasser, Aysha Al-Safty, Proceedings of the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA). the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)Maged Shalaby, Tarek Mehrez, Amr El Mougy, Khalid Abdulnasser, and Aysha Al-Safty. Au- tomatic algorithm recognition of source-code using machine learning. In Proceedings of the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 170-177, 2017.\n\nAlgorithm identification in programming assignments. Pranshu Chourasia, Ganesh Ramakrishnan, Varsha Apte, Suraj Kumar, Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension (ICPC). the 30th IEEE/ACM International Conference on Program Comprehension (ICPC)Pranshu Chourasia, Ganesh Ramakrishnan, Varsha Apte, and Suraj Kumar. Algorithm iden- tification in programming assignments. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension (ICPC), page 471-481, 2022.\n\nIdentifying algorithm in program code based on structural features using cnn classification model. Yutaka Watanobe, Md Md Mostafizer Rahman, Raihan Faizul Ibne Amin, Kabir, Applied Intelligence. 5310Yutaka Watanobe, Md Mostafizer Rahman, Md Faizul Ibne Amin, and Raihan Kabir. Identi- fying algorithm in program code based on structural features using cnn classification model. Applied Intelligence, 53(10):12210-12236, 2023.\n\nAre large pre-trained language models leaking your personal information?. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Findings of the Association for Computational Linguistics: EMNLP. Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language mod- els leaking your personal information? In Findings of the Association for Computational Linguistics: EMNLP, pages 2038-2047, 2022.\n\nTo what extent do deep learningbased code recommenders generate predictions by cloning code from the training set?. Matteo Ciniselli, Luca Pascarella, Gabriele Bavota, Proceedings of the 19th International Conference on Mining Software Repositories (MSR). the 19th International Conference on Mining Software Repositories (MSR)Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. To what extent do deep learning- based code recommenders generate predictions by cloning code from the training set? In Proceedings of the 19th International Conference on Mining Software Repositories (MSR), pages 167--178, 2022.\n\nDawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, 30th USENIX Security Symposium (USENIX Security). Extracting training data from large language modelsNicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather- ine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Sympo- sium (USENIX Security), pages 2633-2650, 2021.\n\nMemorization vs. generalization : Quantifying data leakage in NLP performance evaluation. Aparna Elangovan, Jiayuan He, Karin Verspoor, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeAparna Elangovan, Jiayuan He, and Karin Verspoor. Memorization vs. generalization : Quan- tifying data leakage in NLP performance evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1325-1335, 2021.\n\nCodex hacks hackerrank: Memorization issues and a framework for code synthesis evaluation. Anjan Karmakar, Julian Aron Prenner, Romain Marco D&apos;ambros, Robbes, arXiv:2212.02684arXiv preprintAnjan Karmakar, Julian Aron Prenner, Marco D'Ambros, and Romain Robbes. Codex hacks hackerrank: Memorization issues and a framework for code synthesis evaluation. arXiv preprint, arXiv:2212.02684, 2022.\n\nOn the robustness of code generation techniques: An empirical study on github copilot. Antonio Mastropaolo, Luca Pascarella, Emanuela Guglielmi, Matteo Ciniselli, Simone Scalabrino, Rocco Oliveto, Gabriele Bavota, arXiv:2302.00438arXiv preprintAntonio Mastropaolo, Luca Pascarella, Emanuela Guglielmi, Matteo Ciniselli, Simone Scal- abrino, Rocco Oliveto, and Gabriele Bavota. On the robustness of code generation techniques: An empirical study on github copilot. arXiv preprint, arXiv:2302.00438, 2023.\n\nReCode: Robustness evaluation of code generation models. Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Dan Murali Krishna Ramanathan, Bing Roth, Xiang, arXiv:2212.10264arXiv preprintShiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. ReCode: Robustness evaluation of code generation models. arXiv preprint, arXiv:2212.10264, 2022.\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, F Paul, Jan Christiano, Ryan Leike, Lowe, Advances in Neural Information Processing Systems (NeurIPS). 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 27730-27744, 2022.\n\nAizu Online Judge. Yutaka Watanobe, Yutaka Watanobe. Aizu Online Judge, 2004. URL https://onlinejudge.u-aizu.ac.jp/.\n\nOnline judge system: Requirements, architecture, and experiences. Md Yutaka Watanobe, Taku Mostafizer Rahman, Matsumoto, International Journal of Software Engineering and Knowledge Engineering. 324Uday Kiran Rage, and Penugonda RavikumarYutaka Watanobe, Md. Mostafizer Rahman, Taku Matsumoto, Uday Kiran Rage, and Penu- gonda Ravikumar. Online judge system: Requirements, architecture, and experiences. Inter- national Journal of Software Engineering and Knowledge Engineering, 32(4):1-30, 2022.\n\nA survey on online judge systems and their applications. Szymon Wasik, Maciej Antczak, Jan Badura, Artur Laskowski, Tomasz Sternal, ACM Comput. Surv. 511Szymon Wasik, Maciej Antczak, Jan Badura, Artur Laskowski, and Tomasz Sternal. A survey on online judge systems and their applications. ACM Comput. Surv., 51(1), 2018.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems (NIPS). 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor- mation Processing Systems (NIPS), volume 30, 2017.\n\n. Openai, arXiv:2303.08774GPT-4 technical report. arXiv preprintOpenAI. GPT-4 technical report. arXiv preprint, arXiv:2303.08774, 2023.\n\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 1877-1901, 2020.\n\nMeasuring coding challenge competence with APPS. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks1Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring cod- ing challenge competence with APPS. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.\n\nSolving linear algebra by program synthesis. Iddo Drori, Nakul Verma, arXiv:2111.08171arXiv preprintIddo Drori and Nakul Verma. Solving linear algebra by program synthesis. arXiv preprint, arXiv:2111.08171, 2021.\n\nSolving probability and statistics problems by probabilistic program synthesis at human level and predicting solvability. Leonard Tang, Elizabeth Ke, Nikhil Singh, Bo Feng, Derek Austin, Nakul Verma, Iddo Drori, Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners' and Doctoral Consortium. Leonard Tang, Elizabeth Ke, Nikhil Singh, Bo Feng, Derek Austin, Nakul Verma, and Iddo Drori. Solving probability and statistics problems by probabilistic program synthesis at hu- man level and predicting solvability. In Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners' and Doctoral Consortium, pages 612--615, 2022.\n\nA neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang, 1192123433119Proceedings of the National Academy of Sciences of the United States of AmericaIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences of the United States of America, 119(32):e2123433119, 2022.\n\nAutomatic generation of programming exercises and code explanations using large language models. Sami Sarsa, Paul Denny, Arto Hellas, Juho Leinonen, Proceedings of the 2022 ACM Conference on International Computing Education Research. the 2022 ACM Conference on International Computing Education Research1Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic generation of program- ming exercises and code explanations using large language models. In Proceedings of the 2022 ACM Conference on International Computing Education Research (ICER) -Volume 1, pages 27--43, 2022.\n\nThe Pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, arXiv:2101.00027arXiv preprintLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint, arXiv:2101.00027, 2020.\n\nBLEU: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL). the 40th Annual Meeting on Association for Computational Linguistics (ACL)Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for au- tomatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL), pages 311-318, 2002.\n\nCodeBLEU: A method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.10297arXiv preprintShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. CodeBLEU: A method for automatic evaluation of code synthesis. arXiv preprint, arXiv:2009.10297, 2020.\n\nSpoc: Search-based pseudocode to code. Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, Percy S Liang, Advances in Neural Information Processing Systems. 32Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. Spoc: Search-based pseudocode to code. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\nAsleep at the keyboard? assessing the security of github copilot's code contributions. H Pearce, B Ahmad, B Tan, B Dolan-Gavitt, R Karri, Proceedings of the IEEE Symposium on Security and Privacy (SP). the IEEE Symposium on Security and Privacy (SP)H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. Asleep at the keyboard? assessing the security of github copilot's code contributions. In Proceedings of the IEEE Symposium on Security and Privacy (SP), pages 980-994, 2022.\n\nA Universally Unique IDentifier (UUID) URN Namespace. Paul J Leach, Rich Salz, Michael H Mealling, RFC. 4122Paul J. Leach, Rich Salz, and Michael H. Mealling. A Universally Unique IDentifier (UUID) URN Namespace. RFC 4122, 2005.\n\nProbing pretrained models of source codes. Sergey Troshin, Nadezhda Chirkova, Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPSergey Troshin and Nadezhda Chirkova. Probing pretrained models of source codes. In Pro- ceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 371-383, 2022.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized BERT pretraining approach. arXiv preprint. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint, arXiv:1907.11692, 2019.\n\nCodeBERT: A pre-trained model for programming and natural languages. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou, Findings of the Association for Computational Linguistics: EMNLP. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP, pages 1536-1547, 2020.\n\nLearning and evaluating contextual embedding of source code. Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi, Proceedings of the 37th International Conference on Machine Learning (ICML). the 37th International Conference on Machine Learning (ICML)119Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluat- ing contextual embedding of source code. In Proceedings of the 37th International Conference on Machine Learning (ICML), volume 119, pages 5110-5121, 2020.\n\nSynCoBERT: Syntax-guided multi-modal contrastive pre-training for code representation. Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, Xin Jiang, arXiv:2108.04556arXiv preprintXin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. SynCoBERT: Syntax-guided multi-modal contrastive pre-training for code representation. arXiv preprint, arXiv:2108.04556, 2021.\n\nTreeBERT: A tree-based pretrained model for programming language. Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, Lei Lyu, Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence. the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence161Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. TreeBERT: A tree-based pre- trained model for programming language. In Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, volume 161, pages 54-63, 2021.\n\nGraphCodeBERT: Pre-training code representations with data flow. Daya Guo, Shuai Shuo Ren, Zhangyin Lu, Duyu Feng, Tang, Liu Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Colin Shao Kun Deng, Dawn Clement, Neel Drain, Jian Sundaresan, Daxin Yin, Ming Jiang, Zhou, International Conference on Learning Representations (ICLR. 2021Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. GraphCodeBERT: Pre-training code representations with data flow. In International Conference on Learning Representations (ICLR), 2021.\n\nBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, 2020.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.\n\nStudying the usage of text-to-text transfer transformer to support code-related tasks. Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, Gabriele Bavota, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshy- vanyk, Rocco Oliveto, and Gabriele Bavota. Studying the usage of text-to-text transfer trans- former to support code-related tasks. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 336-347, 2021.\n\nCoTexT: Multi-task learning with code-text transformer. Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Annibal, Alec Peltekian, Yanfang Ye, Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog). the 1st Workshop on Natural Language Processing for Programming (NLP4Prog)Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Annibal, Alec Peltekian, and Yanfang Ye. CoTexT: Multi-task learning with code-text transformer. In Proceedings of the 1st Work- shop on Natural Language Processing for Programming (NLP4Prog), pages 40-47, 2021.\n\nImproving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, 2016.\n\nCode generation for unknown libraries via reading api documentations. Koki Washio, Yusuke Miyao, arXiv:2202.07806arXiv preprintKoki Washio and Yusuke Miyao. Code generation for unknown libraries via reading api docu- mentations. arXiv preprint, arXiv:2202.07806, 2022.\n\nDocPrompting: Generating code by retrieving the docs. Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, Graham Neubig, International Conference on Learning Representations (ICLR). 2023Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. DocPrompting: Generating code by retrieving the docs. In International Conference on Learning Representa- tions (ICLR), 2023.\n\nCompilable neural code generation with compiler feedback. Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu, Findings of the Association for Computational Linguistics: ACL. Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. Compilable neural code generation with compiler feedback. In Findings of the Association for Computational Linguistics: ACL, pages 9-19, 2022.\n\nNatural language to code translation with execution. Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I Wang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingFreda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natu- ral language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3533-3546, 2022.\n\nUniXcoder: Unified cross-modal pre-training for code representation. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsLong Papers1Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. UniXcoder: Unified cross-modal pre-training for code representation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7212-7225, 2022.\n\nDeepCoder: Learning to write programs. Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, Daniel Tarlow, International Conference on Learning Representations. Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. DeepCoder: Learning to write programs. In International Conference on Learning Represen- tations (ICLR), 2017.\n\nCodeNet: A large-scale ai for code dataset for learning a diversity of coding tasks. Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, T Julian, Jie Dolby, Mihir Chen, Lindsey Choudhury, Veronika Decker, Veronika Thost, Luca Thost, Saurabh Buratti, Shyam Pujar, Ulrich Ramji, Susan Finkler, Frederick Malaika, Reiss, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks1Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolo- tov, Julian T Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Freder- ick Reiss. CodeNet: A large-scale ai for code dataset for learning a diversity of coding tasks. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021.\n\nThe robots are coming: Exploring the implications of openai codex on introductory programming. James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, James Prather, Proceedings of the Australasian Computing Education Conference (ACE). the Australasian Computing Education Conference (ACE)James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather. The robots are coming: Exploring the implications of openai codex on introductory program- ming. In Proceedings of the Australasian Computing Education Conference (ACE), pages 10--19, 2022.\n\nWhat do pre-trained code models know about code. Anjan Karmakar, Romain Robbes, 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). Anjan Karmakar and Romain Robbes. What do pre-trained code models know about code? In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 1332-1336, 2021.\n\nLess is more: Summary of long instructions is better for program synthesis. Kirby Kuznia, Swaroop Mishra, Mihir Parmar, Chitta Baral, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingKirby Kuznia, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Less is more: Summary of long instructions is better for program synthesis. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4532-4552, 2022.\n\nHow much do language models copy from their training data? evaluating linguistic novelty in text generation using RAVEN. R , Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz, arXiv:2111.09509arXiv preprintR. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? evaluating linguistic novelty in text generation using RAVEN. arXiv preprint, arXiv:2111.09509, 2021.\n\nCode summarization: Do transformers really understand code?. Manasi Ankita Nandkishor Sontakke, Lovekesh Patwardhan, Raveendra Vig, Ravindra Kumar Medicherla, Gautam Naik, Shroff, Deep Learning for Code Workshop. Ankita Nandkishor Sontakke, Manasi Patwardhan, Lovekesh Vig, Raveendra Kumar Medicherla, Ravindra Naik, and Gautam Shroff. Code summarization: Do transformers re- ally understand code? In Deep Learning for Code Workshop, 2022.\n\nDS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen Tau Yih, Daniel Fried, Sida Wang, Tao Yu, arXiv:2211.11501arXiv preprintYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. DS-1000: A natural and reliable benchmark for data science code generation. arXiv preprint, arXiv:2211.11501, 2022.\n\nA simple, yet effective approach to finding biases in code generation. Spyridon Mouselinos, Mateusz Malinowski, Henryk Michalewski, arXiv:2211.00609arXiv preprintSpyridon Mouselinos, Mateusz Malinowski, and Henryk Michalewski. A simple, yet effective approach to finding biases in code generation. arXiv preprint, arXiv:2211.00609, 2022.\n\nLarge language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, Denny Zhou, arXiv:2302.00093arXiv preprintFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. arXiv preprint, arXiv:2302.00093, 2023.\n", "annotations": {"author": "[{\"end\":115,\"start\":97},{\"end\":132,\"start\":116},{\"end\":187,\"start\":133},{\"end\":205,\"start\":188},{\"end\":263,\"start\":206},{\"end\":318,\"start\":264},{\"end\":373,\"start\":319},{\"end\":413,\"start\":374}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":105},{\"end\":131,\"start\":123},{\"end\":143,\"start\":140},{\"end\":204,\"start\":195},{\"end\":219,\"start\":211},{\"end\":274,\"start\":271},{\"end\":329,\"start\":323}]", "author_first_name": "[{\"end\":104,\"start\":97},{\"end\":122,\"start\":116},{\"end\":139,\"start\":133},{\"end\":194,\"start\":188},{\"end\":210,\"start\":206},{\"end\":270,\"start\":264},{\"end\":322,\"start\":319}]", "author_affiliation": "[{\"end\":186,\"start\":145},{\"end\":262,\"start\":221},{\"end\":317,\"start\":276},{\"end\":372,\"start\":331},{\"end\":412,\"start\":375}]", "title": "[{\"end\":83,\"start\":1},{\"end\":496,\"start\":414}]", "venue": null, "abstract": "[{\"end\":1979,\"start\":509}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2420,\"start\":2417},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2423,\"start\":2420},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2426,\"start\":2423},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2429,\"start\":2426},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2435,\"start\":2432},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2438,\"start\":2435},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2441,\"start\":2438},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2445,\"start\":2441},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2449,\"start\":2445},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2453,\"start\":2449},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2457,\"start\":2453},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2528,\"start\":2525},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2530,\"start\":2528},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2532,\"start\":2530},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2536,\"start\":2532},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2540,\"start\":2536},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2544,\"start\":2540},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2636,\"start\":2632},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2640,\"start\":2636},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2644,\"start\":2640},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2648,\"start\":2644},{\"end\":2751,\"start\":2731},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2855,\"start\":2851},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2859,\"start\":2855},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2863,\"start\":2859},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2867,\"start\":2863},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2871,\"start\":2867},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2875,\"start\":2871},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2879,\"start\":2875},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2977,\"start\":2973},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2981,\"start\":2977},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2985,\"start\":2981},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4494,\"start\":4490},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4664,\"start\":4660},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4781,\"start\":4777},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4784,\"start\":4781},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4823,\"start\":4819},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4955,\"start\":4951},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5080,\"start\":5076},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5763,\"start\":5760},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5774,\"start\":5771},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5792,\"start\":5788},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6734,\"start\":6730},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6737,\"start\":6734},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7276,\"start\":7272},{\"end\":7775,\"start\":7772},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7786,\"start\":7784},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8513,\"start\":8512},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9217,\"start\":9213},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10186,\"start\":10185},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10281,\"start\":10280},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10414,\"start\":10412},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13110,\"start\":13106},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13270,\"start\":13266},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13442,\"start\":13438},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13507,\"start\":13505},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13576,\"start\":13574},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14140,\"start\":14136},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14238,\"start\":14235},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14253,\"start\":14250},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14269,\"start\":14265},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14282,\"start\":14279},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14299,\"start\":14296},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14341,\"start\":14339},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14368,\"start\":14364},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14396,\"start\":14392},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15188,\"start\":15185},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15210,\"start\":15206},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15523,\"start\":15519},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15750,\"start\":15746},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15754,\"start\":15750},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15758,\"start\":15754},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15776,\"start\":15772},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15779,\"start\":15776},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15822,\"start\":15818},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15945,\"start\":15943},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16088,\"start\":16085},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16116,\"start\":16112},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":16341,\"start\":16337},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16845,\"start\":16843},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17155,\"start\":17153},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17222,\"start\":17220},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17585,\"start\":17581},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18647,\"start\":18643},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19949,\"start\":19946},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22254,\"start\":22250},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22269,\"start\":22265},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22313,\"start\":22309},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22332,\"start\":22328},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22344,\"start\":22341},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22353,\"start\":22350},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23088,\"start\":23085},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23091,\"start\":23088},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23561,\"start\":23558},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23579,\"start\":23576},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23594,\"start\":23590},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24961,\"start\":24958},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25703,\"start\":25699},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31165,\"start\":31162},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":33346,\"start\":33342},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":49372,\"start\":49368},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":54382,\"start\":54378},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":54496,\"start\":54492},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":54513,\"start\":54509},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":54633,\"start\":54629},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54646,\"start\":54642},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":54666,\"start\":54662},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":54851,\"start\":54847},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":55007,\"start\":55003},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":55136,\"start\":55132},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55149,\"start\":55146},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":55281,\"start\":55278},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":55297,\"start\":55293},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":55310,\"start\":55307},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":55323,\"start\":55320},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":55344,\"start\":55341},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":55434,\"start\":55430},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55521,\"start\":55518},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":55720,\"start\":55716},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":55732,\"start\":55728},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":56019,\"start\":56015},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":56032,\"start\":56028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":56045,\"start\":56041},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":56076,\"start\":56072},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":56158,\"start\":56155},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":56485,\"start\":56481},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":56615,\"start\":56611},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":56752,\"start\":56749},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":56878,\"start\":56874},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":56899,\"start\":56895},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57238,\"start\":57234},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":57241,\"start\":57238},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":57335,\"start\":57332},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":57338,\"start\":57335},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":57358,\"start\":57354},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":57465,\"start\":57462},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":57582,\"start\":57578},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":58136,\"start\":58132},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":58402,\"start\":58399},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":58566,\"start\":58563},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":58595,\"start\":58591},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":58774,\"start\":58770},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":59293,\"start\":59289},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":59320,\"start\":59316},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":59564,\"start\":59560},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":60029,\"start\":60025},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":60032,\"start\":60029},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":60099,\"start\":60095},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":60129,\"start\":60125},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":60228,\"start\":60224},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":60418,\"start\":60414},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":60697,\"start\":60693},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":61458,\"start\":61454},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":61722,\"start\":61718},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":61993,\"start\":61989},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":62239,\"start\":62235},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":62464,\"start\":62460},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":62762,\"start\":62758},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":73223,\"start\":73220},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":73225,\"start\":73224}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":66414,\"start\":66284},{\"attributes\":{\"id\":\"fig_2\"},\"end\":66530,\"start\":66415},{\"attributes\":{\"id\":\"fig_4\"},\"end\":66583,\"start\":66531},{\"attributes\":{\"id\":\"fig_6\"},\"end\":66957,\"start\":66584},{\"attributes\":{\"id\":\"fig_7\"},\"end\":67123,\"start\":66958},{\"attributes\":{\"id\":\"fig_8\"},\"end\":67181,\"start\":67124},{\"attributes\":{\"id\":\"fig_9\"},\"end\":67250,\"start\":67182},{\"attributes\":{\"id\":\"fig_11\"},\"end\":67339,\"start\":67251},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":67978,\"start\":67340},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":68852,\"start\":67979},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":69232,\"start\":68853},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":69682,\"start\":69233},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69981,\"start\":69683},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":70515,\"start\":69982},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":71047,\"start\":70516},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":71853,\"start\":71048},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":72553,\"start\":71854},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":72758,\"start\":72554},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":73143,\"start\":72759}]", "paragraph": "[{\"end\":2986,\"start\":1995},{\"end\":3710,\"start\":2988},{\"end\":4348,\"start\":3712},{\"end\":5477,\"start\":4350},{\"end\":6020,\"start\":5479},{\"end\":6606,\"start\":6022},{\"end\":6834,\"start\":6608},{\"end\":7405,\"start\":6836},{\"end\":7961,\"start\":7407},{\"end\":8448,\"start\":7963},{\"end\":8587,\"start\":8450},{\"end\":9074,\"start\":8620},{\"end\":9899,\"start\":9076},{\"end\":10902,\"start\":9901},{\"end\":11595,\"start\":10904},{\"end\":11714,\"start\":11597},{\"end\":12236,\"start\":11736},{\"end\":12537,\"start\":12238},{\"end\":12919,\"start\":12553},{\"end\":13509,\"start\":12931},{\"end\":14102,\"start\":13511},{\"end\":14300,\"start\":14113},{\"end\":14494,\"start\":14302},{\"end\":15165,\"start\":14496},{\"end\":15611,\"start\":15175},{\"end\":15823,\"start\":15613},{\"end\":15947,\"start\":15825},{\"end\":16065,\"start\":15949},{\"end\":16291,\"start\":16077},{\"end\":17381,\"start\":16293},{\"end\":17555,\"start\":17400},{\"end\":18024,\"start\":17557},{\"end\":18461,\"start\":18026},{\"end\":18798,\"start\":18463},{\"end\":19003,\"start\":18800},{\"end\":19490,\"start\":19005},{\"end\":19694,\"start\":19519},{\"end\":19804,\"start\":19696},{\"end\":20185,\"start\":19806},{\"end\":20996,\"start\":20187},{\"end\":21327,\"start\":20998},{\"end\":21524,\"start\":21364},{\"end\":21857,\"start\":21526},{\"end\":22109,\"start\":21889},{\"end\":22802,\"start\":22157},{\"end\":23225,\"start\":22828},{\"end\":23747,\"start\":23261},{\"end\":24229,\"start\":23770},{\"end\":24242,\"start\":24231},{\"end\":24259,\"start\":24244},{\"end\":24282,\"start\":24261},{\"end\":24300,\"start\":24284},{\"end\":24329,\"start\":24302},{\"end\":24430,\"start\":24331},{\"end\":24568,\"start\":24432},{\"end\":24852,\"start\":24570},{\"end\":25004,\"start\":24875},{\"end\":25544,\"start\":25006},{\"end\":25746,\"start\":25562},{\"end\":26063,\"start\":25748},{\"end\":26210,\"start\":26065},{\"end\":26661,\"start\":26212},{\"end\":26995,\"start\":26683},{\"end\":27206,\"start\":26997},{\"end\":27508,\"start\":27208},{\"end\":28234,\"start\":27510},{\"end\":28671,\"start\":28275},{\"end\":29058,\"start\":28673},{\"end\":29400,\"start\":29060},{\"end\":29715,\"start\":29402},{\"end\":30122,\"start\":29717},{\"end\":30355,\"start\":30124},{\"end\":30921,\"start\":30357},{\"end\":31274,\"start\":30923},{\"end\":31850,\"start\":31309},{\"end\":32233,\"start\":31852},{\"end\":32811,\"start\":32263},{\"end\":33112,\"start\":32813},{\"end\":33803,\"start\":33114},{\"end\":33808,\"start\":33805},{\"end\":33979,\"start\":33821},{\"end\":33984,\"start\":33981},{\"end\":34535,\"start\":34003},{\"end\":35028,\"start\":34537},{\"end\":35478,\"start\":35030},{\"end\":35930,\"start\":35508},{\"end\":36227,\"start\":35932},{\"end\":36641,\"start\":36250},{\"end\":36941,\"start\":36643},{\"end\":37479,\"start\":36975},{\"end\":38131,\"start\":37481},{\"end\":38673,\"start\":38192},{\"end\":39334,\"start\":38696},{\"end\":39678,\"start\":39336},{\"end\":40102,\"start\":39680},{\"end\":40865,\"start\":40104},{\"end\":41651,\"start\":40867},{\"end\":42214,\"start\":41653},{\"end\":42444,\"start\":42275},{\"end\":43022,\"start\":42446},{\"end\":43301,\"start\":43024},{\"end\":43520,\"start\":43350},{\"end\":43923,\"start\":43522},{\"end\":44199,\"start\":43925},{\"end\":44909,\"start\":44201},{\"end\":45496,\"start\":44911},{\"end\":45718,\"start\":45498},{\"end\":46012,\"start\":45720},{\"end\":46353,\"start\":46014},{\"end\":46478,\"start\":46355},{\"end\":47017,\"start\":46480},{\"end\":47731,\"start\":47052},{\"end\":48178,\"start\":47761},{\"end\":48541,\"start\":48180},{\"end\":48893,\"start\":48543},{\"end\":49465,\"start\":48895},{\"end\":49874,\"start\":49495},{\"end\":50492,\"start\":49876},{\"end\":51253,\"start\":50494},{\"end\":51865,\"start\":51308},{\"end\":52394,\"start\":51867},{\"end\":52790,\"start\":52396},{\"end\":53707,\"start\":52792},{\"end\":54006,\"start\":53709},{\"end\":54213,\"start\":54023},{\"end\":54447,\"start\":54230},{\"end\":55086,\"start\":54449},{\"end\":55601,\"start\":55088},{\"end\":56616,\"start\":55603},{\"end\":57730,\"start\":56618},{\"end\":58117,\"start\":57782},{\"end\":58855,\"start\":58119},{\"end\":59170,\"start\":58894},{\"end\":59521,\"start\":59172},{\"end\":59901,\"start\":59523},{\"end\":60130,\"start\":59925},{\"end\":60640,\"start\":60132},{\"end\":61663,\"start\":60642},{\"end\":61969,\"start\":61665},{\"end\":62439,\"start\":61971},{\"end\":62886,\"start\":62441},{\"end\":63120,\"start\":62901},{\"end\":63576,\"start\":63122},{\"end\":64229,\"start\":63578},{\"end\":64814,\"start\":64231},{\"end\":65231,\"start\":64816},{\"end\":65354,\"start\":65277},{\"end\":65456,\"start\":65364},{\"end\":65583,\"start\":65467},{\"end\":65674,\"start\":65597},{\"end\":65783,\"start\":65687},{\"end\":65914,\"start\":65797}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":21888,\"start\":21858},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22156,\"start\":22110},{\"attributes\":{\"id\":\"formula_2\"},\"end\":66284,\"start\":65915}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40162,\"start\":40155},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41963,\"start\":41956},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":47327,\"start\":47320},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":48916,\"start\":48909},{\"end\":49381,\"start\":49374},{\"end\":49668,\"start\":49661},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":50660,\"start\":50653},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":51090,\"start\":51083}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1993,\"start\":1981},{\"attributes\":{\"n\":\"2\"},\"end\":8618,\"start\":8590},{\"attributes\":{\"n\":\"3\"},\"end\":11734,\"start\":11717},{\"attributes\":{\"n\":\"4\"},\"end\":12551,\"start\":12540},{\"attributes\":{\"n\":\"4.1\"},\"end\":12929,\"start\":12922},{\"attributes\":{\"n\":\"4.2\"},\"end\":14111,\"start\":14105},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":15173,\"start\":15168},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":16075,\"start\":16068},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":17398,\"start\":17384},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":19517,\"start\":19493},{\"attributes\":{\"n\":\"4.3\"},\"end\":21340,\"start\":21330},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":21362,\"start\":21343},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":22826,\"start\":22805},{\"attributes\":{\"n\":\"4.4\"},\"end\":23259,\"start\":23228},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":23768,\"start\":23750},{\"end\":24873,\"start\":24855},{\"end\":25560,\"start\":25547},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":26681,\"start\":26664},{\"end\":28273,\"start\":28237},{\"attributes\":{\"n\":\"4.5\"},\"end\":31307,\"start\":31277},{\"attributes\":{\"n\":\"4.5.1\"},\"end\":32261,\"start\":32236},{\"end\":33819,\"start\":33811},{\"end\":34001,\"start\":33987},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":35506,\"start\":35481},{\"attributes\":{\"n\":\"4.5.3\"},\"end\":36248,\"start\":36230},{\"attributes\":{\"n\":\"4.5.4\"},\"end\":36973,\"start\":36944},{\"attributes\":{\"n\":\"5\"},\"end\":38156,\"start\":38134},{\"attributes\":{\"n\":\"5.1\"},\"end\":38190,\"start\":38159},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":38694,\"start\":38676},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":42234,\"start\":42217},{\"end\":42273,\"start\":42237},{\"end\":43348,\"start\":43304},{\"attributes\":{\"n\":\"5.2\"},\"end\":47050,\"start\":47020},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":47759,\"start\":47734},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":49493,\"start\":49468},{\"attributes\":{\"n\":\"5.2.3\"},\"end\":51274,\"start\":51256},{\"attributes\":{\"n\":\"5.2.4\"},\"end\":51306,\"start\":51277},{\"attributes\":{\"n\":\"6\"},\"end\":54021,\"start\":54009},{\"attributes\":{\"n\":\"6.1\"},\"end\":54228,\"start\":54216},{\"attributes\":{\"n\":\"6.2\"},\"end\":57780,\"start\":57733},{\"attributes\":{\"n\":\"6.3\"},\"end\":58892,\"start\":58858},{\"end\":59923,\"start\":59904},{\"attributes\":{\"n\":\"7\"},\"end\":62899,\"start\":62889},{\"end\":65263,\"start\":65234},{\"end\":65275,\"start\":65266},{\"end\":65362,\"start\":65357},{\"end\":65465,\"start\":65459},{\"end\":65595,\"start\":65586},{\"end\":65685,\"start\":65677},{\"end\":65795,\"start\":65786},{\"end\":66295,\"start\":66285},{\"end\":66426,\"start\":66416},{\"end\":66542,\"start\":66532},{\"end\":66595,\"start\":66585},{\"end\":66969,\"start\":66959},{\"end\":67135,\"start\":67125},{\"end\":67193,\"start\":67183},{\"end\":67262,\"start\":67252},{\"end\":67350,\"start\":67341},{\"end\":67989,\"start\":67980},{\"end\":68863,\"start\":68854},{\"end\":69693,\"start\":69684},{\"end\":69992,\"start\":69983},{\"end\":70526,\"start\":70517},{\"end\":71058,\"start\":71049}]", "table": "[{\"end\":67978,\"start\":67658},{\"end\":68852,\"start\":68229},{\"end\":69232,\"start\":69215},{\"end\":69981,\"start\":69695},{\"end\":70515,\"start\":69994},{\"end\":71047,\"start\":70528},{\"end\":71853,\"start\":71060},{\"end\":72553,\"start\":72202},{\"end\":72758,\"start\":72593},{\"end\":73143,\"start\":72762}]", "figure_caption": "[{\"end\":66414,\"start\":66297},{\"end\":66530,\"start\":66428},{\"end\":66583,\"start\":66544},{\"end\":66957,\"start\":66597},{\"end\":67123,\"start\":66971},{\"end\":67181,\"start\":67137},{\"end\":67250,\"start\":67195},{\"end\":67339,\"start\":67264},{\"end\":67658,\"start\":67352},{\"end\":68229,\"start\":67991},{\"end\":69215,\"start\":68865},{\"end\":69682,\"start\":69235},{\"end\":72202,\"start\":71856},{\"end\":72593,\"start\":72556},{\"end\":72762,\"start\":72761}]", "figure_ref": "[{\"end\":7795,\"start\":7787},{\"end\":8813,\"start\":8804},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12918,\"start\":12910},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24535,\"start\":24527},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24819,\"start\":24811},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24971,\"start\":24963},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25713,\"start\":25705},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":26177,\"start\":26169},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":27166,\"start\":27158},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33633,\"start\":33625},{\"end\":43643,\"start\":43642}]", "bib_author_first_name": "[{\"end\":74009,\"start\":74002},{\"end\":74027,\"start\":74021},{\"end\":74039,\"start\":74033},{\"end\":74055,\"start\":74051},{\"end\":74072,\"start\":74066},{\"end\":74081,\"start\":74078},{\"end\":74096,\"start\":74090},{\"end\":74659,\"start\":74655},{\"end\":74671,\"start\":74666},{\"end\":74686,\"start\":74680},{\"end\":74698,\"start\":74692},{\"end\":74713,\"start\":74705},{\"end\":74744,\"start\":74739},{\"end\":74758,\"start\":74753},{\"end\":74772,\"start\":74768},{\"end\":74788,\"start\":74780},{\"end\":74801,\"start\":74797},{\"end\":74816,\"start\":74812},{\"end\":74826,\"start\":74822},{\"end\":74841,\"start\":74833},{\"end\":74858,\"start\":74851},{\"end\":74872,\"start\":74867},{\"end\":74887,\"start\":74881},{\"end\":74902,\"start\":74896},{\"end\":74918,\"start\":74912},{\"end\":74930,\"start\":74925},{\"end\":74941,\"start\":74937},{\"end\":74956,\"start\":74949},{\"end\":74972,\"start\":74965},{\"end\":74986,\"start\":74980},{\"end\":75003,\"start\":74995},{\"end\":75021,\"start\":75014},{\"end\":75038,\"start\":75030},{\"end\":75058,\"start\":75055},{\"end\":75071,\"start\":75067},{\"end\":75092,\"start\":75084},{\"end\":76429,\"start\":76425},{\"end\":76454,\"start\":76448},{\"end\":76471,\"start\":76462},{\"end\":76492,\"start\":76485},{\"end\":76889,\"start\":76885},{\"end\":76901,\"start\":76899},{\"end\":76915,\"start\":76908},{\"end\":76929,\"start\":76925},{\"end\":76938,\"start\":76934},{\"end\":76951,\"start\":76945},{\"end\":76964,\"start\":76958},{\"end\":76982,\"start\":76975},{\"end\":77322,\"start\":77316},{\"end\":77335,\"start\":77330},{\"end\":77353,\"start\":77348},{\"end\":77363,\"start\":77359},{\"end\":77374,\"start\":77370},{\"end\":77389,\"start\":77384},{\"end\":77400,\"start\":77395},{\"end\":77413,\"start\":77408},{\"end\":77423,\"start\":77419},{\"end\":77441,\"start\":77437},{\"end\":77812,\"start\":77807},{\"end\":77822,\"start\":77817},{\"end\":77837,\"start\":77829},{\"end\":77849,\"start\":77845},{\"end\":77865,\"start\":77859},{\"end\":77885,\"start\":77881},{\"end\":77898,\"start\":77895},{\"end\":77912,\"start\":77907},{\"end\":77927,\"start\":77922},{\"end\":77943,\"start\":77936},{\"end\":77947,\"start\":77944},{\"end\":77960,\"start\":77954},{\"end\":77974,\"start\":77969},{\"end\":77988,\"start\":77981},{\"end\":78018,\"start\":78014},{\"end\":78037,\"start\":78031},{\"end\":78050,\"start\":78044},{\"end\":78066,\"start\":78058},{\"end\":78078,\"start\":78074},{\"end\":78092,\"start\":78086},{\"end\":78110,\"start\":78105},{\"end\":78125,\"start\":78119},{\"end\":78127,\"start\":78126},{\"end\":78836,\"start\":78833},{\"end\":78849,\"start\":78843},{\"end\":78860,\"start\":78857},{\"end\":78877,\"start\":78869},{\"end\":78887,\"start\":78883},{\"end\":78903,\"start\":78893},{\"end\":78915,\"start\":78909},{\"end\":79206,\"start\":79197},{\"end\":79224,\"start\":79218},{\"end\":79238,\"start\":79233},{\"end\":79254,\"start\":79247},{\"end\":79268,\"start\":79262},{\"end\":79281,\"start\":79277},{\"end\":79295,\"start\":79291},{\"end\":79322,\"start\":79315},{\"end\":79339,\"start\":79330},{\"end\":79354,\"start\":79348},{\"end\":79371,\"start\":79365},{\"end\":79384,\"start\":79379},{\"end\":79396,\"start\":79390},{\"end\":79420,\"start\":79412},{\"end\":79435,\"start\":79429},{\"end\":79443,\"start\":79441},{\"end\":79456,\"start\":79452},{\"end\":79472,\"start\":79462},{\"end\":79487,\"start\":79482},{\"end\":79504,\"start\":79501},{\"end\":79514,\"start\":79511},{\"end\":79525,\"start\":79519},{\"end\":79543,\"start\":79538},{\"end\":79555,\"start\":79550},{\"end\":79573,\"start\":79566},{\"end\":79585,\"start\":79582},{\"end\":79602,\"start\":79593},{\"end\":79616,\"start\":79612},{\"end\":79628,\"start\":79622},{\"end\":79641,\"start\":79635},{\"end\":79658,\"start\":79652},{\"end\":79675,\"start\":79669},{\"end\":79687,\"start\":79681},{\"end\":79707,\"start\":79701},{\"end\":79721,\"start\":79716},{\"end\":79733,\"start\":79729},{\"end\":79749,\"start\":79744},{\"end\":79763,\"start\":79757},{\"end\":79775,\"start\":79770},{\"end\":79795,\"start\":79786},{\"end\":79808,\"start\":79802},{\"end\":79823,\"start\":79814},{\"end\":79834,\"start\":79830},{\"end\":79852,\"start\":79847},{\"end\":81404,\"start\":81399},{\"end\":81429,\"start\":81420},{\"end\":81446,\"start\":81441},{\"end\":81461,\"start\":81455},{\"end\":81476,\"start\":81469},{\"end\":81489,\"start\":81482},{\"end\":81496,\"start\":81494},{\"end\":81508,\"start\":81504},{\"end\":81517,\"start\":81515},{\"end\":81527,\"start\":81524},{\"end\":81535,\"start\":81532},{\"end\":81542,\"start\":81540},{\"end\":81554,\"start\":81548},{\"end\":81564,\"start\":81561},{\"end\":81576,\"start\":81571},{\"end\":81588,\"start\":81581},{\"end\":81607,\"start\":81600},{\"end\":81622,\"start\":81614},{\"end\":81639,\"start\":81630},{\"end\":81648,\"start\":81645},{\"end\":81665,\"start\":81656},{\"end\":81675,\"start\":81672},{\"end\":82277,\"start\":82273},{\"end\":82285,\"start\":82282},{\"end\":82300,\"start\":82292},{\"end\":82323,\"start\":82317},{\"end\":82349,\"start\":82334},{\"end\":82759,\"start\":82758},{\"end\":82770,\"start\":82767},{\"end\":82781,\"start\":82775},{\"end\":82801,\"start\":82788},{\"end\":83138,\"start\":83135},{\"end\":83151,\"start\":83145},{\"end\":83164,\"start\":83158},{\"end\":83177,\"start\":83171},{\"end\":83181,\"start\":83178},{\"end\":83693,\"start\":83688},{\"end\":83701,\"start\":83699},{\"end\":83709,\"start\":83706},{\"end\":83720,\"start\":83715},{\"end\":83728,\"start\":83725},{\"end\":84075,\"start\":84070},{\"end\":84084,\"start\":84080},{\"end\":84094,\"start\":84090},{\"end\":84106,\"start\":84100},{\"end\":84120,\"start\":84114},{\"end\":84143,\"start\":84135},{\"end\":84157,\"start\":84152},{\"end\":84171,\"start\":84167},{\"end\":84184,\"start\":84179},{\"end\":84196,\"start\":84192},{\"end\":84205,\"start\":84203},{\"end\":84216,\"start\":84210},{\"end\":84229,\"start\":84223},{\"end\":84240,\"start\":84236},{\"end\":84254,\"start\":84247},{\"end\":84267,\"start\":84263},{\"end\":84278,\"start\":84274},{\"end\":84288,\"start\":84285},{\"end\":84299,\"start\":84295},{\"end\":84319,\"start\":84312},{\"end\":84341,\"start\":84335},{\"end\":85067,\"start\":85063},{\"end\":85081,\"start\":85075},{\"end\":85104,\"start\":85095},{\"end\":85117,\"start\":85110},{\"end\":85780,\"start\":85770},{\"end\":85804,\"start\":85799},{\"end\":85823,\"start\":85814},{\"end\":86207,\"start\":86204},{\"end\":86234,\"start\":86226},{\"end\":86246,\"start\":86242},{\"end\":86263,\"start\":86256},{\"end\":86281,\"start\":86272},{\"end\":86668,\"start\":86662},{\"end\":86690,\"start\":86683},{\"end\":86710,\"start\":86706},{\"end\":87390,\"start\":87385},{\"end\":87405,\"start\":87399},{\"end\":87740,\"start\":87734},{\"end\":87757,\"start\":87749},{\"end\":87769,\"start\":87762},{\"end\":87785,\"start\":87779},{\"end\":88259,\"start\":88253},{\"end\":88284,\"start\":88279},{\"end\":88554,\"start\":88550},{\"end\":88572,\"start\":88566},{\"end\":88588,\"start\":88583},{\"end\":88845,\"start\":88841},{\"end\":88873,\"start\":88867},{\"end\":89325,\"start\":89324},{\"end\":89335,\"start\":89334},{\"end\":89342,\"start\":89341},{\"end\":89351,\"start\":89350},{\"end\":89360,\"start\":89359},{\"end\":89796,\"start\":89789},{\"end\":89808,\"start\":89804},{\"end\":89826,\"start\":89821},{\"end\":89838,\"start\":89836},{\"end\":89847,\"start\":89843},{\"end\":89861,\"start\":89857},{\"end\":90180,\"start\":90175},{\"end\":90195,\"start\":90190},{\"end\":90207,\"start\":90204},{\"end\":90210,\"start\":90208},{\"end\":90224,\"start\":90218},{\"end\":90243,\"start\":90238},{\"end\":90793,\"start\":90786},{\"end\":90811,\"start\":90805},{\"end\":90832,\"start\":90826},{\"end\":90844,\"start\":90839},{\"end\":91362,\"start\":91356},{\"end\":91375,\"start\":91373},{\"end\":91404,\"start\":91398},{\"end\":91761,\"start\":91758},{\"end\":91775,\"start\":91769},{\"end\":91798,\"start\":91782},{\"end\":92214,\"start\":92208},{\"end\":92230,\"start\":92226},{\"end\":92251,\"start\":92243},{\"end\":92774,\"start\":92766},{\"end\":92791,\"start\":92784},{\"end\":92804,\"start\":92800},{\"end\":92821,\"start\":92814},{\"end\":92838,\"start\":92833},{\"end\":92862,\"start\":92853},{\"end\":92872,\"start\":92868},{\"end\":92885,\"start\":92882},{\"end\":93411,\"start\":93405},{\"end\":93430,\"start\":93423},{\"end\":93440,\"start\":93435},{\"end\":94065,\"start\":94060},{\"end\":94082,\"start\":94076},{\"end\":94087,\"start\":94083},{\"end\":94103,\"start\":94097},{\"end\":94461,\"start\":94454},{\"end\":94479,\"start\":94475},{\"end\":94500,\"start\":94492},{\"end\":94518,\"start\":94512},{\"end\":94536,\"start\":94530},{\"end\":94554,\"start\":94549},{\"end\":94572,\"start\":94564},{\"end\":94934,\"start\":94929},{\"end\":94946,\"start\":94941},{\"end\":94958,\"start\":94951},{\"end\":94973,\"start\":94965},{\"end\":94986,\"start\":94980},{\"end\":95000,\"start\":94993},{\"end\":95013,\"start\":95008},{\"end\":95027,\"start\":95021},{\"end\":95042,\"start\":95033},{\"end\":95057,\"start\":95048},{\"end\":95072,\"start\":95066},{\"end\":95087,\"start\":95084},{\"end\":95119,\"start\":95115},{\"end\":95542,\"start\":95538},{\"end\":95558,\"start\":95551},{\"end\":95565,\"start\":95563},{\"end\":95578,\"start\":95573},{\"end\":95595,\"start\":95588},{\"end\":95614,\"start\":95608},{\"end\":95629,\"start\":95624},{\"end\":95645,\"start\":95637},{\"end\":95663,\"start\":95655},{\"end\":95675,\"start\":95671},{\"end\":95685,\"start\":95681},{\"end\":95701,\"start\":95696},{\"end\":95716,\"start\":95710},{\"end\":95729,\"start\":95725},{\"end\":95744,\"start\":95738},{\"end\":95759,\"start\":95753},{\"end\":95773,\"start\":95768},{\"end\":95785,\"start\":95784},{\"end\":95795,\"start\":95792},{\"end\":95812,\"start\":95808},{\"end\":96375,\"start\":96369},{\"end\":96536,\"start\":96534},{\"end\":96558,\"start\":96554},{\"end\":97028,\"start\":97022},{\"end\":97042,\"start\":97036},{\"end\":97055,\"start\":97052},{\"end\":97069,\"start\":97064},{\"end\":97087,\"start\":97081},{\"end\":97320,\"start\":97314},{\"end\":97334,\"start\":97330},{\"end\":97348,\"start\":97344},{\"end\":97362,\"start\":97357},{\"end\":97379,\"start\":97374},{\"end\":97392,\"start\":97387},{\"end\":97394,\"start\":97393},{\"end\":97407,\"start\":97402},{\"end\":97935,\"start\":97932},{\"end\":97951,\"start\":97943},{\"end\":97962,\"start\":97958},{\"end\":97977,\"start\":97970},{\"end\":97992,\"start\":97987},{\"end\":97994,\"start\":97993},{\"end\":98011,\"start\":98003},{\"end\":98028,\"start\":98022},{\"end\":98048,\"start\":98042},{\"end\":98062,\"start\":98056},{\"end\":98077,\"start\":98071},{\"end\":98094,\"start\":98086},{\"end\":98109,\"start\":98104},{\"end\":98132,\"start\":98124},{\"end\":98145,\"start\":98142},{\"end\":98161,\"start\":98156},{\"end\":98175,\"start\":98169},{\"end\":98190,\"start\":98184},{\"end\":98207,\"start\":98200},{\"end\":98219,\"start\":98212},{\"end\":98233,\"start\":98228},{\"end\":98245,\"start\":98241},{\"end\":98256,\"start\":98252},{\"end\":98272,\"start\":98265},{\"end\":99186,\"start\":99183},{\"end\":99204,\"start\":99198},{\"end\":99219,\"start\":99213},{\"end\":99236,\"start\":99230},{\"end\":99250,\"start\":99246},{\"end\":99263,\"start\":99258},{\"end\":99275,\"start\":99269},{\"end\":99288,\"start\":99283},{\"end\":99304,\"start\":99298},{\"end\":99313,\"start\":99309},{\"end\":99325,\"start\":99320},{\"end\":99874,\"start\":99870},{\"end\":99887,\"start\":99882},{\"end\":100168,\"start\":100161},{\"end\":100184,\"start\":100175},{\"end\":100195,\"start\":100189},{\"end\":100205,\"start\":100203},{\"end\":100217,\"start\":100212},{\"end\":100231,\"start\":100226},{\"end\":100243,\"start\":100239},{\"end\":100972,\"start\":100968},{\"end\":100985,\"start\":100980},{\"end\":100998,\"start\":100993},{\"end\":101020,\"start\":101013},{\"end\":101033,\"start\":101027},{\"end\":101047,\"start\":101038},{\"end\":101057,\"start\":101052},{\"end\":101068,\"start\":101063},{\"end\":101080,\"start\":101075},{\"end\":101093,\"start\":101087},{\"end\":101106,\"start\":101101},{\"end\":101119,\"start\":101113},{\"end\":101133,\"start\":101127},{\"end\":101135,\"start\":101134},{\"end\":101149,\"start\":101143},{\"end\":101160,\"start\":101157},{\"end\":101175,\"start\":101170},{\"end\":101189,\"start\":101183},{\"end\":101201,\"start\":101194},{\"end\":101892,\"start\":101888},{\"end\":101904,\"start\":101900},{\"end\":101916,\"start\":101912},{\"end\":101929,\"start\":101925},{\"end\":102447,\"start\":102444},{\"end\":102459,\"start\":102453},{\"end\":102473,\"start\":102470},{\"end\":102489,\"start\":102481},{\"end\":102505,\"start\":102499},{\"end\":102520,\"start\":102513},{\"end\":102534,\"start\":102529},{\"end\":102548,\"start\":102542},{\"end\":102558,\"start\":102553},{\"end\":102569,\"start\":102566},{\"end\":102586,\"start\":102581},{\"end\":102602,\"start\":102596},{\"end\":102988,\"start\":102981},{\"end\":103004,\"start\":102999},{\"end\":103017,\"start\":103013},{\"end\":103032,\"start\":103024},{\"end\":103513,\"start\":103509},{\"end\":103529,\"start\":103524},{\"end\":103539,\"start\":103535},{\"end\":103550,\"start\":103544},{\"end\":103561,\"start\":103557},{\"end\":103571,\"start\":103567},{\"end\":103582,\"start\":103578},{\"end\":103603,\"start\":103595},{\"end\":103615,\"start\":103610},{\"end\":103930,\"start\":103924},{\"end\":103946,\"start\":103938},{\"end\":103962,\"start\":103956},{\"end\":103976,\"start\":103972},{\"end\":103986,\"start\":103982},{\"end\":103998,\"start\":103994},{\"end\":104013,\"start\":104006},{\"end\":104374,\"start\":104373},{\"end\":104384,\"start\":104383},{\"end\":104393,\"start\":104392},{\"end\":104400,\"start\":104399},{\"end\":104416,\"start\":104415},{\"end\":104829,\"start\":104825},{\"end\":104831,\"start\":104830},{\"end\":104843,\"start\":104839},{\"end\":104857,\"start\":104850},{\"end\":104859,\"start\":104858},{\"end\":105050,\"start\":105044},{\"end\":105068,\"start\":105060},{\"end\":105560,\"start\":105555},{\"end\":105577,\"start\":105569},{\"end\":105591,\"start\":105585},{\"end\":105605,\"start\":105597},{\"end\":106248,\"start\":106242},{\"end\":106258,\"start\":106254},{\"end\":106269,\"start\":106264},{\"end\":106284,\"start\":106277},{\"end\":106295,\"start\":106289},{\"end\":106308,\"start\":106303},{\"end\":106319,\"start\":106315},{\"end\":106330,\"start\":106326},{\"end\":106342,\"start\":106338},{\"end\":106363,\"start\":106356},{\"end\":106774,\"start\":106766},{\"end\":106785,\"start\":106781},{\"end\":106795,\"start\":106791},{\"end\":106805,\"start\":106802},{\"end\":106821,\"start\":106812},{\"end\":106832,\"start\":106828},{\"end\":106845,\"start\":106839},{\"end\":106856,\"start\":106852},{\"end\":106866,\"start\":106862},{\"end\":106877,\"start\":106872},{\"end\":106889,\"start\":106885},{\"end\":107325,\"start\":107319},{\"end\":107340,\"start\":107334},{\"end\":107356,\"start\":107351},{\"end\":107377,\"start\":107371},{\"end\":107860,\"start\":107857},{\"end\":107874,\"start\":107867},{\"end\":107884,\"start\":107881},{\"end\":107895,\"start\":107889},{\"end\":107905,\"start\":107902},{\"end\":107915,\"start\":107911},{\"end\":107923,\"start\":107921},{\"end\":107931,\"start\":107928},{\"end\":107939,\"start\":107936},{\"end\":107948,\"start\":107945},{\"end\":108286,\"start\":108283},{\"end\":108301,\"start\":108294},{\"end\":108313,\"start\":108309},{\"end\":108324,\"start\":108319},{\"end\":108332,\"start\":108329},{\"end\":108819,\"start\":108815},{\"end\":108830,\"start\":108825},{\"end\":108849,\"start\":108841},{\"end\":108858,\"start\":108854},{\"end\":108874,\"start\":108871},{\"end\":108887,\"start\":108883},{\"end\":108897,\"start\":108894},{\"end\":108910,\"start\":108904},{\"end\":108932,\"start\":108925},{\"end\":108944,\"start\":108937},{\"end\":108958,\"start\":108953},{\"end\":108978,\"start\":108974},{\"end\":108992,\"start\":108988},{\"end\":109004,\"start\":109000},{\"end\":109022,\"start\":109017},{\"end\":109032,\"start\":109028},{\"end\":109600,\"start\":109596},{\"end\":109614,\"start\":109608},{\"end\":109625,\"start\":109620},{\"end\":109639,\"start\":109633},{\"end\":109666,\"start\":109655},{\"end\":109680,\"start\":109676},{\"end\":109694,\"start\":109687},{\"end\":109709,\"start\":109705},{\"end\":110334,\"start\":110329},{\"end\":110347,\"start\":110343},{\"end\":110361,\"start\":110357},{\"end\":110380,\"start\":110371},{\"end\":110392,\"start\":110386},{\"end\":110408,\"start\":110401},{\"end\":110422,\"start\":110417},{\"end\":110432,\"start\":110429},{\"end\":110442,\"start\":110437},{\"end\":110444,\"start\":110443},{\"end\":110855,\"start\":110848},{\"end\":110875,\"start\":110869},{\"end\":110894,\"start\":110888},{\"end\":110908,\"start\":110903},{\"end\":110914,\"start\":110909},{\"end\":110929,\"start\":110924},{\"end\":110947,\"start\":110942},{\"end\":110965,\"start\":110957},{\"end\":111432,\"start\":111428},{\"end\":111443,\"start\":111439},{\"end\":111456,\"start\":111450},{\"end\":111465,\"start\":111461},{\"end\":111479,\"start\":111474},{\"end\":111493,\"start\":111489},{\"end\":111512,\"start\":111505},{\"end\":112018,\"start\":112014},{\"end\":112034,\"start\":112029},{\"end\":112052,\"start\":112043},{\"end\":112560,\"start\":112556},{\"end\":112575,\"start\":112569},{\"end\":112816,\"start\":112810},{\"end\":112826,\"start\":112823},{\"end\":112838,\"start\":112833},{\"end\":112840,\"start\":112839},{\"end\":112853,\"start\":112845},{\"end\":112867,\"start\":112861},{\"end\":113200,\"start\":113197},{\"end\":113214,\"start\":113207},{\"end\":113224,\"start\":113221},{\"end\":113233,\"start\":113230},{\"end\":113244,\"start\":113238},{\"end\":113255,\"start\":113249},{\"end\":113265,\"start\":113262},{\"end\":113274,\"start\":113271},{\"end\":113282,\"start\":113279},{\"end\":113293,\"start\":113290},{\"end\":113671,\"start\":113666},{\"end\":113683,\"start\":113677},{\"end\":113697,\"start\":113691},{\"end\":113717,\"start\":113713},{\"end\":113735,\"start\":113731},{\"end\":113737,\"start\":113736},{\"end\":114229,\"start\":114225},{\"end\":114240,\"start\":114235},{\"end\":114248,\"start\":114245},{\"end\":114261,\"start\":114255},{\"end\":114272,\"start\":114268},{\"end\":114283,\"start\":114279},{\"end\":114783,\"start\":114778},{\"end\":114800,\"start\":114791},{\"end\":114802,\"start\":114801},{\"end\":114814,\"start\":114810},{\"end\":114838,\"start\":114829},{\"end\":114854,\"start\":114848},{\"end\":115210,\"start\":115204},{\"end\":115222,\"start\":115217},{\"end\":115234,\"start\":115229},{\"end\":115247,\"start\":115244},{\"end\":115262,\"start\":115255},{\"end\":115283,\"start\":115275},{\"end\":115294,\"start\":115293},{\"end\":115306,\"start\":115303},{\"end\":115319,\"start\":115314},{\"end\":115333,\"start\":115326},{\"end\":115353,\"start\":115345},{\"end\":115370,\"start\":115362},{\"end\":115382,\"start\":115378},{\"end\":115397,\"start\":115390},{\"end\":115412,\"start\":115407},{\"end\":115426,\"start\":115420},{\"end\":115439,\"start\":115434},{\"end\":115458,\"start\":115449},{\"end\":116215,\"start\":116210},{\"end\":116235,\"start\":116231},{\"end\":116248,\"start\":116243},{\"end\":116250,\"start\":116249},{\"end\":116265,\"start\":116259},{\"end\":116286,\"start\":116281},{\"end\":116755,\"start\":116750},{\"end\":116772,\"start\":116766},{\"end\":117142,\"start\":117137},{\"end\":117158,\"start\":117151},{\"end\":117172,\"start\":117167},{\"end\":117187,\"start\":117181},{\"end\":117729,\"start\":117728},{\"end\":117738,\"start\":117732},{\"end\":117750,\"start\":117746},{\"end\":117765,\"start\":117762},{\"end\":117782,\"start\":117774},{\"end\":117792,\"start\":117788},{\"end\":118146,\"start\":118140},{\"end\":118183,\"start\":118175},{\"end\":118205,\"start\":118196},{\"end\":118219,\"start\":118211},{\"end\":118244,\"start\":118238},{\"end\":118602,\"start\":118596},{\"end\":118615,\"start\":118608},{\"end\":118626,\"start\":118620},{\"end\":118639,\"start\":118633},{\"end\":118652,\"start\":118647},{\"end\":118664,\"start\":118660},{\"end\":118683,\"start\":118678},{\"end\":118703,\"start\":118697},{\"end\":118715,\"start\":118711},{\"end\":118725,\"start\":118722},{\"end\":119094,\"start\":119086},{\"end\":119114,\"start\":119107},{\"end\":119133,\"start\":119127},{\"end\":119429,\"start\":119424},{\"end\":119441,\"start\":119435},{\"end\":119456,\"start\":119448},{\"end\":119470,\"start\":119464},{\"end\":119484,\"start\":119479},{\"end\":119494,\"start\":119492},{\"end\":119509,\"start\":119500},{\"end\":119524,\"start\":119519}]", "bib_author_last_name": "[{\"end\":74019,\"start\":74010},{\"end\":74031,\"start\":74028},{\"end\":74049,\"start\":74040},{\"end\":74064,\"start\":74056},{\"end\":74076,\"start\":74073},{\"end\":74088,\"start\":74082},{\"end\":74105,\"start\":74097},{\"end\":74664,\"start\":74660},{\"end\":74678,\"start\":74672},{\"end\":74690,\"start\":74687},{\"end\":74703,\"start\":74699},{\"end\":74737,\"start\":74714},{\"end\":74751,\"start\":74745},{\"end\":74766,\"start\":74759},{\"end\":74778,\"start\":74773},{\"end\":74795,\"start\":74789},{\"end\":74810,\"start\":74802},{\"end\":74820,\"start\":74817},{\"end\":74831,\"start\":74827},{\"end\":74849,\"start\":74842},{\"end\":74865,\"start\":74859},{\"end\":74879,\"start\":74873},{\"end\":74894,\"start\":74888},{\"end\":74910,\"start\":74903},{\"end\":74923,\"start\":74919},{\"end\":74935,\"start\":74931},{\"end\":74947,\"start\":74942},{\"end\":74963,\"start\":74957},{\"end\":74978,\"start\":74973},{\"end\":74993,\"start\":74987},{\"end\":75012,\"start\":75004},{\"end\":75028,\"start\":75022},{\"end\":75053,\"start\":75039},{\"end\":75065,\"start\":75059},{\"end\":75082,\"start\":75072},{\"end\":75102,\"start\":75093},{\"end\":75111,\"start\":75104},{\"end\":76446,\"start\":76430},{\"end\":76460,\"start\":76455},{\"end\":76483,\"start\":76472},{\"end\":76496,\"start\":76493},{\"end\":76503,\"start\":76498},{\"end\":76897,\"start\":76890},{\"end\":76906,\"start\":76902},{\"end\":76923,\"start\":76916},{\"end\":76932,\"start\":76930},{\"end\":76943,\"start\":76939},{\"end\":76956,\"start\":76952},{\"end\":76973,\"start\":76965},{\"end\":76988,\"start\":76983},{\"end\":77328,\"start\":77323},{\"end\":77346,\"start\":77336},{\"end\":77357,\"start\":77354},{\"end\":77368,\"start\":77364},{\"end\":77382,\"start\":77375},{\"end\":77393,\"start\":77390},{\"end\":77406,\"start\":77401},{\"end\":77417,\"start\":77414},{\"end\":77435,\"start\":77424},{\"end\":77447,\"start\":77442},{\"end\":77815,\"start\":77813},{\"end\":77827,\"start\":77823},{\"end\":77843,\"start\":77838},{\"end\":77857,\"start\":77850},{\"end\":77879,\"start\":77866},{\"end\":77893,\"start\":77886},{\"end\":77905,\"start\":77899},{\"end\":77920,\"start\":77913},{\"end\":77934,\"start\":77928},{\"end\":77952,\"start\":77948},{\"end\":77967,\"start\":77961},{\"end\":77979,\"start\":77975},{\"end\":78012,\"start\":77989},{\"end\":78029,\"start\":78019},{\"end\":78042,\"start\":78038},{\"end\":78056,\"start\":78051},{\"end\":78072,\"start\":78067},{\"end\":78084,\"start\":78079},{\"end\":78103,\"start\":78093},{\"end\":78117,\"start\":78111},{\"end\":78137,\"start\":78128},{\"end\":78841,\"start\":78837},{\"end\":78855,\"start\":78850},{\"end\":78867,\"start\":78861},{\"end\":78881,\"start\":78878},{\"end\":78891,\"start\":78888},{\"end\":78907,\"start\":78904},{\"end\":78920,\"start\":78916},{\"end\":79216,\"start\":79207},{\"end\":79231,\"start\":79225},{\"end\":79245,\"start\":79239},{\"end\":79260,\"start\":79255},{\"end\":79275,\"start\":79269},{\"end\":79289,\"start\":79282},{\"end\":79302,\"start\":79296},{\"end\":79313,\"start\":79304},{\"end\":79328,\"start\":79323},{\"end\":79346,\"start\":79340},{\"end\":79363,\"start\":79355},{\"end\":79377,\"start\":79372},{\"end\":79388,\"start\":79385},{\"end\":79410,\"start\":79397},{\"end\":79427,\"start\":79421},{\"end\":79439,\"start\":79436},{\"end\":79450,\"start\":79444},{\"end\":79460,\"start\":79457},{\"end\":79480,\"start\":79473},{\"end\":79499,\"start\":79488},{\"end\":79509,\"start\":79505},{\"end\":79517,\"start\":79515},{\"end\":79536,\"start\":79526},{\"end\":79548,\"start\":79544},{\"end\":79564,\"start\":79556},{\"end\":79580,\"start\":79574},{\"end\":79591,\"start\":79586},{\"end\":79610,\"start\":79603},{\"end\":79620,\"start\":79617},{\"end\":79633,\"start\":79629},{\"end\":79650,\"start\":79642},{\"end\":79667,\"start\":79659},{\"end\":79679,\"start\":79676},{\"end\":79699,\"start\":79688},{\"end\":79714,\"start\":79708},{\"end\":79727,\"start\":79722},{\"end\":79742,\"start\":79734},{\"end\":79755,\"start\":79750},{\"end\":79768,\"start\":79764},{\"end\":79784,\"start\":79776},{\"end\":79800,\"start\":79796},{\"end\":79812,\"start\":79809},{\"end\":79828,\"start\":79824},{\"end\":79845,\"start\":79835},{\"end\":79860,\"start\":79853},{\"end\":79867,\"start\":79862},{\"end\":81418,\"start\":81405},{\"end\":81439,\"start\":81430},{\"end\":81453,\"start\":81447},{\"end\":81467,\"start\":81462},{\"end\":81480,\"start\":81477},{\"end\":81492,\"start\":81490},{\"end\":81502,\"start\":81497},{\"end\":81513,\"start\":81509},{\"end\":81522,\"start\":81518},{\"end\":81530,\"start\":81528},{\"end\":81538,\"start\":81536},{\"end\":81546,\"start\":81543},{\"end\":81559,\"start\":81555},{\"end\":81569,\"start\":81565},{\"end\":81579,\"start\":81577},{\"end\":81598,\"start\":81589},{\"end\":81612,\"start\":81608},{\"end\":81628,\"start\":81623},{\"end\":81643,\"start\":81640},{\"end\":81654,\"start\":81649},{\"end\":81670,\"start\":81666},{\"end\":81679,\"start\":81676},{\"end\":81692,\"start\":81681},{\"end\":82280,\"start\":82278},{\"end\":82290,\"start\":82286},{\"end\":82315,\"start\":82301},{\"end\":82332,\"start\":82324},{\"end\":82353,\"start\":82350},{\"end\":82765,\"start\":82760},{\"end\":82773,\"start\":82771},{\"end\":82786,\"start\":82782},{\"end\":82808,\"start\":82802},{\"end\":82821,\"start\":82810},{\"end\":83143,\"start\":83139},{\"end\":83156,\"start\":83152},{\"end\":83169,\"start\":83165},{\"end\":83185,\"start\":83182},{\"end\":83697,\"start\":83694},{\"end\":83704,\"start\":83702},{\"end\":83713,\"start\":83710},{\"end\":83723,\"start\":83721},{\"end\":83732,\"start\":83729},{\"end\":84078,\"start\":84076},{\"end\":84088,\"start\":84085},{\"end\":84098,\"start\":84095},{\"end\":84112,\"start\":84107},{\"end\":84133,\"start\":84121},{\"end\":84150,\"start\":84144},{\"end\":84165,\"start\":84158},{\"end\":84177,\"start\":84172},{\"end\":84190,\"start\":84185},{\"end\":84201,\"start\":84197},{\"end\":84208,\"start\":84206},{\"end\":84221,\"start\":84217},{\"end\":84234,\"start\":84230},{\"end\":84245,\"start\":84241},{\"end\":84261,\"start\":84255},{\"end\":84272,\"start\":84268},{\"end\":84283,\"start\":84279},{\"end\":84293,\"start\":84289},{\"end\":84310,\"start\":84300},{\"end\":84333,\"start\":84320},{\"end\":84344,\"start\":84342},{\"end\":84349,\"start\":84346},{\"end\":85073,\"start\":85068},{\"end\":85093,\"start\":85082},{\"end\":85108,\"start\":85105},{\"end\":85123,\"start\":85118},{\"end\":85797,\"start\":85781},{\"end\":85812,\"start\":85805},{\"end\":85833,\"start\":85824},{\"end\":85841,\"start\":85835},{\"end\":86224,\"start\":86208},{\"end\":86240,\"start\":86235},{\"end\":86254,\"start\":86247},{\"end\":86270,\"start\":86264},{\"end\":86290,\"start\":86282},{\"end\":86298,\"start\":86292},{\"end\":86681,\"start\":86669},{\"end\":86704,\"start\":86691},{\"end\":86713,\"start\":86711},{\"end\":86725,\"start\":86715},{\"end\":87397,\"start\":87391},{\"end\":87414,\"start\":87406},{\"end\":87747,\"start\":87741},{\"end\":87760,\"start\":87758},{\"end\":87777,\"start\":87770},{\"end\":87792,\"start\":87786},{\"end\":88251,\"start\":88249},{\"end\":88277,\"start\":88260},{\"end\":88293,\"start\":88285},{\"end\":88303,\"start\":88295},{\"end\":88564,\"start\":88555},{\"end\":88581,\"start\":88573},{\"end\":88597,\"start\":88589},{\"end\":88865,\"start\":88846},{\"end\":88879,\"start\":88874},{\"end\":88887,\"start\":88881},{\"end\":89332,\"start\":89326},{\"end\":89339,\"start\":89336},{\"end\":89348,\"start\":89343},{\"end\":89357,\"start\":89352},{\"end\":89373,\"start\":89361},{\"end\":89802,\"start\":89797},{\"end\":89819,\"start\":89809},{\"end\":89834,\"start\":89827},{\"end\":89841,\"start\":89839},{\"end\":89855,\"start\":89848},{\"end\":89872,\"start\":89862},{\"end\":90188,\"start\":90181},{\"end\":90202,\"start\":90196},{\"end\":90216,\"start\":90211},{\"end\":90236,\"start\":90225},{\"end\":90252,\"start\":90244},{\"end\":90803,\"start\":90794},{\"end\":90824,\"start\":90812},{\"end\":90837,\"start\":90833},{\"end\":90850,\"start\":90845},{\"end\":91371,\"start\":91363},{\"end\":91396,\"start\":91376},{\"end\":91421,\"start\":91405},{\"end\":91428,\"start\":91423},{\"end\":91767,\"start\":91762},{\"end\":91780,\"start\":91776},{\"end\":91804,\"start\":91799},{\"end\":92224,\"start\":92215},{\"end\":92241,\"start\":92231},{\"end\":92258,\"start\":92252},{\"end\":92782,\"start\":92775},{\"end\":92798,\"start\":92792},{\"end\":92812,\"start\":92805},{\"end\":92831,\"start\":92822},{\"end\":92851,\"start\":92839},{\"end\":92866,\"start\":92863},{\"end\":92880,\"start\":92873},{\"end\":92891,\"start\":92886},{\"end\":93421,\"start\":93412},{\"end\":93433,\"start\":93431},{\"end\":93449,\"start\":93441},{\"end\":94074,\"start\":94066},{\"end\":94095,\"start\":94088},{\"end\":94123,\"start\":94104},{\"end\":94131,\"start\":94125},{\"end\":94473,\"start\":94462},{\"end\":94490,\"start\":94480},{\"end\":94510,\"start\":94501},{\"end\":94528,\"start\":94519},{\"end\":94547,\"start\":94537},{\"end\":94562,\"start\":94555},{\"end\":94579,\"start\":94573},{\"end\":94939,\"start\":94935},{\"end\":94949,\"start\":94947},{\"end\":94963,\"start\":94959},{\"end\":94978,\"start\":94974},{\"end\":94991,\"start\":94987},{\"end\":95006,\"start\":95001},{\"end\":95019,\"start\":95014},{\"end\":95031,\"start\":95028},{\"end\":95046,\"start\":95043},{\"end\":95064,\"start\":95058},{\"end\":95082,\"start\":95073},{\"end\":95113,\"start\":95088},{\"end\":95124,\"start\":95120},{\"end\":95131,\"start\":95126},{\"end\":95549,\"start\":95543},{\"end\":95561,\"start\":95559},{\"end\":95571,\"start\":95566},{\"end\":95586,\"start\":95579},{\"end\":95606,\"start\":95596},{\"end\":95622,\"start\":95615},{\"end\":95635,\"start\":95630},{\"end\":95653,\"start\":95646},{\"end\":95669,\"start\":95664},{\"end\":95679,\"start\":95676},{\"end\":95694,\"start\":95686},{\"end\":95708,\"start\":95702},{\"end\":95723,\"start\":95717},{\"end\":95736,\"start\":95730},{\"end\":95751,\"start\":95745},{\"end\":95766,\"start\":95760},{\"end\":95782,\"start\":95774},{\"end\":95790,\"start\":95786},{\"end\":95806,\"start\":95796},{\"end\":95818,\"start\":95813},{\"end\":95824,\"start\":95820},{\"end\":96384,\"start\":96376},{\"end\":96552,\"start\":96537},{\"end\":96576,\"start\":96559},{\"end\":96587,\"start\":96578},{\"end\":97034,\"start\":97029},{\"end\":97050,\"start\":97043},{\"end\":97062,\"start\":97056},{\"end\":97079,\"start\":97070},{\"end\":97095,\"start\":97088},{\"end\":97328,\"start\":97321},{\"end\":97342,\"start\":97335},{\"end\":97355,\"start\":97349},{\"end\":97372,\"start\":97363},{\"end\":97385,\"start\":97380},{\"end\":97400,\"start\":97395},{\"end\":97414,\"start\":97408},{\"end\":97426,\"start\":97416},{\"end\":97730,\"start\":97724},{\"end\":97941,\"start\":97936},{\"end\":97956,\"start\":97952},{\"end\":97968,\"start\":97963},{\"end\":97985,\"start\":97978},{\"end\":98001,\"start\":97995},{\"end\":98020,\"start\":98012},{\"end\":98040,\"start\":98029},{\"end\":98054,\"start\":98049},{\"end\":98069,\"start\":98063},{\"end\":98084,\"start\":98078},{\"end\":98102,\"start\":98095},{\"end\":98122,\"start\":98110},{\"end\":98140,\"start\":98133},{\"end\":98154,\"start\":98146},{\"end\":98167,\"start\":98162},{\"end\":98182,\"start\":98176},{\"end\":98198,\"start\":98191},{\"end\":98210,\"start\":98208},{\"end\":98226,\"start\":98220},{\"end\":98239,\"start\":98234},{\"end\":98250,\"start\":98246},{\"end\":98263,\"start\":98257},{\"end\":98279,\"start\":98273},{\"end\":99196,\"start\":99187},{\"end\":99211,\"start\":99205},{\"end\":99228,\"start\":99220},{\"end\":99244,\"start\":99237},{\"end\":99256,\"start\":99251},{\"end\":99267,\"start\":99264},{\"end\":99281,\"start\":99276},{\"end\":99296,\"start\":99289},{\"end\":99307,\"start\":99305},{\"end\":99318,\"start\":99314},{\"end\":99336,\"start\":99326},{\"end\":99880,\"start\":99875},{\"end\":99893,\"start\":99888},{\"end\":100173,\"start\":100169},{\"end\":100187,\"start\":100185},{\"end\":100201,\"start\":100196},{\"end\":100210,\"start\":100206},{\"end\":100224,\"start\":100218},{\"end\":100237,\"start\":100232},{\"end\":100249,\"start\":100244},{\"end\":100978,\"start\":100973},{\"end\":100991,\"start\":100986},{\"end\":101011,\"start\":100999},{\"end\":101025,\"start\":101021},{\"end\":101036,\"start\":101034},{\"end\":101050,\"start\":101048},{\"end\":101061,\"start\":101058},{\"end\":101073,\"start\":101069},{\"end\":101085,\"start\":101081},{\"end\":101099,\"start\":101094},{\"end\":101111,\"start\":101107},{\"end\":101125,\"start\":101120},{\"end\":101141,\"start\":101136},{\"end\":101155,\"start\":101150},{\"end\":101168,\"start\":101161},{\"end\":101181,\"start\":101176},{\"end\":101192,\"start\":101190},{\"end\":101208,\"start\":101202},{\"end\":101898,\"start\":101893},{\"end\":101910,\"start\":101905},{\"end\":101923,\"start\":101917},{\"end\":101938,\"start\":101930},{\"end\":102451,\"start\":102448},{\"end\":102468,\"start\":102460},{\"end\":102479,\"start\":102474},{\"end\":102497,\"start\":102490},{\"end\":102511,\"start\":102506},{\"end\":102527,\"start\":102521},{\"end\":102540,\"start\":102535},{\"end\":102551,\"start\":102549},{\"end\":102564,\"start\":102559},{\"end\":102579,\"start\":102570},{\"end\":102594,\"start\":102587},{\"end\":102608,\"start\":102603},{\"end\":102997,\"start\":102989},{\"end\":103011,\"start\":103005},{\"end\":103022,\"start\":103018},{\"end\":103036,\"start\":103033},{\"end\":103522,\"start\":103514},{\"end\":103533,\"start\":103530},{\"end\":103542,\"start\":103540},{\"end\":103555,\"start\":103551},{\"end\":103565,\"start\":103562},{\"end\":103576,\"start\":103572},{\"end\":103593,\"start\":103583},{\"end\":103608,\"start\":103604},{\"end\":103622,\"start\":103616},{\"end\":103626,\"start\":103624},{\"end\":103936,\"start\":103931},{\"end\":103954,\"start\":103947},{\"end\":103970,\"start\":103963},{\"end\":103980,\"start\":103977},{\"end\":103992,\"start\":103987},{\"end\":104004,\"start\":103999},{\"end\":104019,\"start\":104014},{\"end\":104381,\"start\":104375},{\"end\":104390,\"start\":104385},{\"end\":104397,\"start\":104394},{\"end\":104413,\"start\":104401},{\"end\":104422,\"start\":104417},{\"end\":104837,\"start\":104832},{\"end\":104848,\"start\":104844},{\"end\":104868,\"start\":104860},{\"end\":105058,\"start\":105051},{\"end\":105077,\"start\":105069},{\"end\":105567,\"start\":105561},{\"end\":105583,\"start\":105578},{\"end\":105595,\"start\":105592},{\"end\":105615,\"start\":105606},{\"end\":106252,\"start\":106249},{\"end\":106262,\"start\":106259},{\"end\":106275,\"start\":106270},{\"end\":106287,\"start\":106285},{\"end\":106301,\"start\":106296},{\"end\":106313,\"start\":106309},{\"end\":106324,\"start\":106320},{\"end\":106336,\"start\":106331},{\"end\":106354,\"start\":106343},{\"end\":106372,\"start\":106364},{\"end\":106381,\"start\":106374},{\"end\":106779,\"start\":106775},{\"end\":106789,\"start\":106786},{\"end\":106800,\"start\":106796},{\"end\":106810,\"start\":106806},{\"end\":106826,\"start\":106822},{\"end\":106837,\"start\":106833},{\"end\":106850,\"start\":106846},{\"end\":106860,\"start\":106857},{\"end\":106870,\"start\":106867},{\"end\":106883,\"start\":106878},{\"end\":106894,\"start\":106890},{\"end\":107332,\"start\":107326},{\"end\":107349,\"start\":107341},{\"end\":107369,\"start\":107357},{\"end\":107381,\"start\":107378},{\"end\":107865,\"start\":107861},{\"end\":107879,\"start\":107875},{\"end\":107887,\"start\":107885},{\"end\":107900,\"start\":107896},{\"end\":107909,\"start\":107906},{\"end\":107919,\"start\":107916},{\"end\":107926,\"start\":107924},{\"end\":107934,\"start\":107932},{\"end\":107943,\"start\":107940},{\"end\":107954,\"start\":107949},{\"end\":108292,\"start\":108287},{\"end\":108307,\"start\":108302},{\"end\":108317,\"start\":108314},{\"end\":108327,\"start\":108325},{\"end\":108336,\"start\":108333},{\"end\":108823,\"start\":108820},{\"end\":108839,\"start\":108831},{\"end\":108852,\"start\":108850},{\"end\":108863,\"start\":108859},{\"end\":108869,\"start\":108865},{\"end\":108881,\"start\":108875},{\"end\":108892,\"start\":108888},{\"end\":108902,\"start\":108898},{\"end\":108923,\"start\":108911},{\"end\":108935,\"start\":108933},{\"end\":108951,\"start\":108945},{\"end\":108972,\"start\":108959},{\"end\":108986,\"start\":108979},{\"end\":108998,\"start\":108993},{\"end\":109015,\"start\":109005},{\"end\":109026,\"start\":109023},{\"end\":109038,\"start\":109033},{\"end\":109044,\"start\":109040},{\"end\":109606,\"start\":109601},{\"end\":109618,\"start\":109615},{\"end\":109631,\"start\":109626},{\"end\":109653,\"start\":109640},{\"end\":109674,\"start\":109667},{\"end\":109685,\"start\":109681},{\"end\":109703,\"start\":109695},{\"end\":109721,\"start\":109710},{\"end\":110341,\"start\":110335},{\"end\":110355,\"start\":110348},{\"end\":110369,\"start\":110362},{\"end\":110384,\"start\":110381},{\"end\":110399,\"start\":110393},{\"end\":110415,\"start\":110409},{\"end\":110427,\"start\":110423},{\"end\":110435,\"start\":110433},{\"end\":110448,\"start\":110445},{\"end\":110867,\"start\":110856},{\"end\":110886,\"start\":110876},{\"end\":110901,\"start\":110895},{\"end\":110922,\"start\":110915},{\"end\":110940,\"start\":110930},{\"end\":110955,\"start\":110948},{\"end\":110972,\"start\":110966},{\"end\":111437,\"start\":111433},{\"end\":111448,\"start\":111444},{\"end\":111459,\"start\":111457},{\"end\":111472,\"start\":111466},{\"end\":111487,\"start\":111480},{\"end\":111503,\"start\":111494},{\"end\":111515,\"start\":111513},{\"end\":112027,\"start\":112019},{\"end\":112041,\"start\":112035},{\"end\":112058,\"start\":112053},{\"end\":112567,\"start\":112561},{\"end\":112581,\"start\":112576},{\"end\":112821,\"start\":112817},{\"end\":112831,\"start\":112827},{\"end\":112843,\"start\":112841},{\"end\":112859,\"start\":112854},{\"end\":112874,\"start\":112868},{\"end\":113205,\"start\":113201},{\"end\":113219,\"start\":113215},{\"end\":113228,\"start\":113225},{\"end\":113236,\"start\":113234},{\"end\":113247,\"start\":113245},{\"end\":113260,\"start\":113256},{\"end\":113269,\"start\":113266},{\"end\":113277,\"start\":113275},{\"end\":113288,\"start\":113283},{\"end\":113297,\"start\":113294},{\"end\":113675,\"start\":113672},{\"end\":113689,\"start\":113684},{\"end\":113711,\"start\":113698},{\"end\":113729,\"start\":113718},{\"end\":113742,\"start\":113738},{\"end\":114233,\"start\":114230},{\"end\":114243,\"start\":114241},{\"end\":114253,\"start\":114249},{\"end\":114266,\"start\":114262},{\"end\":114277,\"start\":114273},{\"end\":114287,\"start\":114284},{\"end\":114789,\"start\":114784},{\"end\":114808,\"start\":114803},{\"end\":114827,\"start\":114815},{\"end\":114846,\"start\":114839},{\"end\":114861,\"start\":114855},{\"end\":115215,\"start\":115211},{\"end\":115227,\"start\":115223},{\"end\":115242,\"start\":115235},{\"end\":115253,\"start\":115248},{\"end\":115273,\"start\":115263},{\"end\":115291,\"start\":115284},{\"end\":115301,\"start\":115295},{\"end\":115312,\"start\":115307},{\"end\":115324,\"start\":115320},{\"end\":115343,\"start\":115334},{\"end\":115360,\"start\":115354},{\"end\":115376,\"start\":115371},{\"end\":115388,\"start\":115383},{\"end\":115405,\"start\":115398},{\"end\":115418,\"start\":115413},{\"end\":115432,\"start\":115427},{\"end\":115447,\"start\":115440},{\"end\":115466,\"start\":115459},{\"end\":115473,\"start\":115468},{\"end\":116229,\"start\":116216},{\"end\":116241,\"start\":116236},{\"end\":116257,\"start\":116251},{\"end\":116279,\"start\":116266},{\"end\":116294,\"start\":116287},{\"end\":116764,\"start\":116756},{\"end\":116779,\"start\":116773},{\"end\":117149,\"start\":117143},{\"end\":117165,\"start\":117159},{\"end\":117179,\"start\":117173},{\"end\":117193,\"start\":117188},{\"end\":117744,\"start\":117739},{\"end\":117760,\"start\":117751},{\"end\":117772,\"start\":117766},{\"end\":117786,\"start\":117783},{\"end\":117804,\"start\":117793},{\"end\":118173,\"start\":118147},{\"end\":118194,\"start\":118184},{\"end\":118209,\"start\":118206},{\"end\":118236,\"start\":118220},{\"end\":118249,\"start\":118245},{\"end\":118257,\"start\":118251},{\"end\":118606,\"start\":118603},{\"end\":118618,\"start\":118616},{\"end\":118631,\"start\":118627},{\"end\":118645,\"start\":118640},{\"end\":118658,\"start\":118653},{\"end\":118676,\"start\":118665},{\"end\":118695,\"start\":118684},{\"end\":118709,\"start\":118704},{\"end\":118720,\"start\":118716},{\"end\":118728,\"start\":118726},{\"end\":119105,\"start\":119095},{\"end\":119125,\"start\":119115},{\"end\":119145,\"start\":119134},{\"end\":119433,\"start\":119430},{\"end\":119446,\"start\":119442},{\"end\":119462,\"start\":119457},{\"end\":119477,\"start\":119471},{\"end\":119490,\"start\":119485},{\"end\":119498,\"start\":119495},{\"end\":119517,\"start\":119510},{\"end\":119529,\"start\":119525}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":256391650},\"end\":74651,\"start\":73931},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b1\"},\"end\":76368,\"start\":74653},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":237304122},\"end\":76801,\"start\":76370},{\"attributes\":{\"doi\":\"arXiv:2203.13474\",\"id\":\"b3\"},\"end\":77252,\"start\":76803},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":248157108},\"end\":77781,\"start\":77254},{\"attributes\":{\"id\":\"b5\"},\"end\":78786,\"start\":77783},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":250920542},\"end\":79193,\"start\":78788},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b7\"},\"end\":81397,\"start\":79195},{\"attributes\":{\"doi\":\"arXiv:2207.11280\",\"id\":\"b8\"},\"end\":82178,\"start\":81399},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":250280117},\"end\":82698,\"start\":82180},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":247158549},\"end\":83026,\"start\":82700},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":237386541},\"end\":83632,\"start\":83028},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":202769028},\"end\":83981,\"start\":83634},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":231855531},\"end\":84998,\"start\":83983},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":232185260},\"end\":85717,\"start\":85000},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219401607},\"end\":86135,\"start\":85719},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":238744039},\"end\":86604,\"start\":86137},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":218673683},\"end\":87317,\"start\":86606},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":236397507},\"end\":87663,\"start\":87319},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235672358},\"end\":88179,\"start\":87665},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":232163152},\"end\":88474,\"start\":88181},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":236359081},\"end\":88783,\"start\":88476},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":250318108},\"end\":89253,\"start\":88785},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":251563966},\"end\":89719,\"start\":89255},{\"attributes\":{\"doi\":\"arXiv:2208.11640\",\"id\":\"b24\"},\"end\":90102,\"start\":89721},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9363667},\"end\":90731,\"start\":90104},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":249928550},\"end\":91255,\"start\":90733},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":252514452},\"end\":91682,\"start\":91257},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":249063119},\"end\":92090,\"start\":91684},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":248178008},\"end\":92704,\"start\":92092},{\"attributes\":{\"id\":\"b30\"},\"end\":93313,\"start\":92706},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":231786600},\"end\":93967,\"start\":93315},{\"attributes\":{\"doi\":\"arXiv:2212.02684\",\"id\":\"b32\"},\"end\":94365,\"start\":93969},{\"attributes\":{\"doi\":\"arXiv:2302.00438\",\"id\":\"b33\"},\"end\":94870,\"start\":94367},{\"attributes\":{\"doi\":\"arXiv:2212.10264\",\"id\":\"b34\"},\"end\":95467,\"start\":94872},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":246426909},\"end\":96348,\"start\":95469},{\"attributes\":{\"id\":\"b36\"},\"end\":96466,\"start\":96350},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":249113039},\"end\":96963,\"start\":96468},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13808159},\"end\":97285,\"start\":96965},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13756489},\"end\":97720,\"start\":97287},{\"attributes\":{\"doi\":\"arXiv:2303.08774\",\"id\":\"b40\"},\"end\":97857,\"start\":97722},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":218971783},\"end\":99132,\"start\":97859},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":234790100},\"end\":99823,\"start\":99134},{\"attributes\":{\"doi\":\"arXiv:2111.08171\",\"id\":\"b43\"},\"end\":100037,\"start\":99825},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":251137338},\"end\":100833,\"start\":100039},{\"attributes\":{\"id\":\"b45\"},\"end\":101789,\"start\":100835},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":249954011},\"end\":102376,\"start\":101791},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b47\"},\"end\":102915,\"start\":102378},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":11080756},\"end\":103444,\"start\":102917},{\"attributes\":{\"doi\":\"arXiv:2009.10297\",\"id\":\"b49\"},\"end\":103883,\"start\":103446},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":186206968},\"end\":104284,\"start\":103885},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":245220588},\"end\":104769,\"start\":104286},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":29284933},\"end\":104999,\"start\":104771},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":256461462},\"end\":105471,\"start\":105001},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":52967399},\"end\":106240,\"start\":105473},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b55\"},\"end\":106695,\"start\":106242},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":211171605},\"end\":107256,\"start\":106697},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":220425306},\"end\":107768,\"start\":107258},{\"attributes\":{\"doi\":\"arXiv:2108.04556\",\"id\":\"b58\"},\"end\":108215,\"start\":107770},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":235195882},\"end\":108748,\"start\":108217},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":221761146},\"end\":109479,\"start\":108750},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":204960716},\"end\":110244,\"start\":109481},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":204838007},\"end\":110759,\"start\":110246},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":231786586},\"end\":111370,\"start\":110761},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":234762936},\"end\":111945,\"start\":111372},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":15600925},\"end\":112484,\"start\":111947},{\"attributes\":{\"doi\":\"arXiv:2202.07806\",\"id\":\"b66\"},\"end\":112754,\"start\":112486},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":252734952},\"end\":113137,\"start\":112756},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":247362946},\"end\":113611,\"start\":113139},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":248377325},\"end\":114154,\"start\":113613},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":247315559},\"end\":114737,\"start\":114156},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":2906360},\"end\":115117,\"start\":114739},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":235195915},\"end\":116113,\"start\":115119},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":246681316},\"end\":116699,\"start\":116115},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":237291515},\"end\":117059,\"start\":116701},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":247475845},\"end\":117605,\"start\":117061},{\"attributes\":{\"doi\":\"arXiv:2111.09509\",\"id\":\"b76\"},\"end\":118077,\"start\":117607},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":252602122},\"end\":118518,\"start\":118079},{\"attributes\":{\"doi\":\"arXiv:2211.11501\",\"id\":\"b78\"},\"end\":119013,\"start\":118520},{\"attributes\":{\"doi\":\"arXiv:2211.00609\",\"id\":\"b79\"},\"end\":119352,\"start\":119015},{\"attributes\":{\"doi\":\"arXiv:2302.00093\",\"id\":\"b80\"},\"end\":119781,\"start\":119354}]", "bib_title": "[{\"end\":74000,\"start\":73931},{\"end\":76423,\"start\":76370},{\"end\":77314,\"start\":77254},{\"end\":77805,\"start\":77783},{\"end\":78831,\"start\":78788},{\"end\":82271,\"start\":82180},{\"end\":82756,\"start\":82700},{\"end\":83133,\"start\":83028},{\"end\":83686,\"start\":83634},{\"end\":84068,\"start\":83983},{\"end\":85061,\"start\":85000},{\"end\":85768,\"start\":85719},{\"end\":86202,\"start\":86137},{\"end\":86660,\"start\":86606},{\"end\":87383,\"start\":87319},{\"end\":87732,\"start\":87665},{\"end\":88247,\"start\":88181},{\"end\":88548,\"start\":88476},{\"end\":88839,\"start\":88785},{\"end\":89322,\"start\":89255},{\"end\":90173,\"start\":90104},{\"end\":90784,\"start\":90733},{\"end\":91354,\"start\":91257},{\"end\":91756,\"start\":91684},{\"end\":92206,\"start\":92092},{\"end\":92764,\"start\":92706},{\"end\":93403,\"start\":93315},{\"end\":95536,\"start\":95469},{\"end\":96532,\"start\":96468},{\"end\":97020,\"start\":96965},{\"end\":97312,\"start\":97287},{\"end\":97930,\"start\":97859},{\"end\":99181,\"start\":99134},{\"end\":100159,\"start\":100039},{\"end\":101886,\"start\":101791},{\"end\":102979,\"start\":102917},{\"end\":103922,\"start\":103885},{\"end\":104371,\"start\":104286},{\"end\":104823,\"start\":104771},{\"end\":105042,\"start\":105001},{\"end\":105553,\"start\":105473},{\"end\":106764,\"start\":106697},{\"end\":107317,\"start\":107258},{\"end\":108281,\"start\":108217},{\"end\":108813,\"start\":108750},{\"end\":109594,\"start\":109481},{\"end\":110327,\"start\":110246},{\"end\":110846,\"start\":110761},{\"end\":111426,\"start\":111372},{\"end\":112012,\"start\":111947},{\"end\":112808,\"start\":112756},{\"end\":113195,\"start\":113139},{\"end\":113664,\"start\":113613},{\"end\":114223,\"start\":114156},{\"end\":114776,\"start\":114739},{\"end\":115202,\"start\":115119},{\"end\":116208,\"start\":116115},{\"end\":116748,\"start\":116701},{\"end\":117135,\"start\":117061},{\"end\":118138,\"start\":118079}]", "bib_author": "[{\"end\":74021,\"start\":74002},{\"end\":74033,\"start\":74021},{\"end\":74051,\"start\":74033},{\"end\":74066,\"start\":74051},{\"end\":74078,\"start\":74066},{\"end\":74090,\"start\":74078},{\"end\":74107,\"start\":74090},{\"end\":74666,\"start\":74655},{\"end\":74680,\"start\":74666},{\"end\":74692,\"start\":74680},{\"end\":74705,\"start\":74692},{\"end\":74739,\"start\":74705},{\"end\":74753,\"start\":74739},{\"end\":74768,\"start\":74753},{\"end\":74780,\"start\":74768},{\"end\":74797,\"start\":74780},{\"end\":74812,\"start\":74797},{\"end\":74822,\"start\":74812},{\"end\":74833,\"start\":74822},{\"end\":74851,\"start\":74833},{\"end\":74867,\"start\":74851},{\"end\":74881,\"start\":74867},{\"end\":74896,\"start\":74881},{\"end\":74912,\"start\":74896},{\"end\":74925,\"start\":74912},{\"end\":74937,\"start\":74925},{\"end\":74949,\"start\":74937},{\"end\":74965,\"start\":74949},{\"end\":74980,\"start\":74965},{\"end\":74995,\"start\":74980},{\"end\":75014,\"start\":74995},{\"end\":75030,\"start\":75014},{\"end\":75055,\"start\":75030},{\"end\":75067,\"start\":75055},{\"end\":75084,\"start\":75067},{\"end\":75104,\"start\":75084},{\"end\":75113,\"start\":75104},{\"end\":76448,\"start\":76425},{\"end\":76462,\"start\":76448},{\"end\":76485,\"start\":76462},{\"end\":76498,\"start\":76485},{\"end\":76505,\"start\":76498},{\"end\":76899,\"start\":76885},{\"end\":76908,\"start\":76899},{\"end\":76925,\"start\":76908},{\"end\":76934,\"start\":76925},{\"end\":76945,\"start\":76934},{\"end\":76958,\"start\":76945},{\"end\":76975,\"start\":76958},{\"end\":76990,\"start\":76975},{\"end\":77330,\"start\":77316},{\"end\":77348,\"start\":77330},{\"end\":77359,\"start\":77348},{\"end\":77370,\"start\":77359},{\"end\":77384,\"start\":77370},{\"end\":77395,\"start\":77384},{\"end\":77408,\"start\":77395},{\"end\":77419,\"start\":77408},{\"end\":77437,\"start\":77419},{\"end\":77449,\"start\":77437},{\"end\":77817,\"start\":77807},{\"end\":77829,\"start\":77817},{\"end\":77845,\"start\":77829},{\"end\":77859,\"start\":77845},{\"end\":77881,\"start\":77859},{\"end\":77895,\"start\":77881},{\"end\":77907,\"start\":77895},{\"end\":77922,\"start\":77907},{\"end\":77936,\"start\":77922},{\"end\":77954,\"start\":77936},{\"end\":77969,\"start\":77954},{\"end\":77981,\"start\":77969},{\"end\":78014,\"start\":77981},{\"end\":78031,\"start\":78014},{\"end\":78044,\"start\":78031},{\"end\":78058,\"start\":78044},{\"end\":78074,\"start\":78058},{\"end\":78086,\"start\":78074},{\"end\":78105,\"start\":78086},{\"end\":78119,\"start\":78105},{\"end\":78139,\"start\":78119},{\"end\":78843,\"start\":78833},{\"end\":78857,\"start\":78843},{\"end\":78869,\"start\":78857},{\"end\":78883,\"start\":78869},{\"end\":78893,\"start\":78883},{\"end\":78909,\"start\":78893},{\"end\":78922,\"start\":78909},{\"end\":79218,\"start\":79197},{\"end\":79233,\"start\":79218},{\"end\":79247,\"start\":79233},{\"end\":79262,\"start\":79247},{\"end\":79277,\"start\":79262},{\"end\":79291,\"start\":79277},{\"end\":79304,\"start\":79291},{\"end\":79315,\"start\":79304},{\"end\":79330,\"start\":79315},{\"end\":79348,\"start\":79330},{\"end\":79365,\"start\":79348},{\"end\":79379,\"start\":79365},{\"end\":79390,\"start\":79379},{\"end\":79412,\"start\":79390},{\"end\":79429,\"start\":79412},{\"end\":79441,\"start\":79429},{\"end\":79452,\"start\":79441},{\"end\":79462,\"start\":79452},{\"end\":79482,\"start\":79462},{\"end\":79501,\"start\":79482},{\"end\":79511,\"start\":79501},{\"end\":79519,\"start\":79511},{\"end\":79538,\"start\":79519},{\"end\":79550,\"start\":79538},{\"end\":79566,\"start\":79550},{\"end\":79582,\"start\":79566},{\"end\":79593,\"start\":79582},{\"end\":79612,\"start\":79593},{\"end\":79622,\"start\":79612},{\"end\":79635,\"start\":79622},{\"end\":79652,\"start\":79635},{\"end\":79669,\"start\":79652},{\"end\":79681,\"start\":79669},{\"end\":79701,\"start\":79681},{\"end\":79716,\"start\":79701},{\"end\":79729,\"start\":79716},{\"end\":79744,\"start\":79729},{\"end\":79757,\"start\":79744},{\"end\":79770,\"start\":79757},{\"end\":79786,\"start\":79770},{\"end\":79802,\"start\":79786},{\"end\":79814,\"start\":79802},{\"end\":79830,\"start\":79814},{\"end\":79847,\"start\":79830},{\"end\":79862,\"start\":79847},{\"end\":79869,\"start\":79862},{\"end\":81420,\"start\":81399},{\"end\":81441,\"start\":81420},{\"end\":81455,\"start\":81441},{\"end\":81469,\"start\":81455},{\"end\":81482,\"start\":81469},{\"end\":81494,\"start\":81482},{\"end\":81504,\"start\":81494},{\"end\":81515,\"start\":81504},{\"end\":81524,\"start\":81515},{\"end\":81532,\"start\":81524},{\"end\":81540,\"start\":81532},{\"end\":81548,\"start\":81540},{\"end\":81561,\"start\":81548},{\"end\":81571,\"start\":81561},{\"end\":81581,\"start\":81571},{\"end\":81600,\"start\":81581},{\"end\":81614,\"start\":81600},{\"end\":81630,\"start\":81614},{\"end\":81645,\"start\":81630},{\"end\":81656,\"start\":81645},{\"end\":81672,\"start\":81656},{\"end\":81681,\"start\":81672},{\"end\":81694,\"start\":81681},{\"end\":82282,\"start\":82273},{\"end\":82292,\"start\":82282},{\"end\":82317,\"start\":82292},{\"end\":82334,\"start\":82317},{\"end\":82355,\"start\":82334},{\"end\":82767,\"start\":82758},{\"end\":82775,\"start\":82767},{\"end\":82788,\"start\":82775},{\"end\":82810,\"start\":82788},{\"end\":82823,\"start\":82810},{\"end\":83145,\"start\":83135},{\"end\":83158,\"start\":83145},{\"end\":83171,\"start\":83158},{\"end\":83187,\"start\":83171},{\"end\":83699,\"start\":83688},{\"end\":83706,\"start\":83699},{\"end\":83715,\"start\":83706},{\"end\":83725,\"start\":83715},{\"end\":83734,\"start\":83725},{\"end\":84080,\"start\":84070},{\"end\":84090,\"start\":84080},{\"end\":84100,\"start\":84090},{\"end\":84114,\"start\":84100},{\"end\":84135,\"start\":84114},{\"end\":84152,\"start\":84135},{\"end\":84167,\"start\":84152},{\"end\":84179,\"start\":84167},{\"end\":84192,\"start\":84179},{\"end\":84203,\"start\":84192},{\"end\":84210,\"start\":84203},{\"end\":84223,\"start\":84210},{\"end\":84236,\"start\":84223},{\"end\":84247,\"start\":84236},{\"end\":84263,\"start\":84247},{\"end\":84274,\"start\":84263},{\"end\":84285,\"start\":84274},{\"end\":84295,\"start\":84285},{\"end\":84312,\"start\":84295},{\"end\":84335,\"start\":84312},{\"end\":84346,\"start\":84335},{\"end\":84351,\"start\":84346},{\"end\":85075,\"start\":85063},{\"end\":85095,\"start\":85075},{\"end\":85110,\"start\":85095},{\"end\":85125,\"start\":85110},{\"end\":85799,\"start\":85770},{\"end\":85814,\"start\":85799},{\"end\":85835,\"start\":85814},{\"end\":85843,\"start\":85835},{\"end\":86226,\"start\":86204},{\"end\":86242,\"start\":86226},{\"end\":86256,\"start\":86242},{\"end\":86272,\"start\":86256},{\"end\":86292,\"start\":86272},{\"end\":86300,\"start\":86292},{\"end\":86683,\"start\":86662},{\"end\":86706,\"start\":86683},{\"end\":86715,\"start\":86706},{\"end\":86727,\"start\":86715},{\"end\":87399,\"start\":87385},{\"end\":87416,\"start\":87399},{\"end\":87749,\"start\":87734},{\"end\":87762,\"start\":87749},{\"end\":87779,\"start\":87762},{\"end\":87794,\"start\":87779},{\"end\":88253,\"start\":88249},{\"end\":88279,\"start\":88253},{\"end\":88295,\"start\":88279},{\"end\":88305,\"start\":88295},{\"end\":88566,\"start\":88550},{\"end\":88583,\"start\":88566},{\"end\":88599,\"start\":88583},{\"end\":88867,\"start\":88841},{\"end\":88881,\"start\":88867},{\"end\":88889,\"start\":88881},{\"end\":89334,\"start\":89324},{\"end\":89341,\"start\":89334},{\"end\":89350,\"start\":89341},{\"end\":89359,\"start\":89350},{\"end\":89375,\"start\":89359},{\"end\":89804,\"start\":89789},{\"end\":89821,\"start\":89804},{\"end\":89836,\"start\":89821},{\"end\":89843,\"start\":89836},{\"end\":89857,\"start\":89843},{\"end\":89874,\"start\":89857},{\"end\":90190,\"start\":90175},{\"end\":90204,\"start\":90190},{\"end\":90218,\"start\":90204},{\"end\":90238,\"start\":90218},{\"end\":90254,\"start\":90238},{\"end\":90805,\"start\":90786},{\"end\":90826,\"start\":90805},{\"end\":90839,\"start\":90826},{\"end\":90852,\"start\":90839},{\"end\":91373,\"start\":91356},{\"end\":91398,\"start\":91373},{\"end\":91423,\"start\":91398},{\"end\":91430,\"start\":91423},{\"end\":91769,\"start\":91758},{\"end\":91782,\"start\":91769},{\"end\":91806,\"start\":91782},{\"end\":92226,\"start\":92208},{\"end\":92243,\"start\":92226},{\"end\":92260,\"start\":92243},{\"end\":92784,\"start\":92766},{\"end\":92800,\"start\":92784},{\"end\":92814,\"start\":92800},{\"end\":92833,\"start\":92814},{\"end\":92853,\"start\":92833},{\"end\":92868,\"start\":92853},{\"end\":92882,\"start\":92868},{\"end\":92893,\"start\":92882},{\"end\":93423,\"start\":93405},{\"end\":93435,\"start\":93423},{\"end\":93451,\"start\":93435},{\"end\":94076,\"start\":94060},{\"end\":94097,\"start\":94076},{\"end\":94125,\"start\":94097},{\"end\":94133,\"start\":94125},{\"end\":94475,\"start\":94454},{\"end\":94492,\"start\":94475},{\"end\":94512,\"start\":94492},{\"end\":94530,\"start\":94512},{\"end\":94549,\"start\":94530},{\"end\":94564,\"start\":94549},{\"end\":94581,\"start\":94564},{\"end\":94941,\"start\":94929},{\"end\":94951,\"start\":94941},{\"end\":94965,\"start\":94951},{\"end\":94980,\"start\":94965},{\"end\":94993,\"start\":94980},{\"end\":95008,\"start\":94993},{\"end\":95021,\"start\":95008},{\"end\":95033,\"start\":95021},{\"end\":95048,\"start\":95033},{\"end\":95066,\"start\":95048},{\"end\":95084,\"start\":95066},{\"end\":95115,\"start\":95084},{\"end\":95126,\"start\":95115},{\"end\":95133,\"start\":95126},{\"end\":95551,\"start\":95538},{\"end\":95563,\"start\":95551},{\"end\":95573,\"start\":95563},{\"end\":95588,\"start\":95573},{\"end\":95608,\"start\":95588},{\"end\":95624,\"start\":95608},{\"end\":95637,\"start\":95624},{\"end\":95655,\"start\":95637},{\"end\":95671,\"start\":95655},{\"end\":95681,\"start\":95671},{\"end\":95696,\"start\":95681},{\"end\":95710,\"start\":95696},{\"end\":95725,\"start\":95710},{\"end\":95738,\"start\":95725},{\"end\":95753,\"start\":95738},{\"end\":95768,\"start\":95753},{\"end\":95784,\"start\":95768},{\"end\":95792,\"start\":95784},{\"end\":95808,\"start\":95792},{\"end\":95820,\"start\":95808},{\"end\":95826,\"start\":95820},{\"end\":96386,\"start\":96369},{\"end\":96554,\"start\":96534},{\"end\":96578,\"start\":96554},{\"end\":96589,\"start\":96578},{\"end\":97036,\"start\":97022},{\"end\":97052,\"start\":97036},{\"end\":97064,\"start\":97052},{\"end\":97081,\"start\":97064},{\"end\":97097,\"start\":97081},{\"end\":97330,\"start\":97314},{\"end\":97344,\"start\":97330},{\"end\":97357,\"start\":97344},{\"end\":97374,\"start\":97357},{\"end\":97387,\"start\":97374},{\"end\":97402,\"start\":97387},{\"end\":97416,\"start\":97402},{\"end\":97428,\"start\":97416},{\"end\":97732,\"start\":97724},{\"end\":97943,\"start\":97932},{\"end\":97958,\"start\":97943},{\"end\":97970,\"start\":97958},{\"end\":97987,\"start\":97970},{\"end\":98003,\"start\":97987},{\"end\":98022,\"start\":98003},{\"end\":98042,\"start\":98022},{\"end\":98056,\"start\":98042},{\"end\":98071,\"start\":98056},{\"end\":98086,\"start\":98071},{\"end\":98104,\"start\":98086},{\"end\":98124,\"start\":98104},{\"end\":98142,\"start\":98124},{\"end\":98156,\"start\":98142},{\"end\":98169,\"start\":98156},{\"end\":98184,\"start\":98169},{\"end\":98200,\"start\":98184},{\"end\":98212,\"start\":98200},{\"end\":98228,\"start\":98212},{\"end\":98241,\"start\":98228},{\"end\":98252,\"start\":98241},{\"end\":98265,\"start\":98252},{\"end\":98281,\"start\":98265},{\"end\":99198,\"start\":99183},{\"end\":99213,\"start\":99198},{\"end\":99230,\"start\":99213},{\"end\":99246,\"start\":99230},{\"end\":99258,\"start\":99246},{\"end\":99269,\"start\":99258},{\"end\":99283,\"start\":99269},{\"end\":99298,\"start\":99283},{\"end\":99309,\"start\":99298},{\"end\":99320,\"start\":99309},{\"end\":99338,\"start\":99320},{\"end\":99882,\"start\":99870},{\"end\":99895,\"start\":99882},{\"end\":100175,\"start\":100161},{\"end\":100189,\"start\":100175},{\"end\":100203,\"start\":100189},{\"end\":100212,\"start\":100203},{\"end\":100226,\"start\":100212},{\"end\":100239,\"start\":100226},{\"end\":100251,\"start\":100239},{\"end\":100980,\"start\":100968},{\"end\":100993,\"start\":100980},{\"end\":101013,\"start\":100993},{\"end\":101027,\"start\":101013},{\"end\":101038,\"start\":101027},{\"end\":101052,\"start\":101038},{\"end\":101063,\"start\":101052},{\"end\":101075,\"start\":101063},{\"end\":101087,\"start\":101075},{\"end\":101101,\"start\":101087},{\"end\":101113,\"start\":101101},{\"end\":101127,\"start\":101113},{\"end\":101143,\"start\":101127},{\"end\":101157,\"start\":101143},{\"end\":101170,\"start\":101157},{\"end\":101183,\"start\":101170},{\"end\":101194,\"start\":101183},{\"end\":101210,\"start\":101194},{\"end\":101900,\"start\":101888},{\"end\":101912,\"start\":101900},{\"end\":101925,\"start\":101912},{\"end\":101940,\"start\":101925},{\"end\":102453,\"start\":102444},{\"end\":102470,\"start\":102453},{\"end\":102481,\"start\":102470},{\"end\":102499,\"start\":102481},{\"end\":102513,\"start\":102499},{\"end\":102529,\"start\":102513},{\"end\":102542,\"start\":102529},{\"end\":102553,\"start\":102542},{\"end\":102566,\"start\":102553},{\"end\":102581,\"start\":102566},{\"end\":102596,\"start\":102581},{\"end\":102610,\"start\":102596},{\"end\":102999,\"start\":102981},{\"end\":103013,\"start\":102999},{\"end\":103024,\"start\":103013},{\"end\":103038,\"start\":103024},{\"end\":103524,\"start\":103509},{\"end\":103535,\"start\":103524},{\"end\":103544,\"start\":103535},{\"end\":103557,\"start\":103544},{\"end\":103567,\"start\":103557},{\"end\":103578,\"start\":103567},{\"end\":103595,\"start\":103578},{\"end\":103610,\"start\":103595},{\"end\":103624,\"start\":103610},{\"end\":103628,\"start\":103624},{\"end\":103938,\"start\":103924},{\"end\":103956,\"start\":103938},{\"end\":103972,\"start\":103956},{\"end\":103982,\"start\":103972},{\"end\":103994,\"start\":103982},{\"end\":104006,\"start\":103994},{\"end\":104021,\"start\":104006},{\"end\":104383,\"start\":104373},{\"end\":104392,\"start\":104383},{\"end\":104399,\"start\":104392},{\"end\":104415,\"start\":104399},{\"end\":104424,\"start\":104415},{\"end\":104839,\"start\":104825},{\"end\":104850,\"start\":104839},{\"end\":104870,\"start\":104850},{\"end\":105060,\"start\":105044},{\"end\":105079,\"start\":105060},{\"end\":105569,\"start\":105555},{\"end\":105585,\"start\":105569},{\"end\":105597,\"start\":105585},{\"end\":105617,\"start\":105597},{\"end\":106254,\"start\":106242},{\"end\":106264,\"start\":106254},{\"end\":106277,\"start\":106264},{\"end\":106289,\"start\":106277},{\"end\":106303,\"start\":106289},{\"end\":106315,\"start\":106303},{\"end\":106326,\"start\":106315},{\"end\":106338,\"start\":106326},{\"end\":106356,\"start\":106338},{\"end\":106374,\"start\":106356},{\"end\":106383,\"start\":106374},{\"end\":106781,\"start\":106766},{\"end\":106791,\"start\":106781},{\"end\":106802,\"start\":106791},{\"end\":106812,\"start\":106802},{\"end\":106828,\"start\":106812},{\"end\":106839,\"start\":106828},{\"end\":106852,\"start\":106839},{\"end\":106862,\"start\":106852},{\"end\":106872,\"start\":106862},{\"end\":106885,\"start\":106872},{\"end\":106896,\"start\":106885},{\"end\":107334,\"start\":107319},{\"end\":107351,\"start\":107334},{\"end\":107371,\"start\":107351},{\"end\":107383,\"start\":107371},{\"end\":107867,\"start\":107857},{\"end\":107881,\"start\":107867},{\"end\":107889,\"start\":107881},{\"end\":107902,\"start\":107889},{\"end\":107911,\"start\":107902},{\"end\":107921,\"start\":107911},{\"end\":107928,\"start\":107921},{\"end\":107936,\"start\":107928},{\"end\":107945,\"start\":107936},{\"end\":107956,\"start\":107945},{\"end\":108294,\"start\":108283},{\"end\":108309,\"start\":108294},{\"end\":108319,\"start\":108309},{\"end\":108329,\"start\":108319},{\"end\":108338,\"start\":108329},{\"end\":108825,\"start\":108815},{\"end\":108841,\"start\":108825},{\"end\":108854,\"start\":108841},{\"end\":108865,\"start\":108854},{\"end\":108871,\"start\":108865},{\"end\":108883,\"start\":108871},{\"end\":108894,\"start\":108883},{\"end\":108904,\"start\":108894},{\"end\":108925,\"start\":108904},{\"end\":108937,\"start\":108925},{\"end\":108953,\"start\":108937},{\"end\":108974,\"start\":108953},{\"end\":108988,\"start\":108974},{\"end\":109000,\"start\":108988},{\"end\":109017,\"start\":109000},{\"end\":109028,\"start\":109017},{\"end\":109040,\"start\":109028},{\"end\":109046,\"start\":109040},{\"end\":109608,\"start\":109596},{\"end\":109620,\"start\":109608},{\"end\":109633,\"start\":109620},{\"end\":109655,\"start\":109633},{\"end\":109676,\"start\":109655},{\"end\":109687,\"start\":109676},{\"end\":109705,\"start\":109687},{\"end\":109723,\"start\":109705},{\"end\":110343,\"start\":110329},{\"end\":110357,\"start\":110343},{\"end\":110371,\"start\":110357},{\"end\":110386,\"start\":110371},{\"end\":110401,\"start\":110386},{\"end\":110417,\"start\":110401},{\"end\":110429,\"start\":110417},{\"end\":110437,\"start\":110429},{\"end\":110450,\"start\":110437},{\"end\":110869,\"start\":110848},{\"end\":110888,\"start\":110869},{\"end\":110903,\"start\":110888},{\"end\":110924,\"start\":110903},{\"end\":110942,\"start\":110924},{\"end\":110957,\"start\":110942},{\"end\":110974,\"start\":110957},{\"end\":111439,\"start\":111428},{\"end\":111450,\"start\":111439},{\"end\":111461,\"start\":111450},{\"end\":111474,\"start\":111461},{\"end\":111489,\"start\":111474},{\"end\":111505,\"start\":111489},{\"end\":111517,\"start\":111505},{\"end\":112029,\"start\":112014},{\"end\":112043,\"start\":112029},{\"end\":112060,\"start\":112043},{\"end\":112569,\"start\":112556},{\"end\":112583,\"start\":112569},{\"end\":112823,\"start\":112810},{\"end\":112833,\"start\":112823},{\"end\":112845,\"start\":112833},{\"end\":112861,\"start\":112845},{\"end\":112876,\"start\":112861},{\"end\":113207,\"start\":113197},{\"end\":113221,\"start\":113207},{\"end\":113230,\"start\":113221},{\"end\":113238,\"start\":113230},{\"end\":113249,\"start\":113238},{\"end\":113262,\"start\":113249},{\"end\":113271,\"start\":113262},{\"end\":113279,\"start\":113271},{\"end\":113290,\"start\":113279},{\"end\":113299,\"start\":113290},{\"end\":113677,\"start\":113666},{\"end\":113691,\"start\":113677},{\"end\":113713,\"start\":113691},{\"end\":113731,\"start\":113713},{\"end\":113744,\"start\":113731},{\"end\":114235,\"start\":114225},{\"end\":114245,\"start\":114235},{\"end\":114255,\"start\":114245},{\"end\":114268,\"start\":114255},{\"end\":114279,\"start\":114268},{\"end\":114289,\"start\":114279},{\"end\":114791,\"start\":114778},{\"end\":114810,\"start\":114791},{\"end\":114829,\"start\":114810},{\"end\":114848,\"start\":114829},{\"end\":114863,\"start\":114848},{\"end\":115217,\"start\":115204},{\"end\":115229,\"start\":115217},{\"end\":115244,\"start\":115229},{\"end\":115255,\"start\":115244},{\"end\":115275,\"start\":115255},{\"end\":115293,\"start\":115275},{\"end\":115303,\"start\":115293},{\"end\":115314,\"start\":115303},{\"end\":115326,\"start\":115314},{\"end\":115345,\"start\":115326},{\"end\":115362,\"start\":115345},{\"end\":115378,\"start\":115362},{\"end\":115390,\"start\":115378},{\"end\":115407,\"start\":115390},{\"end\":115420,\"start\":115407},{\"end\":115434,\"start\":115420},{\"end\":115449,\"start\":115434},{\"end\":115468,\"start\":115449},{\"end\":115475,\"start\":115468},{\"end\":116231,\"start\":116210},{\"end\":116243,\"start\":116231},{\"end\":116259,\"start\":116243},{\"end\":116281,\"start\":116259},{\"end\":116296,\"start\":116281},{\"end\":116766,\"start\":116750},{\"end\":116781,\"start\":116766},{\"end\":117151,\"start\":117137},{\"end\":117167,\"start\":117151},{\"end\":117181,\"start\":117167},{\"end\":117195,\"start\":117181},{\"end\":117732,\"start\":117728},{\"end\":117746,\"start\":117732},{\"end\":117762,\"start\":117746},{\"end\":117774,\"start\":117762},{\"end\":117788,\"start\":117774},{\"end\":117806,\"start\":117788},{\"end\":118175,\"start\":118140},{\"end\":118196,\"start\":118175},{\"end\":118211,\"start\":118196},{\"end\":118238,\"start\":118211},{\"end\":118251,\"start\":118238},{\"end\":118259,\"start\":118251},{\"end\":118608,\"start\":118596},{\"end\":118620,\"start\":118608},{\"end\":118633,\"start\":118620},{\"end\":118647,\"start\":118633},{\"end\":118660,\"start\":118647},{\"end\":118678,\"start\":118660},{\"end\":118697,\"start\":118678},{\"end\":118711,\"start\":118697},{\"end\":118722,\"start\":118711},{\"end\":118730,\"start\":118722},{\"end\":119107,\"start\":119086},{\"end\":119127,\"start\":119107},{\"end\":119147,\"start\":119127},{\"end\":119435,\"start\":119424},{\"end\":119448,\"start\":119435},{\"end\":119464,\"start\":119448},{\"end\":119479,\"start\":119464},{\"end\":119492,\"start\":119479},{\"end\":119500,\"start\":119492},{\"end\":119519,\"start\":119500},{\"end\":119531,\"start\":119519}]", "bib_venue": "[{\"end\":74326,\"start\":74225},{\"end\":83352,\"start\":83278},{\"end\":84516,\"start\":84442},{\"end\":85410,\"start\":85276},{\"end\":87010,\"start\":86877},{\"end\":87931,\"start\":87871},{\"end\":89038,\"start\":88972},{\"end\":89496,\"start\":89444},{\"end\":90447,\"start\":90359},{\"end\":91017,\"start\":90943},{\"end\":92419,\"start\":92348},{\"end\":93678,\"start\":93573},{\"end\":98504,\"start\":98357},{\"end\":99503,\"start\":99429},{\"end\":102095,\"start\":102026},{\"end\":103203,\"start\":103129},{\"end\":104535,\"start\":104488},{\"end\":105264,\"start\":105180},{\"end\":105888,\"start\":105761},{\"end\":107520,\"start\":107460},{\"end\":108497,\"start\":108426},{\"end\":109884,\"start\":109812},{\"end\":111682,\"start\":111608},{\"end\":112221,\"start\":112149},{\"end\":113903,\"start\":113832},{\"end\":114450,\"start\":114378},{\"end\":115640,\"start\":115566},{\"end\":116419,\"start\":116366},{\"end\":117354,\"start\":117283},{\"end\":74223,\"start\":74107},{\"end\":75149,\"start\":75129},{\"end\":76569,\"start\":76505},{\"end\":76883,\"start\":76803},{\"end\":77507,\"start\":77449},{\"end\":78233,\"start\":78139},{\"end\":78980,\"start\":78922},{\"end\":81781,\"start\":81710},{\"end\":82414,\"start\":82355},{\"end\":82854,\"start\":82823},{\"end\":83276,\"start\":83187},{\"end\":83793,\"start\":83734},{\"end\":84440,\"start\":84351},{\"end\":85274,\"start\":85125},{\"end\":85902,\"start\":85843},{\"end\":86358,\"start\":86300},{\"end\":86875,\"start\":86727},{\"end\":87475,\"start\":87416},{\"end\":87869,\"start\":87794},{\"end\":88313,\"start\":88305},{\"end\":88615,\"start\":88599},{\"end\":88970,\"start\":88889},{\"end\":89442,\"start\":89375},{\"end\":89787,\"start\":89721},{\"end\":90357,\"start\":90254},{\"end\":90941,\"start\":90852},{\"end\":91450,\"start\":91430},{\"end\":91870,\"start\":91806},{\"end\":92346,\"start\":92260},{\"end\":92941,\"start\":92893},{\"end\":93571,\"start\":93451},{\"end\":94058,\"start\":93969},{\"end\":94452,\"start\":94367},{\"end\":94927,\"start\":94872},{\"end\":95885,\"start\":95826},{\"end\":96367,\"start\":96350},{\"end\":96660,\"start\":96589},{\"end\":97113,\"start\":97097},{\"end\":97484,\"start\":97428},{\"end\":98355,\"start\":98281},{\"end\":99427,\"start\":99338},{\"end\":99868,\"start\":99825},{\"end\":100419,\"start\":100251},{\"end\":100966,\"start\":100835},{\"end\":102024,\"start\":101940},{\"end\":102442,\"start\":102378},{\"end\":103127,\"start\":103038},{\"end\":103507,\"start\":103446},{\"end\":104070,\"start\":104021},{\"end\":104486,\"start\":104424},{\"end\":104873,\"start\":104870},{\"end\":105178,\"start\":105079},{\"end\":105759,\"start\":105617},{\"end\":106461,\"start\":106399},{\"end\":106960,\"start\":106896},{\"end\":107458,\"start\":107383},{\"end\":107855,\"start\":107770},{\"end\":108424,\"start\":108338},{\"end\":109104,\"start\":109046},{\"end\":109810,\"start\":109723},{\"end\":110486,\"start\":110450},{\"end\":111048,\"start\":110974},{\"end\":111606,\"start\":111517},{\"end\":112147,\"start\":112060},{\"end\":112554,\"start\":112486},{\"end\":112935,\"start\":112876},{\"end\":113361,\"start\":113299},{\"end\":113830,\"start\":113744},{\"end\":114376,\"start\":114289},{\"end\":114915,\"start\":114863},{\"end\":115564,\"start\":115475},{\"end\":116364,\"start\":116296},{\"end\":116864,\"start\":116781},{\"end\":117281,\"start\":117195},{\"end\":117726,\"start\":117607},{\"end\":118290,\"start\":118259},{\"end\":118594,\"start\":118520},{\"end\":119084,\"start\":119015},{\"end\":119422,\"start\":119354}]"}}}, "year": 2023, "month": 12, "day": 17}