{"id": 219687757, "updated": "2023-10-06 14:23:25.564", "metadata": {"title": "FinBERT: A Pretrained Language Model for Financial Communications", "authors": "[{\"first\":\"Yi\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"UY\",\"last\":\"MarkChristopherSiy\",\"middle\":[]},{\"first\":\"Allen\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 15}, "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at https://github.com/yya518/FinBERT. We hope this will be useful for practitioners and researchers working on financial NLP tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2006.08097", "mag": "3037252472", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2006-08097", "doi": null}}, "content": {"source": {"pdf_hash": "3578a7792904e6af3db8ffefdff86ab6a387c7c3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.08097v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "89b3dba0fea7c838055a5bd9a443f23215ee209a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3578a7792904e6af3db8ffefdff86ab6a387c7c3.txt", "contents": "\nFinBERT: A Pretrained Language Model for Financial Communications\n9 Jul 2020\n\nYi Yang \nSchool of Business and Management\nHong Kong University of Science and Technology\n\n\nMark Christopher \nSchool of Business and Management\nHong Kong University of Science and Technology\n\n\nSiy Uy \nSchool of Business and Management\nHong Kong University of Science and Technology\n\n\nAllen Huang acahuang@ust.hk \nSchool of Business and Management\nHong Kong University of Science and Technology\n\n\nFinBERT: A Pretrained Language Model for Financial Communications\n9 Jul 2020\nContextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text resources. Financial sector also accumulates large amount of financial communication text. However, there is no pretrained finance specific language models available. In this work, we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora.Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at https://github.com/yya518/FinBERT. We hope this will be useful for practitioners and researchers working on financial NLP tasks.\n\nIntroduction\n\nThe growing maturity of NLP techniques and resources is drastically changing the landscape of finanical domain. Capital market practitioners and researchers have keen interests in using NLP techniques to monitor market sentiment in real time from online news articles or social media posts, since sentiment can be used as a directional signal for trading purposes. Intuitively, if there is positive information about a particular company, we expect that company's stock price to increase, and vice versa. For example, Bloomberg, the financial media company, reports that trading sentiment portfolios outperform the benchmark index significantly (Cui et al., 2016). Prior financial economics research also reports that news article and social media sentiment could be used to predict market return and firm performance (Tetlock, 2007;Tetlock et al., 2008).\n\nRecently, unsupervised pre-training of language models on large corpora has significantly improved the performance of many NLP tasks. The language models are pretained on generic corpora such as Wikipedia. However, sentiment analysis is a strongly domain dependent task. Financial sector has accumulated large scale of text of financial and business communications. Therefore, leveraging the success of unsupervised pretraining and large amount of financial text could potentially benefit wide range of financial applications.\n\nTo fill the gap, we pretrain FinBERT, a finance domain specific BERT model on a large financial communication corpora of 4.9 billion tokens, including corporate reports, earnings conference call transcripts and analyst reports. We document the financial corpora and the FinBERT pretraining details. Experiments on three financial sentiment classification tasks shows that FinBERT outperforms the generic BERT models. Our contribution is straightforward: we compile a large scale of text corpora that are the most representative in financial and business communications. We pretrain and release FinBERT, a new resource demonstrated to improve performance on financial sentiment analysis.\n\n\nRelated Work\n\nRecently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), ELMo (Peters et al., 2018), ULM-Fit (Howard and Ruder, 2018), XLNet, and GPT (Radford et al., 2019) has significantly improved performance on many natural language processing tasks, from sentence classification to question answering. Unlike traditional word embedding (Mikolov et al., 2013;Pennington et al., 2014) where word is represented as a single vector representation, these language model returns contextualized embeddings for each word token which can be fed into downstream tasks.\n\nThe released language models are trained on general domain corpora such as news articles and Wikipedia. Even though it is easy to fine tune the language model using downstream task, it has been shown that pre-training a language model using large-scale domain corpora can further improve the task performance than fine-tuning the generic language model. To this end, several domain-specific BERT models are trained and released. BioBERT  pretrains a biomedical domain-specific language representation model using large-scale biomedical corpora. Similarly, ClinicalBERT (Huang et al., 2019) applies BERT model to clinical notes for hospital readmission prediction task, and (Alsentzer et al., 2019) applies BERT on clinical notes and discharge summaries.\n\nSciBERT (Beltagy et al., 2019) trains a scientific domain-specific BERT model using a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We are the first to pre-train and release a finance domain specific BERT model.\n\n\nFinancial Corpora\n\nWe compile a large financial domain corpora that are most representative in finance and business communications. Corporate Reports 10-K & 10-Q The most important text data in finance and business communication is corporate report. In the United States, the Securities Exchange Commission (SEC) mandates all publicly traded companies to file annual reports, known as Form 10-K, and quarterly reports, known as Form 10-Q. This document provides a comprehensive overview of the company's business and financial condition. Laws and regulations prohibit companies from making materially false or misleading statements in the 10-Ks. The Form 10-Ks and 10-Qs are publicly available and can be accesses from SEC website. 1 We obtain 60,490 Form 10-Ks and 142,622 Form 10-Qs of Russell 3000 firms during 1994 and 2019 from SEC website. We only include sections that are textual components, such as Item 1 (Business) in 10-Ks, Item 1A (Risk Factors) in both 10-Ks and 10-Qs and Item 7 (Managements Discussion and Analysis) in 10-Ks. 1 http://www.sec.gov/edgar.shtml Earnings Call Transcripts Earnings calls are quarterly conference calls that company executives hold with investors and analysts to discuss firm overall performance. During an earnings call, executives such as CEOs and CFOs read forwardlooking statements and provide their information and interpretation of their firms performance during the quarter. Analysts also have the opportunity to request managers to clarify information. Institutional and individual investors listen to the earnings call and spot the tones of executives that portend good or bad news for the company. We obtain 136,578 earnings conference call transcripts of 7,740 public firms between 2004 and 2019. The earnings call transcripts are obtained from the website Seeking Alpha 2 . Analyst Reports Analyst reports are another useful source of information for institutional and individual investors (sri International, 1987). An analyst report typically provides several quantitative summary measures, including a stock recommendation, an earnings forecast, and sometimes a target price. It also provides a detailed, mostly textual analysis of the company. Institutional investors spend millions of dollars annually to purchase the full content of analyst reports to read the written textual analysis. We obtain analyst reports in the Investext database issued for S&P firms during the 1995-2008 period, which yields a set of 488,494 reports. Overall Corpora Statistics The total size of all 4 corpora is approximately 4.9 billion tokens. We present the pretraining financial corpora statistics in Table1. As a comparison, BERT's pre-training corpora consists of two textual corpora with a total of 3.3 billion tokens.\n\n\nCorpus\n\n# of tokens Corporate Reports 10-K & 10-Q 2.5B Earnings Call Transcripts 1.3B Analyst Reports 1.1B FinBERT-BaseVocab, uncased/cased: Model is initialized from the original BERT-Base uncased/cased model, and is further pretrained on the financial corpora for 250K iterations at a smaller learning rate of 2e \u22125 , which is recommended by BERT code.\n\nFinBERT-FinVocab, uncased/cased: Model is trained from scratch using a new uncased/cased financial vocabulary FinVocab for 1M iterations. Training The entire training is done using a NVIDIA DGX-1 machine. The server has 4 Tesla P100 GPUs, providing a total of 128 GB of GPU memory. This machine enables us to train the BERT models using a batch size of 128. We utilize Horovord framework (Sergeev and Del Balso, 2018) for multi-GPU training. Overall, the total time taken to perform pretraining for one model is approximately 2 days. With the release of FinBERT, we hope financial practitioners and researchers can benefit from FinBERT model without the necessity of the significant computational resources required to train the model.\n\n\nFinancial Sentiment Experiments\n\nGiven the importance of sentiment analysis in financial NLP tasks, we conduct experiments on financial sentiment classification datasets.\n\n\nDataset\n\nFinancial Phrase Bank is a public dataset for financial sentiment classification (Malo et al., 2014). The dataset contains 4,840 sentences selected from financial news. The dataset is manually labeled by 16 researchers with adequate back-3 https://github.com/google-research/bert ground knowledge on financial markets. The sentiment label is either positive, neutral or negative. AnalystTone Dataset is a dataset to gauge the opinions in analyst reports, which is commonly used in Accounting and Finance literature (Huang et al., 2014). The dataset contains randomly selected 10,000 sentences from analyst reports in the Investext database. Each sentence is manually annotated into one of three categories: positive, negative and neutral. This classification yields a total of 3,580 positive, 1,830 negative, and 4,590 neutral sentences in the dataset. FiQA Dataset is an open challenge dataset for financial sentiment analysis, containing 1,111 text sentences 4 . Given an English text sentence in the financial domain (microblog message, news statement), the task of this challenge is to predict the associated numeric sentiment score, ranged from -1 to 1. We convert the original regression task into a binary classification task for consistent comparison with the above two datasets.\n\nWe randomly split each dataset into 90% training and 10% testing 10 times and report the average. Since all dataset are used for sentiment classification, we report the accuracy metrics in the experiments.\n\n\nFine-tune Strategy\n\nWe follow the same fine-tune architecture and optimization choices used in (Devlin et al., 2019). We use a simple linear layer, as our classification layer, with a softmax activation function. We also use cross-entropy loss as the loss function. Note that an alternative is to feed the contextualized word embeddings of each token into a deep architectures, such as Bi-LSTM, atop frozen BERT embeddings. We choose not to use this strategy as it has shown to perform significantly worse than fine-tune BERT model (Beltagy et al., 2019).\n\n\nExperiment Results\n\nWe compare FinBERT with original BERT-Base model (Devlin et al., 2019), and we evaluate both cased and uncased versions of this model. The main results of financial sentiment analysis tasks are present in Table 2.     Table 3. It shows that FinBERT trained on all corpora achieves the overall best performance indicating that combining additional financial communication corpus could improve the language model quality. Among three datasets, Analyst Reports dataset appears to perform well in three different tasks, even though it only has 1.1 billion word tokens. Prior research finds that corporate report such as 10-Ks and 10-Qs contains redundant content, and that a substantial amount of textual volume contained in 10-K reports is attributable to managerial discretion in how firms respond to mandatory disclosure requirements (Cazier and Pfeiffer, 2016). Does it suggest that Analyst Reports data contains more information content than corporate reports and earnings call transcripts? We leave it for future research.\n\n\nConclusion\n\nIn this work, we pre-train a financial-task oriented BERT model, FinBERT. The FinBERT model is trained on a large financial corpora that are representative of English financial communications. We show that FinBERT outperforms generic BERT models on three financial sentiment classification tasks. With the release of FinBERT, we hope practitioners and researchers can utilize Fin-BERT for a wider range of applications where the prediction target goes beyond sentiment, such as financial-related outcomes including stock returns, stock volatilities, corporate fraud, etc.\n\n\nFinBERT vs. BERT The results show substantial improvement of FinBERT models over the generic BERT models. On PhraseBank dataset, the best model uncased FinBERT-FinVocab achieves the BERT\n\nTable 1 :\n1Size of pretraining financial corpora.4 FinBERT Training \n\nVocabulary We construct FinVocab, a new Word-\nPiece vocabulary on our financial corpora using \nthe SentencePiece library. We produce both cased \nand uncased versions of FinVocab, with sizes of \n\n\nTable 2 :\n2Performance of different BERT models on three financial sentiment analysis tasks.10-Ks/10-Qs Earnings Call Analyst Reports \nAll \nPhraseBank \n0.835 \n0.843 \n0.845 \n0.856 \nBaseVocab \nFiQA \n0.707 \n0.731 \n0.744 \n0.767 \nAnalystTone \n0.845 \n0.862 \n0.871 \n0.872 \nPhraseBank \n0.847 \n0.860 \n0.861 \n0.864 \nFinVocab \nFiQA \n0.766 \n0.778 \n0.796 \n0.814 \nAnalystTone \n0.858 \n0.870 \n0.872 \n0.876 \n\n\n\nTable 3 :\n3Performance of pretraining on different financial corpus.FinVocab vs. BaseVocab We assess the importance of an in-domain financial vocabulary by pretraining different FinBERT models using BaseVocab and FinVocab. For both uncased and cased model, we see that FinBERT-FinVocab outperforms its BaseVocab counterpart. However, the performance improvement is quite marginal on PhraseBank and AnalystTone task. Only do we see substantial improvement on FiQA task (0.844 vs. 0.796). Given the magnitude of improvement, we suspect that while an in-domain vocabulary is helpful, FinBERT benefits most from the financial communication corpora pretraining.Cased vs. Uncased We follow(Devlin et al., 2019) in using both the cased model and the uncased model for all tasks. Experiments result suggest that uncased models perform better than cased models in all tasks. This result is consistent with prior work of Scientific domain and Biomedical domain BERT models. Corpus Contribution We also train different Fin-BERT models on three financial corpus separately. The performance of different FinBERT models (cased version) on different tasks are present inaccuracy of 0.872, a 4.4% improvement over un-\ncased BERT model and 15.4% improvement over \ncased BERT model. On FiQA dataset, the best \nmodel uncased FinBERT-FinVocab achieves the \naccuracy of 0.844, a 15.6% improvement over \nuncased BERT model and a 29.2% improvement \nover cased BERT model. Lastly, on the Analyst-\nTone dataset, the best model uncased FinBERT-\nFinVocab improves the uncased and cased BERT \nmodel by 4.3% and 5.5% respectively. Overall \nspeaking, pretraining on financial corpora, as ex-\npected, is effective and enhances the downstream \nfinancial sentiment classification tasks. In finan-\ncial markets where capturing the accurate senti-\nment signal is of utmost importance, we believe \nthe overall FinBERT improvement demonstrates \nits practical utility. \n\n\nhttps://seekingalpha.com/\nhttps://sites.google.com/view/fiqa/home\nAcknowledgmentsThis work was supported by Theme-based Research Scheme (No. T31-604/18-N) from Research Grants Council in Hong Kong.\nPublicly available clinical bert embeddings. Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, Matthew Mcdermott, Proceedings of the 2nd Clinical Natural Language Processing Workshop. the 2nd Clinical Natural Language Processing WorkshopEmily Alsentzer, John Murphy, William Boag, Wei- Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clin- ical bert embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72-78.\n\nScibert: Pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of EMNLP. EMNLPIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- ert: Pretrained language model for scientific text. In Proceedings of EMNLP.\n\nWhy are 10-k filings so long? Accounting Horizons. A Richard, Cazier, J Ray, Pfeiffer, 30Richard A Cazier and Ray J Pfeiffer. 2016. Why are 10-k filings so long? Accounting Horizons, 30(1):1- 21.\n\nEmbedded value in bloomberg news and social sentiment data. X Cui, A Lam, Verma, Bloomberg LP. X Cui, D Lam, and A Verma. 2016. Embedded value in bloomberg news and social sentiment data. Bloomberg LP.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of NAACL. NAACLJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In Proceedings of NAACL, pages 4171-4186.\n\nUniversal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, Proceedings ACL. ACLJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings ACL, pages 328-339.\n\nEvidence on the information content of text in analyst reports. H Allen, Amy Y Huang, Rong Zang, Zheng, The Accounting Review. 896Allen H Huang, Amy Y Zang, and Rong Zheng. 2014. Evidence on the information content of text in an- alyst reports. The Accounting Review, 89(6):2151- 2180.\n\nClinicalbert: Modeling clinical notes and predicting hospital readmission. Kexin Huang, Jaan Altosaar, Rajesh Ranganath, arXiv:1904.05342sri International. 1987. Investor information needs and the annual report. Financial Executives Research Foundation. Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019. Clinicalbert: Modeling clinical notes and pre- dicting hospital readmission. arXiv:1904.05342. sri International. 1987. Investor information needs and the annual report. Financial Executives Research Foundation.\n\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics.\n\nGood debt or bad debt: Detecting semantic orientations in economic texts. Pekka Malo, Ankur Sinha, Pekka Korhonen, Journal of the Association for Information Science and Technology. 654Jyrki Wallenius, and Pyry TakalaPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal- lenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782-796.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, Proceedings of NIPS. NIPSTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality. In Proceedings of NIPS, pages 3111-3119.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of EMNLP. EMNLPJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP, pages 1532-1543.\n\nDeep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proc. of NAACL. of NAACLMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proc. of NAACL.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nHorovod: fast and easy distributed deep learning in tensorflow. Alexander Sergeev, Mike Del Balso, arXiv:1802.05799arXiv preprintAlexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easy distributed deep learning in tensorflow. arXiv preprint arXiv:1802.05799.\n\nGiving content to investor sentiment: The role of media in the stock market. C Paul, Tetlock, The Journal of finance. 623Paul C Tetlock. 2007. Giving content to investor sen- timent: The role of media in the stock market. The Journal of finance, 62(3):1139-1168.\n\nMore than words: Quantifying language to measure firms' fundamentals. The Journal of Finance. Maytal Paul C Tetlock, Sofus Saar-Tsechansky, Macskassy, 63Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus Macskassy. 2008. More than words: Quantifying language to measure firms' fundamentals. The Jour- nal of Finance, 63(3):1437-1467.\n", "annotations": {"author": "[{\"end\":170,\"start\":79},{\"end\":271,\"start\":171},{\"end\":362,\"start\":272},{\"end\":474,\"start\":363}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":82},{\"end\":187,\"start\":176},{\"end\":278,\"start\":276},{\"end\":374,\"start\":369}]", "author_first_name": "[{\"end\":81,\"start\":79},{\"end\":175,\"start\":171},{\"end\":275,\"start\":272},{\"end\":368,\"start\":363}]", "author_affiliation": "[{\"end\":169,\"start\":88},{\"end\":270,\"start\":189},{\"end\":361,\"start\":280},{\"end\":473,\"start\":392}]", "title": "[{\"end\":66,\"start\":1},{\"end\":540,\"start\":475}]", "venue": null, "abstract": "[{\"end\":1352,\"start\":552}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2031,\"start\":2013},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2201,\"start\":2186},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2222,\"start\":2201},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3563,\"start\":3542},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3591,\"start\":3570},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3625,\"start\":3601},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3664,\"start\":3642},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3855,\"start\":3833},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3879,\"start\":3855},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4646,\"start\":4626},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4754,\"start\":4730},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4842,\"start\":4820},{\"end\":5825,\"start\":5824},{\"end\":7062,\"start\":7038},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8633,\"start\":8604},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9236,\"start\":9217},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9671,\"start\":9651},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10749,\"start\":10728},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11187,\"start\":11165},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11281,\"start\":11260},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12071,\"start\":12044},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14376,\"start\":14355}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":13010,\"start\":12822},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":13276,\"start\":13011},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":13670,\"start\":13277},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":15605,\"start\":13671}]", "paragraph": "[{\"end\":2223,\"start\":1368},{\"end\":2751,\"start\":2225},{\"end\":3439,\"start\":2753},{\"end\":4055,\"start\":3456},{\"end\":4810,\"start\":4057},{\"end\":5089,\"start\":4812},{\"end\":7857,\"start\":5111},{\"end\":8214,\"start\":7868},{\"end\":8951,\"start\":8216},{\"end\":9124,\"start\":8987},{\"end\":10423,\"start\":9136},{\"end\":10630,\"start\":10425},{\"end\":11188,\"start\":10653},{\"end\":12235,\"start\":11211},{\"end\":12821,\"start\":12250}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11423,\"start\":11416},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11436,\"start\":11429}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1366,\"start\":1354},{\"attributes\":{\"n\":\"2\"},\"end\":3454,\"start\":3442},{\"attributes\":{\"n\":\"3\"},\"end\":5109,\"start\":5092},{\"end\":7866,\"start\":7860},{\"attributes\":{\"n\":\"5\"},\"end\":8985,\"start\":8954},{\"attributes\":{\"n\":\"5.1\"},\"end\":9134,\"start\":9127},{\"attributes\":{\"n\":\"5.2\"},\"end\":10651,\"start\":10633},{\"attributes\":{\"n\":\"5.3\"},\"end\":11209,\"start\":11191},{\"attributes\":{\"n\":\"6\"},\"end\":12248,\"start\":12238},{\"end\":13021,\"start\":13012},{\"end\":13287,\"start\":13278},{\"end\":13681,\"start\":13672}]", "table": "[{\"end\":13276,\"start\":13061},{\"end\":13670,\"start\":13370},{\"end\":15605,\"start\":14827}]", "figure_caption": "[{\"end\":13010,\"start\":12824},{\"end\":13061,\"start\":13023},{\"end\":13370,\"start\":13289},{\"end\":14827,\"start\":13683}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":15854,\"start\":15849},{\"end\":15870,\"start\":15866},{\"end\":15886,\"start\":15879},{\"end\":15901,\"start\":15893},{\"end\":15910,\"start\":15908},{\"end\":15925,\"start\":15918},{\"end\":15942,\"start\":15935},{\"end\":16385,\"start\":16383},{\"end\":16399,\"start\":16395},{\"end\":16409,\"start\":16404},{\"end\":16624,\"start\":16623},{\"end\":16643,\"start\":16642},{\"end\":16830,\"start\":16829},{\"end\":16837,\"start\":16836},{\"end\":17059,\"start\":17054},{\"end\":17076,\"start\":17068},{\"end\":17090,\"start\":17084},{\"end\":17104,\"start\":17096},{\"end\":17410,\"start\":17404},{\"end\":17428,\"start\":17419},{\"end\":17660,\"start\":17659},{\"end\":17671,\"start\":17668},{\"end\":17673,\"start\":17672},{\"end\":17685,\"start\":17681},{\"end\":17962,\"start\":17957},{\"end\":17974,\"start\":17970},{\"end\":17991,\"start\":17985},{\"end\":18504,\"start\":18497},{\"end\":18516,\"start\":18510},{\"end\":18531,\"start\":18523},{\"end\":18546,\"start\":18537},{\"end\":18558,\"start\":18552},{\"end\":18568,\"start\":18564},{\"end\":18582,\"start\":18576},{\"end\":18895,\"start\":18890},{\"end\":18907,\"start\":18902},{\"end\":18920,\"start\":18915},{\"end\":19355,\"start\":19350},{\"end\":19369,\"start\":19365},{\"end\":19384,\"start\":19381},{\"end\":19395,\"start\":19391},{\"end\":19412,\"start\":19405},{\"end\":19702,\"start\":19695},{\"end\":19722,\"start\":19715},{\"end\":19744,\"start\":19731},{\"end\":19989,\"start\":19982},{\"end\":19991,\"start\":19990},{\"end\":20004,\"start\":20000},{\"end\":20019,\"start\":20014},{\"end\":20031,\"start\":20027},{\"end\":20052,\"start\":20041},{\"end\":20066,\"start\":20060},{\"end\":20076,\"start\":20072},{\"end\":20354,\"start\":20350},{\"end\":20368,\"start\":20364},{\"end\":20378,\"start\":20373},{\"end\":20391,\"start\":20386},{\"end\":20403,\"start\":20398},{\"end\":20416,\"start\":20412},{\"end\":20643,\"start\":20634},{\"end\":20657,\"start\":20653},{\"end\":20661,\"start\":20658},{\"end\":20919,\"start\":20918},{\"end\":21205,\"start\":21199},{\"end\":21227,\"start\":21222}]", "bib_author_last_name": "[{\"end\":15864,\"start\":15855},{\"end\":15877,\"start\":15871},{\"end\":15891,\"start\":15887},{\"end\":15906,\"start\":15902},{\"end\":15916,\"start\":15911},{\"end\":15933,\"start\":15926},{\"end\":15952,\"start\":15943},{\"end\":16393,\"start\":16386},{\"end\":16402,\"start\":16400},{\"end\":16415,\"start\":16410},{\"end\":16632,\"start\":16625},{\"end\":16640,\"start\":16634},{\"end\":16647,\"start\":16644},{\"end\":16657,\"start\":16649},{\"end\":16834,\"start\":16831},{\"end\":16841,\"start\":16838},{\"end\":16848,\"start\":16843},{\"end\":17066,\"start\":17060},{\"end\":17082,\"start\":17077},{\"end\":17094,\"start\":17091},{\"end\":17114,\"start\":17105},{\"end\":17417,\"start\":17411},{\"end\":17434,\"start\":17429},{\"end\":17666,\"start\":17661},{\"end\":17679,\"start\":17674},{\"end\":17690,\"start\":17686},{\"end\":17697,\"start\":17692},{\"end\":17968,\"start\":17963},{\"end\":17983,\"start\":17975},{\"end\":18001,\"start\":17992},{\"end\":18508,\"start\":18505},{\"end\":18521,\"start\":18517},{\"end\":18535,\"start\":18532},{\"end\":18550,\"start\":18547},{\"end\":18562,\"start\":18559},{\"end\":18574,\"start\":18569},{\"end\":18587,\"start\":18583},{\"end\":18900,\"start\":18896},{\"end\":18913,\"start\":18908},{\"end\":18929,\"start\":18921},{\"end\":19363,\"start\":19356},{\"end\":19379,\"start\":19370},{\"end\":19389,\"start\":19385},{\"end\":19403,\"start\":19396},{\"end\":19417,\"start\":19413},{\"end\":19713,\"start\":19703},{\"end\":19729,\"start\":19723},{\"end\":19752,\"start\":19745},{\"end\":19998,\"start\":19992},{\"end\":20012,\"start\":20005},{\"end\":20025,\"start\":20020},{\"end\":20039,\"start\":20032},{\"end\":20058,\"start\":20053},{\"end\":20070,\"start\":20067},{\"end\":20088,\"start\":20077},{\"end\":20362,\"start\":20355},{\"end\":20371,\"start\":20369},{\"end\":20384,\"start\":20379},{\"end\":20396,\"start\":20392},{\"end\":20410,\"start\":20404},{\"end\":20426,\"start\":20417},{\"end\":20651,\"start\":20644},{\"end\":20667,\"start\":20662},{\"end\":20924,\"start\":20920},{\"end\":20933,\"start\":20926},{\"end\":21220,\"start\":21206},{\"end\":21243,\"start\":21228},{\"end\":21254,\"start\":21245}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":102352093},\"end\":16325,\"start\":15804},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":202558505},\"end\":16570,\"start\":16327},{\"attributes\":{\"id\":\"b2\"},\"end\":16767,\"start\":16572},{\"attributes\":{\"id\":\"b3\"},\"end\":16970,\"start\":16769},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52967399},\"end\":17340,\"start\":16972},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":40100965},\"end\":17593,\"start\":17342},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":147633751},\"end\":17880,\"start\":17595},{\"attributes\":{\"doi\":\"arXiv:1904.05342\",\"id\":\"b7\",\"matched_paper_id\":119308351},\"end\":18403,\"start\":17882},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":59291975},\"end\":18814,\"start\":18405},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7700237},\"end\":19271,\"start\":18816},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":16447573},\"end\":19646,\"start\":19273},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1957433},\"end\":19938,\"start\":19648},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3626819},\"end\":20295,\"start\":19940},{\"attributes\":{\"id\":\"b13\"},\"end\":20568,\"start\":20297},{\"attributes\":{\"doi\":\"arXiv:1802.05799\",\"id\":\"b14\"},\"end\":20839,\"start\":20570},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":18509529},\"end\":21103,\"start\":20841},{\"attributes\":{\"id\":\"b16\"},\"end\":21437,\"start\":21105}]", "bib_title": "[{\"end\":15847,\"start\":15804},{\"end\":16381,\"start\":16327},{\"end\":16827,\"start\":16769},{\"end\":17052,\"start\":16972},{\"end\":17402,\"start\":17342},{\"end\":17657,\"start\":17595},{\"end\":17955,\"start\":17882},{\"end\":18495,\"start\":18405},{\"end\":18888,\"start\":18816},{\"end\":19348,\"start\":19273},{\"end\":19693,\"start\":19648},{\"end\":19980,\"start\":19940},{\"end\":20916,\"start\":20841}]", "bib_author": "[{\"end\":15866,\"start\":15849},{\"end\":15879,\"start\":15866},{\"end\":15893,\"start\":15879},{\"end\":15908,\"start\":15893},{\"end\":15918,\"start\":15908},{\"end\":15935,\"start\":15918},{\"end\":15954,\"start\":15935},{\"end\":16395,\"start\":16383},{\"end\":16404,\"start\":16395},{\"end\":16417,\"start\":16404},{\"end\":16634,\"start\":16623},{\"end\":16642,\"start\":16634},{\"end\":16649,\"start\":16642},{\"end\":16659,\"start\":16649},{\"end\":16836,\"start\":16829},{\"end\":16843,\"start\":16836},{\"end\":16850,\"start\":16843},{\"end\":17068,\"start\":17054},{\"end\":17084,\"start\":17068},{\"end\":17096,\"start\":17084},{\"end\":17116,\"start\":17096},{\"end\":17419,\"start\":17404},{\"end\":17436,\"start\":17419},{\"end\":17668,\"start\":17659},{\"end\":17681,\"start\":17668},{\"end\":17692,\"start\":17681},{\"end\":17699,\"start\":17692},{\"end\":17970,\"start\":17957},{\"end\":17985,\"start\":17970},{\"end\":18003,\"start\":17985},{\"end\":18510,\"start\":18497},{\"end\":18523,\"start\":18510},{\"end\":18537,\"start\":18523},{\"end\":18552,\"start\":18537},{\"end\":18564,\"start\":18552},{\"end\":18576,\"start\":18564},{\"end\":18589,\"start\":18576},{\"end\":18902,\"start\":18890},{\"end\":18915,\"start\":18902},{\"end\":18931,\"start\":18915},{\"end\":19365,\"start\":19350},{\"end\":19381,\"start\":19365},{\"end\":19391,\"start\":19381},{\"end\":19405,\"start\":19391},{\"end\":19419,\"start\":19405},{\"end\":19715,\"start\":19695},{\"end\":19731,\"start\":19715},{\"end\":19754,\"start\":19731},{\"end\":20000,\"start\":19982},{\"end\":20014,\"start\":20000},{\"end\":20027,\"start\":20014},{\"end\":20041,\"start\":20027},{\"end\":20060,\"start\":20041},{\"end\":20072,\"start\":20060},{\"end\":20090,\"start\":20072},{\"end\":20364,\"start\":20350},{\"end\":20373,\"start\":20364},{\"end\":20386,\"start\":20373},{\"end\":20398,\"start\":20386},{\"end\":20412,\"start\":20398},{\"end\":20428,\"start\":20412},{\"end\":20653,\"start\":20634},{\"end\":20669,\"start\":20653},{\"end\":20926,\"start\":20918},{\"end\":20935,\"start\":20926},{\"end\":21222,\"start\":21199},{\"end\":21245,\"start\":21222},{\"end\":21256,\"start\":21245}]", "bib_venue": "[{\"end\":16077,\"start\":16024},{\"end\":16444,\"start\":16439},{\"end\":17143,\"start\":17138},{\"end\":17456,\"start\":17453},{\"end\":19444,\"start\":19440},{\"end\":19781,\"start\":19776},{\"end\":20114,\"start\":20106},{\"end\":16022,\"start\":15954},{\"end\":16437,\"start\":16417},{\"end\":16621,\"start\":16572},{\"end\":16862,\"start\":16850},{\"end\":17136,\"start\":17116},{\"end\":17451,\"start\":17436},{\"end\":17720,\"start\":17699},{\"end\":18134,\"start\":18019},{\"end\":18603,\"start\":18589},{\"end\":18996,\"start\":18931},{\"end\":19438,\"start\":19419},{\"end\":19774,\"start\":19754},{\"end\":20104,\"start\":20090},{\"end\":20348,\"start\":20297},{\"end\":20632,\"start\":20570},{\"end\":20957,\"start\":20935},{\"end\":21197,\"start\":21105}]"}}}, "year": 2023, "month": 12, "day": 17}