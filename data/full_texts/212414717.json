{"id": 212414717, "updated": "2023-11-10 23:29:36.119", "metadata": {"title": "Talking-Heads Attention", "authors": "[{\"first\":\"Noam\",\"last\":\"Shazeer\",\"middle\":[]},{\"first\":\"Zhenzhong\",\"last\":\"Lan\",\"middle\":[]},{\"first\":\"Youlong\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Nan\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Le\",\"last\":\"Hou\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 3, "day": 5}, "abstract": "We introduce\"talking-heads attention\"- a variation on multi-head attention which includes linearprojections across the attention-heads dimension, immediately before and after the softmax operation.While inserting only a small number of additional parameters and a moderate amount of additionalcomputation, talking-heads attention leads to better perplexities on masked language modeling tasks, aswell as better quality when transfer-learning to language comprehension and question answering tasks.", "fields_of_study": "[\"Computer Science\",\"Engineering\",\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "3010476030", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2003-02436", "doi": null}}, "content": {"source": {"pdf_hash": "26080498fb851b6239114f0871a4957bea3d3684", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.02436v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a76e6953b96899fd835cf54c34a9e2fdeb8df74b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/26080498fb851b6239114f0871a4957bea3d3684.txt", "contents": "\nTalking-Heads Attention\nMarch 6, 2020\n\nNoam Shazeer \nGoogle \nZhenzhong Lan \nGoogle \nYoulong Cheng \nGoogle \nNan Ding dingnan@google.com \nGoogle \nLe Hou lehou@google.com \nGoogle \nTalking-Heads Attention\nMarch 6, 2020\nWe introduce \"talking-heads attention\" -a variation on multi-head attention which includes linear projections across the attention-heads dimension, immediately before and after the softmax operation. While inserting only a small number of additional parameters and a moderate amount of additional computation, talking-heads attention leads to better perplexities on masked language modeling tasks, as well as better quality when transfer-learning to language comprehension and question answering tasks. * Noam Shazeer devised the talking-heads architecture, ran the T5 experiments and wrote most of the paper. Zhenzhong Lan had the initial idea of talking-heads attention, designed and coordinated part of the experiments. Youlong Cheng reproduced BERT in MeshTensorFlow and run all the talking heads experiments for MeshTensorFlow BERT. Nan Ding ran the ALBERT experiments. Le Hou visualized and analyzed the learned weights of talking-heads.1 Section (A) of table 3 in[Vaswani et al., 2017]. Also the first sections of tables 1 and 5 of this paper.We use einsum notation for generalized contractions between tensors of arbitrary dimension. The computation is numerically equivalent to broadcasting each input to have the union of all dimensions, multiplying component-wise, and summing across all dimensions not in the output. Rather than identifying the dimensions by an equation, as in TensorFlow and numpy, the dimensions are indentified by the dimension-list annotations on the arguments and on the result. For example, multiplying two matrices would be expressed as:Review of Attention AlgorithmsDot-Product AttentionSimple dot-product attention can be described by the pseudocode below. The logits L are computed as the dot-products of the query-vectors and the memory-vectors. For each query, the logits are passed through a softmax function to produce weights, and the different memory-vectors are averaged together, weighted by those weights. In this code, we show the case where there are n different queries all attending to the same m memory-vectors. If there is only one query, the code is identical except that the \"n\" dimension is removed from all tensors.[Vaswani et al., 2017]propose a dimensionality-reduction to reduce the computational complexity of the attention algorithm. In this version, instead of computing the attention algorithm directly on the inputs X and M , we first project the inputs using the learned linear projections P q , P k and P v , to produce lower-dimensional query-vectors, key-vectors and value-vectors Q, K and V . We use a fourth learned linear projection, P o , to produce the output.def D o t P r o d u c t A t t e n t i o n ( X [n , d ] , # n query -vectors with dimensio nality d M [m , d ]): # m memory -vectors with d imension ality d L [n , m ] = einsum ( X [n , d ] , M [m , d ]) # Attention logits W [n , m ] = softmax ( L [n , m ] , reduced_dim = m ) # Attention weights Y [n , d ] = einsum ( W [n , m ] , M [m , d ]) return Y [n , d ]Dot-Product Attention With Projectionsdef D o t P r o d u c t A t t e n t i o n W i t h P r o j e c t i o n s( X [n , d_X ] , # n vectors with dim ensional ity d_X M [m , d_M ] , # m vectors with dim ensional ity d_M P_q [ d_X , d_k ] , # learned linear projection to produce queries P_k [ d_M , d_k ] , # learned linear projection to produce keys P_v [ d_M , d_v ] , # learned linear projection to produce values P_o [ d_Y , d_v ]): # learned linear projection of output Q [n , d_k ] = einsum ( X [n , d_X ] , P_q [ d_X , d_k ]) # queries K [m , d_k ] = einsum ( M [m , d_M ] , P_k [ d_M , d_k ]) # keys V [m , d_v ] = einsum ( M [m , d_M ] , P_v [ d_M , d_v ]) # values L [n , m ] = einsum ( Q [n , d_k ] , K [m , d_k ]) # Attention logits W [n , m ] = softmax ( L [n , m ] , reduced_dim = m ) # Attention weights O [n , d_v ] = einsum ( W [n , m ] , V [m , d_v ]) Y [n , d_Y ] = einsum ( O [n , d_v ] , P_o [ d_Y , d_v ]) return Y [n , d_Y ]Multi-Head AttentionThe multi-head attention described in[Vaswani et al., 2017]consists of the sum of multiple parallel attention layers. This can be represented by adding a \"heads\" dimension h to the above computation.\n\nIntroduction\n\nNeural Attention was introduced by [Bahdanau et al., 2014] as a way of extracting information from variablelength representations. The Transformer model [Vaswani et al., 2017] uses \"multi-head\" attention, consisting of multiple attention layers (\"heads\") in parallel, each with different projections on its inputs and outputs. By using a dimensionality reduction in the input projections, the computational cost is kept similar to that of basic attention. Quality is improved, presumably due to the ability to attend to multiple positions simultaneously based on multiple different types of relationships.\n\nAs noted in [Vaswani et al., 2017] 1 , taking this process to the extreme (more attention heads projected to lower dimensionality) becomes counterproductive. We believe that this is due to the fact that the query-vectors and key-vectors become so low-dimensional that their dot product can no longer constitute an informative matching function.\n\nIn this paper, we introduce a new variant, \"talking-heads attention\", that addresses this problem by inserting a learned linear projection across the attention-heads dimension of the attention-logits tensor. This allows each attention function to depend on all of the keys and queries. We also insert a second such projection immediately following the softmax.\n\nWe show experimentally that inserting these \"talking-heads\" projections leads to better perplexities on masked language modeling tasks, as well as better quality when transfer-learning to language comprehension and question answering tasks.\n\n\nNotation\n\nIn our pseudocode, we use capital letters to represent tensors and lower-case letters to represent their dimensions. Each tensor is followed by a dimension list in brackets. For example, a 4-dimensional image-def M u l t i H ea d A t t e n t i o n ( X [n , d_X ] , # n vectors with dimensi onality d_X M [m , d_M ] , # m vectors with dimensi onality d_M P_q [ d_X , d_k , h ] , # learned linear projection to produce queries P_k [ d_M , d_k , h ] , # learned linear projection to produce keys P_v [ d_M , d_v , h ] , # learned linear projection to produce values P_o [ d_Y , d_v , h ] The pseudo-code above illustrates the practical step-by-step computation of multi-head attention. The costs of the einsum operations (the number of multiplications in a naive implementation) are shown in the comments. The equivalent pseudo-code below uses multi-way einsums and is more concise: def M u l t i H e a d A t t e n t i o n C o n c i s e (X , M , P_q , P_k , P_v , P_o ) Note: [Vaswani et al., 2017] include a constant scaling factor on the logits. We omit this in our code, as it can be folded into the linear projections P q or P k .\n\n\nTalking-Heads Attention\n\nIn multi-head attention, the different attention heads perform separate computations, which are then summed at the end. Our new variation, which we call \"Talking-Heads Attention\" breaks that separation. We insert two additional learned linear projections, P l and P w , which transform the attention-logits and the attentionweights respectively, moving information across attention heads. 2 Instead of one \"heads\" dimension h across the whole computation, we now have three separate heads dimensions: h k , h, and h v , which can optionally differ in size (number of \"heads\"). h k refers to the number of attention heads for the keys and the queries. h refers to the number of attention heads for the logits and the weights, and h v refers to the number of attention heads for the values. The algorithm is shown by the pseudo-code below. The costs of the einsum operations are shown in the comments. \n\n\nComplexity Analysis\n\nIf we assume that d X = d Y , then the number of scalar multiplications in multi-head attention is:\nh \u00b7 (d k + d v ) \u00b7 (n \u00b7 d X + m \u00b7 d M + n \u00b7 m)\nThe number of scalar multiplications in talking-heads attention is:\n(d k \u00b7 h k + d v \u00b7 h v ) \u00b7 (n \u00b7 d X + m \u00b7 d M + n \u00b7 m) + n \u00b7 m \u00b7 h \u00b7 (h k + h v )\nThe first term in this expression matches up with the cost of multi-head attention. The second term is due to the talking-heads projections. If h < d k and h < d v , the the costs of the new talking-heads projections, n \u00b7 m \u00b7 h \u00b7 h k and n \u00b7 m \u00b7 h \u00b7 h v are less than the existing terms n \u00b7 m \u00b7 d k \u00b7 h k and n \u00b7 m \u00b7 d v \u00b7 h v , respectively.\n\nIn practice, the talking-heads projections may be expensive on some neural-network accelerators due to the small dimension sizes involved.\n\n\nOne More Way To Look At It\n\nMathematically, one can view multi-head attention and talking-heads attention as two special cases of the same general function, which we will call \"general bilinear multihead attention\" (GBMA). GBMA uses two three-dimensional parameter tensors, as defined in the pseudocode below. Due to its high computational cost, GBMA may have no practical use. Multi-head attention is mathematically equivalent to a version of GBMA where each of the two parameter tensors is expressed as the product of two factors, as shown below. Talking-heads attention is mathematically equivalent to a version of GBMA where each of the two parameter tensors is expressed as the product of three factors, as shown below.\n\ndef G e n e r a l B i l i n e a r M u l t i h e a d A t t e n t i o n ( return G e n e r a l B i l i n e a r M u l t i h e a d A t t e n t i o n (X , M , P , Q )\n\n\nExperiments\n\n\nText-to-Text Transfer Transformer (T5)\n\nWe test various configurations of multi-head attention and talking-heads attention on the transfer-learning setup from [Raffel et al., 2019]. An encoder-decoder transformer model [Vaswani et al., 2017] is pre-trained on a denoising objective of predicting missing text segments (average span length 3) from the C4 dataset [Raffel et al., 2019] 3 , and subsequently fine-tuned on various language understanding tasks. We use the same code base and model architecture as the base model from [Raffel et al., 2019]. The encoder and decoder each consist of 12 layers, with d model = 768 and d f f = 3072. Each encoder layer contains a multi-head self-attention layer, and each decoder layer contains a multi-head self-attention layer and a multi-head attention-over-encoder layer. For their base model, [Raffel et al., 2019] follow [Devlin et al., 2018] and others, using h = 12 and d k = d v = 64 for all of these attention layers. We compare this setting to a variety of other configurations of multi-head and talking-heads attention, as detailed in table 1. Similar to [Raffel et al., 2019], we pre-train our models for 524288 steps. Each training batch consists of 128 examples, each of which has an input of 512 tokens and an output of 114 tokens, the output containing multiple spans of tokens which were deleted from the input. Similarly to [Raffel et al., 2019], we use the Adafactor optimizer [Shazeer and Stern, 2018] and an inverse-square-root learning-rate schedule. We also decay the learning rate linearly for the final 10 percent of the training steps. Our main departure from [Raffel et al., 2019] is that we, as suggested by [Lan et al., 2019], use no dropout during pre-training. We find this to produce superior results. We compute the log-perplexity on the training objective on a held-out shard of C4, which we believe to be a good indicator of model quality. For each configuration, we train one model for the \"full\" 524288 steps and four models for a shorter time (65536 steps) to measure inter-run variability. The results are listed in table 1.\n\nWe then fine-tune each of the models on an examples-proportional mixture of SQUAD [Rajpurkar et al., 2016], GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019]. Fine-tuning consists of 131072 additional steps with a learning rate of 10 \u22123 . Following [Raffel et al., 2019], we use a dropout rate 0.1 on the layer outputs, feed-forward hidden-layers and attention weights. The embedding matrix (also used as the projection in the final classifier layer) is fixed during fine-tuning. Tables 1, 2, 3 and 4 include results for SQUAD and MNLI-m. Results for all other tasks are listed in the appendix.\n\n\nMulti-Head vs Talking-Heads Attention\n\nIn table 1, we compare multi-head attention to talking-heads attention. For each of the two algorithms, we test versions with 6, 12, 24 and 48 heads. Following [Vaswani et al., 2017], as we increase the number of heads, we decrease the key/value dimensionality d k and d v , so as to keep the number of parameters constant. For each number of heads, talking-heads attention improves over multi-head attention on all quality metrics. Additionally, multi-head attention gets worse as we increase the number of heads from 24 to 48 and decrease the key and value dimensionalty from 32 to 16, while talking-heads attention gets better. We presume that this is due to the keys being too short to produce a good matching signal.\n\nFor additional comparison, we include in table 1 two models with significantly more parameters and computation in the attention layers. In the first, we double the number of heads in our baseline model from 12 to 24 without reducing d k and d v , resulting in a multi-head attention layer with double the parameters and double the computation. In the second, we use \"general bilinear multihead attention\", as described in section 6.\n\nWe also list the results from [Raffel et al., 2019]. We believe that their results are worse due to their use of dropout during pre-training. In table 2, we experiment with independently varying the sizes of the three heads-dimensions. From the results, it appears that all three are good to increase, but that the softmax-heads dimension h is particularly important.\n\n\nVarying the Heads-Dimensions Separately\n\n\nLogits-Projection Only and Weights-Projection Only\n\nIn the middle two experiments of table 3, we examine hybrids of multi-head attention and talking-heads attention, where there is a projection on one but not both of the logits and the weights.\n\n\nEncoder vs. Decoder\n\nThe transformer model contains three types of attention layers -self-attention in the encoder, self-attention in the decoder, and attention-over-encoder in the decoder. In each of the middle three experiments of table 4,  we employ talking-heads attention in only one of these types of attention layers, and multi-head attention in the others. We find that modifying the encoder-self-attention layers has the biggest effect on the downstream language-understanding tasks. This is unsurprising, given that these tasks have more to do with analyzing the input than with generating output. [Lan et al., 2019] introduce ALBERT, a variation on BERT [Devlin et al., 2018]. The main difference between the ALBERT and BERT architectures is that ALBERT shares layer parameters among all layers, significantly reducing the number of parameters. For example, a 12-layer ALBERT model has about 1/12 the number of parameters in the attention and feed-forward layers as a similar BERT model. Another difference is that the ALBERT model factorizes the word embedding as the product of two matrices with smaller bases, again significantly reducing the parameter count. This makes ALBERT appealing for memory limited devices such as mobiles. Besides above architecture differences, ALBERT also uses sentence order prediction (SOP) to replace next sentence prediction (NSP) in BERT. We report here experiments done with the base setting for ALBERT: a Transformer network with 12 layers of attention, the hidden and embedding size set to 768. The pre-training and fine-tuning hyperparameter settings are also exactly the same as in [Lan et al., 2019]. We use the English Wikipedia and book corpus datasets [Devlin et al., 2018] to pre-train various models with different head sizes and talking-heads configurations. We evaluate the resulting representations by using them as a starting point to finetune for the SQuAD task (SQuAD1.1, SQuAD2.0 dev set) and various tasks (MNLI, SST-2, RACE) from the GLUE benchmark. Results are in Table 5. We find that as the number of heads increases beyond 12 and the dimensionality of the attention-keys and attention-values decreases below 64, the performance of multi-head attention decays. On the other hand, the performance of talking-head attention keeps improving.\n\n\nALBERT\n\nIn addition, we also compare the logits projection and the weight projection separately with multi-head and talking-heads attention. The results are shown in Table 6. Similar to our observation in T5 experiments, only applying either the logits projection or the weight projection does not result in significant improvement compared to without them. These results again confirm the importance of having both projections. \n\n\nBERT\n\nWe test various configurations of talking-heads attention based on [Devlin et al., 2018]. All of our experiments use the simplified relative position embeddings [Raffel et al., 2019] instead of fixed position embedding. We first pre-train a 12 Transformer layers using the same dataset as [Devlin et al., 2018]. And then we finetune for the SQuAD1.1 task and the MNLI from the GLUE dataset. Our experiments show that quality continues to improve when we grow the number of heads up to 768 and decrease the key and value dimensionality down to 1 4 \n\n\nVisualizing the Projection Matrices of Talking-Heads\n\nTo illustrate how different heads exchange information with each other, we visualize the projection matrices (P l and P w ) of a 12 layer BERT with 12 talking-heads in figure 1. Since P w is applied after P l (although there is a softmax non-linearity in between), we also visualize the combined transformation P l \u00d7 P w in figure 1. As can be observed, the main diagonals of the projection matrices do not have significant greater values than other entries. This is expected because with talking-heads, a pair of query and key do not corresponds to any specific value-vector. All keys and queries jointly decide how the values in each head interchange data. Additionally, all projection matrices are well conditioned (magnitude of determinant above 10 \u22129 with smallest eigenvalue above 10 \u22123 ), indicating that no significant approximation can be achieved. P l P w P l P w Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Figure 1: Visualization of the learned projection matrices P l , P w , and the multiplication P l \u00d7 P w . All entries in each matrix are normalized to [-1, 1]. The fact that these matrices are not diagonal or sparse at all, shows that there are significant data exchange across different attention heads.\n\n\nConclusions and Future Work\n\nWe have proposed talking-heads attention and shown some promising results. One potential challenge is speed on modern deep-learning accelerators, which are optimized for large-dimension matrix multiplications. We imagine that this will be an area of future work. One approach is to build hardware which is better at smalldimension matrix-multiplication. Another potential approach is to decrease the number of memory-positions considered for each query-position -for example, by using the local-attention and memory-compressedattention approaches described in [Liu et al., 2018]. We look forward to more applications of talking-heads attention, as well as to further architectural improvements. \n\n\nA Variation: Dynamic Projections\n\nIn the basic talking-heads attention algorithm described in section 4, the talking-heads projections are represented by two learned weight matrices P l [h k , h] and P w [h, h v ]. In an additional wrinkle, we can make these projections matrices themselves input-dependent, adding terms to the projection matrices that are themselves learned linear projections of the inputs X and M . The algorithm is described by the pseudo-code below.\n\ndef T a l k i n g H e a d s A t t e n t i o n W i t h D y n a m i c P r o j e c t i o n s ( X We observed that the model only trained well if we initialized the projection-generating parameter matrices (P xl , P ml , P xw , P xw ) to contain small enough values. We used normal initializers with standard deviations of \n0.1/ \u221a d X \u00b7 h k , 0.1/ \u221a d M \u00b7 h k , 0.1/ \u221a d X \u00b7 h, and 0.1/ \u221a d M \u00b7 h, respectively.\n\nA.1 Experiments\n\nWe evaluate talking-heads attention with dynamic projections on T5 [Raffel et al., 2019] in a set of experiments similar to those described in section 7.1. Table 8 compares multi-head attention, taking-heads attention with static projections, and talking-heads attention with dynamic projections. The dynamic projections reduce perplexity on the pre-training task. However, in our experiments, we did not see an improvement on the downstream tasks.  Table 9 examines the effects of the four dynamic projections employed individually. The middle four rows represent experiments where only one of the four dynamic projections were employed. These are compared to static projections (top row) and all four dynamic projections together (bottom row). \n\n\nA.1.1 Comparing the Four Dynamic Projections\n\n\nA.1.2 Talking-Heads in Encoder Only\n\nIn section 7.1.4 we saw that talking heads were particularly useful in the encoder part of the model. Table 10 presents a set of experiments where the decoder uses only multi-head attention, while the encoder uses either multi-head attention (top row), talking-heads attention with static projections (middle row), or talking-heads attention with dynamic projections (bottom row). We observe that in this case, the dynamic projections do not appear to degrade performance on the downstream tasks.\n\n\nB T5 Fine-Tuning Full Results\n\nTables 11, 12 and 13 present the results of fine-tuning the models in section 7.1 and appendix A on the GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019], and Stanford Question-Answering Dataset (SQuAD) [Rajpurkar et al., 2016] benchmarks.   Table 13: T5 on SQuAD [Rajpurkar et al., 2016] v1.1 (dev). Experiments described in Section 7.1 and\n\n\ndef T a l k i n g H e a d s A t t e n t i o n( X [n , d_X ] , # n vectors with dimensi onality d_X M [m , d_M ] , # m vectors with dimensi onality d_M P_q [ d_X , d_k , h_k ] , # learned linear projection to produce queries P_k [ d_M , d_k , h_k ] , # learned linear projection to produce keys P_v [ d_M , d_v , h_v ] , # learned linear projection to produce values P_o [ d_Y , d_v , h_v ] , # learned linear projection of outputAgain, we can write this more concisely using multi-way einsum operations: def T a l k i n g H e a d s A t t e n t i o n C o n c i s e (X , M , P_q , P_k , P_v , P_o , P_l , P_w ):P_l [ h_k , h ] , \n# talking -heads projection for logits \nP_w [h , h_v ]): \n# talking -heads projection for weights \nQ [n , d_k , h_k ] = einsum ( X [n , d_X ] , P_q [ d_X , d_k , h_k ]) # queries \nn * d_X * d_k * h_k \nK [m , d_k , h_k ] = einsum ( M [m , d_M ] , P_k [ d_M , d_k , h_k ]) # keys \nm * d_M * d_k * h_k \nV [m , d_v , h_v ] = einsum ( M [m , d_M ] , P_v [ d_M , d_v , h_v ]) # values \nm * d_M * d_v * h_v \nJ [n , m , h_k ] = einsum ( Q [n , d_k , h_k ] , K [m , d_k , h_k ]) \n# dot prod . \nn * m * d_k * h_k \nL [n , m , h ] = einsum ( J [n , m , h_k ] , P_l [ h_k , h ]) \n# Talking -heads proj . n * m * h * h_k \nW [n , m , h ] = softmax ( L [n , m , h ] , reduced_dim = m ) \n# Attention weights \nU [n , m , h_v ] = einsum ( W [n , m , h ] , P_w [h , h_v ]) \n# Talking -heads proj . n * m * h * h_v \nO [n , d_v , h_v ] = einsum ( U [n , m , h_v ] , V [m , d_v , h_v ]) \n# \nn * m * d_v * h_v \nY [n , d_Y ] = einsum ( O [n , d_v , h_v ] , P_o [ d_Y , d_v , h_v ]) # \nn * d_Y * d_v * h_v \nreturn Y [n , d_Y ] \n\nL [n , m , h ] = einsum ( X [n , d_X ] , \nM [m , d_M ] , \nP_q [ d_X , d_k , h_k ] , \nP_k [ d_M , d_k , h_k ] , \nP_l [ h_k , h ]) \nW [n , m , h ] = softmax ( L [n , m , h ] , reduced_dim = m ) \nY [n , d_Y ] = einsum ( W [n , m , h ] , \nM [m , d_M ] , \nP_v [ d_M , d_v , h_v ] , \nP_o [ d_Y , d_v , h_v ] , \nP_w [h , h_v ]) \nreturn Y [n , d_Y ] \n\n\n\nTable 1 :\n1Multi-Head vs. Talking-Heads attention on T5ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k h h v \nd k \nd v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \nmulti-head \n6 \n128 128 2.010 (0.005) \n1.695 \n89.88 \n85.34 \n0.14 \n2359296 \n1.611 \u00b7 10 9 \nmulti-head \n12 \n64 \n64 \n1.982 (0.003) \n1.678 \n90.87 \n86.20 \n0.15 \n2359296 \n1.611 \u00b7 10 9 \nmulti-head \n24 \n32 \n32 \n1.989 (0.009) \n1.669 \n91.04 \n86.41 \n0.17 \n2359296 \n1.611 \u00b7 10 9 \nmulti-head \n48 \n16 \n16 \n2.011 (0.004) \n1.682 \n90.35 \n85.32 \n0.21 \n2359296 \n1.611 \u00b7 10 9 \ntalking-heads \n6 \n6 \n6 128 128 1.965 (0.009) \n1.659 \n90.51 \n85.99 \n0.16 \n2359368 \n1.629 \u00b7 10 9 \ntalking-heads \n12 12 12 64 \n64 \n1.932 (0.004) \n1.641 \n91.38 \n86.19 \n0.18 \n2359584 \n1.686 \u00b7 10 9 \ntalking-heads \n24 24 24 32 \n32 \n1.910 (0.001) \n1.624 \n91.83 \n87.42 \n0.22 \n2360448 \n1.913 \u00b7 10 9 \ntalking-heads \n48 48 48 16 \n16 1.903 (0.006) \n1.603 \n91.90 \n87.50 \n0.32 \n2363904 \n2.819 \u00b7 10 9 \nmulti-head \n24 \n64 \n64 \n1.950 (0.005) \n1.625 \n91.46 \n86.58 \n0.22 \n4718592 \n3.221 \u00b7 10 9 \ngeneral bilinear \n12 \n768 768 1.921 (0.011) \n1.586 \n90.83 \n86.50 \n0.47 \n14155776 \n7.650 \u00b7 10 9 \n[Raffel et al., 2019] \n12 \n64 \n64 \n89.66 \n84.85 \n2359296 \n1.611 \u00b7 10 9 \n\n\n\nTable 2 :\n2Talking-heads attention has three \"heads\" dimensions that can vary independently.ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k h h v \nd k \nd v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \ntalking-heads 6 \n6 \n6 128 128 1.965 (0.009) \n1.659 \n90.51 \n85.99 \n0.16 \n2359368 \n1.629 \u00b7 10 9 \ntalking-heads 6 24 6 128 128 1.941 (0.009) \n1.641 \n90.91 \n86.29 \n0.18 \n2359584 \n1.686 \u00b7 10 9 \ntalking-heads 24 6 24 32 \n32 \n1.959 (0.008) \n1.667 \n90.77 \n86.15 \n0.20 \n2359584 \n1.686 \u00b7 10 9 \ntalking-heads 6 24 24 128 32 \n1.939 (0.011) \n1.633 \n91.06 \n86.31 \n0.20 \n2360016 \n1.799 \u00b7 10 9 \ntalking-heads 24 24 6 \n32 128 1.931 (0.013) \n1.628 \n90.98 \n86.81 \n0.21 \n2360016 \n1.799 \u00b7 10 9 \ntalking-heads 24 24 24 32 \n32 1.910 (0.001) \n1.624 \n91.83 \n87.42 \n0.22 \n2360448 \n1.913 \u00b7 10 9 \n\n\n\nTable 3 :\n3The logits-projection and the weights-projection can be employed separately.ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k h h v d k d v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \nmulti-head \n24 \n32 32 1.989 (0.009) \n1.669 \n91.04 \n86.41 \n0.17 \n2359296 \n1.611 \u00b7 10 9 \nproject logits \n24 24 \n32 32 1.969 (0.004) \n1.652 \n91.29 \n85.86 \n0.23 \n2359872 \n1.762 \u00b7 10 9 \nproject weights \n24 24 32 32 1.951 (0.009) \n1.636 \n91.03 \n86.12 \n0.23 \n2359872 \n1.762 \u00b7 10 9 \ntalking-heads \n24 24 24 32 32 1.910 (0.001) \n1.624 \n91.83 \n87.42 \n0.22 \n2360448 \n1.913 \u00b7 10 9 \n\n\n\nTable 4 :\n4In each of the middle three experiments, talking-heads attention is employed in only one of the three types of attention layers in the model.ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k \nh \nh v d k d v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \nmulti-head \n24 \n32 32 1.989 (0.009) \n1.669 \n91.04 \n86.41 \n0.17 \n2359296 \n1.611 \u00b7 10 9 \nTH-enc-self \n24* 24 24* 32 32 1.969 (0.002) \n1.655 \n91.63 \n87.00 \n0.21 \nvarious \nvarious \nTH-dec-self \n24* 24 24* 32 32 1.981 (0.005) \n1.671 \n90.56 \n85.56 \n0.17 \nvarious \nvarious \nTH-encdec \n24* 24 24* 32 32 1.942 (0.003) \n1.646 \n90.86 \n86.07 \n0.18 \nvarious \nvarious \ntalking-heads 24 24 24 32 32 1.910 (0.001) \n1.624 \n91.83 \n87.42 \n0.22 \n2360448 \n1.913 \u00b7 10 9 \n\n\n\nTable 5 :\n5Multi-Head vs. Talking-Heads attention on ALBERT.heads d k = d v SQuAD1.1 (f1) SQuAD2.0 (f1) MNLI SST-2 RACE MLM SOP Average \nMulti-head \n6 \n128 \n88.5 \n78.8 \n79.9 \n88.6 \n62.7 \n54.3 \n85.9 \n79.7 \nMulti-head \n12 \n64 \n88.8 \n79.3 \n80.2 \n89.9 \n63.4 \n54.5 \n86.2 \n80.32 \nMulti-head \n24 \n32 \n88.8 \n79.1 \n79.9 \n87.7 \n62.1 \n54.4 \n85.9 \n79.52 \nMulti-head \n48 \n16 \n87.9 \n78.8 \n79.6 \n88.4 \n61.8 \n53.8 \n85.3 \n79.3 \nTalking-heads \n6 \n128 \n88.7 \n78 \n80 \n88.5 \n62 \n54.1 \n85.2 \n79.44 \nTalking-heads \n12 \n64 \n89.2 \n79.9 \n80.5 \n89 \n65.3 \n54.9 \n87.6 \n80.78 \nTalking-heads \n24 \n32 \n89.3 \n80.5 \n80.5 \n87.6 \n65.6 \n55.3 \n86.3 \n80.7 \nTalking-heads \n48 \n16 \n89.6 \n80.9 \n80.9 \n89.3 \n66.5 \n55.7 \n86.5 \n81.44 \n\n\nTable 6 :\n6The logits-projection and the weights-projection can be employed separately.heads d k = d v SQuAD1.1 (f1) SQuAD2.0 (f1) MNLI SST-2 RACE MLM SOP Average \nMulti-head \n12 \n64 \n88.8 \n79.3 \n80.2 \n89.9 \n63.4 \n54.5 \n86.2 \n80.32 \nLogit-project-only \n12 \n64 \n88.5 \n78.8 \n79.8 \n89.3 \n63 \n54.6 \n85.8 \n79.88 \nWeight-project-only \n12 \n64 \n88.9 \n79.6 \n80.3 \n89 \n64 \n54.7 \n85.8 \n80.36 \nTalking-heads \n12 \n64 \n89.2 \n79.9 \n80.5 \n89 \n65.3 \n54.9 87.6 \n80.78 \n\n\n\nTable 7 :\n7Talking-Heads attention on BERT.heads d k = d v SQuAD1.1 (f1) MNLI \nMulti-head \n12 \n64 \n88.51 \n82.6 \nTalking-heads \n6 \n128 \n88.8 \n83.4 \nTalking-heads \n12 \n64 \n89.2 \n83.6 \nTalking-heads \n24 \n32 \n89.4 \n83.6 \nTalking-heads \n48 \n16 \n89.5 \n83.4 \nTalking-heads \n64 \n12 \n89.9 \n83.8 \nTalking-heads \n96 \n8 \n89.3 \n83.6 \nTalking-heads \n192 \n4 \n89.8 \n83.9 \nTalking-heads \n384 \n2 \n90.5 \n83.9 \nTalking-heads \n768 \n1 \n90.5 \n84.2 \n\n\n\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,   and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprintarXiv:1905arXiv: .00537, 2019 \n\n\nd_X , d_k , h_k ] , # learned linear projection to produce queries P_k [ d_M , d_k , h_k ] , # learned linear projection to produce keys P_v [ d_M , d_v , h_v ] , # learned linear projection to produce values P_o [ d_Y , d_v , h_v ] , # learned linear projection of output P_l [ h_k , h ] , # learned static talking -heads proj . on logits P_Xl [ d_X , h_k , h ] , # learned projection to generate dynamic talking -heads projection P_Ml [ d_M , h_k , h ] , # learned projection to generate dynamic talking -heads projection P_w [h , h_v ] , # learned static talking -heads proj . on weights P_Xw [ d_X , h , h_v ] , # learned projection to generate dynamic talking -heads projection P_Mw [ d_X , h , h_v ]) # learned projection to generate dynamic talking -heads projection Q [n , d_k , h_k ] = einsum ( X [n , d_X ] , P_q [ d_X , d_k , h_k ]) R_Ml [n , h_k , h ] = einsum ( M [m , d_M ] , P_Ml [ d_M , h_k , h ]) # dynamic proj . n * d_M * h_k * h L [n , m , h ] = ( einsum ( J [n , m , h_k ] , P_l [ h_k , h ]) + # Static talking -heads proj . n * m * h * h_k einsum ( J [n , m , h_k ] , R_Xl [n , h_k , h ]) + # Dynamic talking -heads proj . n * m * h * h_k einsum ( J [n , m , h_k ] , R_Ml [m , h_k , h ])) # Dynamic talking -heads proj . n * m * h * h_k W [n , m , h ] = softmax ( L [n , m , h ] , reduced_dim = m ) # Attention weights R_Xw [n , h , h_v ] = einsum ( X [n , d_X ] , P_Xw [ d_X , h , h_v ]) # dynamic proj . n * d_X * h * h_v R_Mw [n , h , h_v ] = einsum ( M [m , d_M ] , P_Mw [ d_M , h , h_v ]) # dynamic proj .[n , d_X ] , \n# n vectors with dimensi onality d_X \nM [m , d_M ] , \n# m vectors with dimensi onality d_M \nP_q [ # queries \nn * d_X * d_k * h_k \nK [m , d_k , h_k ] = einsum ( M [m , d_M ] , P_k [ d_M , d_k , h_k ]) # keys \nm * d_M * d_k * h_k \nV [m , d_v , h_v ] = einsum ( M [m , d_M ] , P_v [ d_M , d_v , h_v ]) # values \nm * d_M * d_v * h_v \nJ [n , m , h_k ] = einsum ( Q [n , d_k , h_k ] , K [m , d_k , h_k ]) \n# dot prod . \nn * m * d_k * h_k \nR_Xl [n , h_k , h ] = einsum ( X [n , d_X ] , P_Xl [ d_X , h_k , h ]) # dynamic proj . \nn * d_X * h_k * h \nn * d_M * h * h_v \nU [n , m , h_v ] = ( \neinsum ( W [n , m , h ] , P_w [h , h_v ]) + \n# Static Talking -heads proj . n * m * h * h_v \neinsum ( W [n , m , h ] , R_Xw [n , h , h_v ]) + \n# Dynamic talking -heads proj . n * m * h * h_v \neinsum ( W [n , m , h ] , R_Mw [m , h , h_v ])) \n# Dynamic talking -heads proj . n * m * h * h_v \nO [n , d_v , h_v ] = einsum ( U [n , m , h_v ] , V [m , d_v , h_v ]) \n# \nn * m * d_v * h_v \nY [n , d_Y ] = einsum ( O [n , d_v , h_v ] , P_o [ d_Y , d_v , h_v ]) # \nn * d_Y * d_v * h_v \nreturn Y [n , d_Y ] \n\n\n\nTable 8 :\n8Dynamic-Projections Results on T5ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k h h v d k d v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \nmulti-head \n12 \n64 64 1.982 (0.003) \n1.678 \n90.87 \n86.20 \n0.15 \n2359296 \n1.611 \u00b7 10 9 \ntalking-heads 12 12 12 64 64 1.932 (0.004) \n1.641 \n91.38 \n86.19 \n0.18 \n2359584 \n1.686 \u00b7 10 9 \ndyn. proj. \n12 12 12 64 64 1.897 (0.007) \n1.595 \n90.17 \n86.18 \n0.36 \n2801952 \n1.913 \u00b7 10 9 \nmulti-head \n24 \n32 32 1.989 (0.009) \n1.669 \n91.04 \n86.41 \n0.17 \n2359296 \n1.611 \u00b7 10 9 \ntalking-heads 24 24 24 32 32 1.910 (0.001) \n1.624 \n91.83 \n87.42 \n0.22 \n2360448 \n1.913 \u00b7 10 9 \ndynamic proj. 24 24 24 32 32 1.873 (0.008) \n1.587 \n90.17 \n85.94 \n0.53 \n4129920 \n2.819 \u00b7 10 9 \n\n\n\nTable 9 :\n9In each of the middle four experiments, only one of the dynamic projections is employed.ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k h h v d k d v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \ntalking-heads \n12 12 12 64 64 1.932 (0.004) \n1.641 \n91.38 \n86.19 \n0.18 \n2359584 \n1.686 \u00b7 10 9 \ndyn. proj. P Xl \n12 12 12 64 64 1.932 (0.011) \n1.634 \n91.34 \n86.32 \n0.19 \n2470176 \n1.743 \u00b7 10 9 \ndyn. proj. P Xw 12 12 12 64 64 1.914 (0.005) \n1.619 \n90.70 \n86.43 \n0.19 \n2470176 \n1.743 \u00b7 10 9 \ndyn. proj. P M l 12 12 12 64 64 1.930 (0.010) \n1.624 \n91.14 \n86.63 \n0.24 \n2470176 \n1.743 \u00b7 10 9 \ndyn. proj. P M w 12 12 12 64 64 1.917 (0.003) \n1.624 \n90.54 \n86.45 \n0.25 \n2470176 \n1.743 \u00b7 10 9 \ndyn. proj. \n12 12 12 64 64 1.897 (0.007) \n1.595 \n90.17 \n86.18 \n0.36 \n2801952 \n1.913 \u00b7 10 9 \n\n\n\nTable 10 :\n10Effects of applying talking-heads attention (with or without dynamic projections) in the encoder only.ln(PPL) \nln(PPL) SQUAD \nstep parameters \nmultiplies \n65536 \n524288 \nv1.1 \nMNLI-m time \nper \nper att. layer \nh k \nh \nh v d k d v \nsteps \nsteps \ndev-f1 \ndev \n(s) \natt. layer \n(n=m=512) \nmulti-head \n24 \n32 32 1.989 (0.009) \n1.669 \n91.04 \n86.41 \n0.17 \n2359296 \n1.611 \u00b7 10 9 \nTH-enc-self 24* 24 24* 32 32 1.969 (0.002) \n1.655 \n91.63 \n87.00 \n0.21 \nvarious \nvarious \nDP-enc-self 24* 24 24* 32 32 1.953 (0.006) \n1.639 \n91.99 \n86.97 \n0.42 \nvarious \nvarious \n\n\n\nTable 11 :\n11T5 on GLUE Language-Understanding Benchmark [Wang et al., 2018] (dev). Experiments described \nin Section 7.1 and appendix A. \n\nScore \nCoLA SST-2 MRPC MRPC STSB STSB QQP \nQQP MNLIm MNLImm QNLI \nRTE \nhk \nh \nhv \ndk \ndv \nAverage MCC \nAcc \nF1 \nAcc \nPCC \nSCC \nF1 \nAcc \nAcc \nAcc \nAcc \nAcc \nmulti-head \n6 \n128 128 \n83.20 \n50.96 \n93.35 \n91.49 \n88.24 \n89.51 \n89.41 \n88.79 \n91.69 \n85.12 \n85.70 \n92.35 \n80.14 \nmulti-head \n12 \n64 \n64 \n84.36 \n55.21 \n93.58 \n92.55 \n89.71 \n90.01 \n89.81 \n88.93 \n91.69 \n86.04 \n86.46 \n92.88 \n81.59 \nmulti-head \n24 \n32 \n32 \n84.37 \n55.24 \n93.92 \n93.59 \n91.18 \n89.79 \n89.68 \n88.96 \n91.68 \n86.06 \n86.16 \n92.73 \n80.87 \nmulti-head \n48 \n16 \n16 \n84.36 \n55.82 \n93.23 \n93.59 \n91.18 \n88.99 \n88.88 \n88.74 \n91.63 \n85.17 \n85.86 \n92.29 \n81.59 \ntalking-heads \n6 \n6 \n6 \n128 128 \n83.64 \n51.53 \n93.69 \n92.17 \n89.22 \n89.30 \n89.18 \n88.69 \n91.54 \n85.92 \n86.68 \n92.77 \n80.87 \ntalking-heads \n12 12 12 \n64 \n64 \n84.42 \n55.22 \n94.38 \n92.50 \n89.46 \n90.71 \n90.49 \n88.99 \n91.77 \n86.11 \n86.37 \n92.93 \n79.78 \ntalking-heads \n24 24 24 \n32 \n32 \n84.75 \n55.39 \n93.92 \n92.34 \n89.46 \n90.14 \n89.87 89.19 91.91 \n87.42 \n87.12 \n93.37 \n82.31 \ntalking-heads \n48 48 48 \n16 \n16 \n84.82 \n52.08 94.61 \n92.97 \n90.44 \n91.16 91.00 88.95 \n91.78 \n87.40 \n87.44 \n93.32 \n83.03 \nmulti-head \n24 \n64 \n64 \n84.82 \n55.99 94.04 \n92.45 \n89.71 \n90.25 \n90.00 \n89.15 \n91.90 \n86.54 \n86.62 \n93.04 \n81.23 \ngeneral bilinear \n12 \n768 768 \n84.62 \n53.47 \n93.92 \n93.12 \n90.44 \n90.10 \n89.84 \n89.19 91.98 \n86.14 \n86.45 \n93.25 \n82.31 \ntalking-heads \n6 \n6 \n6 \n128 128 \n83.64 \n51.53 \n93.69 \n92.17 \n89.22 \n89.30 \n89.18 \n88.69 \n91.54 \n85.92 \n86.68 \n92.77 \n80.87 \ntalking-heads \n6 \n24 \n6 \n128 128 \n84.08 \n50.87 \n94.27 \n93.10 \n90.44 \n89.76 \n89.49 \n88.75 \n91.63 \n86.29 \n86.32 \n92.81 \n82.67 \ntalking-heads \n24 \n6 \n24 \n32 \n32 \n83.71 \n51.11 \n93.81 \n91.59 \n88.48 \n90.03 \n89.88 \n89.02 \n91.78 \n86.08 \n86.63 \n92.97 \n79.42 \ntalking-heads \n6 \n24 24 128 32 \n84.18 \n53.66 \n93.92 \n93.07 \n90.44 \n89.82 \n89.80 \n89.04 \n91.84 \n86.31 \n86.48 \n92.82 \n81.59 \ntalking-heads \n24 24 \n6 \n32 128 \n84.85 \n51.80 \n94.04 \n93.12 \n90.44 \n89.90 \n89.75 \n89.15 \n91.83 \n86.81 \n86.81 \n93.08 84.84 \ntalking-heads \n24 24 24 \n32 \n32 \n84.75 \n55.39 \n93.92 \n92.34 \n89.46 \n90.14 \n89.87 89.19 91.91 \n87.42 \n87.12 \n93.37 \n82.31 \nmulti-head \n24 \n32 \n32 \n84.37 \n55.24 \n93.92 \n93.59 \n91.18 \n89.79 \n89.68 \n88.96 \n91.68 \n86.06 \n86.16 \n92.73 \n80.87 \nproject logits \n24 24 \n32 \n32 \n84.24 \n54.08 \n93.92 \n91.81 \n88.73 \n90.38 \n90.25 \n89.03 \n91.78 \n85.86 \n86.30 \n92.92 \n81.59 \nproject weights \n24 24 \n32 \n32 \n83.95 \n51.28 \n93.92 \n92.23 \n89.22 \n89.69 \n89.47 \n88.96 \n91.77 \n86.04 \n86.13 \n93.10 \n82.67 \ntalking-heads \n24 24 24 \n32 \n32 \n84.75 \n55.39 \n93.92 \n92.34 \n89.46 \n90.14 \n89.87 89.19 91.91 \n87.42 \n87.12 \n93.37 \n82.31 \nmulti-head \n24 \n32 \n32 \n84.37 \n55.24 \n93.92 \n93.59 \n91.18 \n89.79 \n89.68 \n88.96 \n91.68 \n86.06 \n86.16 \n92.73 \n80.87 \nTH-enc-self \n24* 24 24* 32 \n32 \n84.49 \n52.80 \n94.38 \n93.07 \n90.44 \n89.70 \n89.61 \n88.99 \n91.79 \n87.00 \n86.80 \n93.12 \n83.39 \nTH-dec-self \n24* 24 24* 32 \n32 \n84.22 \n55.22 \n94.15 \n92.61 \n89.71 \n89.92 \n89.85 \n88.83 \n91.71 \n85.56 \n86.06 \n92.79 \n81.23 \nTH-encdec \n24* 24 24* 32 \n32 \n84.32 \n54.58 \n93.92 \n92.17 \n88.97 \n89.76 \n89.67 \n89.00 \n91.78 \n86.07 \n85.82 \n92.99 \n80.51 \ntalking-heads \n24 24 24 \n32 \n32 \n84.75 \n55.39 \n93.92 \n92.34 \n89.46 \n90.14 \n89.87 89.19 91.91 \n87.42 \n87.12 \n93.37 \n82.31 \nmulti-head \n12 \n64 \n64 \n84.36 \n55.21 \n93.58 \n92.55 \n89.71 \n90.01 \n89.81 \n88.93 \n91.69 \n86.04 \n86.46 \n92.88 \n81.59 \ntalking-heads \n12 12 12 \n64 \n64 \n84.42 \n55.22 \n94.38 \n92.50 \n89.46 \n90.71 \n90.49 \n88.99 \n91.77 \n86.11 \n86.37 \n92.93 \n79.78 \ndyn. proj. \n12 12 12 \n64 \n64 \n83.81 \n49.75 \n93.81 \n92.20 \n89.22 \n90.27 \n90.15 \n89.13 \n91.83 \n86.11 \n86.17 \n92.07 \n80.51 \nmulti-head \n24 \n32 \n32 \n84.37 \n55.24 \n93.92 \n93.59 \n91.18 \n89.79 \n89.68 \n88.96 \n91.68 \n86.06 \n86.16 \n92.73 \n80.87 \ntalking-heads \n24 24 24 \n32 \n32 \n84.75 \n55.39 \n93.92 \n92.34 \n89.46 \n90.14 \n89.87 89.19 91.91 \n87.42 \n87.12 \n93.37 \n82.31 \ndyn. proj. \n24 24 24 \n32 \n32 \n83.25 \n49.42 \n93.23 \n92.55 \n89.71 \n89.64 \n89.39 \n88.80 \n91.57 \n85.94 \n85.91 \n92.35 \n79.06 \ntalking-heads \n12 12 12 \n64 \n64 \n84.42 \n55.22 \n94.38 \n92.50 \n89.46 \n90.71 \n90.49 \n88.99 \n91.77 \n86.11 \n86.37 \n92.93 \n79.78 \ndyn. proj. PXl \n12 12 12 \n64 \n64 \n84.42 \n53.14 \n94.15 \n91.45 \n87.99 \n90.01 \n89.98 \n88.79 \n91.64 \n86.13 \n86.72 \n93.34 \n81.23 \ndyn. proj. PXw \n12 12 12 \n64 \n64 \n84.35 \n55.98 \n94.04 \n93.10 \n90.44 \n89.79 \n89.74 \n88.88 \n91.68 \n86.43 \n86.34 \n92.40 \n80.87 \ndyn. proj. PMl \n12 12 12 \n64 \n64 \n84.09 \n51.86 \n94.27 \n92.17 \n89.22 \n89.91 \n89.80 \n89.15 \n91.88 \n86.35 \n87.18 \n92.77 \n80.51 \ndyn. proj. PMw \n12 12 12 \n64 \n64 \n84.20 \n54.17 \n93.58 \n92.47 \n89.71 \n89.57 \n89.62 \n88.93 \n91.65 \n86.45 \n86.27 \n92.90 \n81.95 \ndyn. proj. \n12 12 12 \n64 \n64 \n83.81 \n49.75 \n93.81 \n92.20 \n89.22 \n90.27 \n90.15 \n89.13 \n91.83 \n86.11 \n86.17 \n92.07 \n80.51 \nmulti-head \n24 \n32 \n32 \n84.37 \n55.24 \n93.92 \n93.59 \n91.18 \n89.79 \n89.68 \n88.96 \n91.68 \n86.06 \n86.16 \n92.73 \n80.87 \nTH-enc-self \n24* 24 24* 32 \n32 \n84.49 \n52.80 \n94.38 \n93.07 \n90.44 \n89.70 \n89.61 \n88.99 \n91.79 \n87.00 \n86.80 \n93.12 \n83.39 \nDP-enc-self \n24* 24 24* 32 \n32 \n84.08 \n51.54 \n94.27 \n91.88 \n88.48 \n90.24 \n90.11 \n89.18 \n91.90 \n86.81 \n87.52 \n93.48 82.67 \n[Raffel et al., 2019] \n12 \n64 \n64 \n83.49 \n53.90 \n92.43 \n92.25 \n89.46 \n87.49 \n87.53 \n88.72 \n91.51 \n84.85 \n84.84 \n90.99 \n77.26 \nibid. stddev. \n0.235 \n1.111 \n0.569 \n0.729 \n1.019 \n0.374 \n0.418 \n0.108 \n0.070 \n0.291 \n0.231 \n0.361 \n1.393 \n\n\nTable 12 :\n12T5 on SuperGLUE Language-Understanding Benchmark[Wang et al., 2019]  (dev). Experiments described in Section 7.1 and appendix A.Score \nBoolQ \nCB \nCB \nCoPA MultiRC MultiRC ReCoRD ReCoRD RTE \nWiC \nWSC \nhk \nh \nhv \ndk \ndv \nAverage \nAcc \nF1 \nAcc \nAcc \nF1 \nEM \nF1 \nEM \nAcc \nAcc \nAcc \nmulti-head \n6 \n128 128 \n72.00 \n78.84 \n78.83 \n87.50 \n70.00 \n75.01 \n36.94 \n72.08 \n71.32 \n83.03 \n69.12 \n79.81 \nmulti-head \n12 \n64 \n64 \n73.59 \n80.31 \n89.15 \n89.29 \n73.00 \n75.13 \n37.57 \n73.92 \n72.94 \n83.75 \n69.28 \n77.88 \nmulti-head \n24 \n32 \n32 \n73.98 \n81.35 \n84.24 \n91.07 \n70.00 \n75.98 \n39.24 \n74.40 \n73.37 \n83.03 \n71.00 \n78.85 \nmulti-head \n48 \n16 \n16 \n71.85 \n80.34 \n77.90 \n87.50 \n67.00 \n75.26 \n37.67 \n72.31 \n71.32 \n84.12 \n69.44 \n77.88 \ntalking-heads \n6 \n6 \n6 \n128 128 \n72.57 \n80.83 \n83.37 \n89.29 \n65.00 \n76.76 \n40.08 \n75.37 \n74.48 \n83.39 \n66.30 \n80.77 \ntalking-heads \n12 12 12 \n64 \n64 \n73.16 \n81.38 \n81.13 \n89.29 \n69.00 \n77.36 \n40.50 \n76.45 \n75.63 \n83.39 \n65.83 \n82.69 \ntalking-heads \n24 24 24 \n32 \n32 \n75.80 \n81.96 90.60 94.64 75.00 \n76.56 \n39.77 \n77.22 \n76.37 \n85.20 \n70.06 \n81.73 \ntalking-heads \n48 48 48 \n16 \n16 \n76.39 \n82.94 87.46 \n91.07 \n74.00 \n78.11 \n42.71 \n77.51 \n76.68 \n84.84 \n69.59 87.50 \nmulti-head \n24 \n64 \n64 \n74.08 \n81.80 \n77.90 \n87.50 \n73.00 \n77.20 \n39.35 \n76.89 \n76.11 \n83.39 \n69.44 \n80.77 \ngeneral bilinear \n12 \n768 768 \n73.35 \n81.47 \n83.46 \n89.29 \n71.00 \n76.69 \n39.24 \n76.80 \n76.02 \n85.92 \n69.59 \n76.92 \ntalking-heads \n6 \n6 \n6 \n128 128 \n72.57 \n80.83 \n83.37 \n89.29 \n65.00 \n76.76 \n40.08 \n75.37 \n74.48 \n83.39 \n66.30 \n80.77 \ntalking-heads \n6 \n24 \n6 \n128 128 \n73.71 \n81.07 \n80.92 \n87.50 \n68.00 \n76.47 \n40.82 \n74.77 \n73.89 \n85.92 \n70.06 \n79.81 \ntalking-heads \n24 \n6 \n24 \n32 \n32 \n73.56 \n80.92 \n83.52 \n89.29 \n75.00 \n75.64 \n37.15 \n74.58 \n73.73 \n81.95 71.32 76.92 \ntalking-heads \n6 \n24 24 128 32 \n74.29 \n80.95 \n87.62 \n91.07 \n74.00 \n76.23 \n37.04 \n76.67 \n75.83 \n83.39 \n68.65 \n82.69 \ntalking-heads \n24 24 \n6 \n32 128 \n76.37 \n81.77 \n88.97 \n92.86 76.00 \n77.63 \n42.81 \n76.72 \n75.88 \n86.28 67.71 87.50 \ntalking-heads \n24 24 24 \n32 \n32 \n75.80 \n81.96 90.60 94.64 75.00 \n76.56 \n39.77 \n77.22 \n76.37 \n85.20 \n70.06 \n81.73 \nmulti-head \n24 \n32 \n32 \n73.98 \n81.35 \n84.24 \n91.07 \n70.00 \n75.98 \n39.24 \n74.40 \n73.37 \n83.03 \n71.00 \n78.85 \nproject logits \n24 24 \n32 \n32 \n72.63 \n81.47 \n83.15 \n89.29 \n71.00 \n76.98 \n39.35 \n75.00 \n74.21 \n85.20 \n69.75 \n79.81 \nproject weights \n24 24 \n32 \n32 \n74.05 \n81.99 \n81.96 \n87.50 \n73.00 \n77.35 \n41.03 \n76.62 \n75.74 \n85.20 \n68.03 \n78.85 \ntalking-heads \n24 24 24 \n32 \n32 \n75.80 \n81.96 90.60 94.64 75.00 \n76.56 \n39.77 \n77.22 \n76.37 \n85.20 \n70.06 \n81.73 \nmulti-head \n24 \n32 \n32 \n73.98 \n81.35 \n84.24 \n91.07 \n70.00 \n75.98 \n39.24 \n74.40 \n73.37 \n83.03 \n71.00 \n78.85 \nTH-enc-self \n24* 24 24* 32 \n32 \n73.90 \n82.72 \n84.20 \n89.29 \n69.00 \n78.18 \n43.55 \n76.07 \n75.34 \n85.92 \n69.75 \n79.81 \nTH-dec-self \n24* 24 24* 32 \n32 \n72.54 \n80.92 \n79.93 \n87.50 \n67.00 \n76.49 \n38.09 \n73.72 \n72.94 \n83.03 \n69.44 \n79.81 \nTH-encdec \n24* 24 24* 32 \n32 \n73.44 \n80.83 \n78.67 \n87.50 \n74.00 \n76.41 \n39.24 \n75.35 \n74.47 \n83.39 \n70.53 \n80.77 \ntalking-heads \n24 24 24 \n32 \n32 \n75.80 \n81.96 90.60 94.64 75.00 \n76.56 \n39.77 \n77.22 \n76.37 \n85.20 \n70.06 \n81.73 \nmulti-head \n12 \n64 \n64 \n73.59 \n80.31 \n89.15 \n89.29 \n73.00 \n75.13 \n37.57 \n73.92 \n72.94 \n83.75 \n69.28 \n77.88 \ntalking-heads \n12 12 12 \n64 \n64 \n73.16 \n81.38 \n81.13 \n89.29 \n69.00 \n77.36 \n40.50 \n76.45 \n75.63 \n83.39 \n65.83 \n82.69 \ndyn. proj. \n12 12 12 \n64 \n64 \n71.98 \n80.52 \n86.31 \n91.07 \n66.00 \n76.47 \n37.67 \n73.24 \n72.42 \n82.31 \n67.40 \n75.00 \nmulti-head \n24 \n32 \n32 \n73.98 \n81.35 \n84.24 \n91.07 \n70.00 \n75.98 \n39.24 \n74.40 \n73.37 \n83.03 \n71.00 \n78.85 \ntalking-heads \n24 24 24 \n32 \n32 \n75.80 \n81.96 90.60 94.64 75.00 \n76.56 \n39.77 \n77.22 \n76.37 \n85.20 \n70.06 \n81.73 \ndyn. proj. \n24 24 24 \n32 \n32 \n72.70 \n79.45 \n78.80 \n87.50 \n72.00 \n74.68 \n35.15 \n72.39 \n71.70 \n80.14 \n68.34 \n82.69 \ntalking-heads \n12 12 12 \n64 \n64 \n73.16 \n81.38 \n81.13 \n89.29 \n69.00 \n77.36 \n40.50 \n76.45 \n75.63 \n83.39 \n65.83 \n82.69 \ndyn. proj. PXl \n12 12 12 \n64 \n64 \n74.55 \n81.19 \n86.19 \n91.07 \n74.00 \n76.20 \n41.45 \n75.45 \n74.63 \n84.84 \n68.81 \n82.69 \ndyn. proj. PXw \n12 12 12 \n64 \n64 \n73.85 \n80.73 \n82.39 \n89.29 \n74.00 \n75.50 \n38.51 \n73.72 \n72.93 \n83.03 \n68.50 \n81.73 \ndyn. proj. PMl \n12 12 12 \n64 \n64 \n74.51 \n81.62 \n86.06 \n92.86 \n72.00 \n75.48 \n38.82 \n75.54 \n74.70 \n83.03 \n68.65 \n82.69 \ndyn. proj. PMw \n12 12 12 \n64 \n64 \n73.61 \n80.61 \n79.72 \n89.29 \n70.00 \n75.36 \n38.20 \n75.19 \n74.41 \n83.39 \n67.55 \n83.65 \ndyn. proj. \n12 12 12 \n64 \n64 \n71.98 \n80.52 \n86.31 \n91.07 \n66.00 \n76.47 \n37.67 \n73.24 \n72.42 \n82.31 \n67.40 \n75.00 \nmulti-head \n24 \n32 \n32 \n73.98 \n81.35 \n84.24 \n91.07 \n70.00 \n75.98 \n39.24 \n74.40 \n73.37 \n83.03 \n71.00 \n78.85 \nTH-enc-self \n24* 24 24* 32 \n32 \n73.90 \n82.72 \n84.20 \n89.29 \n69.00 \n78.18 \n43.55 \n76.07 \n75.34 \n85.92 \n69.75 \n79.81 \nDP-enc-self \n24* 24 24* 32 \n32 \n74.65 \n82.60 \n84.76 \n91.07 \n69.00 \n77.32 \n42.29 \n77.88 \n76.99 \n84.48 \n71.16 \n79.81 \n[Raffel et al., 2019] \n12 \n64 \n64 \n72.53 \n76.85 \n94.37 \n94.64 \n70.00 \n67.64 \n28.75 \n70.84 \n69.90 \n74.73 \n67.71 \n77.88 \nibid. stddev. \n0.416 \n0.365 \n3.237 \n2.560 \n2.741 \n0.716 \n1.011 \n0.370 \n0.379 \n1.228 \n0.850 \n2.029 \n\nAppendix A presents a variation on this, where the projection matrices themselves are input-dependent.\nThis is identical to one of the training objecives described in[Raffel et al., 2019] \nThese extreme hyperparameter settings likely have no practical use, due to the massive amount of computation.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2014. 1\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 7arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 5, 7, 8\n\nAlbert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, 57Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations, 2019. 5, 7\n\nGenerating wikipedia by summarizing long sequences. J Peter, Mohammad Liu, Etienne Saleh, Ben Pot, Ryan Goodrich, Lukasz Sepassi, Noam Kaiser, Shazeer, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In Proceedings of the International Conference on Learning Representations, 2018. 9\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, 1415arXiv e-printsColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. 5, 6, 8, 11, 13, 14, 15\n\nSquad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.052501215arXiv preprintPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. 5, 12, 15\n\nNoam Shazeer, Mitchell Stern, arXiv:1804.04235Adafactor: Adaptive learning rates with sublinear memory cost. arXiv preprintNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. arXiv preprint arXiv:1804.04235, 2018. 5\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 1, 2, 3, 5\n\nGLUE: A multitask benchmark and analysis platform for natural language understanding. Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.074611213arXiv preprintAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi- task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 5, 12, 13\n", "annotations": {"author": "[{\"end\":53,\"start\":40},{\"end\":61,\"start\":54},{\"end\":76,\"start\":62},{\"end\":84,\"start\":77},{\"end\":99,\"start\":85},{\"end\":107,\"start\":100},{\"end\":136,\"start\":108},{\"end\":144,\"start\":137},{\"end\":169,\"start\":145},{\"end\":177,\"start\":170}]", "publisher": null, "author_last_name": "[{\"end\":52,\"start\":45},{\"end\":60,\"start\":54},{\"end\":75,\"start\":72},{\"end\":83,\"start\":77},{\"end\":98,\"start\":93},{\"end\":106,\"start\":100},{\"end\":116,\"start\":112},{\"end\":143,\"start\":137},{\"end\":151,\"start\":148},{\"end\":176,\"start\":170}]", "author_first_name": "[{\"end\":44,\"start\":40},{\"end\":71,\"start\":62},{\"end\":92,\"start\":85},{\"end\":111,\"start\":108},{\"end\":147,\"start\":145}]", "author_affiliation": null, "title": "[{\"end\":24,\"start\":1},{\"end\":201,\"start\":178}]", "venue": null, "abstract": "[{\"end\":4374,\"start\":216}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4448,\"start\":4425},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4565,\"start\":4543},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5031,\"start\":5009},{\"end\":6272,\"start\":6262},{\"end\":6333,\"start\":6316},{\"end\":6404,\"start\":6387},{\"end\":6472,\"start\":6455},{\"end\":6542,\"start\":6525},{\"end\":6924,\"start\":6892},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6953,\"start\":6931},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9906,\"start\":9886},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9968,\"start\":9946},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10110,\"start\":10089},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10277,\"start\":10256},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10586,\"start\":10565},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10615,\"start\":10594},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10855,\"start\":10834},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11131,\"start\":11110},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11189,\"start\":11164},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11375,\"start\":11354},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11422,\"start\":11404},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11939,\"start\":11915},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11965,\"start\":11946},{\"end\":11999,\"start\":11980},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12112,\"start\":12091},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12660,\"start\":12638},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13686,\"start\":13665},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14920,\"start\":14902},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14980,\"start\":14959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15946,\"start\":15928},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16023,\"start\":16002},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17131,\"start\":17110},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17225,\"start\":17204},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17353,\"start\":17332},{\"end\":18778,\"start\":18771},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19534,\"start\":19516},{\"end\":19867,\"start\":19858},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20641,\"start\":20620},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22044,\"start\":22025},{\"end\":22078,\"start\":22059},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22152,\"start\":22128},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22213,\"start\":22189},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45042,\"start\":45021}]", "figure": "[{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24244,\"start\":22267},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":25484,\"start\":24245},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":26332,\"start\":25485},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":26974,\"start\":26333},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":27762,\"start\":26975},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":28454,\"start\":27763},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":28908,\"start\":28455},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":29337,\"start\":28909},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":29594,\"start\":29338},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":32222,\"start\":29595},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":32999,\"start\":32223},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":33857,\"start\":33000},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":34424,\"start\":33858},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":39791,\"start\":34425},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":44854,\"start\":39792}]", "paragraph": "[{\"end\":4995,\"start\":4390},{\"end\":5341,\"start\":4997},{\"end\":5703,\"start\":5343},{\"end\":5945,\"start\":5705},{\"end\":7089,\"start\":5958},{\"end\":8017,\"start\":7117},{\"end\":8140,\"start\":8041},{\"end\":8255,\"start\":8188},{\"end\":8680,\"start\":8338},{\"end\":8820,\"start\":8682},{\"end\":9547,\"start\":8851},{\"end\":9710,\"start\":9549},{\"end\":11831,\"start\":9767},{\"end\":12436,\"start\":11833},{\"end\":13199,\"start\":12478},{\"end\":13633,\"start\":13201},{\"end\":14002,\"start\":13635},{\"end\":14291,\"start\":14099},{\"end\":16602,\"start\":14315},{\"end\":17034,\"start\":16613},{\"end\":17590,\"start\":17043},{\"end\":18924,\"start\":17647},{\"end\":19651,\"start\":18956},{\"end\":20125,\"start\":19688},{\"end\":20446,\"start\":20127},{\"end\":21299,\"start\":20553},{\"end\":21882,\"start\":21386},{\"end\":22266,\"start\":21916}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8187,\"start\":8141},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8337,\"start\":8256},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20534,\"start\":20447}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":16333,\"start\":16326},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":16778,\"start\":16771},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":20716,\"start\":20709},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":21010,\"start\":21003},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21496,\"start\":21488},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":22175,\"start\":22167}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4388,\"start\":4376},{\"attributes\":{\"n\":\"2\"},\"end\":5956,\"start\":5948},{\"attributes\":{\"n\":\"4\"},\"end\":7115,\"start\":7092},{\"attributes\":{\"n\":\"5\"},\"end\":8039,\"start\":8020},{\"attributes\":{\"n\":\"6\"},\"end\":8849,\"start\":8823},{\"attributes\":{\"n\":\"7\"},\"end\":9724,\"start\":9713},{\"attributes\":{\"n\":\"7.1\"},\"end\":9765,\"start\":9727},{\"attributes\":{\"n\":\"7.1.1\"},\"end\":12476,\"start\":12439},{\"attributes\":{\"n\":\"7.1.2\"},\"end\":14044,\"start\":14005},{\"attributes\":{\"n\":\"7.1.3\"},\"end\":14097,\"start\":14047},{\"attributes\":{\"n\":\"7.1.4\"},\"end\":14313,\"start\":14294},{\"attributes\":{\"n\":\"7.2\"},\"end\":16611,\"start\":16605},{\"attributes\":{\"n\":\"7.3\"},\"end\":17041,\"start\":17037},{\"attributes\":{\"n\":\"7.4\"},\"end\":17645,\"start\":17593},{\"attributes\":{\"n\":\"8\"},\"end\":18954,\"start\":18927},{\"end\":19686,\"start\":19654},{\"end\":20551,\"start\":20536},{\"end\":21346,\"start\":21302},{\"end\":21384,\"start\":21349},{\"end\":21914,\"start\":21885},{\"end\":24255,\"start\":24246},{\"end\":25495,\"start\":25486},{\"end\":26343,\"start\":26334},{\"end\":26985,\"start\":26976},{\"end\":27773,\"start\":27764},{\"end\":28465,\"start\":28456},{\"end\":28919,\"start\":28910},{\"end\":32233,\"start\":32224},{\"end\":33010,\"start\":33001},{\"end\":33869,\"start\":33859},{\"end\":34436,\"start\":34426},{\"end\":39803,\"start\":39793}]", "table": "[{\"end\":24244,\"start\":22878},{\"end\":25484,\"start\":24301},{\"end\":26332,\"start\":25578},{\"end\":26974,\"start\":26421},{\"end\":27762,\"start\":27128},{\"end\":28454,\"start\":27824},{\"end\":28908,\"start\":28543},{\"end\":29337,\"start\":28953},{\"end\":32222,\"start\":31129},{\"end\":32999,\"start\":32268},{\"end\":33857,\"start\":33100},{\"end\":34424,\"start\":33974},{\"end\":39791,\"start\":34439},{\"end\":44854,\"start\":39934}]", "figure_caption": "[{\"end\":22878,\"start\":22269},{\"end\":24301,\"start\":24257},{\"end\":25578,\"start\":25497},{\"end\":26421,\"start\":26345},{\"end\":27128,\"start\":26987},{\"end\":27824,\"start\":27775},{\"end\":28543,\"start\":28467},{\"end\":28953,\"start\":28921},{\"end\":29594,\"start\":29340},{\"end\":31129,\"start\":29597},{\"end\":32268,\"start\":32235},{\"end\":33100,\"start\":33012},{\"end\":33974,\"start\":33872},{\"end\":39934,\"start\":39806}]", "figure_ref": "[{\"end\":18628,\"start\":18620}]", "bib_author_first_name": "[{\"end\":45233,\"start\":45226},{\"end\":45253,\"start\":45244},{\"end\":45265,\"start\":45259},{\"end\":45411,\"start\":45406},{\"end\":45428,\"start\":45420},{\"end\":45442,\"start\":45436},{\"end\":45456,\"start\":45448},{\"end\":45466,\"start\":45457},{\"end\":45863,\"start\":45854},{\"end\":45875,\"start\":45869},{\"end\":45891,\"start\":45882},{\"end\":45906,\"start\":45901},{\"end\":45921,\"start\":45915},{\"end\":45934,\"start\":45930},{\"end\":46183,\"start\":46182},{\"end\":46199,\"start\":46191},{\"end\":46212,\"start\":46205},{\"end\":46223,\"start\":46220},{\"end\":46233,\"start\":46229},{\"end\":46250,\"start\":46244},{\"end\":46264,\"start\":46260},{\"end\":46739,\"start\":46734},{\"end\":46752,\"start\":46748},{\"end\":46766,\"start\":46762},{\"end\":46785,\"start\":46776},{\"end\":46797,\"start\":46791},{\"end\":46813,\"start\":46806},{\"end\":46827,\"start\":46822},{\"end\":46837,\"start\":46834},{\"end\":46847,\"start\":46842},{\"end\":47191,\"start\":47185},{\"end\":47207,\"start\":47203},{\"end\":47225,\"start\":47215},{\"end\":47240,\"start\":47235},{\"end\":47464,\"start\":47460},{\"end\":47482,\"start\":47474},{\"end\":47754,\"start\":47748},{\"end\":47768,\"start\":47764},{\"end\":47782,\"start\":47778},{\"end\":47796,\"start\":47791},{\"end\":47813,\"start\":47808},{\"end\":47826,\"start\":47821},{\"end\":47828,\"start\":47827},{\"end\":47842,\"start\":47836},{\"end\":47856,\"start\":47851},{\"end\":48145,\"start\":48141},{\"end\":48160,\"start\":48152},{\"end\":48174,\"start\":48168},{\"end\":48189,\"start\":48184},{\"end\":48200,\"start\":48196},{\"end\":48213,\"start\":48207},{\"end\":48215,\"start\":48214}]", "bib_author_last_name": "[{\"end\":45242,\"start\":45234},{\"end\":45257,\"start\":45254},{\"end\":45272,\"start\":45266},{\"end\":45418,\"start\":45412},{\"end\":45434,\"start\":45429},{\"end\":45446,\"start\":45443},{\"end\":45471,\"start\":45467},{\"end\":45867,\"start\":45864},{\"end\":45880,\"start\":45876},{\"end\":45899,\"start\":45892},{\"end\":45913,\"start\":45907},{\"end\":45928,\"start\":45922},{\"end\":45942,\"start\":45935},{\"end\":46189,\"start\":46184},{\"end\":46203,\"start\":46200},{\"end\":46218,\"start\":46213},{\"end\":46227,\"start\":46224},{\"end\":46242,\"start\":46234},{\"end\":46258,\"start\":46251},{\"end\":46271,\"start\":46265},{\"end\":46280,\"start\":46273},{\"end\":46746,\"start\":46740},{\"end\":46760,\"start\":46753},{\"end\":46774,\"start\":46767},{\"end\":46789,\"start\":46786},{\"end\":46804,\"start\":46798},{\"end\":46820,\"start\":46814},{\"end\":46832,\"start\":46828},{\"end\":46840,\"start\":46838},{\"end\":46851,\"start\":46848},{\"end\":47201,\"start\":47192},{\"end\":47213,\"start\":47208},{\"end\":47233,\"start\":47226},{\"end\":47246,\"start\":47241},{\"end\":47472,\"start\":47465},{\"end\":47488,\"start\":47483},{\"end\":47762,\"start\":47755},{\"end\":47776,\"start\":47769},{\"end\":47789,\"start\":47783},{\"end\":47806,\"start\":47797},{\"end\":47819,\"start\":47814},{\"end\":47834,\"start\":47829},{\"end\":47849,\"start\":47843},{\"end\":47867,\"start\":47857},{\"end\":48150,\"start\":48146},{\"end\":48166,\"start\":48161},{\"end\":48182,\"start\":48175},{\"end\":48194,\"start\":48190},{\"end\":48205,\"start\":48201},{\"end\":48222,\"start\":48216}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45404,\"start\":45155},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b1\"},\"end\":45774,\"start\":45406},{\"attributes\":{\"id\":\"b2\"},\"end\":46128,\"start\":45776},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3608234},\"end\":46649,\"start\":46130},{\"attributes\":{\"id\":\"b4\"},\"end\":47122,\"start\":46651},{\"attributes\":{\"doi\":\"arXiv:1606.05250\",\"id\":\"b5\"},\"end\":47458,\"start\":47124},{\"attributes\":{\"doi\":\"arXiv:1804.04235\",\"id\":\"b6\"},\"end\":47719,\"start\":47460},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13756489},\"end\":48053,\"start\":47721},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b8\"},\"end\":48482,\"start\":48055}]", "bib_title": "[{\"end\":46180,\"start\":46130},{\"end\":47746,\"start\":47721}]", "bib_author": "[{\"end\":45244,\"start\":45226},{\"end\":45259,\"start\":45244},{\"end\":45274,\"start\":45259},{\"end\":45420,\"start\":45406},{\"end\":45436,\"start\":45420},{\"end\":45448,\"start\":45436},{\"end\":45473,\"start\":45448},{\"end\":45869,\"start\":45854},{\"end\":45882,\"start\":45869},{\"end\":45901,\"start\":45882},{\"end\":45915,\"start\":45901},{\"end\":45930,\"start\":45915},{\"end\":45944,\"start\":45930},{\"end\":46191,\"start\":46182},{\"end\":46205,\"start\":46191},{\"end\":46220,\"start\":46205},{\"end\":46229,\"start\":46220},{\"end\":46244,\"start\":46229},{\"end\":46260,\"start\":46244},{\"end\":46273,\"start\":46260},{\"end\":46282,\"start\":46273},{\"end\":46748,\"start\":46734},{\"end\":46762,\"start\":46748},{\"end\":46776,\"start\":46762},{\"end\":46791,\"start\":46776},{\"end\":46806,\"start\":46791},{\"end\":46822,\"start\":46806},{\"end\":46834,\"start\":46822},{\"end\":46842,\"start\":46834},{\"end\":46853,\"start\":46842},{\"end\":47203,\"start\":47185},{\"end\":47215,\"start\":47203},{\"end\":47235,\"start\":47215},{\"end\":47248,\"start\":47235},{\"end\":47474,\"start\":47460},{\"end\":47490,\"start\":47474},{\"end\":47764,\"start\":47748},{\"end\":47778,\"start\":47764},{\"end\":47791,\"start\":47778},{\"end\":47808,\"start\":47791},{\"end\":47821,\"start\":47808},{\"end\":47836,\"start\":47821},{\"end\":47851,\"start\":47836},{\"end\":47869,\"start\":47851},{\"end\":48152,\"start\":48141},{\"end\":48168,\"start\":48152},{\"end\":48184,\"start\":48168},{\"end\":48196,\"start\":48184},{\"end\":48207,\"start\":48196},{\"end\":48224,\"start\":48207}]", "bib_venue": "[{\"end\":45224,\"start\":45155},{\"end\":45563,\"start\":45489},{\"end\":45852,\"start\":45776},{\"end\":46353,\"start\":46282},{\"end\":46732,\"start\":46651},{\"end\":47183,\"start\":47124},{\"end\":47567,\"start\":47506},{\"end\":47873,\"start\":47869},{\"end\":48139,\"start\":48055},{\"end\":46411,\"start\":46355}]"}}}, "year": 2023, "month": 12, "day": 17}