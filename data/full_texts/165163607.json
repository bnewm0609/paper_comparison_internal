{"id": 165163607, "updated": "2023-10-07 02:11:52.436", "metadata": {"title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "authors": "[{\"first\":\"Christopher\",\"last\":\"Clark\",\"middle\":[]},{\"first\":\"Kenton\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Ming-Wei\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Kwiatkowski\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Collins\",\"middle\":[]},{\"first\":\"Kristina\",\"last\":\"Toutanova\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "In this paper we study yes/no questions that are naturally occurring \u2014 meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1905.10044", "mag": "2953271402", "acl": "N19-1300", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/ClarkLCK0T19", "doi": "10.18653/v1/n19-1300"}}, "content": {"source": {"pdf_hash": "9770fff7379a7ab9006b48939462354dda9a2053", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/N19-1300.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f7ee29041a0a49bf2af671ce937335d3173ff204", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9770fff7379a7ab9006b48939462354dda9a2053.txt", "contents": "\nBoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJune 2 -June 7, 2019. 2019\n\nChristopher Clark \nSchool of CSE\nUniversity of Washington\n\n\nKenton Lee \nSchool of CSE\nUniversity of Washington\n\n\nMing-Wei Chang \nSchool of CSE\nUniversity of Washington\n\n\nTom Kwiatkowski \nSchool of CSE\nUniversity of Washington\n\n\nMichael Collins \nSchool of CSE\nUniversity of Washington\n\n\nKristina Toutanova \nSchool of CSE\nUniversity of Washington\n\n\nPaul G Allen \nSchool of CSE\nUniversity of Washington\n\n\nBoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n\nProceedings of NAACL-HLT 2019\nNAACL-HLT 2019Minneapolis, MinnesotaAssociation for Computational LinguisticsJune 2 -June 7, 2019. 20192924\nIn this paper we study yes/no questions that are naturally occurring -meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.\n\nIntroduction\n\nUnderstanding what facts can be inferred to be true or false from text is an essential part of natural language understanding. In many cases, these inferences can go well beyond what is immediately stated in the text. For example, a simple sentence like \"Hanna Huyskova won the gold medal for Belarus in freestyle skiing.\" implies that (1) Belarus is a country, (2) Hanna Huyskova is an athlete, (3) Belarus won at least one Olympic event, (4) the USA did not win the freestyle skiing event, and so on.\n\nTo test a model's ability to make these kinds of inferences, previous work in natural language in-Figure 1: Example yes/no questions from the BoolQ dataset. Each example consists of a question (Q), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018;Jia and Liang, 2017;McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning.\n\nIn this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find such questions often query for non-factoid information, and that human annotators need to apply a wide range of inferential abilities when answering them. As a result, they can be used to construct highly inferential reading comprehension datasets that have the added benefit of being directly related to the practical end-task of answering user yes/no questions.\n\nYes/No questions do appear as a subset of some existing datasets (Reddy et al., 2018;. However, these datasets are primarily intended to test other aspects of question answering (QA), such as conversational QA or multi-step reasoning, and do not contain naturally occurring questions.\n\nWe follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return \"yes\" or \"no\" as output. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasing. Therefore, it is not clear what the best data sources to transfer from are, or if it will be sufficient to just transfer from powerful pretrained language models such as BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018). We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few other supervised datasets.\n\nWe found that transferring from MultiNLI, and the unsupervised pre-training in BERT, gave us the best results. Notably, we found these approaches are surprisingly complementary and can be combined to achieve a large gain in performance. Overall, our best model reaches 80.43% accuracy, compared to 62.31% for the majority baseline and 90% human accuracy. In light of the fact BERT on its own has achieved human-like performance on several NLP tasks, this demonstrates the high degree of difficulty of our dataset. We present our data and code at https://goo.gl/boolq.\n\n\nRelated Work\n\nYes/No questions make up a subset of the reading comprehension datasets CoQA (Reddy et al., 2018), QuAC , and Hot-PotQA , and are present in the ShARC (Saeidi et al., 2018) dataset. These datasets were built to challenge models to understand conversational QA (for CoQA, ShARC and QuAC) or multi-step reasoning (for HotPotQA), which complicates our goal of using yes/no questions to test inferential abilities. Of the four, QuAC is the only one where the question authors were not allowed to view the text being used to answer their questions, making it the best candidate to contain naturally occurring questions. However, QuAC still heavily prompts users, including limiting their questions to be about pre-selected Wikipedia articles, and is highly class imbalanced with 80% \"yes\" answers.\n\nThe MS Marco dataset (Nguyen et al., 2016), which contains questions with free-form text answers, also includes some yes/no questions. We experiment with heuristically identifying them in Section 4, but this process can be noisy and the quality of the resulting annotations is unknown. We also found the resulting dataset is class imbalanced, with 80% \"yes\" answers.\n\nYes/No QA has been used in other contexts, such as the templated bAbI stories (Weston et al.) or some Visual QA datasets (Antol et al., 2015;Wu et al., 2017). We focus on answering yes/no questions using natural language text.\n\nQuestion answering for reading comprehension in general has seen a great deal of recent work (Rajpurkar et al., 2016;Joshi et al., 2017), and there have been many recent attempts to construct QA datasets that require advanced reasoning abilities Welbl et al., 2018;Mihaylov et al., 2018;Zellers et al., 2018;. However, these attempts typically involve engineering data to be more difficult by, for example, explicitly prompting users to write multi-step questions Mihaylov et al., 2018), or filtering out easy questions (Zellers et al., 2018). This risks resulting in models that do not have obvious end-use applications since they are optimized to perform in an artificial setting. In this paper, we show that yes/no questions have the benefit of being very challenging even when they are gathered from natural sources.\n\nNatural language inference is also a well studied area of research, particularly on the MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) datasets. Other sources of entailment data include the PASCAL RTE challenges (Bentivogli et al., 2009(Bentivogli et al., , 2011 or Sci-Tail . We note that, although Sc-iTail, RTE-6 and RTE-7 did not use crowd workers to generate candidate statements, they still use sources (multiple choices questions or document summaries) that were written by humans with knowledge of the premise text. Using naturally occurring yes/no questions ensures even greater independence between the questions and premise text, and ties our dataset to a clear end-task. BoolQ also requires detecting entailment in paragraphs instead of sentence pairs. Transfer learning for entailment has been studied in GLUE (Wang et al., 2018) and SentEval (Conneau and Kiela, 2018). Unsupervised pre-training in general has recently shown excellent results on many datasets, including entailment data (Peters et al., 2018;Devlin et al., 2018;Radford et al., 2018).\n\nConverting short-answer or multiple choice questions into entailment examples, as we do when experimenting with transfer learning, has been proposed in several prior works (Demszky et al., 2018;Poliak et al., 2018;. In this paper we found some evidence suggesting that these approaches are less effective than using crowd-sourced entailment examples when it comes to transferring to natural yes/no questions.\n\nContemporaneously with our work, Phang et al. (2018) showed that pre-training on supervised tasks could be beneficial even when using pretrained language models, especially for a textual entailment task. Our work confirms these results for yes/no question answering.\n\nThis work builds upon the Natural Questions (NQ) (Kwiatkowski et al., 2019), which contains some natural yes/no questions. However, there are too few (about 1% of the corpus) to make yes/no QA a very important aspect of that task. In this paper, we gather a large number of additional yes/no questions in order to construct a dedicated yes/no QA dataset.\n\n\nThe BoolQ Dataset\n\nAn example in our dataset consists of a question, a paragraph from a Wikipedia article, the title of the article, and an answer, which is either \"yes\" or \"no\". We include the article title since it can potentially help resolve ambiguities (e.g., coreferent phrases) in the passage, although none of the models presented in this paper make use of them.\n\n\nData Collection\n\nWe gather data using the pipeline from NQ (Kwiatkowski et al., 2019), but with an additional filtering step to focus on yes/no questions. We summarize the complete pipeline here, but refer to their paper for a more detailed description.\n\nQuestions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words 3 and are of sufficient length, to be effective.\n\nQuestions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing.\n\nAnnotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as \"not answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is \"yes\" or \"no\". Annotating data in this manner is quite expensive since annotators need to search entire Wikipedia documents for relevant evidence and read the text carefully.\n\nNote that, unlike in NQ, we only use questions that were marked as having a yes/no answer, and pair each question with the selected passage instead of the entire document. This helps reduce ambiguity (ex., avoiding cases where the document supplies conflicting answers in different paragraphs), and keeps the input small enough so that existing entailment models can easily be applied to our dataset.\n\nWe combine 13k questions gathered from this pipeline with an additional 3k questions with yes/no answers from the NQ training set to reach a total of 16k questions. We split these questions into a 3.2k dev set, 3.2k test set, and 9.4k train set, ensuring questions from NQ are always in the train set. \"Yes\" answers are slightly more common (62.31% in the train set). The queries are typically short (average length 8.9 tokens) with longer passages (average length 108 tokens).\n\n\nAnalysis\n\nIn the following section we analyze our dataset to better understand the nature of the questions, the annotation quality, and the kinds of reasoning abilities required to answer them.\n\n\nAnnotation Quality\n\nFirst, in order to assess annotation quality, three of the authors labelled 110 randomly chosen examples. If there was a disagreement, the authors conferred and selected a single answer by mutual agreement. We call the resulting labels \"gold-standard\" labels. On the 110 selected examples, the answer annotations reached 90% accuracy compared to the gold-standard labels. Of the cases where the answer annotation differed from the gold-standard, six were ambiguous or debatable cases, and five were errors where the annotator misunderstood the passage. Since the agreement was sufficiently high, we elected to use singly-annotated examples in the training/dev/test sets in order to be able to gather a larger dataset.\n\n\nQuestion Types\n\nPart of the value of this dataset is that it contains questions that people genuinely want to answer.\n\nTo explore this further, we manually define a set of topics that questions can be about. An author categorized 200 questions into these topics. The results can be found in the upper half of Table 1.\n\nQuestions were often about entertainment media (including T.V., movies, and music), along with other popular topics like sports. However, there are still a good portion of questions asking for more general factual knowledge, including ones about historical events or the natural world.\n\nWe also broke the questions into categories based on what kind of information they were requesting, shown in the lower half of Table 1. Roughly one-sixth of the questions are about whether anything with a particular property exists (Existence), another sixth are about whether a particular event occurred (Event Occurrence), and another sixth ask whether an object is known by a particular name, or belongs to a particular category (Definitional). The questions that do not fall into these three categories were split between requesting facts about a specific entity, or requesting more general factual information.\n\nWe do find a correlation between the nature of the question and the likelihood of a \"yes\" answer. However, this correlation is too weak to help outperform the majority baseline because, even if the topic or type is known, it is never best to guess the minority class. We also found that question-only models perform very poorly on this task (see Section 5.3), which helps confirm that the questions  do not contain sufficient information to predict the answer on their own.\n\n\nTypes of Inference\n\nFinally, we categorize the kinds of inference required to answer the questions in BoolQ 4 . The definitions and results are shown in Table 2. Less than 40% of the examples can be solved by detecting paraphrases. Instead, many questions require making additional inferences (categories \"Factual Reasoning\", \"By Example\", and \"Other Inference\") to connect what is stated in the passage to the question. There is also a significant class of questions (categories \"Implicit\" and \"Missing Mention\") that require a subtler kind of inference based on how the passage is written.\n\n\nDiscussion\n\nWhy do natural yes/no questions require inference so often? We hypothesize that there are several factors. First, we notice factoid questions that ask about simple properties of entities, such as \"Was Obama born in 1962?\", are rare. We suspect this is because people will almost always prefer to 4 Note the dataset has been updated since we carried out this analysis, so it might be slighly out-of-date. phrase such questions as short-answer questions (e.g., \"When was Obama born?\"). Thus, there is a natural filtering effect where people tend to use yes/no questions exactly when they want more complex kinds of information.\n\nSecond, both the passages and questions rarely include negation. As a result, detecting a \"no\" answer typically requires understanding that a positive assertion in the text excludes, or makes unlikely, a positive assertion in the question. This requires reasoning that goes beyond paraphrasing (see the \"Other-Inference\" or \"Implicit\" examples).\n\nWe also think it was important that annotators only had to answer questions, rather than generate them. For example, imagine trying to construct questions that fall into the categories of \"Missing Mention\" or \"Implicit\". While possible, it would require a great deal of thought and creativity. On the other hand, detecting when a yes/no question can be answered using these strategies seems much easier and more intuitive. Thus, having annotators answer pre-existing questions opens the door to building datasets that contain more inference and have higher quality labels.\n\n\nTraining Yes/No QA Models\n\nModels on this dataset need to predict an output class given two pieces of input text, which is a well studied paradigm (Wang et al., 2018). We find training models on our train set alone to be relatively ineffective. Our best model reaches 69.6% accuracy, only 8% better than the majority baseline. Therefore, we follow the recent trend in NLP of using transfer learning. In particular, we experiment with pre-training models on related tasks that have larger datasets, and then fine-tuning them on our training data. We list the sources we consider for pre-training below.\n\n\nEntailment:\n\nWe consider two entailment datasets, MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). We choose these datasets since they are widely-used and large enough to use for pre-training. We also experiment with ablating classes from MultiNLI. During fine-tuning we use the probability the model assigns to the \"entailment\" class as the probability of predicting a \"yes\" answer.\n\nMultiple-Choice QA: We use a multiple choice reading comprehension dataset, RACE (Lai et al., 2017), which contains stories or short essays paired with questions built to test the reader's comprehension of the text. Following what was done in SciTail , we convert questions and answer-options to statements by either substituting the answer-option for the blanks in fill-in-the-blank questions, or appending a separator token and the answer-option to the question.\n\nDuring training, we have models independently assign a score to each statement, and then apply the softmax operator between all statements per each question to get statement probabilities. We use the negative log probability of the correct statement as a loss function. To fine-tune on BoolQ, we apply the sigmoid operator to the score of the question given its passage to get the probability of a \"yes\" answer.\n\nExtractive QA: We consider several methods of leveraging extractive QA datasets, where the model must answer questions by selecting text from a relevant passage. Preliminary experiments found that simply transferring the lower-level weights of extractive QA models was ineffective, so we instead consider three methods of con-structing entailment-like data from extractive QA data.\n\nFirst, we use the QNLI task from GLUE (Wang et al., 2018), where the model must determine if a sentence from SQuAD 1.1 (Rajpurkar et al., 2016) contains the answer to an input question or not. Following previous work , we also try building entailment-like training data from SQuAD 2.0 (Rajpurkar et al., 2018). We concatenate questions with either the correct answer, or with the incorrect \"distractor\" answer candidate provided by the dataset, and train the model to classify which is which given the question's supporting text.\n\nFinally, we also experiment with leveraging the long-answer portion of NQ, where models must select a paragraph containing the answer to a question from a document. Following our method for Multiple-Choice QA, we train a model to assign a score to (question, paragraph) pairs, apply the softmax operator on paragraphs from the same document to get a probability distribution over the paragraphs, and train the model on the negative log probability of selecting an answer-containing paragraph. We only train on questions that were marked as having an answer, and select an answer-containing paragraph and up to 15 randomly chosen non-answer-containing paragraphs for each question. On BoolQ, we compute the probability of a \"yes\" answer by applying the sigmoid operator to the score the model gives to the input question and passage.\n\nParaphrasing: We use the Quora Question Paraphrasing (QQP) dataset, which consists of pairs of questions labelled as being paraphrases or not. 5 Paraphrasing is related to entailment since we expect, at least in some cases, passages will contain a paraphrase of the question.\n\nHeuristic Yes/No: We attempt to heuristically construct a corpus of yes/no questions from the MS Marco corpus (Nguyen et al., 2016). MS Marco has free-form answers paired with snippets of related web documents. We search for answers starting with \"yes\" or \"no\", and then pair the corresponding questions with snippets marked as being related to the question. We call this task Y/N MS Marco; in total we gather 38k examples, 80% of which are \"yes\" answers.\n\nUnsupervised: It is well known that unsupervised pre-training using language-modeling objectives (Peters et al., 2018;Devlin et al., 2018;Radford et al., 2018), can improve performance on many tasks. We experiment with these methods by using the pre-trained models from ELMo, BERT, and OpenAI's Generative Pre-trained Transformer (OpenAI GPT) (see Section 5.2).\n\n\nResults\n\n\nShallow Models\n\nFirst, we experiment with using a linear classifier on our task. In general, we found features such as word overlap or TF-IDF statistics were not sufficient to achieve better than the majorityclass baseline accuracy (62.17% on the dev set). We did find there was a correlation between the number of times question words occurred in the passage and the answer being \"yes\", but the correlation was not strong enough to build an effective classifier. \"Yes\" is the most common answer even among questions with zero shared words between the question and passage (with a 51% majority), and more common in other cases.\n\n\nNeural Models\n\nFor our experiments that do not use unsupervised pre-training (except the use of pre-trained word vectors), we use a standard recurrent model with attention. Our experiments using unsupervised pre-training use the models provided by the authors. In more detail:\n\nOur Recurrent model follows a standard recurrent plus attention architecture for text-pair classification (Wang et al., 2018). It embeds the premise/hypothesis text using fasttext word vectors (Mikolov et al., 2018) and learned character vectors, applies a shared bidirectional LSTM to both parts, applies co-attention (Parikh et al., 2016) to share information between the two parts, applies another bi-LSTM to both parts, pools the result, and uses the pooled representation to predict the final class. See Appendix A.2 for details. Our Recurrent +ELMo model uses the language model from Peters et al. (2018) to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors.\n\nOur OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from Radford et al. (2018), which has been pretrained as a language model on the Books corpus (Zhu et al., 2015).\n\nOur BERT L model fine-tunes the 24 layer 1024 dimensional transformer from Devlin et al. (2018), which has been trained on next-sentence-selection and masked language modelling on the Book Corpus and Wikipedia.\n\nWe fine-tune the BERT L and the OpenAI GPT models using the optimizers recommended by the authors, but found it important to tune the optimization parameters to achieve the best results. We use a batch size of 24, learning rate of 1e-5, and 5 training epochs for BERT and a learning rate of 6.25e-5, batch size of 6, language model loss of 0.5, and 3 training epochs for OpenAI GPT.\n\n\nQuestion/Passage Only Results\n\nFollowing the recommendation of Gururangan et al. (2018), we first experiment with models that are only allowed to observe the question or the passage. The pre-trained BERT L model reached 64.48% dev set accuracy using just the question and 66.74% using just the passage. Given that the majority baseline is 62.17%, this suggests there is little signal in the question by itself, but that some language patterns in the passage correlate with the answer. Possibly, passages that present more straightforward factual information (like Wikipedia introduction paragraphs) correlate with \"yes\" answers.\n\n\nTransfer Learning Results\n\nThe results of our transfer learning methods are shown in Table 3. All results are averaged over five runs. For models pre-trained on supervised datasets, both the pre-training and the fine-tuning stages were repeated. For unsupervised pretraining, we use the pre-trained models provided by the authors, but continue to average over five runs of fine-tuning.\n\nQA Results: We were unable to transfer from RACE or SQuAD 2.0. For RACE, the problem might be domain mismatch. In RACE the passages are stories, and the questions often query for passage-specific information such as the author's intent or the state of a particular entity from the passage, instead of general knowledge.\n\nWe would expect SQuAD 2.0 to be a better match for BoolQ since it is also Wikipediabased, but its possible detecting the adversarially-  Table 3: Transfer learning results on the BoolQ dev set after fine-tuning on the BoolQ training set. Results are averaged over five runs. In all cases directly using the pre-trained model without fine-tuning did not achieve results better than the majority baseline, so we do not include them here.\n\nconstructed distractors used for negative examples does not relate well to yes/no QA.\n\nWe got better results using QNLI, and even better results using NQ. This shows the task of selecting text relevant to a question is partially transferable to yes/no QA, although we are only able to gain a few points over the baseline.\n\nEntailment Results: The MultiNLI dataset out-performed all other supervised methods by a large margin. Remarkably, this approach is only a few points behind BERT despite using orders of magnitude less training data and a much more light-weight model, showing high-quality pre-training data can help compensate for these deficiencies.\n\nOur ablation results show that removing the neutral class from MultiNLI hurt transfer slightly, and removing either of the other classes was very harmful, suggesting the neutral examples had limited value. SNLI transferred better than other datasets, but worse than MultiNLI. We suspect this is due to limitations of the photo-caption domain it was constructed from.  Table 4: Test set results on BoolQ, \"+MultiNLI\" indicates models that were additionally pre-trained on MultiNLI before being fine-tuned on the train set.\n\nMarco. Although Y/N MS Marco is a yes/no QA dataset, its small size and class imbalance likely contributed to its limited effectiveness. The web snippets it uses as passages also present a large domain shift from the Wikipedia passages in BoolQ.\n\nUnsupervised Results: Following results on other datasets (Wang et al., 2018), we found BERT L to be the most effective unsupervised method, surpassing all other methods of pretraining.\n\n\nMulti-Step Transfer Results\n\nOur best single-step transfer learning results were from using the pre-trained BERT L model and MultiNLI. We also experiment with combining these approaches using a two-step pre-training regime. In particular, we fine-tune the pre-trained BERT L on MultiNLI, and then fine-tune the resulting model again on the BoolQ train set. We found decreasing the number of training epochs to 3 resulted in a slight improvement when using the model pre-trained on MultiNLI.\n\nWe show the test set results for this model, and some other pre-training variations, in Table 4. For these results we train five versions of each model using different training seeds, and show the model that had the best dev-set performance.\n\nGiven how extensively the BERT L model has been pre-trained, and how successful it has been across many NLP tasks, the additional gain of 3.5 points due to using MultiNLI is remarkable. This suggests MultiNLI contains signal orthogonal to what is found in BERT's unsupervised objectives.\n\n\nSample Efficiency\n\nIn Figure 2, we graph model accuracy as more of the training data is used for fine-tuning, both with and without initially pre-training on MultiNLI. Pre-training on MultiNLI gives at least a 5-6 point gain, and nearly a 10 point gain for BERT L when only using 1000 examples. For small numbers of examples, the recurrent model with MultiNLI pretraining actually out-performs BERT L .\n\n\nDiscussion\n\nA surprising result from our work is that the datasets that more closely resemble the format of BoolQ, meaning they contain questions and multisentence passages, such as SQuAD 2.0, RACE, or 1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000 9,000 60 Y/N MS Marco, were not very useful for transfer. The entailment datasets were stronger despite consisting of sentence pairs. This suggests that adapting from sentence-pair input to question/passage input was not a large obstacle to achieving transfer. Preliminary work found attempting to convert the yes/no questions in BoolQ into declarative statements did not improve transfer from MultiNLI, which supports this hypothesis. The success of MultiNLI might also be surprising given recent concerns about the generalization abilities of models trained on it (Glockner et al., 2018), particularly related to \"annotation artifacts\" caused by using crowd workers to write the hypothesis statements (Gururangan et al., 2018). We have shown that, despite these weaknesses, it can still be an important starting point for models being used on natural data.\n\nWe hypothesize that a key advantage of MultiNLI is that it contains examples of contradictions. The other sources of transfer we consider, including the next-sentence-selection objective in BERT, are closer to providing examples of entailed text vs. neutral/unrelated text. Indeed, we found that our two step transfer procedure only reaches 78.43% dev set accuracy if we remove the contradiction class from MultiNLI, regressing its performance close to the level of BERT L when just using unsupervised pre-training.\n\nNote that it is possible to pre-train a model on several of the suggested datasets, either in succession or in a multi-task setup. We leave these experiments to future work. Our results also suggest pre-training on MultiNLI would be helpful for other corpora that contain yes/no questions.\n\n\nConclusion\n\nWe have introduced BoolQ, a new reading comprehension dataset of naturally occurring yes/no questions. We have shown these questions are challenging and require a wide range of inference abilities to solve. We have also studied how transfer learning performs on this task, and found crowd-sourced entailment datasets can be leveraged to boost performance even on top of language model pre-training. Future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application. embeddings h 1 , h 2 , ... . Then pool these embeddings by computing attention scores a i = w \u00b7 h i , p = sof tmax(a), and then the sum v * = i p i h i . Likewise we compute p * from the premise.\n\nClassify: Finally we feed [v * ; p * ] into a fully connected layer, and then through a softmax layer to predict the output class.\n\nWe apply dropout at a rate of 0.2 between all layers, and train the model using the Adam optimizer (Kingma and Ba, 2014). The learning rate is decayed by 0.999 every 100 steps. We use 200 dimensional LSTMs and a 100 dimensional fully connected layer.\n\n\nIs there a catalytic converter on a diesel? (Y)\n\nA catalytic converter is an exhaust emission control device that converts toxic gases and pollutants in exhaust gas from an internal combustion engine into less-toxic pollutants by catalyzing a redox reaction (an oxidation and a reduction reaction). Catalytic converters are usually used with internal combustion engines fueled by either gasoline or diesel-including lean-burn engines as well as kerosene heaters and stoves.\n\nIs there a season 2 of Pride and Prejudice? (N)\n\nPride and Prejudice is a six-episode 1995 British television drama, adapted by Andrew Davies from Jane Austen's 1813 novel of the same name. Jennifer Ehle and Colin Firth starred as Elizabeth Bennet and Mr. Darcy. Produced by Sue Birtwistle and directed by Simon Langton, the serial was a BBC production with additional funding from the American A&E Network. BBC1 originally broadcast the 55-minute episodes from 24 September to 29 October 1995. The A&E Network aired the series in double episodes on three consecutive nights beginning 14 January 1996. There are six episodes in the series.\n\n\nIs Saving Private Ryan based on a book? (N)\n\nIn 1994, Robert Rodat wrote the script for the film. Rodat's script was submitted to producer Mark Gordon, who liked it and in turn passed it along to Spielberg to direct. The film is loosely based on the World War II life stories of the Niland brothers. A shooting date was set for June 27, 1997.\n\nIs The Talk the same as The View? (N) In November 2008, the show's post-election day telecast garnered the biggest audience in the show's history at 6.2 million in total viewers, becoming the week's most-watched program in daytime television. It was surpassed on July 29, 2010, during which former President Barack Obama first appeared as a guest on The View, which garnered a total of 6.6 million viewers. In 2013, the show was reported to be averaging 3.1 million daily viewers, which outpaced rival talk show The Talk.\n\nDoes the concept of a contact force apply to both a macroscopic scale and an atomic scale? (N)\n\nIn the Standard Model of modern physics, the four fundamental forces of nature are known to be non-contact forces. The strong and weak interaction primarily deal with forces within atoms, while gravitational effects are only obvious on an ultra-macroscopic scale. Molecular and quantum physics show that the electromagnetic force is the fundamental interaction responsible for contact forces. The interaction between macroscopic objects can be roughly described as resulting from the electromagnetic interactions between protons and electrons of the atomic constituents of these objects. Everyday objects do not actually touch; rather, contact forces are the result of the interactions of the electrons at or near the surfaces of the objects.\n\n\nLegal to break out of prison in Germany? (Y)\n\nIn Mexico, Belgium, Germany and Austria, the philosophy of the law holds that it is human nature to want to escape. In those countries, escapees who do not break any other laws are not charged for anything and no extra time is added to their sentence. However, in Mexico, officers are allowed to shoot prisoners attempting to escape, and an escape is illegal if violence is used against prison personnel or property, or if prison inmates or officials aid the escape.\n\nIs the movie sand pebbles based on a true story? (N)\n\nThe Sand Pebbles is a 1966 American war film directed by Robert Wise in Panavision. It tells the story of an independent, rebellious U.S. Navy machinist's mate, first class aboard the fictional gunboat USS San Pablo in 1920s China.\n\n\nIs Burberrys of London the same as Burberry? (Y)\n\nBurberry was founded in 1856 when 21-year-old Thomas Burberry, a former draper's apprentice, opened his own store in Basingstoke, Hampshire, England. By 1870, the business had established itself by focusing on the development of outdoors attire. In 1879, Burberry introduced in his brand the gabardine, a hardwearing, water-resistant yet breathable fabric, in which the yarn is waterproofed before weaving. \"Burberry\" was the original name until it became \"Burberrys\", due to many customers from around the world began calling it \"Burberrys of London\". In 1999, the name was reverted to the original, \"Burberry\". However, the name \"Burberrys of London\" is still visible on many older Burberry products. In 1891, Burberry opened a shop in the Haymarket, London. Before being termed as trench, it was known as the Tielocken worn by the British officers and featured a belt with no buttons, was double breasted, and protected the body from neck to knees.\n\n\nIs the Saturn Vue the same as the Chevy Equinox? (N)\n\nRiding on the GM Theta platform, the unibody is mechanically similar to the Saturn Vue and the Suzuki XL7. However, the Equinox and the Torrent are larger than the Vue, riding on a 112.5 in (2,858mm) wheelbase, 5.9 in (150mm) longer than the Vue. Front-wheel drive is standard, with optional all-wheel drive. They are not designed for serious off-roading like the truck-based Chevrolet Tahoe and Chevrolet TrailBlazer.\n\n\nIs Destin FL on the Gulf of Mexico? (Y)\n\nThe city is located on a peninsula separating the Gulf of Mexico from Choctawhatchee Bay. The peninsula was originally an island; hurricanes and sea level changes gradually connected the island to the mainland. \n\n\nFigure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples.\n\nFigure 2 :\n2Accuracy for various models on the BoolQ dev set as the number of training examples varies.\n\nFigure 3 :\n3Randomly sampled examples from the BoolQ train set.\n\n\nReasoning Types Yes/No Question Answering Examples Paraphrasing (38.7%) Q: Is Tim Brown in the Hall of Fame? The passage explicitly asserts or refutes what is stated in the question. P: Brown has also played for the Tampa Bay Buccaneers. In 2015, he was inducted into the Pro Football Hall of Fame. A: Yes. [\"inducted into\" directly implies he is in Hall of Fame.] By Example (11.8%) Q: Are there any nuclear power plants in Michigan? The passage provides an example or counter-example to what is asserted by the question. P: . . . three nuclear power plants supply Michigan with about 30% of its electricity. A: Yes. [Since there must be at least three.] Factual Reasoning (8.5%) Q: Was designated survivor filmed in the White House? Answering the question requires using world-knowledge to connect what is stated in the passage to the question. P: The series is. . . filmed in Toronto, Ontario. A: No. [The White House is not located in Toronto.] Implicit (8.5%) Q: Is static pressure the same as atmospheric pressure? The passage mentions or describes entities in the question in way that would not make sense if the answer was not yes/no. P: The aircraft designer's objective is to ensure the pressure in the aircraft's static pressure system is as close as possible to the atmospheric pressure. . . A: No. [It would not make sense to bring them \"as close as possible\" if those terms referred to the same thing.] Missing Mention (6.6%) Q: Did Bonnie Blair's daughter make the Olympic team? We can conclude the answer is yes or no because, if this was not the case, it would have been mentioned in the passage. P: Blair and Cruikshank have two children: a son, Grant, and daughter, Blair.... Blair Cruikshank competed at the 2018 United States Olympic speed skating trials at the 500 meter distance. A: No. [The passage describes Blair Cruikshank's daughter's skating accomplishments, so it would have mentioned it if she had qualified.] Other Inference (25.9%) Q: Is the sea snake the most venomous snake? The passage states a fact that can be used to infer whether the answer is true or false, and does not fall into any of the other categories. P: . . . the venom of the inland taipan, drop by drop, is the most toxic among all snakes A: No. [If inland taipan is the most venomous snake, the sea snake must not be.]\n\nTable 2 :\n2Kinds of reasoning needed in the BoolQ dataset.\n\n\nOther Supervised Results: We obtained a small amount of transfer using QQP and Y/N MSModel \nDev Acc. Test Acc. \n\nMajority Class \n62.17 \n62.31 \nRecurrent \n70.28 \n67.52 \n\n+MultiNLI \n\n76.15 \n74.24 \nPre-trained BERT L \n78.09 \n76.70 \n\n+MultiNLI \n\n82.20 \n80.43 \n\n\nWork completed while interning at Google. 2 Also affiliated with Columbia University, work done at Google.\nThe full set is: {\"did\", \"do\", \"does\", \"is\", \"are\", \"was\", \"were\", \"have\", \"has\", \"can\", \"could\", \"will\", \"would\"}.\ndata.quora.com/First-Quora-Dataset-Release-Question-Pairs\nA AppendicesA.1 Randomly Selected ExamplesWe include a number of randomly selected examples from the BoolQ train set inFigure 3. For each example we show the question in bold, followed by the answer in parentheses, and then the passage below.A.2 Recurrent ModelOur recurrent model is a standard model from the text pair classification literature, similar to the one used in the GLUE baseline(Wang et al., 2018)and the model fromChen et al. (2017). Our model has the following stages:Embed: Embed the words using a character CNN following what was done bySeo et al. (2017), and the fasttext crawl word embeddings(Mikolov et al., 2018). Then run a BiLSTM over the results to get context-aware word hypothesis embeddings u 1 , u 2 , u 3 , ... and premise embeddings v 1 , v 2 , v 3 , ... .Co-Attention: Compute a co-attention matrix, A, between the hypothesis and premise where\u2022 is elementwise multiplication, and w 1 , w 2 , and w 3 are weights to be learned.Attend: For each row in A, apply the softmax operator and use the results to compute a weighed sum of the hypothesis embeddings, resulting in attended vectors \u0169 1 ,\u0169 2 , ... . We use the transpose of A to compute vectors \u1e7d 1 ,\u1e7d 2 , ... from the premise embeddings in a similar manner.Pool:Run another BiLSTM over [v 1 ;\u1e7d 1 ;\u1e7d 1 \u2022 v 1 ], [v 2 ;\u1e7d 2 ;\u1e7d 2 \u2022 v 2 ], ... to get\nVQA: Visual Question Answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question An- swering. In Proceedings of the IEEE international conference on computer vision.\n\nThe Sixth PASCAL Recognizing Textual Entailment Challenge. Luisa Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo, TAC. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The Sixth PASCAL Recogniz- ing Textual Entailment Challenge. In TAC.\n\nThe Seventh PASCAL Recognizing Textual Entailment Challenge. Luisa Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo, TAC. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2011. The Seventh PASCAL Recog- nizing Textual Entailment Challenge. In TAC.\n\nA Large Annotated Corpus for Learning Natural Language Inference. Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075EMNLPSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A Large Anno- tated Corpus for Learning Natural Language Infer- ence. EMNLP.\n\nEnhanced LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen, 10.18653/v1/P17-1152ACL. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for Natural Language Inference. In ACL.\n\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, QuaC: Question Answering in Context. EMNLP. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen- tau Yih, Yejin Choi, Percy Liang, and Luke Zettle- moyer. 2018. QuaC: Question Answering in Con- text. EMNLP.\n\nSenteval: An Evaluation Toolkit for Universal Sentence Representations. Alexis Conneau, Douwe Kiela, LREC. Alexis Conneau and Douwe Kiela. 2018. Senteval: An Evaluation Toolkit for Universal Sentence Rep- resentations. In LREC.\n\nTransforming Question Answering Datasets Into Natural Language Inference Datasets. Dorottya Demszky, Kelvin Guu, Percy Liang, arXiv:1809.029222Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming Question Answering Datasets Into Natural Language Inference Datasets. arXiv:1809.02922. Version 2.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. arXiv:1810.04805. Version 1.\n\nBreaking NLI Systems with Sentences that Require Simple Lexical Inferences. Max Glockner, Vered Shwartz, Yoav Goldberg, Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI Systems with Sentences that Require Simple Lexical Inferences. ACL.\n\nSwabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Schwartz, Noah A Samuel R Bowman, Smith, 10.18653/v1/N18-2017Annotation Artifacts in Natural Language Inference Data. NAACL. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation Artifacts in Natu- ral Language Inference Data. NAACL.\n\nRead+ Verify: Machine Reading Comprehension with Unanswerable Questions. Minghao Hu, Yuxing Peng, Zhen Huang, Nan Yang, Ming Zhou, CoRRMinghao Hu, Yuxing Peng, Zhen Huang, Nan Yang, Ming Zhou, et al. 2018. Read+ Verify: Machine Reading Comprehension with Unanswerable Ques- tions. CoRR.\n\nAdversarial Examples for Evaluating Reading Comprehension Systems. Robin Jia, Percy Liang, 10.18653/v1/D17-1215EMNLP. Robin Jia and Percy Liang. 2017. Adversarial Ex- amples for Evaluating Reading Comprehension Sys- tems. EMNLP.\n\nTriviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, 10.18653/v1/P17-1147ACLMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A Large Scale Dis- tantly Supervised Challenge Dataset for Reading Comprehension. ACL.\n\nSciTail: A Textual Entailment Dataset from Science Question Answering. Tushar Khot, Ashish Sabharwal, Peter Clark, Proceedings of AAAI. AAAITushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTail: A Textual Entailment Dataset from Science Question Answering. In Proceedings of AAAI.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLRDiederik P Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. ICLR.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Natural Questions: a Benchmark for Question Answering Research. TACL. Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav PetrovKenton LeeTom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu- ral Questions: a Benchmark for Question Answering Research. TACL.\n\nRace: Large-Scale Reading Comprehension Dataset from Examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, 10.18653/v1/D17-1082EMNLP. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-Scale Read- ing Comprehension Dataset from Examinations. EMNLP.\n\nThomas Mccoy, Ellie Pavlick, Tal Linzen, arXiv:1902.01007.Ver-sion1Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. Computing Research Repository. R Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. Comput- ing Research Repository, arXiv:1902.01007. Ver- sion 1.\n\nCan a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, EMNLP. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Ques- tion Answering. In EMNLP.\n\nAdvances in Pre-Training Distributed Word Representations. Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin, Proceedings of the International Conference on Language Resources and Evaluation (LREC. the International Conference on Language Resources and Evaluation (LRECTomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Ad- vances in Pre-Training Distributed Word Represen- tations. In Proceedings of the International Confer- ence on Language Resources and Evaluation (LREC 2018).\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, arXiv:1611.09268MS MARCO: A Human Generated Machine Reading Comprehension Dataset. Version 3Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Gen- erated Machine Reading Comprehension Dataset. arXiv:1611.09268. Version 3.\n\nA Decomposable Attention Model for Natural Language Inference. P Ankur, Oscar Parikh, Dipanjan T\u00e4ckstr\u00f6m, Jakob Das, Uszkoreit, 10.18653/v1/D16-1244EMNLP. Ankur P Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. 2016. A Decomposable Attention Model for Natural Language Inference. In EMNLP.\n\nDeep Contextualized Word Representations. E Matthew, Mark Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, 10.18653/v1/N18-1202NAACL. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Representations. In NAACL.\n\nJason Phang, Thibault F\u00e9vry, Samuel R Bowman, arXiv:1811.01088Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks. Computing Research Repository. 2Jason Phang, Thibault F\u00e9vry, and Samuel R Bowman. 2018. Sentence Encoders on STILTs: Supplemen- tary Training on Intermediate Labeled-data Tasks. Computing Research Repository, arXiv:1811.01088. Version 2.\n\nCollecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. Adam Poliak, Aparajita Haldar, Rachel Rudinger, Edward Hu, Ellie Pavlick, Aaron Steven White, Benjamin Van Durme, EMNLP. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J Ed- ward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. 2018. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation. In EMNLP.\n\nImproving Language Understanding by Generative Pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya SutskeverAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Under- standing by Generative Pre-training.\n\nPranav Rajpurkar, Robin Jia, Percy Liang, Know What You Don't Know: Unanswerable Questions for SQuAD. ACL. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don't Know: Unanswerable Ques- tions for SQuAD. ACL.\n\nSquad: 100,000+ Questions for Machine Comprehension of Text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264EMNLP. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ Questions for Machine Comprehension of Text. EMNLP.\n\nCoQA: A Conversational Question Answering Challenge. Siva Reddy, Danqi Chen, Christopher D Manning, TACL. Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. CoQA: A Conversational Question Answer- ing Challenge. In TACL.\n\nInterpretation of Natural Language Rules in Conversational Machine Reading. Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, Sebastian Riedel, EMNLP. Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpreta- tion of Natural Language Rules in Conversational Machine Reading. In EMNLP.\n\nBidirectional Attention Flow for Machine Comprehension. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi, ICLR. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional Attention Flow for Machine Comprehension. In ICLR.\n\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Plat- form for Natural Language Understanding.\n\nConstructing Datasets for Multi-hop Reading Comprehension Across Documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, ACL. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing Datasets for Multi-hop Reading Comprehension Across Documents. In ACL.\n\nTowards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merri\u00ebnboer, Armand Joulin, Tomas Mikolov, ICLR. Jason Weston, Antoine Bordes, Sumit Chopra, Alexan- der M Rush, Bart van Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. Towards AI-Complete Ques- tion Answering: A Set of Prerequisite Toy Tasks. In ICLR.\n\nA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. Adina Williams, Nikita Nangia, Samuel R Bowman, 10.18653/v1/N18-1101Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A Broad-Coverage Challenge Cor- pus for Sentence Understanding through Inference. NACL.\n\nVisual Question Answering: A Survey of Methods and Datasets. Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, Anton Van Den, Hengel, Computer Vision and Image Understanding. ElsevierQi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. 2017. Visual Question Answering: A Survey of Methods and Datasets. In Computer Vision and Image Un- derstanding. Elsevier.\n\nHotpotqa: A Dataset for Diverse, Explainable Multi-hop Question Answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, W William, Ruslan Cohen, Christopher D Salakhutdinov, Manning, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingEMNLPZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A Dataset for Diverse, Explainable Multi-hop Question An- swering. In Proceedings of the Conference on Em- pirical Methods in Natural Language Processing (EMNLP).\n\nSwag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, EMNLP. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In EMNLP.\n\nReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme, arXiv:1810.12885Computing Research Repository. 1Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension. Computing Research Repository, arXiv:1810.12885. Version 1.\n\nAligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: To- wards Story-Like Visual Explanations by Watching Movies and Reading Books. In Proceedings of the IEEE international conference on computer vision, pages 19-27.\n", "annotations": {"author": "[{\"end\":251,\"start\":192},{\"end\":304,\"start\":252},{\"end\":361,\"start\":305},{\"end\":419,\"start\":362},{\"end\":477,\"start\":420},{\"end\":538,\"start\":478},{\"end\":593,\"start\":539}]", "publisher": "[{\"end\":113,\"start\":72},{\"end\":773,\"start\":732}]", "author_last_name": "[{\"end\":209,\"start\":204},{\"end\":262,\"start\":259},{\"end\":319,\"start\":314},{\"end\":377,\"start\":366},{\"end\":435,\"start\":428},{\"end\":496,\"start\":487},{\"end\":551,\"start\":546}]", "author_first_name": "[{\"end\":203,\"start\":192},{\"end\":258,\"start\":252},{\"end\":313,\"start\":305},{\"end\":365,\"start\":362},{\"end\":427,\"start\":420},{\"end\":486,\"start\":478},{\"end\":543,\"start\":539},{\"end\":545,\"start\":544}]", "author_affiliation": "[{\"end\":250,\"start\":211},{\"end\":303,\"start\":264},{\"end\":360,\"start\":321},{\"end\":418,\"start\":379},{\"end\":476,\"start\":437},{\"end\":537,\"start\":498},{\"end\":592,\"start\":553}]", "title": "[{\"end\":71,\"start\":1},{\"end\":664,\"start\":594}]", "venue": "[{\"end\":695,\"start\":666}]", "abstract": "[{\"end\":1733,\"start\":804}]", "bib_ref": "[{\"end\":2480,\"start\":2477},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2829,\"start\":2804},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2849,\"start\":2829},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2868,\"start\":2849},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3875,\"start\":3855},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4170,\"start\":4144},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4540,\"start\":4521},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4943,\"start\":4922},{\"end\":4973,\"start\":4947},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5845,\"start\":5825},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5920,\"start\":5899},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6584,\"start\":6563},{\"end\":7003,\"start\":6988},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7051,\"start\":7031},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7067,\"start\":7051},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7255,\"start\":7231},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7274,\"start\":7255},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7403,\"start\":7384},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7425,\"start\":7403},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7446,\"start\":7425},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7624,\"start\":7602},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7680,\"start\":7658},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8080,\"start\":8057},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8111,\"start\":8090},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8213,\"start\":8189},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8239,\"start\":8213},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8819,\"start\":8800},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8858,\"start\":8833},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8999,\"start\":8978},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9019,\"start\":8999},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9040,\"start\":9019},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9237,\"start\":9215},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9257,\"start\":9237},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9505,\"start\":9486},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9796,\"start\":9770},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10536,\"start\":10510},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17789,\"start\":17770},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18309,\"start\":18286},{\"end\":18340,\"start\":18314},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18727,\"start\":18709},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19947,\"start\":19928},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20033,\"start\":20009},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20199,\"start\":20175},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21663,\"start\":21642},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22107,\"start\":22086},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22127,\"start\":22107},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22148,\"start\":22127},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23396,\"start\":23377},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23486,\"start\":23464},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23611,\"start\":23590},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23881,\"start\":23861},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24104,\"start\":24083},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24190,\"start\":24172},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24288,\"start\":24268},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28071,\"start\":28052},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30451,\"start\":30428},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30590,\"start\":30565},{\"end\":34183,\"start\":34180}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38143,\"start\":38044},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38248,\"start\":38144},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38313,\"start\":38249},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40637,\"start\":38314},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40697,\"start\":40638},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40957,\"start\":40698}]", "paragraph": "[{\"end\":2251,\"start\":1749},{\"end\":3011,\"start\":2253},{\"end\":3788,\"start\":3013},{\"end\":4074,\"start\":3790},{\"end\":5162,\"start\":4076},{\"end\":5731,\"start\":5164},{\"end\":6540,\"start\":5748},{\"end\":6908,\"start\":6542},{\"end\":7136,\"start\":6910},{\"end\":7958,\"start\":7138},{\"end\":9041,\"start\":7960},{\"end\":9451,\"start\":9043},{\"end\":9719,\"start\":9453},{\"end\":10075,\"start\":9721},{\"end\":10448,\"start\":10097},{\"end\":10704,\"start\":10468},{\"end\":11019,\"start\":10706},{\"end\":11211,\"start\":11021},{\"end\":11949,\"start\":11213},{\"end\":12351,\"start\":11951},{\"end\":12830,\"start\":12353},{\"end\":13026,\"start\":12843},{\"end\":13766,\"start\":13049},{\"end\":13886,\"start\":13785},{\"end\":14086,\"start\":13888},{\"end\":14373,\"start\":14088},{\"end\":14990,\"start\":14375},{\"end\":15465,\"start\":14992},{\"end\":16059,\"start\":15488},{\"end\":16699,\"start\":16074},{\"end\":17046,\"start\":16701},{\"end\":17620,\"start\":17048},{\"end\":18224,\"start\":17650},{\"end\":18626,\"start\":18240},{\"end\":19092,\"start\":18628},{\"end\":19505,\"start\":19094},{\"end\":19888,\"start\":19507},{\"end\":20419,\"start\":19890},{\"end\":21253,\"start\":20421},{\"end\":21530,\"start\":21255},{\"end\":21987,\"start\":21532},{\"end\":22350,\"start\":21989},{\"end\":22990,\"start\":22379},{\"end\":23269,\"start\":23008},{\"end\":23987,\"start\":23271},{\"end\":24191,\"start\":23989},{\"end\":24403,\"start\":24193},{\"end\":24787,\"start\":24405},{\"end\":25418,\"start\":24821},{\"end\":25806,\"start\":25448},{\"end\":26127,\"start\":25808},{\"end\":26564,\"start\":26129},{\"end\":26651,\"start\":26566},{\"end\":26887,\"start\":26653},{\"end\":27222,\"start\":26889},{\"end\":27745,\"start\":27224},{\"end\":27992,\"start\":27747},{\"end\":28179,\"start\":27994},{\"end\":28672,\"start\":28211},{\"end\":28915,\"start\":28674},{\"end\":29204,\"start\":28917},{\"end\":29609,\"start\":29226},{\"end\":30720,\"start\":29624},{\"end\":31237,\"start\":30722},{\"end\":31528,\"start\":31239},{\"end\":32298,\"start\":31543},{\"end\":32430,\"start\":32300},{\"end\":32682,\"start\":32432},{\"end\":33158,\"start\":32734},{\"end\":33207,\"start\":33160},{\"end\":33799,\"start\":33209},{\"end\":34144,\"start\":33847},{\"end\":34667,\"start\":34146},{\"end\":34763,\"start\":34669},{\"end\":35507,\"start\":34765},{\"end\":36022,\"start\":35556},{\"end\":36076,\"start\":36024},{\"end\":36309,\"start\":36078},{\"end\":37313,\"start\":36362},{\"end\":37788,\"start\":37370},{\"end\":38043,\"start\":37832}]", "formula": null, "table_ref": "[{\"end\":14085,\"start\":14078},{\"end\":14509,\"start\":14502},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15628,\"start\":15621},{\"end\":25513,\"start\":25506},{\"end\":26273,\"start\":26266},{\"end\":27599,\"start\":27592},{\"end\":28769,\"start\":28762}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1747,\"start\":1735},{\"attributes\":{\"n\":\"2\"},\"end\":5746,\"start\":5734},{\"attributes\":{\"n\":\"3\"},\"end\":10095,\"start\":10078},{\"attributes\":{\"n\":\"3.1\"},\"end\":10466,\"start\":10451},{\"attributes\":{\"n\":\"3.2\"},\"end\":12841,\"start\":12833},{\"attributes\":{\"n\":\"3.3\"},\"end\":13047,\"start\":13029},{\"attributes\":{\"n\":\"3.4\"},\"end\":13783,\"start\":13769},{\"attributes\":{\"n\":\"3.5\"},\"end\":15486,\"start\":15468},{\"attributes\":{\"n\":\"3.6\"},\"end\":16072,\"start\":16062},{\"attributes\":{\"n\":\"4\"},\"end\":17648,\"start\":17623},{\"end\":18238,\"start\":18227},{\"attributes\":{\"n\":\"5\"},\"end\":22360,\"start\":22353},{\"attributes\":{\"n\":\"5.1\"},\"end\":22377,\"start\":22363},{\"attributes\":{\"n\":\"5.2\"},\"end\":23006,\"start\":22993},{\"attributes\":{\"n\":\"5.3\"},\"end\":24819,\"start\":24790},{\"attributes\":{\"n\":\"5.4\"},\"end\":25446,\"start\":25421},{\"attributes\":{\"n\":\"5.5\"},\"end\":28209,\"start\":28182},{\"attributes\":{\"n\":\"5.6\"},\"end\":29224,\"start\":29207},{\"attributes\":{\"n\":\"5.7\"},\"end\":29622,\"start\":29612},{\"attributes\":{\"n\":\"6\"},\"end\":31541,\"start\":31531},{\"end\":32732,\"start\":32685},{\"end\":33845,\"start\":33802},{\"end\":35554,\"start\":35510},{\"end\":36360,\"start\":36312},{\"end\":37368,\"start\":37316},{\"end\":37830,\"start\":37791},{\"end\":38155,\"start\":38145},{\"end\":38260,\"start\":38250},{\"end\":40648,\"start\":40639}]", "table": "[{\"end\":40957,\"start\":40785}]", "figure_caption": "[{\"end\":38143,\"start\":38046},{\"end\":38248,\"start\":38157},{\"end\":38313,\"start\":38262},{\"end\":40637,\"start\":38316},{\"end\":40697,\"start\":40650},{\"end\":40785,\"start\":40700}]", "figure_ref": "[{\"end\":3375,\"start\":3367},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29237,\"start\":29229}]", "bib_author_first_name": "[{\"end\":42609,\"start\":42600},{\"end\":42626,\"start\":42617},{\"end\":42642,\"start\":42636},{\"end\":42655,\"start\":42647},{\"end\":42671,\"start\":42666},{\"end\":42687,\"start\":42679},{\"end\":42701,\"start\":42697},{\"end\":43126,\"start\":43121},{\"end\":43144,\"start\":43139},{\"end\":43155,\"start\":43152},{\"end\":43169,\"start\":43163},{\"end\":43396,\"start\":43391},{\"end\":43414,\"start\":43409},{\"end\":43425,\"start\":43422},{\"end\":43439,\"start\":43433},{\"end\":43673,\"start\":43668},{\"end\":43702,\"start\":43691},{\"end\":43724,\"start\":43711},{\"end\":43977,\"start\":43973},{\"end\":43991,\"start\":43984},{\"end\":44004,\"start\":43997},{\"end\":44013,\"start\":44011},{\"end\":44022,\"start\":44019},{\"end\":44035,\"start\":44030},{\"end\":44211,\"start\":44205},{\"end\":44220,\"start\":44218},{\"end\":44230,\"start\":44225},{\"end\":44242,\"start\":44238},{\"end\":44258,\"start\":44252},{\"end\":44269,\"start\":44264},{\"end\":44281,\"start\":44276},{\"end\":44293,\"start\":44289},{\"end\":44592,\"start\":44586},{\"end\":44607,\"start\":44602},{\"end\":44834,\"start\":44826},{\"end\":44850,\"start\":44844},{\"end\":44861,\"start\":44856},{\"end\":45057,\"start\":45052},{\"end\":45074,\"start\":45066},{\"end\":45088,\"start\":45082},{\"end\":45102,\"start\":45094},{\"end\":45478,\"start\":45475},{\"end\":45494,\"start\":45489},{\"end\":45508,\"start\":45504},{\"end\":45661,\"start\":45655},{\"end\":45685,\"start\":45681},{\"end\":45702,\"start\":45699},{\"end\":45725,\"start\":45719},{\"end\":46086,\"start\":46079},{\"end\":46097,\"start\":46091},{\"end\":46108,\"start\":46104},{\"end\":46119,\"start\":46116},{\"end\":46130,\"start\":46126},{\"end\":46366,\"start\":46361},{\"end\":46377,\"start\":46372},{\"end\":46620,\"start\":46614},{\"end\":46634,\"start\":46628},{\"end\":46642,\"start\":46641},{\"end\":46655,\"start\":46651},{\"end\":46943,\"start\":46937},{\"end\":46956,\"start\":46950},{\"end\":46973,\"start\":46968},{\"end\":47201,\"start\":47200},{\"end\":47217,\"start\":47212},{\"end\":47326,\"start\":47323},{\"end\":47350,\"start\":47340},{\"end\":47367,\"start\":47361},{\"end\":47385,\"start\":47378},{\"end\":47400,\"start\":47395},{\"end\":47414,\"start\":47409},{\"end\":47432,\"start\":47424},{\"end\":47447,\"start\":47442},{\"end\":47467,\"start\":47460},{\"end\":47481,\"start\":47476},{\"end\":48115,\"start\":48109},{\"end\":48126,\"start\":48121},{\"end\":48139,\"start\":48132},{\"end\":48151,\"start\":48145},{\"end\":48164,\"start\":48158},{\"end\":48353,\"start\":48347},{\"end\":48366,\"start\":48361},{\"end\":48379,\"start\":48376},{\"end\":48841,\"start\":48836},{\"end\":48857,\"start\":48852},{\"end\":48871,\"start\":48865},{\"end\":48884,\"start\":48878},{\"end\":49139,\"start\":49134},{\"end\":49156,\"start\":49149},{\"end\":49169,\"start\":49164},{\"end\":49191,\"start\":49182},{\"end\":49207,\"start\":49201},{\"end\":49633,\"start\":49630},{\"end\":49645,\"start\":49642},{\"end\":49660,\"start\":49657},{\"end\":49675,\"start\":49667},{\"end\":49688,\"start\":49681},{\"end\":49703,\"start\":49697},{\"end\":49716,\"start\":49714},{\"end\":50081,\"start\":50080},{\"end\":50094,\"start\":50089},{\"end\":50111,\"start\":50103},{\"end\":50128,\"start\":50123},{\"end\":50363,\"start\":50362},{\"end\":50377,\"start\":50373},{\"end\":50391,\"start\":50386},{\"end\":50405,\"start\":50401},{\"end\":50424,\"start\":50413},{\"end\":50440,\"start\":50434},{\"end\":50452,\"start\":50448},{\"end\":50674,\"start\":50669},{\"end\":50690,\"start\":50682},{\"end\":50706,\"start\":50698},{\"end\":51157,\"start\":51153},{\"end\":51175,\"start\":51166},{\"end\":51190,\"start\":51184},{\"end\":51207,\"start\":51201},{\"end\":51217,\"start\":51212},{\"end\":51232,\"start\":51227},{\"end\":51239,\"start\":51233},{\"end\":51255,\"start\":51247},{\"end\":51573,\"start\":51569},{\"end\":51590,\"start\":51583},{\"end\":51779,\"start\":51773},{\"end\":51796,\"start\":51791},{\"end\":51807,\"start\":51802},{\"end\":52067,\"start\":52061},{\"end\":52083,\"start\":52079},{\"end\":52101,\"start\":52091},{\"end\":52116,\"start\":52111},{\"end\":52350,\"start\":52346},{\"end\":52363,\"start\":52358},{\"end\":52383,\"start\":52370},{\"end\":52604,\"start\":52597},{\"end\":52616,\"start\":52613},{\"end\":52633,\"start\":52626},{\"end\":52647,\"start\":52641},{\"end\":52658,\"start\":52655},{\"end\":52676,\"start\":52672},{\"end\":52695,\"start\":52686},{\"end\":52715,\"start\":52706},{\"end\":53020,\"start\":53013},{\"end\":53035,\"start\":53026},{\"end\":53049,\"start\":53046},{\"end\":53067,\"start\":53059},{\"end\":53320,\"start\":53316},{\"end\":53336,\"start\":53327},{\"end\":53350,\"start\":53344},{\"end\":53365,\"start\":53360},{\"end\":53376,\"start\":53372},{\"end\":53389,\"start\":53383},{\"end\":53666,\"start\":53658},{\"end\":53680,\"start\":53674},{\"end\":53701,\"start\":53692},{\"end\":53940,\"start\":53935},{\"end\":53956,\"start\":53949},{\"end\":53970,\"start\":53965},{\"end\":53988,\"start\":53979},{\"end\":53990,\"start\":53989},{\"end\":54001,\"start\":53997},{\"end\":54025,\"start\":54019},{\"end\":54039,\"start\":54034},{\"end\":54345,\"start\":54340},{\"end\":54362,\"start\":54356},{\"end\":54379,\"start\":54371},{\"end\":54620,\"start\":54618},{\"end\":54631,\"start\":54625},{\"end\":54643,\"start\":54639},{\"end\":54657,\"start\":54650},{\"end\":54671,\"start\":54664},{\"end\":54683,\"start\":54678},{\"end\":55041,\"start\":55035},{\"end\":55052,\"start\":55048},{\"end\":55065,\"start\":55057},{\"end\":55079,\"start\":55073},{\"end\":55089,\"start\":55088},{\"end\":55105,\"start\":55099},{\"end\":55126,\"start\":55113},{\"end\":55687,\"start\":55682},{\"end\":55704,\"start\":55697},{\"end\":55714,\"start\":55711},{\"end\":55730,\"start\":55725},{\"end\":55987,\"start\":55982},{\"end\":56003,\"start\":55995},{\"end\":56017,\"start\":56009},{\"end\":56031,\"start\":56023},{\"end\":56042,\"start\":56037},{\"end\":56056,\"start\":56048},{\"end\":56468,\"start\":56463},{\"end\":56478,\"start\":56474},{\"end\":56490,\"start\":56486},{\"end\":56504,\"start\":56498},{\"end\":56526,\"start\":56520},{\"end\":56543,\"start\":56536},{\"end\":56559,\"start\":56554}]", "bib_author_last_name": "[{\"end\":42615,\"start\":42610},{\"end\":42634,\"start\":42627},{\"end\":42645,\"start\":42643},{\"end\":42664,\"start\":42656},{\"end\":42677,\"start\":42672},{\"end\":42695,\"start\":42688},{\"end\":42708,\"start\":42702},{\"end\":43137,\"start\":43127},{\"end\":43150,\"start\":43145},{\"end\":43161,\"start\":43156},{\"end\":43181,\"start\":43170},{\"end\":43407,\"start\":43397},{\"end\":43420,\"start\":43415},{\"end\":43431,\"start\":43426},{\"end\":43451,\"start\":43440},{\"end\":43689,\"start\":43674},{\"end\":43709,\"start\":43703},{\"end\":43730,\"start\":43725},{\"end\":43739,\"start\":43732},{\"end\":43982,\"start\":43978},{\"end\":43995,\"start\":43992},{\"end\":44009,\"start\":44005},{\"end\":44017,\"start\":44014},{\"end\":44028,\"start\":44023},{\"end\":44042,\"start\":44036},{\"end\":44216,\"start\":44212},{\"end\":44223,\"start\":44221},{\"end\":44236,\"start\":44231},{\"end\":44250,\"start\":44243},{\"end\":44262,\"start\":44259},{\"end\":44274,\"start\":44270},{\"end\":44287,\"start\":44282},{\"end\":44305,\"start\":44294},{\"end\":44600,\"start\":44593},{\"end\":44613,\"start\":44608},{\"end\":44842,\"start\":44835},{\"end\":44854,\"start\":44851},{\"end\":44867,\"start\":44862},{\"end\":45064,\"start\":45058},{\"end\":45080,\"start\":45075},{\"end\":45092,\"start\":45089},{\"end\":45112,\"start\":45103},{\"end\":45487,\"start\":45479},{\"end\":45502,\"start\":45495},{\"end\":45517,\"start\":45509},{\"end\":45679,\"start\":45662},{\"end\":45697,\"start\":45686},{\"end\":45707,\"start\":45703},{\"end\":45717,\"start\":45709},{\"end\":45741,\"start\":45726},{\"end\":45748,\"start\":45743},{\"end\":46089,\"start\":46087},{\"end\":46102,\"start\":46098},{\"end\":46114,\"start\":46109},{\"end\":46124,\"start\":46120},{\"end\":46135,\"start\":46131},{\"end\":46370,\"start\":46367},{\"end\":46383,\"start\":46378},{\"end\":46626,\"start\":46621},{\"end\":46639,\"start\":46635},{\"end\":46649,\"start\":46643},{\"end\":46660,\"start\":46656},{\"end\":46673,\"start\":46662},{\"end\":46948,\"start\":46944},{\"end\":46966,\"start\":46957},{\"end\":46979,\"start\":46974},{\"end\":47210,\"start\":47202},{\"end\":47224,\"start\":47218},{\"end\":47228,\"start\":47226},{\"end\":47338,\"start\":47327},{\"end\":47359,\"start\":47351},{\"end\":47376,\"start\":47368},{\"end\":47393,\"start\":47386},{\"end\":47407,\"start\":47401},{\"end\":47422,\"start\":47415},{\"end\":47440,\"start\":47433},{\"end\":47458,\"start\":47448},{\"end\":47474,\"start\":47468},{\"end\":47488,\"start\":47482},{\"end\":48119,\"start\":48116},{\"end\":48130,\"start\":48127},{\"end\":48143,\"start\":48140},{\"end\":48156,\"start\":48152},{\"end\":48169,\"start\":48165},{\"end\":48359,\"start\":48354},{\"end\":48374,\"start\":48367},{\"end\":48386,\"start\":48380},{\"end\":48850,\"start\":48842},{\"end\":48863,\"start\":48858},{\"end\":48876,\"start\":48872},{\"end\":48894,\"start\":48885},{\"end\":49147,\"start\":49140},{\"end\":49162,\"start\":49157},{\"end\":49180,\"start\":49170},{\"end\":49199,\"start\":49192},{\"end\":49214,\"start\":49208},{\"end\":49640,\"start\":49634},{\"end\":49655,\"start\":49646},{\"end\":49665,\"start\":49661},{\"end\":49679,\"start\":49676},{\"end\":49695,\"start\":49689},{\"end\":49712,\"start\":49704},{\"end\":49721,\"start\":49717},{\"end\":50087,\"start\":50082},{\"end\":50101,\"start\":50095},{\"end\":50121,\"start\":50112},{\"end\":50132,\"start\":50129},{\"end\":50143,\"start\":50134},{\"end\":50371,\"start\":50364},{\"end\":50384,\"start\":50378},{\"end\":50399,\"start\":50392},{\"end\":50411,\"start\":50406},{\"end\":50432,\"start\":50425},{\"end\":50446,\"start\":50441},{\"end\":50456,\"start\":50453},{\"end\":50469,\"start\":50458},{\"end\":50680,\"start\":50675},{\"end\":50696,\"start\":50691},{\"end\":50713,\"start\":50707},{\"end\":51164,\"start\":51158},{\"end\":51182,\"start\":51176},{\"end\":51199,\"start\":51191},{\"end\":51210,\"start\":51208},{\"end\":51225,\"start\":51218},{\"end\":51245,\"start\":51240},{\"end\":51265,\"start\":51256},{\"end\":51581,\"start\":51574},{\"end\":51601,\"start\":51591},{\"end\":51789,\"start\":51780},{\"end\":51800,\"start\":51797},{\"end\":51813,\"start\":51808},{\"end\":52077,\"start\":52068},{\"end\":52089,\"start\":52084},{\"end\":52109,\"start\":52102},{\"end\":52122,\"start\":52117},{\"end\":52356,\"start\":52351},{\"end\":52368,\"start\":52364},{\"end\":52391,\"start\":52384},{\"end\":52611,\"start\":52605},{\"end\":52624,\"start\":52617},{\"end\":52639,\"start\":52634},{\"end\":52653,\"start\":52648},{\"end\":52670,\"start\":52659},{\"end\":52684,\"start\":52677},{\"end\":52704,\"start\":52696},{\"end\":52722,\"start\":52716},{\"end\":53024,\"start\":53021},{\"end\":53044,\"start\":53036},{\"end\":53057,\"start\":53050},{\"end\":53078,\"start\":53068},{\"end\":53325,\"start\":53321},{\"end\":53342,\"start\":53337},{\"end\":53358,\"start\":53351},{\"end\":53370,\"start\":53366},{\"end\":53381,\"start\":53377},{\"end\":53396,\"start\":53390},{\"end\":53672,\"start\":53667},{\"end\":53690,\"start\":53681},{\"end\":53708,\"start\":53702},{\"end\":53947,\"start\":53941},{\"end\":53963,\"start\":53957},{\"end\":53977,\"start\":53971},{\"end\":53995,\"start\":53991},{\"end\":54017,\"start\":54002},{\"end\":54032,\"start\":54026},{\"end\":54047,\"start\":54040},{\"end\":54354,\"start\":54346},{\"end\":54369,\"start\":54363},{\"end\":54386,\"start\":54380},{\"end\":54623,\"start\":54621},{\"end\":54637,\"start\":54632},{\"end\":54648,\"start\":54644},{\"end\":54662,\"start\":54658},{\"end\":54676,\"start\":54672},{\"end\":54691,\"start\":54684},{\"end\":54699,\"start\":54693},{\"end\":55046,\"start\":55042},{\"end\":55055,\"start\":55053},{\"end\":55071,\"start\":55066},{\"end\":55086,\"start\":55080},{\"end\":55097,\"start\":55090},{\"end\":55111,\"start\":55106},{\"end\":55140,\"start\":55127},{\"end\":55149,\"start\":55142},{\"end\":55695,\"start\":55688},{\"end\":55709,\"start\":55705},{\"end\":55723,\"start\":55715},{\"end\":55735,\"start\":55731},{\"end\":55993,\"start\":55988},{\"end\":56007,\"start\":56004},{\"end\":56021,\"start\":56018},{\"end\":56035,\"start\":56032},{\"end\":56046,\"start\":56043},{\"end\":56066,\"start\":56057},{\"end\":56472,\"start\":56469},{\"end\":56484,\"start\":56479},{\"end\":56496,\"start\":56491},{\"end\":56518,\"start\":56505},{\"end\":56534,\"start\":56527},{\"end\":56552,\"start\":56544},{\"end\":56566,\"start\":56560}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3180429},\"end\":43060,\"start\":42568},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":858065},\"end\":43328,\"start\":43062},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5791809},\"end\":43600,\"start\":43330},{\"attributes\":{\"doi\":\"10.18653/v1/D15-1075\",\"id\":\"b3\"},\"end\":43925,\"start\":43602},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1152\",\"id\":\"b4\",\"matched_paper_id\":34032948},\"end\":44203,\"start\":43927},{\"attributes\":{\"id\":\"b5\"},\"end\":44512,\"start\":44205},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3932228},\"end\":44741,\"start\":44514},{\"attributes\":{\"doi\":\"arXiv:1809.02922\",\"id\":\"b7\"},\"end\":45050,\"start\":44743},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b8\"},\"end\":45397,\"start\":45052},{\"attributes\":{\"id\":\"b9\"},\"end\":45653,\"start\":45399},{\"attributes\":{\"doi\":\"10.18653/v1/N18-2017\",\"id\":\"b10\"},\"end\":46004,\"start\":45655},{\"attributes\":{\"id\":\"b11\"},\"end\":46292,\"start\":46006},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1215\",\"id\":\"b12\",\"matched_paper_id\":7228830},\"end\":46522,\"start\":46294},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1147\",\"id\":\"b13\"},\"end\":46864,\"start\":46524},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":24462950},\"end\":47154,\"start\":46866},{\"attributes\":{\"id\":\"b15\"},\"end\":47321,\"start\":47156},{\"attributes\":{\"id\":\"b16\"},\"end\":48040,\"start\":47323},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1082\",\"id\":\"b17\",\"matched_paper_id\":6826032},\"end\":48345,\"start\":48042},{\"attributes\":{\"doi\":\"arXiv:1902.01007.Ver-sion1\",\"id\":\"b18\"},\"end\":48745,\"start\":48347},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52183757},\"end\":49073,\"start\":48747},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":19426100},\"end\":49628,\"start\":49075},{\"attributes\":{\"doi\":\"arXiv:1611.09268\",\"id\":\"b21\"},\"end\":50015,\"start\":49630},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1244\",\"id\":\"b22\",\"matched_paper_id\":8495258},\"end\":50318,\"start\":50017},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1202\",\"id\":\"b23\",\"matched_paper_id\":3626819},\"end\":50667,\"start\":50320},{\"attributes\":{\"doi\":\"arXiv:1811.01088\",\"id\":\"b24\"},\"end\":51056,\"start\":50669},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52123220},\"end\":51506,\"start\":51058},{\"attributes\":{\"id\":\"b26\"},\"end\":51771,\"start\":51508},{\"attributes\":{\"id\":\"b27\"},\"end\":51998,\"start\":51773},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1264\",\"id\":\"b28\",\"matched_paper_id\":11816014},\"end\":52291,\"start\":52000},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52055325},\"end\":52519,\"start\":52293},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52165754},\"end\":52955,\"start\":52521},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8535316},\"end\":53227,\"start\":52957},{\"attributes\":{\"id\":\"b32\"},\"end\":53580,\"start\":53229},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":9192723},\"end\":53860,\"start\":53582},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3178759},\"end\":54258,\"start\":53862},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1101\",\"id\":\"b35\"},\"end\":54555,\"start\":54260},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":11746788},\"end\":54958,\"start\":54557},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52822214},\"end\":55604,\"start\":54960},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52019251},\"end\":55894,\"start\":55606},{\"attributes\":{\"doi\":\"arXiv:1810.12885\",\"id\":\"b39\",\"matched_paper_id\":53116244},\"end\":56357,\"start\":55896},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6866988},\"end\":56996,\"start\":56359}]", "bib_title": "[{\"end\":42598,\"start\":42568},{\"end\":43119,\"start\":43062},{\"end\":43389,\"start\":43330},{\"end\":43971,\"start\":43927},{\"end\":44584,\"start\":44514},{\"end\":46359,\"start\":46294},{\"end\":46935,\"start\":46866},{\"end\":48107,\"start\":48042},{\"end\":48834,\"start\":48747},{\"end\":49132,\"start\":49075},{\"end\":50078,\"start\":50017},{\"end\":50360,\"start\":50320},{\"end\":51151,\"start\":51058},{\"end\":52059,\"start\":52000},{\"end\":52344,\"start\":52293},{\"end\":52595,\"start\":52521},{\"end\":53011,\"start\":52957},{\"end\":53656,\"start\":53582},{\"end\":53933,\"start\":53862},{\"end\":54616,\"start\":54557},{\"end\":55033,\"start\":54960},{\"end\":55680,\"start\":55606},{\"end\":55980,\"start\":55896},{\"end\":56461,\"start\":56359}]", "bib_author": "[{\"end\":42617,\"start\":42600},{\"end\":42636,\"start\":42617},{\"end\":42647,\"start\":42636},{\"end\":42666,\"start\":42647},{\"end\":42679,\"start\":42666},{\"end\":42697,\"start\":42679},{\"end\":42710,\"start\":42697},{\"end\":43139,\"start\":43121},{\"end\":43152,\"start\":43139},{\"end\":43163,\"start\":43152},{\"end\":43183,\"start\":43163},{\"end\":43409,\"start\":43391},{\"end\":43422,\"start\":43409},{\"end\":43433,\"start\":43422},{\"end\":43453,\"start\":43433},{\"end\":43691,\"start\":43668},{\"end\":43711,\"start\":43691},{\"end\":43732,\"start\":43711},{\"end\":43741,\"start\":43732},{\"end\":43984,\"start\":43973},{\"end\":43997,\"start\":43984},{\"end\":44011,\"start\":43997},{\"end\":44019,\"start\":44011},{\"end\":44030,\"start\":44019},{\"end\":44044,\"start\":44030},{\"end\":44218,\"start\":44205},{\"end\":44225,\"start\":44218},{\"end\":44238,\"start\":44225},{\"end\":44252,\"start\":44238},{\"end\":44264,\"start\":44252},{\"end\":44276,\"start\":44264},{\"end\":44289,\"start\":44276},{\"end\":44307,\"start\":44289},{\"end\":44602,\"start\":44586},{\"end\":44615,\"start\":44602},{\"end\":44844,\"start\":44826},{\"end\":44856,\"start\":44844},{\"end\":44869,\"start\":44856},{\"end\":45066,\"start\":45052},{\"end\":45082,\"start\":45066},{\"end\":45094,\"start\":45082},{\"end\":45114,\"start\":45094},{\"end\":45489,\"start\":45475},{\"end\":45504,\"start\":45489},{\"end\":45519,\"start\":45504},{\"end\":45681,\"start\":45655},{\"end\":45699,\"start\":45681},{\"end\":45709,\"start\":45699},{\"end\":45719,\"start\":45709},{\"end\":45743,\"start\":45719},{\"end\":45750,\"start\":45743},{\"end\":46091,\"start\":46079},{\"end\":46104,\"start\":46091},{\"end\":46116,\"start\":46104},{\"end\":46126,\"start\":46116},{\"end\":46137,\"start\":46126},{\"end\":46372,\"start\":46361},{\"end\":46385,\"start\":46372},{\"end\":46628,\"start\":46614},{\"end\":46641,\"start\":46628},{\"end\":46651,\"start\":46641},{\"end\":46662,\"start\":46651},{\"end\":46675,\"start\":46662},{\"end\":46950,\"start\":46937},{\"end\":46968,\"start\":46950},{\"end\":46981,\"start\":46968},{\"end\":47212,\"start\":47200},{\"end\":47226,\"start\":47212},{\"end\":47230,\"start\":47226},{\"end\":47340,\"start\":47323},{\"end\":47361,\"start\":47340},{\"end\":47378,\"start\":47361},{\"end\":47395,\"start\":47378},{\"end\":47409,\"start\":47395},{\"end\":47424,\"start\":47409},{\"end\":47442,\"start\":47424},{\"end\":47460,\"start\":47442},{\"end\":47476,\"start\":47460},{\"end\":47490,\"start\":47476},{\"end\":48121,\"start\":48109},{\"end\":48132,\"start\":48121},{\"end\":48145,\"start\":48132},{\"end\":48158,\"start\":48145},{\"end\":48171,\"start\":48158},{\"end\":48361,\"start\":48347},{\"end\":48376,\"start\":48361},{\"end\":48388,\"start\":48376},{\"end\":48852,\"start\":48836},{\"end\":48865,\"start\":48852},{\"end\":48878,\"start\":48865},{\"end\":48896,\"start\":48878},{\"end\":49149,\"start\":49134},{\"end\":49164,\"start\":49149},{\"end\":49182,\"start\":49164},{\"end\":49201,\"start\":49182},{\"end\":49216,\"start\":49201},{\"end\":49642,\"start\":49630},{\"end\":49657,\"start\":49642},{\"end\":49667,\"start\":49657},{\"end\":49681,\"start\":49667},{\"end\":49697,\"start\":49681},{\"end\":49714,\"start\":49697},{\"end\":49723,\"start\":49714},{\"end\":50089,\"start\":50080},{\"end\":50103,\"start\":50089},{\"end\":50123,\"start\":50103},{\"end\":50134,\"start\":50123},{\"end\":50145,\"start\":50134},{\"end\":50373,\"start\":50362},{\"end\":50386,\"start\":50373},{\"end\":50401,\"start\":50386},{\"end\":50413,\"start\":50401},{\"end\":50434,\"start\":50413},{\"end\":50448,\"start\":50434},{\"end\":50458,\"start\":50448},{\"end\":50471,\"start\":50458},{\"end\":50682,\"start\":50669},{\"end\":50698,\"start\":50682},{\"end\":50715,\"start\":50698},{\"end\":51166,\"start\":51153},{\"end\":51184,\"start\":51166},{\"end\":51201,\"start\":51184},{\"end\":51212,\"start\":51201},{\"end\":51227,\"start\":51212},{\"end\":51247,\"start\":51227},{\"end\":51267,\"start\":51247},{\"end\":51583,\"start\":51569},{\"end\":51603,\"start\":51583},{\"end\":51791,\"start\":51773},{\"end\":51802,\"start\":51791},{\"end\":51815,\"start\":51802},{\"end\":52079,\"start\":52061},{\"end\":52091,\"start\":52079},{\"end\":52111,\"start\":52091},{\"end\":52124,\"start\":52111},{\"end\":52358,\"start\":52346},{\"end\":52370,\"start\":52358},{\"end\":52393,\"start\":52370},{\"end\":52613,\"start\":52597},{\"end\":52626,\"start\":52613},{\"end\":52641,\"start\":52626},{\"end\":52655,\"start\":52641},{\"end\":52672,\"start\":52655},{\"end\":52686,\"start\":52672},{\"end\":52706,\"start\":52686},{\"end\":52724,\"start\":52706},{\"end\":53026,\"start\":53013},{\"end\":53046,\"start\":53026},{\"end\":53059,\"start\":53046},{\"end\":53080,\"start\":53059},{\"end\":53327,\"start\":53316},{\"end\":53344,\"start\":53327},{\"end\":53360,\"start\":53344},{\"end\":53372,\"start\":53360},{\"end\":53383,\"start\":53372},{\"end\":53398,\"start\":53383},{\"end\":53674,\"start\":53658},{\"end\":53692,\"start\":53674},{\"end\":53710,\"start\":53692},{\"end\":53949,\"start\":53935},{\"end\":53965,\"start\":53949},{\"end\":53979,\"start\":53965},{\"end\":53997,\"start\":53979},{\"end\":54019,\"start\":53997},{\"end\":54034,\"start\":54019},{\"end\":54049,\"start\":54034},{\"end\":54356,\"start\":54340},{\"end\":54371,\"start\":54356},{\"end\":54388,\"start\":54371},{\"end\":54625,\"start\":54618},{\"end\":54639,\"start\":54625},{\"end\":54650,\"start\":54639},{\"end\":54664,\"start\":54650},{\"end\":54678,\"start\":54664},{\"end\":54693,\"start\":54678},{\"end\":54701,\"start\":54693},{\"end\":55048,\"start\":55035},{\"end\":55057,\"start\":55048},{\"end\":55073,\"start\":55057},{\"end\":55088,\"start\":55073},{\"end\":55099,\"start\":55088},{\"end\":55113,\"start\":55099},{\"end\":55142,\"start\":55113},{\"end\":55151,\"start\":55142},{\"end\":55697,\"start\":55682},{\"end\":55711,\"start\":55697},{\"end\":55725,\"start\":55711},{\"end\":55737,\"start\":55725},{\"end\":55995,\"start\":55982},{\"end\":56009,\"start\":55995},{\"end\":56023,\"start\":56009},{\"end\":56037,\"start\":56023},{\"end\":56048,\"start\":56037},{\"end\":56068,\"start\":56048},{\"end\":56474,\"start\":56463},{\"end\":56486,\"start\":56474},{\"end\":56498,\"start\":56486},{\"end\":56520,\"start\":56498},{\"end\":56536,\"start\":56520},{\"end\":56554,\"start\":56536},{\"end\":56568,\"start\":56554}]", "bib_venue": "[{\"end\":42777,\"start\":42710},{\"end\":43186,\"start\":43183},{\"end\":43456,\"start\":43453},{\"end\":43666,\"start\":43602},{\"end\":44067,\"start\":44064},{\"end\":44349,\"start\":44307},{\"end\":44619,\"start\":44615},{\"end\":44824,\"start\":44743},{\"end\":45210,\"start\":45130},{\"end\":45473,\"start\":45399},{\"end\":45832,\"start\":45770},{\"end\":46077,\"start\":46006},{\"end\":46410,\"start\":46405},{\"end\":46612,\"start\":46524},{\"end\":47000,\"start\":46981},{\"end\":47198,\"start\":47156},{\"end\":47558,\"start\":47490},{\"end\":48196,\"start\":48191},{\"end\":48535,\"start\":48414},{\"end\":48901,\"start\":48896},{\"end\":49302,\"start\":49216},{\"end\":49804,\"start\":49739},{\"end\":50170,\"start\":50165},{\"end\":50496,\"start\":50491},{\"end\":50848,\"start\":50731},{\"end\":51272,\"start\":51267},{\"end\":51567,\"start\":51508},{\"end\":51878,\"start\":51815},{\"end\":52149,\"start\":52144},{\"end\":52397,\"start\":52393},{\"end\":52729,\"start\":52724},{\"end\":53084,\"start\":53080},{\"end\":53314,\"start\":53229},{\"end\":53713,\"start\":53710},{\"end\":54053,\"start\":54049},{\"end\":54338,\"start\":54260},{\"end\":54740,\"start\":54701},{\"end\":55232,\"start\":55151},{\"end\":55742,\"start\":55737},{\"end\":56113,\"start\":56084},{\"end\":56635,\"start\":56568},{\"end\":42831,\"start\":42779},{\"end\":47006,\"start\":47002},{\"end\":47675,\"start\":47665},{\"end\":49375,\"start\":49304},{\"end\":55300,\"start\":55234},{\"end\":56689,\"start\":56637}]"}}}, "year": 2023, "month": 12, "day": 17}