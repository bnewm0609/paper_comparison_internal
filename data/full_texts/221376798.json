{"id": 221376798, "updated": "2023-11-08 02:12:31.35", "metadata": {"title": "Unpaired Learning of Deep Image Denoising", "authors": "[{\"first\":\"Xiaohe\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Ming\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Dongwei\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Wangmeng\",\"last\":\"Zuo\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": 8, "day": 31}, "abstract": "We investigate the task of learning blind image denoising networks from an unpaired set of clean and noisy images. Such problem setting generally is practical and valuable considering that it is feasible to collect unpaired noisy and clean images in most real-world applications. And we further assume that the noise can be signal dependent but is spatially uncorrelated. In order to facilitate unpaired learning of denoising network, this paper presents a two-stage scheme by incorporating self-supervised learning and knowledge distillation. For self-supervised learning, we suggest a dilated blind-spot network (D-BSN) to learn denoising solely from real noisy images. Due to the spatial independence of noise, we adopt a network by stacking 1x1 convolution layers to estimate the noise level map for each image. Both the D-BSN and image-specific noise model (CNN\\_est) can be jointly trained via maximizing the constrained log-likelihood. Given the output of D-BSN and estimated noise level map, improved denoising performance can be further obtained based on the Bayes' rule. As for knowledge distillation, we first apply the learned noise models to clean images to synthesize a paired set of training images, and use the real noisy images and the corresponding denoising results in the first stage to form another paired set. Then, the ultimate denoising model can be distilled by training an existing denoising network using these two paired sets. Experiments show that our unpaired learning method performs favorably on both synthetic noisy images and real-world noisy photographs in terms of quantitative and qualitative evaluation.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2008.13711", "mag": "3097262261", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/Wu0CRZ20", "doi": "10.1007/978-3-030-58548-8_21"}}, "content": {"source": {"pdf_hash": "d30920e7619a5016f9efe8c7d1d720920a0bd058", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.13711v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.13711", "status": "GREEN"}}, "grobid": {"id": "210f0d6b566b42effa9c8f333f8f5a9c1ef9efa4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d30920e7619a5016f9efe8c7d1d720920a0bd058.txt", "contents": "\nUnpaired Learning of Deep Image Denoising\n\n\nXiaohe Wu \nHarbin Institute of Technology\nChina\n\nMing Liu \nHarbin Institute of Technology\nChina\n\nYue Cao \nHarbin Institute of Technology\nChina\n\nDongwei Ren rendongweihit@gmail.com \nUniversity of Tianjin\nChina\n\nWangmeng Zuo wmzuo@hit.edu.com \nHarbin Institute of Technology\nChina\n\nPeng Cheng Lab\nChina\n\nUnpaired Learning of Deep Image Denoising\nImage denoisingunpaired learningconvolutional networksself-supervised learning\nWe investigate the task of learning blind image denoising networks from an unpaired set of clean and noisy images. Such problem setting generally is practical and valuable considering that it is feasible to collect unpaired noisy and clean images in most real-world applications. And we further assume that the noise can be signal dependent but is spatially uncorrelated. In order to facilitate unpaired learning of denoising network, this paper presents a two-stage scheme by incorporating selfsupervised learning and knowledge distillation. For self-supervised learning, we suggest a dilated blind-spot network (D-BSN) to learn denoising solely from real noisy images. Due to the spatial independence of noise, we adopt a network by stacking 1 \u00d7 1 convolution layers to estimate the noise level map for each image. Both the D-BSN and image-specific noise model (CNNest) can be jointly trained via maximizing the constrained log-likelihood. Given the output of D-BSN and estimated noise level map, improved denoising performance can be further obtained based on the Bayes' rule. As for knowledge distillation, we first apply the learned noise models to clean images to synthesize a paired set of training images, and use the real noisy images and the corresponding denoising results in the first stage to form another paired set. Then, the ultimate denoising model can be distilled by training an existing denoising network using these two paired sets. Experiments show that our unpaired learning method performs favorably on both synthetic noisy images and real-world noisy photographs in terms of quantitative and qualitative evaluation. Code is available at https://github.com/XHWXD/DBSN.\n\nIntroduction\n [28]\n), Noise2Noise [26] (MWCNN(N2N)), Noise2Void (N2V [22]), Self-supervised learning (Laine19 [24]), and our unpaired learning scheme.\n\nN3Net [35], and NLRN [27], have been presented and achieved noteworthy improvement in denoising performance against traditional methods such as BM3D [9] and WNNM [14]. Subsequently, attempts have been made to apply CNN denoisers for handling more sophisticated types of image noise [18,36] as well as removing noise from real-world noisy photographs [2,6,8,15].\n\nAlbeit breakthrough performance has been achieved, the success of most existing CNN denoisers heavily depend on supervised learning with large amount of paired noisy-clean images [2,6,15,31,39,41,43]. On the one hand, given the form and parameters of noise model, one can synthesize noisy images from noiseless clean images to constitute a paired training set. However, real noise usually is complex, and the in-camera signal processing (ISP) pipeline in real-world photography further increases the complexity of noise, making it difficult to be fully characterized by basic parametric noise model. On the other hand, one can build the paired set by designing suitable approaches to acquire the nearly noise-free (or clean) image corresponding to a given real noisy image. For real-world photography, nearly noise-free images can be acquired by averaging multiple noisy images [2,32] or by aligning and post-processing low ISO images [34]. Unfortunately, the nearly noise-free images may suffer from over-smoothing issue and are cost-expensive to acquire. Moreover, such nearly noise-free image acquisition may not be applicable to other imaging mechanisms (e.g., microscopy or medical imaging), making it yet a challenging problem for acquiring noisy-clean image pairs for other imaging mechanisms.\n\nInstead of supervised learning with paired training set, Lehtinen et al. [26] suggest a Noise2Noise (N2N) model to learn the mapping from pairs of noisy instances. However, it requires that the underlying clean images in each pair are exactly the same and the noises are independently drawn from the same distribution, thereby limiting its practicability. Recently, Krull et al. [22] introduce a practically more feasible Noise2Void (N2V) model which adopts a blind-spot network (BSN) to learn CNN denoisers solely from noisy images. Unfortunately, BSN is computationally very inefficient in training and fails to exploit the pixel value at blind spot, giving rise to degraded denoising performance (See Fig. 1). Subsequently, self-supervised model [24] and probabilistic N2V [23] have been further suggested to improve training efficiency via masked convolution [24] and to improve denoising performance via probabilistic inference [23,24]. N2V [22] and self-supervised model [24], however, fail to exploit clean images in training. Nonetheless, albeit it is difficult to acquire the nearly noise-free image corresponding to a given noisy image, it is practically feasible to collect a set of unpaired clean images. Moreover, specially designed BSN architecture generally is required to facilitate self-supervised learning, and cannot employ the progress in state-of-the-art networks [27,28,31,35,39,[41][42][43] to improve denoising performance. Chen et al. [8] suggest an unpaired learning based blind denoising method GCBD based on the generative adversarial network (GAN) [17], but only achieve limited performance on real-world noisy photographs.\n\nIn this paper, we present a two-stage scheme, i.e., self-supervised learning and knowledge distillation, to learn blind image denoising network from an unpaired set of clean and noisy images. Instead of GAN-based unpaired learning [7,8], we first exploit only the noisy images to learn a BSN as well as an image-specific noise level estimation network CNN est for image denoising and noise modeling. Then, the learned noise models are applied to clean images for synthesizing a paired set of training images, and we also use the real noisy images and the corresponding denoising results in the first stage to form another paired set. As for knowledge distillation, we simply train a state-of-the-art CNN denoiser, e.g., MWCNN [28], using the above two paired sets.\n\nIn particular, the clean image is assumed to be spatially correlated, making it feasible to exploit the BSN architecture for learning blind denoising network solely from noisy images. To improve the training efficiency, we present a novel dilated BSN (i.e., D-BSN) leveraging dilated convolution and fully convolutional network (FCN), allowing to predict the denoising result of all pixels with a single forward pass during training. We further assume that the noise is pixel-wise independent but can be signal dependent. Hence, the noise level of a pixel can be either a constant or only depends on the individual pixel value. Considering that the noise model and parameters may vary with different images, we suggest an image-specific CNN est by stacking 1 \u00d7 1 convolution layers to meet the above requirements. Using unorganized collections of noisy images, both D-BSN and CNN est can be jointly trained via maximizing the constrained log-likelihood. Given the outputs of D-BSN and CNN est , we use the Bayes' rule to obtain the denoising result in the first stage. As for a given clean image in the second stage, an image-specific CNN est is randomly selected to synthesize a noisy image.\n\nExtensive experiments are conducted to evaluate our D-BSN and unpaired learning method, e.g., MWCNN(unpaired). On various types of synthetic noise (e.g., AWGN, heteroscedastic Gaussian, multivariate Gaussian), our D-BSN is efficient in training and is effective in image denoising and noise modeling. While our MWCNN(unpaired) performs better than the self-supervised model Laine19 [24], and on par with the fully-supervised counterpart (e.g., MWCNN [28]) (See Fig. 1). Experiments on real-world noisy photographs further validate the effectiveness of our MWCNN(unpaired). As for real-world noisy photographs, due to the effect of demosaicking, the noise violates the pixel-wise independent noise assumption, and we simply train our blind-spot network on pixel-shuffle down-sampled noisy images to circumvent this issue. The results show that our MWCNN(unpaired) also performs well and significantly surpasses GAN-based unpaired learning GCBD [8] on DND [34].\n\nThe contributions are summarized as follows:\n\n1. A novel two-stage scheme by incorporating self-supervised learning and knowledge distillation is presented to learn blind image denoising network from an unpaired set of clean and noisy images. In particular, self-supervised learning is adopted for image denoising and noise modeling, consequently resulting in two complementary paired set to distill the ultimate denoising network. 2. A novel dilated blind-spot network (D-BSN) and an image-specific noise level estimation network CNN est are elaborated to improve the training efficiency and to meet the assumed noise characteristics. Using unorganized collections of noisy images, D-BSN and CNN est can be jointly trained via maximizing the constrained log-likelihood. 3. Experiments on various types of synthetic noise show that our unpaired learning method performs better than N2V [22] and Laine19 [24], and on par with its fully-supervised counterpart. MWCNN(unpaired) also performs well on real-world photographs and significantly surpasses GAN-based unpaired learning (GCBD) [8] on the DND [34] dataset.\n\n\nRelated Work\n\n\nDeep Image Denoising\n\nIn the last few years, significant progress has been made in developing deep CNN denoisers. Zhang et al. [41] suggested DnCNN by incorporating residual learning and batch normalization, and achieved superior performance than most traditional methods for AWGN removal. Subsequently, numerous building modules have been introduced to deep denoising networks, such as dilated convolution [42], channel attention [4], memory block [39], and wavelet transform [28]. To fulfill the aim of image denoising, researchers also modified the representative network architectures, e.g., U-Net [28,31], Residual learning [41], and non-local network [27], and also introduced several new ones [19,35]. For handling AWGN with different noise levels and spatially variant noise, FFDNet was proposed by taking both noise level map and noisy image as network input. All these studies have consistently improved the denoising performance of deep networks [4,19,27,28,31,35,39,42]. However, CNN denoisers for AWGN usually generalize poorly to complex noise, especially real-world noisy images [34]. Besides AWGN, complex noise models, e.g., heteroscedastic Gaussian [34] and Gaussian-Poisson [11], have also been suggested, but are still not sufficient to approximate real sensor noise. As for real-world photography, the introduction of ISP pipeline makes the noise both signal-dependent and spatially correlated, further increasing the complexity of noise model [13,29]. Instead of noise modeling, several methods have been proposed to acquire nearly noise-free images by averaging multiple noisy images [2,32] or by aligning and post-processing low ISO images [34]. With the set of noisy-clean image pairs, several well-designed CNNs were developed to learn a direct mapping for removing noise from realworld noisy RAW and sRGB images [6,15,40]. Based upon the GLOW architecture [20], Abdelhamed et al. [1] introduced a deep compact noise model, i.e., Noise Flow, to characterize the real noise distribution from noisy-clean image pairs. In this work, we learn a FCN with 1 \u00d7 1 convolution from noisy images for modeling pixel-independent signal-dependent noise. To exploit the progress in CNN denoising, we further train state-of-the-art CNN denoiser using the synthetic noisy images generated with the learned noise model.\n\n\nLearning CNN Denoisers without Paired Noisy-Clean Images\n\nSoltanayev and Chun [38] developed a Steins unbiased risk estimator (SURE) based method on noisy images. Zhussip et al. [45] further extended SURE to learn CNN denoisers from correlated pairs of noisy images. But the above methods only handle AWGN and require that noise level is known.\n\nLehtinen et al. [26] suggested to learn an N2N model from a training set of noisy image pairs, which avoids the acquisition of nearly noise-free images but remains limited in practice. Subsequently, N2V [22] (Noise2Self [5]) has been proposed to learn (calibrate) denoisers by requiring the output at a position does not depend on the input value at the same position. However, N2V [22] is inefficient in training and fails to exploit the pixel value at blind spot. To address these issues, Laine19 [24] and probabilistic N2V [23] were then suggested by introducing masked convolution [24] and probabilistic inference [23,24].\n\nN2V and follow-up methods are solely based on noisy images without exploiting unpaired clean images. Chen et.al [8] presented a GAN-based model, i.e., GCBD, to learn CNN denoisers using unpaired noisy and clean images, but its performance on real-world noisy photographs still falls behind. In contrast to [8], we develop a non-GAN based method for unpaired learning. Our method involves two stage in training, i.e., self-supervised learning and knowledge distillation. As opposed to [22][23][24], our self-supervised learning method elaborately incorporates dilated convolution with FCN to design a D-BSN for improving training efficiency. For modeling pixel-independent signal-dependent noise, we adopt an image-specific FCN with 1 \u00d7 1 convolution. And constrained log-likelihood is then introduced to train D-BSN and image-specific CNN est .\n\n\nProposed Method\n\nIn this section, we present our two-stage training scheme, i.e., self-supervised learning and knowledge distillation, for learning CNN denoisers from an unpaired set of noisy and clean images. After describing the problem setting and assumptions, we first introduce the main modules of our scheme and explain the knowledge distillation stage in details. Then, we turn to self-supervised learning by describing the dilated blind-spot network (D-BSN), image-specific noise model CNN est and self-supervised loss. For handling real-world noisy photographs, we finally introduce a pixel-shuffle down-sampling strategy to apply our method. \n\n\nTwo-Stage Training and Knowledge Distillation\n\nThis work tackles the task of learning CNN denoisers from an unpaired set of clean and noisy images. Thus, the training set can be given by two independent sets of clean images X and noisy images Y. Here, x denotes a clean image from X , and y a noisy image from Y. With the unpaired setting, both the real noisy observation of x and the noise-free image of y are unavailable. Denote byx the underlying clean image of y. The real noisy image y can be written as,\ny =x + n,(1)\nwhere n denotes the noise in y. Following [22], we assume that the image x is spatially correlated and the noise n is pixel-independent and signal-dependent Gaussian. That is, the noise variance (or noise level) at pixel i is determined only by the underlying noise-free pixel valuex i at pixel i,\nvar(n i ) = gx(x i ).(2)\nThus, gx(x) can be regarded as a kind of noise level function (NLF) in multivariate heteroscedastic Gaussian model [11,34]. Instead of linear NLF in [11,34], gx(x) can be any nonlinear function, and thus is more expressive in noise modeling. We also note that the NLF may vary with images (e.g., AWGN with different variance), and thus image-specific gx(x) is adopted for the flexibility issue.\n\nWith the above problem setting and assumptions, Fig. 2 illustrates our twostage training scheme involving self-supervised learning and knowledge distillation. In the first stage, we elaborate a novel blind-spot network, i.e., D-BSN, and an image-specific noise model CNN est (Refer to Sec. 3.2 for details). Then, selfsupervised loss is introduced to jointly train D-BSN and CNN est solely based on Y via maximizing the constrained log-likelihood (Refer to Sec. 3.3 for details). For a given real noisy image y \u2208 Y, D-BSN and CNN est collaborate to produce the first stage denoising resultx y and the estimated NLF g y (y). It is worth noting that we modify the NLF in Eqn. (2) by defining it on the noisy image y for practical feasibility.\n\nIn the second stage, we adopt the knowledge distillation strategy, and exploit X , Y,X (1) = {x y |y \u2208 Y}, and the set of image-specific NLFs {g y (y)|y \u2208 Y} to distill a state-of-the-art deep denoising network in a fully-supervised manner. On the one hand, for a given clean image x \u2208 X , we randomly select an image-specific NLF g y (y), and use g y (x) to generate a NLF for x. Denote by n 0 \u223c N (0, 1) a random Gaussian noise of zero mean and one variance. The synthetic noisy image\u1ef9 corresponding to x can then be obtained by,\ny = x + g y (x) \u00b7 n 0 .(3)\nConsequently, we build the first set of paired noisy-clean images {(x,\u1ef9)|x \u2208 X }.\n\nOn the other hand, given a real noisy image y, we have its denoising resultx y in the first stage, thereby forming the second set of paired noisy-clean images {(x y , y)|y \u2208 Y}.\n\nThe above two paired sets are then used to distill a state-of-the-art convolutional denoising network (CDN) by minimizing the following loss,\nL distill = x\u2208X CDN(\u1ef9) \u2212 x 2 + \u03bb y\u2208Y CDN(y) \u2212x y 2 ,(4)\nwhere \u03bb = 0.1 is the tradeoff parameter. We note that the two paired sets are complementary, and both benefit the denoising performance. In particular, for {(x,\u1ef9)|x \u2208 X }, the synthetic noisy image\u1ef9 may not fully capture real noise complexity when the estimated image-specific noise model CNN est is not accurate. Nonetheless, the clean image x are real, which is beneficial to learn denoising network with visually pleasing result and fine details. As for {(x y , y)|y \u2208 Y}, the noisy image y is real, which is helpful in compensating the estimation error in noise model. The denoising resultx y in the first stage may suffer from the over-smoothing effect, which, fortunately, can be mitigated by the real clean images in {(x,\u1ef9)|x \u2208 X }. In our two-stage training scheme, the convolutional denoising network CDN can be any existing CNN denoisers, and we consider MWCNN [28] as an example in our implementation. Our two-stage training scheme offers a novel and effective approach to train CNN denoisers with unpaired learning. In contrast to GAN-based method [8], we present a self-supervised method for joint estimation of denoising result and image-specific noise model. Furthermore, knowledge distillation with two complementary paired sets is exploited to learn a deep denoising network in a fully-supervised manner. The network structures and loss function of our selfsupervised learning method are also different with [22][23][24], which will be explained in the subsequent subsections.\n\n\nD-BSN and CNN est for Self-Supervised Learning\n\nBlind-spot network (BSN) generally is required for self-supervised learning of CNN denoisers. Among existing BSN solutions, N2V [22] is computationally very inefficient in training. Laine et al. [24] greatly multigate the efficiency issue, but still require four network branches or four rotated versions of each input N2V [22] Laine19 [24] D-BSN image, thereby leaving some leeway to further improve training efficiency. To tackle this issue, we elaborately incorporate dilated convolution with FCN to design a D-BSN. Besides, Laine et al. [24] assume the form of noise distribution is known, e.g., AWGN and Poisson noise, and adopt a U-Net to estimate the parameters of noise model. In comparison, our model is based on a more general assumption that the noise n is pixel-independent, signal-dependent, and imagespecific. To meet this assumption, we exploit a FCN with 1 \u00d7 1 convolution (i.e., CNN est ) to produce an image-specific NLF for noise modeling. In the following, we respectively introduce the structures of D-BSN and CNN est .\n\n\nD-BSN.\n\nFor the output at a position, the core of BSN is to exclude the effect of the input value at the same position (i.e., blind-spot requirement). For the first convolution layer, we can easily meet this requirement using masked convolution [33]. Denote by y a real noisy image, and w k the k-th 3 \u00d7 3 convolutional kernel. We introduce a 3 \u00d7 3 binary mask m, and assign 0 to the central element of m and 1 to the others. The centrally masked convolution is then defined as,\nf (1) k = y * (w k \u2022 m) ,(5)\nwhere f (1) k denotes the k-th channel of feature map in the first layer, * and \u2022 denotes the convolution operator and element-wise product, respectively. Obviously, the blind-spot requirement can be satisfied for f (1) k , but will certainly be broken when further stacking centrally masked convolution layers.\n\nFortunately, as shown in Fig. 3(a), the blind-spot requirement can be maintained by stacking dilated convolution layers with scale factor s = 2 upon 3 \u00d7 3 centrally masked convolution. Analogously, it is also feasible to stacking dilated convolution layers with s = 3 upon 5 \u00d7 5 centrally masked convolution. Moreover, 1 \u00d7 1 convolution and skip connection also do not break the blind-spot requirement. Thus, we can leverage centrally masked convolution, dilated convolution, and 1 \u00d7 1 convolution to elaborate a FCN, i.e., D-BSN, while satisfying the blind-spot requirement. In comparison to [24], neither four network branches or four rotated versions of input image are required by our D-BSN. Detailed description for the blind-spot mechanisms illustrated in Fig.3(a) can be found in the supplementary materials. Fig. 3(c) illustrates the network structure of our D-BSN. In general, our D-BSN begins with a 1 \u00d7 1 convolution layer, and follows by two network branches. Each branch is composed of a 3 \u00d7 3 (5 \u00d7 5) centrally masked convolution layer following by seven multiple dilated convolution (MDC) modules with s = 2 (s = 3). Then, the feature maps of the two branches are concatenated and three 1 \u00d7 1 convolution layers are further deployed to produce the network output. As shown in Fig. 3(c), the MDC module adopts a residual learning formulation and involves three sub-branches. In these sub-branches, zero, one, and two 3 \u00d7 3 convolution layers are respectively stacked upon a 1 \u00d7 1 convolution layer. The outputs of these sub-branches are then concatenated, followed by another 1 \u00d7 1 convolution layer, and added with the input of the MDC module. Then, the last 1 \u00d7 1 convolution layer is deployed to produce the output feature map. Finally, by concatenating the feature maps from the two network branches, we further apply three 1 \u00d7 1 convolution layers to produce the D-BSN output. Please refer to Fig. 3(c) for the detailed structure of D-BSN.\n\nCNN est . The noise is assumed to be conditionally pixel-wise independent given the underlying clean image. We assume that the noise is signal-dependent multivariate Gaussian with the NLF gx(x), and further require that the NLF is image-specific to improve the model flexibility. Taking these requirements into account, we adopt a FCN architecture CNN est with 1 \u00d7 1 convolution to learn the noise model. Benefited from all 1 \u00d7 1 convolution layers, the noise level at a position can be guaranteed to only depends on the input value at the same position. Note that the input of gx(x) is a clean image and we only have noisy images in self-supervised learning. Thus, CNN est takes the noisy image as the input and learns the NLF g y (y) to approximate gx(x). For an input image of C channels (C = 1 for gray level image and 3 for color image), the output at a position i is a C \u00d7 C covariance matrix \u03a3 n i , thereby making it feasible in modeling channel-correlated noise. Furthermore, we require each noisy image has its own network parameters in CNN est to learn image specific NLF. From Fig. 3(b), our CNN est consists of five 1 \u00d7 1 convolution layers of 16 channels. And the ReLU nonlinearity [21] is deployed for all convolution layers except the last one.\n\n\nSelf-Supervised Loss and Bayes Denoising\n\nIn our unpaired learning setting, the underlying clean imagex and ground-truth NLF of real noisy image y are unavailable. We thus resort to self-supervised learning to train D-BSN and CNN est . For a given position i, we have y i =x i +n i with n i \u223c N (0, \u03a3 n i ). Here, y i ,x i , n i , and 0 all are C \u00d7 1 vectors. Let \u00b5 be the directly predicted clean image by D-BSN. We assume \u00b5 =x + n \u00b5 with n \u00b5 i \u223c N (0, \u03a3 \u00b5 i ), and further assume that n i and \u00b5 i are independent. It is noted that \u00b5 is closer tox than y, and usually we have |\u03a3 n i | |\u03a3 \u00b5 i | \u2248 0. Considering thatx i is not available, a new variable i = y i \u2212 \u00b5 i is introduced and it has i \u223c N (0, \u03a3 n i + \u03a3 \u00b5 i ). The negative log-likelihood of y i \u2212 \u00b5 i can be written as,\nL i = 1 2 (y i \u2212 \u00b5 i ) (\u03a3 n i + \u03a3 \u00b5 i ) \u22121 (y i \u2212 \u00b5 i ) + 1 2 log |\u03a3 n i + \u03a3 \u00b5 i |,(6)\nwhere | \u00b7 | denotes the determinant of a matrix. However, the above loss ignores the constraint |\u03a3 n i | |\u03a3 \u00b5 i | \u2248 0. Actually, when taking this constraint into account, the term log |\u03a3 n i + \u03a3 \u00b5 i | in Eqn. (6) can be well approximated by its first-order Taylor expansion at the point\n\u03a3 n i , log |\u03a3 n i + \u03a3 \u00b5 i | \u2248 log |\u03a3 n i | + tr (\u03a3 n i ) \u22121 \u03a3 \u00b5 i ,(7)\nwhere tr(\u00b7) denotes the trace of a matrix. Note that \u03a3 n i and \u03a3 \u00b5 i are treated equally in the left term. While in the right term, smaller \u03a3 \u00b5 i and larger \u03a3 n i are favored based on tr (\u03a3 n i ) \u22121 \u03a3 \u00b5 i , which is consistent with |\u03a3 n i | |\u03a3 \u00b5 i | \u2248 0. Actually, \u00b5 i and \u03a3 \u00b5 i can be estimated as the output of D-BSN at position i, i.e.,\u03bc i = (D-BSN \u00b5 (y)) i and\u03a3 \u00b5 i = (D-BSN \u03a3 (y)) i . \u03a3 n i can be estimated as the output of CNN est at position i, i.e.,\u03a3 n i = (CNN est (y)) i . By substituting Eqn. (7) into Eqn. (6), and replacing \u00b5 i , \u03a3 \u00b5 i and \u03a3 n i with the network outputs, we adopt the constrained negative log-likelihood for learning D-BSN and CNN est ,\nL self = i 1 2 (y i \u2212\u03bc i ) (\u03a3 \u00b5 i +\u03a3 n i ) \u22121 (y i \u2212\u03bc i )+log |\u03a3 n i |+tr (\u03a3 n i ) \u22121\u03a3\u00b5\ni . (8) After self-supervised learning, given the output D-BSN \u00b5 (y), D-BSN \u03a3 (y) and CNN est (y), the denoising result in the first stage can be obtained using the Bayes' rule to each pixel,\nx i = (\u03a3 \u00b5 i +\u03a3 n i ) \u22121 (\u03a3 \u00b5 i y i +\u03a3 n i\u03bci ).(9)\n\nExtension to Real-world Noisy Photographs\n\nDue to the effect of demosaicking, the noise in real-world photographs is spatially correlated and violates the pixel-independent noise assumption, thereby restricting the direct application of our method. Nonetheless, such assumption is critical in separating signal (spatially correlated) and noise (pixel-independent). Fortunately, the noise is only correlated within a short range. Thus, we can break this dilemma by training D-BSN on the pixel-shuffle downsampled images with factor 4. Considering the noise distributions on sum-images are different, we assign the 16 sub-images to 4 groups according to the Bayer pattern. The results of 16 sub-images are then pixel-shuffle upsampled to form an image of the original size, and the guided filter [16] with radius of 1 and penalty value of 0.01 is applied to obtain the final denoising image. We note that denoising on pixel-shuffle sub-images slightly degrades the performance. Nonetheless, our method can still obtain visually satisfying results on real-world noisy photographs.\n\n\nExperimental Results\n\nIn this section, we first describe the implementation details and conduct ablation study of our method. Then, extensive experiments are carried out to evaluate our method on synthetic and real-world noisy images. The evaluation is performed on a PC with Intel(R) Core (TM) i9-7940X CPU @ 3.1GHz and an Nvidia Titan RTX GPU. The source code and pre-trained models will be publicly available. \n\n\nImplementation details\n\nOur unpaired learning consists of two stages: (i) self-supervised training of D-BSN and CNN est and (ii) knowledge distillation for training MWCNN [28], which are respectively described as follows.\n\nSelf-Supervised Training. For synthetic noises, the clean images are from the validation set of ILSVRC2012 (ImageNet) [10] while excluding the images smaller than 256 \u00d7 256. Several basic noise models, e.g., AWGN, multivariate Gaussian, and heteroscedastic Gaussian, are adopted to synthesize the noisy images Y. While for real noisy images, we simply use the testing dataset as Y.\n\nDuring the training, we randomly crop 48, 000 patches with size 96 \u00d7 96 in each epoch and finish the training after 180 epochs. The Adam optimizer is employed to train D-BSN and CNN est . The learning rate is initialized as 3 \u00d7 10 \u22124 , and is decayed by factor 10 after every 30 epochs until reaching 3 \u00d7 10 \u22127 .\n\nKnowledge Distillation. For both gray and color images, we adopt DIV2K [3], WED [30] and CBSD [37] training set as clean image set X . Then, we exploit both {(x y , y)|y \u2208 Y} and {(x,\u1ef9)|x \u2208 X } to train a state-of-the-art CNN denoiser from scratch. And MWCNN with original setting [28] on learning algorithm is adopted to train the CNN denoiser on our data.\n\n\nComparison of Different Supervision Settings\n\nCNN denoisers can be trained with different supervision settings, such as N2C, N2N [26], N2V [22], Laine19 [24], GCBD [8], our D-BSN and MWCNN(unpaired). For a fair comparison, we retrain two MWCNN models with the N2C and N2N [26] settings, respectively. The results of N2V [22], Laine19 [24] and GCBD [8] are from the original papers.\n\nResults on Gray Level Images. We consider two basic noise models, i.e., AWGN with \u03c3 = 15, 25 and 50, and heteroscedastic Gaussian (HG) [34] n i \u223c N (0, \u03b1 2 x i + \u03b4 2 ) with \u03b1 = 40 and \u03b4 = 10. From Table 1, on BSD68 [37] our D-BSN performs better than N2V [22] and on par with GCBD [8], but is inferior to Laine19 [24]. Laine19 [24] learns the denoisers solely from noisy images and does not exploit unpaired clean images in training, making it still poorer than MWCNN(N2C). By exploiting both noisy and clean images, our MWCNN(unpaired) (also Ours(full) in Table 1) outperforms both Laine19 [24] [43] n \u223c N (0, \u03a3) with \u03a3 = 75 2 \u00b7U\u039bU T . Here, U is a random unitary matrix, \u039b is a diagonal matrix of three random values in the range (0, 1). GCBD [8] and N2V [22] did not report their results on color image denoising. On CBSD68 [37] our MWCNN(unpaired) is consistently better than Laine19 [24], and notably outperforms MWCNN(N2N) [26] for HG noise. We have noted that both MWCNN(unpaired) and D-BSN perform well in handling multivariate Gaussian with cross-channel correlation.\n\n\nExperiments on Synthetic Noisy Images\n\nIn this subsection, we assess our MWCNN(unpaired) in handling different types of synthetic noise, and compare it with the state-of-the-art image denoising methods. The competing methods include BM3D [9], DnCNN [41], NLRN [27] and N3Net [35] for gray level image denoising on BSD68 [37], and CBM3D [9], CDnCNN [41], FFDNet [43] and CBDNet [15] for color image denoising on BSD68 [37], KODAK24 [12], McMaster [44].  [27] and better than BM3D [9], DnCNN [41] and N3Net [35]. Tables 2 and 3 list the results of color image denoising. Benefited from unpaired learning and the use of MWCNN [28], our MWCNN(unpaired) outperforms CBM3D [9], CDnCNN [41] and FFDNet [43] by a non-trivial margin for all noise levels and on the three datasets.\n\nHeteroscedastic Gaussian. We further test our MWCNN(unpaired) in handling heteroscedastic Gaussian (HG) noise which is usually adopted for modeling RAW image noise. It can be seen from Table 2 that all the CNN denoisers outperform CBM3D [9] by a large margin on CBSD68. CBDNet [15] takes both HG noise and in-camera signal processing pipeline into account when training the deep denoising network, and thus is superior to FFDNet [43]. Our MWCNN(unpaired) can leverage the progress in CNN denoisers and well exploit the unpaired noisy and clean images, and achieves a PSNR gain of 0.8dB against CBDNet [15].\n\nMultivariate Gaussian. Finally, we evaluate our MWCNN(unpaired) in handling multivariate Gaussian (MG) noise with cross-channel correlation. Some image processing operations, e.g., image demosaicking, may introduce cross-channel correlated noise. From Table 2, FFDNet [43] is flexible in handle HG noise, while our unpaired learning method, MWCNN(unpaired), performs on par with FFD-Net [43] on CBSD68.\n\n\nExperiments on Real-world Noisy Photographs\n\nFinally, we conduct comparison experiments on two widely adopted datasets of real-world noisy photographs, i.e, DND [34] and CC15 [32]. Our methods is compared with both traditional denoising method, i.e., CBM3D [9], deep Gaussian denoiser, i.e., DnCNN [41], deep blind denoisers, i.e., CBDNet [15] and VDN [40], and unsupervised learning methods, i.e., GCBD [8], N2S [26] and N2V [22], in terms of both quantitative and qualitative results. The average PSNR and SSIM metrics are presented in Table 4. On CC15, our MWCNN(unpaired) outperforms the other unsupervised learning methods (i.e., N2S [26] and N2V [22]) by a large margin (0.5dB in PSNR). On DND, our MWCNN(unpaired) achieves a PSNR gain of 2.3dB against GCBD. The results clearly show the merits of our methods in exploiting unpaired noisy and clean images. Actually, our method Noisy BM3D [9] DnCNN [41] CBDNet [15] Ours (full) Fig. 4: Denoising results of different methods on real-world images from CC15(up) and DND(dowm) datasets.\n\nis only inferior to deep models specified for real-world noisy photography, e.g., CBDNet [15] and VDN [40]. Such results should not be criticized considering that our MWCNN(unpaired) has no access to neither the details of ISP [15] and the paired noisy-clean images [40]. Moreover, we adopt the pixel-shuffle downsampling to decouple spatially correlated noise, which also gives rise to moderate performance degradation of our method. Nonetheless, it can be seen from Fig. 4 that our MWCNN(unpaired) achieves comparable or better denoising results in comparison to all the competing methods on DND and CC15.\n\n\nConcluding Remarks\n\nThis paper presented a novel unpaired learning method by incorporating selfsupervised learning and knowledge distillation for training CNN denoisers. In self-supervised learning, we proposed a dilated blind-spot network (D-BSN) and a FCN with 1 \u00d7 1 convolution, which can be efficiently trained via maximizing constrained log-likelihood from unorganized collections of noisy images. For knowledge distillation, the estimated denoising image and noise model are used to distill the state-of-the-art CNN denoisers, such as DnCNN and MWCNN. Experimental results showed that the proposed method is effective on both images with synthetic noise (e.g., AWGN, heteroscedastic Gaussian, multivariate Gaussian) and real-world noisy photographs. Compared with [22,24] and GAN-based unpaired learning of CNN denoisers [8], our method has several merits in efficiently training blind-spot networks and exploiting unpaired noisy and clean images. However, there remain a number of challenges to be addressed: (i) our method is based on the assumption of pixel-independent heteroscedastic Gaussian noise, while real noise can be more complex and spatially correlated. (ii) In self-supervised learning, estimation error may be unavoidable for clean images and noise models, and it is interesting to develop robust and accurate distillation of CNN denoisers.  From top to bottom: noisy images, denoised images by CBM3D [9], denoised images by CBDNet [15], denoised images by our MWCNN(unpaired).\n\nFig. 1 :\n1Supervision settings for CNN denoisers, including Noise2Clean (MWCNN(N2C)\n\nFig. 2 :\n2Illustration of our two-stage training scheme involving self-supervised learning and knowledge distillation.\n\nFig. 3 :\n3Mechanisms of BSNs, and network structures of D-BSN and CNN est .\n\nFig. 6 :\n6Denoising results of different methods on real noisy images from RNI15.\n\nTable 1 :\n1Average PNSR(dB) results of different methods on the BSD68 dataset \nwith noise levels 15, 25 and 50, and heteroscedastic Gaussian (HG) noise with \n\u03b1 = 40, \u03b4 = 10. \n\nNoise Para. \nBM3D \n[9] \n\nDnCNN \n[41] \n\nNLRN \n[27] \n\nN3Net \n[35] \n\nN2V \n[22] \n\nLaine19 \n[24] \n\nGCBD \n[8] \n\nD-BSN \n(ours) \n\nN2C \n[28] \n\nN2N \n[26] \n\nOurs \n(full) \n\nAWGN \n\n\u03c3 = 15 31.07 \n31.72 31.88 \n-\n-\n-\n31.37 31.63 31.86 31.71 31.82 \n\u03c3 = 25 28.57 \n29.23 29.41 29.30 27.71 29.27 \n28.83 29.12 29.41 29.33 29.38 \n\u03c3 = 50 25.62 \n26.23 26.47 26.39 \n-\n-\n-\n26.19 26.53 26.52 26.51 \n\nHG \n\u03b1 = 40 \n\u03b4 = 10 \n23.84 \n-\n-\n-\n-\n-\n-\n29.16 30.16 29.53 30.10 \n\n\n\nTable 2 :\n2Average PNSR(dB) results of different methods on the CBSD68 dataset \nwith noise levels 15, 25 and 50, heteroscedastic Gaussian (HG) noise with \u03b1 = 40, \n\u03b4 = 10, and multivariate Gaussian (MG) noise. \n\nNoise \nPara. \nCBM3D \n[9] \n\nCDnCNN \n[41] \n\nFFDNet \n[43] \n\nCBDNet \n[15] \n\nLaine19 \n[24] \n\nD-BSN \n(ours) \n\nN2C \n[28] \n\nN2N \n[26] \n\nOurs \n(full) \n\nAWGN \n\n\u03c3 = 15 \n33.52 \n33.89 \n33.87 \n-\n-\n33.56 34.08 33.76 34.02 \n\u03c3 = 25 \n30.71 \n30.71 \n31.21 \n-\n31.35 \n30.61 31.40 31.22 31.40 \n\u03c3 = 50 \n27.38 \n27.92 \n27.96 \n-\n-\n27.66 28.26 27.79 28.25 \n\nHG \n\u03b1 = 40 \n\u03b4 = 10 \n23.21 \n-\n28.67 \n30.89 \n-\n30.56 32.10 31.13 31.72 \n\nMG \n\n\u03a3=75 2 \u00b7U\u039bU T \n\u03bbc\u2208(0,1) \nU T U=I \n\n24.07 \n-\n26.78 \n21.50 \n-\n26.48 26.89 26.59 26.81 \n\n\n\nTable 3 :\n3Average PNSR(dB) results of different methods on KODAK24 and \nMcMaster datasets. \n\nDataset \nPara. CBM3D [9] CDnCNN [41] FFDNet [43] Laine19 [24] D-BSN (ours) Ours (full) \n\nKODAK24 \n\n\u03c3 = 15 \n34.28 \n34.48 \n34.63 \n-\n33.74 \n34.82 \n\u03c3 = 25 \n31.68 \n32.03 \n32.13 \n32.33 \n31.64 \n32.35 \n\u03c3 = 50 \n28.46 \n28.85 \n28.98 \n-\n28.69 \n29.36 \n\nMcMaster \n\n\u03c3 = 15 \n34.06 \n33.44 \n34.66 \n-\n33.85 \n34.87 \n\u03c3 = 25 \n31.66 \n31.51 \n32.35 \n32.52 \n31.56 \n32.54 \n\u03c3 = 50 \n28.51 \n28.61 \n29.18 \n-\n28.87 \n29.58 \n\nand MWCNN(N2N) in most cases, and is on par with MWCNN(N2C). In terms \nof training time, Laine19 takes about 14 hours using four Tesla V100 GPUs \non NVIDIA DGX-1 servers. In contrast, our D-BSN takes about 10 hours on \ntwo 2080Ti GPUs and thus is more efficient. In terms of testing time, Our \nMWCNN(unpaired) takes 0.020s to process a 320 \u00d7 480 image, while N2V [22] \nneeds 0.034s and Laine19 [24] spends 0.044s. \nResults on Color Images. Besides AWGN and HG, we further consider an-\nother noise model, i.e., multivariate Gaussian (MG) \n\nTable 4 :\n4The quantitative results (PSNR/SSIM) of different methods on the CC15 and DND (sRGB images) datasets.AWGN. FromTable 1, it can be seen that our MWCNN(unpaired) performs on par with NLRNMethod BM3D [9]DnCNN [41]CBDNet [15]VDN [40]N2S [5]N2V [22]GCBD[8]MWCNN(unpaired) \nSupervised \n-\nYes \nYes \nYes \nNot \nNot \nNot \nNot \n\nCC15 \n35.19 \n33.86 \n36.47 \n-\n35.38 35.27 \n-\n35.90 \n0.9063 \n0.8636 \n0.9392 \n-\n0.9204 0.9158 \n-\n0.9370 \n\nDND \n34.51 \n32.43 \n38.05 \n39.38 \n-\n-\n35.58 \n37.93 \n0.8507 \n0.7900 \n0.9421 \n0.9518 \n-\n-\n0.9217 \n0.9373 \n\n\n\n\nFig. 5: Denoising results of different methods on real noisy images from RNI6. From top to bottom: noisy images, denoised images by BM3D[9], denoised images by DnCNN-B[41], denoised images by our MWCNN(unpaired).Chupa Chups \nDavid Hilbert \nMarilyn \nOld Tom Morris \n\nAudrey Hepburn \nSinger \nPattern1 \nFlowers \n\n\nAcknowledgement. This work is partially supported by the National Natural Science Foundation of China (NSFC) under Grant No.s 61671182, U19A2073.Unpaired Learning of Deep Image DenoisingAppendixA Description for the blind-spot mechanisms.To reveal the blind-spot mechanism of D-BSN, here we use a 7 \u00d7 7 input image y ij (i = 1, ...7, j = 1, ..., 7) as an example. For simplicity, we only consider the position (i = 4, j = 4). After the 3 \u00d7 3 masked convolution (Eqn.(5)), the feature f (1) (4, 4) is only affected by {y 33 , y 34 , y 35 , y 43 , y 45 , y 53 , y 54 , y 55 }, thereby satisfying the blind-spot requirement. When dilated convolution with s = 2 is applied to f (1) to obtained f(2), one can see that f(2)B Additional Visualization ResultsMore denoising results for the real noisy images without ground-truth are provided for comparison. We present the visualization results from RNI6[25]and RNI15[25]datasets to compare with the benchmark method BM3D[9], and the representative discriminative learning methods DnCNN-B[41]and CBDNet[15]. For better view, we recommend to zoom in the images on a computer screen.Fig. 5andFig. 6show the denoising results on real noisy images from RNI6 and RNI15, respectively. One can note that, on gray level images, MWCNN(unpaired) outperforms DnCNN-B[41]and performs favorably against the benchmark method BM3D[9]. On color images, MWCNN(unpaired) achieves comparable or better denoising results in comparison to CBM3D[9]. Even compared with the CBDNet[15], our method shows comparable visualization results without the consideration of the details of ISP and paired noisy-clean images.\nNoise Flow: Noise modeling with conditional normalizing flows. A Abdelhamed, M A Brubaker, M S Brown, ICCV. pp. 5Abdelhamed, A., Brubaker, M.A., Brown, M.S.: Noise Flow: Noise modeling with conditional normalizing flows. In: ICCV. pp. 3165-3173 (2019) 5\n\nA high-quality denoising dataset for smartphone cameras. A Abdelhamed, S Lin, M S Brown, CVPR. 25Abdelhamed, A., Lin, S., Brown, M.S.: A high-quality denoising dataset for smartphone cameras. In: CVPR. pp. 1692-1700 (2018) 2, 5\n\nNtire 2017 challenge on single image super-resolution: Dataset and study. E Agustsson, R Timofte, CVPR Workshops. 11Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: CVPR Workshops (2017) 11\n\nReal image denoising with feature attention. S Anwar, N Barnes, ICCV. pp. 4Anwar, S., Barnes, N.: Real image denoising with feature attention. In: ICCV. pp. 3155-3164 (2019) 4\n\nNoise2self: Blind denoising by self-supervision pp. J Batson, L Royer, 513Batson, J., Royer, L.: Noise2self: Blind denoising by self-supervision pp. 524-533 (2019) 5, 13\n\nUnprocessing images for learned raw denoising. T Brooks, B Mildenhall, T Xue, J Chen, D Sharlet, J T Barron, CVPR. 25Brooks, T., Mildenhall, B., Xue, T., Chen, J., Sharlet, D., Barron, J.T.: Unprocessing images for learned raw denoising. In: CVPR. pp. 11036-11045 (2019) 2, 5\n\nTo learn image super-resolution, use a gan to learn how to do image degradation first. A Bulat, J Yang, G Tzimiropoulos, ECCV. 3Bulat, A., Yang, J., Tzimiropoulos, G.: To learn image super-resolution, use a gan to learn how to do image degradation first. In: ECCV. pp. 185-200 (2018) 3\n\nImage blind denoising with generative adversarial network based noise modeling. J Chen, J Chen, H Chao, M Yang, CVPR. pp. 1314Chen, J., Chen, J., Chao, H., Yang, M.: Image blind denoising with generative adversarial network based noise modeling. In: CVPR. pp. 3155-3164 (2018) 2, 3, 4, 5, 7, 11, 12, 13, 14\n\nImage denoising by sparse 3-d transform-domain collaborative filtering. K Dabov, A Foi, V Katkovnik, K Egiazarian, TIP. 16820Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-d transform-domain collaborative filtering. TIP 16(8), 2080-2095 (2007) 2, 11, 12, 13, 14, 18, 19, 20\n\nPractical Poissonian-Gaussian noise modeling and fitting for single-image raw-data. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, A Foi, M Trimeche, V Katkovnik, K Egiazarian, CVPR. 116Imagenet: A large-scale hierarchical image databaseDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR. pp. 248-255 (2009) 11 11. Foi, A., Trimeche, M., Katkovnik, V., Egiazarian, K.: Practical Poissonian-Gaussian noise modeling and fitting for single-image raw-data. TIP 17(10), 1737-1754 (2008) 4, 6\n\nKodak lossless true color image suite. R Franzen, 12Franzen, R.: Kodak lossless true color image suite. source: http://r0k. us/graphics/kodak 4 (1999) 12\n\nModeling the space of camera response functions. M D Grossberg, S K Nayar, TPAMI. 26104Grossberg, M.D., Nayar, S.K.: Modeling the space of camera response functions. TPAMI 26(10), 1272-1282 (2004) 4\n\nWeighted nuclear norm minimization with application to image denoising. S Gu, L Zhang, W Zuo, X Feng, CVPR. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization with application to image denoising. In: CVPR. pp. 2862-2869 (2014) 2\n\nToward convolutional blind denoising of real photographs. S Guo, Z Yan, K Zhang, W Zuo, L Zhang, CVPR. 220Guo, S., Yan, Z., Zhang, K., Zuo, W., Zhang, L.: Toward convolutional blind denoising of real photographs. In: CVPR. pp. 1712-1722 (2019) 2, 5, 12, 13, 14, 18, 20\n\nGuided image filtering. K He, J Sun, X Tang, ECCV. pp. 10He, K., Sun, J., Tang, X.: Guided image filtering. In: ECCV. pp. 1-14 (2010) 10\n\n. Ian J Goodfellow, Jean Pouget-Abadie, M M B X D W F S O A C Bengio, Y , Generative Adversarial Networks. In: NIPS. p. 3Ian J. Goodfellow, Jean Pouget-Abadie, M.M.B.X.D.W.F.S.O.A.C., Bengio, Y.: Generative Adversarial Networks. In: NIPS. p. 2672C2680 (2014) 3\n\nMixed gaussian-impulse noise reduction from images using convolutional neural network. M T Islam, S M Rahman, M O Ahmad, M Swamy, Image Communication. 682Signal ProcessingIslam, M.T., Rahman, S.M., Ahmad, M.O., Swamy, M.: Mixed gaussian-impulse noise reduction from images using convolutional neural network. Signal Processing: Image Communication 68, 26-41 (2018) 2\n\nFOCNet: A fractional optimal control network for image denoising. X Jia, S Liu, X Feng, L Zhang, CVPR. 4Jia, X., Liu, S., Feng, X., Zhang, L.: FOCNet: A fractional optimal control network for image denoising. In: CVPR. pp. 6054-6063 (2019) 4\n\nGlow: Generative flow with invertible 1x1 convolutions. D P Kingma, P Dhariwal, NIPS. 5Kingma, D.P., Dhariwal, P.: Glow: Generative flow with invertible 1x1 convolutions. In: NIPS. pp. 10215-10224 (2018) 5\n\nA Krizhevsky, I Sutskever, G E Hinton, Imagenet classification with deep convolutional neural networks. In: NIPS. 9Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. pp. 1097-1105 (2012) 9\n\nNoise2void-learning denoising from single noisy images. A Krull, T O Buchholz, F Jug, CVPR. 1314Krull, A., Buchholz, T.O., Jug, F.: Noise2void-learning denoising from single noisy images. In: CVPR. pp. 2129-2137 (2019) 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14\n\nA Krull, T Vicar, F Jug, arXiv:1906.00651Probabilistic noise2void: Unsupervised content-aware denoising. 27arXiv preprintKrull, A., Vicar, T., Jug, F.: Probabilistic noise2void: Unsupervised content-aware denoising. arXiv preprint arXiv:1906.00651 (2019) 2, 3, 5, 7\n\nHigh-quality self-supervised deep image denoising. S Laine, T Karras, J Lehtinen, T Aila, NIPS. pp. 1214Laine, S., Karras, T., Lehtinen, J., Aila, T.: High-quality self-supervised deep image denoising. In: NIPS. pp. 6968-6978 (2019) 2, 3, 4, 5, 7, 8, 9, 11, 12, 14\n\nThe noise clinic: a blind image denoising algorithm. M Lebrun, M Colom, J M Morel, Image Processing On Line. 518Lebrun, M., Colom, M., Morel, J.M.: The noise clinic: a blind image denoising algorithm. Image Processing On Line 5, 1-54 (2015) 18\n\nNoise2noise: Learning image restoration without clean data. J Lehtinen, J Munkberg, J Hasselgren, S Laine, T Karras, M Aittala, T Aila, ICML. 213Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., Aila, T.: Noise2noise: Learning image restoration without clean data. In: ICML. pp. 2965-2974 (2018) 2, 5, 11, 12, 13\n\nNon-local recurrent network for image restoration. D Liu, B Wen, Y Fan, C C Loy, T S Huang, NIPS. pp. 1213Liu, D., Wen, B., Fan, Y., Loy, C.C., Huang, T.S.: Non-local recurrent network for image restoration. In: NIPS. pp. 1673-1682 (2018) 2, 3, 4, 11, 12, 13\n\nMulti-level wavelet-CNN for image restoration. P Liu, H Zhang, K Zhang, L Lin, W Zuo, CVPR Workshops. 1213Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-CNN for image restoration. In: CVPR Workshops. pp. 773-782 (2018) 1, 2, 3, 4, 7, 11, 12, 13\n\nPractical signal-dependent noise parameter estimation from a single noisy image. X Liu, M Tanaka, M Okutomi, TIP. 23104Liu, X., Tanaka, M., Okutomi, M.: Practical signal-dependent noise parameter estimation from a single noisy image. TIP 23(10), 4361-4371 (2014) 4\n\nWaterloo exploration database: New challenges for image quality assessment models. K Ma, Z Duanmu, Q Wu, Z Wang, H Yong, H Li, L Zhang, TIP. 26211Ma, K., Duanmu, Z., Wu, Q., Wang, Z., Yong, H., Li, H., Zhang, L.: Waterloo exploration database: New challenges for image quality assessment models. TIP 26(2), 1004-1016 (2016) 11\n\nImage restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. X Mao, C Shen, Y B Yang, NIPS. pp. 14Mao, X., Shen, C., Yang, Y.B.: Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. In: NIPS. pp. 2802-2810 (2016) 1, 2, 3, 4\n\nA holistic approach to cross-channel image noise modeling and its application to image denoising. S Nam, Y Hwang, Y Matsushita, S Kim, CVPR. 213Nam, S., Hwang, Y., Matsushita, Y., Joo Kim, S.: A holistic approach to cross-channel image noise modeling and its application to image denoising. In: CVPR. pp. 1683-1691 (2016) 2, 5, 13\n\nA Oord, N Kalchbrenner, K Kavukcuoglu, Pixel Recurrent Neural Networks. In: ICML. p. 8Oord, A.v.d., Kalchbrenner, N., Kavukcuoglu, K.: Pixel Recurrent Neural Networks. In: ICML. p. 1747C1756 (2016) 8\n\nBenchmarking denoising algorithms with real photographs. T Plotz, S Roth, CVPR. pp. 13Plotz, T., Roth, S.: Benchmarking denoising algorithms with real photographs. In: CVPR. pp. 1586-1595 (2017) 2, 4, 5, 6, 11, 13\n\nNeural Nearest Neighbors Networks. T Pl\u00f6tz, S Roth, NIPS. 1213Pl\u00f6tz, T., Roth, S.: Neural Nearest Neighbors Networks. In: NIPS. pp. 1087-1098 (2018) 2, 3, 4, 11, 12, 13\n\nClass-aware fully convolutional gaussian and poisson denoising. T Remez, O Litany, R Giryes, A M Bronstein, TIP. 2711Remez, T., Litany, O., Giryes, R., Bronstein, A.M.: Class-aware fully convolutional gaussian and poisson denoising. TIP 27(11), 5707-5722 (2018) 2\n\nFields of experts. S Roth, M J Black, IJCV. 82212Roth, S., Black, M.J.: Fields of experts. IJCV 82(2), 205 (2009) 11, 12\n\nTraining deep learning based denoisers without ground truth data. S Soltanayev, S Y Chun, NIPS. pp. 5Soltanayev, S., Chun, S.Y.: Training deep learning based denoisers without ground truth data. In: NIPS. pp. 3257-3267 (2018) 5\n\nImage super-resolution via deep recursive residual network. Y Tai, J Yang, X Liu, CVPR. pp. 24Tai, Y., Yang, J., Liu, X.: Image super-resolution via deep recursive residual network. In: CVPR. pp. 3147-3155 (2017) 2, 3, 4\n\nVariational Denoising Network: Toward blind noise modeling and removal. Z Yue, H Yong, Q Zhao, D Meng, L Zhang, NIPS. 514Yue, Z., Yong, H., Zhao, Q., Meng, D., Zhang, L.: Variational Denoising Network: Toward blind noise modeling and removal. In: NIPS. pp. 1688-1699 (2019) 5, 13, 14\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. K Zhang, W Zuo, Y Chen, D Meng, L Zhang, TIP. 26719Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. TIP 26(7), 3142-3155 (2017) 1, 2, 3, 4, 11, 12, 13, 14, 18, 19\n\nLearning deep cnn denoiser prior for image restoration. K Zhang, W Zuo, S Gu, L Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition34Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image restoration. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3929-3938 (2017) 3, 4\n\nFFDNet: Toward a fast and flexible solution for cnn-based image denoising. K Zhang, W Zuo, L Zhang, TIP. 27913Zhang, K., Zuo, W., Zhang, L.: FFDNet: Toward a fast and flexible solution for cnn-based image denoising. TIP 27(9), 4608-4622 (2018) 2, 3, 12, 13\n\nColor demosaicking by local directional interpolation and nonlocal adaptive thresholding. L Zhang, X Wu, A Buades, X Li, Journal of Electronic imaging. 20212Zhang, L., Wu, X., Buades, A., Li, X.: Color demosaicking by local directional interpolation and nonlocal adaptive thresholding. Journal of Electronic imaging 20(2), 023016 (2011) 12\n\nTraining deep learning based image denoisers from undersampled measurements without ground truth and without image prior. M Zhussip, S Soltanayev, S Y Chun, CVPR. 5Zhussip, M., Soltanayev, S., Chun, S.Y.: Training deep learning based image denoisers from undersampled measurements without ground truth and without image prior. In: CVPR. pp. 10255-10264 (2019) 5\n", "annotations": {"author": "[{\"end\":93,\"start\":45},{\"end\":141,\"start\":94},{\"end\":188,\"start\":142},{\"end\":254,\"start\":189},{\"end\":346,\"start\":255}]", "publisher": null, "author_last_name": "[{\"end\":54,\"start\":52},{\"end\":102,\"start\":99},{\"end\":149,\"start\":146},{\"end\":200,\"start\":197},{\"end\":267,\"start\":264}]", "author_first_name": "[{\"end\":51,\"start\":45},{\"end\":98,\"start\":94},{\"end\":145,\"start\":142},{\"end\":196,\"start\":189},{\"end\":263,\"start\":255}]", "author_affiliation": "[{\"end\":92,\"start\":56},{\"end\":140,\"start\":104},{\"end\":187,\"start\":151},{\"end\":253,\"start\":226},{\"end\":323,\"start\":287},{\"end\":345,\"start\":325}]", "title": "[{\"end\":42,\"start\":1},{\"end\":388,\"start\":347}]", "venue": null, "abstract": "[{\"end\":2160,\"start\":468}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2180,\"start\":2176},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2200,\"start\":2196},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2235,\"start\":2231},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2276,\"start\":2272},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2324,\"start\":2320},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2339,\"start\":2335},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2480,\"start\":2476},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2600,\"start\":2596},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2603,\"start\":2600},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2667,\"start\":2664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2669,\"start\":2667},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2671,\"start\":2669},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2674,\"start\":2671},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2859,\"start\":2856},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2861,\"start\":2859},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2864,\"start\":2861},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2867,\"start\":2864},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2870,\"start\":2867},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2873,\"start\":2870},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2876,\"start\":2873},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3558,\"start\":3555},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3561,\"start\":3558},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3616,\"start\":3612},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4056,\"start\":4052},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4362,\"start\":4358},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4732,\"start\":4728},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4759,\"start\":4755},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4846,\"start\":4842},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4916,\"start\":4912},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4919,\"start\":4916},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4929,\"start\":4925},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4960,\"start\":4956},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5368,\"start\":5364},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5371,\"start\":5368},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5374,\"start\":5371},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5377,\"start\":5374},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5380,\"start\":5377},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5384,\"start\":5380},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5388,\"start\":5384},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5392,\"start\":5388},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5442,\"start\":5439},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5560,\"start\":5556},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5867,\"start\":5864},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5869,\"start\":5867},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6363,\"start\":6359},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7979,\"start\":7975},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8047,\"start\":8043},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8539,\"start\":8536},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8551,\"start\":8547},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9444,\"start\":9440},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9461,\"start\":9457},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9640,\"start\":9637},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9656,\"start\":9652},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9814,\"start\":9810},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10094,\"start\":10090},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10117,\"start\":10114},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10136,\"start\":10132},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10164,\"start\":10160},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10289,\"start\":10285},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10292,\"start\":10289},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10316,\"start\":10312},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10344,\"start\":10340},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10387,\"start\":10383},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10390,\"start\":10387},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10643,\"start\":10640},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10646,\"start\":10643},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10649,\"start\":10646},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10652,\"start\":10649},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10655,\"start\":10652},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10658,\"start\":10655},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10661,\"start\":10658},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10664,\"start\":10661},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10781,\"start\":10777},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10854,\"start\":10850},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11152,\"start\":11148},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11155,\"start\":11152},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11293,\"start\":11290},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11296,\"start\":11293},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11351,\"start\":11347},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11525,\"start\":11522},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11528,\"start\":11525},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11531,\"start\":11528},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11570,\"start\":11566},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11593,\"start\":11590},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12096,\"start\":12092},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12196,\"start\":12192},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12380,\"start\":12376},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12567,\"start\":12563},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12583,\"start\":12580},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12746,\"start\":12742},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12863,\"start\":12859},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12890,\"start\":12886},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12949,\"start\":12945},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12982,\"start\":12978},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12985,\"start\":12982},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13103,\"start\":13100},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13297,\"start\":13294},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13476,\"start\":13472},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13480,\"start\":13476},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13484,\"start\":13480},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15059,\"start\":15055},{\"end\":15455,\"start\":15451},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15458,\"start\":15455},{\"end\":15489,\"start\":15485},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15492,\"start\":15489},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18368,\"start\":18364},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18556,\"start\":18553},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18922,\"start\":18918},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18926,\"start\":18922},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18930,\"start\":18926},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19169,\"start\":19165},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19236,\"start\":19232},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19364,\"start\":19360},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19377,\"start\":19373},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19582,\"start\":19578},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20329,\"start\":20325},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20807,\"start\":20804},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21498,\"start\":21494},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24061,\"start\":24057},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25871,\"start\":25868},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27147,\"start\":27143},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28020,\"start\":28016},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28190,\"start\":28186},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28839,\"start\":28836},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28849,\"start\":28845},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28863,\"start\":28859},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29050,\"start\":29046},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29258,\"start\":29254},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29268,\"start\":29264},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29282,\"start\":29278},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29292,\"start\":29289},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29401,\"start\":29397},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29449,\"start\":29445},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29463,\"start\":29459},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29476,\"start\":29473},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29647,\"start\":29643},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29727,\"start\":29723},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29767,\"start\":29763},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29792,\"start\":29789},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29825,\"start\":29821},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29839,\"start\":29835},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30103,\"start\":30099},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30108,\"start\":30104},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30256,\"start\":30253},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30269,\"start\":30265},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30339,\"start\":30335},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30400,\"start\":30396},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30441,\"start\":30437},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30828,\"start\":30825},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30840,\"start\":30836},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30851,\"start\":30847},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30866,\"start\":30862},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30911,\"start\":30907},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30926,\"start\":30923},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30939,\"start\":30935},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30952,\"start\":30948},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30968,\"start\":30964},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31008,\"start\":31004},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31022,\"start\":31018},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31037,\"start\":31033},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31044,\"start\":31040},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31069,\"start\":31066},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31081,\"start\":31077},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31096,\"start\":31092},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31214,\"start\":31210},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31257,\"start\":31254},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31270,\"start\":31266},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31286,\"start\":31282},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31600,\"start\":31597},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31641,\"start\":31637},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31793,\"start\":31789},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31965,\"start\":31961},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32240,\"start\":32236},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32359,\"start\":32355},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32538,\"start\":32534},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32552,\"start\":32548},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32633,\"start\":32630},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32675,\"start\":32671},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32716,\"start\":32712},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32729,\"start\":32725},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32780,\"start\":32777},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32790,\"start\":32786},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32803,\"start\":32799},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33016,\"start\":33012},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33029,\"start\":33025},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33271,\"start\":33268},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33282,\"start\":33278},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33294,\"start\":33290},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33507,\"start\":33503},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33520,\"start\":33516},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33645,\"start\":33641},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33684,\"start\":33680},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34798,\"start\":34794},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34801,\"start\":34798},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34854,\"start\":34851},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35450,\"start\":35447},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":35482,\"start\":35478},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38914,\"start\":38911},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38946,\"start\":38942}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35608,\"start\":35524},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35728,\"start\":35609},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35805,\"start\":35729},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35888,\"start\":35806},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36504,\"start\":35889},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37209,\"start\":36505},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38234,\"start\":37210},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38772,\"start\":38235},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39085,\"start\":38773}]", "paragraph": "[{\"end\":2312,\"start\":2181},{\"end\":2675,\"start\":2314},{\"end\":3977,\"start\":2677},{\"end\":5631,\"start\":3979},{\"end\":6397,\"start\":5633},{\"end\":7591,\"start\":6399},{\"end\":8552,\"start\":7593},{\"end\":8598,\"start\":8554},{\"end\":9665,\"start\":8600},{\"end\":12011,\"start\":9705},{\"end\":12358,\"start\":12072},{\"end\":12986,\"start\":12360},{\"end\":13832,\"start\":12988},{\"end\":14487,\"start\":13852},{\"end\":14999,\"start\":14537},{\"end\":15310,\"start\":15013},{\"end\":15730,\"start\":15336},{\"end\":16472,\"start\":15732},{\"end\":17005,\"start\":16474},{\"end\":17114,\"start\":17033},{\"end\":17293,\"start\":17116},{\"end\":17436,\"start\":17295},{\"end\":18986,\"start\":17493},{\"end\":20077,\"start\":19037},{\"end\":20558,\"start\":20088},{\"end\":20899,\"start\":20588},{\"end\":22859,\"start\":20901},{\"end\":24121,\"start\":22861},{\"end\":24902,\"start\":24166},{\"end\":25276,\"start\":24990},{\"end\":26016,\"start\":25349},{\"end\":26296,\"start\":26105},{\"end\":27426,\"start\":26392},{\"end\":27842,\"start\":27451},{\"end\":28066,\"start\":27869},{\"end\":28449,\"start\":28068},{\"end\":28763,\"start\":28451},{\"end\":29122,\"start\":28765},{\"end\":29506,\"start\":29171},{\"end\":30584,\"start\":29508},{\"end\":31358,\"start\":30626},{\"end\":31966,\"start\":31360},{\"end\":32370,\"start\":31968},{\"end\":33412,\"start\":32418},{\"end\":34021,\"start\":33414},{\"end\":35523,\"start\":34044}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15012,\"start\":15000},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15335,\"start\":15311},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17032,\"start\":17006},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17492,\"start\":17437},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20587,\"start\":20559},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24989,\"start\":24903},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25348,\"start\":25277},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26104,\"start\":26017},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26347,\"start\":26297}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29712,\"start\":29705},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30072,\"start\":30065},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31112,\"start\":31098},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31552,\"start\":31545},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32227,\"start\":32220},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32918,\"start\":32911}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2174,\"start\":2162},{\"attributes\":{\"n\":\"2\"},\"end\":9680,\"start\":9668},{\"attributes\":{\"n\":\"2.1\"},\"end\":9703,\"start\":9683},{\"attributes\":{\"n\":\"2.2\"},\"end\":12070,\"start\":12014},{\"attributes\":{\"n\":\"3\"},\"end\":13850,\"start\":13835},{\"attributes\":{\"n\":\"3.1\"},\"end\":14535,\"start\":14490},{\"attributes\":{\"n\":\"3.2\"},\"end\":19035,\"start\":18989},{\"end\":20086,\"start\":20080},{\"attributes\":{\"n\":\"3.3\"},\"end\":24164,\"start\":24124},{\"attributes\":{\"n\":\"3.4\"},\"end\":26390,\"start\":26349},{\"attributes\":{\"n\":\"4\"},\"end\":27449,\"start\":27429},{\"attributes\":{\"n\":\"4.1\"},\"end\":27867,\"start\":27845},{\"attributes\":{\"n\":\"4.2\"},\"end\":29169,\"start\":29125},{\"attributes\":{\"n\":\"4.3\"},\"end\":30624,\"start\":30587},{\"attributes\":{\"n\":\"4.4\"},\"end\":32416,\"start\":32373},{\"attributes\":{\"n\":\"5\"},\"end\":34042,\"start\":34024},{\"end\":35533,\"start\":35525},{\"end\":35618,\"start\":35610},{\"end\":35738,\"start\":35730},{\"end\":35815,\"start\":35807},{\"end\":35899,\"start\":35890},{\"end\":36515,\"start\":36506},{\"end\":37220,\"start\":37211},{\"end\":38245,\"start\":38236}]", "table": "[{\"end\":36504,\"start\":35901},{\"end\":37209,\"start\":36517},{\"end\":38234,\"start\":37222},{\"end\":38772,\"start\":38432},{\"end\":39085,\"start\":38987}]", "figure_caption": "[{\"end\":35608,\"start\":35535},{\"end\":35728,\"start\":35620},{\"end\":35805,\"start\":35740},{\"end\":35888,\"start\":35817},{\"end\":38432,\"start\":38247},{\"end\":38987,\"start\":38775}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4689,\"start\":4683},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8060,\"start\":8054},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15786,\"start\":15780},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20935,\"start\":20926},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21671,\"start\":21663},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21726,\"start\":21717},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22201,\"start\":22192},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22822,\"start\":22813},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23959,\"start\":23950},{\"end\":33313,\"start\":33307},{\"end\":33888,\"start\":33882}]", "bib_author_first_name": "[{\"end\":40784,\"start\":40783},{\"end\":40798,\"start\":40797},{\"end\":40800,\"start\":40799},{\"end\":40812,\"start\":40811},{\"end\":40814,\"start\":40813},{\"end\":41033,\"start\":41032},{\"end\":41047,\"start\":41046},{\"end\":41054,\"start\":41053},{\"end\":41056,\"start\":41055},{\"end\":41279,\"start\":41278},{\"end\":41292,\"start\":41291},{\"end\":41498,\"start\":41497},{\"end\":41507,\"start\":41506},{\"end\":41682,\"start\":41681},{\"end\":41692,\"start\":41691},{\"end\":41848,\"start\":41847},{\"end\":41858,\"start\":41857},{\"end\":41872,\"start\":41871},{\"end\":41879,\"start\":41878},{\"end\":41887,\"start\":41886},{\"end\":41898,\"start\":41897},{\"end\":41900,\"start\":41899},{\"end\":42165,\"start\":42164},{\"end\":42174,\"start\":42173},{\"end\":42182,\"start\":42181},{\"end\":42445,\"start\":42444},{\"end\":42453,\"start\":42452},{\"end\":42461,\"start\":42460},{\"end\":42469,\"start\":42468},{\"end\":42745,\"start\":42744},{\"end\":42754,\"start\":42753},{\"end\":42761,\"start\":42760},{\"end\":42774,\"start\":42773},{\"end\":43064,\"start\":43063},{\"end\":43072,\"start\":43071},{\"end\":43080,\"start\":43079},{\"end\":43090,\"start\":43089},{\"end\":43092,\"start\":43091},{\"end\":43098,\"start\":43097},{\"end\":43104,\"start\":43103},{\"end\":43115,\"start\":43114},{\"end\":43122,\"start\":43121},{\"end\":43134,\"start\":43133},{\"end\":43147,\"start\":43146},{\"end\":43585,\"start\":43584},{\"end\":43750,\"start\":43749},{\"end\":43752,\"start\":43751},{\"end\":43765,\"start\":43764},{\"end\":43767,\"start\":43766},{\"end\":43973,\"start\":43972},{\"end\":43979,\"start\":43978},{\"end\":43988,\"start\":43987},{\"end\":43995,\"start\":43994},{\"end\":44211,\"start\":44210},{\"end\":44218,\"start\":44217},{\"end\":44225,\"start\":44224},{\"end\":44234,\"start\":44233},{\"end\":44241,\"start\":44240},{\"end\":44447,\"start\":44446},{\"end\":44453,\"start\":44452},{\"end\":44460,\"start\":44459},{\"end\":44565,\"start\":44562},{\"end\":44567,\"start\":44566},{\"end\":44584,\"start\":44580},{\"end\":44601,\"start\":44600},{\"end\":44621,\"start\":44602},{\"end\":44631,\"start\":44630},{\"end\":44910,\"start\":44909},{\"end\":44912,\"start\":44911},{\"end\":44921,\"start\":44920},{\"end\":44923,\"start\":44922},{\"end\":44933,\"start\":44932},{\"end\":44935,\"start\":44934},{\"end\":44944,\"start\":44943},{\"end\":45257,\"start\":45256},{\"end\":45264,\"start\":45263},{\"end\":45271,\"start\":45270},{\"end\":45279,\"start\":45278},{\"end\":45490,\"start\":45489},{\"end\":45492,\"start\":45491},{\"end\":45502,\"start\":45501},{\"end\":45641,\"start\":45640},{\"end\":45655,\"start\":45654},{\"end\":45668,\"start\":45667},{\"end\":45670,\"start\":45669},{\"end\":45956,\"start\":45955},{\"end\":45965,\"start\":45964},{\"end\":45967,\"start\":45966},{\"end\":45979,\"start\":45978},{\"end\":46156,\"start\":46155},{\"end\":46165,\"start\":46164},{\"end\":46174,\"start\":46173},{\"end\":46474,\"start\":46473},{\"end\":46483,\"start\":46482},{\"end\":46493,\"start\":46492},{\"end\":46505,\"start\":46504},{\"end\":46742,\"start\":46741},{\"end\":46752,\"start\":46751},{\"end\":46761,\"start\":46760},{\"end\":46763,\"start\":46762},{\"end\":46994,\"start\":46993},{\"end\":47006,\"start\":47005},{\"end\":47018,\"start\":47017},{\"end\":47032,\"start\":47031},{\"end\":47041,\"start\":47040},{\"end\":47051,\"start\":47050},{\"end\":47062,\"start\":47061},{\"end\":47329,\"start\":47328},{\"end\":47336,\"start\":47335},{\"end\":47343,\"start\":47342},{\"end\":47350,\"start\":47349},{\"end\":47352,\"start\":47351},{\"end\":47359,\"start\":47358},{\"end\":47361,\"start\":47360},{\"end\":47585,\"start\":47584},{\"end\":47592,\"start\":47591},{\"end\":47601,\"start\":47600},{\"end\":47610,\"start\":47609},{\"end\":47617,\"start\":47616},{\"end\":47887,\"start\":47886},{\"end\":47894,\"start\":47893},{\"end\":47904,\"start\":47903},{\"end\":48155,\"start\":48154},{\"end\":48161,\"start\":48160},{\"end\":48171,\"start\":48170},{\"end\":48177,\"start\":48176},{\"end\":48185,\"start\":48184},{\"end\":48193,\"start\":48192},{\"end\":48199,\"start\":48198},{\"end\":48506,\"start\":48505},{\"end\":48513,\"start\":48512},{\"end\":48521,\"start\":48520},{\"end\":48523,\"start\":48522},{\"end\":48821,\"start\":48820},{\"end\":48828,\"start\":48827},{\"end\":48837,\"start\":48836},{\"end\":48851,\"start\":48850},{\"end\":49055,\"start\":49054},{\"end\":49063,\"start\":49062},{\"end\":49079,\"start\":49078},{\"end\":49313,\"start\":49312},{\"end\":49322,\"start\":49321},{\"end\":49506,\"start\":49505},{\"end\":49515,\"start\":49514},{\"end\":49705,\"start\":49704},{\"end\":49714,\"start\":49713},{\"end\":49724,\"start\":49723},{\"end\":49734,\"start\":49733},{\"end\":49736,\"start\":49735},{\"end\":49925,\"start\":49924},{\"end\":49933,\"start\":49932},{\"end\":49935,\"start\":49934},{\"end\":50094,\"start\":50093},{\"end\":50108,\"start\":50107},{\"end\":50110,\"start\":50109},{\"end\":50317,\"start\":50316},{\"end\":50324,\"start\":50323},{\"end\":50332,\"start\":50331},{\"end\":50551,\"start\":50550},{\"end\":50558,\"start\":50557},{\"end\":50566,\"start\":50565},{\"end\":50574,\"start\":50573},{\"end\":50582,\"start\":50581},{\"end\":50843,\"start\":50842},{\"end\":50852,\"start\":50851},{\"end\":50859,\"start\":50858},{\"end\":50867,\"start\":50866},{\"end\":50875,\"start\":50874},{\"end\":51144,\"start\":51143},{\"end\":51153,\"start\":51152},{\"end\":51160,\"start\":51159},{\"end\":51166,\"start\":51165},{\"end\":51598,\"start\":51597},{\"end\":51607,\"start\":51606},{\"end\":51614,\"start\":51613},{\"end\":51871,\"start\":51870},{\"end\":51880,\"start\":51879},{\"end\":51886,\"start\":51885},{\"end\":51896,\"start\":51895},{\"end\":52244,\"start\":52243},{\"end\":52255,\"start\":52254},{\"end\":52269,\"start\":52268},{\"end\":52271,\"start\":52270}]", "bib_author_last_name": "[{\"end\":40795,\"start\":40785},{\"end\":40809,\"start\":40801},{\"end\":40820,\"start\":40815},{\"end\":41044,\"start\":41034},{\"end\":41051,\"start\":41048},{\"end\":41062,\"start\":41057},{\"end\":41289,\"start\":41280},{\"end\":41300,\"start\":41293},{\"end\":41504,\"start\":41499},{\"end\":41514,\"start\":41508},{\"end\":41689,\"start\":41683},{\"end\":41698,\"start\":41693},{\"end\":41855,\"start\":41849},{\"end\":41869,\"start\":41859},{\"end\":41876,\"start\":41873},{\"end\":41884,\"start\":41880},{\"end\":41895,\"start\":41888},{\"end\":41907,\"start\":41901},{\"end\":42171,\"start\":42166},{\"end\":42179,\"start\":42175},{\"end\":42196,\"start\":42183},{\"end\":42450,\"start\":42446},{\"end\":42458,\"start\":42454},{\"end\":42466,\"start\":42462},{\"end\":42474,\"start\":42470},{\"end\":42751,\"start\":42746},{\"end\":42758,\"start\":42755},{\"end\":42771,\"start\":42762},{\"end\":42785,\"start\":42775},{\"end\":43069,\"start\":43065},{\"end\":43077,\"start\":43073},{\"end\":43087,\"start\":43081},{\"end\":43095,\"start\":43093},{\"end\":43101,\"start\":43099},{\"end\":43112,\"start\":43105},{\"end\":43119,\"start\":43116},{\"end\":43131,\"start\":43123},{\"end\":43144,\"start\":43135},{\"end\":43158,\"start\":43148},{\"end\":43593,\"start\":43586},{\"end\":43762,\"start\":43753},{\"end\":43773,\"start\":43768},{\"end\":43976,\"start\":43974},{\"end\":43985,\"start\":43980},{\"end\":43992,\"start\":43989},{\"end\":44000,\"start\":43996},{\"end\":44215,\"start\":44212},{\"end\":44222,\"start\":44219},{\"end\":44231,\"start\":44226},{\"end\":44238,\"start\":44235},{\"end\":44247,\"start\":44242},{\"end\":44450,\"start\":44448},{\"end\":44457,\"start\":44454},{\"end\":44465,\"start\":44461},{\"end\":44578,\"start\":44568},{\"end\":44598,\"start\":44585},{\"end\":44628,\"start\":44622},{\"end\":44918,\"start\":44913},{\"end\":44930,\"start\":44924},{\"end\":44941,\"start\":44936},{\"end\":44950,\"start\":44945},{\"end\":45261,\"start\":45258},{\"end\":45268,\"start\":45265},{\"end\":45276,\"start\":45272},{\"end\":45285,\"start\":45280},{\"end\":45499,\"start\":45493},{\"end\":45511,\"start\":45503},{\"end\":45652,\"start\":45642},{\"end\":45665,\"start\":45656},{\"end\":45677,\"start\":45671},{\"end\":45962,\"start\":45957},{\"end\":45976,\"start\":45968},{\"end\":45983,\"start\":45980},{\"end\":46162,\"start\":46157},{\"end\":46171,\"start\":46166},{\"end\":46178,\"start\":46175},{\"end\":46480,\"start\":46475},{\"end\":46490,\"start\":46484},{\"end\":46502,\"start\":46494},{\"end\":46510,\"start\":46506},{\"end\":46749,\"start\":46743},{\"end\":46758,\"start\":46753},{\"end\":46769,\"start\":46764},{\"end\":47003,\"start\":46995},{\"end\":47015,\"start\":47007},{\"end\":47029,\"start\":47019},{\"end\":47038,\"start\":47033},{\"end\":47048,\"start\":47042},{\"end\":47059,\"start\":47052},{\"end\":47067,\"start\":47063},{\"end\":47333,\"start\":47330},{\"end\":47340,\"start\":47337},{\"end\":47347,\"start\":47344},{\"end\":47356,\"start\":47353},{\"end\":47367,\"start\":47362},{\"end\":47589,\"start\":47586},{\"end\":47598,\"start\":47593},{\"end\":47607,\"start\":47602},{\"end\":47614,\"start\":47611},{\"end\":47621,\"start\":47618},{\"end\":47891,\"start\":47888},{\"end\":47901,\"start\":47895},{\"end\":47912,\"start\":47905},{\"end\":48158,\"start\":48156},{\"end\":48168,\"start\":48162},{\"end\":48174,\"start\":48172},{\"end\":48182,\"start\":48178},{\"end\":48190,\"start\":48186},{\"end\":48196,\"start\":48194},{\"end\":48205,\"start\":48200},{\"end\":48510,\"start\":48507},{\"end\":48518,\"start\":48514},{\"end\":48528,\"start\":48524},{\"end\":48825,\"start\":48822},{\"end\":48834,\"start\":48829},{\"end\":48848,\"start\":48838},{\"end\":48855,\"start\":48852},{\"end\":49060,\"start\":49056},{\"end\":49076,\"start\":49064},{\"end\":49091,\"start\":49080},{\"end\":49319,\"start\":49314},{\"end\":49327,\"start\":49323},{\"end\":49512,\"start\":49507},{\"end\":49520,\"start\":49516},{\"end\":49711,\"start\":49706},{\"end\":49721,\"start\":49715},{\"end\":49731,\"start\":49725},{\"end\":49746,\"start\":49737},{\"end\":49930,\"start\":49926},{\"end\":49941,\"start\":49936},{\"end\":50105,\"start\":50095},{\"end\":50115,\"start\":50111},{\"end\":50321,\"start\":50318},{\"end\":50329,\"start\":50325},{\"end\":50336,\"start\":50333},{\"end\":50555,\"start\":50552},{\"end\":50563,\"start\":50559},{\"end\":50571,\"start\":50567},{\"end\":50579,\"start\":50575},{\"end\":50588,\"start\":50583},{\"end\":50849,\"start\":50844},{\"end\":50856,\"start\":50853},{\"end\":50864,\"start\":50860},{\"end\":50872,\"start\":50868},{\"end\":50881,\"start\":50876},{\"end\":51150,\"start\":51145},{\"end\":51157,\"start\":51154},{\"end\":51163,\"start\":51161},{\"end\":51172,\"start\":51167},{\"end\":51604,\"start\":51599},{\"end\":51611,\"start\":51608},{\"end\":51620,\"start\":51615},{\"end\":51877,\"start\":51872},{\"end\":51883,\"start\":51881},{\"end\":51893,\"start\":51887},{\"end\":51899,\"start\":51897},{\"end\":52252,\"start\":52245},{\"end\":52266,\"start\":52256},{\"end\":52276,\"start\":52272}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":201313522},\"end\":40973,\"start\":40720},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52059988},\"end\":41202,\"start\":40975},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4493958},\"end\":41450,\"start\":41204},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":118713138},\"end\":41627,\"start\":41452},{\"attributes\":{\"id\":\"b4\"},\"end\":41798,\"start\":41629},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53770387},\"end\":42075,\"start\":41800},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51882734},\"end\":42362,\"start\":42077},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":51989956},\"end\":42670,\"start\":42364},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1475121},\"end\":42977,\"start\":42672},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9044077},\"end\":43543,\"start\":42979},{\"attributes\":{\"id\":\"b10\"},\"end\":43698,\"start\":43545},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13542286},\"end\":43898,\"start\":43700},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1663191},\"end\":44150,\"start\":43900},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":49672261},\"end\":44420,\"start\":44152},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1264129},\"end\":44558,\"start\":44422},{\"attributes\":{\"id\":\"b15\"},\"end\":44820,\"start\":44560},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53528301},\"end\":45188,\"start\":44822},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195803430},\"end\":45431,\"start\":45190},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":49657329},\"end\":45638,\"start\":45433},{\"attributes\":{\"id\":\"b19\"},\"end\":45897,\"start\":45640},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53751136},\"end\":46153,\"start\":45899},{\"attributes\":{\"doi\":\"arXiv:1906.00651\",\"id\":\"b21\"},\"end\":46420,\"start\":46155},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":173990648},\"end\":46686,\"start\":46422},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17093085},\"end\":46931,\"start\":46688},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3846544},\"end\":47275,\"start\":46933},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":47007607},\"end\":47535,\"start\":47277},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":29151865},\"end\":47803,\"start\":47537},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3004151},\"end\":48069,\"start\":47805},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4840263},\"end\":48397,\"start\":48071},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10987457},\"end\":48720,\"start\":48399},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6253552},\"end\":49052,\"start\":48722},{\"attributes\":{\"id\":\"b31\"},\"end\":49253,\"start\":49054},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9715523},\"end\":49468,\"start\":49255},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":53116261},\"end\":49638,\"start\":49470},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":51717690},\"end\":49903,\"start\":49640},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":13058320},\"end\":50025,\"start\":49905},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3707854},\"end\":50254,\"start\":50027},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":21618854},\"end\":50476,\"start\":50256},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":201667906},\"end\":50761,\"start\":50478},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":996788},\"end\":51085,\"start\":50763},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1900475},\"end\":51520,\"start\":51087},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":10514149},\"end\":51778,\"start\":51522},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15614128},\"end\":52119,\"start\":51780},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":56475900},\"end\":52482,\"start\":52121}]", "bib_title": "[{\"end\":40781,\"start\":40720},{\"end\":41030,\"start\":40975},{\"end\":41276,\"start\":41204},{\"end\":41495,\"start\":41452},{\"end\":41845,\"start\":41800},{\"end\":42162,\"start\":42077},{\"end\":42442,\"start\":42364},{\"end\":42742,\"start\":42672},{\"end\":43061,\"start\":42979},{\"end\":43747,\"start\":43700},{\"end\":43970,\"start\":43900},{\"end\":44208,\"start\":44152},{\"end\":44444,\"start\":44422},{\"end\":44907,\"start\":44822},{\"end\":45254,\"start\":45190},{\"end\":45487,\"start\":45433},{\"end\":45953,\"start\":45899},{\"end\":46471,\"start\":46422},{\"end\":46739,\"start\":46688},{\"end\":46991,\"start\":46933},{\"end\":47326,\"start\":47277},{\"end\":47582,\"start\":47537},{\"end\":47884,\"start\":47805},{\"end\":48152,\"start\":48071},{\"end\":48503,\"start\":48399},{\"end\":48818,\"start\":48722},{\"end\":49310,\"start\":49255},{\"end\":49503,\"start\":49470},{\"end\":49702,\"start\":49640},{\"end\":49922,\"start\":49905},{\"end\":50091,\"start\":50027},{\"end\":50314,\"start\":50256},{\"end\":50548,\"start\":50478},{\"end\":50840,\"start\":50763},{\"end\":51141,\"start\":51087},{\"end\":51595,\"start\":51522},{\"end\":51868,\"start\":51780},{\"end\":52241,\"start\":52121}]", "bib_author": "[{\"end\":40797,\"start\":40783},{\"end\":40811,\"start\":40797},{\"end\":40822,\"start\":40811},{\"end\":41046,\"start\":41032},{\"end\":41053,\"start\":41046},{\"end\":41064,\"start\":41053},{\"end\":41291,\"start\":41278},{\"end\":41302,\"start\":41291},{\"end\":41506,\"start\":41497},{\"end\":41516,\"start\":41506},{\"end\":41691,\"start\":41681},{\"end\":41700,\"start\":41691},{\"end\":41857,\"start\":41847},{\"end\":41871,\"start\":41857},{\"end\":41878,\"start\":41871},{\"end\":41886,\"start\":41878},{\"end\":41897,\"start\":41886},{\"end\":41909,\"start\":41897},{\"end\":42173,\"start\":42164},{\"end\":42181,\"start\":42173},{\"end\":42198,\"start\":42181},{\"end\":42452,\"start\":42444},{\"end\":42460,\"start\":42452},{\"end\":42468,\"start\":42460},{\"end\":42476,\"start\":42468},{\"end\":42753,\"start\":42744},{\"end\":42760,\"start\":42753},{\"end\":42773,\"start\":42760},{\"end\":42787,\"start\":42773},{\"end\":43071,\"start\":43063},{\"end\":43079,\"start\":43071},{\"end\":43089,\"start\":43079},{\"end\":43097,\"start\":43089},{\"end\":43103,\"start\":43097},{\"end\":43114,\"start\":43103},{\"end\":43121,\"start\":43114},{\"end\":43133,\"start\":43121},{\"end\":43146,\"start\":43133},{\"end\":43160,\"start\":43146},{\"end\":43595,\"start\":43584},{\"end\":43764,\"start\":43749},{\"end\":43775,\"start\":43764},{\"end\":43978,\"start\":43972},{\"end\":43987,\"start\":43978},{\"end\":43994,\"start\":43987},{\"end\":44002,\"start\":43994},{\"end\":44217,\"start\":44210},{\"end\":44224,\"start\":44217},{\"end\":44233,\"start\":44224},{\"end\":44240,\"start\":44233},{\"end\":44249,\"start\":44240},{\"end\":44452,\"start\":44446},{\"end\":44459,\"start\":44452},{\"end\":44467,\"start\":44459},{\"end\":44580,\"start\":44562},{\"end\":44600,\"start\":44580},{\"end\":44630,\"start\":44600},{\"end\":44634,\"start\":44630},{\"end\":44920,\"start\":44909},{\"end\":44932,\"start\":44920},{\"end\":44943,\"start\":44932},{\"end\":44952,\"start\":44943},{\"end\":45263,\"start\":45256},{\"end\":45270,\"start\":45263},{\"end\":45278,\"start\":45270},{\"end\":45287,\"start\":45278},{\"end\":45501,\"start\":45489},{\"end\":45513,\"start\":45501},{\"end\":45654,\"start\":45640},{\"end\":45667,\"start\":45654},{\"end\":45679,\"start\":45667},{\"end\":45964,\"start\":45955},{\"end\":45978,\"start\":45964},{\"end\":45985,\"start\":45978},{\"end\":46164,\"start\":46155},{\"end\":46173,\"start\":46164},{\"end\":46180,\"start\":46173},{\"end\":46482,\"start\":46473},{\"end\":46492,\"start\":46482},{\"end\":46504,\"start\":46492},{\"end\":46512,\"start\":46504},{\"end\":46751,\"start\":46741},{\"end\":46760,\"start\":46751},{\"end\":46771,\"start\":46760},{\"end\":47005,\"start\":46993},{\"end\":47017,\"start\":47005},{\"end\":47031,\"start\":47017},{\"end\":47040,\"start\":47031},{\"end\":47050,\"start\":47040},{\"end\":47061,\"start\":47050},{\"end\":47069,\"start\":47061},{\"end\":47335,\"start\":47328},{\"end\":47342,\"start\":47335},{\"end\":47349,\"start\":47342},{\"end\":47358,\"start\":47349},{\"end\":47369,\"start\":47358},{\"end\":47591,\"start\":47584},{\"end\":47600,\"start\":47591},{\"end\":47609,\"start\":47600},{\"end\":47616,\"start\":47609},{\"end\":47623,\"start\":47616},{\"end\":47893,\"start\":47886},{\"end\":47903,\"start\":47893},{\"end\":47914,\"start\":47903},{\"end\":48160,\"start\":48154},{\"end\":48170,\"start\":48160},{\"end\":48176,\"start\":48170},{\"end\":48184,\"start\":48176},{\"end\":48192,\"start\":48184},{\"end\":48198,\"start\":48192},{\"end\":48207,\"start\":48198},{\"end\":48512,\"start\":48505},{\"end\":48520,\"start\":48512},{\"end\":48530,\"start\":48520},{\"end\":48827,\"start\":48820},{\"end\":48836,\"start\":48827},{\"end\":48850,\"start\":48836},{\"end\":48857,\"start\":48850},{\"end\":49062,\"start\":49054},{\"end\":49078,\"start\":49062},{\"end\":49093,\"start\":49078},{\"end\":49321,\"start\":49312},{\"end\":49329,\"start\":49321},{\"end\":49514,\"start\":49505},{\"end\":49522,\"start\":49514},{\"end\":49713,\"start\":49704},{\"end\":49723,\"start\":49713},{\"end\":49733,\"start\":49723},{\"end\":49748,\"start\":49733},{\"end\":49932,\"start\":49924},{\"end\":49943,\"start\":49932},{\"end\":50107,\"start\":50093},{\"end\":50117,\"start\":50107},{\"end\":50323,\"start\":50316},{\"end\":50331,\"start\":50323},{\"end\":50338,\"start\":50331},{\"end\":50557,\"start\":50550},{\"end\":50565,\"start\":50557},{\"end\":50573,\"start\":50565},{\"end\":50581,\"start\":50573},{\"end\":50590,\"start\":50581},{\"end\":50851,\"start\":50842},{\"end\":50858,\"start\":50851},{\"end\":50866,\"start\":50858},{\"end\":50874,\"start\":50866},{\"end\":50883,\"start\":50874},{\"end\":51152,\"start\":51143},{\"end\":51159,\"start\":51152},{\"end\":51165,\"start\":51159},{\"end\":51174,\"start\":51165},{\"end\":51606,\"start\":51597},{\"end\":51613,\"start\":51606},{\"end\":51622,\"start\":51613},{\"end\":51879,\"start\":51870},{\"end\":51885,\"start\":51879},{\"end\":51895,\"start\":51885},{\"end\":51901,\"start\":51895},{\"end\":52254,\"start\":52243},{\"end\":52268,\"start\":52254},{\"end\":52278,\"start\":52268}]", "bib_venue": "[{\"end\":40830,\"start\":40822},{\"end\":41068,\"start\":41064},{\"end\":41316,\"start\":41302},{\"end\":41524,\"start\":41516},{\"end\":41679,\"start\":41629},{\"end\":41913,\"start\":41909},{\"end\":42202,\"start\":42198},{\"end\":42484,\"start\":42476},{\"end\":42790,\"start\":42787},{\"end\":43164,\"start\":43160},{\"end\":43582,\"start\":43545},{\"end\":43780,\"start\":43775},{\"end\":44006,\"start\":44002},{\"end\":44253,\"start\":44249},{\"end\":44475,\"start\":44467},{\"end\":44678,\"start\":44634},{\"end\":44971,\"start\":44952},{\"end\":45291,\"start\":45287},{\"end\":45517,\"start\":45513},{\"end\":45752,\"start\":45679},{\"end\":45989,\"start\":45985},{\"end\":46258,\"start\":46196},{\"end\":46520,\"start\":46512},{\"end\":46795,\"start\":46771},{\"end\":47073,\"start\":47069},{\"end\":47377,\"start\":47369},{\"end\":47637,\"start\":47623},{\"end\":47917,\"start\":47914},{\"end\":48210,\"start\":48207},{\"end\":48538,\"start\":48530},{\"end\":48861,\"start\":48857},{\"end\":49137,\"start\":49093},{\"end\":49337,\"start\":49329},{\"end\":49526,\"start\":49522},{\"end\":49751,\"start\":49748},{\"end\":49947,\"start\":49943},{\"end\":50125,\"start\":50117},{\"end\":50346,\"start\":50338},{\"end\":50594,\"start\":50590},{\"end\":50886,\"start\":50883},{\"end\":51251,\"start\":51174},{\"end\":51625,\"start\":51622},{\"end\":51930,\"start\":51901},{\"end\":52282,\"start\":52278},{\"end\":51315,\"start\":51253}]"}}}, "year": 2023, "month": 12, "day": 17}