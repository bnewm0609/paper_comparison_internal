{"id": 257913374, "updated": "2023-10-05 02:32:30.445", "metadata": {"title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models", "authors": "[{\"first\":\"Zhihang\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Niu\",\"middle\":[]},{\"first\":\"Jiawei\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Wenyu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xinggang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yuzhang\",\"last\":\"Shang\",\"middle\":[]},{\"first\":\"Guangyu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Jiaxiang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Bingzhe\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large-scale language models (LLMs) have demonstrated impressive performance, but their deployment presents challenges due to their significant memory usage. This issue can be alleviated through quantization. In this paper, we identify that the challenge in quantizing activations in LLMs arises from varying ranges across channels, rather than solely the presence of outliers. To address this challenge, we introduce a quantization method called RPTQ, which utilizes a reorder-based approach. By rearranging the channels and quantizing them in clusters, RPTQ effectively mitigates the impact of range differences between channels. To minimize the overhead of the reorder operation, we fuse it into the layer norm operation and weights in linear layers. In our experiments, RPTQ achieved a significant breakthrough by utilizing 3-bit activation in LLMs for the first time, resulting in a substantial reduction in memory usage. For instance, quantizing OPT-175b can lead to a memory consumption reduction of up to 80%.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.01089", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-01089", "doi": "10.48550/arxiv.2304.01089"}}, "content": {"source": {"pdf_hash": "2a44c6b7f291f625314a82ba3131e605009fd533", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.01089v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7d3d43de20fa853799b7fdec702615c43068ddfa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2a44c6b7f291f625314a82ba3131e605009fd533.txt", "contents": "\nRPTQ: Reorder-based Post-training Quantization for Large Language Models\n\n\nZhihang Yuan \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nLin Niu \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nJiawei Liu \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nWenyu Liu \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nXinggang Wang \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nYuzhang Shang \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nGuangyu Sun \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nQiang Wu \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nHoumo Ai \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nJiaxiang Wu \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nBingzhe Wu \nTencent AI Lab\nTencent AI Lab\nHuazhong University of Science & Technology\nIllinois Institute of Technology\nPeking University\nHoumoAI\n\nRPTQ: Reorder-based Post-training Quantization for Large Language Models\n\nLarge-scale language models (LLMs) have demonstrated impressive performance, but their deployment presents challenges due to their significant memory usage. This issue can be alleviated through quantization. In this paper, we identify that the challenge in quantizing activations in LLMs arises from varying ranges across channels, rather than solely the presence of outliers. To address this challenge, we introduce a quantization method called RPTQ, which utilizes a reorder-based approach. By rearranging the channels and quantizing them in clusters, RPTQ effectively mitigates the impact of range differences between channels. To minimize the overhead of the reorder operation, we fuse it into the layer norm operation and weights in linear layers. In our experiments, RPTQ achieved a significant breakthrough by utilizing 3-bit activation in LLMs for the first time, resulting in a substantial reduction in memory usage. For instance, quantizing OPT-175b can lead to a memory consumption reduction of up to 80%. The code is in https://github.com/hahnyuan/RPTQ4LLM.\n\nIntroduction\n\nLarge-scale language models (LLMs) have demonstrated impressive performance in various tasks, but their deployment poses challenges due to their enormous model size. For example, the OPT-175B model [40] contains 175 billion parameters, which require significant memory to store As the sequence length and batch size increase, the problem of memory consumption becomes more severe because activations. In some cases, the key and value cache can consume more than 100 times the memory of the weights. However, a single GPU or server does not possess sufficient memory capacity to store such massive weights and activations. To address this issue, LLMs are often divided into multiple chunks and stored on different devices. However, this requires data to be transferred between devices during computation, leading to significant bandwidth and energy consumption [1; 30].\n\nTo address the challenges posed by LLMs' high memory usage, model quantization has emerged as a promising solution. This technique involves quantizing both the weights and activations of LLMs using low-bit integers, resulting in a significant reduction in storage and computational costs. Specifically, quantization reduces memory requirements for saving weights and activations and accelerates compute-intensive operations like Matrix Multiplication and linear layers. By quantizing weights and activations, storage and communication overhead is reduced, leading to improved efficiency and faster inference times. Quantization methods are typically divided into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). While QAT methods can lead to higher accuracy in most cases, they require significant computational resources to train the models, making them less practical for LLMs that already have significant training costs. In contrast, PTQ methods can quantize pre-trained models without additional training, making them more practical for larger models that require significant computational and memory resources. This paper focuses on PTQ for LLMs.\n\nIn this paper, we highlights the challenge of quantizing the activations of LLMs, which is attributed to the significant variations in the values across different channels 2 , as shown in Figure 1. Two observations can be made from this figure: 1) Some channels exhibit significant outliers, with maximum or minimum values that are hundreds of times larger than those of other channels. Previous studies [34; 11] have also identified this issue and proposed special treatment for outliers. 2) Different channels exhibit significant difference in the range of values. Quantizing different channels using the same quantization parameter can lead to substantial quantization errors. Even if two channels have the same absolute value of outliers, they can exhibit strong difference in the range of numerical values. For instance, one channel may have a range of -100 to -50, while another channel may have a range of 80 to 100. Using the same quantization parameters for them can lead to significant quantization errors, which is a challenge that has not been effectively addressed in previous works.\n\nTo address the issue of quantizing activations with channels that have significantly different ranges, we propose a method called RPTQ. This method involves clustering channels in activations that exhibit similar value ranges, followed by the quantization with the same quantization parameter to the values within each cluster. Consequently, channels displaying considerable discrepancies in numerical ranges can utilize distinct quantization parameters, leading to a significant reduction in quantization error. Furthermore, we propose strategies to avoid explicit reordering, thereby decreasing computational overhead and enhancing inference efficiency. We propose a modified layer norm operation to yield reordered activations directly, obviating the necessity for explicit channel adjustments during the inference process. In addition, we reorganize the weights of linear layers to enable them to directly accept and produce activations in a sorted order.\n\nOur experiments demonstrate that RTPQ is an effective solution for addressing the issue of quantizing the activations of LLMs. Clustering the channels in only a small number of clusters can significantly reduce quantization errors and improve the accuracy of quantized LLMs. The results show that RPTQ can achieve significant reductions in memory for LLMs while maintaining high levels of accuracy. For instance, by quantizing OPT-175b, memory usage can be reduced by 73% with a perplexity loss of less than 0.5 or by 80% with a perplexity loss of less than 1.5 across different sequence lengths and batch sizes.\n\n\nRelated Work\n\n\nLarge Language Model\n\nLarge Language Models (LLMs) have shown immense potential for various applications. A lot of LLMs have been developed [12; 4; 29; 39; 2; 32]. These models have exhibited exceptional performance, but at the cost of significant memory consumption, which poses significant challenges for their deployment [19]. Ways to solve this problem include model compression [14], distributed computing [1] and computational graph optimization [10]. In this study, we focus on compressing LLMs through quantization.\n\n\nQuantization\n\nQuantization is an important technique for reducing the computational and memory requirements of deep neural networks (DNNs). There are two main categories of quantization methods for DNNs: posttraining quantization (PTQ) [41; 33; 25; 20; 38] and quantization-aware training (QAT) [8; 42; 5; 17]. PTQ methods involve quantizing pre-trained models, while QAT methods involve training models with quantization constraints. While QAT methods have been shown to improve the accuracy of DNNs in some cases, they require significant computational resources to train the models. For instance, LSQ introduces a differentiable quantization function, which enables gradient-based optimization during training [13]. LSQ involves quantizing and de-quantizing the activation and weights, which requires additional computations. Additionally, LSQ involves optimizing the quantization parameters, which requires extra training epochs to solve and update the gradient of the parameters. This makes them less practical for large-scale language models (LLMs) that already have high training costs. In contrast, PTQ methods are more feasible for LLMs, as they involve quantizing pre-trained models, which do not require additional training time.\n\nRecently, there are some multi-billion scale transformer quantization methods designed for LLMs. ZeroQuant [35] proposes a fine-grained quantization scheme that can be applied to both weights and activations. It treats each layer of the transformer as a small neural network and uses the FP model to distill the quantization model. nuQmm [27] utilizes group-wise binary-coding non-uniform quantization scheme, and propose a specialized multiplicative kernel to speed up the operation. LLM.int8() [11] observes that a significant contributor to poor quantization performance is outliers in activations. The method fixes this with mixed-precision quantization. SmoothQuant [34] migrates the quantization difficulty from activations to weights by proposing a mathematically equivalent per-channel scaling transformation. This transformation smooths the magnitude across channels, making the model quantization-friendly. GPTQ [15] uses second-order approximation to quantize weights, enabling the weight quantization of LLMs into 4-bit -the first post-training method to do so. However, these methods can only achieve the quantization of activations to 8 bits. Comprehensive study [36] has improved ZeroQuant, treating each linear layer of the transformer as a small neural network for distillation, and achieved usable performance at W4A8 quantization. PTQ-SL [37] proposed that adjusting the channel order of weights can lead to higher accuracy in finely-quantized networks. However, PTQ-SL mainly focuses on the quantization of weights in convolutional networks, and does not address the quantization issues of activations. PGQ [3] employs a range-based permutation of the embedding dimensions and share quantization parameters among elements in the same group to address the problem of activation quantization. Nonetheless, it only consider for the dynamic range and utilizes uniformly divided groups, rendering it less efficacious for LLMs.\n\n\nPTQ on LLM\n\n\nPost-training Quantization\n\nPost-training quantization is a powerful technique for compressing neural networks. Although non-uniform quantization can achieve a relatively small quantization error, they require specialized hardware that is not widely accessible [16]. In contrast, uniform quantization is a more practical and feasible approach that can be efficiently executed on regular hardware devices. Therefore, our study focuses on uniform quantization techniques. Typically, we use uniform quantization function Q k to transform a float value x to k bits integer x q as follows:\nx q = Q k (x, s, z) = clamp(round( x s ) + z, \u22122 k\u22121 , 2 k\u22121 \u2212 1),(1)\nwhere s represents the scaling factor, z denotes the zero point, and the clamp function constrains the value within the range of a k-bit integer, specifically\n[\u22122 k\u22121 , 2 k\u22121 \u2212 1]\n. For a 4-bit integer, the range is [-8, 7]. The integer x q can be de-quantized tox = s(x q \u2212 z) \u2248 x. The de-quantized valuex is a float. The quantization parameters, scale factor s, and zero point z must be stored in memory for both quantization and de-quantization processes. To further reduce the storage and computational overhead of quantization, multiple weights or activation values X = {x 1 , ..., x n } share the same quantization parameters.\n\nThere are three steps in post-training quantization (PTQ). The first step is to specify the quantization settings, which include the bit-width k and quantization type. The bit-width determines the number of bits used to represent a numerical value in a quantized format. The quantization types include static and dynamic quantization. Static quantization is a method in which the quantization parameters of activations are computed prior to deployment, and all inputs during runtime are quantized using the same set of parameters. Dynamic quantization, on the other hand, set the quantization parameters during runtime [35]. Dynamic quantization is generally more accurate than static quantization, as it can adapt to the distribution of activations. However, it can result in slower inference times, as the quantization parameters should be computed on-the-fly for each input data. We focus on static quantization in this paper.\n\nNext, a calibration dataset consisting of input samples is used to compute the activations of each layer. The purpose of this step is to capture the distribution of numerical values that are likely to appear during inference. Finally, the quantization parameters for activations and weights are selected. There are many methods to select the parameters. One of the commonly used methods is Min-Max method. This method involves computing the maximum value X max = max(X) and minimum value X min = min(X) of tensor X that share the quantization parameters. The scaling factor s and zero point z are set as follows:\ns = X max \u2212 X min 2 k , z = \u2212round( X max + X min 2s ).(2)\n\nChallenges in Activation Quantization\n\nRecently, the weights of LLMs have been successfully quantized to 4 bits or even lower [15] with PTQ. However, quantizing the activations in LLMs remains a challenging task. As shown in Figure 1, the activations in LLMs exhibit significant variations in value range across different channels, which can have a significant impact on the quantization process. Per-tensor quantization techniques, which quantize the entire tensor using the same quantization parameters, may not be effective. The reason is setting the quantization range to cover a large value range may result in channels with small numerical values taking a large quantization error, while setting it to cover a small value range may lead to significant truncation of outliers and resulting in significant quantization errors. For instance, one channel may have a range of -100 to -50, while another channel may have a range of 80 to 100. Attempting to cover their ranges by quantizing from -100 to 100 would result in a significant quantization error for both channels.\n\nPrevious research has proposed several methods to address the issue of quantizing activations in LLMs. As shown in Figure 2(a), LLM.int8() [11] utilizes mixed-precision quantization by using high-precision data types (FP16) to quantize the outliers in activations and low-precision data types (INT8) for the remaining values. Using FP16 for these few exceptional channels can prevent errors caused by quantization. As shown in Figure 2(b), SmoothQuant [34] tackles the quantization difficulty by introducing a mathematically equivalent per-channel scaling transformation that smooths the magnitude across channels, making the activations more amenable to quantization. Specifically, the activation channels with large values are multiplied by a small factor \u03b1, and the corresponding weights for processing these channels in the next layer are multiplied by 1/\u03b1. However, SmoothQuant may not be effective in addressing the high variance of value range across channels. Additionally, this approach may also lead to issues with weight quantization. As shown in Figure 2(c), Per-vector scaled quantization (VSQ) [9; 18; 11], has been proposed sharing quantization parameters for adjacent columns or rows of n activation values. Although the approach of setting a quantization parameter for adjacent n values can help alleviate the issue of varying numerical values, the computation and storage overhead of calculating and storing these parameters for each set can be significant.\n\n\nReorder-based Quantization\n\n\nClustering and Reordering of Channels\n\nIn the section above, it was observed that there are notable variations in the activations across channels.\n\nTo solve this problem, we propose a novel reorder-based quantization approach called RPTQ. The main idea of this approach is to cluster the channels in the activations and reorganize them for quantization as shown in Figure 2(d).\n\nTo implement our reorder-based quantization approach, we first utilize a calibration dataset as input for inference, from which we derive the maximum and minimum values for each activation channel. Subsequently, we employ the K-Means algorithm [21] to categorize the distinct channels into g clusters, based on the points formed by each channel's maximum and minimum values. Once the clusters are established, we proceed with channel reordering by positioning channels from the same cluster together. Following the reordering process, we quantize the activations within each cluster. Specifically, we calculate the quantization parameters (scale s and zero point z) individually for each cluster. As a result, channels with analogous maximum and minimum values are assembled together and share the same quantization parameters. This method guarantees optimization of the quantization process for every cluster, ultimately reducing the quantization error.\n\nWe formalize the reordering process as follows: Let X \u2208 R B\u00d7N \u00d7C be the activation tensor, where B is the number of calibration samples, C is the number of channels and N is the number of tokens. We first compute the minimum and maximum values of each channel, denoted as X min \u2208 R C and X max \u2208 R C , respectively, using the calibration dataset:\nX min = N min n=1 B min b=1 X b,n , X max = N max n=1 B max b=1 X b,n .(3)\nThen, we group the channels into g clusters using K-means clustering based on the values of (X min,i , X max,i ) for each channel. Let S 1 , S 2 , ..., S g be the sets of channels' indexes in each cluster, where\nS i \u2286 {1, 2, ..., C} and g i=1 S i = {1, 2, ..., C}.\nFinally, we reorder the channels in X based on the indexes. We concatenates all the indices as a vector S = [S 1 , S 2 , ..., S g ]. We obtain the reordered activation tensorX :,:,i = X :,:,Si .\n\nAs illustrated in Figure 2, our approach presents several advantages compared to previous methods. Firstly, RPTQ is more adept at addressing the challenge of channel difference in activations. By clustering channels with similar value ranges, it diminishes the influence of both outliers and range differences. Secondly, RPTQ exhibits memory and computation efficiency, as it only requires managing quantization parameters for each cluster rather than each individual channel or vector.\n\n\nAvoid Explicit Reordering and Misalignment\n\nExplicit reordering is an operation in which the channels in activations are rearranged during run-time by physically relocating the data of different channels from one memory location to another. The reordering process will increase inference latency, particularly for large-scale models with a high number of channels. Additionally, storing both source and target activation tensors contributes to memory overhead. To minimize the overhead of the reorder operation, we fuse it into other operations.\n\nFirstly, we fuse the reorder operation into the layer norm operation. Specifically, after computing the layer norm results for each channel, the results are written back to DRAM. The write address is additionally offset based on the reorder index. This enables layer norm to directly store the results as reordered activations without affecting the weight and bias parameters of the transformer layer. As illustrated in Figure 3, we modify the two layer norm operations in the transformer layer.\n\nSecondly, we adjust the weight parameters of the network to allow linear layers to directly accept reordered activations and output reordered activations. Let W \u2208 R C2\u00d7C1 be the weight matrix. The original computation of a linear layer can be expressed as Y = bmm(X, W ), where bmm denotes batched matrix multiplication. We reorder the columns of the weight matrix (dimension C1) based on the input activation reorder index, and the rows (dimension C2) based on the output activation reorder index. The new weight matrixW \u2208 R C2\u00d7C1 is obtained by rearranging the rows and columns of the original weight matrix W . The modified linear layer can be expressed as\u1ef8 = bmm(X,W ), where X is the reordered input. The channel ordering of output tensor\u1ef8 adheres to the same order as the channel ordering of dimension C2 of the weight. Note that the weight reordering can be completed before deployment, resulting in zero overhead related to reordering during inference.\n\nMisalignment between two tensors' channels refers to a situation in which the ordering of channels in one tensor differs from that in the other tensor. This can occur when applying operations such as matrix multiplication or element-wise addition between tensors with different channel orders. In such instances, misalignment can cause errors in calculation, leading to incorrect results. It is crucial to ensure that the tensors' channel orders are aligned to prevent such errors.\n\nIn the Transformer layer, there is a residual connection wherein the layer input is added to the output of the out projection layer, and the result is added to the output of the final linear layer. If the out projection layer's output is reordered, the channels would not align with those of the input. The same applies to the output of the final linear layer. Therefore, we do not reorder the output of the out projection layer and the final linear layer to maintain channel consistency with the original input. To maintain channel consistency with the original input, we don't reorder the output of the out projection layer and the final linear layer.\n\nLastly, we note that the channel dimensions of query activation X Q and key activation X K are multiplied and summed together in the QK-MatMul of self-attention: X Q (X K ) T . To avoid misalignment, X Q and X K should share the same reorder index. We collect the maximum and minimum values of each channel in Q and K and combine them into a quaternion point (X Q max,i , X Q min,i , X K max,i , X K min,i ) for K-means clustering. The clustering result is then employed for reordering in both X Q and X K . The reordering for each activation are demonstrated in Figure 3.\n\n\nExperiments\n\n\nSettings\n\nWe will evaluate our proposed reorder-based post-training quantization (RPTQ) on OPT models [40]. As our work focus on processing the problem in quantizing activations, we use GPTQ [15] to quantize the weights in LLMs 3 . We apply static quantization to all the weights and input activations. For each cluster of activation, we use the Min-Max method to set the parameters of asymmetric quantization (scaling factor s and zero point z). The quantization of the output activation of layer norm and softmax is performed at 8-bit precision. We calculate the R2 and R3 indices for each head in self-attention. We use 256 samples randomly selected from WikiText2 [23], Pen Treebank [22], and C4 [28] for calibration dataset. We will report on perplexity, performance on zero-shot tasks, and memory consumption. Additionally, we conducted an ablation study on the number of clusters to explore the impact on the quantization performance. The experiments are conducted on a server equipped with 8 Nvidia A6000 GPUs.\n\n\nResults on LLM\n\nWe conducted an experiment to evaluate OPT across various model scales. Specifically, we evaluated OPT's performance under three distinct bit-width configurations: W4A16, W4A8, and W4A4. Here, W4A4 refers to weight quantization with 4 bits and activation quantization with 4 bits. Additionally, we developed a new quantization scheme, W4A4KV, W4A3KV, and W3A3KV, focusing solely on quantizing the key cache and value cache, which are the major memory consumers when using large sequence length or batch size.\n\nThe same as GPTQ [15], we evaluate the perplexity and the prediction accuracy on various zero shot tasks. The results are presented in Table 1 and Table 2, respectively. From the table, we can make the following observations: In general, the performance of the models tends to decrease as the bit-width for activation quantization decreases. For instance, by quantizing OPT-175b, W4A8 achieves a perplexity loss of less than 0.5 and W4A4 achieves a perplexity loss of less than 3. For the key cache and value cache quantization schemes (W4A4KV, W4A3KV, and W3A3KV), it is noticeable that their performance are better. In most cases, the performance of the quantized models is close to the FP16 baseline. For instance, by quantizing OPT-175b, W4A4KV achieves a perplexity loss of less than 0.5 and W3A3KV achieves a perplexity loss of less than 1.5. This suggests that focusing on quantizing key and value caches can be beneficial to maintain the performance while reducing memory consumption.\n\nOther methods, such as SmoothQuant [34] and PEG [3], encounters difficulties when attempting to push quantization to 4 bits. See Appendix A.2 for detail. The ignorance of range difference prevent them from successfully quantizing activations of LLMs at low bit-widths. By carefully considering the range distribution in activation values, our method achieves a significant breakthrough in quantizing LLMs with 3-bit activation quantization.\n\n\nMemory Consumption\n\nThe huge memory consumption is a major challenge in the deployment of LLMs. Limited memory capacity can result in significant performance bottlenecks [30]. There are three sources contributing to the memory usage of LLMs: Firstly, the weights in LLMs should be saved in memory, which can be significantly reduced through weight quantization. Secondly, memory is required for temporary activations generated during network execution. As these temporary activations can be released after usage and the memory usage of attention matrices can be greatly reduced through operation fusion [10], their memory footprint is minimal. Lastly, caching of key and value activations is necessary for predicting subsequent words. It is noteworthy that the key and value caches consume a majority of the memory when batch size and sequence length are high. See Appendix A.6 for details.   Table 3 presents the memory usage under various settings, where we observe that lower-bit activations can substantially reduce the memory usage, particularly when the batch size and sequence length are high. For instance, we observe that W4A8 can reduce about 63% memory and W4A4 can reduce about 75% memory. Therefore, adopting activation quantization can greatly reduce the memory pressure in long-text tasks or large-batch scenarios. Quantizing solely the key and value cache also considerably diminishes memory consumption. We observe that W4A4KV can reduce about 73% memory and W3A3KV can reduce about 80% memory.\n\n\nAblation Study\n\nIn this study, we perform an ablation analysis to investigate the impact of varying the number of clusters on model performance. Figure 4 presents the results of adjusting the number of clusters for each reorder (R1 to R5) while keeping other reorders fixed. As the number of clusters increases, the perplexity generally decreases. The fluctuations observed in R2 are an intriguing problem. We have found that increasing the size of the calibration dataset is helpful in alleviating this issue. See Appendix A.5 for details.. We suspect that this could be due to the limited amount of calibration data, which may not accurately capture the data distribution of certain samples. We have also noticed that larger networks are more sensitive to the number of clusters. For instance, if the number of clusters is set to 1, larger networks may crash, while smaller networks may have a lower quantization error. Because RPTQ reorder each self-attention heads separately, the overhead associated with reordering for each head in R2 and R3 is substantial when the number of self-attention heads is high. In our experiments, we utilize 32 clusters for R1, R4, and R5, and 4 clusters for R2 and R3.\n\n\nConclusion\n\nIn this paper, we identify that the main challenge in quantizing large-scale language models (LLMs) stems from the differences value ranges across channels, rather than just the issue of outliers. We have proposed a novel reorder-based quantization approach, RPTQ, that involves rearranging the channels in the activations and quantizing them in clusters. By quantizing the weights and activations, we have significantly reduced the memory usage. Our experiments demonstrate that our proposed approach successfully addresses the issue of numerical differences among activation channels and achieves a significant breakthrough by quantizing LLM to 3 bit activation for the first time.  \n\n\nA Appendix\n\n\nA.1 Distribution of different channels\n\nWe analyzed the distribution of different channels in the OPT-30b and OPT-66b by plotting the (maximum value, minimum value) points of each channel in the activation, demonstrated in Figure 5 and Figure 6 Our analysis revealed that there were significant differences in the data range across different layers and even within different channels in the same layer. This finding motivated us to develop a clustering approach to reduce the impact of these differences during quantization. By clustering channels with similar magnitude ranges together, we can reduce the quantization error and improve the efficiency of quantized LLMs.\n\n\nA.2 Comparing with Other Methods\n\nAs depicted in Table 4, we compared RPTQ with SmoothQuant [34]. SmoothQuant tackles the quantization difficulty by introducing a mathematically equivalent per-channel scaling transformation that smooths the magnitude across channels, making the activations more amenable to quantization. It cannot addressing the high difference of value ranges across channels. Additionally, this approach may also lead to issues with weight quantization. We observed that SmoothQuant exhibits significant performance degradation on larger networks. This can be attributed to the larger range differences among different channels in large networks compared to smaller networks. As depicted in Table 5, we compared RPTQ with PEG [3]. Due to the original paper was only tested on small models, we applied its method to the OPT model and used the same group settings for PEG as in RPTQ. It was observed that PEG also incurs significant quantization loss on large models.\n\n\nA.3 Combine RPTQ with GPTQ\n\nGenerative Pre-trained Transformer Quantization (GPTQ) is a post-training quantization method for Generative Pre-trained Transformers (GPTs) that focuses on quantizing the weights in the learner layers of transformers. The primary goal of GPTQ is to minimize the error introduced by quantizing weights. This is achieved through the optimization target:\nargmin\u0174 ||XW \u2212 X\u0174 || 2 2 ,(4)\nwhere W is the weight of a linear layer, and X is the input of the layer collected using a calibration dataset. By solving this layer-wise quantization problem with a Hessian-based approximation, GPTQ can find the quantized weight that minimizes the quantization error.\n\nTo achieve different quantization for each input cluster of weight, we apply GPTQ on each input cluster of weights in the layer to minimize the quantization error. To combine Random Perturbation Training Quantization (RPTQ) with GPTQ, we first reorder the weights in the linear layers before applying GPTQ.\n\n\nA.4 Computation under cluster-based quantization\n\nAfter quantizing the network with RPTQ, each activation X is adjusted by channel to becomeX, which is divided into g clusters. Each activation clusterX i has different asymmetric quantization parameters (scale and zero point). We denote the scale and zero point for the i-th cluster as s X i and z X i , respectively. The activations in the i-th cluster are quantized with the quantization parameters:\nX q,i = Q k (X i , s X i , z X i ),(5)\nwhere Q k represents the quantization function that quantizes the input using the k-bit quantization parameters. The weights W in the next layer are also adjusted based on the input activation's reorder index to becomeW . It is also divided into g clusters, where each cluster has different asymmetric quantization parameters. For one output channel, each weight clusterW i is also quantized with different asymmetric quantization parameters (scale and zero point), which are denoted as s W i and z W i , respectively. The weights in the i-th cluster are quantized with the quantization parameters:\nW q,i = Q k (W i , s W i , z W i ).(6)\nThis process ensures that both the activations and weights are quantized with different quantization parameters for each cluster.\n\nWe describe two methods for computing the output tensor Y using the quantized activations and weights. The first method dequantizes the weight and activation values back to floating-point numbers, while the second method performs computation directly in the integer domain. The first method involves dequantizing the quantized activations and weights back to floating-point values as follows:\nXi = s X i (Xq, i \u2212 z X i ),\u0174 i = s W i (W q, i \u2212 z W i ).(7)\nThe dequantized values are then concatenated to form the full activationsX and the full weights\u0174 . Matrix multiplication is performed using the dequantized values:\nY =X\u0174 .(8)\nThe second method computes the output tensor Y directly in the integer domain. The quantized activations and weights in each cluster are multiplied using matrix and vector multiplication:\nY q,i =Xq, iW q, i \u2212 z X iW q, i \u2212 z W iX q, i + z X i z W i .(9)\nThe results are then dequantized, and the dequantized results are summed up:\nY = g i=1 (s X i s W i Y q,i ).(10)\nThe second method is computationally efficient, but it requires hardware support for integer arithmetic. For instance, the 3-bit value is not supported in most GPUs. In such cases, these values are cast to higher hardware-supported datatypes such as 4-bit or 8-bit on GPUs.\n\nA.5 More Results for Ablation Study Figure 7 displays the outcomes associated with 128/256 calibration samples. The observed fluctuations in PT and C4 within R2 and R3 present an intriguing issue. We hypothesize that these fluctuations may be attributable to the limited quantity of calibration data, which might not accurately represent the data distribution for specific samples. However, using a larger calibration dataset during the calibration phase would demand more memory and computational resources, which is why we have not yet conducted this experiment in this paper. In the future, with additional resources, we intend to conduct more extensive analyses using larger networks and calibration datasets to gain a deeper understanding of this matter.\n\n\nA.6 Memory Consumption of Different Parts\n\nWe reported the memory consumption of different parts in LLMs, as shown in the Table 6.    \n\nFigure 1 :\n1Demonstration of the distribution of different channels in OPT decoder layers. Each point is (maximum value, minimum value) of a channel in the activation.\n\nFigure 2 :\n2Demonstration of different methods to address the problem in quantizing activations.\n\nFigure 3 :\n3An overview of the inference process for a quantized transformer layer with reordered weight and activation. The reordering indexes are represented by the symbols R1 to R5. Below the figure illustrates how the weights and activations are reordered in a linear layer, where points with darker colors represent larger values.\n\nFigure 4 :\n4The ablation study to evaluate the performance of the clustering method under the W16A4 configuration. We tested different numbers of clusters(1, 2, 4, 8, and 32)  for R1 to R5.\n\nFigure 5 :\n5Demonstration of the distribution of different channels in OPT-30b decoder layers. Each point is (maximum value, minimum value) of a channel in the activation.\n\nFigure 6 :\n6Demonstration of the distribution of different channels in OPT-66b decoder layers. Each point is (maximum value, minimum value) of a channel in the activation.\n\nFigure 7 :\n7The ablation study to evaluate the performance of the clustering method under the W16A4 configuration. We tested different numbers of clusters(1, 2, 4, 8, and 32)  for R1 to R5.\n\nTable 1 :\n1Perplexity scores of various models under diverse quantization configurations on three datasets: WikiText2 (WIKI), Pen Treebank (PT), and C4. 19.94 16.92 11.92 14.13 12.61 11.15 13.90 12.04 11.62 14.95 11.96 10.88 14.69 11.36 9.39 13.45 11.27 W3A3KV 18.45 21.33 18.26 12.42 14.48 13.13 11.47 14.08 12.41 11.76 14.98 12.22 11.47 15.03 11.75 10.03 13.82 11.30Model \nOPT-1.3b \nOPT-6.7b \nOPT-13b \nOPT-30b \nOPT-66b \nOPT-175b \n\nTask \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \n\nFP16 \n14.63 16.96 14.72 10.86 13.09 11.74 10.13 12.34 11.20 9.56 11.84 10.69 9.34 11.36 10.28 8.34 12.01 10.13 \nW4A16 \n14.78 17.21 14.92 11.18 13.62 12.07 10.29 12.45 11.27 9.55 11.91 10.74 9.30 11.42 10.31 8.37 12.31 10.26 \nW4A8 \n15.39 17.79 15.48 11.21 13.74 12.11 10.90 13.40 11.62 10.22 12.41 11.01 9.46 11.73 10.57 8.43 12.24 10.49 \nW4A4 \n16.88 19.23 16.55 12.00 15.17 12.85 12.74 15.76 14.71 11.15 14.11 13.48 12.23 18.87 15.93 10.60 15.59 12.28 \nW4A4KV 15.26 17.65 15.37 11.26 13.44 12.03 10.59 12.80 11.54 9.99 12.18 11.01 9.75 11.64 10.61 8.40 12.38 10.54 \nW4A3KV 17.22 \n\nTable 2 :\n2Accuracy of OPT models under diverse quantization configurations on different zero-shot tasks: LAMBADA(OpenAI), PIQA, ARC(Easy), ARC(Challenge), OpenBookQA, BoolQ.Task \nLAMBADA(OpenAI) [26] \nPIQA [31] \n\nModel \n1.3b \n6.7b \n13b \n30b \n66b \n1.3b \n6.7b \n13b \n30b \n66b \n\nFP16 \n57.98% 61.84% 68.60% 71.41% 67.14% 72.47% 74.53% 76.87% 78.01% 78.12% \nW4A16 \n57.46% 60.78% 68.50% 71.37% 67.06% 71.59% 74.80% 76.93% 78.29% 78.18% \nW4A8 \n52.39% 67.35% 62.44% 64.99% 67.02% 69.69% 75.89% 75.46% 76.93% 77.52% \nW4A4 \n49.34% 64.93% 60.23% 63.92% 68.50% 68.66% 75.40% 73.55% 76.16% 77.14% \nW4A4KV 52.90% 67.39% 62.77% 64.89% 69.99% 69.26% 76.00% 74.42% 76.65% 76.98% \nW4A3KV 47.02% 64.97% 61.05% 59.20% 66.23% 68.22% 75.73% 73.23% 67.46% 74.21% \nW3A3KV 42.84% 64.11% 60.02% 58.33% 65.28% 68.22% 74.64% 74.10% 67.51% 75.13% \n\nTask \nARC(Easy) [7] \nARC(Challenge) [7] \n\nModel \n1.3b \n6.7b \n13b \n30b \n66b \n1.3b \n6.7b \n13b \n30b \n66b \n\nFP16 \n51.05% 58.03% 61.91% 65.31% 64.68% 29.69% 33.61% 35.66% 38.05% 38.99% \nW4A16 \n51.17% 57.02% 61.82% 65.10% 64.89% 30.03% 32.59% 35.49% 37.96% 38.99% \nW4A8 \n48.35% 60.18% 60.94% 63.46% 64.60% 26.36% 34.04% 35.58% 37.45% 38.82% \nW4A4 \n47.55% 56.90% 58.41% 62.12% 63.76% 25.85% 34.30% 33.95% 36.17% 37.20% \nW4A4KV 47.76% 57.74% 58.54% 63.59% 63.67% 27.64% 33.95% 34.21% 37.37% 37.71% \nW4A3KV 46.29% 56.69% 56.10% 48.44% 59.00% 26.02% 33.95% 33.95% 30.71% 36.77% \nW3A3KV 44.02% 55.59% 53.74% 50.42% 57.65% 26.53% 32.16% 32.50% 30.71% 34.98% \n\nTask \nOpenBookQA [24] \nBoolQ [6] \n\nModel \n1.3b \n6.7b \n13b \n30b \n66b \n1.3b \n6.7b \n13b \n30b \n66b \n\nFP16 \n33.00% 38.00% 39.00% 40.20% 41.60% 57.73% 67.03% 65.90% 70.45% 70.85% \nW4A16 \n31.80% 37.40% 39.20% 40.60% 42.00% 58.99% 59.72% 66.66% 70.70% 70.55% \nW4A8 \n32.40% 38.00% 38.60% 39.40% 41.80% 46.88% 65.93% 66.57% 70.64% 71.07% \nW4A4 \n32.60% 38.40% 38.00% 38.60% 42.00% 41.37% 65.44% 58.47% 67.70% 70.24% \nW4A4KV 32.60% 38.40% 38.00% 39.80% 41.60% 43.33% 62.11% 62.47% 68.22% 70.79% \nW4A3KV 32.80% 36.80% 37.00% 34.00% 39.40% 42.84% 61.31% 57.76% 61.74% 67.06% \nW3A3KV 28.40% 35.20% 37.20% 32.40% 38.60% 46.23% 60.79% 65.07% 63.08% 67.49% \n\n\n\nTable 3 :\n3Memory consumption (GB) of LLMs on different batch sizes and sequence lengths.Batch Size \n1 \n8 \n64 \n\nSequence Length 2048 4096 8192 2048 4096 8192 2048 \n4096 \n8192 \n\nOPT-30b \n\nW16A16 \n59.4 \n62.3 \n68.1 \n79.7 102.9 149.3 242.0 427.5 \n798.6 \nW4A16 \n17.0 \n19.9 \n25.7 \n37.3 \n60.5 106.9 199.6 385.2 \n756.2 \nW4A8 \n15.6 \n17.1 \n20.1 \n26.0 \n38.0 \n61.8 109.5 204.9 \n395.7 \nW4A4 \n14.9 \n15.7 \n17.3 \n20.4 \n26.7 \n39.3 \n64.5 \n114.8 \n215.4 \nW4A4KV \n15.0 \n15.9 \n17.7 \n21.2 \n28.3 \n42.6 \n71.0 \n127.9 \n241.7 \nW4A3KV \n14.8 \n15.6 \n17.0 \n19.9 \n25.7 \n37.2 \n60.3 \n106.5 \n198.8 \nW3A3KV \n11.3 \n12.0 \n13.5 \n16.4 \n22.1 \n33.7 \n56.8 \n102.9 \n195.3 \n\nOPT-66b \n\nW16A16 \n128.1 133.0 142.7 162.1 200.9 278.5 433.8 744.3 1365.3 \nW4A16 \n35.7 \n40.5 \n50.2 \n69.6 108.4 186.1 341.3 651.9 1272.9 \nW4A8 \n33.3 \n35.8 \n40.7 \n50.6 \n70.5 110.1 189.5 348.1 \n665.4 \nW4A4 \n32.1 \n33.4 \n36.0 \n41.2 \n51.5 \n72.2 113.5 196.2 \n361.6 \nW4A4KV \n32.2 \n33.7 \n36.5 \n42.2 \n53.6 \n76.4 122.0 213.1 \n395.4 \nW4A3KV \n32.0 \n33.1 \n35.4 \n39.9 \n49.0 \n67.2 103.7 176.5 \n322.3 \nW3A3KV \n24.3 \n25.4 \n27.7 \n32.2 \n41.3 \n59.5 \n96.0 \n168.8 \n314.6 \n\nOPT-175b \n\nW16A16 \n335.4 344.9 363.8 401.7 477.5 629.0 932.0 1538.0 2750.1 \nW4A16 \n91.0 100.4 119.4 157.2 233.0 384.5 687.5 1293.5 2505.6 \nW4A8 \n86.3 \n91.1 100.7 119.9 158.4 235.3 389.0 696.5 1311.6 \nW4A4 \n84.0 \n86.4 \n91.4 101.3 121.1 160.6 239.8 398.0 \n714.6 \nW4A4KV \n84.1 \n86.8 \n92.1 102.7 123.9 166.3 251.0 420.5 \n759.6 \nW4A3KV \n83.6 \n85.7 \n89.8 \n98.1 114.8 148.1 214.6 347.8 \n614.1 \nW3A3KV \n63.2 \n65.3 \n69.4 \n77.8 \n94.4 127.7 194.3 327.4 \n593.7 \n\n12 4 8 \n16 \n32 \n\n12 \n\n14 \n\n16 \n\n18 \n\n20 \n\nperplexity \n\nR1 \n\n12 4 8 \n16 \n32 \n\n12 \n\n14 \n\n16 \n\n18 \n\n20 \n\nperplexity \n\nR2 \n\n12 4 8 \n16 \n32 \n\n12 \n\n14 \n\n16 \n\n18 \n\nperplexity \n\nR3 \n\n12 4 8 \n16 \n32 \n\n15 \n\n20 \n\n25 \n\n30 \n\nperplexity \n\nR4 \n\n12 4 8 \n16 \n32 \n\n12 \n\n14 \n\n16 \n\n18 \n\n20 \n\nperplexity \n\nR5 \n\nopt-1.3b WIKI \nopt-1.3b PT \nopt-1.3b C4 \nopt-6.7b WIKI \nopt-6.7b PT \nopt-6.7b C4 \n\nnumber of clusters \n\n\n\nTable 4 :\n4Comparing RPTQ with SmoothQuant on perplexity scores of various models under diverse \nquantization configurations on three datasets: WikiText2 (WIKI), Pen Treebank (PT), and C4. \n\nModel \nOPT-1.3b \nOPT-6.7b \nOPT-13b \nOPT-30b \n\nTask \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \n\nW4A8 \nSmoothQuant 16.89 19.35 16.26 11.62 14.04 12.47 12.55 \n14.73 \n12.2 \n9.96 \n12.12 \n11.01 \nRPTQ \n15.39 17.79 15.48 11.21 13.74 12.11 10.90 \n13.40 \n11.62 \n10.22 \n12.41 \n11.01 \n\nW4A4 \nSmoothQuant 68.25 68.44 53.77 51.19 67.14 73.44 235.38 285.85 187.96 18435.66 70175.91 11297.63 \nRPTQ \n16.88 19.23 16.55 12.00 15.17 12.85 12.74 \n15.76 \n14.71 \n11.15 \n14.11 \n13.48 \n\nW4A4KV \nSmoothQuant 19.45 22394 18.46 12.68 16.13 13.72 11.62 \n16.44 \n12.21 \n11.61 \n15.43 \n11.74 \nRPTQ \n15.26 17.65 15.37 11.26 13.44 12.03 10.59 \n12.80 \n11.54 \n9.99 \n12.18 \n11.01 \n\n\n\nTable 5 :\n5Comparing RPTQ with PEG on perplexity scores of various models under W4A4 on three \ndatasets: WikiText2 (WIKI), Pen Treebank (PT), and C4. \n\nModel \nOPT-1.3b \nOPT-6.7b \nOPT-13b \nOPT-30b \nOPT-66b \n\nTask \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nWIKI \nPT \nC4 \nPEG \n19.18 22.44 18.57 12.30 15.04 13.61 14.28 17.78 19.92 17.87 34.94 45.43 15.57 23.59 20.36 \nRPTQ 16.88 19.23 16.55 12.00 15.17 12.85 12.74 15.76 14.71 11.15 14.11 13.48 12.23 18.87 15.93 \n\n\n\nTable 6 :\n6The memory proportion of different parts in LLMs. 46% 55.59% 12.95% 10.29% 72.76% 16.94%Batch Size \n1 \n64 \n\nSequence Length \n2048 \n8192 \n2048 \n8192 \n\nModel \nPrecision \nWeight \nK/V \nDynamic Weight \nK/V \nDynamic Weight \nK/V \nDynamic Weight \nK/V \nDynamic \nFP16 \n95.12% 4.42% \n0.46% \n82.97% 15.42% \n1.61% \n23.34% 69.42% \n7.24% \n7.07% 84.15% \n8.77% \nW4A16 \n82.97% 15.42% \n1.61% \n54.92% 40.83% \n4.26% \n7.07% 84.15% \n8.77% \n1.87% 88.87% \n9.26% \nW4A8 \n90.45% 8.41% \n1.14% \n70.32% 26.14% \n3.54% \n12.90% 76.70% 10.40% \n3.57% 84.92% 11.51% \nW4A4 \n94.73% 4.40% \n0.87% \n81.79% 15.20% \n3.01% \n21.91% 65.17% 12.92% \n6.56% 77.98% 15.46% \nW4A4KV 94.08% 4.37% \n1.55% \n79.89% 14.85% \n5.26% \n19.89% 59.14% 20.97% \n5.84% 69.51% 24.64% \nW4A3KV 95.14% 3.32% \n1.54% \n83.04% 11.57% \n5.39% \n23.43% 52.24% 24.33% \n7.10% 63.38% 29.52% \nW3A3KV 93.62% 4.35% \n2.03% \n78.59% 14.61% \n6.80% \n18.66% 55.49% 25.84% \n5.42% 64.53% 30.05% \n\nOPT-66b \n\nFP16 \n96.21% 3.51% \n0.27% \n86.40% 12.62% \n0.99% \n28.42% 66.39% \n5.19% \n9.03% 84.38% \n6.60% \nW4A16 \n86.40% 12.62% \n0.99% \n61.36% 35.84% \n2.80% \n9.03% 84.38% \n6.60% \n2.42% 90.50% \n7.08% \nW4A8 \n92.56% 6.76% \n0.69% \n75.66% 22.10% \n2.25% \n16.27% 76.01% \n7.73% \n4.63% 86.57% \n8.80% \nW4A4 \n95.98% 3.50% \n0.52% \n85.64% 12.51% \n1.86% \n27.15% 63.42% \n9.43% \n8.52% 79.64% 11.84% \nW4A4KV 95.58% 3.49% \n0.93% \n84.40% 12.32% \n3.28% \n25.27% 59.04% 15.70% \n7.79% 72.84% 19.37% \nW4A3KV 96.44% 2.64% \n0.92% \n87.13% 9.54% \n3.33% \n29.72% 52.08% 18.19% \n9.56% 67.03% 23.41% \nW3A3KV 95.31% 3.48% \n1.22% \n83.54% 12.20% \n4.26% \n24.08% 56.27% 19.65% \n7.35% 68.67% 23.98% \n\nOPT-175b \n\nFP16 \n97.18% 2.68% \n0.14% \n89.59% 9.89% \n0.52% \n34.98% 61.80% \n3.22% \n11.85% 83.78% \n4.37% \nW4A16 \n89.59% 9.89% \n0.52% \n68.27% 30.16% \n1.57% \n11.85% 83.78% \n4.37% \n3.25% 91.95% \n4.79% \nW4A8 \n94.43% 5.21% \n0.35% \n80.92% 17.87% \n1.21% \n20.95% 74.03% \n5.02% \n6.21% 87.83% \n5.95% \nW4A4 \n97.05% 2.68% \n0.27% \n89.18% 9.85% \n0.98% \n33.99% 60.06% \n5.95% \n11.40% 80.61% \n7.99% \nW4A4KV 96.85% 2.67% \n0.47% \n88.49% 9.77% \n1.73% \n32.46% 57.37% 10.17% 10.73% 75.83% 13.44% \nW4A3KV 97.51% 2.02% \n0.47% \n90.73% 7.52% \n1.75% \n37.97% 50.32% 11.72% 13.27% 70.35% 16.38% \nW3A3KV 96.71% 2.67% \n0.62% \n88.02% 9.72% \n2.26% \n31.\nFor simplicity, we use the term \"channel\" to refer to the dimension of the hidden size. See Appendix A.1 for more results.\nThe GPTQ method is solely designed for weight quantization and cannot be used for activation quantization. For details on how to combine the GPTQ method for weight quantization with the RPTQ method for activation quantization, please refer to the Appendix A.3.\n\nDeepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, 2022 SC22: International Conference for High Performance Computing, Networking, Storage and Analysis (SC). IEEE Computer SocietyReza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In 2022 SC22: International Conference for High Performance Computing, Networking, Storage and Analysis (SC), pages 646-660. IEEE Computer Society, 2022.\n\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, arXiv:2204.06745Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprintSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.\n\nUnderstanding and overcoming the challenges of efficient transformer quantization. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7947-7969, 2021.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, I-Jen Pierce, Vijayalakshmi Chuang, Kailash Srinivasan, Gopalakrishnan, arXiv:1805.06085Pact: Parameterized clipping activation for quantized neural networks. arXiv preprintJungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.\n\nBoolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.10044arXiv preprintChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.05457arXiv preprintPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio, arXiv:1602.02830Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprintMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.\n\nVs-quant: Per-vector scaled quantization for accurate low-precision neural network inference. Steve Dai, Rangha Venkatesan, Mark Ren, Brian Zimmer, William Dally, Brucek Khailany, Proceedings of Machine Learning and Systems. Machine Learning and Systems3Steve Dai, Rangha Venkatesan, Mark Ren, Brian Zimmer, William Dally, and Brucek Khailany. Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference. Proceedings of Machine Learning and Systems, 3:873-884, 2021.\n\nFlashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9, Advances in Neural Information Processing Systems. 35Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nK Steven, Jeffrey L Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, arXiv:1902.08153Learned step size quantization. arXiv preprintSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n\nMassive language models can be accurately pruned in one-shot. Elias Frantar, Dan Alistarh, arXiv:2301.00774arXiv preprintElias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nAnt: Exploiting adaptive numerical data type for low-bit deep neural network quantization. Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, Yuhao Zhu, 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEECong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. Ant: Exploiting adaptive numerical data type for low-bit deep neural network quantization. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 1414-1433. IEEE, 2022.\n\nLearning to quantize deep networks by optimizing quantization intervals with task loss. Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, Changkyu Choi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4350-4359, 2019.\n\nA 17-95.6 tops/w deep learning inference accelerator with per-vector scaled 4-bit quantization for transformers in 5nm. Ben Keller, Rangharajan Venkatesan, Steve Dai, G Stephen, Brian Tell, Zimmer, J William, Thomas Dally, Brucek Gray, Khailany, 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits). IEEEBen Keller, Rangharajan Venkatesan, Steve Dai, Stephen G Tell, Brian Zimmer, William J Dally, C Thomas Gray, and Brucek Khailany. A 17-95.6 tops/w deep learning inference accelerator with per-vector scaled 4-bit quantization for transformers in 5nm. In 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), pages 16-17. IEEE, 2022.\n\nLarge language models with controllable working memory. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, Sanjiv Kumar, arXiv:2211.05110arXiv preprintDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. Large language models with controllable working memory. arXiv preprint arXiv:2211.05110, 2022.\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, arXiv:2102.05426arXiv preprintYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.\n\nClassification and analysis of multivariate observations. J Macqueen, 5th Berkeley Symp. Math. Statist. Probability. University of California Los Angeles LA USAJ MacQueen. Classification and analysis of multivariate observations. In 5th Berkeley Symp. Math. Statist. Probability, pages 281-297. University of California Los Angeles LA USA, 1967.\n\nThe penn treebank: Annotating predicate argument structure. Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, Human Language Technology: Proceedings of a Workshop. Plainsboro, New JerseyMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.02789arXiv preprintTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLRMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fern\u00e1ndez, arXiv:1606.06031The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprintDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nnuqmm: Quantized matmul for efficient inference of large-scale generative language models. Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, arXiv:2206.09557arXiv preprintGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Matthias Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nHigh-throughput generative inference of large language models with a single gpu. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Y Daniel, Zhiqiang Fu, Beidi Xie, Clark Chen, Joseph E Barrett, Gonzalez, arXiv:2303.06865arXiv preprintYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.\n\nPiqa: An algebra for querying protein data sets. Sandeep Tata, M Jignesh, Patel, 15th International Conference on Scientific and Statistical Database Management. IEEESandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein data sets. In 15th International Conference on Scientific and Statistical Database Management, 2003., pages 141-150. IEEE, 2003.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nTowards accurate post-training network quantization via bit-split and stitching. Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng, International Conference on Machine Learning. PMLRPeisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In International Conference on Machine Learning, pages 9847-9856. PMLR, 2020.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.10438arXiv preprintGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.01861arXiv preprintZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nZhewei Yao, Cheng Li, Xiaoxia Wu, arXiv:2303.08302Stephen Youn, and Yuxiong He. A comprehensive study on post-training quantization for large language models. arXiv preprintZhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study on post-training quantization for large language models. arXiv preprint arXiv:2303.08302, 2023.\n\nPtq-sl: Exploring the sub-layerwise post-training quantization. Zhihang Yuan, Yiqi Chen, Chenhao Xue, Chenguang Zhang, Qiankun Wang, Guangyu Sun, arXiv:2110.07809arXiv preprintZhihang Yuan, Yiqi Chen, Chenhao Xue, Chenguang Zhang, Qiankun Wang, and Guangyu Sun. Ptq-sl: Exploring the sub-layerwise post-training quantization. arXiv preprint arXiv:2110.07809, 2021.\n\nPtq4vit: Post-training quantization for vision transformers with twin uniform quantization. Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, Guangyu Sun, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part XIIZhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XII, pages 191-207. Springer, 2022.\n\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. arXiv preprintAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nImproving neural network quantization without retraining using outlier channel splitting. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, Zhiru Zhang, International conference on machine learning. PMLRRitchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural net- work quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543-7552. PMLR, 2019.\n\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou, arXiv:1606.06160Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprintShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\n\n34% 87.69% 10.97% W4A8 87.05% 11.14% 1.81% 62.69% 32.09% 5.22% 9. 550% 77.83% 12.66% 2.56% 83.81% 13.64% W4A4 92.66% 5.93% 1.41% 75.94% 19.44% 4.62% 16.47% 67.47% 1634% 87.69% 10.97% W4A8 87.05% 11.14% 1.81% 62.69% 32.09% 5.22% 9.50% 77.83% 12.66% 2.56% 83.81% 13.64% W4A4 92.66% 5.93% 1.41% 75.94% 19.44% 4.62% 16.47% 67.47% 16.05%\n\nOPT-30b. OPT-30b\n", "annotations": {"author": "[{\"end\":223,\"start\":76},{\"end\":366,\"start\":224},{\"end\":512,\"start\":367},{\"end\":657,\"start\":513},{\"end\":806,\"start\":658},{\"end\":955,\"start\":807},{\"end\":1102,\"start\":956},{\"end\":1246,\"start\":1103},{\"end\":1390,\"start\":1247},{\"end\":1537,\"start\":1391},{\"end\":1683,\"start\":1538}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":84},{\"end\":231,\"start\":228},{\"end\":377,\"start\":374},{\"end\":522,\"start\":519},{\"end\":671,\"start\":667},{\"end\":820,\"start\":815},{\"end\":967,\"start\":964},{\"end\":1111,\"start\":1109},{\"end\":1255,\"start\":1253},{\"end\":1402,\"start\":1400},{\"end\":1548,\"start\":1546}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":227,\"start\":224},{\"end\":373,\"start\":367},{\"end\":518,\"start\":513},{\"end\":666,\"start\":658},{\"end\":814,\"start\":807},{\"end\":963,\"start\":956},{\"end\":1108,\"start\":1103},{\"end\":1252,\"start\":1247},{\"end\":1399,\"start\":1391},{\"end\":1545,\"start\":1538}]", "author_affiliation": "[{\"end\":222,\"start\":90},{\"end\":365,\"start\":233},{\"end\":511,\"start\":379},{\"end\":656,\"start\":524},{\"end\":805,\"start\":673},{\"end\":954,\"start\":822},{\"end\":1101,\"start\":969},{\"end\":1245,\"start\":1113},{\"end\":1389,\"start\":1257},{\"end\":1536,\"start\":1404},{\"end\":1682,\"start\":1550}]", "title": "[{\"end\":73,\"start\":1},{\"end\":1756,\"start\":1684}]", "venue": null, "abstract": "[{\"end\":2827,\"start\":1758}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3045,\"start\":3041},{\"end\":3710,\"start\":3703},{\"end\":5318,\"start\":5310},{\"end\":7757,\"start\":7735},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7923,\"start\":7919},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7982,\"start\":7978},{\"end\":8009,\"start\":8006},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8051,\"start\":8047},{\"end\":8377,\"start\":8357},{\"end\":8430,\"start\":8416},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8838,\"start\":8834},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9474,\"start\":9470},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9705,\"start\":9701},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9863,\"start\":9859},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10038,\"start\":10034},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10289,\"start\":10285},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10544,\"start\":10540},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10724,\"start\":10720},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10993,\"start\":10990},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11585,\"start\":11581},{\"end\":12198,\"start\":12191},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13232,\"start\":13228},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14343,\"start\":14339},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15432,\"start\":15428},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15745,\"start\":15741},{\"end\":16408,\"start\":16397},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17423,\"start\":17419},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23342,\"start\":23338},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23431,\"start\":23427},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23908,\"start\":23904},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23927,\"start\":23923},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23940,\"start\":23936},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24804,\"start\":24800},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25816,\"start\":25812},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25828,\"start\":25825},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26394,\"start\":26390},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26827,\"start\":26823},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30423,\"start\":30419},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31076,\"start\":31073}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35903,\"start\":35735},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36001,\"start\":35904},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36338,\"start\":36002},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36529,\"start\":36339},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36702,\"start\":36530},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36875,\"start\":36703},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37066,\"start\":36876},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38171,\"start\":37067},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40282,\"start\":38172},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42206,\"start\":40283},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43057,\"start\":42207},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43535,\"start\":43058},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45724,\"start\":43536}]", "paragraph": "[{\"end\":3711,\"start\":2843},{\"end\":4904,\"start\":3713},{\"end\":6002,\"start\":4906},{\"end\":6963,\"start\":6004},{\"end\":7577,\"start\":6965},{\"end\":8118,\"start\":7617},{\"end\":9361,\"start\":8135},{\"end\":11304,\"start\":9363},{\"end\":11904,\"start\":11348},{\"end\":12133,\"start\":11975},{\"end\":12607,\"start\":12155},{\"end\":13538,\"start\":12609},{\"end\":14152,\"start\":13540},{\"end\":15287,\"start\":14252},{\"end\":16764,\"start\":15289},{\"end\":16942,\"start\":16835},{\"end\":17173,\"start\":16944},{\"end\":18129,\"start\":17175},{\"end\":18477,\"start\":18131},{\"end\":18764,\"start\":18553},{\"end\":19012,\"start\":18818},{\"end\":19500,\"start\":19014},{\"end\":20048,\"start\":19547},{\"end\":20545,\"start\":20050},{\"end\":21507,\"start\":20547},{\"end\":21990,\"start\":21509},{\"end\":22645,\"start\":21992},{\"end\":23219,\"start\":22647},{\"end\":24254,\"start\":23246},{\"end\":24781,\"start\":24273},{\"end\":25775,\"start\":24783},{\"end\":26217,\"start\":25777},{\"end\":27731,\"start\":26240},{\"end\":28938,\"start\":27750},{\"end\":29638,\"start\":28953},{\"end\":30324,\"start\":29694},{\"end\":31312,\"start\":30361},{\"end\":31695,\"start\":31343},{\"end\":31995,\"start\":31726},{\"end\":32303,\"start\":31997},{\"end\":32757,\"start\":32356},{\"end\":33395,\"start\":32797},{\"end\":33564,\"start\":33435},{\"end\":33958,\"start\":33566},{\"end\":34184,\"start\":34021},{\"end\":34383,\"start\":34196},{\"end\":34526,\"start\":34450},{\"end\":34836,\"start\":34563},{\"end\":35597,\"start\":34838},{\"end\":35734,\"start\":35643}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11974,\"start\":11905},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12154,\"start\":12134},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14211,\"start\":14153},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18552,\"start\":18478},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18817,\"start\":18765},{\"attributes\":{\"id\":\"formula_5\"},\"end\":31725,\"start\":31696},{\"attributes\":{\"id\":\"formula_6\"},\"end\":32796,\"start\":32758},{\"attributes\":{\"id\":\"formula_7\"},\"end\":33434,\"start\":33396},{\"attributes\":{\"id\":\"formula_8\"},\"end\":34020,\"start\":33959},{\"attributes\":{\"id\":\"formula_9\"},\"end\":34195,\"start\":34185},{\"attributes\":{\"id\":\"formula_10\"},\"end\":34449,\"start\":34384},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34562,\"start\":34527}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24937,\"start\":24918},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27120,\"start\":27113},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30383,\"start\":30376},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31045,\"start\":31038},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35729,\"start\":35722}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2841,\"start\":2829},{\"attributes\":{\"n\":\"2\"},\"end\":7592,\"start\":7580},{\"attributes\":{\"n\":\"2.1\"},\"end\":7615,\"start\":7595},{\"attributes\":{\"n\":\"2.2\"},\"end\":8133,\"start\":8121},{\"attributes\":{\"n\":\"3\"},\"end\":11317,\"start\":11307},{\"attributes\":{\"n\":\"3.1\"},\"end\":11346,\"start\":11320},{\"attributes\":{\"n\":\"3.2\"},\"end\":14250,\"start\":14213},{\"attributes\":{\"n\":\"4\"},\"end\":16793,\"start\":16767},{\"attributes\":{\"n\":\"4.1\"},\"end\":16833,\"start\":16796},{\"attributes\":{\"n\":\"4.2\"},\"end\":19545,\"start\":19503},{\"attributes\":{\"n\":\"5\"},\"end\":23233,\"start\":23222},{\"attributes\":{\"n\":\"5.1\"},\"end\":23244,\"start\":23236},{\"attributes\":{\"n\":\"5.2\"},\"end\":24271,\"start\":24257},{\"attributes\":{\"n\":\"5.3\"},\"end\":26238,\"start\":26220},{\"attributes\":{\"n\":\"5.4\"},\"end\":27748,\"start\":27734},{\"attributes\":{\"n\":\"6\"},\"end\":28951,\"start\":28941},{\"end\":29651,\"start\":29641},{\"end\":29692,\"start\":29654},{\"end\":30359,\"start\":30327},{\"end\":31341,\"start\":31315},{\"end\":32354,\"start\":32306},{\"end\":35641,\"start\":35600},{\"end\":35746,\"start\":35736},{\"end\":35915,\"start\":35905},{\"end\":36013,\"start\":36003},{\"end\":36350,\"start\":36340},{\"end\":36541,\"start\":36531},{\"end\":36714,\"start\":36704},{\"end\":36887,\"start\":36877},{\"end\":37077,\"start\":37068},{\"end\":38182,\"start\":38173},{\"end\":40293,\"start\":40284},{\"end\":42217,\"start\":42208},{\"end\":43068,\"start\":43059},{\"end\":43546,\"start\":43537}]", "table": "[{\"end\":38171,\"start\":37436},{\"end\":40282,\"start\":38347},{\"end\":42206,\"start\":40373},{\"end\":43057,\"start\":42219},{\"end\":43535,\"start\":43070},{\"end\":45724,\"start\":43636}]", "figure_caption": "[{\"end\":35903,\"start\":35748},{\"end\":36001,\"start\":35917},{\"end\":36338,\"start\":36015},{\"end\":36529,\"start\":36352},{\"end\":36702,\"start\":36543},{\"end\":36875,\"start\":36716},{\"end\":37066,\"start\":36889},{\"end\":37436,\"start\":37079},{\"end\":38347,\"start\":38184},{\"end\":40373,\"start\":40295},{\"end\":43636,\"start\":43548}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5102,\"start\":5094},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14446,\"start\":14438},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15412,\"start\":15404},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15724,\"start\":15716},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16355,\"start\":16347},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17169,\"start\":17161},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19040,\"start\":19032},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20478,\"start\":20470},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23218,\"start\":23210},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27887,\"start\":27879},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29885,\"start\":29877},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29898,\"start\":29890},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34882,\"start\":34874}]", "bib_author_first_name": "[{\"end\":46210,\"start\":46206},{\"end\":46236,\"start\":46230},{\"end\":46255,\"start\":46250},{\"end\":46261,\"start\":46256},{\"end\":46273,\"start\":46268},{\"end\":46280,\"start\":46278},{\"end\":46290,\"start\":46285},{\"end\":46306,\"start\":46298},{\"end\":46321,\"start\":46315},{\"end\":46335,\"start\":46329},{\"end\":46347,\"start\":46343},{\"end\":46895,\"start\":46892},{\"end\":46909,\"start\":46903},{\"end\":46924,\"start\":46920},{\"end\":46942,\"start\":46935},{\"end\":46955,\"start\":46952},{\"end\":46969,\"start\":46961},{\"end\":46985,\"start\":46979},{\"end\":46996,\"start\":46990},{\"end\":47008,\"start\":47004},{\"end\":47466,\"start\":47459},{\"end\":47485,\"start\":47479},{\"end\":47499,\"start\":47493},{\"end\":47970,\"start\":47967},{\"end\":47986,\"start\":47978},{\"end\":47997,\"start\":47993},{\"end\":48012,\"start\":48005},{\"end\":48027,\"start\":48022},{\"end\":48029,\"start\":48028},{\"end\":48046,\"start\":48038},{\"end\":48063,\"start\":48057},{\"end\":48083,\"start\":48077},{\"end\":48097,\"start\":48091},{\"end\":48112,\"start\":48106},{\"end\":48454,\"start\":48446},{\"end\":48465,\"start\":48461},{\"end\":48479,\"start\":48472},{\"end\":48500,\"start\":48495},{\"end\":48522,\"start\":48509},{\"end\":48538,\"start\":48531},{\"end\":48986,\"start\":48975},{\"end\":49000,\"start\":48994},{\"end\":49014,\"start\":49006},{\"end\":49025,\"start\":49022},{\"end\":49046,\"start\":49039},{\"end\":49064,\"start\":49056},{\"end\":49408,\"start\":49403},{\"end\":49421,\"start\":49416},{\"end\":49434,\"start\":49430},{\"end\":49450,\"start\":49444},{\"end\":49463,\"start\":49457},{\"end\":49482,\"start\":49475},{\"end\":49500,\"start\":49494},{\"end\":49779,\"start\":49771},{\"end\":49797,\"start\":49793},{\"end\":49812,\"start\":49806},{\"end\":49824,\"start\":49821},{\"end\":49841,\"start\":49835},{\"end\":50322,\"start\":50317},{\"end\":50334,\"start\":50328},{\"end\":50351,\"start\":50347},{\"end\":50362,\"start\":50357},{\"end\":50378,\"start\":50371},{\"end\":50392,\"start\":50386},{\"end\":50804,\"start\":50801},{\"end\":50813,\"start\":50810},{\"end\":50825,\"start\":50818},{\"end\":50837,\"start\":50833},{\"end\":50856,\"start\":50845},{\"end\":51132,\"start\":51129},{\"end\":51147,\"start\":51143},{\"end\":51161,\"start\":51155},{\"end\":51175,\"start\":51171},{\"end\":51458,\"start\":51453},{\"end\":51475,\"start\":51467},{\"end\":51489,\"start\":51483},{\"end\":51503,\"start\":51495},{\"end\":51513,\"start\":51504},{\"end\":51815,\"start\":51814},{\"end\":51831,\"start\":51824},{\"end\":51833,\"start\":51832},{\"end\":51848,\"start\":51841},{\"end\":51872,\"start\":51860},{\"end\":51894,\"start\":51882},{\"end\":52218,\"start\":52213},{\"end\":52231,\"start\":52228},{\"end\":52494,\"start\":52489},{\"end\":52511,\"start\":52504},{\"end\":52531,\"start\":52528},{\"end\":52865,\"start\":52861},{\"end\":52875,\"start\":52871},{\"end\":52890,\"start\":52883},{\"end\":52902,\"start\":52897},{\"end\":52911,\"start\":52908},{\"end\":52924,\"start\":52918},{\"end\":52935,\"start\":52930},{\"end\":52946,\"start\":52941},{\"end\":53415,\"start\":53409},{\"end\":53431,\"start\":53422},{\"end\":53445,\"start\":53437},{\"end\":53457,\"start\":53451},{\"end\":53471,\"start\":53463},{\"end\":53485,\"start\":53477},{\"end\":53496,\"start\":53492},{\"end\":53499,\"start\":53497},{\"end\":53515,\"start\":53507},{\"end\":54109,\"start\":54106},{\"end\":54129,\"start\":54118},{\"end\":54147,\"start\":54142},{\"end\":54154,\"start\":54153},{\"end\":54169,\"start\":54164},{\"end\":54185,\"start\":54184},{\"end\":54201,\"start\":54195},{\"end\":54215,\"start\":54209},{\"end\":54746,\"start\":54739},{\"end\":54756,\"start\":54751},{\"end\":54776,\"start\":54770},{\"end\":54788,\"start\":54785},{\"end\":54801,\"start\":54795},{\"end\":54818,\"start\":54811},{\"end\":54830,\"start\":54825},{\"end\":54841,\"start\":54835},{\"end\":55175,\"start\":55169},{\"end\":55186,\"start\":55180},{\"end\":55195,\"start\":55193},{\"end\":55205,\"start\":55201},{\"end\":55216,\"start\":55212},{\"end\":55223,\"start\":55221},{\"end\":55238,\"start\":55231},{\"end\":55246,\"start\":55243},{\"end\":55256,\"start\":55253},{\"end\":55566,\"start\":55565},{\"end\":55919,\"start\":55914},{\"end\":55933,\"start\":55928},{\"end\":55943,\"start\":55939},{\"end\":55947,\"start\":55944},{\"end\":55969,\"start\":55963},{\"end\":55984,\"start\":55981},{\"end\":55995,\"start\":55991},{\"end\":56011,\"start\":56006},{\"end\":56024,\"start\":56018},{\"end\":56455,\"start\":56448},{\"end\":56471,\"start\":56464},{\"end\":56484,\"start\":56479},{\"end\":56502,\"start\":56495},{\"end\":56775,\"start\":56770},{\"end\":56791,\"start\":56786},{\"end\":56805,\"start\":56799},{\"end\":56818,\"start\":56812},{\"end\":57121,\"start\":57115},{\"end\":57133,\"start\":57129},{\"end\":57137,\"start\":57134},{\"end\":57149,\"start\":57145},{\"end\":57170,\"start\":57162},{\"end\":57186,\"start\":57180},{\"end\":57485,\"start\":57480},{\"end\":57501,\"start\":57495},{\"end\":57522,\"start\":57514},{\"end\":57538,\"start\":57534},{\"end\":57554,\"start\":57545},{\"end\":57567,\"start\":57561},{\"end\":57583,\"start\":57578},{\"end\":57599,\"start\":57594},{\"end\":57614,\"start\":57608},{\"end\":58105,\"start\":58100},{\"end\":58120,\"start\":58112},{\"end\":58129,\"start\":58127},{\"end\":58151,\"start\":58141},{\"end\":58165,\"start\":58157},{\"end\":58178,\"start\":58171},{\"end\":58521,\"start\":58516},{\"end\":58534,\"start\":58530},{\"end\":58548,\"start\":58544},{\"end\":58567,\"start\":58558},{\"end\":58579,\"start\":58573},{\"end\":58595,\"start\":58588},{\"end\":58609,\"start\":58604},{\"end\":58619,\"start\":58616},{\"end\":58631,\"start\":58624},{\"end\":58962,\"start\":58956},{\"end\":58989,\"start\":58978},{\"end\":59000,\"start\":58995},{\"end\":59014,\"start\":59008},{\"end\":59030,\"start\":59024},{\"end\":59042,\"start\":59037},{\"end\":59061,\"start\":59052},{\"end\":59067,\"start\":59062},{\"end\":59086,\"start\":59078},{\"end\":59105,\"start\":59097},{\"end\":59567,\"start\":59563},{\"end\":59582,\"start\":59575},{\"end\":59597,\"start\":59590},{\"end\":59611,\"start\":59604},{\"end\":59619,\"start\":59616},{\"end\":59631,\"start\":59630},{\"end\":59648,\"start\":59640},{\"end\":59658,\"start\":59653},{\"end\":59669,\"start\":59664},{\"end\":59682,\"start\":59676},{\"end\":59684,\"start\":59683},{\"end\":60058,\"start\":60051},{\"end\":60066,\"start\":60065},{\"end\":60374,\"start\":60370},{\"end\":60391,\"start\":60384},{\"end\":60407,\"start\":60400},{\"end\":60423,\"start\":60417},{\"end\":60444,\"start\":60434},{\"end\":60462,\"start\":60454},{\"end\":60477,\"start\":60472},{\"end\":60500,\"start\":60496},{\"end\":60514,\"start\":60508},{\"end\":60958,\"start\":60951},{\"end\":60970,\"start\":60965},{\"end\":60984,\"start\":60977},{\"end\":60993,\"start\":60989},{\"end\":61364,\"start\":61355},{\"end\":61373,\"start\":61371},{\"end\":61386,\"start\":61379},{\"end\":61401,\"start\":61395},{\"end\":61415,\"start\":61411},{\"end\":61750,\"start\":61744},{\"end\":61760,\"start\":61756},{\"end\":61768,\"start\":61761},{\"end\":61786,\"start\":61780},{\"end\":61801,\"start\":61794},{\"end\":61814,\"start\":61806},{\"end\":61826,\"start\":61819},{\"end\":62091,\"start\":62085},{\"end\":62102,\"start\":62097},{\"end\":62114,\"start\":62107},{\"end\":62512,\"start\":62505},{\"end\":62523,\"start\":62519},{\"end\":62537,\"start\":62530},{\"end\":62552,\"start\":62543},{\"end\":62567,\"start\":62560},{\"end\":62581,\"start\":62574},{\"end\":62906,\"start\":62899},{\"end\":62920,\"start\":62913},{\"end\":62930,\"start\":62926},{\"end\":62942,\"start\":62937},{\"end\":62954,\"start\":62947},{\"end\":63370,\"start\":63365},{\"end\":63381,\"start\":63377},{\"end\":63396,\"start\":63387},{\"end\":63406,\"start\":63401},{\"end\":63418,\"start\":63413},{\"end\":63428,\"start\":63424},{\"end\":63441,\"start\":63435},{\"end\":63453,\"start\":63448},{\"end\":63463,\"start\":63458},{\"end\":63475,\"start\":63471},{\"end\":63773,\"start\":63768},{\"end\":63788,\"start\":63781},{\"end\":63802,\"start\":63797},{\"end\":63815,\"start\":63810},{\"end\":63829,\"start\":63825},{\"end\":63843,\"start\":63836},{\"end\":63861,\"start\":63850},{\"end\":63873,\"start\":63869},{\"end\":63884,\"start\":63880},{\"end\":63891,\"start\":63889},{\"end\":64315,\"start\":64308},{\"end\":64327,\"start\":64322},{\"end\":64338,\"start\":64332},{\"end\":64352,\"start\":64347},{\"end\":64355,\"start\":64353},{\"end\":64365,\"start\":64360},{\"end\":64671,\"start\":64663},{\"end\":64683,\"start\":64678},{\"end\":64693,\"start\":64688},{\"end\":64703,\"start\":64698},{\"end\":64712,\"start\":64710},{\"end\":64724,\"start\":64718}]", "bib_author_last_name": "[{\"end\":46228,\"start\":46211},{\"end\":46248,\"start\":46237},{\"end\":46266,\"start\":46262},{\"end\":46276,\"start\":46274},{\"end\":46283,\"start\":46281},{\"end\":46296,\"start\":46291},{\"end\":46313,\"start\":46307},{\"end\":46327,\"start\":46322},{\"end\":46341,\"start\":46336},{\"end\":46354,\"start\":46348},{\"end\":46901,\"start\":46896},{\"end\":46918,\"start\":46910},{\"end\":46933,\"start\":46925},{\"end\":46950,\"start\":46943},{\"end\":46959,\"start\":46956},{\"end\":46977,\"start\":46970},{\"end\":46988,\"start\":46986},{\"end\":47002,\"start\":46997},{\"end\":47017,\"start\":47009},{\"end\":47477,\"start\":47467},{\"end\":47491,\"start\":47486},{\"end\":47511,\"start\":47500},{\"end\":47976,\"start\":47971},{\"end\":47991,\"start\":47987},{\"end\":48003,\"start\":47998},{\"end\":48020,\"start\":48013},{\"end\":48036,\"start\":48030},{\"end\":48055,\"start\":48047},{\"end\":48075,\"start\":48064},{\"end\":48089,\"start\":48084},{\"end\":48104,\"start\":48098},{\"end\":48119,\"start\":48113},{\"end\":48459,\"start\":48455},{\"end\":48470,\"start\":48466},{\"end\":48493,\"start\":48480},{\"end\":48507,\"start\":48501},{\"end\":48529,\"start\":48523},{\"end\":48549,\"start\":48539},{\"end\":48565,\"start\":48551},{\"end\":48992,\"start\":48987},{\"end\":49004,\"start\":49001},{\"end\":49020,\"start\":49015},{\"end\":49037,\"start\":49026},{\"end\":49054,\"start\":49047},{\"end\":49074,\"start\":49065},{\"end\":49414,\"start\":49409},{\"end\":49428,\"start\":49422},{\"end\":49442,\"start\":49435},{\"end\":49455,\"start\":49451},{\"end\":49473,\"start\":49464},{\"end\":49492,\"start\":49483},{\"end\":49508,\"start\":49501},{\"end\":49791,\"start\":49780},{\"end\":49804,\"start\":49798},{\"end\":49819,\"start\":49813},{\"end\":49833,\"start\":49825},{\"end\":49848,\"start\":49842},{\"end\":50326,\"start\":50323},{\"end\":50345,\"start\":50335},{\"end\":50355,\"start\":50352},{\"end\":50369,\"start\":50363},{\"end\":50384,\"start\":50379},{\"end\":50401,\"start\":50393},{\"end\":50808,\"start\":50805},{\"end\":50816,\"start\":50814},{\"end\":50831,\"start\":50826},{\"end\":50843,\"start\":50838},{\"end\":50859,\"start\":50857},{\"end\":51141,\"start\":51133},{\"end\":51153,\"start\":51148},{\"end\":51169,\"start\":51162},{\"end\":51187,\"start\":51176},{\"end\":51465,\"start\":51459},{\"end\":51481,\"start\":51476},{\"end\":51493,\"start\":51490},{\"end\":51518,\"start\":51514},{\"end\":51822,\"start\":51816},{\"end\":51839,\"start\":51834},{\"end\":51858,\"start\":51849},{\"end\":51880,\"start\":51873},{\"end\":51904,\"start\":51895},{\"end\":51911,\"start\":51906},{\"end\":52226,\"start\":52219},{\"end\":52240,\"start\":52232},{\"end\":52502,\"start\":52495},{\"end\":52526,\"start\":52512},{\"end\":52539,\"start\":52532},{\"end\":52549,\"start\":52541},{\"end\":52869,\"start\":52866},{\"end\":52881,\"start\":52876},{\"end\":52895,\"start\":52891},{\"end\":52906,\"start\":52903},{\"end\":52916,\"start\":52912},{\"end\":52928,\"start\":52925},{\"end\":52939,\"start\":52936},{\"end\":52950,\"start\":52947},{\"end\":53420,\"start\":53416},{\"end\":53435,\"start\":53432},{\"end\":53449,\"start\":53446},{\"end\":53461,\"start\":53458},{\"end\":53475,\"start\":53472},{\"end\":53490,\"start\":53486},{\"end\":53505,\"start\":53500},{\"end\":53520,\"start\":53516},{\"end\":54116,\"start\":54110},{\"end\":54140,\"start\":54130},{\"end\":54151,\"start\":54148},{\"end\":54162,\"start\":54155},{\"end\":54174,\"start\":54170},{\"end\":54182,\"start\":54176},{\"end\":54193,\"start\":54186},{\"end\":54207,\"start\":54202},{\"end\":54220,\"start\":54216},{\"end\":54230,\"start\":54222},{\"end\":54749,\"start\":54747},{\"end\":54768,\"start\":54757},{\"end\":54783,\"start\":54777},{\"end\":54793,\"start\":54789},{\"end\":54809,\"start\":54802},{\"end\":54823,\"start\":54819},{\"end\":54833,\"start\":54831},{\"end\":54847,\"start\":54842},{\"end\":55178,\"start\":55176},{\"end\":55191,\"start\":55187},{\"end\":55199,\"start\":55196},{\"end\":55210,\"start\":55206},{\"end\":55219,\"start\":55217},{\"end\":55229,\"start\":55224},{\"end\":55241,\"start\":55239},{\"end\":55251,\"start\":55247},{\"end\":55259,\"start\":55257},{\"end\":55575,\"start\":55567},{\"end\":55926,\"start\":55920},{\"end\":55937,\"start\":55934},{\"end\":55961,\"start\":55948},{\"end\":55979,\"start\":55970},{\"end\":55989,\"start\":55985},{\"end\":56004,\"start\":55996},{\"end\":56016,\"start\":56012},{\"end\":56036,\"start\":56025},{\"end\":56462,\"start\":56456},{\"end\":56477,\"start\":56472},{\"end\":56493,\"start\":56485},{\"end\":56509,\"start\":56503},{\"end\":56784,\"start\":56776},{\"end\":56797,\"start\":56792},{\"end\":56810,\"start\":56806},{\"end\":56828,\"start\":56819},{\"end\":57127,\"start\":57122},{\"end\":57143,\"start\":57138},{\"end\":57160,\"start\":57150},{\"end\":57178,\"start\":57171},{\"end\":57198,\"start\":57187},{\"end\":57493,\"start\":57486},{\"end\":57512,\"start\":57502},{\"end\":57532,\"start\":57523},{\"end\":57543,\"start\":57539},{\"end\":57559,\"start\":57555},{\"end\":57576,\"start\":57568},{\"end\":57592,\"start\":57584},{\"end\":57606,\"start\":57600},{\"end\":57621,\"start\":57615},{\"end\":57632,\"start\":57623},{\"end\":58110,\"start\":58106},{\"end\":58125,\"start\":58121},{\"end\":58139,\"start\":58130},{\"end\":58155,\"start\":58152},{\"end\":58169,\"start\":58166},{\"end\":58182,\"start\":58179},{\"end\":58528,\"start\":58522},{\"end\":58542,\"start\":58535},{\"end\":58556,\"start\":58549},{\"end\":58571,\"start\":58568},{\"end\":58586,\"start\":58580},{\"end\":58602,\"start\":58596},{\"end\":58614,\"start\":58610},{\"end\":58622,\"start\":58620},{\"end\":58635,\"start\":58632},{\"end\":58976,\"start\":58963},{\"end\":58993,\"start\":58990},{\"end\":59006,\"start\":59001},{\"end\":59022,\"start\":59015},{\"end\":59035,\"start\":59031},{\"end\":59050,\"start\":59043},{\"end\":59076,\"start\":59068},{\"end\":59095,\"start\":59087},{\"end\":59110,\"start\":59106},{\"end\":59117,\"start\":59112},{\"end\":59573,\"start\":59568},{\"end\":59588,\"start\":59583},{\"end\":59602,\"start\":59598},{\"end\":59614,\"start\":59612},{\"end\":59628,\"start\":59620},{\"end\":59638,\"start\":59632},{\"end\":59651,\"start\":59649},{\"end\":59662,\"start\":59659},{\"end\":59674,\"start\":59670},{\"end\":59692,\"start\":59685},{\"end\":59702,\"start\":59694},{\"end\":60063,\"start\":60059},{\"end\":60074,\"start\":60067},{\"end\":60081,\"start\":60076},{\"end\":60382,\"start\":60375},{\"end\":60398,\"start\":60392},{\"end\":60415,\"start\":60408},{\"end\":60432,\"start\":60424},{\"end\":60452,\"start\":60445},{\"end\":60470,\"start\":60463},{\"end\":60494,\"start\":60478},{\"end\":60506,\"start\":60501},{\"end\":60521,\"start\":60515},{\"end\":60528,\"start\":60523},{\"end\":60963,\"start\":60959},{\"end\":60975,\"start\":60971},{\"end\":60987,\"start\":60985},{\"end\":60999,\"start\":60994},{\"end\":61369,\"start\":61365},{\"end\":61377,\"start\":61374},{\"end\":61393,\"start\":61387},{\"end\":61409,\"start\":61402},{\"end\":61419,\"start\":61416},{\"end\":61754,\"start\":61751},{\"end\":61778,\"start\":61769},{\"end\":61792,\"start\":61787},{\"end\":61804,\"start\":61802},{\"end\":61817,\"start\":61815},{\"end\":61829,\"start\":61827},{\"end\":62095,\"start\":62092},{\"end\":62105,\"start\":62103},{\"end\":62117,\"start\":62115},{\"end\":62517,\"start\":62513},{\"end\":62528,\"start\":62524},{\"end\":62541,\"start\":62538},{\"end\":62558,\"start\":62553},{\"end\":62572,\"start\":62568},{\"end\":62585,\"start\":62582},{\"end\":62911,\"start\":62907},{\"end\":62924,\"start\":62921},{\"end\":62935,\"start\":62931},{\"end\":62945,\"start\":62943},{\"end\":62958,\"start\":62955},{\"end\":63375,\"start\":63371},{\"end\":63385,\"start\":63382},{\"end\":63399,\"start\":63397},{\"end\":63411,\"start\":63407},{\"end\":63422,\"start\":63419},{\"end\":63433,\"start\":63429},{\"end\":63446,\"start\":63442},{\"end\":63456,\"start\":63454},{\"end\":63469,\"start\":63464},{\"end\":63479,\"start\":63476},{\"end\":63779,\"start\":63774},{\"end\":63795,\"start\":63789},{\"end\":63808,\"start\":63803},{\"end\":63823,\"start\":63816},{\"end\":63834,\"start\":63830},{\"end\":63848,\"start\":63844},{\"end\":63867,\"start\":63862},{\"end\":63878,\"start\":63874},{\"end\":63887,\"start\":63885},{\"end\":63904,\"start\":63892},{\"end\":64320,\"start\":64316},{\"end\":64330,\"start\":64328},{\"end\":64345,\"start\":64339},{\"end\":64358,\"start\":64356},{\"end\":64371,\"start\":64366},{\"end\":64676,\"start\":64672},{\"end\":64686,\"start\":64684},{\"end\":64696,\"start\":64694},{\"end\":64708,\"start\":64704},{\"end\":64716,\"start\":64713},{\"end\":64728,\"start\":64725}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":250243681},\"end\":46890,\"start\":46110},{\"attributes\":{\"doi\":\"arXiv:2204.06745\",\"id\":\"b1\"},\"end\":47374,\"start\":46892},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":237940329},\"end\":47926,\"start\":47376},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":48444,\"start\":47928},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b4\"},\"end\":48901,\"start\":48446},{\"attributes\":{\"doi\":\"arXiv:1905.10044\",\"id\":\"b5\"},\"end\":49321,\"start\":48903},{\"attributes\":{\"doi\":\"arXiv:1803.05457\",\"id\":\"b6\"},\"end\":49769,\"start\":49323},{\"attributes\":{\"doi\":\"arXiv:1602.02830\",\"id\":\"b7\"},\"end\":50221,\"start\":49771},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":231855747},\"end\":50722,\"start\":50223},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":249151871},\"end\":51127,\"start\":50724},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b10\"},\"end\":51451,\"start\":51129},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b11\"},\"end\":51812,\"start\":51453},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b12\"},\"end\":52149,\"start\":51814},{\"attributes\":{\"doi\":\"arXiv:2301.00774\",\"id\":\"b13\"},\"end\":52404,\"start\":52151},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b14\"},\"end\":52768,\"start\":52406},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":251928917},\"end\":53319,\"start\":52770},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53719799},\"end\":53984,\"start\":53321},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":251000444},\"end\":54681,\"start\":53986},{\"attributes\":{\"doi\":\"arXiv:2211.05110\",\"id\":\"b18\"},\"end\":55087,\"start\":54683},{\"attributes\":{\"doi\":\"arXiv:2102.05426\",\"id\":\"b19\"},\"end\":55505,\"start\":55089},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6278891},\"end\":55852,\"start\":55507},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5151364},\"end\":56413,\"start\":55854},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b22\"},\"end\":56679,\"start\":56415},{\"attributes\":{\"doi\":\"arXiv:1809.02789\",\"id\":\"b23\"},\"end\":57051,\"start\":56681},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":216056295},\"end\":57478,\"start\":57053},{\"attributes\":{\"doi\":\"arXiv:1606.06031\",\"id\":\"b25\"},\"end\":58007,\"start\":57480},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b26\"},\"end\":58431,\"start\":58009},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204838007},\"end\":58954,\"start\":58433},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b28\"},\"end\":59480,\"start\":58956},{\"attributes\":{\"doi\":\"arXiv:2303.06865\",\"id\":\"b29\"},\"end\":60000,\"start\":59482},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1545102},\"end\":60368,\"start\":60002},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b31\"},\"end\":60868,\"start\":60370},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":221506623},\"end\":61263,\"start\":60870},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b33\"},\"end\":61649,\"start\":61265},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b34\"},\"end\":62083,\"start\":61651},{\"attributes\":{\"doi\":\"arXiv:2303.08302\",\"id\":\"b35\"},\"end\":62439,\"start\":62085},{\"attributes\":{\"doi\":\"arXiv:2110.07809\",\"id\":\"b36\"},\"end\":62805,\"start\":62441},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":251105613},\"end\":63363,\"start\":62807},{\"attributes\":{\"doi\":\"arXiv:2210.02414\",\"id\":\"b38\"},\"end\":63766,\"start\":63365},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b39\"},\"end\":64216,\"start\":63768},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":59413897},\"end\":64661,\"start\":64218},{\"attributes\":{\"doi\":\"arXiv:1606.06160\",\"id\":\"b41\"},\"end\":65055,\"start\":64663},{\"attributes\":{\"id\":\"b42\"},\"end\":65389,\"start\":65057},{\"attributes\":{\"id\":\"b43\"},\"end\":65407,\"start\":65391}]", "bib_title": "[{\"end\":46204,\"start\":46110},{\"end\":47457,\"start\":47376},{\"end\":47965,\"start\":47928},{\"end\":50315,\"start\":50223},{\"end\":50799,\"start\":50724},{\"end\":52859,\"start\":52770},{\"end\":53407,\"start\":53321},{\"end\":54104,\"start\":53986},{\"end\":55563,\"start\":55507},{\"end\":55912,\"start\":55854},{\"end\":57113,\"start\":57053},{\"end\":58514,\"start\":58433},{\"end\":60049,\"start\":60002},{\"end\":60949,\"start\":60870},{\"end\":62897,\"start\":62807},{\"end\":64306,\"start\":64218}]", "bib_author": "[{\"end\":46230,\"start\":46206},{\"end\":46250,\"start\":46230},{\"end\":46268,\"start\":46250},{\"end\":46278,\"start\":46268},{\"end\":46285,\"start\":46278},{\"end\":46298,\"start\":46285},{\"end\":46315,\"start\":46298},{\"end\":46329,\"start\":46315},{\"end\":46343,\"start\":46329},{\"end\":46356,\"start\":46343},{\"end\":46903,\"start\":46892},{\"end\":46920,\"start\":46903},{\"end\":46935,\"start\":46920},{\"end\":46952,\"start\":46935},{\"end\":46961,\"start\":46952},{\"end\":46979,\"start\":46961},{\"end\":46990,\"start\":46979},{\"end\":47004,\"start\":46990},{\"end\":47019,\"start\":47004},{\"end\":47479,\"start\":47459},{\"end\":47493,\"start\":47479},{\"end\":47513,\"start\":47493},{\"end\":47978,\"start\":47967},{\"end\":47993,\"start\":47978},{\"end\":48005,\"start\":47993},{\"end\":48022,\"start\":48005},{\"end\":48038,\"start\":48022},{\"end\":48057,\"start\":48038},{\"end\":48077,\"start\":48057},{\"end\":48091,\"start\":48077},{\"end\":48106,\"start\":48091},{\"end\":48121,\"start\":48106},{\"end\":48461,\"start\":48446},{\"end\":48472,\"start\":48461},{\"end\":48495,\"start\":48472},{\"end\":48509,\"start\":48495},{\"end\":48531,\"start\":48509},{\"end\":48551,\"start\":48531},{\"end\":48567,\"start\":48551},{\"end\":48994,\"start\":48975},{\"end\":49006,\"start\":48994},{\"end\":49022,\"start\":49006},{\"end\":49039,\"start\":49022},{\"end\":49056,\"start\":49039},{\"end\":49076,\"start\":49056},{\"end\":49416,\"start\":49403},{\"end\":49430,\"start\":49416},{\"end\":49444,\"start\":49430},{\"end\":49457,\"start\":49444},{\"end\":49475,\"start\":49457},{\"end\":49494,\"start\":49475},{\"end\":49510,\"start\":49494},{\"end\":49793,\"start\":49771},{\"end\":49806,\"start\":49793},{\"end\":49821,\"start\":49806},{\"end\":49835,\"start\":49821},{\"end\":49850,\"start\":49835},{\"end\":50328,\"start\":50317},{\"end\":50347,\"start\":50328},{\"end\":50357,\"start\":50347},{\"end\":50371,\"start\":50357},{\"end\":50386,\"start\":50371},{\"end\":50403,\"start\":50386},{\"end\":50810,\"start\":50801},{\"end\":50818,\"start\":50810},{\"end\":50833,\"start\":50818},{\"end\":50845,\"start\":50833},{\"end\":50861,\"start\":50845},{\"end\":51143,\"start\":51129},{\"end\":51155,\"start\":51143},{\"end\":51171,\"start\":51155},{\"end\":51189,\"start\":51171},{\"end\":51467,\"start\":51453},{\"end\":51483,\"start\":51467},{\"end\":51495,\"start\":51483},{\"end\":51520,\"start\":51495},{\"end\":51824,\"start\":51814},{\"end\":51841,\"start\":51824},{\"end\":51860,\"start\":51841},{\"end\":51882,\"start\":51860},{\"end\":51906,\"start\":51882},{\"end\":51913,\"start\":51906},{\"end\":52228,\"start\":52213},{\"end\":52242,\"start\":52228},{\"end\":52504,\"start\":52489},{\"end\":52528,\"start\":52504},{\"end\":52541,\"start\":52528},{\"end\":52551,\"start\":52541},{\"end\":52871,\"start\":52861},{\"end\":52883,\"start\":52871},{\"end\":52897,\"start\":52883},{\"end\":52908,\"start\":52897},{\"end\":52918,\"start\":52908},{\"end\":52930,\"start\":52918},{\"end\":52941,\"start\":52930},{\"end\":52952,\"start\":52941},{\"end\":53422,\"start\":53409},{\"end\":53437,\"start\":53422},{\"end\":53451,\"start\":53437},{\"end\":53463,\"start\":53451},{\"end\":53477,\"start\":53463},{\"end\":53492,\"start\":53477},{\"end\":53507,\"start\":53492},{\"end\":53522,\"start\":53507},{\"end\":54118,\"start\":54106},{\"end\":54142,\"start\":54118},{\"end\":54153,\"start\":54142},{\"end\":54164,\"start\":54153},{\"end\":54176,\"start\":54164},{\"end\":54184,\"start\":54176},{\"end\":54195,\"start\":54184},{\"end\":54209,\"start\":54195},{\"end\":54222,\"start\":54209},{\"end\":54232,\"start\":54222},{\"end\":54751,\"start\":54739},{\"end\":54770,\"start\":54751},{\"end\":54785,\"start\":54770},{\"end\":54795,\"start\":54785},{\"end\":54811,\"start\":54795},{\"end\":54825,\"start\":54811},{\"end\":54835,\"start\":54825},{\"end\":54849,\"start\":54835},{\"end\":55180,\"start\":55169},{\"end\":55193,\"start\":55180},{\"end\":55201,\"start\":55193},{\"end\":55212,\"start\":55201},{\"end\":55221,\"start\":55212},{\"end\":55231,\"start\":55221},{\"end\":55243,\"start\":55231},{\"end\":55253,\"start\":55243},{\"end\":55261,\"start\":55253},{\"end\":55577,\"start\":55565},{\"end\":55928,\"start\":55914},{\"end\":55939,\"start\":55928},{\"end\":55963,\"start\":55939},{\"end\":55981,\"start\":55963},{\"end\":55991,\"start\":55981},{\"end\":56006,\"start\":55991},{\"end\":56018,\"start\":56006},{\"end\":56038,\"start\":56018},{\"end\":56464,\"start\":56448},{\"end\":56479,\"start\":56464},{\"end\":56495,\"start\":56479},{\"end\":56511,\"start\":56495},{\"end\":56786,\"start\":56770},{\"end\":56799,\"start\":56786},{\"end\":56812,\"start\":56799},{\"end\":56830,\"start\":56812},{\"end\":57129,\"start\":57115},{\"end\":57145,\"start\":57129},{\"end\":57162,\"start\":57145},{\"end\":57180,\"start\":57162},{\"end\":57200,\"start\":57180},{\"end\":57495,\"start\":57480},{\"end\":57514,\"start\":57495},{\"end\":57534,\"start\":57514},{\"end\":57545,\"start\":57534},{\"end\":57561,\"start\":57545},{\"end\":57578,\"start\":57561},{\"end\":57594,\"start\":57578},{\"end\":57608,\"start\":57594},{\"end\":57623,\"start\":57608},{\"end\":57634,\"start\":57623},{\"end\":58112,\"start\":58100},{\"end\":58127,\"start\":58112},{\"end\":58141,\"start\":58127},{\"end\":58157,\"start\":58141},{\"end\":58171,\"start\":58157},{\"end\":58184,\"start\":58171},{\"end\":58530,\"start\":58516},{\"end\":58544,\"start\":58530},{\"end\":58558,\"start\":58544},{\"end\":58573,\"start\":58558},{\"end\":58588,\"start\":58573},{\"end\":58604,\"start\":58588},{\"end\":58616,\"start\":58604},{\"end\":58624,\"start\":58616},{\"end\":58637,\"start\":58624},{\"end\":58978,\"start\":58956},{\"end\":58995,\"start\":58978},{\"end\":59008,\"start\":58995},{\"end\":59024,\"start\":59008},{\"end\":59037,\"start\":59024},{\"end\":59052,\"start\":59037},{\"end\":59078,\"start\":59052},{\"end\":59097,\"start\":59078},{\"end\":59112,\"start\":59097},{\"end\":59119,\"start\":59112},{\"end\":59575,\"start\":59563},{\"end\":59590,\"start\":59575},{\"end\":59604,\"start\":59590},{\"end\":59616,\"start\":59604},{\"end\":59630,\"start\":59616},{\"end\":59640,\"start\":59630},{\"end\":59653,\"start\":59640},{\"end\":59664,\"start\":59653},{\"end\":59676,\"start\":59664},{\"end\":59694,\"start\":59676},{\"end\":59704,\"start\":59694},{\"end\":60065,\"start\":60051},{\"end\":60076,\"start\":60065},{\"end\":60083,\"start\":60076},{\"end\":60384,\"start\":60370},{\"end\":60400,\"start\":60384},{\"end\":60417,\"start\":60400},{\"end\":60434,\"start\":60417},{\"end\":60454,\"start\":60434},{\"end\":60472,\"start\":60454},{\"end\":60496,\"start\":60472},{\"end\":60508,\"start\":60496},{\"end\":60523,\"start\":60508},{\"end\":60530,\"start\":60523},{\"end\":60965,\"start\":60951},{\"end\":60977,\"start\":60965},{\"end\":60989,\"start\":60977},{\"end\":61001,\"start\":60989},{\"end\":61371,\"start\":61355},{\"end\":61379,\"start\":61371},{\"end\":61395,\"start\":61379},{\"end\":61411,\"start\":61395},{\"end\":61421,\"start\":61411},{\"end\":61756,\"start\":61744},{\"end\":61780,\"start\":61756},{\"end\":61794,\"start\":61780},{\"end\":61806,\"start\":61794},{\"end\":61819,\"start\":61806},{\"end\":61831,\"start\":61819},{\"end\":62097,\"start\":62085},{\"end\":62107,\"start\":62097},{\"end\":62119,\"start\":62107},{\"end\":62519,\"start\":62505},{\"end\":62530,\"start\":62519},{\"end\":62543,\"start\":62530},{\"end\":62560,\"start\":62543},{\"end\":62574,\"start\":62560},{\"end\":62587,\"start\":62574},{\"end\":62913,\"start\":62899},{\"end\":62926,\"start\":62913},{\"end\":62937,\"start\":62926},{\"end\":62947,\"start\":62937},{\"end\":62960,\"start\":62947},{\"end\":63377,\"start\":63365},{\"end\":63387,\"start\":63377},{\"end\":63401,\"start\":63387},{\"end\":63413,\"start\":63401},{\"end\":63424,\"start\":63413},{\"end\":63435,\"start\":63424},{\"end\":63448,\"start\":63435},{\"end\":63458,\"start\":63448},{\"end\":63471,\"start\":63458},{\"end\":63481,\"start\":63471},{\"end\":63781,\"start\":63768},{\"end\":63797,\"start\":63781},{\"end\":63810,\"start\":63797},{\"end\":63825,\"start\":63810},{\"end\":63836,\"start\":63825},{\"end\":63850,\"start\":63836},{\"end\":63869,\"start\":63850},{\"end\":63880,\"start\":63869},{\"end\":63889,\"start\":63880},{\"end\":63906,\"start\":63889},{\"end\":64322,\"start\":64308},{\"end\":64332,\"start\":64322},{\"end\":64347,\"start\":64332},{\"end\":64360,\"start\":64347},{\"end\":64373,\"start\":64360},{\"end\":64678,\"start\":64663},{\"end\":64688,\"start\":64678},{\"end\":64698,\"start\":64688},{\"end\":64710,\"start\":64698},{\"end\":64718,\"start\":64710},{\"end\":64730,\"start\":64718}]", "bib_venue": "[{\"end\":46461,\"start\":46356},{\"end\":47113,\"start\":47035},{\"end\":47599,\"start\":47513},{\"end\":48170,\"start\":48121},{\"end\":48652,\"start\":48583},{\"end\":48973,\"start\":48903},{\"end\":49401,\"start\":49323},{\"end\":49974,\"start\":49866},{\"end\":50446,\"start\":50403},{\"end\":50910,\"start\":50861},{\"end\":51258,\"start\":51205},{\"end\":51610,\"start\":51536},{\"end\":51959,\"start\":51929},{\"end\":52211,\"start\":52151},{\"end\":52487,\"start\":52406},{\"end\":53023,\"start\":52952},{\"end\":53603,\"start\":53522},{\"end\":54314,\"start\":54232},{\"end\":54737,\"start\":54683},{\"end\":55167,\"start\":55089},{\"end\":55622,\"start\":55577},{\"end\":56090,\"start\":56038},{\"end\":56446,\"start\":56415},{\"end\":56768,\"start\":56681},{\"end\":57244,\"start\":57200},{\"end\":57722,\"start\":57650},{\"end\":58098,\"start\":58009},{\"end\":58677,\"start\":58637},{\"end\":59191,\"start\":59135},{\"end\":59561,\"start\":59482},{\"end\":60162,\"start\":60083},{\"end\":60591,\"start\":60546},{\"end\":61045,\"start\":61001},{\"end\":61353,\"start\":61265},{\"end\":61742,\"start\":61651},{\"end\":62242,\"start\":62135},{\"end\":62503,\"start\":62441},{\"end\":63011,\"start\":62960},{\"end\":63542,\"start\":63497},{\"end\":63966,\"start\":63922},{\"end\":64417,\"start\":64373},{\"end\":64837,\"start\":64746},{\"end\":65121,\"start\":65057},{\"end\":65398,\"start\":65391},{\"end\":47672,\"start\":47601},{\"end\":50476,\"start\":50448},{\"end\":53671,\"start\":53605},{\"end\":55667,\"start\":55624},{\"end\":56114,\"start\":56092},{\"end\":63029,\"start\":63013}]"}}}, "year": 2023, "month": 12, "day": 17}