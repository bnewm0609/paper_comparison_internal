{"id": 234482766, "updated": "2023-10-06 02:52:52.46", "metadata": {"title": "Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration", "authors": "[{\"first\":\"Edward\",\"last\":\"Johns\",\"middle\":[]}]", "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA)", "journal": "2021 IEEE International Conference on Robotics and Automation (ICRA)", "publication_date": {"year": 2021, "month": 5, "day": 13}, "abstract": "We introduce a simple new method for visual imitation learning, which allows a novel robot manipulation task to be learned from a single human demonstration, without requiring any prior knowledge of the object being interacted with. Our method models imitation learning as a state estimation problem, with the state defined as the end-effector's pose at the point where object interaction begins, as observed from the demonstration. By then modelling a manipulation task as a coarse, approach trajectory followed by a fine, interaction trajectory, this state estimator can be trained in a self-supervised manner, by automatically moving the end-effector's camera around the object. At test time, the end-effector moves to the estimated state through a linear path, at which point the original demonstration's end-effector velocities are simply replayed. This enables convenient acquisition of a complex interaction trajectory, without actually needing to explicitly learn a policy. Real-world experiments on 8 everyday tasks show that our method can learn a diverse range of skills from a single human demonstration, whilst also yielding a stable and interpretable controller.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.06411", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/Johns21", "doi": "10.1109/icra48506.2021.9560942"}}, "content": {"source": {"pdf_hash": "1563b3cf96bd3ffb257ee89cf2f1120b41f37d6e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.06411v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2105.06411", "status": "GREEN"}}, "grobid": {"id": "7cc8096d71b506813069a616e45b06b6d09ca719", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1563b3cf96bd3ffb257ee89cf2f1120b41f37d6e.txt", "contents": "\nCoarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration\n\n\nEdward Johns \nCoarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration\n\nWe introduce a simple new method for visual imitation learning, which allows a novel robot manipulation task to be learned from a single human demonstration, without requiring any prior knowledge of the object being interacted with. Our method models imitation learning as a state estimation problem, with the state defined as the end-effector's pose at the point where object interaction begins, as observed from the demonstration. By then modelling a manipulation task as a coarse, approach trajectory followed by a fine, interaction trajectory, this state estimator can be trained in a self-supervised manner, by automatically moving the endeffector's camera around the object. At test time, the endeffector moves to the estimated state through a linear path, at which point the original demonstration's end-effector velocities are simply replayed. This enables convenient acquisition of a complex interaction trajectory, without actually needing to explicitly learn a policy. Real-world experiments on 8 everyday tasks show that our method can learn a diverse range of skills from a single human demonstration, whilst also yielding a stable and interpretable controller. Videos are available at: www.robot-learning.uk/coarse-to-fine-imitation-learning.\n\nI. INTRODUCTION\n\nThe goal of imitation learning can be described as teaching a robot how to perform a novel task from human demonstrations, whilst minimising two criteria: (1) the amount of physical interaction required of the human, and (2) the amount of prior task knowledge required by the algorithm. Existing solutions typically fail in one or both of these. Methods that learn from demonstrations alone, such as behavioural cloning [1], require a large number of demonstrations. Methods that bootstrap from demonstrations with additional self-exploration, such as reinforcement learning [2], require environment resetting via manual intervention or task-specific apparatus. Methods that transfer knowledge from other similar tasks, such as meta-imitation learning [3], require prior knowledge of the task family. And methods that analytically model tasks in a manually-defined state space, such as dynamic movement primitives [4], require prior knowledge of the specific task. In this paper, we introduce a simple new method which addresses both these criteria: our method learns everyday robot manipulation tasks from just a single human demonstration, without requiring any prior knowledge of the object which is being interacted with.\n\nOur method models the robot's motion as a coarseto-fine trajectory, as shown in Figure 1. In typical object manipulation tasks, we observe that the end-effector first approaches the object's bottleneck in a coarse manner through Fig. 1: We model a robot manipulation task as a coarse approach trajectory, followed by a fine interaction trajectory. Training for the approach trajectory is self-supervised, and the interaction trajectory needs only a single demonstration. Here, we illustrate teaching the robot how to use a hammer. free space, and then physically interacts with the object in a fine manner. We can thus model the approach analytically with a simple linear path. And therefore, rather than requiring demonstrations in this space, a camera mounted to the endeffector can be moved around the object automatically to collect observations from multiple viewpoints, thus achieving self-supervised generalisation across the task space.\n\nBuilding on this foundation, our method then models imitation learning as a state estimation problem. Typically, state estimation for robot manipulation requires prior knowledge of the object, such as a pre-defined pose estimator [4] or fiducial markers [5], [6]. But we show how to achieve this with a novel object, by using the demonstration itself to define a state in 3D space: the object's bottleneck. Intuitively, the bottleneck represents the end-effector's pose at the point where object interaction should begin, as observed from the demonstration, which the robot should then attempt to reach during testing. A bottleneck pose predictor can then be trained using self-supervised learning, as introduced above. During testing, the robot estimates the bottleneck pose and moves the end-effector directly there. This then leads to the crux of our method: if the robot can reach the bottleneck pose with sufficient accuracy, we propose that the robot can then just replay the original demonstration's end-effector velocities from the bottleneck onwards, with the simplest form of behavioural cloning. This enables convenient acquisition of a complex interaction trajectory without needing to explicitly learn a policy. And since the robot explores only in free space, and not during object interaction, we avoid the need for the repeated environment resetting which reinforcement learning suffers from.\n\nIn summary, we propose Coarse-to-Fine Imitation Learning, which combines the strengths of both analytical modelling and machine learning, for a data-efficient yet flexible framework. Furthermore, since machine learning is used only for pose estimation and not for policy learning, the controller itself is analytical, so we provide stability and interpretability in contrast to many recent methods in the field which adopt black-box, end-to-end policy learning. In real-world experiments, we study a number of different implementations of our method, and show how it can learn a diverse range of everyday tasks. Videos are available at: www.robot-learning.uk/coarse-to-fine-imitation-learning.\n\n\nII. RELATED WORK\n\nWhilst there is significant prior work on imitation learning using manually-defined low-dimensional state representations [4], [5], [6], these require task-specific prior knowledge, such as pre-defined pose estimators, or artificial experimental setups, such as fiducial markers. Therefore, in this section, we focus on methods where a robot can learn a new task from a human without requiring state-space engineering, particularly through visual observations.\n\nBehavioural cloning methods use supervised learning to map observations to actions. This is often achieved through end-to-end learning, where a range of experimental setups have been designed to capture actions [1], [7], [8], [9], [10], [11]. Image representations can be regularised through keypoint-based architectures [1] or self-supervision [8]. Similar to our coarse-to-fine formulation, [12] combine planning with behavioural cloning. However, since all these methods explicitly learn an end-to-end policy, they require a large number of human demonstrations to achieve generalisation, whereas our method requires only a single demonstration. Exploration-based methods use self-exploration of the environment to learn a policy, bootstrapping from the original demonstrations. This can be done by pre-training the policy [13], learning a residual policy [14], [15], pre-loading the replay buffer [2], adding a behavioural cloning loss to the actor [16], or restricting the exploration space [17], [18]. A reward function can be inferred from a goal image [19], inverse reinforcement learning [20], active learning [21], or contrastive learning [22]. Generative methods can attempt to reconstruct the demonstrations [23], [24], [25]. Exploration data can also be used to learn a dynamics model for trajectory optimisation, whose cost is defined by a demonstration [26], [27]. However, self-exploration methods can be unsafe, and additionally require environment resetting via human intervention or task-specific apparatus, whereas our method does not require any further exploration of object interaction.\n\nTransfer learning methods aim to exploit prior knowledge of similar tasks, to learn a new task from one or a few demonstrations. This can be achieved by supervised learning across a task family [28], or by jointly learning a task embedding and policy [29]. Meta-learning an initialisation of network parameters for fast adaptation [30] can be deployed for behavioural cloning [3] and learning from observations of humans [31]. However, the efficient adaptation in these methods comes at the cost of requiring prior training on similar tasks to the new task being learned, whereas our method assumes no prior knowledge of the task family.\n\nOur method is also related to visual servoing. [32], [33] attempt to align the robot's current image observation with a goal image, but require task-specific controllers to be manually defined for object interaction, whereas our method can learn from just a demonstration. [34] use visual servoing to directly track a demonstration, but require close initial alignment and only evaluate on tasks with very limited object interaction, whereas our method generalises across a wide task space and can facilitate complex interaction trajectories.\n\n\nIII. METHOD\n\n\nA. Coordinate Frames\n\nWe first define the following coordinate frames: the robot's base R, the robot's end-effector E, and the object's bottleneck B, as shown in Figure 2. The bottleneck is a \"virtual\" frame and does not represent a physical body in the current scene, but represents where the end-effector should be at the point where interaction begins, such that all coarse trajectories converge at the bottleneck. B can considered to be fixed relative to the object. A homogeneous transformation matrix T EB represents frame B expressed in frame E.\n\n\nB. Coarse-to-Fine Trajectories\n\nWe observe that typical manipulation tasks begin with a coarse approach trajectory, followed by a fine interaction trajectory. During the approach, only the destination is important and not the specific trajectory, whereas during the interaction, the specific trajectory is crucial. We therefore execute the approach trajectory as a simple linear path using inverse kinematics. This does not require human demonstrations, and so we only require a demonstration of the interaction trajectory. From here, two questions now emerge.\n\n(1) During testing, what should the robot execute as the interaction trajectory? (2) During testing, where should the interaction trajectory begin? We now introduce two axioms for our assumed environment setup, which answer these.\n\nThe first axiom is that, for a controller which outputs actions relative to the end-effector's local frame, those actions should depend only on the relative pose between the end-effector and the object. Therefore, if we record the demonstration's local end-effector velocities from the bottleneck point onwards, then during testing, we can move the end-effector to the bottleneck and then replay those velocities for the interaction trajectory. This answers our first question. Note that since the interaction trajectory is executed open loop, any error in reaching the bottleneck would result\nR B Training E 2 3 1 R Testing E 1 2 B 3 T EB I f : I \u2192 T EB T EB I g : I \u2192 T EB dataset dataset \u2130 Fig. 2:\nVisualising the different stages of training and testing. During training, 1 first a single human demonstration is provided from the bottleneck (e.g. here, opening a lid), 2 then the robot moves its end-effector and camera around the bottleneck in the blue region, whilst collecting a dataset D of images I and bottleneck poses T EB to train network f (I), 3 then the robot collects a similar dataset E but in the smaller green region, to train network g(I). During testing, with the object now moved, 1 first the robot uses f (I) to estimate the bottleneck pose T EB and move towards it, 2 then the robot makes a last-inch correction to its estimate using g(I), 3 then the robot replays the demonstration's end-effector velocities.\n\nin a growing offset from the demonstration, during the interaction trajectory. Therefore, we minimise the length of the interaction trajectory by beginning the demonstration as close to the object as possible, and define the bottleneck pose T RB during training as the demonstration's initial endeffector pose T RE . The demonstration should begin no lower than the object's highest point, to avoid collisions during the self-supervised data collection (see Section III-C).\n\nThe second axiom is that the camera's image is a function of the relative pose between the camera and the object. Since we mount the camera rigidly to the end-effector, and the bottleneck is fixed relative to the object, it follows that this image is also a function of the relative pose between the endeffector and the bottleneck, T EB . Therefore, we can collect images and associated T EB values by automatically moving the camera around the object, and then train a function which predicts T EB from an image. Then, during testing, the robot can predict the pose of the bottleneck, move the end-effector there, and then execute the interaction trajectory. This answers our second question. Note that in reality, this axiom is an approximation due to observation noise, illumination effects, and variation in background, although these are not significant in our experimental setup and could potentially be mitigated by image augmentation.\n\n\nC. Self-Supervised Learning\n\nData collection. We define f (I) as the above function to predict T EB from image I. After the demonstration, we collect dataset D = {(I, T EB ) i }, with T EB calculated by:\nT EB = T ER T RB ,(1)\nwhere T ER is calculated using forward kinematics, and T RB is the bottleneck pose defined from the demonstration. Note that Equation 1 assumes that the object is in the same pose when T RB is first defined, as when D is collected. Therefore, if the object moves during the human demonstration, then either it should be re-positioned, or alternatively, D can be collected before the human demonstration, by first manually setting the bottleneck pose with the end-effector, and later returning to this pose to start the demonstration. Now, since the approach trajectory is a simple linear path and does not require human demonstrations, we can collect D automatically, by moving the end-effector around in the space above the bottleneck (the blue region in Figure 2), to capture the appearance of the object from a range of viewpoints. We do this over a number of trajectories, where each first moves the robot to a random initial pose above the object, and then moves in a straight line at a constant speed towards a pose sampled from near the bottleneck, whilst adding (I, T EB ) pairs to D. Given dataset D, we then train f (I) as a convolutional neural network using supervised learning. This is done by directly regressing the pose, and therefore we do not require any camera calibration, as long as the camera remains rigidly mounted to the end-effector.\n\nSimplifying the prediction space. Now, since the specific motion during the approach trajectory is not important, we can constrain the space of bottleneck poses which the trajectory terminates at, and hence simplify the prediction space of the pose estimator f (I). We assume a typical tabletop setup, where the object is limited to horizontal translation and rotation about the vertical axis. To capture only this rotation dimension, we constrain the end-effector poses to be \"vertical\" and pointing directly downwards, and allow end-effector rotation only about the vertical axis, both when collecting D, and during the approach trajectory when testing. Given the known height of the bottleneck, this reduces the prediction space of f (I) to three dimensions: two horizontal translations, and one rotation about the vertical. However, all this assumes that the bottleneck is indeed vertical, even though the end-effector's pose at the start of the demonstration may not be. Therefore, we define the bottleneck as having the same position as the demonstration's initial pose, but with a vertical orientation. We then record R, the 3D rotation between the bottleneck and this initial pose, which is later applied during testing at the end of the approach trajectory, to re-orientate with the demonstration. Note that during testing, the interaction trajectory can still be 6-DOF despite the bottleneck pose estimation being 3-DOF.\n\nLast-inch correction. Together with collecting dataset D to train f (I), we also collect a second dataset E to train a second network g(I), to enable a \"last-inch\" correction to the bottleneck pose estimate. E is collected in a similar manner to D, except that the end-effector's height is fixed at the bottleneck's height throughout data collection, such that each trajectory only moves horizontally with rotation about the vertical (the green region in Figure 2). This ensures that g(I) specialises in predictions where the end-effector is already very close to the bottleneck, without images from the remainder of the task space consuming network capacity. During testing, at the end of the approach trajectory, g(I) is then used to make one final estimate of the bottleneck pose. Algorithm 1 summarises the overall training procedure. \n\n\nD. Sequential State Estimation\n\nDuring the approach trajectory, we assume that the object is stationary. Therefore, every prediction of the bottleneck pose is a prediction of the same value. So rather than only using the prediction from the image at the current time step, we can also consider fusing together multiple sequential predictions. First, let us denotex t as 3-dimensional vector representing the horizontal position and vertical orientation of the predicted bottleneck pose at time t, such thatx t \u2261 f (I t ). We also assume a Gaussian uncertainty for this vector, represented as a standard deviation\u03c3 t , which will be discussed below. Then, let us denotex t as the estimate returned by our sequential estimation method, which depends on predictionsx 0 . . .x t . This is also assigned a Gaussian uncertainty\u03c3 t . We now propose two simple methods to calculatex t : firstly, by averaging the individual predictions as a Batch, and secondly, by Filtering the individual predictions.\n\nBatch. In batch estimation,x t is re-estimated at each time step using all previous individual predictions,x 0 . . .x t . Batch estimation then uses inverse-variance weighting [35] to combine eachx, which is the optimal estimator under Gaussian uncertainty \u03c3. The estimate is calculated by:\nx t = \u03c4 =t \u03c4 =0x \u03c4 /\u03c3 2 \u03c4 \u03c4 =t \u03c4 =0 1/\u03c3 2 \u03c4 .\n(2)\n\nFiltering. In estimation via filtering,x t is re-estimated at each time step using the estimate from the previous time step,x t\u22121 , and the prediction at the current time step,x t , together with the associated uncertainties:\nx t = (x t\u22121 /\u03c3 2 t\u22121 ) + (x t /\u03c3 2 t ) (1/\u03c3 2 t\u22121 ) + (1/\u03c3 2 t ) ,(3)\u03c3 2 t = 1 (1/\u03c3 2 t\u22121 ) + (1/\u03c3 2 t ) ,(4)\nwhere the initialx 0 is set tox 0 , and the initial\u03c3 0 is set to the Prior uncertainty, defined below. This is a form of Bayes filter [36], and is derived from a Kalman filter with zero process noise, since we assume that kinematic errors are insignificant relative to the uncertainty in f (I)'s predictions. Estimating Uncertainty. Both the Batch and Filtering sequential estimation methods require a Gaussian uncertaint\u0177 \u03c3 associated with each predictionx. In our experiments, we investigated three approaches to calculating\u03c3. The first, is to use Dropout to model f (I) as an approximate Bayesian neural network [37], where we retain Dropout during inference, and set\u03c3 to the standard deviation of the predictions. The second, is to train a function which predicts\u03c3 given x, in an attempt to capture any dependency between state and uncertainty. For example, it was observed that images captured at a greater distance from the object generally resulted in greater prediction uncertainty. To train this, we took the validation subset of D used for early stopping of f (I), computed predicted poses for all images, and calculated their errors. Then we trained a small neural network to predict the errors from the predicted poses. The third, is to use f (I)'s validation error as a constant, prior uncertainty for all predictions. We refer to these methods in later experiments as Dropout, Predicted, and Prior, respectively.\n\n\nE. Task Execution\n\nFor testing, the robot executes the approach trajectory by moving along a linear path towards the latest estimate of the bottleneck pose. This pose T RB is calculated by:\nT RB = T RE T EB ,(5)\nwhere T RE is calculated using forward kinematics, and T EB is the prediction from f (I). When the end-effector reaches the bottleneck's height, g(I) is used to calculate a final estimate of T RB . The robot then moves to this pose, and executes the interaction trajectory by replaying the demonstration's end-effector velocities. Algorithm 2 summarises the overall testing procedure. \n\n\nIV. EXPERIMENTS\n\n\nA. Implementation Details\n\nWe used a basic convolutional neural network for both f (I) and g(I), consisting of four convolutional layers with ReLU and max-pool downsampling, followed by four fullyconnected layers with ReLU and Dropout. A mean squarederror loss function was used, with coefficients to balance translation and rotation determined using [38], which was calculated using a small dataset and then fixed for all experiments. Early stopping was done with a 20% validation subset. Real-world experiments used a 7-DOF Sawyer robot operating at 30 Hz, with a camera capturing 64-by-64 RGB images. We found that as the camera is very close to the object by the end of the approach trajectory, higher resolution images were not necessary. The task space was a 40-by-40 cm region on a table below the robot. During training, the object was placed at the centre of the task space, and the robot's approach trajectories began at 50 cm above the table, with initial horizontal positions uniformly sampled from the task space, and initial orientations uniformly sampled over a range of 90 degrees. During each test, the object was placed randomly in the task space, and over a range of 90 degrees relative to its pose during training, and the robot's approach trajectories began at 50 cm above the centre of the task space. For the trajectories used to train f (I), we uniformly sampled a target pose over ranges of 5 cm and 5 \u2022 relative to the bottleneck, with ranges of 2 cm and 20 \u2022 for g(I).\n\n\nB. Target Reaching\n\nWe first evaluate a range of methods for bottleneck pose estimation using f (I), including the sequential methods described in Section III-D. We created a target reaching task to test the approach trajectory, where the end-effector was first manually moved to a random point above an object to define an artificial bottleneck, with the task then being to estimate that pose and move to it through the approach trajectory. We performed experiments with 7 random objects (different to those for the later imitation learning experiments), using 50 trajectories on each to collect D. We tested each object in 5 different poses, each time setting the end-effector by eye as close to the artificial bottleneck as possible, to record the ground-truth bottleneck pose.\n\nMethods. We implemented 11 different methods for estimating the bottleneck pose. Oracle uses the ground-truth bottleneck to reveal the unavoidable error in the position controller. First Image deploys f (I) only for the first image in the trajectory, and uses that same estimate throughout the trajectory. Best Image continually deploys f (I) for all images, and uses the estimate with the lowest uncertainty from all images captured so far (using Dropout or Predicted). Visual Servoing continually deploys f (I) for all images, and uses the estimate only for the current image without performing sequential estimation. Batch and Filtering are described in Section III-D, and perform sequential estimation by fusing predictions from multiple images along the trajectory (using Dropout, Predicted, or Prior).  Results. Table I shows the results, which reveal three insights. Firstly, the best methods for fusing multiple predictions typically outperformed basic visual servoing. This demonstrates the effectiveness of classical, explicit state estimation over implicit state estimation via deep learning, and supports our proposal to model imitation learning as a state estimation problem. Secondly, methods which assume a constant uncertainty for all predictions (Prior) typically outperformed those which estimate uncertainty dynamically (Dropout and Predicted). Since the formulations in Section III-D assume Gaussian uncertainties, we can conclude that this assumption introduces a harmful bias since estimating Gaussian uncertainties is challenging with deep learning. Thirdly, filtering methods performed at least as well as batch methods despite discarding past data, which could be explained by the bias towards more recent images captured closer to the object, which are typically more discriminative.\n\n\nC. Imitation Learning\n\nThe target reaching experiments evaluated the approach trajectory, and we now evaluate our full framework for imitation learning, by introducing human demonstrations and the interaction trajectory. We used 50 trajectories for collecting each of D and E. Videos are available at: www.robot-learning.uk/coarse-to-fine-imitation-learning.\n\nTasks. We chose 8 everyday tasks which reflect a range of different challenges, as shown in Figure 3. Bottle requires a lid to be placed onto a bottle. Plate requires a plate to be inserted into a specific slot in a rack. Screwdriver requires a toy screwdriver to be inserted into a screw in a toy aeroplane, and then twisted by 90 degrees. Lid requires a lid on a box to be lifted upright. Knife requires a wooden knife to be inserted into a knife rack. Hammer requires a toy nail to be knocked into a hole using a toy hammer. Scoop requires a small bag to be scooped up. Plug requires an electrical plug to be inserted into a socket. For each task, specific success criteria were defined based on the object configuration at the end of the robot's attempt, which were then judged by eye.   Methods. We tested two different methods for the coarse trajectory, each with and without the last-inch correction. First, we evaluated using only the current image to estimate the bottleneck pose at each time step, denoted Visual Servoing. Second, we evaluated using Filtering with Prior uncertainty, since on average this performed at least as well as any other method in the target reaching experiments. We also implemented a method based on residual reinforcement learning following [19], using the negative 1 distance between the current image and the demonstration's final image as a dense reward, and with the baseline policy trained with behavioural cloning on the single demonstration. However, we found that even with hyper-parameter tuning, this was not able to solve any of the tasks. Whilst this method has previously performed well for insertion tasks using multiple demonstrations [19], we found that a single demonstration was not sufficient to constrain exploration, and the 1 image distance was often not a meaningful signal for our tasks. We did not compare to behavioural cloning, as this requires more than one demonstration, nor meta-learning, as current methods require prior training on similar tasks.\n\nResults. Table II shows the results for our imitation learning experiments, with 20 object poses tested per task. We see that for the full implementation, using both filtering and the last-inch correction, we are able to learn a range of everyday tasks with encouraging success rates, given that only a single demonstration is required. Three of the tasks were successfully executed in all 20 of the tested object poses. As with the target reaching experiments, sequential state estimation proves to be superior to basic visual servoing. We also see the benefit of the last-inch correction, which significantly increased success rates. Note that since the bottleneck pose estimation is self-supervised, our method can automatically improve in performance with more data collection, without requiring further human demonstrations.\n\nWe also notice a large variation in performance across the different tasks. Task difficulty can be defined by two orthogonal properties: first, the required precision to complete the task, and second, the suitability of the object's shape and texture for 2D image-based pose estimation. For example, the Knife task was very challenging because not only is the slot in the rack very thin, but the rack itself lacks texture, and appears in an image as just a region of relatively uniform colour, making orientation prediction particularly difficult. The Plug task was also challenging, since this is contactrich and requires significant force to overcome resistance. However, our full method solved this in 9 out of 20 poses of the socket, which is surprisingly high considering only a simple velocity controller is used for contact-rich insertion. However, had the socket not been free to compliantly slide along the table during interaction, the success rates would likely have been lower. Another important observation is how well our method performed on the Hammer and Scoop tasks. These both require accurate motion at high velocities during object interaction, and are thus well suited to our method of directly replicating the demonstration's velocities. This interaction would be challenging to achieve via explicit policy learning, such as reinforcement learning.\n\n\nV. CONCLUSIONS\n\nWe have proposed and evaluated Coarse-to-Fine Imitation Learning, a framework which models a robot manipulation task as a coarse, approach trajectory, followed by a fine, interaction trajectory, and models imitation learning as a state estimation problem. We have shown that this can learn a range of novel, real-world, everyday tasks, from a single human demonstration, whereas existing methods typically require either multiple demonstrations, repeated environment resetting, or prior task knowledge. A key component is the ability to simply replay the original demonstration's velocities, enabling convenient acquisition of complex motion, without needing to explicitly learn a policy. Furthermore, the controller is analytical, stable, and interpretable, which typically cannot be said of many visual imitation learning methods today based on end-to-end policy learning. Future work includes improving the bottleneck pose estimator with 3D computer vision, introducing closed-loop control during object interaction, and extending to multi-stage tasks.\n\nAlgorithm 1 :\n1Training algorithm. Collect one demo, record initial pose and velocities U Set bottleneck to initial pose with vertical orientation Set R to rotation between bottleneck and initial pose Set h to height of bottleneck Initialise empty dataset D for each trajectory do Move robot to random starting pose Sample random target pose near to bottleneck while robot is above h do Capture image I and pose TEB Store (I, TEB) in D Move robot towards target pose along linear path Collect dataset E Train f (I) on D Train g(I) on E\n\nFig. 3 :\n3The set of 8 everyday tasks we used for real-world imitation learning experiments.\n\nAlgorithm 2 :\n2Testing algorithm.Retrieve U, R, h, f (I), and g(I) from Algorithm 1 \nwhile robot is above h do \nCapture image I \nPredict bottleneck using f (I) \nUpdate bottleneck with sequential state estimation \nMove robot towards bottleneck along linear path \n\nPredict bottleneck using g(I) \nMove robot to bottleneck \nRotate robot by R \nExecute velocities U \n\n\n\nTABLE I :\nITarget reaching results for the approach trajectory, \nfor different bottleneck pose estimation methods. Mean, min \nand max errors were calculated across 5 poses per object, \nthen averaged across all 7 objects. Positions are in mm, \norientations in degrees. The three best results per column \n(excluding Oracle) are in bold, with the best also underlined. \n\n\n\nTABLE II :\nIIReal-world imitation learning results, showing % success rate over 20 object poses for each task. The best result per column is in bold.\n\nDeep imitation learning for complex manipulation tasks from virtual reality teleoperation. T Zhang, Z Mccarthy, O Jowl, D Lee, X Chen, K Goldberg, P Abbeel, IEEE International Conference on Robotics and Automation (ICRA). T. Zhang, Z. McCarthy, O. Jowl, D. Lee, X. Chen, K. Goldberg, and P. Abbeel, \"Deep imitation learning for complex manipulation tasks from virtual reality teleoperation,\" in IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nLeveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. M Vecerik, T Hester, J Scholz, F Wang, O Pietquin, B Piot, N Heess, T Roth\u00f6rl, T Lampe, M Riedmiller, arXiv:1707.08817arXiv preprintM. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Roth\u00f6rl, T. Lampe, and M. Riedmiller, \"Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards,\" arXiv preprint arXiv:1707.08817, 2017.\n\nOne-shot visual imitation learning via meta-learning. C Finn, T Yu, T Zhang, P Abbeel, S Levine, Conference on Robot Learning (CoRL). C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, \"One-shot visual imitation learning via meta-learning,\" in Conference on Robot Learning (CoRL), 2017.\n\nLearning and generalization of motor skills by learning from demonstration. P Pastor, H Hoffmann, T Asfour, S Schaal, IEEE International Conference on Robotics and Automation (ICRA). P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, \"Learning and generalization of motor skills by learning from demonstration,\" in IEEE International Conference on Robotics and Automation (ICRA), 2009.\n\nLearning manipulation skills from a single demonstration. P Englert, M Toussaint, The International Journal of Robotics Research. P. Englert and M. Toussaint, \"Learning manipulation skills from a single demonstration,\" The International Journal of Robotics Research, 2018.\n\nDROID: Minimizing the reality gap using single-shot human demonstration. Y Y Tsai, H Xu, Z Ding, C Zhang, E Johns, B Huang, IEEE Robotics and Automation Letters. 2021RA-LY. Y. Tsai, H. Xu, Z. Ding, C. Zhang, E. Johns, and B. Huang, \"DROID: Minimizing the reality gap using single-shot human demon- stration,\" IEEE Robotics and Automation Letters (RA-L), 2021.\n\nVisionbased multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. R Rahmatizadeh, P Abolghasemi, L B\u00f6l\u00f6ni, S Levine, IEEE International Conference on Robotics and Automation (ICRA). R. Rahmatizadeh, P. Abolghasemi, L. B\u00f6l\u00f6ni, and S. Levine, \"Vision- based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration,\" in IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nSelf-supervised correspondence in visuomotor policy learning. P Florence, L Manuelli, R Tedrake, IEEE Robotics and Automation Letters. RA-LP. Florence, L. Manuelli, and R. Tedrake, \"Self-supervised correspon- dence in visuomotor policy learning,\" IEEE Robotics and Automation Letters (RA-L), 2019.\n\nVisual imitation made easy. S Young, D Gandhi, S Tulsiani, A Gupta, P Abbeel, L Pinto, Conference on Robot Learning (CoRL). 2020S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto, \"Visual imitation made easy,\" in Conference on Robot Learning (CoRL), 2020.\n\nTransferring end-to-end visuomotor control from simulation to real world for a multi-stage task. S James, A J Davison, E Johns, Conference on Robot Learning (CoRL). S. James, A. J. Davison, and E. Johns, \"Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task,\" in Conference on Robot Learning (CoRL), 2017.\n\nSAFARI: Safe and active robot imitation learning with imagination. N , Di Palo, E Johns, arXiv:2011.09586arXiv preprintN. Di Palo and E. Johns, \"SAFARI: Safe and active robot imitation learning with imagination,\" arXiv preprint arXiv:2011.09586, 2020.\n\nIntermittent visual servoing: Efficiently learning policies robust to instrument changes for highprecision surgical manipulation. S Paradis, M Hwang, B Thananjeyan, J Ichnowski, D Seita, D Fer, T Low, J E Gonzalez, K Goldberg, arXiv:2011.06163arXiv preprintS. Paradis, M. Hwang, B. Thananjeyan, J. Ichnowski, D. Seita, D. Fer, T. Low, J. E. Gonzalez, and K. Goldberg, \"Intermittent visual servoing: Efficiently learning policies robust to instrument changes for high- precision surgical manipulation,\" arXiv preprint arXiv:2011.06163, 2020.\n\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations. A Rajeswaran, V Kumar, A Gupta, G Vezzani, J Schulman, E Todorov, S Levine, Robotics: Science and Systems (RSS). A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine, \"Learning complex dexterous manipulation with deep reinforcement learning and demonstrations,\" in Robotics: Science and Systems (RSS), 2018.\n\nResidual reinforcement learning for robot control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, IEEE International Conference on Robotics and Automation (ICRA). T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine, \"Residual reinforcement learning for robot control,\" in IEEE International Conference on Robotics and Automation (ICRA), 2019.\n\nResidual policy learning. T Silver, K Allen, J Tenenbaum, L Kaelbling, arXiv:1812.06298arXiv preprintT. Silver, K. Allen, J. Tenenbaum, and L. Kaelbling, \"Residual policy learning,\" arXiv preprint arXiv:1812.06298, 2018.\n\nOvercoming exploration in reinforcement learning with demonstrations. A Nair, B Mcgrew, M Andrychowicz, W Zaremba, P Abbeel, IEEE International Conference on Robotics and Automation (ICRA). A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, \"Overcoming exploration in reinforcement learning with demonstra- tions,\" in IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nSafety augmented value estimation from demonstrations (SAVED): Safe deep modelbased RL for sparse cost robotic tasks. B Thananjeyan, A Balakrishna, U Rosolia, F Li, R Mcallister, J E Gonzalez, S Levine, F Borrelli, K Goldberg, IEEE Robotics and Automation Letters. 2020RA-LB. Thananjeyan, A. Balakrishna, U. Rosolia, F. Li, R. McAllister, J. E. Gonzalez, S. Levine, F. Borrelli, and K. Goldberg, \"Safety augmented value estimation from demonstrations (SAVED): Safe deep model- based RL for sparse cost robotic tasks,\" IEEE Robotics and Automation Letters (RA-L), 2020.\n\nConstrained space optimization and reinforcement learning for complex tasks. Y.-Y Tsai, B Xiao, E Johns, G.-Z Yang, IEEE Robotics and Automation Letters. 2020RA-LY.-Y. Tsai, B. Xiao, E. Johns, and G.-Z. Yang, \"Constrained space optimization and reinforcement learning for complex tasks,\" IEEE Robotics and Automation Letters (RA-L), 2020.\n\nDeep reinforcement learning for industrial insertion tasks with visual inputs and natural rewards. G Schoettler, A Nair, J Luo, S Bahl, J A Ojea, E Solowjow, S Levine, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020G. Schoettler, A. Nair, J. Luo, S. Bahl, J. A. Ojea, E. Solowjow, and S. Levine, \"Deep reinforcement learning for industrial insertion tasks with visual inputs and natural rewards,\" in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\n\nGuided cost learning: Deep inverse optimal control via policy optimization. C Finn, S Levine, P Abbeel, International Conference on Machine Learning (ICML). C. Finn, S. Levine, and P. Abbeel, \"Guided cost learning: Deep inverse optimal control via policy optimization,\" in International Conference on Machine Learning (ICML), 2016.\n\nEndto-end robotic reinforcement learning without reward engineering. A Singh, L Yang, K Hartikainen, C Finn, S Levine, Robotics: Science and Systems (RSS). A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine, \"End- to-end robotic reinforcement learning without reward engineering,\" in Robotics: Science and Systems (RSS), 2019.\n\nTime-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, G Brain, IEEE International Conference on Robotics and Automation (ICRA). P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain, \"Time-contrastive networks: Self-supervised learning from video,\" in IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nGenerative adversarial imitation learning. J Ho, S Ermon, Advances in Neural Information Processing Systems (NeurIPS). J. Ho and S. Ermon, \"Generative adversarial imitation learning,\" in Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nReinforcement and imitation learning for diverse visuomotor skills. Y Zhu, Z Wang, J Merel, A Rusu, T Erez, S Cabi, S Tunyasuvunakool, J Kram\u00e1r, R Hadsell, N Freitas, N Heess, Robotics: Science and Systems (RSS). Y. Zhu, Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tun- yasuvunakool, J. Kram\u00e1r, R. Hadsell, N. Freitas, and N. Heess, \"Reinforcement and imitation learning for diverse visuomotor skills,\" in Robotics: Science and Systems (RSS), 2018.\n\nGenerative predecessor models for sample-efficient imitation learning. Y Schroecker, M Vecerik, J Scholz, International Conference on Learning Representations (ICLR). Y. Schroecker, M. Vecerik, and J. Scholz, \"Generative predecessor models for sample-efficient imitation learning,\" in International Con- ference on Learning Representations (ICLR), 2019.\n\nCombining self-supervised learning and imitation for vision-based rope manipulation. A Nair, D Chen, P Agrawal, P Isola, P Abbeel, J Malik, S Levine, IEEE International Conference on Robotics and Automation (ICRA). A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine, \"Combining self-supervised learning and imitation for vision-based rope manipulation,\" in IEEE International Conference on Robotics and Automation (ICRA), 2017.\n\nGraphstructured visual imitation. M Sieb, Z Xian, A Huang, O Kroemer, K Fragkiadaki, Conference on Robot Learning (CoRL). 2020M. Sieb, Z. Xian, A. Huang, O. Kroemer, and K. Fragkiadaki, \"Graph- structured visual imitation,\" in Conference on Robot Learning (CoRL), 2020.\n\nOne-shot imitation learning. Y Duan, M Andrychowicz, B C Stadie, J Ho, J Schneider, I Sutskever, P Abbeel, W Zaremba, Advances in Neural Information Processing Systems (NeurIPS). Y. Duan, M. Andrychowicz, B. C. Stadie, J. Ho, J. Schneider, I. Sutskever, P. Abbeel, and W. Zaremba, \"One-shot imitation learning,\" in Advances in Neural Information Processing Systems (NeurIPS), 2017.\n\nTask-embedded control networks for few-shot imitation learning. S James, M Bloesch, A J Davison, Conference on Robot Learning (CoRL). S. James, M. Bloesch, and A. J. Davison, \"Task-embedded control networks for few-shot imitation learning,\" in Conference on Robot Learning (CoRL), 2018.\n\nModel-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, International Conference on Machine Learning (ICML). C. Finn, P. Abbeel, and S. Levine, \"Model-agnostic meta-learning for fast adaptation of deep networks,\" in International Conference on Machine Learning (ICML), 2017.\n\nOne-shot imitation from observing humans via domain-adaptive meta-learning. T Yu, C Finn, A Xie, S Dasari, T Zhang, P Abbeel, S Levine, Robotics: Science and Systems (RSS). T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine, \"One-shot imitation from observing humans via domain-adaptive meta-learning,\" in Robotics: Science and Systems (RSS), 2018.\n\nSiamese convolutional neural network for sub-millimeter-accurate camera pose estimation and visual servoing. C Yu, Z Cai, H Pham, Q.-C Pham, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). C. Yu, Z. Cai, H. Pham, and Q.-C. Pham, \"Siamese convolutional neural network for sub-millimeter-accurate camera pose estimation and visual servoing,\" in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019.\n\nKOVIS: Keypoint-based visual servoing with zero-shot sim-to-real transfer for robotics manipulation. E Y Puang, K Peng Tee, W Jing, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020E. Y. Puang, K. Peng Tee, and W. Jing, \"KOVIS: Keypoint-based visual servoing with zero-shot sim-to-real transfer for robotics manip- ulation,\" in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\n\nFlowcontrol: Optical flow based visual servoing. M Argus, L Hermann, J Long, T Brox, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020M. Argus, L. Hermann, J. Long, and T. Brox, \"Flowcontrol: Optical flow based visual servoing,\" in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\n\nStatistical meta-analysis with applications. J Hartung, G Knapp, B K Sinha, John Wiley & SonsJ. Hartung, G. Knapp, and B. K. Sinha, Statistical meta-analysis with applications. John Wiley & Sons, 2011.\n\nProbabilistic robotics. S Thrun, Communications of the ACM. S. Thrun, \"Probabilistic robotics,\" Communications of the ACM, 2002.\n\nDropout as a Bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, International Conference on Machine Learning (ICML). Y. Gal and Z. Ghahramani, \"Dropout as a Bayesian approximation: Representing model uncertainty in deep learning,\" in International Conference on Machine Learning (ICML), 2016.\n\nPoseNet: A convolutional network for real-time 6-DOF camera relocalization. A Kendall, M Grimes, R Cipolla, IEEE International Conference on Computer Vision (ICCV). A. Kendall, M. Grimes, and R. Cipolla, \"PoseNet: A convolutional network for real-time 6-DOF camera relocalization,\" in IEEE Interna- tional Conference on Computer Vision (ICCV), 2015.\n", "annotations": {"author": "[{\"end\":98,\"start\":85}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":92}]", "author_first_name": "[{\"end\":91,\"start\":85}]", "author_affiliation": null, "title": "[{\"end\":82,\"start\":1},{\"end\":180,\"start\":99}]", "venue": null, "abstract": "[{\"end\":1438,\"start\":182}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1880,\"start\":1877},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2035,\"start\":2032},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2212,\"start\":2209},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2374,\"start\":2371},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3863,\"start\":3860},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3887,\"start\":3884},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3892,\"start\":3889},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5879,\"start\":5876},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5884,\"start\":5881},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5889,\"start\":5886},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6430,\"start\":6427},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6435,\"start\":6432},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6440,\"start\":6437},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6445,\"start\":6442},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6451,\"start\":6447},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6457,\"start\":6453},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6540,\"start\":6537},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6564,\"start\":6561},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6613,\"start\":6609},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7046,\"start\":7042},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7079,\"start\":7075},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7085,\"start\":7081},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7173,\"start\":7169},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7216,\"start\":7212},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7222,\"start\":7218},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7280,\"start\":7276},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7317,\"start\":7313},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7339,\"start\":7335},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7369,\"start\":7365},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7440,\"start\":7436},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7446,\"start\":7442},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7588,\"start\":7584},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7594,\"start\":7590},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8025,\"start\":8021},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8082,\"start\":8078},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8162,\"start\":8158},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8206,\"start\":8203},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8252,\"start\":8248},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8517,\"start\":8513},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8523,\"start\":8519},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8743,\"start\":8739},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18266,\"start\":18262},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18903,\"start\":18899},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19384,\"start\":19380},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21167,\"start\":21163},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26547,\"start\":26543},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26956,\"start\":26952}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31095,\"start\":30559},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31189,\"start\":31096},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31553,\"start\":31190},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31923,\"start\":31554},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32074,\"start\":31924}]", "paragraph": "[{\"end\":2682,\"start\":1457},{\"end\":3628,\"start\":2684},{\"end\":5038,\"start\":3630},{\"end\":5733,\"start\":5040},{\"end\":6214,\"start\":5754},{\"end\":7825,\"start\":6216},{\"end\":8464,\"start\":7827},{\"end\":9008,\"start\":8466},{\"end\":9577,\"start\":9047},{\"end\":10140,\"start\":9612},{\"end\":10372,\"start\":10142},{\"end\":10967,\"start\":10374},{\"end\":11807,\"start\":11075},{\"end\":12282,\"start\":11809},{\"end\":13226,\"start\":12284},{\"end\":13432,\"start\":13258},{\"end\":14814,\"start\":13455},{\"end\":16246,\"start\":14816},{\"end\":17087,\"start\":16248},{\"end\":18084,\"start\":17122},{\"end\":18376,\"start\":18086},{\"end\":18426,\"start\":18423},{\"end\":18653,\"start\":18428},{\"end\":20191,\"start\":18765},{\"end\":20383,\"start\":20213},{\"end\":20791,\"start\":20406},{\"end\":22307,\"start\":20839},{\"end\":23090,\"start\":22330},{\"end\":24901,\"start\":23092},{\"end\":25262,\"start\":24927},{\"end\":27281,\"start\":25264},{\"end\":28112,\"start\":27283},{\"end\":29484,\"start\":28114},{\"end\":30558,\"start\":29503}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11074,\"start\":10968},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13454,\"start\":13433},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18422,\"start\":18377},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18724,\"start\":18654},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18764,\"start\":18724},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20405,\"start\":20384}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23917,\"start\":23910},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27300,\"start\":27292}]", "section_header": "[{\"end\":1455,\"start\":1440},{\"end\":5752,\"start\":5736},{\"end\":9022,\"start\":9011},{\"end\":9045,\"start\":9025},{\"end\":9610,\"start\":9580},{\"end\":13256,\"start\":13229},{\"end\":17120,\"start\":17090},{\"end\":20211,\"start\":20194},{\"end\":20809,\"start\":20794},{\"end\":20837,\"start\":20812},{\"end\":22328,\"start\":22310},{\"end\":24925,\"start\":24904},{\"end\":29501,\"start\":29487},{\"end\":30573,\"start\":30560},{\"end\":31105,\"start\":31097},{\"end\":31204,\"start\":31191},{\"end\":31564,\"start\":31555},{\"end\":31935,\"start\":31925}]", "table": "[{\"end\":31553,\"start\":31224},{\"end\":31923,\"start\":31566}]", "figure_caption": "[{\"end\":31095,\"start\":30575},{\"end\":31189,\"start\":31107},{\"end\":31224,\"start\":31206},{\"end\":32074,\"start\":31938}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2772,\"start\":2764},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2919,\"start\":2913},{\"end\":9195,\"start\":9187},{\"end\":14219,\"start\":14211},{\"end\":16711,\"start\":16703},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25364,\"start\":25356}]", "bib_author_first_name": "[{\"end\":32168,\"start\":32167},{\"end\":32177,\"start\":32176},{\"end\":32189,\"start\":32188},{\"end\":32197,\"start\":32196},{\"end\":32204,\"start\":32203},{\"end\":32212,\"start\":32211},{\"end\":32224,\"start\":32223},{\"end\":32644,\"start\":32643},{\"end\":32655,\"start\":32654},{\"end\":32665,\"start\":32664},{\"end\":32675,\"start\":32674},{\"end\":32683,\"start\":32682},{\"end\":32695,\"start\":32694},{\"end\":32703,\"start\":32702},{\"end\":32712,\"start\":32711},{\"end\":32723,\"start\":32722},{\"end\":32732,\"start\":32731},{\"end\":33088,\"start\":33087},{\"end\":33096,\"start\":33095},{\"end\":33102,\"start\":33101},{\"end\":33111,\"start\":33110},{\"end\":33121,\"start\":33120},{\"end\":33399,\"start\":33398},{\"end\":33409,\"start\":33408},{\"end\":33421,\"start\":33420},{\"end\":33431,\"start\":33430},{\"end\":33767,\"start\":33766},{\"end\":33778,\"start\":33777},{\"end\":34056,\"start\":34055},{\"end\":34058,\"start\":34057},{\"end\":34066,\"start\":34065},{\"end\":34072,\"start\":34071},{\"end\":34080,\"start\":34079},{\"end\":34089,\"start\":34088},{\"end\":34098,\"start\":34097},{\"end\":34449,\"start\":34448},{\"end\":34465,\"start\":34464},{\"end\":34480,\"start\":34479},{\"end\":34490,\"start\":34489},{\"end\":34870,\"start\":34869},{\"end\":34882,\"start\":34881},{\"end\":34894,\"start\":34893},{\"end\":35135,\"start\":35134},{\"end\":35144,\"start\":35143},{\"end\":35154,\"start\":35153},{\"end\":35166,\"start\":35165},{\"end\":35175,\"start\":35174},{\"end\":35185,\"start\":35184},{\"end\":35478,\"start\":35477},{\"end\":35487,\"start\":35486},{\"end\":35489,\"start\":35488},{\"end\":35500,\"start\":35499},{\"end\":35798,\"start\":35797},{\"end\":35803,\"start\":35801},{\"end\":35811,\"start\":35810},{\"end\":36114,\"start\":36113},{\"end\":36125,\"start\":36124},{\"end\":36134,\"start\":36133},{\"end\":36149,\"start\":36148},{\"end\":36162,\"start\":36161},{\"end\":36171,\"start\":36170},{\"end\":36178,\"start\":36177},{\"end\":36185,\"start\":36184},{\"end\":36187,\"start\":36186},{\"end\":36199,\"start\":36198},{\"end\":36619,\"start\":36618},{\"end\":36633,\"start\":36632},{\"end\":36642,\"start\":36641},{\"end\":36651,\"start\":36650},{\"end\":36662,\"start\":36661},{\"end\":36674,\"start\":36673},{\"end\":36685,\"start\":36684},{\"end\":37012,\"start\":37011},{\"end\":37025,\"start\":37024},{\"end\":37033,\"start\":37032},{\"end\":37041,\"start\":37040},{\"end\":37048,\"start\":37047},{\"end\":37057,\"start\":37056},{\"end\":37068,\"start\":37067},{\"end\":37070,\"start\":37069},{\"end\":37078,\"start\":37077},{\"end\":37090,\"start\":37089},{\"end\":37421,\"start\":37420},{\"end\":37431,\"start\":37430},{\"end\":37440,\"start\":37439},{\"end\":37453,\"start\":37452},{\"end\":37687,\"start\":37686},{\"end\":37695,\"start\":37694},{\"end\":37705,\"start\":37704},{\"end\":37721,\"start\":37720},{\"end\":37732,\"start\":37731},{\"end\":38138,\"start\":38137},{\"end\":38153,\"start\":38152},{\"end\":38168,\"start\":38167},{\"end\":38179,\"start\":38178},{\"end\":38185,\"start\":38184},{\"end\":38199,\"start\":38198},{\"end\":38201,\"start\":38200},{\"end\":38213,\"start\":38212},{\"end\":38223,\"start\":38222},{\"end\":38235,\"start\":38234},{\"end\":38670,\"start\":38666},{\"end\":38678,\"start\":38677},{\"end\":38686,\"start\":38685},{\"end\":38698,\"start\":38694},{\"end\":39029,\"start\":39028},{\"end\":39043,\"start\":39042},{\"end\":39051,\"start\":39050},{\"end\":39058,\"start\":39057},{\"end\":39066,\"start\":39065},{\"end\":39068,\"start\":39067},{\"end\":39076,\"start\":39075},{\"end\":39088,\"start\":39087},{\"end\":39522,\"start\":39521},{\"end\":39530,\"start\":39529},{\"end\":39540,\"start\":39539},{\"end\":39848,\"start\":39847},{\"end\":39857,\"start\":39856},{\"end\":39865,\"start\":39864},{\"end\":39880,\"start\":39879},{\"end\":39888,\"start\":39887},{\"end\":40178,\"start\":40177},{\"end\":40190,\"start\":40189},{\"end\":40199,\"start\":40198},{\"end\":40211,\"start\":40210},{\"end\":40218,\"start\":40217},{\"end\":40226,\"start\":40225},{\"end\":40236,\"start\":40235},{\"end\":40246,\"start\":40245},{\"end\":40593,\"start\":40592},{\"end\":40599,\"start\":40598},{\"end\":40873,\"start\":40872},{\"end\":40880,\"start\":40879},{\"end\":40888,\"start\":40887},{\"end\":40897,\"start\":40896},{\"end\":40905,\"start\":40904},{\"end\":40913,\"start\":40912},{\"end\":40921,\"start\":40920},{\"end\":40940,\"start\":40939},{\"end\":40950,\"start\":40949},{\"end\":40961,\"start\":40960},{\"end\":40972,\"start\":40971},{\"end\":41331,\"start\":41330},{\"end\":41345,\"start\":41344},{\"end\":41356,\"start\":41355},{\"end\":41700,\"start\":41699},{\"end\":41708,\"start\":41707},{\"end\":41716,\"start\":41715},{\"end\":41727,\"start\":41726},{\"end\":41736,\"start\":41735},{\"end\":41746,\"start\":41745},{\"end\":41755,\"start\":41754},{\"end\":42102,\"start\":42101},{\"end\":42110,\"start\":42109},{\"end\":42118,\"start\":42117},{\"end\":42127,\"start\":42126},{\"end\":42138,\"start\":42137},{\"end\":42368,\"start\":42367},{\"end\":42376,\"start\":42375},{\"end\":42392,\"start\":42391},{\"end\":42394,\"start\":42393},{\"end\":42404,\"start\":42403},{\"end\":42410,\"start\":42409},{\"end\":42423,\"start\":42422},{\"end\":42436,\"start\":42435},{\"end\":42446,\"start\":42445},{\"end\":42786,\"start\":42785},{\"end\":42795,\"start\":42794},{\"end\":42806,\"start\":42805},{\"end\":42808,\"start\":42807},{\"end\":43077,\"start\":43076},{\"end\":43085,\"start\":43084},{\"end\":43095,\"start\":43094},{\"end\":43401,\"start\":43400},{\"end\":43407,\"start\":43406},{\"end\":43415,\"start\":43414},{\"end\":43422,\"start\":43421},{\"end\":43432,\"start\":43431},{\"end\":43441,\"start\":43440},{\"end\":43451,\"start\":43450},{\"end\":43803,\"start\":43802},{\"end\":43809,\"start\":43808},{\"end\":43816,\"start\":43815},{\"end\":43827,\"start\":43823},{\"end\":44249,\"start\":44248},{\"end\":44251,\"start\":44250},{\"end\":44260,\"start\":44259},{\"end\":44272,\"start\":44271},{\"end\":44639,\"start\":44638},{\"end\":44648,\"start\":44647},{\"end\":44659,\"start\":44658},{\"end\":44667,\"start\":44666},{\"end\":44981,\"start\":44980},{\"end\":44992,\"start\":44991},{\"end\":45001,\"start\":45000},{\"end\":45003,\"start\":45002},{\"end\":45163,\"start\":45162},{\"end\":45355,\"start\":45354},{\"end\":45362,\"start\":45361},{\"end\":45682,\"start\":45681},{\"end\":45693,\"start\":45692},{\"end\":45703,\"start\":45702}]", "bib_author_last_name": "[{\"end\":32174,\"start\":32169},{\"end\":32186,\"start\":32178},{\"end\":32194,\"start\":32190},{\"end\":32201,\"start\":32198},{\"end\":32209,\"start\":32205},{\"end\":32221,\"start\":32213},{\"end\":32231,\"start\":32225},{\"end\":32652,\"start\":32645},{\"end\":32662,\"start\":32656},{\"end\":32672,\"start\":32666},{\"end\":32680,\"start\":32676},{\"end\":32692,\"start\":32684},{\"end\":32700,\"start\":32696},{\"end\":32709,\"start\":32704},{\"end\":32720,\"start\":32713},{\"end\":32729,\"start\":32724},{\"end\":32743,\"start\":32733},{\"end\":33093,\"start\":33089},{\"end\":33099,\"start\":33097},{\"end\":33108,\"start\":33103},{\"end\":33118,\"start\":33112},{\"end\":33128,\"start\":33122},{\"end\":33406,\"start\":33400},{\"end\":33418,\"start\":33410},{\"end\":33428,\"start\":33422},{\"end\":33438,\"start\":33432},{\"end\":33775,\"start\":33768},{\"end\":33788,\"start\":33779},{\"end\":34063,\"start\":34059},{\"end\":34069,\"start\":34067},{\"end\":34077,\"start\":34073},{\"end\":34086,\"start\":34081},{\"end\":34095,\"start\":34090},{\"end\":34104,\"start\":34099},{\"end\":34462,\"start\":34450},{\"end\":34477,\"start\":34466},{\"end\":34487,\"start\":34481},{\"end\":34497,\"start\":34491},{\"end\":34879,\"start\":34871},{\"end\":34891,\"start\":34883},{\"end\":34902,\"start\":34895},{\"end\":35141,\"start\":35136},{\"end\":35151,\"start\":35145},{\"end\":35163,\"start\":35155},{\"end\":35172,\"start\":35167},{\"end\":35182,\"start\":35176},{\"end\":35191,\"start\":35186},{\"end\":35484,\"start\":35479},{\"end\":35497,\"start\":35490},{\"end\":35506,\"start\":35501},{\"end\":35808,\"start\":35804},{\"end\":35817,\"start\":35812},{\"end\":36122,\"start\":36115},{\"end\":36131,\"start\":36126},{\"end\":36146,\"start\":36135},{\"end\":36159,\"start\":36150},{\"end\":36168,\"start\":36163},{\"end\":36175,\"start\":36172},{\"end\":36182,\"start\":36179},{\"end\":36196,\"start\":36188},{\"end\":36208,\"start\":36200},{\"end\":36630,\"start\":36620},{\"end\":36639,\"start\":36634},{\"end\":36648,\"start\":36643},{\"end\":36659,\"start\":36652},{\"end\":36671,\"start\":36663},{\"end\":36682,\"start\":36675},{\"end\":36692,\"start\":36686},{\"end\":37022,\"start\":37013},{\"end\":37030,\"start\":37026},{\"end\":37038,\"start\":37034},{\"end\":37045,\"start\":37042},{\"end\":37054,\"start\":37049},{\"end\":37065,\"start\":37058},{\"end\":37075,\"start\":37071},{\"end\":37087,\"start\":37079},{\"end\":37097,\"start\":37091},{\"end\":37428,\"start\":37422},{\"end\":37437,\"start\":37432},{\"end\":37450,\"start\":37441},{\"end\":37463,\"start\":37454},{\"end\":37692,\"start\":37688},{\"end\":37702,\"start\":37696},{\"end\":37718,\"start\":37706},{\"end\":37729,\"start\":37722},{\"end\":37739,\"start\":37733},{\"end\":38150,\"start\":38139},{\"end\":38165,\"start\":38154},{\"end\":38176,\"start\":38169},{\"end\":38182,\"start\":38180},{\"end\":38196,\"start\":38186},{\"end\":38210,\"start\":38202},{\"end\":38220,\"start\":38214},{\"end\":38232,\"start\":38224},{\"end\":38244,\"start\":38236},{\"end\":38675,\"start\":38671},{\"end\":38683,\"start\":38679},{\"end\":38692,\"start\":38687},{\"end\":38703,\"start\":38699},{\"end\":39040,\"start\":39030},{\"end\":39048,\"start\":39044},{\"end\":39055,\"start\":39052},{\"end\":39063,\"start\":39059},{\"end\":39073,\"start\":39069},{\"end\":39085,\"start\":39077},{\"end\":39095,\"start\":39089},{\"end\":39527,\"start\":39523},{\"end\":39537,\"start\":39531},{\"end\":39547,\"start\":39541},{\"end\":39854,\"start\":39849},{\"end\":39862,\"start\":39858},{\"end\":39877,\"start\":39866},{\"end\":39885,\"start\":39881},{\"end\":39895,\"start\":39889},{\"end\":40187,\"start\":40179},{\"end\":40196,\"start\":40191},{\"end\":40208,\"start\":40200},{\"end\":40215,\"start\":40212},{\"end\":40223,\"start\":40219},{\"end\":40233,\"start\":40227},{\"end\":40243,\"start\":40237},{\"end\":40252,\"start\":40247},{\"end\":40596,\"start\":40594},{\"end\":40605,\"start\":40600},{\"end\":40877,\"start\":40874},{\"end\":40885,\"start\":40881},{\"end\":40894,\"start\":40889},{\"end\":40902,\"start\":40898},{\"end\":40910,\"start\":40906},{\"end\":40918,\"start\":40914},{\"end\":40937,\"start\":40922},{\"end\":40947,\"start\":40941},{\"end\":40958,\"start\":40951},{\"end\":40969,\"start\":40962},{\"end\":40978,\"start\":40973},{\"end\":41342,\"start\":41332},{\"end\":41353,\"start\":41346},{\"end\":41363,\"start\":41357},{\"end\":41705,\"start\":41701},{\"end\":41713,\"start\":41709},{\"end\":41724,\"start\":41717},{\"end\":41733,\"start\":41728},{\"end\":41743,\"start\":41737},{\"end\":41752,\"start\":41747},{\"end\":41762,\"start\":41756},{\"end\":42107,\"start\":42103},{\"end\":42115,\"start\":42111},{\"end\":42124,\"start\":42119},{\"end\":42135,\"start\":42128},{\"end\":42150,\"start\":42139},{\"end\":42373,\"start\":42369},{\"end\":42389,\"start\":42377},{\"end\":42401,\"start\":42395},{\"end\":42407,\"start\":42405},{\"end\":42420,\"start\":42411},{\"end\":42433,\"start\":42424},{\"end\":42443,\"start\":42437},{\"end\":42454,\"start\":42447},{\"end\":42792,\"start\":42787},{\"end\":42803,\"start\":42796},{\"end\":42816,\"start\":42809},{\"end\":43082,\"start\":43078},{\"end\":43092,\"start\":43086},{\"end\":43102,\"start\":43096},{\"end\":43404,\"start\":43402},{\"end\":43412,\"start\":43408},{\"end\":43419,\"start\":43416},{\"end\":43429,\"start\":43423},{\"end\":43438,\"start\":43433},{\"end\":43448,\"start\":43442},{\"end\":43458,\"start\":43452},{\"end\":43806,\"start\":43804},{\"end\":43813,\"start\":43810},{\"end\":43821,\"start\":43817},{\"end\":43832,\"start\":43828},{\"end\":44257,\"start\":44252},{\"end\":44269,\"start\":44261},{\"end\":44277,\"start\":44273},{\"end\":44645,\"start\":44640},{\"end\":44656,\"start\":44649},{\"end\":44664,\"start\":44660},{\"end\":44672,\"start\":44668},{\"end\":44989,\"start\":44982},{\"end\":44998,\"start\":44993},{\"end\":45009,\"start\":45004},{\"end\":45169,\"start\":45164},{\"end\":45359,\"start\":45356},{\"end\":45373,\"start\":45363},{\"end\":45690,\"start\":45683},{\"end\":45700,\"start\":45694},{\"end\":45711,\"start\":45704}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3720790},\"end\":32541,\"start\":32076},{\"attributes\":{\"doi\":\"arXiv:1707.08817\",\"id\":\"b1\"},\"end\":33031,\"start\":32543},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":22221787},\"end\":33320,\"start\":33033},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8011664},\"end\":33706,\"start\":33322},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3978028},\"end\":33980,\"start\":33708},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":231986200},\"end\":34341,\"start\":33982},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5043670},\"end\":34805,\"start\":34343},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202578098},\"end\":35104,\"start\":34807},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":221095826},\"end\":35378,\"start\":35106},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":915858},\"end\":35728,\"start\":35380},{\"attributes\":{\"doi\":\"arXiv:2011.09586\",\"id\":\"b10\"},\"end\":35981,\"start\":35730},{\"attributes\":{\"doi\":\"arXiv:2011.06163\",\"id\":\"b11\"},\"end\":36523,\"start\":35983},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4780901},\"end\":36958,\"start\":36525},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":54464184},\"end\":37392,\"start\":36960},{\"attributes\":{\"doi\":\"arXiv:1812.06298\",\"id\":\"b14\"},\"end\":37614,\"start\":37394},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3543784},\"end\":38017,\"start\":37616},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204008531},\"end\":38587,\"start\":38019},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":210932034},\"end\":38927,\"start\":38589},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":189762093},\"end\":39443,\"start\":38929},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8121626},\"end\":39776,\"start\":39445},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":119295213},\"end\":40111,\"start\":39778},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3997350},\"end\":40547,\"start\":40113},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16153365},\"end\":40802,\"start\":40549},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3582900},\"end\":41257,\"start\":40804},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":68071305},\"end\":41612,\"start\":41259},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17868859},\"end\":42065,\"start\":41614},{\"attributes\":{\"id\":\"b26\"},\"end\":42336,\"start\":42067},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8270841},\"end\":42719,\"start\":42338},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52942444},\"end\":43007,\"start\":42721},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6719686},\"end\":43322,\"start\":43009},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3618072},\"end\":43691,\"start\":43324},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":75136127},\"end\":44145,\"start\":43693},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220830726},\"end\":44587,\"start\":44147},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":220280145},\"end\":44933,\"start\":44589},{\"attributes\":{\"id\":\"b34\"},\"end\":45136,\"start\":44935},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14552983},\"end\":45266,\"start\":45138},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":160705},\"end\":45603,\"start\":45268},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12888763},\"end\":45954,\"start\":45605}]", "bib_title": "[{\"end\":32165,\"start\":32076},{\"end\":33085,\"start\":33033},{\"end\":33396,\"start\":33322},{\"end\":33764,\"start\":33708},{\"end\":34053,\"start\":33982},{\"end\":34446,\"start\":34343},{\"end\":34867,\"start\":34807},{\"end\":35132,\"start\":35106},{\"end\":35475,\"start\":35380},{\"end\":36616,\"start\":36525},{\"end\":37009,\"start\":36960},{\"end\":37684,\"start\":37616},{\"end\":38135,\"start\":38019},{\"end\":38664,\"start\":38589},{\"end\":39026,\"start\":38929},{\"end\":39519,\"start\":39445},{\"end\":39845,\"start\":39778},{\"end\":40175,\"start\":40113},{\"end\":40590,\"start\":40549},{\"end\":40870,\"start\":40804},{\"end\":41328,\"start\":41259},{\"end\":41697,\"start\":41614},{\"end\":42099,\"start\":42067},{\"end\":42365,\"start\":42338},{\"end\":42783,\"start\":42721},{\"end\":43074,\"start\":43009},{\"end\":43398,\"start\":43324},{\"end\":43800,\"start\":43693},{\"end\":44246,\"start\":44147},{\"end\":44636,\"start\":44589},{\"end\":45160,\"start\":45138},{\"end\":45352,\"start\":45268},{\"end\":45679,\"start\":45605}]", "bib_author": "[{\"end\":32176,\"start\":32167},{\"end\":32188,\"start\":32176},{\"end\":32196,\"start\":32188},{\"end\":32203,\"start\":32196},{\"end\":32211,\"start\":32203},{\"end\":32223,\"start\":32211},{\"end\":32233,\"start\":32223},{\"end\":32654,\"start\":32643},{\"end\":32664,\"start\":32654},{\"end\":32674,\"start\":32664},{\"end\":32682,\"start\":32674},{\"end\":32694,\"start\":32682},{\"end\":32702,\"start\":32694},{\"end\":32711,\"start\":32702},{\"end\":32722,\"start\":32711},{\"end\":32731,\"start\":32722},{\"end\":32745,\"start\":32731},{\"end\":33095,\"start\":33087},{\"end\":33101,\"start\":33095},{\"end\":33110,\"start\":33101},{\"end\":33120,\"start\":33110},{\"end\":33130,\"start\":33120},{\"end\":33408,\"start\":33398},{\"end\":33420,\"start\":33408},{\"end\":33430,\"start\":33420},{\"end\":33440,\"start\":33430},{\"end\":33777,\"start\":33766},{\"end\":33790,\"start\":33777},{\"end\":34065,\"start\":34055},{\"end\":34071,\"start\":34065},{\"end\":34079,\"start\":34071},{\"end\":34088,\"start\":34079},{\"end\":34097,\"start\":34088},{\"end\":34106,\"start\":34097},{\"end\":34464,\"start\":34448},{\"end\":34479,\"start\":34464},{\"end\":34489,\"start\":34479},{\"end\":34499,\"start\":34489},{\"end\":34881,\"start\":34869},{\"end\":34893,\"start\":34881},{\"end\":34904,\"start\":34893},{\"end\":35143,\"start\":35134},{\"end\":35153,\"start\":35143},{\"end\":35165,\"start\":35153},{\"end\":35174,\"start\":35165},{\"end\":35184,\"start\":35174},{\"end\":35193,\"start\":35184},{\"end\":35486,\"start\":35477},{\"end\":35499,\"start\":35486},{\"end\":35508,\"start\":35499},{\"end\":35801,\"start\":35797},{\"end\":35810,\"start\":35801},{\"end\":35819,\"start\":35810},{\"end\":36124,\"start\":36113},{\"end\":36133,\"start\":36124},{\"end\":36148,\"start\":36133},{\"end\":36161,\"start\":36148},{\"end\":36170,\"start\":36161},{\"end\":36177,\"start\":36170},{\"end\":36184,\"start\":36177},{\"end\":36198,\"start\":36184},{\"end\":36210,\"start\":36198},{\"end\":36632,\"start\":36618},{\"end\":36641,\"start\":36632},{\"end\":36650,\"start\":36641},{\"end\":36661,\"start\":36650},{\"end\":36673,\"start\":36661},{\"end\":36684,\"start\":36673},{\"end\":36694,\"start\":36684},{\"end\":37024,\"start\":37011},{\"end\":37032,\"start\":37024},{\"end\":37040,\"start\":37032},{\"end\":37047,\"start\":37040},{\"end\":37056,\"start\":37047},{\"end\":37067,\"start\":37056},{\"end\":37077,\"start\":37067},{\"end\":37089,\"start\":37077},{\"end\":37099,\"start\":37089},{\"end\":37430,\"start\":37420},{\"end\":37439,\"start\":37430},{\"end\":37452,\"start\":37439},{\"end\":37465,\"start\":37452},{\"end\":37694,\"start\":37686},{\"end\":37704,\"start\":37694},{\"end\":37720,\"start\":37704},{\"end\":37731,\"start\":37720},{\"end\":37741,\"start\":37731},{\"end\":38152,\"start\":38137},{\"end\":38167,\"start\":38152},{\"end\":38178,\"start\":38167},{\"end\":38184,\"start\":38178},{\"end\":38198,\"start\":38184},{\"end\":38212,\"start\":38198},{\"end\":38222,\"start\":38212},{\"end\":38234,\"start\":38222},{\"end\":38246,\"start\":38234},{\"end\":38677,\"start\":38666},{\"end\":38685,\"start\":38677},{\"end\":38694,\"start\":38685},{\"end\":38705,\"start\":38694},{\"end\":39042,\"start\":39028},{\"end\":39050,\"start\":39042},{\"end\":39057,\"start\":39050},{\"end\":39065,\"start\":39057},{\"end\":39075,\"start\":39065},{\"end\":39087,\"start\":39075},{\"end\":39097,\"start\":39087},{\"end\":39529,\"start\":39521},{\"end\":39539,\"start\":39529},{\"end\":39549,\"start\":39539},{\"end\":39856,\"start\":39847},{\"end\":39864,\"start\":39856},{\"end\":39879,\"start\":39864},{\"end\":39887,\"start\":39879},{\"end\":39897,\"start\":39887},{\"end\":40189,\"start\":40177},{\"end\":40198,\"start\":40189},{\"end\":40210,\"start\":40198},{\"end\":40217,\"start\":40210},{\"end\":40225,\"start\":40217},{\"end\":40235,\"start\":40225},{\"end\":40245,\"start\":40235},{\"end\":40254,\"start\":40245},{\"end\":40598,\"start\":40592},{\"end\":40607,\"start\":40598},{\"end\":40879,\"start\":40872},{\"end\":40887,\"start\":40879},{\"end\":40896,\"start\":40887},{\"end\":40904,\"start\":40896},{\"end\":40912,\"start\":40904},{\"end\":40920,\"start\":40912},{\"end\":40939,\"start\":40920},{\"end\":40949,\"start\":40939},{\"end\":40960,\"start\":40949},{\"end\":40971,\"start\":40960},{\"end\":40980,\"start\":40971},{\"end\":41344,\"start\":41330},{\"end\":41355,\"start\":41344},{\"end\":41365,\"start\":41355},{\"end\":41707,\"start\":41699},{\"end\":41715,\"start\":41707},{\"end\":41726,\"start\":41715},{\"end\":41735,\"start\":41726},{\"end\":41745,\"start\":41735},{\"end\":41754,\"start\":41745},{\"end\":41764,\"start\":41754},{\"end\":42109,\"start\":42101},{\"end\":42117,\"start\":42109},{\"end\":42126,\"start\":42117},{\"end\":42137,\"start\":42126},{\"end\":42152,\"start\":42137},{\"end\":42375,\"start\":42367},{\"end\":42391,\"start\":42375},{\"end\":42403,\"start\":42391},{\"end\":42409,\"start\":42403},{\"end\":42422,\"start\":42409},{\"end\":42435,\"start\":42422},{\"end\":42445,\"start\":42435},{\"end\":42456,\"start\":42445},{\"end\":42794,\"start\":42785},{\"end\":42805,\"start\":42794},{\"end\":42818,\"start\":42805},{\"end\":43084,\"start\":43076},{\"end\":43094,\"start\":43084},{\"end\":43104,\"start\":43094},{\"end\":43406,\"start\":43400},{\"end\":43414,\"start\":43406},{\"end\":43421,\"start\":43414},{\"end\":43431,\"start\":43421},{\"end\":43440,\"start\":43431},{\"end\":43450,\"start\":43440},{\"end\":43460,\"start\":43450},{\"end\":43808,\"start\":43802},{\"end\":43815,\"start\":43808},{\"end\":43823,\"start\":43815},{\"end\":43834,\"start\":43823},{\"end\":44259,\"start\":44248},{\"end\":44271,\"start\":44259},{\"end\":44279,\"start\":44271},{\"end\":44647,\"start\":44638},{\"end\":44658,\"start\":44647},{\"end\":44666,\"start\":44658},{\"end\":44674,\"start\":44666},{\"end\":44991,\"start\":44980},{\"end\":45000,\"start\":44991},{\"end\":45011,\"start\":45000},{\"end\":45171,\"start\":45162},{\"end\":45361,\"start\":45354},{\"end\":45375,\"start\":45361},{\"end\":45692,\"start\":45681},{\"end\":45702,\"start\":45692},{\"end\":45713,\"start\":45702}]", "bib_venue": "[{\"end\":32296,\"start\":32233},{\"end\":32641,\"start\":32543},{\"end\":33165,\"start\":33130},{\"end\":33503,\"start\":33440},{\"end\":33836,\"start\":33790},{\"end\":34142,\"start\":34106},{\"end\":34562,\"start\":34499},{\"end\":34940,\"start\":34904},{\"end\":35228,\"start\":35193},{\"end\":35543,\"start\":35508},{\"end\":35795,\"start\":35730},{\"end\":36111,\"start\":35983},{\"end\":36729,\"start\":36694},{\"end\":37162,\"start\":37099},{\"end\":37418,\"start\":37394},{\"end\":37804,\"start\":37741},{\"end\":38282,\"start\":38246},{\"end\":38741,\"start\":38705},{\"end\":39171,\"start\":39097},{\"end\":39600,\"start\":39549},{\"end\":39932,\"start\":39897},{\"end\":40317,\"start\":40254},{\"end\":40666,\"start\":40607},{\"end\":41015,\"start\":40980},{\"end\":41424,\"start\":41365},{\"end\":41827,\"start\":41764},{\"end\":42187,\"start\":42152},{\"end\":42515,\"start\":42456},{\"end\":42853,\"start\":42818},{\"end\":43155,\"start\":43104},{\"end\":43495,\"start\":43460},{\"end\":43908,\"start\":43834},{\"end\":44353,\"start\":44279},{\"end\":44748,\"start\":44674},{\"end\":44978,\"start\":44935},{\"end\":45196,\"start\":45171},{\"end\":45426,\"start\":45375},{\"end\":45768,\"start\":45713}]"}}}, "year": 2023, "month": 12, "day": 17}