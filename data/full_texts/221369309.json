{"id": 221369309, "updated": "2022-01-22 12:34:13.479", "metadata": {"title": "Probabilistic Learning on Graphs via Contextual Architectures", "authors": "[{\"middle\":[],\"last\":\"Bacciu\",\"first\":\"Davide\"},{\"middle\":[],\"last\":\"Errica\",\"first\":\"Federico\"},{\"middle\":[],\"last\":\"Micheli\",\"first\":\"Alessio\"},{\"middle\":[],\"last\":\"Kohli\",\"first\":\"Pushmeet\"}]", "venue": "J. Mach. Learn. Res.", "journal": "J. Mach. Learn. Res.", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex\u2019s neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3047991353", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jmlr/BacciuEM20", "doi": null}}, "content": {"source": {"pdf_hash": "d2eef675f761fba10f6d820a9310db3d32812954", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "60983f3a962ef65dccb15f5cfabe7730c212a1d0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d2eef675f761fba10f6d820a9310db3d32812954.txt", "contents": "\nProbabilistic Learning on Graphs via Contextual Architectures\n2020\n\nDavide Bacciu bacciu@di.unipi.it \nDepartment of Computer Science\nUniversity of Pisa\n56127PIItaly\n\nFederico Errica federico.errica@phd.unipi.it \nDepartment of Computer Science\nUniversity of Pisa\n56127PIItaly\n\nAlessio Micheli micheli@di.unipi.it \nDepartment of Computer Science\nUniversity of Pisa\n56127PIItaly\n\nProbabilistic Learning on Graphs via Contextual Architectures\n\nJournal of Machine Learning Research\n212020Submitted 6/19; Revised 3/20;Editor: Pushmeet KohliStructured domainsdeep graph networksgraph neural networksdeep learningmaximum likelihoodgraph classificationnode classification\nWe propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex's neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature.\n\nIntroduction\n\nThe development of machine learning methodologies has always been driven by the interest in modelling the human brain's inner workings, from the Perceptron (Rosenblatt, 1957) to Spiking neurons (Izhikevich, 2003) and feedforward neural networks, to name a few. These models can only process flat inputs, i.e., vectors, so they are not the best candidates to deal with more involved data representations. Indeed, we often reason about the complex dynamics of the real world in relational terms by considering entities that interact with each other. The complexity of these interactions gives rise to different structures, such as sequences, trees, and graphs. A sequence may be well suited for temporal data, but it is not adequate to represent a molecule or a social network where entities have multiple relationships. Hence, choosing a particular structure to solve a problem affects the kind of information we want to preserve in the data, also referred to as relational inductive bias (Battaglia et al., 2018). Structured domains allow representing more articulated informa-tion than flat data. However, they also need models that explicitly take into account the chosen relational inductive bias and learn to extract structural patterns. For these reasons, the adaptive processing of structures is of fundamental importance in machine learning, data mining, and, more generally, artificial intelligence. Many impactive applications can take advantage of graph processing methods, such as chemistry , bioinformatics (Sharan and Ideker, 2006), and social science (Yanardag and Vishwanathan, 2015). Consequently, further investigation is necessary from both theoretical and practical perspectives.\n\nDepending on the assumptions made about the underlying interactions, researchers developed a plethora of models: Recurrent Neural Networks (Elman, 1990) and Hidden Markov Models (Rabiner and Juang, 1986) for sequences; Recursive Neural Networks (Goller and Kuchler, 1996;Sperduti and Starita, 1997) and Hidden Tree Markov Models (Frasconi et al., 1998;Diligenti et al., 2003;Bacciu et al., 2012) for trees and directed acyclic graphs (Micheli et al., 2004); lastly, Deep Neural Graph Networks (DNGNs) as defined in Bacciu et al. (2020) for arbitrary structures, i.e., graphs, which are the focus of this paper. Notice that the term DNGN is very general as it comprises, in our context, both iterative and convolution-based neural architectures (Section 2).\n\nYet, dealing with structured data is often associated with higher computational costs and theoretical issues. As a matter of fact, we cannot use an (unconstrained) Recursive Neural Network on the cyclic dependencies present in a graph because that can induce a non-converging process to compute the representation of its entities. In the recent past, kernels have been a very successful approach to deal with graph-structured information Da San Martino et al., 2012), allowing to map pairs of graphs to similarity scores between the structures. Interestingly, there also exist hybrid methods between kernels, relational, and logical learning (Frasconi et al., 2014). However, kernels are typically non-adaptive, as they rely on the computation of human-defined similarity measures that are often expensive to compute, restricting their applicability to medium-sized data sets. This is why, in recent years, researchers have designed architectures for graph learning that focus on each entity's neighbourhood. There are at least two reasons for such rapid change: first, the adaptive exploitation of graphs, whose richness of representation is far superior to simpler structures, could replace kernels' feature engineering. Secondly, the increasing availability of this kind of data will soon require methods that scale to large data sets. Nonetheless, technical obstacles often limit the interaction between far-away entities, such as the vanishing of the gradient and over-smoothing (Li et al., 2018). As a result, convolution-based DNGNs are typically not very deep networks with less than five layers (Kipf and Welling, 2017;Xu et al., 2019), making it challenging to capture long dependencies.\n\nIn this paper, we propose to address these limitations with a novel technique for graphstructured data called Contextual Graph Markov Model (CGMM), which is flexible in its construction, efficient, and scalable. It comprises a stack of Bayesian Networks trained in an incremental layer-wise fashion, whose role is to extract different patterns for each entity in the structure. As the model can learn from data without any supervised target label, it is suitable for embedding purposes like struc2vec (Ribeiro et al., 2017) and transferlearning. To evaluate its effectiveness, we target graph and node classification, and we obtain competitive performances when compared to expensive kernels and neural models, thus empirically showing the richness of the information captured by the model. Notably, computational complexity is linear in the number of edges, and we can achieve fine-grained scalability by distributing each vertex's job across different computational resources.\n\nWe extend our preliminary work (Bacciu et al., 2018a) in the following ways. First, we devise a more general formulation that accounts for both discrete and continuous vertex representations, thus increasing the expressiveness of the model. Secondly, we present a new set of experiments that compare our work with kernels and state of the art DNGNs.\n\nIn particular, we thoroughly analyze the effect of depth on performances, showing that we can build deeper architectures than the typical neural network counterparts. Moreover, we simplify the construction of the architecture with a considerable saving of training time, and we study the model's behaviour by visualizing how messages flow in a real graph. Lastly, we investigate the usefulness of some commonly used classification benchmarks, showing that, for some of them, shallow networks seem to be sufficient.\n\nThe rest of the article is organized as follows: Section 2 reviews the literature, focusing on models that are relevant from an architectural point of view; Section 3 presents our method; Section 4 gives details about the experimental setting; Section 5 discusses our key findings; finally, Section 6 contains our last remarks.\n\n\nBackground\n\nThe methods for processing graphs can be divided into four main families, namely kernels, spectral methods, DNGNs, and probabilistic models. Despite the importance of the former two, we prioritize vertex-centric ones because they are more efficient and based on the same underlying principles as our work, though radically different from an architectural point of view. The rest of the section is intended to be an analysis of different architectures rather than a comprehensive review of graph processing methodologies. From now on, we will refer to entities as vertices and the relations between them as edges. Finally, the set of vertices to which one entity is related is called a neighbourhood.\n\nWe can trace the first ideas on neural networks for graphs back to Sperduti and Starita (1997), who proposed the Generalized Recursive Neuron (GRN) for supervised learning on hierarchical structures (trees and Directed Positional Acyclic Graphs). Notably, the authors introduced the idea of neighbourhood aggregation to gather information from a variable number of vertices. More recently, Micheli (2009) and Scarselli et al. (2009) managed to deal with cyclic structures using two different neural architectures: the Neural Network for Graphs (NN4G) and the Graph Neural Network (GNN). These are based on layering and iterative message-passing schemes, respectively; nowadays, the former goes by the name of \"spatial convolution\". NN4G relaxes the causality assumption of the GRN to use a feedforward network, and it exploits Cascade Correlation (Fahlman and Lebiere, 1990) to automatize the construction of the multi-layer architecture; we take inspiration from its incremental and layer-wise approach to design our model. The Graph Neural Network, instead, models a graph as a dynamical system, where vertices are guaranteed to reach a stable state by imposing contractive constraints on the recursive dynamics of the units. However, the procedure may be slow to converge, which results in higher computational requirements for training and inference.\n\nInterestingly, these two formalisms can be unified under an abstract framework called Message Passing Neural Network (MPNN) (Gilmer et al., 2017); in fact, layers implement an information spreading mechanism which is very similar to message-passing, as we will see in the following sections. Nonetheless, the content of the messages is not the same in different models, being heavily dependent on the architecture.\n\nRecently, the Graph Convolutional Network (GCN) (Kipf and Welling, 2017) has become a popular model that fits into the MPNN framework. GCN's formulation approximates the spectral decomposition of the Laplacian matrix L with a Chebyshev polynomial of degree one, i.e., L + I where I is the identity matrix. Moreover, the convolution layer is a weighted sum of a vertex's neighbourhood, where the weight is given by the combination of learned parameters and Laplacian entries. The final architecture is a stack of such layers interleaved by non-linear activations. Li et al. (2018) recently proved that GCNs suffer from the oversmoothing effect, meaning that the vertices' representations tend to become similar after a few layers. As a result, the convolution itself prevents the model from learning long dependencies.\n\nPATCHY-SAN (PSCN) (Niepert et al., 2016) processes graphs thanks to a very different strategy. In particular, the method requires determining a normalized neighbourhood ordering for each vertex that is consistent across the entire data set, as well as to impose an upper bound on the neighbourhood size. The ordering is obtained via a non-adaptive procedure, which may not work well in all situations, and fixing the size of the neighbourhood makes it necessary to introduce padding or to exclude some of the neighbours. This is why, generally speaking, an ordering-free procedure should be preferred. GraphSAGE (Hamilton et al., 2017), on the other hand, exploits a spatial convolution mechanism very similar to that of NN4G. However, the authors test different neighbourhood aggregation schemes and extend the self-supervised link-reconstruction loss of Kipf and Welling (2016). Similarly to PSCN, GraphSAGE relies on a neighbourhood sampling scheme, which needs additional hyper-parameters, to keep computational complexity constant. Instead, the Graph Isomorphism Network (GIN) (Xu et al., 2019) builds upon the limitations of GraphSAGE by extending it with arbitrary aggregation functions on multi-sets. The model is proven to be as theoretically powerful as the 1-dim Weisfeiler-Lehman test of graph isomorphism. Very recently, Wagstaff et al. (2019) gave an upper bound to the number of hidden units needed to realize permutation-invariant functions over sets and multi-sets.\n\nSome models also apply a pooling scheme after convolutional layers to reduce the size of a graph. Among those, we find the Edge-Conditioned Convolution (ECC) (Simonovsky and Komodakis, 2017). ECC is different from most DNGNs in that it learns different parameters for each edge. Therefore, edge parameters act as weights for the neighbours' aggregation, which we can interpret as a simple form of attention (Bahdanau et al., 2015). The pooling scheme of ECC is based on a predefined algorithm, which outputs a pooling map that maintains differentiability. On the other hand, DiffPool (Ying et al., 2018) proposes an adaptive pooling mechanism that collapses vertices based on a supervised criterion. DiffPool is more of a framework rather than a single model: it combines a differentiable encoder for graphs, such as a GCN, with this pooling strategy so that each part of the architecture is trainable. DiffPool may have restricted application in contexts where supervised labels are not available, as there is no easy way to define a self-supervised loss and learn the pooling maps. ARMA (Bianchi et al., 2019) is another DNGN whose pooling strategy deserves mention. For large data sets, ARMA uses the Fiedler vector of a graph to drop approximately half of the vertices in the graph at each pooling step. The convolutional layer, based on spectral theory, exploits the ARMA filter, which is known to be more flexible than the polynomial one. Similarly to ECC, the pooling scheme of ARMA is not adaptive as it depends on the Laplacian of each graph.\n\nThe attention mechanism in the context of graphs was introduced by the Graph Attention Network (GAT) (Velickovic et al., 2018). Indeed, GAT uses multi-head attention on pairs of vertices to determine the importance of an edge, and weight sharing reduces the number of parameters. Similarly to other DNGNs, GAT's architecture was fixed to three layers, and it seems to be slower than simpler methods . The authors report good experimental results on vertex classification tasks, which were later revised by Shchur et al. (2018).\n\nThe Deep Graph Convolutional Neural Network (DGCNN) (Zhang et al., 2018) differs from other works in two ways. First, it concatenates the output of several graph convolutional layers; secondly, vertices are sorted and aligned by a specific algorithm called SortPool that produces truncated graph representations. A 1-dimensional Convolutional Neural Network, followed by fully-connected layers, is then used to solve graph classification tasks. Its extension, the Parametric Graph Convolution DGCNN (PGC-DGCNN) (Tran et al., 2018), further generalizes the model in order to consider neighbourhoods of radius larger than one. The sorting scheme of these models, inspired by graph colouring algorithms, relies on the ability of the convolutional layers to extract consistent colouring across graphs, but this ability may be hampered by the presence of noise in the data. Our proposal does not need a sorting scheme because it aggregates the representations of all vertices in the graph.\n\nFrom the Reservoir Computing field, the GraphESN (Gallicchio and Micheli, 2010) is an Echo State Network for graphs. GraphESN generates a graph encoding when vertices reach a stable state in a recursive system subject to contractive constraints. While this may seem similar to the Graph Neural Network of Scarselli et al. (2009), GraphESN does not require training except for the last layer, thus addressing the efficiency issue of the former approach.\n\nA different strategy is taken by Deep Graph Infomax (DGI) (Velickovic et al., 2019), which incorporates DNGNs into a framework that maximizes the mutual information between vertex and graph representations. DGI relies on a corruption function to generate a corrupted version of the input graph, and it learns representations that allow discriminating between the original and corrupted vertices without having access to supervised target labels. In this sense, the DNGN that produces such representations is encouraged to highlight differences between graphs, while our approach focuses on modelling the distribution of a vertex neighbourhood. When solving classification tasks, DGI trains a standard supervised model on the learned representations, which is the same approach we will adopt in this work.\n\nThe family of probabilistic models for graph data is much smaller than the previous ones, possibly because there still is no straightforward way to formalize neighbourhood aggregation in probability terms. The foundational work of Macskassy and Provost (2007) summarized the first relational probabilistic classifiers for network data. All the proposed classifiers are motivated by the homophily assumption, which states that linked entities tend to belong to the same class. More recently, the Variational Graph Auto-Encoder (VGAE) (Kipf and Welling, 2016) was proposed as an extension of the Variational Auto-Encoder (Kingma and Welling, 2014) to address link-prediction on graphs. VGAE can implement both its encoder and decoder as DNGNs, and the model is trained to reconstruct the graph structure through vertex embeddings. This model has shown good performances on link prediction tasks, but it is not directly applicable to other problems such as vertex or graph classification.\n\nAnother model that takes advantage of DNGNs to approximate probability distributions is the Graph Markov Neural Network (GMNN) Qu et al. (2019), which is defined for semi-supervised vertex classification but can also be applied to link prediction tasks. Its formulation borrows from statistical relational learning theory, and it approximates intractable distributions through DNGNs. Specifically, training is performed using the Variational Expectation-Maximization algorithm (Neal and Hinton, 1998), with an E-step that captures the posterior distribution of each vertex target label conditioned on its neighbours and an M-step that maximizes the probability of the labelled vertices (given the posterior distribution computed at the E-step). Independent DNGNs approximate both variational steps, and the overall architecture is trained in an end-to-end fashion. Similarly to VGAE, GMNN cannot be readily applied to graph classification tasks.\n\nOn the other hand, Graph2Gauss ) is a probabilistic model that learns to map a vertex's attributes to the sufficient statistics of a multivariate Gaussian distribution i.e., mean and covariance matrix. Consequently, the graph structure is not used at inference time while being taken into account during learning. The catch is the following: vertices that are structurally closer to each other in the graph should have similar representations, i.e., statistics of a Gaussian distribution. Graph2Gauss shows impressive performances on link prediction and an efficient inference strategy, but it cannot deal with attributed edges, which usually carry important information e.g., the bond type in molecules.\n\nWith respect to the above architectural choices, we propose the first deep probabilistic model for graphs that can handle arbitrary vertex features (with an extension for discrete edge features) and model the learning process in a fully probabilistic way. To this end, we generalize Hidden Tree Markov models (Bacciu et al., 2012) to graphs, using an incremental layering policy similar to NN4G's that is necessary to correctly define the probabilistic layers of our model. As a by-product of this incremental construction, the model does not suffer from the vanishing of the gradient: this means that each vertex can capture longer dependencies by just increasing the depth of the architecture as much as we need, without having to introduce other techniques such as Batch Normalization (Ioffe and Szegedy, 2015) and Layer Normalization (Ba et al., 2016). Thanks to layering analyses (Sections 5.3-5.5), we will show how context spreads across the graph and what are the benefits of depth in terms of classification performance. Instead of relying on a fixed neighbourhood or sampling schemes, we consider all vertices without discarding information. The probabilistic formulation efficiently leverages examples with respect to their structure and vertex features, without the need for supervised target labels. Then, the model is combined with standard classification methods to solve graph and vertex classification tasks. As we will see, our model recalls a probabilistic version of deep neural graph networks, also called Deep Bayesian Graph Networks (DBGNs) in Bacciu et al. (2020).\n\n\nContextual Graph Markov Model\n\nWith CGMM, we aim to bridge the gap between probabilistic learners for trees and Deep Neural Graph Networks. Our convolutional layer is indeed a Bayesian Network that captures the neighbourhood's distribution of each vertex. As in convolutional DNGNs, stacking layers is necessary to spread information between vertices, but CGMM differs from them in the construction of the architecture. The model is trained incrementally (layer by layer) on a set of graphs to construct vertex or graph embeddings without relying on any supervised label related to the classification task. Then, classical methods such as logistic regression or a Multi-Layer Perceptron (MLP) use these embeddings to solve vertex/graph supervised tasks.\n\nWe adopt a top-down approach to describe CGMM. To begin with, we give a high-level overview of the whole architecture so that the reader can understand the rationale behind the design of the core parts. Next, we discuss the learning process and a straightforward extension via a structure-aware approximation technique.\n\n\nNotation\n\nWe consider the problem of learning from a population of graphs G = {g 1 , . . . , g N }, where each sample g i is a graph with varying topology. A graph g = (V g , E g , X g , A g ) is formally defined by a set of vertices V g and by a set of edges E g between pairs of vertices. Each vertex u has an associated feature vector x u \u2208 X g , which can be continuous or discrete. A directed edge (u,v) between vertices u and v may hold a feature vector a uv \u2208 A g . Instead, an undirected edge can be modeled by two directed edges having the same features. The neighbourhood of a vertex u \u2208 V g is defined as N (u) = {v \u2208 V g |(v, u) \u2208 E g }, that is the set of vertices associated to incoming edges. Without loss of generality, we consider an open neighbourhood of u i.e., one that does not include u itself unless an explicit self-loop is present. Depending on the vertex or graph classification task at hand, we will rely on supervised target labels y v \u2200v \u2208 V g and y g , respectively. Finally, we speak of context of a vertex u to denote the set of vertices that, directly or indirectly, influence vertex u. Figure 1 presents the overall structure of the architecture. The input is a graph represented by black interconnected vertices, which is initially fed to the first layer of our model. For now, it is sufficient to mention that each layer maps every vertex into a vector. This is called an isomorphic transduction (Frasconi et al., 1998), and details are given in Section 3.3. Any subsequent layer can use these intermediate representations together with the input graph; in general, the output of each layer will be conditioned on an arbitrary subset L( ) of previous layers' outputs.\n\n\nArchitecture\n\nOnce the process completes, there will be n different vectors for each vertex, where n is the total number of layers of our architecture. Subsequent concatenation yields a richer representation for all vertices in the graph; if one wants to tackle vertex classification tasks, a classifier must be trained on these representations. However, in case we are interested in graph classification, the outputs have to be first aggregated into a single graph embedding; Section 3.2.2 describes this process in detail.  a vertex target label y v (shown for one vertex only in the figure) or a graph target label y g depending on the supervised task at hand. We incrementally build the network, one layer after another, by fitting all layers on data without using supervised target labels. Each layer produces a vector representation for each vertex of the input graph. Dashed arrows emphasize that what is passed on to subsequent layers is contextual information. The final representation of a vertex v is obtained by concatenating intermediate representations of that vertex across different layers. Such representations are used to solve both vertex classification and graph classification by applying a supervised method to vertex or graph representations.\n\nOur probabilistic architecture puts forward two significant differences with respect to recent DNGNs. First of all, the construction of the model is incremental as training is performed one layer at a time, and parameters are frozen before moving to the next layer. Secondly, CGMM models the distribution of each vertex in a fully probabilistic fashion, whereas DNGNs usually rely on supervised labels or self-supervised loss functions (see Section 2). Incremental training was introduced by Fahlman and Lebiere (1990) in a supervised learning context, with the nice advantage that there are no exploding/vanishing gradient issues. In our case, the incremental construction is also necessary to define the layers of the model by relaxing the causality assumptions, i.e., avoiding mutual dependencies between latent variables in the same layer, as discussed in the following Section 3.3.1. Moreover, it allows us to construct deeper and efficient networks capable of spreading information without being limited by gradient-related issues (Li et al., 2018). , the vertex has access to its information only. Then, from layer two, the vertex is allowed to see its neighbourhood. By the same token, at layer three, the neighbouring vertices have already encoded their own neighbourhood knowledge, so the context window broadens.\n\n\nContext Spreading\n\nOne can see how the architecture of Figure 1 resembles Convolutional Neural Networks (CNNs) (LeCun et al., 1995). Layers are indeed responsible for the aggregation of local information by generalizing the concept of convolution to non-Euclidean spaces (Bruna et al., 2014). Intuitively, by repeated application of such convolution, we increase the context window that each vertex can capture, i.e., the local receptive field. We clarify this aspect in Figure 2. Consider the central vertex of the depicted graph; at the beginning, context is limited to the vertex itself. From layer two, the vertex will always process information coming from its neighbourhood, and at the same time every other neighbour will do the same. Therefore, the context that a vertex captures at layer three is given by the union of the neighbouring contexts computed at layer two; for a formal description and proof of this context behaviour, please refer to Micheli (2009). In a similar vein to hierarchical models for flat data, our goal is to extract distinct patterns from different layers of the network. When the structure is of practical importance the architecture will be deeper, but there may be cases in which a shallow model is sufficient; Section 5 provides an interesting example.\n\n\nGraph Fingerprint Construction\n\nWe can build a graph fingerprint in 2 steps, as shown in Figure 3. Vertex representations of different layers are concatenated into |V g | vectors of longer size. We choose concatenation as the most conservative choice because others do not guarantee the same expressivity (e.g., a weighted sum). This choice is also necessary to keep the process completely separated from any supervised target label. At this point, the concatenated vectors have to be merged together to obtain the graph fingerprint, so we use either sum or mean. Notice that the mean abstracts from the size of a graph and focuses on vertex distributions, whereas the sum tells us which features are most prevalent in the graph. It is reasonable to assume that the best choice is task-dependent. To simplify our analysis and generate task-agnostic\n|V g | \u00d7 C Layer 1 Layer 2 concatenate |V g | \u00d7 2C aggregate 1 \u00d7 2C\nGraph Embedding Figure 3: Example of a graph embedding construction in an architecture of two layers. Each layer outputs a representation of size C for each vertex u \u2208 V g . After a concatenation step, vertex representations (of size 2C) are aggregated to form the required graph embedding. The permutation invariant function over multisets, which we use to perform the aggregation, influences the final result.\n\ngraph representations, we do not incorporate adaptive aggregation functions as in Zaheer et al. (2017) and Xu et al. (2019). So far, we have described how we construct a deep architecture to create vertex and graph embeddings. We now turn our attention to the core module of CGMM that enables fast probabilistic reasoning on vertices.\n\n\nProbabilistic Layer\n\nThe goal of each CGMM's probabilistic layer is to learn a mapping from a vertex to a fixed-size representation, conditioned on the graph structure. To this end, we develop a Bayesian Network that models the generation of a vertex's features while dealing with a variable number of neighbours.\n\n\nModel Definition\n\nThe network of a generic layer is shown in Figure 4. We follow the convention that a shaded vertex corresponds to an observable random variable, whereas a white vertex is associated with an unobserved random variable. A latent factor Q u with C attainable values is responsible for the generation of vertex u's features x u via an emission distribution P (x u |Q u ). Here, we assume that all vertices are i.i.d when the value of Q u is observable, which works well because structural dependencies are encoded in Q u through layering (as discussed in Section 3.2.1). To take into account u's neighbourhood N (u), we condition the posterior distribution of the variable Q u on the set of vertex states q inferred at previous layers. The state of a vertex u is defined as the posterior distribution of its latent factor Q u , which has the form of a vector of size C. Specifically, we identify with q v the observable state of vertex v that has been inferred at layer \u2212 1, with j-th component q v (j). We then formally define the probability of a graph at layer as\nP (g) = u\u2208Vg C i=1 P (x u |Q u = i)P (Q u = i|q \u22121 N (u) ).(1)\nBy means of marginalization, we have introduced the latent variable Q u ; furthermore, q \u22121\nN (u)\ndenotes the set of observable states associated to u's neighbourhood. Importantly, not knowing the size of N (u) makes the definition of the posterior distribution of a vertex challenging Layer \u2113 vertices denote observable and unobserved random variables, respectively. The generation of a vertex features x u is conditioned on a latent factor Q u , which in turn depends on the contextual information coming from previous layers (dashed arrows). Depending on the form of x u , we can use a multivariate Gaussian for continuous attributes or a categorical distribution for discrete ones. Finally, dashed vertices correspond to the latent factors of other vertices that depend on different sets of observable neighbours.\nq \u2113\u22121 N (u) q \u2113\u22121 2 q \u2113\u22121 1 x u Q |V g | Q u\u22121 Q 1 Q u . . . . . . . . .\nto formalize, and the joint distribution of all neighbours quickly becomes intractable (due to the exponential growth in the number of parameters needed). Notice that the cardinality of q \u22121 N (u) may vary, and if we do not assume any vertex ordering we can weigh contributions of the states equally:\nP (Q u = i|q \u22121 N (u) ) \u2248 1 |N (u)| C j P (Q = i|q = j) v\u2208N (u) q \u22121 v (j),(2)\ni.e., we take the average of the elements in q \u22121 N (u) . We want to stress that this may not be the only possible choice, but it returns a correct probability value. With Equation 2, we extend the formalization used in Bacciu et al. (2018a) to handle observable states defined in terms of their posterior distribution. To recover the discrete formulation, it suffices to represent a state q v \u2208 {1, . . . , C} as a one-hot vector where all the probability mass is placed on the i-th entry of maximum value. Nonetheless, the extension leads to a different form of the learning equations. Overall, the parameters of our model are the emission distribution P (x|Q) and the transition distribution P (Q|q).\n\nBefore we move to the training and inference procedures, we have to make important remarks. The probabilistic model we have just described is an extension of the Hidden Tree Markov Model (HTMM) of Bacciu et al. (2012) to the processing of cyclic dependencies. Because a Bayesian Network is defined for acyclic dependencies only, we cannot connect random variables according to the graph structure, as done in HTMMs for trees. Indeed, if two latent variables Q u and Q v had mutual dependencies due to an explicit cycle or undirected edges, we would incur in a recursive and impractical definition of the latent states. Therefore, we must rely on a strategy that relaxes the causality assumptions between variables of the same layer. As discussed in Figure 4, we achieve this by considering a frozen representation of the neighbouring states of the latent Q u , thus approximating the effect of mutual dependencies with what has been computed at previous layers and avoiding the need for recursive definitions of the state space. Not only is the incremental construction functional to the spreading of context between the vertices (Figure 1), but it is necessary to implement the Bayesian Network and to enhance efficiency. In this context, an \"end-to-end\" training of many layers is impractical because it would require not to freeze the previous latent variables. In turn, this amounts to consider exponentially large combinations of hidden states that depend on the structure of the graph, thus hampering the creation of deep architectures.\n\nSecondly, a stationary assumption on all parameters is used to cope with graphs of varying sizes, i.e., we learn the same distributions for all vertices. This means that stationarity allows the model to generalize to unseen instances rather than assuming a maximum graph size in the true data distribution. To cope with varying neighbourhood dimensions, we aggregate neighbours as done in other works (Micheli, 2009;Hamilton et al., 2017). This strategy does not require any kind of neighbourhood sampling scheme, nor it imposes limitations on the neighbourhood size.\n\n\nTraining\n\nWe define learning as a maximum likelihood estimation problem. Given the graphical model in Figure 4, the likelihood of a data set G of i.i.d graphs at level is\nL(\u03b8|G) = g\u2208G u\u2208Vg C i=1 P (x u |Q u = i)P (Q u = i|q \u22121 N (u) ),(3)\nwhich we now know how to compute from Section 3.3.1. Equation 3 is similar to a mixture model with a variable number of additional \"inputs\"; when q is not available, as is the case for the first layer, we recover the usual mixture model (Bishop, 2006). Likelihood can be thus maximized with the Expectation Maximization (Moon, 1996) algorithm; to this aim we define the indicator variable z uij , which is one when vertex u is in state i and its neighbours are in state j.\n\nDuring the E-Step, we need to compute the posterior of the indicator variables, which can be shown to be equivalent to\nE[z uij |G, q] = P (Q u = i, q = j|G, q) = 1 Z P (x u |Q u = i)P (Q u = i|q = j) v\u2208N (u) q \u22121 v (j) |N (u)| ,(4)\nwhere Z is a simple normalization term. Such posterior probabilities are then used to update the model parameters at the M-Step:\nP (Q = i|q = j) = 1 Z g\u2208G u\u2208Vg E[z uij |G, q].\nInstead, the update rule for a categorical emission with K attainable values is\nP (y = k|Q = i) = 1 Z g\u2208G u\u2208Vg \u03b4(x u , k)E[z ui |G, q],\nwhere \u03b4(\u00b7, \u00b7) is the Kronecker delta, and E[z ui |G, q] is obtained by straightforward marginalization of the posterior term computed at the E-step. When dealing with a continuous label, one could use a univariate Gaussian distribution, and the corresponding M-step equations are\n\u00b5 i = g\u2208G u\u2208Vg x u E[z ui |G, q] g\u2208G u\u2208Vg E[z ui |G, q] , \u03c3 i = g\u2208G u\u2208Vg E[z ui |G, q](x u \u2212 \u00b5 old i ) 2 g\u2208G u\u2208Vg E[z ui |G, q] .\n\nInference\n\nThe inference phase takes the most likely index associated to the posterior of Q u as representative for vertex u. In other words, it assigns u to one of the C potential clusters. Formally, this can be expressed as\nmax i P (Q u = i|g, q \u22121 N (u) ) = max i P (x u |Q = i)P (Q = i|q \u22121 N (u) ) $ $ $ $ $ $ $ P (x u |q \u22121 N (u) )\n.\n\nThe equivalence is obtained by straightforward application of the Bayes Theorem; moreover, the denominator does not contribute to the maximization because it is independent of i, so it can be dropped. To compute the numerator, we apply again Equation 2.\n\n\nDifferent Vertex Representations\n\nAs noted previously, the inference phase returns the most likely index of Q u . Hence, we convert this discrete value into a one-hot vector of C entries, as discussed above. This way, the graph fingerprint is constructed as a bag-of-states encoding that counts how many vertices are in a particular cluster at each layer. However, consider the case of the posterior distributions of two vertices, with C = 3, being (0.4, 0, 0.6) and (0, 0.4, 0.6); picking the most likely cluster discards important information about the probability of being in the others. This is why we may want to use the continuous vector of posterior probabilities. The trade-off is between a less noisy but approximate representation and a possibly noisy but exact one; we treat this choice as a hyper-parameter. In both cases, we call such C-sized representation a unigram.\n\nIn addition, we can augment the vertex representation with a bigram, i.e., a C 2 -sized vector which reflects how neighbours of vertices are distributed. Formally, the bigram \u03a6(u) (Bacciu et al., 2018b) of a vertex u is defined as\n\u03a6 i j (u) = v\u2208N (u) q u (i)q v (j), i, j \u2208 1, . . . , C.\nIn the remainder of this paper, whenever a bigram is used, we concatenate it with its corresponding unigram, and the resulting vector is called unibigram. Unigrams are clearly less expensive to compute; in turn, unibigrams constitute a richer vertex representation.\n\n\nExtending CGMM to Incorporate Contributions from Multiple Previous Layers\n\nSo far, we have assumed that only the states computed at the previous layer contribute to the maximization of Equation 1. However, it is possible to modify the CGMM layer to consider contributions from an arbitrary subset L( ) of previous layers. To this aim, we introduce a random variable L u , also called Switching Parent (SP) variable (Saul and Jordan, 1999; Bacciu et al., 2013). Formally, an SP's role is to decompose a complex distribution into a convex combination of simpler ones:\nP (i|i 1 , i 2 , ..., i k ) \u2248 k \u00b5=1 P (SP = \u00b5)P \u00b5 (i|i \u00b5 ).\nIn this work, the role of L u is to weigh states according to the layer they belong to. Simply put, this variable approximates the original posterior distribution by \"grouping\" the previous states q by their layer. Similarly to other neural networks for graphs (Micheli, 2009;Kipf and Welling, 2017), the use of L u can be seen as a form of weighted skip connections over previous layers. The finite cardinality of the set L( ) makes it possible to apply the SP approximation to our problem. We therefore use the random variable L u to rewrite Equation 1 as\nP (g) \u2248 u\u2208Vg C i=1 P (x u |Q u = i) \u2208L( ) P (L u = )P (Q u = i|q N (u) ).(6)\nHere, q N (u) is the subset of u's neighbouring states that are associated to layer . Notice that this definition requires additional transition distributions P (Q u = i|q N (u) ) \u2200 \u2208 L( ). From a learning point of view, the introduction of a SP variable only requires the addition of a new indicator variable in the posterior estimate, i.e., E[z ui j |G, q] = P (Q u = i, L u = , q = j|G, q). The interested reader can find the complete derivations in Appendix A, where we also show how to deal with discrete edge features using the same technique. Figure 5: From left to right: two non-regular graphs that can be discriminated by adding degree information to each vertex; two regular graphs that cannot be discriminated by the 1-dimensional Weisfeiler-Lehman test of graphs isomorphism.\n\nImportantly, the SP approximation fits inside our framework because it does not require reasoning about all layers at the same time. Different decompositions, e.g., conditioning the state of a vertex on the \"history\" of previous posteriors (like RNNs or Hidden Markov Models), assume that states are not frozen and can change altogether through a shared transition function. While more flexible than an SP approximation, it is not possible to apply any of these to CGMM, as it would reintroduce mutual dependencies between unobserved variables (that we have broken thanks to the incremental construction). This technique could be helpful in some tasks, and it provides a principled way to consider skip connections in a probabilistic framework. In Section 5, we study how considering all previous layers affects performances.\n\n\nLimitations\n\nAs we have seen, the probabilistic nature of the model allows it to learn the distribution of each vertex's neighbourhood. However, care must be taken when discriminating between structures with different connectivity and the same local distributions. For example, the two leftmost graphs in Figure 5 have neither vertex nor edge features, and they share the same structure but for one edge. From a probabilistic point of view, the distribution of the neighbourhood's states is always the same, regardless of a vertex's degree, which makes learning difficult. We can mitigate this issue by introducing the notion of vertex degree via a special state \u22a5 called bottom. The idea is to connect each vertex to several \"ghost\" vertices with a special edge type and apply an SP variable for discrete edges that weighs the importance of \"bottom\" states (see Appendix A). Thus, by denoting with deg(g) the maximum degree of a graph g, we can connect vertex u to deg(g) \u2212 deg(u) ghost vertices in bottom state. Moreover, we can also encode vertex u's degree into x u and use a Gaussian emission distribution.\n\nConversely, there are classes of graphs that cannot be distinguished so easily, such as k-regular graphs, that is those such that deg(u) = k \u2200u \u2208 g. In particular, this is the family of structures that can be discriminated by the k-dim Weisfeiler-Lehman (WL) test of Graph Isomorphism (Douglas, 2011). Recently, Xu et al. (2019) formally proved that almost all convolutional DNGNs are at most as powerful as the 1-dim WL test. We provide an example of two regular graphs on the right-hand side of Figure 5. To be able to discriminate these Store q G ;\n13 end 14 R V G \u2190 concatenate(q G \u2200 \u2208 {1, . . . , max }); 15 R G \u2190 aggregate(R V G ); 16 return R V G , R G ;\ngraphs, Morris et al. (2019) proposed a DNGN that can be theoretically as powerful as the k-dim WL test, with a computational cost that, however, grows exponentially with k.\n\n\nComplexity and Scalability\n\nThanks to CGMM's architectural flexibility, training cost of each layer can range from constant (|L( )| = 1) to depth-specific (|L( )| = \u2212 1). Time and space complexity of a training epoch can be bounded by the cost of learning the transition and emission distributions, which is O(|V G |(|L( )||C 2 | + KC)), where V G stands for the set of vertices in the whole data set. The computation of q, on the other hand, has time complexity O(|E G |) because it only needs access to the structure. The overall time computation is therefore bounded by the sum of these two asymptotic terms that can be written as O(|V G | + |E G |).\n\nSimilarly to DNGNs, the i.i.d assumption on vertices given the latent factors makes it easy to implement mini-batch training, with which we can arbitrarily reduce the memory fingerprint; this is especially important in hardware-constrained scenarios. Also, data parallelism could be trivially achieved by distributing the epoch's mini-batches on different computing vertices, such as CPUs or a cluster of machines. For these reasons, CGMM is a suitable candidate to handle large-scale graph learning. Up to now, our implementation 1 does not exploit intra-epoch parallelization: each training epoch was just fast enough for our purposes. To summarize this Section, we provide a pseudo-code of the incremental training procedure in Algorithm 1, showing how the ideas described so far are put together to construct vertex (line 14) and graph (line 15) representations.\n\n\nExperiments\n\nThe Section illustrates the experimental setting. First of all, we present the data sets used to assess CGMM's efficacy. Then, in order to foster reproducibility, the set of hyper-parameters and the risk assessment procedure are thoroughly described.\n\n\nData Sets\n\nWe extend the comparison in Bacciu et al. (2018a) with six common graph classification benchmarks (Kersting et al., 2020) and one vertex classification task. As regards graph classification, three data sets come from the chemical domain, while the others are social network data sets. D&D (Dobson and Doig, 2003) is a data set of proteins in which we have to distinguish between enzymes and non-enzymes; this data set is computationally challenging for kernels because of the high connectivity and size of each graph. Similarly, in PROTEINS (Borgwardt et al., 2005), vertices are connected if they are neighbours in the amino-acid sequence or 3D space. The last chemical data set, NCI1 (Wale et al., 2008), is the biggest in terms of the number of graphs. It contains two balanced subsets of chemical compounds that need to be classified according to their ability to suppress the growth of a panel of human tumour cell lines. Instead, IMDB-BINARY and IMDB-MULTI (Yanardag and Vishwanathan, 2015) are movie-collaboration data sets where vertices represent actors/actresses, and there is an edge every time two of them appeared in the same movie; the task is to classify the associated movie genre. Finally, COLLAB (Leskovec et al., 2005) is a scientific-collaboration benchmark derived from 3 other data sets, and the task is to classify the specific research field each community belongs to. To test CGMM performance on vertex classification tasks, we use the protein-protein interaction data set (PPI) introduced in Hamilton et al. (2017). In this task, we are given a set of distinct large graphs, and our goal is to classify their vertices.\n\nWe provide a summary of the data sets statistics in Table 1. Notice that all collaboration data sets have no information except their structure. The absence of vertex features is a degenerate case that prevents a probabilistic model from learning any vertex distribution.\n\nAs done in Niepert et al. (2016), when a vertex has no features, we add its degree as a continuous value as well as the bottom states discussed in Section 3.3.6.\n\n\nHyper-parameters\n\nWe now list the hyper-parameters needed by CGMM and by the supervised classifier working on graph and vertex representations. We achieve constant per-layer complexity by setting |L| = 1, but we also evaluate the impact of all previous layers at the cost of additional computation. The number of EM epochs is fixed to 10: the reason is that likelihood always stabilizes around that value in our experiments.\n\nMoreover, we tried to use both continuous and discrete vertex representations, as described in Section 3.3.4. On the contrary, the aggregation function (sum or mean) was usually fixed after an initial experimental screening, which helped to reduce the grid dimension. Overall, the different configurations of CGMM's architecture depended on two hyper-parameters only: the number of hidden states C and the number of layers. Furthermore, we can exploit the incremental nature of our model to reduce the dimension of the grid search space: if the set of depth hyper-parameters has maximum value N , we train a single network of N layers and \"cut\" it to obtain different architectures of depth M < N .\n\nContrarily to our previous work (Bacciu et al., 2018a), we did not use the \"pooling\" strategy of Fahlman and Lebiere (1990): in short, it consists of training a pool of candidate Bayesian Networks at each layer and selecting the best one according to some metric, e.g., validation performances. This choice is motivated by empirical studies that will be presented in Section 5.4. The training time improvement factor is equal to the number of candidates that were used in Bacciu et al. (2018a), i.e., 10 \u00d7.\n\nAnother point to consider is whether the learned representations are sufficient to linearly separate classes; to this end, we introduced a logistic regressor in the grid search. However, we add a flexible alternative in the form of a one-hidden-layer MLP with ReLU activations. Both of them are trained with Adam (Kingma and Ba, 2015) optimizer and Cross-Entropy loss (Mean Squared Error for PPI); to contrast overfitting, we introduced L2 regularization. Also, we used the early stopping technique called Generalization Loss (Prechelt, 1998) with \u03b1 = 5, which is considered to be a good compromise between training time and performances. In Table 5, we report the number of epochs after which early stopping starts; at the beginning of training, validation loss smoothly oscillates and accuracy does not steadily increase, so it would be unwise to apply this technique immediately. For space reasons, we list the hyper-parameters table in Appendix B.\n\n\nRisk Assessment\n\nTo assess CGMM's classification performance, we used a Double Cross-Validation with 10 external folds and 5 internal folds. The internal folds are needed for grid-search model selection, which returns a biased estimate (optimization is done on validation folds) of the best hyper-parameters for each external split. Then, these configurations are retrained on the corresponding external training fold (using a validation set to do early stopping) and evaluated on the external test fold. We repeat each final retraining three times and average results in order to mitigate possible bad random initializations. The output of this procedure is an estimate of CGMM's performance. Importantly, the test fold was never   (2018), where the authors performed an external 10-fold cross-validation with a simple hold-out internal model selection procedure. Their process is repeated ten times with different splits, and results are averaged. This approach, which is similar to bootstrapping, is less subject to unlucky splits with consequent variance reduction; on the other hand, it might be more biased than a k-fold cross validation (Kohavi, 1995), which is why we use the latter. Despite these differences, both evaluations are robust and comparable estimates of a model's performance. A train/validation/test split was already provided for PPI, so we used a standard hold-out technique for this task.\n\n\nResults and Discussion\n\nIn the following, we present CGMM's results on graph and vertex classification, along with empirical studies on the beneficial effects of depth. Indeed, depth gives hints on CGMM's ability to extract useful information, as well as how much structural features are essential for the task at hand. We support our reasoning via stability results, which led us to a relevant insight into some of the data sets used. Then, we perform hyper-parameters' analyses to study their impact on the final performance. Finally, we visualize how the model moves information among vertices and what is the average contribution of an input state to the new output representation.\n\n\nGraph Classification\n\nWe evaluate the performance of our method against different kernels and deep learning techniques for graph classification. Table 2 provides a comparison between the kernels considered in terms of computational costs. As we can see, kernels are not adequate when it comes to large scale training and inference because of their (at least) quadratic time complexity in the number of graphs. Moreover, the considered kernel for graphs are not applicable to continuous vertex features, which limits their scope. Apart from these and some of the DNGNs introduced in Section 2, we will consider the Deep Graphlet Kernel  Best results are reported in bold. We report CGMM's accuracy on NCI1 in bold because it performs better than the other neural models. CGMM-nb indicates that the model is not using bigram features, whereas CGMM-full represents the extended CGMM using all previous layers rather than just the previous one.\n\n(DGK) framework (Yanardag and Vishwanathan, 2015) and the Diffusion Convolutional Neural Network (DCNN) (Atwood and Towsley, 2016), which is a precursor of GCN. Results for graph classification are shown in Table 3. CGMM performs well in all data sets (scoring top-3 on five of them), even though the probabilistic architecture was not optimized to solve a classification task. In particular, we achieve state of the art results on all three collaborative data sets, and we improve the best result on NCI1 compared to all supervised DNGNs. This suggests that learning the distribution of a vertex's neighbourhood at different abstraction's levels produces a good representation. As a matter of fact, in 9 out of 10 external folds on NCI1, the model selection procedure chose a configuration with 20 layers; in contrast, the DNGNs of Table 3 exploit a maximum of 4 layers only, possibly interleaved by batch normalization (Ioffe and Szegedy, 2015) to accelerate training. Furthermore, results also highlight that CGMM performs well even when the only source of information is structural.\n\nNote that kernels can process and compare graphs more explicitly than vertex-centric models: one of the reasons why the WL kernel has higher accuracy on NCI1 may be due to the kind of structural patterns it uses to compute the similarity score. However, the number of scores to compute may quickly become impractical, as we can see from Table 2. Please notice the difference between context-expansion via neighbourhood aggregation and the standard kernel approach: the former provides higher-level representations of a graph, while the latter looks for similar substructures between each pair of vertices or graphs.\n\nSome chemical benchmarks deserve a more in-depth analysis. Because the accuracy on D&D and PROTEINS seems comparable for both kernels and vertex-centric models, we decided to study the effect of context on these data sets. Section 5.4 will show that we can trivially achieve state of the art performances on PROTEINS and D&D. This raises the question of whether or not these benchmarks should be considered for future evaluations of deep learning methods for graphs in that the structure does not seem to be that relevant for the task.\n\n\nVertex Classification\n\nWe now turn our attention to vertex classification. Because CGMM and most of the other DNGNs in literature produce vertex embeddings, it is reasonable to assess the model performance on PPI, a common and robust benchmark for vertex classification. To this aim, we compare against GraphSAGE (Hamilton et al., 2017) and DGI (Velickovic et al., 2019), as well as a structure-agnostic baseline that applies logistic regression to the vertex features. Indeed, GraphSAGE, DGI, and CGMM all share a first pre-training step in which vertex embeddings are learned on data with no supervised target labels, before being given to a supervised classifier for vertex prediction. Results are shown in Table 4: we can see that CGMM has good performances, improving against all GraphSAGE variants except for DGI. Considering that DGI uses GraphSAGE as part of its framework, it seems that the learning procedure is what generates the gap between the two methods. While GraphSAGE relies on a link prediction loss and an entropy penalization term, DGI learns to discriminate vertices according to a contrastive noise procedure that maximizes the mutual information between the input and the target value.  Table 4: CGMM's results of inductive vertex classification on PPI. We report the Micro Average F1 score. Here, X refers to the input features and A to the adjacency matrix information.\n\n\nHyper-parameters Analysis\n\nWe enrich our empirical analysis with three further studies. The first concerns the impact of the unibigram technique (introduced in Section 3.3.4) on performances, whereas the second regards the potential advantages of using all previous layers when training a new layer. Finally, the last one investigates whether a wider model with fewer layers can perform as well as a deep model with fewer hidden states. With the exception of Section 5.3.2, in our analyses we will refer to the standard version of CGMM with L( ) = { \u2212 1}.\n\n\nUnibigram Ablation\n\nIn Table 3, we re-evaluated the model on all data sets by constraining CGMM to only use unigrams. Results indicate a slight performance drop on chemical data sets and a larger decrease in social data sets. This result suggests that, especially when we only have access to structural information (i.e., the degree distribution), computing a graph representation that takes the structure into account can be helpful. Nevertheless, CGMM performances remain good with respect to the state of the art.\n\n\nImpact of Previous Layers\n\nSimilarly to Section 5.3.1, we have repeated all experiments by conditioning each layer of the architecture on the entire subset of previous layers, i.e., L( ) = {1, . . . , \u2212 1}. This way, each layer is free to weigh the previous layers to maximize the likelihood of a graph. Results (Table 3 and 4, CGMM-full) indicate that the model performs almost always on par (with a difference within the standard deviation intervals) compared to the significantly more efficient version that does not use the SP variable L u . Nonetheless, despite the negligible performance advantage obtained on these tasks, we recommend treating the use of L u as a hyper-parameter of the model when dealing with other node or graph classification tasks.\n\n\nSensitivity Analysis\n\nWhen designing any deep network (let it be neural or probabilistic), it is useful to analyze the relation between the dimension of each layer's hidden representation and the number of layers in terms of performance. In the case of CGMM (and more generally DNGNs), the depth of the architecture is functional to context spreading, so we would expect that having a larger hidden representation for each layer is not enough to compensate for the flow of information between vertices. To show this, we provide an example in Figure 6, where we show how validation accuracy of a logistic regressor on NCI1 varies while changing the number of hidden states C. For instance, we observe that the graph representation associated with point A of size 60 is not sufficient to achieve the performance of the representation associated with point B, which has size 45. This means that smaller representations associated with deeper networks can encode more information than those associated with shallower ones.\n\n\nOn the Effects of Depth\n\nThis Section is meant to answer to two further research questions. First, we want to quantify the effect of depth on the architecture when coupled with a classifier. The second is about understanding how much the model's performance is affected by a random initialization of the layers. To address both questions, we took a random train-validation-test split of NCI1 to conduct a new experiment. With its 4110 graphs, NCI1 was chosen to minimize the effect of the data split on results, and consequently on the random initialization of the classifier. In contrast, the other chemical data sets are much more split-dependent. We trained a 20-layer network for each configuration defined in Table 5. We repeated each process five times, and the results were averaged. Figure 7 reports accuracy against the number of layers for one of such configurations, with logistic regression (Figure 7a) and MLP ( Figure  Figure 6: This picture shows that having a larger graph representation with fewer layers is not always enough to reach the same performance of a deeper network with a smaller graph representation. Points A and B are associated with representations of dimensions 60 and 45, respectively. However, we see that the latter is associated with better validation performances of a logistic regressor on NCI1, which means that the flowing of contextual information is indeed more beneficial than increasing the number of hidden states C. Results are averaged over five independent runs. 7b) classifiers. We see that, in both cases, depth has a beneficial effect on test accuracies, with slightly worse results on test accuracy from logistic regression due to its strong bias. Notice how test curves in Figure 7b tend to an asymptote after ten layers; this information may be used as stopping criterion when constructing the architecture for supervised tasks, as proposed in Marquez et al. (2018) for CNNs on images.\n\nOne interesting thing to notice is that we do not necessarily incur in the curse of dimensionality as the size of the fingerprint grows with the layers. This is because the fingerprint construction at layer is guided by layer \u2212 1, and this dependency may well cause the fingerprint to lie in a sub-space of the original one. This may explain why we do not quickly overfit after the first few layers.\n\nIn addition to the above considerations, we note the following. Since training accuracy on NCI1 does not significantly vary between different runs (due to different weights' initialization), that is an indication that the pooling strategy of Fahlman and Lebiere (1990), mentioned in Section 4.2, brings negligible advantage in our case. This also holds for PRO- , PROTEINS and D&D that shows how accuracy varies from 1 to 20 layers. Results are averaged over five independent runs. Colored bands denote standard deviation. While the effect of depth is beneficial for NCI1, stability experiment on PROTEINS and D&D report a different behavior, which is not to be attributed to random splits. Indeed, we discover that competitive results can be achieved by trivial methods on PROTEINS and D&D.\n\nTEINS and D&D when the architecture is very shallow (up to three layers), though results need to be interpreted from a different perspective, as we are about to show.\n\n\nA Critical Investigation of Chemical Benchmarks\n\nThe stability experiments we did on PROTEINS and D&D led to surprisingly different results. We can see from Figure 7c and 7d that accuracy on validation and test tends to decrease almost immediately, while standard deviation increases as we add layers. One may think this behaviour is caused by the way the data set was randomly split. However, repeating the experiments did not change the final result. It appears that a minimal number of layers (even 1) is enough to get the best results. Thus, we took inspiration from  and from the technique of molecular fingerprint to investigate the matter and validate our hypothesis. In order to assess the practical relevance of the structured information for each chemical task, we constructed graph fingerprints by only counting atoms of the same type. Then, a 10-fold Nested Cross-Validation with Hold-out model selection was used to assess the performance of different MLP classifiers: the average test accuracy was 76.0 \u00b1 3.4 for PROTEINS and 77.33 \u00b1 3.7 for D&D, entirely in line with the state of the art. Instead, this trivial approach was not enough to reach a satisfying accuracy on NCI1 (69.80 \u00b1 3.0), meaning that being able to capture structural patterns is very important for discrimination purposes on this task. This investigation shows how important it is to establish structure-agnostic baselines, like the one we used, to understand how much the structure plays a role in the final result. Our study on depth effects, which is rather lacking in DNGNs' literature, revealed that more care should be taken when considering benchmark results. Our beliefs further strengthen similar statements made in Shchur et al. (2018) for semi-supervised vertex classification tasks. Figure 8: Context flow on a real NCI1 graph. We focus on the highly regular rightmost subgraph, which is influenced by the left part of the structure. Heatmaps show that, although relatively small, posteriors change as we expected.\n\n\nVisualization: a Case Study\n\nThis part provides a visual exploration and interpretation of CGMM's internal dynamics. This kind of analysis is meant to demonstrate how the model extracts different patterns at each layer of the architecture in a way that is consistent with what stated in Section 3.2.1. Therefore, Figure 8 shows how information spreads in a real NCI1 molecule. We represent each vertex's posterior as a circle chart with C different colours; in this experiment, we use a 4-layer CGMM with C = 3. Please keep in mind that colours assignment between different layers is irrelevant. The rightmost six atoms of the molecule have the same atomic symbol so that they will be assigned an identical state at layer 1. What is more, these six atoms alone form a 2-regular subgraph, which means that their state can only change if context flows from left to right, as shown by the dashed arrows on the top-left side of the figure. As discussed in Section 3.3.6, if it were not for the five leftmost vertices context could not flow, because all \"messages\" would be identical (both qualitatively and quantitatively). This is indeed what happens: at each layer, we highlight the vertices we are interested in via dashed regions, and the associated heatmap (one state per row) proves that posteriors change as we expected. Thanks to the simplicity of Equation 2, it is easy to understand how CGMM behaves in this particular situation. Similarly, we can look at how the model exploits the learned parameters (and the \"bottom\" states introduced in Section 3.3.6) through E[z uij ]: Figure 9 illustrates what CGMM infers on average. First of all, each layer has a different average \"activation\" pattern that adds new useful information to the final fingerprint. Moreover, we can inspect how the \u22a5 contribution, i.e., degree information, is encoded into the new states. In particular, some components of the previous states, called \"C2\" because they now include the \u22a5 symbol, have little influence on the average posterior of the random variables, as is the case for component 2 at layer 2. Instead, bottom states are always taken into consideration, possibly because they encode information about the degree of a vertex.\n\n\nConclusions and Future Works\n\nWe have introduced a new probabilistic perspective for scalable deep learning on graphstructured data. CGMM processes graphs by exploiting an incremental construction of the architecture to spread contextual information between vertices. Moreover, the model does not suffer from the vanishing like most typical DNGNs. This new probabilistic formulation allows us to efficiently and effectively cope with graph and node classification tasks, with good performances compared to state of the art models. Also, we have thoroughly analyzed the impact of depth on performance improvements. Therefore, we hope that our work may lay the foundation of Deep Bayesian Graph Networks (DBGNs), a new probabilistic family of graph processing methods.\n\nSeveral research directions can further enhance this architecture for representation learning on structures. One is the automatic choice of the hyper-parameter C via Bayesian nonparametric methods, while another is the use of more general aggregation functions (Castellana and Bacciu, 2019). Other than these, it would be critical to automatically determine when to stop in the construction of the architecture with a criterion that depends on information-theoretic properties of the inferred states, e.g., the entropy, or to assess transfer learning capabilities in social or chemical domains.\n\n\nLayer 1\n\nLayer 2\n\nLayer 3 Layer 4 Figure 9: Unnormalized posteriors computed on NCI1 data set and averaged over vertices. By inspection, we can understand how much of a value q(j) is mapped, on average, to the different components of the new state. The first layer displays averaged states.\n\nIn conclusion, the study of probabilistic models for graphs that automatize the choice of their hyper-parameters opens appealing challenges for the future development of the field.\n\nWe start again from Equation 6 to derive the learning equations for a generic layer ; remember that layer 0 corresponds to a simple mixture model, whose learning equations are well known. Here, however, we also deal with discrete edge types (taken from a finite alphabet {1, . . . , A}) by introducing an SP variable S u that weights the contribution of each edge. The only additional modifications are the introduction of a different transition distribution for each edge type a, an additional indicator variable, and the definition of q ,a N (u) as the set of neighbouring posteriors of u that are connected to u via edge a and were computed at layer . The resulting model is a more general version than that of Bacciu et al. (2018a), in which continuous rather than discrete vertex posteriors are considered. Finally, note that time and space complexities are now bounded by O(|V G |(|L|A|C 2 |+KC)). The extended graphical model is shown in Figure 10.\n\nWith the addition of the SP variable S u , the likelihood of the model becomes\nL = g\u2208G u\u2208Vg C i P (x u |Q u = i) \u2208L( ) P (L u = )P (Q u = i|q N (u) ) = g\u2208G u\u2208Vg C i P (x u |Q u = i) \u2208L( ) P (L u = ) A a=1 P (S u = a)P ,a (Q u = i|q ,a N (u) ) = g\u2208G u\u2208Vg C i P (x u |Q u = i) \u2208L( ) P (L u = ) A a=1 P (S u = a)\u00d7 \u00d7 C j P ,a (Q = i|q = j) v\u2208N a (u) q v (j) |N a (u)| ,\nwhere N a (u) is the set of u's neighbours connected via an edge of type a. We dropped the subscript from |N a (u)| because it is independent from the specific layer. In pratice, S u splits q Nu into multiple groups of vertices depending on the type of edge they are connected to u. Notice that we have introduced all the random variables via marginalization. By means of the indicator variables introduced in Section 3.3, we can write the complete log-likelihood\nQ u Q 1 Q u\u22121 Q |V g | S u\n. . . . . . \n\nThe E-step of the EM algorithm requires to compute the expectation of the log likelihood conditioned on the data. With slight abuse of notation and when clear from the context, Moreover, the expectation of an indicator variable is just the probability of its associated event:\nE[z ui |G, q] = P (Q u = i|G, q) E[z ui |G, q] = P (Q u = i, L u = |G, q)\nE[z ui a |G, q] = P (Q u = i, L u = , S u = a|G, q) E[z ui aj |G, q] = P (Q u = i, L u = , S u = a, K u = j|G, q).\n\nK u is a categorical random variable with C possible states, such that P ,a (K u = j) = v\u2208N a (u) q v (j)/|N a (u)|. Consequently, we can apply the Bayes Theorem on E[z ui aj |G, q], yielding E[z ui aj |G, q] = P (Q u = i, L u = , S u = a, K u = j|G, q) = P (x u |Q u = i)P (Q u = i, L u = , S u = a, K u = j|q) P (x u |q) = P (x u |Q u = i)P (Q u = i|L u = , S u = a, K u = j, q)P (L u = )P (S u = a)P ,a (K u = j) Z = P (x u |Q u = i)P ,a (Q u = i|q = j)P (L u = )P (S u = a)P ,a (K u = j) Z = P (x u |Q u = i)P (L u = )P (S u = a)P ,a (Q = i|q = j)P ,a (K u = j) Z ,\n\nwhere Z is the normalization term, obtained by P (x u |q) via marginalization over all the latent variables ( .\n\n(10)\n\nThe derivations for the categorical emission and edge selector distribution are not shown because they are almost identical to the one above. The corresponding Lagrangian constraints are expressed as C i \u03bb i ( K k P (x = k |Q = i ) \u2212 1) and \u2208L( ) \u03b1 ( A a =1 P (S = a ) \u2212 1). Hence, we obtain the following learning rules:  .\nP (x = k|Q = i) = g\u2208G u\u2208Vg \u03b4(x u , k)E[z ui |G, q]\nWe now show the update rules for the transition distribution, which is slightly more troublesome.\n\n\u2202E Z|G,q [log L c (\u03b8|G, q, Z)] \u2212 ,a ,j \u00b5 ,a ,j ( Deriving with respect to the specific Lagrange multiplier \u00b5 l,a,j and using the above formula yields .\n\nTo conclude, we note that the learning equations for the emission distributions are identical to those of a mixture model, with the only difference that we substitute the expectation on the prior with E[z ui ], which depends on the model's parameters. This is why we do not repeat the derivation for other emission distribution e.g., a univariate or multivariate Gaussian. Moreover, to introduce the bottom state \u22a5, it suffices to use an additional edge feature and an additional value for the variable Q.  Table 5: Hyper-parameters tried. IMDB-* refers to both binary and multi-class tasks. Early-Stopping refers to the epoch after which we apply early stopping on validation loss. The number of hidden units per layer are ignored when using logistic regression.\n\nFigure 1 :\n1The figure shows a high-level overview of the architecture. The goal is to output\n\nFigure 2 :\n2We show how the context window of the central vertex in the graph widens. At the beginning (left graph)\n\nFigure 4 :\n4The Bayesian Network that implements layer of our model. Shaded and white\n\nAlgorithm 1 :\n1Probabilistic Incremental Training Input: Data set G, maximum number of layers max and epochs epoch max . Output: A data set of node graph representations 1 for \u2190 1 to max do 2 Initialize layer according to L( ) and C; 3 Load q G \u2200 \u2208 L( ); 4 for epoch \u2190 1 to epoch max do 5 \u2206 likelihood , posteriors \u2190 E-Step(G,\n\n\nNeumann et al. (2012) WL O(|G|hm + |G| 2 hn) Shervashidze et al. (2011) CGMM O(|G|n + |G|m)\n\nFigure 7 :\n7Stability experiment on NCI1\n\nFigure 10 :P\n10The extended Bayesian Network implementing layer of our model. The SP variables L u and S u are used to weigh the contribution of previous layers and different discrete edge features. ,a (Q = i|q = j) v\u2208N a (u) q v (j) log P (x u |Q = i) aj log P ,a (Q = i|q = j) v\u2208N a (u) q v (j) |N a (u)| .\n\n\nwe use q in place of q N (u) :E Z|q,G [log L c (\u03b8|G, q, Z)] z ui |G, q] log P (x u |Q = i) z ui aj |G, q] log P ,a (Q = i|q = j) v\u2208N a (u) q v (j)|N a (u)| .\n\n\nincluding K u ). Similarly, E[z ui ], E[z ui ] and E[z ui a ] can be obtained by straightforward marginalization of E[z ui aj ]. We now develop the M-step equations exploiting the expectations computed in the E-step. If we compute the gradient with respect of P (L = ) and we add a Lagrange multiplier to enforce probability requirements we get \u2202E Z|G,q [log L c (\u03b8|G, q, Z)] \u2212 \u03b3( \u2208L( ) P (L = ) respect to \u03b3 yields \u2202E Z|G,q [log L c (\u03b8|G, q, Z)] \u2212 \u03b3( \u2208L( ) P (L = ) z ui |G, q] = \u03b3, which can be plugged into Equation 9 to obtain the update rule for P (L = ): ( ) E[z ui |G, q]\n\n\n(x u , k )E[z ui |G, q]\n\na\n=1 E[z ui a |G, q]\n\nPE\n,a (Q = i |q = j ) \u2212 1)\u2202P ,a (Q = i|q = j) ui a j |G, q] log P ,a (Q=i |q=j ) v\u2208N a (u) q v (j ) |N a (u)| \u2202P ,a (Q = i|q = j) \u2212 \u2202 ,a ,j \u00b5 ,a ,j ( C i =1 P ,a (Q = i |q = j ) \u2212 1) \u2202P ,a (Q = i|q = j) = g\u2208G u\u2208Vg E[z ui aj |G, q] P ,a (Q = i|q = j)\u2212 \u00b5 ,a,[z ui aj |G, q] \u00b5 ,a,j = P ,a (Q = i|q = j).\n\nPE\n,a (Q = i |q = j) [z ui aj |G, q] = \u00b5 ,a,j . This leaves us with the transition distribution update formula:P ,a (Q = i|q = j) = g\u2208G u\u2208Vg E[z ui aj |G, q] g\u2208G u\u2208Vg C i =1 E[z ui aj |G, q]\n\nTable 2 :\n2Computational costs of graph kernels compared to CGMM. We assume that all graphs have size n = |V g |, m = |E g | edges and maximum degree d. Moreover, k is the size of the graphlets that GK counts, and h is the number of iterations needed by different procedures to compute the final similarity scores.seen throughout the entire process; unfortunately, we have observed this is not always the \ncase in DNGNs' literature, where model selection's best results, i.e., those of a k-fold cross-\nvalidation, are reported. This trend is also criticized in Klicpera et al. (2019). Consequently, \nwe compare with results from Tran et al. \n\nTable 3 :\n3CGMM's results of a 10-Fold Double Cross Validation for graph classification.\n. The implementation is available here: https://github.com/diningphil/CGMM.\nAcknowledgmentsThis work has been partially supported by the Italian Ministry of Education, University, and Research (MIUR) under project SIR 2014 LIST-IT (grant n. RBSI14STDE).Appendix A. Learning Equations\nDiffusion-convolutional neural networks. James Atwood, Don Towsley, Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS). the 30th Conference on Neural Information Processing Systems (NIPS)James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS), pages 1993-2001, 2016.\n\nLayer normalization. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, Neural Information Processing Systems (NIPS) Deep Learning Symposium. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. In Neural Information Processing Systems (NIPS) Deep Learning Symposium, 2016.\n\nCompositional generative mapping for tree-structured data -part I: Bottom-up probabilistic modeling of trees. Davide Bacciu, Alessio Micheli, Alessandro Sperduti, IEEE Transactions on Neural Networks and Learning Systems. 2312IEEEPublisherDavide Bacciu, Alessio Micheli, and Alessandro Sperduti. Compositional generative map- ping for tree-structured data -part I: Bottom-up probabilistic modeling of trees. IEEE Transactions on Neural Networks and Learning Systems, 23(12):1987-2002, 2012. Pub- lisher: IEEE.\n\nAn inputoutput hidden Markov model for tree transductions. Davide Bacciu, Alessio Micheli, Alessandro Sperduti, Neurocomputing. 112ElsevierDavide Bacciu, Alessio Micheli, and Alessandro Sperduti. An inputoutput hidden Markov model for tree transductions. Neurocomputing, 112:34-46, 2013. Publisher: Elsevier.\n\nContextual Graph Markov Model: A deep and generative approach to graph processing. Davide Bacciu, Federico Errica, Alessio Micheli, Proceedings of the 35th International Conference on Machine Learning (ICML). the 35th International Conference on Machine Learning (ICML)PMLR80Davide Bacciu, Federico Errica, and Alessio Micheli. Contextual Graph Markov Model: A deep and generative approach to graph processing. In Proceedings of the 35th Inter- national Conference on Machine Learning (ICML), volume 80, pages 294-303. PMLR, 2018a.\n\nGenerative kernels for treestructured data. Davide Bacciu, Alessio Micheli, Alessandro Sperduti, IEEE Transactions on Neural Networks and Learning Systems. 2910IEEEDavide Bacciu, Alessio Micheli, and Alessandro Sperduti. Generative kernels for tree- structured data. IEEE Transactions on Neural Networks and Learning Systems, 29(10): 4932-4946, 2018b. Publisher: IEEE.\n\nA gentle introduction to deep learning for graphs. Davide Bacciu, Federico Errica, Alessio Micheli, Marco Podda, Neural Networks. 129Davide Bacciu, Federico Errica, Alessio Micheli, and Marco Podda. A gentle introduction to deep learning for graphs. Neural Networks, 129:203-221, 2020.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Proceedings of the 3rd International Conference on Learning Representations (ICLR). the 3rd International Conference on Learning Representations (ICLR)Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.\n\nW Peter, Jessica B Battaglia, Victor Hamrick, Alvaro Bapst, Vinicius Sanchez-Gonzalez, Mateusz Zambaldi, Andrea Malinowski, David Tacchetti, Adam Raposo, Ryan Santoro, Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. arXiv preprintPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nMaria Filippo, Daniele Bianchi, Lorenzo Grattarola, Cesare Livi, Alippi, arXiv:1901.01343Graph neural networks with convolutional arma filters. arXiv preprintFilippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks with convolutional arma filters. arXiv preprint arXiv:1901.01343, 2019.\n\nM Christopher, Bishop, Pattern Recognition and Machine Learning. SpringerChristopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n\nDeep gaussian embedding of graphs: Unsupervised inductive learning via ranking. Aleksandar Bojchevski, Stephan Gnnemann, Proceedings of the 6th International Conference on Learning Representations (ICLR). the 6th International Conference on Learning Representations (ICLR)Aleksandar Bojchevski and Stephan Gnnemann. Deep gaussian embedding of graphs: Un- supervised inductive learning via ranking. Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.\n\nProtein function prediction via graph kernels. M Karsten, Borgwardt, Cheng Soon, Stefan Ong, Schnauer, Alex J Svn Vishwanathan, Hans-Peter Smola, Kriegel, Oxford University Press21BioinformaticssupplKarsten M Borgwardt, Cheng Soon Ong, Stefan Schnauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinfor- matics, 21(suppl 1):i47-i56, 2005. Publisher: Oxford University Press.\n\nSpectral networks and locally connected networks on graphs. Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, Proceedings of the 2nd International Conference on Learning Representations (ICLR). the 2nd International Conference on Learning Representations (ICLR)Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014.\n\nBayesian tensor factorisation for bottom-up hidden tree Markov models. Daniele Castellana, Davide Bacciu, Proceedings of the International Joint Conference on Neural Networks (IJCNN). the International Joint Conference on Neural Networks (IJCNN)Daniele Castellana and Davide Bacciu. Bayesian tensor factorisation for bottom-up hidden tree Markov models. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), 2019.\n\nA tree-based kernel for graphs. Giovanni Da San, Nicolo Martino, Alessandro Navarin, Sperduti, Proceedings of the 12th International Conference on Data Mining (ICDM). the 12th International Conference on Data Mining (ICDM)SIAMGiovanni Da San Martino, Nicolo Navarin, and Alessandro Sperduti. A tree-based kernel for graphs. In Proceedings of the 12th International Conference on Data Mining (ICDM), pages 975-986. SIAM, 2012.\n\nHidden tree Markov models for document image classification. Michelangelo Diligenti, Paolo Frasconi, Marco Gori, IEEE Transactions on Pattern Analysis and Machine Intelligence. 254IEEEMichelangelo Diligenti, Paolo Frasconi, and Marco Gori. Hidden tree Markov models for document image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(4):519-523, 2003. Publisher: IEEE.\n\nDistinguishing enzyme structures from non-enzymes without alignments. D Paul, Andrew J Dobson, Doig, Journal of Molecular Biology. 3304ElsevierPaul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of Molecular Biology, 330(4):771-783, 2003. Publisher: Elsevier.\n\nL Brendan, Douglas, arXiv:1101.5211The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprintBrendan L Douglas. The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211, 2011.\n\nFinding structure in time. Jeffrey L Elman, Cognitive Science. 142Wiley Online LibraryPublisherJeffrey L Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990. Pub- lisher: Wiley Online Library.\n\nThe Cascade-Correlation learning architecture. E Scott, Christian Fahlman, Lebiere, Proceedings of the 3rd Conference on Neural Information Processing Systems (NIPS). the 3rd Conference on Neural Information Processing Systems (NIPS)Scott E. Fahlman and Christian Lebiere. The Cascade-Correlation learning architecture. In Proceedings of the 3rd Conference on Neural Information Processing Systems (NIPS), pages 524-532, 1990.\n\nFast graph representation learning with PyTorch Geometric. Matthias Fey, Jan Eric Lenssen, Workshop on Representation Learning on Graphs and Manifolds, International Conference on Learning Representations (ICLR). Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with PyTorch Ge- ometric. Workshop on Representation Learning on Graphs and Manifolds, International Conference on Learning Representations (ICLR), 2019.\n\nA general framework for adaptive processing of data structures. Paolo Frasconi, Marco Gori, Alessandro Sperduti, IEEE Transactions on Neural Networks. 95IEEEPaolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework for adaptive processing of data structures. IEEE Transactions on Neural Networks, 9(5):768-786, 1998. Publisher: IEEE.\n\nklog: A language for logical and relational learning with kernels. Paolo Frasconi, Fabrizio Costa, Luc De Raedt, Kurt De Grave, Artificial Intelligence. 217ElsevierPaolo Frasconi, Fabrizio Costa, Luc De Raedt, and Kurt De Grave. klog: A language for logical and relational learning with kernels. Artificial Intelligence, 217:117-143, 2014. Publisher: Elsevier.\n\nGraph echo state networks. Claudio Gallicchio, Alessio Micheli, Proceedings of the International Joint Conference on Neural Networks (IJCNN). the International Joint Conference on Neural Networks (IJCNN)IEEEClaudio Gallicchio and Alessio Micheli. Graph echo state networks. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2010.\n\nNeural message passing for quantum chemistry. Justin Gilmer, S Samuel, Schoenholz, F Patrick, Oriol Riley, George E Vinyals, Dahl, Proceedings of the 34th International Conference on Machine Learning (ICML). the 34th International Conference on Machine Learning (ICML)Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 1263-1272, 2017.\n\nLearning task-dependent distributed representations by backpropagation through structure. Christoph Goller, Andreas Kuchler, Proceedings of the International Conference on Neural Networks (ICNN). the International Conference on Neural Networks (ICNN)IEEE1Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representa- tions by backpropagation through structure. In Proceedings of the International Confer- ence on Neural Networks (ICNN), volume 1, pages 347-352. IEEE, 1996.\n\nInductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS). the 31st Conference on Neural Information Processing Systems (NIPS)Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS), pages 1024-1034, 2017.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, Proceedings of the 32nd International Conference on Machine Learning (ICML). the 32nd International Conference on Machine Learning (ICML)Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network train- ing by reducing internal covariate shift. In Proceedings of the 32nd International Confer- ence on Machine Learning (ICML), pages 448-456, 2015.\n\nSimple model of spiking neurons. M Eugene, Izhikevich, IEEE Transactions on Neural Networks. 146Eugene M Izhikevich. Simple model of spiking neurons. IEEE Transactions on Neural Networks, 14(6):1569-1572, 2003.\n\nBenchmark data sets for graph kernels. Kristian Kersting, Nils M Kriege, Christopher Morris, Petra Mutzel, Marion Neumann, Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels. 2020. URL http://www.graphlearning.io/.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Proceedings of the 3rd International Conference on Learning Representations (ICLR). the 3rd International Conference on Learning Representations (ICLR)Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.\n\nAuto-encoding variational Bayes. P Diederik, Max Kingma, Welling, Proceedings of the 2nd International Conference on Learning Representations (ICLR). the 2nd International Conference on Learning Representations (ICLR)Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014.\n\nVariational graph auto-encoders. N Thomas, Max Kipf, Welling, Workshop on Bayesian Deep Learning, Neural Information Processing System (NIPS). Thomas N Kipf and Max Welling. Variational graph auto-encoders. In Workshop on Bayesian Deep Learning, Neural Information Processing System (NIPS), 2016.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, Proceedings of the 5th International Conference on Learning Representations (ICLR. the 5th International Conference on Learning Representations (ICLRThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.\n\nPredict then propagate: graph neural networks meet personalized PageRank. Johannes Klicpera, Aleksandar Bojchevski, Stephan Gnnemann, Proceedings of the 7th International Conference on Learning Representations (ICLR). the 7th International Conference on Learning Representations (ICLR)Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gnnemann. Predict then prop- agate: graph neural networks meet personalized PageRank. In Proceedings of the 7th International Conference on Learning Representations (ICLR), 2019.\n\nA study of cross-validation and bootstrap for accuracy estimation and model selection. Ron Kohavi, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). the International Joint Conference on Artificial Intelligence (IJCAI)14Ron Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), volume 14, pages 1137-1145, 1995.\n\nConvolutional networks for images, speech, and time series. The Handbook of Brain Theory and Neural Networks. Yann Lecun, Yoshua Bengio, 3361Yann LeCun, Yoshua Bengio, and others. Convolutional networks for images, speech, and time series. The Handbook of Brain Theory and Neural Networks, 3361(10):1995, 1995.\n\nGraphs over time: densification laws, shrinking diameters and possible explanations. Jure Leskovec, Jon Kleinberg, Christos Faloutsos, Proceedings of the 11th International Conference on Knowledge Discovery in Data Mining (SIGKDD). the 11th International Conference on Knowledge Discovery in Data Mining (SIGKDD)ACMJure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: densification laws, shrinking diameters and possible explanations. In Proceedings of the 11th International Conference on Knowledge Discovery in Data Mining (SIGKDD), pages 177-187. ACM, 2005.\n\nDeeper insights into graph convolutional networks for semi-supervised learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI). the 32nd AAAI Conference on Artificial Intelligence (AAAI)Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.\n\nClassification in networked data: A toolkit and a univariate case study. A Sofus, Foster Macskassy, Provost, Journal of Machine Learning Research. 8Sofus A Macskassy and Foster Provost. Classification in networked data: A toolkit and a univariate case study. Journal of Machine Learning Research, 8(May):935-983, 2007.\n\nDeep cascade learning. S Enrique, Jonathon S Marquez, Mahesan Hare, Niranjan, IEEE Transactions on Neural Networks and Learning Systems. 2911IEEEEnrique S Marquez, Jonathon S Hare, and Mahesan Niranjan. Deep cascade learning. IEEE Transactions on Neural Networks and Learning Systems, 29(11):5475-5485, 2018. Publisher: IEEE.\n\nNeural network for graphs: A contextual constructive approach. Alessio Micheli, IEEE Transactions on Neural Networks. 203Publisher: IEEEAlessio Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions on Neural Networks, 20(3):498-511, 2009. Publisher: IEEE.\n\nContextual processing of structured data by recursive cascade correlation. Alessio Micheli, Diego Sona, Alessandro Sperduti, IEEE Transactions on Neural Networks. 156IEEEAlessio Micheli, Diego Sona, and Alessandro Sperduti. Contextual processing of structured data by recursive cascade correlation. IEEE Transactions on Neural Networks, 15(6): 1396-1410, 2004. Publisher: IEEE.\n\nThe expectation-maximization algorithm. K Todd, Moon, IEEE Signal Processing Magazine. 136Publisher: IEEETodd K Moon. The expectation-maximization algorithm. IEEE Signal Processing Magazine, 13(6):47-60, 1996. Publisher: IEEE.\n\nWeisfeiler and leman go neural: Higher-order graph neural networks. Christopher Morris, Martin Ritzert, Matthias Fey, L William, Jan Eric Hamilton, Gaurav Lenssen, Martin Rattan, Grohe, Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI). the 33rd AAAI Conference on Artificial Intelligence (AAAI)33Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI), volume 33, pages 4602-4609, 2019.\n\nA view of the em algorithm that justifies incremental, sparse, and other variants. M Radford, Geoffrey E Neal, Hinton, Learning in graphical models. SpringerRadford M Neal and Geoffrey E Hinton. A view of the em algorithm that justifies incremen- tal, sparse, and other variants. In Learning in graphical models, pages 355-368. Springer, 1998.\n\nEfficient graph kernels by randomization. Marion Neumann, Novi Patricia, Roman Garnett, Kristian Kersting, Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD). the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)Marion Neumann, Novi Patricia, Roman Garnett, and Kristian Kersting. Efficient graph kernels by randomization. In Proceedings of the Joint European Conference on Ma- chine Learning and Knowledge Discovery in Databases (ECML PKDD), pages 378-393.\n\n. Springer, Springer, 2012.\n\nLearning convolutional neural networks for graphs. Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov, Proceedings of the 33rd International Conference on Machine Learning (ICML). the 33rd International Conference on Machine Learning (ICML)Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neu- ral networks for graphs. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 2014-2023, 2016.\n\nEarly stopping-but when?. Lutz Prechelt, Neural Networks: Tricks of the trade. SpringerLutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55-69. Springer, 1998.\n\nGMNN: Graph Markov Neural Networks. Meng Qu, Yoshua Bengio, Jian Tang, Proceedings of the 36th International Conference on Machine Learning (ICML). the 36th International Conference on Machine Learning (ICML)Meng Qu, Yoshua Bengio, and Jian Tang. GMNN: Graph Markov Neural Networks. In Proceedings of the 36th International Conference on Machine Learning (ICML), pages 5241-5250, 2019.\n\nAn introduction to hidden Markov models. R Lawrence, Biing-Hwang Rabiner, Juang, IEEE ASSP Magazine. 31Publisher: IEEELawrence R Rabiner and Biing-Hwang Juang. An introduction to hidden Markov models. IEEE ASSP Magazine, 3(1):4-16, 1986. Publisher: IEEE.\n\nGraph kernels for chemical informatics. Liva Ralaivola, J Sanjay, Hiroto Swamidass, Pierre Saigo, Baldi, Neural Networks. 188ElsevierLiva Ralaivola, Sanjay J Swamidass, Hiroto Saigo, and Pierre Baldi. Graph kernels for chemical informatics. Neural Networks, 18(8):1093-1110, 2005. Publisher: Elsevier.\n\nLearning node representations from structural identity. Leonardo Fr Ribeiro, H P Pedro, Daniel R Saverese, Figueiredo, Proceedings of the 23rd International Conference on Knowledge Discovery and Data Mining (SIGKDD). the 23rd International Conference on Knowledge Discovery and Data Mining (SIGKDD)ACM2Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node representations from structural identity. In Proceedings of the 23rd International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 385-394. ACM, 2017.\n\nThe Perceptron, a Perceiving and Recognizing Automaton Project Para. Frank Rosenblatt, Cornell Aeronautical LaboratoryFrank Rosenblatt. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory, 1957.\n\nMixed memory Markov models: Decomposing complex stochastic processes as mixtures of simpler ones. K Lawrence, Michael I Jordan Saul, Machine Learning. Springer37Lawrence K Saul and Michael I Jordan. Mixed memory Markov models: Decomposing complex stochastic processes as mixtures of simpler ones. Machine Learning, 37(1):75- 87, 1999. Publisher: Springer.\n\nAh Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. Franco Scarselli, Marco Gori, IEEE Transactions on Neural Networks. 201IEEEFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Mon- fardini. The graph neural network model. IEEE Transactions on Neural Networks, 20 (1):61-80, 2009. Publisher: IEEE.\n\nModeling cellular machinery through biological network comparison. Roded Sharan, Trey Ideker, Nature Biotechnology. 244Nature Publishing GroupRoded Sharan and Trey Ideker. Modeling cellular machinery through biological network comparison. Nature Biotechnology, 24(4):427, 2006. Publisher: Nature Publishing Group.\n\nPitfalls of graph neural network evaluation. Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Gnnemann, Workshop on Relational Representation Learning, Neural Information Processing Systems (NeurIPS). Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gnnemann. Pitfalls of graph neural network evaluation. Workshop on Relational Representation Learning, Neural Information Processing Systems (NeurIPS), 2018.\n\nEfficient graphlet kernels for large graph comparison. Nino Shervashidze, Tobias Vishwanathan, Kurt Petri, Karsten Mehlhorn, Borgwardt, Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS). the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borg- wardt. Efficient graphlet kernels for large graph comparison. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 488- 495, 2009.\n\nWeisfeiler-lehman graph kernels. Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, Karsten M Borgwardt, Journal of Machine Learning Research. 12Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539-2561, 2011.\n\nDynamic edge-conditioned filters in convolutional neural networks on graphs. Martin Simonovsky, Nikos Komodakis, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolu- tional neural networks on graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3693-3702, 2017.\n\nSupervised neural networks for the classification of structures. Alessandro Sperduti, Antonina Starita, IEEE Transactions on Neural Networks. 83Publisher: IEEEAlessandro Sperduti and Antonina Starita. Supervised neural networks for the classification of structures. IEEE Transactions on Neural Networks, 8(3):714-735, 1997. Publisher: IEEE.\n\nKernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity. S Joshua Swamidass, Jonathan Chen, Jocelyne Bruand, Peter Phung, Liva Ralaivola, Pierre Baldi, Bioinformatics. 211Oxford University PresssupplS Joshua Swamidass, Jonathan Chen, Jocelyne Bruand, Peter Phung, Liva Ralaivola, and Pierre Baldi. Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity. Bioinformatics, 21(suppl 1):i359-i368, 2005. Publisher: Oxford University Press.\n\nOn filter size in graph convolutional networks. V Dinh, Nicol Tran, Alessandro Navarin, Sperduti, IEEE Symposium Series on Computational Intelligence (SSCI). IEEEDinh V Tran, Nicol Navarin, and Alessandro Sperduti. On filter size in graph convolutional networks. In IEEE Symposium Series on Computational Intelligence (SSCI), pages 1534- 1541. IEEE, 2018.\n\nGraph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, Proceedings of the 6th International Conference on Learning Representations (ICLR). the 6th International Conference on Learning Representations (ICLR)Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In Proceedings of the 6th International Con- ference on Learning Representations (ICLR), 2018.\n\nDeep Graph Infomax. Petar Velickovic, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio, R Devon Hjelm, Proceedings of the 7th International Conference on Learning Representations (ICLR). the 7th International Conference on Learning Representations (ICLR)New Orleans, LA, USAPetar Velickovic, William Fedus, William L. Hamilton, Pietro Li, Yoshua Bengio, and R. De- von Hjelm. Deep Graph Infomax. In Proceedings of the 7th International Conference on Learning Representations (ICLR), New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nGraph kernels. S Vichy, N Vishwanathan, N Nicol, Risi Schraudolph, Karsten M Kondor, Borgwardt, Journal of Machine Learning Research. 11S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph kernels. Journal of Machine Learning Research, 11(Apr):1201-1242, 2010.\n\nOn the limitations of representing functions on sets. Edward Wagstaff, B Fabian, Martin Fuchs, Ingmar Engelcke, Michael Posner, Osborne, Proceedings of the 36th International Conference on Machine Learning (ICML). the 36th International Conference on Machine Learning (ICML)Edward Wagstaff, Fabian B Fuchs, Martin Engelcke, Ingmar Posner, and Michael Osborne. On the limitations of representing functions on sets. In Proceedings of the 36th Interna- tional Conference on Machine Learning (ICML), pages 6487-6494, 2019.\n\nComparison of descriptor spaces for chemical compound retrieval and classification. Nikil Wale, A Ian, George Watson, Karypis, Knowledge and Information Systems. 143SpringerNikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chem- ical compound retrieval and classification. Knowledge and Information Systems, 14(3): 347-375, 2008. Publisher: Springer.\n\nHow powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, Proceedings of the 7th International Conference on Learning Representations (ICLR). the 7th International Conference on Learning Representations (ICLR)Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Proceedings of the 7th International Conference on Learning Repre- sentations (ICLR), 2019.\n\nDeep graph kernels. Pinar Yanardag, Vishwanathan, Proceedings of the 21th International Conference on Knowledge Discovery and Data Mining (SIGKDD. the 21th International Conference on Knowledge Discovery and Data Mining (SIGKDDACMPinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th International Conference on Knowledge Discovery and Data Mining (SIGKDD, pages 1365-1374. ACM, 2015.\n\nHierarchical graph representation learning with differentiable pooling. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS). the 32nd Conference on Neural Information Processing Systems (NeurIPS)Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Pro- ceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), 2018.\n\nDeep sets. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, R Ruslan, Alexander J Salakhutdinov, Smola, Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS). the 31st Conference on Neural Information Processing Systems (NIPS)Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhut- dinov, and Alexander J Smola. Deep sets. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS), pages 3391-3401, 2017.\n\nAn end-to-end deep learning architecture for graph classification. Muhan Zhang, Zhicheng Cui, Marion Neumann, Yixin Chen, Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI). the 32nd AAAI Conference on Artificial Intelligence (AAAI)Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.\n", "annotations": {"author": "[{\"start\":\"69\",\"end\":\"166\"},{\"start\":\"167\",\"end\":\"276\"},{\"start\":\"277\",\"end\":\"377\"}]", "publisher": null, "author_last_name": "[{\"start\":\"76\",\"end\":\"82\"},{\"start\":\"176\",\"end\":\"182\"},{\"start\":\"285\",\"end\":\"292\"}]", "author_first_name": "[{\"start\":\"69\",\"end\":\"75\"},{\"start\":\"167\",\"end\":\"175\"},{\"start\":\"277\",\"end\":\"284\"}]", "author_affiliation": "[{\"start\":\"103\",\"end\":\"165\"},{\"start\":\"213\",\"end\":\"275\"},{\"start\":\"314\",\"end\":\"376\"}]", "title": "[{\"start\":\"1\",\"end\":\"62\"},{\"start\":\"378\",\"end\":\"439\"}]", "venue": "[{\"start\":\"441\",\"end\":\"477\"}]", "abstract": "[{\"start\":\"664\",\"end\":\"1691\"}]", "bib_ref": "[{\"start\":\"1863\",\"end\":\"1881\",\"attributes\":{\"ref_id\":\"b55\"}},{\"start\":\"1901\",\"end\":\"1919\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"2695\",\"end\":\"2719\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3226\",\"end\":\"3251\",\"attributes\":{\"ref_id\":\"b58\"}},{\"start\":\"3272\",\"end\":\"3305\",\"attributes\":{\"ref_id\":\"b72\"}},{\"start\":\"3546\",\"end\":\"3559\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"3585\",\"end\":\"3610\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"3652\",\"end\":\"3678\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"3678\",\"end\":\"3705\",\"attributes\":{\"ref_id\":\"b63\"}},{\"start\":\"3736\",\"end\":\"3759\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"3759\",\"end\":\"3782\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"3782\",\"end\":\"3802\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3841\",\"end\":\"3863\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"3922\",\"end\":\"3942\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"4603\",\"end\":\"4631\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"4807\",\"end\":\"4830\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"5649\",\"end\":\"5666\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"5769\",\"end\":\"5793\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"5793\",\"end\":\"5809\",\"attributes\":{\"ref_id\":\"b71\"}},{\"start\":\"6365\",\"end\":\"6387\",\"attributes\":{\"ref_id\":\"b54\"}},{\"start\":\"6875\",\"end\":\"6897\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"8821\",\"end\":\"8848\",\"attributes\":{\"ref_id\":\"b63\"}},{\"start\":\"9144\",\"end\":\"9158\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"9163\",\"end\":\"9186\",\"attributes\":{\"ref_id\":\"b57\"}},{\"start\":\"9601\",\"end\":\"9628\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"10234\",\"end\":\"10255\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"10574\",\"end\":\"10598\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"11089\",\"end\":\"11105\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"11363\",\"end\":\"11385\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"11957\",\"end\":\"11980\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"12201\",\"end\":\"12224\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"12427\",\"end\":\"12444\",\"attributes\":{\"ref_id\":\"b71\"}},{\"start\":\"12679\",\"end\":\"12701\",\"attributes\":{\"ref_id\":\"b69\"}},{\"start\":\"12987\",\"end\":\"13019\",\"attributes\":{\"ref_id\":\"b62\"}},{\"start\":\"13236\",\"end\":\"13259\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"13413\",\"end\":\"13432\",\"attributes\":{\"ref_id\":\"b73\"}},{\"start\":\"13918\",\"end\":\"13940\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"14483\",\"end\":\"14508\",\"attributes\":{\"ref_id\":\"b66\"}},{\"start\":\"14888\",\"end\":\"14908\",\"attributes\":{\"ref_id\":\"b59\"}},{\"start\":\"14963\",\"end\":\"14983\",\"attributes\":{\"ref_id\":\"b75\"}},{\"start\":\"15422\",\"end\":\"15441\",\"attributes\":{\"ref_id\":\"b65\"}},{\"start\":\"15946\",\"end\":\"15976\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"16202\",\"end\":\"16225\",\"attributes\":{\"ref_id\":\"b57\"}},{\"start\":\"16409\",\"end\":\"16434\",\"attributes\":{\"ref_id\":\"b67\"}},{\"start\":\"17388\",\"end\":\"17416\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"17776\",\"end\":\"17802\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"18271\",\"end\":\"18287\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"18621\",\"end\":\"18644\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"20106\",\"end\":\"20127\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"20585\",\"end\":\"20610\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"20635\",\"end\":\"20652\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"21364\",\"end\":\"21384\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"23897\",\"end\":\"23920\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"26475\",\"end\":\"26492\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"26875\",\"end\":\"26895\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"27035\",\"end\":\"27055\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"27719\",\"end\":\"27733\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"29469\",\"end\":\"29489\",\"attributes\":{\"ref_id\":\"b74\"}},{\"start\":\"29494\",\"end\":\"29510\",\"attributes\":{\"ref_id\":\"b71\"}},{\"start\":\"32675\",\"end\":\"32696\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"33357\",\"end\":\"33377\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"35105\",\"end\":\"35120\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"35120\",\"end\":\"35142\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"35750\",\"end\":\"35764\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"38602\",\"end\":\"38624\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"39417\",\"end\":\"39437\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"39865\",\"end\":\"39880\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"39880\",\"end\":\"39903\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"43282\",\"end\":\"43298\",\"attributes\":{\"ref_id\":\"b71\"}},{\"start\":\"43640\",\"end\":\"43660\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"45637\",\"end\":\"45658\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"45707\",\"end\":\"45729\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"46141\",\"end\":\"46174\"},{\"start\":\"46295\",\"end\":\"46314\",\"attributes\":{\"ref_id\":\"b70\"}},{\"start\":\"46572\",\"end\":\"46605\",\"attributes\":{\"ref_id\":\"b72\"}},{\"start\":\"46823\",\"end\":\"46846\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"47127\",\"end\":\"47149\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"47539\",\"end\":\"47560\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"48850\",\"end\":\"48872\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"48927\",\"end\":\"48941\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"49290\",\"end\":\"49311\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"49651\",\"end\":\"49660\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"49852\",\"end\":\"49868\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"51424\",\"end\":\"51438\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"53342\",\"end\":\"53375\",\"attributes\":{\"ref_id\":\"b72\"}},{\"start\":\"53430\",\"end\":\"53456\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"54247\",\"end\":\"54272\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"55882\",\"end\":\"55905\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"55914\",\"end\":\"55939\",\"attributes\":{\"ref_id\":\"b67\"}},{\"start\":\"61726\",\"end\":\"61747\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"62424\",\"end\":\"62438\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"64841\",\"end\":\"64861\",\"attributes\":{\"ref_id\":\"b59\"}},{\"start\":\"68395\",\"end\":\"68424\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"69919\",\"end\":\"69940\",\"attributes\":{\"ref_id\":\"b4\"}}]", "figure": "[{\"start\":\"73582\",\"end\":\"73676\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"73677\",\"end\":\"73793\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"73794\",\"end\":\"73880\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"73881\",\"end\":\"74208\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"74209\",\"end\":\"74302\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"74303\",\"end\":\"74344\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"74345\",\"end\":\"74654\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"74655\",\"end\":\"74814\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"74815\",\"end\":\"75395\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"75396\",\"end\":\"75421\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"75422\",\"end\":\"75443\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"75444\",\"end\":\"75745\",\"attributes\":{\"id\":\"fig_12\"}},{\"start\":\"75746\",\"end\":\"75937\",\"attributes\":{\"id\":\"fig_13\"}},{\"start\":\"75938\",\"end\":\"76580\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"76581\",\"end\":\"76670\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1707\",\"end\":\"3405\"},{\"start\":\"3407\",\"end\":\"4163\"},{\"start\":\"4165\",\"end\":\"5862\"},{\"start\":\"5864\",\"end\":\"6842\"},{\"start\":\"6844\",\"end\":\"7193\"},{\"start\":\"7195\",\"end\":\"7709\"},{\"start\":\"7711\",\"end\":\"8038\"},{\"start\":\"8053\",\"end\":\"8752\"},{\"start\":\"8754\",\"end\":\"10108\"},{\"start\":\"10110\",\"end\":\"10524\"},{\"start\":\"10526\",\"end\":\"11343\"},{\"start\":\"11345\",\"end\":\"12827\"},{\"start\":\"12829\",\"end\":\"14380\"},{\"start\":\"14382\",\"end\":\"14909\"},{\"start\":\"14911\",\"end\":\"15895\"},{\"start\":\"15897\",\"end\":\"16349\"},{\"start\":\"16351\",\"end\":\"17155\"},{\"start\":\"17157\",\"end\":\"18142\"},{\"start\":\"18144\",\"end\":\"19089\"},{\"start\":\"19091\",\"end\":\"19795\"},{\"start\":\"19797\",\"end\":\"21385\"},{\"start\":\"21419\",\"end\":\"22141\"},{\"start\":\"22143\",\"end\":\"22462\"},{\"start\":\"22475\",\"end\":\"24168\"},{\"start\":\"24185\",\"end\":\"25436\"},{\"start\":\"25438\",\"end\":\"26761\"},{\"start\":\"26783\",\"end\":\"28054\"},{\"start\":\"28089\",\"end\":\"28905\"},{\"start\":\"28974\",\"end\":\"29385\"},{\"start\":\"29387\",\"end\":\"29721\"},{\"start\":\"29745\",\"end\":\"30037\"},{\"start\":\"30058\",\"end\":\"31120\"},{\"start\":\"31184\",\"end\":\"31275\"},{\"start\":\"31282\",\"end\":\"32001\"},{\"start\":\"32075\",\"end\":\"32375\"},{\"start\":\"32455\",\"end\":\"33158\"},{\"start\":\"33160\",\"end\":\"34702\"},{\"start\":\"34704\",\"end\":\"35271\"},{\"start\":\"35284\",\"end\":\"35444\"},{\"start\":\"35513\",\"end\":\"35985\"},{\"start\":\"35987\",\"end\":\"36105\"},{\"start\":\"36219\",\"end\":\"36347\"},{\"start\":\"36395\",\"end\":\"36474\"},{\"start\":\"36531\",\"end\":\"36810\"},{\"start\":\"36953\",\"end\":\"37167\"},{\"start\":\"37280\",\"end\":\"37281\"},{\"start\":\"37283\",\"end\":\"37536\"},{\"start\":\"37573\",\"end\":\"38420\"},{\"start\":\"38422\",\"end\":\"38652\"},{\"start\":\"38710\",\"end\":\"38975\"},{\"start\":\"39053\",\"end\":\"39543\"},{\"start\":\"39604\",\"end\":\"40161\"},{\"start\":\"40239\",\"end\":\"41027\"},{\"start\":\"41029\",\"end\":\"41854\"},{\"start\":\"41870\",\"end\":\"42968\"},{\"start\":\"42970\",\"end\":\"43521\"},{\"start\":\"43632\",\"end\":\"43805\"},{\"start\":\"43836\",\"end\":\"44461\"},{\"start\":\"44463\",\"end\":\"45329\"},{\"start\":\"45345\",\"end\":\"45595\"},{\"start\":\"45609\",\"end\":\"47253\"},{\"start\":\"47255\",\"end\":\"47526\"},{\"start\":\"47528\",\"end\":\"47689\"},{\"start\":\"47710\",\"end\":\"48116\"},{\"start\":\"48118\",\"end\":\"48816\"},{\"start\":\"48818\",\"end\":\"49324\"},{\"start\":\"49326\",\"end\":\"50277\"},{\"start\":\"50297\",\"end\":\"51693\"},{\"start\":\"51720\",\"end\":\"52381\"},{\"start\":\"52406\",\"end\":\"53324\"},{\"start\":\"53326\",\"end\":\"54412\"},{\"start\":\"54414\",\"end\":\"55029\"},{\"start\":\"55031\",\"end\":\"55566\"},{\"start\":\"55592\",\"end\":\"56964\"},{\"start\":\"56994\",\"end\":\"57522\"},{\"start\":\"57545\",\"end\":\"58041\"},{\"start\":\"58071\",\"end\":\"58803\"},{\"start\":\"58828\",\"end\":\"59824\"},{\"start\":\"59852\",\"end\":\"61767\"},{\"start\":\"61769\",\"end\":\"62168\"},{\"start\":\"62170\",\"end\":\"62961\"},{\"start\":\"62963\",\"end\":\"63129\"},{\"start\":\"63181\",\"end\":\"65142\"},{\"start\":\"65174\",\"end\":\"67363\"},{\"start\":\"67396\",\"end\":\"68132\"},{\"start\":\"68134\",\"end\":\"68728\"},{\"start\":\"68740\",\"end\":\"68747\"},{\"start\":\"68749\",\"end\":\"69021\"},{\"start\":\"69023\",\"end\":\"69203\"},{\"start\":\"69205\",\"end\":\"70160\"},{\"start\":\"70162\",\"end\":\"70240\"},{\"start\":\"70528\",\"end\":\"70991\"},{\"start\":\"71019\",\"end\":\"71031\"},{\"start\":\"71033\",\"end\":\"71309\"},{\"start\":\"71384\",\"end\":\"71498\"},{\"start\":\"71500\",\"end\":\"72069\"},{\"start\":\"72071\",\"end\":\"72182\"},{\"start\":\"72184\",\"end\":\"72188\"},{\"start\":\"72190\",\"end\":\"72514\"},{\"start\":\"72566\",\"end\":\"72663\"},{\"start\":\"72665\",\"end\":\"72816\"},{\"start\":\"72818\",\"end\":\"73581\"}]", "formula": "[{\"start\":\"28906\",\"end\":\"28973\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"31121\",\"end\":\"31183\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"31276\",\"end\":\"31281\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"32002\",\"end\":\"32074\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"32376\",\"end\":\"32454\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"35445\",\"end\":\"35512\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"36106\",\"end\":\"36218\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"36348\",\"end\":\"36394\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"36475\",\"end\":\"36530\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"36811\",\"end\":\"36940\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"37168\",\"end\":\"37279\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"38653\",\"end\":\"38709\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"39544\",\"end\":\"39603\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"40162\",\"end\":\"40238\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"43522\",\"end\":\"43631\",\"attributes\":{\"id\":\"formula_15\"}},{\"start\":\"70241\",\"end\":\"70527\",\"attributes\":{\"id\":\"formula_16\"}},{\"start\":\"70992\",\"end\":\"71018\",\"attributes\":{\"id\":\"formula_17\"}},{\"start\":\"71310\",\"end\":\"71383\",\"attributes\":{\"id\":\"formula_19\"}},{\"start\":\"72515\",\"end\":\"72565\",\"attributes\":{\"id\":\"formula_21\"}}]", "table_ref": "[{\"start\":\"47307\",\"end\":\"47314\"},{\"start\":\"49968\",\"end\":\"49975\"},{\"start\":\"52529\",\"end\":\"52536\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"53533\",\"end\":\"53540\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"54159\",\"end\":\"54166\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"54751\",\"end\":\"54758\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"56279\",\"end\":\"56286\"},{\"start\":\"56780\",\"end\":\"56787\"},{\"start\":\"57548\",\"end\":\"57555\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"58356\",\"end\":\"58364\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"60541\",\"end\":\"60548\"},{\"start\":\"73325\",\"end\":\"73332\"}]", "section_header": "[{\"start\":\"1693\",\"end\":\"1705\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"8041\",\"end\":\"8051\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"21388\",\"end\":\"21417\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"22465\",\"end\":\"22473\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"24171\",\"end\":\"24183\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"26764\",\"end\":\"26781\",\"attributes\":{\"n\":\"3.2.1.\"}},{\"start\":\"28057\",\"end\":\"28087\",\"attributes\":{\"n\":\"3.2.2.\"}},{\"start\":\"29724\",\"end\":\"29743\",\"attributes\":{\"n\":\"3.3.\"}},{\"start\":\"30040\",\"end\":\"30056\",\"attributes\":{\"n\":\"3.3.1.\"}},{\"start\":\"35274\",\"end\":\"35282\",\"attributes\":{\"n\":\"3.3.2.\"}},{\"start\":\"36942\",\"end\":\"36951\",\"attributes\":{\"n\":\"3.3.3.\"}},{\"start\":\"37539\",\"end\":\"37571\",\"attributes\":{\"n\":\"3.3.4.\"}},{\"start\":\"38978\",\"end\":\"39051\",\"attributes\":{\"n\":\"3.3.5.\"}},{\"start\":\"41857\",\"end\":\"41868\",\"attributes\":{\"n\":\"3.3.6.\"}},{\"start\":\"43808\",\"end\":\"43834\",\"attributes\":{\"n\":\"3.4.\"}},{\"start\":\"45332\",\"end\":\"45343\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"45598\",\"end\":\"45607\",\"attributes\":{\"n\":\"4.1.\"}},{\"start\":\"47692\",\"end\":\"47708\",\"attributes\":{\"n\":\"4.2.\"}},{\"start\":\"50280\",\"end\":\"50295\",\"attributes\":{\"n\":\"4.3.\"}},{\"start\":\"51696\",\"end\":\"51718\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"52384\",\"end\":\"52404\",\"attributes\":{\"n\":\"5.1.\"}},{\"start\":\"55569\",\"end\":\"55590\",\"attributes\":{\"n\":\"5.2.\"}},{\"start\":\"56967\",\"end\":\"56992\",\"attributes\":{\"n\":\"5.3.\"}},{\"start\":\"57525\",\"end\":\"57543\",\"attributes\":{\"n\":\"5.3.1.\"}},{\"start\":\"58044\",\"end\":\"58069\",\"attributes\":{\"n\":\"5.3.2.\"}},{\"start\":\"58806\",\"end\":\"58826\",\"attributes\":{\"n\":\"5.3.3.\"}},{\"start\":\"59827\",\"end\":\"59850\",\"attributes\":{\"n\":\"5.4.\"}},{\"start\":\"63132\",\"end\":\"63179\",\"attributes\":{\"n\":\"5.4.1.\"}},{\"start\":\"65145\",\"end\":\"65172\",\"attributes\":{\"n\":\"5.5.\"}},{\"start\":\"67366\",\"end\":\"67394\",\"attributes\":{\"n\":\"6.\"}},{\"start\":\"68731\",\"end\":\"68738\"},{\"start\":\"73583\",\"end\":\"73593\"},{\"start\":\"73678\",\"end\":\"73688\"},{\"start\":\"73795\",\"end\":\"73805\"},{\"start\":\"73882\",\"end\":\"73895\"},{\"start\":\"74304\",\"end\":\"74314\"},{\"start\":\"74346\",\"end\":\"74358\"},{\"start\":\"75423\",\"end\":\"75424\"},{\"start\":\"75445\",\"end\":\"75447\"},{\"start\":\"75747\",\"end\":\"75749\"},{\"start\":\"75939\",\"end\":\"75948\"},{\"start\":\"76582\",\"end\":\"76591\"}]", "table": "[{\"start\":\"76253\",\"end\":\"76580\"}]", "figure_caption": "[{\"start\":\"73595\",\"end\":\"73676\"},{\"start\":\"73690\",\"end\":\"73793\"},{\"start\":\"73807\",\"end\":\"73880\"},{\"start\":\"73897\",\"end\":\"74208\"},{\"start\":\"74211\",\"end\":\"74302\"},{\"start\":\"74316\",\"end\":\"74344\"},{\"start\":\"74361\",\"end\":\"74654\"},{\"start\":\"74657\",\"end\":\"74814\"},{\"start\":\"74817\",\"end\":\"75395\"},{\"start\":\"75398\",\"end\":\"75421\"},{\"start\":\"75425\",\"end\":\"75443\"},{\"start\":\"75448\",\"end\":\"75745\"},{\"start\":\"75750\",\"end\":\"75937\"},{\"start\":\"75950\",\"end\":\"76253\"},{\"start\":\"76593\",\"end\":\"76670\"}]", "figure_ref": "[{\"start\":\"23585\",\"end\":\"23593\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"26819\",\"end\":\"26827\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"27235\",\"end\":\"27243\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"28146\",\"end\":\"28154\"},{\"start\":\"28990\",\"end\":\"28998\"},{\"start\":\"30101\",\"end\":\"30109\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"33909\",\"end\":\"33917\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"34290\",\"end\":\"34299\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"35376\",\"end\":\"35384\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"40789\",\"end\":\"40797\"},{\"start\":\"42162\",\"end\":\"42170\"},{\"start\":\"43467\",\"end\":\"43475\"},{\"start\":\"59348\",\"end\":\"59356\"},{\"start\":\"60618\",\"end\":\"60626\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"60730\",\"end\":\"60740\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"60752\",\"end\":\"60768\"},{\"start\":\"61554\",\"end\":\"61563\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"63289\",\"end\":\"63298\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"64911\",\"end\":\"64919\"},{\"start\":\"65458\",\"end\":\"65466\"},{\"start\":\"66726\",\"end\":\"66734\"},{\"start\":\"68765\",\"end\":\"68773\"},{\"start\":\"70150\",\"end\":\"70159\",\"attributes\":{\"ref_id\":\"fig_1\"}}]", "bib_author_first_name": "[{\"start\":\"76996\",\"end\":\"77001\"},{\"start\":\"77010\",\"end\":\"77013\"},{\"start\":\"77377\",\"end\":\"77382\"},{\"start\":\"77383\",\"end\":\"77386\"},{\"start\":\"77391\",\"end\":\"77396\"},{\"start\":\"77397\",\"end\":\"77401\"},{\"start\":\"77409\",\"end\":\"77417\"},{\"start\":\"77418\",\"end\":\"77419\"},{\"start\":\"77764\",\"end\":\"77770\"},{\"start\":\"77779\",\"end\":\"77786\"},{\"start\":\"77796\",\"end\":\"77806\"},{\"start\":\"78224\",\"end\":\"78230\"},{\"start\":\"78239\",\"end\":\"78246\"},{\"start\":\"78256\",\"end\":\"78266\"},{\"start\":\"78558\",\"end\":\"78564\"},{\"start\":\"78573\",\"end\":\"78581\"},{\"start\":\"78590\",\"end\":\"78597\"},{\"start\":\"79052\",\"end\":\"79058\"},{\"start\":\"79067\",\"end\":\"79074\"},{\"start\":\"79084\",\"end\":\"79094\"},{\"start\":\"79429\",\"end\":\"79435\"},{\"start\":\"79444\",\"end\":\"79452\"},{\"start\":\"79461\",\"end\":\"79468\"},{\"start\":\"79478\",\"end\":\"79483\"},{\"start\":\"79736\",\"end\":\"79743\"},{\"start\":\"79754\",\"end\":\"79763\"},{\"start\":\"79769\",\"end\":\"79775\"},{\"start\":\"80152\",\"end\":\"80153\"},{\"start\":\"80161\",\"end\":\"80168\"},{\"start\":\"80169\",\"end\":\"80170\"},{\"start\":\"80182\",\"end\":\"80188\"},{\"start\":\"80198\",\"end\":\"80204\"},{\"start\":\"80212\",\"end\":\"80220\"},{\"start\":\"80239\",\"end\":\"80246\"},{\"start\":\"80257\",\"end\":\"80263\"},{\"start\":\"80276\",\"end\":\"80281\"},{\"start\":\"80293\",\"end\":\"80297\"},{\"start\":\"80306\",\"end\":\"80310\"},{\"start\":\"80717\",\"end\":\"80722\"},{\"start\":\"80732\",\"end\":\"80739\"},{\"start\":\"80749\",\"end\":\"80756\"},{\"start\":\"80769\",\"end\":\"80775\"},{\"start\":\"81046\",\"end\":\"81047\"},{\"start\":\"81280\",\"end\":\"81290\"},{\"start\":\"81303\",\"end\":\"81310\"},{\"start\":\"81736\",\"end\":\"81737\"},{\"start\":\"81770\",\"end\":\"81776\"},{\"start\":\"81792\",\"end\":\"81796\"},{\"start\":\"81797\",\"end\":\"81798\"},{\"start\":\"81817\",\"end\":\"81827\"},{\"start\":\"82187\",\"end\":\"82191\"},{\"start\":\"82199\",\"end\":\"82207\"},{\"start\":\"82217\",\"end\":\"82223\"},{\"start\":\"82231\",\"end\":\"82235\"},{\"start\":\"82676\",\"end\":\"82683\"},{\"start\":\"82696\",\"end\":\"82702\"},{\"start\":\"83079\",\"end\":\"83087\"},{\"start\":\"83096\",\"end\":\"83102\"},{\"start\":\"83112\",\"end\":\"83122\"},{\"start\":\"83535\",\"end\":\"83547\"},{\"start\":\"83559\",\"end\":\"83564\"},{\"start\":\"83575\",\"end\":\"83580\"},{\"start\":\"83948\",\"end\":\"83949\"},{\"start\":\"83956\",\"end\":\"83964\"},{\"start\":\"84198\",\"end\":\"84199\"},{\"start\":\"84687\",\"end\":\"84688\"},{\"start\":\"84696\",\"end\":\"84705\"},{\"start\":\"85127\",\"end\":\"85135\"},{\"start\":\"85141\",\"end\":\"85144\"},{\"start\":\"85145\",\"end\":\"85149\"},{\"start\":\"85570\",\"end\":\"85575\"},{\"start\":\"85586\",\"end\":\"85591\"},{\"start\":\"85598\",\"end\":\"85608\"},{\"start\":\"85923\",\"end\":\"85928\"},{\"start\":\"85939\",\"end\":\"85947\"},{\"start\":\"85955\",\"end\":\"85958\"},{\"start\":\"85969\",\"end\":\"85973\"},{\"start\":\"85974\",\"end\":\"85976\"},{\"start\":\"86245\",\"end\":\"86252\"},{\"start\":\"86265\",\"end\":\"86272\"},{\"start\":\"86643\",\"end\":\"86649\"},{\"start\":\"86658\",\"end\":\"86659\"},{\"start\":\"86680\",\"end\":\"86681\"},{\"start\":\"86691\",\"end\":\"86696\"},{\"start\":\"86704\",\"end\":\"86710\"},{\"start\":\"86711\",\"end\":\"86712\"},{\"start\":\"87192\",\"end\":\"87201\"},{\"start\":\"87210\",\"end\":\"87217\"},{\"start\":\"87652\",\"end\":\"87656\"},{\"start\":\"87667\",\"end\":\"87673\"},{\"start\":\"87680\",\"end\":\"87684\"},{\"start\":\"88149\",\"end\":\"88155\"},{\"start\":\"88163\",\"end\":\"88172\"},{\"start\":\"88588\",\"end\":\"88589\"},{\"start\":\"88806\",\"end\":\"88814\"},{\"start\":\"88825\",\"end\":\"88829\"},{\"start\":\"88830\",\"end\":\"88831\"},{\"start\":\"88840\",\"end\":\"88851\"},{\"start\":\"88860\",\"end\":\"88865\"},{\"start\":\"88874\",\"end\":\"88880\"},{\"start\":\"89103\",\"end\":\"89104\"},{\"start\":\"89115\",\"end\":\"89120\"},{\"start\":\"89488\",\"end\":\"89489\"},{\"start\":\"89500\",\"end\":\"89503\"},{\"start\":\"89864\",\"end\":\"89865\"},{\"start\":\"89874\",\"end\":\"89877\"},{\"start\":\"90195\",\"end\":\"90196\"},{\"start\":\"90205\",\"end\":\"90208\"},{\"start\":\"90638\",\"end\":\"90646\"},{\"start\":\"90657\",\"end\":\"90667\"},{\"start\":\"90680\",\"end\":\"90687\"},{\"start\":\"91170\",\"end\":\"91173\"},{\"start\":\"91672\",\"end\":\"91676\"},{\"start\":\"91684\",\"end\":\"91690\"},{\"start\":\"91959\",\"end\":\"91963\"},{\"start\":\"91974\",\"end\":\"91977\"},{\"start\":\"91989\",\"end\":\"91997\"},{\"start\":\"92535\",\"end\":\"92540\"},{\"start\":\"92545\",\"end\":\"92552\"},{\"start\":\"92558\",\"end\":\"92567\"},{\"start\":\"92984\",\"end\":\"92985\"},{\"start\":\"92993\",\"end\":\"92999\"},{\"start\":\"93254\",\"end\":\"93255\"},{\"start\":\"93265\",\"end\":\"93273\"},{\"start\":\"93274\",\"end\":\"93275\"},{\"start\":\"93285\",\"end\":\"93292\"},{\"start\":\"93621\",\"end\":\"93628\"},{\"start\":\"93926\",\"end\":\"93933\"},{\"start\":\"93943\",\"end\":\"93948\"},{\"start\":\"93955\",\"end\":\"93965\"},{\"start\":\"94270\",\"end\":\"94271\"},{\"start\":\"94526\",\"end\":\"94537\"},{\"start\":\"94546\",\"end\":\"94552\"},{\"start\":\"94562\",\"end\":\"94570\"},{\"start\":\"94576\",\"end\":\"94577\"},{\"start\":\"94587\",\"end\":\"94590\"},{\"start\":\"94591\",\"end\":\"94595\"},{\"start\":\"94606\",\"end\":\"94612\"},{\"start\":\"94622\",\"end\":\"94628\"},{\"start\":\"95164\",\"end\":\"95165\"},{\"start\":\"95175\",\"end\":\"95183\"},{\"start\":\"95184\",\"end\":\"95185\"},{\"start\":\"95468\",\"end\":\"95474\"},{\"start\":\"95484\",\"end\":\"95488\"},{\"start\":\"95499\",\"end\":\"95504\"},{\"start\":\"95514\",\"end\":\"95522\"},{\"start\":\"96073\",\"end\":\"96080\"},{\"start\":\"96090\",\"end\":\"96097\"},{\"start\":\"96105\",\"end\":\"96115\"},{\"start\":\"96501\",\"end\":\"96505\"},{\"start\":\"96709\",\"end\":\"96713\"},{\"start\":\"96718\",\"end\":\"96724\"},{\"start\":\"96733\",\"end\":\"96737\"},{\"start\":\"97101\",\"end\":\"97102\"},{\"start\":\"97113\",\"end\":\"97124\"},{\"start\":\"97356\",\"end\":\"97360\"},{\"start\":\"97372\",\"end\":\"97373\"},{\"start\":\"97382\",\"end\":\"97388\"},{\"start\":\"97400\",\"end\":\"97406\"},{\"start\":\"97696\",\"end\":\"97697\"},{\"start\":\"97698\",\"end\":\"97699\"},{\"start\":\"97707\",\"end\":\"97713\"},{\"start\":\"97714\",\"end\":\"97715\"},{\"start\":\"98250\",\"end\":\"98255\"},{\"start\":\"98524\",\"end\":\"98525\"},{\"start\":\"98536\",\"end\":\"98552\"},{\"start\":\"98876\",\"end\":\"98882\"},{\"start\":\"98894\",\"end\":\"98899\"},{\"start\":\"99219\",\"end\":\"99224\"},{\"start\":\"99233\",\"end\":\"99237\"},{\"start\":\"99512\",\"end\":\"99521\"},{\"start\":\"99530\",\"end\":\"99540\"},{\"start\":\"99548\",\"end\":\"99558\"},{\"start\":\"99571\",\"end\":\"99578\"},{\"start\":\"99971\",\"end\":\"99975\"},{\"start\":\"99990\",\"end\":\"99996\"},{\"start\":\"100011\",\"end\":\"100015\"},{\"start\":\"100023\",\"end\":\"100030\"},{\"start\":\"100546\",\"end\":\"100550\"},{\"start\":\"100565\",\"end\":\"100571\"},{\"start\":\"100584\",\"end\":\"100588\"},{\"start\":\"100589\",\"end\":\"100592\"},{\"start\":\"100606\",\"end\":\"100610\"},{\"start\":\"100621\",\"end\":\"100628\"},{\"start\":\"100629\",\"end\":\"100630\"},{\"start\":\"100956\",\"end\":\"100962\"},{\"start\":\"100975\",\"end\":\"100980\"},{\"start\":\"101443\",\"end\":\"101453\"},{\"start\":\"101464\",\"end\":\"101472\"},{\"start\":\"101819\",\"end\":\"101827\"},{\"start\":\"101839\",\"end\":\"101847\"},{\"start\":\"101854\",\"end\":\"101862\"},{\"start\":\"101871\",\"end\":\"101876\"},{\"start\":\"101884\",\"end\":\"101888\"},{\"start\":\"101900\",\"end\":\"101906\"},{\"start\":\"102289\",\"end\":\"102290\"},{\"start\":\"102297\",\"end\":\"102302\"},{\"start\":\"102309\",\"end\":\"102319\"},{\"start\":\"102624\",\"end\":\"102629\"},{\"start\":\"102642\",\"end\":\"102649\"},{\"start\":\"102660\",\"end\":\"102667\"},{\"start\":\"102678\",\"end\":\"102685\"},{\"start\":\"102694\",\"end\":\"102700\"},{\"start\":\"102706\",\"end\":\"102712\"},{\"start\":\"103115\",\"end\":\"103120\"},{\"start\":\"103133\",\"end\":\"103140\"},{\"start\":\"103148\",\"end\":\"103155\"},{\"start\":\"103156\",\"end\":\"103157\"},{\"start\":\"103168\",\"end\":\"103174\"},{\"start\":\"103179\",\"end\":\"103185\"},{\"start\":\"103194\",\"end\":\"103195\"},{\"start\":\"103196\",\"end\":\"103201\"},{\"start\":\"103648\",\"end\":\"103649\"},{\"start\":\"103657\",\"end\":\"103658\"},{\"start\":\"103673\",\"end\":\"103674\"},{\"start\":\"103682\",\"end\":\"103686\"},{\"start\":\"103700\",\"end\":\"103707\"},{\"start\":\"103708\",\"end\":\"103709\"},{\"start\":\"103985\",\"end\":\"103991\"},{\"start\":\"104002\",\"end\":\"104003\"},{\"start\":\"104012\",\"end\":\"104018\"},{\"start\":\"104026\",\"end\":\"104032\"},{\"start\":\"104043\",\"end\":\"104050\"},{\"start\":\"104535\",\"end\":\"104540\"},{\"start\":\"104547\",\"end\":\"104548\"},{\"start\":\"104554\",\"end\":\"104560\"},{\"start\":\"104876\",\"end\":\"104882\"},{\"start\":\"104887\",\"end\":\"104893\"},{\"start\":\"104898\",\"end\":\"104902\"},{\"start\":\"104913\",\"end\":\"104921\"},{\"start\":\"105297\",\"end\":\"105302\"},{\"start\":\"105765\",\"end\":\"105771\"},{\"start\":\"105778\",\"end\":\"105785\"},{\"start\":\"105791\",\"end\":\"105802\"},{\"start\":\"105811\",\"end\":\"105816\"},{\"start\":\"105822\",\"end\":\"105826\"},{\"start\":\"105837\",\"end\":\"105841\"},{\"start\":\"106282\",\"end\":\"106288\"},{\"start\":\"106297\",\"end\":\"106303\"},{\"start\":\"106312\",\"end\":\"106318\"},{\"start\":\"106332\",\"end\":\"106340\"},{\"start\":\"106349\",\"end\":\"106350\"},{\"start\":\"106359\",\"end\":\"106368\"},{\"start\":\"106369\",\"end\":\"106370\"},{\"start\":\"106849\",\"end\":\"106854\"},{\"start\":\"106862\",\"end\":\"106870\"},{\"start\":\"106876\",\"end\":\"106882\"},{\"start\":\"106892\",\"end\":\"106897\"}]", "bib_author_last_name": "[{\"start\":\"77002\",\"end\":\"77008\"},{\"start\":\"77014\",\"end\":\"77021\"},{\"start\":\"77387\",\"end\":\"77389\"},{\"start\":\"77402\",\"end\":\"77407\"},{\"start\":\"77420\",\"end\":\"77426\"},{\"start\":\"77771\",\"end\":\"77777\"},{\"start\":\"77787\",\"end\":\"77794\"},{\"start\":\"77807\",\"end\":\"77815\"},{\"start\":\"78231\",\"end\":\"78237\"},{\"start\":\"78247\",\"end\":\"78254\"},{\"start\":\"78267\",\"end\":\"78275\"},{\"start\":\"78565\",\"end\":\"78571\"},{\"start\":\"78582\",\"end\":\"78588\"},{\"start\":\"78598\",\"end\":\"78605\"},{\"start\":\"79059\",\"end\":\"79065\"},{\"start\":\"79075\",\"end\":\"79082\"},{\"start\":\"79095\",\"end\":\"79103\"},{\"start\":\"79436\",\"end\":\"79442\"},{\"start\":\"79453\",\"end\":\"79459\"},{\"start\":\"79469\",\"end\":\"79476\"},{\"start\":\"79484\",\"end\":\"79489\"},{\"start\":\"79744\",\"end\":\"79752\"},{\"start\":\"79764\",\"end\":\"79767\"},{\"start\":\"79776\",\"end\":\"79782\"},{\"start\":\"80154\",\"end\":\"80159\"},{\"start\":\"80171\",\"end\":\"80180\"},{\"start\":\"80189\",\"end\":\"80196\"},{\"start\":\"80205\",\"end\":\"80210\"},{\"start\":\"80221\",\"end\":\"80237\"},{\"start\":\"80247\",\"end\":\"80255\"},{\"start\":\"80264\",\"end\":\"80274\"},{\"start\":\"80282\",\"end\":\"80291\"},{\"start\":\"80298\",\"end\":\"80304\"},{\"start\":\"80311\",\"end\":\"80318\"},{\"start\":\"80320\",\"end\":\"80328\"},{\"start\":\"80723\",\"end\":\"80730\"},{\"start\":\"80740\",\"end\":\"80747\"},{\"start\":\"80757\",\"end\":\"80767\"},{\"start\":\"80776\",\"end\":\"80780\"},{\"start\":\"80782\",\"end\":\"80788\"},{\"start\":\"81048\",\"end\":\"81059\"},{\"start\":\"81061\",\"end\":\"81067\"},{\"start\":\"81291\",\"end\":\"81301\"},{\"start\":\"81311\",\"end\":\"81319\"},{\"start\":\"81738\",\"end\":\"81745\"},{\"start\":\"81747\",\"end\":\"81756\"},{\"start\":\"81758\",\"end\":\"81768\"},{\"start\":\"81777\",\"end\":\"81780\"},{\"start\":\"81782\",\"end\":\"81790\"},{\"start\":\"81799\",\"end\":\"81815\"},{\"start\":\"81828\",\"end\":\"81833\"},{\"start\":\"81835\",\"end\":\"81842\"},{\"start\":\"82192\",\"end\":\"82197\"},{\"start\":\"82208\",\"end\":\"82215\"},{\"start\":\"82224\",\"end\":\"82229\"},{\"start\":\"82236\",\"end\":\"82241\"},{\"start\":\"82684\",\"end\":\"82694\"},{\"start\":\"82703\",\"end\":\"82709\"},{\"start\":\"83088\",\"end\":\"83094\"},{\"start\":\"83103\",\"end\":\"83110\"},{\"start\":\"83123\",\"end\":\"83130\"},{\"start\":\"83132\",\"end\":\"83140\"},{\"start\":\"83548\",\"end\":\"83557\"},{\"start\":\"83565\",\"end\":\"83573\"},{\"start\":\"83581\",\"end\":\"83585\"},{\"start\":\"83950\",\"end\":\"83954\"},{\"start\":\"83965\",\"end\":\"83971\"},{\"start\":\"83973\",\"end\":\"83977\"},{\"start\":\"84200\",\"end\":\"84207\"},{\"start\":\"84209\",\"end\":\"84216\"},{\"start\":\"84452\",\"end\":\"84467\"},{\"start\":\"84689\",\"end\":\"84694\"},{\"start\":\"84706\",\"end\":\"84713\"},{\"start\":\"84715\",\"end\":\"84722\"},{\"start\":\"85136\",\"end\":\"85139\"},{\"start\":\"85150\",\"end\":\"85157\"},{\"start\":\"85576\",\"end\":\"85584\"},{\"start\":\"85592\",\"end\":\"85596\"},{\"start\":\"85609\",\"end\":\"85617\"},{\"start\":\"85929\",\"end\":\"85937\"},{\"start\":\"85948\",\"end\":\"85953\"},{\"start\":\"85959\",\"end\":\"85967\"},{\"start\":\"85977\",\"end\":\"85982\"},{\"start\":\"86253\",\"end\":\"86263\"},{\"start\":\"86273\",\"end\":\"86280\"},{\"start\":\"86650\",\"end\":\"86656\"},{\"start\":\"86660\",\"end\":\"86666\"},{\"start\":\"86668\",\"end\":\"86678\"},{\"start\":\"86682\",\"end\":\"86689\"},{\"start\":\"86697\",\"end\":\"86702\"},{\"start\":\"86713\",\"end\":\"86720\"},{\"start\":\"86722\",\"end\":\"86726\"},{\"start\":\"87202\",\"end\":\"87208\"},{\"start\":\"87218\",\"end\":\"87225\"},{\"start\":\"87657\",\"end\":\"87665\"},{\"start\":\"87674\",\"end\":\"87678\"},{\"start\":\"87685\",\"end\":\"87693\"},{\"start\":\"88156\",\"end\":\"88161\"},{\"start\":\"88173\",\"end\":\"88180\"},{\"start\":\"88590\",\"end\":\"88596\"},{\"start\":\"88598\",\"end\":\"88608\"},{\"start\":\"88815\",\"end\":\"88823\"},{\"start\":\"88832\",\"end\":\"88838\"},{\"start\":\"88852\",\"end\":\"88858\"},{\"start\":\"88866\",\"end\":\"88872\"},{\"start\":\"88881\",\"end\":\"88888\"},{\"start\":\"89105\",\"end\":\"89113\"},{\"start\":\"89121\",\"end\":\"89127\"},{\"start\":\"89129\",\"end\":\"89131\"},{\"start\":\"89490\",\"end\":\"89498\"},{\"start\":\"89504\",\"end\":\"89510\"},{\"start\":\"89512\",\"end\":\"89519\"},{\"start\":\"89866\",\"end\":\"89872\"},{\"start\":\"89878\",\"end\":\"89882\"},{\"start\":\"89884\",\"end\":\"89891\"},{\"start\":\"90197\",\"end\":\"90203\"},{\"start\":\"90209\",\"end\":\"90213\"},{\"start\":\"90215\",\"end\":\"90222\"},{\"start\":\"90647\",\"end\":\"90655\"},{\"start\":\"90668\",\"end\":\"90678\"},{\"start\":\"90688\",\"end\":\"90696\"},{\"start\":\"91174\",\"end\":\"91180\"},{\"start\":\"91677\",\"end\":\"91682\"},{\"start\":\"91691\",\"end\":\"91697\"},{\"start\":\"91964\",\"end\":\"91972\"},{\"start\":\"91978\",\"end\":\"91987\"},{\"start\":\"91998\",\"end\":\"92007\"},{\"start\":\"92541\",\"end\":\"92543\"},{\"start\":\"92553\",\"end\":\"92556\"},{\"start\":\"92568\",\"end\":\"92570\"},{\"start\":\"92986\",\"end\":\"92991\"},{\"start\":\"93000\",\"end\":\"93009\"},{\"start\":\"93011\",\"end\":\"93018\"},{\"start\":\"93256\",\"end\":\"93263\"},{\"start\":\"93276\",\"end\":\"93283\"},{\"start\":\"93293\",\"end\":\"93297\"},{\"start\":\"93299\",\"end\":\"93307\"},{\"start\":\"93629\",\"end\":\"93636\"},{\"start\":\"93934\",\"end\":\"93941\"},{\"start\":\"93949\",\"end\":\"93953\"},{\"start\":\"93966\",\"end\":\"93974\"},{\"start\":\"94272\",\"end\":\"94276\"},{\"start\":\"94278\",\"end\":\"94282\"},{\"start\":\"94538\",\"end\":\"94544\"},{\"start\":\"94553\",\"end\":\"94560\"},{\"start\":\"94571\",\"end\":\"94574\"},{\"start\":\"94578\",\"end\":\"94585\"},{\"start\":\"94596\",\"end\":\"94604\"},{\"start\":\"94613\",\"end\":\"94620\"},{\"start\":\"94629\",\"end\":\"94635\"},{\"start\":\"94637\",\"end\":\"94642\"},{\"start\":\"95166\",\"end\":\"95173\"},{\"start\":\"95186\",\"end\":\"95190\"},{\"start\":\"95192\",\"end\":\"95198\"},{\"start\":\"95475\",\"end\":\"95482\"},{\"start\":\"95489\",\"end\":\"95497\"},{\"start\":\"95505\",\"end\":\"95512\"},{\"start\":\"95523\",\"end\":\"95531\"},{\"start\":\"95995\",\"end\":\"96003\"},{\"start\":\"96081\",\"end\":\"96088\"},{\"start\":\"96098\",\"end\":\"96103\"},{\"start\":\"96116\",\"end\":\"96123\"},{\"start\":\"96506\",\"end\":\"96514\"},{\"start\":\"96714\",\"end\":\"96716\"},{\"start\":\"96725\",\"end\":\"96731\"},{\"start\":\"96738\",\"end\":\"96742\"},{\"start\":\"97103\",\"end\":\"97111\"},{\"start\":\"97125\",\"end\":\"97132\"},{\"start\":\"97134\",\"end\":\"97139\"},{\"start\":\"97361\",\"end\":\"97370\"},{\"start\":\"97374\",\"end\":\"97380\"},{\"start\":\"97389\",\"end\":\"97398\"},{\"start\":\"97407\",\"end\":\"97412\"},{\"start\":\"97414\",\"end\":\"97419\"},{\"start\":\"97675\",\"end\":\"97694\"},{\"start\":\"97700\",\"end\":\"97705\"},{\"start\":\"97716\",\"end\":\"97724\"},{\"start\":\"97726\",\"end\":\"97736\"},{\"start\":\"98256\",\"end\":\"98266\"},{\"start\":\"98526\",\"end\":\"98534\"},{\"start\":\"98553\",\"end\":\"98557\"},{\"start\":\"98883\",\"end\":\"98892\"},{\"start\":\"98900\",\"end\":\"98904\"},{\"start\":\"99225\",\"end\":\"99231\"},{\"start\":\"99238\",\"end\":\"99244\"},{\"start\":\"99522\",\"end\":\"99528\"},{\"start\":\"99541\",\"end\":\"99546\"},{\"start\":\"99559\",\"end\":\"99569\"},{\"start\":\"99579\",\"end\":\"99587\"},{\"start\":\"99976\",\"end\":\"99988\"},{\"start\":\"99997\",\"end\":\"100009\"},{\"start\":\"100016\",\"end\":\"100021\"},{\"start\":\"100031\",\"end\":\"100039\"},{\"start\":\"100041\",\"end\":\"100050\"},{\"start\":\"100551\",\"end\":\"100563\"},{\"start\":\"100572\",\"end\":\"100582\"},{\"start\":\"100593\",\"end\":\"100604\"},{\"start\":\"100611\",\"end\":\"100619\"},{\"start\":\"100631\",\"end\":\"100640\"},{\"start\":\"100963\",\"end\":\"100973\"},{\"start\":\"100981\",\"end\":\"100990\"},{\"start\":\"101454\",\"end\":\"101462\"},{\"start\":\"101473\",\"end\":\"101480\"},{\"start\":\"101828\",\"end\":\"101837\"},{\"start\":\"101848\",\"end\":\"101852\"},{\"start\":\"101863\",\"end\":\"101869\"},{\"start\":\"101877\",\"end\":\"101882\"},{\"start\":\"101889\",\"end\":\"101898\"},{\"start\":\"101907\",\"end\":\"101912\"},{\"start\":\"102291\",\"end\":\"102295\"},{\"start\":\"102303\",\"end\":\"102307\"},{\"start\":\"102320\",\"end\":\"102327\"},{\"start\":\"102329\",\"end\":\"102337\"},{\"start\":\"102630\",\"end\":\"102640\"},{\"start\":\"102650\",\"end\":\"102658\"},{\"start\":\"102668\",\"end\":\"102676\"},{\"start\":\"102686\",\"end\":\"102692\"},{\"start\":\"102701\",\"end\":\"102704\"},{\"start\":\"102713\",\"end\":\"102719\"},{\"start\":\"103121\",\"end\":\"103131\"},{\"start\":\"103141\",\"end\":\"103146\"},{\"start\":\"103158\",\"end\":\"103166\"},{\"start\":\"103175\",\"end\":\"103177\"},{\"start\":\"103186\",\"end\":\"103192\"},{\"start\":\"103202\",\"end\":\"103207\"},{\"start\":\"103650\",\"end\":\"103655\"},{\"start\":\"103659\",\"end\":\"103671\"},{\"start\":\"103675\",\"end\":\"103680\"},{\"start\":\"103687\",\"end\":\"103698\"},{\"start\":\"103710\",\"end\":\"103716\"},{\"start\":\"103718\",\"end\":\"103727\"},{\"start\":\"103992\",\"end\":\"104000\"},{\"start\":\"104004\",\"end\":\"104010\"},{\"start\":\"104019\",\"end\":\"104024\"},{\"start\":\"104033\",\"end\":\"104041\"},{\"start\":\"104051\",\"end\":\"104057\"},{\"start\":\"104059\",\"end\":\"104066\"},{\"start\":\"104541\",\"end\":\"104545\"},{\"start\":\"104549\",\"end\":\"104552\"},{\"start\":\"104561\",\"end\":\"104567\"},{\"start\":\"104569\",\"end\":\"104576\"},{\"start\":\"104883\",\"end\":\"104885\"},{\"start\":\"104894\",\"end\":\"104896\"},{\"start\":\"104903\",\"end\":\"104911\"},{\"start\":\"104922\",\"end\":\"104929\"},{\"start\":\"105303\",\"end\":\"105311\"},{\"start\":\"105313\",\"end\":\"105325\"},{\"start\":\"105772\",\"end\":\"105776\"},{\"start\":\"105786\",\"end\":\"105789\"},{\"start\":\"105803\",\"end\":\"105809\"},{\"start\":\"105817\",\"end\":\"105820\"},{\"start\":\"105827\",\"end\":\"105835\"},{\"start\":\"105842\",\"end\":\"105850\"},{\"start\":\"106289\",\"end\":\"106295\"},{\"start\":\"106304\",\"end\":\"106310\"},{\"start\":\"106319\",\"end\":\"106330\"},{\"start\":\"106341\",\"end\":\"106347\"},{\"start\":\"106351\",\"end\":\"106357\"},{\"start\":\"106371\",\"end\":\"106384\"},{\"start\":\"106386\",\"end\":\"106391\"},{\"start\":\"106855\",\"end\":\"106860\"},{\"start\":\"106871\",\"end\":\"106874\"},{\"start\":\"106883\",\"end\":\"106890\"},{\"start\":\"106898\",\"end\":\"106902\"}]", "bib_entry": "[{\"start\":\"76955\",\"end\":\"77354\",\"attributes\":{\"matched_paper_id\":\"15483870\",\"id\":\"b0\"}},{\"start\":\"77356\",\"end\":\"77652\",\"attributes\":{\"matched_paper_id\":\"8236317\",\"id\":\"b1\"}},{\"start\":\"77654\",\"end\":\"78163\",\"attributes\":{\"matched_paper_id\":\"2363600\",\"id\":\"b2\"}},{\"start\":\"78165\",\"end\":\"78473\",\"attributes\":{\"matched_paper_id\":\"27146070\",\"id\":\"b3\"}},{\"start\":\"78475\",\"end\":\"79006\",\"attributes\":{\"matched_paper_id\":\"44137413\",\"id\":\"b4\"}},{\"start\":\"79008\",\"end\":\"79376\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"79378\",\"end\":\"79663\",\"attributes\":{\"matched_paper_id\":\"209516216\",\"id\":\"b6\"}},{\"start\":\"79665\",\"end\":\"80150\",\"attributes\":{\"matched_paper_id\":\"11212020\",\"id\":\"b7\"}},{\"start\":\"80152\",\"end\":\"80715\",\"attributes\":{\"id\":\"b8\",\"doi\":\"arXiv:1806.01261\"}},{\"start\":\"80717\",\"end\":\"81044\",\"attributes\":{\"id\":\"b9\",\"doi\":\"arXiv:1901.01343\"}},{\"start\":\"81046\",\"end\":\"81198\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"81200\",\"end\":\"81687\",\"attributes\":{\"matched_paper_id\":\"4630420\",\"id\":\"b11\"}},{\"start\":\"81689\",\"end\":\"82125\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"82127\",\"end\":\"82603\",\"attributes\":{\"matched_paper_id\":\"17682909\",\"id\":\"b13\"}},{\"start\":\"82605\",\"end\":\"83045\",\"attributes\":{\"matched_paper_id\":\"173188008\",\"id\":\"b14\"}},{\"start\":\"83047\",\"end\":\"83472\",\"attributes\":{\"matched_paper_id\":\"18448541\",\"id\":\"b15\"}},{\"start\":\"83474\",\"end\":\"83876\",\"attributes\":{\"matched_paper_id\":\"9651365\",\"id\":\"b16\"}},{\"start\":\"83878\",\"end\":\"84196\",\"attributes\":{\"matched_paper_id\":\"5719990\",\"id\":\"b17\"}},{\"start\":\"84198\",\"end\":\"84423\",\"attributes\":{\"id\":\"b18\",\"doi\":\"arXiv:1101.5211\"}},{\"start\":\"84425\",\"end\":\"84638\",\"attributes\":{\"matched_paper_id\":\"2763403\",\"id\":\"b19\"}},{\"start\":\"84640\",\"end\":\"85066\",\"attributes\":{\"matched_paper_id\":\"30443043\",\"id\":\"b20\"}},{\"start\":\"85068\",\"end\":\"85504\",\"attributes\":{\"matched_paper_id\":\"70349949\",\"id\":\"b21\"}},{\"start\":\"85506\",\"end\":\"85854\",\"attributes\":{\"matched_paper_id\":\"6197973\",\"id\":\"b22\"}},{\"start\":\"85856\",\"end\":\"86216\",\"attributes\":{\"matched_paper_id\":\"376148\",\"id\":\"b23\"}},{\"start\":\"86218\",\"end\":\"86595\",\"attributes\":{\"matched_paper_id\":\"17011026\",\"id\":\"b24\"}},{\"start\":\"86597\",\"end\":\"87100\",\"attributes\":{\"matched_paper_id\":\"9665943\",\"id\":\"b25\"}},{\"start\":\"87102\",\"end\":\"87599\",\"attributes\":{\"matched_paper_id\":\"6536466\",\"id\":\"b26\"}},{\"start\":\"87601\",\"end\":\"88053\",\"attributes\":{\"matched_paper_id\":\"4755450\",\"id\":\"b27\"}},{\"start\":\"88055\",\"end\":\"88553\",\"attributes\":{\"matched_paper_id\":\"5808102\",\"id\":\"b28\"}},{\"start\":\"88555\",\"end\":\"88765\",\"attributes\":{\"matched_paper_id\":\"814743\",\"id\":\"b29\"}},{\"start\":\"88767\",\"end\":\"89057\",\"attributes\":{\"id\":\"b30\"}},{\"start\":\"89059\",\"end\":\"89453\",\"attributes\":{\"matched_paper_id\":\"6628106\",\"id\":\"b31\"}},{\"start\":\"89455\",\"end\":\"89829\",\"attributes\":{\"matched_paper_id\":\"216078090\",\"id\":\"b32\"}},{\"start\":\"89831\",\"end\":\"90127\",\"attributes\":{\"matched_paper_id\":\"14249137\",\"id\":\"b33\"}},{\"start\":\"90129\",\"end\":\"90562\",\"attributes\":{\"matched_paper_id\":\"3144218\",\"id\":\"b34\"}},{\"start\":\"90564\",\"end\":\"91081\",\"attributes\":{\"matched_paper_id\":\"67855539\",\"id\":\"b35\"}},{\"start\":\"91083\",\"end\":\"91560\",\"attributes\":{\"matched_paper_id\":\"2702042\",\"id\":\"b36\"}},{\"start\":\"91562\",\"end\":\"91872\",\"attributes\":{\"id\":\"b37\"}},{\"start\":\"91874\",\"end\":\"92453\",\"attributes\":{\"matched_paper_id\":\"9077635\",\"id\":\"b38\"}},{\"start\":\"92455\",\"end\":\"92909\",\"attributes\":{\"matched_paper_id\":\"11118105\",\"id\":\"b39\"}},{\"start\":\"92911\",\"end\":\"93229\",\"attributes\":{\"matched_paper_id\":\"10171536\",\"id\":\"b40\"}},{\"start\":\"93231\",\"end\":\"93556\",\"attributes\":{\"matched_paper_id\":\"3335695\",\"id\":\"b41\"}},{\"start\":\"93558\",\"end\":\"93849\",\"attributes\":{\"matched_paper_id\":\"17486263\",\"id\":\"b42\"}},{\"start\":\"93851\",\"end\":\"94228\",\"attributes\":{\"matched_paper_id\":\"12370239\",\"id\":\"b43\"}},{\"start\":\"94230\",\"end\":\"94456\",\"attributes\":{\"matched_paper_id\":\"11846408\",\"id\":\"b44\"}},{\"start\":\"94458\",\"end\":\"95079\",\"attributes\":{\"matched_paper_id\":\"52919090\",\"id\":\"b45\"}},{\"start\":\"95081\",\"end\":\"95424\",\"attributes\":{\"matched_paper_id\":\"17947141\",\"id\":\"b46\"}},{\"start\":\"95426\",\"end\":\"95991\",\"attributes\":{\"matched_paper_id\":\"7218366\",\"id\":\"b47\"}},{\"start\":\"95993\",\"end\":\"96020\",\"attributes\":{\"id\":\"b48\"}},{\"start\":\"96022\",\"end\":\"96473\",\"attributes\":{\"matched_paper_id\":\"1430801\",\"id\":\"b49\"}},{\"start\":\"96475\",\"end\":\"96671\",\"attributes\":{\"matched_paper_id\":\"14049040\",\"id\":\"b50\"}},{\"start\":\"96673\",\"end\":\"97058\",\"attributes\":{\"matched_paper_id\":\"155093091\",\"id\":\"b51\"}},{\"start\":\"97060\",\"end\":\"97314\",\"attributes\":{\"matched_paper_id\":\"6240141\",\"id\":\"b52\"}},{\"start\":\"97316\",\"end\":\"97617\",\"attributes\":{\"matched_paper_id\":\"8539222\",\"id\":\"b53\"}},{\"start\":\"97619\",\"end\":\"98179\",\"attributes\":{\"matched_paper_id\":\"3948366\",\"id\":\"b54\"}},{\"start\":\"98181\",\"end\":\"98424\",\"attributes\":{\"id\":\"b55\"}},{\"start\":\"98426\",\"end\":\"98781\",\"attributes\":{\"matched_paper_id\":\"2032766\",\"id\":\"b56\"}},{\"start\":\"98783\",\"end\":\"99150\",\"attributes\":{\"id\":\"b57\"}},{\"start\":\"99152\",\"end\":\"99465\",\"attributes\":{\"matched_paper_id\":\"7406564\",\"id\":\"b58\"}},{\"start\":\"99467\",\"end\":\"99914\",\"attributes\":{\"matched_paper_id\":\"53303554\",\"id\":\"b59\"}},{\"start\":\"99916\",\"end\":\"100511\",\"attributes\":{\"matched_paper_id\":\"17557614\",\"id\":\"b60\"}},{\"start\":\"100513\",\"end\":\"100877\",\"attributes\":{\"matched_paper_id\":\"1797579\",\"id\":\"b61\"}},{\"start\":\"100879\",\"end\":\"101376\",\"attributes\":{\"matched_paper_id\":\"17648673\",\"id\":\"b62\"}},{\"start\":\"101378\",\"end\":\"101718\",\"attributes\":{\"matched_paper_id\":\"5942593\",\"id\":\"b63\"}},{\"start\":\"101720\",\"end\":\"102239\",\"attributes\":{\"matched_paper_id\":\"12471299\",\"id\":\"b64\"}},{\"start\":\"102241\",\"end\":\"102596\",\"attributes\":{\"matched_paper_id\":\"53749157\",\"id\":\"b65\"}},{\"start\":\"102598\",\"end\":\"103093\",\"attributes\":{\"matched_paper_id\":\"3292002\",\"id\":\"b66\"}},{\"start\":\"103095\",\"end\":\"103631\",\"attributes\":{\"matched_paper_id\":\"52877454\",\"id\":\"b67\"}},{\"start\":\"103633\",\"end\":\"103929\",\"attributes\":{\"matched_paper_id\":\"1729012\",\"id\":\"b68\"}},{\"start\":\"103931\",\"end\":\"104449\",\"attributes\":{\"matched_paper_id\":\"59292002\",\"id\":\"b69\"}},{\"start\":\"104451\",\"end\":\"104833\",\"attributes\":{\"matched_paper_id\":\"2596211\",\"id\":\"b70\"}},{\"start\":\"104835\",\"end\":\"105275\",\"attributes\":{\"matched_paper_id\":\"52895589\",\"id\":\"b71\"}},{\"start\":\"105277\",\"end\":\"105691\",\"attributes\":{\"matched_paper_id\":\"207227372\",\"id\":\"b72\"}},{\"start\":\"105693\",\"end\":\"106269\",\"attributes\":{\"matched_paper_id\":\"49420315\",\"id\":\"b73\"}},{\"start\":\"106271\",\"end\":\"106780\",\"attributes\":{\"matched_paper_id\":\"4870287\",\"id\":\"b74\"}},{\"start\":\"106782\",\"end\":\"107246\",\"attributes\":{\"matched_paper_id\":\"4770492\",\"id\":\"b75\"}}]", "bib_title": "[{\"start\":\"76955\",\"end\":\"76994\"},{\"start\":\"77356\",\"end\":\"77375\"},{\"start\":\"77654\",\"end\":\"77762\"},{\"start\":\"78165\",\"end\":\"78222\"},{\"start\":\"78475\",\"end\":\"78556\"},{\"start\":\"79008\",\"end\":\"79050\"},{\"start\":\"79378\",\"end\":\"79427\"},{\"start\":\"79665\",\"end\":\"79734\"},{\"start\":\"81200\",\"end\":\"81278\"},{\"start\":\"82127\",\"end\":\"82185\"},{\"start\":\"82605\",\"end\":\"82674\"},{\"start\":\"83047\",\"end\":\"83077\"},{\"start\":\"83474\",\"end\":\"83533\"},{\"start\":\"83878\",\"end\":\"83946\"},{\"start\":\"84425\",\"end\":\"84450\"},{\"start\":\"84640\",\"end\":\"84685\"},{\"start\":\"85068\",\"end\":\"85125\"},{\"start\":\"85506\",\"end\":\"85568\"},{\"start\":\"85856\",\"end\":\"85921\"},{\"start\":\"86218\",\"end\":\"86243\"},{\"start\":\"86597\",\"end\":\"86641\"},{\"start\":\"87102\",\"end\":\"87190\"},{\"start\":\"87601\",\"end\":\"87650\"},{\"start\":\"88055\",\"end\":\"88147\"},{\"start\":\"88555\",\"end\":\"88586\"},{\"start\":\"89059\",\"end\":\"89101\"},{\"start\":\"89455\",\"end\":\"89486\"},{\"start\":\"89831\",\"end\":\"89862\"},{\"start\":\"90129\",\"end\":\"90193\"},{\"start\":\"90564\",\"end\":\"90636\"},{\"start\":\"91083\",\"end\":\"91168\"},{\"start\":\"91874\",\"end\":\"91957\"},{\"start\":\"92455\",\"end\":\"92533\"},{\"start\":\"92911\",\"end\":\"92982\"},{\"start\":\"93231\",\"end\":\"93252\"},{\"start\":\"93558\",\"end\":\"93619\"},{\"start\":\"93851\",\"end\":\"93924\"},{\"start\":\"94230\",\"end\":\"94268\"},{\"start\":\"94458\",\"end\":\"94524\"},{\"start\":\"95081\",\"end\":\"95162\"},{\"start\":\"95426\",\"end\":\"95466\"},{\"start\":\"96022\",\"end\":\"96071\"},{\"start\":\"96475\",\"end\":\"96499\"},{\"start\":\"96673\",\"end\":\"96707\"},{\"start\":\"97060\",\"end\":\"97099\"},{\"start\":\"97316\",\"end\":\"97354\"},{\"start\":\"97619\",\"end\":\"97673\"},{\"start\":\"98426\",\"end\":\"98522\"},{\"start\":\"98783\",\"end\":\"98874\"},{\"start\":\"99152\",\"end\":\"99217\"},{\"start\":\"99467\",\"end\":\"99510\"},{\"start\":\"99916\",\"end\":\"99969\"},{\"start\":\"100513\",\"end\":\"100544\"},{\"start\":\"100879\",\"end\":\"100954\"},{\"start\":\"101378\",\"end\":\"101441\"},{\"start\":\"101720\",\"end\":\"101817\"},{\"start\":\"102241\",\"end\":\"102287\"},{\"start\":\"102598\",\"end\":\"102622\"},{\"start\":\"103095\",\"end\":\"103113\"},{\"start\":\"103633\",\"end\":\"103646\"},{\"start\":\"103931\",\"end\":\"103983\"},{\"start\":\"104451\",\"end\":\"104533\"},{\"start\":\"104835\",\"end\":\"104874\"},{\"start\":\"105277\",\"end\":\"105295\"},{\"start\":\"105693\",\"end\":\"105763\"},{\"start\":\"106271\",\"end\":\"106280\"},{\"start\":\"106782\",\"end\":\"106847\"}]", "bib_author": "[{\"start\":\"76996\",\"end\":\"77010\"},{\"start\":\"77010\",\"end\":\"77023\"},{\"start\":\"77377\",\"end\":\"77391\"},{\"start\":\"77391\",\"end\":\"77409\"},{\"start\":\"77409\",\"end\":\"77428\"},{\"start\":\"77764\",\"end\":\"77779\"},{\"start\":\"77779\",\"end\":\"77796\"},{\"start\":\"77796\",\"end\":\"77817\"},{\"start\":\"78224\",\"end\":\"78239\"},{\"start\":\"78239\",\"end\":\"78256\"},{\"start\":\"78256\",\"end\":\"78277\"},{\"start\":\"78558\",\"end\":\"78573\"},{\"start\":\"78573\",\"end\":\"78590\"},{\"start\":\"78590\",\"end\":\"78607\"},{\"start\":\"79052\",\"end\":\"79067\"},{\"start\":\"79067\",\"end\":\"79084\"},{\"start\":\"79084\",\"end\":\"79105\"},{\"start\":\"79429\",\"end\":\"79444\"},{\"start\":\"79444\",\"end\":\"79461\"},{\"start\":\"79461\",\"end\":\"79478\"},{\"start\":\"79478\",\"end\":\"79491\"},{\"start\":\"79736\",\"end\":\"79754\"},{\"start\":\"79754\",\"end\":\"79769\"},{\"start\":\"79769\",\"end\":\"79784\"},{\"start\":\"80152\",\"end\":\"80161\"},{\"start\":\"80161\",\"end\":\"80182\"},{\"start\":\"80182\",\"end\":\"80198\"},{\"start\":\"80198\",\"end\":\"80212\"},{\"start\":\"80212\",\"end\":\"80239\"},{\"start\":\"80239\",\"end\":\"80257\"},{\"start\":\"80257\",\"end\":\"80276\"},{\"start\":\"80276\",\"end\":\"80293\"},{\"start\":\"80293\",\"end\":\"80306\"},{\"start\":\"80306\",\"end\":\"80320\"},{\"start\":\"80320\",\"end\":\"80330\"},{\"start\":\"80717\",\"end\":\"80732\"},{\"start\":\"80732\",\"end\":\"80749\"},{\"start\":\"80749\",\"end\":\"80769\"},{\"start\":\"80769\",\"end\":\"80782\"},{\"start\":\"80782\",\"end\":\"80790\"},{\"start\":\"81046\",\"end\":\"81061\"},{\"start\":\"81061\",\"end\":\"81069\"},{\"start\":\"81280\",\"end\":\"81303\"},{\"start\":\"81303\",\"end\":\"81321\"},{\"start\":\"81736\",\"end\":\"81747\"},{\"start\":\"81747\",\"end\":\"81758\"},{\"start\":\"81758\",\"end\":\"81770\"},{\"start\":\"81770\",\"end\":\"81782\"},{\"start\":\"81782\",\"end\":\"81792\"},{\"start\":\"81792\",\"end\":\"81817\"},{\"start\":\"81817\",\"end\":\"81835\"},{\"start\":\"81835\",\"end\":\"81844\"},{\"start\":\"82187\",\"end\":\"82199\"},{\"start\":\"82199\",\"end\":\"82217\"},{\"start\":\"82217\",\"end\":\"82231\"},{\"start\":\"82231\",\"end\":\"82243\"},{\"start\":\"82676\",\"end\":\"82696\"},{\"start\":\"82696\",\"end\":\"82711\"},{\"start\":\"83079\",\"end\":\"83096\"},{\"start\":\"83096\",\"end\":\"83112\"},{\"start\":\"83112\",\"end\":\"83132\"},{\"start\":\"83132\",\"end\":\"83142\"},{\"start\":\"83535\",\"end\":\"83559\"},{\"start\":\"83559\",\"end\":\"83575\"},{\"start\":\"83575\",\"end\":\"83587\"},{\"start\":\"83948\",\"end\":\"83956\"},{\"start\":\"83956\",\"end\":\"83973\"},{\"start\":\"83973\",\"end\":\"83979\"},{\"start\":\"84198\",\"end\":\"84209\"},{\"start\":\"84209\",\"end\":\"84218\"},{\"start\":\"84452\",\"end\":\"84469\"},{\"start\":\"84687\",\"end\":\"84696\"},{\"start\":\"84696\",\"end\":\"84715\"},{\"start\":\"84715\",\"end\":\"84724\"},{\"start\":\"85127\",\"end\":\"85141\"},{\"start\":\"85141\",\"end\":\"85159\"},{\"start\":\"85570\",\"end\":\"85586\"},{\"start\":\"85586\",\"end\":\"85598\"},{\"start\":\"85598\",\"end\":\"85619\"},{\"start\":\"85923\",\"end\":\"85939\"},{\"start\":\"85939\",\"end\":\"85955\"},{\"start\":\"85955\",\"end\":\"85969\"},{\"start\":\"85969\",\"end\":\"85984\"},{\"start\":\"86245\",\"end\":\"86265\"},{\"start\":\"86265\",\"end\":\"86282\"},{\"start\":\"86643\",\"end\":\"86658\"},{\"start\":\"86658\",\"end\":\"86668\"},{\"start\":\"86668\",\"end\":\"86680\"},{\"start\":\"86680\",\"end\":\"86691\"},{\"start\":\"86691\",\"end\":\"86704\"},{\"start\":\"86704\",\"end\":\"86722\"},{\"start\":\"86722\",\"end\":\"86728\"},{\"start\":\"87192\",\"end\":\"87210\"},{\"start\":\"87210\",\"end\":\"87227\"},{\"start\":\"87652\",\"end\":\"87667\"},{\"start\":\"87667\",\"end\":\"87680\"},{\"start\":\"87680\",\"end\":\"87695\"},{\"start\":\"88149\",\"end\":\"88163\"},{\"start\":\"88163\",\"end\":\"88182\"},{\"start\":\"88588\",\"end\":\"88598\"},{\"start\":\"88598\",\"end\":\"88610\"},{\"start\":\"88806\",\"end\":\"88825\"},{\"start\":\"88825\",\"end\":\"88840\"},{\"start\":\"88840\",\"end\":\"88860\"},{\"start\":\"88860\",\"end\":\"88874\"},{\"start\":\"88874\",\"end\":\"88890\"},{\"start\":\"89103\",\"end\":\"89115\"},{\"start\":\"89115\",\"end\":\"89129\"},{\"start\":\"89129\",\"end\":\"89133\"},{\"start\":\"89488\",\"end\":\"89500\"},{\"start\":\"89500\",\"end\":\"89512\"},{\"start\":\"89512\",\"end\":\"89521\"},{\"start\":\"89864\",\"end\":\"89874\"},{\"start\":\"89874\",\"end\":\"89884\"},{\"start\":\"89884\",\"end\":\"89893\"},{\"start\":\"90195\",\"end\":\"90205\"},{\"start\":\"90205\",\"end\":\"90215\"},{\"start\":\"90215\",\"end\":\"90224\"},{\"start\":\"90638\",\"end\":\"90657\"},{\"start\":\"90657\",\"end\":\"90680\"},{\"start\":\"90680\",\"end\":\"90698\"},{\"start\":\"91170\",\"end\":\"91182\"},{\"start\":\"91672\",\"end\":\"91684\"},{\"start\":\"91684\",\"end\":\"91699\"},{\"start\":\"91959\",\"end\":\"91974\"},{\"start\":\"91974\",\"end\":\"91989\"},{\"start\":\"91989\",\"end\":\"92009\"},{\"start\":\"92535\",\"end\":\"92545\"},{\"start\":\"92545\",\"end\":\"92558\"},{\"start\":\"92558\",\"end\":\"92572\"},{\"start\":\"92984\",\"end\":\"92993\"},{\"start\":\"92993\",\"end\":\"93011\"},{\"start\":\"93011\",\"end\":\"93020\"},{\"start\":\"93254\",\"end\":\"93265\"},{\"start\":\"93265\",\"end\":\"93285\"},{\"start\":\"93285\",\"end\":\"93299\"},{\"start\":\"93299\",\"end\":\"93309\"},{\"start\":\"93621\",\"end\":\"93638\"},{\"start\":\"93926\",\"end\":\"93943\"},{\"start\":\"93943\",\"end\":\"93955\"},{\"start\":\"93955\",\"end\":\"93976\"},{\"start\":\"94270\",\"end\":\"94278\"},{\"start\":\"94278\",\"end\":\"94284\"},{\"start\":\"94526\",\"end\":\"94546\"},{\"start\":\"94546\",\"end\":\"94562\"},{\"start\":\"94562\",\"end\":\"94576\"},{\"start\":\"94576\",\"end\":\"94587\"},{\"start\":\"94587\",\"end\":\"94606\"},{\"start\":\"94606\",\"end\":\"94622\"},{\"start\":\"94622\",\"end\":\"94637\"},{\"start\":\"94637\",\"end\":\"94644\"},{\"start\":\"95164\",\"end\":\"95175\"},{\"start\":\"95175\",\"end\":\"95192\"},{\"start\":\"95192\",\"end\":\"95200\"},{\"start\":\"95468\",\"end\":\"95484\"},{\"start\":\"95484\",\"end\":\"95499\"},{\"start\":\"95499\",\"end\":\"95514\"},{\"start\":\"95514\",\"end\":\"95533\"},{\"start\":\"95995\",\"end\":\"96005\"},{\"start\":\"96073\",\"end\":\"96090\"},{\"start\":\"96090\",\"end\":\"96105\"},{\"start\":\"96105\",\"end\":\"96125\"},{\"start\":\"96501\",\"end\":\"96516\"},{\"start\":\"96709\",\"end\":\"96718\"},{\"start\":\"96718\",\"end\":\"96733\"},{\"start\":\"96733\",\"end\":\"96744\"},{\"start\":\"97101\",\"end\":\"97113\"},{\"start\":\"97113\",\"end\":\"97134\"},{\"start\":\"97134\",\"end\":\"97141\"},{\"start\":\"97356\",\"end\":\"97372\"},{\"start\":\"97372\",\"end\":\"97382\"},{\"start\":\"97382\",\"end\":\"97400\"},{\"start\":\"97400\",\"end\":\"97414\"},{\"start\":\"97414\",\"end\":\"97421\"},{\"start\":\"97675\",\"end\":\"97696\"},{\"start\":\"97696\",\"end\":\"97707\"},{\"start\":\"97707\",\"end\":\"97726\"},{\"start\":\"97726\",\"end\":\"97738\"},{\"start\":\"98250\",\"end\":\"98268\"},{\"start\":\"98524\",\"end\":\"98536\"},{\"start\":\"98536\",\"end\":\"98559\"},{\"start\":\"98876\",\"end\":\"98894\"},{\"start\":\"98894\",\"end\":\"98906\"},{\"start\":\"99219\",\"end\":\"99233\"},{\"start\":\"99233\",\"end\":\"99246\"},{\"start\":\"99512\",\"end\":\"99530\"},{\"start\":\"99530\",\"end\":\"99548\"},{\"start\":\"99548\",\"end\":\"99571\"},{\"start\":\"99571\",\"end\":\"99589\"},{\"start\":\"99971\",\"end\":\"99990\"},{\"start\":\"99990\",\"end\":\"100011\"},{\"start\":\"100011\",\"end\":\"100023\"},{\"start\":\"100023\",\"end\":\"100041\"},{\"start\":\"100041\",\"end\":\"100052\"},{\"start\":\"100546\",\"end\":\"100565\"},{\"start\":\"100565\",\"end\":\"100584\"},{\"start\":\"100584\",\"end\":\"100606\"},{\"start\":\"100606\",\"end\":\"100621\"},{\"start\":\"100621\",\"end\":\"100642\"},{\"start\":\"100956\",\"end\":\"100975\"},{\"start\":\"100975\",\"end\":\"100992\"},{\"start\":\"101443\",\"end\":\"101464\"},{\"start\":\"101464\",\"end\":\"101482\"},{\"start\":\"101819\",\"end\":\"101839\"},{\"start\":\"101839\",\"end\":\"101854\"},{\"start\":\"101854\",\"end\":\"101871\"},{\"start\":\"101871\",\"end\":\"101884\"},{\"start\":\"101884\",\"end\":\"101900\"},{\"start\":\"101900\",\"end\":\"101914\"},{\"start\":\"102289\",\"end\":\"102297\"},{\"start\":\"102297\",\"end\":\"102309\"},{\"start\":\"102309\",\"end\":\"102329\"},{\"start\":\"102329\",\"end\":\"102339\"},{\"start\":\"102624\",\"end\":\"102642\"},{\"start\":\"102642\",\"end\":\"102660\"},{\"start\":\"102660\",\"end\":\"102678\"},{\"start\":\"102678\",\"end\":\"102694\"},{\"start\":\"102694\",\"end\":\"102706\"},{\"start\":\"102706\",\"end\":\"102721\"},{\"start\":\"103115\",\"end\":\"103133\"},{\"start\":\"103133\",\"end\":\"103148\"},{\"start\":\"103148\",\"end\":\"103168\"},{\"start\":\"103168\",\"end\":\"103179\"},{\"start\":\"103179\",\"end\":\"103194\"},{\"start\":\"103194\",\"end\":\"103209\"},{\"start\":\"103648\",\"end\":\"103657\"},{\"start\":\"103657\",\"end\":\"103673\"},{\"start\":\"103673\",\"end\":\"103682\"},{\"start\":\"103682\",\"end\":\"103700\"},{\"start\":\"103700\",\"end\":\"103718\"},{\"start\":\"103718\",\"end\":\"103729\"},{\"start\":\"103985\",\"end\":\"104002\"},{\"start\":\"104002\",\"end\":\"104012\"},{\"start\":\"104012\",\"end\":\"104026\"},{\"start\":\"104026\",\"end\":\"104043\"},{\"start\":\"104043\",\"end\":\"104059\"},{\"start\":\"104059\",\"end\":\"104068\"},{\"start\":\"104535\",\"end\":\"104547\"},{\"start\":\"104547\",\"end\":\"104554\"},{\"start\":\"104554\",\"end\":\"104569\"},{\"start\":\"104569\",\"end\":\"104578\"},{\"start\":\"104876\",\"end\":\"104887\"},{\"start\":\"104887\",\"end\":\"104898\"},{\"start\":\"104898\",\"end\":\"104913\"},{\"start\":\"104913\",\"end\":\"104931\"},{\"start\":\"105297\",\"end\":\"105313\"},{\"start\":\"105313\",\"end\":\"105327\"},{\"start\":\"105765\",\"end\":\"105778\"},{\"start\":\"105778\",\"end\":\"105791\"},{\"start\":\"105791\",\"end\":\"105811\"},{\"start\":\"105811\",\"end\":\"105822\"},{\"start\":\"105822\",\"end\":\"105837\"},{\"start\":\"105837\",\"end\":\"105852\"},{\"start\":\"106282\",\"end\":\"106297\"},{\"start\":\"106297\",\"end\":\"106312\"},{\"start\":\"106312\",\"end\":\"106332\"},{\"start\":\"106332\",\"end\":\"106349\"},{\"start\":\"106349\",\"end\":\"106359\"},{\"start\":\"106359\",\"end\":\"106386\"},{\"start\":\"106386\",\"end\":\"106393\"},{\"start\":\"106849\",\"end\":\"106862\"},{\"start\":\"106862\",\"end\":\"106876\"},{\"start\":\"106876\",\"end\":\"106892\"},{\"start\":\"106892\",\"end\":\"106904\"}]", "bib_venue": "[{\"start\":\"77107\",\"end\":\"77174\"},{\"start\":\"78684\",\"end\":\"78744\"},{\"start\":\"79868\",\"end\":\"79935\"},{\"start\":\"81405\",\"end\":\"81472\"},{\"start\":\"82327\",\"end\":\"82394\"},{\"start\":\"82789\",\"end\":\"82850\"},{\"start\":\"83214\",\"end\":\"83269\"},{\"start\":\"84807\",\"end\":\"84873\"},{\"start\":\"86360\",\"end\":\"86421\"},{\"start\":\"86805\",\"end\":\"86865\"},{\"start\":\"87298\",\"end\":\"87352\"},{\"start\":\"87779\",\"end\":\"87846\"},{\"start\":\"88259\",\"end\":\"88319\"},{\"start\":\"89217\",\"end\":\"89284\"},{\"start\":\"89605\",\"end\":\"89672\"},{\"start\":\"90307\",\"end\":\"90373\"},{\"start\":\"90782\",\"end\":\"90849\"},{\"start\":\"91268\",\"end\":\"91337\"},{\"start\":\"92106\",\"end\":\"92186\"},{\"start\":\"92647\",\"end\":\"92705\"},{\"start\":\"94719\",\"end\":\"94777\"},{\"start\":\"95648\",\"end\":\"95746\"},{\"start\":\"96202\",\"end\":\"96262\"},{\"start\":\"96821\",\"end\":\"96881\"},{\"start\":\"97836\",\"end\":\"97917\"},{\"start\":\"100154\",\"end\":\"100239\"},{\"start\":\"101078\",\"end\":\"101147\"},{\"start\":\"102805\",\"end\":\"102872\"},{\"start\":\"103293\",\"end\":\"103380\"},{\"start\":\"104145\",\"end\":\"104205\"},{\"start\":\"105015\",\"end\":\"105082\"},{\"start\":\"105424\",\"end\":\"105504\"},{\"start\":\"105939\",\"end\":\"106009\"},{\"start\":\"106477\",\"end\":\"106544\"},{\"start\":\"106979\",\"end\":\"107037\"},{\"start\":\"77023\",\"end\":\"77105\"},{\"start\":\"77428\",\"end\":\"77496\"},{\"start\":\"77817\",\"end\":\"77874\"},{\"start\":\"78277\",\"end\":\"78291\"},{\"start\":\"78607\",\"end\":\"78682\"},{\"start\":\"79105\",\"end\":\"79162\"},{\"start\":\"79491\",\"end\":\"79506\"},{\"start\":\"79784\",\"end\":\"79866\"},{\"start\":\"80346\",\"end\":\"80408\"},{\"start\":\"80806\",\"end\":\"80859\"},{\"start\":\"81069\",\"end\":\"81109\"},{\"start\":\"81321\",\"end\":\"81403\"},{\"start\":\"81689\",\"end\":\"81734\"},{\"start\":\"82243\",\"end\":\"82325\"},{\"start\":\"82711\",\"end\":\"82787\"},{\"start\":\"83142\",\"end\":\"83212\"},{\"start\":\"83587\",\"end\":\"83649\"},{\"start\":\"83979\",\"end\":\"84007\"},{\"start\":\"84233\",\"end\":\"84291\"},{\"start\":\"84469\",\"end\":\"84486\"},{\"start\":\"84724\",\"end\":\"84805\"},{\"start\":\"85159\",\"end\":\"85279\"},{\"start\":\"85619\",\"end\":\"85655\"},{\"start\":\"85984\",\"end\":\"86007\"},{\"start\":\"86282\",\"end\":\"86358\"},{\"start\":\"86728\",\"end\":\"86803\"},{\"start\":\"87227\",\"end\":\"87296\"},{\"start\":\"87695\",\"end\":\"87777\"},{\"start\":\"88182\",\"end\":\"88257\"},{\"start\":\"88610\",\"end\":\"88646\"},{\"start\":\"88767\",\"end\":\"88804\"},{\"start\":\"89133\",\"end\":\"89215\"},{\"start\":\"89521\",\"end\":\"89603\"},{\"start\":\"89893\",\"end\":\"89972\"},{\"start\":\"90224\",\"end\":\"90305\"},{\"start\":\"90698\",\"end\":\"90780\"},{\"start\":\"91182\",\"end\":\"91266\"},{\"start\":\"91562\",\"end\":\"91670\"},{\"start\":\"92009\",\"end\":\"92104\"},{\"start\":\"92572\",\"end\":\"92645\"},{\"start\":\"93020\",\"end\":\"93056\"},{\"start\":\"93309\",\"end\":\"93366\"},{\"start\":\"93638\",\"end\":\"93674\"},{\"start\":\"93976\",\"end\":\"94012\"},{\"start\":\"94284\",\"end\":\"94315\"},{\"start\":\"94644\",\"end\":\"94717\"},{\"start\":\"95200\",\"end\":\"95228\"},{\"start\":\"95533\",\"end\":\"95646\"},{\"start\":\"96125\",\"end\":\"96200\"},{\"start\":\"96516\",\"end\":\"96552\"},{\"start\":\"96744\",\"end\":\"96819\"},{\"start\":\"97141\",\"end\":\"97159\"},{\"start\":\"97421\",\"end\":\"97436\"},{\"start\":\"97738\",\"end\":\"97834\"},{\"start\":\"98181\",\"end\":\"98248\"},{\"start\":\"98559\",\"end\":\"98575\"},{\"start\":\"98906\",\"end\":\"98942\"},{\"start\":\"99246\",\"end\":\"99266\"},{\"start\":\"99589\",\"end\":\"99684\"},{\"start\":\"100052\",\"end\":\"100152\"},{\"start\":\"100642\",\"end\":\"100678\"},{\"start\":\"100992\",\"end\":\"101076\"},{\"start\":\"101482\",\"end\":\"101518\"},{\"start\":\"101914\",\"end\":\"101928\"},{\"start\":\"102339\",\"end\":\"102397\"},{\"start\":\"102721\",\"end\":\"102803\"},{\"start\":\"103209\",\"end\":\"103291\"},{\"start\":\"103729\",\"end\":\"103765\"},{\"start\":\"104068\",\"end\":\"104143\"},{\"start\":\"104578\",\"end\":\"104611\"},{\"start\":\"104931\",\"end\":\"105013\"},{\"start\":\"105327\",\"end\":\"105422\"},{\"start\":\"105852\",\"end\":\"105937\"},{\"start\":\"106393\",\"end\":\"106475\"},{\"start\":\"106904\",\"end\":\"106977\"}]"}}}, "year": 2023, "month": 12, "day": 17}