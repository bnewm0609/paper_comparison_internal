{"id": 232233777, "updated": "2023-10-06 05:31:39.904", "metadata": {"title": "Uncertainty-guided Model Generalization to Unseen Domains", "authors": "[{\"first\":\"Fengchun\",\"last\":\"Qiao\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Peng\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 3, "day": 12}, "abstract": "We study a worst-case scenario in generalization: Out-of-domain generalization from a single source. The goal is to learn a robust model from a single source and expect it to generalize over many unknown distributions. This challenging problem has been seldom investigated while existing solutions suffer from various limitations. In this paper, we propose a new solution. The key idea is to augment the source capacity in both input and label spaces, while the augmentation is guided by uncertainty assessment. To the best of our knowledge, this is the first work to (1) access the generalization uncertainty from a single source and (2) leverage it to guide both input and label augmentation for robust generalization. The model training and deployment are effectively organized in a Bayesian meta-learning framework. We conduct extensive comparisons and ablation study to validate our approach. The results prove our superior performance in a wide scope of tasks including image classification, semantic segmentation, text classification, and speech recognition.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.07531", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Qiao021", "doi": "10.1109/cvpr46437.2021.00672"}}, "content": {"source": {"pdf_hash": "2c0ccf919d5347b87677e7a16a3ba5e555f51710", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.07531v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.07531", "status": "GREEN"}}, "grobid": {"id": "c9816839eb16835f2fea97bf5d81da0f0c67d42c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2c0ccf919d5347b87677e7a16a3ba5e555f51710.txt", "contents": "\nUncertainty-guided Model Generalization to Unseen Domains\n\n\nFengchun Qiao fengchun@udel.edu \nUniversity of Delaware\nUniversity of Delaware\n\n\nXi Peng xipeng@udel.edu \nUniversity of Delaware\nUniversity of Delaware\n\n\nUncertainty-guided Model Generalization to Unseen Domains\n\nWe study a worst-case scenario in generalization: Outof-domain generalization from a single source. The goal is to learn a robust model from a single source and expect it to generalize over many unknown distributions. This challenging problem has been seldom investigated while existing solutions suffer from various limitations. In this paper, we propose a new solution. The key idea is to augment the source capacity in both input and label spaces, while the augmentation is guided by uncertainty assessment. To the best of our knowledge, this is the first work to (1) access the generalization uncertainty from a single source and (2) leverage it to guide both input and label augmentation for robust generalization. The model training and deployment are effectively organized in a Bayesian meta-learning framework. We conduct extensive comparisons and ablation study to validate our approach. The results prove our superior performance in a wide scope of tasks including image classification, semantic segmentation, text classification, and speech recognition.\n\nIntroduction\n\nExisting machine learning algorithms have achieved remarkable success under the assumption that training and test data are sampled from similar distributions. When this assumption no longer holds, even strong models (e.g., deep neural networks) may fail to produce reliable predictions. In this paper, we study a worst-case scenario in generalization: Out-of-domain generalization from a single source. A model learned from a single source is expected to generalize over a series of unknown distributions. This problem is more challenging than domain adaptation [40,43,63,36] which usually requires the assessment of target distributions during training, and domain generalization [42,14,35,4,9] which often assumes the availability of multiple sources. For example, there exists significant distribution difference in medical images collected across different hospitals. The intelligent diagnosis system is required to process images unexplored during training where model update is infeasible due to time or resource limitations.\n\nRecently, [59] casts this problem in an ensemble framework. It learns a group of models each of which tackles an unseen test domain. This is achieved by performing adversarial training [15] on the source to mimic the unseen test distributions. Yet, its generalization capability is limited due to the proposed semantic constraint, which allows only a small amount of data augmentation to avoid semantic changes in the label space. To address this limitation, [46] proposes adversarial domain augmentation to relax the constraint. By maximizing the Wasserstein distance between the source and augmentation, the domain transportation is significantly enlarged in the input space.\n\nHowever, existing data (domain) augmentation based methods [59,45,8,6,22] merely consider to increase the source capacity by perturbing the input space. Few of them investigate the possibility of label augmentation. An exception is Mixup [66] which pioneers label augmentation by randomly interpolating two data examples in both input and label spaces. However, Mixup can hardly address the out-of-domain generalization problem since it is restricted in creating in-domain generations due to the linear interpolation assumption. Besides, the interpolations are randomly sampled from a fixed distribution, which also largely restricts the flexibility of domain mixtures, yielding sub-optimal performance for unseen domain generalization.\n\nAnother limitation of existing work [42,14,35,4,9] is they usually overlook the potential risk of leveraging augmented data in tackling out-of-domain generalization. This raises serious safety and security concerns in mission-critical applications [11]. For instance, when deploying self-driving cars in unknown environments, it is crucial to be aware of the predictive uncertainty in risk assessment.\n\nTo tackle the aforementioned limitations, we propose uncertain out-of-domain generalization. The key idea is to increase the source capacity guided by uncertainty estimation in both input and label spaces. More specifically, in the input space, instead of directly augmenting raw data [59,46], we apply uncertainty-guided perturbations to latent features, yielding a domain-knowledge-free solution for various modalities such as image, text, and audio. In the label space, we leverage the uncertainty associated with feature perturbations to augment labels via interpolation, improving generalization over unseen domains. Moreover, we explicitly model the domain uncertainty as a byproduct of feature perturbation and label mixup, guaranteeing fast risk assessment without repeated sampling. Finally, we organize the training and deployment in a Bayesian meta-learning framework that is specially tailored for single source generalization. To summarize, our contribution is multi-fold:\n\n\u2022 To the best of our knowledge, we are the first to access the uncertainty from a single source. We leverage the uncertainty assessment to gradually improve the domain generalization in a curriculum learning scheme.\n\n\u2022 For the first time, we propose learnable label mixup in addition to widely used input augmentation, further increasing the domain capacity and reinforcing generalization over unseen domains.\n\n\u2022 We propose a Bayesian meta-learning method to effectively organize domain augmentation and model training. Bayesian inference is crucial in maximizing the posterior of domain augmentations, such that they can approximate the distribution of unseen domains.\n\n\u2022 Extensive comparisons and ablation study prove our superior performance in a wide scope of tasks including image classification, semantic segmentation, text classification, and speech recognition.\n\n\nRelated Work\n\nOut-of-Domain Generalization. Domain generalization [14,34,18,51,4,9] has been intensively studied in recent years. JiGen [4] proposed to generate jigsaw puzzles from source domains and leverage them as self-supervised signals. Wang et al. [61] leveraged both extrinsic relationship supervision and intrinsic self-supervision for domain generalization. Specially, GUD [59] proposed adversarial data augmentation to solve single domain generalization, and learned an ensemble model for stable training. M-ADA [46] extended it to create augmentations with large domain transportation, and designed an efficient meta-learning scheme within a single unified model. Both GUD [59] and M-ADA [46] fail to assess the uncertainty of augmentations and only augment the input, while our method explicitly model the uncertainty and leverage it to increase the augmentation capacity in both input and label spaces. Several methods [39,60,21] proposed to leverage adversarial training [15] to learn robust models, which can also be applied in single source generalization. PAR [60] proposed to learn robust global representations by penalizing the predictive power of local representations. [21] applied self-supervised learning to improve the model robustness.\n\nAdversarial training. Szegedy et al. [56] discovered the intriguing weakness of deep neural networks to minor adversarial perturbations. Goodfellow et al. [15] proposed adversarial training to improve model robustness against adversarial samples. Madry et al. [39] illustrated that adversarial samples generated through projected gradient descent can provide robustness guarantees. Sinha et al. [53] proposed principled adversarial training with robustness guarantees through distributionally robust optimization. More recently, Stutz et al. [54] illustrated that on-manifold adversarial samples can improve generalization. Therefore, models with both robustness and generalization can be achieved at the same time. In our work, we leverage adversarial training to create feature perturbations for domain augmentation instead of directly perturbing raw data.\n\nMeta-learning. Meta-learning [50,57] is a long standing topic on learning models to generalize over a distribution of tasks. Model-Agnostic Meta-Learning (MAML) [10] is a recent gradient-based method for fast adaptation to new tasks. In this paper, we propose a modified MAML to make the model generalize over the distribution of domain augmentation. Several approaches [35,1,9] have been proposed to learn domain generalization in a meta-learning framework. Li et al. [35] firstly applied MAML in domain generalization by adopting an episodic training paradigm. Balaji et al. [1] proposed to meta-learn a regularization function to train networks which can be easily generalized to different domains. Dou et al. [9] incorporated global and local constraints for learning semantic feature spaces in a meta-learning framework. However, these methods cannot be directly applied for single source generalization since there is only one distribution available during training.\n\nUncertainty Assessment. Bayesian neural networks [23,17,3] have been intensively studied to integrate uncertainty into weights of deep networks. Instead, we apply Bayesian inference to assess the uncertainty of domain augmentations. Several Bayesian meta-learning frameworks [16,11,64,32] have been proposed to model the uncertainty of few-shot tasks. Grant et al. [16] proposed the first Bayesian variant of MAML [10] using the Laplace approximation. Yoon et al. [64] proposed a novel Bayesian MAML with a stein variational inference framework and chaser loss. Finn et al. [11] approximated MAP inference of the task-specific weights while maintain uncertainty only in the global weights. Lee et al. [32] proposed a Bayesian meta-learning framework to deal with class/task imbalance and out-of-distribution tasks. Lee et al. [33] proposed meta-dropout which generates learnable perturbations to regularize few-shot learning models. In this paper, instead of modelling the uncertainty of tasks, we propose a novel Bayesian meta-learning framework to maximize the posterior distribution of domain augmentations. \n\n\nMethod\n\nWe first describe our problem setting and overall framework design. The goal is to learn a robust model from a single domain S and we expect the model to generalize over an unknown domain distribution {T 1 , T 2 , \u00b7 \u00b7 \u00b7 } \u223c p(T ). This problem is more challenging than domain adaptation (assuming p(T ) is given) and domain generalization (assuming multiple source domains {S 1 , S 2 , \u00b7 \u00b7 \u00b7 } are available). We create a series of domain augmentations {S + 1 , S + 2 , \u00b7 \u00b7 \u00b7 } \u223c p(S + ) to approximate p(T ), from which the backbone \u03b8 can learn to generalize over unseen domains.\n\nUncertainty-guided domain generalization. We assume that S + should integrate uncertainty assessment for efficient domain generalization. To achieve it, we introduce the auxiliary \u03c8 = {\u03c6 p , \u03c6 m } to explicitly model the uncertainty with respect to \u03b8 and leverage it to create S + by increasing the capacity in both input and label spaces. In input space, we introduce \u03c6 p to create feature augmentations h + via adding perturbation e sampled from N (\u00b5, \u03c3). In label space, we integrate the same uncertainty encoded in (\u00b5, \u03c3) into \u03c6 m and propose learnable mixup to generate y + (together with h + ) through three variables (a, b, \u03c4 ), yielding consistent augmentation in both input and output spaces. To effectively organize domain augmentation and model training, we propose a Bayesian meta-learning framework to maximizing a posterior of p(S + ) by jointly optimizing the backbone \u03b8 and the auxiliary \u03c8. The overall framework is shown in Fig. 1 and full algorithm is summarized in Alg. 1.\n\nMerits of uncertainty assessment. Assessing the uncertainty of S + plays a key role in our design. First, it provides consistent guidance to the augmentation in both input and label spaces when inferring S + , which has never been studied before. Second, we can gradually enlarge the domain transportation by increasing the uncertainty of S + in a curriculum learning scheme [2]. Last, we can easily assess the domain uncertainty by checking the value of \u03c3, which measures how unsure it is when deploying on unseen domains T (Sec. 3.3). Meta-update: Update \u03b8 and \u03c8 using Eq. 6 10 end\n\n\nUncertainty-Guided Input Augmentation\n\nThe goal is to create S + from S such that p(S + ) can approximate the out-of-domain distribution of S. One the one hand, we expect a large domain transportation from S to S + to best accommodate the unseen testing distribution p(T ). On the other hand, we prefer the transportation is domain-knowledge-free with uncertainty guarantee for broad and safe domain generalization. Towards this goal, we introduce \u03c6 p to create feature augmentation h + with large domain transportation through increasing the uncertainty with respect to \u03b8.\n\nAdversarial Domain Augmentation. To encourage large domain transportation, we cast the problem in a worstcase scenario [53] and propose to learn the auxiliary mapping \u03c6 p via adversarial domain augmentation:\nmaximize \u03c6p L(\u03b8; S + ) Main task \u2212\u03b2 z \u2212 z + 2 2 Constraint .\n(1)\n\nHere, L denotes empirical loss such as cross-entropy loss for classification. The second term is the worst-case constraint, bounding the largest domain discrepancy between S and S + in embedding space. z denotes the FC-layer output right before the activation layer, which is distinguished from h that denotes the Conv-layer outputs.\n\nOne merit of the proposed uncertainty-guided augmentation is that we can effectively relax the constraint to encourage large domain transportation in a curriculum learning scheme, which is significantly more efficient than [46] that has to train an extra WAE-GAN [58] to achieve this goal. We introduce the detailed form of h + as follows.\n\nVariational feature perturbation. To achieve adversarial domain augmentation, we apply uncertainty-guided perturbations to latent features instead of directly augmenting raw data, yielding domain-knowledge-free augmentation. We propose to learn layer-wise feature perturbations e that transport latent features h \u2192 h + for efficient do-main augmentation S \u2192 S + . Instead of a direct generation e = f \u03c6p (x, h) widely used in previous work [59,46], we assume e follows a multivariate Gaussian distribution N (\u00b5, \u03c3), which can be used to easily access the uncertainty. More specifically, the Gaussian parameters are learnable via variational inference (\u00b5, \u03c3) = f \u03c6p (S, \u03b8), such that:\nh + \u2190 h + Softplus(e), where e \u223c N (\u00b5, \u03c3),(2)\nwhere Softplus(\u00b7) is applied to stabilize the training. \u03c6 p can create a series of feature augmentations {h + 1 , h + 2 , \u00b7 \u00b7 \u00b7 } in different training iterations. In Sec. 4.5, we empirically show that {h + 1 , h + 2 , \u00b7 \u00b7 \u00b7 } gradually enlarge the transportation through increasing the uncertainty of augmentations in a curriculum learning scheme and enable the model to learn from \"easy\" to \"hard\" domains.\n\n\nUncertainty-Guided Label Mixup\n\nFeature perturbations not only augment the input but also yield label uncertainty. To explicitly model the label uncertainty, we leverage the input uncertainty, encoded in (\u00b5, \u03c3), to infer the label uncertainty encoded in (a, b, \u03c4 ) through \u03c6 m as shown in Fig. 1. We leverage the label uncertainty to propose learnable label mixup, yielding consistent augmentation in both input and output spaces and further reinforcing generalization over unseen domains.\n\nRandom Mixup. We start by introducing random mixup [66] for robust learning. The key idea is to regularize the training to favor simple linear behavior in-between examples. More specifically, mixup performs training on convex interpolations of pairs of examples (x i , x j ) and their labels (y i , y j ):\nx + = \u03bbx i + (1 \u2212 \u03bb)x j , y + = \u03bby i + (1 \u2212 \u03bb)y j ,\nwhere \u03bb \u223c Beta(\u03b1, \u03b1) and the mixup hyper-parameter \u03b1 \u2208 (0, +\u221e) controls the interpolation strength.\n\nLearnable Label Mixup. We improve mixup by casting it in a learnable framework specially tailored for single source generalization. First, instead of mixing up pairs of examples, we mix up S and S + to achieve in-between domain interpolations. Second, we leverage the uncertainty encoded in (\u00b5, \u03c3) to predict learnable parameters (a, b), which controls the direction and strength of domain interpolations:\nh + = \u03bbh + (1 \u2212 \u03bb)h + , y + = \u03bby + (1 \u2212 \u03bb)\u1ef9, (3)\nwhere \u03bb \u223c Beta(a, b) and\u1ef9 denotes a label-smoothing [55] version of y. More specifically, we perform label smoothing by a chance of \u03c4 , such that we assign \u03c1 \u2208 (0, 1) to the true category and equally distribute 1\u2212\u03c1 c\u22121 to the others, where c counts categories. The Beta distribution (a, b) and the lottery \u03c4 are jointly inferred by (a, b, \u03c4 ) = f \u03c6m (\u00b5, \u03c3) to integrate the uncertainty of domain augmentation.\n\n\nA Unified Framework\n\nTo effectively organize domain augmentation and model training, we propose a Bayesian meta-learning framework to maximize a posterior of p(S + ) by jointly optimizing the backbone \u03b8 and the auxiliary \u03c8 = {\u03c6 p , \u03c6 m }. Specifically, we meta-train the backbone \u03b8 on the source S and metatest its generalization capability over p(S + ), where S + is generated by performing data augmentation in both input (Sec. 3.1) and output (Sec. 3.2) spaces through the auxiliary \u03c8. Finally, we meta-update {\u03b8, \u03c8} using gradient:\n\u2207 \u03b8,\u03c8 E p(S + ) [L(\u03b8 * ; S + )],where \u03b8 * \u2261 \u03b8\u2212\u03b1\u2207 \u03b8 L(\u03b8; S). (4)\nHere \u03b8 * is the meta-trained backbone on S and \u03b1 is the learning rate. After training, the backbone \u03b8 is expected to bound the generalization uncertainty over unseen populations p(T ) in a worst-case scenario (Sec. 3.1) while \u03c8 can be used to access the value of uncertainty efficiently.\n\nBayesian Meta-learning. The goal is to maximize the conditional likelihood of the augmented domain S + : log p (y + |x, h + ; \u03b8 * ). However, solving it involves the true posterior p (h + |x; \u03b8 * , \u03c8), which is intractable [32]. Thus, we resort to amortized variational inference with a tractable form of approximate posterior q (h + |x; \u03b8 * , \u03c8). The approximated lower bound is as follows:\nL \u03b8,\u03c8 = E q(h + |x;\u03b8 * ,\u03c8) [log p (y + |x, h + ; \u03b8 * ) q (h + |x; \u03b8 * , \u03c8) ].(5)\nWe leverage Monte-Carlo (MC) sampling to maximize the lower bound L \u03b8,\u03c8 by:\nmin \u03b8,\u03c8 1 K K k=1 \u2212 log p y + k |x, h + k ; \u03b8 * + KL q h + |x; \u03b8 * , \u03c8 p h + |x; \u03b8 * , \u03c8 ,(6)\nwhere h + k \u223c q (h + |x; \u03b8 * , \u03c8) and K is the number of MC samples. Instead of setting the prior distribution to N (0, I) in [26], we assume that q (h + |x; \u03b8 * , \u03c8) is expected to approximate p (h + |x; \u03b8 * , \u03c8) through the adversarial training on \u03c6 p in Eq. 1. Thank to the Bayesian meta-learning framework, the generalization uncertainty on unseen domains is significantly suppressed (Sec. 4.5). More importantly, a few examples of the target domain can quickly adapt \u03b8 to be domain-specific, yielding largely improved performance for few-shot domain adaptation (Sec. 4.1).\n\nUncertainty Estimation. At testing time, given a novel domain T , we propose a normalized domain uncertainty score, | \u03c3(T )\u2212\u03c3(S) \u03c3(S) |, to estimate its uncertainty with respect to learned \u03b8. Considering \u03c8 is usually much smaller than \u03b8, this score can be calculated efficiently by one-pass data forwarding through \u03c8. In Sec. 4.1, we empirically prove that our estimation is consistent with conventional Bayesian methods [3], while the time consumption is significantly reduced by an order of magnitude. \n\n\nExperiments\n\nTo best validate the performance, we conduct a series of experiments to compare our approach with existing methods that can be roughly grouped in four categories: 1) Adversarial training: PAR [60], Self-super [21], and PGD [39].\n\n2) Data augmentation: Mixup [66], JiGen [4], Cutout [8], and AutoAug [6]. 3) Domain adaptation: DIRT-T [52], SE [12], SBADA [48], FADA [40], and CCSA [41]. 4) Domain generalization: ERM [27], GUD [59], and M-ADA [46]. The experimental results prove that our method achieves superior performance on a wide scope of tasks, including image classification [20], semantic segmentation [47], text classification [5], and speech recognition [62]. Please refer to supplementary for more details about experiment setup.\n\n\nImage Classification\n\nDatasets. We validate our method on the following two benchmark datasets for image classification. (1) Digits is used for digit classification and consists of five sub-datasets: MNIST [30], MNIST-M [13], SVHN [44], SYN [13], and USPS [7]. Each sub-dataset can be viewed as a different domain. Each image in these datasets contains one single digit with different styles and backgrounds. (2) CIFAR-10-C [20] is a robustness benchmark consisting of 19 corruptions types with five levels of severity applied to the test set of CIFAR-10 [28]. The corruptions consist of four main categories: noise, blur, weather, and digital. Each corruption has fivelevel severities and \"5\" indicates the most corrupted one.\n\nSetup. Digits: following the setup in [59], we use 10,000 samples in the training set of MNIST for training, and evaluate models on the other four sub-datasets. We use a ConvNet [29] with architecture conv-pool-conv-pool-fc-fc-softmax as the backbone. All images are resized to 32\u00d732, and the channels of MNIST and USPS are duplicated to make them as RGB images. CIFAR-10-C: we train models on CIFAR-10 and evaluate them on CIFAR-10-C. Following the setting of [22], we evaluate the model on 15 corruptions. We train models on AllConvNet (AllConv) [49] and Wide Residual Network (WRN) [65] with 40 layers and width of 2.\n\nResults. 1) Classification accuracy. Tab. 1 shows the classification results of Digits and CIFAR-10-C. On the experiment of Digits, GUD [59], M-ADA [46], and our method outperform all baselines of the second block. And our method outperforms M-ADA [46] on SYN and the average accuracy by 8.1% and 1.8%, respectively. On the experiment of CIFAR-10-C, our method consistently outperforms all baselines on two different backbones, suggesting its strong generalization on various image corruptions. 2) Uncertainty estimation. We compare the proposed domain uncertainty score (Sec.3.3) with a more time-consuming one based on Bayesian models [3]. The former computes the uncertainty through one-pass forwarding, while the latter computes the variance of the output through repeated sampling of 30 times. Fig. 2 show the results of uncertainty estimation on Digits and CIFAR-10-C. As seen, our estimation shows consistent results with Bayesian uncertainty estimation on both Digits and CIFAR-10-C, suggesting its high efficiency. 3) Few-shot domain adaptation. Although our method is designed for single domain generalization, we also show that our method can be easily applied for few-shot domain adaptation [40] due to the meta-learning training scheme. Following the setup in [46], the model is first pretrained on the source domain S and then fine-tuned on the target domain T . We conduct three few-shot domain adap-    \n\n\nSemantic Segmentation\n\nDatasets. SYTHIA [47] is a synthetic dataset of urban scenes, used for semantic segmentation in the context of driving scenarios. This dataset consists of photo-realistic frames rendered from virtual cities and comes with precise pixel-level semantic annotations. It is composed of the same traffic situation but under different locations (Highway, New York-like City, and Old European Town are selected) and different weather/illumination/season conditions (Dawn, Fog, Night, Spring, and Winter are selected).\n\nSetup. In this experiment, Highway is the source domain, and New York-like City together with Old European Town are unseen domains. Following the protocol in [59,46], we only use the images from the left front camera and 900 images are randomly sample from each source domain. We use FCN-32s [37] with the backbone of ResNet-50 [19].\n\nResults. We report the mean Intersection Over Union (mIoU) of SYTHIA in Tab. 3. As can be observed, our method outperforms previous SOTA in most unseen environments. Results demonstrate that our model can better generalize to the changes of locations, weather, and time. We provide visual comparison in the supplementary.\n\n\nText Classification\n\nDatasets. Amazon Reviews [5] contains reviews of products belonging to four categories -books(b), DVD(d), electronics(e) and kitchen appliances(k). The difference in textual description of the four product categories manifests as domain shift. Following [13], we use unigrams and bigrams as features resulting in 5000 dimensional representations.\n\nSetup. We train the models on one source domain (books or dvd), and evaluate them on the other three domains. Similar to [13], we use a neural network with two hidden layers (both with 50 neurons) as the backbone.\n\nResults. Tab. 10 shows the results of text classification on Amazon Reviews [5]. It appears that our method outperform previous ones on all the three unseen domains when the source domain is \"books\". We note that there is a little drop in performance on \"electronics\" when the source domain is \"dvd\". One possible reason is that \"electronics\" and \"dvd\" may share a similar distribution. And our method creates large distribution shift, degrading the performance on \"electronics\".     \n\n\nSpeech Recognition\n\nDatasets. Google Commands [62] contains 65000 utterances (one second long) from thousands of people. The goal is to classify them to 30 command words. There are 56196, 7477, and 6835 examples for training, validation, and test. To simulate domain shift in real-world scenario, we apply five common corruptions in both time and frequency domains. This creates five test sets that are \"harder\" than training sets, namely amplitude change (Amp.), pitch change (Pit.), background noise (Noise), stretch (Stretch), and time shift (Shift). In detail, the range of \"amplitude change\" is (0.7,1.1). The maximum scales of \"pitch change\", \"background noise\", and \"stretch\" are 0.2, 0.45, and 0.2, respectively. The maximum shift of \"time shift\" is 8.\n\nSetup. We train the models on the clean train set, and evaluate them on the corrupted test sets. We encode each audio into a mel-spectrogram with the size of 1x32x32 and feed them to LeNet [31] as one-channel input.\n\nResults. Tab. 5 shows the results of speech recognition on Google Commands [62]. Our method outperforms the other three methods on all the five corrupted test sets, indicating its strong generalization ability in both time and frequency domain. In detail, our method outperforms the second best by 0.8% on \"amplitude change\", 1.4% on \"pitch change\", 0.4% on \"background noise\", 1.2% on \"stretch\", and 1.1% on \"time shift\", respectively. We can see that the improvements on \"pitch change\", \"stretch\", and \"time shift\" are more significant than those on \"amplitude change\" and \"background noise\".\n\n\nAblation Study\n\nIn this section, we perform ablation study to investigate key components of our method. For Digits [59], we report the average performance of all unseen domains. For CIFAR-10-C [20], we report the average performance of all types of corruptions at the highest level of severity.\n\nUncertainty assessment. We visualize feature perturbation |e| = |h + \u2212 h| and the embedding of domains at different training iterations T on MNIST [30]. We use t-SNE [38] to visualize the source and augmented domains without and with uncertainty assessment in the embedding  Table 6: Ablation study of feature perturbation.\n\nspace. Results are shown in Fig. 3. In the model without uncertainty (left), the feature perturbation e is sampled from N (0, I) without learnable parameters. In the model with uncertainty (right), we observe that most perturbations are located in the background area which increases the variation of S + while keeping the category unchanged. As a result, models with uncertainty can create large domain transportation in a curriculum learning scheme, yielding safe augmentation and improved accuracy on unseen domains. We visualize the density of y + in Fig. 4. As seen, models with uncertainty can significantly augment the label space.\n\nVariational feature perturbation. We investigate different designs of feature perturbation: 1) Random Gaussian: the feature perturbation e is sampled from N (0, I) without learnable parameters. 2) Deterministic perturbation: we directly add the learned \u00b5 to h without sampling, yielding h + \u2190 h + Softplus(\u00b5). 3) Random \u00b5: the feature perturbation e is sampled from N (0, \u03c3), where \u00b5 = 0. 4) Random \u03c3: e is sampled from N (\u00b5, I), where \u03c3 = I. Results on these different choices are shown in Tab. 6. As seen, Random Gaussian yields the lowest accuracy on both datasets, indicating the necessity of learnable perturbations. Deterministic perturbation is inferior to Random \u00b5 and Random \u03c3, suggesting that sampling-based perturbation can effectively increase the domain capacity. Finally, either Random \u00b5 or Random \u03c3 is slightly worse than the full model. We conclude that both learnable \u00b5 and learnable \u03c3 contribute to the final performance.\n\nLearnable label mixup. We implement two variants of label mixup: 1) Without mixup: the model is trained without label augmentation. 2) Random mixup: the mixup coefficient \u03bb is sampled from a fixed distribution Beta(1, 1). Results on the two variants are reported in Tab. 7. We notice that Random mixup achieves better performance than without mixup. The results support our claim that label augmentation can further improve the model performance. The learnable mixup (full model) achieves the best results, suggesting that the proposed learning label mixup can create informative domain interpolations for robust learning.\n\nTraining strategy. At last, we compare different training strategies. 1) Without adversarial training: models are learned without adversarial training (Eq. 1). 2) Without metalearning: the source S and augmentations S + are trained   together without the meta-learning scheme. 3) Without minimizing \u03c6 p : \u03c6 p is not optimized in Eq. 6. Results are reported in Tab. 8. The adversarial training contributes most to the improvements: 9.5% on Digits and 10.2% on CIFAR-10-C. Meta-learning consistently improve the accuracy and reduce the deviation on both datasets. We notice that the accuracy is slightly dropped without minimization of \u03c6 p , possibly due to the excessive accumulation of perturbations.\n\n\nConclusion\n\nIn this work, we introduced uncertain out-of-domain generalization to tackle the problem of single source generalization. Our method explicitly model the uncertainty of domain augmentations in both input and label spaces. In input space, the proposed uncertainty-guided feature perturbation resolves the limitation of raw data augmentation, yielding a domain-knowledge-free solution for various modalities. In label space, the proposed uncertainty-guided label mixup further increases the domain capacity. The proposed domain uncertainty score is capable of estimating uncertainty with high efficiency, which plays a crucial role for safe deployment. Finally, the proposed Bayesian meta-learning framework can maximize the posterior distribution of domain augmentations, such that the learned model can generalize well on unseen domains. The experimental results prove that our method achieves superior performance on a wide scope of tasks, including image classification, semantic segmentation, text classification, and speech recognition. In addition to the superior performances we achieved through these experiments, a series of ablation studies further validate the effectiveness of key components in our method. In the future, we expect to extend our work to semi-supervised learning or knowledge transferring in multimodal learning.\n\n\nAppendix A. Architecture Design and Setup\n\nWe provide more experimental details on the five datasets: Digits [59], CIFAR-10-C [20], SYTHIA [47], Amazon Reviews [5], and Google Commands [62]. In learnable label mixup, we use Gaussian parameters of feature perturbations from the first layer. We choose specific backbone models, and design different auxiliary models as well as training strategies according to characteristics of each dataset.\n\nIn Digits [59], the backbone model is conv-pool-convpool-fc-fc-softmax. There are two 5 \u00d7 5 convolutional layers with 64 and 128 channels respectively. Each convolutional layer is followed by a max pooling layer with the size of 2 \u00d7 2. The size of the two Fully-Connected (FC) layers is 1024 and the size of the softmax layer is 10. We inject perturbations to latent features of the two convolutional layers. The detailed architecture is presented in Fig. 5 (a). We employ Adam [25] for optimization with batch size of 32. We train for total 10K iterations with learning rate of 10 \u22124 .\n\nIn CIFAR-10-C [20], we evaluate our method on two backbones: AllConvNet (AllConv) [49] and Wide Residual Network (WRN) [65] with 40 layers and the width of 2. In AllConv [49], the model starts with three 3 \u00d7 3 convolutional layers with 96 channels. Each layer is followed by batch normalization (BN) [24] and GELU. They convert the original image with three channels to feature maps of 96 channels. Then, the features go though three 3 \u00d7 3 convolutional layers with 192 channels. After that, the features are fed into two 1 \u00d7 1 convolutional layers with 192 channels and an average pooling layer with the size of 8 \u00d7 8. Finally, a softmax layer with the size of 10 is used for classification. In WRN [65]. The first layer is a 3\u00d73 convolutional layer. It converts the original image with three channels to feature maps of 16 channels. Then the features go through three blocks of 3\u00d73 convolutional layers. Each block consists of six basic blocks and each basic block is composed of two convolutional layers with the same number of channels. And their channels are {32, 64, 128} respectively. Each layer is followed by batch normalization (BN) [24]. An average pooling layer with the size of 8 \u00d7 8 is appended to the output of the third block. Finally, a softmax layer with the size of 10 is used for prediction. In both AllConv [49] and WRN [65], we only inject perturbations to the latent features of the first convolutional layer. We also tried to inject perturbations in the next few layers or blocks, however, we found the performance degraded severely mainly due to its large effect on the semantic feature, i.e., outputs before the activation layer. The detailed architecture with backbone of WRN is shown in Fig. 5 (b). Following the training procedure in [65], we use SGD with Nesterov momentum and set the batch size to 128. The initial learning rate is 0.1 with a linear decay and the number of epochs is 200.\n\nIn SYTHIA [47], we use FCN-32s [37] with the backbone of ResNet-50 [19]. The model consists of a feature extractor and a classifier. We use ResNet-50 [19] as the feature extractor, which is composed of a 7\u00d77 convolutional layer with 64 channels and four convolutional blocks. The classifier consists of a 3\u00d73 convolutional layer with 512 channels, a 1\u00d71 convolutional layer with 14 channels, and a bilinear layer used to up-sample the coarse outputs to the original size. We use Adam with the learning rate \u03b1 = 0.0001. We set the batch size to 8 and the number of epochs to 50. In Amazon Reviews [5], reviews are assigned binary labels -0 if the rating of the product is up to 3 stars, and 1 if the rating is 4 or 5 stars. The extracted features are fed into two FC layers with the size of 50. A softmax layer with the size of two is used to classify the sentiment of reviews into \"positive\" or \"negative\". All models are trained using Adam [25] optimizer with learning rate of 10 \u22124 and batch size of 32 for 1000 iterations. In Google Commands [62], the mel-spectrogram features are fed into LeNet [31] as one-channel input. The original image is fed into two 5 \u00d7 5 convolutional layers with the channels of 6 and 16, respectively. Next, the features go through two FC layers with the size of 120 and 84, respectively. Finally, a softmax layer with the size of 30 is leveraged to predict the spoken word. Models are trained using Adam [25] with learning rate 10 \u22124 and batch size of 128 for 30 epoches. For the corrupted test sets, the range of \"amplitude change\" is (0.7, 1.1). The maximum scales of \"pitch change\", \"background noise\", and \"stretch\" are 0.2, 0.45, and 0.2, respectively. The maximum shift of \"time shift\" is 8. In the experiments of Amazon Reviews [5] and Google Commands [62], feature perturbations are appended to the first layer. The detailed architectures used for Google Commands [62] and Amazon Reviews [5] are presented in Fig. 5 (c) and (d), respectively. Appendix B. Additional Results B.1. Image Classification 1) Classification accuracy on CIFAR-10-C [20]. We train all models on clean data, i.e., CIFAR-10, and test them on corruption data, i.e., CIFAR-10-C. In this case, there are totally 15 unseen testing domains. We compare our method with the other three methods for domain generalization: ERM [27], GUD [59], and M-ADA [46]. The classification results on corruptions across five levels of severity are shown in Fig. 7. As seen, our method outperforms other methods across all levels of corruption severity. Specifically, the gap between M-ADA [46] (previous SOTA) and our method gets larger with the level of severity increasing. Fig. 6 shows more detailed comparison of all corruptions at the highest level of severity. As seen, our method achieves substantial gains across a wide variety of corruptions, with a small drop of performance in only two corruption types: brightness and contrast. Especially, accuracy is significantly improved by 20% on corruptions of noise. Results demonstrate its strong      Results of the three tasks are shown in Tab. 9. As seen, the result on the hardest task (M\u2192S) is even competitive to that of SBADA [48] which requires all images of the target domain during training. Specifically, our method achieves the best performance on the average of the three tasks. Note that, when the target domain changes, our method only needs to fine-tune the pre-trained model with a few samples within a small number of iterations, while other methods have to train entirely new models.\n\n\nB.2. Semantic Segmentation\n\nIn the experiment on SYTHIA [47], Highway is the source domain, and New York-like City together with Old European Town are unseen domains. Visual comparison on SYTHIA [47] is shown in Fig. 8. Results demonstrate that our model can better generalize to the changes of locations, weather, and time.\n\n\nB.3. Text Classification\n\nWe train the models on one source domain, and evaluate them on the other three domains. Tab. 10 shows the results of text classification on Amazon Reviews [5] . We found that our method outperform previous ones on all the three unseen domains when the source domain is \"books\" or \"kitchen\". Specially, our method outperforms ERM [27] Table 10: Text classification accuracy (%) on Amazon Reviews [5]. The models are trained on only one text domain and evaluated on other unseen text domains. Our method outperforms others in all settings except \"dvd \u2192 electronics\" and \"electronics \u2192 dvd\". The possible reason is that \"dvd\" and \"electronics\" may share a similar distribution while our method creates large distribution shift.  in accuracy on \"dvd \u2192 electronics\" and \"electronics \u2192 dvd\". One possible reason is that \"electronics\" and \"dvd\" may share a similar distribution. And our method creates large distribution shift, degrading the performance on them.\n\n\nB.4. Ablation Study\n\nWe study the effect of two important hyper-parameters of our model: the number of Monte-Carlo (MC) samples (K) and the coefficient of constraint (\u03b2). We report the average accuracy on the four unseen domains (MNIST-M [13], SVHN [44], SYN [13], and USPS [7]). We present the classification results under different hyper-parameters in Fig. 9.\n\n1) Number of MC samples (K). The classification accuracy on Digits [59] with different K is shown in Fig. 9 (a). We notice that the average accuracy gradually increases from K = 1 to K = 15 and remains stable when K = 20. Average Accuracy (%) Figure 9: Ablation study on hyper-parameters K and \u03b2. The average accuracy on the four unseen domains (MNIST-M [13], SVHN [44], SYN [13], and USPS [7]) is presented. We set K = 15 and \u03b2 = 1 according to the best classification accuracy.\n\n2) Coefficient of constraint (\u03b2). The constraint is used to make adversarial domain augmentation satisfy the worstcase constraint. Results on Digits [59] with different \u03b2 is presented in Fig. 9 (b). As seen, the accuracy falls dramatically when \u03b2 = 10, because large \u03b2 may severely limit the domain transportation and create domain augmentations similar to the source.\n\nFigure 1 :\n1The main and auxiliary models.\n\nFigure 2 :\n2tion tasks: USPS(U)\u2192MNIST(M), MNIST(M)\u2192SVHN(S), and SVHN(S)\u2192MNIST(M). Results of the three tasks are shown in Tab. 2.Our method achieves the best performance on the average of three tasks. The result on the hardest task (M\u2192S) is even competitive to that of SBADA[48] which Uncertainty estimation on Digits (left) and CIFAR-10-C (right). Our prediction of domain uncertainty is consistent with Bayesian uncertainty, while our method is an order of magnitude faster since we forward data only once.\n\nFigure 3 :\n3Visualization of feature perturbation |e| = |h + \u2212 h| (Top) and embedding of domains (Bottom) at different training iterations T on MNIST. Left: Models w/o uncertainty; Right: Models w/ uncertainty. Most perturbations are located in the background area and models w/ uncertainty can create large domain transportation in a curriculum learning scheme.\n\nFigure 4 :\n4Visualization of label mixup y + on MNIST. Models w/ uncertainty can encourage more smoothing labels and significantly increase the capacity of label space.\n\nFigure 5 :Figure 6 :\n56Architectures of main and auxiliary models. From left to right: (a) Digits [59]; (b) CIFAR-10-C [20]; (c) SYTHIA [47]; (d) Google Commands[62], and (e) Amazon Reviews[5]. Classification accuracy on fifteen corruptions of CIFAR-10-C using the backbone of WRN (40-2). FollowingFig. 7, the accuracy of each corruption with the highest level of severity is presented. Our method achieves 20% improvements on corruptions of noise.\n\nFigure 7 :\n7Classification accuracy (%) on five levels of corruption severity. Our method has the smallest degradation under the highest level of corruption severity.Method|T | U \u2192 M M \u2192 S S \u2192 M Avg. DIRT-T[52] All\n\n9 :\n9Few-shot domain adaptation accuracy (%) on MNIST(M), USPS(U), and SVHN(S). |T | denotes the number of target samples (per class) used during model training. generalization capability on severe corruptions. 2) Few-shot domain adaptation. We conduct three few-shot domain adaption tasks: USPS(U)\u2192MNIST(M), MNIST(M)\u2192SVHN(S), and SVHN(S)\u2192MNIST(M).\n\nFigure 8 :\n8Examples of semantic segmentation on SYTHIA[47]. From left to right: (a) images from unseen domains; (b) ground truth; (c) results of ERM[27]; (d) results of M-ADA[46]; and (e) results of our method. Best viewed in color and zoom in for details.\n\nAlgorithm 1 :\n1Unseen Domain Generalization. Input: Source domain S, # of MC samples K. Output: Learned backbone \u03b8 and auxiliary \u03c8. 1 while not converged do Meta-train: Compute \u03b8 * on S using Eq. 4Generate S + from S using Eq. 12 \n\n3 \n\n4 \n\nfor k = 1, ..., K do \n\n5 \n\nSample feature perturbation h + \nk using Eq. 2 \n\n6 \n\nGenerate label mixup y + \nk using Eq. 3 \n\n7 \n\nMeta-test: Evaluate L(\u03b8  *  ; S + ) w.r.t. S + \n\n8 \n\nend \n\n9 \n\n\n\n\nDomainMixup[66] PAR[60] Self-super[21] JiGen[4] ERM[27] GUD[59] M-ADA[46] OursTable 1: Image classification accuracy (%) on Digits[59] (top) and CIFAR-10-C[20] (bottom). We compare with robust training(Columns 1-4)and domain generalization (Columns 5-7). For Digits, all models are trained on MNIST[30]. For CIFAR-10-C, two widely employed backbones are evaluated. Our method outperforms M-ADA[46] (previous SOTA) consistently in all settings.SVHN [30] \n28.5 \n30.5 \n30.0 \n33.8 \n27.8 \n35.5 \n42.6 \n43.3 \nMNIST-M [13] \n54.0 \n58.4 \n58.1 \n57.8 \n52.8 \n60.4 \n67.9 \n67.4 \nSYN [13] \n41.2 \n44.1 \n41.9 \n43.8 \n39.9 \n45.3 \n49.0 \n57.1 \nUSPS [7] \n76.6 \n76.9 \n77.1 \n77.2 \n76.5 \n77.3 \n78.5 \n77.4 \n\nAvg. \n50.1 \n52.5 \n51.8 \n53.1 \n49.3 \n54.6 \n59.5 \n61.3 \n\nModel \nMixup [66] Cutout [8] AutoAug [6] PGD [39] ERM [27] GUD [59] M-ADA [46] Ours \nAllConv [49] \n75.4 \n67.1 \n70.8 \n71.9 \n69.2 \n73.6 \n75.9 \n79.6 \nWRN [65] \n77.7 \n73.2 \n76.1 \n73.8 \n73.1 \n75.3 \n80.2 \n83.4 \n\n\n\nTable 2 :\n2Few-shot domain adaptation \naccuracy (%) on MNIST(M), USPS(U), \nand SVHN(S). |T | denotes the number \nof target samples (per class) used during \nmodel training. \n\n\n\nTable 3 :\n3Semantic segmentation mIoU (%) on SYNTHIA[47]. All models are trained on the single source from Highway and evaluated on unseen environments from New York-like City and Old European Town.uses all images of the target domain for training. Full results \nare provided in supplementary. \n\n\n\nTable 4 :\n4Text classification accuracy (%) on Amazon Reviews. Models are trained on one text domain and evaluated on unseen text domains. Our method outperforms others in all settings except \"dvd \u2192 electronics\".Time \nFrequency \n\nMethod \nAmp. Pit. Noise Stretch Shift \nERM [27] \n63.8 71.6 73.9 \n72.9 \n70.5 \nGUD [59] \n64.1 72.1 74.8 \n73.1 \n70.9 \nM-ADA [46] 64.5 71.9 75.4 \n73.8 \n71.4 \nOurs \n65.3 73.5 75.8 \n75.0 \n72.5 \n\n\n\nTable 5 :\n5Speech recognition accuracy (%) on Google Com-\nmands. Models are trained on clean set and evaluated on five \ncorrupted sets. Results validate our strong generalization on \ncorruptions in both time and frequency domains. \n\n\n\nTable 7 :\n7Ablation study of label mixup.Digits [59] CIFAR-10-C [20] \nFull Model \n61.3\u00b10.73 \n70.2\u00b10.62 \nw/o adv. training \n51.8\u00b10.71 \n60.0\u00b10.55 \nw/o meta-learning \n60.9\u00b11.24 \n68.7\u00b10.81 \nw/o minimizing \u03c6 p 60.6\u00b10.91 \n69.6\u00b10.75 \n\n\n\nTable 8 :\n8Ablation study of training strategy.\n\nTable\n\n\n\nby 3.5% on \"books \u2192 electronics\". We observe that there is a little drop ADA [46] 79.4 76.1 65.3 78.8 82.6 74.3 75.2 77.3 82.3 69.0 73.7 84.8books \n\ndvd \nkitchen \nelectronics \n\nMethod \nd \nk \ne \nb \nk \ne \nb \nd \ne \nb \nd \nk \nERM [27] \n78.7 74.6 63.6 78.5 82.1 75.2 75.4 76.0 81.2 69.4 74.8 83.9 \nGUD [59] \n79.1 75.6 64.7 78.1 82.0 74.6 74.9 76.7 81.6 68.9 74.2 84.4 \nM-Ours \n80.2 76.8 67.1 80.1 83.5 75.0 76.1 78.2 83.5 70.2 74.5 85.7 \n\n\n\nTowards domain generalization using metaregularization. Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa, Metareg, NeurIPS. Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, pages 998-1008, 2018. 2\n\nCurriculum learning. Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, Jason Weston, ICML. 3Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, pages 41-48, 2009. 3\n\nWeight uncertainty in neural network. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, ICML. 5Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In ICML, pages 1613-1622, 2015. 2, 4, 5\n\nDomain generalization by solving jigsaw puzzles. M Fabio, Antonio D&apos; Carlucci, Silvia Innocente, Barbara Bucci, Tatiana Caputo, Tommasi, CVPR. 15Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Bar- bara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In CVPR, pages 2229-2238, 2019. 1, 2, 5\n\nMarginalized denoising autoencoders for domain adaptation. Minmin Chen, Zhixiang Xu, Q Kilian, Fei Weinberger, Sha, ICML. 1213Minmin Chen, Zhixiang Xu, Kilian Q Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adap- tation. In ICML, pages 1627-1634, 2012. 5, 6, 11, 12, 13\n\nAutoaugment: Learning augmentation strategies from data. Barret Ekin D Cubuk, Dandelion Zoph, Vijay Mane, Quoc V Vasudevan, Le, CVPR. 15Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In CVPR, pages 113-123, 2019. 1, 5\n\nNeural network recognizer for hand-written zip code digits. John S Denker, Hans Peter Gardner, Donnie Graf, Richard E Henderson, W Howard, Hubbard, D Lawrence, Jackel, S Henry, Isabelle Baird, Guyon, NeurIPS. 513John S Denker, WR Gardner, Hans Peter Graf, Donnie Hen- derson, Richard E Howard, W Hubbard, Lawrence D Jackel, Henry S Baird, and Isabelle Guyon. Neural network recog- nizer for hand-written zip code digits. In NeurIPS, pages 323-331, 1989. 5, 13\n\nTerrance Devries, W Graham, Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. 15arXiv preprintTerrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 1, 5\n\nDomain generalization via model-agnostic learning of semantic features. Qi Dou, Daniel Coelho De Castro, Konstantinos Kamnitsas, Ben Glocker, NeurIPS. 1Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, pages 6447-6458, 2019. 1, 2\n\nModelagnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, ICML. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126-1135, 2017. 2\n\nProbabilistic model-agnostic meta-learning. Chelsea Finn, Kelvin Xu, Sergey Levine, NeurIPS. 1Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In NeurIPS, pages 9516-9527, 2018. 1, 2\n\nSelfensembling for visual domain adaptation. Geoffrey French, Michal Mackiewicz, Mark Fisher, ICLR. 612Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self- ensembling for visual domain adaptation. In ICLR, 2018. 5, 6, 12\n\nUnsupervised Domain Adaptation by Backpropagation. Yaroslav Ganin, Victor Lempitsky, ICML. 613Yaroslav Ganin and Victor Lempitsky. Unsupervised Domain Adaptation by Backpropagation. In ICML, pages 1180-1189, 2015. 5, 6, 13\n\nDomain generalization for object recognition with multi-task autoencoders. Muhammad Ghifary, Mengjie Bastiaan Kleijn, David Zhang, Balduzzi, ICCV. 1Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, pages 2551-2559, 2015. 1, 2\n\nExplaining and harnessing adversarial examples. J Ian, Jonathon Goodfellow, Christian Shlens, Szegedy, ICLR. 1Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015. 1, 2\n\nRecasting gradient-based meta-learning as hierarchical bayes. Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, Thomas Griffiths, In ICLR. 2Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. In ICLR, 2018. 2\n\nPractical variational inference for neural networks. Alex Graves, NeurIPS. Alex Graves. Practical variational inference for neural net- works. In NeurIPS, pages 2348-2356, 2011. 2\n\nMulti-domain transfer component analysis for domain generalization. Thomas Grubinger, Adriana Birlutiu, Holger Sch\u00f6ner, Thomas Natschl\u00e4ger, Tom Heskes, Neural Processing Letters (NPL). 463Thomas Grubinger, Adriana Birlutiu, Holger Sch\u00f6ner, Thomas Natschl\u00e4ger, and Tom Heskes. Multi-domain trans- fer component analysis for domain generalization. Neural Processing Letters (NPL), 46(3):845-855, 2017. 2\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 611Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In CVPR, pages 770-778, 2016. 6, 11\n\nBenchmarking neural network robustness to common corruptions and perturbations. ICLR. Dan Hendrycks, Thomas Dietterich, 1112Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. ICLR, 2019. 5, 7, 8, 11, 12\n\nUsing self-supervised learning can improve model robustness and uncertainty. Dan Hendrycks, Mantas Mazeika, NeurIPS. 25Saurav Kadavath, and Dawn SongDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty. In NeurIPS, pages 15637- 15648, 2019. 2, 5\n\nAugmix: A simple data processing method to improve robustness and uncertainty. Dan Hendrycks, Norman Mu, D Ekin, Barret Cubuk, Justin Zoph, Balaji Gilmer, Lakshminarayanan, ICLR, 2020. 15Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. 1, 5\n\nKeeping the neural networks simple by minimizing the description length of the weights. E Geoffrey, Drew Hinton, Van Camp, COLT. Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In COLT, pages 5-13, 1993. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. 11Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448-456, 2015. 11\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.698011cs.LGDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In arXiv:1412.6980 [cs.LG], 2014. 11\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, ICLR. Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. In ICLR, 2014. 4\n\nOracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole d'Et\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008. Vladimir Koltchinskii, 2033Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole d'Et\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008, volume 2033.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5\n\nBackpropagation applied to handwritten zip code recognition. Yann Lecun, Bernhard Boser, S John, Donnie Denker, Richard E Henderson, Wayne Howard, Lawrence D Hubbard, Jackel, Neural Computation (NC). 14Yann LeCun, Bernhard Boser, John S Denker, Donnie Hen- derson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation (NC), 1(4):541-551, 1989. 5\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. the IEEE867Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 5, 7\n\nGradient-Based Learning Applied to Document Recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 861111Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11), 1998. 7, 11\n\nLearning to balance: Bayesian meta-learning for imbalanced and outof-distribution tasks. Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, Sung Ju Hwang, ICLR, 2020. 24Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, and Sung Ju Hwang. Learning to balance: Bayesian meta-learning for imbalanced and out- of-distribution tasks. In ICLR, 2020. 2, 4\n\nMeta dropout: Learning to perturb latent features for generalization. Hae Beom Lee, Taewook Nam, Eunho Yang, Sung Ju Hwang, ICLR. Hae Beom Lee, Taewook Nam, Eunho Yang, and Sung Ju Hwang. Meta dropout: Learning to perturb latent features for generalization. In ICLR, 2019. 2\n\nDeeper, Broader and Artier Domain Generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, ICCV. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, Broader and Artier Domain Generaliza- tion. In ICCV, pages 5542-5550, 2017. 2\n\nLearning to Generalize: Meta-Learning for Domain Generalization. Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales, AAAI. 1Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to Generalize: Meta-Learning for Do- main Generalization. In AAAI, 2018. 1, 2\n\nTransferable adversarial training: A general approach to adapting deep classifiers. Hong Liu, Mingsheng Long, Jianmin Wang, Michael Jordan, ICML. Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable adversarial training: A general approach to adapting deep classifiers. In ICML, pages 4013-4022, 2019. 1\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, CVPR. 611Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431-3440, 2015. 6, 11\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of Machine Learning Research. 97JMLRLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research (JMLR), 9(Nov):2579-2605, 2008. 7\n\nTowards deep learning models resistant to adversarial attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, ICLR. 25Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018. 2, 5\n\nFew-shot adversarial domain adaptation. Saeid Motiian, Quinn Jones, Seyed Iranmanesh, Gianfranco Doretto, NeurIPS. 612Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gian- franco Doretto. Few-shot adversarial domain adaptation. In NeurIPS, pages 6670-6680, 2017. 1, 5, 6, 12\n\nUnified Deep Supervised Domain Adaptation and Generalization. Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, Gianfranco Doretto, ICCV. 612Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gian- franco Doretto. Unified Deep Supervised Domain Adaptation and Generalization. In ICCV, pages 5715-5725, 2017. 5, 6, 12\n\nDomain Generalization via Invariant Feature Representation. Krikamol Muandet, David Balduzzi, Bernhard Sch\u00f6lkopf, ICML. Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain Generalization via Invariant Feature Representation. In ICML, pages 10-18, 2013. 1\n\nImage to image translation for domain adaptation. Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, Kyungnam Kim, CVPR. Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ra- mamoorthi, and Kyungnam Kim. Image to image translation for domain adaptation. In CVPR, pages 4500-4509, 2018. 1\n\nReading digits in natural images with unsupervised feature learning. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y Ng, NIPS Workshop on Deep Learning and Unsupervised Feature Learning. 513Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. 5, 13\n\nJointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation. Xi Peng, Zhiqiang Tang, Fei Yang, S Rogerio, Dimitris Feris, Metaxas, CVPR. Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and Dimitris Metaxas. Jointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation. In CVPR, pages 2226-2234, 2018. 1\n\nLearning to learn single domain generalization. Fengchun Qiao, Long Zhao, Xi Peng, CVPR. 13Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In CVPR, 2020. 1, 2, 3, 4, 5, 6, 7, 11, 13\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M Lopez, CVPR. 1213German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmenta- tion of urban scenes. In CVPR, pages 3234-3243, 2016. 5, 6, 11, 12, 13\n\nFrom source to target and back: symmetric bi-directional adaptive gan. Paolo Russo, Fabio M Carlucci, Tatiana Tommasi, Barbara Caputo, CVPR. 612Paolo Russo, Fabio M Carlucci, Tatiana Tommasi, and Bar- bara Caputo. From source to target and back: symmetric bi-directional adaptive gan. In CVPR, pages 8099-8108, 2018. 5, 6, 12\n\nWeight normalization: A simple reparameterization to accelerate training of deep neural networks. Tim Salimans, P Durk, Kingma, NeurIPS. 511Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NeurIPS, pages 901-909, 2016. 5, 11\n\nEvolutionary principles in selfreferential learning. J\u00fcrgen Schmidhuber, Technische Universit\u00e4t M\u00fcnchenPhD thesisJ\u00fcrgen Schmidhuber. Evolutionary principles in self- referential learning. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987. 2\n\nGeneralizing Across Domains via Cross-Gradient Training. Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, Sunita Sarawagi, ICLR. Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing Across Domains via Cross-Gradient Training. In ICLR, 2018. 2\n\nA dirt-t approach to unsupervised domain adaptation. Rui Shu, H Hung, Hirokazu Bui, Stefano Narui, Ermon, ICLR. 612Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. 5, 6, 12\n\nCertifying distributional robustness with principled adversarial training. Aman Sinha, Hongseok Namkoong, John Duchi, ICLR. 23Aman Sinha, Hongseok Namkoong, and John Duchi. Cer- tifying distributional robustness with principled adversarial training. In ICLR, 2018. 2, 3\n\nDisentangling adversarial robustness and generalization. David Stutz, Matthias Hein, Bernt Schiele, CVPR. David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In CVPR, pages 6976-6987, 2019. 2\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, CVPR. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception ar- chitecture for computer vision. In CVPR, pages 2818-2826, 2016. 4\n\nIntriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, In ICLR. 2Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014. 2\n\nLearning to learn. Sebastian Thrun, Lorien Pratt, Springer Science & Business MediaSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012. 2\n\nWasserstein auto-encoders. I Tolstikhin, Bousquet, B Gelly, Sch\u00f6lkopf, In ICLR. 3I Tolstikhin, O Bousquet, S Gelly, and B Sch\u00f6lkopf. Wasser- stein auto-encoders. In ICLR, 2018. 3\n\nGeneralizing to unseen domains via adversarial data augmentation. Riccardo Volpi, Hongseok Namkoong, Ozan Sener, C John, Vittorio Duchi, Silvio Murino, Savarese, NeurIPS. 1213Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In NeurIPS, pages 5334-5344, 2018. 1, 2, 4, 5, 6, 7, 8, 11, 12, 13\n\nLearning robust global representations by penalizing local predictive power. Haohan Wang, Songwei Ge, Zachary Lipton, Eric P Xing, NeurIPS. 25Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, pages 10506-10518, 2019. 2, 5\n\nLearning from extrinsic and intrinsic supervisions for domain generalization. Shujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, Pheng-Ann Heng, ECCV. SpringerShujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, and Pheng- Ann Heng. Learning from extrinsic and intrinsic supervi- sions for domain generalization. In ECCV, pages 159-176. Springer, 2020. 2\n\nPete Warden, arXiv:1804.03209Speech commands: A dataset for limited-vocabulary speech recognition. 1112arXiv preprintPete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018. 5, 7, 11, 12\n\nGurumurthy Swaminathan, and Orchid Majumder. d-sne: Domain adaptation using stochastic neighborhood embedding. Xiang Xu, Xiong Zhou, Ragav Venkatesan, CVPR. Xiang Xu, Xiong Zhou, Ragav Venkatesan, Gurumurthy Swaminathan, and Orchid Majumder. d-sne: Domain adap- tation using stochastic neighborhood embedding. In CVPR, pages 2497-2506, 2019. 1\n\nBayesian model-agnostic meta-learning. Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, Sungjin Ahn, NeurIPS. Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. In NeurIPS, pages 7332-7342, 2018. 2\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, BMVC. 511Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. 5, 11\n\nmixup: Beyond empirical risk minimization. Hongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, ICLR. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. 1, 4, 5\n", "annotations": {"author": "[{\"end\":141,\"start\":61},{\"end\":214,\"start\":142}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":149,\"start\":145}]", "author_first_name": "[{\"end\":69,\"start\":61},{\"end\":144,\"start\":142}]", "author_affiliation": "[{\"end\":140,\"start\":94},{\"end\":213,\"start\":167}]", "title": "[{\"end\":58,\"start\":1},{\"end\":272,\"start\":215}]", "venue": null, "abstract": "[{\"end\":1338,\"start\":274}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1920,\"start\":1916},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1923,\"start\":1920},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":1926,\"start\":1923},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1929,\"start\":1926},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2039,\"start\":2035},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2042,\"start\":2039},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2045,\"start\":2042},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2047,\"start\":2045},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2049,\"start\":2047},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2401,\"start\":2397},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2576,\"start\":2572},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2850,\"start\":2846},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3129,\"start\":3125},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3132,\"start\":3129},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3134,\"start\":3132},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3136,\"start\":3134},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3139,\"start\":3136},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":3308,\"start\":3304},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3844,\"start\":3840},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3847,\"start\":3844},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3850,\"start\":3847},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3852,\"start\":3850},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3854,\"start\":3852},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4056,\"start\":4052},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4496,\"start\":4492},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4499,\"start\":4496},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6136,\"start\":6132},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6139,\"start\":6136},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6142,\"start\":6139},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6145,\"start\":6142},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6147,\"start\":6145},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6149,\"start\":6147},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6205,\"start\":6202},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6324,\"start\":6320},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6452,\"start\":6448},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6592,\"start\":6588},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6754,\"start\":6750},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6769,\"start\":6765},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7002,\"start\":6998},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7005,\"start\":7002},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7008,\"start\":7005},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7055,\"start\":7051},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7147,\"start\":7143},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7261,\"start\":7257},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7370,\"start\":7366},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7488,\"start\":7484},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7593,\"start\":7589},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7728,\"start\":7724},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7875,\"start\":7871},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8222,\"start\":8218},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8225,\"start\":8222},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8354,\"start\":8350},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8563,\"start\":8559},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8565,\"start\":8563},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8567,\"start\":8565},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8662,\"start\":8658},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8769,\"start\":8766},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8905,\"start\":8902},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9216,\"start\":9212},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9219,\"start\":9216},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9221,\"start\":9219},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9442,\"start\":9438},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9445,\"start\":9442},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9448,\"start\":9445},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9451,\"start\":9448},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9532,\"start\":9528},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9581,\"start\":9577},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9631,\"start\":9627},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9741,\"start\":9737},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9868,\"start\":9864},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9993,\"start\":9989},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12238,\"start\":12235},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13144,\"start\":13140},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13857,\"start\":13853},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":13897,\"start\":13893},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":14415,\"start\":14411},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14418,\"start\":14415},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":15658,\"start\":15654},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16573,\"start\":16569},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18045,\"start\":18041},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18591,\"start\":18587},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19464,\"start\":19461},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19756,\"start\":19752},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19773,\"start\":19769},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19787,\"start\":19783},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":19822,\"start\":19818},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19833,\"start\":19830},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19845,\"start\":19842},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19862,\"start\":19859},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19897,\"start\":19893},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19906,\"start\":19902},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19918,\"start\":19914},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19929,\"start\":19925},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19944,\"start\":19940},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19980,\"start\":19976},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":19990,\"start\":19986},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20006,\"start\":20002},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20146,\"start\":20142},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20174,\"start\":20170},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20199,\"start\":20196},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":20228,\"start\":20224},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20513,\"start\":20509},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20527,\"start\":20523},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20538,\"start\":20534},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20548,\"start\":20544},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20562,\"start\":20559},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20731,\"start\":20727},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20862,\"start\":20858},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":21074,\"start\":21070},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21214,\"start\":21210},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21497,\"start\":21493},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21584,\"start\":21580},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":21621,\"start\":21617},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":21794,\"start\":21790},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21806,\"start\":21802},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21906,\"start\":21902},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22294,\"start\":22291},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22861,\"start\":22857},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22931,\"start\":22927},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23120,\"start\":23116},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23773,\"start\":23769},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23776,\"start\":23773},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23907,\"start\":23903},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23943,\"start\":23939},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24319,\"start\":24316},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24549,\"start\":24545},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24764,\"start\":24760},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24933,\"start\":24930},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":25391,\"start\":25387},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26296,\"start\":26292},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":26399,\"start\":26395},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27036,\"start\":27032},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27114,\"start\":27110},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27364,\"start\":27360},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27383,\"start\":27379},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31913,\"start\":31909},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31930,\"start\":31926},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31943,\"start\":31939},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31963,\"start\":31960},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":31989,\"start\":31985},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":32257,\"start\":32253},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32725,\"start\":32721},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32849,\"start\":32845},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32917,\"start\":32913},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32954,\"start\":32950},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33005,\"start\":33001},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33135,\"start\":33131},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":33535,\"start\":33531},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33978,\"start\":33974},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":34163,\"start\":34159},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":34176,\"start\":34172},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":34598,\"start\":34594},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34766,\"start\":34762},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34787,\"start\":34783},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34823,\"start\":34819},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34906,\"start\":34902},{\"end\":35329,\"start\":35326},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35351,\"start\":35348},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35697,\"start\":35693},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":35801,\"start\":35797},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35855,\"start\":35851},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36192,\"start\":36188},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36522,\"start\":36519},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":36547,\"start\":36543},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":36660,\"start\":36656},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36683,\"start\":36680},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36837,\"start\":36833},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37087,\"start\":37083},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":37097,\"start\":37093},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37113,\"start\":37109},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37337,\"start\":37333},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37934,\"start\":37930},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":38362,\"start\":38358},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":38501,\"start\":38497},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38813,\"start\":38810},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38988,\"start\":38984},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39053,\"start\":39050},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39855,\"start\":39851},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":39866,\"start\":39862},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39876,\"start\":39872},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39890,\"start\":39887},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":40047,\"start\":40043},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":40334,\"start\":40330},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40345,\"start\":40341},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":40355,\"start\":40351},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40369,\"start\":40366},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":40610,\"start\":40606},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":41149,\"start\":41145},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":42080,\"start\":42076},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":42107,\"start\":42104},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":42575,\"start\":42571},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":42990,\"start\":42986},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":43084,\"start\":43080},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43110,\"start\":43106},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":43637,\"start\":43633},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":43645,\"start\":43641},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":43660,\"start\":43656},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":43669,\"start\":43666},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":43677,\"start\":43673},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":43685,\"start\":43681},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43695,\"start\":43691},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":43756,\"start\":43752},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43781,\"start\":43777},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":43924,\"start\":43920},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44019,\"start\":44015},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":44798,\"start\":44794}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40869,\"start\":40826},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41379,\"start\":40870},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41743,\"start\":41380},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41913,\"start\":41744},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42363,\"start\":41914},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42579,\"start\":42364},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42929,\"start\":42580},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43188,\"start\":42930},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43619,\"start\":43189},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44564,\"start\":43620},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44740,\"start\":44565},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45038,\"start\":44741},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45459,\"start\":45039},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45694,\"start\":45460},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45924,\"start\":45695},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":45973,\"start\":45925},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":45981,\"start\":45974},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":46417,\"start\":45982}]", "paragraph": "[{\"end\":2385,\"start\":1354},{\"end\":3064,\"start\":2387},{\"end\":3802,\"start\":3066},{\"end\":4205,\"start\":3804},{\"end\":5192,\"start\":4207},{\"end\":5409,\"start\":5194},{\"end\":5603,\"start\":5411},{\"end\":5863,\"start\":5605},{\"end\":6063,\"start\":5865},{\"end\":7327,\"start\":6080},{\"end\":8187,\"start\":7329},{\"end\":9161,\"start\":8189},{\"end\":10274,\"start\":9163},{\"end\":10865,\"start\":10285},{\"end\":11858,\"start\":10867},{\"end\":12443,\"start\":11860},{\"end\":13019,\"start\":12485},{\"end\":13228,\"start\":13021},{\"end\":13293,\"start\":13290},{\"end\":13628,\"start\":13295},{\"end\":13969,\"start\":13630},{\"end\":14654,\"start\":13971},{\"end\":15109,\"start\":14701},{\"end\":15601,\"start\":15144},{\"end\":15908,\"start\":15603},{\"end\":16060,\"start\":15961},{\"end\":16467,\"start\":16062},{\"end\":16926,\"start\":16517},{\"end\":17464,\"start\":16950},{\"end\":17816,\"start\":17529},{\"end\":18209,\"start\":17818},{\"end\":18366,\"start\":18291},{\"end\":19038,\"start\":18461},{\"end\":19544,\"start\":19040},{\"end\":19788,\"start\":19560},{\"end\":20300,\"start\":19790},{\"end\":21030,\"start\":20325},{\"end\":21652,\"start\":21032},{\"end\":23073,\"start\":21654},{\"end\":23609,\"start\":23099},{\"end\":23944,\"start\":23611},{\"end\":24267,\"start\":23946},{\"end\":24637,\"start\":24291},{\"end\":24852,\"start\":24639},{\"end\":25338,\"start\":24854},{\"end\":26101,\"start\":25361},{\"end\":26318,\"start\":26103},{\"end\":26914,\"start\":26320},{\"end\":27211,\"start\":26933},{\"end\":27536,\"start\":27213},{\"end\":28176,\"start\":27538},{\"end\":29117,\"start\":28178},{\"end\":29741,\"start\":29119},{\"end\":30443,\"start\":29743},{\"end\":31797,\"start\":30458},{\"end\":32241,\"start\":31843},{\"end\":32829,\"start\":32243},{\"end\":34750,\"start\":32831},{\"end\":38299,\"start\":34752},{\"end\":38626,\"start\":38330},{\"end\":39610,\"start\":38655},{\"end\":39974,\"start\":39634},{\"end\":40455,\"start\":39976},{\"end\":40825,\"start\":40457}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13289,\"start\":13229},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14700,\"start\":14655},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15960,\"start\":15909},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16516,\"start\":16468},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17528,\"start\":17465},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18290,\"start\":18210},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18460,\"start\":18367}]", "table_ref": "[{\"end\":27495,\"start\":27488},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38997,\"start\":38989}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1352,\"start\":1340},{\"attributes\":{\"n\":\"2.\"},\"end\":6078,\"start\":6066},{\"attributes\":{\"n\":\"3.\"},\"end\":10283,\"start\":10277},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12483,\"start\":12446},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15142,\"start\":15112},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16948,\"start\":16929},{\"attributes\":{\"n\":\"4.\"},\"end\":19558,\"start\":19547},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20323,\"start\":20303},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23097,\"start\":23076},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24289,\"start\":24270},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25359,\"start\":25341},{\"attributes\":{\"n\":\"4.5.\"},\"end\":26931,\"start\":26917},{\"attributes\":{\"n\":\"5.\"},\"end\":30456,\"start\":30446},{\"end\":31841,\"start\":31800},{\"end\":38328,\"start\":38302},{\"end\":38653,\"start\":38629},{\"end\":39632,\"start\":39613},{\"end\":40837,\"start\":40827},{\"end\":40881,\"start\":40871},{\"end\":41391,\"start\":41381},{\"end\":41755,\"start\":41745},{\"end\":41935,\"start\":41915},{\"end\":42375,\"start\":42365},{\"end\":42584,\"start\":42581},{\"end\":42941,\"start\":42931},{\"end\":43203,\"start\":43190},{\"end\":44575,\"start\":44566},{\"end\":44751,\"start\":44742},{\"end\":45049,\"start\":45040},{\"end\":45470,\"start\":45461},{\"end\":45705,\"start\":45696},{\"end\":45935,\"start\":45926},{\"end\":45980,\"start\":45975}]", "table": "[{\"end\":43619,\"start\":43418},{\"end\":44564,\"start\":44065},{\"end\":44740,\"start\":44577},{\"end\":45038,\"start\":44940},{\"end\":45459,\"start\":45252},{\"end\":45694,\"start\":45472},{\"end\":45924,\"start\":45737},{\"end\":46417,\"start\":46125}]", "figure_caption": "[{\"end\":40869,\"start\":40839},{\"end\":41379,\"start\":40883},{\"end\":41743,\"start\":41393},{\"end\":41913,\"start\":41757},{\"end\":42363,\"start\":41938},{\"end\":42579,\"start\":42377},{\"end\":42929,\"start\":42586},{\"end\":43188,\"start\":42943},{\"end\":43418,\"start\":43205},{\"end\":44065,\"start\":43622},{\"end\":44940,\"start\":44753},{\"end\":45252,\"start\":45051},{\"end\":45737,\"start\":45707},{\"end\":45973,\"start\":45937},{\"end\":46125,\"start\":45984}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11814,\"start\":11808},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15407,\"start\":15401},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22459,\"start\":22453},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27572,\"start\":27566},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28099,\"start\":28093},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29339,\"start\":29329},{\"end\":32704,\"start\":32694},{\"end\":34556,\"start\":34546},{\"end\":36707,\"start\":36701},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37207,\"start\":37201},{\"end\":37426,\"start\":37420},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38520,\"start\":38514},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39973,\"start\":39967},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40083,\"start\":40077},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40227,\"start\":40219},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40650,\"start\":40644}]", "bib_author_first_name": "[{\"end\":46481,\"start\":46475},{\"end\":46495,\"start\":46490},{\"end\":46518,\"start\":46514},{\"end\":46740,\"start\":46734},{\"end\":46755,\"start\":46749},{\"end\":46772,\"start\":46767},{\"end\":46789,\"start\":46784},{\"end\":46970,\"start\":46963},{\"end\":46987,\"start\":46981},{\"end\":47004,\"start\":46999},{\"end\":47022,\"start\":47018},{\"end\":47243,\"start\":47242},{\"end\":47266,\"start\":47251},{\"end\":47283,\"start\":47277},{\"end\":47302,\"start\":47295},{\"end\":47317,\"start\":47310},{\"end\":47590,\"start\":47584},{\"end\":47605,\"start\":47597},{\"end\":47611,\"start\":47610},{\"end\":47623,\"start\":47620},{\"end\":47885,\"start\":47879},{\"end\":47909,\"start\":47900},{\"end\":47921,\"start\":47916},{\"end\":47934,\"start\":47928},{\"end\":48207,\"start\":48203},{\"end\":48213,\"start\":48208},{\"end\":48229,\"start\":48223},{\"end\":48243,\"start\":48236},{\"end\":48245,\"start\":48244},{\"end\":48258,\"start\":48257},{\"end\":48277,\"start\":48276},{\"end\":48297,\"start\":48296},{\"end\":48313,\"start\":48305},{\"end\":48597,\"start\":48589},{\"end\":48608,\"start\":48607},{\"end\":48956,\"start\":48954},{\"end\":48968,\"start\":48962},{\"end\":48999,\"start\":48987},{\"end\":49014,\"start\":49011},{\"end\":49294,\"start\":49287},{\"end\":49307,\"start\":49301},{\"end\":49322,\"start\":49316},{\"end\":49539,\"start\":49532},{\"end\":49552,\"start\":49546},{\"end\":49563,\"start\":49557},{\"end\":49764,\"start\":49756},{\"end\":49779,\"start\":49773},{\"end\":49796,\"start\":49792},{\"end\":49998,\"start\":49990},{\"end\":50012,\"start\":50006},{\"end\":50246,\"start\":50238},{\"end\":50263,\"start\":50256},{\"end\":50286,\"start\":50281},{\"end\":50545,\"start\":50544},{\"end\":50559,\"start\":50551},{\"end\":50581,\"start\":50572},{\"end\":50799,\"start\":50795},{\"end\":50814,\"start\":50807},{\"end\":50827,\"start\":50821},{\"end\":50842,\"start\":50836},{\"end\":50858,\"start\":50852},{\"end\":51096,\"start\":51092},{\"end\":51294,\"start\":51288},{\"end\":51313,\"start\":51306},{\"end\":51330,\"start\":51324},{\"end\":51346,\"start\":51340},{\"end\":51363,\"start\":51360},{\"end\":51676,\"start\":51669},{\"end\":51688,\"start\":51681},{\"end\":51704,\"start\":51696},{\"end\":51714,\"start\":51710},{\"end\":51956,\"start\":51953},{\"end\":51974,\"start\":51968},{\"end\":52217,\"start\":52214},{\"end\":52235,\"start\":52229},{\"end\":52552,\"start\":52549},{\"end\":52570,\"start\":52564},{\"end\":52576,\"start\":52575},{\"end\":52589,\"start\":52583},{\"end\":52603,\"start\":52597},{\"end\":52616,\"start\":52610},{\"end\":52943,\"start\":52942},{\"end\":52958,\"start\":52954},{\"end\":53238,\"start\":53232},{\"end\":53255,\"start\":53246},{\"end\":53484,\"start\":53483},{\"end\":53500,\"start\":53495},{\"end\":53684,\"start\":53683},{\"end\":53698,\"start\":53695},{\"end\":53956,\"start\":53948},{\"end\":54209,\"start\":54205},{\"end\":54230,\"start\":54222},{\"end\":54409,\"start\":54405},{\"end\":54425,\"start\":54417},{\"end\":54434,\"start\":54433},{\"end\":54447,\"start\":54441},{\"end\":54463,\"start\":54456},{\"end\":54465,\"start\":54464},{\"end\":54482,\"start\":54477},{\"end\":54499,\"start\":54491},{\"end\":54501,\"start\":54500},{\"end\":54835,\"start\":54831},{\"end\":54847,\"start\":54843},{\"end\":54862,\"start\":54856},{\"end\":54878,\"start\":54871},{\"end\":55160,\"start\":55156},{\"end\":55172,\"start\":55168},{\"end\":55187,\"start\":55181},{\"end\":55203,\"start\":55196},{\"end\":55500,\"start\":55497},{\"end\":55517,\"start\":55511},{\"end\":55531,\"start\":55523},{\"end\":55543,\"start\":55536},{\"end\":55556,\"start\":55549},{\"end\":55568,\"start\":55563},{\"end\":55579,\"start\":55575},{\"end\":55582,\"start\":55580},{\"end\":55886,\"start\":55883},{\"end\":55904,\"start\":55897},{\"end\":55915,\"start\":55910},{\"end\":55926,\"start\":55922},{\"end\":55929,\"start\":55927},{\"end\":56141,\"start\":56139},{\"end\":56153,\"start\":56146},{\"end\":56166,\"start\":56160},{\"end\":56180,\"start\":56173},{\"end\":56182,\"start\":56181},{\"end\":56416,\"start\":56414},{\"end\":56428,\"start\":56421},{\"end\":56441,\"start\":56435},{\"end\":56455,\"start\":56448},{\"end\":56457,\"start\":56456},{\"end\":56714,\"start\":56710},{\"end\":56729,\"start\":56720},{\"end\":56743,\"start\":56736},{\"end\":56757,\"start\":56750},{\"end\":57015,\"start\":57007},{\"end\":57026,\"start\":57022},{\"end\":57044,\"start\":57038},{\"end\":57246,\"start\":57239},{\"end\":57271,\"start\":57263},{\"end\":57543,\"start\":57533},{\"end\":57561,\"start\":57551},{\"end\":57577,\"start\":57571},{\"end\":57595,\"start\":57587},{\"end\":57611,\"start\":57605},{\"end\":57846,\"start\":57841},{\"end\":57861,\"start\":57856},{\"end\":57874,\"start\":57869},{\"end\":57897,\"start\":57887},{\"end\":58146,\"start\":58141},{\"end\":58161,\"start\":58156},{\"end\":58180,\"start\":58174},{\"end\":58182,\"start\":58181},{\"end\":58202,\"start\":58192},{\"end\":58471,\"start\":58463},{\"end\":58486,\"start\":58481},{\"end\":58505,\"start\":58497},{\"end\":58725,\"start\":58722},{\"end\":58739,\"start\":58733},{\"end\":58754,\"start\":58749},{\"end\":58769,\"start\":58765},{\"end\":58791,\"start\":58783},{\"end\":59043,\"start\":59038},{\"end\":59055,\"start\":59052},{\"end\":59066,\"start\":59062},{\"end\":59085,\"start\":59075},{\"end\":59098,\"start\":59096},{\"end\":59111,\"start\":59103},{\"end\":59533,\"start\":59531},{\"end\":59548,\"start\":59540},{\"end\":59558,\"start\":59555},{\"end\":59566,\"start\":59565},{\"end\":59584,\"start\":59576},{\"end\":59884,\"start\":59876},{\"end\":59895,\"start\":59891},{\"end\":59904,\"start\":59902},{\"end\":60159,\"start\":60153},{\"end\":60170,\"start\":60165},{\"end\":60186,\"start\":60180},{\"end\":60205,\"start\":60200},{\"end\":60224,\"start\":60215},{\"end\":60556,\"start\":60551},{\"end\":60569,\"start\":60564},{\"end\":60571,\"start\":60570},{\"end\":60589,\"start\":60582},{\"end\":60606,\"start\":60599},{\"end\":60908,\"start\":60905},{\"end\":60920,\"start\":60919},{\"end\":61176,\"start\":61170},{\"end\":61419,\"start\":61415},{\"end\":61435,\"start\":61429},{\"end\":61451,\"start\":61445},{\"end\":61475,\"start\":61465},{\"end\":61494,\"start\":61487},{\"end\":61509,\"start\":61503},{\"end\":61766,\"start\":61763},{\"end\":61773,\"start\":61772},{\"end\":61788,\"start\":61780},{\"end\":61801,\"start\":61794},{\"end\":62038,\"start\":62034},{\"end\":62054,\"start\":62046},{\"end\":62069,\"start\":62065},{\"end\":62292,\"start\":62287},{\"end\":62308,\"start\":62300},{\"end\":62320,\"start\":62315},{\"end\":62543,\"start\":62534},{\"end\":62560,\"start\":62553},{\"end\":62578,\"start\":62572},{\"end\":62589,\"start\":62586},{\"end\":62606,\"start\":62598},{\"end\":62851,\"start\":62842},{\"end\":62869,\"start\":62861},{\"end\":62883,\"start\":62879},{\"end\":62899,\"start\":62895},{\"end\":62914,\"start\":62907},{\"end\":62925,\"start\":62922},{\"end\":62941,\"start\":62938},{\"end\":63160,\"start\":63151},{\"end\":63174,\"start\":63168},{\"end\":63364,\"start\":63363},{\"end\":63566,\"start\":63558},{\"end\":63582,\"start\":63574},{\"end\":63597,\"start\":63593},{\"end\":63606,\"start\":63605},{\"end\":63621,\"start\":63613},{\"end\":63635,\"start\":63629},{\"end\":63983,\"start\":63977},{\"end\":63997,\"start\":63990},{\"end\":64009,\"start\":64002},{\"end\":64022,\"start\":64018},{\"end\":64024,\"start\":64023},{\"end\":64304,\"start\":64298},{\"end\":64317,\"start\":64311},{\"end\":64327,\"start\":64322},{\"end\":64340,\"start\":64332},{\"end\":64354,\"start\":64345},{\"end\":64570,\"start\":64566},{\"end\":64935,\"start\":64930},{\"end\":64945,\"start\":64940},{\"end\":64957,\"start\":64952},{\"end\":65209,\"start\":65203},{\"end\":65222,\"start\":65216},{\"end\":65235,\"start\":65228},{\"end\":65250,\"start\":65241},{\"end\":65262,\"start\":65256},{\"end\":65278,\"start\":65271},{\"end\":65485,\"start\":65479},{\"end\":65502,\"start\":65497},{\"end\":65656,\"start\":65650},{\"end\":65673,\"start\":65664},{\"end\":65686,\"start\":65681}]", "bib_author_last_name": "[{\"end\":46488,\"start\":46482},{\"end\":46512,\"start\":46496},{\"end\":46528,\"start\":46519},{\"end\":46537,\"start\":46530},{\"end\":46747,\"start\":46741},{\"end\":46765,\"start\":46756},{\"end\":46782,\"start\":46773},{\"end\":46796,\"start\":46790},{\"end\":46979,\"start\":46971},{\"end\":46997,\"start\":46988},{\"end\":47016,\"start\":47005},{\"end\":47031,\"start\":47023},{\"end\":47249,\"start\":47244},{\"end\":47275,\"start\":47267},{\"end\":47293,\"start\":47284},{\"end\":47308,\"start\":47303},{\"end\":47324,\"start\":47318},{\"end\":47333,\"start\":47326},{\"end\":47595,\"start\":47591},{\"end\":47608,\"start\":47606},{\"end\":47618,\"start\":47612},{\"end\":47634,\"start\":47624},{\"end\":47639,\"start\":47636},{\"end\":47898,\"start\":47886},{\"end\":47914,\"start\":47910},{\"end\":47926,\"start\":47922},{\"end\":47944,\"start\":47935},{\"end\":47948,\"start\":47946},{\"end\":48201,\"start\":48188},{\"end\":48221,\"start\":48214},{\"end\":48234,\"start\":48230},{\"end\":48255,\"start\":48246},{\"end\":48265,\"start\":48259},{\"end\":48274,\"start\":48267},{\"end\":48286,\"start\":48278},{\"end\":48294,\"start\":48288},{\"end\":48303,\"start\":48298},{\"end\":48319,\"start\":48314},{\"end\":48326,\"start\":48321},{\"end\":48605,\"start\":48598},{\"end\":48615,\"start\":48609},{\"end\":48623,\"start\":48617},{\"end\":48960,\"start\":48957},{\"end\":48985,\"start\":48969},{\"end\":49009,\"start\":49000},{\"end\":49022,\"start\":49015},{\"end\":49299,\"start\":49295},{\"end\":49314,\"start\":49308},{\"end\":49329,\"start\":49323},{\"end\":49544,\"start\":49540},{\"end\":49555,\"start\":49553},{\"end\":49570,\"start\":49564},{\"end\":49771,\"start\":49765},{\"end\":49790,\"start\":49780},{\"end\":49803,\"start\":49797},{\"end\":50004,\"start\":49999},{\"end\":50022,\"start\":50013},{\"end\":50254,\"start\":50247},{\"end\":50279,\"start\":50264},{\"end\":50292,\"start\":50287},{\"end\":50302,\"start\":50294},{\"end\":50549,\"start\":50546},{\"end\":50570,\"start\":50560},{\"end\":50588,\"start\":50582},{\"end\":50597,\"start\":50590},{\"end\":50805,\"start\":50800},{\"end\":50819,\"start\":50815},{\"end\":50834,\"start\":50828},{\"end\":50850,\"start\":50843},{\"end\":50868,\"start\":50859},{\"end\":51103,\"start\":51097},{\"end\":51304,\"start\":51295},{\"end\":51322,\"start\":51314},{\"end\":51338,\"start\":51331},{\"end\":51358,\"start\":51347},{\"end\":51370,\"start\":51364},{\"end\":51679,\"start\":51677},{\"end\":51694,\"start\":51689},{\"end\":51708,\"start\":51705},{\"end\":51718,\"start\":51715},{\"end\":51966,\"start\":51957},{\"end\":51985,\"start\":51975},{\"end\":52227,\"start\":52218},{\"end\":52243,\"start\":52236},{\"end\":52562,\"start\":52553},{\"end\":52573,\"start\":52571},{\"end\":52581,\"start\":52577},{\"end\":52595,\"start\":52590},{\"end\":52608,\"start\":52604},{\"end\":52623,\"start\":52617},{\"end\":52641,\"start\":52625},{\"end\":52952,\"start\":52944},{\"end\":52965,\"start\":52959},{\"end\":52975,\"start\":52967},{\"end\":53244,\"start\":53239},{\"end\":53263,\"start\":53256},{\"end\":53493,\"start\":53485},{\"end\":53507,\"start\":53501},{\"end\":53511,\"start\":53509},{\"end\":53693,\"start\":53685},{\"end\":53705,\"start\":53699},{\"end\":53714,\"start\":53707},{\"end\":53969,\"start\":53957},{\"end\":54220,\"start\":54210},{\"end\":54237,\"start\":54231},{\"end\":54415,\"start\":54410},{\"end\":54431,\"start\":54426},{\"end\":54439,\"start\":54435},{\"end\":54454,\"start\":54448},{\"end\":54475,\"start\":54466},{\"end\":54489,\"start\":54483},{\"end\":54509,\"start\":54502},{\"end\":54517,\"start\":54511},{\"end\":54841,\"start\":54836},{\"end\":54854,\"start\":54848},{\"end\":54869,\"start\":54863},{\"end\":54886,\"start\":54879},{\"end\":55166,\"start\":55161},{\"end\":55179,\"start\":55173},{\"end\":55194,\"start\":55188},{\"end\":55211,\"start\":55204},{\"end\":55509,\"start\":55501},{\"end\":55521,\"start\":55518},{\"end\":55534,\"start\":55532},{\"end\":55547,\"start\":55544},{\"end\":55561,\"start\":55557},{\"end\":55573,\"start\":55569},{\"end\":55588,\"start\":55583},{\"end\":55895,\"start\":55887},{\"end\":55908,\"start\":55905},{\"end\":55920,\"start\":55916},{\"end\":55935,\"start\":55930},{\"end\":56144,\"start\":56142},{\"end\":56158,\"start\":56154},{\"end\":56171,\"start\":56167},{\"end\":56193,\"start\":56183},{\"end\":56419,\"start\":56417},{\"end\":56433,\"start\":56429},{\"end\":56446,\"start\":56442},{\"end\":56468,\"start\":56458},{\"end\":56718,\"start\":56715},{\"end\":56734,\"start\":56730},{\"end\":56748,\"start\":56744},{\"end\":56764,\"start\":56758},{\"end\":57020,\"start\":57016},{\"end\":57036,\"start\":57027},{\"end\":57052,\"start\":57045},{\"end\":57261,\"start\":57247},{\"end\":57278,\"start\":57272},{\"end\":57549,\"start\":57544},{\"end\":57569,\"start\":57562},{\"end\":57585,\"start\":57578},{\"end\":57603,\"start\":57596},{\"end\":57617,\"start\":57612},{\"end\":57854,\"start\":57847},{\"end\":57867,\"start\":57862},{\"end\":57885,\"start\":57875},{\"end\":57905,\"start\":57898},{\"end\":58154,\"start\":58147},{\"end\":58172,\"start\":58162},{\"end\":58190,\"start\":58183},{\"end\":58210,\"start\":58203},{\"end\":58479,\"start\":58472},{\"end\":58495,\"start\":58487},{\"end\":58515,\"start\":58506},{\"end\":58731,\"start\":58726},{\"end\":58747,\"start\":58740},{\"end\":58763,\"start\":58755},{\"end\":58781,\"start\":58770},{\"end\":58795,\"start\":58792},{\"end\":59050,\"start\":59044},{\"end\":59060,\"start\":59056},{\"end\":59073,\"start\":59067},{\"end\":59094,\"start\":59086},{\"end\":59101,\"start\":59099},{\"end\":59114,\"start\":59112},{\"end\":59538,\"start\":59534},{\"end\":59553,\"start\":59549},{\"end\":59563,\"start\":59559},{\"end\":59574,\"start\":59567},{\"end\":59590,\"start\":59585},{\"end\":59599,\"start\":59592},{\"end\":59889,\"start\":59885},{\"end\":59900,\"start\":59896},{\"end\":59909,\"start\":59905},{\"end\":60163,\"start\":60160},{\"end\":60178,\"start\":60171},{\"end\":60198,\"start\":60187},{\"end\":60213,\"start\":60206},{\"end\":60230,\"start\":60225},{\"end\":60562,\"start\":60557},{\"end\":60580,\"start\":60572},{\"end\":60597,\"start\":60590},{\"end\":60613,\"start\":60607},{\"end\":60917,\"start\":60909},{\"end\":60925,\"start\":60921},{\"end\":60933,\"start\":60927},{\"end\":61188,\"start\":61177},{\"end\":61427,\"start\":61420},{\"end\":61443,\"start\":61436},{\"end\":61463,\"start\":61452},{\"end\":61485,\"start\":61476},{\"end\":61501,\"start\":61495},{\"end\":61518,\"start\":61510},{\"end\":61770,\"start\":61767},{\"end\":61778,\"start\":61774},{\"end\":61792,\"start\":61789},{\"end\":61807,\"start\":61802},{\"end\":61814,\"start\":61809},{\"end\":62044,\"start\":62039},{\"end\":62063,\"start\":62055},{\"end\":62075,\"start\":62070},{\"end\":62298,\"start\":62293},{\"end\":62313,\"start\":62309},{\"end\":62328,\"start\":62321},{\"end\":62551,\"start\":62544},{\"end\":62570,\"start\":62561},{\"end\":62584,\"start\":62579},{\"end\":62596,\"start\":62590},{\"end\":62612,\"start\":62607},{\"end\":62859,\"start\":62852},{\"end\":62877,\"start\":62870},{\"end\":62893,\"start\":62884},{\"end\":62905,\"start\":62900},{\"end\":62920,\"start\":62915},{\"end\":62936,\"start\":62926},{\"end\":62948,\"start\":62942},{\"end\":63166,\"start\":63161},{\"end\":63180,\"start\":63175},{\"end\":63351,\"start\":63339},{\"end\":63361,\"start\":63353},{\"end\":63370,\"start\":63365},{\"end\":63381,\"start\":63372},{\"end\":63572,\"start\":63567},{\"end\":63591,\"start\":63583},{\"end\":63603,\"start\":63598},{\"end\":63611,\"start\":63607},{\"end\":63627,\"start\":63622},{\"end\":63642,\"start\":63636},{\"end\":63652,\"start\":63644},{\"end\":63988,\"start\":63984},{\"end\":64000,\"start\":63998},{\"end\":64016,\"start\":64010},{\"end\":64029,\"start\":64025},{\"end\":64309,\"start\":64305},{\"end\":64320,\"start\":64318},{\"end\":64330,\"start\":64328},{\"end\":64343,\"start\":64341},{\"end\":64359,\"start\":64355},{\"end\":64577,\"start\":64571},{\"end\":64938,\"start\":64936},{\"end\":64950,\"start\":64946},{\"end\":64968,\"start\":64958},{\"end\":65214,\"start\":65210},{\"end\":65226,\"start\":65223},{\"end\":65239,\"start\":65236},{\"end\":65254,\"start\":65251},{\"end\":65269,\"start\":65263},{\"end\":65282,\"start\":65279},{\"end\":65495,\"start\":65486},{\"end\":65512,\"start\":65503},{\"end\":65662,\"start\":65657},{\"end\":65679,\"start\":65674},{\"end\":65701,\"start\":65687},{\"end\":65712,\"start\":65703}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":46711,\"start\":46419},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":873046},\"end\":46923,\"start\":46713},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":39895556},\"end\":47191,\"start\":46925},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":81978372},\"end\":47523,\"start\":47193},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10686834},\"end\":47820,\"start\":47525},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":196208260},\"end\":48126,\"start\":47822},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15860720},\"end\":48587,\"start\":48128},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b7\"},\"end\":48880,\"start\":48589},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202768984},\"end\":49219,\"start\":48882},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6719686},\"end\":49486,\"start\":49221},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":46977722},\"end\":49709,\"start\":49488},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3411732},\"end\":49937,\"start\":49711},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6755881},\"end\":50161,\"start\":49939},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12825123},\"end\":50494,\"start\":50163},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6706414},\"end\":50731,\"start\":50496},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3484654},\"end\":51037,\"start\":50733},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14885866},\"end\":51218,\"start\":51039},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":25534524},\"end\":51621,\"start\":51220},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":51865,\"start\":51623},{\"attributes\":{\"id\":\"b19\"},\"end\":52135,\"start\":51867},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195750576},\"end\":52468,\"start\":52137},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208637407},\"end\":52852,\"start\":52470},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9346534},\"end\":53136,\"start\":52854},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5808102},\"end\":53437,\"start\":53138},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b24\"},\"end\":53648,\"start\":53439},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":216078090},\"end\":53808,\"start\":53650},{\"attributes\":{\"id\":\"b26\"},\"end\":54148,\"start\":53810},{\"attributes\":{\"id\":\"b27\"},\"end\":54342,\"start\":54150},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":41312633},\"end\":54772,\"start\":54344},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14542261},\"end\":55097,\"start\":54774},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14542261},\"end\":55406,\"start\":55099},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":170078603},\"end\":55811,\"start\":55408},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":213950511},\"end\":56087,\"start\":55813},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6037691},\"end\":56347,\"start\":56089},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1883787},\"end\":56624,\"start\":56349},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":174799948},\"end\":56949,\"start\":56626},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1629541},\"end\":57207,\"start\":56951},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5855042},\"end\":57468,\"start\":57209},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3488815},\"end\":57799,\"start\":57470},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":116505},\"end\":58077,\"start\":57801},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3172259},\"end\":58401,\"start\":58079},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2630174},\"end\":58670,\"start\":58403},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":13815143},\"end\":58967,\"start\":58672},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16852518},\"end\":59416,\"start\":58969},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":43969617},\"end\":59826,\"start\":59418},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":214713888},\"end\":60048,\"start\":59828},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206594095},\"end\":60478,\"start\":60050},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":43564031},\"end\":60805,\"start\":60480},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":151231},\"end\":61115,\"start\":60807},{\"attributes\":{\"id\":\"b49\"},\"end\":61356,\"start\":61117},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":13754527},\"end\":61708,\"start\":61358},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":3461223},\"end\":61957,\"start\":61710},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":13900194},\"end\":62228,\"start\":61959},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":54439061},\"end\":62473,\"start\":62230},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":206593880},\"end\":62798,\"start\":62475},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":604334},\"end\":63130,\"start\":62800},{\"attributes\":{\"id\":\"b56\"},\"end\":63310,\"start\":63132},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3833554},\"end\":63490,\"start\":63312},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":44178122},\"end\":63898,\"start\":63492},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":173188134},\"end\":64218,\"start\":63900},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":220647520},\"end\":64564,\"start\":64220},{\"attributes\":{\"doi\":\"arXiv:1804.03209\",\"id\":\"b61\"},\"end\":64817,\"start\":64566},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":170078598},\"end\":65162,\"start\":64819},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":47020609},\"end\":65453,\"start\":65164},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":15276198},\"end\":65605,\"start\":65455},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":3162051},\"end\":65853,\"start\":65607}]", "bib_title": "[{\"end\":46473,\"start\":46419},{\"end\":46732,\"start\":46713},{\"end\":46961,\"start\":46925},{\"end\":47240,\"start\":47193},{\"end\":47582,\"start\":47525},{\"end\":47877,\"start\":47822},{\"end\":48186,\"start\":48128},{\"end\":48952,\"start\":48882},{\"end\":49285,\"start\":49221},{\"end\":49530,\"start\":49488},{\"end\":49754,\"start\":49711},{\"end\":49988,\"start\":49939},{\"end\":50236,\"start\":50163},{\"end\":50542,\"start\":50496},{\"end\":50793,\"start\":50733},{\"end\":51090,\"start\":51039},{\"end\":51286,\"start\":51220},{\"end\":51667,\"start\":51623},{\"end\":52212,\"start\":52137},{\"end\":52547,\"start\":52470},{\"end\":52940,\"start\":52854},{\"end\":53230,\"start\":53138},{\"end\":53681,\"start\":53650},{\"end\":54403,\"start\":54344},{\"end\":54829,\"start\":54774},{\"end\":55154,\"start\":55099},{\"end\":55495,\"start\":55408},{\"end\":55881,\"start\":55813},{\"end\":56137,\"start\":56089},{\"end\":56412,\"start\":56349},{\"end\":56708,\"start\":56626},{\"end\":57005,\"start\":56951},{\"end\":57237,\"start\":57209},{\"end\":57531,\"start\":57470},{\"end\":57839,\"start\":57801},{\"end\":58139,\"start\":58079},{\"end\":58461,\"start\":58403},{\"end\":58720,\"start\":58672},{\"end\":59036,\"start\":58969},{\"end\":59529,\"start\":59418},{\"end\":59874,\"start\":59828},{\"end\":60151,\"start\":60050},{\"end\":60549,\"start\":60480},{\"end\":60903,\"start\":60807},{\"end\":61413,\"start\":61358},{\"end\":61761,\"start\":61710},{\"end\":62032,\"start\":61959},{\"end\":62285,\"start\":62230},{\"end\":62532,\"start\":62475},{\"end\":62840,\"start\":62800},{\"end\":63337,\"start\":63312},{\"end\":63556,\"start\":63492},{\"end\":63975,\"start\":63900},{\"end\":64296,\"start\":64220},{\"end\":64928,\"start\":64819},{\"end\":65201,\"start\":65164},{\"end\":65477,\"start\":65455},{\"end\":65648,\"start\":65607}]", "bib_author": "[{\"end\":46490,\"start\":46475},{\"end\":46514,\"start\":46490},{\"end\":46530,\"start\":46514},{\"end\":46539,\"start\":46530},{\"end\":46749,\"start\":46734},{\"end\":46767,\"start\":46749},{\"end\":46784,\"start\":46767},{\"end\":46798,\"start\":46784},{\"end\":46981,\"start\":46963},{\"end\":46999,\"start\":46981},{\"end\":47018,\"start\":46999},{\"end\":47033,\"start\":47018},{\"end\":47251,\"start\":47242},{\"end\":47277,\"start\":47251},{\"end\":47295,\"start\":47277},{\"end\":47310,\"start\":47295},{\"end\":47326,\"start\":47310},{\"end\":47335,\"start\":47326},{\"end\":47597,\"start\":47584},{\"end\":47610,\"start\":47597},{\"end\":47620,\"start\":47610},{\"end\":47636,\"start\":47620},{\"end\":47641,\"start\":47636},{\"end\":47900,\"start\":47879},{\"end\":47916,\"start\":47900},{\"end\":47928,\"start\":47916},{\"end\":47946,\"start\":47928},{\"end\":47950,\"start\":47946},{\"end\":48203,\"start\":48188},{\"end\":48223,\"start\":48203},{\"end\":48236,\"start\":48223},{\"end\":48257,\"start\":48236},{\"end\":48267,\"start\":48257},{\"end\":48276,\"start\":48267},{\"end\":48288,\"start\":48276},{\"end\":48296,\"start\":48288},{\"end\":48305,\"start\":48296},{\"end\":48321,\"start\":48305},{\"end\":48328,\"start\":48321},{\"end\":48607,\"start\":48589},{\"end\":48617,\"start\":48607},{\"end\":48625,\"start\":48617},{\"end\":48962,\"start\":48954},{\"end\":48987,\"start\":48962},{\"end\":49011,\"start\":48987},{\"end\":49024,\"start\":49011},{\"end\":49301,\"start\":49287},{\"end\":49316,\"start\":49301},{\"end\":49331,\"start\":49316},{\"end\":49546,\"start\":49532},{\"end\":49557,\"start\":49546},{\"end\":49572,\"start\":49557},{\"end\":49773,\"start\":49756},{\"end\":49792,\"start\":49773},{\"end\":49805,\"start\":49792},{\"end\":50006,\"start\":49990},{\"end\":50024,\"start\":50006},{\"end\":50256,\"start\":50238},{\"end\":50281,\"start\":50256},{\"end\":50294,\"start\":50281},{\"end\":50304,\"start\":50294},{\"end\":50551,\"start\":50544},{\"end\":50572,\"start\":50551},{\"end\":50590,\"start\":50572},{\"end\":50599,\"start\":50590},{\"end\":50807,\"start\":50795},{\"end\":50821,\"start\":50807},{\"end\":50836,\"start\":50821},{\"end\":50852,\"start\":50836},{\"end\":50870,\"start\":50852},{\"end\":51105,\"start\":51092},{\"end\":51306,\"start\":51288},{\"end\":51324,\"start\":51306},{\"end\":51340,\"start\":51324},{\"end\":51360,\"start\":51340},{\"end\":51372,\"start\":51360},{\"end\":51681,\"start\":51669},{\"end\":51696,\"start\":51681},{\"end\":51710,\"start\":51696},{\"end\":51720,\"start\":51710},{\"end\":51968,\"start\":51953},{\"end\":51987,\"start\":51968},{\"end\":52229,\"start\":52214},{\"end\":52245,\"start\":52229},{\"end\":52564,\"start\":52549},{\"end\":52575,\"start\":52564},{\"end\":52583,\"start\":52575},{\"end\":52597,\"start\":52583},{\"end\":52610,\"start\":52597},{\"end\":52625,\"start\":52610},{\"end\":52643,\"start\":52625},{\"end\":52954,\"start\":52942},{\"end\":52967,\"start\":52954},{\"end\":52977,\"start\":52967},{\"end\":53246,\"start\":53232},{\"end\":53265,\"start\":53246},{\"end\":53495,\"start\":53483},{\"end\":53509,\"start\":53495},{\"end\":53513,\"start\":53509},{\"end\":53695,\"start\":53683},{\"end\":53707,\"start\":53695},{\"end\":53716,\"start\":53707},{\"end\":53971,\"start\":53948},{\"end\":54222,\"start\":54205},{\"end\":54239,\"start\":54222},{\"end\":54417,\"start\":54405},{\"end\":54433,\"start\":54417},{\"end\":54441,\"start\":54433},{\"end\":54456,\"start\":54441},{\"end\":54477,\"start\":54456},{\"end\":54491,\"start\":54477},{\"end\":54511,\"start\":54491},{\"end\":54519,\"start\":54511},{\"end\":54843,\"start\":54831},{\"end\":54856,\"start\":54843},{\"end\":54871,\"start\":54856},{\"end\":54888,\"start\":54871},{\"end\":55168,\"start\":55156},{\"end\":55181,\"start\":55168},{\"end\":55196,\"start\":55181},{\"end\":55213,\"start\":55196},{\"end\":55511,\"start\":55497},{\"end\":55523,\"start\":55511},{\"end\":55536,\"start\":55523},{\"end\":55549,\"start\":55536},{\"end\":55563,\"start\":55549},{\"end\":55575,\"start\":55563},{\"end\":55590,\"start\":55575},{\"end\":55897,\"start\":55883},{\"end\":55910,\"start\":55897},{\"end\":55922,\"start\":55910},{\"end\":55937,\"start\":55922},{\"end\":56146,\"start\":56139},{\"end\":56160,\"start\":56146},{\"end\":56173,\"start\":56160},{\"end\":56195,\"start\":56173},{\"end\":56421,\"start\":56414},{\"end\":56435,\"start\":56421},{\"end\":56448,\"start\":56435},{\"end\":56470,\"start\":56448},{\"end\":56720,\"start\":56710},{\"end\":56736,\"start\":56720},{\"end\":56750,\"start\":56736},{\"end\":56766,\"start\":56750},{\"end\":57022,\"start\":57007},{\"end\":57038,\"start\":57022},{\"end\":57054,\"start\":57038},{\"end\":57263,\"start\":57239},{\"end\":57280,\"start\":57263},{\"end\":57551,\"start\":57533},{\"end\":57571,\"start\":57551},{\"end\":57587,\"start\":57571},{\"end\":57605,\"start\":57587},{\"end\":57619,\"start\":57605},{\"end\":57856,\"start\":57841},{\"end\":57869,\"start\":57856},{\"end\":57887,\"start\":57869},{\"end\":57907,\"start\":57887},{\"end\":58156,\"start\":58141},{\"end\":58174,\"start\":58156},{\"end\":58192,\"start\":58174},{\"end\":58212,\"start\":58192},{\"end\":58481,\"start\":58463},{\"end\":58497,\"start\":58481},{\"end\":58517,\"start\":58497},{\"end\":58733,\"start\":58722},{\"end\":58749,\"start\":58733},{\"end\":58765,\"start\":58749},{\"end\":58783,\"start\":58765},{\"end\":58797,\"start\":58783},{\"end\":59052,\"start\":59038},{\"end\":59062,\"start\":59052},{\"end\":59075,\"start\":59062},{\"end\":59096,\"start\":59075},{\"end\":59103,\"start\":59096},{\"end\":59116,\"start\":59103},{\"end\":59540,\"start\":59531},{\"end\":59555,\"start\":59540},{\"end\":59565,\"start\":59555},{\"end\":59576,\"start\":59565},{\"end\":59592,\"start\":59576},{\"end\":59601,\"start\":59592},{\"end\":59891,\"start\":59876},{\"end\":59902,\"start\":59891},{\"end\":59911,\"start\":59902},{\"end\":60165,\"start\":60153},{\"end\":60180,\"start\":60165},{\"end\":60200,\"start\":60180},{\"end\":60215,\"start\":60200},{\"end\":60232,\"start\":60215},{\"end\":60564,\"start\":60551},{\"end\":60582,\"start\":60564},{\"end\":60599,\"start\":60582},{\"end\":60615,\"start\":60599},{\"end\":60919,\"start\":60905},{\"end\":60927,\"start\":60919},{\"end\":60935,\"start\":60927},{\"end\":61190,\"start\":61170},{\"end\":61429,\"start\":61415},{\"end\":61445,\"start\":61429},{\"end\":61465,\"start\":61445},{\"end\":61487,\"start\":61465},{\"end\":61503,\"start\":61487},{\"end\":61520,\"start\":61503},{\"end\":61772,\"start\":61763},{\"end\":61780,\"start\":61772},{\"end\":61794,\"start\":61780},{\"end\":61809,\"start\":61794},{\"end\":61816,\"start\":61809},{\"end\":62046,\"start\":62034},{\"end\":62065,\"start\":62046},{\"end\":62077,\"start\":62065},{\"end\":62300,\"start\":62287},{\"end\":62315,\"start\":62300},{\"end\":62330,\"start\":62315},{\"end\":62553,\"start\":62534},{\"end\":62572,\"start\":62553},{\"end\":62586,\"start\":62572},{\"end\":62598,\"start\":62586},{\"end\":62614,\"start\":62598},{\"end\":62861,\"start\":62842},{\"end\":62879,\"start\":62861},{\"end\":62895,\"start\":62879},{\"end\":62907,\"start\":62895},{\"end\":62922,\"start\":62907},{\"end\":62938,\"start\":62922},{\"end\":62950,\"start\":62938},{\"end\":63168,\"start\":63151},{\"end\":63182,\"start\":63168},{\"end\":63353,\"start\":63339},{\"end\":63363,\"start\":63353},{\"end\":63372,\"start\":63363},{\"end\":63383,\"start\":63372},{\"end\":63574,\"start\":63558},{\"end\":63593,\"start\":63574},{\"end\":63605,\"start\":63593},{\"end\":63613,\"start\":63605},{\"end\":63629,\"start\":63613},{\"end\":63644,\"start\":63629},{\"end\":63654,\"start\":63644},{\"end\":63990,\"start\":63977},{\"end\":64002,\"start\":63990},{\"end\":64018,\"start\":64002},{\"end\":64031,\"start\":64018},{\"end\":64311,\"start\":64298},{\"end\":64322,\"start\":64311},{\"end\":64332,\"start\":64322},{\"end\":64345,\"start\":64332},{\"end\":64361,\"start\":64345},{\"end\":64579,\"start\":64566},{\"end\":64940,\"start\":64930},{\"end\":64952,\"start\":64940},{\"end\":64970,\"start\":64952},{\"end\":65216,\"start\":65203},{\"end\":65228,\"start\":65216},{\"end\":65241,\"start\":65228},{\"end\":65256,\"start\":65241},{\"end\":65271,\"start\":65256},{\"end\":65284,\"start\":65271},{\"end\":65497,\"start\":65479},{\"end\":65514,\"start\":65497},{\"end\":65664,\"start\":65650},{\"end\":65681,\"start\":65664},{\"end\":65703,\"start\":65681},{\"end\":65714,\"start\":65703}]", "bib_venue": "[{\"end\":54921,\"start\":54913},{\"end\":46546,\"start\":46539},{\"end\":46802,\"start\":46798},{\"end\":47037,\"start\":47033},{\"end\":47339,\"start\":47335},{\"end\":47645,\"start\":47641},{\"end\":47954,\"start\":47950},{\"end\":48335,\"start\":48328},{\"end\":48709,\"start\":48641},{\"end\":49031,\"start\":49024},{\"end\":49335,\"start\":49331},{\"end\":49579,\"start\":49572},{\"end\":49809,\"start\":49805},{\"end\":50028,\"start\":50024},{\"end\":50308,\"start\":50304},{\"end\":50603,\"start\":50599},{\"end\":50877,\"start\":50870},{\"end\":51112,\"start\":51105},{\"end\":51403,\"start\":51372},{\"end\":51724,\"start\":51720},{\"end\":51951,\"start\":51867},{\"end\":52252,\"start\":52245},{\"end\":52653,\"start\":52643},{\"end\":52981,\"start\":52977},{\"end\":53269,\"start\":53265},{\"end\":53481,\"start\":53439},{\"end\":53720,\"start\":53716},{\"end\":53946,\"start\":53810},{\"end\":54203,\"start\":54150},{\"end\":54542,\"start\":54519},{\"end\":54911,\"start\":54888},{\"end\":55236,\"start\":55213},{\"end\":55600,\"start\":55590},{\"end\":55941,\"start\":55937},{\"end\":56199,\"start\":56195},{\"end\":56474,\"start\":56470},{\"end\":56770,\"start\":56766},{\"end\":57058,\"start\":57054},{\"end\":57316,\"start\":57280},{\"end\":57623,\"start\":57619},{\"end\":57914,\"start\":57907},{\"end\":58216,\"start\":58212},{\"end\":58521,\"start\":58517},{\"end\":58801,\"start\":58797},{\"end\":59180,\"start\":59116},{\"end\":59605,\"start\":59601},{\"end\":59915,\"start\":59911},{\"end\":60236,\"start\":60232},{\"end\":60619,\"start\":60615},{\"end\":60942,\"start\":60935},{\"end\":61168,\"start\":61117},{\"end\":61524,\"start\":61520},{\"end\":61820,\"start\":61816},{\"end\":62081,\"start\":62077},{\"end\":62334,\"start\":62330},{\"end\":62618,\"start\":62614},{\"end\":62957,\"start\":62950},{\"end\":63149,\"start\":63132},{\"end\":63390,\"start\":63383},{\"end\":63661,\"start\":63654},{\"end\":64038,\"start\":64031},{\"end\":64365,\"start\":64361},{\"end\":64663,\"start\":64595},{\"end\":64974,\"start\":64970},{\"end\":65291,\"start\":65284},{\"end\":65518,\"start\":65514},{\"end\":65718,\"start\":65714}]"}}}, "year": 2023, "month": 12, "day": 17}