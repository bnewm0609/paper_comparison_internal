{"id": 258557672, "updated": "2023-10-05 01:10:18.481", "metadata": {"title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", "authors": "[{\"first\":\"Tao\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Chengqi\",\"last\":\"Lyu\",\"middle\":[]},{\"first\":\"Shilong\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yudong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Miao\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Qian\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Kuikun\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Wenwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint training of language-only and visual-language instructions with the \\emph{same} instruction template effectively improves dialogue performance. Various demos show the ability of continuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are at https://github.com/open-mmlab/Multimodal-GPT", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.04790", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-04790", "doi": "10.48550/arxiv.2305.04790"}}, "content": {"source": {"pdf_hash": "81e7e82245c2f230eeb8aaaa1a2b2604c143754a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.04790v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e72de3a5bdcbf314c2b7af2a04f04acca3895a9b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/81e7e82245c2f230eeb8aaaa1a2b2604c143754a.txt", "contents": "\nMultiModal-GPT: A Vision and Language Model for Dialogue with Humans\n\n\nTao Gong gongtao@pjlab.org.cn \nShanghai AI Laboratory\n\n\nChengqi Lyu lvchengqi@pjlab.org.cn \nShanghai AI Laboratory\n\n\nShilong Zhang zhangshilong@pjlab.org.cn \nShanghai AI Laboratory\n\n\nThe University of Hong\nKong\n\nYudong Wang wangyudong@pjlab.org.cn \nShanghai AI Laboratory\n\n\nSchool of Electrical and Information Engineering\nTianjin University\n\n\nMiao Zheng zhengmiao@pjlab.org.cn \nShanghai AI Laboratory\n\n\nQian Zhao zhaoqian@pjlab.org.cn \nShanghai AI Laboratory\n\n\nKuikun Liu liukuikun@pjlab.org.cn \nShanghai AI Laboratory\n\n\nWenwei Zhang zhangwenwei@pjlab.org.cn \nShanghai AI Laboratory\n\n\nPing Luo \nShanghai AI Laboratory\n\n\nThe University of Hong\nKong\n\nKai Chen chenkai@pjlab.org.cn \nShanghai AI Laboratory\n\n\nMultiModal-GPT: A Vision and Language Model for Dialogue with Humans\n\nWe present a vision and language model named MultiModal-GPT to conduct multiround dialogue with humans. MultiModal-GPT is capable of following diverse instructions, such as generating detailed captions, counting specific objects, and addressing general inquiries posed by users. The model is efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) incorporated in both the gated-cross-attention and self-attention components of the language model. Our approach involves constructing instruction templates that incorporate vision and language data for multi-modality instruction tuning, enabling the model to comprehend and adhere to human directives. We observe that the quality of training data is crucial for effective dialogue performance, as a limited dataset with short responses may cause the model to generate brief replies to any instruction. To further enhance MultiModal-GPT's conversational abilities, we employ language-only instructionfollowing data for joint training alongside visual-language instructions. Utilizing the same instruction template for both types of data results in a significant improvement in dialogue performance. Our experiments demonstrate MultiModal-GPT's proficiency in maintaining continuous dialogues with humans. The code, dataset, and demo can be found at https://github.com/open-mmlab/Multimodal-GPT.Recently,  has demonstrated remarkable proficiency in multi-modal dialogues with humans. Although  exceptional capabilities have been observed, the mechanisms underpinning its outstanding performance remain elusive. Studies such as Mini- GPT4 [18]  and LLaVA [8]  have sought to replicate this performance by aligning visual representations with the input space of LLM, subsequently utilizing the original self-attention in the LLM to process visual information. However, incorporating such models with detailed or spatiotemporal visual information can be computationally intensive due to the potentially large number of image tokens. Furthermore, both models employ Preprint. Under review.\n\nIntroduction\n\nHumans interact with the world through multiple channels, including vision and language, each of which has a unique advantage in representing and conveying certain concepts of the world, thus contributing to a better understanding of the world. A central objective of artificial intelligence research is to create a versatile assistant capable of effectively following multimodal vision-andlanguage instructions that align with human intentions, in order to accomplish a diverse array of real-world tasks.\n\nvicuna [2], an open-source chatbot refined through fine-tuning LLaMA [16] on user-generated conversations from ChatGPT, which omits the language instruction tuning phase in their research.\n\nTo address these challenges, we build upon the open-source Flamingo framework [1], a multimodal pre-trained model that deploys a perceiver resampler to efficiently extract visual information from the vision encoder, while also employing gated cross-attention layers for image-text interactions. This model has been pre-trained on an extensive dataset of image-text pairs, showcasing robust few-shot visual comprehension capabilities. Nevertheless, it lacks the capacity to engage in zero-shot multiturn image-text dialogues. As a result, our goal is to fine-tune OpenFlamingo using comprehensive datasets of image and text instructions, enabling the model to conduct conversations that more closely align with human preferences. By capitalizing on OpenFlamingo's foundational strengths, we aspire to narrow the performance gap between the model's existing capabilities and the desired outcome of more accurate, human-like interactions in multimodal dialogues. We have dubbed our multimodal chatbot MultiModal-GPT.\n\nWe also use a unified instruction template for both language and visual instruction data during model training. We first construct instruction templates with vision and language data to train the MultiModal-GPT. We find the training data is vital with respect to the performance of the MultiModal-GPT. Some datasets, such as VQA v2.0 [3], OKVQA [9], GQA [5], CLEVR [6] and NLVR [15] datasets, will degrade the dialogue performance of the MultiModal-GPT, since the response in these datasets is restricted to one or two words (e.g., yes/no). Consequently, when these datasets are incorporated into the training process, the model exhibits a tendency to generate answers comprising merely one or two words. This brevity is not conducive to user-friendliness.\n\nTo further enhance the ability of MultiModal-GPT to chat with people, we also collect language data and define a unified instruction template to jointly train the MultiModal-GPT. The joint training of language-only instructions and visual and language instructions effectively improves the performance of the model. We show various demos to show the ability of continuous dialogue of MultiModal-GPT with humans.\n\n\nUnified Instruction Template\n\nWe propose a unified template for the integration of unimodal linguistic data and multimodal visionand-language data, with the objective of effectively training the MultiModal-GPT model in a synergistic manner. This unified approach aims to enhance the model's performance across diverse tasks by leveraging the complementary strengths of both data modalities and fostering a more profound understanding of the underlying concepts.\n\n\nLanguage-only Instruction Template\n\n<BOS> Below is an instruction that describes a task. Write a response that appropriately completes the request ### Instruction: {instruction} ### Input: {input} ### Response: {response} <EOS> We employ the Dolly 15k and Alpaca GPT4 datasets [12] as resources for assessing languageonly instruction-following capabilities. These datasets have been specifically designed to improve the performance of language models in executing instruction-based tasks. To ensure consistent instruction-following format, we utilize the prompt template presented in Table 1 \n\n\nVision and Language Instruction Template\n\nWe utilize a diverse selection of vision and language instruction-following datasets in our study, including LLaVA [8], Mini-GPT4 [18], A-OKVQA [14], COCO Caption [7], and OCR VQA [10]. These datasets encompass a wide array of applications and domains, thereby facilitating the comprehensive evolving of our model's performance.\n\nIn order to present the text in a consistent, instruction-following format, we adopt the prompt delineated in Table 2 as a template for structuring these datasets. By adhering to a standardized format, we ensure that our model is better equipped to process the information and respond accordingly.\n\nIt is important to note that the COCO Caption dataset generally does not include instructional content, as it predominantly consists of descriptive captions. To overcome this limitation and incorporate instructional data, we employ the GPT-4 [11] model to generate pertinent instructions for the COCO Caption dataset. This integration of synthesized instructions enriches the dataset, enabling our model to achieve a more robust capabilities in processing and responding to human instructions. Table 3 showcases a variety of examples illustrating the instructions generated for the COCO Caption dataset, demonstrating the effectiveness of our approach in adapting the dataset to better suit our research objectives. \n\n\nMethod\n\n\nArchitecture\n\nThe proposed MultiModal-GPT is based on the open-flamingo model [1]. As shown in Figure 1, MultiModal-GPT consists of a vision encoder from CLIP [13], a perceiver resampler to receive the spatial features from the vision encoder, and a language decoder LLaMA [16]. Note that the language decoder is conditioned on the spatial features from the perceiver resampler by cross-attention in order to encode the feature of vision into text. Please refer to [1] for more details of the model architecture.\n\n\nJoint Training\n\nWe use both language-only instruction-following data and vision and language instruction-following data to train the MultiModal-GPT jointly. As shown in Fig.1, We freeze the whole open-flamingo model and add LoRA [4] to the self-attention, cross-attention, and FFW part in the language decoder to finetune MultiModal-GPT. The MultiModal-GPT is trained by predicting the next token of the text, and only the {response} and <EOS> tokens in the input sequence are involved in the loss calculation.\n\n\nExperiments\n\n\nImplementation Details\n\nWe jointly train the MultiModal-GPT model using a comprehensive mix of language data and vision and language data sources to enhance its performance. The language datasets include Dolly 15k and Alpaca GPT4 [12], while the vision and language datasets encompass LLaVA [8], Mini-GPT4 [18], A-OKVQA [14], COCO Caption [7], and OCR VQA [10]. Other vision and language instruction datasets, such as MultiInstruct [17] can also be explored, and we left it as future work. This combination of datasets aims to provide a diverse and rich training environment for the MultiModal-GPT model.\n\nTo effectively train the model, we incorporate the entire text corpus from the Dolly 15k and Alpaca GPT4 datasets. Similarly, we include all image-text pairs available from the LLaVA and Mini-GPT4 datasets to ensure adequate exposure to various contexts and situations. However, the quality of the A-OKVQA, COCO Caption, and OCR VQA datasets is considered inferior compared to LLaVA and Mini-GPT4. To account for this disparity while still benefiting from the additional data, we include a random sample of 5000 image-text pairs from the A-OKVQA dataset and 512 image-text pairs each from the COCO Caption and OCR VQA datasets in the training process.\n\nTo train the model, we utilize 8 A100 GPUs and complete the training process within a single epoch. The batch size per GPU for both vision and language instruction following data and language-only instruction following data is set to 1. We use gradient accumulation, and update the parameters of the LoRA every 16 iterations. Each iteration encompasses one vision-language pair and one language-only instruction data. Consequently, the aggregate batch size amounts to 256. We employ a learning rate of 1e-5, using a cosine learning rate scheduler to adjust the learning rate during the training process.\n\n\nThe Quality of Data Matters\n\nIn order to enhance the diversity of vision and language data, we initially incorporated a multitude of datasets in our preliminary experiments. However, we observed that the quality of certain datasets, including VQA v2.0 [3], OKVQA [9], GQA [5], CLEVR [6], and NLVR [15], was suboptimal. This is primarily due to the fact that the {response} in these datasets is restricted to one or two words (e.g., yes/no). Consequently, when these datasets are incorporated into the training process, the model exhibits a tendency to generate answers comprising merely one or two words. This brevity is not conducive to user-friendliness. As a result, we have opted to exclude these datasets from the final version of our study.\n\n\nDemos\n\nWe present a variety of demos that exhibit the capabilities of MultiModal-GPT in engaging in conversation with humans. As illustrated in Figure 2, MultiModal-GPT can successfully identify lasagna and provide a comprehensive recipe for preparing the dish when prompted by the user in the initial dialogue. In the subsequent dialogue, MultiModal-GPT offers reasonable suggestions on where to dine when the user inquires about potential eateries. This exemplifies the model's capacity for maintaining an ongoing dialogue.\n\nAdditionally, Figure 4 demonstrates MultiModal-GPT's counting abilities, while Figure 6 showcases its OCR capabilities. MultiModal-GPT is also adept at addressing general inquiries regarding users' travel plans, as depicted in Figure 7. More demos can be found at https://github.com/openmmlab/Multimodal-GPT.       \n\nFigure 1 :\n1The overall framework of MultiModal-GPT. MultiModal-GPT consists of a vision encoder, a perceiver resampler to receive the spatial features from the vision encoder, and a language decoder which is conditioned on the spatial features from the perceiver resampler by cross-attention in order to encode the feature of vision into text. We freeze the whole open-flamingo model and add LoRA to the self-attention part, the cross-attention part, and the FFN part in the language decoder to finetune MultiModal-GPT.\n\nFigure 2 :\n2The MultiModal-GPT can give a recipe to bake lasagna, and tell users where to eat it.\n\nFigure 3 :\n3The MultiModal-GPT can recognize the image of Elon Musk, and answer questions about Elon Musk.\n\nFigure 4 :\n4The MultiModal-GPT knows there are 4 women in the image, and knows what they are doing.\n\nFigure 5 :\n5The MultiModal-GPT knows the film and knows which studio made the film.\n\nFigure 6 :\n6The MultiModal-GPT can recognize the characters in the image and knows the author of the book.\n\nFigure 7 :\n7The MultiModal-GPT can answer general questions about the traveling of users.\n\nFigure 8 :\n8The MultiModal-GPT can generate a detailed description for the image and has the ability to reason the season in the image.\n\nTable 1 :\n1The input sequence of language data used to train the model. The {instruction}, {input} and {response} are texts from the source data. Only the {response} part and <EOS> token will be calculated loss.\n\n\nfor structuring the dataset input. <BOS> Below is an instruction that describes a task. Write a response that appropriately completes the request ### Image: <image_token> ### Instruction: {question} ### Response: {response}<EOS> ### Instruction: {question} ### Response: {response} <EOS>\n\nTable 2 :\n2The input sequence of vision and language data used to train the model. The {question} and {response} are texts from the source data. <image_token> is a token denoting the existence of image. Note that there are multi-round dialogues if the dataset has. Only the {response} part and <EOS> token will be calculated loss.\u2022 Can you describe the image? \u2022 Could you provide a description of the image? \u2022 What do you see in this image? \u2022 Share your thoughts on the content of the image. \u2022 Please narrate what's happening in the picture. \u2022 Can you give a brief explanation of the image? \u2022 Describe the main elements and details present in the image. \u2022 In your own words, what is depicted in the image? \u2022 How would you describe the image's content in a caption? \u2022 Can you suggest an insightful caption that highlights the underlying message of the image?\n\nTable 3 :\n3The list of instructions for image caption.\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Reynolds, Advances in Neural Information Processing Systems. 35Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.\n\nJ Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709, 2019.\n\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901-2910, 2017.\n\nDeep visual-semantic alignments for generating image descriptions. Andrej Karpathy, Li Fei-Fei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128-3137, 2015.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. arXiv preprintHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n\nOk-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognitionKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195-3204, 2019.\n\nOcr-vqa: Visual question answering by reading text in images. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty, ICDAR. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019.\n\nGpt-4 technical report. Openai, OpenAI. Gpt-4 technical report. 2023.\n\nBaolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprintBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLRAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.\n\nA-okvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerProceedings, Part VIIIDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII, pages 146-162. Springer, 2022.\n\nA corpus of natural language for visual reasoning. Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsShort Papers2Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, 2017.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nMultiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, arXiv:2212.10773arXiv preprintZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n", "annotations": {"author": "[{\"end\":127,\"start\":72},{\"end\":188,\"start\":128},{\"end\":283,\"start\":189},{\"end\":415,\"start\":284},{\"end\":475,\"start\":416},{\"end\":533,\"start\":476},{\"end\":593,\"start\":534},{\"end\":657,\"start\":594},{\"end\":721,\"start\":658},{\"end\":777,\"start\":722},{\"end\":127,\"start\":72},{\"end\":188,\"start\":128},{\"end\":283,\"start\":189},{\"end\":415,\"start\":284},{\"end\":475,\"start\":416},{\"end\":533,\"start\":476},{\"end\":593,\"start\":534},{\"end\":657,\"start\":594},{\"end\":721,\"start\":658},{\"end\":777,\"start\":722}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":76},{\"end\":139,\"start\":136},{\"end\":202,\"start\":197},{\"end\":295,\"start\":291},{\"end\":426,\"start\":421},{\"end\":485,\"start\":481},{\"end\":544,\"start\":541},{\"end\":606,\"start\":601},{\"end\":666,\"start\":663},{\"end\":730,\"start\":726},{\"end\":80,\"start\":76},{\"end\":139,\"start\":136},{\"end\":202,\"start\":197},{\"end\":295,\"start\":291},{\"end\":426,\"start\":421},{\"end\":485,\"start\":481},{\"end\":544,\"start\":541},{\"end\":606,\"start\":601},{\"end\":666,\"start\":663},{\"end\":730,\"start\":726}]", "author_first_name": "[{\"end\":75,\"start\":72},{\"end\":135,\"start\":128},{\"end\":196,\"start\":189},{\"end\":290,\"start\":284},{\"end\":420,\"start\":416},{\"end\":480,\"start\":476},{\"end\":540,\"start\":534},{\"end\":600,\"start\":594},{\"end\":662,\"start\":658},{\"end\":725,\"start\":722},{\"end\":75,\"start\":72},{\"end\":135,\"start\":128},{\"end\":196,\"start\":189},{\"end\":290,\"start\":284},{\"end\":420,\"start\":416},{\"end\":480,\"start\":476},{\"end\":540,\"start\":534},{\"end\":600,\"start\":594},{\"end\":662,\"start\":658},{\"end\":725,\"start\":722}]", "author_affiliation": "[{\"end\":126,\"start\":103},{\"end\":187,\"start\":164},{\"end\":253,\"start\":230},{\"end\":282,\"start\":255},{\"end\":344,\"start\":321},{\"end\":414,\"start\":346},{\"end\":474,\"start\":451},{\"end\":532,\"start\":509},{\"end\":592,\"start\":569},{\"end\":656,\"start\":633},{\"end\":691,\"start\":668},{\"end\":720,\"start\":693},{\"end\":776,\"start\":753},{\"end\":126,\"start\":103},{\"end\":187,\"start\":164},{\"end\":253,\"start\":230},{\"end\":282,\"start\":255},{\"end\":344,\"start\":321},{\"end\":414,\"start\":346},{\"end\":474,\"start\":451},{\"end\":532,\"start\":509},{\"end\":592,\"start\":569},{\"end\":656,\"start\":633},{\"end\":691,\"start\":668},{\"end\":720,\"start\":693},{\"end\":776,\"start\":753}]", "title": "[{\"end\":69,\"start\":1},{\"end\":846,\"start\":778},{\"end\":69,\"start\":1},{\"end\":846,\"start\":778}]", "venue": null, "abstract": "[{\"end\":2895,\"start\":848},{\"end\":2895,\"start\":848}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3428,\"start\":3425},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3491,\"start\":3487},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3689,\"start\":3686},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4960,\"start\":4957},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4971,\"start\":4968},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4980,\"start\":4977},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4991,\"start\":4988},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5005,\"start\":5001},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6540,\"start\":6536},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7014,\"start\":7011},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7030,\"start\":7026},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7044,\"start\":7040},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7062,\"start\":7059},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7080,\"start\":7076},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7771,\"start\":7767},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8334,\"start\":8331},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8416,\"start\":8412},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8530,\"start\":8526},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8721,\"start\":8718},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9000,\"start\":8997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9529,\"start\":9525},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9589,\"start\":9586},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9605,\"start\":9601},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9619,\"start\":9615},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9637,\"start\":9634},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9655,\"start\":9651},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9731,\"start\":9727},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11415,\"start\":11412},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11426,\"start\":11423},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11435,\"start\":11432},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11446,\"start\":11443},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11461,\"start\":11457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3428,\"start\":3425},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3491,\"start\":3487},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3689,\"start\":3686},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4960,\"start\":4957},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4971,\"start\":4968},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4980,\"start\":4977},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4991,\"start\":4988},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5005,\"start\":5001},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6540,\"start\":6536},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7014,\"start\":7011},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7030,\"start\":7026},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7044,\"start\":7040},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7062,\"start\":7059},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7080,\"start\":7076},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7771,\"start\":7767},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8334,\"start\":8331},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8416,\"start\":8412},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8530,\"start\":8526},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8721,\"start\":8718},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9000,\"start\":8997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9529,\"start\":9525},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9589,\"start\":9586},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9605,\"start\":9601},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9619,\"start\":9615},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9637,\"start\":9634},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9655,\"start\":9651},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9731,\"start\":9727},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11415,\"start\":11412},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11426,\"start\":11423},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11435,\"start\":11432},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11446,\"start\":11443},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11461,\"start\":11457}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":13273,\"start\":12752},{\"attributes\":{\"id\":\"fig_1\"},\"end\":13372,\"start\":13274},{\"attributes\":{\"id\":\"fig_2\"},\"end\":13480,\"start\":13373},{\"attributes\":{\"id\":\"fig_3\"},\"end\":13581,\"start\":13481},{\"attributes\":{\"id\":\"fig_4\"},\"end\":13666,\"start\":13582},{\"attributes\":{\"id\":\"fig_5\"},\"end\":13774,\"start\":13667},{\"attributes\":{\"id\":\"fig_6\"},\"end\":13865,\"start\":13775},{\"attributes\":{\"id\":\"fig_7\"},\"end\":14002,\"start\":13866},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":14215,\"start\":14003},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":14505,\"start\":14216},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":15364,\"start\":14506},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":15420,\"start\":15365},{\"attributes\":{\"id\":\"fig_0\"},\"end\":13273,\"start\":12752},{\"attributes\":{\"id\":\"fig_1\"},\"end\":13372,\"start\":13274},{\"attributes\":{\"id\":\"fig_2\"},\"end\":13480,\"start\":13373},{\"attributes\":{\"id\":\"fig_3\"},\"end\":13581,\"start\":13481},{\"attributes\":{\"id\":\"fig_4\"},\"end\":13666,\"start\":13582},{\"attributes\":{\"id\":\"fig_5\"},\"end\":13774,\"start\":13667},{\"attributes\":{\"id\":\"fig_6\"},\"end\":13865,\"start\":13775},{\"attributes\":{\"id\":\"fig_7\"},\"end\":14002,\"start\":13866},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":14215,\"start\":14003},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":14505,\"start\":14216},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":15364,\"start\":14506},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":15420,\"start\":15365}]", "paragraph": "[{\"end\":3416,\"start\":2911},{\"end\":3606,\"start\":3418},{\"end\":4621,\"start\":3608},{\"end\":5379,\"start\":4623},{\"end\":5792,\"start\":5381},{\"end\":6256,\"start\":5825},{\"end\":6851,\"start\":6295},{\"end\":7224,\"start\":6896},{\"end\":7523,\"start\":7226},{\"end\":8241,\"start\":7525},{\"end\":8765,\"start\":8267},{\"end\":9278,\"start\":8784},{\"end\":9899,\"start\":9319},{\"end\":10552,\"start\":9901},{\"end\":11157,\"start\":10554},{\"end\":11906,\"start\":11189},{\"end\":12434,\"start\":11916},{\"end\":12751,\"start\":12436},{\"end\":3416,\"start\":2911},{\"end\":3606,\"start\":3418},{\"end\":4621,\"start\":3608},{\"end\":5379,\"start\":4623},{\"end\":5792,\"start\":5381},{\"end\":6256,\"start\":5825},{\"end\":6851,\"start\":6295},{\"end\":7224,\"start\":6896},{\"end\":7523,\"start\":7226},{\"end\":8241,\"start\":7525},{\"end\":8765,\"start\":8267},{\"end\":9278,\"start\":8784},{\"end\":9899,\"start\":9319},{\"end\":10552,\"start\":9901},{\"end\":11157,\"start\":10554},{\"end\":11906,\"start\":11189},{\"end\":12434,\"start\":11916},{\"end\":12751,\"start\":12436}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6850,\"start\":6843},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":7343,\"start\":7336},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":8026,\"start\":8019},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6850,\"start\":6843},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":7343,\"start\":7336},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":8026,\"start\":8019}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2909,\"start\":2897},{\"attributes\":{\"n\":\"2\"},\"end\":5823,\"start\":5795},{\"attributes\":{\"n\":\"2.1\"},\"end\":6293,\"start\":6259},{\"attributes\":{\"n\":\"2.2\"},\"end\":6894,\"start\":6854},{\"attributes\":{\"n\":\"3\"},\"end\":8250,\"start\":8244},{\"attributes\":{\"n\":\"3.1\"},\"end\":8265,\"start\":8253},{\"attributes\":{\"n\":\"3.2\"},\"end\":8782,\"start\":8768},{\"attributes\":{\"n\":\"4\"},\"end\":9292,\"start\":9281},{\"attributes\":{\"n\":\"4.1\"},\"end\":9317,\"start\":9295},{\"attributes\":{\"n\":\"4.2\"},\"end\":11187,\"start\":11160},{\"attributes\":{\"n\":\"4.3\"},\"end\":11914,\"start\":11909},{\"end\":12763,\"start\":12753},{\"end\":13285,\"start\":13275},{\"end\":13384,\"start\":13374},{\"end\":13492,\"start\":13482},{\"end\":13593,\"start\":13583},{\"end\":13678,\"start\":13668},{\"end\":13786,\"start\":13776},{\"end\":13877,\"start\":13867},{\"end\":14013,\"start\":14004},{\"end\":14516,\"start\":14507},{\"end\":15375,\"start\":15366},{\"attributes\":{\"n\":\"1\"},\"end\":2909,\"start\":2897},{\"attributes\":{\"n\":\"2\"},\"end\":5823,\"start\":5795},{\"attributes\":{\"n\":\"2.1\"},\"end\":6293,\"start\":6259},{\"attributes\":{\"n\":\"2.2\"},\"end\":6894,\"start\":6854},{\"attributes\":{\"n\":\"3\"},\"end\":8250,\"start\":8244},{\"attributes\":{\"n\":\"3.1\"},\"end\":8265,\"start\":8253},{\"attributes\":{\"n\":\"3.2\"},\"end\":8782,\"start\":8768},{\"attributes\":{\"n\":\"4\"},\"end\":9292,\"start\":9281},{\"attributes\":{\"n\":\"4.1\"},\"end\":9317,\"start\":9295},{\"attributes\":{\"n\":\"4.2\"},\"end\":11187,\"start\":11160},{\"attributes\":{\"n\":\"4.3\"},\"end\":11914,\"start\":11909},{\"end\":12763,\"start\":12753},{\"end\":13285,\"start\":13275},{\"end\":13384,\"start\":13374},{\"end\":13492,\"start\":13482},{\"end\":13593,\"start\":13583},{\"end\":13678,\"start\":13668},{\"end\":13786,\"start\":13776},{\"end\":13877,\"start\":13867},{\"end\":14013,\"start\":14004},{\"end\":14516,\"start\":14507},{\"end\":15375,\"start\":15366}]", "table": null, "figure_caption": "[{\"end\":13273,\"start\":12765},{\"end\":13372,\"start\":13287},{\"end\":13480,\"start\":13386},{\"end\":13581,\"start\":13494},{\"end\":13666,\"start\":13595},{\"end\":13774,\"start\":13680},{\"end\":13865,\"start\":13788},{\"end\":14002,\"start\":13879},{\"end\":14215,\"start\":14015},{\"end\":14505,\"start\":14218},{\"end\":15364,\"start\":14518},{\"end\":15420,\"start\":15377},{\"end\":13273,\"start\":12765},{\"end\":13372,\"start\":13287},{\"end\":13480,\"start\":13386},{\"end\":13581,\"start\":13494},{\"end\":13666,\"start\":13595},{\"end\":13774,\"start\":13680},{\"end\":13865,\"start\":13788},{\"end\":14002,\"start\":13879},{\"end\":14215,\"start\":14015},{\"end\":14505,\"start\":14218},{\"end\":15364,\"start\":14518},{\"end\":15420,\"start\":15377}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8356,\"start\":8348},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8942,\"start\":8937},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12061,\"start\":12053},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12458,\"start\":12450},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":12523,\"start\":12515},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":12671,\"start\":12663},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8356,\"start\":8348},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8942,\"start\":8937},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12061,\"start\":12053},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12458,\"start\":12450},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":12523,\"start\":12515},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":12671,\"start\":12663}]", "bib_author_first_name": "[{\"end\":15483,\"start\":15479},{\"end\":15514,\"start\":15507},{\"end\":15531,\"start\":15524},{\"end\":15541,\"start\":15537},{\"end\":15553,\"start\":15549},{\"end\":15565,\"start\":15560},{\"end\":15580,\"start\":15574},{\"end\":15596,\"start\":15587},{\"end\":15612,\"start\":15605},{\"end\":16060,\"start\":16053},{\"end\":16076,\"start\":16069},{\"end\":16083,\"start\":16081},{\"end\":16093,\"start\":16089},{\"end\":16109,\"start\":16101},{\"end\":16117,\"start\":16114},{\"end\":16132,\"start\":16125},{\"end\":16146,\"start\":16140},{\"end\":16162,\"start\":16155},{\"end\":16177,\"start\":16171},{\"end\":16179,\"start\":16178},{\"end\":16193,\"start\":16190},{\"end\":16206,\"start\":16202},{\"end\":16208,\"start\":16207},{\"end\":16575,\"start\":16571},{\"end\":16588,\"start\":16583},{\"end\":16602,\"start\":16595},{\"end\":16622,\"start\":16617},{\"end\":16634,\"start\":16630},{\"end\":17067,\"start\":17066},{\"end\":17082,\"start\":17076},{\"end\":17094,\"start\":17087},{\"end\":17107,\"start\":17101},{\"end\":17123,\"start\":17116},{\"end\":17140,\"start\":17135},{\"end\":17147,\"start\":17145},{\"end\":17160,\"start\":17154},{\"end\":17547,\"start\":17546},{\"end\":17567,\"start\":17554},{\"end\":18068,\"start\":18062},{\"end\":18085,\"start\":18078},{\"end\":18104,\"start\":18097},{\"end\":18123,\"start\":18121},{\"end\":18141,\"start\":18133},{\"end\":18155,\"start\":18151},{\"end\":18684,\"start\":18678},{\"end\":18697,\"start\":18695},{\"end\":19060,\"start\":19053},{\"end\":19074,\"start\":19066},{\"end\":19087,\"start\":19079},{\"end\":19096,\"start\":19092},{\"end\":19100,\"start\":19097},{\"end\":19370,\"start\":19363},{\"end\":19387,\"start\":19379},{\"end\":19402,\"start\":19399},{\"end\":19419,\"start\":19412},{\"end\":19903,\"start\":19898},{\"end\":19920,\"start\":19912},{\"end\":19941,\"start\":19930},{\"end\":19956,\"start\":19949},{\"end\":20209,\"start\":20203},{\"end\":20224,\"start\":20216},{\"end\":20556,\"start\":20552},{\"end\":20570,\"start\":20566},{\"end\":20575,\"start\":20571},{\"end\":20586,\"start\":20581},{\"end\":20602,\"start\":20596},{\"end\":20618,\"start\":20611},{\"end\":20632,\"start\":20624},{\"end\":20648,\"start\":20642},{\"end\":20663,\"start\":20657},{\"end\":20678,\"start\":20672},{\"end\":20692,\"start\":20688},{\"end\":21135,\"start\":21129},{\"end\":21151,\"start\":21145},{\"end\":21175,\"start\":21164},{\"end\":21190,\"start\":21183},{\"end\":21206,\"start\":21199},{\"end\":21689,\"start\":21684},{\"end\":21700,\"start\":21696},{\"end\":21713,\"start\":21708},{\"end\":21723,\"start\":21719},{\"end\":22150,\"start\":22146},{\"end\":22167,\"start\":22160},{\"end\":22183,\"start\":22176},{\"end\":22199,\"start\":22193},{\"end\":22220,\"start\":22210},{\"end\":22238,\"start\":22230},{\"end\":22253,\"start\":22248},{\"end\":22276,\"start\":22272},{\"end\":22290,\"start\":22284},{\"end\":22733,\"start\":22726},{\"end\":22742,\"start\":22738},{\"end\":22753,\"start\":22749},{\"end\":23043,\"start\":23038},{\"end\":23052,\"start\":23049},{\"end\":23067,\"start\":23059},{\"end\":23079,\"start\":23074},{\"end\":23091,\"start\":23084},{\"end\":15483,\"start\":15479},{\"end\":15514,\"start\":15507},{\"end\":15531,\"start\":15524},{\"end\":15541,\"start\":15537},{\"end\":15553,\"start\":15549},{\"end\":15565,\"start\":15560},{\"end\":15580,\"start\":15574},{\"end\":15596,\"start\":15587},{\"end\":15612,\"start\":15605},{\"end\":16060,\"start\":16053},{\"end\":16076,\"start\":16069},{\"end\":16083,\"start\":16081},{\"end\":16093,\"start\":16089},{\"end\":16109,\"start\":16101},{\"end\":16117,\"start\":16114},{\"end\":16132,\"start\":16125},{\"end\":16146,\"start\":16140},{\"end\":16162,\"start\":16155},{\"end\":16177,\"start\":16171},{\"end\":16179,\"start\":16178},{\"end\":16193,\"start\":16190},{\"end\":16206,\"start\":16202},{\"end\":16208,\"start\":16207},{\"end\":16575,\"start\":16571},{\"end\":16588,\"start\":16583},{\"end\":16602,\"start\":16595},{\"end\":16622,\"start\":16617},{\"end\":16634,\"start\":16630},{\"end\":17067,\"start\":17066},{\"end\":17082,\"start\":17076},{\"end\":17094,\"start\":17087},{\"end\":17107,\"start\":17101},{\"end\":17123,\"start\":17116},{\"end\":17140,\"start\":17135},{\"end\":17147,\"start\":17145},{\"end\":17160,\"start\":17154},{\"end\":17547,\"start\":17546},{\"end\":17567,\"start\":17554},{\"end\":18068,\"start\":18062},{\"end\":18085,\"start\":18078},{\"end\":18104,\"start\":18097},{\"end\":18123,\"start\":18121},{\"end\":18141,\"start\":18133},{\"end\":18155,\"start\":18151},{\"end\":18684,\"start\":18678},{\"end\":18697,\"start\":18695},{\"end\":19060,\"start\":19053},{\"end\":19074,\"start\":19066},{\"end\":19087,\"start\":19079},{\"end\":19096,\"start\":19092},{\"end\":19100,\"start\":19097},{\"end\":19370,\"start\":19363},{\"end\":19387,\"start\":19379},{\"end\":19402,\"start\":19399},{\"end\":19419,\"start\":19412},{\"end\":19903,\"start\":19898},{\"end\":19920,\"start\":19912},{\"end\":19941,\"start\":19930},{\"end\":19956,\"start\":19949},{\"end\":20209,\"start\":20203},{\"end\":20224,\"start\":20216},{\"end\":20556,\"start\":20552},{\"end\":20570,\"start\":20566},{\"end\":20575,\"start\":20571},{\"end\":20586,\"start\":20581},{\"end\":20602,\"start\":20596},{\"end\":20618,\"start\":20611},{\"end\":20632,\"start\":20624},{\"end\":20648,\"start\":20642},{\"end\":20663,\"start\":20657},{\"end\":20678,\"start\":20672},{\"end\":20692,\"start\":20688},{\"end\":21135,\"start\":21129},{\"end\":21151,\"start\":21145},{\"end\":21175,\"start\":21164},{\"end\":21190,\"start\":21183},{\"end\":21206,\"start\":21199},{\"end\":21689,\"start\":21684},{\"end\":21700,\"start\":21696},{\"end\":21713,\"start\":21708},{\"end\":21723,\"start\":21719},{\"end\":22150,\"start\":22146},{\"end\":22167,\"start\":22160},{\"end\":22183,\"start\":22176},{\"end\":22199,\"start\":22193},{\"end\":22220,\"start\":22210},{\"end\":22238,\"start\":22230},{\"end\":22253,\"start\":22248},{\"end\":22276,\"start\":22272},{\"end\":22290,\"start\":22284},{\"end\":22733,\"start\":22726},{\"end\":22742,\"start\":22738},{\"end\":22753,\"start\":22749},{\"end\":23043,\"start\":23038},{\"end\":23052,\"start\":23049},{\"end\":23067,\"start\":23059},{\"end\":23079,\"start\":23074},{\"end\":23091,\"start\":23084}]", "bib_author_last_name": "[{\"end\":15505,\"start\":15484},{\"end\":15522,\"start\":15515},{\"end\":15535,\"start\":15532},{\"end\":15547,\"start\":15542},{\"end\":15558,\"start\":15554},{\"end\":15572,\"start\":15566},{\"end\":15585,\"start\":15581},{\"end\":15603,\"start\":15597},{\"end\":15621,\"start\":15613},{\"end\":15631,\"start\":15623},{\"end\":16067,\"start\":16061},{\"end\":16079,\"start\":16077},{\"end\":16087,\"start\":16084},{\"end\":16099,\"start\":16094},{\"end\":16112,\"start\":16110},{\"end\":16123,\"start\":16118},{\"end\":16138,\"start\":16133},{\"end\":16153,\"start\":16147},{\"end\":16169,\"start\":16163},{\"end\":16188,\"start\":16180},{\"end\":16200,\"start\":16194},{\"end\":16213,\"start\":16209},{\"end\":16581,\"start\":16576},{\"end\":16593,\"start\":16589},{\"end\":16615,\"start\":16603},{\"end\":16628,\"start\":16623},{\"end\":16641,\"start\":16635},{\"end\":17074,\"start\":17068},{\"end\":17085,\"start\":17083},{\"end\":17099,\"start\":17095},{\"end\":17114,\"start\":17108},{\"end\":17133,\"start\":17124},{\"end\":17143,\"start\":17141},{\"end\":17152,\"start\":17148},{\"end\":17165,\"start\":17161},{\"end\":17171,\"start\":17167},{\"end\":17177,\"start\":17173},{\"end\":17552,\"start\":17548},{\"end\":17574,\"start\":17568},{\"end\":17583,\"start\":17576},{\"end\":18076,\"start\":18069},{\"end\":18095,\"start\":18086},{\"end\":18119,\"start\":18105},{\"end\":18131,\"start\":18124},{\"end\":18149,\"start\":18142},{\"end\":18164,\"start\":18156},{\"end\":18693,\"start\":18685},{\"end\":18705,\"start\":18698},{\"end\":19064,\"start\":19061},{\"end\":19077,\"start\":19075},{\"end\":19090,\"start\":19088},{\"end\":19104,\"start\":19101},{\"end\":19377,\"start\":19371},{\"end\":19397,\"start\":19388},{\"end\":19410,\"start\":19403},{\"end\":19428,\"start\":19420},{\"end\":19910,\"start\":19904},{\"end\":19928,\"start\":19921},{\"end\":19947,\"start\":19942},{\"end\":19968,\"start\":19957},{\"end\":20162,\"start\":20156},{\"end\":20214,\"start\":20210},{\"end\":20227,\"start\":20225},{\"end\":20564,\"start\":20557},{\"end\":20579,\"start\":20576},{\"end\":20594,\"start\":20587},{\"end\":20609,\"start\":20603},{\"end\":20622,\"start\":20619},{\"end\":20640,\"start\":20633},{\"end\":20655,\"start\":20649},{\"end\":20670,\"start\":20664},{\"end\":20686,\"start\":20679},{\"end\":20698,\"start\":20693},{\"end\":21143,\"start\":21136},{\"end\":21162,\"start\":21152},{\"end\":21181,\"start\":21176},{\"end\":21197,\"start\":21191},{\"end\":21215,\"start\":21207},{\"end\":21694,\"start\":21690},{\"end\":21706,\"start\":21701},{\"end\":21717,\"start\":21714},{\"end\":21729,\"start\":21724},{\"end\":22158,\"start\":22151},{\"end\":22174,\"start\":22168},{\"end\":22191,\"start\":22184},{\"end\":22208,\"start\":22200},{\"end\":22228,\"start\":22221},{\"end\":22246,\"start\":22239},{\"end\":22270,\"start\":22254},{\"end\":22282,\"start\":22277},{\"end\":22297,\"start\":22291},{\"end\":22304,\"start\":22299},{\"end\":22736,\"start\":22734},{\"end\":22747,\"start\":22743},{\"end\":22759,\"start\":22754},{\"end\":23047,\"start\":23044},{\"end\":23057,\"start\":23053},{\"end\":23072,\"start\":23068},{\"end\":23082,\"start\":23080},{\"end\":23101,\"start\":23092},{\"end\":15505,\"start\":15484},{\"end\":15522,\"start\":15515},{\"end\":15535,\"start\":15532},{\"end\":15547,\"start\":15542},{\"end\":15558,\"start\":15554},{\"end\":15572,\"start\":15566},{\"end\":15585,\"start\":15581},{\"end\":15603,\"start\":15597},{\"end\":15621,\"start\":15613},{\"end\":15631,\"start\":15623},{\"end\":16067,\"start\":16061},{\"end\":16079,\"start\":16077},{\"end\":16087,\"start\":16084},{\"end\":16099,\"start\":16094},{\"end\":16112,\"start\":16110},{\"end\":16123,\"start\":16118},{\"end\":16138,\"start\":16133},{\"end\":16153,\"start\":16147},{\"end\":16169,\"start\":16163},{\"end\":16188,\"start\":16180},{\"end\":16200,\"start\":16194},{\"end\":16213,\"start\":16209},{\"end\":16581,\"start\":16576},{\"end\":16593,\"start\":16589},{\"end\":16615,\"start\":16603},{\"end\":16628,\"start\":16623},{\"end\":16641,\"start\":16635},{\"end\":17074,\"start\":17068},{\"end\":17085,\"start\":17083},{\"end\":17099,\"start\":17095},{\"end\":17114,\"start\":17108},{\"end\":17133,\"start\":17124},{\"end\":17143,\"start\":17141},{\"end\":17152,\"start\":17148},{\"end\":17165,\"start\":17161},{\"end\":17171,\"start\":17167},{\"end\":17177,\"start\":17173},{\"end\":17552,\"start\":17548},{\"end\":17574,\"start\":17568},{\"end\":17583,\"start\":17576},{\"end\":18076,\"start\":18069},{\"end\":18095,\"start\":18086},{\"end\":18119,\"start\":18105},{\"end\":18131,\"start\":18124},{\"end\":18149,\"start\":18142},{\"end\":18164,\"start\":18156},{\"end\":18693,\"start\":18685},{\"end\":18705,\"start\":18698},{\"end\":19064,\"start\":19061},{\"end\":19077,\"start\":19075},{\"end\":19090,\"start\":19088},{\"end\":19104,\"start\":19101},{\"end\":19377,\"start\":19371},{\"end\":19397,\"start\":19388},{\"end\":19410,\"start\":19403},{\"end\":19428,\"start\":19420},{\"end\":19910,\"start\":19904},{\"end\":19928,\"start\":19921},{\"end\":19947,\"start\":19942},{\"end\":19968,\"start\":19957},{\"end\":20162,\"start\":20156},{\"end\":20214,\"start\":20210},{\"end\":20227,\"start\":20225},{\"end\":20564,\"start\":20557},{\"end\":20579,\"start\":20576},{\"end\":20594,\"start\":20587},{\"end\":20609,\"start\":20603},{\"end\":20622,\"start\":20619},{\"end\":20640,\"start\":20633},{\"end\":20655,\"start\":20649},{\"end\":20670,\"start\":20664},{\"end\":20686,\"start\":20679},{\"end\":20698,\"start\":20693},{\"end\":21143,\"start\":21136},{\"end\":21162,\"start\":21152},{\"end\":21181,\"start\":21176},{\"end\":21197,\"start\":21191},{\"end\":21215,\"start\":21207},{\"end\":21694,\"start\":21690},{\"end\":21706,\"start\":21701},{\"end\":21717,\"start\":21714},{\"end\":21729,\"start\":21724},{\"end\":22158,\"start\":22151},{\"end\":22174,\"start\":22168},{\"end\":22191,\"start\":22184},{\"end\":22208,\"start\":22200},{\"end\":22228,\"start\":22221},{\"end\":22246,\"start\":22239},{\"end\":22270,\"start\":22254},{\"end\":22282,\"start\":22277},{\"end\":22297,\"start\":22291},{\"end\":22304,\"start\":22299},{\"end\":22736,\"start\":22734},{\"end\":22747,\"start\":22743},{\"end\":22759,\"start\":22754},{\"end\":23047,\"start\":23044},{\"end\":23057,\"start\":23053},{\"end\":23072,\"start\":23068},{\"end\":23082,\"start\":23080},{\"end\":23101,\"start\":23092}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":248476411},\"end\":15976,\"start\":15422},{\"attributes\":{\"id\":\"b1\"},\"end\":16469,\"start\":15978},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8081284},\"end\":17064,\"start\":16471},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b3\"},\"end\":17455,\"start\":17066},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":152282269},\"end\":17972,\"start\":17457},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15458100},\"end\":18609,\"start\":17974},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8517067},\"end\":19051,\"start\":18611},{\"attributes\":{\"doi\":\"arXiv:2304.08485\",\"id\":\"b7\"},\"end\":19285,\"start\":19053},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":173991173},\"end\":19834,\"start\":19287},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209413409},\"end\":20130,\"start\":19836},{\"attributes\":{\"id\":\"b10\"},\"end\":20201,\"start\":20132},{\"attributes\":{\"doi\":\"arXiv:2304.03277\",\"id\":\"b11\"},\"end\":20479,\"start\":20203},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":231591445},\"end\":21053,\"start\":20481},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":249375629},\"end\":21631,\"start\":21055},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":19435386},\"end\":22144,\"start\":21633},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b15\"},\"end\":22644,\"start\":22146},{\"attributes\":{\"doi\":\"arXiv:2212.10773\",\"id\":\"b16\"},\"end\":22948,\"start\":22646},{\"attributes\":{\"doi\":\"arXiv:2304.10592\",\"id\":\"b17\"},\"end\":23330,\"start\":22950},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":248476411},\"end\":15976,\"start\":15422},{\"attributes\":{\"id\":\"b1\"},\"end\":16469,\"start\":15978},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8081284},\"end\":17064,\"start\":16471},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b3\"},\"end\":17455,\"start\":17066},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":152282269},\"end\":17972,\"start\":17457},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15458100},\"end\":18609,\"start\":17974},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8517067},\"end\":19051,\"start\":18611},{\"attributes\":{\"doi\":\"arXiv:2304.08485\",\"id\":\"b7\"},\"end\":19285,\"start\":19053},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":173991173},\"end\":19834,\"start\":19287},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209413409},\"end\":20130,\"start\":19836},{\"attributes\":{\"id\":\"b10\"},\"end\":20201,\"start\":20132},{\"attributes\":{\"doi\":\"arXiv:2304.03277\",\"id\":\"b11\"},\"end\":20479,\"start\":20203},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":231591445},\"end\":21053,\"start\":20481},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":249375629},\"end\":21631,\"start\":21055},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":19435386},\"end\":22144,\"start\":21633},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b15\"},\"end\":22644,\"start\":22146},{\"attributes\":{\"doi\":\"arXiv:2212.10773\",\"id\":\"b16\"},\"end\":22948,\"start\":22646},{\"attributes\":{\"doi\":\"arXiv:2304.10592\",\"id\":\"b17\"},\"end\":23330,\"start\":22950}]", "bib_title": "[{\"end\":15477,\"start\":15422},{\"end\":16569,\"start\":16471},{\"end\":17544,\"start\":17457},{\"end\":18060,\"start\":17974},{\"end\":18676,\"start\":18611},{\"end\":19361,\"start\":19287},{\"end\":19896,\"start\":19836},{\"end\":20550,\"start\":20481},{\"end\":21127,\"start\":21055},{\"end\":21682,\"start\":21633},{\"end\":15477,\"start\":15422},{\"end\":16569,\"start\":16471},{\"end\":17544,\"start\":17457},{\"end\":18060,\"start\":17974},{\"end\":18676,\"start\":18611},{\"end\":19361,\"start\":19287},{\"end\":19896,\"start\":19836},{\"end\":20550,\"start\":20481},{\"end\":21127,\"start\":21055},{\"end\":21682,\"start\":21633}]", "bib_author": "[{\"end\":15507,\"start\":15479},{\"end\":15524,\"start\":15507},{\"end\":15537,\"start\":15524},{\"end\":15549,\"start\":15537},{\"end\":15560,\"start\":15549},{\"end\":15574,\"start\":15560},{\"end\":15587,\"start\":15574},{\"end\":15605,\"start\":15587},{\"end\":15623,\"start\":15605},{\"end\":15633,\"start\":15623},{\"end\":16069,\"start\":16053},{\"end\":16081,\"start\":16069},{\"end\":16089,\"start\":16081},{\"end\":16101,\"start\":16089},{\"end\":16114,\"start\":16101},{\"end\":16125,\"start\":16114},{\"end\":16140,\"start\":16125},{\"end\":16155,\"start\":16140},{\"end\":16171,\"start\":16155},{\"end\":16190,\"start\":16171},{\"end\":16202,\"start\":16190},{\"end\":16215,\"start\":16202},{\"end\":16583,\"start\":16571},{\"end\":16595,\"start\":16583},{\"end\":16617,\"start\":16595},{\"end\":16630,\"start\":16617},{\"end\":16643,\"start\":16630},{\"end\":17076,\"start\":17066},{\"end\":17087,\"start\":17076},{\"end\":17101,\"start\":17087},{\"end\":17116,\"start\":17101},{\"end\":17135,\"start\":17116},{\"end\":17145,\"start\":17135},{\"end\":17154,\"start\":17145},{\"end\":17167,\"start\":17154},{\"end\":17173,\"start\":17167},{\"end\":17179,\"start\":17173},{\"end\":17554,\"start\":17546},{\"end\":17576,\"start\":17554},{\"end\":17585,\"start\":17576},{\"end\":18078,\"start\":18062},{\"end\":18097,\"start\":18078},{\"end\":18121,\"start\":18097},{\"end\":18133,\"start\":18121},{\"end\":18151,\"start\":18133},{\"end\":18166,\"start\":18151},{\"end\":18695,\"start\":18678},{\"end\":18707,\"start\":18695},{\"end\":19066,\"start\":19053},{\"end\":19079,\"start\":19066},{\"end\":19092,\"start\":19079},{\"end\":19106,\"start\":19092},{\"end\":19379,\"start\":19363},{\"end\":19399,\"start\":19379},{\"end\":19412,\"start\":19399},{\"end\":19430,\"start\":19412},{\"end\":19912,\"start\":19898},{\"end\":19930,\"start\":19912},{\"end\":19949,\"start\":19930},{\"end\":19970,\"start\":19949},{\"end\":20164,\"start\":20156},{\"end\":20216,\"start\":20203},{\"end\":20229,\"start\":20216},{\"end\":20566,\"start\":20552},{\"end\":20581,\"start\":20566},{\"end\":20596,\"start\":20581},{\"end\":20611,\"start\":20596},{\"end\":20624,\"start\":20611},{\"end\":20642,\"start\":20624},{\"end\":20657,\"start\":20642},{\"end\":20672,\"start\":20657},{\"end\":20688,\"start\":20672},{\"end\":20700,\"start\":20688},{\"end\":21145,\"start\":21129},{\"end\":21164,\"start\":21145},{\"end\":21183,\"start\":21164},{\"end\":21199,\"start\":21183},{\"end\":21217,\"start\":21199},{\"end\":21696,\"start\":21684},{\"end\":21708,\"start\":21696},{\"end\":21719,\"start\":21708},{\"end\":21731,\"start\":21719},{\"end\":22160,\"start\":22146},{\"end\":22176,\"start\":22160},{\"end\":22193,\"start\":22176},{\"end\":22210,\"start\":22193},{\"end\":22230,\"start\":22210},{\"end\":22248,\"start\":22230},{\"end\":22272,\"start\":22248},{\"end\":22284,\"start\":22272},{\"end\":22299,\"start\":22284},{\"end\":22306,\"start\":22299},{\"end\":22738,\"start\":22726},{\"end\":22749,\"start\":22738},{\"end\":22761,\"start\":22749},{\"end\":23049,\"start\":23038},{\"end\":23059,\"start\":23049},{\"end\":23074,\"start\":23059},{\"end\":23084,\"start\":23074},{\"end\":23103,\"start\":23084},{\"end\":15507,\"start\":15479},{\"end\":15524,\"start\":15507},{\"end\":15537,\"start\":15524},{\"end\":15549,\"start\":15537},{\"end\":15560,\"start\":15549},{\"end\":15574,\"start\":15560},{\"end\":15587,\"start\":15574},{\"end\":15605,\"start\":15587},{\"end\":15623,\"start\":15605},{\"end\":15633,\"start\":15623},{\"end\":16069,\"start\":16053},{\"end\":16081,\"start\":16069},{\"end\":16089,\"start\":16081},{\"end\":16101,\"start\":16089},{\"end\":16114,\"start\":16101},{\"end\":16125,\"start\":16114},{\"end\":16140,\"start\":16125},{\"end\":16155,\"start\":16140},{\"end\":16171,\"start\":16155},{\"end\":16190,\"start\":16171},{\"end\":16202,\"start\":16190},{\"end\":16215,\"start\":16202},{\"end\":16583,\"start\":16571},{\"end\":16595,\"start\":16583},{\"end\":16617,\"start\":16595},{\"end\":16630,\"start\":16617},{\"end\":16643,\"start\":16630},{\"end\":17076,\"start\":17066},{\"end\":17087,\"start\":17076},{\"end\":17101,\"start\":17087},{\"end\":17116,\"start\":17101},{\"end\":17135,\"start\":17116},{\"end\":17145,\"start\":17135},{\"end\":17154,\"start\":17145},{\"end\":17167,\"start\":17154},{\"end\":17173,\"start\":17167},{\"end\":17179,\"start\":17173},{\"end\":17554,\"start\":17546},{\"end\":17576,\"start\":17554},{\"end\":17585,\"start\":17576},{\"end\":18078,\"start\":18062},{\"end\":18097,\"start\":18078},{\"end\":18121,\"start\":18097},{\"end\":18133,\"start\":18121},{\"end\":18151,\"start\":18133},{\"end\":18166,\"start\":18151},{\"end\":18695,\"start\":18678},{\"end\":18707,\"start\":18695},{\"end\":19066,\"start\":19053},{\"end\":19079,\"start\":19066},{\"end\":19092,\"start\":19079},{\"end\":19106,\"start\":19092},{\"end\":19379,\"start\":19363},{\"end\":19399,\"start\":19379},{\"end\":19412,\"start\":19399},{\"end\":19430,\"start\":19412},{\"end\":19912,\"start\":19898},{\"end\":19930,\"start\":19912},{\"end\":19949,\"start\":19930},{\"end\":19970,\"start\":19949},{\"end\":20164,\"start\":20156},{\"end\":20216,\"start\":20203},{\"end\":20229,\"start\":20216},{\"end\":20566,\"start\":20552},{\"end\":20581,\"start\":20566},{\"end\":20596,\"start\":20581},{\"end\":20611,\"start\":20596},{\"end\":20624,\"start\":20611},{\"end\":20642,\"start\":20624},{\"end\":20657,\"start\":20642},{\"end\":20672,\"start\":20657},{\"end\":20688,\"start\":20672},{\"end\":20700,\"start\":20688},{\"end\":21145,\"start\":21129},{\"end\":21164,\"start\":21145},{\"end\":21183,\"start\":21164},{\"end\":21199,\"start\":21183},{\"end\":21217,\"start\":21199},{\"end\":21696,\"start\":21684},{\"end\":21708,\"start\":21696},{\"end\":21719,\"start\":21708},{\"end\":21731,\"start\":21719},{\"end\":22160,\"start\":22146},{\"end\":22176,\"start\":22160},{\"end\":22193,\"start\":22176},{\"end\":22210,\"start\":22193},{\"end\":22230,\"start\":22210},{\"end\":22248,\"start\":22230},{\"end\":22272,\"start\":22248},{\"end\":22284,\"start\":22272},{\"end\":22299,\"start\":22284},{\"end\":22306,\"start\":22299},{\"end\":22738,\"start\":22726},{\"end\":22749,\"start\":22738},{\"end\":22761,\"start\":22749},{\"end\":23049,\"start\":23038},{\"end\":23059,\"start\":23049},{\"end\":23074,\"start\":23059},{\"end\":23084,\"start\":23074},{\"end\":23103,\"start\":23084}]", "bib_venue": "[{\"end\":16784,\"start\":16722},{\"end\":17734,\"start\":17668},{\"end\":18307,\"start\":18245},{\"end\":18848,\"start\":18786},{\"end\":19579,\"start\":19513},{\"end\":21286,\"start\":21270},{\"end\":21892,\"start\":21820},{\"end\":16784,\"start\":16722},{\"end\":17734,\"start\":17668},{\"end\":18307,\"start\":18245},{\"end\":18848,\"start\":18786},{\"end\":19579,\"start\":19513},{\"end\":21286,\"start\":21270},{\"end\":21892,\"start\":21820},{\"end\":15682,\"start\":15633},{\"end\":16051,\"start\":15978},{\"end\":16720,\"start\":16643},{\"end\":17239,\"start\":17195},{\"end\":17666,\"start\":17585},{\"end\":18243,\"start\":18166},{\"end\":18784,\"start\":18707},{\"end\":19147,\"start\":19122},{\"end\":19511,\"start\":19430},{\"end\":19975,\"start\":19970},{\"end\":20154,\"start\":20132},{\"end\":20321,\"start\":20245},{\"end\":20744,\"start\":20700},{\"end\":21268,\"start\":21217},{\"end\":21818,\"start\":21731},{\"end\":22367,\"start\":22322},{\"end\":22724,\"start\":22646},{\"end\":23036,\"start\":22950},{\"end\":15682,\"start\":15633},{\"end\":16051,\"start\":15978},{\"end\":16720,\"start\":16643},{\"end\":17239,\"start\":17195},{\"end\":17666,\"start\":17585},{\"end\":18243,\"start\":18166},{\"end\":18784,\"start\":18707},{\"end\":19147,\"start\":19122},{\"end\":19511,\"start\":19430},{\"end\":19975,\"start\":19970},{\"end\":20154,\"start\":20132},{\"end\":20321,\"start\":20245},{\"end\":20744,\"start\":20700},{\"end\":21268,\"start\":21217},{\"end\":21818,\"start\":21731},{\"end\":22367,\"start\":22322},{\"end\":22724,\"start\":22646},{\"end\":23036,\"start\":22950}]"}}}, "year": 2023, "month": 12, "day": 17}