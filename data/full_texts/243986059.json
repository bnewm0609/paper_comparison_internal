{"id": 243986059, "updated": "2023-10-05 19:39:09.331", "metadata": {"title": "Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic", "authors": "[{\"first\":\"Wei\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Zhaojian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Huilin\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Wanchen\",\"last\":\"Ge\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL), a powerful data-driven control method, has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic network (MA2C) is developed with a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is proposed to incorporate fuel efficiency, driving comfort, and safety of autonomous driving. Comprehensive experimental results, conducted under three different traffic densities and various levels of human driver aggressiveness, show that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety and driver comfort.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.06318", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/auinsy/ZhouCYLYG22", "doi": "10.1007/s43684-022-00023-5"}}, "content": {"source": {"pdf_hash": "56b8480dfec8bd44d95339c9952dc460609dfc5d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.06318v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a34e9eff1317dd4efc0890239369c3e6a913b0aa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/56b8480dfec8bd44d95339c9952dc460609dfc5d.txt", "contents": "\nSpringer Nature 2021 L A T E X template Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic\n\n\nWei Zhou \nSchool of Electronic and Information Engineering\nTongji University\nCaoangong Street201804ShanghaiChina\n\nDong Chen chendon9@msu.edu \nMechanical Engineering\nMichigan State University\n48824LansingUSA\n\n\u2020 \nJun Yan \nSchool of Electronic and Information Engineering\nTongji University\nCaoangong Street201804ShanghaiChina\n\nZhaojian Li \nMechanical Engineering\nMichigan State University\n48824LansingUSA\n\n\u2020 \nHuilin Yin yinhuilin@tongji.edu.cn \nSchool of Electronic and Information Engineering\nTongji University\nCaoangong Street201804ShanghaiChina\n\nWanchen Ge \nSchool of Electronic and Information Engineering\nTongji University\nCaoangong Street201804ShanghaiChina\n\nSpringer Nature 2021 L A T E X template Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic\n\u2020 These authors contributed equally to this work.Multi-agent deep reinforcement learninglane-changingconnected autonomous vehiclesmixed traffic\nAutonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL), a powerful datadriven control method, has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with humandriven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic network (MA2C) is developed with arXiv:2111.06318v1 [cs.LG] 11 Nov 2021Springer Nature 2021 L A T E X template 2Autonomous Intelligent Systems a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is proposed to incorporate fuel efficiency, driving comfort, and safety of autonomous driving. Comprehensive experimental results, conducted under three different traffic densities and various levels of human driver aggressiveness, show that our proposed MARL framework consistently outperforms several stateof-the-art benchmarks in terms of efficiency, safety and driver comfort.\n\nIntroduction\n\nAutonomous driving has received significant research interests in the past two decades due to its many potential societal and economical benefits. Compared to traditional vehicles, autonomous vehicles (AVs) not only promise fewer emissions [1] but are also expected to improve safety and efficiency. Despite remarkable progress, high-level decision-making in AVs remains a big challenge due to the complex and dynamic traffic environment, especially in mixed traffic co-existing with other road users. Lane changing is one such challenging high-level decision-making in AVs, which has significant influences on traffic safety and efficiency [2,3], and is the focus of this paper.\n\nThe considered lane-changing scenario is illustrated in Fig. 1, where AVs and HDVs coexist on a one-way highway with two lanes. The AVs aim to safely travel through the traffic while making necessary lane changes to overtake slow-moving vehicles for improved efficiency. Furthermore, in the presence of multiple AVs, the AVs are expected to collaboratively learn a policy to adapt to HDVs and enable safe and efficient lane changes. As HDVs bring unknown/uncertain behaviors, planning and control in such mixed traffic to realize safe and efficient maneuvers is a challenging task [4]. the use of RL in AV lane-changing [4,7,8], which consider a single AV setting where the ego vehicle learns a lane-changing behavior by taking all other vehicles as part of the driving environment for decision making. While completely scalable, this single-agent approach will lead to unsatisfactory performance in the complex environment like multi-AV lane-changing in mixed traffic that requires close collaboration and coordination among AVs [9].\n\nOn the other hand, multi-agent reinforcement learning (MARL) has been greatly advanced and successfully applied to a variety of complex multi-agent systems such as games [10], traffic light control [11] and fleet management [12]. The applications of MARL to autonomous driving also exist [13][14][15][16], with the objective of accomplishing autonomous driving tasks cooperatively and reacting timely to HDVs. In particular, previous works [15,17] also apply the MARL to highway lane change tasks and show promising and scalable performance, in which AVs learn cooperatively via sharing the same objective (i.e., reward/cost function) that considers safety and efficiency. However, those reward designs often ignore the passengers' comfort, which may lead to sudden acceleration and deceleration that can cause ride discomfort. In addition, they assume that the HDVs follow unchanged, universal human driver behaviors, which is clearly oversimplified and impractical in the real world as different human drivers tend to behave quite differently. Learning algorithms should thus work with different human driving behaviors, e.g., aggressive or mild.\n\nTo address the above issues, we develop a multi-agent reinforcement learning algorithm by employing a multi-agent advantage actor-critic network (MA2C) for multi-AV lane-changing decision making, featuring a novel local reward design that incorporates the safety, efficiency and passenger comfort as well as a parameter sharing scheme to foster inter-agent collaborations. The main contributions and the technical advancements of this paper are summarized as follows.\n\n1. We formulate the multi-AV highway lane changing in mixed traffic is modeled as a decentralized cooperative MARL problem, where agents cooperatively learn a safe and efficient driving policy. 2. We develop a novel, efficient, and scalable MARL algorithm, multiagent advantage actor-critic network, by introducing a parameter-sharing mechanism and effective reward function design. 3. We conduct comprehensive experiments on three different traffic densities and two levels of drivers' behavior modes, and the results show that the proposed approach consistently outperforms several state-of-the-art algorithms in terms of driving safety, efficiency and driver comfort. The rest of the paper is organized as follows. Section 2 reviews the state-of-the-art dynamics-based and RL/MARL algorithms for autonomous driving tasks. The preliminaries of RL and the proposed MARL algorithm are introduced in Section 3. Experiments, results, and discussions are presented in Section 4. Finally, we summarize the paper and discuss future work in Section 5.\n\n\nRelated Work\n\nIn this section, we survey the existing literature on decision-making tasks in autonomous driving, which can be mainly classified into two categories: nondata-driven and data-driven methods.\n\n\nNon-data-driven Methods\n\nConventional rule-based or model-based approaches [18][19][20] rely on hard-coded rules or dynamical models to construct predefined logic mechanisms to determine the behaviors of ego vehicles under different situations. For instance, in [18], lane-changing guidance is provided by establishing virtual trajectory references for every vehicle, and a safe trajectory is then planned by considering the trajectories of other vehicles. In [19], a low-complexity lane-changing algorithm is developed by following heuristic rules such as keeping appropriate inter-vehicle traffic gaps and time instances to perform the maneuver. In addition, an optimization-based lane change approach is proposed in [20], which formulates the trajectory planning problem as coupled longitudinal and lateral predictive control problems and is then solved via Quadratic Programs under specific system constraints. However, the rules and optimization criteria for real-world driving problems, especially in mixed-traffic scenarios with unknown and stochastic driver's behaviors, may become too complex to be explicitly formulated for all scenarios.\n\n\nData-Driven Methods\n\nRecently, data-driven methods, such as reinforcement learning (RL), have received great attention and been widely explored for autonomous driving tasks. Particularly, a model-free RL approach based on deep deterministic policy gradient (DDPG) is proposed in [5] to learn a continuous control policy efficient lane changing. In [4], a safe RL framework is presented by integrating a lane-changing regret model into a safety supervisor based on an extended double deep Q-network (DDQN). In [21], a hierarchical RL algorithm is developed to learn lane-changing behaviors in dense traffic by applying the designed temporal and spatial attention strategies, and promising performance are demonstrated in the TORCS simulator under various lane change scenarios. However, the aforementioned methods are designed for the singleagent (i.e., one ego vehicle) scenarios, treating all other vehicles as part of the environment, which makes them implausible for the considered multi-agent lane-changing setting where collaboration and coordination among AVs are required.\n\nOn the other hand, multi-agent reinforcement learning (MARL) has also been explored for autonomous driving tasks [13,14,22,23]. In particular, in [13], a MARL algorithm with hard-coded safety constraints is proposed to solve the double-merge problem. Also, a hierarchical temporal abstraction method is applied to reduce the effective horizon and the variance of the gradient estimation error. In [23], a MARL algorithm is proposed to solve the on-ramp merging problem with safety enhancement by a novel priority-based safety supervisor. In addition, the authors in [22] propose a novel MARL approach combining Graphic Convolution Neural Network (GCN) [24] and Deep Q Network (DQN) [25] to better fuse the acquired information from collaborative sensing, showing promising results on a 3-lane freeway containing 2 off-ramps highway environment. While these MARL algorithms only consider the efficiency and safety in their designed reward function, another important factor, the passenger comfort, is not considered in their reward function design. Furthermore, those approaches assume the HDVs follow a constant, universal driving behavior, which has limited implications for real-world applications as different human drivers may behave totally differently.\n\nIn this paper, we formulate the decision making of multiple AVs on highway lane changing as a MARL problem, where a multi-objective reward function is proposed to simultaneously promote safety, efficiency and passenger comfort. A parameter-sharing scheme is exploited to foster inter-agent collaborations. Experimental results on three different traffic densities with two levels of driver aggressiveness show the proposed MARL performs well on different lane change scenarios.\n\n\nProblem Formulation\n\nIn this section, we review the preliminaries of RL and formulate the considered highway lane-changing problem as a partially observable Markov decision process (POMDP). Then we present the proposed multi-agent actor-critic algorithm, featuring a parameter-sharing mechanism and efficient reward function design, to solve the formulated POMDP.\n\n\nPreliminary of RL\n\nIn the standard RL setting, the agent aims to learn an optimal policy \u03c0 * to maximize the accumulated future reward R t = T k=0 \u03b3 k r t+k from the time step t with discount factor \u03b3 \u2208 (0, 1] by continuous interacting with the environment. Specially, at time step t, the agent receives a state s t \u2208 R n from the environment and selects an action a t \u2208 A m according to its policy \u03c0 : S \u2192 Pr(A). As a result, the agent receives the next state s t+1 and receives a scalar reward r t . If the agent can only observe a part of the state s t , the underlying dynamics becomes a POMDP [26] and the goal is then to learn a policy that maps from the partial observation to an appropriate action to maximize the rewards.\n\nThe action-value function Q \u03c0 (s, a) = E[R t |s = s t , a] is defined as the expected return obtained by selecting an action a in state s t and following policy \u03c0 afterwards. The optimal Q-function is given by Q * (s, a) = max \u03c0 Q \u03c0 (s, a) for state s and action a. Similarly, the state-value function is defined as V \u03c0 (s t ) = E \u03c0 [R t |s = s t ] representing the expected return for following the policy \u03c0 from state s t . In model-free RL methods, the policy is often represented by a neural network denoted as \u03c0 \u03b8 (a t |s t ), where \u03b8 is the learnable parameters. In actor-critic (A2C) algorithms [27], a critic network, parameterized by \u03c9, learns the state-value function V \u03c0 \u03b8 \u03c9 (s t ) and an actor network \u03c0 \u03b8 (a t |s t ) parameterized by \u03b8 is applied to update the policy distribution in the direction suggested by the critic network as follows:\n\u03b8 \u2190 \u03b8 + E \u03c0 \u03b8 \u2207 \u03b8 log \u03c0 \u03b8 (a t |s t ) A t ,(1)\nwhere the advantage function [27] is introduced to reduce the sample variance. The parameters of the state-value function are then updated by minimizing the following loss function:\nA t = Q \u03c0 \u03b8 (s, a) \u2212 V \u03c0 \u03b8 \u03c9 (s t )min \u03c9 E B R t + \u03b3V \u03c9 (s t+1 ) \u2212 V \u03c9 (s t ) 2 ,(2)\nwhere B is the experience replay buffer that stores previously encountered trajectories and \u03c9 denotes the parameters of the target network [25].\n\n\nLane Changing as MARL\n\nIn this subsection, we develop a decentralized, MARL-based approach for highway lane-changing of multiple AVs. In particular, we model the mixed-traffic lane-changing environment as a multi-agent network: G = (\u03bd, \u03b5), where each agent (i.e., ego vehicle) i \u2208 \u03bd communicates with its neighbors N i via the communication link \u03b5 ij \u2208 \u03b5. The corresponding POMDP is characterized as\n({A i , O i , R i } i\u2286\u03bd , T ), where O i \u2208 S i\nis the partial description of the environment state as stated in [28]. In a multi-agent POMDP, each agent i follows a decentralized policy \u03c0 i : O i \u00d7 S i \u2192 [0, 1] to choose the action a t at time step t. The described POMDP is defined as:\n1. State Space: The state space O i of Agent i is defined as a matrix N Ni \u00d7F,\nwhere N Ni is the number of detected vehicles, and F is the number of features, which is used to represent the current state of vehicles. It includes the longitudinal position x, the lateral position y of the observed vehicle relative to the ego vehicle, the longitudinal speed v x , and the lateral speed v y of the observed vehicle relative to the ego vehicle. 2. Action Space: The action space A i of agent i is defined as a set of highlevel control decisions, including speed up, slow down, cruising, turn left, and turn right. The action space combination for AVs is defined as A = A 1 \u00d7 A 2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 A N , where N is the total number of vehicles in the scene. 3. Reward Function: Multiple metrics including safety, traffic efficiency and passenger's comfort are considered in the reward function design:\n\n\u2022 safety evaluation r s : The vehicle should operate without collisions.\n\n\u2022 headway evaluation r d : The vehicle should maintain a safe distance from the preceding vehicles during driving to avoid collisions. \u2022 speed evaluation r v : Under the premise of ensuring safety, the vehicle is expected to drive at a high and stable speed.\n\n\u2022 driving comfort r c : Smooth acceleration and deceleration are expected to ensure safety and comfort. In addition, frequent lane changes should be avoided.\n\nAs such, a multi-objective reward r i,t at the time step t is defined as:\nr i,t = \u03c9 s r s + \u03c9 d r d + \u03c9 v r v \u2212 \u03c9 c r c ,(3)\nwhere \u03c9 s , \u03c9 d , \u03c9 v and \u03c9 c are the weighting coefficients. We set the safety factor \u03c9 s to a large value, because safety is the most important criterion during driving. The details of the four performance measurements are discussed next:\n\n(1) If there is no collision, the collision evaluation r s is set to 0, otherwise, r s is set as -1.\n\n(2) The headway evaluation is defined as\nr d = log d headway v t t d ,(4)\nwhere d headway is the distance to the preceding vehicle, and v t and t d are the current vehicle speed and time headway threshold, respectively. (3) The speed evaluation r v is defined as\nr v = min v t \u2212 v min v max \u2212 v min , 1 ,(5)\nwhere v t , v min and v max are the current, minimum, and maximum speeds of the ego vehicle, respectively. Within the specified speed range, higher speed is preferred to improve the driving efficiency. (4) The driving comfort r c is defined as\nr c = r a + r lc ,(6)\nwhere\nr a = \u22121, |a t | \u2265 a th 0, |a t | < a th\nis the penalty term of rapid acceleration and deceleration than a given threshold a th . Here a t presents the acceleration at time t. \n\n\nMA2C for AVs\n\nIn this paper, we extend the actor-critic network [27] to the multi-agent setting as a multi-agent actor-critic network (i.e., MA2C). MA2C improves the stability and scalability of the learning process by allowing certain communication among agents [28]. To take the advantage of homogeneous agents in the considered MARL setting, we assume all the agents share the same network structure and parameters, while they are still able to make different maneuvers according to different input states. The goal in cooperative MARL setting is to maximize the global reward of all the agents. To overcome the communication overhead and the credit assignment problem [29], we adopt the local reward design [23] as follows:\nr i,t = 1 | \u03bd i | j\u2208\u03bdi r j,t ,(7)\nwhere | \u03bd i | denotes the cardinality of a set containing the ego vehicle and its close neighbors. Compared to the global reward design previously used in [22,30], the designed local reward design mitigates the impact of remote agents. The backbone of the proposed MA2C network is shown in Fig. 2, in which states separated by physical units are first processed by separate 64-neuron fully connected (FC) layers. Then all hidden units are combined and fed into the 128-neuron FC layer. Then the shared actor-critic network will update the policy and value networks with the extracted features. As mentioned in [23], the adopted parameter sharing scheme [12] between the actor and value networks can greatly improve the learning efficiency.\n\nThe pseudo-code of the proposed MA2C algorithm is shown in Algorithm 1. The hyperparameters include: the (time)-discount factor \u03b3, the learning rate \u03b7, the politeness coefficient p and the epoch length T . Specifically, the agent receives the observation O i,t from the environment and updates the action by its policy (Line 3-6). After each episode is completed, the network parameters are updated accordingly (Line 9-11). If an episode is completed or a collision occurs, the \"DONE\" signal is released and the environment will be reset to its initial state to start a new epoch (Line 13-14).\n\n\nExperiments and Discussion\n\nIn this section, we evaluate the performance of the proposed MARL algorithm in terms of training efficiency, safety and driving comfort in the considered highway lane changing scenario shown in Fig. 1. Observe o i,t ;\n\n\n5:\n\nUpdate a i,t \u223c \u03c0 \u03b8i,t ; Update t = t + 1;\n\n\n8:\n\nif DONE then 9: for i \u2208 V do 10:\n\nUpdate \u03b8 i \u2190 \u03b8 i + \u03b7\u2207 \u03b8i J(\u03b8 i ); \n\n\nHDV Models\n\nIn this experiment, we assume that the longitudinal control of HDVs follows the Intelligent Driver Model (IDM) [31], which is a deterministic continuoustime model describing the dynamics of the position and speed of each vehicle. It takes into account the expected speed, distance between the vehicles and the behavior of the acceleration/deceleration process caused by the different driving habits. In addition, Minimize Overall Braking Induced By Lane Change model (MOBIL) [32] is adopted for the lateral control. It takes vehicle acceleration as the input variable of the model and can work well with most car-following models. The acceleration expression is defined as follows:\na n \u2265 \u2212b saf e ,(8)\nwhere\u00e3 n is the acceleration of the new follower after the lane change, and b saf e is the maximum braking imposed to the new follower. If the inequality in Eqn. 8 is satisfied, the ego vehicle is able to change lanes. The incentive condition is defined as:\na c \u2212 a c ego vehicle + p \u00e3 n \u2212 a n new follower +\u00e3 o \u2212 a o old follower \u2265 \u2206a th ,(9)\nwhere a and\u00e3 are the acceleration of the ego vehicle before and after the lane change, respectively, \u2206a th is the threshold that determines whether to trigger the lane change or not, and p is a politeness coefficient that controls how much effect we want to take into account for the followers, where p = 1 represents the most considerate drivers whose decision on change lanes may give way to the following blocked vehicles whereas p = 0 characterizes the most aggressive drivers where the HDV makes selfish lane-changing decisions by only considering their own speed gains and ignoring other vehicles. The performance evaluation of different p values is discussed in Section 4.3.3.\n\n\nExperimental Settings\n\nThe simulation environment is modified from the gym-based highway-env simulator [33]. We set the highway road length to 520 m, and the vehicles beyond the road are ignored. The vehicles are randomly spawned on the highway with different initial speeds 25 \u2212 30 m/s (56 mph \u2212 67 mph). The vehicle control sampling frequency is set as the default value of 5 Hz. The motions of HDVs follow the IDM and MOBIL model, where the maximum deceleration for safety purposes is limited by b saf e = \u22129 m/s 2 , politeness factor p is 0, and the lanechanging threshold \u2206a th is set as 0.1 m/s 2 . To evaluate the effectiveness of the proposed methods, three traffic density levels are employed, which correspond to low, middle, high levels of traffic congestion, respectively. The number of vehicles in different traffic modes is shown in Table 4.2. We train the MARL algorithms for 1 million steps (10,000 epochs) by applying two different random seeds and the same random seed is shared among agents. We evaluate each model 3 times every 200 training episodes. The parameters \u03b3 and learning rate \u03b7 are set as 0.99 and 5 \u00d7 10 \u22124 , respectively. The weighting coefficients in the reward function are set as \u03c9 s = 200, \u03c9 d = 4, \u03c9 v = 1 and \u03c9 c = 1, respectively. These experiments are conducted on a macOS server with a 2.7 GHz Intel Core i5 processor and 8GB of memory. Fig. 3 shows the performance comparison between the proposed local reward and the global reward design [22,30] (with shared actor-critic parameters). In all three traffic modes, the local reward design consistently outperforms the global reward design in terms of larger evaluation rewards and smaller variance. In addition, the performance gaps are enlarged as the number of vehicles increases. This is due to the fact that the global reward design is more likely to cause credit assignment issues as mentioned in [29].  Fig. 4 shows the performance comparison between strategies with or without sharing the actor-critic network parameters during training. Obviously, sharing an actor-critic network has better performance than without sharing. Specifically, sharing actor-critic parameters in all three modes results in higher rewards and lower variance. The reason is that, in separate actor-critic networks, the critic network can only guide the actor network to the correct training direction until the critic network is well-trained which may take a long time to achieve. In contrast, the actor network can benefit from the shared state representation via the critic network in a shared actor-critic network [23,34] \n\n\nResults & Analysis\n\n\nLocal v.s. Global Reward Designs\n\n\nSharing v.s. Separate Actor-critic Network\n\n\nVerification of Driving Comfort\n\nIn this subsection, we evaluate the effectiveness of the proposed multi-objective reward function with the driving comfort in Eqn. 3. Fig. 5 shows the acceleration and deceleration of the AV with or without the comfort measurement defined in Eqn. 6. It is clear that the proposed reward design with the comfort measurement has a low variance (average deviation: 0.237m/s 2 ) and is \n\n\nAdaptability of the Proposed Method\n\nIn this subsection, we evaluate the proposed MA2C under different HDV behaviors, which is controlled by the politeness coefficient p denoted in Eqn. 9, in which p = 0 means the most aggressive behavior while p = 1 represents the most polite behavior. Fig. 6 shows the training performance of two different HDV models (i.e., aggressive or politeness) under different traffic densities. It is clear that the proposed algorithm achieves scalable and stable performance whenever the HDVs take aggressive or courteous behaviors. \n\n\nComparison with the state-of-the-art benchmarks\n\nIn order to demonstrate the performance of the proposed MARL approach, we compared it with several state-of-the-art MARL methods: 1. Multi-agent Deep Q-Network (MADQN) [35]: This is the multi-agent version of Deep Q-Network (DQN) [25], which is an off-policy RL method by applying a deep neural network to approximate the value function and an experience replay buffer to break the correlations between samples to stabilize the training. 2. Multi-agent actor-critic using Kronecker-Factored Trust Region (MAACKTR): This is the multi-agent version of actor-critic using Kronecker-Factored Trust Region (ACKTR) [36], which is an onpolicy RL algorithm by optimizing both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region. 3. Multi-agent Proximal Policy Optimization (MAPPO) [37]: This is a multi-agent version of Proximal Policy Optimization (PPO) [38], which improves the trust region policy optimization (TRPO) [39] by using a clipped surrogate objective and adaptive KL penalty coefficient. 4. The Proposed MA2C : This is our proposed method with the designed multi-objective reward function design, parameter sharing and local reward design schemes. Table 2 shows the average return for the MARL algorithms during the evaluation. Obviously, the proposed MA2C algorithm shows the best performance under the density1 scenario than other MARL algorithms. It also shows promising results on the density2 and density3 scenarios and outperforms MAACKTR and MAPPO algorithms. Note that even though MADQN shows a better average reward than the MA2C algorithm, it shows larger reward deviations which may cause unstable training and safety issues. Similarly, the evaluation curves during the training process are shown in Fig. 7. As expected, the proposed MA2C algorithm outperforms other benchmarks in terms of evaluation reward and reward standard deviations.  \n\n\nPolicy Interpretation\n\nIn this subsection, we attempt to interpret the learned AVs' behavior. Fig. 8 shows the snapshots during testing at time steps 20, 28, and 40. As shown in Fig. 8(a), ego vehicle attempts to make a lane change to achieve a higher speed. To make a safe lane change, ego vehicle and ego vehicle are expected to work cooperatively. Specially, the ego vehicle should slow down to make space for the ego vehicle to avoid collisions, which is also represented in Fig. 9, where the ego vehicle starts to slow down at about 20-time steps. Then the ego vehicle begins to speed up to make the lane change as shown in Fig. 8(b) and Fig. 9. Meanwhile, the ego vehicle continues to slow down to ensure a safe headway distance with ego vehicle as shown in Fig. 9. Fig. 8(c) shows the completed lane changes, at which time the ego vehicle starts to speed up. This demonstration shows the proposed MARL framework learns a reasonable and cooperative policy for ego vehicles.\n\n\nConclusion\n\nIn this paper, we formulated the highway lane-changing problem in mixed traffic as an on-policy MARL problem, and extended the A2C into the multiagent setting, featuring a novel local reward design and parameter sharing   schemes. Specifically, a multi-objective reward function is proposed to simultaneously promote the driving efficiency, comfort, and safety of autonomous driving. Comprehensive experimental results, conducted on three different traffic densities under different levels of HDV aggressiveness, show that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety and driver comfort.\n\nFig. 1\n1Illustration of the considered lane-changing scenario (green: AVs, blue: HDVs, arrow curve: a possible trajectory of the ego vehicle AV1 to make the lane change).\n\n\nas the lane change penalty. Excessive lane changes can cause discomfort and safety issues. 4. Transition Probability: The transition probability T (s , | s, a) characterizes the transition from one state to another. Since our MARL algorithm is a model-free design, we do not assume any prior knowledge about transition probability.\n\nFig. 2\n2The architecture of the proposed MA2C network with shared actor-critic network design, where x and y are the longitudinal and lateral position of the observed vehicle relative to the ego vehicle, and vx and vy are the longitudinal and lateral speed of the observed vehicle relative to the ego vehicle. Algorithm 1 MARL for AVs. Parameter: \u03b3, \u03b7, p, T . Output: \u03b8. 1: Initialize o 0 , t \u2190 0.\n\n\n16:  until Stop condition is reached\n\nFig. 3\n3Performance comparisons between local and global reward designs. The shaded region denotes the standard deviation over 2 random seeds.\n\n\n.\n\nFig. 4 Fig. 5\n45Performance comparisons between with and without actor-critic network sharing. more smooth than the reward design without comfort term (average deviation: 0.582m/s 2 ), which shows the proposed reward design presents good driving comfort. Performance comparisons of acceleration between the reward design with or without comfort measurement.\n\nFig. 6\n6Performance comparisons on different politeness coefficients p under different traffic densities.\n\nFig. 7\n7Performance comparisons on accumulated rewards in MADQN, MA2C, MAACKTR, and MAPPO.\n\nFig. 8\n8Lane change in simulation environment (vehicles -: HDVs, vehicles -: AVs).\n\nFig. 9\n9Speeds of the AVs , and HDVs .\n\nTable 1\n1Traffic density modes.Traffic density modes AVs HDVs Explanation \n1 \n1-3 \n1-3 \nlow level \n2 \n2-4 \n2-4 \nmiddle level \n3 \n4-6 \n4-6 \nhigh level \n\n\n\nTable 2\n2Mean episode reward in different traffic flow scenario.Method \nDensity 1 \nDensity 2 \nDensity 3 \nMADQN \n47.451 \n51.568 \n48.509 \n(\u00b127.948) (\u00b132.943) (\u00b124.078) \nMA2C \n58.000 \n44.744 \n32.579 \n(\u00b19.308) \n(\u00b110.895) \n( \u00b18.160) \nMAACKTR \n8.812 \n3.759 \n4.892 \n(\u00b16.217) \n(\u00b110.858) (\u00b110.986) \nMAPPO \n31.988 \n19.300 \n5.073 \n(\u00b16.567) \n(\u00b116.097) (\u00b119.762) \n\nAutonomous Intelligent Systems conference on machine learning, pages 1889-1897. PMLR, 2015.\nAcknowledgments. The authors are grateful for the efforts of our colleagues in the Sino-German Center of Intelligent Systems, Tongji University. We are grateful for the suggestions on our manuscript from Dr. Qi Deng.Authors' Contribution. Wei Zhou, Dong Chen, Jun Yan, and Prof. Zhaojian Li participated in the framework design and manuscript writing, and Wei Zhou implemented experiments inspired by the encouragement and guidance of Dong Chen. Prof. Huilin Yin helped revise the manuscript. Prof. Wancheng Ge is the master supervisor of Wei Zhou in Tongji University which provides the opportunity to complete the research work. Availability of data and materials. Not applicable.Code availability. Not applicable.Competing interests. There are no conflicts of interest for this paper.\nA survey of motion planning and control techniques for selfdriving urban vehicles. Brian Paden, Michal C\u00e1p, Zheng Sze, Dmitry S Yong, Emilio Yershov, Frazzoli, IEEE Trans. Intell. Veh. 11Brian Paden, Michal C\u00e1p, Sze Zheng Yong, Dmitry S. Yershov, and Emilio Frazzoli. A survey of motion planning and control techniques for self- driving urban vehicles. IEEE Trans. Intell. Veh., 1(1):33-55, 2016.\n\nMinimizing the disruption of traffic flow of automated vehicles during lane changes. Divya Desiraju, Thidapat Chantem, Kevin Heaslip, IEEE Transactions on Intelligent Transportation Systems. 163Divya Desiraju, Thidapat Chantem, and Kevin Heaslip. Minimizing the disruption of traffic flow of automated vehicles during lane changes. IEEE Transactions on Intelligent Transportation Systems, 16(3):1249-1258, 2014.\n\nA cooperative lane change model for connected and automated vehicles. Tingting Li, Jianping Wu, Ching-Yao Chan, Mingyu Liu, Chunli Zhu, Weixin Lu, Kezhen Hu, IEEE Access. 8Tingting Li, Jianping Wu, Ching-Yao Chan, Mingyu Liu, Chunli Zhu, Weixin Lu, and Kezhen Hu. A cooperative lane change model for connected and automated vehicles. IEEE Access, 8:54940-54951, 2020.\n\nAutonomous driving using safe reinforcement learning by incorporating a regretbased human lane-changing decision model. Dong Chen, Longsheng Jiang, Yue Wang, Zhaojian Li, 2020 American Control Conference (ACC). IEEEDong Chen, Longsheng Jiang, Yue Wang, and Zhaojian Li. Autonomous driving using safe reinforcement learning by incorporating a regret- based human lane-changing decision model. In 2020 American Control Conference (ACC), pages 4355-4361. IEEE, 2020.\n\nContinuous control for automated lane change behavior based on deep deterministic policy gradient algorithm. Pin Wang, Hanhan Li, Ching-Yao Chan, 2019 IEEE Intelligent Vehicles Symposium (IV). IEEEPin Wang, Hanhan Li, and Ching-Yao Chan. Continuous control for auto- mated lane change behavior based on deep deterministic policy gradient algorithm. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 1454-1460. IEEE, 2019.\n\nEfficient motion planning for automated lane change based on imitation learning and mixed-integer optimization. Chenyang Xi, Tianyu Shi, Yuankai Wu, Lijun Sun, 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). IEEEChenyang Xi, Tianyu Shi, Yuankai Wu, and Lijun Sun. Efficient motion planning for automated lane change based on imitation learning and mixed-integer optimization. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1-6. IEEE, 2020.\n\nHarmonious lane changing via deep reinforcement learning. Guan Wang, Jianming Hu, Zhiheng Li, Li Li, IEEE Transactions on Intelligent Transportation Systems. Guan Wang, Jianming Hu, Zhiheng Li, and Li Li. Harmonious lane chang- ing via deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems, 2021.\n\nA cooperative control framework for cav lane change in a mixed traffic environment. Runjia Du, Sikai Chen, Yujie Li, Jiqian Dong, Paul Young Joun Ha, Samuel Labi, arXiv:2010.05439arXiv preprintRunjia Du, Sikai Chen, Yujie Li, Jiqian Dong, Paul Young Joun Ha, and Samuel Labi. A cooperative control framework for cav lane change in a mixed traffic environment. arXiv preprint arXiv:2010.05439, 2020.\n\nAutomated speed and lane change decision making using deep reinforcement learning. Carl-Johan Hoel, Krister Wolff, Leo Laine, 21st International Conference on Intelligent Transportation Systems. Wei-Bin Zhang, Alexandre M. Bayen, Javier J. S\u00e1nchez Medina, and Matthew J. BarthIEEECarl-Johan Hoel, Krister Wolff, and Leo Laine. Automated speed and lane change decision making using deep reinforcement learning. In Wei-Bin Zhang, Alexandre M. Bayen, Javier J. S\u00e1nchez Medina, and Matthew J. Barth, editors, 21st International Conference on Intelligent Transportation Systems, pages 2148-2155. IEEE, 2018.\n\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.\n\nMulti-agent deep reinforcement learning for large-scale traffic signal control. T Chu, J Wang, Lara Codeca, Z Li, IEEE Transactions on Intelligent Transportation Systems. T. Chu, J. Wang, Lara Codeca, and Z. Li. Multi-agent deep reinforce- ment learning for large-scale traffic signal control. IEEE Transactions on Intelligent Transportation Systems, pages 1-10, 2019.\n\nEfficient large-scale fleet management via multi-agent deep reinforcement learning. Kaixiang Lin, Renyu Zhao, Zhe Xu, Jiayu Zhou, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningKaixiang Lin, Renyu Zhao, Zhe Xu, and Jiayu Zhou. Efficient large-scale fleet management via multi-agent deep reinforcement learning. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1774-1783, 2018.\n\nSafe, multiagent, reinforcement learning for autonomous driving. Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua, arXiv:1610.03295arXiv preprintShai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi- agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.\n\nMulti-agent graph reinforcement learning for connected automated driving. Jiawei Wang, Tianyu Shi, Yuankai Wu, Luis Miranda-Moreno, Lijun Sun, Jiawei Wang, Tianyu Shi, Yuankai Wu, Luis Miranda-Moreno, and Lijun Sun. Multi-agent graph reinforcement learning for connected automated driving.\n\nLeveraging the capabilities of connected and autonomous vehicles and multi-agent reinforcement learning to mitigate highway bottleneck congestion. Paul Young Joun Ha, Sikai Chen, Jiqian Dong, Runjia Du, Yujie Li, Samuel Labi, arXiv:2010.05436arXiv preprintPaul Young Joun Ha, Sikai Chen, Jiqian Dong, Runjia Du, Yujie Li, and Samuel Labi. Leveraging the capabilities of connected and autonomous vehicles and multi-agent reinforcement learning to mitigate highway bottleneck congestion. arXiv preprint arXiv:2010.05436, 2020.\n\nMulti-agent connected autonomous driving using deep reinforcement learning. Praveen Palanisamy, 2020 International Joint Conference on Neural Networks (IJCNN). IEEEPraveen Palanisamy. Multi-agent connected autonomous driving using deep reinforcement learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-7. IEEE, 2020.\n\nGraph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles. Sikai Chen, Jiqian Dong, Paul Ha, Yujie Li, Samuel Labi, Computer-Aided Civil and Infrastructure Engineering. 367Sikai Chen, Jiqian Dong, Paul Ha, Yujie Li, and Samuel Labi. Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles. Computer-Aided Civil and Infrastructure Engineering, 36(7):838-857, 2021.\n\nLane change algorithm for autonomous vehicles via virtual curvature method. Man Lung Ho, Ping T Chan, Journal of advanced Transportation. 431Man Lung Ho, Ping T Chan, and AB Rad. Lane change algorithm for autonomous vehicles via virtual curvature method. Journal of advanced Transportation, 43(1):47-70, 2009.\n\nIf, when, and how to perform lane change maneuvers on highways. Julia Nilsson, Jonatan Silvlin, Mattias Brannstrom, Erik Coelingh, Jonas Fredriksson, IEEE Intelligent Transportation Systems Magazine. 84Julia Nilsson, Jonatan Silvlin, Mattias Brannstrom, Erik Coelingh, and Jonas Fredriksson. If, when, and how to perform lane change maneuvers on highways. IEEE Intelligent Transportation Systems Magazine, 8(4):68- 78, 2016.\n\nLane change maneuvers for automated vehicles. Julia Nilsson, Mattias Br\u00e4nnstr\u00f6m, Erik Coelingh, Jonas Fredriksson, IEEE Transactions on Intelligent Transportation Systems. 185Julia Nilsson, Mattias Br\u00e4nnstr\u00f6m, Erik Coelingh, and Jonas Fredriksson. Lane change maneuvers for automated vehicles. IEEE Transactions on Intelligent Transportation Systems, 18(5):1087-1096, 2016.\n\nAttention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. Yilun Chen, Chiyu Dong, Praveen Palanisamy, Priyantha Mudalige, Katharina Muelling, John M Dolan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsYilun Chen, Chiyu Dong, Praveen Palanisamy, Priyantha Mudalige, Katharina Muelling, and John M Dolan. Attention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driv- ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0-0, 2019.\n\nA drl-based multiagent cooperative control framework for cav networks: a graphic convolution q network. Jiqian Dong, Sikai Chen, Paul Young Joun Ha, Yujie Li, Samuel Labi, arXiv:2010.05437arXiv preprintJiqian Dong, Sikai Chen, Paul Young Joun Ha, Yujie Li, and Samuel Labi. A drl-based multiagent cooperative control framework for cav networks: a graphic convolution q network. arXiv preprint arXiv:2010.05437, 2020.\n\nDeep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic. Dong Chen, Zhaojian Li, Yongqiang Wang, Longsheng Jiang, Yue Wang, arXiv:2105.05701arXiv preprintDong Chen, Zhaojian Li, Yongqiang Wang, Longsheng Jiang, and Yue Wang. Deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic. arXiv preprint arXiv:2105.05701, 2021.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.02907arXiv preprintThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n\nPlaying atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A Riedmiller, abs/1312.5602CoRRVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.\n\nPartially observable markov decision processes. T J Matthijs, Spaan, Reinforcement Learning. SpringerMatthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement Learning, pages 387-414. Springer, 2012.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLRVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937. PMLR, 2016.\n\nMulti-agent reinforcement learning for networked system control. Tianshu Chu, Sandeep Chinchali, Sachin Katti, arXiv:2004.01339arXiv preprintTianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for networked system control. arXiv preprint arXiv:2004.01339, 2020.\n\nReinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nParameter sharing reinforcement learning architecture for multi agent driving behaviors. Meha Kaushik, Madhava Krishna, arXiv:1811.07214arXiv preprintMeha Kaushik, K Madhava Krishna, et al. Parameter sharing rein- forcement learning architecture for multi agent driving behaviors. arXiv preprint arXiv:1811.07214, 2018.\n\nCongested traffic states in empirical observations and microscopic simulations. M Treiber, A Hennecke, D Helbing, Physical Review E. 62M. Treiber, A. Hennecke, and D. Helbing. Congested traffic states in empirical observations and microscopic simulations. Physical Review E, 62:1805-1824, 2000.\n\nConnectivity statistics of store-and-forward intervehicle communication. Arne Kesting, Martin Treiber, Dirk Helbing, IEEE Trans. Intell. Transp. Syst. 111Arne Kesting, Martin Treiber, and Dirk Helbing. Connectivity statis- tics of store-and-forward intervehicle communication. IEEE Trans. Intell. Transp. Syst., 11(1):172-181, 2010.\n\nAn environment for autonomous driving decisionmaking. Edouard Leurent, Edouard Leurent. An environment for autonomous driving decision- making. https://github.com/eleurent/highway-env, 2018.\n\nFoundations of deep reinforcement learning: theory and practice in Python. Laura Graesser, Wah Loon Keng, Addison-Wesley ProfessionalLaura Graesser and Wah Loon Keng. Foundations of deep reinforcement learning: theory and practice in Python. Addison-Wesley Professional, 2019.\n\nTowards safe control of continuum manipulator using shielded multiagent reinforcement learning. Guanglin Ji, Junyan Yan, Jingxin Du, Wanquan Yan, Jibiao Chen, Yongkang Lu, Juan Rojas, Shing Shin Cheng, IEEE Robotics Autom. Lett. 64Guanglin Ji, Junyan Yan, Jingxin Du, Wanquan Yan, Jibiao Chen, Yongkang Lu, Juan Rojas, and Shing Shin Cheng. Towards safe control of continuum manipulator using shielded multiagent reinforcement learning. IEEE Robotics Autom. Lett., 6(4):7461-7468, 2021.\n\nScalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, Jimmy Ba, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. NYuhuai Wu, Elman Mansimov, Roger B. Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.\n\nRoman Vishwanathan, Garnett, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Vishwanathan, and Roman Garnett, editors, Advances in Neural Infor- mation Processing Systems 30: Annual Conference on Neural Information Processing Systems, pages 5279-5288, 2017.\n\nProximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.06347CoRRJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International\n", "annotations": {"author": "[{\"end\":275,\"start\":162},{\"end\":369,\"start\":276},{\"end\":372,\"start\":370},{\"end\":485,\"start\":373},{\"end\":564,\"start\":486},{\"end\":567,\"start\":565},{\"end\":707,\"start\":568},{\"end\":823,\"start\":708}]", "publisher": null, "author_last_name": "[{\"end\":170,\"start\":166},{\"end\":285,\"start\":281},{\"end\":380,\"start\":377},{\"end\":497,\"start\":495},{\"end\":578,\"start\":575},{\"end\":718,\"start\":716}]", "author_first_name": "[{\"end\":165,\"start\":162},{\"end\":280,\"start\":276},{\"end\":371,\"start\":370},{\"end\":376,\"start\":373},{\"end\":494,\"start\":486},{\"end\":566,\"start\":565},{\"end\":574,\"start\":568},{\"end\":715,\"start\":708}]", "author_affiliation": "[{\"end\":274,\"start\":172},{\"end\":368,\"start\":304},{\"end\":484,\"start\":382},{\"end\":563,\"start\":499},{\"end\":706,\"start\":604},{\"end\":822,\"start\":720}]", "title": "[{\"end\":159,\"start\":1},{\"end\":982,\"start\":824}]", "venue": null, "abstract": "[{\"end\":2846,\"start\":1127}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3105,\"start\":3102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3506,\"start\":3503},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3508,\"start\":3506},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4127,\"start\":4124},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4166,\"start\":4163},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4168,\"start\":4166},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4170,\"start\":4168},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4576,\"start\":4573},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4753,\"start\":4749},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4781,\"start\":4777},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4807,\"start\":4803},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4871,\"start\":4867},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4875,\"start\":4871},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4879,\"start\":4875},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4883,\"start\":4879},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5023,\"start\":5019},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5026,\"start\":5023},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7532,\"start\":7528},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7536,\"start\":7532},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7540,\"start\":7536},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7719,\"start\":7715},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7917,\"start\":7913},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8176,\"start\":8172},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8886,\"start\":8883},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8955,\"start\":8952},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9117,\"start\":9113},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9802,\"start\":9798},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9805,\"start\":9802},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9808,\"start\":9805},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9811,\"start\":9808},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9835,\"start\":9831},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10086,\"start\":10082},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10255,\"start\":10251},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10341,\"start\":10337},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10371,\"start\":10367},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12393,\"start\":12389},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13129,\"start\":13125},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13458,\"start\":13454},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13835,\"start\":13831},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14355,\"start\":14351},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17206,\"start\":17202},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17405,\"start\":17401},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17814,\"start\":17810},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17853,\"start\":17849},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18059,\"start\":18055},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18062,\"start\":18059},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18514,\"start\":18510},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18557,\"start\":18553},{\"end\":19552,\"start\":19550},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19735,\"start\":19731},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20099,\"start\":20095},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21459,\"start\":21455},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22837,\"start\":22833},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22840,\"start\":22837},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23249,\"start\":23245},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23948,\"start\":23944},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23951,\"start\":23948},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25259,\"start\":25255},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25321,\"start\":25317},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25700,\"start\":25696},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25910,\"start\":25906},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25984,\"start\":25980},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26049,\"start\":26045}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28821,\"start\":28650},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29155,\"start\":28822},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29554,\"start\":29156},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29593,\"start\":29555},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29737,\"start\":29594},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29741,\"start\":29738},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30100,\"start\":29742},{\"attributes\":{\"id\":\"fig_8\"},\"end\":30207,\"start\":30101},{\"attributes\":{\"id\":\"fig_9\"},\"end\":30299,\"start\":30208},{\"attributes\":{\"id\":\"fig_11\"},\"end\":30383,\"start\":30300},{\"attributes\":{\"id\":\"fig_12\"},\"end\":30423,\"start\":30384},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30577,\"start\":30424},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30930,\"start\":30578}]", "paragraph": "[{\"end\":3541,\"start\":2862},{\"end\":4577,\"start\":3543},{\"end\":5727,\"start\":4579},{\"end\":6196,\"start\":5729},{\"end\":7243,\"start\":6198},{\"end\":7450,\"start\":7260},{\"end\":8601,\"start\":7478},{\"end\":9683,\"start\":8625},{\"end\":10943,\"start\":9685},{\"end\":11422,\"start\":10945},{\"end\":11788,\"start\":11446},{\"end\":12521,\"start\":11810},{\"end\":13377,\"start\":12523},{\"end\":13606,\"start\":13425},{\"end\":13836,\"start\":13692},{\"end\":14238,\"start\":13862},{\"end\":14525,\"start\":14286},{\"end\":15415,\"start\":14605},{\"end\":15489,\"start\":15417},{\"end\":15749,\"start\":15491},{\"end\":15908,\"start\":15751},{\"end\":15983,\"start\":15910},{\"end\":16275,\"start\":16035},{\"end\":16377,\"start\":16277},{\"end\":16419,\"start\":16379},{\"end\":16641,\"start\":16453},{\"end\":16930,\"start\":16687},{\"end\":16958,\"start\":16953},{\"end\":17135,\"start\":17000},{\"end\":17865,\"start\":17152},{\"end\":18639,\"start\":17900},{\"end\":19234,\"start\":18641},{\"end\":19482,\"start\":19265},{\"end\":19530,\"start\":19489},{\"end\":19569,\"start\":19537},{\"end\":19605,\"start\":19571},{\"end\":20301,\"start\":19620},{\"end\":20579,\"start\":20322},{\"end\":21349,\"start\":20666},{\"end\":23952,\"start\":21375},{\"end\":24471,\"start\":24089},{\"end\":25035,\"start\":24511},{\"end\":26990,\"start\":25087},{\"end\":27972,\"start\":27016},{\"end\":28649,\"start\":27987}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13424,\"start\":13378},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13642,\"start\":13607},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13691,\"start\":13642},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14285,\"start\":14239},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14604,\"start\":14526},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16034,\"start\":15984},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16452,\"start\":16420},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16686,\"start\":16642},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16952,\"start\":16931},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16999,\"start\":16959},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17899,\"start\":17866},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20321,\"start\":20302},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20665,\"start\":20580}]", "table_ref": "[{\"end\":22206,\"start\":22199},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26293,\"start\":26286}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2860,\"start\":2848},{\"attributes\":{\"n\":\"2\"},\"end\":7258,\"start\":7246},{\"attributes\":{\"n\":\"2.1\"},\"end\":7476,\"start\":7453},{\"attributes\":{\"n\":\"2.2\"},\"end\":8623,\"start\":8604},{\"attributes\":{\"n\":\"3\"},\"end\":11444,\"start\":11425},{\"attributes\":{\"n\":\"3.1\"},\"end\":11808,\"start\":11791},{\"attributes\":{\"n\":\"3.2\"},\"end\":13860,\"start\":13839},{\"attributes\":{\"n\":\"3.3\"},\"end\":17150,\"start\":17138},{\"attributes\":{\"n\":\"4\"},\"end\":19263,\"start\":19237},{\"end\":19487,\"start\":19485},{\"end\":19535,\"start\":19533},{\"attributes\":{\"n\":\"4.1\"},\"end\":19618,\"start\":19608},{\"attributes\":{\"n\":\"4.2\"},\"end\":21373,\"start\":21352},{\"attributes\":{\"n\":\"4.3\"},\"end\":23973,\"start\":23955},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":24008,\"start\":23976},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":24053,\"start\":24011},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":24087,\"start\":24056},{\"attributes\":{\"n\":\"4.3.4\"},\"end\":24509,\"start\":24474},{\"attributes\":{\"n\":\"4.3.5\"},\"end\":25085,\"start\":25038},{\"attributes\":{\"n\":\"4.3.6\"},\"end\":27014,\"start\":26993},{\"attributes\":{\"n\":\"5\"},\"end\":27985,\"start\":27975},{\"end\":28657,\"start\":28651},{\"end\":29163,\"start\":29157},{\"end\":29601,\"start\":29595},{\"end\":29756,\"start\":29743},{\"end\":30108,\"start\":30102},{\"end\":30215,\"start\":30209},{\"end\":30307,\"start\":30301},{\"end\":30391,\"start\":30385},{\"end\":30432,\"start\":30425},{\"end\":30586,\"start\":30579}]", "table": "[{\"end\":30577,\"start\":30456},{\"end\":30930,\"start\":30643}]", "figure_caption": "[{\"end\":28821,\"start\":28659},{\"end\":29155,\"start\":28824},{\"end\":29554,\"start\":29165},{\"end\":29593,\"start\":29557},{\"end\":29737,\"start\":29603},{\"end\":29741,\"start\":29740},{\"end\":30100,\"start\":29759},{\"end\":30207,\"start\":30110},{\"end\":30299,\"start\":30217},{\"end\":30383,\"start\":30309},{\"end\":30423,\"start\":30393},{\"end\":30456,\"start\":30434},{\"end\":30643,\"start\":30588}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3605,\"start\":3599},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18196,\"start\":18190},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19465,\"start\":19459},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22736,\"start\":22730},{\"end\":23258,\"start\":23252},{\"end\":24229,\"start\":24223},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24768,\"start\":24762},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":26855,\"start\":26849},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":27093,\"start\":27087},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":27180,\"start\":27171},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":27478,\"start\":27472},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":27631,\"start\":27622},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":27642,\"start\":27636},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":27763,\"start\":27757},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":27774,\"start\":27765}]", "bib_author_first_name": "[{\"end\":31899,\"start\":31894},{\"end\":31913,\"start\":31907},{\"end\":31924,\"start\":31919},{\"end\":31936,\"start\":31930},{\"end\":31938,\"start\":31937},{\"end\":31951,\"start\":31945},{\"end\":32299,\"start\":32294},{\"end\":32318,\"start\":32310},{\"end\":32333,\"start\":32328},{\"end\":32700,\"start\":32692},{\"end\":32713,\"start\":32705},{\"end\":32727,\"start\":32718},{\"end\":32740,\"start\":32734},{\"end\":32752,\"start\":32746},{\"end\":32764,\"start\":32758},{\"end\":32775,\"start\":32769},{\"end\":33115,\"start\":33111},{\"end\":33131,\"start\":33122},{\"end\":33142,\"start\":33139},{\"end\":33157,\"start\":33149},{\"end\":33568,\"start\":33565},{\"end\":33581,\"start\":33575},{\"end\":33595,\"start\":33586},{\"end\":34005,\"start\":33997},{\"end\":34016,\"start\":34010},{\"end\":34029,\"start\":34022},{\"end\":34039,\"start\":34034},{\"end\":34474,\"start\":34470},{\"end\":34489,\"start\":34481},{\"end\":34501,\"start\":34494},{\"end\":34508,\"start\":34506},{\"end\":34831,\"start\":34825},{\"end\":34841,\"start\":34836},{\"end\":34853,\"start\":34848},{\"end\":34864,\"start\":34858},{\"end\":34886,\"start\":34871},{\"end\":34897,\"start\":34891},{\"end\":35234,\"start\":35224},{\"end\":35248,\"start\":35241},{\"end\":35259,\"start\":35256},{\"end\":35826,\"start\":35821},{\"end\":35840,\"start\":35836},{\"end\":35861,\"start\":35853},{\"end\":35863,\"start\":35862},{\"end\":35882,\"start\":35875},{\"end\":35898,\"start\":35892},{\"end\":35915,\"start\":35907},{\"end\":35924,\"start\":35923},{\"end\":35939,\"start\":35932},{\"end\":35950,\"start\":35946},{\"end\":35964,\"start\":35959},{\"end\":36357,\"start\":36356},{\"end\":36364,\"start\":36363},{\"end\":36375,\"start\":36371},{\"end\":36385,\"start\":36384},{\"end\":36738,\"start\":36730},{\"end\":36749,\"start\":36744},{\"end\":36759,\"start\":36756},{\"end\":36769,\"start\":36764},{\"end\":37285,\"start\":37281},{\"end\":37308,\"start\":37302},{\"end\":37323,\"start\":37318},{\"end\":37606,\"start\":37600},{\"end\":37619,\"start\":37613},{\"end\":37632,\"start\":37625},{\"end\":37641,\"start\":37637},{\"end\":37663,\"start\":37658},{\"end\":37979,\"start\":37964},{\"end\":37989,\"start\":37984},{\"end\":38002,\"start\":37996},{\"end\":38015,\"start\":38009},{\"end\":38025,\"start\":38020},{\"end\":38036,\"start\":38030},{\"end\":38426,\"start\":38419},{\"end\":38817,\"start\":38812},{\"end\":38830,\"start\":38824},{\"end\":38841,\"start\":38837},{\"end\":38851,\"start\":38846},{\"end\":38862,\"start\":38856},{\"end\":39258,\"start\":39255},{\"end\":39263,\"start\":39259},{\"end\":39272,\"start\":39268},{\"end\":39274,\"start\":39273},{\"end\":39559,\"start\":39554},{\"end\":39576,\"start\":39569},{\"end\":39593,\"start\":39586},{\"end\":39610,\"start\":39606},{\"end\":39626,\"start\":39621},{\"end\":39967,\"start\":39962},{\"end\":39984,\"start\":39977},{\"end\":40001,\"start\":39997},{\"end\":40017,\"start\":40012},{\"end\":40402,\"start\":40397},{\"end\":40414,\"start\":40409},{\"end\":40428,\"start\":40421},{\"end\":40450,\"start\":40441},{\"end\":40470,\"start\":40461},{\"end\":40487,\"start\":40481},{\"end\":41098,\"start\":41092},{\"end\":41110,\"start\":41105},{\"end\":41132,\"start\":41117},{\"end\":41142,\"start\":41137},{\"end\":41153,\"start\":41147},{\"end\":41496,\"start\":41492},{\"end\":41511,\"start\":41503},{\"end\":41525,\"start\":41516},{\"end\":41541,\"start\":41532},{\"end\":41552,\"start\":41549},{\"end\":41853,\"start\":41852},{\"end\":41865,\"start\":41862},{\"end\":42105,\"start\":42096},{\"end\":42117,\"start\":42112},{\"end\":42136,\"start\":42131},{\"end\":42149,\"start\":42145},{\"end\":42165,\"start\":42158},{\"end\":42182,\"start\":42178},{\"end\":42199,\"start\":42193},{\"end\":42201,\"start\":42200},{\"end\":42479,\"start\":42478},{\"end\":42481,\"start\":42480},{\"end\":42720,\"start\":42711},{\"end\":42732,\"start\":42727},{\"end\":42745,\"start\":42733},{\"end\":42758,\"start\":42753},{\"end\":42770,\"start\":42766},{\"end\":42786,\"start\":42779},{\"end\":42801,\"start\":42798},{\"end\":42815,\"start\":42810},{\"end\":42829,\"start\":42824},{\"end\":43234,\"start\":43227},{\"end\":43247,\"start\":43240},{\"end\":43265,\"start\":43259},{\"end\":43500,\"start\":43499},{\"end\":43516,\"start\":43510},{\"end\":43518,\"start\":43517},{\"end\":43732,\"start\":43728},{\"end\":44041,\"start\":44040},{\"end\":44052,\"start\":44051},{\"end\":44064,\"start\":44063},{\"end\":44333,\"start\":44329},{\"end\":44349,\"start\":44343},{\"end\":44363,\"start\":44359},{\"end\":44651,\"start\":44644},{\"end\":44862,\"start\":44857},{\"end\":45164,\"start\":45156},{\"end\":45175,\"start\":45169},{\"end\":45188,\"start\":45181},{\"end\":45200,\"start\":45193},{\"end\":45212,\"start\":45206},{\"end\":45227,\"start\":45219},{\"end\":45236,\"start\":45232},{\"end\":45254,\"start\":45244},{\"end\":45655,\"start\":45649},{\"end\":45665,\"start\":45660},{\"end\":45681,\"start\":45676},{\"end\":45683,\"start\":45682},{\"end\":45696,\"start\":45692},{\"end\":45708,\"start\":45703},{\"end\":46066,\"start\":46061},{\"end\":46431,\"start\":46427},{\"end\":46447,\"start\":46442},{\"end\":46464,\"start\":46456},{\"end\":46479,\"start\":46475},{\"end\":46493,\"start\":46489},{\"end\":46673,\"start\":46669},{\"end\":46689,\"start\":46684},{\"end\":46706,\"start\":46698},{\"end\":46721,\"start\":46717},{\"end\":46735,\"start\":46731},{\"end\":47013,\"start\":47009},{\"end\":47030,\"start\":47024},{\"end\":47045,\"start\":47039},{\"end\":47061,\"start\":47054},{\"end\":47077,\"start\":47070}]", "bib_author_last_name": "[{\"end\":31905,\"start\":31900},{\"end\":31917,\"start\":31914},{\"end\":31928,\"start\":31925},{\"end\":31943,\"start\":31939},{\"end\":31959,\"start\":31952},{\"end\":31969,\"start\":31961},{\"end\":32308,\"start\":32300},{\"end\":32326,\"start\":32319},{\"end\":32341,\"start\":32334},{\"end\":32703,\"start\":32701},{\"end\":32716,\"start\":32714},{\"end\":32732,\"start\":32728},{\"end\":32744,\"start\":32741},{\"end\":32756,\"start\":32753},{\"end\":32767,\"start\":32765},{\"end\":32778,\"start\":32776},{\"end\":33120,\"start\":33116},{\"end\":33137,\"start\":33132},{\"end\":33147,\"start\":33143},{\"end\":33160,\"start\":33158},{\"end\":33573,\"start\":33569},{\"end\":33584,\"start\":33582},{\"end\":33600,\"start\":33596},{\"end\":34008,\"start\":34006},{\"end\":34020,\"start\":34017},{\"end\":34032,\"start\":34030},{\"end\":34043,\"start\":34040},{\"end\":34479,\"start\":34475},{\"end\":34492,\"start\":34490},{\"end\":34504,\"start\":34502},{\"end\":34511,\"start\":34509},{\"end\":34834,\"start\":34832},{\"end\":34846,\"start\":34842},{\"end\":34856,\"start\":34854},{\"end\":34869,\"start\":34865},{\"end\":34889,\"start\":34887},{\"end\":34902,\"start\":34898},{\"end\":35239,\"start\":35235},{\"end\":35254,\"start\":35249},{\"end\":35265,\"start\":35260},{\"end\":35834,\"start\":35827},{\"end\":35851,\"start\":35841},{\"end\":35873,\"start\":35864},{\"end\":35890,\"start\":35883},{\"end\":35905,\"start\":35899},{\"end\":35921,\"start\":35916},{\"end\":35930,\"start\":35925},{\"end\":35944,\"start\":35940},{\"end\":35957,\"start\":35951},{\"end\":35971,\"start\":35965},{\"end\":35981,\"start\":35973},{\"end\":36361,\"start\":36358},{\"end\":36369,\"start\":36365},{\"end\":36382,\"start\":36376},{\"end\":36388,\"start\":36386},{\"end\":36742,\"start\":36739},{\"end\":36754,\"start\":36750},{\"end\":36762,\"start\":36760},{\"end\":36774,\"start\":36770},{\"end\":37300,\"start\":37286},{\"end\":37316,\"start\":37309},{\"end\":37331,\"start\":37324},{\"end\":37611,\"start\":37607},{\"end\":37623,\"start\":37620},{\"end\":37635,\"start\":37633},{\"end\":37656,\"start\":37642},{\"end\":37667,\"start\":37664},{\"end\":37982,\"start\":37980},{\"end\":37994,\"start\":37990},{\"end\":38007,\"start\":38003},{\"end\":38018,\"start\":38016},{\"end\":38028,\"start\":38026},{\"end\":38041,\"start\":38037},{\"end\":38437,\"start\":38427},{\"end\":38822,\"start\":38818},{\"end\":38835,\"start\":38831},{\"end\":38844,\"start\":38842},{\"end\":38854,\"start\":38852},{\"end\":38867,\"start\":38863},{\"end\":39266,\"start\":39264},{\"end\":39279,\"start\":39275},{\"end\":39567,\"start\":39560},{\"end\":39584,\"start\":39577},{\"end\":39604,\"start\":39594},{\"end\":39619,\"start\":39611},{\"end\":39638,\"start\":39627},{\"end\":39975,\"start\":39968},{\"end\":39995,\"start\":39985},{\"end\":40010,\"start\":40002},{\"end\":40029,\"start\":40018},{\"end\":40407,\"start\":40403},{\"end\":40419,\"start\":40415},{\"end\":40439,\"start\":40429},{\"end\":40459,\"start\":40451},{\"end\":40479,\"start\":40471},{\"end\":40493,\"start\":40488},{\"end\":41103,\"start\":41099},{\"end\":41115,\"start\":41111},{\"end\":41135,\"start\":41133},{\"end\":41145,\"start\":41143},{\"end\":41158,\"start\":41154},{\"end\":41501,\"start\":41497},{\"end\":41514,\"start\":41512},{\"end\":41530,\"start\":41526},{\"end\":41547,\"start\":41542},{\"end\":41557,\"start\":41553},{\"end\":41860,\"start\":41854},{\"end\":41870,\"start\":41866},{\"end\":41879,\"start\":41872},{\"end\":42110,\"start\":42106},{\"end\":42129,\"start\":42118},{\"end\":42143,\"start\":42137},{\"end\":42156,\"start\":42150},{\"end\":42176,\"start\":42166},{\"end\":42191,\"start\":42183},{\"end\":42212,\"start\":42202},{\"end\":42490,\"start\":42482},{\"end\":42497,\"start\":42492},{\"end\":42725,\"start\":42721},{\"end\":42751,\"start\":42746},{\"end\":42764,\"start\":42759},{\"end\":42777,\"start\":42771},{\"end\":42796,\"start\":42787},{\"end\":42808,\"start\":42802},{\"end\":42822,\"start\":42816},{\"end\":42841,\"start\":42830},{\"end\":43238,\"start\":43235},{\"end\":43257,\"start\":43248},{\"end\":43271,\"start\":43266},{\"end\":43508,\"start\":43501},{\"end\":43525,\"start\":43519},{\"end\":43532,\"start\":43527},{\"end\":43740,\"start\":43733},{\"end\":43757,\"start\":43742},{\"end\":44049,\"start\":44042},{\"end\":44061,\"start\":44053},{\"end\":44072,\"start\":44065},{\"end\":44341,\"start\":44334},{\"end\":44357,\"start\":44350},{\"end\":44371,\"start\":44364},{\"end\":44659,\"start\":44652},{\"end\":44871,\"start\":44863},{\"end\":44886,\"start\":44873},{\"end\":45167,\"start\":45165},{\"end\":45179,\"start\":45176},{\"end\":45191,\"start\":45189},{\"end\":45204,\"start\":45201},{\"end\":45217,\"start\":45213},{\"end\":45230,\"start\":45228},{\"end\":45242,\"start\":45237},{\"end\":45260,\"start\":45255},{\"end\":45658,\"start\":45656},{\"end\":45674,\"start\":45666},{\"end\":45690,\"start\":45684},{\"end\":45701,\"start\":45697},{\"end\":45711,\"start\":45709},{\"end\":46079,\"start\":46067},{\"end\":46088,\"start\":46081},{\"end\":46440,\"start\":46432},{\"end\":46454,\"start\":46448},{\"end\":46473,\"start\":46465},{\"end\":46487,\"start\":46480},{\"end\":46500,\"start\":46494},{\"end\":46682,\"start\":46674},{\"end\":46696,\"start\":46690},{\"end\":46715,\"start\":46707},{\"end\":46729,\"start\":46722},{\"end\":46742,\"start\":46736},{\"end\":47022,\"start\":47014},{\"end\":47037,\"start\":47031},{\"end\":47052,\"start\":47046},{\"end\":47068,\"start\":47062},{\"end\":47084,\"start\":47078}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1229096},\"end\":32207,\"start\":31811},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16412132},\"end\":32620,\"start\":32209},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":214692574},\"end\":32989,\"start\":32622},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":204401771},\"end\":33454,\"start\":32991},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":174802349},\"end\":33883,\"start\":33456},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":212634186},\"end\":34410,\"start\":33885},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":234243731},\"end\":34739,\"start\":34412},{\"attributes\":{\"doi\":\"arXiv:2010.05439\",\"id\":\"b7\"},\"end\":35139,\"start\":34741},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4311258},\"end\":35743,\"start\":35141},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":204972004},\"end\":36274,\"start\":35745},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":75136065},\"end\":36644,\"start\":36276},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3334421},\"end\":37214,\"start\":36646},{\"attributes\":{\"doi\":\"arXiv:1610.03295\",\"id\":\"b12\"},\"end\":37524,\"start\":37216},{\"attributes\":{\"id\":\"b13\"},\"end\":37815,\"start\":37526},{\"attributes\":{\"doi\":\"arXiv:2010.05436\",\"id\":\"b14\"},\"end\":38341,\"start\":37817},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207852404},\"end\":38692,\"start\":38343},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235474935},\"end\":39177,\"start\":38694},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":109223633},\"end\":39488,\"start\":39179},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16011689},\"end\":39914,\"start\":39490},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":21859902},\"end\":40289,\"start\":39916},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":198119613},\"end\":40986,\"start\":40291},{\"attributes\":{\"doi\":\"arXiv:2010.05437\",\"id\":\"b21\"},\"end\":41404,\"start\":40988},{\"attributes\":{\"doi\":\"arXiv:2105.05701\",\"id\":\"b22\"},\"end\":41784,\"start\":41406},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b23\"},\"end\":42046,\"start\":41786},{\"attributes\":{\"doi\":\"abs/1312.5602\",\"id\":\"b24\"},\"end\":42428,\"start\":42048},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1502764},\"end\":42655,\"start\":42430},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6875312},\"end\":43160,\"start\":42657},{\"attributes\":{\"doi\":\"arXiv:2004.01339\",\"id\":\"b27\"},\"end\":43456,\"start\":43162},{\"attributes\":{\"id\":\"b28\"},\"end\":43637,\"start\":43458},{\"attributes\":{\"doi\":\"arXiv:1811.07214\",\"id\":\"b29\"},\"end\":43958,\"start\":43639},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1100293},\"end\":44254,\"start\":43960},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14986359},\"end\":44588,\"start\":44256},{\"attributes\":{\"id\":\"b32\"},\"end\":44780,\"start\":44590},{\"attributes\":{\"id\":\"b33\"},\"end\":45058,\"start\":44782},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":235435866},\"end\":45546,\"start\":45060},{\"attributes\":{\"id\":\"b35\"},\"end\":46059,\"start\":45548},{\"attributes\":{\"id\":\"b36\"},\"end\":46384,\"start\":46061},{\"attributes\":{\"doi\":\"abs/1707.06347\",\"id\":\"b37\"},\"end\":46667,\"start\":46386},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b38\"},\"end\":46973,\"start\":46669},{\"attributes\":{\"id\":\"b39\"},\"end\":47217,\"start\":46975}]", "bib_title": "[{\"end\":31892,\"start\":31811},{\"end\":32292,\"start\":32209},{\"end\":32690,\"start\":32622},{\"end\":33109,\"start\":32991},{\"end\":33563,\"start\":33456},{\"end\":33995,\"start\":33885},{\"end\":34468,\"start\":34412},{\"end\":35222,\"start\":35141},{\"end\":35819,\"start\":35745},{\"end\":36354,\"start\":36276},{\"end\":36728,\"start\":36646},{\"end\":38417,\"start\":38343},{\"end\":38810,\"start\":38694},{\"end\":39253,\"start\":39179},{\"end\":39552,\"start\":39490},{\"end\":39960,\"start\":39916},{\"end\":40395,\"start\":40291},{\"end\":42476,\"start\":42430},{\"end\":42709,\"start\":42657},{\"end\":44038,\"start\":43960},{\"end\":44327,\"start\":44256},{\"end\":45154,\"start\":45060}]", "bib_author": "[{\"end\":31907,\"start\":31894},{\"end\":31919,\"start\":31907},{\"end\":31930,\"start\":31919},{\"end\":31945,\"start\":31930},{\"end\":31961,\"start\":31945},{\"end\":31971,\"start\":31961},{\"end\":32310,\"start\":32294},{\"end\":32328,\"start\":32310},{\"end\":32343,\"start\":32328},{\"end\":32705,\"start\":32692},{\"end\":32718,\"start\":32705},{\"end\":32734,\"start\":32718},{\"end\":32746,\"start\":32734},{\"end\":32758,\"start\":32746},{\"end\":32769,\"start\":32758},{\"end\":32780,\"start\":32769},{\"end\":33122,\"start\":33111},{\"end\":33139,\"start\":33122},{\"end\":33149,\"start\":33139},{\"end\":33162,\"start\":33149},{\"end\":33575,\"start\":33565},{\"end\":33586,\"start\":33575},{\"end\":33602,\"start\":33586},{\"end\":34010,\"start\":33997},{\"end\":34022,\"start\":34010},{\"end\":34034,\"start\":34022},{\"end\":34045,\"start\":34034},{\"end\":34481,\"start\":34470},{\"end\":34494,\"start\":34481},{\"end\":34506,\"start\":34494},{\"end\":34513,\"start\":34506},{\"end\":34836,\"start\":34825},{\"end\":34848,\"start\":34836},{\"end\":34858,\"start\":34848},{\"end\":34871,\"start\":34858},{\"end\":34891,\"start\":34871},{\"end\":34904,\"start\":34891},{\"end\":35241,\"start\":35224},{\"end\":35256,\"start\":35241},{\"end\":35267,\"start\":35256},{\"end\":35836,\"start\":35821},{\"end\":35853,\"start\":35836},{\"end\":35875,\"start\":35853},{\"end\":35892,\"start\":35875},{\"end\":35907,\"start\":35892},{\"end\":35923,\"start\":35907},{\"end\":35932,\"start\":35923},{\"end\":35946,\"start\":35932},{\"end\":35959,\"start\":35946},{\"end\":35973,\"start\":35959},{\"end\":35983,\"start\":35973},{\"end\":36363,\"start\":36356},{\"end\":36371,\"start\":36363},{\"end\":36384,\"start\":36371},{\"end\":36390,\"start\":36384},{\"end\":36744,\"start\":36730},{\"end\":36756,\"start\":36744},{\"end\":36764,\"start\":36756},{\"end\":36776,\"start\":36764},{\"end\":37302,\"start\":37281},{\"end\":37318,\"start\":37302},{\"end\":37333,\"start\":37318},{\"end\":37613,\"start\":37600},{\"end\":37625,\"start\":37613},{\"end\":37637,\"start\":37625},{\"end\":37658,\"start\":37637},{\"end\":37669,\"start\":37658},{\"end\":37984,\"start\":37964},{\"end\":37996,\"start\":37984},{\"end\":38009,\"start\":37996},{\"end\":38020,\"start\":38009},{\"end\":38030,\"start\":38020},{\"end\":38043,\"start\":38030},{\"end\":38439,\"start\":38419},{\"end\":38824,\"start\":38812},{\"end\":38837,\"start\":38824},{\"end\":38846,\"start\":38837},{\"end\":38856,\"start\":38846},{\"end\":38869,\"start\":38856},{\"end\":39268,\"start\":39255},{\"end\":39281,\"start\":39268},{\"end\":39569,\"start\":39554},{\"end\":39586,\"start\":39569},{\"end\":39606,\"start\":39586},{\"end\":39621,\"start\":39606},{\"end\":39640,\"start\":39621},{\"end\":39977,\"start\":39962},{\"end\":39997,\"start\":39977},{\"end\":40012,\"start\":39997},{\"end\":40031,\"start\":40012},{\"end\":40409,\"start\":40397},{\"end\":40421,\"start\":40409},{\"end\":40441,\"start\":40421},{\"end\":40461,\"start\":40441},{\"end\":40481,\"start\":40461},{\"end\":40495,\"start\":40481},{\"end\":41105,\"start\":41092},{\"end\":41117,\"start\":41105},{\"end\":41137,\"start\":41117},{\"end\":41147,\"start\":41137},{\"end\":41160,\"start\":41147},{\"end\":41503,\"start\":41492},{\"end\":41516,\"start\":41503},{\"end\":41532,\"start\":41516},{\"end\":41549,\"start\":41532},{\"end\":41559,\"start\":41549},{\"end\":41862,\"start\":41852},{\"end\":41872,\"start\":41862},{\"end\":41881,\"start\":41872},{\"end\":42112,\"start\":42096},{\"end\":42131,\"start\":42112},{\"end\":42145,\"start\":42131},{\"end\":42158,\"start\":42145},{\"end\":42178,\"start\":42158},{\"end\":42193,\"start\":42178},{\"end\":42214,\"start\":42193},{\"end\":42492,\"start\":42478},{\"end\":42499,\"start\":42492},{\"end\":42727,\"start\":42711},{\"end\":42753,\"start\":42727},{\"end\":42766,\"start\":42753},{\"end\":42779,\"start\":42766},{\"end\":42798,\"start\":42779},{\"end\":42810,\"start\":42798},{\"end\":42824,\"start\":42810},{\"end\":42843,\"start\":42824},{\"end\":43240,\"start\":43227},{\"end\":43259,\"start\":43240},{\"end\":43273,\"start\":43259},{\"end\":43510,\"start\":43499},{\"end\":43527,\"start\":43510},{\"end\":43534,\"start\":43527},{\"end\":43742,\"start\":43728},{\"end\":43759,\"start\":43742},{\"end\":44051,\"start\":44040},{\"end\":44063,\"start\":44051},{\"end\":44074,\"start\":44063},{\"end\":44343,\"start\":44329},{\"end\":44359,\"start\":44343},{\"end\":44373,\"start\":44359},{\"end\":44661,\"start\":44644},{\"end\":44873,\"start\":44857},{\"end\":44888,\"start\":44873},{\"end\":45169,\"start\":45156},{\"end\":45181,\"start\":45169},{\"end\":45193,\"start\":45181},{\"end\":45206,\"start\":45193},{\"end\":45219,\"start\":45206},{\"end\":45232,\"start\":45219},{\"end\":45244,\"start\":45232},{\"end\":45262,\"start\":45244},{\"end\":45660,\"start\":45649},{\"end\":45676,\"start\":45660},{\"end\":45692,\"start\":45676},{\"end\":45703,\"start\":45692},{\"end\":45713,\"start\":45703},{\"end\":46081,\"start\":46061},{\"end\":46090,\"start\":46081},{\"end\":46442,\"start\":46427},{\"end\":46456,\"start\":46442},{\"end\":46475,\"start\":46456},{\"end\":46489,\"start\":46475},{\"end\":46502,\"start\":46489},{\"end\":46684,\"start\":46669},{\"end\":46698,\"start\":46684},{\"end\":46717,\"start\":46698},{\"end\":46731,\"start\":46717},{\"end\":46744,\"start\":46731},{\"end\":47024,\"start\":47009},{\"end\":47039,\"start\":47024},{\"end\":47054,\"start\":47039},{\"end\":47070,\"start\":47054},{\"end\":47086,\"start\":47070}]", "bib_venue": "[{\"end\":31994,\"start\":31971},{\"end\":32398,\"start\":32343},{\"end\":32791,\"start\":32780},{\"end\":33200,\"start\":33162},{\"end\":33647,\"start\":33602},{\"end\":34129,\"start\":34045},{\"end\":34568,\"start\":34513},{\"end\":34823,\"start\":34741},{\"end\":35334,\"start\":35267},{\"end\":35989,\"start\":35983},{\"end\":36445,\"start\":36390},{\"end\":36872,\"start\":36776},{\"end\":37279,\"start\":37216},{\"end\":37598,\"start\":37526},{\"end\":37962,\"start\":37817},{\"end\":38501,\"start\":38439},{\"end\":38920,\"start\":38869},{\"end\":39315,\"start\":39281},{\"end\":39688,\"start\":39640},{\"end\":40086,\"start\":40031},{\"end\":40586,\"start\":40495},{\"end\":41090,\"start\":40988},{\"end\":41490,\"start\":41406},{\"end\":41850,\"start\":41786},{\"end\":42094,\"start\":42048},{\"end\":42521,\"start\":42499},{\"end\":42887,\"start\":42843},{\"end\":43225,\"start\":43162},{\"end\":43497,\"start\":43458},{\"end\":43726,\"start\":43639},{\"end\":44091,\"start\":44074},{\"end\":44405,\"start\":44373},{\"end\":44642,\"start\":44590},{\"end\":44855,\"start\":44782},{\"end\":45287,\"start\":45262},{\"end\":45647,\"start\":45548},{\"end\":46202,\"start\":46090},{\"end\":46425,\"start\":46386},{\"end\":46799,\"start\":46760},{\"end\":47007,\"start\":46975},{\"end\":36955,\"start\":36874},{\"end\":40664,\"start\":40588}]"}}}, "year": 2023, "month": 12, "day": 17}