{"id": 246826180, "updated": "2023-07-16 20:59:07.918", "metadata": {"title": "DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos", "authors": "[{\"first\":\"Yu\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Xizi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Mingze\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Zelin\",\"last\":\"Pu\",\"middle\":[]},{\"first\":\"Yuchen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Ella\",\"last\":\"Atkins\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Crandall\",\"middle\":[\"J.\"]}]", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "publication_date": {"year": 2023, "month": 1, "day": 1}, "abstract": "Video anomaly detection (VAD) has been extensively studied for static cameras but is much more challenging in egocentric driving videos where the scenes are extremely dynamic. This paper proposes an unsupervised method for traffic VAD based on future object localization. The idea is to predict future locations of traffic participants over a short horizon, and then monitor the accuracy and consistency of these predictions as evidence of an anomaly. Inconsistent predictions tend to indicate an anomaly has occurred or is about to occur. To evaluate our method, we introduce a new large-scale benchmark dataset called Detection of Traffic Anomaly (DoTA)containing 4,677 videos with temporal, spatial, and categorical annotations. We also propose a new VAD evaluation metric, called spatial-temporal area under curve (STAUC), and show that it captures how well a model detects both temporal and spatial locations of anomalies unlike existing metrics that focus only on temporal localization. Experimental results show our method outperforms state-of-the-art methods on DoTA in terms of both metrics. We offer rich categorical annotations in DoTA to benchmark video action detection and online action detection methods. The DoTA dataset has been made available at: https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "35157576", "pubmedcentral": null, "dblp": "journals/pami/YaoWXPWAC23", "doi": "10.1109/tpami.2022.3150763"}}, "content": {"source": {"pdf_hash": "c9177332181c3d09fbe11a146634da3f8387f685", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e193e0a0ab87100b9f969d97b8648ff80cc14dc2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c9177332181c3d09fbe11a146634da3f8387f685.txt", "contents": "\nDoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos\n\n\nMember, IEEEYu Yao \nMember, IEEEXizi Wang \nMember, IEEEMingze Xu \nZelin Pu \nMember, IEEEYuchen Wang \nSenior Member, IEEEElla Atkins \nMember, IEEEDavid J Crandall \nDoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos\n10.1109/TPAMI.2022.3150763Index Terms-Video anomaly detectionTraffic accident detectionFuture object localizationVideo action recognition\nVideo anomaly detection (VAD) has been extensively studied for static cameras but is much more challenging in egocentric driving videos where the scenes are extremely dynamic. This paper proposes an unsupervised method for traffic VAD based on future object localization. The idea is to predict future locations of traffic participants over a short horizon, and then monitor the accuracy and consistency of these predictions as evidence of an anomaly. Inconsistent predictions tend to indicate an anomaly has occurred or is about to occur. To evaluate our method, we introduce a new large-scale benchmark dataset called Detection of Traffic Anomaly (DoTA)containing 4,677 videos with temporal, spatial, and categorical annotations. We also propose a new VAD evaluation metric, called spatial-temporal area under curve (STAUC), and show that it captures how well a model detects both temporal and spatial locations of anomalies unlike existing metrics that focus only on temporal localization. Experimental results show our method outperforms state-of-the-art methods on DoTA in terms of both metrics. We offer rich categorical annotations in DoTA to benchmark video action detection and online action detection methods. The DoTA dataset has been made available at: https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly\n\nINTRODUCTION\n\nA UTONOMOUS driving has the potential to transform the world as we know it, revolutionizing transportation to be faster, safer, cheaper, and less labor intensive. But building autonomous systems that can accurately perceive and safely react to the huge diversity of situations that are encountered on real-world roadways is a major challenge. Driving obeys a long-tailed distribution, such that a few common situations make up the vast majority of what a driver encounters, while a virtually infinite variety of rare scenarios -animals running into the roadway, cars driving on the wrong side of the street, etc. -make up the rest. While each of these individual unusual scenarios is rare, they can and do happen. In fact, the chances that one of them will occur on any given day are actually quite high.\n\nMuch work in computer vision has studied detecting anomalous events from dashboard-mounted cameras [1], [2], [3]. Most of these methods focus on identifying frames in which an anomaly is occurring, but do not attempt to localize where in the frame the anomaly is happening or to identify which road participants are involved -critical information for real-world driving assistance systems. Other work on spatio-temporal action recognition (STAR) explores simultaneously predicting the bounding boxes (called \"action tubes\") of action performers and their associated classes [4], [5], [6], [7]. Applying these approaches to detecting anomalous events would require large-scale training data for each of a pre-defined set of events. However, the long-tailed distribution of driving events means that it may be impossible to collect training data for rare scenarios, or to even anticipate which scenarios might occur [8]. In fact, some studies indicate that driverless cars would need to be tested for billions of miles before enough of these rare situations occur to even accurately measure system safety [9], much less to collect sufficient training data to make them work well.\n\nThis paper proposes an alternative approach based on unsupervised video anomaly detection (VAD), which avoids modeling all possible driving events by recognizing \"normal\" roadway conditions and then signaling an anomaly when events that do not fit the model are observed. Any observation that does not fit a normal model is assigned a high anomaly score, regardless of its anomaly type. Unlike fully-supervised classification-based work or STAR, this unsupervised approach may not be able to identify exactly which anomaly has occurred, but it can potentially capture any type of anomalous event, even those not previously observed during training [10]. Unsupervised VAD has been widely applied to static surveillance camera datasets [8], [11], [12], [13], [14], [15] by training deep neural networks to reconstruct or predict video frames and computing the reconstruction or prediction errors as anomaly scores. However, these methods do not generalize well to driving videos since frame prediction and reconstruction are extremely difficult when cameras are rapidly moving, as in the driving scenario.\n\nThis paper side-steps the difficulty of predicting whole future frames for traffic VAD by tracking objects and predicting their future locations as opposed to attempting prediction of whole frames. We propose a novel approach that learns a future object localization (FOL) network for traffic participants (e.g., cars, bikes, pedestrians, etc.) in the field of view of a dashboard-mounted camera on a moving egovehicle. Our FOL model consists of two modules: an egomotion Recurrent Neural Network (RNN) encoder-decoder to predict future odometry of the ego-vehicle,and a twostream RNN encoder-decoder incorporating predicted egomotion into future object bounding box predictions. This model can be easily learned from massive collections of dashboard-mounted video of normal driving, and no manual labeling is required thanks to well-performing off-theshelf object detectors and trackers.\n\nExisting unsupervised VAD methods [8], [11], [13], [16], [17], [18] compute prediction error with respect to ground truth as the anomaly score, which may cause problems in our case due to imperfect object detection and tracking. To address this issue, we propose two alternatives. First, we take object boxes as the foreground to generate binary foreground-background masks and compute the IoU between predicted and ground truth masks as anomaly scores to reduce the impact of imperfect tracking. Second, we predict an object's location at time t based on the observed data from multiple previous time steps (e.g., t \u00c0 1, t \u00c0 2, etc.), collect all these predicted boxes of an object, and compute their consistency as the anomaly score. Such a prediction consistency metric does not compare prediction with detected objects, and therefore reduces the influence of imperfect detection and tracking. We compare with prediction accuracy methods and show the effectiveness of prediction consistency in traffic VAD experiments.\n\nThis paper also introduces a new large-scale benchmark dataset for traffic VAD called Detection of Traffic Anomaly (DoTA). DoTA contains 4,677 videos with 18 anomaly categories [19] and multiple anomaly participants in different driving scenarios. DoTA provides rich annotation for each anomaly: type (category), temporal annotation, and anomalous object bounding box tracklets. Current anomaly datasets contain only temporal annotations, so they cannot be used to evaluate the accuracy of spatial localization -where in the frame an anomaly is occurring. However, accurately locating the anomalous region is essential for model explainability and downstream applications such as collision avoidance. Taking advantage of this large-scale dataset with rich anomalous object annotations, we propose a novel VAD evaluation metric called Spatio-temporal Area Under Curve (STAUC) as a complement to frame-level AUC metrics. While AUC uses a per-frame anomaly score which is usually averaged from a pixel-level or object-level score map, STAUC takes the score map and computes its overlap with the annotated anomalous region. This overlap ratio is used as a weighting factor for true positive predictions with STAUC such that AUC is an upper bound on STAUC. Our STAUC can be considered as a combination of the mean average-precision (mAP) metric used in STAR or semantic segmentation evaluation and the traditional AUC metric used in VAD evaluation. STAUC has the advantage of mAP by computing the spatial overlap between prediction and ground truth, and it also has the advantage of AUC so that it is feasible for VAD evaluation tasks where the dataset is highly imbalanced due to the rareness of anomalous events. We benchmark existing VAD baselines and state-ofthe-art methods on DoTA using both AUC and STAUC, and show the advantage of using STAUC.\n\nThe DoTA dataset can also be used for video action recognition and online action detection given its categorical annotations. Video action recognition takes a video clip as input to predict its anomaly type, e.g., oncoming collision or out-of-control vehicle, while online action detection processes a video frame-by-frame to classify each frame as normal or one type of anomaly. We provide benchmarks of state-of-the-art methods such as SlowFast [20] and TRN [21] on these two tasks. Experiments show that applying generalized video action recognition and online action detection methods to traffic anomaly understanding is far from perfect, motivating more research in this area.\n\nThis paper offers four contributions. First, we propose a future object localization-based unsupervised traffic video anomaly detection method and two anomaly score computation methods to address problems caused by imperfect object detection and tracking. Second, we introduce DoTA, a large-scale egocentric traffic video dataset which, to the best of our knowledge, is the largest traffic video anomaly dataset to date and the first containing detailed temporal, spatial, and categorical annotations. Third, we identify problems with the commonly-used AUC metric and propose a new spatio-temporal evaluation metric (STAUC) to address them. We benchmark state-of-the-art VAD methods with both AUC and STAUC and show the advantages of our new metric. Finally, we provide benchmarks of state-ofthe-art video action recognition and online action detection algorithms on DoTA, which we hope will encourage further research on challenging egocentric traffic video scenarios. This paper is based on a preliminary version that was published at the International Conference on Intelligent Robots and Systems (IROS) [22]. We extend the previous version by: 1) Providing a large-scale traffic VAD dataset, called DoTA, with temporal, spatial, and categorical annotations; 2) Introducing a new spatio-temporal evaluation metric for VAD, called STAUC, with a number of advantages over existing metrics; 3) Providing benchmarks of state-ofthe-art VAD, video action recognition, and online action detection methods on the DoTA dataset; and 4) Introducing FOL-Ensemble, a new and strong baseline for traffic VAD.\n\n\nRELATED WORK\n\nExisting Video Anomaly Detection (VAD) datasets. Are typically from surveillance cameras. For example, UCSD Ped1/ Ped2 [23], CUHK Avenue [11], and ShanghaiTech [8] were collected from campus surveillance cameras and include anomalies like prohibited objects and abnormal movements, while UCF-Crime [24] includes accidents, robbery, and theft. Anomaly detection in egocentric traffic videos has very recently attracted attention. Chan et al. [1] propose the Stree-tAccident dataset of on-road accidents with 620 video clips collected from dashboard cameras. The last ten frames of each clip are annotated as anomalous. Yao et al. [22] propose the A3D dataset containing 1,500 anomalous videos in which abnormal events are annotated with start and end times. Fang et al. [3] introduce the DADA dataset for driver attention prediction in accidents, while Herzig et al. [2] extract a collision dataset with 803 videos from BDD100K [25]. In very recent work, conducted contemporaneously to ours, Bao et al. [26] collected a car crash dataset with 1,500 videos for traffic accident anticipation which contains environmental attributes and cause-of-accident annotations. Our DoTA dataset is much larger (4,677 videos) and, more importantly, contains richer annotations that support traffic video anomaly analysis from spatial (location of anomalies), temporal (start and end of anomalies), and categorical (type of anomalies) perspectives.\n\nExisting VAD models. Mainly focus on detecting the start and end of anomalous events and only implicitly relate to spatial localization. Hasan et al. [12] propose a convolutional Auto-Encoder to model the normality of video frames by reconstructing stacked input frames. A Convolutional LSTM Auto-Encoder is used in [16], [17], [18] to capture regular visual and motion patterns. Luo et al. [13] propose a stacked RNN for temporally-coherent sparse coding. Liu et al. [8] detect anomalies by looking for differences between predicted future frames and actual observations. Gong et al. [27] propose a MemAE network to query pre-saved memory units for reconstruction, while Wang et al. [28] design generalized oneclass sub-spaces for discriminative regularity modeling. Other work has recently studied object-centric approaches. Ionescu et al. [15] cluster object features and train multiple support vector machine (SVM) classifiers using the confidence as an anomaly score. Morais et al. [14] model human skeleton regularity with local-global autoencoders and compute per-object anomaly scores. Although these methods have achieved promising results on VAD tasks in surveillance cameras, egocentric traffic scenarios are challenging due to the moving camera, dynamic foreground and background, and complex scenes. Instead of modeling the whole scene, we propose to predict future locations of traffic participants and compute prediction consistency as an anomaly score. We benchmark our method and state-of-the-art VAD methods on our new DoTA dataset.\n\nTrajectory Prediction. Has been extensively investigated in computer vision research, and is often posed as a sequence-to-sequence generation problem. In this paper, we propose a VAD method based on trajectory prediction. Much work in trajectory prediction has focused on social interaction modeling [29], [30], multimodal prediction [31], [32], [33], [34], [35], [36], and goal estimation [37], [38], [39]. However, these methods are designed for third-person views from static cameras, whereas trajectory prediction in first-person, egocentric videos in which the camera itself is moving (e.g., a dashboard camera) can be even more challenging. Yagi et al. [40] incorporate different kinds of cues into a convolution-deconvolution network to predict pedestrians' future locations in first-person video. Yao et al. [41] extend this work to autonomous driving scenarios by proposing a multi-stream RNN encoder-decoder architecture with both past vehicle locations and image features as inputs. Malla et al. [42] and Rasouli et al. [43] use a similar structure and introduce intention and action priors to boost trajectory prediction accuracy. We propose a VAD method for driving videos based on [41] and introduce a prediction consistency metric to improve anomaly detection performance.\n\nAction Understanding. Techniques can be applied to traffic anomaly classification after an anomaly is detected (e.g., to distinguish front-collision from turning-collision or vehiclepedestrian collision, etc.). Two-stream networks [44] and temporal segment networks (TSN) [45] leverage RGB and optical flow data. Tran et al. [46] proposed 3D convolutional networks (C3D) for spatiotemporal modeling, followed by an inflated model [47]. Recent work substitutes 3D convolution with 2D and 1D convolution blocks (R(2+1)D [48]) to improve effectiveness and efficiency. Feichtenhofer et al. [20] propose the SlowFast model to extract video features from low and high frame rate streams. For real-time applications in untrimmed, streaming videos, online action detectors have been developed to classify video frames with only past observations [21], [49], [50], [51], [52], [53], [54], [55], [56]. In this paper we benchmark video action recognition (VAR) and online action detection (OAD) methods on our new DoTA dataset to show how anomaly events can be classified after being detected.\n\nRecently, spatio-temporal action recognition (STAR) has been proposed to detect objects and action types simultaneously [4], [5], [6], [7], [57], [58] and has the potential to be applied to driving videos. However, STAR requires detailed annotations of each bounding box and action type during training and only detects pre-defined categories; due to the long-tailed distribution of traffic events, it is nearly impossible to pre-define all the anomalous event categories and to collect enough data for training [10], which makes it difficult to apply STAR to anomaly detection. In contrast, VAD does not require pre-defined anomaly categories because it models normality in the videos without supervision, and then identifies events that do not follow typical patterns as anomalies.\n\nVideo Object Detection (VOD). [59] is similar to detecting objects in static images, but incorporates temporal constraints and tries to handle the unique challenges of video such as motion blur. Early methods solved this problem by detecting objects in each individual frame. To capture the spatial-temporal nature of video and to increase the accuracy and efficiency of detection, recent methods [60], [61], [62], [63], [64], [65], [66] use deep learning. Some methods [67], [68] use Convolutional Long Short Term Memories (LSTMs) [69] to model long-term relations and select important features. Liu et al. [67] combine an image-based object detector with a convolutional LSTM to add temporal information to input features, in order to detect objects in videos. They improve inference speed by using large and small feature extractors in [68]. Other methods [61], [62], [63], [64], [65], [66] use local feature aggregation for detection in a given frame. Inspired by humans' object localization ability across multiple time steps, Chen et al. [70] jointly consider global semantic information and key frame object localization, and propose the Memory Enhanced Global-Local Aggregation module, which achieves 85.4% on the Image-Net VID dataset. Han et al. [71] propose the Hierarchical Video Relation Network which models the Inter-Video Proposal Relation in addition to Intra-Video Proposal Relation.\n\nAlthough Video Object Detection is related to the VAD task we consider here because they both involve spatial-temporal modeling, our work is significantly different in several ways: 1) VAD aims at localizing anomalous events in video while VOD detects object bounding boxes and types; 2) our DoTA dataset focuses on the richness of anomalous behaviors rather than only objects; 3) our DoTA dataset provides labels for anomalous behaviors, not normal objects and their behaviors.\n\n\nMETHOD\n\nAutonomous vehicles must monitor the roadway for signs of unexpected activity that may require evasive action. A natural way to detect these anomalies is to look for unexpected or rare movements in the first-person perspective of a front-facing, dashboard-mounted camera on a moving ego-vehicle. Prior work [8] proposes monitoring for unexpected scenarios by using past video frames to predict the current video frame, and then looking for major differences. However, this does not work well for moving cameras on vehicles, where the perceived optical motion in the frame is induced by both moving objects and camera ego-motion. More importantly, anomaly detection systems do not need to accurately predict all information in the frame, since anomalies are unlikely to involve peripheral objects such as houses or billboards by the roadside. This paper thus assumes that an anomaly may exist if an object's real-world observed trajectory deviates from the predicted trajectory. For example, when a vehicle should move through an intersection but instead suddenly stops, a collision may have occurred.\n\nOur model is trained with a large-scale dataset of normal, non-anomalous driving videos. This allows the model to learn normal patterns of object and ego motions, then recognize deviations without the need to explicitly train the model with examples of every possible anomaly. This video dataset is easy to obtain and does not require hand labeling since the object trajectories can be estimated using off-theshelf object detection and tracking algorithms [41].\n\nConsidering the influence of ego-motion on perceived object location, we incorporate a future ego-motion prediction module as an additional input. At test time, we use the model to predict the current locations of objects based on the last few frames of data and determine if an abnormal event has happened based on three different anomaly detection strategies, described in Section 3.2.\n\n\nFuture Object Localization (FOL)\n\n\nBounding Box Prediction\n\nFollowing [41], we denote an observed object's bounding box X t \u00bc \u00bdc x t ; c y t ; w t ; h t at time t, where (c x t , c y t ) is the location of the center of the box and w t and h t are its width and height in pixels, respectively. We denote the object's future bounding box trajectory for the d frames after time t to be Y t \u00bc fY t\u00fe1 ; Y t\u00fe2 ; . . . ; Y t\u00fed g, where each Y t is a bounding box parameterized by center, width, and height. Given the image evidence O t observed at time t, a visible object's location X t , and its corresponding historical information H t\u00c01 , our future object localization model predicts Y t . This model is inspired by the multi-stream RNN encoder-decoder framework of Yao et al. [41], but with a completely different network structure [21] to allow for online training and inference. For each frame, [41] receives and re-processes the previous ten frames before making a decision, whereas our model only needs to process the current information, making it much faster at inference time.\n\nOur model is shown in Fig. 2. Two encoders (Enc) based on gated recurrent units (GRUs) receive an object's current bounding box and pixel-level spatiotemporal features as inputs, respectively, and update the object's hidden states. In particular, the spatiotemporal features are extracted by a region-of-interest pooling (RoIPool) operation using bilinear interpolation from precomputed optical flow fields. The updated hidden states are used by a location decoder (Dec) to recurrently predict the bounding boxes of the immediate future. We train the FOL model using a mean squared error loss between the predicted and target future bounding boxes. \n\n\nEgo-Motion Cue\n\nEgo-motion information of the moving camera has been shown necessary for accurate future object localization [41], [72]. Let E t be the ego-vehicle's pose at time t; E t \u00bc ff t ; x t ; z t g where f t is the yaw angle and x t and z t are positions along the ground plane with respect to the vehicle's starting position in the first video frame. We predict the ego-vehicle's odometry by using another GRU encoderdecoder module to encode ego-position change vector E t \u00c0 E t\u00c01 and decode future ego-position changes E \u00bc f\u00ca t\u00fe1 \u00c0E t ;\u00ca t\u00fe2 \u00c0 E t ; . . . ;\u00ca t\u00fed \u00c0 E t g. We use the change in egoposition to eliminate accumulated odometry errors. The output E is then combined with the hidden state of the future object localization decoder using average pooling to form the input into the next time step.\n\n\nMissed Objects\n\nWe build a list of trackers Trks per [73] to record the current bounding box Trks\u00bdi:X t , the predicted future boxes Trks\u00bdi:\u0176 t , and the tracker age Trks\u00bdi:age of each object. We denote all maintained track IDs as D (both observed and missed), all currently observed track IDs as C, and the missed object IDs as D \u00c0 C. At each time step, we update the observed trackers and initialize a new tracker when a new object is detected. We use a temporarily-missing or occluded object's previously predicted bounding boxes to estimate current location, running future object localization with RoIPool features from predicted boxes (Algorithm 1). Missing object handling is essential in our prediction-based anomaly detection method to eliminate the impact of failed object detection or tracking in any given frame. For example, if an object with a normal motion pattern is missed for several frames, the FOL model is still expected to give reasonable predictions except for some accumulated deviations. On the other hand, if an anomalous object is missed during tracking [73], we make a prediction using its previously predicted bounding box whose region can be substantially displaced and can result in inaccurate predictions. In this case, some false alarms and false negatives can be eliminated by using the metrics presented in Section 3.2.3. \n\n\nAlgorithm 1. FOL-Track Algorithm\n\n\nTraffic Anomaly Detection using FOL\n\nUnsupervised anomaly detection methods compute anomaly scores based on prediction or reconstruction accuracy [8], [12], [14], [15]. In this section, we first present the basic anomaly metric computed from predicted bounding box accuracy. The key idea is that object trajectories and locations in non-anomalous events can be precisely predicted, while deviations from predicted behaviors suggest an anomaly. Next we propose two different strategies to compute anomaly scores using: 1) the foreground-background mask generated from predictions and 2) the prediction consistency.\n\n\nPredicted Bounding Box Accuracy\n\nOne simple method for recognizing abnormal events is to directly measure the similarity between predicted object bounding boxes and their corresponding observations. The FOL model predicts bounding boxes of the next d future frames, i.e., at each time t each object has a bounding box predicted at each time from t \u00c0 d to t \u00c0 1. We first average the positions of the d bounding boxes, then compute intersection over union (IoU) between the averaged bounding box and the observed box location, where higher IoU means greater agreement between the two boxes. We average computed IoU values over all observed objects, and then compute an aggregate anomaly score L bbox 2 \u00bd0; 1,\nL bbox \u00bc 1 \u00c0 1 N X N i\u00bc1 IoU 1 d X d j\u00bc1\u0176 i t;t\u00c0j ; Y i t 0 ;(1)\nwhere N is the total number of observed objects, and\u0176 i t;t\u00c0j is the predicted bounding box from time t \u00c0 j of object i at time t. This method, which we call FOL-IoU, relies upon accurate object tracking to match predicted and observed bounding boxes. \n\n\nPredicted Box Mask Accuracy\n\nAlthough tracking algorithms such as Deep-SORT [73] offer reasonable accuracy, it is still possible to lose or mis-track objects. We found that inaccurate tracking particularly happens in severe traffic accidents because of the twist and distortion of object appearances. Moreover, severe ego-motion also results in inaccurate tracking due to sudden changes in object locations. This increases the number of false negatives of the metric proposed above, which simply ignores objects that are not successfully tracked in a given frame. To solve this problem, we first convert all areas within the predicted bounding boxes to binary masks, with areas inside the boxes having value 1 and backgrounds having 0, and do the same with the observed boxes. We then calculate an anomaly score as the IoU between these two binary masks,\nI \u00f0u;v\u00de \u00bc 1, if pixel (u,v) within box Xi, 8i, 0; otherwise, &(2)L mask \u00bc 1 \u00c0 IoU \u00c0\u00ce t;t\u00c01 ; I t \u00c1 ;(3)\nwhere I \u00f0u;v\u00de is pixel \u00f0u; v\u00de on mask I, X i is the ith bounding box,\u00ce t;t\u00c01 is the predicted mask from time t \u00c0 1, and I t is the observed mask at t. In other words, while the bounding box accuracy metric compares bounding boxes on an object-byobject basis, this metric simply compares the bounding boxes of all objects simultaneously. The main idea is that accurate prediction results will still have a relatively large IoU compared to the ground truth observation. We denote the mask accuracy-based method FOL-Mask.\n\n\nPredicted Bounding Box Consistency\n\nThe above methods rely on accurate detection of objects in concurrent frames to compute anomaly scores. However, the detection of anomaly participants is not always accurate due to changes in appearance and mutual occlusions. We hypothesize that visual and motion features related to an anomaly do not only appear once it happens, but are usually accompanied by a salient pre-event. We thus propose another strategy, called FOL-STD, to detect anomalies by computing consistency of future object localization outputs from several previous frames while eliminating the effect of inaccurate detection and tracking.\n\nAs discussed in Section 3.2.1, for each object in video frame at time t, we can collect d bounding boxes predicted from time t \u00c0 1; t \u00c0 2; ::; t \u00c0 d. We compute the standard deviation (STD) between all d predicted bounding boxes to measure their similarity,\nL pred \u00bc 1 N X N i\u00bc1\nmax fc x ;c y ;w;hg STD\u00f0\u00bd\u0176 \u00f0i\u00de t;t\u00c0j j\u00bcd j\u00bc1 \u00de:\n\nwhere\u0176 \u00f0i\u00de t;t\u00c0j is the bounding box of the ith object in frame at time t predicted from the frame at time t \u00c0 j, and c x ; c y ; w; h are the center coordinates and the width and height of a bounding box. We compute the maximum standard deviation over the four components of the bounding boxes since different anomalies may be indicated by different effects on the bounding box, e.g., suddenly stopped cross traffic may only have large variance along the horizontal axis. A low standard deviation suggests the object is following normal movement patterns thus the predictions are stable, while a high standard deviation suggestions abnormal motion. For all three methods, we follow [8] to normalize computed anomaly scores for evaluation.\n\n\nFrame-Object Ensemble Anomaly Detection\n\nOur FOL based methods are object-centric by encodingdecoding object information. Frame-level VAD methods focus on appearance while object-centric methods focus more on object motion. We are not aware of any method combining the two. Appearance-only methods may fail with large changes in lighting conditions, and motion-only methods may fail when trajectory prediction is imperfect. We combine FOL-STD with the frame prediction method AnoPred [8], yielding what we call the FOL-Ensemble method. AnoPred predicts one anomaly score per image pixel while our method predicts one anomaly score per object. We first map our object anomaly score to a per-pixel score by putting a Gaussian function at the center of each object (as introduced in Section 5). We train each module of FOL-Ensemble independently and apply average pooling on the computed per-pixel scores from two modules to compute a final anomaly score. We observed this late fusion is better than fusing hidden features at an early stage and training the two models together, since their hidden features are scaled differently. AnoPred encodes one feature per frame, while FOL-STD has one feature per object.\n\n\nDETECTION OF TRAFFIC ANOMALY (DOTA) DATASET\n\nWe introduce DoTA, the first publicly-available traffic video anomaly dataset with temporal, spatial, and categorical annotations. To build DoTA, we collected more than 6,000 video clips mainly from two YouTube channels 1 which provides traffic accident videos for driver education purposes. We selected diverse dash camera accident videos from different areas (e.g., East Asia, North America, Europe etc.) under different weather (e.g., sunny, cloudy, raining, snowing, etc.) and lighting conditions (day and night). We avoided videos with accidents that were not visible or where the camera dislodged during the accident, yielding 4,677 videos with 1280 \u00c2 720 resolution, each containing exactly one anomalous event. Although the original videos are at 30 fps, we extracted frames at 10 fps for annotations and experiments in this paper. Table 1 compares DoTA with other ego-centric traffic anomaly datasets. We annotated the dataset using a custom tool based on Scalabel. 2 Labeling traffic anomalies is subjective, especially for properties like start and end times. To produce high quality annotations, each video was labeled by three annotators, and the temporal and spatial (categorical) annotations were merged by taking average (mode) to minimize individual biases. Temporal Annotations. Each DoTA video is annotated with anomaly start and end times, which separates it into three temporal partitions: precursor, which is normal video preceding the anomaly, the anomaly window, and post-anomaly, which is normal activity following the anomaly. Duration distributions are shown in Fig. 5a. Since early 1. https://youtube.com/user/CarCrashesTime and https://youtube.com/channel/UC-Oa3wml6F3YcptlFwaLgDA 2. https://scalabel.ai/ detection is essential for on-road anomalies [1], [75], we asked the annotators to estimate the anomaly start as the time when the anomaly was inevitable. The anomaly end was approximated as the time when all anomalous objects are out of the field of view or are stationary. Our annotation is different from [3] where a frame is marked as an anomaly start if half of an anomaly participant appears in the camera view; such a start time can be too early because anomaly participants often appear for a while before they start to behave abnormally. Our annotation is also distinct from [1] and [22] where the anomaly start is marked when a crash happens, which does not support early detection.\n\nSpatial Annotations. DoTA is the first traffic anomaly dataset to provide detailed spatio-temporal annotation of anomalous objects. Each anomaly participant is assigned a unique track ID, and each participant's bounding box is labeled from anomaly start to anomaly end or until the object is out of view. We consider seven common traffic participant categories: person (pedestrian), car, truck, bus, motorcycle, bicycle, and rider, following the BDD100K style [25]. Statistics of object categories and per-video anomalous object numbers are shown in Figs. 5b and 5c. DADA [3] also provides spatial annotations by capturing video observers' eye-gaze for driver attention studies. However, they have shown that eye-gaze does not always coincide with the anomalous region, and that gaze can have a 1-2 second delay from anomaly start. Our tracklets thus provide improved annotation for spatio-temporal anomaly detection studies.\n\nAnomaly Categories. Each DoTA video is assigned one of the 9 categories listed in Table 2 as defined in [19]. We have observed that even within the same anomaly category, different viewpoints cause large visual variation (e.g., whether the ego-vehicle is a participant in the accident or if it is simply observing it), as shown in Fig. 4. Therefore we split each category into ego-involved and non-ego (marked with *), resulting in 18 categories total. Sometimes the category can be ambiguous, particularly when one anomaly is followed by another. For example, an oncoming out-of-control (OO*) vehicle might result in an oncoming collision (OC) with the ego vehicle. In such cases, we annotate the anomaly category as the dominant one in the video, i.e, the one that lasts longer during the anomaly period. The distribution of videos in each category is shown in Fig. 5b.\n\n\nA NEW VAD EVALUATION METRIC\n\n\nCritique of Current VAD Evaluation\n\nMost VAD methods compute an anomaly score for each frame, and evaluate by plotting receiver operating characteristic (ROC) curves using temporally concatenated scores and  computing area under curve (AUC). AUC measures how well a VAD method locates an anomaly along the temporal axis but ignores accuracy on spatial axes since an averaged anomaly score lacks spatial information. Frame-level AUC does not evaluate the performance of spatial localization [76], while anomaly region localization is necessary in VAD. We argue AUC is insufficient to fully evaluate VAD performance.\n\nIn computing AUC, a true positive occurs when the model predicts a high anomaly score for a positive frame. Fig. 6 shows two positive frames and their corresponding score maps computed by the four benchmarked VAD methods. Although the maps are different, the anomaly scores averaged from these maps are similar, meaning they are treated similarly in AUC evaluation. This results in similar AUCs among all methods, which leads to a conclusion that all perform similarly. However, AnoPred (Fig. 6b) predicts high scores for trees and other noise, while AnoPred+Mask and FOL-STD (Figs. 6c and 6d) predict high scores for unrelated vehicles. Ensemble (Fig. 6d) alleviates these problems but still has high anomaly scores outside the labeled anomalous regions. (Note that FOL-STD and Ensemble are pseudo-maps introduced in Section 5.2.) Although these methods yield similar AUCs, VAD methods should be distinguished by their abilities to localize anomalous regions. Anomalous spatial localization is essential because it improves reaction to anomalies -permitting collision avoidance, for example -and allows for model explainability. This motivates a new metric to evaluate how well a model detects both the temporal and spatial location of the anomaly.\n\n\nSpatial-Temporal Area Under Curve (STAUC) Metric\n\nFor each positive frame, we first calculate the true anomalous region rate (TARR), which is a scalar describing how much of the anomaly score is located within the true anomalous region,\nTARR t \u00bc P i2m t DI\u00f0i\u00de P i2M DI\u00f0i\u00de ;(5)\nwhere DI\u00f0i\u00de is the anomaly score at pixel i, M represents all frame pixels, and m t is the annotated anomalous frame region (i.e., the union of all annotated bounding boxes) at time t. TARR is inspired by anomaly segmentation tasks where the overlap between prediction and annotation is computed [77]. Another metric related to TARR is intersection over union (IoU) used in object detection, where the predicted box has IoU equal to 1.0 if it is the same as the labeled box. However, in contrast to object detection, not all pixels in an anomalous event region are necessarily anomalous: the tree pixels or street surface pixels in the car bounding boxes in Fig. 6a, for example. The proposed TARR value measures how well a model concentrates on the true anomalous region but does not require it to predict high anomaly scores for all pixels within the true anomalous region, thus making it more appropriate to the anomalous event detection task. Next, we calculate the spatio-temporal true positive rate (STTPR),\nSTTPR \u00bc P t2TP TARR t jP j ;(6)\nwhere TP represents all true positive predictions and P represents all ground truth positive frames. STTPR is a true positive rate where each true positive is weighted by its TARR. We then use STTPR and the false positive rate to plot an ROC curve (which we call Spatial-Temporal ROC or STROC) and calculate the area under the curve, which gives the STAUC. Note that STAUC AUC; the two are equal in the best case where TARR t \u00bc 1 8t.\n\nObject-centric VAD [14], [15], [22] computes per-object anomaly scores s k instead of an anomaly score map DI. To generalize the STAUC metric to object-centric methods, we first create pseudo-anomaly score maps as illustrated in Fig. 6d. Each object has a 2D Gaussian distribution centered in its bounding box. The pixel score is then computed as the sum of the scores calculated from all boxes it occupies,\nDI pseudo \u00f0i\u00de \u00bc X 8k;i2B k s k e \u00c0 jix\u00c0x k j 2 2w k \u00c0 jiy\u00c0y k j 2 2h k ;(7)\nwhere i x and i y are coordinates of pixel i and \u00bdx k ; y k ; w k ; h k are center location, width, and height of object bounding box B k . For the Ensemble method, we take the average of DI and DI pseudo as the anomaly score map in Fig. 6e. This map is used as DI in Eq. (5) to compute TARR and STAUC. TARR is not robust to anomalous region size m t . When m t ( M, TARR could be small even though all anomaly scores are high in m t . We thus propose selecting the top N% of pixels with the largest anomaly scores as candidates, and compute TARR from these candidates instead of all pixels. An extremely small N such as 0.01 may result in a biased candidate set dominated by false or true detections such that TARR \u00bc 0 or 1. To address this issue, we compute an adaptive N for each frame based on the size of its annotated anomalous region, N adaptive \u00bc number of pixels in anomalous region Total number of pixels \u00c2 100: (8) The average N adaptive of the test data in DoTA is 11.12 with a standard deviation 13.09. The minimum and maximum N adaptive values are 0.005 and 95.8, showing extreme cases where the anomalous object is very small (far away) or large (nearby). A critical consideration for any new metric is its robustness to hyper parameters. We have tested STAUC with N \u00bc \u00bd1; 5; 10; 20; 50; 100; N adaptive for different VAD methods. As shown in Fig. 7a, STAUC slightly decreases with increasing N but stabilizes when N is large, indicating that STAUC is relatively robust. Fig. 7b shows that STROC curves with  different N are close, especially when N ! 5, and their upper bound is the traditional ROC. N adaptive is selected for our benchmarks based on each frame's annotation and its corresponding mid-range STAUC value.\n\n\nComparison With Spatio-Temporal Action Recognition Metrics\n\nTwo existing evaluation metrics related to our STAUC are the supervoxel-AUC [78], [79] and mean average precision (mAP) [4] used for spatio-temporal action recognition (STAR). Supervoxel-AUC. Uses an overlap threshold between predicted and ground truth supervoxels to determine whether a supervoxel is considered positive or negative. The authors report multiple AUC values at different overlap thresholds (e.g., 10%, 20%, etc.). However, most VAD methods predict pixel-level anomaly score maps rather than supervoxels, therefore supervoxel-AUC cannot be directly applied to VAD evaluation. Our STAUC, on the other hand, uses TARR as a measure of how positive a frame is so that we do not need to evaluate STAUC under different thresholds, making STAUC more universal than supervoxel-AUC. Another advantage of STAUC is that it can evaluate different output and ground truth formats: STAUC can be applied to pixel-level, objectlevel and, essentially, supervoxel-level output since it computes how well anomaly scores locate within the ground truth region regardless of the region format.\n\nMean Average Precision (mAP). Has been used for evaluating object-level action recognition. mAP computes the IoU between detected and ground truth bounding boxes and uses a fixed IoU threshold (e.g., 0.5 in [4]) to categorize into positives and negatives. The AP is then computed for each action type and the mean value is reported. However, the precision-recall curve used to compute AP can perform differently for balanced versus unbalanced data distributions, which makes mAP less appropriate for VAD tasks where anomalous events are rare compared to normal events. Our STAUC can be considered as a combination of the mAP idea and the AUC metric so that it evaluates spatial accuracy and can be applied to imbalanced data.\n\nCompared to supervoxel-AUC and mAP, our STAUC is specifically designed to extend VAD evaluation from temporal to spatio-temporal.\n\n\nEXPERIMENTS\n\nWe benchmarked VAD baselines and our methods on the new DoTA dataset. DoTA also provides categorical annotations to suit video action recognition (VAR) and online action detection tasks, thus we provide extra benchmarks for stateof-the-art methods for these two tasks using the DoTA dataset. We randomly partitioned DoTA into 3,275 training and 1,402 test videos and use these splits for all tasks. Unsupervised VAD models must be trained only with non-anomalous data, so we use the precursor frames from each video for training. VAR and online action detection models are fully-supervised and thus are trained using all training data.\n\n\nTask 1: Video Anomaly Detection (VAD)\n\n\nBenchmarked Methods\n\nWe benchmark three frame-level VAD method, Con-vAE [12], ConvLSTMAE [17], and AnoPred [8] and their variants as baselines. Frame-level methods detect anomalies by either reconstructing past frames or predicting future frames, and computing the reconstruction or prediction error as the anomaly score.\n\nConvAE. [12] is a spatio-temporal autoencoder model which encodes temporally stacked images with 2D convolutional encoders, and decodes with deconvolutional layers to reconstruct the input (Fig. 8a). The per-pixel reconstruction error forms an anomaly score map DI, and mean squared error (MSE) is computed as a frame-level anomaly score. To further compare the effectiveness of image and motion features, we implemented ConvAE(gray) and ConvAE(flow) to reconstruct the grayscale image and dense optical flow, respectively. The input to ConvAE(flow) is a stacked historical flow map with size 20 \u00c2 227 \u00c2 227 acquired from pretrained FlowNet2 [80]. We trained both variants using Ada-Grad with learning rate 0.01 and batch size 24.\n\nConvLSTMAE. [17] is similar to ConvAE but models spatial and temporal features separately. A 2D CNN encoder first captures spatial information from each frame, then a multi-layer ConvLSTM recurrently encodes temporal features. Another 2D CNN decoder then reconstructs input video clips (Fig. 8b). We implemented ConvLSTMAE(gray) and ConvLSTMAE(flow). We trained both variants using AdaGrad with learning rate 0.01 and batch size 24.\n\nAnoPred. [8] is a frame-level VAD method that takes four contiguous previous RGB frames as input and applies UNet to predict a future RGB frame (Fig. 8c). AnoPred boosts prediction accuracy with a multi-task loss incorporating image intensity, optical flow, gradient, and adversarial losses. AnoPred was proposed for surveillance cameras. However, traffic videos are much more dynamic, making future frame prediction difficult. Therefore we also benchmarked a variant of AnoPred to focus on video foregrounds. We used Mask-RCNN [81] pre-trained on Cityscapes [82] to acquire object instance masks for each frame, and apply instance masks to input and target images, resulting in an AnoPred+Mask method that only predicts foreground objects and ignores noisy backgrounds such as trees and billboards. In contrast to [12], [17], AnoPred uses Peak Signal to Noise Ratio as the anomaly score with better results. Both variants are trained based on the original paper.\n\nImplementation Details. We used the published implementations of ConvAE, ConvLSTMAE, and AnoPred and modified the input layer size to suit grayscale or optical flow input. All these models were trained according to the original papers. Our FOL-based methods use hidden size 128 for all GRU modules and are trained using the RMSprop [83] optimizer with batch size 16, learning rate 0.0001, and no weight decay. All models are trained and evaluated on NVidia Titan XP GPUs. For evaluation, we ignore videos with unknown category or without objects, resulting in 1,305 test videos.\n\n\nResults\n\nOverall Results. The top four rows of Table 3 show performance of ConvAE and ConvLSTMAE with grayscale or optical flow inputs. Generally, using optical flow achieves better AUC, indicating that motion is an informative feature for this task. However, all baselines achieve low STAUC, meaning that they cannot localize anomalous regions well. AnoPred achieves 67.5 AUC but only 24.4 STAUC, while AnoPred with masked RGB input (AnoPred+mask) has 2.7 lower AUC but 17.7 higher STAUC. By applying instance masks, the model focuses on foreground objects to avoid computing high scores for the background, resulting in slightly lower AUC but much higher STAUC. This supports our hypothesis that higher AUC does not always imply a better VAD model, while STAUC better captures the ability to localize anomalous regions. Table 3 also shows evaluations of four variants of our methods: FOL-IoU (prediction accuracy), FOL-Mask (prediction mask accuracy), FOL-STD (prediction consistency), and FOL-Ensemble, where FOL-Ensemble is an ensemble model of FOL-STD and AnoPred+Mask. FOL-Mask outperforms FOL-IoU as it is more robust to inaccurate object tracking. FOL-STD outperforms FOL-IoU and FOL-Mask by a large margin, which shows the effectiveness of our proposed consistency metric over the existing accuracy based metric. Its higher STAUC also shows that FOL-STD is more robust to scenarios where objects are not accurately detected or tracked. FOL-STD outperforms AnoPred on both metrics by specifically focusing on object motion and location, both of which are important indicators of traffic anomalies. An ablation study for FOL-STD is also shown in Table 3 to evaluate the effectiveness of different modules in the FOL network. Although FOL-STD with bounding box input serves as a reasonable baseline, adding ego motion and optical flow streams further boosts the AUC and STAUC values, indicating the effectiveness of our multi-stream FOL network. Finally, the FOL-Ensemble method achieves the best AUC and STAUC among all methods, indicating that combining frame-level appearance and motion features is a direction worth investigating in future VAD research.\n\nPer-class Results. Table 4 shows results of AnoPred, AnoPred+Mask, FOL-STD, and FOL-Ensemble broken out according to the type of anomaly. We observe that STAUC (unlike AUC) can break out results by anomaly type. For example, Ensemble has comparable AUCs on the egoinvolved OC (oncoming collision) and VP (vehicle-pedestrian) anomalies (73.4 vs 70.1) but significantly different STAUCs (56.6 vs 35.2), showing that anomalous region localization is harder for vehicle-pedestrian. Similar trends exist for the non-ego AH* (ahead collision), LA* (lateral collision), VP* (vehicle-pedestrian), and VO* (vehicle-obstacle) anomalies. Second, frame-level and object-centric methods compensate each other in VAD as shown by the Ensemble method's highest AUC and STAUC values in most columns. Third, localizing anomalous regions in non-ego anomalies (marked with *) is more difficult, perhaps because ego-involved anomalies have better dashcam visibility and larger anomalous regions. We also note that certain classes are especially challenging for various reasons; for example: pedestrians in vehicle-pedestrian (VP) videos become occluded or disappear quickly after an anomaly happens, the impacting vehicle in non-ego ahead (AH*) is often occluded by the vehicle it impacts, obstacles in non-ego vehicle-obstacle (VO*) such as bumpers or traffic cones are often occluded or not detected, and vehicles in nonego lateral (LA*) usually move toward each other slowly until they collide and stop, making the anomaly relatively subtle.\n\nQualitative Results. Fig. 9a shows per-frame anomaly scores and TARRs of three methods on a video in which they all achieve high AUCs. AnoPred+Mask has low TARR along the video, indicating failure to localize anomalous regions. FOL-STD computes high anomaly scores but low TARR in the left example due to inaccurate trajectory prediction for the left car. In the right image, it finds an anomalous car but marks an unrelated car by mistake. Ensemble combines the benefits of both with scores for the 20-30th anomaly frames always higher than normal frames. It yields high TARR during the 10-20th anomaly frames as shown in the left score map. The right map shows a failure case combining the failure of AnoPred+Mask and FOL-STD. Although these methods achieve high AUC, their spatial localization is limited according to TARR. Fig. 9b shows an ego-involved ahead collision (AH). AnoPred+Mask incorrectly computes a high anomaly score in the early frames because the prediction of the left car is inaccurate. FOL-STD computes a low anomaly score for this frame, so the Ensemble method benefits. The right example shows that FOL-STD computes a high score correctly for the car ahead but incorrectly for the bus. Ensemble benefits from AnoPred+Mask so that it focuses more attention on the car instead of the bus.\n\n\nTask 2: Video Action Recognition (VAR)\n\nVAD detects the temporal range of an anomalous event but does not understand the anomaly type. The goal of Video Action Recognition (VAR) is to classify each video clip into one anomaly category. Taking advantage of the rich categorical annotation of the DoTA dataset, we benchmark seven VAR methods: C3D [46], I3D [47], R3D [48], MC3 [48], R(2+1)D [48], TSN [45], and Slow-Fast [20]. The previous training/test split is used. Unknown UK(*) anomalies are ignored, yielding 3216 training and 1369 test videos. We trained all models with SGD, learning rate 0.01 and batch size 16 on NVidia Titan XP GPUs. Models are initialized with pre-trained weights from Sports-1M [84] for C3D and Kinetics [85] for the other methods; 0.5 probability random horizontal flips create data augmentation. For evaluation, we randomly selected ten clips from each test video (as in [20]), except for TSN which uses 25 clips per video. Table 5 shows the results. Although newer methods R (2+1)D and SlowFast achieve higher average accuracy, all suffer from low accuracy on DoTA, indicating that traffic anomaly classification is challenging. First, distant anomalies and occluded objects create difficulties; VO (vehicle-obstacle) and VO* are particularly hard to classify due to low visibility and diverse obstacle types (as seen in Section 6.1). AH* (ahead collision) and OC* (oncoming collision) are also difficult since oncoming vehicles are often occluded. Second, some anomalies are visually similar. For example, ST (start/stop/stationary) and ST* are rare and look similar to AH (ahead) and AH* or LA (lateral) and LA* (Fig. 4) -the only difference is whether the vehicle is starting, stopping, or stationary. Third, the anomaly category is often determined near the anomaly start time, while the later frames do not reveal this category clearly. We have observed 2-4% accuracy improvement when testing models only on the first half of each clip. Additional benchmarks are available in our supplementary material, which can be found on the Computer Society Digital Library at http://doi. ieeecomputersociety.org/10.1109/TPAMI.2022.3150763.\n\n\nTask 3: Online Action Detection (OAD)\n\nFinally we provide benchmarks for online video action detection on the DoTA dataset. OAD recognizes the anomaly type by only observing the current and past frames,  making it suitable for autonomous driving applications. Since OAD does not have a full observation of the whole video sequence, it is more difficult than traditional VAR. We benchmark four OAD methods: FC, LSTM, Encoderdecoder, and TRN. The classifiers are designed to predict one out of the 16 anomaly categories. We use the same training configurations as in [21]. Table 6 shows the per-class average precision (AP) and the mean average prediction (mAP). We observe that although TRN, a state-of-the-art method, achieves the highest mAP, all methods suffer from low precision on DoTA. Similar to what we have observed in the VAD and VAR experiments, online action detection is also difficult for ST (start/stop/stationary), ST*, VP (vehiclepedestrian), VP*, VO (vehicle-obstacle), and VO*. AH* (ahead) an OC* (oncoming) are also difficult due to the highly occluded front of a typical oncoming vehicle. We also observe that ego-involved anomalies are easier to recognize than non-ego anomalies due to their higher visibility. \n\n\nCONCLUSION AND FUTURE WORK\n\nThis paper proposed a novel FOL-based unsupervised video anomaly detection (VAD) method for driving videos. A prediction consistency metric was introduced for computing anomaly scores which is robust to inaccurate object detection and tracking in driving videos. We further introduce an ensemble method to combine object-and frame-level VAD methods to boost performance. We also introduced DoTA, a large-scale dataset containing temporal, spatial, and categorical annotations and benchmarked state-of-the-art VAD methods. We proposed a new spatial-temporal area under curve (STAUC) metric to better evaluate VAD performance. Experimental results show that our method achieves state-of-the-art results on DoTA in terms of both AUC and STAUC. Our DoTA dataset also enables research on video action recognition (VAR) and online action detection in driving scenarios; both of these problems are far from solved according to experimental results. Future work will include but not limited to spatio-temporal localization of anomalies in driving scenarios, early detection of traffic accidents, and validation and verification of autonomous driving systems.   Table 4 Caption for Class Abbreviations See Table 4 caption for class abbreviations.\n\nFig. 1 .\n1Overview of our method based on future object localization (FOL) using a sample video from our DoTA dataset. Annotated bounding boxes (filled) and predicted boxes are presented. For each time step, we collect FOL predictions of all traffic participants from different past time steps and compute the bounding box standard deviation, called consistency, as the anomaly score.\n\nFig. 2 .\n2Overview of the future object localization model. Blocks with dashed outlines are encoders, and solid lines are decoders. The decoder recurrences are unfolded to visualize the prediction horizon.\n\nFig. 3 .\n3Overview of our unsupervised VAD methods. The three brackets correspond to: (1) Predicted bounding box accuracy method (green); (2) Predicted box mask accuracy method (orange); (3) Predicted bounding box consistency method (blue). All methods use multiple previous FOL outputs to compute anomaly scores.\n\nFig. 5 .\n5DoTA dataset statistics.\n\nFig. 6 .\n6Anomaly score maps computed by four methods. Ground truth anomalous regions are labeled by bounding boxes. Brighter color indicates higher score.\n\nFig. 7 .\n7(a) STAUC values of different methods using different top N%; (b) ROC curve and STROC curves of the Ensemble method with different top N%.\n\nFig. 8 .\n8Network architectures of methods for video anomaly detection.\n\nFig. 9 .\n9Per-frame anomaly scores and TARRs of three methods for two different videos. The left and right columns show sample video frames and corresponding score maps, with an arrow indicating their temporal position within the video. Note that TARR only exists in positive frames.\n\n\nInput: Observed bounding boxes fX \u00f0i\u00de t g where i 2 C, observed image evidence O t , trackers of all objects Trks with track IDs D Output: Updated trackers Trks 1: A is the maximum age of a tracker 2: for i 2 C do // update observed trackers 3: if i = for j 2 D \u00c0 C do// update missed trackers 11: if Trks\u00bdj:age > A then 12: remove Trks\u00bdj from Trks 13: else 14: Trks\u00bdj:X t \u00bc Trks\u00bdj:\u0176 t\u00c01 15: Trks\u00bdj:\u0176 t \u00bc FOL\u00f0Trks\u00bdj:X t ; O t \u00de 16: end 17: end2 D then \n4: \ninitialize Trks\u00bdi \n5: else \n6: \nTrks\u00bdi:X t \u00bc X \n\n\u00f0i\u00de \nt \n\n7: \nTrks\u00bdi:\u0176 t \u00bc FOL\u00f0X \n\n\u00f0i\u00de \n\nt ; O t \u00de \n8: end \n9: end \n10: \n\nTABLE 1\n1Comparison of Published Driving Video Anomaly DatasetsDataset \ntype \n# anomaly videos \n# frames \nAnnotations \n\nUCSD Ped1/2 [74] \nSurveillance \n98 \n18560 \n(30fps) \ntemporal \nCUHK Avenue [11] \n37 \n30,652 \n(30fps) \ntemporal \nUCF-Crime [24] \n1,900 \n13,769,300 \n(30fps) \ntemporal \nShanghaiTech [13] \n437 \n317,398 \n(30fps) \ntemporal \nStreetAccident [1] \nDashcam \n620 \n62,000 \n(20fps) \ntemporal \nA3D [22] \n1,500 \n128,175 \n(10fps) \ntemporal \nCCD [26] \n1,500 \n75,000 \n(10fps) \ntemporal, spatial (tracklets), causation \nDADA [3] \n2,000 \n648,476 \n(30fps) \ntemporal, spatial (eye-gaze) \nDoTA \n4,677 \n731,932 \n(10fps) \ntemporal, spatial (tracklets), categories \n\n\n\nTABLE 2 Traffic\n2Anomaly Categories in the DoTA DatasetLabel Anomaly Category \n\nST \nCollision with another vehicle that starts, stops, or is \nstationary \nAH \nCollision with another vehicle moving ahead or \nwaiting \nLA \nCollision with another vehicle moving laterally in \nsame direction \nOC \nCollision with another oncoming vehicle \nTC \nCollision with another vehicle that turns into or \ncrosses a road \nVP \nCollision between vehicle and pedestrian \nVO \nCollision with an obstacle in the roadway \nOO \nOut-of-control and leaving the roadway to the left or \nright \nUK \nUnknown \n\n\n\nTABLE 4 Video\n4Anomaly Detection Results for Each Type of Anomaly Class ST: collision with another vehicle that starts, stops, or is stationary; AH: ahead collision; LA: lateral collision; OC: oncoming collision; TC: turning or crossing collision; VP: vehicle-pedestrian collision; VO: vehicle-obstacle collision; OO: out-of-control; UK: unknown. Ego-involved and non-ego (*) anomalies shown separately. VO and OO are not shown because they do not contain anomalous traffic participants.Ego involved anomaly classes \nNon-ego involved anomaly classes \n\nST \nAH \nLA \nOC \nTC \nVP \nST* \nAH* \nLA* \nOC* \nTC* \nVP* \nVO* \nOO* \n\nIndividual Anomaly Class AUC: \nAnoPred [8] \n69.9 \n73.6 \n75.2 \n69.7 \n73.5 \n66.3 \n70.9 \n62.6 \n60.1 \n65.6 \n65.4 \n64.9 \n64.2 \n57.8 \nAnoPred [8]+Mask \n66.3 \n72.2 \n64.2 \n65.4 \n65.6 \n66.6 \n72.9 \n63.7 \n60.6 \n66.9 \n65.7 \n64.0 \n58.8 \n59.9 \nFOL-STD \n67.3 \n77.4 \n71.1 \n68.6 \n69.2 \n65.1 \n75.1 \n66.2 \n66.8 \n74.1 \n72.0 \n69.7 \n63.8 \n69.2 \nFOL-Ensemble \n73.3 \n81.2 \n74.0 \n73.4 \n75.1 \n70.1 \n77.5 \n69.8 \n68.1 \n76.7 \n73.9 \n71.2 \n65.2 \n69.6 \n\nIndividual Anomaly Class STAUC: \nAnoPred [8] \n37.4 \n31.5 \n32.8 \n34.3 \n33.6 \n24.9 \n25.9 \n15.0 \n12.5 \n13.0 \n20.9 \n14.0 \n8.2 \n8.8 \nAnoPred [8]+Mask \n51.8 \n51.9 \n45.1 \n50.3 \n47.5 \n41.0 \n45.3 \n31.1 \n33.8 \n42.5 \n40.3 \n25.3 \n22.9 \n33.8 \nFOL-STD \n47.4 \n55.6 \n46.3 \n52.2 \n47.2 \n26.6 \n45.1 \n33.6 \n38.5 \n46.9 \n39.3 \n25.6 \n29.0 \n44.4 \nFOL-Ensemble \n54.4 \n60.3 \n53.8 \n56.5 \n54.9 \n35.2 \n52.4 \n36.4 \n40.8 \n51.9 \n44.7 \n28.6 \n28.6 \n43.5 \n\n\n\nTABLE 3\n3Comparison of Video Anomaly Techniques on the DoTA Dataset, According to the AUC and STAUC MetricsMethod \nInput \nAUC \" STAUC \" \n\nConvAE [12] \nGray \n64.3 \n7.4 \nFlow \n66.3 \n7.9 \nConvLSTMAE [17] Gray \n53.8 \n12.7 \nFlow \n62.5 \n12.2 \nAnoPred [8] \nRGB \n67.5 \n24.4 \nMasked RGB \n64.8 \n42.1 \n\nFOL-IoU \nBox + Flow + Ego \n61.2 \n34.6 \nFOL-Mask \nBox + Flow + Ego \n64.0 \n35.0 \nFOL-STD \nBox \n66.7 \n40.9 \nBox + Ego \n67.8 \n42.1 \nBox + Flow \n69.1 \n43.1 \nBox + Flow + Ego \n69.7 \n43.7 \nFOL-Ensemble \nRGB + Box + Flow + Ego 73.0 \n48.5 \n\n\nTABLE 5\n5Video Action Recognition Per-Class and Mean Top-1 Accuracies on DoTA\n\nTABLE 6 Online\n6Video Action Detection Average Precisions on DoTA Ego involved anomaly classes Non-ego involved anomaly classes Method ST AH LA OC TC VP VO OO ST* AH* LA* OC* TC* VP* VO* OO* mAPFC \n2.5 13.9 10.6 6.2 16.3 0.8 1.2 21.0 0.6 \n2.9 \n3.0 \n0.6 \n8.0 \n1.2 \n0.7 \n7.6 \n9.9 \nLSTM \n0.6 19.9 15.1 9.2 25.3 2.4 0.6 34.3 0.6 \n3.8 \n5.0 \n1.5 11.0 1.2 \n0.5 \n13.3 12.9 \nEncoder-Decoder 0.5 20.1 15.6 10.4 28.1 2.9 0.7 39.9 0.8 \n3.7 \n7.4 \n2.5 14.7 1.2 \n0.5 \n13.2 14.5 \nTRN \n1.0 22.8 20.6 15.5 30.0 1.5 0.7 32.3 0.7 \n4.0 \n10.2 2.9 17.0 1.2 \n0.7 \n13.8 15.3 \n\n\nYAO ET AL.: DOTA: UNSUPERVISED DETECTION OF TRAFFIC ANOMALY IN DRIVING VIDEOS\n\" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.\nACKNOWLEDGMENTSThe views and conclusions contained in this paper are those of the authors and should not be interpreted as representing the official policies, either expressly or implied, of the U.S. Government or any sponsor.Ella Atkins (Senior Member, IEEE) received the BS and MS degrees in aeronautics and astronautics from MIT, and the MS and PhD degrees in computer science and engineering from the University of Michigan. She is currently a professor of aerospace engineering and robotics with the University of Michigan, where she directs the Autonomous Aerospace Systems Lab. She is the editorin-chief of the AIAA Journal of Aerospace Information Systems (JAIS) and a fellow of the American Institute of Aeronautics and Astronautics. She has served on multiple National Academy study panels, as a member of Aeronautics and Space Engineering Board, and pursues research in aerospace system contingency management, autonomy, and safety. His research interests include computer vision, machine learning, and data mining. He was the recipient of the National Science Foundation CAREER Award, two Google Faculty Research Awards, IU Trustees Teaching Award, Grant Thornton Fellowship, Luddy named professorship, and numerous best paper awards and nominations. He is an associate editor for IEEE TPAMI and IEEE TMM.\nAnticipating accidents in dashcam videos. F.-H Chan, Y.-T Chen, Y Xiang, M Sun, Proc. Asian Conf. Comput. Vis. Asian Conf. Comput. VisF.-H. Chan, Y.-T. Chen, Y. Xiang, and M. Sun, \"Anticipating acci- dents in dashcam videos,\" in Proc. Asian Conf. Comput. Vis., 2016, pp. 136-153.\n\nSpatio-temporal action graph networks. R Herzig, Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshop. IEEE/CVF Int. Conf. Comput. Vis. WorkshopR. Herzig et al., \"Spatio-temporal action graph networks,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshop, 2019, pp. 2347- 2356.\n\nDADA: A large-scale benchmark and model for driver attention prediction in accidental scenarios. J Fang, D Yan, J Qiao, J Xue, arXiv:1912.12148J. Fang, D. Yan, J. Qiao, and J. Xue, \"DADA: A large-scale bench- mark and model for driver attention prediction in accidental scenarios,\" 2019, arXiv:1912.12148.\n\nAVA: A video dataset of spatio-temporally localized atomic visual actions. C Gu, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitC. Gu et al., \"AVA: A video dataset of spatio-temporally localized atomic visual actions,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pat- tern Recognit., 2018, pp. 6047-6056.\n\nA better baseline for Ava. R Girdhar, J Carreira, C Doersch, A Zisserman, arXiv:1807.10066R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, \"A better baseline for Ava,\" 2018, arXiv: 1807.10066.\n\nTube convolutional neural network (T-CNN) for action detection in videos. R Hou, C Chen, M Shah, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisR. Hou, C. Chen, and M. Shah, \"Tube convolutional neural net- work (T-CNN) for action detection in videos,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 5823-5832.\n\nAsynchronous interaction aggregation for action detection. J Tang, J Xia, X Mu, B Pang, C Lu, arXiv:2004.074852020J. Tang, J. Xia, X. Mu, B. Pang, and C. Lu, \"Asynchronous interaction aggregation for action detection,\" 2020, arXiv: 2004.07485.\n\nFuture frame prediction for anomaly detection-a new baseline. W Liu, W Luo, D Lian, S Gao, Proc. IEEE/CVF Conf. Comput. Vis. IEEE/CVF Conf. Comput. VisW. Liu, W. Luo, D. Lian, and S. Gao, \"Future frame prediction for anomaly detection-a new baseline,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6536-6545.\n\nDriving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?. N Kalra, S M Paddock, Transportation Research Part A: Policy and Practice. Santa Monica, CA, USARAND CorporationN. Kalra and S. M. Paddock, \"Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?,\" in Transportation Research Part A: Policy and Practice, Santa Monica, CA, USA: RAND Corporation, 2016.\n\nA comprehensive review on deep learning-based methods for video anomaly detection. R Nayak, U C Pati, S K Das, Image Vis. Comput. 106Art. no. 104078R. Nayak, U. C. Pati, and S. K. Das, \"A comprehensive review on deep learning-based methods for video anomaly detection,\" Image Vis. Comput., vol. 106, 2020, Art. no. 104078.\n\nAbnormal event detection at 150 FPS in MATLAB. C Lu, J Shi, J Jia, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisC. Lu, J. Shi, and J. Jia, \"Abnormal event detection at 150 FPS in MATLAB,\" in Proc. IEEE Int. Conf. Comput. Vis., 2013, pp. 2720-2727.\n\nLearning temporal regularity in video sequences. M Hasan, J Choi, J Neumann, A K Roy-Chowdhury, L S Davis, Proc. nullM. Hasan, J. Choi, J. Neumann, A. K. Roy-Chowdhury , and L. S. Davis, \"Learning temporal regularity in video sequences,\" in Proc.\n\nIeee/Cvf, Conf, Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2016, pp. 733-742.\n\nA revisit of sparse coding based anomaly detection in stacked RNN framework. W Luo, W Liu, S Gao, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisW. Luo, W. Liu, and S. Gao, \"A revisit of sparse coding based anomaly detection in stacked RNN framework,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 341-349.\n\nLearning regularity in skeleton trajectories for anomaly detection in videos. R Morais, V Le, T Tran, B Saha, M Mansour, S Venkatesh, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitR. Morais, V. Le, T. Tran, B. Saha, M. Mansour, and S. Venkatesh, \"Learning regularity in skeleton trajectories for anomaly detection in videos,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 11988-11996.\n\nObjectcentric auto-encoders and dummy anomalies for abnormal event detection in video. R T Ionescu, F S Khan, M.-I Georgescu, L Shao, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitR. T. Ionescu, F. S. Khan, M.-I. Georgescu, and L. Shao, \"Object- centric auto-encoders and dummy anomalies for abnormal event detection in video,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 7834-7843.\n\nAnomaly detection in video using predictive convolutional long short-term memory networks. J R Medel, A Savakis, arXiv:1612.00390J. R. Medel and A. Savakis, \"Anomaly detection in video using predictive convolutional long short-term memory networks,\" 2016, arXiv:1612.00390.\n\nAbnormal event detection in videos using spatiotemporal autoencoder. Y S Chong, Y H Tay, Proc. Int. Symp. Neural Netw. Int. Symp. Neural NetwY. S. Chong and Y. H. Tay, \"Abnormal event detection in videos using spatiotemporal autoencoder,\" in Proc. Int. Symp. Neural Netw., 2017, pp. 189-196.\n\nRemembering history with convolutional LSTM for anomaly detection. W Luo, W Liu, S Gao, Proc. IEEE Int. Conf. Multimedia Expo. IEEE Int. Conf. Multimedia ExpoW. Luo, W. Liu, and S. Gao, \"Remembering history with convolu- tional LSTM for anomaly detection,\" in Proc. IEEE Int. Conf. Multi- media Expo, 2017, pp. 439-444.\n\nIGLAD-International harmonized indepth accident data. J Bakker, H Jeppsson, L Hannawald, F Spitzh\u20ac Uttl, A Longton, E Tomasch, Proc. 25th Int. Tech. Conf. Enhanced Saf. Veh. 25th Int. Tech. Conf. Enhanced Saf. VehJ. Bakker, H. Jeppsson, L. Hannawald, F. Spitzh\u20ac uttl, A. Long- ton, and E. Tomasch, \"IGLAD-International harmonized in- depth accident data,\" in Proc. 25th Int. Tech. Conf. Enhanced Saf. Veh., 2017, pp. 1-12.\n\nSlowfast networks for video recognition. C Feichtenhofer, H Fan, J Malik, K He, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisC. Feichtenhofer, H. Fan, J. Malik, and K. He, \"Slowfast networks for video recognition,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 6201-6210.\n\nTemporal recurrent networks for online action detection. M Xu, M Gao, Y.-T Chen, L S Davis, D Crandall, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisM. Xu, M. Gao, Y.-T. Chen, L. S. Davis, and D. Crandall, \"Temporal recurrent networks for online action detection,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 5531-5540.\n\nUnsupervised traffic accident detection in first-person videos. Y Yao, M Xu, Y Wang, D Crandall, E M Atkins, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. IEEE/RSJ Int. Conf. Intell. Robots SystY. Yao, M. Xu, Y. Wang, D. Crandall, and E. M. Atkins, \"Unsupervised traffic accident detection in first-person videos,\" in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2019, pp. 273-280.\n\nAnomaly detection and localization in crowded scenes. W Li, V Mahadevan, N Vasconcelos, IEEE Trans. Pattern Anal. Mach. Intell. 361W. Li, V. Mahadevan, and N. Vasconcelos, \"Anomaly detection and localization in crowded scenes,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1, pp. 18-32, Jan. 2014.\n\nReal-world anomaly detection in surveillance videos. W Sultani, C Chen, M Shah, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitW. Sultani, C. Chen, and M. Shah, \"Real-world anomaly detection in surveillance videos,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pat- tern Recognit., 2018, pp. 6479-6488s.\n\nBdDD100K: A diverse driving video database with scalable annotation tooling. F Yu, W Xian, Y Chen, F Liu, M Liao, V Madhavan, T Darrell, arXiv:1805.04687F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Dar- rell, \"BdDD100K: A diverse driving video database with scalable annotation tooling,\" 2018, arXiv: 1805.04687.\n\nUncertainty-based traffic accident anticipation with spatio-temporal relational learning. W Bao, Q Yu, Y Kong, Proc. 28th ACM Int. Conf. Multimedia, 2020. 28th ACM Int. Conf. Multimedia, 2020W. Bao, Q. Yu, and Y. Kong, \"Uncertainty-based traffic accident anticipation with spatio-temporal relational learning,\" in Proc. 28th ACM Int. Conf. Multimedia, 2020, pp. 2682-2690.\n\nMemorizing normality to detect anomaly, Memoryaugmented deep autoencoder for unsupervised anomaly detection. D Gong, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisD. Gong et al., \"Memorizing normality to detect anomaly, Memory- augmented deep autoencoder for unsupervised anomaly detection,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 1705-1714.\n\nGODS: Generalized one-class discriminative subspaces for anomaly detection. J Wang, A Cherian, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisJ. Wang and A. Cherian, \"GODS: Generalized one-class discrimi- native subspaces for anomaly detection,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 8200-8210.\n\nSocial LSTM: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitA. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei , and S. Savarese, \"Social LSTM: Human trajectory prediction in crowded spaces,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2016, pp. 961-971.\n\nSocial GAN: Socially acceptable trajectories with generative adversarial networks. A Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi, Proc. IEEE/CVF Conf. Comput. Vis. IEEE/CVF Conf. Comput. VisA. Gupta, J. Johnson, L. Fei-Fei , S. Savarese, and A. Alahi, \"Social GAN: Socially acceptable trajectories with generative adversarial networks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2255-2264.\n\nDesire: Distant future prediction in dynamic scenes with interacting agents. N Lee, W Choi, P Vernaza, C B Choy, P H Torr, M Chandraker, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitN. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. Torr, and M. Chan- draker, \"Desire: Distant future prediction in dynamic scenes with interacting agents,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2017, pp. 2165-2174.\n\nMulti-modal trajectory prediction of surrounding vehicles with maneuver based LSTMs. N Deo, M M Trivedi, Proc. IEEE Intell. Veh. Symp. IEEE Intell. Veh. SympN. Deo and M. M. Trivedi, \"Multi-modal trajectory prediction of surrounding vehicles with maneuver based LSTMs,\" in Proc. IEEE Intell. Veh. Symp., 2018, pp. 1179-1184.\n\nGenerative modeling of multimodal multi-human behavior. B Ivanovic, E Schmerling, K Leung, M Pavone, Proc. IEEE/ RSJ Int. IEEE/ RSJ IntB. Ivanovic, E. Schmerling, K. Leung, and M. Pavone, \"Generative modeling of multimodal multi-human behavior,\" in Proc. IEEE/ RSJ Int. Conf. Intell. Robots Syst., 2018, pp. 3088-3095.\n\nDrogon: A causal reasoning framework for future trajectory forecast. C Choi, A Patil, S Malla, arXiv:1908.00024C. Choi, A. Patil, and S. Malla, \"Drogon: A causal reasoning framework for future trajectory forecast,\" 2019, arXiv: 1908.00024.\n\nThe trajectron: Probabilistic multiagent trajectory modeling with dynamic spatiotemporal graphs. B Ivanovic, M Pavone, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisB. Ivanovic and M. Pavone, \"The trajectron: Probabilistic multi- agent trajectory modeling with dynamic spatiotemporal graphs,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 2375-2384.\n\nTrajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control. T Salzmann, B Ivanovic, P Chakravarty, M Pavone, arXiv:2001.030932020T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, \"Trajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control,\" 2020, arXiv: 2001.03093.\n\nTNT: Target-driven trajectory prediction. H Zhao, arXiv:2008.082942020H. Zhao et al., \"TNT: Target-driven trajectory prediction,\" 2020, arXiv: 2008.08294.\n\nBiTraP: Bi-directional pedestrian trajectory prediction with multi-modal goal estimation. Y Yao, E Atkins, M Johnson-Roberson, R Vasudevan, X Du, IEEE Robot. Automat. Lett. 64Y. Yao, E. Atkins, M. Johnson-Roberson , R. Vasudevan, and X. Du, \"BiTraP: Bi-directional pedestrian trajectory prediction with multi-modal goal estimation,\" IEEE Robot. Automat. Lett., vol 6, no. 4, pp. 1463-1470, Apr. 2021.\n\nStepwise goal-driven networks for trajectory prediction. C Wang, Y Wang, M Xu, D Crandall, IEEE Robot. Automat. Lett. 72C. Wang, Y. Wang, M. Xu, and D. Crandall, \"Stepwise goal-driven networks for trajectory prediction,\" IEEE Robot. Automat. Lett., vol. 7, no. 2, pp. 2716-2723, Apr. 2022.\n\nFuture person localization in first-person videos. T Yagi, K Mangalam, R Yonetani, Y Sato, Proc. IEEE/CVF Conf. Comput. Vis. IEEE/CVF Conf. Comput. VisT. Yagi, K. Mangalam, R. Yonetani, and Y. Sato, \"Future person localization in first-person videos,\" in Proc. IEEE/CVF Conf. Com- put. Vis. Pattern Recognit., 2018, pp. 7593-7602.\n\nEgocentric vision-based future vehicle localization for intelligent driving assistance systems. Y Yao, M Xu, C Choi, D Crandall, E M Atkins, B Dariush, Proc. Int. Conf. Robot. Automat. Int. Conf. Robot. AutomatY. Yao, M. Xu, C. Choi, D. Crandall, E. M. Atkins, and B. Dariush, \"Egocentric vision-based future vehicle localization for intelligent driving assistance systems,\" in Proc. Int. Conf. Robot. Automat., 2019, pp. 9711-9717.\n\nTitan: Future forecast using action priors. S Malla, B Dariush, C Choi, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitS. Malla, B. Dariush, and C. Choi, \"Titan: Future forecast using action priors,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog- nit., 2020, pp. 11183-11193.\n\nPIE: A largescale dataset and models for pedestrian intention estimation and trajectory prediction. A Rasouli, I Kotseruba, T Kunic, J K Tsotsos, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisA. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, \"PIE: A large- scale dataset and models for pedestrian intention estimation and trajectory prediction,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 6261-6270.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Proc. 27th Int. Conf. Neural Inf. Process. Syst. 27th Int. Conf. Neural Inf. ess. SystK. Simonyan and A. Zisserman, \"Two-stream convolutional net- works for action recognition in videos,\" in Proc. 27th Int. Conf. Neural Inf. Process. Syst., 2014, pp. 568-576.\n\nTemporal segment networks: Towards good practices for deep action recognition. L Wang, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisL. Wang et al., \"Temporal segment networks: Towards good prac- tices for deep action recognition,\" in Proc. Eur. Conf. Comput. Vis., 2016, pp. 20-36.\n\nLearning spatiotemporal features with 3D convolutional networks. D Tran, L Bourdev, R Fergus, L Torresani, M Paluri, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisD. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \"Learning spatiotemporal features with 3D convolutional networks,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 4489-4497.\n\nQuo vadis, action recognition? A new model and the kinetics dataset. J Carreira, A Zisserman, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJ. Carreira and A. Zisserman, \"Quo vadis, action recognition? A new model and the kinetics dataset,\" in Proc. IEEE/CVF Conf. Com- put. Vis. Pattern Recognit., 2017, pp. 4724-4733.\n\nA closer look at spatiotemporal convolutions for action recognition. D Tran, H Wang, L Torresani, J Ray, Y Lecun, M Paluri, Proc. IEEE/CVF Conf. Comput. Vis. IEEE/CVF Conf. Comput. VisD. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, \"A closer look at spatiotemporal convolutions for action recog- nition,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6450-6459.\n\nOnline action detection. R De Geest, E Gavves, A Ghodrati, Z Li, C Snoek, T Tuytelaars, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisR. De Geest , E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuyte- laars, \"Online action detection,\" in Proc. Eur. Conf. Comput. Vis., 2016, pp. 269-284.\n\nREE: Reinforced encoderdecoder networks for action anticipation. J Gao, Z Yang, R Nevatia, Proc. Brit. Mach. Vis. Conf. Brit. Mach. Vis. ConfJ. Gao, Z. Yang, and R. Nevatia, \"REE: Reinforced encoder- decoder networks for action anticipation,\" in Proc. Brit. Mach. Vis. Conf., 2017.\n\nOnline detection of action start in untrimmed, streaming videos. Z Shou, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisZ. Shou et al., \"Online detection of action start in untrimmed, stream- ing videos,\" in Proc. Eur. Conf. Comput. Vis., 2018, pp. 534-551.\n\nStartNet: Online detection of action start in untrimmed videos. M Gao, M Xu, L S Davis, R Socher, C Xiong, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisM. Gao, M. Xu, L. S. Davis, R. Socher, and C. Xiong, \"StartNet: Online detection of action start in untrimmed videos,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 5541-5550.\n\nLearning to discriminate information for online action detection. H Eun, J Moon, J Park, C Jung, C Kim, Proc. IEEE/ CVF Conf. Comput. Vis. Pattern Recognit. IEEE/ CVF Conf. Comput. Vis. Pattern RecognitH. Eun, J. Moon, J. Park, C. Jung, and C. Kim, \"Learning to dis- criminate information for online action detection,\" in Proc. IEEE/ CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 806-815.\n\nWOAD: Weakly supervised online action detection in untrimmed videos. M Gao, Y Zhou, R Xu, R Socher, C Xiong, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitM. Gao, Y. Zhou, R. Xu, R. Socher, and C. Xiong, \"WOAD: Weakly supervised online action detection in untrimmed vid- eos,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1915-1923.\n\nTemporal filtering networks for online action detection. H Eun, J Moon, J Park, C Jung, C Kim, Pattern Recognit. 111Art. no. 107695H. Eun, J. Moon, J. Park, C. Jung, and C. Kim, \"Temporal filtering networks for online action detection,\" Pattern Recognit., vol. 111, 2021, Art. no. 107695.\n\nLong short-term transformer for online action detection. M Xu, Proc. Conf. Neural Inf. Process. Syst. Conf. Neural Inf. ess. SystM. Xu et al., \"Long short-term transformer for online action detection,\" in Proc. Conf. Neural Inf. Process. Syst., 2021.\n\nActor-context-actor relation network for spatio-temporal action localization. J Pan, S Chen, M Z Shou, Y Liu, J Shao, H Li, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitJ. Pan, S. Chen, M. Z. Shou, Y. Liu, J. Shao, and H. Li, \"Actor-con- text-actor relation network for spatio-temporal action local- ization,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 464-474.\n\nTuber: Tube-transformer for action detection. J Zhao, arXiv:2104.009692021J. Zhao et al., \"Tuber: Tube-transformer for action detection,\" 2021, arXiv:2104.00969.\n\nA review of video object detection: Datasets, metrics and methods. H Zhu, H Wei, B Li, X Yuan, N Kehtarnavaz, Appl. Sci. 107834H. Zhu, H. Wei, B. Li, X. Yuan, and N. Kehtarnavaz, \"A review of video object detection: Datasets, metrics and methods,\" Appl. Sci., vol. 10, 2020, Art. no. 7834.\n\nObject detection from video tubelets with convolutional neural networks. K Kang, W Ouyang, H Li, X Wang, Proc. nullK. Kang, W. Ouyang, H. Li, and X. Wang, \"Object detection from video tubelets with convolutional neural networks,\" in Proc.\n\nIeee/Cvf, Conf, Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2016, pp. 817-825.\n\nDeep feature flow for video recognition. X Zhu, Y Xiong, J Dai, L Yuan, Y Wei, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitX. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, \"Deep feature flow for video recognition,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pat- tern Recognit., 2017, pp. 4141-4150.\n\nFlow-guided feature aggregation for video object detection. X Zhu, Y Wang, J Dai, L Yuan, Y Wei, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisX. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, \"Flow-guided fea- ture aggregation for video object detection,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 408-417.\n\nTowards high performance video object detection. X Zhu, J Dai, L Yuan, Y Wei, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitX. Zhu, J. Dai, L. Yuan, and Y. Wei, \"Towards high performance video object detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pat- tern Recognit., 2018, pp. 7210-7218.\n\nDetect to track and track to detect. C Feichtenhofer, A Pinz, A Zisserman, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisC. Feichtenhofer, A. Pinz, and A. Zisserman, \"Detect to track and track to detect,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 3057-3065.\n\nFully motion-aware network for video object detection. S Wang, Y Zhou, J Yan, Z Deng, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisS. Wang, Y. Zhou, J. Yan, and Z. Deng, \"Fully motion-aware net- work for video object detection,\" in Proc. Eur. Conf. Comput. Vis., 2018, pp. 557-573.\n\nVideo object detection with an aligned spatialtemporal memory. F Xiao, Y J Lee, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisF. Xiao and Y. J. Lee, \"Video object detection with an aligned spatial- temporal memory,\" in Proc. Eur. Conf. Comput. Vis., 2018, pp. 494-510.\n\nMobile video object detection with temporally-aware feature maps. M Liu, M Zhu, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisM. Liu and M. Zhu, \"Mobile video object detection with tempo- rally-aware feature maps,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 5686-5695.\n\nLooking fast and slow: Memory-guided mobile video object detection. M Liu, M Zhu, M White, Y Li, D Kalenichenko, arXiv:1903.10172M. Liu, M. Zhu, M. White, Y. Li, and D. Kalenichenko, \"Looking fast and slow: Memory-guided mobile video object detection,\" 2019, arXiv:1903.10172.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n\nMemory enhanced globallocal aggregation for video object detection. Y Chen, Y Cao, H Hu, L Wang, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitY. Chen, Y. Cao, H. Hu, and L. Wang, \"Memory enhanced global- local aggregation for video object detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 10337-10346.\n\nMining inter-video proposal relations for video object detection. M Han, Y Wang, X Chang, Y Qiao, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisM. Han, Y. Wang, X. Chang, and Y. Qiao, \"Mining inter-video proposal relations for video object detection,\" in Proc. Eur. Conf. Comput. Vis., 2020, pp. 431-446.\n\nLong-term on-board prediction of people in traffic scenes under uncertainty. A Bhattacharyya, M Fritz, B Schiele, Proc. IEEE/ CVF Conf. Comput. Vis. IEEE/ CVF Conf. Comput. VisA. Bhattacharyya, M. Fritz, and B. Schiele, \"Long-term on-board pre- diction of people in traffic scenes under uncertainty,\" in Proc. IEEE/ CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 4194-4202.\n\nSimple online and realtime tracking with a deep association metric. N Wojke, A Bewley, D Paulus, Proc. IEEE Int. Conf. Image Process. IEEE Int. Conf. Image essN. Wojke, A. Bewley, and D. Paulus, \"Simple online and realtime tracking with a deep association metric,\" in Proc. IEEE Int. Conf. Image Process., 2017, pp. 3645-3649.\n\nAnomaly detection and localization in crowded scenes. W Li, V Mahadevan, N Vasconcelos, IEEE Trans. Pattern Anal. Mach. Intell. 361W. Li, V. Mahadevan, and N. Vasconcelos, \"Anomaly detection and localization in crowded scenes,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1, pp. 18-32, 2013.\n\nAnticipating traffic accidents with adaptive loss and large-scale incident db. T Suzuki, H Kataoka, Y Aoki, Y Satoh, Proc. nullT. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, \"Anticipating traffic accidents with adaptive loss and large-scale incident db,\" in Proc.\n\n. Ieee/Cvf, Conf, Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3521-3529.\n\nA survey of singlescene video anomaly detection. B Ramachandra, M Jones, R R Vatsavai, 10.1109/TPAMI.2020.3040591IEEE Trans. Pattern Anal. Mach. Intell., early access. B. Ramachandra, M. Jones, and R. R. Vatsavai, \"A survey of single- scene video anomaly detection,\" IEEE Trans. Pattern Anal. Mach. Intell., early access, Nov. 25, 2020, doi: 10.1109/TPAMI.2020.3040591.\n\nMVtec ADa comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitP. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, \"MVtec AD- a comprehensive real-world dataset for unsupervised anomaly detection,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 9584-9592.\n\nVideo action detection with relational dynamic-poselets. L Wang, Y Qiao, X Tang, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisL. Wang, Y. Qiao, and X. Tang, \"Video action detection with rela- tional dynamic-poselets,\" in Proc. Eur. Conf. Comput. Vis., 2014, pp. 565-580.\n\nAction localization in videos through context walk. K Soomro, H Idrees, M Shah, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisK. Soomro, H. Idrees, and M. Shah, \"Action localization in videos through context walk,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 3280-3288.\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitE. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, \"Flownet 2.0: Evolution of optical flow estimation with deep networks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1647-1655.\n\nMask R-CNN. K He, G Gkioxari, P , R Girshick, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisK. He, G. Gkioxari, P. Doll ar, and R. Girshick, \"Mask R-CNN,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2980-2988.\n\nThe Cityscapes dataset for semantic urban scene understanding. M Cordts, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitM. Cordts et al.\"The Cityscapes dataset for semantic urban scene understanding,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Rec- ognit., 2016, pp. 3213-3223.\n\nNeural networks for machine learning, lecture 6a: Overview of mini-batch gradient descent. G Hinton, N Srivastava, K Swersky, Cited On. 148G. Hinton, N. Srivastava, and K. Swersky, \"Neural networks for machine learning, lecture 6a: Overview of mini-batch gradient descent,\" Cited On, vol. 14, no. 8, p. 2, 2012.\n\nLarge-scale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitA. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, \"Large-scale video classification with convolutional neural networks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2014, pp. 1725-1732.\n\nThe kinetics human action video dataset. W Kay, arXiv:1705.06950W. Kay et al., \"The kinetics human action video dataset,\" 2017, arXiv: 1705.06950.\n", "annotations": {"author": "[{\"end\":88,\"start\":69},{\"end\":111,\"start\":89},{\"end\":134,\"start\":112},{\"end\":144,\"start\":135},{\"end\":169,\"start\":145},{\"end\":201,\"start\":170},{\"end\":231,\"start\":202}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":110,\"start\":106},{\"end\":133,\"start\":131},{\"end\":143,\"start\":141},{\"end\":168,\"start\":164},{\"end\":200,\"start\":194},{\"end\":230,\"start\":222}]", "author_first_name": "[{\"end\":83,\"start\":81},{\"end\":105,\"start\":101},{\"end\":130,\"start\":124},{\"end\":140,\"start\":135},{\"end\":163,\"start\":157},{\"end\":193,\"start\":189},{\"end\":219,\"start\":214},{\"end\":221,\"start\":220}]", "author_affiliation": null, "title": "[{\"end\":66,\"start\":1},{\"end\":297,\"start\":232}]", "venue": null, "abstract": "[{\"end\":1757,\"start\":436}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2681,\"start\":2678},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2686,\"start\":2683},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2691,\"start\":2688},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3156,\"start\":3153},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3161,\"start\":3158},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3166,\"start\":3163},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3171,\"start\":3168},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3496,\"start\":3493},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3685,\"start\":3682},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4410,\"start\":4406},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4495,\"start\":4492},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4501,\"start\":4497},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4507,\"start\":4503},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4513,\"start\":4509},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4519,\"start\":4515},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4525,\"start\":4521},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5790,\"start\":5787},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5796,\"start\":5792},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5802,\"start\":5798},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5808,\"start\":5804},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5814,\"start\":5810},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5820,\"start\":5816},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6957,\"start\":6953},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9075,\"start\":9071},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9088,\"start\":9084},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10418,\"start\":10414},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11044,\"start\":11040},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11062,\"start\":11058},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11084,\"start\":11081},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11223,\"start\":11219},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11365,\"start\":11362},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11554,\"start\":11550},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11693,\"start\":11690},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11790,\"start\":11787},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11852,\"start\":11848},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11927,\"start\":11923},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12509,\"start\":12505},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12675,\"start\":12671},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12681,\"start\":12677},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12687,\"start\":12683},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12750,\"start\":12746},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12826,\"start\":12823},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12944,\"start\":12940},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13043,\"start\":13039},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13201,\"start\":13197},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13346,\"start\":13342},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14211,\"start\":14207},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14217,\"start\":14213},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14245,\"start\":14241},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14251,\"start\":14247},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14257,\"start\":14253},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14263,\"start\":14259},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14269,\"start\":14265},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14275,\"start\":14271},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14301,\"start\":14297},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14307,\"start\":14303},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14313,\"start\":14309},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14570,\"start\":14566},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14727,\"start\":14723},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14918,\"start\":14914},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14942,\"start\":14938},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15106,\"start\":15102},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15431,\"start\":15427},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15472,\"start\":15468},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15525,\"start\":15521},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15630,\"start\":15626},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15718,\"start\":15714},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15786,\"start\":15782},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16038,\"start\":16034},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16044,\"start\":16040},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":16050,\"start\":16046},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16056,\"start\":16052},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16062,\"start\":16058},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16068,\"start\":16064},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16074,\"start\":16070},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16080,\"start\":16076},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16086,\"start\":16082},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16403,\"start\":16400},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16408,\"start\":16405},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16413,\"start\":16410},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16418,\"start\":16415},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16424,\"start\":16420},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16430,\"start\":16426},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16796,\"start\":16792},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17099,\"start\":17095},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":17466,\"start\":17462},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17472,\"start\":17468},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17478,\"start\":17474},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":17484,\"start\":17480},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":17490,\"start\":17486},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":17496,\"start\":17492},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":17502,\"start\":17498},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":17539,\"start\":17535},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":17545,\"start\":17541},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":17601,\"start\":17597},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":17677,\"start\":17673},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":17908,\"start\":17904},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17928,\"start\":17924},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17934,\"start\":17930},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":17940,\"start\":17936},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":17946,\"start\":17942},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":17952,\"start\":17948},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":17958,\"start\":17954},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":18113,\"start\":18109},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":18325,\"start\":18321},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19267,\"start\":19264},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20519,\"start\":20515},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20986,\"start\":20982},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21692,\"start\":21688},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21748,\"start\":21744},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21813,\"start\":21809},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22778,\"start\":22774},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":22784,\"start\":22780},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":23525,\"start\":23521},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":24553,\"start\":24549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25012,\"start\":25009},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25018,\"start\":25014},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25024,\"start\":25020},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25030,\"start\":25026},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":26587,\"start\":26583},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29650,\"start\":29647},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30193,\"start\":30190},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32744,\"start\":32741},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":32750,\"start\":32746},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33007,\"start\":33004},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33283,\"start\":33280},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33292,\"start\":33288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33854,\"start\":33850},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33965,\"start\":33962},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34425,\"start\":34421},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":35715,\"start\":35711},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":37666,\"start\":37662},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38870,\"start\":38866},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38876,\"start\":38872},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38882,\"start\":38878},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40256,\"start\":40253},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":41209,\"start\":41205},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":41215,\"start\":41211},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41252,\"start\":41249},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42427,\"start\":42424},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43843,\"start\":43839},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43860,\"start\":43856},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43877,\"start\":43874},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44102,\"start\":44098},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":44736,\"start\":44732},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44838,\"start\":44834},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":45268,\"start\":45265},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":45788,\"start\":45784},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":45819,\"start\":45815},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":46075,\"start\":46071},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46081,\"start\":46077},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":46557,\"start\":46553},{\"end\":49366,\"start\":49352},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":52154,\"start\":52150},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":52164,\"start\":52160},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":52174,\"start\":52170},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":52184,\"start\":52180},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":52198,\"start\":52194},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":52208,\"start\":52204},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":52228,\"start\":52224},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":52515,\"start\":52511},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":52541,\"start\":52537},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":52710,\"start\":52706},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":54542,\"start\":54538}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56859,\"start\":56474},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57066,\"start\":56860},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57381,\"start\":57067},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57417,\"start\":57382},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57574,\"start\":57418},{\"attributes\":{\"id\":\"fig_5\"},\"end\":57724,\"start\":57575},{\"attributes\":{\"id\":\"fig_6\"},\"end\":57797,\"start\":57725},{\"attributes\":{\"id\":\"fig_7\"},\"end\":58082,\"start\":57798},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58662,\"start\":58083},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59323,\"start\":58663},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59901,\"start\":59324},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61364,\"start\":59902},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61889,\"start\":61365},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":61968,\"start\":61890},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62522,\"start\":61969}]", "paragraph": "[{\"end\":2577,\"start\":1773},{\"end\":3756,\"start\":2579},{\"end\":4861,\"start\":3758},{\"end\":5751,\"start\":4863},{\"end\":6774,\"start\":5753},{\"end\":8622,\"start\":6776},{\"end\":9305,\"start\":8624},{\"end\":10904,\"start\":9307},{\"end\":12353,\"start\":10921},{\"end\":13905,\"start\":12355},{\"end\":15194,\"start\":13907},{\"end\":16278,\"start\":15196},{\"end\":17063,\"start\":16280},{\"end\":18466,\"start\":17065},{\"end\":18946,\"start\":18468},{\"end\":20057,\"start\":18957},{\"end\":20520,\"start\":20059},{\"end\":20909,\"start\":20522},{\"end\":21995,\"start\":20972},{\"end\":22646,\"start\":21997},{\"end\":23465,\"start\":22665},{\"end\":24825,\"start\":23484},{\"end\":25476,\"start\":24900},{\"end\":26186,\"start\":25512},{\"end\":26504,\"start\":26252},{\"end\":27361,\"start\":26536},{\"end\":27984,\"start\":27466},{\"end\":28634,\"start\":28023},{\"end\":28893,\"start\":28636},{\"end\":28962,\"start\":28915},{\"end\":29703,\"start\":28964},{\"end\":30914,\"start\":29747},{\"end\":33388,\"start\":30962},{\"end\":34315,\"start\":33390},{\"end\":35188,\"start\":34317},{\"end\":35835,\"start\":35257},{\"end\":37086,\"start\":35837},{\"end\":37325,\"start\":37139},{\"end\":38379,\"start\":37366},{\"end\":38845,\"start\":38412},{\"end\":39254,\"start\":38847},{\"end\":41066,\"start\":39331},{\"end\":42215,\"start\":41129},{\"end\":42942,\"start\":42217},{\"end\":43073,\"start\":42944},{\"end\":43724,\"start\":43089},{\"end\":44088,\"start\":43788},{\"end\":44820,\"start\":44090},{\"end\":45254,\"start\":44822},{\"end\":46219,\"start\":45256},{\"end\":46799,\"start\":46221},{\"end\":48965,\"start\":46811},{\"end\":50490,\"start\":48967},{\"end\":51802,\"start\":50492},{\"end\":53970,\"start\":51845},{\"end\":55205,\"start\":54012},{\"end\":56473,\"start\":55236}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":26251,\"start\":26187},{\"attributes\":{\"id\":\"formula_1\"},\"end\":27427,\"start\":27362},{\"attributes\":{\"id\":\"formula_2\"},\"end\":27465,\"start\":27427},{\"attributes\":{\"id\":\"formula_3\"},\"end\":28914,\"start\":28894},{\"attributes\":{\"id\":\"formula_5\"},\"end\":37365,\"start\":37326},{\"attributes\":{\"id\":\"formula_6\"},\"end\":38411,\"start\":38380},{\"attributes\":{\"id\":\"formula_7\"},\"end\":39330,\"start\":39255}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31809,\"start\":31802},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":34406,\"start\":34399},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":46856,\"start\":46849},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47631,\"start\":47624},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":48462,\"start\":48455},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":48993,\"start\":48986},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":52766,\"start\":52759},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":54551,\"start\":54544},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":56396,\"start\":56389},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":56440,\"start\":56433}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1771,\"start\":1759},{\"attributes\":{\"n\":\"2\"},\"end\":10919,\"start\":10907},{\"attributes\":{\"n\":\"3\"},\"end\":18955,\"start\":18949},{\"attributes\":{\"n\":\"3.1\"},\"end\":20944,\"start\":20912},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":20970,\"start\":20947},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":22663,\"start\":22649},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":23482,\"start\":23468},{\"end\":24860,\"start\":24828},{\"attributes\":{\"n\":\"3.2\"},\"end\":24898,\"start\":24863},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":25510,\"start\":25479},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":26534,\"start\":26507},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":28021,\"start\":27987},{\"attributes\":{\"n\":\"3.3\"},\"end\":29745,\"start\":29706},{\"attributes\":{\"n\":\"4\"},\"end\":30960,\"start\":30917},{\"attributes\":{\"n\":\"5\"},\"end\":35218,\"start\":35191},{\"attributes\":{\"n\":\"5.1\"},\"end\":35255,\"start\":35221},{\"attributes\":{\"n\":\"5.2\"},\"end\":37137,\"start\":37089},{\"attributes\":{\"n\":\"5.3\"},\"end\":41127,\"start\":41069},{\"attributes\":{\"n\":\"6\"},\"end\":43087,\"start\":43076},{\"attributes\":{\"n\":\"6.1\"},\"end\":43764,\"start\":43727},{\"attributes\":{\"n\":\"6.1.1\"},\"end\":43786,\"start\":43767},{\"attributes\":{\"n\":\"6.1.2\"},\"end\":46809,\"start\":46802},{\"attributes\":{\"n\":\"6.2\"},\"end\":51843,\"start\":51805},{\"attributes\":{\"n\":\"6.3\"},\"end\":54010,\"start\":53973},{\"attributes\":{\"n\":\"7\"},\"end\":55234,\"start\":55208},{\"end\":56483,\"start\":56475},{\"end\":56869,\"start\":56861},{\"end\":57076,\"start\":57068},{\"end\":57391,\"start\":57383},{\"end\":57427,\"start\":57419},{\"end\":57584,\"start\":57576},{\"end\":57734,\"start\":57726},{\"end\":57807,\"start\":57799},{\"end\":58671,\"start\":58664},{\"end\":59340,\"start\":59325},{\"end\":59916,\"start\":59903},{\"end\":61373,\"start\":61366},{\"end\":61898,\"start\":61891},{\"end\":61984,\"start\":61970}]", "table": "[{\"end\":58662,\"start\":58528},{\"end\":59323,\"start\":58727},{\"end\":59901,\"start\":59380},{\"end\":61364,\"start\":60390},{\"end\":61889,\"start\":61473},{\"end\":62522,\"start\":62164}]", "figure_caption": "[{\"end\":56859,\"start\":56485},{\"end\":57066,\"start\":56871},{\"end\":57381,\"start\":57078},{\"end\":57417,\"start\":57393},{\"end\":57574,\"start\":57429},{\"end\":57724,\"start\":57586},{\"end\":57797,\"start\":57736},{\"end\":58082,\"start\":57809},{\"end\":58528,\"start\":58085},{\"end\":58727,\"start\":58673},{\"end\":59380,\"start\":59342},{\"end\":60390,\"start\":59918},{\"end\":61473,\"start\":61375},{\"end\":61968,\"start\":61900},{\"end\":62164,\"start\":61986}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22025,\"start\":22019},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32558,\"start\":32551},{\"end\":34654,\"start\":34648},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35187,\"start\":35180},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35951,\"start\":35945},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36333,\"start\":36324},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36492,\"start\":36484},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38031,\"start\":38024},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39083,\"start\":39076},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39571,\"start\":39564},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40696,\"start\":40689},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40824,\"start\":40817},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":44288,\"start\":44279},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45117,\"start\":45108},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45409,\"start\":45400},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":50520,\"start\":50513},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":51326,\"start\":51319},{\"end\":53458,\"start\":53450}]", "bib_author_first_name": "[{\"end\":64085,\"start\":64081},{\"end\":64096,\"start\":64092},{\"end\":64104,\"start\":64103},{\"end\":64113,\"start\":64112},{\"end\":64360,\"start\":64359},{\"end\":64691,\"start\":64690},{\"end\":64699,\"start\":64698},{\"end\":64706,\"start\":64705},{\"end\":64714,\"start\":64713},{\"end\":64976,\"start\":64975},{\"end\":65276,\"start\":65275},{\"end\":65287,\"start\":65286},{\"end\":65299,\"start\":65298},{\"end\":65310,\"start\":65309},{\"end\":65523,\"start\":65522},{\"end\":65530,\"start\":65529},{\"end\":65538,\"start\":65537},{\"end\":65836,\"start\":65835},{\"end\":65844,\"start\":65843},{\"end\":65851,\"start\":65850},{\"end\":65857,\"start\":65856},{\"end\":65865,\"start\":65864},{\"end\":66084,\"start\":66083},{\"end\":66091,\"start\":66090},{\"end\":66098,\"start\":66097},{\"end\":66106,\"start\":66105},{\"end\":66459,\"start\":66458},{\"end\":66468,\"start\":66467},{\"end\":66470,\"start\":66469},{\"end\":66895,\"start\":66894},{\"end\":66904,\"start\":66903},{\"end\":66906,\"start\":66905},{\"end\":66914,\"start\":66913},{\"end\":66916,\"start\":66915},{\"end\":67183,\"start\":67182},{\"end\":67189,\"start\":67188},{\"end\":67196,\"start\":67195},{\"end\":67451,\"start\":67450},{\"end\":67460,\"start\":67459},{\"end\":67468,\"start\":67467},{\"end\":67479,\"start\":67478},{\"end\":67481,\"start\":67480},{\"end\":67498,\"start\":67497},{\"end\":67500,\"start\":67499},{\"end\":67841,\"start\":67840},{\"end\":67848,\"start\":67847},{\"end\":67855,\"start\":67854},{\"end\":68168,\"start\":68167},{\"end\":68178,\"start\":68177},{\"end\":68184,\"start\":68183},{\"end\":68192,\"start\":68191},{\"end\":68200,\"start\":68199},{\"end\":68211,\"start\":68210},{\"end\":68633,\"start\":68632},{\"end\":68635,\"start\":68634},{\"end\":68646,\"start\":68645},{\"end\":68648,\"start\":68647},{\"end\":68659,\"start\":68655},{\"end\":68672,\"start\":68671},{\"end\":69093,\"start\":69092},{\"end\":69095,\"start\":69094},{\"end\":69104,\"start\":69103},{\"end\":69346,\"start\":69345},{\"end\":69348,\"start\":69347},{\"end\":69357,\"start\":69356},{\"end\":69359,\"start\":69358},{\"end\":69637,\"start\":69636},{\"end\":69644,\"start\":69643},{\"end\":69651,\"start\":69650},{\"end\":69945,\"start\":69944},{\"end\":69955,\"start\":69954},{\"end\":69967,\"start\":69966},{\"end\":69980,\"start\":69979},{\"end\":69996,\"start\":69995},{\"end\":70007,\"start\":70006},{\"end\":70356,\"start\":70355},{\"end\":70373,\"start\":70372},{\"end\":70380,\"start\":70379},{\"end\":70389,\"start\":70388},{\"end\":70665,\"start\":70664},{\"end\":70671,\"start\":70670},{\"end\":70681,\"start\":70677},{\"end\":70689,\"start\":70688},{\"end\":70691,\"start\":70690},{\"end\":70700,\"start\":70699},{\"end\":71015,\"start\":71014},{\"end\":71022,\"start\":71021},{\"end\":71028,\"start\":71027},{\"end\":71036,\"start\":71035},{\"end\":71048,\"start\":71047},{\"end\":71050,\"start\":71049},{\"end\":71392,\"start\":71391},{\"end\":71398,\"start\":71397},{\"end\":71411,\"start\":71410},{\"end\":71699,\"start\":71698},{\"end\":71710,\"start\":71709},{\"end\":71718,\"start\":71717},{\"end\":72069,\"start\":72068},{\"end\":72075,\"start\":72074},{\"end\":72083,\"start\":72082},{\"end\":72091,\"start\":72090},{\"end\":72098,\"start\":72097},{\"end\":72106,\"start\":72105},{\"end\":72118,\"start\":72117},{\"end\":72413,\"start\":72412},{\"end\":72420,\"start\":72419},{\"end\":72426,\"start\":72425},{\"end\":72806,\"start\":72805},{\"end\":73142,\"start\":73141},{\"end\":73150,\"start\":73149},{\"end\":73448,\"start\":73447},{\"end\":73457,\"start\":73456},{\"end\":73465,\"start\":73464},{\"end\":73479,\"start\":73478},{\"end\":73492,\"start\":73491},{\"end\":73503,\"start\":73502},{\"end\":73910,\"start\":73909},{\"end\":73919,\"start\":73918},{\"end\":73930,\"start\":73929},{\"end\":73941,\"start\":73940},{\"end\":73953,\"start\":73952},{\"end\":74324,\"start\":74323},{\"end\":74331,\"start\":74330},{\"end\":74339,\"start\":74338},{\"end\":74350,\"start\":74349},{\"end\":74352,\"start\":74351},{\"end\":74360,\"start\":74359},{\"end\":74362,\"start\":74361},{\"end\":74370,\"start\":74369},{\"end\":74796,\"start\":74795},{\"end\":74803,\"start\":74802},{\"end\":74805,\"start\":74804},{\"end\":75093,\"start\":75092},{\"end\":75105,\"start\":75104},{\"end\":75119,\"start\":75118},{\"end\":75128,\"start\":75127},{\"end\":75426,\"start\":75425},{\"end\":75434,\"start\":75433},{\"end\":75443,\"start\":75442},{\"end\":75695,\"start\":75694},{\"end\":75707,\"start\":75706},{\"end\":76065,\"start\":76064},{\"end\":76077,\"start\":76076},{\"end\":76089,\"start\":76088},{\"end\":76104,\"start\":76103},{\"end\":76358,\"start\":76357},{\"end\":76562,\"start\":76561},{\"end\":76569,\"start\":76568},{\"end\":76579,\"start\":76578},{\"end\":76599,\"start\":76598},{\"end\":76612,\"start\":76611},{\"end\":76931,\"start\":76930},{\"end\":76939,\"start\":76938},{\"end\":76947,\"start\":76946},{\"end\":76953,\"start\":76952},{\"end\":77216,\"start\":77215},{\"end\":77224,\"start\":77223},{\"end\":77236,\"start\":77235},{\"end\":77248,\"start\":77247},{\"end\":77593,\"start\":77592},{\"end\":77600,\"start\":77599},{\"end\":77606,\"start\":77605},{\"end\":77614,\"start\":77613},{\"end\":77626,\"start\":77625},{\"end\":77628,\"start\":77627},{\"end\":77638,\"start\":77637},{\"end\":77975,\"start\":77974},{\"end\":77984,\"start\":77983},{\"end\":77995,\"start\":77994},{\"end\":78362,\"start\":78361},{\"end\":78373,\"start\":78372},{\"end\":78386,\"start\":78385},{\"end\":78395,\"start\":78394},{\"end\":78397,\"start\":78396},{\"end\":78758,\"start\":78757},{\"end\":78770,\"start\":78769},{\"end\":79123,\"start\":79122},{\"end\":79399,\"start\":79398},{\"end\":79407,\"start\":79406},{\"end\":79418,\"start\":79417},{\"end\":79428,\"start\":79427},{\"end\":79441,\"start\":79440},{\"end\":79771,\"start\":79770},{\"end\":79783,\"start\":79782},{\"end\":80142,\"start\":80141},{\"end\":80150,\"start\":80149},{\"end\":80158,\"start\":80157},{\"end\":80171,\"start\":80170},{\"end\":80178,\"start\":80177},{\"end\":80187,\"start\":80186},{\"end\":80498,\"start\":80497},{\"end\":80510,\"start\":80509},{\"end\":80520,\"start\":80519},{\"end\":80532,\"start\":80531},{\"end\":80538,\"start\":80537},{\"end\":80547,\"start\":80546},{\"end\":80835,\"start\":80834},{\"end\":80842,\"start\":80841},{\"end\":80850,\"start\":80849},{\"end\":81118,\"start\":81117},{\"end\":81381,\"start\":81380},{\"end\":81388,\"start\":81387},{\"end\":81394,\"start\":81393},{\"end\":81396,\"start\":81395},{\"end\":81405,\"start\":81404},{\"end\":81415,\"start\":81414},{\"end\":81732,\"start\":81731},{\"end\":81739,\"start\":81738},{\"end\":81747,\"start\":81746},{\"end\":81755,\"start\":81754},{\"end\":81763,\"start\":81762},{\"end\":82131,\"start\":82130},{\"end\":82138,\"start\":82137},{\"end\":82146,\"start\":82145},{\"end\":82152,\"start\":82151},{\"end\":82162,\"start\":82161},{\"end\":82524,\"start\":82523},{\"end\":82531,\"start\":82530},{\"end\":82539,\"start\":82538},{\"end\":82547,\"start\":82546},{\"end\":82555,\"start\":82554},{\"end\":82814,\"start\":82813},{\"end\":83087,\"start\":83086},{\"end\":83094,\"start\":83093},{\"end\":83102,\"start\":83101},{\"end\":83104,\"start\":83103},{\"end\":83112,\"start\":83111},{\"end\":83119,\"start\":83118},{\"end\":83127,\"start\":83126},{\"end\":83492,\"start\":83491},{\"end\":83676,\"start\":83675},{\"end\":83683,\"start\":83682},{\"end\":83690,\"start\":83689},{\"end\":83696,\"start\":83695},{\"end\":83704,\"start\":83703},{\"end\":83973,\"start\":83972},{\"end\":83981,\"start\":83980},{\"end\":83991,\"start\":83990},{\"end\":83997,\"start\":83996},{\"end\":84295,\"start\":84294},{\"end\":84302,\"start\":84301},{\"end\":84311,\"start\":84310},{\"end\":84318,\"start\":84317},{\"end\":84326,\"start\":84325},{\"end\":84659,\"start\":84658},{\"end\":84666,\"start\":84665},{\"end\":84674,\"start\":84673},{\"end\":84681,\"start\":84680},{\"end\":84689,\"start\":84688},{\"end\":84976,\"start\":84975},{\"end\":84983,\"start\":84982},{\"end\":84990,\"start\":84989},{\"end\":84998,\"start\":84997},{\"end\":85306,\"start\":85305},{\"end\":85323,\"start\":85322},{\"end\":85331,\"start\":85330},{\"end\":85606,\"start\":85605},{\"end\":85614,\"start\":85613},{\"end\":85622,\"start\":85621},{\"end\":85629,\"start\":85628},{\"end\":85904,\"start\":85903},{\"end\":85912,\"start\":85911},{\"end\":85914,\"start\":85913},{\"end\":86183,\"start\":86182},{\"end\":86190,\"start\":86189},{\"end\":86480,\"start\":86479},{\"end\":86487,\"start\":86486},{\"end\":86494,\"start\":86493},{\"end\":86503,\"start\":86502},{\"end\":86509,\"start\":86508},{\"end\":86714,\"start\":86713},{\"end\":86728,\"start\":86727},{\"end\":86950,\"start\":86949},{\"end\":86958,\"start\":86957},{\"end\":86965,\"start\":86964},{\"end\":86971,\"start\":86970},{\"end\":87330,\"start\":87329},{\"end\":87337,\"start\":87336},{\"end\":87345,\"start\":87344},{\"end\":87354,\"start\":87353},{\"end\":87653,\"start\":87652},{\"end\":87670,\"start\":87669},{\"end\":87679,\"start\":87678},{\"end\":88024,\"start\":88023},{\"end\":88033,\"start\":88032},{\"end\":88043,\"start\":88042},{\"end\":88338,\"start\":88337},{\"end\":88344,\"start\":88343},{\"end\":88357,\"start\":88356},{\"end\":88666,\"start\":88665},{\"end\":88676,\"start\":88675},{\"end\":88687,\"start\":88686},{\"end\":88695,\"start\":88694},{\"end\":89018,\"start\":89017},{\"end\":89033,\"start\":89032},{\"end\":89042,\"start\":89041},{\"end\":89044,\"start\":89043},{\"end\":89419,\"start\":89418},{\"end\":89431,\"start\":89430},{\"end\":89441,\"start\":89440},{\"end\":89455,\"start\":89454},{\"end\":89833,\"start\":89832},{\"end\":89841,\"start\":89840},{\"end\":89849,\"start\":89848},{\"end\":90107,\"start\":90106},{\"end\":90117,\"start\":90116},{\"end\":90127,\"start\":90126},{\"end\":90417,\"start\":90416},{\"end\":90424,\"start\":90423},{\"end\":90433,\"start\":90432},{\"end\":90443,\"start\":90442},{\"end\":90453,\"start\":90452},{\"end\":90468,\"start\":90467},{\"end\":90803,\"start\":90802},{\"end\":90809,\"start\":90808},{\"end\":90821,\"start\":90820},{\"end\":90825,\"start\":90824},{\"end\":91086,\"start\":91085},{\"end\":91444,\"start\":91443},{\"end\":91454,\"start\":91453},{\"end\":91468,\"start\":91467},{\"end\":91735,\"start\":91734},{\"end\":91747,\"start\":91746},{\"end\":91759,\"start\":91758},{\"end\":91769,\"start\":91768},{\"end\":91778,\"start\":91777},{\"end\":91792,\"start\":91791},{\"end\":92167,\"start\":92166}]", "bib_author_last_name": "[{\"end\":64090,\"start\":64086},{\"end\":64101,\"start\":64097},{\"end\":64110,\"start\":64105},{\"end\":64117,\"start\":64114},{\"end\":64367,\"start\":64361},{\"end\":64696,\"start\":64692},{\"end\":64703,\"start\":64700},{\"end\":64711,\"start\":64707},{\"end\":64718,\"start\":64715},{\"end\":64979,\"start\":64977},{\"end\":65284,\"start\":65277},{\"end\":65296,\"start\":65288},{\"end\":65307,\"start\":65300},{\"end\":65320,\"start\":65311},{\"end\":65527,\"start\":65524},{\"end\":65535,\"start\":65531},{\"end\":65543,\"start\":65539},{\"end\":65841,\"start\":65837},{\"end\":65848,\"start\":65845},{\"end\":65854,\"start\":65852},{\"end\":65862,\"start\":65858},{\"end\":65868,\"start\":65866},{\"end\":66088,\"start\":66085},{\"end\":66095,\"start\":66092},{\"end\":66103,\"start\":66099},{\"end\":66110,\"start\":66107},{\"end\":66465,\"start\":66460},{\"end\":66478,\"start\":66471},{\"end\":66901,\"start\":66896},{\"end\":66911,\"start\":66907},{\"end\":66920,\"start\":66917},{\"end\":67186,\"start\":67184},{\"end\":67193,\"start\":67190},{\"end\":67200,\"start\":67197},{\"end\":67457,\"start\":67452},{\"end\":67465,\"start\":67461},{\"end\":67476,\"start\":67469},{\"end\":67495,\"start\":67482},{\"end\":67506,\"start\":67501},{\"end\":67657,\"start\":67649},{\"end\":67663,\"start\":67659},{\"end\":67845,\"start\":67842},{\"end\":67852,\"start\":67849},{\"end\":67859,\"start\":67856},{\"end\":68175,\"start\":68169},{\"end\":68181,\"start\":68179},{\"end\":68189,\"start\":68185},{\"end\":68197,\"start\":68193},{\"end\":68208,\"start\":68201},{\"end\":68221,\"start\":68212},{\"end\":68643,\"start\":68636},{\"end\":68653,\"start\":68649},{\"end\":68669,\"start\":68660},{\"end\":68677,\"start\":68673},{\"end\":69101,\"start\":69096},{\"end\":69112,\"start\":69105},{\"end\":69354,\"start\":69349},{\"end\":69363,\"start\":69360},{\"end\":69641,\"start\":69638},{\"end\":69648,\"start\":69645},{\"end\":69655,\"start\":69652},{\"end\":69952,\"start\":69946},{\"end\":69964,\"start\":69956},{\"end\":69977,\"start\":69968},{\"end\":69993,\"start\":69981},{\"end\":70004,\"start\":69997},{\"end\":70015,\"start\":70008},{\"end\":70370,\"start\":70357},{\"end\":70377,\"start\":70374},{\"end\":70386,\"start\":70381},{\"end\":70392,\"start\":70390},{\"end\":70668,\"start\":70666},{\"end\":70675,\"start\":70672},{\"end\":70686,\"start\":70682},{\"end\":70697,\"start\":70692},{\"end\":70709,\"start\":70701},{\"end\":71019,\"start\":71016},{\"end\":71025,\"start\":71023},{\"end\":71033,\"start\":71029},{\"end\":71045,\"start\":71037},{\"end\":71057,\"start\":71051},{\"end\":71395,\"start\":71393},{\"end\":71408,\"start\":71399},{\"end\":71423,\"start\":71412},{\"end\":71707,\"start\":71700},{\"end\":71715,\"start\":71711},{\"end\":71723,\"start\":71719},{\"end\":72072,\"start\":72070},{\"end\":72080,\"start\":72076},{\"end\":72088,\"start\":72084},{\"end\":72095,\"start\":72092},{\"end\":72103,\"start\":72099},{\"end\":72115,\"start\":72107},{\"end\":72126,\"start\":72119},{\"end\":72417,\"start\":72414},{\"end\":72423,\"start\":72421},{\"end\":72431,\"start\":72427},{\"end\":72811,\"start\":72807},{\"end\":73147,\"start\":73143},{\"end\":73158,\"start\":73151},{\"end\":73454,\"start\":73449},{\"end\":73462,\"start\":73458},{\"end\":73476,\"start\":73466},{\"end\":73489,\"start\":73480},{\"end\":73500,\"start\":73493},{\"end\":73512,\"start\":73504},{\"end\":73916,\"start\":73911},{\"end\":73927,\"start\":73920},{\"end\":73938,\"start\":73931},{\"end\":73950,\"start\":73942},{\"end\":73959,\"start\":73954},{\"end\":74328,\"start\":74325},{\"end\":74336,\"start\":74332},{\"end\":74347,\"start\":74340},{\"end\":74357,\"start\":74353},{\"end\":74367,\"start\":74363},{\"end\":74381,\"start\":74371},{\"end\":74800,\"start\":74797},{\"end\":74813,\"start\":74806},{\"end\":75102,\"start\":75094},{\"end\":75116,\"start\":75106},{\"end\":75125,\"start\":75120},{\"end\":75135,\"start\":75129},{\"end\":75431,\"start\":75427},{\"end\":75440,\"start\":75435},{\"end\":75449,\"start\":75444},{\"end\":75704,\"start\":75696},{\"end\":75714,\"start\":75708},{\"end\":76074,\"start\":76066},{\"end\":76086,\"start\":76078},{\"end\":76101,\"start\":76090},{\"end\":76111,\"start\":76105},{\"end\":76363,\"start\":76359},{\"end\":76566,\"start\":76563},{\"end\":76576,\"start\":76570},{\"end\":76596,\"start\":76580},{\"end\":76609,\"start\":76600},{\"end\":76615,\"start\":76613},{\"end\":76936,\"start\":76932},{\"end\":76944,\"start\":76940},{\"end\":76950,\"start\":76948},{\"end\":76962,\"start\":76954},{\"end\":77221,\"start\":77217},{\"end\":77233,\"start\":77225},{\"end\":77245,\"start\":77237},{\"end\":77253,\"start\":77249},{\"end\":77597,\"start\":77594},{\"end\":77603,\"start\":77601},{\"end\":77611,\"start\":77607},{\"end\":77623,\"start\":77615},{\"end\":77635,\"start\":77629},{\"end\":77646,\"start\":77639},{\"end\":77981,\"start\":77976},{\"end\":77992,\"start\":77985},{\"end\":78000,\"start\":77996},{\"end\":78370,\"start\":78363},{\"end\":78383,\"start\":78374},{\"end\":78392,\"start\":78387},{\"end\":78405,\"start\":78398},{\"end\":78767,\"start\":78759},{\"end\":78780,\"start\":78771},{\"end\":79128,\"start\":79124},{\"end\":79404,\"start\":79400},{\"end\":79415,\"start\":79408},{\"end\":79425,\"start\":79419},{\"end\":79438,\"start\":79429},{\"end\":79448,\"start\":79442},{\"end\":79780,\"start\":79772},{\"end\":79793,\"start\":79784},{\"end\":80147,\"start\":80143},{\"end\":80155,\"start\":80151},{\"end\":80168,\"start\":80159},{\"end\":80175,\"start\":80172},{\"end\":80184,\"start\":80179},{\"end\":80194,\"start\":80188},{\"end\":80507,\"start\":80499},{\"end\":80517,\"start\":80511},{\"end\":80529,\"start\":80521},{\"end\":80535,\"start\":80533},{\"end\":80544,\"start\":80539},{\"end\":80558,\"start\":80548},{\"end\":80839,\"start\":80836},{\"end\":80847,\"start\":80843},{\"end\":80858,\"start\":80851},{\"end\":81123,\"start\":81119},{\"end\":81385,\"start\":81382},{\"end\":81391,\"start\":81389},{\"end\":81402,\"start\":81397},{\"end\":81412,\"start\":81406},{\"end\":81421,\"start\":81416},{\"end\":81736,\"start\":81733},{\"end\":81744,\"start\":81740},{\"end\":81752,\"start\":81748},{\"end\":81760,\"start\":81756},{\"end\":81767,\"start\":81764},{\"end\":82135,\"start\":82132},{\"end\":82143,\"start\":82139},{\"end\":82149,\"start\":82147},{\"end\":82159,\"start\":82153},{\"end\":82168,\"start\":82163},{\"end\":82528,\"start\":82525},{\"end\":82536,\"start\":82532},{\"end\":82544,\"start\":82540},{\"end\":82552,\"start\":82548},{\"end\":82559,\"start\":82556},{\"end\":82817,\"start\":82815},{\"end\":83091,\"start\":83088},{\"end\":83099,\"start\":83095},{\"end\":83109,\"start\":83105},{\"end\":83116,\"start\":83113},{\"end\":83124,\"start\":83120},{\"end\":83130,\"start\":83128},{\"end\":83497,\"start\":83493},{\"end\":83680,\"start\":83677},{\"end\":83687,\"start\":83684},{\"end\":83693,\"start\":83691},{\"end\":83701,\"start\":83697},{\"end\":83716,\"start\":83705},{\"end\":83978,\"start\":83974},{\"end\":83988,\"start\":83982},{\"end\":83994,\"start\":83992},{\"end\":84002,\"start\":83998},{\"end\":84147,\"start\":84139},{\"end\":84153,\"start\":84149},{\"end\":84299,\"start\":84296},{\"end\":84308,\"start\":84303},{\"end\":84315,\"start\":84312},{\"end\":84323,\"start\":84319},{\"end\":84330,\"start\":84327},{\"end\":84663,\"start\":84660},{\"end\":84671,\"start\":84667},{\"end\":84678,\"start\":84675},{\"end\":84686,\"start\":84682},{\"end\":84693,\"start\":84690},{\"end\":84980,\"start\":84977},{\"end\":84987,\"start\":84984},{\"end\":84995,\"start\":84991},{\"end\":85002,\"start\":84999},{\"end\":85320,\"start\":85307},{\"end\":85328,\"start\":85324},{\"end\":85341,\"start\":85332},{\"end\":85611,\"start\":85607},{\"end\":85619,\"start\":85615},{\"end\":85626,\"start\":85623},{\"end\":85634,\"start\":85630},{\"end\":85909,\"start\":85905},{\"end\":85918,\"start\":85915},{\"end\":86187,\"start\":86184},{\"end\":86194,\"start\":86191},{\"end\":86484,\"start\":86481},{\"end\":86491,\"start\":86488},{\"end\":86500,\"start\":86495},{\"end\":86506,\"start\":86504},{\"end\":86522,\"start\":86510},{\"end\":86725,\"start\":86715},{\"end\":86740,\"start\":86729},{\"end\":86955,\"start\":86951},{\"end\":86962,\"start\":86959},{\"end\":86968,\"start\":86966},{\"end\":86976,\"start\":86972},{\"end\":87334,\"start\":87331},{\"end\":87342,\"start\":87338},{\"end\":87351,\"start\":87346},{\"end\":87359,\"start\":87355},{\"end\":87667,\"start\":87654},{\"end\":87676,\"start\":87671},{\"end\":87687,\"start\":87680},{\"end\":88030,\"start\":88025},{\"end\":88040,\"start\":88034},{\"end\":88050,\"start\":88044},{\"end\":88341,\"start\":88339},{\"end\":88354,\"start\":88345},{\"end\":88369,\"start\":88358},{\"end\":88673,\"start\":88667},{\"end\":88684,\"start\":88677},{\"end\":88692,\"start\":88688},{\"end\":88701,\"start\":88696},{\"end\":88860,\"start\":88852},{\"end\":88866,\"start\":88862},{\"end\":89030,\"start\":89019},{\"end\":89039,\"start\":89034},{\"end\":89053,\"start\":89045},{\"end\":89428,\"start\":89420},{\"end\":89438,\"start\":89432},{\"end\":89452,\"start\":89442},{\"end\":89462,\"start\":89456},{\"end\":89838,\"start\":89834},{\"end\":89846,\"start\":89842},{\"end\":89854,\"start\":89850},{\"end\":90114,\"start\":90108},{\"end\":90124,\"start\":90118},{\"end\":90132,\"start\":90128},{\"end\":90421,\"start\":90418},{\"end\":90430,\"start\":90425},{\"end\":90440,\"start\":90434},{\"end\":90450,\"start\":90444},{\"end\":90465,\"start\":90454},{\"end\":90473,\"start\":90469},{\"end\":90806,\"start\":90804},{\"end\":90818,\"start\":90810},{\"end\":90834,\"start\":90826},{\"end\":91093,\"start\":91087},{\"end\":91451,\"start\":91445},{\"end\":91465,\"start\":91455},{\"end\":91476,\"start\":91469},{\"end\":91744,\"start\":91736},{\"end\":91756,\"start\":91748},{\"end\":91766,\"start\":91760},{\"end\":91775,\"start\":91770},{\"end\":91789,\"start\":91779},{\"end\":91800,\"start\":91793},{\"end\":92171,\"start\":92168}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":45520437},\"end\":64318,\"start\":64039},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":204078399},\"end\":64591,\"start\":64320},{\"attributes\":{\"doi\":\"arXiv:1912.12148\",\"id\":\"b2\"},\"end\":64898,\"start\":64593},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":688013},\"end\":65246,\"start\":64900},{\"attributes\":{\"doi\":\"arXiv:1807.10066\",\"id\":\"b4\"},\"end\":65446,\"start\":65248},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206771624},\"end\":65774,\"start\":65448},{\"attributes\":{\"doi\":\"arXiv:2004.07485\",\"id\":\"b6\"},\"end\":66019,\"start\":65776},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3865699},\"end\":66349,\"start\":66021},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18112817},\"end\":66809,\"start\":66351},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":229422154},\"end\":67133,\"start\":66811},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6070091},\"end\":67399,\"start\":67135},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2429016},\"end\":67647,\"start\":67401},{\"attributes\":{\"id\":\"b12\"},\"end\":67761,\"start\":67649},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":19052864},\"end\":68087,\"start\":67763},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":72941049},\"end\":68543,\"start\":68089},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":54475483},\"end\":68999,\"start\":68545},{\"attributes\":{\"doi\":\"arXiv:1612.00390\",\"id\":\"b16\"},\"end\":69274,\"start\":69001},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2012925},\"end\":69567,\"start\":69276},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":137205},\"end\":69888,\"start\":69569},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":40979014},\"end\":70312,\"start\":69890},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":54463801},\"end\":70605,\"start\":70314},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53711301},\"end\":70948,\"start\":70607},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":67855477},\"end\":71335,\"start\":70950},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3143815},\"end\":71643,\"start\":71337},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1610415},\"end\":71989,\"start\":71645},{\"attributes\":{\"doi\":\"arXiv:1805.04687\",\"id\":\"b25\"},\"end\":72320,\"start\":71991},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220935624},\"end\":72694,\"start\":72322},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":102353587},\"end\":73063,\"start\":72696},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":201058600},\"end\":73385,\"start\":73065},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9854676},\"end\":73824,\"start\":73387},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4461350},\"end\":74244,\"start\":73826},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8394584},\"end\":74708,\"start\":74246},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":21657320},\"end\":75034,\"start\":74710},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3735827},\"end\":75354,\"start\":75036},{\"attributes\":{\"doi\":\"arXiv:1908.00024\",\"id\":\"b34\"},\"end\":75595,\"start\":75356},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":129945241},\"end\":75965,\"start\":75597},{\"attributes\":{\"doi\":\"arXiv:2001.03093\",\"id\":\"b36\"},\"end\":76313,\"start\":75967},{\"attributes\":{\"doi\":\"arXiv:2008.08294\",\"id\":\"b37\"},\"end\":76469,\"start\":76315},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":220845847},\"end\":76871,\"start\":76471},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":232380341},\"end\":77162,\"start\":76873},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4406882},\"end\":77494,\"start\":77164},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":52306520},\"end\":77928,\"start\":77496},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":214727763},\"end\":78259,\"start\":77930},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":204959605},\"end\":78687,\"start\":78261},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11797475},\"end\":79041,\"start\":78689},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":5711057},\"end\":79331,\"start\":79043},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1122604},\"end\":79699,\"start\":79333},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206596127},\"end\":80070,\"start\":79701},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206596999},\"end\":80470,\"start\":80072},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1623213},\"end\":80767,\"start\":80472},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":41037339},\"end\":81050,\"start\":80769},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":3412530},\"end\":81314,\"start\":81052},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":85502269},\"end\":81663,\"start\":81316},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":209140399},\"end\":82059,\"start\":81665},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":219531472},\"end\":82464,\"start\":82061},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":225110748},\"end\":82754,\"start\":82466},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":236150308},\"end\":83006,\"start\":82756},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":219687106},\"end\":83443,\"start\":83008},{\"attributes\":{\"doi\":\"arXiv:2104.00969\",\"id\":\"b58\"},\"end\":83606,\"start\":83445},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":228835893},\"end\":83897,\"start\":83608},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":2215922},\"end\":84137,\"start\":83899},{\"attributes\":{\"id\":\"b61\"},\"end\":84251,\"start\":84139},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":623484},\"end\":84596,\"start\":84253},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":206771431},\"end\":84924,\"start\":84598},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":28963224},\"end\":85266,\"start\":84926},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":3716771},\"end\":85548,\"start\":85268},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":52965526},\"end\":85838,\"start\":85550},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":3904282},\"end\":86114,\"start\":85840},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":4434361},\"end\":86409,\"start\":86116},{\"attributes\":{\"doi\":\"arXiv:1903.10172\",\"id\":\"b69\"},\"end\":86687,\"start\":86411},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":1915014},\"end\":86879,\"start\":86689},{\"attributes\":{\"id\":\"b71\"},\"end\":87261,\"start\":86881},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":226308428},\"end\":87573,\"start\":87263},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":19515346},\"end\":87953,\"start\":87575},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":74506},\"end\":88281,\"start\":87955},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":3143815},\"end\":88584,\"start\":88283},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":4713643},\"end\":88848,\"start\":88586},{\"attributes\":{\"id\":\"b77\"},\"end\":88966,\"start\":88850},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2020.3040591\",\"id\":\"b78\"},\"end\":89337,\"start\":88968},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":189857704},\"end\":89773,\"start\":89339},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":15068174},\"end\":90052,\"start\":89775},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":10748824},\"end\":90344,\"start\":90054},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":3759573},\"end\":90788,\"start\":90346},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":54465873},\"end\":91020,\"start\":90790},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":502946},\"end\":91350,\"start\":91022},{\"attributes\":{\"id\":\"b85\"},\"end\":91663,\"start\":91352},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":206592218},\"end\":92123,\"start\":91665},{\"attributes\":{\"doi\":\"arXiv:1705.06950\",\"id\":\"b87\"},\"end\":92271,\"start\":92125}]", "bib_title": "[{\"end\":64079,\"start\":64039},{\"end\":64357,\"start\":64320},{\"end\":64973,\"start\":64900},{\"end\":65520,\"start\":65448},{\"end\":66081,\"start\":66021},{\"end\":66456,\"start\":66351},{\"end\":66892,\"start\":66811},{\"end\":67180,\"start\":67135},{\"end\":67448,\"start\":67401},{\"end\":67838,\"start\":67763},{\"end\":68165,\"start\":68089},{\"end\":68630,\"start\":68545},{\"end\":69343,\"start\":69276},{\"end\":69634,\"start\":69569},{\"end\":69942,\"start\":69890},{\"end\":70353,\"start\":70314},{\"end\":70662,\"start\":70607},{\"end\":71012,\"start\":70950},{\"end\":71389,\"start\":71337},{\"end\":71696,\"start\":71645},{\"end\":72410,\"start\":72322},{\"end\":72803,\"start\":72696},{\"end\":73139,\"start\":73065},{\"end\":73445,\"start\":73387},{\"end\":73907,\"start\":73826},{\"end\":74321,\"start\":74246},{\"end\":74793,\"start\":74710},{\"end\":75090,\"start\":75036},{\"end\":75692,\"start\":75597},{\"end\":76559,\"start\":76471},{\"end\":76928,\"start\":76873},{\"end\":77213,\"start\":77164},{\"end\":77590,\"start\":77496},{\"end\":77972,\"start\":77930},{\"end\":78359,\"start\":78261},{\"end\":78755,\"start\":78689},{\"end\":79120,\"start\":79043},{\"end\":79396,\"start\":79333},{\"end\":79768,\"start\":79701},{\"end\":80139,\"start\":80072},{\"end\":80495,\"start\":80472},{\"end\":80832,\"start\":80769},{\"end\":81115,\"start\":81052},{\"end\":81378,\"start\":81316},{\"end\":81729,\"start\":81665},{\"end\":82128,\"start\":82061},{\"end\":82521,\"start\":82466},{\"end\":82811,\"start\":82756},{\"end\":83084,\"start\":83008},{\"end\":83673,\"start\":83608},{\"end\":83970,\"start\":83899},{\"end\":84292,\"start\":84253},{\"end\":84656,\"start\":84598},{\"end\":84973,\"start\":84926},{\"end\":85303,\"start\":85268},{\"end\":85603,\"start\":85550},{\"end\":85901,\"start\":85840},{\"end\":86180,\"start\":86116},{\"end\":86711,\"start\":86689},{\"end\":86947,\"start\":86881},{\"end\":87327,\"start\":87263},{\"end\":87650,\"start\":87575},{\"end\":88021,\"start\":87955},{\"end\":88335,\"start\":88283},{\"end\":88663,\"start\":88586},{\"end\":89015,\"start\":88968},{\"end\":89416,\"start\":89339},{\"end\":89830,\"start\":89775},{\"end\":90104,\"start\":90054},{\"end\":90414,\"start\":90346},{\"end\":90800,\"start\":90790},{\"end\":91083,\"start\":91022},{\"end\":91441,\"start\":91352},{\"end\":91732,\"start\":91665}]", "bib_author": "[{\"end\":64092,\"start\":64081},{\"end\":64103,\"start\":64092},{\"end\":64112,\"start\":64103},{\"end\":64119,\"start\":64112},{\"end\":64369,\"start\":64359},{\"end\":64698,\"start\":64690},{\"end\":64705,\"start\":64698},{\"end\":64713,\"start\":64705},{\"end\":64720,\"start\":64713},{\"end\":64981,\"start\":64975},{\"end\":65286,\"start\":65275},{\"end\":65298,\"start\":65286},{\"end\":65309,\"start\":65298},{\"end\":65322,\"start\":65309},{\"end\":65529,\"start\":65522},{\"end\":65537,\"start\":65529},{\"end\":65545,\"start\":65537},{\"end\":65843,\"start\":65835},{\"end\":65850,\"start\":65843},{\"end\":65856,\"start\":65850},{\"end\":65864,\"start\":65856},{\"end\":65870,\"start\":65864},{\"end\":66090,\"start\":66083},{\"end\":66097,\"start\":66090},{\"end\":66105,\"start\":66097},{\"end\":66112,\"start\":66105},{\"end\":66467,\"start\":66458},{\"end\":66480,\"start\":66467},{\"end\":66903,\"start\":66894},{\"end\":66913,\"start\":66903},{\"end\":66922,\"start\":66913},{\"end\":67188,\"start\":67182},{\"end\":67195,\"start\":67188},{\"end\":67202,\"start\":67195},{\"end\":67459,\"start\":67450},{\"end\":67467,\"start\":67459},{\"end\":67478,\"start\":67467},{\"end\":67497,\"start\":67478},{\"end\":67508,\"start\":67497},{\"end\":67659,\"start\":67649},{\"end\":67665,\"start\":67659},{\"end\":67847,\"start\":67840},{\"end\":67854,\"start\":67847},{\"end\":67861,\"start\":67854},{\"end\":68177,\"start\":68167},{\"end\":68183,\"start\":68177},{\"end\":68191,\"start\":68183},{\"end\":68199,\"start\":68191},{\"end\":68210,\"start\":68199},{\"end\":68223,\"start\":68210},{\"end\":68645,\"start\":68632},{\"end\":68655,\"start\":68645},{\"end\":68671,\"start\":68655},{\"end\":68679,\"start\":68671},{\"end\":69103,\"start\":69092},{\"end\":69114,\"start\":69103},{\"end\":69356,\"start\":69345},{\"end\":69365,\"start\":69356},{\"end\":69643,\"start\":69636},{\"end\":69650,\"start\":69643},{\"end\":69657,\"start\":69650},{\"end\":69954,\"start\":69944},{\"end\":69966,\"start\":69954},{\"end\":69979,\"start\":69966},{\"end\":69995,\"start\":69979},{\"end\":70006,\"start\":69995},{\"end\":70017,\"start\":70006},{\"end\":70372,\"start\":70355},{\"end\":70379,\"start\":70372},{\"end\":70388,\"start\":70379},{\"end\":70394,\"start\":70388},{\"end\":70670,\"start\":70664},{\"end\":70677,\"start\":70670},{\"end\":70688,\"start\":70677},{\"end\":70699,\"start\":70688},{\"end\":70711,\"start\":70699},{\"end\":71021,\"start\":71014},{\"end\":71027,\"start\":71021},{\"end\":71035,\"start\":71027},{\"end\":71047,\"start\":71035},{\"end\":71059,\"start\":71047},{\"end\":71397,\"start\":71391},{\"end\":71410,\"start\":71397},{\"end\":71425,\"start\":71410},{\"end\":71709,\"start\":71698},{\"end\":71717,\"start\":71709},{\"end\":71725,\"start\":71717},{\"end\":72074,\"start\":72068},{\"end\":72082,\"start\":72074},{\"end\":72090,\"start\":72082},{\"end\":72097,\"start\":72090},{\"end\":72105,\"start\":72097},{\"end\":72117,\"start\":72105},{\"end\":72128,\"start\":72117},{\"end\":72419,\"start\":72412},{\"end\":72425,\"start\":72419},{\"end\":72433,\"start\":72425},{\"end\":72813,\"start\":72805},{\"end\":73149,\"start\":73141},{\"end\":73160,\"start\":73149},{\"end\":73456,\"start\":73447},{\"end\":73464,\"start\":73456},{\"end\":73478,\"start\":73464},{\"end\":73491,\"start\":73478},{\"end\":73502,\"start\":73491},{\"end\":73514,\"start\":73502},{\"end\":73918,\"start\":73909},{\"end\":73929,\"start\":73918},{\"end\":73940,\"start\":73929},{\"end\":73952,\"start\":73940},{\"end\":73961,\"start\":73952},{\"end\":74330,\"start\":74323},{\"end\":74338,\"start\":74330},{\"end\":74349,\"start\":74338},{\"end\":74359,\"start\":74349},{\"end\":74369,\"start\":74359},{\"end\":74383,\"start\":74369},{\"end\":74802,\"start\":74795},{\"end\":74815,\"start\":74802},{\"end\":75104,\"start\":75092},{\"end\":75118,\"start\":75104},{\"end\":75127,\"start\":75118},{\"end\":75137,\"start\":75127},{\"end\":75433,\"start\":75425},{\"end\":75442,\"start\":75433},{\"end\":75451,\"start\":75442},{\"end\":75706,\"start\":75694},{\"end\":75716,\"start\":75706},{\"end\":76076,\"start\":76064},{\"end\":76088,\"start\":76076},{\"end\":76103,\"start\":76088},{\"end\":76113,\"start\":76103},{\"end\":76365,\"start\":76357},{\"end\":76568,\"start\":76561},{\"end\":76578,\"start\":76568},{\"end\":76598,\"start\":76578},{\"end\":76611,\"start\":76598},{\"end\":76617,\"start\":76611},{\"end\":76938,\"start\":76930},{\"end\":76946,\"start\":76938},{\"end\":76952,\"start\":76946},{\"end\":76964,\"start\":76952},{\"end\":77223,\"start\":77215},{\"end\":77235,\"start\":77223},{\"end\":77247,\"start\":77235},{\"end\":77255,\"start\":77247},{\"end\":77599,\"start\":77592},{\"end\":77605,\"start\":77599},{\"end\":77613,\"start\":77605},{\"end\":77625,\"start\":77613},{\"end\":77637,\"start\":77625},{\"end\":77648,\"start\":77637},{\"end\":77983,\"start\":77974},{\"end\":77994,\"start\":77983},{\"end\":78002,\"start\":77994},{\"end\":78372,\"start\":78361},{\"end\":78385,\"start\":78372},{\"end\":78394,\"start\":78385},{\"end\":78407,\"start\":78394},{\"end\":78769,\"start\":78757},{\"end\":78782,\"start\":78769},{\"end\":79130,\"start\":79122},{\"end\":79406,\"start\":79398},{\"end\":79417,\"start\":79406},{\"end\":79427,\"start\":79417},{\"end\":79440,\"start\":79427},{\"end\":79450,\"start\":79440},{\"end\":79782,\"start\":79770},{\"end\":79795,\"start\":79782},{\"end\":80149,\"start\":80141},{\"end\":80157,\"start\":80149},{\"end\":80170,\"start\":80157},{\"end\":80177,\"start\":80170},{\"end\":80186,\"start\":80177},{\"end\":80196,\"start\":80186},{\"end\":80509,\"start\":80497},{\"end\":80519,\"start\":80509},{\"end\":80531,\"start\":80519},{\"end\":80537,\"start\":80531},{\"end\":80546,\"start\":80537},{\"end\":80560,\"start\":80546},{\"end\":80841,\"start\":80834},{\"end\":80849,\"start\":80841},{\"end\":80860,\"start\":80849},{\"end\":81125,\"start\":81117},{\"end\":81387,\"start\":81380},{\"end\":81393,\"start\":81387},{\"end\":81404,\"start\":81393},{\"end\":81414,\"start\":81404},{\"end\":81423,\"start\":81414},{\"end\":81738,\"start\":81731},{\"end\":81746,\"start\":81738},{\"end\":81754,\"start\":81746},{\"end\":81762,\"start\":81754},{\"end\":81769,\"start\":81762},{\"end\":82137,\"start\":82130},{\"end\":82145,\"start\":82137},{\"end\":82151,\"start\":82145},{\"end\":82161,\"start\":82151},{\"end\":82170,\"start\":82161},{\"end\":82530,\"start\":82523},{\"end\":82538,\"start\":82530},{\"end\":82546,\"start\":82538},{\"end\":82554,\"start\":82546},{\"end\":82561,\"start\":82554},{\"end\":82819,\"start\":82813},{\"end\":83093,\"start\":83086},{\"end\":83101,\"start\":83093},{\"end\":83111,\"start\":83101},{\"end\":83118,\"start\":83111},{\"end\":83126,\"start\":83118},{\"end\":83132,\"start\":83126},{\"end\":83499,\"start\":83491},{\"end\":83682,\"start\":83675},{\"end\":83689,\"start\":83682},{\"end\":83695,\"start\":83689},{\"end\":83703,\"start\":83695},{\"end\":83718,\"start\":83703},{\"end\":83980,\"start\":83972},{\"end\":83990,\"start\":83980},{\"end\":83996,\"start\":83990},{\"end\":84004,\"start\":83996},{\"end\":84149,\"start\":84139},{\"end\":84155,\"start\":84149},{\"end\":84301,\"start\":84294},{\"end\":84310,\"start\":84301},{\"end\":84317,\"start\":84310},{\"end\":84325,\"start\":84317},{\"end\":84332,\"start\":84325},{\"end\":84665,\"start\":84658},{\"end\":84673,\"start\":84665},{\"end\":84680,\"start\":84673},{\"end\":84688,\"start\":84680},{\"end\":84695,\"start\":84688},{\"end\":84982,\"start\":84975},{\"end\":84989,\"start\":84982},{\"end\":84997,\"start\":84989},{\"end\":85004,\"start\":84997},{\"end\":85322,\"start\":85305},{\"end\":85330,\"start\":85322},{\"end\":85343,\"start\":85330},{\"end\":85613,\"start\":85605},{\"end\":85621,\"start\":85613},{\"end\":85628,\"start\":85621},{\"end\":85636,\"start\":85628},{\"end\":85911,\"start\":85903},{\"end\":85920,\"start\":85911},{\"end\":86189,\"start\":86182},{\"end\":86196,\"start\":86189},{\"end\":86486,\"start\":86479},{\"end\":86493,\"start\":86486},{\"end\":86502,\"start\":86493},{\"end\":86508,\"start\":86502},{\"end\":86524,\"start\":86508},{\"end\":86727,\"start\":86713},{\"end\":86742,\"start\":86727},{\"end\":86957,\"start\":86949},{\"end\":86964,\"start\":86957},{\"end\":86970,\"start\":86964},{\"end\":86978,\"start\":86970},{\"end\":87336,\"start\":87329},{\"end\":87344,\"start\":87336},{\"end\":87353,\"start\":87344},{\"end\":87361,\"start\":87353},{\"end\":87669,\"start\":87652},{\"end\":87678,\"start\":87669},{\"end\":87689,\"start\":87678},{\"end\":88032,\"start\":88023},{\"end\":88042,\"start\":88032},{\"end\":88052,\"start\":88042},{\"end\":88343,\"start\":88337},{\"end\":88356,\"start\":88343},{\"end\":88371,\"start\":88356},{\"end\":88675,\"start\":88665},{\"end\":88686,\"start\":88675},{\"end\":88694,\"start\":88686},{\"end\":88703,\"start\":88694},{\"end\":88862,\"start\":88852},{\"end\":88868,\"start\":88862},{\"end\":89032,\"start\":89017},{\"end\":89041,\"start\":89032},{\"end\":89055,\"start\":89041},{\"end\":89430,\"start\":89418},{\"end\":89440,\"start\":89430},{\"end\":89454,\"start\":89440},{\"end\":89464,\"start\":89454},{\"end\":89840,\"start\":89832},{\"end\":89848,\"start\":89840},{\"end\":89856,\"start\":89848},{\"end\":90116,\"start\":90106},{\"end\":90126,\"start\":90116},{\"end\":90134,\"start\":90126},{\"end\":90423,\"start\":90416},{\"end\":90432,\"start\":90423},{\"end\":90442,\"start\":90432},{\"end\":90452,\"start\":90442},{\"end\":90467,\"start\":90452},{\"end\":90475,\"start\":90467},{\"end\":90808,\"start\":90802},{\"end\":90820,\"start\":90808},{\"end\":90824,\"start\":90820},{\"end\":90836,\"start\":90824},{\"end\":91095,\"start\":91085},{\"end\":91453,\"start\":91443},{\"end\":91467,\"start\":91453},{\"end\":91478,\"start\":91467},{\"end\":91746,\"start\":91734},{\"end\":91758,\"start\":91746},{\"end\":91768,\"start\":91758},{\"end\":91777,\"start\":91768},{\"end\":91791,\"start\":91777},{\"end\":91802,\"start\":91791},{\"end\":92173,\"start\":92166}]", "bib_venue": "[{\"end\":64148,\"start\":64119},{\"end\":64416,\"start\":64369},{\"end\":64688,\"start\":64593},{\"end\":65031,\"start\":64981},{\"end\":65273,\"start\":65248},{\"end\":65578,\"start\":65545},{\"end\":65833,\"start\":65776},{\"end\":66144,\"start\":66112},{\"end\":66531,\"start\":66480},{\"end\":66939,\"start\":66922},{\"end\":67235,\"start\":67202},{\"end\":67512,\"start\":67508},{\"end\":67694,\"start\":67665},{\"end\":67894,\"start\":67861},{\"end\":68273,\"start\":68223},{\"end\":68729,\"start\":68679},{\"end\":69090,\"start\":69001},{\"end\":69393,\"start\":69365},{\"end\":69694,\"start\":69657},{\"end\":70062,\"start\":70017},{\"end\":70427,\"start\":70394},{\"end\":70744,\"start\":70711},{\"end\":71104,\"start\":71059},{\"end\":71463,\"start\":71425},{\"end\":71775,\"start\":71725},{\"end\":72066,\"start\":71991},{\"end\":72475,\"start\":72433},{\"end\":72846,\"start\":72813},{\"end\":73193,\"start\":73160},{\"end\":73564,\"start\":73514},{\"end\":73993,\"start\":73961},{\"end\":74433,\"start\":74383},{\"end\":74843,\"start\":74815},{\"end\":75156,\"start\":75137},{\"end\":75423,\"start\":75356},{\"end\":75749,\"start\":75716},{\"end\":76062,\"start\":75967},{\"end\":76355,\"start\":76315},{\"end\":76642,\"start\":76617},{\"end\":76989,\"start\":76964},{\"end\":77287,\"start\":77255},{\"end\":77679,\"start\":77648},{\"end\":78052,\"start\":78002},{\"end\":78440,\"start\":78407},{\"end\":78829,\"start\":78782},{\"end\":79158,\"start\":79130},{\"end\":79483,\"start\":79450},{\"end\":79845,\"start\":79795},{\"end\":80228,\"start\":80196},{\"end\":80588,\"start\":80560},{\"end\":80887,\"start\":80860},{\"end\":81153,\"start\":81125},{\"end\":81456,\"start\":81423},{\"end\":81820,\"start\":81769},{\"end\":82220,\"start\":82170},{\"end\":82577,\"start\":82561},{\"end\":82856,\"start\":82819},{\"end\":83182,\"start\":83132},{\"end\":83489,\"start\":83445},{\"end\":83727,\"start\":83718},{\"end\":84008,\"start\":84004},{\"end\":84184,\"start\":84155},{\"end\":84382,\"start\":84332},{\"end\":84728,\"start\":84695},{\"end\":85054,\"start\":85004},{\"end\":85376,\"start\":85343},{\"end\":85664,\"start\":85636},{\"end\":85948,\"start\":85920},{\"end\":86224,\"start\":86196},{\"end\":86477,\"start\":86411},{\"end\":86760,\"start\":86742},{\"end\":87028,\"start\":86978},{\"end\":87389,\"start\":87361},{\"end\":87722,\"start\":87689},{\"end\":88087,\"start\":88052},{\"end\":88409,\"start\":88371},{\"end\":88707,\"start\":88703},{\"end\":88897,\"start\":88868},{\"end\":89134,\"start\":89081},{\"end\":89514,\"start\":89464},{\"end\":89884,\"start\":89856},{\"end\":90167,\"start\":90134},{\"end\":90525,\"start\":90475},{\"end\":90869,\"start\":90836},{\"end\":91145,\"start\":91095},{\"end\":91486,\"start\":91478},{\"end\":91852,\"start\":91802},{\"end\":92164,\"start\":92125},{\"end\":64173,\"start\":64150},{\"end\":64459,\"start\":64418},{\"end\":65077,\"start\":65033},{\"end\":65607,\"start\":65580},{\"end\":66172,\"start\":66146},{\"end\":66554,\"start\":66533},{\"end\":67264,\"start\":67237},{\"end\":67518,\"start\":67514},{\"end\":67923,\"start\":67896},{\"end\":68319,\"start\":68275},{\"end\":68775,\"start\":68731},{\"end\":69417,\"start\":69395},{\"end\":69727,\"start\":69696},{\"end\":70103,\"start\":70064},{\"end\":70456,\"start\":70429},{\"end\":70773,\"start\":70746},{\"end\":71145,\"start\":71106},{\"end\":71821,\"start\":71777},{\"end\":72513,\"start\":72477},{\"end\":72875,\"start\":72848},{\"end\":73222,\"start\":73195},{\"end\":73610,\"start\":73566},{\"end\":74021,\"start\":73995},{\"end\":74479,\"start\":74435},{\"end\":74867,\"start\":74845},{\"end\":75171,\"start\":75158},{\"end\":75778,\"start\":75751},{\"end\":77315,\"start\":77289},{\"end\":77706,\"start\":77681},{\"end\":78098,\"start\":78054},{\"end\":78469,\"start\":78442},{\"end\":78868,\"start\":78831},{\"end\":79182,\"start\":79160},{\"end\":79512,\"start\":79485},{\"end\":79891,\"start\":79847},{\"end\":80256,\"start\":80230},{\"end\":80612,\"start\":80590},{\"end\":80910,\"start\":80889},{\"end\":81177,\"start\":81155},{\"end\":81485,\"start\":81458},{\"end\":81867,\"start\":81822},{\"end\":82266,\"start\":82222},{\"end\":82885,\"start\":82858},{\"end\":83228,\"start\":83184},{\"end\":84014,\"start\":84010},{\"end\":84428,\"start\":84384},{\"end\":84757,\"start\":84730},{\"end\":85100,\"start\":85056},{\"end\":85405,\"start\":85378},{\"end\":85688,\"start\":85666},{\"end\":85972,\"start\":85950},{\"end\":86248,\"start\":86226},{\"end\":87074,\"start\":87030},{\"end\":87413,\"start\":87391},{\"end\":87751,\"start\":87724},{\"end\":88114,\"start\":88089},{\"end\":88713,\"start\":88709},{\"end\":89560,\"start\":89516},{\"end\":89908,\"start\":89886},{\"end\":90196,\"start\":90169},{\"end\":90571,\"start\":90527},{\"end\":90898,\"start\":90871},{\"end\":91191,\"start\":91147},{\"end\":91898,\"start\":91854}]"}}}, "year": 2023, "month": 12, "day": 17}