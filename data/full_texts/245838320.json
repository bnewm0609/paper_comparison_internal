{"id": 245838320, "updated": "2023-06-13 16:37:27.48", "metadata": {"title": "ALICE++: Adversarial Training for Robust and Effective Temporal Reasoning", "authors": "[{\"first\":\"Lis\",\"last\":\"Pereira\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Masayuki\",\"last\":\"Asahara\",\"middle\":[]},{\"first\":\"Ichiro\",\"last\":\"Kobayashi\",\"middle\":[]}]", "venue": "PACLIC", "journal": "Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We propose an enhanced adversarial training algorithm for \ufb01ne-tuning transformer-based language models (i.e", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2021.paclic-1.40", "pubmed": null, "pubmedcentral": null, "dblp": "conf/paclic/PereiraCAK21", "doi": null}}, "content": {"source": {"pdf_hash": "00dea030e47950e55d6497a5c8acf5053baebcbd", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.paclic-1.40.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "810fdb7dd4ebecad94eac560fa4ac2e140a4c440", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/00dea030e47950e55d6497a5c8acf5053baebcbd.txt", "contents": "\nALICE++ : Adversarial Training for Robust and Effective Temporal Reasoning\n\n\nKanashiro Lis kanashiro.pereira@ocha.ac.jp \nPereira \nFei Cheng feicheng@i.kyoto-u.ac.jp \nMasayuki Asahara masayu-a@ninjal.ac.jp \nNinjal \nJapan \nIchiro Kobayashi \n\nOchanomizu University\nKyoto University\nJapan\n\n\nOchanomizu University\nJapan\n\nALICE++ : Adversarial Training for Robust and Effective Temporal Reasoning\n\nWe propose an enhanced adversarial training algorithm for fine-tuning transformer-based language models (i.e., RoBERTa) and apply it to the temporal reasoning task. Instead of adding the perturbation only to the embedding layer, our algorithm searches for the best combination of layers to add the adversarial perturbation. We further enhance this algorithm with f -divergences, i.e., the Jensen-Shannon divergence. Moreover, we enrich this model with general commonsense knowledge by leveraging data from the general commonsense knowledge task in a multi-task learning scenario. Our results show that our model can improve performance on both English and Japanese temporal reasoning benchmarks, and establishes new state-of-the-art results.\n\nAlthough recent pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa  have achieved great success in a wide range of natural language processing (NLP) tasks, these models may still perform poorly on temporal reasoning scenarios. Ribeiro et al. (2020) has shown that such models often fail to make even simple temporal distinctions, for example, to distinguish the words before and after, resulting in degraded performance.\n\nFollowing best practices from recent work on enhancing model generalization and robustness, we propose a model that effectively leverages pretrained representations (i.e. RoBERTa), adversarial training, and multi-task learning for robust temporal reasoning. More specifically, our main contributions are: 1) we propose an enhanced adversarial training algorithm for fine-tuning transformer-based language models that boosts the fine-tuning performance of RoBERTa. More specifically, our algorithm generates and adds the perturbation to a combination of layers during adversarial training. We hypothesize this might encourage the model to generate more diverse adversarial examples, and improve model generalization capability. Common adversarial training approaches for NLP add the perturbation only to the embedding layer (Zhu et al., 2019;Jiang et al., 2019;Liu et al., 2020;Pereira et al., 2020). In addition, we further enhance this algorithm with f -divergences (i.e., the Jensen-Shannon divergence), recently proposed by Cheng et al. (2021); 2) we enrich this model with general commonsense knowledge by leveraging data from the general commonsense knowledge task in a multi-task learning scenario; 3) we apply our model to several temporal reasoning tasks and improve state-of-the-art results.\n\n\nBackground\n\nIn this section, we describe the temporal reasoning tasks we tackle in this work. All tasks are challenging since they require deep understanding of the temporal properties of language. Event Ordering Prediction Task: This task involves predicting the temporal relationship between a pair of input events in a span of text. We use the MATRES dataset (Ning et al., 2018). It originally contains 13,577 pairs of events annotated with a temporal relation (BEFORE, AFTER, EQUAL, VAGUE). The temporal annotations are performed on 256 English documents (and 20 more for evalua-tion) from the TimeBank (Pustejovsky et al., 2003), AQUAINT (Graff, 2002) and Platinum (UzZaman et al., 2013) datasets. An example of a sentence with two events (in bold) that hold the BEFORE relation is below:\n\nAt one point, when it (e1:became) clear controllers could not contact the plane, someone (e2:said) a prayer.\n\nWe follow Zhou et al. (2021), and we train and evaluate only the instances with a label of either \"BE-FORE\" or \"AFTER\". Event Duration Prediction Task: This task consists of deciding whether a given event has a duration longer or shorter than a day. We use TimeML (Saur\u00ed et al., 2006;Pan et al., 2006), a dataset with event duration annotated as lower and upper bounds. An example of a sentence with an event (in bold) that has a duration shorter than a day is shown below:\n\nIn Singapore, stocks hit a five year low.\n\nStory Cloze Task (SCT): This task involves choosing an ending to a story. We use the Story Cloze Task dataset (Mostafazadeh et al., 2017), where the task is to choose the correct ending, among two choices, to a 4-sentence story. It captures a rich set of causal and temporal commonsense relations between daily events. An example from the dataset is below. The correct answer is in bold.\n\nStory: Danny bought a boat. His nearby marina was having a race. He decided to enter. Danny and his best friend manned the boat. a) Danny decided to go to sleep. b) They prepared for the start of the race.\n\nTemporal Commonsense Reasoning Task: This task focuses on temporal commonsense reasoning. We use the MC-TACO  dataset. It considers five temporal properties: (1) duration (how long an event takes), (2) temporal ordering (typical order of events), (3) typical time (when an event occurs), (4) frequency (how often an event occurs), and (5) stationarity (whether a state is maintained for a very long time or indefinitely). It contains 13k tuples, each consisting of a sentence, a question, and a candidate answer, that should be judged as plausible or not. The sentences are taken from different sources such as news, Wikipedia, and textbooks. An example from the dataset is below. The correct answer is in bold. In the next section, we introduce our temporal reasoning model.\n\n\nParagraph\n\n\nTemporal Reasoning Model\n\nOur model uses RoBERTa  as the text encoder as it has obtained high performance on several natural language understanding (NLU) benchmarks. We focus on exploring adversarial training and multi-task learning, as detailed below. Adversarial training (ADV): Adversarial training works as an online data augmentation method and can help improve model performance, especially in low-resource scenarios. It can also help improve model performance without increasing the model size, which is helpful in scenarios where computational resources are limited. Adversarial training has proven effective in improving model generalization and robustness in computer vision (Madry et al., 2017;Goodfellow et al., 2014) and more recently in natural language processing (NLP) (Zhu et al., 2019;Jiang et al., 2019;Cheng et al., 2019;Liu et al., 2020;Pereira et al., 2020). It works by augmenting the input with a small perturbation that maximizes the adversarial loss:\nmin \u03b8 E (x,y)\u223cD [max \u03b4 l(f (x + \u03b4; \u03b8), y)],\nwhere the inner maximization can be solved by projected gradient descent (Madry et al., 2017). Recently, adversarial training has been successfully applied to NLP as well (Zhu et al., 2019;Jiang et al., 2019;Pereira et al., 2020). In our work, we propose to enhance the ALICE (Pereira et al., 2020) algorithm. ALICE combines two approaches to estimate the perturbation \u03b4: one that uses the label y (Zhu et al., 2019) and another that uses the model prediction f (x; \u03b8), i.e., a \"virtual\" label (Miyato et al., 2018;Jiang et al., 2019):\nmin \u03b8 E (x,y)\u223cD [max \u03b4 1 l(f (x + \u03b4 1 ; \u03b8), y)+ \u03b1 max \u03b4 2 l(f (x + \u03b4 2 ; \u03b8), f (x; \u03b8))],(1)\nwhere \u03b4 1 and \u03b4 2 are two different perturbations, bounded by a general l p norm ball, estimated by a fixed K steps of the gradient-based optimization approach. In our experiments, we set p = \u221e. Effectively, the second term encourages smoothness in the input neighborhood, and \u03b1 is a hyperparameter that controls the trade-off between standard errors and adversarial errors. ALICE has been originally proposed for the commonsense reasoning task, however, it is a general algorithm that can be applied to other tasks as well. In our work, we show its applicability to the temporal reasoning tasks described in Section 1. Moreover, we propose to further enhance this algorithm with f -divergences, recently proposed by Cheng et al. (2021). Specifically, we consider the posterior regularization with the Jensen-Shannon divergence (JSD) (Lin, 1991), instead of the KL-divergence, originally proposed for ALICE. JSD is a smoothed and symmetric version of the KL-Divergence. We show in our experiments that JSD outperforms the KL-divergence on the temporal tasks. In addition, we investigate which combination of layers is best for adding the perturbation during training. ALICE originally adds the perturbation only to the embedding layer. We show that adding the perturbation to a combination of the transformer's layers instead leads to better results. We first set a maximum layer (among all RoBERTa layers, including the embedding layer) where the adversarial perturbation can be added. In each epoch, for each mini-batch selected, we first sample noise vectors \u03b4 1 and \u03b4 2 from N (0, \u03c3 2 I), with mean 0 and variation of \u03c3 2 . A layer among the embedding layer and the maximum layer previously set is randomly chosen and the model performs adversarial steps from this layer byK gradient steps. The noise inputs are then constructed by adding the perturbations \u03b4 1 and \u03b4 2 to the hidden state vector of the randomly chosen layer. Specifically, the model first performs a forward pass up to the chosen layer, then the perturbations \u03b4 1 and \u03b4 2 are separately added to its hidden states, generating two different noise inputs. For example, if the second RoBERTa layer is set as the maximum layer, a layer among the embedding layer, the first, and the second layer is randomly chosen for each mini-batch selected, and adversarial training is performed from this layer. The model is then updated according to the task-specific objective for the task. The best layer combination is chosen by using a development set. We name our enhanced model ALICE++ .\n\nMulti-task learning (MTL): Multi-task learning is an effective training paradigm to promote model generalization ability and performance (Caruana, 1997;Liu et al., 2015;Ruder, 2017;Collobert et al., 2011). It works by leveraging data from many (related) tasks. We propose to enrich the training of the temporal commonsense reasoning task and Story Cloze Task by leveraging data from the general commonsense knowledge task. Since the commonsense reasoning task commonly involves reasoning about temporal events, e.g. what event(s) might happen before or after the current event, we hypothesize that those tasks might benefit from it. In our experiments, we use the CosmosQA (Huang et al., 2019) dataset. It has 35,888 questions on 21,886 distinct contexts taken from blogs of personal narratives. Each question has four answer candidates, one of which is correct. An example from this dataset is below. The correct answer is in bold.\n\nParagraph: Did some errands today. My prime objectives were to get textbooks, find a computer lab, find career services, get some groceries, turn in payment plan application, and find out when KEES money kicks in. I think it acts as a refund at the end of the semester at Murray, but I would be quite happy if it would work now.\n\nQuestion: What happens after I get the refund?\n\nOption 1: I can pay my bills.\n\nOption 2: I can relax.\n\nOption 3: I can sleep.\n\nOption 4: None of the above choices.\n\nWe use the MT-DNN framework Liu et al., 2020), which incorporates RoBERTa as the shared text encoding layer (shared across all tasks), while the top layers are task-specific. We used the pre-trained RoBERTa model to initialize the shared layers and refined them via MTL on the temporal reasoning tasks.\n\n\nExperiments\n\n\nDatasets and Evaluation Metrics\n\nThe English datasets used in our experiments are summarized in Table 1. For TimeML, we follow the train and test splits as in . For MCTACO, we follow . For the MATRES dataset, we follow Ning et al. (2018). For the Story Cloze Task, we use the 2016 and 2018 data releases after removing duplicates. We set 20% of the TimeML, MATRES, and Story Cloze Task training data as the development set to tune the hyperparameters. For the MC-TACO dataset, no training set is available. Following Zhou et al (2019), we use the dev set for fine-tuning the model. We use 20% of this data for fine-tuning the parameters.\n\nWe evaluate the performance on MATRES in terms of accuracy and F1-score, and TimeML and Story Cloze Task in terms of accuracy. For the MC-TACO dataset, we report the exact match (EM) and F1 scores, following . EM measures how many questions a system correctly labeled all candidate answers, while F1 measures the average overlap between one's predictions and the ground truth.\n\n\nImplementation Details\n\nOur model implementation is based on the MT-DNN framework Liu et al., 2020). We use RoBERTa LARGE  as the text encoder. We used ADAM (Kingma and Ba, 2014) as our optimizer with a learning rate in the range \u2208 {9 \u00d7 10 \u22126 , 1 \u00d7 10 \u22125 } and a batch size in the range \u2208 {16, 32, 64}. The maximum number of epochs was set to 10. A linear learning rate decay schedule with warm-up over 0.1 was used unless stated otherwise. To avoid gradient exploding, we clipped the gradient norm within 1. All the texts were tokenized using WordPiece and were chopped to spans no longer than 512 tokens. We also set the dropout rate of all the task-specific layers as 0.3. During adversarial training, we follow (Jiang et al., 2019) and set the perturbation size to 1 \u00d7 10 \u22125 , the step size to 1\u00d710 \u22123 , and to 1\u00d710 \u22125 the variance for initializing perturbation. We search the regularization weight \u03b1 in {0.01, 0.1, 1}. We set the number of projected gradient steps to 1.\n\n\nMain Results\n\nWe present our results in Table 2. We compare our model, ALICE++ , with other state-of-the-art models. Overall, the adversarial methods, i.e., AL-ICE and ALICE++ , were able to outperform the standard fine-tuning approach (STD) and the other baselines, without using any additional knowledge source, and without using any additional dataset other than the target task datasets. These results suggest that adversarial training leads to a more robust model and helps generalize better on unseen data.\n\nBoth ALICE++ (JSD), the model that uses the Jensen-Shannon Divergence, and ALICE++ (JSD + Best layers selection) , the model that uses JSD and the best layer combination to add the perturbation, were able to outperform ALICE and the other baselines. Overall, ALICE++ (JSD + Best layers selection) obtained better performance. This indicates that adding the adversarial perturbation to the other layers of the model in addition to the embedding layer can improve the model generalization capability.\n\nFor example, on the MATRES dataset, ALICE++ (JSD + Best layers selection) obtained a 89.82% F1-score, a 2.52% improvement over SYMTIME (Zhou et al., 2021), a T5 model that exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. On the TimeML dataset, AL-ICE++ (JSD + Best layers selection) outperformed TacoML ), a BERT model pre-trained on explicit and implicit mentions of temporal common sense, extracted from a large corpus using pattern rules, and obtained an accuracy of 84.45%, an absolute gain of 2.75%. On the MC-TACO dataset, ALICE++ (JSD + Best layers selection) outperforms the T5-3B model (Kaddari et al., 2020) in terms of F1-score, obtaining an F1-score of 80.09%, an improvement of 0.63%, and an EM score of 58.56%, only 0.52% lower than T5-3B   STD denotes the standard fine-tuning procedure where we fine-tune RoBERTa on each task specific temporal reasoning dataset. ALICE++ denotes our proposed models. ALICE++ (JSD) denotes the model that uses the Jensen-Shannon Divergence, ALICE++ (JSD + Best layers selection) denotes the model that uses JSD and the best layer combination to add the perturbation, and ALICE++ (JSD + Best layers selection), MT CosmosQA) denotes the model that trains jointly with the CosmosQA dataset, in the multi-task learning setting. Note that STD, ALICE, and all ALICE++ models use RoBERTa LARGE as the text encoder, and for a fair comparison, all these results are produced by ourselves.\n\nmodel. When we train this dataset together with the CosmosQA in the multi-task learning setting, ALICE++ ( JSD + Best layers selection, MT CosmosQA) outperformed the T5-3B model on both F1 and EM, with score of 80.88% and 59.90%, respectively. We emphasize that both SYMTIME and T5-3B use T5, a much larger model (with 3B parameters) than RoBERTa (300M parameters), used in our experiments. On the Story Cloze Task (SCT) dataset, AL-ICE++ (JSD + Best layers selection) largely outperformed GDIN (Tian et al., 2020), a model that enhances BERT and ALBERT (Lan et al., 2019) word representations with knowledge sources. It obtained an accuracy of 97.49%, while GDIN obtained an score of 91.90%.\n\n\nEvaluation on Japanese dataset\n\nWe also explore the feasibility of our model on a Japanese dataset. Table 4 describes our results on the Japanese event ordering prediction task. We use the BCCWJ-Timebank corpus (Asahara et al., 2014). It consists of four tasks: 1) DCT, which denotes relations between a time expression of document creation time (DCT) and an event instance; 2) T2E, which denotes relations between a time expression (non-DCT) and an event instance within one sentence; 3) E2E, which denotes relations between two consecutive event instances; and 4) MAT, which denotes relations between two consecutive matrix verbs of event instances. We perform the documentlevel 5-fold cross-validation. In each split, we randomly select 15% documents as the development set. We follow a merged 6-relation set ('BE-FORE', 'BEFOREOR-OVERLAP', 'OVERLAP', 'OVERLAP-ORAFTER', 'AFTER', and 'VAGUE') as in Yoshikawa et al. (2014). The statistics of the corpus are shown in Table 3. An example from the corpus on the E2E task is shown below.\n\n\nTask: E2E\n\n\u5869\u5c11\u3005\u3092 (e1:\u3075\u3063)\u3066\u3057\u3070\u3089\u304f(e2:\u304a\u304d)\u3001\u6c34\u5206 \u3092\u3075\u304f\u3002 (e1:Shake) the salt a little and (e2:leave) it for a while to wipe off the water.\n\nLabel: BEFORE Moreover, we train all tasks jointly using multitask learning, following . We use a Japanese BERT BASE model 1 as the text encoder. Compared to standard fine-tuning and the other baselines, ALICE++ could improve on all tasks. It outperformed the model by , a BERT BASE model that dynamically updates event representations. ALICE++ also outperformed the model by Yoshikawa et al. (2014), a feature-based SVM classifier.\n\n\nDCT E2T\n\nE2E MAT 2,873 1,469 1,862 776  \n\n\nAnalysis of RoBERTa layers when adding the adversarial perturbation\n\nIn this Section, we show a brief analysis of the best combination of layers for adding the adversarial perturbation. Figure 1 shows the accuracy on the 1 https://nlp.ist.i.kyoto-u.ac.jp/?ku bert japanese TimeML, Story Cloze Task, MC-TACO, and MA-TRES development sets as we change the layer combination to add the adversarial perturbation. We can observe that adding the adversarial perturbation to the other layers of the model in addition to the embedding layer leads to better performance compared to adding the perturbation to the embedding layer only.\n\nA similar tendency is observed on the BCCWJ-Timebank, as shown in Figure 2.\n\n\nConclusion\n\nWe proposed an adversarial training algorithm for fine-tuning transformer-based language models, ALICE++ , that boosts the fine-tuning performance of RoBERTa. Our experiments demonstrated that it achieves state-of-the-art results on several temporal reasoning tasks. Although in this paper we focused on the temporal reasoning task, ALICE++ can be generalized to solve other downstream tasks as well, and we will explore this direction as to future work.\n\n(a) Accuracy on the TimeML development set as we change the layer combination to add the adversarial perturbation.\n\n(b) Accuracy on the Story Cloze Task development set as we change the layer combination to add the adversarial perturbation.\n\n(c) F1-score on the MC-TACO development set as we change the layer combination to add the adversarial perturbation.\n\n(d) F1-score on the MATRES development set as we change the layer combination to add the adversarial perturbation. Figure 1: Performance on the TimeML, Story Cloze Task, MC-TACO, and MATRES development sets as we change the layer combination to add the adversarial perturbation. max layer = 0 denotes that the adversarial perturbation is added to the embedding layer only. All the other values denote that, for each mini-batch, a layer among the embedding layer and max layer is randomly chosen and the model performs adversarial training from this layer. The model is then updated according to the task-specific objective for the task.\n\n(a) Accuracy on the BCCWJ-Timebank DCT task development set as we change the layer combination to add the adversarial perturbation.\n\n(b) Accuracy on the BCCWJ-Timebank T2E task development set as we change the layer combination to add the adversarial perturbation.\n\n(c) Accuracy on the BCCWJ-Timebank E2E task development set as we change the layer combination to add the adversarial perturbation.\n\n(d) Accuracy on the BCCWJ-Timebank MAT task development set as we change the layer combination to add the adversarial perturbation. Figure 2: Accuracy on the BCCWJ-Timebank development sets as we change the layer combination to add the adversarial perturbation. max layer = 0 denotes that the adversarial perturbation is added to the embedding layer only. All the other values denote that, for each mini-batch, a layer among the embedding layer and max layer is randomly chosen and the model performs adversarial training from this layer. The model is then updated according to the task-specific objective for the task.\n\n:\nGrowing up on a farm near St. Paul, L. Mark Bailey didn't dream of becoming a judge.Question: How many years did it take for Mark to become a\n\nTable 1 :\n1Summary of the four English evaluation datasets: MATRES, TimeML, Story Cloze Task (SCT), and MC-\nTACO. \n\nMATRES \nTimeML \nMC-TACO \nSCT \n\nModel \nAcc \nF1 \nAcc \nEM \nF1 \nAcc \nHuman \n-\n-\n87.70 \n75.80 87.10 \nSTD \n91.12 88.93 \n81.06 \n51.05 76.85 96.37 \nALICE (Pereira et al., 2020) \n91.69 89.10 \n82.75 \n56.45 79.50 96.85 \nALICE++ (JSD) \n91.55 89.37 \n83.15 \n58.10 80.20 97.17 \nALICE++ (JSD + Best layers selection) \n91.98 89.82 \n84.45 \n58.56 80.09 97.38 \nALICE++ ( JSD + Best layers selection, MT CosmosQA) \n-\n-\n-\n59.90 80.88 97.49 \nT5-3B (Kaddari et al., 2020) \n-\n-\n-\n59.08 79.46 \n-\nTacoML (Zhou et al., 2020) \n-\n-\n81.70 \n-\n-\n-\nSYMTIME (Zhou et al., 2021) \n-\n87.30 \n-\n-\n-\n-\nGDIN (Tian et al., 2020) \n-\n-\n-\n-\n-\n91.90 \n\n\n\nTable 2 :\n2Test results of MATRES, TimeML, Story Cloze Task (SCT), and MC-TACO. The best results are in bold.\n\nTable 3 :\n3Number of TLINKs in the BCCWJ-Timebank dataset. A \u2329TLINK\u232a defines the temporal ordering of temporal information expressions and event expressions.Model \nDCT E2T \nE2E MAT \nSTD \n83.04 65.15 68.54 63.50 \nYoshikawa et al. (2014) 75.60 55.70 59.90 50.00 \nCheng et al. (2020) \n81.60 60.70 64.50 64.60 \nALICE++ \n83.22 66.61 68.96 64.63 \n\n\n\nTable 4 :\n4Accuracy test results on the BCCWJ-Timebank dataset. ALICE++ denotes the model that uses JSD and the best layer combination to add the perturbation.\nAcknowledgmentsThis work has been supported by the project KAK-ENHI ID: 18H05521 and by project KAKENHI ID: 21K17802.\nBccwjtimebank: Temporal and event information annotation on japanese text. Masayuki Asahara, Sachi Kato, Hikari Konishi, Mizuho Imada, Kikuo Maekawa, In International Journal of Computational Linguistics and Chinese Language Processing. 193Masayuki Asahara, Sachi Kato, Hikari Konishi, Mizuho Imada, and Kikuo Maekawa. Bccwjtimebank: Tem- poral and event information annotation on japanese text. In International Journal of Computational Lin- guistics and Chinese Language Processing, Volume 19, Number 3, September 2014..\n\nMultitask learning. Machine learning. Rich Caruana, 28Caruana, Rich. Multitask learning. Machine learning, 28(1), 41-75.\n\nDynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning. F Cheng, M Asahara, I Kobayashi, S Kurohashi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsCheng, F., Asahara, M., Kobayashi, I., and Kurohashi, S. Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (pp. 1352-1357)..\n\nPosterior Differential Regularization with f-divergence for Improving Model Robustness. H Cheng, X Liu, L Pereira, Y Yu, J Gao, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesCheng, H. and Liu, X. and Pereira, L. and Yu, Y. and Gao, J. 2021. Posterior Differential Regulariza- tion with f-divergence for Improving Model Robust- ness. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, pages 1078-1089. June 6-11, 2021.\n\n. Yong Cheng, Lu Jiang, Wolfgang Macherey, arXiv:1906.02443Robust Neural Machine Translation with Doubly Adversarial Inputs. arXiv preprintYong Cheng and Lu Jiang and Wolfgang Macherey. Ro- bust Neural Machine Translation with Doubly Adver- sarial Inputs. arXiv preprint arXiv:1906.02443.\n\nNatural language processing (almost) from scratch. Ronan Collobert, Weston , Jason Bottou, L\u00e9on , Karlen , Michael Kavukcuoglu, Koray Kuksa, Pavel , Journal of machine learning research. 12Collobert, Ronan and Weston, Jason and Bottou, L\u00e9on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel. Natural language processing (almost) from scratch. Journal of machine learning research, 12(ARTICLE), 2493-2537.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Chang , Ming-Wei Lee, Kenton Toutanova, Kristina , Proceedings of NAACL-HLT 2019. NAACL-HLT 2019Minneapolis, MinnesotaDevlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. 2018. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of NAACL-HLT 2019, pages 4171-4186, Minneapolis, Minnesota, June 2 -June 7, 2019.\n\nExplaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, arXiv:1412.6572arXiv preprintGoodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian. Explaining and harnessing adversarial ex- amples. arXiv preprint arXiv:1412.6572.\n\nThe AQUAINT corpus of English news text. David Graff, Xinhua News Service. Linguistic Data Consortium. Associated Press, Inccontent copyrightGraff, David. 2002. The AQUAINT corpus of En- glish news text:[content copyright] Portions\u00a9 1998- 2000 New York Times, Inc.,\u00a9 1998-2000 Associated Press, Inc.,\u00a9 1996-2000 Xinhua News Service. Lin- guistic Data Consortium.\n\nCosmos qa: Machine reading comprehension with contextual commonsense reasoning. L Huang, R L Bras, C Bhagavatula, Y Choi, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaHuang, L. and Bras, R. L. and Bhagavatula, C. and Choi, Y. 2019. Cosmos qa: Machine reading com- prehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2391-2401, Hong Kong, China, November 3-7, 2019.\n\nSMART: Robust and Efficient Fine-Tuning for Pretrained Natural Language Models through Principled Regularized Optimization. Haoming Jiang, He, Pengcheng, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Tuo Zhao, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo. SMART: Robust and Efficient Fine-Tuning for Pre- trained Natural Language Models through Principled Regularized Optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2177-2190 July 5 -10, 2020.\n\nApplying the T5 language model and duration units normalization to address temporal common sense understanding on the MCTACO dataset. Z Kaddari, Y Mellah, J Berrich, T Bouchentouf, M G Belkasmi, arXiv:1412.69802020 International Conference on Intelligent Systems and Computer Vision (ISCV) (pp. 1-4). IEEE. Kingma, Diederik P and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint. Kaddari, Z. and Mellah, Y. and Berrich, J. and Bouchen- touf, T. and Belkasmi, M. G. 2020. Applying the T5 language model and duration units normalization to address temporal common sense understanding on the MCTACO dataset. In 2020 International Con- ference on Intelligent Systems and Computer Vision (ISCV) (pp. 1-4). IEEE. Kingma, Diederik P and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint. arXiv:1412.6980.\n\nAlbert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.11942arXiv preprintLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.\n\nJ Lin, Divergence measures based on the Shannon entropy. arXiv preprint. IEEE Transactions on Information theory. 37Lin, J. Divergence measures based on the Shannon en- tropy. arXiv preprint. IEEE Transactions on Informa- tion theory, 37(1), 145-151.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintLiu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n\nRepresentation learning using multi-task deep neural networks for semantic classification and information retrieval. Xiaodong Liu, Jianfeng Gao, He, Xiaodong, Li Deng, Kevin Duh, Ye-Yi Wang, Proceeding of The 2015 Annual Conference of the North American Chapter of the ACL. eeding of The 2015 Annual Conference of the North American Chapter of the ACLDenver, ColoradoLiu, Xiaodong and Gao, Jianfeng and He, Xiaodong and Deng, Li and Duh, Kevin and Wang, Ye-Yi. Represen- tation learning using multi-task deep neural networks for semantic classification and information retrieval. In Proceeding of The 2015 Annual Conference of the North American Chapter of the ACL, pages 912-921, Denver, Colorado, May 31 -June 5, 2015.\n\nMulti-task deep neural networks for natural language understanding. Xiaodong Liu, He, Pengcheng, Weizhu Chen, Jianfeng Gao, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsLiu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pp. 4487-4496. 2019.\n\nXiaodong Liu, Cheng, Hao, He, Pengcheng, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao, arXiv:2004.08994Adversarial Training for Large Neural Language Models. arXiv preprintLiu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng. Adversarial Training for Large Neural Language Models. arXiv preprint arXiv:2004.08994.\n\nThe Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding. Xiaodong Liu, Yu Wang, Ji , Jianshu Cheng, Hao, Xueyun Zhu, Emmanuel Awa, He, Pengcheng, Weizhu Chen, Hoifung Poon, Guihong Cao, Jianfeng Gao, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsLiu, Xiaodong and Wang, Yu and Ji, Jianshu and Cheng, Hao and Zhu, Xueyun and Awa, Emmanuel and He, Pengcheng and Chen, Weizhu and Poon, Hoifung and Cao, Guihong and Jianfeng Gao. The Microsoft Toolkit of Multi-Task Deep Neural Networks for Nat- ural Language Understanding. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 118-126 July 5 -July 10, 2020..\n\nVirtual adversarial training: a regularization method for supervised and semisupervised learning. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian ; Vladu, Takeru, Maeda, - Shin, Masanori Koyama, Shin Ishii, arXiv:1706.06083IEEE transactions. 8arXiv preprintTowards deep learning models resistant to adversarial attacksMadry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083. Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin. Virtual adversarial train- ing: a regularization method for supervised and semi- supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8), 1979-1993.\n\nLsdsem 2017 shared task: The story cloze test. N Mostafazadeh, M Roth, A Louis, N Chambers, Allen , J , Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourselevel Semantics. the 2nd Workshop on Linking Models of Lexical, Sentential and Discourselevel SemanticsMostafazadeh, N., Roth, M., Louis, A., Chambers, N., and Allen, J. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse- level Semantics (pp. 46-51).\n\nA multi-axis annotation scheme for event temporal relations. Qiang Ning, Wu, Roth Hao, Dan , arXiv:1804.07828arXiv preprintNing, Qiang and Wu, Hao and Roth, Dan. 2019. A multi-axis annotation scheme for event temporal rela- tions. arXiv preprint arXiv:1804.07828\n\nExtending TimeML with typical durations of events. Feng Pan, Rutu Mulkar-Mehta, Jerry R Hobbs, Proceedings of the Workshop on Annotating and Reasoning about Time and Events. the Workshop on Annotating and Reasoning about Time and EventsPan, Feng and Mulkar-Mehta, Rutu and Hobbs, Jerry R. 2006. Extending TimeML with typical durations of events. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events. (pp. 38-45).\n\nLis Pereira, Xiaodong Liu, Fei Cheng, Masayuki Asahara, Ichiro Kobayashi, arXiv:2005.08156Adversarial Training for Commonsense Inference. arXiv preprintPereira, Lis and Liu, Xiaodong and Cheng, Fei and Asa- hara, Masayuki and Kobayashi, Ichiro. Adversarial Training for Commonsense Inference. arXiv preprint arXiv:2005.08156.\n\nThe timebank corpus. James Pustejovsky, Patrick Hanks, Sauri, Roser, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, Corpus linguistics. 40Pustejovsky, James and Hanks, Patrick and Sauri, Roser and See, Andrew and Gaizauskas, Robert and Setzer, Andrea and Radev, Dragomir and Sundheim, Beth and Day, David and Ferro, Lisa and others. 2003. The timebank corpus. In Corpus linguistics (Vol. 2003, p. 40).\n\nMarco Ribeiro, Tulio, Tongshuang Wu, Carlos Guestrin, Sameer Singh, arXiv:2005.04118Beyond accuracy: Behavioral testing of NLP models with Check-List. arXiv preprintRibeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer. 2020. Beyond accu- racy: Behavioral testing of NLP models with Check- List. arXiv preprint arXiv:2005.04118\n\nAn overview of multi-task learning in. Sebastian Ruder, arXiv:1706.05098deep neural networks. arXiv preprintRuder, Sebastian. 2017. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098.\n\n. Roser Saur\u00ed, Jessica Littman, Bob Knippen, Robert Gaizauskas, Andrea Setzer, Pustejovsky , TimeML annotation guidelines. Version. 1131Saur\u00ed, Roser and Littman, Jessica and Knippen, Bob and Gaizauskas, Robert and Setzer, Andrea and Puste- jovsky, James 2006. TimeML annotation guidelines. Version, 1(1), 31.\n\nScene Restoring for Narrative Machine Reading Comprehension. Z Tian, Y Zhang, K Liu, J Zhao, Y Jia, Z Sheng, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Tian, Z., Zhang, Y., Liu, K., Zhao, J., Jia, Y., and Sheng, Z. 2020. Scene Restoring for Narrative Machine Reading Comprehension. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP) (pp. 3063-3073).\n\nEstimating temporal order relation for bccwj-timebank. Naushad Uzzaman, Hector Llorens, Leon Derczynski, Allen , James Verhagen, Marc Pustejovsky, James , Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations Second Joint Conference on Lexical and Computational Semantics (* SEM). Katsumasa Yoshikawa, and Masayuki Asahara, and Ryu Iida2Proceedings of the Japanese Annual Conference on NLP. in JapaneseUzZaman, Naushad and Llorens, Hector and Derczynski, Leon and Allen, James and Verhagen, Marc and Puste- jovsky, James 2013. Semeval-2013 task 1: Tempeval- 3: Evaluating time expressions, events, and tempo- ral relations Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Pro- ceedings of the Seventh International Workshop on Se- mantic Evaluation (SemEval 2013), pp.1-9, 2013. Katsumasa Yoshikawa, and Masayuki Asahara, and Ryu Iida. 2014. Estimating temporal order relation for bccwj-timebank. In Proceedings of the Japanese An- nual Conference on NLP. (in Japanese).\n\nGoing on a vacation\" takes longer than\" Going for a walk\": A Study of Temporal Commonsense Understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingZhou, Ben and Khashabi, Daniel and Ning, Qiang and Roth, Dan. 2019. \"Going on a vacation\" takes longer than\" Going for a walk\": A Study of Temporal Com- monsense Understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Con- ference on Natural Language Processing (EMNLP- IJCNLP), pp. 3363-3369. 2019.\n\nTemporal common sense acquisition with minimal supervision. Ben Zhou, Qiang Ning, Daniel Khashabi, Roth , Dan , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsZhou, Ben and Ning, Qiang and Khashabi, Daniel and Roth, Dan. 2020. Temporal common sense acquisi- tion with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics, pages 7579-7589 July 5 -10, 2020.\n\nFreeLB: Enhanced Adversarial Training for Language Understanding. B Zhou, K Richardson, Q Ning, T Khot, A Sabharwal, D Roth, Chen Zhu, Yu Cheng, Zhe Gan, Sun, Siqi, Thomas Goldstein, Jingjing Liu, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesICLRZhou, B. and Richardson, K. and Ning, Q., Khot, T., Sab- harwal, A. and Roth, D. 2021. Temporal reasoning on implicit events from distant supervision. In Pro- ceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1361-1371 June 6-11, 2021. Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Thomas and Liu, Jingjing. 2019. FreeLB: Enhanced Adversarial Training for Language Understanding. In ICLR, 2020.\n", "annotations": {"author": "[{\"end\":121,\"start\":78},{\"end\":130,\"start\":122},{\"end\":166,\"start\":131},{\"end\":206,\"start\":167},{\"end\":214,\"start\":207},{\"end\":221,\"start\":215},{\"end\":239,\"start\":222},{\"end\":286,\"start\":240},{\"end\":316,\"start\":287}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":88},{\"end\":129,\"start\":122},{\"end\":140,\"start\":135},{\"end\":183,\"start\":176},{\"end\":238,\"start\":229}]", "author_first_name": "[{\"end\":87,\"start\":78},{\"end\":134,\"start\":131},{\"end\":175,\"start\":167},{\"end\":213,\"start\":207},{\"end\":220,\"start\":215},{\"end\":228,\"start\":222}]", "author_affiliation": "[{\"end\":285,\"start\":241},{\"end\":315,\"start\":288}]", "title": "[{\"end\":75,\"start\":1},{\"end\":391,\"start\":317}]", "venue": null, "abstract": "[{\"end\":1134,\"start\":393}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1214,\"start\":1193},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1408,\"start\":1387},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2423,\"start\":2405},{\"end\":2442,\"start\":2423},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2459,\"start\":2442},{\"end\":2480,\"start\":2459},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2628,\"start\":2609},{\"end\":3266,\"start\":3247},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3518,\"start\":3492},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3541,\"start\":3528},{\"end\":3577,\"start\":3546},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3818,\"start\":3800},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4074,\"start\":4054},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4091,\"start\":4074},{\"end\":4445,\"start\":4418},{\"end\":6399,\"start\":6379},{\"end\":6423,\"start\":6399},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6497,\"start\":6479},{\"end\":6516,\"start\":6497},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6535,\"start\":6516},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6552,\"start\":6535},{\"end\":6573,\"start\":6552},{\"end\":6808,\"start\":6788},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6904,\"start\":6886},{\"end\":6923,\"start\":6904},{\"end\":6944,\"start\":6923},{\"end\":7013,\"start\":6991},{\"end\":7230,\"start\":7209},{\"end\":7249,\"start\":7230},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8079,\"start\":8060},{\"end\":8188,\"start\":8177},{\"end\":10045,\"start\":10030},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10062,\"start\":10045},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10074,\"start\":10062},{\"end\":10097,\"start\":10074},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10586,\"start\":10566},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11367,\"start\":11350},{\"end\":11878,\"start\":11860},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12758,\"start\":12741},{\"end\":13394,\"start\":13374},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14804,\"start\":14786},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15359,\"start\":15337},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16685,\"start\":16666},{\"end\":16743,\"start\":16725},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17099,\"start\":17077},{\"end\":17791,\"start\":17768},{\"end\":18431,\"start\":18408}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21843,\"start\":21699},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22566,\"start\":21844},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22677,\"start\":22567},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":23021,\"start\":22678},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":23182,\"start\":23022}]", "paragraph": "[{\"end\":1580,\"start\":1136},{\"end\":2882,\"start\":1582},{\"end\":3678,\"start\":2897},{\"end\":3788,\"start\":3680},{\"end\":4263,\"start\":3790},{\"end\":4306,\"start\":4265},{\"end\":4695,\"start\":4308},{\"end\":4902,\"start\":4697},{\"end\":5679,\"start\":4904},{\"end\":6670,\"start\":5720},{\"end\":7250,\"start\":6715},{\"end\":9891,\"start\":7343},{\"end\":10825,\"start\":9893},{\"end\":11155,\"start\":10827},{\"end\":11203,\"start\":11157},{\"end\":11234,\"start\":11205},{\"end\":11258,\"start\":11236},{\"end\":11282,\"start\":11260},{\"end\":11320,\"start\":11284},{\"end\":11624,\"start\":11322},{\"end\":12278,\"start\":11674},{\"end\":12656,\"start\":12280},{\"end\":13634,\"start\":12683},{\"end\":14149,\"start\":13651},{\"end\":14649,\"start\":14151},{\"end\":16169,\"start\":14651},{\"end\":16863,\"start\":16171},{\"end\":17902,\"start\":16898},{\"end\":18030,\"start\":17916},{\"end\":18464,\"start\":18032},{\"end\":18507,\"start\":18476},{\"end\":19135,\"start\":18579},{\"end\":19212,\"start\":19137},{\"end\":19681,\"start\":19227},{\"end\":19797,\"start\":19683},{\"end\":19923,\"start\":19799},{\"end\":20040,\"start\":19925},{\"end\":20678,\"start\":20042},{\"end\":20811,\"start\":20680},{\"end\":20944,\"start\":20813},{\"end\":21077,\"start\":20946},{\"end\":21698,\"start\":21079}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6714,\"start\":6671},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7342,\"start\":7251}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11744,\"start\":11737},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13684,\"start\":13677},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16973,\"start\":16966},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17842,\"start\":17835}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2895,\"start\":2885},{\"end\":5691,\"start\":5682},{\"attributes\":{\"n\":\"2\"},\"end\":5718,\"start\":5694},{\"attributes\":{\"n\":\"3\"},\"end\":11638,\"start\":11627},{\"attributes\":{\"n\":\"3.1\"},\"end\":11672,\"start\":11641},{\"attributes\":{\"n\":\"3.2\"},\"end\":12681,\"start\":12659},{\"attributes\":{\"n\":\"3.3\"},\"end\":13649,\"start\":13637},{\"attributes\":{\"n\":\"3.4\"},\"end\":16896,\"start\":16866},{\"end\":17914,\"start\":17905},{\"end\":18474,\"start\":18467},{\"attributes\":{\"n\":\"4\"},\"end\":18577,\"start\":18510},{\"attributes\":{\"n\":\"5\"},\"end\":19225,\"start\":19215},{\"end\":21701,\"start\":21700},{\"end\":21854,\"start\":21845},{\"end\":22577,\"start\":22568},{\"end\":22688,\"start\":22679},{\"end\":23032,\"start\":23023}]", "table": "[{\"end\":22566,\"start\":21856},{\"end\":23021,\"start\":22836}]", "figure_caption": "[{\"end\":21843,\"start\":21702},{\"end\":22677,\"start\":22579},{\"end\":22836,\"start\":22690},{\"end\":23182,\"start\":23034}]", "figure_ref": "[{\"end\":18704,\"start\":18696},{\"end\":19211,\"start\":19203},{\"end\":20165,\"start\":20157},{\"end\":21219,\"start\":21211}]", "bib_author_first_name": "[{\"end\":23384,\"start\":23376},{\"end\":23399,\"start\":23394},{\"end\":23412,\"start\":23406},{\"end\":23428,\"start\":23422},{\"end\":23441,\"start\":23436},{\"end\":23867,\"start\":23863},{\"end\":24058,\"start\":24057},{\"end\":24067,\"start\":24066},{\"end\":24078,\"start\":24077},{\"end\":24091,\"start\":24090},{\"end\":24657,\"start\":24656},{\"end\":24666,\"start\":24665},{\"end\":24673,\"start\":24672},{\"end\":24684,\"start\":24683},{\"end\":24690,\"start\":24689},{\"end\":25318,\"start\":25314},{\"end\":25328,\"start\":25326},{\"end\":25344,\"start\":25336},{\"end\":25658,\"start\":25653},{\"end\":25676,\"start\":25670},{\"end\":25684,\"start\":25679},{\"end\":25697,\"start\":25693},{\"end\":25706,\"start\":25700},{\"end\":25716,\"start\":25709},{\"end\":25735,\"start\":25730},{\"end\":25748,\"start\":25743},{\"end\":26105,\"start\":26100},{\"end\":26119,\"start\":26114},{\"end\":26130,\"start\":26122},{\"end\":26142,\"start\":26136},{\"end\":26162,\"start\":26154},{\"end\":26546,\"start\":26543},{\"end\":26548,\"start\":26547},{\"end\":26569,\"start\":26561},{\"end\":26587,\"start\":26578},{\"end\":26818,\"start\":26813},{\"end\":27217,\"start\":27216},{\"end\":27226,\"start\":27225},{\"end\":27228,\"start\":27227},{\"end\":27236,\"start\":27235},{\"end\":27251,\"start\":27250},{\"end\":28080,\"start\":28073},{\"end\":28109,\"start\":28103},{\"end\":28124,\"start\":28116},{\"end\":28138,\"start\":28130},{\"end\":28147,\"start\":28144},{\"end\":28804,\"start\":28803},{\"end\":28815,\"start\":28814},{\"end\":28825,\"start\":28824},{\"end\":28836,\"start\":28835},{\"end\":28851,\"start\":28850},{\"end\":28853,\"start\":28852},{\"end\":29588,\"start\":29587},{\"end\":29595,\"start\":29594},{\"end\":29603,\"start\":29602},{\"end\":29614,\"start\":29613},{\"end\":29624,\"start\":29623},{\"end\":29634,\"start\":29633},{\"end\":29859,\"start\":29858},{\"end\":30116,\"start\":30110},{\"end\":30126,\"start\":30122},{\"end\":30137,\"start\":30132},{\"end\":30152,\"start\":30145},{\"end\":30163,\"start\":30157},{\"end\":30176,\"start\":30171},{\"end\":30187,\"start\":30183},{\"end\":30198,\"start\":30194},{\"end\":30210,\"start\":30206},{\"end\":30231,\"start\":30224},{\"end\":30719,\"start\":30711},{\"end\":30733,\"start\":30725},{\"end\":30755,\"start\":30753},{\"end\":30767,\"start\":30762},{\"end\":30778,\"start\":30773},{\"end\":31392,\"start\":31384},{\"end\":31419,\"start\":31413},{\"end\":31434,\"start\":31426},{\"end\":31861,\"start\":31853},{\"end\":31900,\"start\":31894},{\"end\":31909,\"start\":31907},{\"end\":31923,\"start\":31916},{\"end\":31938,\"start\":31930},{\"end\":32333,\"start\":32325},{\"end\":32341,\"start\":32339},{\"end\":32350,\"start\":32348},{\"end\":32360,\"start\":32353},{\"end\":32379,\"start\":32373},{\"end\":32393,\"start\":32385},{\"end\":32420,\"start\":32414},{\"end\":32434,\"start\":32427},{\"end\":32448,\"start\":32441},{\"end\":32462,\"start\":32454},{\"end\":33145,\"start\":33135},{\"end\":33163,\"start\":33153},{\"end\":33179,\"start\":33173},{\"end\":33197,\"start\":33189},{\"end\":33213,\"start\":33207},{\"end\":33215,\"start\":33214},{\"end\":33239,\"start\":33238},{\"end\":33254,\"start\":33246},{\"end\":33267,\"start\":33263},{\"end\":33892,\"start\":33891},{\"end\":33908,\"start\":33907},{\"end\":33916,\"start\":33915},{\"end\":33925,\"start\":33924},{\"end\":33941,\"start\":33936},{\"end\":33945,\"start\":33944},{\"end\":34438,\"start\":34433},{\"end\":34453,\"start\":34449},{\"end\":34462,\"start\":34459},{\"end\":34691,\"start\":34687},{\"end\":34701,\"start\":34697},{\"end\":34721,\"start\":34716},{\"end\":34723,\"start\":34722},{\"end\":35081,\"start\":35078},{\"end\":35099,\"start\":35091},{\"end\":35108,\"start\":35105},{\"end\":35124,\"start\":35116},{\"end\":35140,\"start\":35134},{\"end\":35431,\"start\":35426},{\"end\":35452,\"start\":35445},{\"end\":35480,\"start\":35474},{\"end\":35492,\"start\":35486},{\"end\":35511,\"start\":35505},{\"end\":35528,\"start\":35520},{\"end\":35540,\"start\":35536},{\"end\":35556,\"start\":35551},{\"end\":35566,\"start\":35562},{\"end\":35866,\"start\":35861},{\"end\":35893,\"start\":35883},{\"end\":35904,\"start\":35898},{\"end\":35921,\"start\":35915},{\"end\":36263,\"start\":36254},{\"end\":36448,\"start\":36443},{\"end\":36463,\"start\":36456},{\"end\":36476,\"start\":36473},{\"end\":36492,\"start\":36486},{\"end\":36511,\"start\":36505},{\"end\":36531,\"start\":36520},{\"end\":36813,\"start\":36812},{\"end\":36821,\"start\":36820},{\"end\":36830,\"start\":36829},{\"end\":36837,\"start\":36836},{\"end\":36845,\"start\":36844},{\"end\":36852,\"start\":36851},{\"end\":37345,\"start\":37338},{\"end\":37361,\"start\":37355},{\"end\":37375,\"start\":37371},{\"end\":37393,\"start\":37388},{\"end\":37401,\"start\":37396},{\"end\":37416,\"start\":37412},{\"end\":37435,\"start\":37430},{\"end\":38436,\"start\":38433},{\"end\":38449,\"start\":38443},{\"end\":38465,\"start\":38460},{\"end\":38475,\"start\":38472},{\"end\":39236,\"start\":39233},{\"end\":39248,\"start\":39243},{\"end\":39261,\"start\":39255},{\"end\":39276,\"start\":39272},{\"end\":39282,\"start\":39279},{\"end\":39772,\"start\":39771},{\"end\":39780,\"start\":39779},{\"end\":39794,\"start\":39793},{\"end\":39802,\"start\":39801},{\"end\":39810,\"start\":39809},{\"end\":39823,\"start\":39822},{\"end\":39834,\"start\":39830},{\"end\":39842,\"start\":39840},{\"end\":39853,\"start\":39850},{\"end\":39876,\"start\":39870},{\"end\":39896,\"start\":39888}]", "bib_author_last_name": "[{\"end\":23392,\"start\":23385},{\"end\":23404,\"start\":23400},{\"end\":23420,\"start\":23413},{\"end\":23434,\"start\":23429},{\"end\":23449,\"start\":23442},{\"end\":23875,\"start\":23868},{\"end\":24064,\"start\":24059},{\"end\":24075,\"start\":24068},{\"end\":24088,\"start\":24079},{\"end\":24101,\"start\":24092},{\"end\":24663,\"start\":24658},{\"end\":24670,\"start\":24667},{\"end\":24681,\"start\":24674},{\"end\":24687,\"start\":24685},{\"end\":24694,\"start\":24691},{\"end\":25324,\"start\":25319},{\"end\":25334,\"start\":25329},{\"end\":25353,\"start\":25345},{\"end\":25668,\"start\":25659},{\"end\":25691,\"start\":25685},{\"end\":25728,\"start\":25717},{\"end\":25741,\"start\":25736},{\"end\":26112,\"start\":26106},{\"end\":26134,\"start\":26131},{\"end\":26152,\"start\":26143},{\"end\":26559,\"start\":26549},{\"end\":26576,\"start\":26570},{\"end\":26595,\"start\":26588},{\"end\":26824,\"start\":26819},{\"end\":27223,\"start\":27218},{\"end\":27233,\"start\":27229},{\"end\":27248,\"start\":27237},{\"end\":27256,\"start\":27252},{\"end\":28086,\"start\":28081},{\"end\":28090,\"start\":28088},{\"end\":28101,\"start\":28092},{\"end\":28114,\"start\":28110},{\"end\":28128,\"start\":28125},{\"end\":28142,\"start\":28139},{\"end\":28152,\"start\":28148},{\"end\":28812,\"start\":28805},{\"end\":28822,\"start\":28816},{\"end\":28833,\"start\":28826},{\"end\":28848,\"start\":28837},{\"end\":28862,\"start\":28854},{\"end\":29592,\"start\":29589},{\"end\":29600,\"start\":29596},{\"end\":29611,\"start\":29604},{\"end\":29621,\"start\":29615},{\"end\":29631,\"start\":29625},{\"end\":29642,\"start\":29635},{\"end\":29863,\"start\":29860},{\"end\":30120,\"start\":30117},{\"end\":30130,\"start\":30127},{\"end\":30143,\"start\":30138},{\"end\":30155,\"start\":30153},{\"end\":30169,\"start\":30164},{\"end\":30181,\"start\":30177},{\"end\":30192,\"start\":30188},{\"end\":30204,\"start\":30199},{\"end\":30222,\"start\":30211},{\"end\":30240,\"start\":30232},{\"end\":30723,\"start\":30720},{\"end\":30737,\"start\":30734},{\"end\":30741,\"start\":30739},{\"end\":30751,\"start\":30743},{\"end\":30760,\"start\":30756},{\"end\":30771,\"start\":30768},{\"end\":30783,\"start\":30779},{\"end\":31396,\"start\":31393},{\"end\":31400,\"start\":31398},{\"end\":31411,\"start\":31402},{\"end\":31424,\"start\":31420},{\"end\":31438,\"start\":31435},{\"end\":31865,\"start\":31862},{\"end\":31872,\"start\":31867},{\"end\":31877,\"start\":31874},{\"end\":31881,\"start\":31879},{\"end\":31892,\"start\":31883},{\"end\":31905,\"start\":31901},{\"end\":31914,\"start\":31910},{\"end\":31928,\"start\":31924},{\"end\":31942,\"start\":31939},{\"end\":32337,\"start\":32334},{\"end\":32346,\"start\":32342},{\"end\":32366,\"start\":32361},{\"end\":32371,\"start\":32368},{\"end\":32383,\"start\":32380},{\"end\":32397,\"start\":32394},{\"end\":32401,\"start\":32399},{\"end\":32412,\"start\":32403},{\"end\":32425,\"start\":32421},{\"end\":32439,\"start\":32435},{\"end\":32452,\"start\":32449},{\"end\":32466,\"start\":32463},{\"end\":33151,\"start\":33146},{\"end\":33171,\"start\":33164},{\"end\":33187,\"start\":33180},{\"end\":33205,\"start\":33198},{\"end\":33221,\"start\":33216},{\"end\":33229,\"start\":33223},{\"end\":33236,\"start\":33231},{\"end\":33244,\"start\":33240},{\"end\":33261,\"start\":33255},{\"end\":33273,\"start\":33268},{\"end\":33905,\"start\":33893},{\"end\":33913,\"start\":33909},{\"end\":33922,\"start\":33917},{\"end\":33934,\"start\":33926},{\"end\":34443,\"start\":34439},{\"end\":34447,\"start\":34445},{\"end\":34457,\"start\":34454},{\"end\":34695,\"start\":34692},{\"end\":34714,\"start\":34702},{\"end\":34729,\"start\":34724},{\"end\":35089,\"start\":35082},{\"end\":35103,\"start\":35100},{\"end\":35114,\"start\":35109},{\"end\":35132,\"start\":35125},{\"end\":35150,\"start\":35141},{\"end\":35443,\"start\":35432},{\"end\":35458,\"start\":35453},{\"end\":35465,\"start\":35460},{\"end\":35472,\"start\":35467},{\"end\":35484,\"start\":35481},{\"end\":35503,\"start\":35493},{\"end\":35518,\"start\":35512},{\"end\":35534,\"start\":35529},{\"end\":35549,\"start\":35541},{\"end\":35560,\"start\":35557},{\"end\":35572,\"start\":35567},{\"end\":35874,\"start\":35867},{\"end\":35881,\"start\":35876},{\"end\":35896,\"start\":35894},{\"end\":35913,\"start\":35905},{\"end\":35927,\"start\":35922},{\"end\":36269,\"start\":36264},{\"end\":36454,\"start\":36449},{\"end\":36471,\"start\":36464},{\"end\":36484,\"start\":36477},{\"end\":36503,\"start\":36493},{\"end\":36518,\"start\":36512},{\"end\":36818,\"start\":36814},{\"end\":36827,\"start\":36822},{\"end\":36834,\"start\":36831},{\"end\":36842,\"start\":36838},{\"end\":36849,\"start\":36846},{\"end\":36858,\"start\":36853},{\"end\":37353,\"start\":37346},{\"end\":37369,\"start\":37362},{\"end\":37386,\"start\":37376},{\"end\":37410,\"start\":37402},{\"end\":37428,\"start\":37417},{\"end\":38441,\"start\":38437},{\"end\":38458,\"start\":38450},{\"end\":38470,\"start\":38466},{\"end\":38480,\"start\":38476},{\"end\":39241,\"start\":39237},{\"end\":39253,\"start\":39249},{\"end\":39270,\"start\":39262},{\"end\":39777,\"start\":39773},{\"end\":39791,\"start\":39781},{\"end\":39799,\"start\":39795},{\"end\":39807,\"start\":39803},{\"end\":39820,\"start\":39811},{\"end\":39828,\"start\":39824},{\"end\":39838,\"start\":39835},{\"end\":39848,\"start\":39843},{\"end\":39857,\"start\":39854},{\"end\":39862,\"start\":39859},{\"end\":39868,\"start\":39864},{\"end\":39886,\"start\":39877},{\"end\":39900,\"start\":39897}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":534806},\"end\":23823,\"start\":23301},{\"attributes\":{\"id\":\"b1\"},\"end\":23945,\"start\":23825},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":226283750},\"end\":24566,\"start\":23947},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":225066789},\"end\":25310,\"start\":24568},{\"attributes\":{\"doi\":\"arXiv:1906.02443\",\"id\":\"b4\"},\"end\":25600,\"start\":25312},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":351666},\"end\":26016,\"start\":25602},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":26493,\"start\":26018},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b7\"},\"end\":26770,\"start\":26495},{\"attributes\":{\"id\":\"b8\"},\"end\":27134,\"start\":26772},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202540590},\"end\":27947,\"start\":27136},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207847598},\"end\":28667,\"start\":27949},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b11\",\"matched_paper_id\":221916874},\"end\":29507,\"start\":28669},{\"attributes\":{\"doi\":\"arXiv:1909.11942\",\"id\":\"b12\"},\"end\":29856,\"start\":29509},{\"attributes\":{\"id\":\"b13\"},\"end\":30108,\"start\":29858},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b14\"},\"end\":30592,\"start\":30110},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11754890},\"end\":31314,\"start\":30594},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":59523594},\"end\":31851,\"start\":31316},{\"attributes\":{\"doi\":\"arXiv:2004.08994\",\"id\":\"b17\"},\"end\":32230,\"start\":31853},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":211171937},\"end\":33035,\"start\":32232},{\"attributes\":{\"doi\":\"arXiv:1706.06083\",\"id\":\"b19\",\"matched_paper_id\":17504174},\"end\":33842,\"start\":33037},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13746570},\"end\":34370,\"start\":33844},{\"attributes\":{\"doi\":\"arXiv:1804.07828\",\"id\":\"b21\"},\"end\":34634,\"start\":34372},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2896894},\"end\":35076,\"start\":34636},{\"attributes\":{\"doi\":\"arXiv:2005.08156\",\"id\":\"b23\"},\"end\":35403,\"start\":35078},{\"attributes\":{\"id\":\"b24\"},\"end\":35859,\"start\":35405},{\"attributes\":{\"doi\":\"arXiv:2005.04118\",\"id\":\"b25\"},\"end\":36213,\"start\":35861},{\"attributes\":{\"doi\":\"arXiv:1706.05098\",\"id\":\"b26\",\"matched_paper_id\":90063862},\"end\":36439,\"start\":36215},{\"attributes\":{\"id\":\"b27\"},\"end\":36749,\"start\":36441},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":226262276},\"end\":37281,\"start\":36751},{\"attributes\":{\"id\":\"b29\"},\"end\":38325,\"start\":37283},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202541184},\"end\":39171,\"start\":38327},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":218581125},\"end\":39703,\"start\":39173},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202889044},\"end\":40690,\"start\":39705}]", "bib_title": "[{\"end\":23374,\"start\":23301},{\"end\":24055,\"start\":23947},{\"end\":24654,\"start\":24568},{\"end\":25651,\"start\":25602},{\"end\":26098,\"start\":26018},{\"end\":26811,\"start\":26772},{\"end\":27214,\"start\":27136},{\"end\":28071,\"start\":27949},{\"end\":28801,\"start\":28669},{\"end\":30709,\"start\":30594},{\"end\":31382,\"start\":31316},{\"end\":32323,\"start\":32232},{\"end\":33133,\"start\":33037},{\"end\":33889,\"start\":33844},{\"end\":34685,\"start\":34636},{\"end\":35424,\"start\":35405},{\"end\":36252,\"start\":36215},{\"end\":36810,\"start\":36751},{\"end\":37336,\"start\":37283},{\"end\":38431,\"start\":38327},{\"end\":39231,\"start\":39173},{\"end\":39769,\"start\":39705}]", "bib_author": "[{\"end\":23394,\"start\":23376},{\"end\":23406,\"start\":23394},{\"end\":23422,\"start\":23406},{\"end\":23436,\"start\":23422},{\"end\":23451,\"start\":23436},{\"end\":23877,\"start\":23863},{\"end\":24066,\"start\":24057},{\"end\":24077,\"start\":24066},{\"end\":24090,\"start\":24077},{\"end\":24103,\"start\":24090},{\"end\":24665,\"start\":24656},{\"end\":24672,\"start\":24665},{\"end\":24683,\"start\":24672},{\"end\":24689,\"start\":24683},{\"end\":24696,\"start\":24689},{\"end\":25326,\"start\":25314},{\"end\":25336,\"start\":25326},{\"end\":25355,\"start\":25336},{\"end\":25670,\"start\":25653},{\"end\":25679,\"start\":25670},{\"end\":25693,\"start\":25679},{\"end\":25700,\"start\":25693},{\"end\":25709,\"start\":25700},{\"end\":25730,\"start\":25709},{\"end\":25743,\"start\":25730},{\"end\":25751,\"start\":25743},{\"end\":26114,\"start\":26100},{\"end\":26122,\"start\":26114},{\"end\":26136,\"start\":26122},{\"end\":26154,\"start\":26136},{\"end\":26165,\"start\":26154},{\"end\":26561,\"start\":26543},{\"end\":26578,\"start\":26561},{\"end\":26597,\"start\":26578},{\"end\":26826,\"start\":26813},{\"end\":27225,\"start\":27216},{\"end\":27235,\"start\":27225},{\"end\":27250,\"start\":27235},{\"end\":27258,\"start\":27250},{\"end\":28088,\"start\":28073},{\"end\":28092,\"start\":28088},{\"end\":28103,\"start\":28092},{\"end\":28116,\"start\":28103},{\"end\":28130,\"start\":28116},{\"end\":28144,\"start\":28130},{\"end\":28154,\"start\":28144},{\"end\":28814,\"start\":28803},{\"end\":28824,\"start\":28814},{\"end\":28835,\"start\":28824},{\"end\":28850,\"start\":28835},{\"end\":28864,\"start\":28850},{\"end\":29594,\"start\":29587},{\"end\":29602,\"start\":29594},{\"end\":29613,\"start\":29602},{\"end\":29623,\"start\":29613},{\"end\":29633,\"start\":29623},{\"end\":29644,\"start\":29633},{\"end\":29865,\"start\":29858},{\"end\":30122,\"start\":30110},{\"end\":30132,\"start\":30122},{\"end\":30145,\"start\":30132},{\"end\":30157,\"start\":30145},{\"end\":30171,\"start\":30157},{\"end\":30183,\"start\":30171},{\"end\":30194,\"start\":30183},{\"end\":30206,\"start\":30194},{\"end\":30224,\"start\":30206},{\"end\":30242,\"start\":30224},{\"end\":30725,\"start\":30711},{\"end\":30739,\"start\":30725},{\"end\":30743,\"start\":30739},{\"end\":30753,\"start\":30743},{\"end\":30762,\"start\":30753},{\"end\":30773,\"start\":30762},{\"end\":30785,\"start\":30773},{\"end\":31398,\"start\":31384},{\"end\":31402,\"start\":31398},{\"end\":31413,\"start\":31402},{\"end\":31426,\"start\":31413},{\"end\":31440,\"start\":31426},{\"end\":31867,\"start\":31853},{\"end\":31874,\"start\":31867},{\"end\":31879,\"start\":31874},{\"end\":31883,\"start\":31879},{\"end\":31894,\"start\":31883},{\"end\":31907,\"start\":31894},{\"end\":31916,\"start\":31907},{\"end\":31930,\"start\":31916},{\"end\":31944,\"start\":31930},{\"end\":32339,\"start\":32325},{\"end\":32348,\"start\":32339},{\"end\":32353,\"start\":32348},{\"end\":32368,\"start\":32353},{\"end\":32373,\"start\":32368},{\"end\":32385,\"start\":32373},{\"end\":32399,\"start\":32385},{\"end\":32403,\"start\":32399},{\"end\":32414,\"start\":32403},{\"end\":32427,\"start\":32414},{\"end\":32441,\"start\":32427},{\"end\":32454,\"start\":32441},{\"end\":32468,\"start\":32454},{\"end\":33153,\"start\":33135},{\"end\":33173,\"start\":33153},{\"end\":33189,\"start\":33173},{\"end\":33207,\"start\":33189},{\"end\":33223,\"start\":33207},{\"end\":33231,\"start\":33223},{\"end\":33238,\"start\":33231},{\"end\":33246,\"start\":33238},{\"end\":33263,\"start\":33246},{\"end\":33275,\"start\":33263},{\"end\":33907,\"start\":33891},{\"end\":33915,\"start\":33907},{\"end\":33924,\"start\":33915},{\"end\":33936,\"start\":33924},{\"end\":33944,\"start\":33936},{\"end\":33948,\"start\":33944},{\"end\":34445,\"start\":34433},{\"end\":34449,\"start\":34445},{\"end\":34459,\"start\":34449},{\"end\":34465,\"start\":34459},{\"end\":34697,\"start\":34687},{\"end\":34716,\"start\":34697},{\"end\":34731,\"start\":34716},{\"end\":35091,\"start\":35078},{\"end\":35105,\"start\":35091},{\"end\":35116,\"start\":35105},{\"end\":35134,\"start\":35116},{\"end\":35152,\"start\":35134},{\"end\":35445,\"start\":35426},{\"end\":35460,\"start\":35445},{\"end\":35467,\"start\":35460},{\"end\":35474,\"start\":35467},{\"end\":35486,\"start\":35474},{\"end\":35505,\"start\":35486},{\"end\":35520,\"start\":35505},{\"end\":35536,\"start\":35520},{\"end\":35551,\"start\":35536},{\"end\":35562,\"start\":35551},{\"end\":35574,\"start\":35562},{\"end\":35876,\"start\":35861},{\"end\":35883,\"start\":35876},{\"end\":35898,\"start\":35883},{\"end\":35915,\"start\":35898},{\"end\":35929,\"start\":35915},{\"end\":36271,\"start\":36254},{\"end\":36456,\"start\":36443},{\"end\":36473,\"start\":36456},{\"end\":36486,\"start\":36473},{\"end\":36505,\"start\":36486},{\"end\":36520,\"start\":36505},{\"end\":36534,\"start\":36520},{\"end\":36820,\"start\":36812},{\"end\":36829,\"start\":36820},{\"end\":36836,\"start\":36829},{\"end\":36844,\"start\":36836},{\"end\":36851,\"start\":36844},{\"end\":36860,\"start\":36851},{\"end\":37355,\"start\":37338},{\"end\":37371,\"start\":37355},{\"end\":37388,\"start\":37371},{\"end\":37396,\"start\":37388},{\"end\":37412,\"start\":37396},{\"end\":37430,\"start\":37412},{\"end\":37438,\"start\":37430},{\"end\":38443,\"start\":38433},{\"end\":38460,\"start\":38443},{\"end\":38472,\"start\":38460},{\"end\":38482,\"start\":38472},{\"end\":39243,\"start\":39233},{\"end\":39255,\"start\":39243},{\"end\":39272,\"start\":39255},{\"end\":39279,\"start\":39272},{\"end\":39285,\"start\":39279},{\"end\":39779,\"start\":39771},{\"end\":39793,\"start\":39779},{\"end\":39801,\"start\":39793},{\"end\":39809,\"start\":39801},{\"end\":39822,\"start\":39809},{\"end\":39830,\"start\":39822},{\"end\":39840,\"start\":39830},{\"end\":39850,\"start\":39840},{\"end\":39859,\"start\":39850},{\"end\":39864,\"start\":39859},{\"end\":39870,\"start\":39864},{\"end\":39888,\"start\":39870},{\"end\":39902,\"start\":39888}]", "bib_venue": "[{\"end\":24282,\"start\":24201},{\"end\":24967,\"start\":24840},{\"end\":26232,\"start\":26196},{\"end\":27581,\"start\":27420},{\"end\":28315,\"start\":28243},{\"end\":30961,\"start\":30868},{\"end\":31601,\"start\":31529},{\"end\":32629,\"start\":32557},{\"end\":34137,\"start\":34051},{\"end\":34872,\"start\":34810},{\"end\":37035,\"start\":36956},{\"end\":38789,\"start\":38644},{\"end\":39446,\"start\":39374},{\"end\":40173,\"start\":40046},{\"end\":23536,\"start\":23451},{\"end\":23861,\"start\":23825},{\"end\":24199,\"start\":24103},{\"end\":24838,\"start\":24696},{\"end\":25787,\"start\":25751},{\"end\":26194,\"start\":26165},{\"end\":26541,\"start\":26495},{\"end\":26873,\"start\":26826},{\"end\":27418,\"start\":27258},{\"end\":28241,\"start\":28154},{\"end\":29068,\"start\":28879},{\"end\":29585,\"start\":29509},{\"end\":29970,\"start\":29865},{\"end\":30313,\"start\":30258},{\"end\":30866,\"start\":30785},{\"end\":31527,\"start\":31440},{\"end\":32013,\"start\":31960},{\"end\":32555,\"start\":32468},{\"end\":33308,\"start\":33291},{\"end\":34049,\"start\":33948},{\"end\":34431,\"start\":34372},{\"end\":34808,\"start\":34731},{\"end\":35214,\"start\":35168},{\"end\":35592,\"start\":35574},{\"end\":36010,\"start\":35945},{\"end\":36307,\"start\":36287},{\"end\":36571,\"start\":36534},{\"end\":36954,\"start\":36860},{\"end\":37601,\"start\":37438},{\"end\":38642,\"start\":38482},{\"end\":39372,\"start\":39285},{\"end\":40044,\"start\":39902}]"}}}, "year": 2023, "month": 12, "day": 17}