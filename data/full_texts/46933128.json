{"id": 46933128, "updated": "2023-11-08 11:27:39.577", "metadata": {"title": "Eye in the Sky: Real-time Drone Surveillance System (DSS) for Violent Individuals Identification using ScatterNet Hybrid Deep Learning Network", "authors": "[{\"first\":\"Amarjot\",\"last\":\"Singh\",\"middle\":[]},{\"first\":\"Devendra\",\"last\":\"Patil\",\"middle\":[]},{\"first\":\"SN\",\"last\":\"Omkar\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "publication_date": {"year": 2018, "month": 6, "day": 3}, "abstract": "Drone systems have been deployed by various law enforcement agencies to monitor hostiles, spy on foreign drug cartels, conduct border control operations, etc. This paper introduces a real-time drone surveillance system to identify violent individuals in public areas. The system first uses the Feature Pyramid Network to detect humans from aerial images. The image region with the human is used by the proposed ScatterNet Hybrid Deep Learning (SHDL) network for human pose estimation. The orientations between the limbs of the estimated pose are next used to identify the violent individuals. The proposed deep network can learn meaningful representations quickly using ScatterNet and structural priors with relatively fewer labeled examples. The system detects the violent individuals in real-time by processing the drone images in the cloud. This research also introduces the aerial violent individual dataset used for training the deep network which hopefully may encourage researchers interested in using deep learning for aerial surveillance. The pose estimation and violent individuals identification performance is compared with the state-of-the-art techniques.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1806.00746", "mag": "2963862187", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/SinghPO18", "doi": "10.1109/cvprw.2018.00214"}}, "content": {"source": {"pdf_hash": "dbf4dce028a54fe67477a7b080111b12487ec39e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1806.00746v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1806.00746", "status": "GREEN"}}, "grobid": {"id": "a1d0025538d049588d0f2d448f12b2300ecccfe3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dbf4dce028a54fe67477a7b080111b12487ec39e.txt", "contents": "\nEye in the Sky: Real-time Drone Surveillance System (DSS) for Violent Individuals Identification using ScatterNet Hybrid Deep Learning Network\n2018\n\nAmarjot Singh \nDepartment of Engineering\nUniversity of Cambridge\nU.K\n\nSN Omkar Indian Institute of Science\nDevendra Patil National Institute of Technology Warangal\nBangaloreIndia, India\n\nEye in the Sky: Real-time Drone Surveillance System (DSS) for Violent Individuals Identification using ScatterNet Hybrid Deep Learning Network\n\nthe IEEE Computer Vision and Pattern Recognition (CVPR)\n2018\nDrone systems have been deployed by various law enforcement agencies to monitor hostiles, spy on foreign drug cartels, conduct border control operations, etc. This paper introduces a real-time drone surveillance system to identify violent individuals in public areas. The system first uses the Feature Pyramid Network to detect humans from aerial images. The image region with the human is used by the proposed ScatterNet Hybrid Deep Learning (SHDL) network for human pose estimation. The orientations between the limbs of the estimated pose are next used to identify the violent individuals. The proposed deep network can learn meaningful representations quickly using ScatterNet and structural priors with relatively fewer labeled examples. The system detects the violent individuals in real-time by processing the drone images in the cloud. This research also introduces the aerial violent individual dataset used for training the deep network which hopefully may encourage researchers interested in using deep learning for aerial surveillance. The pose estimation and violent individuals identification performance is compared with the state-ofthe-art techniques.\n\nIntroduction\n\nThe rate of criminal activities by individuals and threats by terrorist groups has been on the rise in recent years. The law enforcement agencies have been motivated to use video surveillance systems to monitor and curb these threats. Many automated video surveillance systems have been developed in the past to monitor abandoned objects (bags) [15], theft [5], fire or smoke [23], violent activities [9], etc.\n\nLi et al. [15] developed a video surveillance system to identify the abandoned objects with the use of Gaussian mixture models and Support Vector Machine. This system is robust to illumination changes and performs with an accuracy of 84.44%. This system is vital for the detection of abandon bags in busy public areas, which may contain bombs. Chuang et al. [5] used Forward-backward ratio histogram and a finite state machine to recognize robberies. This system has proven to be very useful around automatic teller machines (ATMs) and has detected 96% cases of the theft. Seebamrungsat et al. [23] presented a fire detection system based on HSV and YCbCr color models as it allowed it to distinguish bright images more efficiently than other RGB models. The system has been shown to detect fire with an accuracy of more than 90.0%. Goya et al. [9] introduced a Public Safety System (PSS) for identifying criminal actions such as purse snatching, child kidnapping, and fighting using distance, velocity, and area to determine the human behavior. This system can identify the criminal actions with an accuracy of around 85%. These reported systems have been very successful in detecting and reporting various criminal activities. Despite their impressive performance (more than 90% accuracy), the area these systems can monitor is limited due to the restricted field of view of the cameras. The law enforcement agencies have been motivated to use aerial surveillance systems to surveil large areas. Governments have recently deployed drones in war zones to monitor hostiles, to spy on foreign drug cartels [18], conducting border control operations [32] as well as finding criminal activity in urban and rural areas [13]. One or more soldiers pilot most of these drones for long durations which makes these systems prone to mistakes due to the human fatigue.\n\nSurya et al. [19] proposed an autonomous drone surveillance system capable of detecting individuals engaged in violent activities in public areas. This first of its kind system used the deformable parts model [6] to estimate human poses which are then used to identify the suspicious individuals. This is an extremely challenging task as the images or videos recorded by the drone can suffer from illumination changes, shadows, poor resolution, and blurring. Also, the humans can appear at different locations, orientations, and scales. Despite the above-explained complications, the system can detect violent activated with an accuracy of around 76% which is far less as compared to the greater than 90% performance of the ground surveillance systems.\n\nThis paper introduces an improved real-time autonomous drone surveillance system to identify violent individuals in public areas. The proposed method first uses the feature pyramid network (FPN) [16] is used to detect the humans from the aerial image. Next, the proposed Scat-terNet Hybrid Deep Learning (SHDL) network is used to estimate the pose for each detected human. Finally, the orientations between the limbs of the estimated pose are used by the support vector machine (SVM) to identify individuals engaged in violent activities.\n\nThe novelties of the proposed system and the advantages over Surya et al.'s [19] technique are detailed below:\n\n\u2022 Accurate Human Pose Estimation: The proposed system uses the SHDL for human pose estimation. Deep networks have achieved the state-of-the-art pose estimation performance with high-level features [14,22,31] which gives the proposed system a competitive edge. \u2022 ScatterNet Hybrid Deep Network: The proposed SHDL network for pose estimation is composed of a hand-crafted ScatterNet front-end and a supervised learning based back-end formed of the modified coarse-to-fine deep regression network [1], referred from now as the regression network (RN). The SHDL network is constructed by replacing the first convolutional, relu and pooling layers of the coarse-tofine deep regression network [1] with the hand-crafted parametric log ScatterNet [27]. This accelerates the learning of the regression network (RN) as the Scatter-Net front-end extracts invariant (translation, rotation, and scale) [24] edge features which can be directly used to learn more complex patterns from the start of learning. The invariant edge features can be beneficial for this application as the humans can appear with these variations in the aerial images. \u2022 Rapid Training with Structural Priors: Training of the SHDL network can be slow as it requires the optimization of several hyperparameters. The training is shown to accelerate by initializing the CNN layer filters of the regression network with structural priors learned (unsupervised) using the PCANet [4] framework (Fig. 3). The initialization with priors also reduces the need for sizeable labeled training datasets for effective training which is especially advantageous for this task or other applications [25,10] as it can be expensive and time-consuming to generate keypoint annotations. \u2022 Real-time Identification: The proposed system performs the computation and memory demanding SHDL network processes along with the activity classification technique on the cloud while keeping short-term navigation onboard. This allows the system to identify violent individuals in real-time which is an improvement over the previous work of Surya et al. [19]. The proposed Drone Surveillance System (DSS) is used to identify the individuals engaged in violent activities from aerial images. The pose estimation and activity classification performance of the system is compared with the stateof-the-art techniques.\n\nThe paper is divided into the following sections. Section 2 presents the introduced AVI dataset while Section 3 introduces the proposed DSS system. Section 4 details the experimental results and Section 5 concludes this research.\n\n\nAerial Violent Individual (AVI) Dataset\n\nThis research proposes an annotated Aerial Violent Individual (AVI) dataset which is used by the proposed SHDL network to learn pose estimation. The dataset is composed of 2000 images where each image contains two and ten humans. The complete datasets consist of 10863 humans with 5124 (48%) engaged in one or more of the five violent activities of (1) Punching, (2) Stabbing, (3) Shooting, (4) Kicking, and (5) Strangling as shown in Fig. 1. Each human in the aerial image frame is annotated with 14 key-points which are utilized by the proposed network as labels for learning pose estimation as shown in Fig. 2. These activities are performed by 25 subjects between the ages of 18-25 years. These images are recorded from the parrot drone at four heights of 2m, 4m, 6m and 8m (m: meters). The violent individual identification task from these aerial images is an extremely challenging problem as these images can be affected by illumination changes, shadows, poor resolution, and blurring. In addition to these variations, the humans can appear at different locations, orientations, and scales. The proposed dataset includes images with the above-detailed variations as these can significantly alter the appearance of the humans and affect the performance of the surveillance systems. The SHDL network, when trained on the AVI dataset with these variations, can learn to recognize humans despite these variations.\n\n\nDrone Surveillance System\n\nThis section presents the Drone Surveillance System (DSS) for the identification of individuals engaging in violent activities. The system first uses the feature pyramid network (FPN) [16] to detect humans from the images recorded by the drone. The proposed ScatterNet Hybrid Deep Learning (SHDL) Network is then used to estimate the pose of each detected human. Finally, the orientations between the limbs of the estimated pose are used to identify the violent individuals. The system uses cloud computation to achieve the identification in real-time. Each part of the Drone Surveillance System (DSS) is explained in the following sub-sections.\n\n\nHuman Detection\n\nThe DSS system makes uses of the feature pyramid network (FPN) [16] to detect humans quickly from the images recorded by the drone. The FPN network detects the humans by leveraging the pyramidal shape of a ConvNets feature hierarchy while creating a feature pyramid that has strong semantics at all scales. The result is a feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale.\n\n\nScatterNet Hybrid Deep Learning Network\n\nThis section details the proposed ScatterNet Hybrid Deep Learning (SHDL) network, inspired from Singh et al.'s work in [28,29,25,30], composed by combining the hand-crafted (front-end) two-layer parametric log Scat-terNet [27] with the regression network (RN) (back-end) shown in Fig. 3. The ScatterNet accelerates the learning of the SHDL network by extracting invariant edge-based features which allow the SHDL network to learn complex features from the start of the learning [28]. The regression network also uses structural priors to expedite the training as well as reduce the dependence on the annotated datasets. The ScatterNet (front-end) and regression network (RN) (back-end) parts of the proposed SHDL network are presented below.\n\nScatterNet (front-end): The parametric log based DTCWT ScatterNet [27] is an improved numerous version of the hand-crafted multi-layer Scattering Networks [3,26,17] proposed over the years. The parametric log Scatter-Net extracts relatively symmetric translation invariant representations using the dual-tree complex wavelet transform (DTCWT) [12] and parametric log transformation layer. The ScatterNet features are denser over scale as they are extracted from multi-resolution images at 1.5 times and twice the size of the input image. Below we present the formulation of the parametric DTCWT ScatterNet for a single input image which may then be applied to each of the multiresolution images.\n\nThe parametric log ScatterNet is a hand-crafted twolayer network which extracts translation invariant feature representation from an input image or signal. The invariant features are obtained at the first layer by filtering the input signal x with dual-tree complex wavelets (better than cosine transforms [11]) \u03c8 j,r at different scales (j) and six predefined orientations (r) fixed to 15 \u2022 , 45 \u2022 , 75 \u2022 , 105 \u2022 , 135 \u2022 and 165 \u2022 . To build a more translation invariant represen- Figure 3. Illustration presents the {textithuman pose estimation pipeline that can be used to detect violent individuals in public areas or large gatherings. The DSS framework uses the image recorded by the drone first to discover the humans within the image using the FPN network [16]. The image regions containing the humans are given as input to the proposed SHDL network to detect 14 key-points on the body for pose estimation. The proposed SHDL network uses the ScatterNet (front-end) to extract hand-crafted features from the input region at L0, L1, and L2 using DTCWT filters at two scales and six fixed orientations. The handcrafted features extracted from the three layers are concatenated and given as input to the 4 Convolutional layers of the Regression Network (RN) (L3, L4, L5, L6) (back-end) with 32, 32, 64 and 64 filters. Each RN convolutional layer is initialized with the PCA based structural priors with the same number of filters. PCA layers can learn the undesired checkerboard filters (shown in red) which are avoided and not used as the prior for the Regression Network. To detect and remove the checkerboard filters from the learned filter set; we used the method defined in [7]. The ScatterNets and Structural priors have shown to improve the training of the proposed SHDL network as compared to the original coarse-to-fine regression network [1] (which was modified to obtain the SHDL) as shown from the convergence graph. The 14 key-points detected on the human are connected to construct the skeleton structure. The hand-crafted filters for the ScatterNet, learned structural PCA priors and the learned filters of the regression network (RN) are shown. tation, a point-wise L 2 non-linearity (complex modulus) is applied to the real and imaginary part of the filtered signal:\nU [\u03bb m=1 ] = |x \u03c8 \u03bb1 | = |x \u03c8 a \u03bb1 | 2 + |x \u03c8 b \u03bb1 | 2 (1)\nThe parametric log transformation layer is then applied to all the oriented representations extracted at the first scale j = 1 with a parameter k j=1 , to reduce the effect of outliers by introducing relative symmetry of pdf [27], as shown below:\nU 1[j] = log(U [j] + k j ), U [j] = |x \u03c8 j |,(2)\nNext, a local average is computed on the envelope |U 1[\u03bb m=1 ]| that aggregates the coefficients to build the desired translation-invariant representation:\nS 1 [\u03bb m=1 ] = |U 1[\u03bb m=1 ]| \u03c6 2 J(3)\nThe high frequency components lost due to smoothing are retrieved by cascaded wavelet filtering performed at the second layer. Translation invarinace is introduced in these features by applying the L2 non-linearity with averaing as explained above for the first layer [27].\n\nThe scattering coefficients at L0, L1, and L2 are:\nS = x \u03c6 2 J , S 1 [\u03bb m=1 ], S 2 [\u03bb m=1 , \u03bb m=2 ] \u03c6 2 J(4)\nThe rotation and scale invariance are next obtained by filtering jointly across the position (u), rotation (\u03b8) and scale(j) variables as detailed in [24].\n\nThe features extracted from each multi-resolution at L0, L1, and L2 are concatenated and given as input to the regression network (RN), to learn high-level features for human pose estimation. The ScatterNet features help the proposed SHDL to converge faster as the convolutional layers of the regression network can learn more complex patterns from the start of learning as it is not necessary to wait for the first layer to learn invariant edges as the ScatterNet already extracts them.\n\nPose Estimation with Structural Priors (back-end): The invariant ScatterNet features are used by the regression network (RN) of the SHDL network to learn pose estimation from the AVI dataset. The regression network was constructed by removing the first convolutional, relu, pooling, and normalization layers of the coarse-to-fine deep regression network [1]. The regression network (RN) of the SHDL is composed of four convolutional (L3 to L6 layers), two fully connected, normalization, and max-pooling layers as shown in Fig. 3.\n\nThe training objective is to estimate the optimal weights of the filters in the convolutional layers using the AVI training dataset D = (S; Y ), which minimizes the Tukey's biweight loss function [1] of the network. Here S are the Scat-terNet features extracted from the input image (X) while Y is a 28 element vector of (x, y) corresponding to the 14 keypoints annotated on the human body as shown in Fig. 2. The network is optimized using backpropagation with stochastic gradient descent. Dropout is utilized to avoid overfitting. Tukey's biweight loss function is very efficient as it suppresses the influence of outliers during backpropagation by reducing the magnitude of the gradient close to zero [1].\n\nStructural Priors: Each convolutional layer (L3 to L6) of the regression network (RN) of the SHDL network is initialized with structural priors to accelerate the training. The Structural priors are obtained for each layer using the PCANet [4] framework that learns a family of orthonormal filters by minimizing the following reconstruction error:\nmin V R z 1 z 2 \u00d7K X \u2212 V V T X 2 F , s.t. V V T = I K(5)\nWhere X are patches sampled from N training features, I K is an identity matrix of size K \u00d7 K. The solution of Eq. 5 in its simplified form represents K leading principal eigenvectors of XX T obtained using Eigen decomposition. The structural priors for layer 3 (L3) are learned on the ScatterNet features, layer 4 (L4) on layer 3 outputs, layer 5 (L5) on layer 4 outputs and so on. The structural priors for L3 to L6 layers learn filters that respond to a hierarchy of features, similar to the features learned by the CNN's. These learned priors are used to initialize each convolutional layer resulting in accelerated training as shown in Fig. 3 (Graph). Since it is swift to determine the structural priors, the whole process is much quicker than training CNN's with random weight initialization. The PCA framework may learn undesired checkerboard filters. To detect the checker-board filters from the learned filter set, we use the method defined in [7]. These checkerboard filters are avoided as filter priors.\n\n\nViolent Individual Classification\n\nThe 14 key-points identified by the SHDL network are connected to form a skeleton structure as shown in Fig. 3. The orientations between the limbs of the skeleton structure are derived as shown in Fig. 4. A support vector machine (SVM) is trained on a vector of these orientations for six classes (five violent activities and one neutral activity) to perform multi-class classification. During test time, the orientations between the limbs of the skeleton are given as input to the SVM which classifies the humans as either neural or assigns the most likely violent activity label.\n\n\nDrone Image Acquisition and Cloud Processing\n\nThe images that form the AVI dataset, presented in Section 2 are recorded using a Parrot AR Drone. The AR Drone 2.0 consists of two cameras, an Inertial Measurement Unit (IMU) including a 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer, and ultrasound and pressure-based altitude sensors. It features a 1 GHz ARM Cortex-A8 as the CPU and runs a Linux operating system. The front-facing camera has a resolution of 1280\u00d7720 at 30fps with a diagonal field of view of 92 \u2022 while the downward facing camera is of the lower resolution of 320\u00d7240 at 60fps with a diagonal field of view of 64 \u2022 . We use the frontfacing camera to record the images due to its higher resolution. The downward facing camera estimates the parameters determining the state of the drone such as roll, pitch, yaw, and altitude using the sensors onboard to measure the horizontal velocity. The horizontal velocity calculation is based on an optical flow-based feature as detailed in [2]. All the sensor measurements are updated at the 200Hz rate.\n\nThe images recorded by the drone are transferred to the Amazon cloud to achieve real-time identification. The slow and memory intensive computations of the SHDL network are processed on the Amazon cloud while keeping shortterm navigation onboard. Cloud computing has given the flexibility of using unlimited computational resources (including GPUs) which provides an edge with for applications requiring vast amounts of computational power periodically [8].\n\n\nExperimental Results\n\nThis section presents the training details and the performance of the Drone Surveillance System (DSS) for the identification of violent individuals on the AVI dataset. The Figure 5. Illustration shows the pose estimation performance via the detection of key-points for the (a) arms region, which constitutes the wrist, shoulder and elbow, (b) legs region, which includes ankle, knee, and hip, and, (c) facial regions with the head and neck.\n\nDSS system uses the FPN network [16] first to detect the humans, the SHDL network for human pose estimation, and then the orientations of the limbs of the estimated pose are used to identify the violent individuals. The next sections detail the performance of each part of the DSS system. The classification performance is also compared with the stateof-the-art technique proposed by Surya et al. [19], used to identify persons of interest from aerial images.\n\n\nHuman Detector\n\nThe FPN network [16] pre-trained on the 80 category COCO detection dataset is used to detect the humans recorded by the drone in the AVI dataset. The FPN network was able to detect 10558 humans out of the 10863 humans, with an accuracy of 97.2%.\n\n\nSHDL Parameters and Training\n\nThe image regions detected by the FPN network are resized to 120 \u00d7 80 and normalized by subtracting the image regions mean and dividing by its standard deviation.\n\nScatterNet: The resultant image region is given as input to the ScatterNet (SHDL front-end) which extracts invariant edge representations at L0, L1, and L2 using DTCWT filters at 2 scales, and 6 fixed orientations.\n\nRegression Network with Structural Priors: The regression network (SHDL back-end) with four convolutional layers (L3-L6) is trained on the concatenated ScatterNet features (L0, L1, and L2) extracted from the 10558 image regions (detected by FPN network, Section 4.1). The network was trained on randomly selected 6334 image regions (60%), validated against 2111 image regions (20%) and tested on the remaining 2113 image regions (20%). The network parameters are as follows: The base learning rate is 10 \u22125 , which we decrease to 10 \u22126 after 20 iterations, the dropout is 0.5, the batch size is 20, and the total number of iterations (epochs) is 90. The filters of the convolutional layers are initialized with structural priors which are shown to accelerate the training as compared to the DeepPose network [31] as detailed in Section 3.2 and illustrated from the convergence graph in Fig. 3. \n\n\nKey-Point Detection Performance\n\nThe pose estimation performance of the SHDL network is evaluated by comparing the coordinates of the detected 14 key-points with their ground truth values on the annotated dataset. The key-point is deemed correctly located if it is within a set distance of d pixels from a marked key-point in the ground truth, as shown in Fig. 5 via the accuracy vs. distance graphs, for different regions of the body.\n\nThe key-points detection analysis for the arms, legs, and facial, region is presented below.\n\nArms Region: The arm region constitutes six points namely: wrist key-points (P5 and P8), shoulder key-points (P3 and P6), and elbow key-points(P4 and P7), as shown in Fig. 2. Fig. 5(a) indicates that the SHDL network can detect the wrist region key-points with an accuracy of around 60%, for a pixel distance of d=5. The detection accuracy is much higher for the elbow and shoulder region at roughly 85% and 95% respectively, for the same pixel distance (d=5).\n\nLegs Region: The leg region constitutes six key-points, namely: hip key-points (P9, P12), knee key-points (P10, P13), and ankle key-points (P11, P14), as shown in Fig. 2. Fig. 5(b) indicates that the SHDL network detects hip keypoints with almost 100% for a pixel distance of d=5. The detection accuracy is between 85% and 90% for the knee key-points while the detection rate falls to around 85% for the ankle key-points.\n\nFacial Region: The facial region constitutes two points, one the head (P1) and the other on the neck (P2), as shown in Fig. 2. The algorithm detects the neck key-point (P2) more accurately as compared the head key-point (P1) with an accuracy of around 95% as opposed to roughly 77% accuracy, for a pixel distance of d=5, as shown in Fig. 5(c). The human pose estimation performance of the SHDL network on the Aerial Violent Individual (AVI) dataset is presented in Table 1. As observed from the Table, [21] and Spatial network [21] based on the detection of the 14 key-points. The evaluation is presented on the AVI dataset for maximum 5 pixels allowed distance (d=5) from the annotated ground truth.\n\nThe human pose estimation performance of the SHDL network is also compared with several state-of-the-art pose estimation methods such as CoordinateNet (CN) [20], Co-ordinateNet extended(CNE) [20], and SpatialNet [21]. The proposed SHDL network outperforms them by a decent margin.\n\n\nViolent Individuals Identification\n\nThe detected key-points are connected to form a skeleton structure as shown in Fig. 3. The orientations between the limbs are concatenated as a vector. A support vector machine (SVM) with a Gaussian kernel is trained on the orientation vector for each class of violent activity and one neutral class for 6334 randomly selected human poses (60%) to perform the multi-class classification. The SVM parameter (c) is selected as 14 while gamma parameter is set to 0.00002 using 5-fold cross-validation on the training set. The classification accuracy on the AVI dataset of each violent activity is presented for 4224 (40%) human poses as shown in Table 2.\n\nThe accuracy of the strangling and shooting activities are  relatively lower due to their similarity as shown in Fig. 6. Next, the classification accuracy for varying number of human subjects engaged in a violent activity per image is shown in Table 3 The accuracy of the DSS system decreases with the increase in the number of humans in the aerial image. This can be due to the inability of the FPN network [16] to locate all the humans or the incapability of the SHDL network to estimate the pose of the humans accurately. The incorrect pose can result in a wrong orientations vector which can lead the SVM to classify the activities incorrectly.\n\nThe results presented in above table are encouraging as the system is more likely to encounter multiple people in an image frame. The DSS framework applied to images with the different number of people engaged in violent activities is shown in Fig. 7.\n\nThe classification performance is also compared with the state-of-the-art technique which was developed to recognize the person of interest from aerial images [19] as shown in Table. 4. The proposed Drone Surveillance System (DSS) was able to outperform the method by more than 10% on the AVI dataset.\n\n\nRuntime Performance\n\nThe runtime performance of the DSS framework is computed on the cloud and consists of three parts: (i) detect-\n\n\nDataset\n\nComparison DSS state-of-the-art [19] AVI 88.8 77.8 Table 4. The table shows the comparison of the violent individual identification performance of the proposed system against the state-of-the-art technique [19] ing humans using the FPN network, (ii) human pose estimation using the SHDL network, and (iii) classification of the estimated pose. The deep learning framework was accelerated using the cuDNN framework and NVIDIA Tesla GPUs. The system detected the violent individuals at 5 fps per second to 16 fps for a maximum of ten and a minimum of two people, respectively, in the aerial image frame. The processing varies depending on the number of individuals within the image frame.\n\n\nConclusions\n\nThe paper proposed the real-time Drone Surveillance System (DSS) framework that can detect one or more individuals engaged in violent activities from aerial images. The framework first uses the FPN network to detect humans after which the proposed SHDL network is used to estimate the pose of the humans. The estimated poses are used by the SVM to identify violent individuals.\n\nThe proposed SHDL network uses ScatterNet features with Structural priors to achieve accelerated training for relatively fewer labeled examples. The utilization of fewer labeled examples for pose estimation is beneficial for this application as it is expensive to collect annotated examples. The paper also introduced the Aerial Violent Individual (AVI) Dataset which can benefit other researcher aiming to use deep learning for aerial surveillance applications. The proposed DSS framework outperforms the state-of-theart technique on the AVI dataset. This framework will be instrumental in detecting individuals engaged in violent activities in public areas or large gatherings.\n\nFigure 1 .\n1Illustration presents the violent activities from the introduced AVI dataset namely (clockwise from top) (i) Strangling, (ii) Punching, (iii) Kicking, (iv) Shooting and (v) Stabbing. The image of shooting activity involves multiple people in the same frame.\n\nFigure 2 .\n2The figure (left) illustrates the 14 body key-points annotated on the human body. The description of the human body points is as Facial Region (Purple): P1-Head, P2-Neck; Arms Region (Red): P3-Right shoulder, P4-Right Elbow, P5-Right Wrist, P6-Left Shoulder, P7-Left Elbow, P8-Left Wrist; Legs Region (Green): P9-Right Hip, P10-Right Knee, P11-Right Ankle, P12-Left Hip, P13-Left Knee, P14-Left Ankle. The figure (right) shows the Parrot AR Drone used to capture the images in the dataset and close-ups of few annotated keypoints.\n\n\u2022\nAerial Violent Individual (AVI) Dataset: The paper presents the Aerial Violent Individual (AVI) dataset of 2000 annotated images (10863 total individuals) of 5124 individuals engaged in violent activities. The AVI dataset contains images with humans recorded at different variations of scale, position, illumination, blurriness, etc. This dataset may encourage researchers interested in using deep learning for aerial surveillance applications.\n\nFigure 4 .\n4The Illustration shows the skeleton corresponding to the humans in an image. The angles (shown in green for few limbs) between the various limbs in this structure are used by the SVM to recognize the humans engaged in violent activities.\n\nFigure 6 .\n6The figure shows the performance of the Drone Surveillance System (DSS) on aerial images with only one violent individual, recorded using the AR parrot drone at four different heights of 2m (Row 1), 4m (Row 2), 6m (Row 3), and 8m (Row 4) (m: meters). The illustration also shows the individual engaged in different violent activities namely: Shooting (Column 1), Stabbing (Column 2), Kicking (Column 3), Strangling (Column 4) and Punching (Column 5). The violent individual detected by the DSS framework is shown in red while the neutral human is shown in cyan color. The estimated pose is also shown on top each detected human.\n\nFigure 7 .\n7The figure shows the performance of the Drone Surveillance System (DSS) on aerial images with multiple humans engaging together in different violent activities. The violent individuals are highlighted in red color and neutral human in cyan color.\n\n\nthe SHDL network estimates the human pose based on the 14 key-points at d = 5 pixel distance from the ground-truth, with 87.6% accuracy.Dataset \nDeep Learning Networks \nSHDL CN CNE SpatialNet \n\nAVI \n\n87.6 \n79.6 80.1 \n83.4 \n\nTable 1. Comparison of the human pose estimation performance of \nSHDL network with Coordinate network (CN) [20], Coordinate \nextended network (CNE) [20] \n\n\nTable 2. Table presents the classification accuracies(%) for the violent activities on Aerial Violent Individual (AVI) dataset.Dataset \n\nViolent Activities \n\nPunching Kicking Strangling Shooting Stabbing \nDSS \n\n89 \n94 \n85 \n82 \n92 \n\nSurya [19] \n\n80 \n84 \n73 \n73 \n79 \n\n\n\n.\nDataset No. of Violent Individuals (Per Image)Table 3. The table presents the classification accuracies(%) with the increase in individuals engaged in the violent activities in the aerial images taken the Aerial Violent Individual (AVI) dataset.1 \n2 \n3 \n4 \n5 \nDSS \n\n94.1 90.6 88.3 87.8 \n84.0 \n\n\n\nRobust optimization for deep regression. V Belagiannis, C Rupprecht, G Carneiro, N Navab, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionV. Belagiannis, C. Rupprecht, G. Carneiro, and N. Navab. Robust optimization for deep regression. In Proceedings of the IEEE International Conference on Computer Vision, pages 2830-2838, 2015.\n\nThe navigation and control technology inside the ar. drone micro uav. P.-J Bristeau, F Callou, D Vissi\u00e8re, N Petit, 18th IFAC World Congress. P.-J. Bristeau, F. Callou, D. Vissi\u00e8re, N. Petit, et al. The navigation and control technology inside the ar. drone micro uav. In 18th IFAC World Congress, 2011.\n\nInvariant scattering convolution networks. J Bruna, S Mallat, IEEE transactions on pattern analysis and machine intelligence. 35J. Bruna and S. Mallat. Invariant scattering convolution net- works. IEEE transactions on pattern analysis and machine intelligence, 35(8):1872-1886, 2013.\n\nPcanet: A simple deep learning baseline for image classification?. T.-H Chan, K Jia, S Gao, J Lu, Z Zeng, Y Ma, IEEE Transactions on Image Processing. T.-H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng, and Y. Ma. Pcanet: A simple deep learning baseline for image classification? IEEE Transactions on Image Processing, 2015.\n\nCarried object detection using ratio histogram and its application to suspicious event analysis. C.-H Chuang, J.-W Hsieh, L.-W Tsai, S.-Y. Chen, K.-C Fan, C.-H. Chuang, J.-W. Hsieh, L.-W. Tsai, S.-Y. Chen, and K.- C. Fan. Carried object detection using ratio histogram and its application to suspicious event analysis. IEEE transactions on circuits and systems for video technology, 2009.\n\nPictorial structures for object recognition. P F Felzenszwalb, D P Huttenlocher, International journal of computer vision. 611P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial struc- tures for object recognition. International journal of com- puter vision, 61(1):55-79, 2005.\n\nAutomatic camera and range sensor calibration using a single shot. A Geiger, F Moosmann, \u00d6 Car, B Schuster, Robotics and Automation (ICRA), 2012 IEEE International Conference on. A. Geiger, F. Moosmann,\u00d6. Car, and B. Schuster. Automatic camera and range sensor calibration using a single shot. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 3936-3943, 2012.\n\nCloud robotics and automation: A survey of related work. K Goldberg, B Kehoe, UCB/EECS-2013-5University of California, BerkeleyTech. Rep.K. Goldberg and B. Kehoe. Cloud robotics and automation: A survey of related work. University of California, Berkeley, Tech. Rep. UCB/EECS-2013-5, 2013.\n\nA method for automatic detection of crimes for public security by using motion analysis. K Goya, X Zhang, K Kitayama, I Nagayama, International Conference on Intelligent Information Hiding and Multimedia Signal Processing. K. Goya, X. Zhang, K. Kitayama, and I. Nagayama. A method for automatic detection of crimes for public security by using motion analysis. In International Conference on Intelligent Information Hiding and Multimedia Signal Pro- cessing, 2009.\n\nA novel method to improve model fitting for stock market prediction. S Jain, S Gupta, A Singh, International Journal of Research in Business and Technology. 31S. Jain, S. Gupta, and A. Singh. A novel method to improve model fitting for stock market prediction. International Jour- nal of Research in Business and Technology, 3(1):78-83.\n\nA review comparison of wavelet and cosine image transforms. V Jeengar, S Omkar, A Singh, M K Yadav, S Keshri, International Journal of Image, Graphics and Signal Processing. 41116V. Jeengar, S. Omkar, A. Singh, M. K. Yadav, and S. Keshri. A review comparison of wavelet and cosine image trans- forms. International Journal of Image, Graphics and Signal Processing, 4(11):16, 2012.\n\nThe dual-tree complex wavelet transform: a new technique for shift invariance and directional filters. N G Kingsbury, Proc. 8th IEEE DSP workshop. 8th IEEE DSP workshop8N. G. Kingsbury. The dual-tree complex wavelet transform: a new technique for shift invariance and directional filters. In Proc. 8th IEEE DSP workshop, volume 8, 1998.\n\nCctv in the sky: police plan to use military-style spy drones. The Guardian. P Lewis, 23P. Lewis. Cctv in the sky: police plan to use military-style spy drones. The Guardian, 23, 2010.\n\n3d human pose estimation from monocular images with deep convolutional neural network. S Li, A B Chan, Asian Conference on Computer Vision. S. Li and A. B. Chan. 3d human pose estimation from monocular images with deep convolutional neural network. In Asian Conference on Computer Vision, pages 332-347, 2014.\n\nAbandoned objects detection using double illumination invariant foreground masks. X Li, C Zhang, D Zhang, Pattern Recognition (ICPR), 2010 20th International Conference on. X. Li, C. Zhang, and D. Zhang. Abandoned objects detec- tion using double illumination invariant foreground masks. In Pattern Recognition (ICPR), 2010 20th International Con- ference on, pages 436-439, 2010.\n\nFeature pyramid networks for object detection. T.-Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, T.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. 2017.\n\nAerial scene understanding using deep wavelet scattering network and conditional random field. S Nadella, A Singh, S Omkar, European Conference on Computer Vision. S. Nadella, A. Singh, and S. Omkar. Aerial scene under- standing using deep wavelet scattering network and condi- tional random field. In European Conference on Computer Vision, pages 205-214, 2016.\n\nDrones join the war against drugs. T Padgett, Time Magazine. T. Padgett. Drones join the war against drugs. Time Maga- zine, June, 2009.\n\nAutonomous uav for suspicious action detection using pictorial human pose estimation and classification. ELCVIA: electronic letters on computer vision and image analysis. S Penmetsa, F Minhuj, A Singh, S Omkar, 13S. Penmetsa, F. Minhuj, A. Singh, and S. Omkar. Au- tonomous uav for suspicious action detection using picto- rial human pose estimation and classification. ELCVIA: electronic letters on computer vision and image analysis, 13(1):18-32, 2014.\n\nAdvancing human pose and gesture recognition. T Pfister, University of OxfordT. Pfister. Advancing human pose and gesture recognition. In University of Oxford, 2015.\n\nFlowing convnets for human pose estimation in videos. T Pfister, J Charles, A Zisserman, IEEE International Conference on Computer Vision. T. Pfister, J. Charles, and A. Zisserman. Flowing convnets for human pose estimation in videos. In IEEE International Conference on Computer Vision, 2015.\n\nDeep convolutional neural networks for efficient pose estimation in gesture videos. T Pfister, K Simonyan, J Charles, A Zisserman, Asian Conference on Computer Vision. T. Pfister, K. Simonyan, J. Charles, and A. Zisserman. Deep convolutional neural networks for efficient pose estimation in gesture videos. In Asian Conference on Computer Vision, pages 538-552, 2014.\n\nFire detection in the buildings using image processing. J Seebamrungsat, S Praising, P Riyamongkol, Student Project Conference (ICT-ISPC). J. Seebamrungsat, S. Praising, and P. Riyamongkol. Fire detection in the buildings using image processing. In Stu- dent Project Conference (ICT-ISPC), 2014 Third ICT Inter- national, pages 95-98, 2014.\n\nRotation, scaling and deformation invariant scattering for texture discrimination. L Sifre, S Mallat, Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. L. Sifre and S. Mallat. Rotation, scaling and deformation invariant scattering for texture discrimination. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Confer- ence on, pages 1233-1240, 2013.\n\nTexture and structure incorporated scatternet hybrid deep learning network (ts-shdl) for brain matter segmentation. A Singh, D Hazarika, A Bhattacharya, International Conference on Computer Vision Workshop. A. Singh, D. Hazarika, and A. Bhattacharya. Texture and structure incorporated scatternet hybrid deep learning net- work (ts-shdl) for brain matter segmentation. International Conference on Computer Vision Workshop, 2017.\n\nMulti-resolution dual-tree wavelet scattering network for signal classification. A Singh, N Kingsbury, International Conference on Mathematics in Signal Processing. A. Singh and N. Kingsbury. Multi-resolution dual-tree wavelet scattering network for signal classification. Inter- national Conference on Mathematics in Signal Processing, 2016.\n\nDual-tree wavelet scattering network with parametric log transformation for object classification. A Singh, N Kingsbury, International Conference on Acoustics, Speech and Signal Processing. A. Singh and N. Kingsbury. Dual-tree wavelet scattering net- work with parametric log transformation for object classifi- cation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017.\n\nEfficient convolutional network learning using parametric log based dual-tree wavelet scatternet. A Singh, N Kingsbury, IEEE International Conference on Computer Vision Workshop. A. Singh and N. Kingsbury. Efficient convolutional network learning using parametric log based dual-tree wavelet scat- ternet. IEEE International Conference on Computer Vision Workshop, 2017.\n\nScatternet hybrid deep learning (shdl) network for object classification. A Singh, N Kingsbury, International Workshop on Machine Learning for Signal Processing. A. Singh and N. Kingsbury. Scatternet hybrid deep learning (shdl) network for object classification. International Work- shop on Machine Learning for Signal Processing, 2017.\n\nGenerative scatternet hybrid deep learning (g-shdl) network with structural priors for semantic image segmentation. A Singh, N Kingsbury, IEEE International Conference on Acoustics, Speech and Signal Processing. A. Singh and N. Kingsbury. Generative scatternet hybrid deep learning (g-shdl) network with structural priors for se- mantic image segmentation. IEEE International Conference on Acoustics, Speech and Signal Processing, 2018.\n\nDeeppose: Human pose estimation via deep neural networks. A Toshev, C Szegedy, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Toshev and C. Szegedy. Deeppose: Human pose estima- tion via deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1653-1660, 2014.\n\nUcav surveillance, high-tech masculinities and oriental others. presentation to A Global Surveillance Society. W Walters, J Weber, W. Walters and J. Weber. Ucav surveillance, high-tech masculinities and oriental others. presentation to A Global Surveillance Society, 2010.\n", "annotations": {"author": "[{\"end\":336,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":163,\"start\":158}]", "author_first_name": "[{\"end\":157,\"start\":150}]", "author_affiliation": "[{\"end\":218,\"start\":165},{\"end\":335,\"start\":220}]", "title": "[{\"end\":143,\"start\":1},{\"end\":479,\"start\":337}]", "venue": "[{\"end\":536,\"start\":481}]", "abstract": "[{\"end\":1709,\"start\":542}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2074,\"start\":2070},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2085,\"start\":2082},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2105,\"start\":2101},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2129,\"start\":2126},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2151,\"start\":2147},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2498,\"start\":2495},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2735,\"start\":2731},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2985,\"start\":2982},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3746,\"start\":3742},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3789,\"start\":3785},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3856,\"start\":3852},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4013,\"start\":4009},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4208,\"start\":4205},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4949,\"start\":4945},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5370,\"start\":5366},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5603,\"start\":5599},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5606,\"start\":5603},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5609,\"start\":5606},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5899,\"start\":5896},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6093,\"start\":6090},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6146,\"start\":6142},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6296,\"start\":6292},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6841,\"start\":6838},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7050,\"start\":7046},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7053,\"start\":7050},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7489,\"start\":7485},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9652,\"start\":9648},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10196,\"start\":10192},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10726,\"start\":10722},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10729,\"start\":10726},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10732,\"start\":10729},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10735,\"start\":10732},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10829,\"start\":10825},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11085,\"start\":11081},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11416,\"start\":11412},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11504,\"start\":11501},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11507,\"start\":11504},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11510,\"start\":11507},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11693,\"start\":11689},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12353,\"start\":12349},{\"end\":12533,\"start\":12525},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12810,\"start\":12806},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13728,\"start\":13725},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13897,\"start\":13894},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14618,\"start\":14614},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15151,\"start\":15147},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15416,\"start\":15412},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16265,\"start\":16262},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16639,\"start\":16636},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17147,\"start\":17144},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17392,\"start\":17389},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18511,\"start\":18508},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20206,\"start\":20203},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20724,\"start\":20721},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21228,\"start\":21224},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21593,\"start\":21589},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21690,\"start\":21686},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23140,\"start\":23136},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25147,\"start\":25143},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25172,\"start\":25168},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25503,\"start\":25499},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25538,\"start\":25534},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25559,\"start\":25555},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26727,\"start\":26723},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27381,\"start\":27377},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27701,\"start\":27697},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27875,\"start\":27871}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29696,\"start\":29426},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30240,\"start\":29697},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30688,\"start\":30241},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30939,\"start\":30689},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31581,\"start\":30940},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31841,\"start\":31582},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32221,\"start\":31842},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32490,\"start\":32222},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32788,\"start\":32491}]", "paragraph": "[{\"end\":2135,\"start\":1725},{\"end\":3994,\"start\":2137},{\"end\":4748,\"start\":3996},{\"end\":5288,\"start\":4750},{\"end\":5400,\"start\":5290},{\"end\":7744,\"start\":5402},{\"end\":7975,\"start\":7746},{\"end\":9434,\"start\":8019},{\"end\":10109,\"start\":9464},{\"end\":10559,\"start\":10129},{\"end\":11344,\"start\":10603},{\"end\":12041,\"start\":11346},{\"end\":14329,\"start\":12043},{\"end\":14635,\"start\":14389},{\"end\":14840,\"start\":14685},{\"end\":15152,\"start\":14879},{\"end\":15204,\"start\":15154},{\"end\":15417,\"start\":15263},{\"end\":15906,\"start\":15419},{\"end\":16438,\"start\":15908},{\"end\":17148,\"start\":16440},{\"end\":17496,\"start\":17150},{\"end\":18569,\"start\":17554},{\"end\":19188,\"start\":18607},{\"end\":20266,\"start\":19237},{\"end\":20725,\"start\":20268},{\"end\":21190,\"start\":20750},{\"end\":21651,\"start\":21192},{\"end\":21915,\"start\":21670},{\"end\":22110,\"start\":21948},{\"end\":22326,\"start\":22112},{\"end\":23222,\"start\":22328},{\"end\":23660,\"start\":23258},{\"end\":23754,\"start\":23662},{\"end\":24216,\"start\":23756},{\"end\":24639,\"start\":24218},{\"end\":25341,\"start\":24641},{\"end\":25623,\"start\":25343},{\"end\":26313,\"start\":25662},{\"end\":26963,\"start\":26315},{\"end\":27216,\"start\":26965},{\"end\":27519,\"start\":27218},{\"end\":27653,\"start\":27543},{\"end\":28351,\"start\":27665},{\"end\":28744,\"start\":28367},{\"end\":29425,\"start\":28746}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14388,\"start\":14330},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14684,\"start\":14636},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14878,\"start\":14841},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15262,\"start\":15205},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17553,\"start\":17497}]", "table_ref": "[{\"end\":25113,\"start\":25106},{\"end\":25142,\"start\":25136},{\"end\":26312,\"start\":26305},{\"end\":26566,\"start\":26559},{\"end\":27400,\"start\":27394},{\"end\":27723,\"start\":27716}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1723,\"start\":1711},{\"attributes\":{\"n\":\"2.\"},\"end\":8017,\"start\":7978},{\"attributes\":{\"n\":\"3.\"},\"end\":9462,\"start\":9437},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10127,\"start\":10112},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10601,\"start\":10562},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18605,\"start\":18572},{\"attributes\":{\"n\":\"3.4.\"},\"end\":19235,\"start\":19191},{\"attributes\":{\"n\":\"4.\"},\"end\":20748,\"start\":20728},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21668,\"start\":21654},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21946,\"start\":21918},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23256,\"start\":23225},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25660,\"start\":25626},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27541,\"start\":27522},{\"end\":27663,\"start\":27656},{\"attributes\":{\"n\":\"5.\"},\"end\":28365,\"start\":28354},{\"end\":29437,\"start\":29427},{\"end\":29708,\"start\":29698},{\"end\":30243,\"start\":30242},{\"end\":30700,\"start\":30690},{\"end\":30951,\"start\":30941},{\"end\":31593,\"start\":31583},{\"end\":32493,\"start\":32492}]", "table": "[{\"end\":32221,\"start\":31980},{\"end\":32490,\"start\":32351},{\"end\":32788,\"start\":32739}]", "figure_caption": "[{\"end\":29696,\"start\":29439},{\"end\":30240,\"start\":29710},{\"end\":30688,\"start\":30244},{\"end\":30939,\"start\":30702},{\"end\":31581,\"start\":30953},{\"end\":31841,\"start\":31595},{\"end\":31980,\"start\":31844},{\"end\":32351,\"start\":32224},{\"end\":32739,\"start\":32494}]", "figure_ref": "[{\"end\":6859,\"start\":6852},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8460,\"start\":8454},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8631,\"start\":8625},{\"end\":10889,\"start\":10883},{\"end\":16437,\"start\":16431},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16848,\"start\":16842},{\"end\":18209,\"start\":18195},{\"end\":18717,\"start\":18711},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18810,\"start\":18804},{\"end\":20930,\"start\":20922},{\"end\":23221,\"start\":23214},{\"end\":23587,\"start\":23581},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23940,\"start\":23923},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24387,\"start\":24381},{\"end\":24398,\"start\":24389},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24766,\"start\":24760},{\"end\":24980,\"start\":24974},{\"end\":25747,\"start\":25741},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26434,\"start\":26428},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27215,\"start\":27209}]", "bib_author_first_name": "[{\"end\":32832,\"start\":32831},{\"end\":32847,\"start\":32846},{\"end\":32860,\"start\":32859},{\"end\":32872,\"start\":32871},{\"end\":33269,\"start\":33265},{\"end\":33281,\"start\":33280},{\"end\":33291,\"start\":33290},{\"end\":33303,\"start\":33302},{\"end\":33544,\"start\":33543},{\"end\":33553,\"start\":33552},{\"end\":33856,\"start\":33852},{\"end\":33864,\"start\":33863},{\"end\":33871,\"start\":33870},{\"end\":33878,\"start\":33877},{\"end\":33884,\"start\":33883},{\"end\":33892,\"start\":33891},{\"end\":34204,\"start\":34200},{\"end\":34217,\"start\":34213},{\"end\":34229,\"start\":34225},{\"end\":34241,\"start\":34236},{\"end\":34252,\"start\":34248},{\"end\":34539,\"start\":34538},{\"end\":34541,\"start\":34540},{\"end\":34557,\"start\":34556},{\"end\":34559,\"start\":34558},{\"end\":34841,\"start\":34840},{\"end\":34851,\"start\":34850},{\"end\":34863,\"start\":34862},{\"end\":34870,\"start\":34869},{\"end\":35223,\"start\":35222},{\"end\":35235,\"start\":35234},{\"end\":35546,\"start\":35545},{\"end\":35554,\"start\":35553},{\"end\":35563,\"start\":35562},{\"end\":35575,\"start\":35574},{\"end\":35992,\"start\":35991},{\"end\":36000,\"start\":35999},{\"end\":36009,\"start\":36008},{\"end\":36321,\"start\":36320},{\"end\":36332,\"start\":36331},{\"end\":36341,\"start\":36340},{\"end\":36350,\"start\":36349},{\"end\":36352,\"start\":36351},{\"end\":36361,\"start\":36360},{\"end\":36746,\"start\":36745},{\"end\":36748,\"start\":36747},{\"end\":37058,\"start\":37057},{\"end\":37254,\"start\":37253},{\"end\":37260,\"start\":37259},{\"end\":37262,\"start\":37261},{\"end\":37560,\"start\":37559},{\"end\":37566,\"start\":37565},{\"end\":37575,\"start\":37574},{\"end\":37910,\"start\":37906},{\"end\":37917,\"start\":37916},{\"end\":37927,\"start\":37926},{\"end\":37939,\"start\":37938},{\"end\":37945,\"start\":37944},{\"end\":37958,\"start\":37957},{\"end\":38192,\"start\":38191},{\"end\":38203,\"start\":38202},{\"end\":38212,\"start\":38211},{\"end\":38496,\"start\":38495},{\"end\":38770,\"start\":38769},{\"end\":38782,\"start\":38781},{\"end\":38792,\"start\":38791},{\"end\":38801,\"start\":38800},{\"end\":39101,\"start\":39100},{\"end\":39276,\"start\":39275},{\"end\":39287,\"start\":39286},{\"end\":39298,\"start\":39297},{\"end\":39601,\"start\":39600},{\"end\":39612,\"start\":39611},{\"end\":39624,\"start\":39623},{\"end\":39635,\"start\":39634},{\"end\":39942,\"start\":39941},{\"end\":39959,\"start\":39958},{\"end\":39971,\"start\":39970},{\"end\":40311,\"start\":40310},{\"end\":40320,\"start\":40319},{\"end\":40728,\"start\":40727},{\"end\":40737,\"start\":40736},{\"end\":40749,\"start\":40748},{\"end\":41123,\"start\":41122},{\"end\":41132,\"start\":41131},{\"end\":41485,\"start\":41484},{\"end\":41494,\"start\":41493},{\"end\":41892,\"start\":41891},{\"end\":41901,\"start\":41900},{\"end\":42240,\"start\":42239},{\"end\":42249,\"start\":42248},{\"end\":42620,\"start\":42619},{\"end\":42629,\"start\":42628},{\"end\":43000,\"start\":42999},{\"end\":43010,\"start\":43009},{\"end\":43465,\"start\":43464},{\"end\":43476,\"start\":43475}]", "bib_author_last_name": "[{\"end\":32844,\"start\":32833},{\"end\":32857,\"start\":32848},{\"end\":32869,\"start\":32861},{\"end\":32878,\"start\":32873},{\"end\":33278,\"start\":33270},{\"end\":33288,\"start\":33282},{\"end\":33300,\"start\":33292},{\"end\":33309,\"start\":33304},{\"end\":33550,\"start\":33545},{\"end\":33560,\"start\":33554},{\"end\":33861,\"start\":33857},{\"end\":33868,\"start\":33865},{\"end\":33875,\"start\":33872},{\"end\":33881,\"start\":33879},{\"end\":33889,\"start\":33885},{\"end\":33895,\"start\":33893},{\"end\":34211,\"start\":34205},{\"end\":34223,\"start\":34218},{\"end\":34234,\"start\":34230},{\"end\":34246,\"start\":34242},{\"end\":34256,\"start\":34253},{\"end\":34554,\"start\":34542},{\"end\":34572,\"start\":34560},{\"end\":34848,\"start\":34842},{\"end\":34860,\"start\":34852},{\"end\":34867,\"start\":34864},{\"end\":34879,\"start\":34871},{\"end\":35232,\"start\":35224},{\"end\":35241,\"start\":35236},{\"end\":35551,\"start\":35547},{\"end\":35560,\"start\":35555},{\"end\":35572,\"start\":35564},{\"end\":35584,\"start\":35576},{\"end\":35997,\"start\":35993},{\"end\":36006,\"start\":36001},{\"end\":36015,\"start\":36010},{\"end\":36329,\"start\":36322},{\"end\":36338,\"start\":36333},{\"end\":36347,\"start\":36342},{\"end\":36358,\"start\":36353},{\"end\":36368,\"start\":36362},{\"end\":36758,\"start\":36749},{\"end\":37064,\"start\":37059},{\"end\":37257,\"start\":37255},{\"end\":37267,\"start\":37263},{\"end\":37563,\"start\":37561},{\"end\":37572,\"start\":37567},{\"end\":37581,\"start\":37576},{\"end\":37914,\"start\":37911},{\"end\":37924,\"start\":37918},{\"end\":37936,\"start\":37928},{\"end\":37942,\"start\":37940},{\"end\":37955,\"start\":37946},{\"end\":37967,\"start\":37959},{\"end\":38200,\"start\":38193},{\"end\":38209,\"start\":38204},{\"end\":38218,\"start\":38213},{\"end\":38504,\"start\":38497},{\"end\":38779,\"start\":38771},{\"end\":38789,\"start\":38783},{\"end\":38798,\"start\":38793},{\"end\":38807,\"start\":38802},{\"end\":39109,\"start\":39102},{\"end\":39284,\"start\":39277},{\"end\":39295,\"start\":39288},{\"end\":39308,\"start\":39299},{\"end\":39609,\"start\":39602},{\"end\":39621,\"start\":39613},{\"end\":39632,\"start\":39625},{\"end\":39645,\"start\":39636},{\"end\":39956,\"start\":39943},{\"end\":39968,\"start\":39960},{\"end\":39983,\"start\":39972},{\"end\":40317,\"start\":40312},{\"end\":40327,\"start\":40321},{\"end\":40734,\"start\":40729},{\"end\":40746,\"start\":40738},{\"end\":40762,\"start\":40750},{\"end\":41129,\"start\":41124},{\"end\":41142,\"start\":41133},{\"end\":41491,\"start\":41486},{\"end\":41504,\"start\":41495},{\"end\":41898,\"start\":41893},{\"end\":41911,\"start\":41902},{\"end\":42246,\"start\":42241},{\"end\":42259,\"start\":42250},{\"end\":42626,\"start\":42621},{\"end\":42639,\"start\":42630},{\"end\":43007,\"start\":43001},{\"end\":43018,\"start\":43011},{\"end\":43473,\"start\":43466},{\"end\":43482,\"start\":43477}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6621028},\"end\":33193,\"start\":32790},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18269050},\"end\":33498,\"start\":33195},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1996316},\"end\":33783,\"start\":33500},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206725404},\"end\":34101,\"start\":33785},{\"attributes\":{\"id\":\"b4\"},\"end\":34491,\"start\":34103},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2277383},\"end\":34771,\"start\":34493},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12339854},\"end\":35163,\"start\":34773},{\"attributes\":{\"doi\":\"UCB/EECS-2013-5\",\"id\":\"b7\"},\"end\":35454,\"start\":35165},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15321096},\"end\":35920,\"start\":35456},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":154598914},\"end\":36258,\"start\":35922},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":62551788},\"end\":36640,\"start\":36260},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":123806696},\"end\":36978,\"start\":36642},{\"attributes\":{\"id\":\"b12\"},\"end\":37164,\"start\":36980},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12870756},\"end\":37475,\"start\":37166},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14501230},\"end\":37857,\"start\":37477},{\"attributes\":{\"id\":\"b15\"},\"end\":38094,\"start\":37859},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":27508613},\"end\":38458,\"start\":38096},{\"attributes\":{\"id\":\"b17\"},\"end\":38596,\"start\":38460},{\"attributes\":{\"id\":\"b18\"},\"end\":39052,\"start\":38598},{\"attributes\":{\"id\":\"b19\"},\"end\":39219,\"start\":39054},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2777703},\"end\":39514,\"start\":39221},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11263623},\"end\":39883,\"start\":39516},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6457255},\"end\":40225,\"start\":39885},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2942296},\"end\":40609,\"start\":40227},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4724870},\"end\":41039,\"start\":40611},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10475239},\"end\":41383,\"start\":41041},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6111205},\"end\":41791,\"start\":41385},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4729258},\"end\":42163,\"start\":41793},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7384425},\"end\":42501,\"start\":42165},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3646461},\"end\":42939,\"start\":42503},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206592152},\"end\":43351,\"start\":42941},{\"attributes\":{\"id\":\"b31\"},\"end\":43625,\"start\":43353}]", "bib_title": "[{\"end\":32829,\"start\":32790},{\"end\":33263,\"start\":33195},{\"end\":33541,\"start\":33500},{\"end\":33850,\"start\":33785},{\"end\":34536,\"start\":34493},{\"end\":34838,\"start\":34773},{\"end\":35543,\"start\":35456},{\"end\":35989,\"start\":35922},{\"end\":36318,\"start\":36260},{\"end\":36743,\"start\":36642},{\"end\":37251,\"start\":37166},{\"end\":37557,\"start\":37477},{\"end\":38189,\"start\":38096},{\"end\":38493,\"start\":38460},{\"end\":39273,\"start\":39221},{\"end\":39598,\"start\":39516},{\"end\":39939,\"start\":39885},{\"end\":40308,\"start\":40227},{\"end\":40725,\"start\":40611},{\"end\":41120,\"start\":41041},{\"end\":41482,\"start\":41385},{\"end\":41889,\"start\":41793},{\"end\":42237,\"start\":42165},{\"end\":42617,\"start\":42503},{\"end\":42997,\"start\":42941}]", "bib_author": "[{\"end\":32846,\"start\":32831},{\"end\":32859,\"start\":32846},{\"end\":32871,\"start\":32859},{\"end\":32880,\"start\":32871},{\"end\":33280,\"start\":33265},{\"end\":33290,\"start\":33280},{\"end\":33302,\"start\":33290},{\"end\":33311,\"start\":33302},{\"end\":33552,\"start\":33543},{\"end\":33562,\"start\":33552},{\"end\":33863,\"start\":33852},{\"end\":33870,\"start\":33863},{\"end\":33877,\"start\":33870},{\"end\":33883,\"start\":33877},{\"end\":33891,\"start\":33883},{\"end\":33897,\"start\":33891},{\"end\":34213,\"start\":34200},{\"end\":34225,\"start\":34213},{\"end\":34236,\"start\":34225},{\"end\":34248,\"start\":34236},{\"end\":34258,\"start\":34248},{\"end\":34556,\"start\":34538},{\"end\":34574,\"start\":34556},{\"end\":34850,\"start\":34840},{\"end\":34862,\"start\":34850},{\"end\":34869,\"start\":34862},{\"end\":34881,\"start\":34869},{\"end\":35234,\"start\":35222},{\"end\":35243,\"start\":35234},{\"end\":35553,\"start\":35545},{\"end\":35562,\"start\":35553},{\"end\":35574,\"start\":35562},{\"end\":35586,\"start\":35574},{\"end\":35999,\"start\":35991},{\"end\":36008,\"start\":35999},{\"end\":36017,\"start\":36008},{\"end\":36331,\"start\":36320},{\"end\":36340,\"start\":36331},{\"end\":36349,\"start\":36340},{\"end\":36360,\"start\":36349},{\"end\":36370,\"start\":36360},{\"end\":36760,\"start\":36745},{\"end\":37066,\"start\":37057},{\"end\":37259,\"start\":37253},{\"end\":37269,\"start\":37259},{\"end\":37565,\"start\":37559},{\"end\":37574,\"start\":37565},{\"end\":37583,\"start\":37574},{\"end\":37916,\"start\":37906},{\"end\":37926,\"start\":37916},{\"end\":37938,\"start\":37926},{\"end\":37944,\"start\":37938},{\"end\":37957,\"start\":37944},{\"end\":37969,\"start\":37957},{\"end\":38202,\"start\":38191},{\"end\":38211,\"start\":38202},{\"end\":38220,\"start\":38211},{\"end\":38506,\"start\":38495},{\"end\":38781,\"start\":38769},{\"end\":38791,\"start\":38781},{\"end\":38800,\"start\":38791},{\"end\":38809,\"start\":38800},{\"end\":39111,\"start\":39100},{\"end\":39286,\"start\":39275},{\"end\":39297,\"start\":39286},{\"end\":39310,\"start\":39297},{\"end\":39611,\"start\":39600},{\"end\":39623,\"start\":39611},{\"end\":39634,\"start\":39623},{\"end\":39647,\"start\":39634},{\"end\":39958,\"start\":39941},{\"end\":39970,\"start\":39958},{\"end\":39985,\"start\":39970},{\"end\":40319,\"start\":40310},{\"end\":40329,\"start\":40319},{\"end\":40736,\"start\":40727},{\"end\":40748,\"start\":40736},{\"end\":40764,\"start\":40748},{\"end\":41131,\"start\":41122},{\"end\":41144,\"start\":41131},{\"end\":41493,\"start\":41484},{\"end\":41506,\"start\":41493},{\"end\":41900,\"start\":41891},{\"end\":41913,\"start\":41900},{\"end\":42248,\"start\":42239},{\"end\":42261,\"start\":42248},{\"end\":42628,\"start\":42619},{\"end\":42641,\"start\":42628},{\"end\":43009,\"start\":42999},{\"end\":43020,\"start\":43009},{\"end\":43475,\"start\":43464},{\"end\":43484,\"start\":43475}]", "bib_venue": "[{\"end\":33001,\"start\":32949},{\"end\":36810,\"start\":36789},{\"end\":43161,\"start\":43099},{\"end\":32947,\"start\":32880},{\"end\":33335,\"start\":33311},{\"end\":33624,\"start\":33562},{\"end\":33934,\"start\":33897},{\"end\":34198,\"start\":34103},{\"end\":34614,\"start\":34574},{\"end\":34950,\"start\":34881},{\"end\":35220,\"start\":35165},{\"end\":35677,\"start\":35586},{\"end\":36077,\"start\":36017},{\"end\":36432,\"start\":36370},{\"end\":36787,\"start\":36760},{\"end\":37055,\"start\":36980},{\"end\":37304,\"start\":37269},{\"end\":37648,\"start\":37583},{\"end\":37904,\"start\":37859},{\"end\":38258,\"start\":38220},{\"end\":38519,\"start\":38506},{\"end\":38767,\"start\":38598},{\"end\":39098,\"start\":39054},{\"end\":39358,\"start\":39310},{\"end\":39682,\"start\":39647},{\"end\":40022,\"start\":39985},{\"end\":40400,\"start\":40329},{\"end\":40816,\"start\":40764},{\"end\":41204,\"start\":41144},{\"end\":41573,\"start\":41506},{\"end\":41970,\"start\":41913},{\"end\":42325,\"start\":42261},{\"end\":42713,\"start\":42641},{\"end\":43097,\"start\":43020},{\"end\":43462,\"start\":43353}]"}}}, "year": 2023, "month": 12, "day": 17}