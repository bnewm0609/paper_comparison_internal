{"id": 235457994, "updated": "2023-10-06 02:23:15.361", "metadata": {"title": "BABEL: Bodies, Action and Behavior with English Labels", "authors": "[{\"first\":\"Abhinanda\",\"last\":\"Punnakkal\",\"middle\":[\"R.\"]},{\"first\":\"Arjun\",\"last\":\"Chandrasekaran\",\"middle\":[]},{\"first\":\"Nikos\",\"last\":\"Athanasiou\",\"middle\":[]},{\"first\":\"Alejandra\",\"last\":\"Quiros-Ramirez\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Systems\",\"middle\":[\"J.\",\"Black\",\"Max\",\"Planck\",\"Institute\",\"for\",\"Intelligent\"]},{\"first\":\"Universitat\",\"last\":\"Konstanz\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 6, "day": 17}, "abstract": "Understanding the semantics of human movement -- the what, how and why of the movement -- is an important problem that requires datasets of human actions with semantic labels. Existing datasets take one of two approaches. Large-scale video datasets contain many action labels but do not contain ground-truth 3D human motion. Alternatively, motion-capture (mocap) datasets have precise body motions but are limited to a small number of actions. To address this, we present BABEL, a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of action labels for about 43 hours of mocap sequences from AMASS. Action labels are at two levels of abstraction -- sequence labels describe the overall action in the sequence, and frame labels describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc. To demonstrate the value of BABEL as a benchmark, we evaluate the performance of models on 3D action recognition. We demonstrate that BABEL poses interesting learning challenges that are applicable to real-world scenarios, and can serve as a useful benchmark of progress in 3D action recognition. The dataset, baseline method, and evaluation code is made available, and supported for academic research purposes at https://babel.is.tue.mpg.de/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.09696", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/PunnakkalCAQB21", "doi": "10.1109/cvpr46437.2021.00078"}}, "content": {"source": {"pdf_hash": "fe9bc34b3e3b181de6caee3ff79e9c5bf1bbcf98", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.09696v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2106.09696", "status": "GREEN"}}, "grobid": {"id": "e6f2e967243896f30103770df6e5a572876b1c7a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fe9bc34b3e3b181de6caee3ff79e9c5bf1bbcf98.txt", "contents": "\nBABEL: Bodies, Action and Behavior with English Labels\n\n\nAbhinanda R Punnakkal apunnakkal@tue.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nArjun Chandrasekaran achandrasekaran@tue.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nNikos Athanasiou nathanasiou@tue.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nAlejandra Quir\u00f3s-Ram\u00edrez alejandra.quiros@tue.mpg.de \nUniversit\u00e4t Konstanz\nKonstanzGermany\n\nMichael J Black black@tue.mpg.de \nMax Planck Institute for Intelligent Systems\nT\u00fcbingenGermany\n\nBABEL: Bodies, Action and Behavior with English Labels\n\nUnderstanding the semantics of human movement -the what, how and why of the movement -is an important problem that requires datasets of human actions with semantic labels. Existing datasets take one of two approaches. Large-scale video datasets contain many action labels but do not contain ground-truth 3D human motion. Alternatively, motion-capture (mocap) datasets have precise body motions but are limited to a small number of actions. To address this, we present BABEL, a large dataset with language labels describing the actions being performed in mocap sequences. BABEL labels about 43 hours of mocap sequences from AMASS. Action labels are at two levels of abstractionsequence labels which describe the overall action in the sequence, and frame labels which describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc. To demonstrate the value of BABEL as a benchmark, we evaluate the performance of models on 3D action recognition. We demonstrate that BABEL poses interesting learning challenges that are applicable to real-world scenarios, and can serve as a useful benchmark of progress in 3D action recognition. The dataset, baseline method, and evaluation code is made available, and supported for academic research purposes at https://babel.is.tue.mpg.de/. arXiv:2106.09696v2 [cs.CV] 23 Jun 2021 ways, tilt head back, move head around, twist head, tilt head to the right, move head in a circle pace quickly, walk away, runway walk, power walk, wobble walk, slow walk back and forth, speed walk, sidle, strolls, catwalk, walking backwards, walking in circles, zigzag, stride, saunter, plod, shamble, strut, wander, hobble, tread, swagger dance with partner, ballet dancing, interpretive dancing, ginga dance, doing a nutty dance, expression dance, slow dancing with imaginary partner, teacup dance, waltz, fish flop, plie, shimmy, sway scratch hands, scratch chin, scratch arm, scratch face, scratch head, scratch nose, scratching side, scratch waist with right hand, itch butt, itch leg lean backwards to the left, lean to left, lean right, lean left knee forward, lean against object, tilt right place object on upper shelf, put object on table, place ice in glass, put book on shelf, place drink, put box on shelf, place object on ground, place item in waist band, loading stuff into a truck twist wrists in a circle, twist body to the left, twist body to the right, jump twist, whirl body, twirl leg, twist left leg, twist head, twist cork, twirl, barrel roll\n\nIntroduction\n\nA key goal in computer vision is to understand human movement in semantic terms. Relevant tasks include predicting semantic labels for a human movement, e.g., action recognition [14], video description [39], temporal localiza-* Denotes equal contribution. Figure 1. People moving naturally often perform multiple actions simultaneously, and sequentially, with transitions between them. BABEL contains sequence labels describing the overall action in the sequence, and frame labels where all frames and all actions are labeled. Each frame label is precisely aligned with the frames representing the action (colored brackets). This includes simultaneous actions (nested brackets) and transitions between actions (shaded gray box). tion [28,41], and generating human movement that is conditioned on semantics, e.g., motion synthesis conditioned on actions [12], or sentences [3,19].\n\nLarge-scale datasets that capture variations in human movement and language descriptions that express the semantics of these movements, are critical to making progress on these challenging problems. Existing datasets contain detailed action descriptions for only 2D videos, e.g., Activ-ityNet [28], AVA [11] and HACS [41]. The large scale 3D datasets that contain action labels, e.g., NTU RGB+D 60 [29] and NTU RGB+D 120 [42] do not contain ground truth 3D human motion but only noisy estimates. On the other hand, motion-capture (mocap) datasets [2,10,13,16] are small in scale and are only sparsely labeled with very few actions. We address this shortcoming with BABEL, a large dataset of diverse, densely annotated, actions with labels for all the actions in a motion capture (mocap) sequence.\n\nWe acquire action labels for sequences in BABEL, at two different levels of resolution. Similar to existing mocap datasets, we collect a sequence label that describes the action being performed in the entire sequence, e.g., Play basketball in Fig. 1. At a finer-grained resolution, the frame labels describe the action being performed at each frame of the sequence, e.g., transfer ball to the left hand, sprint, etc. The frame labels are precisely aligned with the corresponding frames in the sequence that represent the action. BABEL also captures simultaneous actions, e.g., sprint and dribble ball with left hand. When collecting frame labels, we ensure that all frames in a sequence are labeled with at least one action, and all the actions in a frame are labeled. This results in dense action annotations for high-quality mocap data.\n\nBABEL leverages the recently introduced AMASS dataset [22] for mocap sequences. AMASS is a large corpus of mocap datasets that are unified with a common representation. It has > 43 hours of mocap data performed by over 346 subjects. The scale and diversity of AMASS presents an opportunity for data-driven learning of semantic representations for 3D human movement.\n\nMost existing large-scale datasets with action labels [2,13,21,23,28,29,41,42] first determine a fixed set of actions that are of interest. Following this, actors performing these actions are captured (3D datasets), or videos containing the actions of interest are mined from the web (2D datasets). While this ensures the presence of the action of interest in the sequence, all other actions remain unlabeled. The sparse action label for a sequence, while useful, serves only as weak supervision for data-driven models that aim to correlate movements with semantic labels. This is suboptimal. 3D datasets such as NTU RGB+D [21,29] and HumanAct12 [42] handle this shortcoming by cropping out segments that do not correspond to the action of interest from natural human movement sequences. While the action labels for the short segments are accurate, the cropped segments are unlike the natural, continuous human movements in the real-world. Thus, the pre-segmented movements are less suitable as training data for real-world applications.\n\nOur key idea with BABEL is that natural human movement often involves multiple actions and transitions between them. Thus, understanding the semantics of natural human movement not only involves modeling the relationship between an isolated action and its corresponding movement but also the relationship between different actions that occur simultaneously and sequentially. With BABEL, our goal is to provide accurate data for statistical learning, which reflects the variety, concurrence and temporal compositions of actions in natural human movement.\n\nBABEL contains action annotations for about 43.5 hours of mocap from AMASS, with 15472 unique language labels. Via a semi-automatic process of semantic clustering followed by manual categorization, we organize these into 260 action categories such as greet, hop, scratch, dance, play instrument, etc. The action categories in BABEL belong to 8 broad semantic categories involving simple actions (throw, jump), complex activities (martial arts, dance), body part interactions (scratch, touch face), etc. (see Sec. 3.4).\n\nBABEL contains a total of 28055 sequence labels, and 63353 frame labels. This corresponds to dense per-frame action annotations for 10892 sequences (> 37 hours of mocap), and sequence-level annotations for all 13220 sequences (> 43 hours of mocap). On average, a single mocap sequence has 6.06 segments, with 4.02 unique action categories. We collect the sequence labels via a web interface of our design, and the frame labels and alignments by adapting an existing web annotation tool, VIA [8] (see Sec. 3.1). Labeling was done by using Amazon Mechanical Turk [1]. We benchmark the performance of models on BA-BEL for the 3D action recognition task [29]. The goal is to predict the action category, given a segment of mocap that corresponds to a single action span. Unlike existing datasets that are carefully constructed for the actions of interest, action recognition with BABEL more closely resembles real-world applications due to the long-tailed distribution of classes in BABEL. We demonstrate that BA-BEL presents interesting learning challenges for an existing action recognition model that performs well on NTU RGB+D 60. In addition to being a useful benchmark for action recognition, we believe that BABEL can be leveraged by the community for tasks like pose estimation, motion synthesis, temporal localization, few shot learning, etc.\n\nIn this work, we make the following contributions: (1) We provide the largest 3D dataset of dense action labels that are precisely aligned with their corresponding movement spans in the mocap sequence. (2) We categorize the raw language labels into over 250 action classes that can be leveraged for tasks requiring categorical label sets such as 3D action recognition. (3) We analyze the actions occurring in BABEL sequences in detail, furthering our semantic understanding of mocap data that is already widely used in vision tasks. (4) We benchmark the performance of baseline 3D action recognition models on BABEL, demonstrating that the distribution of actions that resembles realworld scenarios, poses interesting learning challenges. (5) The dataset, baseline models and evaluation code are publicly available for academic research purposes at https: //babel.is.tue.mpg.de/.\n\n\nRelated Work\n\nLanguage labels and 3D mocap data. We first briefly review the action categories in large-scale 3D datasets, followed by a more detailed comparison in Table 1. The CMU Graphics Lab Motion Capture Database (CMU) [2] is widely used, and has 2605 sequences. The dataset has 6 semantic categories (e.g., 'human interaction', 'interaction with environment') that, overall, contain 23 subcategories, e.g., 'two subjects', 'playground', 'pantomime'. Human3.6M [16]  dog', 'walking pair'), 'full body upright variations' ('greeting', 'posing'), etc. MoVi [10] consists of everyday actions and sports movements e.g., 'clapping hands', 'pretending to take picture', etc. KIT Whole-Body Human Motion Database (KIT) [23] focuses on both human movement and human-object interaction [34] containing grasping and manipulation actions in addition to activities such as climbing and playing sports. LaFan1 [13] is a recent dataset containing 15 different actions, including locomotion on uneven terrain, free dancing, fight movements, etc. These characterize the movement in the entire mocap sequence via simple tags or keywords. In contrast, the KIT Motion-Language Dataset [23] describes motion sequences with natural language sentences, e.g., 'A person walks backward at a slow speed'. While our motivation to learn semantic representations of movement is similar, action labels in BABEL are precisely aligned with the sequence.\n\nFrame actions labels in 3D mocap. The CMU MMAC dataset [32] contains precise frame labels for a fixed set of 17 cooking actions (including 'none'). Arikan et al. [4] and Muller et al. [25] partially automate labeling temporal segments for mocap using action classifiers. While these works assume a known, fixed set of classes, in BABEL, we identify and precisely label all actions that occur in each frame.\n\nAction labels and tracked 3D data. NTU RGB+D 60 [29] and 120 [21] are large, widely used datasets for 3D action recognition. In NTU RGB+D, RGBD sequences are captured via 3 Kinect sensors which track joint positions of the human skeleton. NTU RGB+D has segmented sequences corresponding to specific actions. There are 3 semantic categories -'Daily actions' ('drink water', 'taking a selfie'), 'Medical conditions' ('sneeze', 'falling down') and 'Mutual actions' ('hugging', 'cheers and drink'). These datasets contain short cropped segments of actions, which differ from BABEL sequences, which are continuous, reflecting natural human movement data. The ability to model actions that can occur simultaneously, sequentially and the transitions between them is important for application to real world data [28]. See Table 1 for further comparison.\n\n2D temporal localization . Many works over the years have contributed to progress in the action localization task [15,31,40]. ActivityNet [28] contains 648 hours of videos and 200 human activities that are relevant to daily life, organized under a rich semantic taxonomy. It has 19,994 (untrimmed) videos, with an average of 1.54 activities per video. More recently, HACS [41] provides a larger temporal localization dataset with 140,000 segments of actions that are cropped from 50,000 videos that span over 200 actions. AVA [11] is another recent large-scale dataset that consists of dense annotations for long video sequences for 80 atomic classes. In [38], the authors introduce a test recorded by Kinect v2 in which they describe activities as compositions of action interactions with different objects. While BABEL also contains temporally annotated labels, it does not assume a fixed set of actions that are of interest. On the other hand, with BABEL, we elicit labels for all actions in the sequence including high-level ('eating'), and low-level actions ('raise right hand to mouth').\n\n\nDataset\n\nWe first provide details regarding the crowdsourced data collection process. We then describe the types of labels in BABEL, and the label processing procedure.\n\n\nData collection\n\nWe collect BABEL by showing rendered videos of mocap sequences from AMASS [22] to human annotators and eliciting action labels (Fig. 2). The mocap is processed to make sure the person in the video faces the annotator in the first frame. We observe that a sequence labeled as pick up object often also involves other actions such as walking to the object, bending down to pick up the object, grasping the object, straightening back up, turning around and walking away. We argue that labeling the entire sequence with the single label is imprecise, and problematic. First, many actions such as turn and grasp are ignored and remain unlabeled although they may be of interest to researchers [34]. Second, sequence labels provide weak supervision to statistical models, which are trained to map the concept of picking up object to the whole sequence when it, in fact, contains many different actions. To illustrate this point, we examine a typical sequence (see Qualitative Example 1 in the project website), and find that only 20% of the duration of the sequence labeled as pick up and place object corresponds to this action. Crucially, walking towards and away from the object -actions that remain unlabeled -account for 40% of the duration. While this makes semantic sense to a human -picking up and placing an object is the only action that changes the state of the world and hence worth mentioning, this might be suboptimal training data to a statistical model, especially when the dataset also contains the confusing classes walk, turn, etc. Finally, using noisy labels as ground truth during evaluation does not accurately reflect the capabilities of models.\n\nWe address this with action labels at two levels of resolution -a label describing the overall action in the entire sequence, and fine-grained labels that are aligned with their corresponding spans of movement in the mocap sequence.\n\n\nBABEL action labels\n\nWe collect BABEL labels in a two-stage process -first, we collect sequence labels, and determine whether the sequence contains multiple actions. We then collect frame labels for the sequences where 2 annotators agree that there are multiple actions.\n\nSequence labels. In this labeling task, annotators answer two questions regarding a sequence. We first ask annotators if the video contains more than one action (yes/no). 1 If the annotator chooses 'no', we ask them to name the action in the video. If they instead choose 'yes', we elicit a sequence label with the question, \"If you had to describe the whole sequence as one action, what would it be?\" We provide the web-based task interface in the project website.\n\nWe ask annotators to enter the sequence labels in a textbox, with the option of choosing from an auto-complete drop-down menu that is populated with a list of basic actions. We specifically elicit free-form labels (as opposed to a fixed list of categories) from annotators to discover the diversity in actions in the mocap sequences. We find that in most cases, annotators tend to enter their own action labels. This also presents a challenge, acting as a source of label variance. Apart from varying vocabulary, free-form descriptions are subject to ambiguity regarding the 'correct' level in the hierarchy of actions [11], e.g., raise left leg, step, walk, walk backwards, walk backwards stylishly, etc.\n\nWe collect 2 labels per sequence, and in case of disagreement regarding multiple actions, a third label. We determine that a sequence contains a single action or multiple actions based on the majority vote of annotators' labels. Overall, BABEL contains 28055 sequence labels 2 .\n\nFrame labels. Frame labels contain language descriptions of all actions that occur in the sequence, and precisely identify the span in the sequence that corresponds to the action. We leverage an existing video annotation tool, VIA [8], and modify the front-end interface and back-end functionality to suit our annotation purposes. For instance, we ensure that every frame in the sequence is annotated with at least one action label. This includes 'transition' which indicates a transition between two actions, or 'unknown' which indicates that the annotator is unclear as to what ac-tion is being performed. This provides us with dense annotations of action labels for the sequence. A screenshot of the AMT task interface for frame label annotation in BABEL is shown in Fig. 2.\n\nTo provide frame labels, an annotator first watches the whole video and enters all the actions in the 'List of Actions' text-box below the video. This populates a set of empty colored box outlines corresponding to each action. The annotator then labels the span of an action by creating a segment (colored rectangular box) with a button press. The duration of the segment and the start/end times can be changed via simple click-and-drag operations. The video frame is continuously updated to the appropriate time-stamp corresponding to the end time of the current active segment. This provides the annotator real-time feedback regarding the exact starting point of the action. Once the segment is placed, its precision can be verified by a 'play segment' option that plays the video span corresponding to the current segment. In case of errors, the segment can be further adjusted. We provide detailed instructions via text, and a video tutorial that explains the task with examples, and demonstrates operation of the annotation interface. The web interface of the task is provided in the project website.\n\nWe collect frame labels for 6663 sequences where both annotators who provided sequence labels agree that the sequence contains multiple actions 3 .\n\nOverall, BABEL contains dense annotations for a total of 66018 action segments for 10892 sequences. This includes both frame labels from sequences containing multiple actions, and sequence labels from sequences containing a single action. If an entire sequence has only a single action, it counts as 1 segment.\n\n\nAnnotators\n\nWe recruit all annotators for our tasks via the Amazon Mechanical Turk (AMT) 4 crowd-sourcing platform. These annotators are located either in the US or Canada. In the sequence label annotation task, we recruit > 850 unique annotators with > 5000 HITs approved and an approval rate > 95%. In the frame labeling task, which is more involved, we first run small-scale tasks to recruit annotators. For further tasks, we only qualify about 130 annotators who demonstrate an understanding of the task and provide satisfactory action labels and precise segments in the sequence. In both tasks, we plan for a median hourly pay of \u223c $12. We also provide bonus pay as an incentive for thorough work in the frame labeling task (details in Sup. Mat.).\n\n\nLabel processing\n\nBABEL contains a total of 15472 unique raw action labels. Note that while each action label is a unique string, labels are often semantically similar (walk, stroll, etc.), are minor variations of an action word (walking, walked, etc.) or are misspelled. Further, tasks like classification require a smaller categorical label set. We organize the raw labels into two smaller sets of semantically higherlevel labels -action categories, and semantic categories.\n\nAction categories. We map the variants of an action into a single category via a semi-automatic process that involves clustering the raw labels, followed by manual adjustment.\n\nWe first pre-process the raw string labels by lowercasing, removing the beginning and ending white-spaces, and lemmatization. We then obtain semantic representations for the raw labels by projecting them into a 300D space via Word2Vec embeddings [24]. Word2Vec is a widely used word embedding model that is based on the distributional hypothesis -words with similar meanings have similar contexts. Given a word, the model is trained to predict surrounding words (context). An intermediate representation from the model serves as a word embedding for the given word. For labels with multiple words, the overall representation is the mean of the Word2Vec embeddings of all words in the label. Labels containing words that are semantically similar, are close in the representation space.\n\nWe cluster labels that are similar in the representation space via K-means (K = 200 clusters). This results in several semantically meaningful clusters, e.g., walk, stroll, stride, etc. which are all mapped to the same cluster. We then manually verify the cluster assignments and fix them to create a semantically meaningful organization of the action labels. Raw labels that are not represented by Word2Vec (e.g., T-pose) are manually organized into relevant categories in this stage. For each cluster, we determine a category name that is either a synonym ('walk' \u2190 {walk, stroll, stride}) or hypernym ('walk' \u2190 {walk forward, walk around}) that describes all action labels in the cluster. Some raw labels, e.g., rotate wrists can be composed into multiple actions like circular movement and wrist movement. Thus, raw labels are occasionally assigned membership to multiple action categories.\n\nOverall, the current version of BABEL has 260 action categories. Interestingly, the most frequent action in BA-BEL is 'transition' -a movement that usually remains unlabeled in most datasets. There are 18447 transitions between different actions in BABEL. Unsurprisingly, the frequency of actions decreases exponentially following Zipf's law -the 50th most frequent action category catch occurs 417 times, the 100th most frequent action category misc. activities, occurs 86 times, and the 200th most frequent action category disagree, occurs 8 times. We visualize the action categories containing the largest number of raw labels (cluster elements) in Fig. 3 (outer circle). Raw bob head, head bang, nod head, nod, move head side Figure 3. Left. 2D t-SNE [36] visualization of the semantic space that we project raw labels into. Similar labels are grouped via K-means clustering. The green shading and points represent the 'dance' cluster and its members respectively. Center. Distribution of (a subset of) action categories (outer circle) under each semantic category (inner circle) in BABEL. The angle occupied by the action category is proportional to the number of unique raw label strings associated with it. Action categories with a large number of fine-grained descriptions are shown. Right. Subset of the fine-grained descriptions associated with selected action categories. labels corresponding to these categories are shown on the right. We provide histograms of duration and number of segments per-action, in the Sup. Mat and project webpage.\n\nSemantic categories of labels. Action categories often reflect qualitatively different types of actions like interacting with objects, actions that describe the trajectory of movement, complex activities involving multiple actions, etc. We formalize the different types of actions in BA-BEL into 8 semantic categories (inner circle in Fig. 3):\n\n1. Simple dynamic actions contain low-level atomic actionswalk, run, kick, punch, etc. 2. Static actions involve transitioning to and/or maintaining a certain posturesit, stand, kneel, etc. 3. Object interaction: e.g., place something, move something, use object, etc. 4. Body part interaction contains actions like touching face, scratch, etc. which typically involve self-contact of body parts. 5. Body part describes the movement of a specific body partraise arm, lower head, rotate wrist, etc. 6. Type of movement contains actions that describe the trajectory of movement of either a body part or the whole bodytwist, circular movement, etc. 7. Activity contains complex actions that often involve multiple low-level actionsplay sports={run, jump}, dance={stretch, bend}, etc. 8. Abstract actions contain actions which often refer to the emotional state of the person and whose physical realizations could have large varianceexcite, endure, learn, find, etc. There are only a few abstract actions in BABEL. The diversity in the types of action labels in BABEL can be leveraged by tasks modeling movement at various levels of semantic abstraction, e.g., movement of body parts like circular movement of wrist at a low level, or high-level semantic activities such as dancing the waltz. Further, depending on the task and model, one can exploit either the discrete set of action categories (e.g., action recognition), or embed the raw action labels into a semantic space to provide a semantic representation of the segment of movement (e.g., action synthesis).\n\nWe provide the full set of semantic categories, action categories, and raw action labels in BABEL in the project webpage.\n\n\nAnalysis\n\nNatural human movement often contains multiple actions and transitions between them. Modeling the likelihood of simultaneous actions and action transitions has applications in reasoning about action affordances in robotics and virtual avatars, motion synthesis [35], activity forecasting [18], animation [33], and action recognition.\n\n\nSimultaneous actions\n\nAlthough people often perform multiple actions simultaneously in real life, this is rarely captured in labeled datasets. Recall from Sec. 3.2 that in BABEL, we ask annotators to label all actions that are occurring in each frame of the sequence. Overall, BABEL has 49952 instances of simultaneous actions that occur with 2907 unique pairs of action categories. Simultaneous actions are defined as actions that overlap for a duration of > 0.1 seconds. We ex-clude the overlap of an action with transition since this implies adjacent actions.\n\nSimultaneous actions often exhibit relationships such as: 1. Hierarchical. Some simultaneous actions reflect the hierarchical structure in actions. For instance, a complex activity & action comprising the activity, e.g., eating food & raise right hand to mouth, and dancing & extend arms. 2. Complementary. The two actions are independent, e.g., hold with left hand & look right.\n\n\n3.\n\nSuperimposed. An action can move a certain body part that partly modifies another action; e.g., carry with right hand modifies the complex activity walk, and right high kick partly modifies the static (full body) action fight stance. 4. Compositional. Actions involving the same body parts that result in a body or part movement that is a function of both actions, e.g., walking & turn.\n\n\nTemporally adjacent actions\n\nThe dense labels in BABEL capture the progression of actions in mocap sequences. We analyze adjacent actions where where action a i follows a j (denoted by a j \u2192 a i ). a i and a j denote action segments, i.e., a contiguous set of frames corresponding to an action (and not the action for a single frame). Thus, a i = a j if the actions are adjacent. We say a j \u2192 a i if the frame succeeding the last frame of a j is the first frame of a i . In practice, we account for imprecise human temporal annotations by ignoring a small overlap in duration (< 0.1 sec.) between a i and a j . We also disregard the separation of actions by transition; i.e., a j \u2192 a i if a j \u2192 a t and a t \u2192 a i , where a t = transition.\n\nWe visualize the frequent transitions between actions, i.e., a j \u2192 a i sorted by Count(a j \u2192 a i ) in BABEL, in Fig. 4. We observe that walk, unsurprisingly, has the most diverse set of adjacent actions, i.e., Count(walk\u2192 a i ) and Count(a j \u2192walk) are large. While transitions between action pairs such as (jog, turn), (walk, t-pose) are bidirectional (with \u223c equal frequency), others have fewer adjacent actions. Some action categories with few transitions illustrate semantically meaningful action chains, e.g., sit \u2192 stand up \u2192 walk and walk \u2192 bend \u2192 pick something up \u2192 place something. Unidirectional transitions such as sit \u2192 stand up and walk \u2192 sit implicitly indicate the arrow of time [26]. Interestingly, the transition from sit \u2192 stand up, and the lack of transition from sit \u2192 stand delineates the subtle difference between the labels stand (static action of 'maintaining an upright position') and stand up (dynamic action of 'rising into an upright position').\n\nGiven the temporally adjacent actions in BABEL, we attempt to model the transition probabilities between actions, i.e., P (a i |a j ). Concretely, we compute   Table 2. Random walk samples based on action transition probabilities learned from BABEL. The generated samples are plausible action sequences simulating natural human movement. P (a i |a i\u22121 , a i\u22122 , a i\u22123 ), an order 3 Markov Chain [9], and observe in Table 2 that random walks along this chain generate plausible action sequences for human movement.\n\n\nBias\n\nThere are a few potential sources of bias in BABEL, which we report in the Sup. Mat. Specifically, we discuss potential biases introduced by the interface design, pay structure, and label processing method. We also analyze the inter-annotator variation in BABEL labels by collecting 5 unique annotations for each of 29 sequences. We find that for the same sequence, annotators vary in the labeled action categories, the number of actions, and segments. In general, the variance appears to be larger for sequences of longer duration. We provide further details in the Sup. Mat.\n\n\nExperiments\n\nThe dense action labels in BABEL can be leveraged for multiple vision tasks like pose estimation, motion synthesis, temporal localization, etc. In this section, we demonstrate the value of BABEL for the 3D action recognition task [21,29], where the goal is to predict a single action category y \u2208 Y, for a given motion segment (x t , \u00b7 \u00b7 \u00b7 , x t ).\n\nMotion representation. A mocap sequence in AMASS, is an array of poses over time, M = (p 1 , \u00b7 \u00b7 \u00b7 , p L ), where p i are pose parameters of the SMPL-H body model [27].\n\nFor consistency with prior work, we predict the 25-joint skeleton used in NTU RGB+D [29] from the vertices of the SMPL-H mesh; see Sup. Mat.\n\nThus, we represent a movement sequence as X = (x 1 , \u00b7 \u00b7 \u00b7 , x L ), where x i \u2208 R J\u00d73 represents the position of the J(= 25) joints in the skeleton, in Cartesian coordinates, (x, y, z).\n\nLabels. In BABEL, a raw action label is mapped to the segment of human movement X s = (x ts , \u00b7 \u00b7 \u00b7 , x te ) corresponding to the action. Recall that a raw action label for a segment can belong to multiple action categories Y s (e.g., rotate wrists \u2192 circular movement, wrist movement). Overall, BABEL contains N movement segments, and their action categories (X s , Y s ) N .\n\nArchitecture. We benchmark performance on BA-BEL with the 2-Stream-Adaptive Graph Convolutional Network (2s-AGCN) [30], a popular architecture that performs graph convolutions spatially (along bones in the skeleton), and temporally (joints across time). Crucially, the graph structure follows the kinematic chain of the skeleton in the first layer but is adaptive -the topology is a function of the layer and the sample. The model achieves good performance on both 2D and 3D action recognition. GCNs remain the architecture of choice even in more recent state-of-theart approaches [6].\n\n2s-AGCN consists of two streams with the same architecture -one which accepts joint positions, and the other, bone lengths and orientations, as input respectively. The final prediction is the average score from the two streams. On NTU RGB+D, 2s-AGCN achieves achieves an accuracy of 88.5% on the cross-subject task. In our experiments, we use only the joint stream, which achieves 2% lower accuracy compared to 2s-AGCN [30].\n\nData pre-processing. We normalize the input skeleton by transforming the coordinates such that the joint position of the middle spine is the origin, the shoulder blades are parallel to the X-axis and the spine to the Y-axis, similar to Shahroudy et al. [29]. We follow the 2s-AGCN preprocessing approach and divide a segment X s into contiguous, non-overlapping 5 sec. chunks, X i s , at 30fps, i.e., X s = (X 1 s , \u00b7 \u00b7 \u00b7 , X K s ). Note that the number of chunks per segment, K = te 5 * 30 . If the Kth chunk X K s has duration < 5 sec., we repeat X K s , and truncate at 5 sec.\n\nThus, a single sample in action recognition is a 5 sec. motion chunk X i s \u2208 X s , and the action category labeled for its corresponding segment y \u2208 Y s in BABEL.\n\nBABEL action recognition splits. BABEL contains 260 action categories with a long-tailed distribution of samples per class, unlike other popular 3D action recognition datasets NTU RGB+D [21,29]. To understand the challenge posed by the long-tailed distribution of action categories in BABEL, we perform experiments with 2 different datasets containing 60 and 120 action categories (see Table  3). These are motion annotation pairs of the 60 and 120 action categories that are obtained from the dense subset of BABEL, containing 10892 sequences described in section 3.2. While BABEL-60 is already long-tailed, BABEL-120 contains both extremely frequent and extremely rare classes. We randomly split the 13220 sequences in BA-BEL into train (60%), val. (20%) and test (20%) sets. We choose the model with best performance on the val. set, and report performance on the test set. We provide the precise distribution of action categories for the train and val. splits in the project webpage.\n\nMetrics. Top-1 measures the accuracy of the highestscoring prediction. Top-5 evaluates whether the groundtruth category is present among the top 5 highest-scoring predictions. It accounts for labeling noise and inherent label ambiguity. Note that it also accounts for the possible presence of multiple action categories Y s , per input movement sequence. Ideal models will score all the categories relevant to a sample highly. Top-1-norm is the mean Top-1 across categories. The magnitude of (Top-1-norm) -(Top-1) illustrates the class-specific bias in the model performance. In BABEL, it reflects the impact of class imbalance on learning.\n\nTraining. We experiment with two losses -standard cross-entropy loss, and the recently introduced focal loss [20]. Focal loss compensates for class imbalance by weighting the cross-entropy loss higher for inaccurate predictions. We observe that a class-balanced loss [7] further improves performance. We refer to this setting of the class-balanced focal loss as Focal in Table 3.\n\nWe use the Adam optimizer [17] with a learning rate of 0.001, and an annealing scheme that decreases the learning rate by a factor of 10 at epochs 20, 40, and 60, following Shi et al. [30]. We used 'Weights & Biases' 5 for tracking experiments [5].\n\nResults. We observe that the decrease in Top-1 and Top-5 performance with the increase in number of classes 5 Table 3. 3D action recognition performance on different subsets of BABEL with 2s-AGCN [30]. CE indicates Cross-Entropy loss and Focal indicates the combination of class-balanced [7] focal loss [20].\n\nis relatively small, in Table 3. Importantly, we note that Top-1-norm is much lower than Top-1. This clearly points to inefficient learning from the long-tailed class distribution in BABEL. The Focal losses significantly improves Top-1-norm performance on all BABEL subsets. This is encouraging for efforts to learn models with lower class-specific biases despite severe class imbalance.\n\nBABEL as a recognition benchmark. On the widely used NTU RGB+D benchmark, Top-1 recognition performance approaches 87% with 2s-AGCN [30]. Note that unlike NTU RGB+D whose distribution of motions and actions is carefully controlled, the diversity and long-tailed distribution of samples in BABEL makes the recognition task more challenging. The few-shot recognition split of NTU RGB+D 120 partly addresses this issue. However, considering few-shot learning as a separate task typically involves measuring performance on only the few-shot classes, ignoring the larger distribution. Models in the real world ideally need to learn and perform well on both the frequent and infrequent classes of an imbalanced distribution [37]. We present BABEL as an additional benchmark for 3D action recognition, which evaluates the ability of models to learn from more realistic distributions of actions.\n\n\nConclusion\n\nWe presented BABEL, a large-scale dataset with dense action labels for mocap sequences. Unlike existing 3D datasets with action labels, BABEL has labels for all actions that occur in the sequence including simultaneously occurring actions, and transitions between actions. We analyzed the relationships between temporally adjacent actions and simultaneous actions occurring in a sequence. We demonstrated that the action recognition task on BABEL is challenging due to the label diversity and long-tailed distribution of samples. We believe that BABEL will serve as a useful additional benchmark for action recognition since it evaluates the ability to model realistic distributions of data. We hope that this large-scale, high quality dataset will accelerate progress in the challenging problem of understanding human movement in semantic terms.\n\nFigure 2 .\n2BABEL annotation interface to collect frame-level action labels. Annotators first name all the actions in the video. They then, precisely align the length of the action segment (colored horizontal bar) with the corresponding duration of the action in the video. This provides dense action labels for the entire sequence.\n\nFigure 4 .\n4Node represent actions, and an edge represents a transition between these actions in the mocap sequence. Edge thickness \u221d Count(ai \u2192 aj) (frequency of transition) in BABEL. # Transition of actions 1 walk, transition, pick up, set down, transition, walk clockwise, transition, stand 2 a-pose, transition, wave hands in and out, wave arms in front of left, transition, cross left leg in circle gesture series, transition, t-pose 3 looking left, standing, transition, looking right, standing 4 stepping forward, standing, turning back, walking back, walking forward, standing, losing balance, transition, turning around, walking, standing 5 step back, stand, transition, walk to the left\n\n\nhttps://wandb.ai # actions Loss type Top-5 Top-1 Top-1-norm60 \nCE \n73.18 41.14 \n24.46 \nFocal \n67.83 33.41 \n30.42 \n\n120 \nCE \n70.49 38.41 \n17.56 \nFocal \n57.96 27.91 \n26.17 \n\n\nNote that the initial 'T-pose' for calibration, followed by standing are considered separate actions with a transition between them.\nNote that a few sequences have additional labels.\nNote that some sequences are labeled by more than 1 annotator. 4 https://www.mturk.com/\n\nAmazon Mechanical Turk. (Date last accessed 16. Amazon Mechanical Turk. (Date last accessed 16- November-2020). 2\n\nDate last accessed 13. 13CMU Graphics LabCMU Graphics Lab. (Date last accessed 13-November- 2020). 1, 2, 3\n\nLan-guage2pose: Natural language grounded pose forecasting. Chaitanya Ahuja, Louis-Philippe Morency, 2019 International Conference on 3D Vision, 3DV 2019. Qu\u00e9bec City, QC, CanadaIEEEChaitanya Ahuja and Louis-Philippe Morency. Lan- guage2pose: Natural language grounded pose forecasting. In 2019 International Conference on 3D Vision, 3DV 2019, Qu\u00e9bec City, QC, Canada, September 16-19, 2019, pages 719-728. IEEE, 2019. 1\n\nMotion synthesis from annotations. Okan Arikan, David A Forsyth, James F O&apos;brien, ACM Transactions on Graphics. 223Okan Arikan, David A. Forsyth, and James F. O'Brien. Motion synthesis from annotations. ACM Transactions on Graphics, 22(3):402-408, 2003. 3\n\nExperiment tracking with weights and biases, 2020. Software available from wandb. Lukas Biewald, Lukas Biewald. Experiment tracking with weights and bi- ases, 2020. Software available from wandb.com. 8\n\nSkeleton-based action recognition with shift graph convolutional network. Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, Hanqing Lu, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, and Hanqing Lu. Skeleton-based action recognition with shift graph convolutional network. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 180-189. IEEE, 2020. 8\n\nClass-balanced loss based on effective number of samples. Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition89Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9268- 9277, 2019. 8, 9\n\nThe VIA annotation software for images, audio and video. Abhishek Dutta, Andrew Zisserman, Proceedings of the 27th ACM International Conference on Multimedia, MM 2019. the 27th ACM International Conference on Multimedia, MM 2019Nice, FranceACM24Abhishek Dutta and Andrew Zisserman. The VIA annota- tion software for images, audio and video. In Proceedings of the 27th ACM International Conference on Multimedia, MM 2019, Nice, France, October 21-25, 2019, pages 2276-2279. ACM, 2019. 2, 4\n\nMarkov chains: from theory to implementation and experimentation. A Paul, Gagniuc, John Wiley & SonsPaul A Gagniuc. Markov chains: from theory to implemen- tation and experimentation. John Wiley & Sons, 2017. 7\n\nMovi: A large multipurpose motion and video dataset. Saeed Ghorbani, Kimia Mahdaviani, Anne Thaler, Konrad Kording, Douglas James Cook, Gunnar Blohm, Nikolaus F Troje, 13Saeed Ghorbani, Kimia Mahdaviani, Anne Thaler, Konrad Kording, Douglas James Cook, Gunnar Blohm, and Niko- laus F. Troje. Movi: A large multipurpose motion and video dataset, 2020. 1, 3\n\nAVA: A video dataset of spatio-temporally localized atomic visual actions. Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer Society14Chunhui Gu, Chen Sun, David A. Ross, Carl Von- drick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya- narasimhan, George Toderici, Susanna Ricco, Rahul Suk- thankar, Cordelia Schmid, and Jitendra Malik. AVA: A video dataset of spatio-temporally localized atomic visual actions. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18- 22, 2018, pages 6047-6056. IEEE Computer Society, 2018. 1, 3, 4\n\nAc-tion2Motion: Conditioned Generation of 3d Human Motions. Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, Li Cheng, MM '20: The 28th ACM International Conference on Multimedia, Virtual Event. Seattle, WA, USAACM13Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac- tion2Motion: Conditioned Generation of 3d Human Mo- tions. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, pages 2021-2029. ACM, 2020. 1, 3\n\nRobust motion in-betweening. G F\u00e9lix, Mike Harvey, Derek Yurick, Christopher J Nowrouzezahrai, Pal, ACM Trans. Graph. 3943F\u00e9lix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher J. Pal. Robust motion in-betweening. ACM Trans. Graph., 39(4):60, 2020. 1, 2, 3\n\nGoing deeper into action recognition: A survey. Image Vision Computing. Samitha Herath, Fatih Mehrtash Tafazzoli Harandi, Porikli, 60Samitha Herath, Mehrtash Tafazzoli Harandi, and Fatih Porikli. Going deeper into action recognition: A survey. Im- age Vision Computing, 60:4-21, 2017. 1\n\nThe THUMOS challenge on action recognition for videos. Haroon Idrees, Yu-Gang Amir Roshan Zamir, Alex Jiang, Ivan Gorban, Rahul Laptev, Mubarak Sukthankar, Shah, Computer Vision and Image Understanding. 155Haroon Idrees, Amir Roshan Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The THUMOS challenge on action recognition for videos \"in the wild\". Computer Vision and Image Understanding, 155:1-23, 2017. 3\n\nHuman3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments. Catalin Ionescu, Dragos Papava, Vlad Olaru, Cristian Sminchisescu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3673Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predic- tive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 36(7):1325-1339, 2014. 1, 2, 3\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Represen- tations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 8\n\nActivity forecasting. Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, Martial Hebert, Computer Vision -ECCV 2012 -12th European Conference on Computer Vision. Florence, ItalySpringer7575Proceedings, Part IVKris M. Kitani, Brian D. Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting. In Computer Vi- sion -ECCV 2012 -12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part IV, volume 7575 of Lecture Notes in Computer Science, pages 201-214. Springer, 2012. 6\n\nLemeng Angela S Lin, Rodolfo Wu, Kevin Corona, Qixing Tai, Raymond J Huang, Mooney, Generating animated videos of human activities from natural language descriptions. Learning. Angela S Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qix- ing Huang, and Raymond J Mooney. Generating animated videos of human activities from natural language descrip- tions. Learning, 2018, 2018. 1\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross B Girshick, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4229Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318-327, 2020. 8, 9\n\nNTU RGB+D 120: A largescale benchmark for 3d human activity understanding. Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C Kot, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4210Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C. Kot. NTU RGB+D 120: A large- scale benchmark for 3d human activity understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10):2684-2701, 2020. 2, 3, 7, 8\n\nAMASS: Archive of motion capture as surface shapes. Naureen Mahmood, Nima Ghorbani, F Nikolaus, Gerard Troje, Michael J Pons-Moll, Black, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). 24Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger- ard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In 2019 IEEE/CVF Inter- national Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pages 5441- 5450. IEEE, 2019. 2, 3, 4\n\nThe KIT whole-body human motion database. Christian Mandery, \u00d6mer Terlemez, Martin Do, Nikolaus Vahrenkamp, Tamim Asfour, International Conference on Advanced Robotics. Istanbul, Turkey23Christian Mandery,\u00d6mer Terlemez, Martin Do, Nikolaus Vahrenkamp, and Tamim Asfour. The KIT whole-body hu- man motion database. In International Conference on Ad- vanced Robotics, ICAR 2015, Istanbul, Turkey, July 27-31, 2015, pages 329-336. IEEE, 2015. 2, 3\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, United StatesProceedings of a meeting heldTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Cor- rado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neu- ral Information Processing Systems 26: 27th Annual Confer- ence on Neural Information Processing Systems 2013. Pro- ceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111-3119, 2013. 5\n\nEfficient and robust annotation of motion capture data. Meinard M\u00fcller, Andreas Baak, Hans-Peter Seidel, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA 2009. the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA 2009New Orleans, Louisiana, USAACMMeinard M\u00fcller, Andreas Baak, and Hans-Peter Seidel. Ef- ficient and robust annotation of motion capture data. In Pro- ceedings of the 2009 ACM SIGGRAPH/Eurographics Sym- posium on Computer Animation, SCA 2009, New Orleans, Louisiana, USA, August 1-2, 2009, pages 17-26. ACM, 2009. 3\n\nSeeing the arrow of time. Lyndsey C Pickup, Zheng Pan, Donglai Wei, Yi-Chang Shih, Changshui Zhang, Andrew Zisserman, Bernhard Sch\u00f6lkopf, William T Freeman, 2014 IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USAIEEE Computer SocietyLyndsey C. Pickup, Zheng Pan, Donglai Wei, Yi-Chang Shih, Changshui Zhang, Andrew Zisserman, Bernhard Sch\u00f6lkopf, and William T. Freeman. Seeing the arrow of time. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 2043-2050. IEEE Computer Society, 2014. 7\n\nEmbodied hands: Modeling and capturing hands and bodies together. Javier Romero, Dimitrios Tzionas, Michael J Black, Proc. SIG-GRAPH Asia). SIG-GRAPH Asia)36Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bod- ies together. ACM Transactions on Graphics, (Proc. SIG- GRAPH Asia), 36(6), Nov. 2017. 8\n\nBenchmarking search and annotation in continuous human skeleton sequences. Jan Sedmidubsk\u00fd, Petr Elias, Pavel Zezula, Proceedings of the 2019 on International Conference on Multimedia Retrieval, ICMR 2019. the 2019 on International Conference on Multimedia Retrieval, ICMR 2019Ottawa, ON, CanadaACM13Jan Sedmidubsk\u00fd, Petr Elias, and Pavel Zezula. Benchmark- ing search and annotation in continuous human skeleton se- quences. In Proceedings of the 2019 on International Con- ference on Multimedia Retrieval, ICMR 2019, Ottawa, ON, Canada, June 10-13, 2019, pages 38-42. ACM, 2019. 1, 2, 3\n\nNTU RGB+D: A large scale dataset for 3d human activity analysis. Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyAmir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. NTU RGB+D: A large scale dataset for 3d human activ- ity analysis. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 1010-1019. IEEE Computer Soci- ety, 2016. 1, 2, 3, 7, 8\n\nTwostream adaptive graph convolutional networks for skeletonbased action recognition. Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA89Computer Vision Foundation / IEEELei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two- stream adaptive graph convolutional networks for skeleton- based action recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 12026-12035. Computer Vision Foundation / IEEE, 2019. 8, 9\n\nHollywood in homes: Crowdsourcing data collection for activity understanding. A Gunnar, G\u00fcl Sigurdsson, Xiaolong Varol, Ali Wang, Ivan Farhadi, Abhinav Laptev, Gupta, Computer Vision -ECCV 2016 -14th European Conference. Amsterdam, The NetherlandsSpringer9905Proceedings, Part IGunnar A. Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity under- standing. In Computer Vision -ECCV 2016 -14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I, volume 9905 of Lecture Notes in Computer Science, pages 510-526. Springer, 2016. 3\n\nTemporal segmentation and activity classification from first-person sensing. Ekaterina H Spriggs, Fernando De La, Torre , Martial Hebert, IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops. Miami, FL, USAIEEE Computer SocietyEkaterina H. Spriggs, Fernando De la Torre, and Martial Hebert. Temporal segmentation and activity classification from first-person sensing. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2009, Mi- ami, FL, USA, 20-25 June, 2009, pages 17-24. IEEE Com- puter Society, 2009. 3\n\nNeural state machine for character-scene interactions. Sebastian Starke, He Zhang, Taku Komura, Jun Saito, 209:1-209:14ACM Transactions on Graphics. 386Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. ACM Transactions on Graphics, 38(6):209:1-209:14, 2019. 6\n\nGRAB: A dataset of whole-body human grasping of objects. Omid Taheri, Nima Ghorbani, Michael J Black, Dimitrios Tzionas, Computer Vision -ECCV 2020 -16th European Conference. Glasgow, UKSpringer123494Proceedings, Part IVOmid Taheri, Nima Ghorbani, Michael J. Black, and Dim- itrios Tzionas. GRAB: A dataset of whole-body human grasping of objects. In Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, volume 12349 of Lecture Notes in Computer Science, pages 581-600. Springer, 2020. 3, 4\n\nHuman motion anticipation with symbolic label. ArXiv, abs/1912.06079. , A Julian Tanke, Juergen Weber, Gall, Julian Tanke, A. Weber, and Juergen Gall. Human motion anticipation with symbolic label. ArXiv, abs/1912.06079, 2019. 6\n\nVisualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of Machine Learning Research. 986Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(86):2579-2605, 2008. 6\n\nFew-shot learning with localization in realistic settings. Davis Wertheimer, Bharath Hariharan, IEEE Conference on Computer Vision and Pattern Recognition. Long Beach, CA, USAComputer Vision Foundation / IEEEDavis Wertheimer and Bharath Hariharan. Few-shot learning with localization in realistic settings. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6558-6567. Computer Vision Foundation / IEEE, 2019. 9\n\nWatch-n-patch: Unsupervised understanding of actions and relations. Chenxia Wu, Jiemi Zhang, Silvio Savarese, Ashutosh Saxena, IEEE Conference on Computer Vision and Pattern Recognition. Boston, MA, USAIEEE Computer SocietyChenxia Wu, Jiemi Zhang, Silvio Savarese, and Ashutosh Saxena. Watch-n-patch: Unsupervised understanding of ac- tions and relations. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4362-4370. IEEE Computer Society, 2015. 3\n\nMSR-VTT: A large video description dataset for bridging video and language. Jun Xu, Tao Mei, Ting Yao, Yong Rui, 2016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USAIEEE Computer SocietyJun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging video and lan- guage. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 5288-5296. IEEE Computer Society, 2016. 1\n\nEvery moment counts: Dense detailed labeling of actions in complex videos. Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, Li Fei-Fei, Int. J. Comput. Vis. 1262-4Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo An- driluka, Greg Mori, and Li Fei-Fei. Every moment counts: Dense detailed labeling of actions in complex videos. Int. J. Comput. Vis., 126(2-4):375-389, 2018. 3\n\nHACS: human action clips and segments dataset for recognition and temporal localization. Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). IEEE13Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. HACS: human action clips and segments dataset for recognition and temporal localization. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pages 8667-8677. IEEE, 2019. 1, 2, 3\n\nPolarization human shape and pose dataset. CoRR, abs. Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chi Xu, Minglun Gong, Li Cheng, Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chi Xu, Minglun Gong, and Li Cheng. Polarization human shape and pose dataset. CoRR, abs/2004.14899, 2020. 1, 2\n", "annotations": {"author": "[{\"end\":164,\"start\":58},{\"end\":275,\"start\":165},{\"end\":378,\"start\":276},{\"end\":470,\"start\":379},{\"end\":566,\"start\":471}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":70},{\"end\":185,\"start\":171},{\"end\":292,\"start\":282},{\"end\":403,\"start\":389},{\"end\":486,\"start\":481}]", "author_first_name": "[{\"end\":67,\"start\":58},{\"end\":69,\"start\":68},{\"end\":170,\"start\":165},{\"end\":281,\"start\":276},{\"end\":388,\"start\":379},{\"end\":478,\"start\":471},{\"end\":480,\"start\":479}]", "author_affiliation": "[{\"end\":163,\"start\":103},{\"end\":274,\"start\":214},{\"end\":377,\"start\":317},{\"end\":469,\"start\":433},{\"end\":565,\"start\":505}]", "title": "[{\"end\":55,\"start\":1},{\"end\":621,\"start\":567}]", "venue": null, "abstract": "[{\"end\":3473,\"start\":623}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3671,\"start\":3667},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3695,\"start\":3691},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4227,\"start\":4223},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4230,\"start\":4227},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4346,\"start\":4342},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4364,\"start\":4361},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4367,\"start\":4364},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4667,\"start\":4663},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4677,\"start\":4673},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4691,\"start\":4687},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4772,\"start\":4768},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4795,\"start\":4791},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4920,\"start\":4917},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4923,\"start\":4920},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4926,\"start\":4923},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4929,\"start\":4926},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6066,\"start\":6062},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6432,\"start\":6429},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6435,\"start\":6432},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6438,\"start\":6435},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6441,\"start\":6438},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6444,\"start\":6441},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6447,\"start\":6444},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6450,\"start\":6447},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6453,\"start\":6450},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7002,\"start\":6998},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7005,\"start\":7002},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7025,\"start\":7021},{\"end\":8486,\"start\":8477},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8983,\"start\":8980},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9053,\"start\":9050},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9143,\"start\":9139},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10580,\"start\":10577},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10948,\"start\":10945},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11191,\"start\":11187},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11285,\"start\":11281},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11442,\"start\":11438},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11507,\"start\":11503},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11627,\"start\":11623},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11896,\"start\":11892},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12209,\"start\":12205},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12315,\"start\":12312},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12338,\"start\":12334},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12610,\"start\":12606},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12623,\"start\":12619},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13366,\"start\":13362},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13523,\"start\":13519},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13526,\"start\":13523},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13529,\"start\":13526},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13547,\"start\":13543},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13781,\"start\":13777},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13935,\"start\":13931},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14064,\"start\":14060},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14767,\"start\":14763},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15381,\"start\":15377},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17032,\"start\":17031},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17950,\"start\":17946},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18548,\"start\":18545},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22322,\"start\":22318},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24513,\"start\":24509},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27617,\"start\":27613},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27644,\"start\":27640},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27660,\"start\":27656},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30466,\"start\":30462},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31141,\"start\":31138},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32091,\"start\":32087},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32094,\"start\":32091},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32374,\"start\":32370},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32465,\"start\":32461},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33202,\"start\":33198},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33668,\"start\":33665},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34094,\"start\":34090},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34354,\"start\":34350},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35032,\"start\":35028},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35035,\"start\":35032},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36586,\"start\":36582},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36743,\"start\":36740},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36884,\"start\":36880},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":37042,\"start\":37038},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37101,\"start\":37098},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":37213,\"start\":37212},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":37304,\"start\":37300},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37395,\"start\":37392},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37411,\"start\":37407},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":37939,\"start\":37935},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":38525,\"start\":38521}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39885,\"start\":39552},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40583,\"start\":39886},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40758,\"start\":40584}]", "paragraph": "[{\"end\":4368,\"start\":3489},{\"end\":5166,\"start\":4370},{\"end\":6006,\"start\":5168},{\"end\":6373,\"start\":6008},{\"end\":7412,\"start\":6375},{\"end\":7967,\"start\":7414},{\"end\":8487,\"start\":7969},{\"end\":9836,\"start\":8489},{\"end\":10717,\"start\":9838},{\"end\":12148,\"start\":10734},{\"end\":12556,\"start\":12150},{\"end\":13403,\"start\":12558},{\"end\":14498,\"start\":13405},{\"end\":14669,\"start\":14510},{\"end\":16351,\"start\":14689},{\"end\":16585,\"start\":16353},{\"end\":16858,\"start\":16609},{\"end\":17325,\"start\":16860},{\"end\":18032,\"start\":17327},{\"end\":18312,\"start\":18034},{\"end\":19091,\"start\":18314},{\"end\":20198,\"start\":19093},{\"end\":20347,\"start\":20200},{\"end\":20659,\"start\":20349},{\"end\":21414,\"start\":20674},{\"end\":21893,\"start\":21435},{\"end\":22070,\"start\":21895},{\"end\":22856,\"start\":22072},{\"end\":23752,\"start\":22858},{\"end\":25307,\"start\":23754},{\"end\":25652,\"start\":25309},{\"end\":27216,\"start\":25654},{\"end\":27339,\"start\":27218},{\"end\":27685,\"start\":27352},{\"end\":28250,\"start\":27710},{\"end\":28631,\"start\":28252},{\"end\":29024,\"start\":28638},{\"end\":29765,\"start\":29056},{\"end\":30741,\"start\":29767},{\"end\":31256,\"start\":30743},{\"end\":31841,\"start\":31265},{\"end\":32205,\"start\":31857},{\"end\":32375,\"start\":32207},{\"end\":32517,\"start\":32377},{\"end\":32704,\"start\":32519},{\"end\":33082,\"start\":32706},{\"end\":33669,\"start\":33084},{\"end\":34095,\"start\":33671},{\"end\":34676,\"start\":34097},{\"end\":34840,\"start\":34678},{\"end\":35829,\"start\":34842},{\"end\":36471,\"start\":35831},{\"end\":36852,\"start\":36473},{\"end\":37102,\"start\":36854},{\"end\":37412,\"start\":37104},{\"end\":37801,\"start\":37414},{\"end\":38690,\"start\":37803},{\"end\":39551,\"start\":38705}]", "formula": null, "table_ref": "[{\"end\":10892,\"start\":10885},{\"end\":13379,\"start\":13372},{\"end\":30910,\"start\":30903},{\"end\":31165,\"start\":31158},{\"end\":35236,\"start\":35228},{\"end\":36851,\"start\":36844},{\"end\":37221,\"start\":37214},{\"end\":37445,\"start\":37438}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3487,\"start\":3475},{\"attributes\":{\"n\":\"2.\"},\"end\":10732,\"start\":10720},{\"attributes\":{\"n\":\"3.\"},\"end\":14508,\"start\":14501},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14687,\"start\":14672},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16607,\"start\":16588},{\"attributes\":{\"n\":\"3.3.\"},\"end\":20672,\"start\":20662},{\"attributes\":{\"n\":\"3.4.\"},\"end\":21433,\"start\":21417},{\"attributes\":{\"n\":\"4.\"},\"end\":27350,\"start\":27342},{\"attributes\":{\"n\":\"4.1.\"},\"end\":27708,\"start\":27688},{\"end\":28636,\"start\":28634},{\"attributes\":{\"n\":\"4.2.\"},\"end\":29054,\"start\":29027},{\"attributes\":{\"n\":\"4.3.\"},\"end\":31263,\"start\":31259},{\"attributes\":{\"n\":\"5.\"},\"end\":31855,\"start\":31844},{\"attributes\":{\"n\":\"6.\"},\"end\":38703,\"start\":38693},{\"end\":39563,\"start\":39553},{\"end\":39897,\"start\":39887}]", "table": "[{\"end\":40758,\"start\":40645}]", "figure_caption": "[{\"end\":39885,\"start\":39565},{\"end\":40583,\"start\":39899},{\"end\":40645,\"start\":40586}]", "figure_ref": "[{\"end\":3753,\"start\":3745},{\"end\":5417,\"start\":5411},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14824,\"start\":14816},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19090,\"start\":19084},{\"end\":24427,\"start\":24406},{\"end\":24492,\"start\":24484},{\"end\":25650,\"start\":25644},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29885,\"start\":29879}]", "bib_author_first_name": "[{\"end\":41323,\"start\":41314},{\"end\":41345,\"start\":41331},{\"end\":41715,\"start\":41711},{\"end\":41729,\"start\":41724},{\"end\":41731,\"start\":41730},{\"end\":41746,\"start\":41741},{\"end\":41748,\"start\":41747},{\"end\":42025,\"start\":42020},{\"end\":42217,\"start\":42215},{\"end\":42230,\"start\":42225},{\"end\":42245,\"start\":42238},{\"end\":42256,\"start\":42250},{\"end\":42267,\"start\":42263},{\"end\":42282,\"start\":42275},{\"end\":42740,\"start\":42737},{\"end\":42753,\"start\":42746},{\"end\":42767,\"start\":42759},{\"end\":42777,\"start\":42773},{\"end\":42789,\"start\":42784},{\"end\":43257,\"start\":43249},{\"end\":43271,\"start\":43265},{\"end\":43749,\"start\":43748},{\"end\":43952,\"start\":43947},{\"end\":43968,\"start\":43963},{\"end\":43985,\"start\":43981},{\"end\":44000,\"start\":43994},{\"end\":44017,\"start\":44010},{\"end\":44023,\"start\":44018},{\"end\":44036,\"start\":44030},{\"end\":44052,\"start\":44044},{\"end\":44054,\"start\":44053},{\"end\":44333,\"start\":44326},{\"end\":44342,\"start\":44338},{\"end\":44353,\"start\":44348},{\"end\":44355,\"start\":44354},{\"end\":44366,\"start\":44362},{\"end\":44385,\"start\":44377},{\"end\":44403,\"start\":44397},{\"end\":44418,\"start\":44408},{\"end\":44443,\"start\":44437},{\"end\":44461,\"start\":44454},{\"end\":44474,\"start\":44469},{\"end\":44495,\"start\":44487},{\"end\":44512,\"start\":44504},{\"end\":45154,\"start\":45149},{\"end\":45166,\"start\":45160},{\"end\":45175,\"start\":45172},{\"end\":45188,\"start\":45182},{\"end\":45201,\"start\":45194},{\"end\":45212,\"start\":45207},{\"end\":45226,\"start\":45219},{\"end\":45235,\"start\":45233},{\"end\":45684,\"start\":45683},{\"end\":45696,\"start\":45692},{\"end\":45710,\"start\":45705},{\"end\":45730,\"start\":45719},{\"end\":45732,\"start\":45731},{\"end\":46004,\"start\":45997},{\"end\":46018,\"start\":46013},{\"end\":46274,\"start\":46268},{\"end\":46290,\"start\":46283},{\"end\":46314,\"start\":46310},{\"end\":46326,\"start\":46322},{\"end\":46340,\"start\":46335},{\"end\":46356,\"start\":46349},{\"end\":46767,\"start\":46760},{\"end\":46783,\"start\":46777},{\"end\":46796,\"start\":46792},{\"end\":46812,\"start\":46804},{\"end\":47212,\"start\":47211},{\"end\":47228,\"start\":47223},{\"end\":47635,\"start\":47631},{\"end\":47637,\"start\":47636},{\"end\":47651,\"start\":47646},{\"end\":47653,\"start\":47652},{\"end\":47668,\"start\":47663},{\"end\":47675,\"start\":47669},{\"end\":47692,\"start\":47685},{\"end\":48146,\"start\":48140},{\"end\":48168,\"start\":48161},{\"end\":48178,\"start\":48173},{\"end\":48193,\"start\":48187},{\"end\":48208,\"start\":48199},{\"end\":48594,\"start\":48586},{\"end\":48605,\"start\":48600},{\"end\":48617,\"start\":48613},{\"end\":48619,\"start\":48618},{\"end\":48981,\"start\":48978},{\"end\":48991,\"start\":48987},{\"end\":49011,\"start\":49003},{\"end\":49023,\"start\":49019},{\"end\":49037,\"start\":49030},{\"end\":49048,\"start\":49044},{\"end\":49050,\"start\":49049},{\"end\":49443,\"start\":49436},{\"end\":49457,\"start\":49453},{\"end\":49469,\"start\":49468},{\"end\":49486,\"start\":49480},{\"end\":49501,\"start\":49494},{\"end\":49503,\"start\":49502},{\"end\":49978,\"start\":49969},{\"end\":49992,\"start\":49988},{\"end\":50009,\"start\":50003},{\"end\":50022,\"start\":50014},{\"end\":50040,\"start\":50035},{\"end\":50455,\"start\":50450},{\"end\":50469,\"start\":50465},{\"end\":50484,\"start\":50481},{\"end\":50498,\"start\":50491},{\"end\":50500,\"start\":50499},{\"end\":50517,\"start\":50510},{\"end\":51170,\"start\":51163},{\"end\":51186,\"start\":51179},{\"end\":51203,\"start\":51193},{\"end\":51729,\"start\":51722},{\"end\":51731,\"start\":51730},{\"end\":51745,\"start\":51740},{\"end\":51758,\"start\":51751},{\"end\":51772,\"start\":51764},{\"end\":51788,\"start\":51779},{\"end\":51802,\"start\":51796},{\"end\":51822,\"start\":51814},{\"end\":51841,\"start\":51834},{\"end\":51843,\"start\":51842},{\"end\":52356,\"start\":52350},{\"end\":52374,\"start\":52365},{\"end\":52391,\"start\":52384},{\"end\":52393,\"start\":52392},{\"end\":52719,\"start\":52716},{\"end\":52737,\"start\":52733},{\"end\":52750,\"start\":52745},{\"end\":53300,\"start\":53296},{\"end\":53315,\"start\":53312},{\"end\":53331,\"start\":53321},{\"end\":53340,\"start\":53336},{\"end\":53853,\"start\":53850},{\"end\":53864,\"start\":53859},{\"end\":53876,\"start\":53872},{\"end\":53891,\"start\":53884},{\"end\":54419,\"start\":54418},{\"end\":54431,\"start\":54428},{\"end\":54452,\"start\":54444},{\"end\":54463,\"start\":54460},{\"end\":54474,\"start\":54470},{\"end\":54491,\"start\":54484},{\"end\":55088,\"start\":55079},{\"end\":55090,\"start\":55089},{\"end\":55108,\"start\":55100},{\"end\":55121,\"start\":55116},{\"end\":55131,\"start\":55124},{\"end\":55623,\"start\":55614},{\"end\":55634,\"start\":55632},{\"end\":55646,\"start\":55642},{\"end\":55658,\"start\":55655},{\"end\":55942,\"start\":55938},{\"end\":55955,\"start\":55951},{\"end\":55973,\"start\":55966},{\"end\":55975,\"start\":55974},{\"end\":55992,\"start\":55983},{\"end\":56500,\"start\":56499},{\"end\":56502,\"start\":56501},{\"end\":56524,\"start\":56517},{\"end\":56696,\"start\":56689},{\"end\":56721,\"start\":56713},{\"end\":56973,\"start\":56968},{\"end\":56993,\"start\":56986},{\"end\":57465,\"start\":57458},{\"end\":57475,\"start\":57470},{\"end\":57489,\"start\":57483},{\"end\":57508,\"start\":57500},{\"end\":57982,\"start\":57979},{\"end\":57990,\"start\":57987},{\"end\":58000,\"start\":57996},{\"end\":58010,\"start\":58006},{\"end\":58486,\"start\":58480},{\"end\":58498,\"start\":58494},{\"end\":58516,\"start\":58512},{\"end\":58530,\"start\":58522},{\"end\":58546,\"start\":58542},{\"end\":58555,\"start\":58553},{\"end\":58900,\"start\":58896},{\"end\":58914,\"start\":58907},{\"end\":58932,\"start\":58925},{\"end\":58952,\"start\":58944},{\"end\":59434,\"start\":59428},{\"end\":59446,\"start\":59440},{\"end\":59458,\"start\":59452},{\"end\":59468,\"start\":59465},{\"end\":59478,\"start\":59475},{\"end\":59490,\"start\":59483},{\"end\":59499,\"start\":59497}]", "bib_author_last_name": "[{\"end\":41329,\"start\":41324},{\"end\":41353,\"start\":41346},{\"end\":41722,\"start\":41716},{\"end\":41739,\"start\":41732},{\"end\":41761,\"start\":41749},{\"end\":42033,\"start\":42026},{\"end\":42223,\"start\":42218},{\"end\":42236,\"start\":42231},{\"end\":42248,\"start\":42246},{\"end\":42261,\"start\":42257},{\"end\":42273,\"start\":42268},{\"end\":42285,\"start\":42283},{\"end\":42744,\"start\":42741},{\"end\":42757,\"start\":42754},{\"end\":42771,\"start\":42768},{\"end\":42782,\"start\":42778},{\"end\":42798,\"start\":42790},{\"end\":43263,\"start\":43258},{\"end\":43281,\"start\":43272},{\"end\":43754,\"start\":43750},{\"end\":43763,\"start\":43756},{\"end\":43961,\"start\":43953},{\"end\":43979,\"start\":43969},{\"end\":43992,\"start\":43986},{\"end\":44008,\"start\":44001},{\"end\":44028,\"start\":44024},{\"end\":44042,\"start\":44037},{\"end\":44060,\"start\":44055},{\"end\":44336,\"start\":44334},{\"end\":44346,\"start\":44343},{\"end\":44360,\"start\":44356},{\"end\":44375,\"start\":44367},{\"end\":44395,\"start\":44386},{\"end\":44406,\"start\":44404},{\"end\":44435,\"start\":44419},{\"end\":44452,\"start\":44444},{\"end\":44467,\"start\":44462},{\"end\":44485,\"start\":44475},{\"end\":44502,\"start\":44496},{\"end\":44518,\"start\":44513},{\"end\":45158,\"start\":45155},{\"end\":45170,\"start\":45167},{\"end\":45180,\"start\":45176},{\"end\":45192,\"start\":45189},{\"end\":45205,\"start\":45202},{\"end\":45217,\"start\":45213},{\"end\":45231,\"start\":45227},{\"end\":45241,\"start\":45236},{\"end\":45690,\"start\":45685},{\"end\":45703,\"start\":45697},{\"end\":45717,\"start\":45711},{\"end\":45747,\"start\":45733},{\"end\":45752,\"start\":45749},{\"end\":46011,\"start\":46005},{\"end\":46045,\"start\":46019},{\"end\":46054,\"start\":46047},{\"end\":46281,\"start\":46275},{\"end\":46308,\"start\":46291},{\"end\":46320,\"start\":46315},{\"end\":46333,\"start\":46327},{\"end\":46347,\"start\":46341},{\"end\":46367,\"start\":46357},{\"end\":46373,\"start\":46369},{\"end\":46775,\"start\":46768},{\"end\":46790,\"start\":46784},{\"end\":46802,\"start\":46797},{\"end\":46825,\"start\":46813},{\"end\":47221,\"start\":47213},{\"end\":47235,\"start\":47229},{\"end\":47239,\"start\":47237},{\"end\":47644,\"start\":47638},{\"end\":47661,\"start\":47654},{\"end\":47683,\"start\":47676},{\"end\":47699,\"start\":47693},{\"end\":48159,\"start\":48147},{\"end\":48171,\"start\":48169},{\"end\":48185,\"start\":48179},{\"end\":48197,\"start\":48194},{\"end\":48214,\"start\":48209},{\"end\":48222,\"start\":48216},{\"end\":48598,\"start\":48595},{\"end\":48611,\"start\":48606},{\"end\":48628,\"start\":48620},{\"end\":48985,\"start\":48982},{\"end\":49001,\"start\":48992},{\"end\":49017,\"start\":49012},{\"end\":49028,\"start\":49024},{\"end\":49042,\"start\":49038},{\"end\":49054,\"start\":49051},{\"end\":49451,\"start\":49444},{\"end\":49466,\"start\":49458},{\"end\":49478,\"start\":49470},{\"end\":49492,\"start\":49487},{\"end\":49513,\"start\":49504},{\"end\":49520,\"start\":49515},{\"end\":49986,\"start\":49979},{\"end\":50001,\"start\":49993},{\"end\":50012,\"start\":50010},{\"end\":50033,\"start\":50023},{\"end\":50047,\"start\":50041},{\"end\":50463,\"start\":50456},{\"end\":50479,\"start\":50470},{\"end\":50489,\"start\":50485},{\"end\":50508,\"start\":50501},{\"end\":50522,\"start\":50518},{\"end\":51177,\"start\":51171},{\"end\":51191,\"start\":51187},{\"end\":51210,\"start\":51204},{\"end\":51738,\"start\":51732},{\"end\":51749,\"start\":51746},{\"end\":51762,\"start\":51759},{\"end\":51777,\"start\":51773},{\"end\":51794,\"start\":51789},{\"end\":51812,\"start\":51803},{\"end\":51832,\"start\":51823},{\"end\":51851,\"start\":51844},{\"end\":52363,\"start\":52357},{\"end\":52382,\"start\":52375},{\"end\":52399,\"start\":52394},{\"end\":52731,\"start\":52720},{\"end\":52743,\"start\":52738},{\"end\":52757,\"start\":52751},{\"end\":53310,\"start\":53301},{\"end\":53319,\"start\":53316},{\"end\":53334,\"start\":53332},{\"end\":53345,\"start\":53341},{\"end\":53857,\"start\":53854},{\"end\":53870,\"start\":53865},{\"end\":53882,\"start\":53877},{\"end\":53894,\"start\":53892},{\"end\":54426,\"start\":54420},{\"end\":54442,\"start\":54432},{\"end\":54458,\"start\":54453},{\"end\":54468,\"start\":54464},{\"end\":54482,\"start\":54475},{\"end\":54498,\"start\":54492},{\"end\":54505,\"start\":54500},{\"end\":55098,\"start\":55091},{\"end\":55114,\"start\":55109},{\"end\":55138,\"start\":55132},{\"end\":55630,\"start\":55624},{\"end\":55640,\"start\":55635},{\"end\":55653,\"start\":55647},{\"end\":55664,\"start\":55659},{\"end\":55949,\"start\":55943},{\"end\":55964,\"start\":55956},{\"end\":55981,\"start\":55976},{\"end\":56000,\"start\":55993},{\"end\":56515,\"start\":56503},{\"end\":56530,\"start\":56525},{\"end\":56536,\"start\":56532},{\"end\":56711,\"start\":56697},{\"end\":56728,\"start\":56722},{\"end\":56984,\"start\":56974},{\"end\":57003,\"start\":56994},{\"end\":57468,\"start\":57466},{\"end\":57481,\"start\":57476},{\"end\":57498,\"start\":57490},{\"end\":57515,\"start\":57509},{\"end\":57985,\"start\":57983},{\"end\":57994,\"start\":57991},{\"end\":58004,\"start\":58001},{\"end\":58014,\"start\":58011},{\"end\":58492,\"start\":58487},{\"end\":58510,\"start\":58499},{\"end\":58520,\"start\":58517},{\"end\":58540,\"start\":58531},{\"end\":58551,\"start\":58547},{\"end\":58563,\"start\":58556},{\"end\":58905,\"start\":58901},{\"end\":58923,\"start\":58915},{\"end\":58942,\"start\":58933},{\"end\":58956,\"start\":58953},{\"end\":59438,\"start\":59435},{\"end\":59450,\"start\":59447},{\"end\":59463,\"start\":59459},{\"end\":59473,\"start\":59469},{\"end\":59481,\"start\":59479},{\"end\":59495,\"start\":59491},{\"end\":59505,\"start\":59500}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":41144,\"start\":41031},{\"attributes\":{\"id\":\"b1\"},\"end\":41252,\"start\":41146},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":195776094},\"end\":41674,\"start\":41254},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13926040},\"end\":41936,\"start\":41676},{\"attributes\":{\"id\":\"b4\"},\"end\":42139,\"start\":41938},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219964813},\"end\":42677,\"start\":42141},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":58014111},\"end\":43190,\"start\":42679},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":173188066},\"end\":43680,\"start\":43192},{\"attributes\":{\"id\":\"b8\"},\"end\":43892,\"start\":43682},{\"attributes\":{\"id\":\"b9\"},\"end\":44249,\"start\":43894},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":688013},\"end\":45087,\"start\":44251},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220870974},\"end\":45652,\"start\":45089},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":221105097},\"end\":45923,\"start\":45654},{\"attributes\":{\"id\":\"b13\"},\"end\":46211,\"start\":45925},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14049355},\"end\":46657,\"start\":46213},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4244548},\"end\":47165,\"start\":46659},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6628106},\"end\":47607,\"start\":47167},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8979634},\"end\":48138,\"start\":47609},{\"attributes\":{\"id\":\"b18\"},\"end\":48515,\"start\":48140},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206771220},\"end\":48901,\"start\":48517},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":152282878},\"end\":49382,\"start\":48903},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":102351100},\"end\":49925,\"start\":49384},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7546733},\"end\":50371,\"start\":49927},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16447573},\"end\":51105,\"start\":50373},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13620847},\"end\":51694,\"start\":51107},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6745452},\"end\":52282,\"start\":51696},{\"attributes\":{\"id\":\"b26\"},\"end\":52639,\"start\":52284},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":174802026},\"end\":53229,\"start\":52641},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15928602},\"end\":53762,\"start\":53231},{\"attributes\":{\"id\":\"b29\"},\"end\":54338,\"start\":53764},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18061547},\"end\":55000,\"start\":54340},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12480670},\"end\":55557,\"start\":55002},{\"attributes\":{\"doi\":\"209:1-209:14\",\"id\":\"b32\",\"matched_paper_id\":207984929},\"end\":55879,\"start\":55559},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221245720},\"end\":56427,\"start\":55881},{\"attributes\":{\"id\":\"b34\"},\"end\":56657,\"start\":56429},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5855042},\"end\":56907,\"start\":56659},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":109931479},\"end\":57388,\"start\":56909},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":7893634},\"end\":57901,\"start\":57390},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206594535},\"end\":58403,\"start\":57903},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3337929},\"end\":58805,\"start\":58405},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":68049510},\"end\":59372,\"start\":58807},{\"attributes\":{\"id\":\"b41\"},\"end\":59665,\"start\":59374}]", "bib_title": "[{\"end\":41312,\"start\":41254},{\"end\":41709,\"start\":41676},{\"end\":42213,\"start\":42141},{\"end\":42735,\"start\":42679},{\"end\":43247,\"start\":43192},{\"end\":44324,\"start\":44251},{\"end\":45147,\"start\":45089},{\"end\":45681,\"start\":45654},{\"end\":46266,\"start\":46213},{\"end\":46758,\"start\":46659},{\"end\":47209,\"start\":47167},{\"end\":47629,\"start\":47609},{\"end\":48584,\"start\":48517},{\"end\":48976,\"start\":48903},{\"end\":49434,\"start\":49384},{\"end\":49967,\"start\":49927},{\"end\":50448,\"start\":50373},{\"end\":51161,\"start\":51107},{\"end\":51720,\"start\":51696},{\"end\":52348,\"start\":52284},{\"end\":52714,\"start\":52641},{\"end\":53294,\"start\":53231},{\"end\":53848,\"start\":53764},{\"end\":54416,\"start\":54340},{\"end\":55077,\"start\":55002},{\"end\":55612,\"start\":55559},{\"end\":55936,\"start\":55881},{\"end\":56687,\"start\":56659},{\"end\":56966,\"start\":56909},{\"end\":57456,\"start\":57390},{\"end\":57977,\"start\":57903},{\"end\":58478,\"start\":58405},{\"end\":58894,\"start\":58807}]", "bib_author": "[{\"end\":41331,\"start\":41314},{\"end\":41355,\"start\":41331},{\"end\":41724,\"start\":41711},{\"end\":41741,\"start\":41724},{\"end\":41763,\"start\":41741},{\"end\":42035,\"start\":42020},{\"end\":42225,\"start\":42215},{\"end\":42238,\"start\":42225},{\"end\":42250,\"start\":42238},{\"end\":42263,\"start\":42250},{\"end\":42275,\"start\":42263},{\"end\":42287,\"start\":42275},{\"end\":42746,\"start\":42737},{\"end\":42759,\"start\":42746},{\"end\":42773,\"start\":42759},{\"end\":42784,\"start\":42773},{\"end\":42800,\"start\":42784},{\"end\":43265,\"start\":43249},{\"end\":43283,\"start\":43265},{\"end\":43756,\"start\":43748},{\"end\":43765,\"start\":43756},{\"end\":43963,\"start\":43947},{\"end\":43981,\"start\":43963},{\"end\":43994,\"start\":43981},{\"end\":44010,\"start\":43994},{\"end\":44030,\"start\":44010},{\"end\":44044,\"start\":44030},{\"end\":44062,\"start\":44044},{\"end\":44338,\"start\":44326},{\"end\":44348,\"start\":44338},{\"end\":44362,\"start\":44348},{\"end\":44377,\"start\":44362},{\"end\":44397,\"start\":44377},{\"end\":44408,\"start\":44397},{\"end\":44437,\"start\":44408},{\"end\":44454,\"start\":44437},{\"end\":44469,\"start\":44454},{\"end\":44487,\"start\":44469},{\"end\":44504,\"start\":44487},{\"end\":44520,\"start\":44504},{\"end\":45160,\"start\":45149},{\"end\":45172,\"start\":45160},{\"end\":45182,\"start\":45172},{\"end\":45194,\"start\":45182},{\"end\":45207,\"start\":45194},{\"end\":45219,\"start\":45207},{\"end\":45233,\"start\":45219},{\"end\":45243,\"start\":45233},{\"end\":45692,\"start\":45683},{\"end\":45705,\"start\":45692},{\"end\":45719,\"start\":45705},{\"end\":45749,\"start\":45719},{\"end\":45754,\"start\":45749},{\"end\":46013,\"start\":45997},{\"end\":46047,\"start\":46013},{\"end\":46056,\"start\":46047},{\"end\":46283,\"start\":46268},{\"end\":46310,\"start\":46283},{\"end\":46322,\"start\":46310},{\"end\":46335,\"start\":46322},{\"end\":46349,\"start\":46335},{\"end\":46369,\"start\":46349},{\"end\":46375,\"start\":46369},{\"end\":46777,\"start\":46760},{\"end\":46792,\"start\":46777},{\"end\":46804,\"start\":46792},{\"end\":46827,\"start\":46804},{\"end\":47223,\"start\":47211},{\"end\":47237,\"start\":47223},{\"end\":47241,\"start\":47237},{\"end\":47646,\"start\":47631},{\"end\":47663,\"start\":47646},{\"end\":47685,\"start\":47663},{\"end\":47701,\"start\":47685},{\"end\":48161,\"start\":48140},{\"end\":48173,\"start\":48161},{\"end\":48187,\"start\":48173},{\"end\":48199,\"start\":48187},{\"end\":48216,\"start\":48199},{\"end\":48224,\"start\":48216},{\"end\":48600,\"start\":48586},{\"end\":48613,\"start\":48600},{\"end\":48630,\"start\":48613},{\"end\":48987,\"start\":48978},{\"end\":49003,\"start\":48987},{\"end\":49019,\"start\":49003},{\"end\":49030,\"start\":49019},{\"end\":49044,\"start\":49030},{\"end\":49056,\"start\":49044},{\"end\":49453,\"start\":49436},{\"end\":49468,\"start\":49453},{\"end\":49480,\"start\":49468},{\"end\":49494,\"start\":49480},{\"end\":49515,\"start\":49494},{\"end\":49522,\"start\":49515},{\"end\":49988,\"start\":49969},{\"end\":50003,\"start\":49988},{\"end\":50014,\"start\":50003},{\"end\":50035,\"start\":50014},{\"end\":50049,\"start\":50035},{\"end\":50465,\"start\":50450},{\"end\":50481,\"start\":50465},{\"end\":50491,\"start\":50481},{\"end\":50510,\"start\":50491},{\"end\":50524,\"start\":50510},{\"end\":51179,\"start\":51163},{\"end\":51193,\"start\":51179},{\"end\":51212,\"start\":51193},{\"end\":51740,\"start\":51722},{\"end\":51751,\"start\":51740},{\"end\":51764,\"start\":51751},{\"end\":51779,\"start\":51764},{\"end\":51796,\"start\":51779},{\"end\":51814,\"start\":51796},{\"end\":51834,\"start\":51814},{\"end\":51853,\"start\":51834},{\"end\":52365,\"start\":52350},{\"end\":52384,\"start\":52365},{\"end\":52401,\"start\":52384},{\"end\":52733,\"start\":52716},{\"end\":52745,\"start\":52733},{\"end\":52759,\"start\":52745},{\"end\":53312,\"start\":53296},{\"end\":53321,\"start\":53312},{\"end\":53336,\"start\":53321},{\"end\":53347,\"start\":53336},{\"end\":53859,\"start\":53850},{\"end\":53872,\"start\":53859},{\"end\":53884,\"start\":53872},{\"end\":53896,\"start\":53884},{\"end\":54428,\"start\":54418},{\"end\":54444,\"start\":54428},{\"end\":54460,\"start\":54444},{\"end\":54470,\"start\":54460},{\"end\":54484,\"start\":54470},{\"end\":54500,\"start\":54484},{\"end\":54507,\"start\":54500},{\"end\":55100,\"start\":55079},{\"end\":55116,\"start\":55100},{\"end\":55124,\"start\":55116},{\"end\":55140,\"start\":55124},{\"end\":55632,\"start\":55614},{\"end\":55642,\"start\":55632},{\"end\":55655,\"start\":55642},{\"end\":55666,\"start\":55655},{\"end\":55951,\"start\":55938},{\"end\":55966,\"start\":55951},{\"end\":55983,\"start\":55966},{\"end\":56002,\"start\":55983},{\"end\":56517,\"start\":56499},{\"end\":56532,\"start\":56517},{\"end\":56538,\"start\":56532},{\"end\":56713,\"start\":56689},{\"end\":56730,\"start\":56713},{\"end\":56986,\"start\":56968},{\"end\":57005,\"start\":56986},{\"end\":57470,\"start\":57458},{\"end\":57483,\"start\":57470},{\"end\":57500,\"start\":57483},{\"end\":57517,\"start\":57500},{\"end\":57987,\"start\":57979},{\"end\":57996,\"start\":57987},{\"end\":58006,\"start\":57996},{\"end\":58016,\"start\":58006},{\"end\":58494,\"start\":58480},{\"end\":58512,\"start\":58494},{\"end\":58522,\"start\":58512},{\"end\":58542,\"start\":58522},{\"end\":58553,\"start\":58542},{\"end\":58565,\"start\":58553},{\"end\":58907,\"start\":58896},{\"end\":58925,\"start\":58907},{\"end\":58944,\"start\":58925},{\"end\":58958,\"start\":58944},{\"end\":59440,\"start\":59428},{\"end\":59452,\"start\":59440},{\"end\":59465,\"start\":59452},{\"end\":59475,\"start\":59465},{\"end\":59483,\"start\":59475},{\"end\":59497,\"start\":59483},{\"end\":59507,\"start\":59497}]", "bib_venue": "[{\"end\":41432,\"start\":41409},{\"end\":42372,\"start\":42356},{\"end\":42949,\"start\":42883},{\"end\":43432,\"start\":43360},{\"end\":44608,\"start\":44585},{\"end\":45335,\"start\":45319},{\"end\":47317,\"start\":47299},{\"end\":47789,\"start\":47774},{\"end\":50112,\"start\":50096},{\"end\":50676,\"start\":50643},{\"end\":51408,\"start\":51305},{\"end\":51935,\"start\":51918},{\"end\":52439,\"start\":52424},{\"end\":52936,\"start\":52847},{\"end\":53441,\"start\":53423},{\"end\":53986,\"start\":53967},{\"end\":54587,\"start\":54561},{\"end\":55230,\"start\":55216},{\"end\":56067,\"start\":56056},{\"end\":57084,\"start\":57065},{\"end\":57592,\"start\":57577},{\"end\":58099,\"start\":58081},{\"end\":41077,\"start\":41031},{\"end\":41167,\"start\":41146},{\"end\":41407,\"start\":41355},{\"end\":41791,\"start\":41763},{\"end\":42018,\"start\":41938},{\"end\":42354,\"start\":42287},{\"end\":42881,\"start\":42800},{\"end\":43358,\"start\":43283},{\"end\":43746,\"start\":43682},{\"end\":43945,\"start\":43894},{\"end\":44583,\"start\":44520},{\"end\":45317,\"start\":45243},{\"end\":45770,\"start\":45754},{\"end\":45995,\"start\":45925},{\"end\":46414,\"start\":46375},{\"end\":46889,\"start\":46827},{\"end\":47297,\"start\":47241},{\"end\":47772,\"start\":47701},{\"end\":48315,\"start\":48224},{\"end\":48692,\"start\":48630},{\"end\":49118,\"start\":49056},{\"end\":49612,\"start\":49522},{\"end\":50094,\"start\":50049},{\"end\":50641,\"start\":50524},{\"end\":51303,\"start\":51212},{\"end\":51916,\"start\":51853},{\"end\":52422,\"start\":52401},{\"end\":52845,\"start\":52759},{\"end\":53421,\"start\":53347},{\"end\":53965,\"start\":53896},{\"end\":54559,\"start\":54507},{\"end\":55214,\"start\":55140},{\"end\":55706,\"start\":55678},{\"end\":56054,\"start\":56002},{\"end\":56497,\"start\":56429},{\"end\":56766,\"start\":56730},{\"end\":57063,\"start\":57005},{\"end\":57575,\"start\":57517},{\"end\":58079,\"start\":58016},{\"end\":58584,\"start\":58565},{\"end\":59048,\"start\":58958},{\"end\":59426,\"start\":59374}]"}}}, "year": 2023, "month": 12, "day": 17}