{"id": 1954789, "updated": "2023-11-11 02:42:17.573", "metadata": {"title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing", "authors": "[{\"first\":\"Denis\",\"last\":\"Kleyko\",\"middle\":[]},{\"first\":\"Evgeny\",\"last\":\"Osipov\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Senior\",\"middle\":[]},{\"first\":\"Asad\",\"last\":\"Khan\",\"middle\":[\"I.\"]},{\"first\":\"Y.\",\"last\":\"cSekerciouglu\",\"middle\":[\"Ahmet\"]}]", "venue": "IEEE transactions on neural networks and learning systems", "journal": "IEEE transactions on neural networks and learning systems", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2950268605", "acl": null, "pubmed": "26978836", "pubmedcentral": null, "dblp": "journals/corr/KleykoOSKS15", "doi": "10.1109/tnnls.2016.2535338"}}, "content": {"source": {"pdf_hash": "4498442aeef7f06349e05b4eb0bf662f72e2a1f1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1501.03784v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1501.03784", "status": "GREEN"}}, "grobid": {"id": "3759e866018ee2d6830e3e55c903b798b6ad0884", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4498442aeef7f06349e05b4eb0bf662f72e2a1f1.txt", "contents": "\nHolographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing\n\n\nDenis Kleyko \nEvgeny Osipov \nAlexander Senior \nAsad I Khan \nY Ahmet\u015fekercioglu \nHolographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing\n1Index Terms-Holographic Graph NeuronPattern recogni- tionVector Symbolic ArchitectureAssociative memoryhyper- dimensional computing\nThis article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.\n\nI. INTRODUCTION\n\nG RAPH Neuron (GN) is an approach for memorizing patterns of generic sensor stimuli for later template matching. It is based on the hypothesis that a better associative memory resource can be created by changing the emphasis from high speed sequential CPU processing to parallel network-centric processing [2], [3]. In contrast to contemporary machine learning approaches, GN allows introduction of new patterns in the learning set without the need for retraining. Whilst doing so, it exhibits a high level of scalability i.e. its performance and accuracy do not degrade as the number of stored patterns increases over time.\n\nVector Symbolic Architectures (VSA) [4] are a bio-inspired method of representing concepts and their meaning for modeling cognitive reasoning. It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation. In the biological world, extremely successful applications of these approaches can be found. One example is the ordinary house fly. A house fly is capable of conducting very complex maneuvers albeit it possesses very little computational capacity 1 . Another interesting biological example is the compound eyes of arthropods. These compound eyes consist of large number of sensors with limited and localized processing capabilities for performing relatively complex sensing tasks [9].\n\nThis article presents contributions in two domains: organization of associative memory and properties of connectionist distributed representation. In the first area the article introduces a novel bio-inspired architecture, called Holographic Graph Neuron (HoloGN), for one-shot pattern learning, which is build upon Graph Neuron's flexible input encoding abstraction and strong reasoning capabilities of the VSA representation. In the second area this article extends understanding of the performance proprieties of distributed representation, which opens a way for new applications.\n\nThe article is structured as follows. Section II presents an overview of the work related to the matter presented in this article. The background information on the theories, concepts and approaches used in HoloGN is described in Section III. Sections IV through VI present the main contribution of this article -the design of the HoloGN architecture and its performance characteristics. The conclusions are presented in Section VII.\n\n\nII. RELATED WORK\n\nAssociative memory (AM) is a sub-domain of artificial neural networks, which utilises the benefits of content-addressable memory (CAM) [10] in microcomputers. The AM concept was originally developed in an effort to utilise the power and speed of existing computer systems for solving large-scale and computationally intensive problems by simulating biological neurosystems.\n\nThe Hierarchical Graph Neuron (HGN) approach [2] is a type of associative memory which signifies the hierarchical structure in its implementation. Hierarchical structures in associative memory models are of interest as these have been shown to improve the rate of recall in pattern recognition applications. The distributed HGN scheme also allows for better control of the network resources. This scheme compares well with contemporary approaches such as Self-Organizing Map and Support Vector Machine in terms of speed and accuracy.\n\nVector Symbolic Architectures [5] are a class of connectionist models that use hyper-dimensional vectors (i.e. vectors of several thousand elements) to encode structured information as distributed or holographic representation. In this technique structured data is represented by performing basic arithmetic operations on field-value tuples. Distributed representations of data structures are an approach actively used in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [11].\n\nIn [12] a VSA-based knowledge-representation architecture is proposed for learning arbitrarily complex, hierarchical, symbolic relationships (patterns) between sensors and actuators in robotics. Recently the theory of hyper-dimensional computing, and VSA in particular, were adopted for implementing novel communication protocols and architectures for collective communications in machine-to-machine communication scenarios [13], [14]. The first work demonstrates unique reliability and timing properties essential in the context of industrial machineto-machine communications. The latter work shows the feasibility of implementing collective communications using current radio technology. This article presents an algorithmic ground for further design of the distributed HoloGN on top of the architecture presented in [13].\n\n\nIII. OVERVIEW OF ESSENTIAL PARTS OF RELEVANT CONCEPTS AND THEORIES\n\nA. Hierarchical Graph Neuron Figure 1 illustrates the Hierarchical Graph Neuron approach. Consider only the bottom layer of the construction without the hierarchy of upper nodes; this bottom level is the original flat network of Graph Neurons [2]. Each GN is a model for a set of generic sensory values. When seen as a network, graph neurons can be modeled by an array where columns are individual GNs and rows are possible symbols, which a neuron can recognize. For example, if there are only two possible symbols, say \"X\" and \"Y\", in the alphabet of a pattern, then only two rows are needed to represent those symbols. The number of columns 2 in the GN array determines the size of patterns, which it can analyse.\n\nAn input pattern is defined as a stimulus produced within the network. In Figure 1, each GN can analyse a symbol (\"X\" or \"Y\") of a pattern comprising of five elements. In each GN (column) only the element with matching value (a row ID) would respond. For example, if the pattern is \"YXYYX\", then in the second column \"X\" element will be activated as response to this input stimulus. If a particular element in the GN-column is activated it sends a report to all adjacent GNs. The report contains the activated GN's element ID (the row index). Otherwise, it simply ignores the stimulus and returns to the idle state.\n\nDuring the next phase, all GNs communicate the indices of the activated elements with the adjacent columns at their level, and additionally communicate the stored bias information to the layer above. The procedure continues in an ascending manner until the collective bias information reaches the top of the hierarchy. The higher level GNs can thus provide a more authoritative assessment of the input pattern. The accuracy of HGN was demonstrated to be comparable to the accuracy of Neural Network with back-propagation [2].\n\n\nB. Fundamentals of Vector Symbolic Architecture and Binary Spatter Codes\n\nVector Symbolic Architecture is an approach for encoding and operations on distributed representation of information. VSA has previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [15].\n\nThe fundamental difference between distributed and localist representations of data is as follows: in traditional (localist) computing architectures each bit and its position within a structure of bits are significant (for example, a field in a database has a predefined offset amongst other fields and a symbolic value has unique representation in ASCII codes); whereas in a distributed representation all entities are represented by binary random vectors of very high dimension. These representations are also called Binary Spatter Codes (BSC), though for the remainder of the article we use term HD-vector when referring to BSC codes, for reasons of brevity. High dimensionality refers to that fact that in HDvectors, several thousand positions (of binary numbers) are used for representing a single entity; [4] proposes the use of vectors of 10000 binary elements. Such entities have the following useful properties: 1) Randomness: Randomness means that the values on each position of an HD-vector are independent of each other, and \"0\" and \"1\" components are equally probable. In very high dimensions, the distances from any arbitrary chosen HD-vector to more than 99.99 % of all other vectors in the representation space are concentrated around 0.5 Hamming distance. Interested readers are referred to [4] and [16] for comprehensive analysis of probabilistic properties of the hyperdimensional representation space.\nPr(k, d, p) = d k p k (1 \u2212 p) d\u2212k .(1)\nWhen d is in the range of several thousand binary elements, the calculation of the binomial coefficient requires sophisticated computations. The following equations use an approximation of the binomial distribution, via the de Moivre-Laplace theorem:\nPr(k, d, p) \u2248 e \u2212(k\u2212dp) 2 2dp(1\u2212p) 2\u03c0dp(1 \u2212 p)(2)\n2) Similarity metric: A similarity between two binary representation is characterized by Hamming distance, which (for two vectors) measures the number of positions in which they differ:\n\u2206 H (A, B) = A\u2297B 1 d = d\u22121 i=0 ai\u2297bi d ,\nwhere a i , b i are bits on positions i in vectors A and B of dimension d, and where \u2297 denotes the bit-wise XOR operation.\n\n3) Generation of HD-vectors: Random binary vectors with the above properties can be generated with Zadoff-Chu sequences [17], a method widely used in telecommunications to generate pseudo-orthogonal preambles. Using this approach a sequence of K vectors, which are pseudo-orthogonal to a given initial random HD-vector A (i.e. Hamming distance between them approximately equals 0.5) are obtained by cyclically shifting A by i positions, where 1 < i \u2264 K \u2264 d. Further in the article this operation is denoted as Sh(A, i). The cyclic shift operation has the following properties: \n\u2022 it is invertible, i.e. if B = Sh(A, i) then A = Sh(B, \u2212i); \u2022 it is associative in the sense that Sh(B, i + j) = Sh(Sh(B, i), j) = Sh(Sh(B, j), i); \u2022 itB\u2297Sh(B,i) 1 d \u2248 0.5.\nNote that the cyclic shift is a special case of the permutation operation [4]. In the context of VSA, permutations were previously used to encode sequences of semantically bound elements.\n\n\n4) Bundling of vectors:\n\nJoining several entities into one structure is done by the bundling operation; it is implemented by a thresholded sum of the HD-vectors representing the entities. A bit-wise thresholded sum of n vectors results in 0 when n/2 or more arguments are 0, and 1 otherwise. Furthermore terms \"thresholded sum\" and \"majority sum\" are used interchangeably and denoted as [A+B +C]. The relevant properties of the majority sum are:\n\n\u2022 the result is a random vector, i.e. the number of '1' components is approximately equal to the number of '0' components; \u2022 the result is similar to all vectors included in the sum;\n\n\u2022 the number of vectors involved into MAJORITY sum must be odd; \u2022 the more HD-vectors that are involved in a majority operation, the closer the Hamming distance between the resultant vector and any HD-vector component is to 0.5; and \u2022 if several copies of any vector are included into a majority sum, the resultant vector is closer to the dominating vector than to other components.\n\nThe algebra on VSA includes other operations e.g., binding, permutation [4]. Since we do not use them in this article, we omit their definitions and properties.\n\n\nIV. HOLOGRAPHIC GRAPH NEURON\n\nThis section presents one of the main contributions of this paper: the adoption of the VSA data representation for implementation of the HGN approach.\n\n\nA. Motivation for Holographic Graph Neuron and outline of the solution\n\nAn important issue in hierarchical models is the overhead of resource requirements, specifically with regards to the number of processing elements required. We propose a holographic approach, which enables a flat GN array to operate with higher level of accuracy and comparable recall time than that of HGN, without the need for a complex topology and additional nodes. The high-level logic of the proposed solution is illustrated in Figure 2. Essentially, we replace the need for maintaining topological relationships by compacting parts of the pattern observed by GNs into one holographic representation.\n\n\nB. Encoding\n\nIn the case of HoloGN, all its elements (i.e. symbols of individual neurons) are indexed uniquely and the index of a particular element is derived as a function of the GN's ID. Let IV j be an initialization high-dimensional vector for GN j. The initialized vectors for different GNs are chosen to be mutually orthogonal. Then the HD-index of element i in GN j is computed as E HD (j,i) = Sh(IV j , i), where Sh() is a cyclic shift operation resulting in the generation of a vector orthogonal to IV j HD-vector [17], [18].\n\n\nC. Construction of VSA-representation of activated GNs\n\nLet n be the number of individual Graph Neurons. When a GN array is exposed to a pattern, the activated elements communicate their HD-represented indices to all other GNs; the holographic representation of the activated elements is then:\nHGN = [ n j=1 (E HD j )],(3)\nwhere E HD j is the HD-index of the activated element in textGN j , and the addition operation is the bundling operation, or thresholded sum as described in III-B. As discussed previously, in the resultant HD-vector the Hamming distances between each component and the composition vector is strictly less than 0.5. We will utilize this property later when constructing data structures for recall of patterns in HoloGN.\n\n\nD. Data structures for storing and retrieving holographic representations in HGN elements\n\nHoloGN will store the holographic representation of the entire pattern (3) observed across all GNs. A possible architecture is for all memorized patterns to be collected and stored centrally at a processing node.\n\nDepending on the particular application of HoloGN, the memorized patterns could be stored either in an unsorted list or in bundles. The first mode of storing HoloGN patterns corresponds to the case where the structure of the observed patterns is unknown; the latter mode is used in the case of a supervised learning. The next section describes different HoloGN usage and recall strategies.\n\n\nV. HOLOGN RECALL STRATEGIES\n\nThis section introduces and evaluates the performance of two major recall modes: the one-shot case and the case of supervised learning.\n\nA. Time-efficient \u03be-accurate recall in an unsorted HoloGN storage\n\nThe common steps in both recall modes is the procedure for the time-efficient search over an (unsorted) list of HoloGN records. Recall that all manipulations with VSAencoded entities are done using simple bit-wise arithmetic operations as well, and calculations to obtain the Hamming distance between entities. However, for this article we assume no particular optimized implementation of VSA's bit-wise operations; this is because such operations are tailored to the architectures of specific microprocessors, which operate with words of substantially lower dimensionalities (typically 32 or 64 bits). Therefore, adopting these methods for implementing the bit-wise operations on words of thousands of bits would be cumbersome. Instead, an easily analyzable computational model is adopted in this article, which could also be adapted to an implementation on specialized computing architectures.\n\nIn what follows, each HoloGN pattern h i is modelled as a row vector of d elements. The list of stored HoloGN patterns is therefore modelled as an l \u00d7d matrix, where l is the number of the learned (stored) HoloGN patterns. Denote this matrix as H. The task of recalling a pattern with a target accuracy of \u03be (\u03be < 0.5) is formulated as finding the rows h i in H with Hamming distance to the query pattern h q less than or equal to \u03be.\n\nThe conventional way of computing Hamming distance between vectors would be to perform the following sequence of computations for each row in H: 1) perform an elementwise XOR with the vector query; 2) sum up all elements in the intermediate result; and then 3) divide the result by the dimensionality of the vectors. The performance of the two implementations of this method using Matlab's repmat and bxf un functions are shown by the top two curves in Figure 3. The curves demonstrate linear but rapid increase in the recall time with the increase in the number of the stored patterns. The lowest curve in the figure shows the performance of matrixvector multiplication of the same size, which is chosen as the reference case. The results were obtained on a Intel Core i7-3520M 2.9 GHz machine with Windows 7 operating system using one processor.\n\n1) Binary spatter codes as complex numbers: In order to improve the efficiency of calculating Hamming distances over a vast number of HoloGN patterns, we propose representing HoloGN patterns using complex numbers, where a binary 0 would be represented by \u221a \u22121 (i.e. the complex number) and a binary 1 would remain 1. The intuition behind this transformation is simple: multiplication of bits in the same position should produce three outcomes: \u22121 = j \u00d7j, 1 = 1\u00d71 and j = 1 \u00d7 j. That is, the multiplication of two similar bits would produce a real number and the multiplication of two different bits produces a imaginary number. In this way the sum of the imaginary parts over all positions in the resulting vector, divided by dimensionality d, will correspond to the Hamming distance between the two vectors. Thus, the suggested method allows us to implement the calculation of Hamming distance through the standard method of matrixvector multiplication, as illustrated below:\nH \u00d7 hq = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed \u221a \u22121 1 \u00b7 \u00b7 \u00b7 \u221a \u22121 1 1 \u00b7 \u00b7 \u00b7 \u221a \u22121 . . . . . . . . . . . . \u221a \u22121 \u221a \u22121 \u00b7 \u00b7 \u00b7 1 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u00d7 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed \u221a \u22121 1 . . . \u221a \u22121 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 = = \uf8eb \uf8ec \uf8ec \uf8ed 254 + 1633j 617 + 3824j . . . 548 + 4952j \uf8f6 \uf8f7 \uf8f7 \uf8f8 .(4)\nThe performance of the proposed method is illustrated by the red dashed curve in Figure 3. It demonstrates that the calculation of the Hamming distance is only two times slower than the usual matrix-vector multiplication. Specifically, to compute Hamming distance from the target vector to each of 20 000 stored patterns takes approximately 200 ms on the test machine. Further optimization of the matrix multiplication and execution on parallel architectures hint on realistic bounds on the recall time over extremely large numbers of patterns. \n\n\nB. Case-study 1: Best-match probing under one-shot learning\n\nThe first usage of HoloGN is probing the existence of the target query pattern amongst the previously memorized patterns. The perfect match in this case would be indicated by a Hamming distance of zero. The deviation from zero, therefore, reflects the degree of proximity of the query to one or several stored HoloGN patterns. In the following example the accuracy of the HoloGN recall was compared to the performance of the original HGN approach. For the sake of fair comparison the 7 by 5 pixels letters of the Roman alphabet (as in [2]) were used. In the memorizing phase a set of noise-free images of letters illustrated in Figure 4 was presented to both architectures. In the recall phase images of the same letters distorted with different levels of random distortions (between 1 bit corresponding to a distortion of 2.9% of the pattern's size and 5 bits equivalent to 14.3% distortion) were presented to the architectures for the recall. An example of a noisy input is presented in Figure 5. In the case of HoloGN the pattern with the lowest Hamming distance to the presented distorted pattern was returned as the output. Figure 6 presents the results of the accuracy comparison between the recall results for the HoloGN approach and the reference HGN architecture. To obtain the results 1000 distorted images of each letter for every level of distortion were presented for recall. The charts show the percentage of the correct recall output. The analysis shows that the performance of the HoloGN-based associative memory at least matches that of the original approach. In most of cases HoloGN appears to be more accurate when recalling patterns with high level of distortion.\n\n\nC. Case-study 2: HoloGN recall under the supervised learning\n\nThe analysis presented above is a very positive result for the proposed bio-inspired associative memory based pattern processing architecture, since the accuracy of the original HGN approach was demonstrated to be as accurate as artificial neural networks with back-propagation [2]. While establishing formal relationships to the framework of artificial neural networksis outside the scope of this work, this section presents the results of the pattern recognition accuracy of the HoloGN architecture under the supervised learning. In this case the HoloGN is presented with a series of randomly distorted patterns for each letter with different level of distortion (between 3% and 43%) as exemplified in Figure 5. In the experiments up to 500 patterns for each letter and every level of distortion were presented for memorizing. For the particular level of distortion i all e presented patterns of the particular letter L i were bundled to a single HoloGN representation as\nh(L) = [ e i=1 (HGN(L i ))].(5)\nThus by the end of the learning phase the HoloGN list will contain 26 high dimensional bundles, each jointly representing all (presented) distorted variants of the particular letter. In the recall phase for each distortion level HoloGN was presented with 500 new distorted patterns of each letter. The accuracy of the recall was measured as the percentage of the correctly recognized letters averaged over the alphabet. Figure 7 illustrates the obtained results: 90% accurate recall was observed when learning symbols distorted up to 15%. While the accuracy predictably decreases rapidly with the increase of distortion in the presented patterns, a reasonable 80% recall accuracy was observed for learning sets with 7 bits distortion (20%). Figure 8 illustrates the convergence of the HoloGN recall accuracy with the number of presented noisy samples for the case of 14.3% distortion (5 bits). This is a definitely positive result for the presented architecture, which illustrates the suitability of the HoloGN in applications requiring supervised learning.  \n\n\nVI. PATTERN DECODING AND SUBPATTERN-BASED\n\n\nANALYSIS\n\nThere is a class of pattern recognition applications which requires an understanding of the details of the recall results. For example, when a recall returns several possible patterns of given recall accuracy, the task would be to understand the overlapping elements. This section considers two aspects of this task: a robust decoding of elementary components out of a distributed VSA representation; and a quantitative metric of the similarity via direct comparison of distributed representations, without the need for decoding those representations.\n\nThe VSA approach of representing data structures by definition makes decoding of the individual components a tedious task, requiring a brute force test on the inclusion of all possible high-dimensional codewords for each GN. The majority sum, which is used for creating HoloGN representations of the observed patterns imposes a limit on the number high-dimensional codewords operands, above which a robust decoding of the individual operands is impossible.\n\n\nA. Preliminaries\n\nDenote the density of a randomly generated HD-vector (i.e. the number of ones in a HD-vector) as k. The probability of picking a random vector of length d with density k, where the probability of 1's appearance, defined as p, is described by (2). The mean density of a random vector is equal to d\u00b7p. Note that in reality the density of randomly generated HD-vectors will obviously deviate from the mean value. However, according to (1) density k is approaches the mean value with the increase of dimensionality d. In other, words the probability of generating HD-vector with k >> d \u00b7 p or k << d \u00b7 p decreases with the increase of dimensionality d. Define thr as the threshold probability of generating a vector with a certain deviation of density being negligibly small. Let k \u2212 and k + characterize the lower and the upper bounds of the interval of possible densities. This is illustrated in Figure 9. The bounds for a given d, p and thr are calculated using (2). The bounds are calculated according to (6) and (7). The value of threshold is chosen to be small (10 \u22126 ).\nk \u2212 (d, p, thr) = max k (Pr(k, d, p) <= thr|k < (d \u00b7 p)) (6)\nk + (d, p, thr) = min k (Pr(k, d, p) <= thr|k > (d \u00b7 p)) (7)\n\n\nB. Capacity of HoloGN representations\n\nSuppose there exists an item memory [4] containing HDvectors representing atomic concepts 3 . Recall that when several HD-vectors are bundled by the majority sum the noise of flipped bits increases with the number of components. For a given dimensionality d there is a limit on the number of bundled HD-vectors beyond which the resulting HD-vector becomes orthogonal to every component vector; hence the A capacity of the resulting vector is defined as the maximal number of mutually orthogonal HD-vectors which can be robustly decoded from their majority sum composition by probing the item memory.\n\nIn order to characterize the capacity of the composition vector for a given dimensionality, one needs to characterize the level of noise p n introduced by the bundling operation. This is calculated as in [19] by (8), where n is a number of atomic vectors in the resulting majority sum vector.\np n (n) = 1 2 \u2212 n \u2212 1 0.5 \u00b7 (n \u2212 1) 2 n .(8)\nConsider an arbitrary HD-vector A to be decoded from a majority sum composition. Let N be a vector of noise imposed by the majority sum operation. Since the components 3 In the case of HoloGN an atomic concept is the code for the particular HoloGN element. are mutually orthogonal, the density of ones in the noise vector is also described the binomial distribution Pr(k, d, p n ). As each new vector is added the level of noise increases, hence the mean of the noise vector density will approach 0.5 as illustrated in Figure 11. Due to the properties of high dimensional space, vector A will be undecodable when the upper bound k + (d, p n , thr) of the density of noise vector N approaches the lower bound k \u2212 (d, 0.5, thr). That is, the noisy version of A becomes orthogonal to its clean version. This logic is illustrated in Figure 10, where the resulting majority sum vector is orthogonal to all components. Thus the capacity of the distributed representation with dimensionality d is computed by:\n\nCapacity(d, thr) = = max n (k + (d, p n (n), thr) \u2264 k \u2212 (d, 0.5, thr)) (9) Fig. 11. Capacity of the component vector versus the dimensionality. The calculations use a the threshold of thr = 10 \u22126 Figure 11 presents the capacity of HD-vector of different dimensionalities calculated for threshold probability thr = 10 \u22126 . Specifically for d = 10000 bits the capacity of the robustly decodable VSA is 89 vectors.\n\n\nC. Calculation of the number of common component vectors in two resulting vectors\n\nGiven the rather conservative limits on the number of robustly decodable elements in a distributed representation it is important that the proposed HoloGN architecture can estimate the similarity between different patterns without decoding them. This subsection provides a method for the quantitatively measuring the number of overlapping elements as a function of their relative Hamming distance. Denote m and n as lengths of two patterns, m <= n, and denote c as the number of common elements in these patterns. Let M be a c \u00d7 d matrix of common elements where each row contains a random HDvector of dimension d encoding element c i . Denote an arbitrary column of matrix M as C. Since rows in M are independent, the density of ones in each column also follows the binomial distribution with p = 0.5 and length c. Denote the number of ones in column C as ||C|| 1 .\n\nIn order to calculate the Hamming distance between the distributed representations of two patterns with known m, n and c, consider all possible cases when bits in the same position are different. The Hamming distance between two patterns can be estimated with equation (10):\n\u2206 H = p(c, m, n) = c ||C||1=0 c ||C||1 2 c \u00b7 (p 1 (m, c, ||C|| 1 )\u00b7\n\u00b7p 0 (n, c, ||C|| 1 )+p 0 (m, c, ||C|| 1 ) \u00b7 p 1 (n, c, ||C|| 1 )); (10) where p i (j, c, ||C|| 1 ) stands for the probability of having i (0 or 1), when the representation consists of j = m or j = n atomic vectors and c of these vectors are overlapped.\n\nDue to the symmetry in the calculation of probabilities, p i (j, c, ||C|| 1 ) is presented only for the case of p 1 (m, c, ||C|| 1 ). There are three possible cases for calculation of p 1 (m, c, ||C|| 1 ):\n\n\u2022 if ||C|| 1 is more than m/2, then the result of the majority sum is '1', i.e. p 1 is 1; \u2022 if number of possible '1's is less than m/2, then probability of p 1 is 0; \u2022 otherwise the probability should take into account all possible combinations, and their probabilities. Equation (11) specifies probability for p 1 (m, c, ||C|| 1 ), and equation (12) does the same for p 0 (m, c, ||C|| 1 ).\np 1 (m, c, ||C|| 1 ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1, when ||C|| 1 > m 2 0, when (m \u2212 c) < m+1 2 \u2212 ||C|| 1 (m\u2212c) i=( m+1 2 \u2212||C||1) m \u2212 c i 2 (m\u2212c) , otherwise(11)\np 0 (m, c, Figure 12 shows the Hamming distances between two resulting vectors for different numbers of overlapping vectors. The results show that the larger the number of common elements, the smaller the Hamming distance between resulting vectors.\n||C|| 1 ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1, when (c \u2212 ||C|| 1 ) > m 2 0, when (m \u2212 c) < m+1 2 \u2212 (c \u2212 ||C|| 1 ) (m\u2212c) i=( m+1 2 \u2212(c\u2212||C||1)) m \u2212 c i 2 (m\u2212c) , otherwise(12)\nThis method opens a way towards constructing and analyzing patterns far beyond VSA's robustly decodable capacity. The problem with practical application of this method, however, comes with the rapid convergence of the Hamming distance indicator to 0.5, making the difference between analyzable patterns indistinguishable as illustrated in Figure 12. For example, for HoloGN representations of patterns with 15 elements, sub-patterns of 3 overlapped elements are robustly detected, while patterns with fewer overlapped elements are indistinguishable. Thus, the minimal number of overlapped elements in two patterns which can be robustly detected using the Hamming distance indicator is called bundle's sensitivity.\n\nThe analysis of the sensitivity is similar to the analysis of the capacity of VSA representation in section VI-B. For two patterns of length m and n elements, and c overlapped components, the sensitivity is calculated by (13): Sensitivity(d, thr, m, n) = min c (k + (d, p(c, n, m), thr) <= k \u2212 (d, 0.5, thr)) (13) Figure 13 demonstrates the development of the sensitivity threshold with the number of elements in the compared patterns. The results show that the number of components for robust detection grows linearly with the size of pattern. Patterns with more than 500 elements should contain at least 14 % overlapped elements to be robustly detected by the proposed method.\n\n\nVII. CONCLUSION\n\nThis article presented Holographic Graph Neuron -a novel approach for memorizing patterns of generic sensor stimuli. HoloGN is built upon the previous Graph Neuron algorithm and adopts a Vector Symbolic representation for encoding of the Graph Neuron's states. The adoption of the Vector Symbolic representation ensures a one-layered design for the approach, which implies the computational simplicity of the operations. The presented approach possesses the number of uniques properties. Prior to all, it enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern. While maintaining the previously reported properties of Hierarchical Graph Neuron, HoloGN is also improving the noise resistance of the architecture by this substantially improving the accuracy of pattern recall.\n\nFig. 1 .\n1Hierarchical GNs with five elements' pattern of two symbols. The short arrows in the figure show how GNs communicate indices of the activated GN-elements with their neighbours creating logical connectivity. Null GN numbers are assigned at the start and at the end of the array for maintaining a consistent reporting scheme where every activated GN reports to both the adjacent GNs.\n\nFig. 2 .\n2A High level illustration of the proposed solution: representation of the Hierarchical Graph Neuron using Vector Symbolic Architecture.\n\nFig. 3 .\n3A comparison of the different approaches to recalling patterns, showing the time taken to calculate the Hamming distance against the number of previously presented patterns.\n\nFig. 4 .\n4List of letters images for comparison.\n\nFig. 5 .\n5An example of presented for recall images distorted by 14.3%.\n\n6 Fig. 6 .\n66Results from testing black and white images of letters using recall patterns with distortions ranging from 2.9 % to 14.3 % .\n\nFig. 7 .\n7Average accuracy of HoloGN recall under the supervised learning memorization as a function of the distortion level.\n\nFig. 8 .\n8Accuracy of HoloGN under the supervised learning memorization as a function of the number of presented examples for a given level of distortion.\n\nFig. 9 .\n9Binomial distribution and its parameters describing HD-vectors.\n\nFig. 10 .\n10Explanation of vector's capacity. Solid line represents random HDvector. Dashed line corresponds to noise introduced by majority sum.\n\nFig. 12 .\n12The Hamming distance between two resulting vectors against number of components in common. The number of atomic vectors is the same, m = n.\n\nFig. 13 .\n13Minimal number of common components, which is sensible between two patterns of the same size against the size of patterns, d = 10000, thr = 10 \u22126 .\n\n\npreserves Hamming weight of the result: B 1 = Sh(B, i) 1 ; and \u2022 the result is dissimilar to the vector being shifted:\nIn[8], a house fly's properties are compared and contrasted with an advanced fighter plane as follows: \"Whereas the F-35 Joint Strike Fighter, the most advanced fighter plane in the world, takes a few measurements -airspeed, rate of climb, rotations, and so on and then plugs them into complex equations, which it must solve in real time, the fly relies on many measurements from a variety of sensors but does relatively little computation.\"\nIn this articles words \"column\" and GN are used interchangeably and refer to a single Graph Neuron. Term \"GN array\" refers to several GNs used to recognize a pattern of several elements, where one neuron is used to recognize one element of the pattern.\nDenote the density of a randomly generated HD-vector (i.e. the number of ones in a HD-vector) as k. The probability of picking a random vector of length d with density k, where the probability of 1's appearance equals p is described by described by the binomial distribution (1):\n\nHolographic graph neuron. E Osipov, A I Khan, A Anang, Computer and Information Sciences (ICCOINS), 2014 International Conference on. IEEEE. Osipov, A. I. Khan, and A. Anang. Holographic graph neuron. In Computer and Information Sciences (ICCOINS), 2014 International Conference on, pages 1-6. IEEE, 2014.\n\nA hierarchical graph neuron scheme for real-time pattern recognition. B B Nasution, A I Khan, IEEE Transactions on Neural Networks. 192B. B. Nasution and A. I. Khan. A hierarchical graph neuron scheme for real-time pattern recognition. IEEE Transactions on Neural Networks, 19(2):212-229, February 2008.\n\nOne-shot associative memory method for distorted pattern recognition. A I Khan, A H Muhamad Amin, AI 2007: Advances in Artificial Intelligence, 20th Australian Joint Conference on Artificial Intelligence. Gold Coast, Australia4830A.I. Khan and A.H. Muhamad Amin. One-shot associative memory method for distorted pattern recognition. In AI 2007: Advances in Artificial Intelligence, 20th Australian Joint Conference on Artificial Intelligence, Gold Coast, Australia, volume 4830, pages 705-709, 2007.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12SpringerP. Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation, Springer, 1(2):139-159, October 2009.\n\nVector symbolic architectures: A new building material for artificial general intelligence. S D Levy, R Gayler, Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference. the 2008 Conference on Artificial General Intelligence 2008: the First AGI ConferenceAmsterdam, The Netherlands, The NetherlandsIOS PressS.D. Levy and R. Gayler. Vector symbolic architectures: A new building material for artificial general intelligence. In Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference, pages 414-418, Amsterdam, The Netherlands, The Netherlands, 2008. IOS Press.\n\nHolographic reduced representations. T Plate, IEEE Transactions on Neural Networks. 63T. Plate. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623-641, 1995.\n\nRepresenting objects, relations, and sequences. S I Gallant, T W Okaywe, Neural Computation. 258S. I. Gallant and T. W. Okaywe. Representing objects, relations, and sequences. Neural Computation, 25(8):2038-2078, 2013.\n\nFly like a fly. R Zbikowski, IEEE Spectrum. 4211R. Zbikowski. Fly like a fly. IEEE Spectrum, 42(11):46-51, November 2005.\n\nDigital cameras with designs inspired by the arthropod eye. Y M Song, Y Xie, V Malyarchuk, J Xiao, I Jung, K-J Choi, Z Liu, H Park, C Lu, R-H Kim, R Li, K B Crozier, Y Huang, J A Rogers, Nature. 4977447Y. M. Song, Y. Xie, V. Malyarchuk, J. Xiao, I. Jung, K-J. Choi, Z. Liu, H. Park, C. Lu, R-H. Kim, R. Li, K. B. Crozier, Y. Huang, and J. A. Rogers. Digital cameras with designs inspired by the arthropod eye. Nature, 497(7447):95-99, May 2013.\n\nK J Schultz, Content Addressable Memory. N. T. Limited. K. J. Schultz. Content Addressable Memory. N. T. Limited, 1999.\n\nA family of binary spatter codes. P Kanerva, ICANN '95: International Conference on Artificial Neural Networks. P. Kanerva. A family of binary spatter codes. In ICANN '95: International Conference on Artificial Neural Networks, pages 517-522, 1995.\n\nLearning behavior hierarchies via high-dimensional sensor projection. S S Bajracharya, R Levy, Gayler, S. Bajracharya S. Levy and R. Gayler. Learning behavior hierarchies via high-dimensional sensor projection. 2013.\n\nDependable mac layer architecture based on holographic data representation using hyperdimensional binary spatter codes. D Kleyko, N Lyamin, E Osipov, L Riliskis, Multiple Access Communications : 5th International Workshop, MACOM 2012. SpringerD. Kleyko, N. Lyamin, E. Osipov, and L. Riliskis. Dependable mac layer architecture based on holographic data representation using hyper- dimensional binary spatter codes. In Multiple Access Communications : 5th International Workshop, MACOM 2012, pages 134-145. Springer, 2012.\n\nCollective communication for dense sensing environments. P Jakimovski, H R Schmidtke, S Sigg, L Weiss-Ferreira-Chaves, M Beigl, Journal of Ambient Intelligence and Smart Environments (JAISE). 42P. Jakimovski, H.R. Schmidtke, S. Sigg, L. Weiss-Ferreira-Chaves, and M. Beigl. Collective communication for dense sensing environments. Journal of Ambient Intelligence and Smart Environments (JAISE), 4(2):123-134, March 2012.\n\nDistributed Representations and Nested Compositional Structure. T Plate, University of TorontoPhD ThesisT. Plate. Distributed Representations and Nested Compositional Struc- ture. University of Toronto, PhD Thesis, 1994.\n\nSparse distributed memory. P Kanerva, The MIT PressP. Kanerva. Sparse distributed memory. The MIT Press, 1988.\n\nGeneralized chirp-like polyphase sequences with optimum correlation properties. B M Popovic, Information Theory, IEEE Transactions on. 38B.M. Popovic. Generalized chirp-like polyphase sequences with opti- mum correlation properties. Information Theory, IEEE Transactions on, 38(4):1406-1409, 1992.\n\nOn bidirectional transitions between localist and distributed representations: The case of common substrings search using vector symbolic architecture. D Kleyko, E Osipov, Procedia Computer Science. D. Kleyko and E. Osipov. On bidirectional transitions between localist and distributed representations: The case of common substrings search using vector symbolic architecture. Procedia Computer Science, pages 1-10, 2014.\n\nFully distributed representation. P Kanerva, Real world computing symposium. P. Kanerva. Fully distributed representation. In Real world computing symposium, pages 358-365, 1997.\n", "annotations": {"author": "[{\"end\":93,\"start\":80},{\"end\":108,\"start\":94},{\"end\":126,\"start\":109},{\"end\":139,\"start\":127},{\"end\":159,\"start\":140}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":86},{\"end\":107,\"start\":101},{\"end\":125,\"start\":119},{\"end\":138,\"start\":134},{\"end\":158,\"start\":142}]", "author_first_name": "[{\"end\":85,\"start\":80},{\"end\":100,\"start\":94},{\"end\":118,\"start\":109},{\"end\":131,\"start\":127},{\"end\":133,\"start\":132},{\"end\":141,\"start\":140}]", "author_affiliation": null, "title": "[{\"end\":77,\"start\":1},{\"end\":236,\"start\":160}]", "venue": null, "abstract": "[{\"end\":947,\"start\":370}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1275,\"start\":1272},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1280,\"start\":1277},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1631,\"start\":1628},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1854,\"start\":1851},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1859,\"start\":1856},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1864,\"start\":1861},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2462,\"start\":2459},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3643,\"start\":3639},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3927,\"start\":3924},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4447,\"start\":4444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4938,\"start\":4935},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4944,\"start\":4940},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4954,\"start\":4950},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5375,\"start\":5371},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5381,\"start\":5377},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5770,\"start\":5766},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6088,\"start\":6085},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7700,\"start\":7697},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8037,\"start\":8034},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8043,\"start\":8039},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8860,\"start\":8857},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9357,\"start\":9354},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9366,\"start\":9362},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10283,\"start\":10279},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10988,\"start\":10985},{\"end\":11603,\"start\":11600},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12191,\"start\":12188},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13670,\"start\":13666},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13676,\"start\":13672},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19869,\"start\":19866},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21360,\"start\":21357},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24476,\"start\":24473},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25195,\"start\":25192},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25239,\"start\":25236},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25506,\"start\":25503},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26276,\"start\":26272},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26283,\"start\":26280},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26575,\"start\":26574},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29190,\"start\":29186},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34729,\"start\":34726}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33189,\"start\":32797},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33336,\"start\":33190},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33521,\"start\":33337},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33571,\"start\":33522},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33644,\"start\":33572},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33783,\"start\":33645},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33910,\"start\":33784},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34066,\"start\":33911},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34141,\"start\":34067},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34288,\"start\":34142},{\"attributes\":{\"id\":\"fig_10\"},\"end\":34441,\"start\":34289},{\"attributes\":{\"id\":\"fig_11\"},\"end\":34602,\"start\":34442},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34723,\"start\":34603}]", "paragraph": "[{\"end\":1590,\"start\":966},{\"end\":2463,\"start\":1592},{\"end\":3048,\"start\":2465},{\"end\":3483,\"start\":3050},{\"end\":3877,\"start\":3504},{\"end\":4412,\"start\":3879},{\"end\":4945,\"start\":4414},{\"end\":5771,\"start\":4947},{\"end\":6557,\"start\":5842},{\"end\":7174,\"start\":6559},{\"end\":7701,\"start\":7176},{\"end\":8044,\"start\":7778},{\"end\":9467,\"start\":8046},{\"end\":9757,\"start\":9507},{\"end\":9993,\"start\":9808},{\"end\":10157,\"start\":10035},{\"end\":10736,\"start\":10159},{\"end\":11098,\"start\":10911},{\"end\":11546,\"start\":11126},{\"end\":11730,\"start\":11548},{\"end\":12114,\"start\":11732},{\"end\":12276,\"start\":12116},{\"end\":12459,\"start\":12309},{\"end\":13140,\"start\":12534},{\"end\":13677,\"start\":13156},{\"end\":13973,\"start\":13736},{\"end\":14421,\"start\":14003},{\"end\":14727,\"start\":14515},{\"end\":15118,\"start\":14729},{\"end\":15285,\"start\":15150},{\"end\":15352,\"start\":15287},{\"end\":16249,\"start\":15354},{\"end\":16683,\"start\":16251},{\"end\":17532,\"start\":16685},{\"end\":18510,\"start\":17534},{\"end\":19267,\"start\":18722},{\"end\":21014,\"start\":19331},{\"end\":22052,\"start\":21079},{\"end\":23144,\"start\":22085},{\"end\":23752,\"start\":23201},{\"end\":24210,\"start\":23754},{\"end\":25303,\"start\":24231},{\"end\":25425,\"start\":25365},{\"end\":26066,\"start\":25467},{\"end\":26360,\"start\":26068},{\"end\":27408,\"start\":26406},{\"end\":27821,\"start\":27410},{\"end\":28773,\"start\":27907},{\"end\":29049,\"start\":28775},{\"end\":29371,\"start\":29118},{\"end\":29578,\"start\":29373},{\"end\":29971,\"start\":29580},{\"end\":30386,\"start\":30138},{\"end\":31273,\"start\":30560},{\"end\":31953,\"start\":31275},{\"end\":32796,\"start\":31973}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9506,\"start\":9468},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9807,\"start\":9758},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10034,\"start\":9994},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10890,\"start\":10737},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10910,\"start\":10890},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14002,\"start\":13974},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18721,\"start\":18511},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22084,\"start\":22053},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25364,\"start\":25304},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26405,\"start\":26361},{\"attributes\":{\"id\":\"formula_10\"},\"end\":29117,\"start\":29050},{\"attributes\":{\"id\":\"formula_11\"},\"end\":30137,\"start\":29972},{\"attributes\":{\"id\":\"formula_12\"},\"end\":30559,\"start\":30387}]", "table_ref": null, "section_header": "[{\"end\":964,\"start\":949},{\"end\":3502,\"start\":3486},{\"end\":5840,\"start\":5774},{\"end\":7776,\"start\":7704},{\"end\":11124,\"start\":11101},{\"end\":12307,\"start\":12279},{\"end\":12532,\"start\":12462},{\"end\":13154,\"start\":13143},{\"end\":13734,\"start\":13680},{\"end\":14513,\"start\":14424},{\"end\":15148,\"start\":15121},{\"end\":19329,\"start\":19270},{\"end\":21077,\"start\":21017},{\"end\":23188,\"start\":23147},{\"end\":23199,\"start\":23191},{\"end\":24229,\"start\":24213},{\"end\":25465,\"start\":25428},{\"end\":27905,\"start\":27824},{\"end\":31971,\"start\":31956},{\"end\":32806,\"start\":32798},{\"end\":33199,\"start\":33191},{\"end\":33346,\"start\":33338},{\"end\":33531,\"start\":33523},{\"end\":33581,\"start\":33573},{\"end\":33656,\"start\":33646},{\"end\":33793,\"start\":33785},{\"end\":33920,\"start\":33912},{\"end\":34076,\"start\":34068},{\"end\":34152,\"start\":34143},{\"end\":34299,\"start\":34290},{\"end\":34452,\"start\":34443}]", "table": null, "figure_caption": "[{\"end\":33189,\"start\":32808},{\"end\":33336,\"start\":33201},{\"end\":33521,\"start\":33348},{\"end\":33571,\"start\":33533},{\"end\":33644,\"start\":33583},{\"end\":33783,\"start\":33659},{\"end\":33910,\"start\":33795},{\"end\":34066,\"start\":33922},{\"end\":34141,\"start\":34078},{\"end\":34288,\"start\":34155},{\"end\":34441,\"start\":34302},{\"end\":34602,\"start\":34455},{\"end\":34723,\"start\":34605}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5879,\"start\":5871},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6641,\"start\":6633},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12976,\"start\":12968},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17146,\"start\":17138},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18811,\"start\":18803},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19967,\"start\":19959},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20328,\"start\":20320},{\"end\":20468,\"start\":20460},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21791,\"start\":21783},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22513,\"start\":22505},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":22834,\"start\":22826},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25133,\"start\":25125},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26934,\"start\":26925},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27244,\"start\":27235},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27492,\"start\":27485},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27615,\"start\":27606},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30158,\"start\":30149},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30908,\"start\":30899},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31598,\"start\":31589}]", "bib_author_first_name": "[{\"end\":35727,\"start\":35726},{\"end\":35737,\"start\":35736},{\"end\":35739,\"start\":35738},{\"end\":35747,\"start\":35746},{\"end\":36078,\"start\":36077},{\"end\":36080,\"start\":36079},{\"end\":36092,\"start\":36091},{\"end\":36094,\"start\":36093},{\"end\":36383,\"start\":36382},{\"end\":36385,\"start\":36384},{\"end\":36393,\"start\":36392},{\"end\":36395,\"start\":36394},{\"end\":36939,\"start\":36938},{\"end\":37274,\"start\":37273},{\"end\":37276,\"start\":37275},{\"end\":37284,\"start\":37283},{\"end\":37900,\"start\":37899},{\"end\":38103,\"start\":38102},{\"end\":38105,\"start\":38104},{\"end\":38116,\"start\":38115},{\"end\":38118,\"start\":38117},{\"end\":38291,\"start\":38290},{\"end\":38458,\"start\":38457},{\"end\":38460,\"start\":38459},{\"end\":38468,\"start\":38467},{\"end\":38475,\"start\":38474},{\"end\":38489,\"start\":38488},{\"end\":38497,\"start\":38496},{\"end\":38507,\"start\":38504},{\"end\":38515,\"start\":38514},{\"end\":38522,\"start\":38521},{\"end\":38530,\"start\":38529},{\"end\":38538,\"start\":38535},{\"end\":38545,\"start\":38544},{\"end\":38551,\"start\":38550},{\"end\":38553,\"start\":38552},{\"end\":38564,\"start\":38563},{\"end\":38573,\"start\":38572},{\"end\":38575,\"start\":38574},{\"end\":38844,\"start\":38843},{\"end\":38846,\"start\":38845},{\"end\":38999,\"start\":38998},{\"end\":39285,\"start\":39284},{\"end\":39287,\"start\":39286},{\"end\":39302,\"start\":39301},{\"end\":39553,\"start\":39552},{\"end\":39563,\"start\":39562},{\"end\":39573,\"start\":39572},{\"end\":39583,\"start\":39582},{\"end\":40013,\"start\":40012},{\"end\":40027,\"start\":40026},{\"end\":40029,\"start\":40028},{\"end\":40042,\"start\":40041},{\"end\":40050,\"start\":40049},{\"end\":40075,\"start\":40074},{\"end\":40442,\"start\":40441},{\"end\":40627,\"start\":40626},{\"end\":40792,\"start\":40791},{\"end\":40794,\"start\":40793},{\"end\":41163,\"start\":41162},{\"end\":41173,\"start\":41172},{\"end\":41467,\"start\":41466}]", "bib_author_last_name": "[{\"end\":35734,\"start\":35728},{\"end\":35744,\"start\":35740},{\"end\":35753,\"start\":35748},{\"end\":36089,\"start\":36081},{\"end\":36099,\"start\":36095},{\"end\":36390,\"start\":36386},{\"end\":36408,\"start\":36396},{\"end\":36947,\"start\":36940},{\"end\":37281,\"start\":37277},{\"end\":37291,\"start\":37285},{\"end\":37906,\"start\":37901},{\"end\":38113,\"start\":38106},{\"end\":38125,\"start\":38119},{\"end\":38301,\"start\":38292},{\"end\":38465,\"start\":38461},{\"end\":38472,\"start\":38469},{\"end\":38486,\"start\":38476},{\"end\":38494,\"start\":38490},{\"end\":38502,\"start\":38498},{\"end\":38512,\"start\":38508},{\"end\":38519,\"start\":38516},{\"end\":38527,\"start\":38523},{\"end\":38533,\"start\":38531},{\"end\":38542,\"start\":38539},{\"end\":38548,\"start\":38546},{\"end\":38561,\"start\":38554},{\"end\":38570,\"start\":38565},{\"end\":38582,\"start\":38576},{\"end\":38854,\"start\":38847},{\"end\":39007,\"start\":39000},{\"end\":39299,\"start\":39288},{\"end\":39307,\"start\":39303},{\"end\":39315,\"start\":39309},{\"end\":39560,\"start\":39554},{\"end\":39570,\"start\":39564},{\"end\":39580,\"start\":39574},{\"end\":39592,\"start\":39584},{\"end\":40024,\"start\":40014},{\"end\":40039,\"start\":40030},{\"end\":40047,\"start\":40043},{\"end\":40072,\"start\":40051},{\"end\":40081,\"start\":40076},{\"end\":40448,\"start\":40443},{\"end\":40635,\"start\":40628},{\"end\":40802,\"start\":40795},{\"end\":41170,\"start\":41164},{\"end\":41180,\"start\":41174},{\"end\":41475,\"start\":41468}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17477431},\"end\":36005,\"start\":35700},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17573325},\"end\":36310,\"start\":36007},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":43344123},\"end\":36811,\"start\":36312},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":733980},\"end\":37179,\"start\":36813},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16535135},\"end\":37860,\"start\":37181},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2352281},\"end\":38052,\"start\":37862},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7958038},\"end\":38272,\"start\":38054},{\"attributes\":{\"id\":\"b7\"},\"end\":38395,\"start\":38274},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4411206},\"end\":38841,\"start\":38397},{\"attributes\":{\"id\":\"b9\"},\"end\":38962,\"start\":38843},{\"attributes\":{\"id\":\"b10\"},\"end\":39212,\"start\":38964},{\"attributes\":{\"id\":\"b11\"},\"end\":39430,\"start\":39214},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":38866387},\"end\":39953,\"start\":39432},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11139857},\"end\":40375,\"start\":39955},{\"attributes\":{\"id\":\"b14\"},\"end\":40597,\"start\":40377},{\"attributes\":{\"id\":\"b15\"},\"end\":40709,\"start\":40599},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8323398},\"end\":41008,\"start\":40711},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":39219588},\"end\":41430,\"start\":41010},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":228274293},\"end\":41610,\"start\":41432}]", "bib_title": "[{\"end\":35724,\"start\":35700},{\"end\":36075,\"start\":36007},{\"end\":36380,\"start\":36312},{\"end\":36936,\"start\":36813},{\"end\":37271,\"start\":37181},{\"end\":37897,\"start\":37862},{\"end\":38100,\"start\":38054},{\"end\":38288,\"start\":38274},{\"end\":38455,\"start\":38397},{\"end\":38996,\"start\":38964},{\"end\":39550,\"start\":39432},{\"end\":40010,\"start\":39955},{\"end\":40789,\"start\":40711},{\"end\":41160,\"start\":41010},{\"end\":41464,\"start\":41432}]", "bib_author": "[{\"end\":35736,\"start\":35726},{\"end\":35746,\"start\":35736},{\"end\":35755,\"start\":35746},{\"end\":36091,\"start\":36077},{\"end\":36101,\"start\":36091},{\"end\":36392,\"start\":36382},{\"end\":36410,\"start\":36392},{\"end\":36949,\"start\":36938},{\"end\":37283,\"start\":37273},{\"end\":37293,\"start\":37283},{\"end\":37908,\"start\":37899},{\"end\":38115,\"start\":38102},{\"end\":38127,\"start\":38115},{\"end\":38303,\"start\":38290},{\"end\":38467,\"start\":38457},{\"end\":38474,\"start\":38467},{\"end\":38488,\"start\":38474},{\"end\":38496,\"start\":38488},{\"end\":38504,\"start\":38496},{\"end\":38514,\"start\":38504},{\"end\":38521,\"start\":38514},{\"end\":38529,\"start\":38521},{\"end\":38535,\"start\":38529},{\"end\":38544,\"start\":38535},{\"end\":38550,\"start\":38544},{\"end\":38563,\"start\":38550},{\"end\":38572,\"start\":38563},{\"end\":38584,\"start\":38572},{\"end\":38856,\"start\":38843},{\"end\":39009,\"start\":38998},{\"end\":39301,\"start\":39284},{\"end\":39309,\"start\":39301},{\"end\":39317,\"start\":39309},{\"end\":39562,\"start\":39552},{\"end\":39572,\"start\":39562},{\"end\":39582,\"start\":39572},{\"end\":39594,\"start\":39582},{\"end\":40026,\"start\":40012},{\"end\":40041,\"start\":40026},{\"end\":40049,\"start\":40041},{\"end\":40074,\"start\":40049},{\"end\":40083,\"start\":40074},{\"end\":40450,\"start\":40441},{\"end\":40637,\"start\":40626},{\"end\":40804,\"start\":40791},{\"end\":41172,\"start\":41162},{\"end\":41182,\"start\":41172},{\"end\":41477,\"start\":41466}]", "bib_venue": "[{\"end\":35832,\"start\":35755},{\"end\":36137,\"start\":36101},{\"end\":36515,\"start\":36410},{\"end\":36970,\"start\":36949},{\"end\":37408,\"start\":37293},{\"end\":37944,\"start\":37908},{\"end\":38145,\"start\":38127},{\"end\":38316,\"start\":38303},{\"end\":38590,\"start\":38584},{\"end\":38897,\"start\":38856},{\"end\":39074,\"start\":39009},{\"end\":39282,\"start\":39214},{\"end\":39665,\"start\":39594},{\"end\":40145,\"start\":40083},{\"end\":40439,\"start\":40377},{\"end\":40624,\"start\":40599},{\"end\":40844,\"start\":40804},{\"end\":41207,\"start\":41182},{\"end\":41507,\"start\":41477},{\"end\":36538,\"start\":36517},{\"end\":37538,\"start\":37410}]"}}}, "year": 2023, "month": 12, "day": 17}