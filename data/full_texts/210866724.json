{"id": 210866724, "updated": "2023-08-30 17:07:35.179", "metadata": {"title": "Deep learning interpretation of echocardiograms", "authors": "[{\"first\":\"Amirata\",\"last\":\"Ghorbani\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Ouyang\",\"middle\":[]},{\"first\":\"Abubakar\",\"last\":\"Abid\",\"middle\":[]},{\"first\":\"Bryan\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Chen\",\"middle\":[\"H.\"]},{\"first\":\"Robert\",\"last\":\"Harrington\",\"middle\":[\"A.\"]},{\"first\":\"David\",\"last\":\"Liang\",\"middle\":[\"H.\"]},{\"first\":\"Euan\",\"last\":\"Ashley\",\"middle\":[\"A.\"]},{\"first\":\"James\",\"last\":\"Zou\",\"middle\":[\"Y.\"]}]", "venue": "NPJ Digital Medicine", "journal": "NPJ Digital Medicine", "publication_date": {"year": 2020, "month": 1, "day": 24}, "abstract": "Echocardiography uses ultrasound technology to capture high temporal and spatial resolution images of the heart and surrounding structures, and is the most common imaging modality in cardiovascular medicine. Using convolutional neural networks on a large new dataset, we show that deep learning applied to echocardiography can identify local cardiac structures, estimate cardiac function, and predict systemic phenotypes that modify cardiovascular risk but not readily identifiable to human interpretation. Our deep learning model, EchoNet, accurately identified the presence of pacemaker leads (AUC\u2009=\u20090.89), enlarged left atrium (AUC\u2009=\u20090.86), left ventricular hypertrophy (AUC\u2009=\u20090.75), left ventricular end systolic and diastolic volumes (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${R}^{2}$$\\end{document}R2\u2009=\u20090.74 and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${R}^{2}$$\\end{document}R2\u2009=\u20090.70), and ejection fraction (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${R}^{2}$$\\end{document}R2\u2009=\u20090.50), as well as predicted systemic phenotypes of age (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${R}^{2}$$\\end{document}R2\u2009=\u20090.46), sex (AUC\u2009=\u20090.88), weight (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${R}^{2}$$\\end{document}R2\u2009=\u20090.56), and height (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${R}^{2}$$\\end{document}R2\u2009=\u20090.33). Interpretation analysis validates that EchoNet shows appropriate attention to key cardiac structures when performing human-explainable tasks and highlights hypothesis-generating regions of interest when predicting systemic phenotypes difficult for human interpretation. Machine learning on echocardiography images can streamline repetitive tasks in the clinical workflow, provide preliminary interpretation in areas with insufficient qualified cardiologists, and predict phenotypes challenging for human evaluation.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3002705197", "acl": null, "pubmed": "33483633", "pubmedcentral": "6981156", "dblp": "journals/npjdm/GhorbaniOAHCHLA20", "doi": "10.1038/s41746-019-0216-8"}}, "content": {"source": {"pdf_hash": "9c7227146e913e30683a6e28b49b2cb1b4e1d25f", "pdf_src": "Springer", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.nature.com/articles/s41746-019-0216-8.pdf", "status": "GOLD"}}, "grobid": {"id": "19c79f215d3aebb14cc505f42dc93c19a8afc839", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9c7227146e913e30683a6e28b49b2cb1b4e1d25f.txt", "contents": "\nARTICLE Deep learning interpretation of echocardiograms\n\n\nAmirata Ghorbani \nDavid Ouyang \nAbubakar Abid \nBryan He \nJonathan H Chen \nRobert A Harrington \nDavid H Liang \nEuan A Ashley \nJames Y Zou \nARTICLE Deep learning interpretation of echocardiograms\n10.1038/s41746-019-0216-8OPEN\nEchocardiography uses ultrasound technology to capture high temporal and spatial resolution images of the heart and surrounding structures, and is the most common imaging modality in cardiovascular medicine. Using convolutional neural networks on a large new dataset, we show that deep learning applied to echocardiography can identify local cardiac structures, estimate cardiac function, and predict systemic phenotypes that modify cardiovascular risk but not readily identifiable to human interpretation. Our deep learning model, EchoNet, accurately identified the presence of pacemaker leads (AUC = 0.89), enlarged left atrium (AUC = 0.86), left ventricular hypertrophy (AUC = 0.75), left ventricular end systolic and diastolic volumes (R 2 = 0.74 and R 2 = 0.70), and ejection fraction (R 2 = 0.50), as well as predicted systemic phenotypes of age (R 2 = 0.46), sex (AUC = 0.88), weight (R 2 = 0.56), and height (R 2 = 0.33). Interpretation analysis validates that EchoNet shows appropriate attention to key cardiac structures when performing human-explainable tasks and highlights hypothesis-generating regions of interest when predicting systemic phenotypes difficult for human interpretation. Machine learning on echocardiography images can streamline repetitive tasks in the clinical workflow, provide preliminary interpretation in areas with insufficient qualified cardiologists, and predict phenotypes challenging for human evaluation.npj Digital Medicine (2020) 3:10 ; https://doi.\n\nINTRODUCTION Cardiovascular disease has a substantial impact on overall health, well-being, and life-expectancy. In addition to being the leading cause of mortality for both men and women, cardiovascular disease is responsible for 17% of the United States' national health expenditures. 1 Even as the burden of cardiovascular disease is expected to rise with an aging population, 1 there continues to be significant racial, socioeconomic, and geographic disparities in both access to care and disease outcomes. 2,3 Variation in access to and quality of cardiovascular imaging has been linked to disparities in outcomes. 3,4 It has been hypothesized that automated image interpretation can enable more available and accurate cardiovascular care and begin to alleviate some of the disparities in cardiovascular care. 5,6 The application of machine learning in cardiology is still in its infancy, however there is significant interest in bringing neural network based approaches to cardiovascular imaging.\n\nMachine learning has transformed many fields, ranging from image processing and voice recognition systems to super-human performance in complex strategy games. 7 Many of the biggest recent advances in machine learning come from computer vision algorithms and processing image data with deep learning. [8][9][10][11] Recent advances in machine learning suggest deep learning can identify human-identifiable characteristics as well as phenotypes unrecognized by human experts. 12,13 Efforts to apply machine learning to other modalities of medical imaging have shown promise in computer-assisted diagnosis. [12][13][14][15][16] Seemingly unrelated imaging of individual organ systems, such as fundoscopic retina images, can predict systemic phenotypes and predict cardiovascular risk factors. 12 Additionally, deep learning algorithms perform well in risk stratification and classification of disease. 14,16 Multiple recent medical examples outside of cardiology show convolutional neural network (CNN) algorithms can match or even exceed human experts in identifying and classifying diseases. 13,14 Echocardiography is a uniquely well-suited approach for the application of deep learning in cardiology. The most readily available and widely used imaging technique to assess cardiac function and structure, echocardiography combines rapid image acquisition with the lack of ionizing radiation to serve as the backbone of cardiovascular imaging. 4,17 Echocardiography is both frequently used as a screening modality for healthy, asymptomatic patients as well as in order to diagnose and manage patients with complex cardiovascular disease. 17 For indications ranging from cardiomyopathies to valvular heart diseases, echocardiography is both necessary and sufficient to diagnose many cardiovascular diseases. Despite its importance in clinical phenotyping, there is variance in the human interpretation of echocardiogram images that could impact clinical care. [18][19][20] Formalized training guidelines for cardiologists recognize the value of experience in interpreting echocardiogram images and basic cardiology training might be insufficient to interpret echocardiograms at the highest level. 21 Given the importance of imaging to cardiovascular care, an automated pipeline for interpreting cardiovascular imaging can improve peri-operative risk stratification, manage the cardiovascular risk of patients with oncologic disease undergoing chemotherapy, and aid in the diagnosis of cardiovascular disease. 1,22,23 While other works applying machine learning to medical imaging required re-annotation of images by human experts, the clinical workflow for echocardiography inherently includes many measurements and calculations and often is reported through structured reporting systems. The ability to use previous annotations and interpretations from clinical reports can greatly accelerate adoption of machine learning in medical imaging. Given the availability of previously annotated clinical reports, the density of information in image and video datasets, and many available machine learning architectures already applied to image datasets, echocardiography is a high impact and highly tractable application of machine learning in medical imaging.\n\nCurrent literature have already shown that it is possible to identify standard echocardiogram views from unlabeled datasets. 5,6,24 Previous works have used CNNs trained on images and videos from echocardiography to perform segmentation to identify cardiac structures and derive cardiac function. In this study, we extend previous analyses to show that EchoNet, our deep learning model using echocardiography images, can reliably identify local cardiac structures and anatomy, estimate volumetric measurements and metrics of cardiac function, and predict systemic human phenotypes that modify cardiovascular risk. Additionally, we show the first application of interpretation frameworks to understand deep learning models from echocardiogram images. Human-identifiable features, such as the presence of pacemaker and defibrillator leads, left ventricular hypertrophy, and abnormal left atrial chamber size identified by our CNN were validated using interpretation frameworks to highlight the most relevant regions of interest. To the best of our knowledge, we develop the first deep learning model that can directly predict age, sex, weight, and height from echocardiogram images and use interpretation methods to understand how the model predicts these systemic phenotypes difficult for human interpreters.\n\n\nRESULTS\n\nWe trained a CNN model on a data set of more than 2.6 million echocardiogram images from 2850 patients to identify local cardiac structures, estimate cardiac function, and predict systemic risk factors (Fig. 1). Echocardiogram images, reports, and measurements were obtained from an accredited echocardiography lab of a large academic medical center (Table 1). Echocardiography visualizes cardiac structures from various different orientations and geometries, so images were classified by cardiac view to homogenize the input data set. Echocardiogram images were sampled from echocardiogram videos, pre-processed by de-identifying the images, and cropped to eliminate information outside of the scanning sector. These processed images were used to train EchoNet on the relevant medical classification or prediction task.\n\nPredicting anatomic structures and local features A standard part of the clinical workflow of echocardiography interpretation is the identification of local cardiac structures and characterization of its location, size, and shape. Local cardiac structures can have significant variation in image characteristics, ranging from bright echos of metallic intracardiac structures to dark regions denoting blood pools in cardiac chambers. As our first task, we trained EchoNet on three classification tasks frequently evaluated by cardiologists that rely on recognition of local features (Fig. 2). Labels of the presence of intracardiac devices (such as catheters, pacemaker, and defibrillator leads), severe left atrial dilation, and left ventricular hypertrophy were extracted from the physician-interpreted report and used to train EchoNet on unlabeled apical-4-chamber input images. The presence of a pacemaker lead was predicted with high accuracy (AUC of 0.89, F1 score of 0.73), followed by the identification of a severely dilated left atrium (AUC of 0.85, F1 score of 0.68), and left ventricular hypertrophy (AUC of 0.75, F1 score of 0.57). Similarly high performance was achieved in predicting right atrium major axis length and left atrial volume estimate. Scatter plots are shown in the Supplemental Materials. To understand the model's predictions, we used gradient-based sensitivity map methods 25 to identify the regions of interest for the interpretation and show that EchoNet highlights relevant areas that correspond to intracardiac devices, the left atrium, and the left ventricle respectively. Models' prediction robustness was additionally examined with direct input image manipulations, including occlusion of human recognizable features, to validate that EchoNet arrives at its predictions by focusing on biologically plausible regions of interest. 26 For example, in the frames in Fig. 2 with pacemaker lead, when we manually mask out the lead in the frame, EchoNet changes its prediction to no pacemaker. Predicting cardiac function Quantification of cardiac function is a crucial assessment addressed by echocardiography. However, it has significant variation in human interpretation. 18,19 The ejection fraction, a measure of the volume change in the left ventricle with each heart beat, is a key metric of cardiac function, but its measurement relies on the time-consuming manual tracing of left ventricular areas and volumes at different times during the cardiac cycle. We trained EchoNet to predict left ventricular end systolic volume (ESV), end diastolic volume (EDV), and ejection fraction from sampled apical-four-chamber view images (Fig. 3). Left ventricular ESV and EDV were accurately predicted. For the prediction of ESV, an R 2 score of 0.74 and mean absolute error (MAE) of 13.3 mL was achieved versus MAE of 25.4 mL if we use mean prediction which is to predict every patient's ESV as the average ESCV value of patients. The result for the EDV prediction was an R 2 score of 0.70 and MAE of 20.5 mL (mean prediction MAE = 35.4 mL). Conventionally, ejection fraction is calculated from a ratio of these two volumetric measurements, however, calculated ejection fraction from the predicted volumes were less accurate ( Fig. 3c) than EchoNet trained directly on the ejection fraction (Fig. 3d). We show the relative performance of a deep learning model undergoing a standard human workflow of evaluating ESV and EDV then subsequently calculating ejection fraction from the two volumetric measurements vs. direct \"end-to-end\" deep learning prediction of ejection fraction and show that the \"end-to-end\" deep learning prediction model had improved performance. Using the trained EchoNet, an R 2 score of 0.50 and MAE of 7:0% is achieved (MAE of mean prediction = 9:9%). For each model, interpretation methods show appropriate attention over left ventricle as the region of interest to generate the predictions. A comparison of model performance based on number of sampled video frames did not show gain in model performance after 11 frames per prediction task.\n\nPredicting systemic cardiovascular risk factors With good performance in identifying local structures and estimating volumetric measurements of the heart, we sought to determine if EchoNet can also identify systemic phenotypes that modify cardiovascular risk. Previous work has shown that deep CNNs have powerful capacity to aggregate the information on visual correlations between medical imaging data and systemic phenotypes. 12 (Fig. 4a). It is recognized that characteristics such as heart chamber size and geometry vary by age, sex, weight, and height, 27,28 however, human interpreters cannot predict these systemic phenotypes from echocardiogram images alone. We also investigated multi-task learning-sharing some of the model parameters while predicting across the different phenotypes-and this did not improve the model performance. Bland-Altman plots of the model accuracy in relationship to the predictions are shown in Fig. 5 and in the Supplemental Materials.\n\nLastly, we used the same gradient-based sensitivity map methods to identify regions of interest for models predicting systemic phenotypes difficult for human experts to predict. These regions of interest for these models tend to be more diffuse, highlighting the models for systemic phenotypes do not rely as much on individual features or local regions (Fig. 4b). The interpretations for models predicting weight and height had particular attention on the apex of the scanning sector, suggesting information related to the thickness and characteristics of the chest wall and extra-cardiac tissue was predictive of weight and height.\n\n\nDISCUSSION\n\nIn this study, we show that deep CNNs trained on standard echocardiograms images can identify local features, humaninterpretable metrics of cardiac function, and systemic phenotypes, such as patient age, sex, weight, and height. Our models achieved high prediction accuracy for tasks readily performed by human interpreters, such as estimating ejection fraction and chamber volumes and identifying of pacemaker leads, as well as for tasks that would be challenging for human interpreters, such as predicting systemic phenotypes from images of the heart alone. Unique from prior work in the field, instead of using handlabeled outcomes, we describe and exemplify an approach of using previously obtained phenotypes and interpretations from clinical records for model training, which can allow for more external validity and more rapid generalization with larger training data sets. One common critique of deep learning models on medical imaging datasets is the \"black-box\" nature of the predictions and the inability to understand the models ability to identity relevant features. In addition to showing the predictive performance of our methods, we validate the model's predictions by highlighting important biologically plausible regions of interest that correspond to each interpretation. These results represent the first presentation of interpretation techniques for deep learning models on echocardiographic images and can build confidence in simple models as the relevant pixels are highlighted when identifying local structures such as pacemaker leads. In addition, this approach of using interpretability frameworks to identify regions of interest may lay additional groundwork toward understanding human physiology when interpreting outputs of deep learning models for challenging, humanunexplainable phenotypes in medical imaging. These results represent a step towards automated image evaluation of echocardiograms through deep learning. We believe this research could supplement future approaches to screen for subclinical cardiovascular disease and understand the biological basis of cardiovascular aging.\n\nWhile age, sex, weight, and height are relatively obvious visual phenotypes, our paper presents models predicting these systemic phenotypes in the roadmap of progressing from simple local feature based predictions, to more complex dynamic measurement predictions, and finally to human-difficult classifications of systemic phenotypes without obvious local features. Previous studies have shown that medical imaging of other organ systems can predict cardiovascular risk factors including age, gender, and blood pressure by identifying local features of systemic phenotypes. 12 Recently, 12-lead ECG based deep learning models have been shown to accurately predict age and sex, further validating a cardiac phenotype for aging and gender dysmorphism. 29 Our results identify another avenue of detecting systemic phenotypes through organ-system specific imaging. These results are supported by previous studies that showed population level normative values for the chamber sizes of cardiac structures as participants vary by age, sex, height, and weight. 27,28 Age-related changes in the heart, in particular changing chamber sizes and diastolic filling parameters, have been well characterized, 30,31 and our study builds upon this body of work to demonstrate that these signals are present to allow for prediction of these phenotypes to a degree of precision not previously reported. As systemic phenotypes of age, sex, and body mass index are highly correlated with cardiovascular outcomes and overall life expectancy, the ability of deep learning models to identify predictive latent features suggest that future work on image-based deep learning models can identify features hidden from human observers and predict outcomes and mortality. [32][33][34] In addition to chamber size, extracardiac characteristics as well as additional unlabeled features, are incorporated in our models to predict patient systemic phenotypes. The area closest to the transducer, representing subcutaneous tissue, chest wall, lung parenchyma, and other extracardiac structures are highlighted in the weight and height prediction models. These interpretation maps are consistent with prior knowledge that obese patients often have challenging image acquisition, 35,36 however, it is surprising the degree of precision it brings to predicting height and weight. Retrospective review of predictions by our model suggest human-interpretable features that show biologic plausibility. In the saliency maps for the age prediction model, significant attention was paid to the crux of the heart, involving the intra-atrial septum, where the aortic annulus as the view becomes closer to an apical-five-chamber view, septal insertion of the mitral and tricuspid leaflets, and the mitral apparatus. This is an area of where differential calcification can be seen, particularly of the aortic valve and mitral annulus, and is known to be highly correlated with age-related changes. 37,38 Images predicted to be of younger patients also show preference for small atria and is consistent with prior studies showing age-related changes to the left atrium. 31,39 The feedback loop between physician and machine learning models with clinician review of appropriate and inappropriately predicted images can assist in greater understanding of normal variation in human echocardiograms as well as identify features previously neglected by human interpreters. Understanding misclassifications, such as patients with young biological age but high predicted age, and further investigation of extreme individuals can potentially help identify subclinical cardiovascular disease and better understand the aging process.\n\nPrior foundational work on deep learning interpretation of echocardiogram images have focused on the mechanics of obtaining the correct echocardiographic view and hand-crafted scenarios with closely curated patient populations and multi-step processing and post-processing feature selection and calculation. 5,24 The work described here focuses on using more modern deep learning architectures and techniques in the framework of using previously adjudicated phenotypes with the potential of rapid scaling of algorithms to clinical practice. With the continued rapid expansion of computational resources, we were able to input higher resolution images (299 \u00d7 299 instead of 60 \u00d7 80 in prior studies) 24 and present an 'end-to-end' approach to predicting complex phenotypes like ejection fraction that has decreased variance over multi-step techniques which require identification of end-systole, end-diastole, and separate segmentation steps. 5 While our model performance improves upon the results of prior work, EchoNet's evaluation of clinical measurements of ESV, EDV, and EF have non-negligible variance and does not surpass human assessment of these metrics. For these tasks, clinical context and understanding of contextual information and other measurements likely has significant relevance to the training task. For example, evaluation of EF as a ratio of ESV and EDV magnifies errors and performs worse than estimation of ESV or EDV individually. Future work requires greater integration of temporal information between frames to better assess cardiac motion and interdependencies in cardiac structures. In addition to quantitative measurements, human evaluation of cardiac structures, such as tracings of the left ventricle, are potentially high value training datasets.\n\nRecent novel machine learning techniques for interpreting network activations are also presented for the first time to understand regions of interest in the interpretation of echocardiogram images. 25 While prior work used hand-labeled outcomes and patient cohorts for the majority of their outcome labels, we describe and showcase an approach of using previously obtained phenotypes and interpretations from clinical records for model training, which can allow for more external validity and more rapid generalization with larger training data sets. Additionally, given the significant difference between images in ImageNet vs. echocardiogram images, pretraining with ImageNet weights did not significantly help model performance, however, our models trained on systemic phenotypes can be good starting weights for future work on training on echocardiogram images of more complex phenotypes.\n\nPrevious studies of deep learning on medical imaging focused on resource-intensive imaging modalities common in resourcerich settings 40,41 or sub-speciality imaging with focused indication. 12,13,16 These modalities often need retrospective annotation by experts as the clinical workflow often does not require detailed measurements or localizations. In the development of any machine learning models to healthcare questions, external validity of first-order importance. An important caveat of our work is that the images obtained were from one type of ultrasound machine and our test dataset was of different patients but also scanned using the sample machine and at the same institution. Our approach trains deep learning models on previous studies and associated annotations from the EMR to leverage past data for rapid deployment of machine learning models. This approach leverages two advantages of echocardiography, first that echocardiography is one of the most frequently using imaging studies in the United States 42 and second, A. Ghorbani et al. echocardiography often uses structured reporting, making advances in deep learning particularly applicable and generalizable. However, such a method depends on the clinical standard, as there is known variability between MRI and echocardiography derived methods and training on clinical reports require rigorous quality control from the institution's echocardiography lab. Future work on deep learning of echocardiography would need to confirm the performance in broader populations and settings. Automation of echocardiography interpretation through deep learning can make cardiovascular care more readily available. With point-of-care ultrasound is being more frequently used by an increasing number of physicians, ranging from emergency room physicians, internists, to anesthesiologists, and deep learning on cardiac ultrasound images can provide accurate predictions and diagnoses to an even wider range of patients.\n\nIn summary, we provide evidence that deep learning can reproduce common human interpretation tasks and leverage additional information to predict systemic phenotypes that could allow for better cardiovascular risk stratification. We used interpretation methods that could feedback relevant regions of interest for further investigation by cardiologists to better understand aging and prevent cardiovascular disease. Our work could enable assessment of cardiac physiology, anatomy, and risk stratification at the population level by automating common workflows in clinical echocardiography and democratize expert interpretation to general patient populations.\n\n\nMETHODS Dataset\n\nThe Stanford Echocardiography Database contains images, physician reports, and clinical data from patients at Stanford Hospital who underwent echocardiography in the course of routine care. The accredited echocardiography laboratory provides cardiac imaging to a range of patients with a variety of cardiac conditions including atrial fibrillation, coronary artery disease, cardiomyopathy, aortic stenosis, and amyloidosis. For this study, we used 3312 consecutive comprehensive non-stress echocardiography studies obtained between June 2018 and December 2018, and randomly split the patients into independent training, validation, and test cohorts. Videos of standard cardiac views, color Doppler videos, and still images comprise each study and is stored in Digital Imaging and Communications in Medicine (DICOM) format. The videos were sampled to obtain 1,624,780 scaled 299 \u00d7 299 pixel images. The sampling rate was chosen to optimize model size and training time while maintaining model performance and additional preprocessing details are described in the Supplementary Materials. For each image, information pertained to image acquisition, identifying information, and other information outside the imaging sector was removed through masking. Human interpretations from the physician-interpreted report and clinical features from the electronic medical record were matched to each echocardiography study for model training. This study was approved by the Stanford University IRB. Written informed consent was waived for retrospective review of imaging obtained in the course of standard care.\n\n\nModel\n\nWe chose a CNN architecture that balances network width and depth in order to manage the computational cost of training. We used the architecture based on Inception-Resnet-v1 10 to predict all of our phenotypes. This architecture has strong performance on benchmark datasets like ILSVR2012 image recognition challenge (Imagenet) 9 and is computationally efficient compared to other networks. 43 Pretraining Inception-ResNet with ImageNet did not significantly increase model performance, and our ultimate model used randomly initiated weights.\n\nFor each prediction task, one CNN architecture was trained on individual frames from each echocardiogram video with output labels that were extracted either from the electronic medical record or from the physician report. From each video, we sampled 20 frames (one frame per 100 milliseconds) starting from the first frame of the video. The final prediction was performed by averaging all the predictions from individual frames. Several alternative methods were explored in order to aggregate framelevel predictions into one patient-level prediction and did not yield better results compared to simple averaging.\n\nModel training was performed using the TensorFlow library 44 which is capable of utilizing parallel-processing capabilites of Graphical Processing Units (GPUs) for fast training of deep learning models. We chose Adam optimizer as our optimization algorithm which is computationally efficient, has little memory usage, and has shown superior performance in many deep learning tasks. 45 As our prediction loss, we used cross-entropy loss for classification tasks and squared error loss for regressions tasks along with using weight-decay regularization loss to prevent over-fitting. 46 We investigated other variants of prediction loss (absolute loss, Huber loss 47 for regression and Focal loss 48 for classification), and they did not improve performance. For each prediction task, we chose the best performing hyper-parameters using grid search (24 models trained for each task) to optimize learning rate and weight decay regularization factor. In order to perform model selection, for each tasks, we split the training data into training and validation set by using 10% of train data as a held-out validation set in; the model with the best performance on the validation set is then examined on the test set to report the final performance results. After the models were trained, they were evaluated on a separate set of test frames gathered from echocardiogram studies of 337 other patients with similar demographics (Table 1). These patients were randomly chosen for a 10% held-out test set and were not seen by the model during training.\n\n\nData augmentation\n\nModel performance improved with increasing input data sample size. Our experiments suggested additional relative improvement with increase in the number of patients represented in the training cohort compared to oversampling of frames per patient. Data augmentation using previously validated methods, 49,50 also greatly improving generalization of model predictions by reducing over-fitting on the training set. Through the training process, at each optimization step each training image is transformed through geometric transformations (such as flipping, reflection, and translation) and changes in contrast and saturation. As a result, the training data set is augmented into a larger effective data set. In this work, mimicking variation in echocardiography image acquisition, we used random rotation and random saturation augmentation for data augmentation (Fig. 1c). During each step of stochastic gradient descent in the training process, we randomly sample 24 training frames, and we perturb each training frame with a random rotation between \u221220 to 20 degrees and with adding a number sampled uniformly between \u22120.1 to 0.1 to image pixels (pixels values are normalized) to increase or decrease brightness of the image. Data augmentation results in improvement for all of the tasks; between 1-4% improvement in AUC metric for classification tasks and 2-10% improvement in R 2 score for regression tasks.\n\n\nCardiac view selection\n\nWe first tried using all echocardiogram images for prediction tasks but given the size of echocardiogram studies, initial efforts struggled with long training times, poor model convergence, and difficulty with model saturation. With the knowledge that, in a single comprehensive echocardiography study, the same cardiac structures are often visualized from multiple views to confirm and corroborate assessments from other views, we experimented with model training using subsets of images by cardiac view. As described in Fig. 1b, a selection of the most common standard echocardiogram views were evaluated for model performance. Images from each study were classified using a previously described supervised training method. 5 We sought to identify the most informationrich views by training separate models on the subsets of dataset images of only one cardiac view. Training a model using only one cardiac view results in one order of magnitude reduction of training time and computational cost with the benefit of maintaining similar predictive performance when information-rich views were used. For each of the prediction tasks and specific choice of hyper-parameters, training a model on the A4C-View data set converges in~30 h using one Titan XP GPU. The training process of the same model and prediction task converges in~240 h using all the views in the dataset. Given the favorable balance of performance to computational cost as well as prior knowledge on which views most cardiologists frequently prioritize, we chose the apical-four-chamber view as the input training set for subsequent experiments on training local features, volumetric estimates and systemic phenotypes.\n\n\nInterpretability\n\nInterpretability methods for deep learning models have been developed to explain the predictions of the black-box deep neural network. One family of interpretations methods are the sensitivity map methods that seek to explain a trained model's prediction on a given input by assigning a scalar importance score to each of the input features or pixels. If the model's input is an image, the resulting sensitivity map could be depicted as a twodimensional heat-map with the same size as the image where more important pixels of the image are brighter than other pixels. The sensitivity map methods compute the importance of each input feature as the effect of its perturbation on model's prediction. If the pixel is not important, the change should be small and vice versa.\n\nIntroduced by Baehrens et al. 51 and applied to deep neural networks by Simonyan et al., 52 the simplest way to compute such score is to have a first-order linear approximation of the model by taking the gradient of the output with respect to the input; the weights of the resulting linear model are the sensitivity of the output to perturbation of their corresponding features (pixels). More formally, given the d-dimensional input x t 2 R d and the model's prediction function f \u00f0:\u00de, the importance score of the j'th feature is j\u2207 x f \u00f0x t \u00de j j. Further extensions to this gradient method were introduced to achieve better interpretations of the model and to output sensitivity maps that are perceptually easier to understand by human users: LRP, 53 DeepLIFT, 54 Integrated Gradients, 55 and so forth. These sensitivity map methods, however, suffer from visual noise 25 and sensitivity to input perturbations. 56 SmoothGrad 25 method alleviates both problems 57 by adding white noise to the image and then take the average of the resulting sensitivity maps. In this work, we use SmoothGrad with the simple gradient method due to its computational efficiency. Other interpretation methods including Integrated Gradients were tested but did not result in better visualizations.\n\n\nLessons from model training and experiments\n\nEchoNet performance greatly improved with efforts to augment data size, homogenize input data, and with optimize model training with hyperparameter search. Our experience shows that increasing number of unique patients in the training set can significantly improve the model, more so than increasing the sampling rate of frames from the same patients. Homogenizing the input images by selection of cardiac view prior to model training greatly improved training speed and decreased computational time without significant loss in model performance. Finally, A. Ghorbani et al.\n\nwe found that results can be significantly improved with careful hyperparameter choice; between 7-9% in AUC metric for classification tasks and 3-10% in R 2 score for regression tasks.\n\n\nReporting summary\n\nFurther information on research design is available in the Nature Research Reporting Summary linked to this article.\n\n\nDATA AVAILABILITY\n\nThe data comes from medical records and imaging from Stanford Healthcare and is not publicly available. The de-identified data is available from the authors upon reasonable request and with permission of the institutional review board.\n\nFig. 1\n1EchoNet machine learning pipeline for outcome prediction. a EchoNet workflow for image selection, cleaning, and model training. b Comparison of model performance with different cardiac views as input. c Examples of data augmentation. The original frame is rotated (left to right) and its intensity is increase (top to bottom) as augmentations.\n\n\nEchoNet predicted systemic phenotypes of age (R 2 = 0.46, MAE = 9.8 year, mean prediction MAE = 13.4 year), sex (AUC = 0.88), weight (R 2 = 0.56, MAE = 10.7 Kg, mean prediction MAE = 15.4 Kg), and height (R 2 = 0.33, MAE = 0.07 m, mean prediction MAE = 0.09 m) with similar performance to previous predictions of cardiac specific features\n\nFig. 2\n2EchoNet performance and interpretation for three clinical interpretations of local structures and features. For each task, representative positive examples are shown side-by-side with regions of interest from the respective model. Shaded areas indicate 95% confidence intervals. A. Ghorbani et al.\n\nFig. 3\n3EchoNet performance and interpretation for ventricular size and function. EchoNet performance for a predicted left ventricular end systolic volume, b predicted end diastolic volume, c calculated ejection fraction from predicted ESV and EDV, and d predicted ejection fraction. e Input image, interpretation, and overlap for ejection fraction model.A. Ghorbani et al.\n\nFig. 4\n4EchoNet performance and interpretation for systemic phenotypes. a EchoNet performance for prediction of four systemic phenotypes (sex, weight, height and age) using apical-4-chamber view images. Shaded areas indicate 95% confidence intervals. b Interpretation of systemic phenotype models with representative positive examples shown side-by-side with regions of interest.\n\nFig. 5\n5Bland-Altman plotsBland-Altman plots of EchoNet performance for regression predictiontasks. The solid black line indicates the median. Orange, red, and blue dashed lines delineate the central 50%, 75%, and 95% of cases based on differences between automated and measured values.A. Ghorbani et al.\n\nTable 1 .\n1Baseline characteristics of patients in the training and test datasets.Characteristics \nComplete data \nA4C view data \n\nTrain data \nTest data \nTrain data \nTest data \n\nNumber of patients \n2850 \n373 \n2546 \n337 \n\nNumber of images \n1,624,780 \n169,880 \n172,080 \n21,540 \n\nSex (% Male) \n52.4% \n52.8% \n52.2% \n53.7% \nAge: mean, years (std) \n61.3 (17.2) \n62.8 (16.8) \n61.1 (17.1) \n63.2 (16.9) \n\nWeight: mean, Kg (std) \n78.8 (22.7) \n78.9 (20.8) \n78.0 (21.7) \n78.5 (20.2) \n\nHeight: mean, m (std) \n1.69 (0.11) \n1.69 (0.11) \n1.69 (0.12) \n1.69 (0.11) \n\nBMI: mean (std) \n27.3 (6.7) \n27.5 (6.5) \n27.1 (6.5) \n27.3 (6.1) \n\nPacemaker or defibrillator lead (% Present) \n13.2 \n14.7 \n13.1 \n15.1 \n\nSevere left atrial enlargement (% Present) \n17.2 \n20.3 \n18.0 \n21.9 \n\nLeft ventricular hypertrophy (% Present) \n33.3 \n38.0 \n32.7 \n37.9 \n\nEnd diastolic volume, mL: mean (std) \n94.3 (47.2) \n94.6 (13.0) \n95.1 (48.2) \n96.9 (48.0) \nEnd systolic volume, mL: mean (std) \n45.6 (38.3) \n46.2 (36.1) \n46.0 (39.3) \n47.0 (36.6) \n\nEjection fraction: mean (std) \n55.2 (12.3) \n54.7 (13.0) \n55.1 (12.2) \n54.8 (13.1) \n\nA. Ghorbani et al. \n\nScripps Research Translational Institutenpj Digital Medicine (2020) 10\nnpj Digital Medicine (2020)10 Scripps Research Translational Institute\n\u00a9 The Author(s) 2020\nACKNOWLEDGEMENTSCODE AVAILABILITYThe code is freely available at https://github.com/amiratag/EchoNet.COMPETING INTERESTSThe authors declare no competing interests.ADDITIONAL INFORMATIONSupplementary information is available for this paper at https://doi.org/10.1038/ s41746-019-0216-8.Correspondence and requests for materials should be addressed to D.O. or J.Y.Z.Reprints and permission information is available at http://www.nature.com/ reprintsPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons. org/licenses/by/4.0/.\nForecasting the future of cardiovascular disease in the united states: a policy statement from the american heart association. P Heidenreich, Circulation. 123Heidenreich, P. et al. Forecasting the future of cardiovascular disease in the united states: a policy statement from the american heart association. Circulation 123, 933-944 (2011).\n\nRacial and ethnic differences in the treatment of acute myocardial infarction: findings from the get with the guidelines-coronary artery disease program. M Cohen, Circulation. 121Cohen, M. et al. Racial and ethnic differences in the treatment of acute myo- cardial infarction: findings from the get with the guidelines-coronary artery dis- ease program. Circulation 121, 2294-2301 (2010).\n\nSocial determinants of risk and outcomes of cardiovascular disease a scientific statement from the american heart association. E Havranek, Circulation. 132Havranek, E. et al. Social determinants of risk and outcomes of cardiovascular disease a scientific statement from the american heart association. Circulation 132, 873-898 (2015).\n\nUS hospital use of echocardiography: Insights from the nationwide inpatient sample. A Madani, J R Ong, A Tiberwal, M R Mofrad, J. Am. Coll. Cardiol. 67Madani, A., Ong, J. R., Tiberwal, A. & Mofrad, M. R. US hospital use of echo- cardiography: Insights from the nationwide inpatient sample. J. Am. Coll. Cardiol. 67, 502-511 (2016).\n\nFully automated echocardiogram interpretation in clinical practice: feasibility and diagnostic accuracy. J Zhang, Circulation. 138Zhang, J. et al. Fully automated echocardiogram interpretation in clinical practice: feasibility and diagnostic accuracy. Circulation 138, 1623-1635 (2018).\n\nDeep echocardiography: dataefficient supervised and semisupervised deep learning towards automated diagnosis of cardiac disease. A Madani, J R Ong, A Tiberwal, M R Mofrad, npj Digital Med. 159Madani, A., Ong, J. R., Tiberwal, A. & Mofrad, M. R. Deep echocardiography: data- efficient supervised and semisupervised deep learning towards automated diagnosis of cardiac disease. npj Digital Med. 1, 59 (2018).\n\nMachine learning and prediction in medicine-beyond the peak of inflated expectations. J H Chen, S M Asch, N. Engl. J. Med. 3762507Chen, J. H. & Asch, S. M. Machine learning and prediction in medicine-beyond the peak of inflated expectations. N. Engl. J. Med. 376, 2507 (2017).\n\nLearning a deep convolutional network for image super-resolution. C Dong, C C Loy, K He, X Tang, European conference on computer vision. SpringerDong, C., Loy, C.C., He, K. & Tang, X. Learning a deep convolutional network for image super-resolution. in European conference on computer vision, 184-199 (Springer, 2014).\n\nImagenet large scale visual recognition challenge. O Russakovsky, Int. j. comp. vis. 115Russakovsky, O. et al. Imagenet large scale visual recognition challenge. Int. j. comp. vis. 115, 211-252 (2015).\n\nInception-v4, inception-resnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, A A Alemi, Thirty-First AAAI Conference on Artificial Intelligence. Szegedy, C., Ioffe, S., Vanhoucke, V. & Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on Artificial Intelligence (AAAI.org, 2017).\n\nLarge-scale video classification with convolutional neural networks. A Karpathy, Proc. of the IEEE conference on Computer Vision and Pattern Recognition. of the IEEE conference on Computer Vision and Pattern RecognitionIEEEKarpathy, A. et al. Large-scale video classification with convolutional neural networks. In Proc. of the IEEE conference on Computer Vision and Pattern Recog- nition, 1725-1732 (IEEE, 2014).\n\nPrediction of cardiovascular risk factors from retinal fundus photographs via deep learning. R Poplin, Nat. Biomed. Eng. 2158Poplin, R. et al. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nat. Biomed. Eng. 2, 158 (2018).\n\nDermatologist-level classification of skin cancer with deep neural networks. A Esteva, Nature. 542115Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115 (2017).\n\nClassification and mutation prediction from non-small cell lung cancer histopathology images using deep learning. N Coudray, Nat. Med. 241559Coudray, N. et al. Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning. Nat. Med. 24, 1559 (2018).\n\nLabelfree prediction of three-dimensional fluorescence images from transmitted-light microscopy. C Ounkomol, S Seshamani, M M Maleckar, F Collman, G R Johnson, Nat. Methods. 15917Ounkomol, C., Seshamani, S., Maleckar, M. M., Collman, F. & Johnson, G. R. Label- free prediction of three-dimensional fluorescence images from transmitted-light microscopy. Nat. Methods 15, 917 (2018).\n\nDevelopment and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. K Nagpal, Digit. Med. 248Nagpal, K. et al. Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. npj Digit. Med. 2, 48 (2019).\n\nAccf/ase/aha/asnc/hfsa/hrs/scai/sccm/scct/scmr 2011 appropriate use criteria for echocardiography. P Douglas, J. Am. Soc. Echocardiogr. 24Douglas, P. et al. Accf/ase/aha/asnc/hfsa/hrs/scai/sccm/scct/scmr 2011 appropriate use criteria for echocardiography. J. Am. Soc. Echocardiogr. 24, 229-267 (2011).\n\nLeft ventricular ejection fraction and volumes: it depends on the imaging method. P W Wood, J B Choy, N C Nanda, H Becher, Echocardiography. 31Wood, P.W., Choy, J.B., Nanda, N.C. & Becher, H. Left ventricular ejection fraction and volumes: it depends on the imaging method. Echocardiography 31, 87-100 (2014).\n\nVariability in echocardiographic measurements of left ventricular function in septic shock patients. D D Geer, A Oscarsson, J Engvall, J. Cardiovasc Ultrasound. 1319Geer, D. D., Oscarsson, A. & Engvall, J. Variability in echocardiographic mea- surements of left ventricular function in septic shock patients. J. Cardiovasc Ultrasound. 13, 19 (2015).\n\nEchocardiographic variables used to estimate pulmonary artery pressure in dogs. A Ja, G.-S Jm, J. Vet. Intern. Med. 31JA, A. & JM, G.-S. Echocardiographic variables used to estimate pulmonary artery pressure in dogs. J. Vet. Intern. Med. 31, 1622-1628 (2017).\n\nACC/AHA/ASE advanced training statement on echocardiography (Revision of the 2003 ACC/AHA Clinical Competence Statement on Echocardiography): a report of the ACC competency management committee. 19ACC/AHA/ASE advanced training statement on echocardiography (Revision of the 2003 ACC/AHA Clinical Competence Statement on Echocardiography): a report of the ACC competency management committee. J. Am. Coll. Cardiol. 19, S0735-S1097 (2019)\n\nSystematic review: prediction of perioperative cardiac complications and mortality by the revised cardiac risk index. F Mk, B Ws, W Dn, Ann. Intern. Med. 152MK, F., WS, B. & DN, W. Systematic review: prediction of perioperative cardiac complications and mortality by the revised cardiac risk index. Ann. Intern. Med. 152, 26-35 (2010).\n\nA population-based study of cardiovascular mortality following early-stage breast cancer. H Abdel-Qadir, JAMA Cardiol. 2Abdel-Qadir, H. et al. A population-based study of cardiovascular mortality fol- lowing early-stage breast cancer. JAMA Cardiol. 2, 88-93 (2017).\n\nFast and accurate view classification of echocardiograms using deep learning. A Madani, R Arnaout, M Mofrad, R Arnaout, npj Digital Med. 16Madani, A., Arnaout, R., Mofrad, M. & Arnaout, R. Fast and accurate view classi- fication of echocardiograms using deep learning. npj Digital Med. 1, 6 (2018).\n\nD Smilkov, N Thorat, B Kim, F Vi\u00e9gas, M Wattenberg, arXiv:1706.03825Smoothgrad: removing noise by adding noise. arXiv preprintSmilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F. & Wattenberg, M. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825 (2017).\n\nGradio: Hassle-free sharing and testing of ml models in the wild. A Abid, Proc. 36th International Conference on Machine Learning. 36th International Conference on Machine Learning72Abid, A. et al. Gradio: Hassle-free sharing and testing of ml models in the wild. in Proc. 36th International Conference on Machine Learning, Vol. 72 (JMLR.org, 2019).\n\nEchocardiographic reference ranges for normal cardiac chamber size: results from the norre study. S Kou, Eur. Heart J. Cardiovasc. Imaging. 15Kou, S. et al. Echocardiographic reference ranges for normal cardiac chamber size: results from the norre study. Eur. Heart J. Cardiovasc. Imaging 15, 680-690 (2014).\n\nSize matters! Impact of age, sex, height, and weight on the normal heart size. S Pfaffenberger, Circ. Cardiovasc. Imaging. 6Pfaffenberger, S. et al. Size matters! Impact of age, sex, height, and weight on the normal heart size. Circ. Cardiovasc. Imaging 6, 1073-1079 (2013).\n\nAge and sex estimation using artificial intelligence from standard 12-lead ecgs. Z Attia, Circ.: Arrhythm. Electrophysiol. 127284Attia, Z. et al. Age and sex estimation using artificial intelligence from standard 12-lead ecgs. Circ.: Arrhythm. Electrophysiol. 12, e007284 (2019).\n\nAssociation of newer diastolic function parameters with age in healthy subjects: a population-based study. V Munagala, J. Am. Soc. Echocardiogr. 16Munagala, V. et al. Association of newer diastolic function parameters with age in healthy subjects: a population-based study. J. Am. Soc. Echocardiogr. 16, 1049-1056 (2003).\n\nLeft atrial volume index in healthy subjects: clinical and echocardiographic correlates. A D&apos;andrea, Echocardiography. 30D'Andrea, A. et al. Left atrial volume index in healthy subjects: clinical and echocardiographic correlates. Echocardiography 30, 1001-1007 (2013).\n\nBody-mass index and mortality among 1.46 million white adults. K Bhaskaran, I Dos Santos Silva, D A Leon, I J Douglas, L Smeeth, N. Engl. J. Med. 363Bhaskaran, K., dos Santos Silva, I., Leon, D. A., Douglas, I. J. & Smeeth, L. Body-mass index and mortality among 1.46 million white adults. N. Engl. J. Med. 363, 2211-2219 (2010).\n\nAssociation of BMI with overall and cause-specific mortality: a population-based cohort study of 3.6 million adults in the UK. A De Gonzalez, B , P , H Jr, C , Lancet Diabetes Endocrinol. 6de Gonzalez A, B., P, H. & JR, C. Association of BMI with overall and cause-specific mortality: a population-based cohort study of 3.6 million adults in the UK. Lancet Diabetes Endocrinol. 6, 944-953 (2018).\n\nAssociation of obesity with mortality over 24 years of weight history findings from the framingham heart study. H Xu, L A Cupples, A Stokes, C T Liu, JAMA Netw. Open. 1184587Xu, H., Cupples, L. A. & Stokes, A., & Liu, C.T. et al. Association of obesity with mortality over 24 years of weight history findings from the framingham heart study. JAMA Netw. Open 1, e184587 (2018).\n\nTransesophageal dobutamine stress echocardiography in the evaluation of myocardial ischemia in morbidly obese subjects. E C Madu, Chest. 117Madu, E. C. Transesophageal dobutamine stress echocardiography in the eva- luation of myocardial ischemia in morbidly obese subjects. Chest. 117, 657-661 (2000).\n\nUse of contrast agents with echocardiography in patients with suboptimal echocardiography. Medical Advisory Secretariat. 10Medical Advisory Secretariat. Use of contrast agents with echocardiography in patients with suboptimal echocardiography. Ont. Health Technol. Assess. Ser. 10, 1-17 (2010).\n\nAortic calcification onset and progression: Association with the development of coronary atherosclerosis. H K\u00e4lsch, J Am Heart Assoc. 65093K\u00e4lsch, H. et al. Aortic calcification onset and progression: Association with the development of coronary atherosclerosis. J Am Heart Assoc. 6, e005093 (2017).\n\nSevere mitral annular calcification: multimodality imaging for therapeutic strategies and interventions. M F Eleid, T A Foley, S M Said, S V Pislaru, C S Rihal, JACC: Cardiovas. Imaging. 9Eleid, M.F., Foley, T.A., Said, S.M., Pislaru, S.V. & Rihal, C.S. Severe mitral annular calcification: multimodality imaging for therapeutic strategies and interventions. JACC: Cardiovas. Imaging 9, 1318-1337 (2016).\n\nLeft atrial volume and geometry in healthy aging: the cardiovascular health study. G Aurigemma, Circ. Cardiovasc. Imaging. 2Aurigemma, G. et al. Left atrial volume and geometry in healthy aging: the cardiovascular health study. Circ. Cardiovasc. Imaging 2, 282-289 (2009).\n\nDeep-learning cardiac motion analysis for human survival prediction. G A Bello, Nat. Mach. Intell. 195Bello, G. A. et al. Deep-learning cardiac motion analysis for human survival pre- diction. Nat. Mach. Intell. 1, 95 (2019).\n\nEnd-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. D Ardila, Nat. Med. 25Ardila, D. et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nat. Med. 25, 954-961 (2019).\n\nTrends in the Use of Echocardiography. Echocardiography Trends. Data Points #20 (prepared by the University of Minnesota DEcIDE Center. B A Virnig, 14-EHC034- EFunder Contract No. HHSA29020100013I). Rockville, MDAHRQ Publication: Agency for Healthcare Research and QualityVirnig, B.A. et al. Trends in the Use of Echocardiography. Echocardiography Trends. Data Points #20 (prepared by the University of Minnesota DEcIDE Center, under Contract No. HHSA29020100013I). Rockville, MD: Agency for Healthcare Research and Quality; May 2014. AHRQ Publication No. 14-EHC034- EF (2007-2011).\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proc. of the IEEE conference on computer vision and pattern recognition. of the IEEE conference on computer vision and pattern recognitionIEEESzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In Proc. of the IEEE conference on computer vision and pattern recognition, 2818-2826 (IEEE, 2016).\n\nTensorflow: A system for large-scale machine learning. M Abadi, 12th {USENIX} Symposium on Operating Systems Design and Implementation. Abadi, M. et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) 265-283 (2016).\n\nAdam: a method for stochastic optimization. D P Kingma, J Ba, 3rd International Conference on Learning Representations. San Diego, CA, USATrack ProceedingsKingma, D.P. & Ba, J. Adam: a method for stochastic optimization. 3rd Interna- tional Conference on Learning Representations, {ICLR} 2015, (San Diego, CA, USA, 2015) Conference Track Proceedings.\n\nA simple weight decay can improve generalization. A Krogh, J A Hertz, Advances in neural information processing systems. Krogh, A. & Hertz, J.A. A simple weight decay can improve generalization. In Advances in neural information processing systems, 950-957 (1992).\n\nRobust estimation of a location parameter. P J Huber, Breakthroughs in statistics. SpringerHuber, P.J. Robust estimation of a location parameter. in Breakthroughs in sta- tistics, 492-518 (Springer, 1992).\n\nFocal loss for dense object detection. T Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, Proc. of the IEEE international conference on computer vision. of the IEEE international conference on computer visionIEEELin, T.Y., Goyal, P., Girshick, R., He, K. & Doll\u00e1r, P. Focal loss for dense object detection. In Proc. of the IEEE international conference on computer vision, 2980-2988 (IEEE, 2017).\n\nThe effectiveness of data augmentation in image classification using deep learning. L Perez, J Wang, arXiv:1712.04621arXiv preprintPerez, L. & Wang, J. The effectiveness of data augmentation in image classifica- tion using deep learning. arXiv preprint arXiv:1712.04621 (2017).\n\nS Lim, I Kim, T Kim, C Kim, S Fast Kim, Autoaugment, Advances in Neural Information Processing Systems. Lim, S., Kim, I., Kim, T., Kim, C. & Kim, S. Fast AutoAugment In Advances in Neural Information Processing Systems, 6662-6672 (2019).\n\nHow to explain individual classification decisions. D Baehrens, Journal of Machine Learning Research. 11Baehrens, D. et al. How to explain individual classification decisions. Journal of Machine Learning Research 11, 1803-1831 (2010).\n\nDeep inside convolutional networks: Visualising image classification models and saliency maps. K Simonyan, A Vedaldi, A Zisserman, arXiv:1312.6034arXiv preprintSimonyan, K., Vedaldi, A. & Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).\n\nOn pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. S Bach, PloS ONE. 10130140Bach, S. et al. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS ONE 10, e0130140 (2015).\n\nLearning important features through propagating activation differences. A Shrikumar, P Greenside, A Kundaje, Proc. of the 34th International Conference on Machine Learning. of the 34th International Conference on Machine LearningJMLR70Shrikumar, A., Greenside, P. & Kundaje, A. Learning important features through propagating activation differences. In Proc. of the 34th International Conference on Machine Learning, vol. 70, 3145-3153 (JMLR, 2017)\n\nAxiomatic attribution for deep networks. M Sundararajan, A Taly, Q Yan, Proc. 34th International Conference on Machine Learning. 34th International Conference on Machine Learning70Sundararajan, M., Taly, A. & Yan, Q. Axiomatic attribution for deep networks. in Proc. 34th International Conference on Machine Learning, Vol. 70, 3319-3328 (JMLR. org, 2017).\n\nInterpretation of neural networks is fragile. A Ghorbani, A Abid, J Zou, Proc. of the AAAI Conference on Artificial Intelligence. of the AAAI Conference on Artificial IntelligenceAAAI.org33Ghorbani, A., Abid, A. & Zou, J. Interpretation of neural networks is fragile. In Proc. of the AAAI Conference on Artificial Intelligence, Vol. 33, 3681-3688 (AAAI.org, 2019).\n\nA Levine, S Singla, S Feizi, arXiv:1905.12105Certifiably robust interpretation in deep learning. arXiv preprintLevine, A., Singla, S. & Feizi, S. Certifiably robust interpretation in deep learning. arXiv preprint arXiv:1905.12105 (2019).\n", "annotations": {"author": "[{\"end\":76,\"start\":59},{\"end\":90,\"start\":77},{\"end\":105,\"start\":91},{\"end\":115,\"start\":106},{\"end\":132,\"start\":116},{\"end\":153,\"start\":133},{\"end\":168,\"start\":154},{\"end\":183,\"start\":169},{\"end\":196,\"start\":184}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":67},{\"end\":89,\"start\":83},{\"end\":104,\"start\":100},{\"end\":114,\"start\":112},{\"end\":131,\"start\":127},{\"end\":152,\"start\":142},{\"end\":167,\"start\":162},{\"end\":182,\"start\":176},{\"end\":195,\"start\":192}]", "author_first_name": "[{\"end\":66,\"start\":59},{\"end\":82,\"start\":77},{\"end\":99,\"start\":91},{\"end\":111,\"start\":106},{\"end\":124,\"start\":116},{\"end\":126,\"start\":125},{\"end\":139,\"start\":133},{\"end\":141,\"start\":140},{\"end\":159,\"start\":154},{\"end\":161,\"start\":160},{\"end\":173,\"start\":169},{\"end\":175,\"start\":174},{\"end\":189,\"start\":184},{\"end\":191,\"start\":190}]", "author_affiliation": null, "title": "[{\"end\":56,\"start\":1},{\"end\":252,\"start\":197}]", "venue": null, "abstract": "[{\"end\":1775,\"start\":283}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2065,\"start\":2064},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2290,\"start\":2288},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2291,\"start\":2290},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2399,\"start\":2397},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2400,\"start\":2399},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2594,\"start\":2592},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2595,\"start\":2594},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2942,\"start\":2941},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3085,\"start\":3082},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3088,\"start\":3085},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3092,\"start\":3088},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3096,\"start\":3092},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3259,\"start\":3256},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3261,\"start\":3259},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3390,\"start\":3386},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3394,\"start\":3390},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3398,\"start\":3394},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3402,\"start\":3398},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3406,\"start\":3402},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3574,\"start\":3572},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3684,\"start\":3681},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3686,\"start\":3684},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3876,\"start\":3873},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3878,\"start\":3876},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4226,\"start\":4224},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4228,\"start\":4226},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4420,\"start\":4418},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4743,\"start\":4739},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4747,\"start\":4743},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4751,\"start\":4747},{\"end\":4978,\"start\":4976},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5290,\"start\":5288},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5293,\"start\":5290},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5295,\"start\":5293},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6163,\"start\":6161},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6165,\"start\":6163},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6167,\"start\":6165},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9582,\"start\":9580},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10045,\"start\":10043},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10385,\"start\":10382},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10387,\"start\":10385},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12700,\"start\":12698},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16588,\"start\":16586},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16764,\"start\":16762},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17068,\"start\":17065},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17070,\"start\":17068},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17758,\"start\":17754},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17762,\"start\":17758},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17766,\"start\":17762},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18258,\"start\":18255},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18260,\"start\":18258},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18965,\"start\":18962},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18967,\"start\":18965},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19136,\"start\":19133},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19138,\"start\":19136},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19998,\"start\":19996},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20000,\"start\":19998},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20389,\"start\":20387},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20631,\"start\":20630},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21670,\"start\":21668},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22501,\"start\":22498},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22503,\"start\":22501},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22558,\"start\":22555},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22561,\"start\":22558},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22563,\"start\":22561},{\"end\":23421,\"start\":23406},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26961,\"start\":26960},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27025,\"start\":27023},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28174,\"start\":28172},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":28373,\"start\":28371},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31519,\"start\":31518},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":33302,\"start\":33300},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33361,\"start\":33359},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":34022,\"start\":34020},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34035,\"start\":34033},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":34060,\"start\":34058},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34142,\"start\":34140},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":34185,\"start\":34183},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39021,\"start\":39019}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36104,\"start\":35752},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36445,\"start\":36105},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36752,\"start\":36446},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37127,\"start\":36753},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37508,\"start\":37128},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37814,\"start\":37509},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38920,\"start\":37815}]", "paragraph": "[{\"end\":2779,\"start\":1777},{\"end\":6034,\"start\":2781},{\"end\":7343,\"start\":6036},{\"end\":8175,\"start\":7355},{\"end\":12268,\"start\":8177},{\"end\":13242,\"start\":12270},{\"end\":13877,\"start\":13244},{\"end\":16010,\"start\":13892},{\"end\":19686,\"start\":16012},{\"end\":21468,\"start\":19688},{\"end\":22362,\"start\":21470},{\"end\":24342,\"start\":22364},{\"end\":25002,\"start\":24344},{\"end\":26621,\"start\":25022},{\"end\":27174,\"start\":26631},{\"end\":27788,\"start\":27176},{\"end\":29332,\"start\":27790},{\"end\":30765,\"start\":29354},{\"end\":32476,\"start\":30792},{\"end\":33268,\"start\":32497},{\"end\":34548,\"start\":33270},{\"end\":35170,\"start\":34596},{\"end\":35356,\"start\":35172},{\"end\":35494,\"start\":35378},{\"end\":35751,\"start\":35516}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7714,\"start\":7705},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29219,\"start\":29210}]", "section_header": "[{\"end\":7353,\"start\":7346},{\"end\":13890,\"start\":13880},{\"end\":25020,\"start\":25005},{\"end\":26629,\"start\":26624},{\"end\":29352,\"start\":29335},{\"end\":30790,\"start\":30768},{\"end\":32495,\"start\":32479},{\"end\":34594,\"start\":34551},{\"end\":35376,\"start\":35359},{\"end\":35514,\"start\":35497},{\"end\":35759,\"start\":35753},{\"end\":36453,\"start\":36447},{\"end\":36760,\"start\":36754},{\"end\":37135,\"start\":37129},{\"end\":37516,\"start\":37510},{\"end\":37825,\"start\":37816}]", "table": "[{\"end\":38920,\"start\":37898}]", "figure_caption": "[{\"end\":36104,\"start\":35761},{\"end\":36445,\"start\":36107},{\"end\":36752,\"start\":36455},{\"end\":37127,\"start\":36762},{\"end\":37508,\"start\":37137},{\"end\":37814,\"start\":37518},{\"end\":37898,\"start\":37827}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7564,\"start\":7557},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8767,\"start\":8759},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10082,\"start\":10076},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10846,\"start\":10839},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11437,\"start\":11430},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11503,\"start\":11494},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12710,\"start\":12701},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13207,\"start\":13201},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13607,\"start\":13598},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30225,\"start\":30216},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31321,\"start\":31314}]", "bib_author_first_name": "[{\"end\":40646,\"start\":40645},{\"end\":41015,\"start\":41014},{\"end\":41378,\"start\":41377},{\"end\":41671,\"start\":41670},{\"end\":41681,\"start\":41680},{\"end\":41683,\"start\":41682},{\"end\":41690,\"start\":41689},{\"end\":41702,\"start\":41701},{\"end\":41704,\"start\":41703},{\"end\":42025,\"start\":42024},{\"end\":42337,\"start\":42336},{\"end\":42347,\"start\":42346},{\"end\":42349,\"start\":42348},{\"end\":42356,\"start\":42355},{\"end\":42368,\"start\":42367},{\"end\":42370,\"start\":42369},{\"end\":42702,\"start\":42701},{\"end\":42704,\"start\":42703},{\"end\":42712,\"start\":42711},{\"end\":42714,\"start\":42713},{\"end\":42960,\"start\":42959},{\"end\":42968,\"start\":42967},{\"end\":42970,\"start\":42969},{\"end\":42977,\"start\":42976},{\"end\":42983,\"start\":42982},{\"end\":43265,\"start\":43264},{\"end\":43500,\"start\":43499},{\"end\":43511,\"start\":43510},{\"end\":43520,\"start\":43519},{\"end\":43533,\"start\":43532},{\"end\":43535,\"start\":43534},{\"end\":43884,\"start\":43883},{\"end\":44323,\"start\":44322},{\"end\":44577,\"start\":44576},{\"end\":44835,\"start\":44834},{\"end\":45120,\"start\":45119},{\"end\":45132,\"start\":45131},{\"end\":45145,\"start\":45144},{\"end\":45147,\"start\":45146},{\"end\":45159,\"start\":45158},{\"end\":45170,\"start\":45169},{\"end\":45172,\"start\":45171},{\"end\":45512,\"start\":45511},{\"end\":45791,\"start\":45790},{\"end\":46077,\"start\":46076},{\"end\":46079,\"start\":46078},{\"end\":46087,\"start\":46086},{\"end\":46089,\"start\":46088},{\"end\":46097,\"start\":46096},{\"end\":46099,\"start\":46098},{\"end\":46108,\"start\":46107},{\"end\":46407,\"start\":46406},{\"end\":46409,\"start\":46408},{\"end\":46417,\"start\":46416},{\"end\":46430,\"start\":46429},{\"end\":46737,\"start\":46736},{\"end\":46746,\"start\":46742},{\"end\":47474,\"start\":47473},{\"end\":47480,\"start\":47479},{\"end\":47486,\"start\":47485},{\"end\":47783,\"start\":47782},{\"end\":48038,\"start\":48037},{\"end\":48048,\"start\":48047},{\"end\":48059,\"start\":48058},{\"end\":48069,\"start\":48068},{\"end\":48260,\"start\":48259},{\"end\":48271,\"start\":48270},{\"end\":48281,\"start\":48280},{\"end\":48288,\"start\":48287},{\"end\":48298,\"start\":48297},{\"end\":48599,\"start\":48598},{\"end\":48982,\"start\":48981},{\"end\":49273,\"start\":49272},{\"end\":49551,\"start\":49550},{\"end\":49858,\"start\":49857},{\"end\":50163,\"start\":50162},{\"end\":50412,\"start\":50411},{\"end\":50425,\"start\":50424},{\"end\":50445,\"start\":50444},{\"end\":50447,\"start\":50446},{\"end\":50455,\"start\":50454},{\"end\":50457,\"start\":50456},{\"end\":50468,\"start\":50467},{\"end\":50807,\"start\":50806},{\"end\":50822,\"start\":50821},{\"end\":50826,\"start\":50825},{\"end\":50830,\"start\":50829},{\"end\":50836,\"start\":50835},{\"end\":51190,\"start\":51189},{\"end\":51196,\"start\":51195},{\"end\":51198,\"start\":51197},{\"end\":51209,\"start\":51208},{\"end\":51219,\"start\":51218},{\"end\":51221,\"start\":51220},{\"end\":51576,\"start\":51575},{\"end\":51578,\"start\":51577},{\"end\":52161,\"start\":52160},{\"end\":52461,\"start\":52460},{\"end\":52463,\"start\":52462},{\"end\":52472,\"start\":52471},{\"end\":52474,\"start\":52473},{\"end\":52483,\"start\":52482},{\"end\":52485,\"start\":52484},{\"end\":52493,\"start\":52492},{\"end\":52495,\"start\":52494},{\"end\":52506,\"start\":52505},{\"end\":52508,\"start\":52507},{\"end\":52845,\"start\":52844},{\"end\":53105,\"start\":53104},{\"end\":53107,\"start\":53106},{\"end\":53372,\"start\":53371},{\"end\":53688,\"start\":53687},{\"end\":53690,\"start\":53689},{\"end\":54195,\"start\":54194},{\"end\":54206,\"start\":54205},{\"end\":54219,\"start\":54218},{\"end\":54228,\"start\":54227},{\"end\":54238,\"start\":54237},{\"end\":54666,\"start\":54665},{\"end\":54966,\"start\":54965},{\"end\":54968,\"start\":54967},{\"end\":54978,\"start\":54977},{\"end\":55324,\"start\":55323},{\"end\":55333,\"start\":55332},{\"end\":55335,\"start\":55334},{\"end\":55583,\"start\":55582},{\"end\":55585,\"start\":55584},{\"end\":55786,\"start\":55785},{\"end\":55788,\"start\":55787},{\"end\":55795,\"start\":55794},{\"end\":55804,\"start\":55803},{\"end\":55816,\"start\":55815},{\"end\":55822,\"start\":55821},{\"end\":56224,\"start\":56223},{\"end\":56233,\"start\":56232},{\"end\":56419,\"start\":56418},{\"end\":56426,\"start\":56425},{\"end\":56433,\"start\":56432},{\"end\":56440,\"start\":56439},{\"end\":56447,\"start\":56446},{\"end\":56452,\"start\":56448},{\"end\":56710,\"start\":56709},{\"end\":56989,\"start\":56988},{\"end\":57001,\"start\":57000},{\"end\":57012,\"start\":57011},{\"end\":57331,\"start\":57330},{\"end\":57576,\"start\":57575},{\"end\":57589,\"start\":57588},{\"end\":57602,\"start\":57601},{\"end\":57995,\"start\":57994},{\"end\":58011,\"start\":58010},{\"end\":58019,\"start\":58018},{\"end\":58357,\"start\":58356},{\"end\":58369,\"start\":58368},{\"end\":58377,\"start\":58376},{\"end\":58677,\"start\":58676},{\"end\":58687,\"start\":58686},{\"end\":58697,\"start\":58696}]", "bib_author_last_name": "[{\"end\":40658,\"start\":40647},{\"end\":41021,\"start\":41016},{\"end\":41387,\"start\":41379},{\"end\":41678,\"start\":41672},{\"end\":41687,\"start\":41684},{\"end\":41699,\"start\":41691},{\"end\":41711,\"start\":41705},{\"end\":42031,\"start\":42026},{\"end\":42344,\"start\":42338},{\"end\":42353,\"start\":42350},{\"end\":42365,\"start\":42357},{\"end\":42377,\"start\":42371},{\"end\":42709,\"start\":42705},{\"end\":42719,\"start\":42715},{\"end\":42965,\"start\":42961},{\"end\":42974,\"start\":42971},{\"end\":42980,\"start\":42978},{\"end\":42988,\"start\":42984},{\"end\":43277,\"start\":43266},{\"end\":43508,\"start\":43501},{\"end\":43517,\"start\":43512},{\"end\":43530,\"start\":43521},{\"end\":43541,\"start\":43536},{\"end\":43893,\"start\":43885},{\"end\":44330,\"start\":44324},{\"end\":44584,\"start\":44578},{\"end\":44843,\"start\":44836},{\"end\":45129,\"start\":45121},{\"end\":45142,\"start\":45133},{\"end\":45156,\"start\":45148},{\"end\":45167,\"start\":45160},{\"end\":45180,\"start\":45173},{\"end\":45519,\"start\":45513},{\"end\":45799,\"start\":45792},{\"end\":46084,\"start\":46080},{\"end\":46094,\"start\":46090},{\"end\":46105,\"start\":46100},{\"end\":46115,\"start\":46109},{\"end\":46414,\"start\":46410},{\"end\":46427,\"start\":46418},{\"end\":46438,\"start\":46431},{\"end\":46740,\"start\":46738},{\"end\":46749,\"start\":46747},{\"end\":47477,\"start\":47475},{\"end\":47483,\"start\":47481},{\"end\":47489,\"start\":47487},{\"end\":47795,\"start\":47784},{\"end\":48045,\"start\":48039},{\"end\":48056,\"start\":48049},{\"end\":48066,\"start\":48060},{\"end\":48077,\"start\":48070},{\"end\":48268,\"start\":48261},{\"end\":48278,\"start\":48272},{\"end\":48285,\"start\":48282},{\"end\":48295,\"start\":48289},{\"end\":48309,\"start\":48299},{\"end\":48604,\"start\":48600},{\"end\":48986,\"start\":48983},{\"end\":49287,\"start\":49274},{\"end\":49557,\"start\":49552},{\"end\":49867,\"start\":49859},{\"end\":50177,\"start\":50164},{\"end\":50422,\"start\":50413},{\"end\":50442,\"start\":50426},{\"end\":50452,\"start\":50448},{\"end\":50465,\"start\":50458},{\"end\":50475,\"start\":50469},{\"end\":50819,\"start\":50808},{\"end\":50833,\"start\":50831},{\"end\":51193,\"start\":51191},{\"end\":51206,\"start\":51199},{\"end\":51216,\"start\":51210},{\"end\":51225,\"start\":51222},{\"end\":51583,\"start\":51579},{\"end\":52168,\"start\":52162},{\"end\":52469,\"start\":52464},{\"end\":52480,\"start\":52475},{\"end\":52490,\"start\":52486},{\"end\":52503,\"start\":52496},{\"end\":52514,\"start\":52509},{\"end\":52855,\"start\":52846},{\"end\":53113,\"start\":53108},{\"end\":53379,\"start\":53373},{\"end\":53697,\"start\":53691},{\"end\":54203,\"start\":54196},{\"end\":54216,\"start\":54207},{\"end\":54225,\"start\":54220},{\"end\":54235,\"start\":54229},{\"end\":54244,\"start\":54239},{\"end\":54672,\"start\":54667},{\"end\":54975,\"start\":54969},{\"end\":54981,\"start\":54979},{\"end\":55330,\"start\":55325},{\"end\":55341,\"start\":55336},{\"end\":55591,\"start\":55586},{\"end\":55792,\"start\":55789},{\"end\":55801,\"start\":55796},{\"end\":55813,\"start\":55805},{\"end\":55819,\"start\":55817},{\"end\":55829,\"start\":55823},{\"end\":56230,\"start\":56225},{\"end\":56238,\"start\":56234},{\"end\":56423,\"start\":56420},{\"end\":56430,\"start\":56427},{\"end\":56437,\"start\":56434},{\"end\":56444,\"start\":56441},{\"end\":56456,\"start\":56453},{\"end\":56469,\"start\":56458},{\"end\":56719,\"start\":56711},{\"end\":56998,\"start\":56990},{\"end\":57009,\"start\":57002},{\"end\":57022,\"start\":57013},{\"end\":57336,\"start\":57332},{\"end\":57586,\"start\":57577},{\"end\":57599,\"start\":57590},{\"end\":57610,\"start\":57603},{\"end\":58008,\"start\":57996},{\"end\":58016,\"start\":58012},{\"end\":58023,\"start\":58020},{\"end\":58366,\"start\":58358},{\"end\":58374,\"start\":58370},{\"end\":58381,\"start\":58378},{\"end\":58684,\"start\":58678},{\"end\":58694,\"start\":58688},{\"end\":58703,\"start\":58698}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":45832842},\"end\":40858,\"start\":40518},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":35725460},\"end\":41248,\"start\":40860},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8548491},\"end\":41584,\"start\":41250},{\"attributes\":{\"id\":\"b3\"},\"end\":41917,\"start\":41586},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":53027436},\"end\":42205,\"start\":41919},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53086650},\"end\":42613,\"start\":42207},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10646588},\"end\":42891,\"start\":42615},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":18874645},\"end\":43211,\"start\":42893},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2930547},\"end\":43414,\"start\":43213},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1023605},\"end\":43812,\"start\":43416},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206592218},\"end\":44227,\"start\":43814},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3386122},\"end\":44497,\"start\":44229},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3767412},\"end\":44718,\"start\":44499},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52289081},\"end\":45020,\"start\":44720},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":196632954},\"end\":45403,\"start\":45022},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53435585},\"end\":45689,\"start\":45405},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":205540616},\"end\":45992,\"start\":45691},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17808353},\"end\":46303,\"start\":45994},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1917000},\"end\":46654,\"start\":46305},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206166906},\"end\":46915,\"start\":46656},{\"attributes\":{\"id\":\"b20\"},\"end\":47353,\"start\":46917},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207535859},\"end\":47690,\"start\":47355},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5083680},\"end\":47957,\"start\":47692},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3852220},\"end\":48257,\"start\":47959},{\"attributes\":{\"doi\":\"arXiv:1706.03825\",\"id\":\"b24\"},\"end\":48530,\"start\":48259},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":174802423},\"end\":48881,\"start\":48532},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":36508894},\"end\":49191,\"start\":48883},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":207608622},\"end\":49467,\"start\":49193},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":201730453},\"end\":49748,\"start\":49469},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":25610595},\"end\":50071,\"start\":49750},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":86499553},\"end\":50346,\"start\":50073},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":75504915},\"end\":50677,\"start\":50348},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53269968},\"end\":51075,\"start\":50679},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":58064685},\"end\":51453,\"start\":51077},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2971140},\"end\":51756,\"start\":51455},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1090738},\"end\":52052,\"start\":51758},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6561573},\"end\":52353,\"start\":52054},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":205853631},\"end\":52759,\"start\":52355},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2278924},\"end\":53033,\"start\":52761},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52936872},\"end\":53260,\"start\":53035},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":159041422},\"end\":53549,\"start\":53262},{\"attributes\":{\"doi\":\"14-EHC034- EF\",\"id\":\"b41\"},\"end\":54133,\"start\":53551},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206593880},\"end\":54608,\"start\":54135},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6287870},\"end\":54919,\"start\":54610},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6628106},\"end\":55271,\"start\":54921},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10137788},\"end\":55537,\"start\":55273},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":61846277},\"end\":55744,\"start\":55539},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":47252984},\"end\":56137,\"start\":55746},{\"attributes\":{\"doi\":\"arXiv:1712.04621\",\"id\":\"b48\"},\"end\":56416,\"start\":56139},{\"attributes\":{\"id\":\"b49\"},\"end\":56655,\"start\":56418},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14664111},\"end\":56891,\"start\":56657},{\"attributes\":{\"doi\":\"arXiv:1312.6034\",\"id\":\"b51\"},\"end\":57228,\"start\":56893},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":9327892},\"end\":57501,\"start\":57230},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3385018},\"end\":57951,\"start\":57503},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":16747630},\"end\":58308,\"start\":57953},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":22172746},\"end\":58674,\"start\":58310},{\"attributes\":{\"doi\":\"arXiv:1905.12105\",\"id\":\"b56\"},\"end\":58913,\"start\":58676}]", "bib_title": "[{\"end\":40643,\"start\":40518},{\"end\":41012,\"start\":40860},{\"end\":41375,\"start\":41250},{\"end\":41668,\"start\":41586},{\"end\":42022,\"start\":41919},{\"end\":42334,\"start\":42207},{\"end\":42699,\"start\":42615},{\"end\":42957,\"start\":42893},{\"end\":43262,\"start\":43213},{\"end\":43497,\"start\":43416},{\"end\":43881,\"start\":43814},{\"end\":44320,\"start\":44229},{\"end\":44574,\"start\":44499},{\"end\":44832,\"start\":44720},{\"end\":45117,\"start\":45022},{\"end\":45509,\"start\":45405},{\"end\":45788,\"start\":45691},{\"end\":46074,\"start\":45994},{\"end\":46404,\"start\":46305},{\"end\":46734,\"start\":46656},{\"end\":47471,\"start\":47355},{\"end\":47780,\"start\":47692},{\"end\":48035,\"start\":47959},{\"end\":48596,\"start\":48532},{\"end\":48979,\"start\":48883},{\"end\":49270,\"start\":49193},{\"end\":49548,\"start\":49469},{\"end\":49855,\"start\":49750},{\"end\":50160,\"start\":50073},{\"end\":50409,\"start\":50348},{\"end\":50804,\"start\":50679},{\"end\":51187,\"start\":51077},{\"end\":51573,\"start\":51455},{\"end\":51847,\"start\":51758},{\"end\":52158,\"start\":52054},{\"end\":52458,\"start\":52355},{\"end\":52842,\"start\":52761},{\"end\":53102,\"start\":53035},{\"end\":53369,\"start\":53262},{\"end\":53685,\"start\":53551},{\"end\":54192,\"start\":54135},{\"end\":54663,\"start\":54610},{\"end\":54963,\"start\":54921},{\"end\":55321,\"start\":55273},{\"end\":55580,\"start\":55539},{\"end\":55783,\"start\":55746},{\"end\":56707,\"start\":56657},{\"end\":57328,\"start\":57230},{\"end\":57573,\"start\":57503},{\"end\":57992,\"start\":57953},{\"end\":58354,\"start\":58310}]", "bib_author": "[{\"end\":40660,\"start\":40645},{\"end\":41023,\"start\":41014},{\"end\":41389,\"start\":41377},{\"end\":41680,\"start\":41670},{\"end\":41689,\"start\":41680},{\"end\":41701,\"start\":41689},{\"end\":41713,\"start\":41701},{\"end\":42033,\"start\":42024},{\"end\":42346,\"start\":42336},{\"end\":42355,\"start\":42346},{\"end\":42367,\"start\":42355},{\"end\":42379,\"start\":42367},{\"end\":42711,\"start\":42701},{\"end\":42721,\"start\":42711},{\"end\":42967,\"start\":42959},{\"end\":42976,\"start\":42967},{\"end\":42982,\"start\":42976},{\"end\":42990,\"start\":42982},{\"end\":43279,\"start\":43264},{\"end\":43510,\"start\":43499},{\"end\":43519,\"start\":43510},{\"end\":43532,\"start\":43519},{\"end\":43543,\"start\":43532},{\"end\":43895,\"start\":43883},{\"end\":44332,\"start\":44322},{\"end\":44586,\"start\":44576},{\"end\":44845,\"start\":44834},{\"end\":45131,\"start\":45119},{\"end\":45144,\"start\":45131},{\"end\":45158,\"start\":45144},{\"end\":45169,\"start\":45158},{\"end\":45182,\"start\":45169},{\"end\":45521,\"start\":45511},{\"end\":45801,\"start\":45790},{\"end\":46086,\"start\":46076},{\"end\":46096,\"start\":46086},{\"end\":46107,\"start\":46096},{\"end\":46117,\"start\":46107},{\"end\":46416,\"start\":46406},{\"end\":46429,\"start\":46416},{\"end\":46440,\"start\":46429},{\"end\":46742,\"start\":46736},{\"end\":46751,\"start\":46742},{\"end\":47479,\"start\":47473},{\"end\":47485,\"start\":47479},{\"end\":47491,\"start\":47485},{\"end\":47797,\"start\":47782},{\"end\":48047,\"start\":48037},{\"end\":48058,\"start\":48047},{\"end\":48068,\"start\":48058},{\"end\":48079,\"start\":48068},{\"end\":48270,\"start\":48259},{\"end\":48280,\"start\":48270},{\"end\":48287,\"start\":48280},{\"end\":48297,\"start\":48287},{\"end\":48311,\"start\":48297},{\"end\":48606,\"start\":48598},{\"end\":48988,\"start\":48981},{\"end\":49289,\"start\":49272},{\"end\":49559,\"start\":49550},{\"end\":49869,\"start\":49857},{\"end\":50179,\"start\":50162},{\"end\":50424,\"start\":50411},{\"end\":50444,\"start\":50424},{\"end\":50454,\"start\":50444},{\"end\":50467,\"start\":50454},{\"end\":50477,\"start\":50467},{\"end\":50821,\"start\":50806},{\"end\":50825,\"start\":50821},{\"end\":50829,\"start\":50825},{\"end\":50835,\"start\":50829},{\"end\":50839,\"start\":50835},{\"end\":51195,\"start\":51189},{\"end\":51208,\"start\":51195},{\"end\":51218,\"start\":51208},{\"end\":51227,\"start\":51218},{\"end\":51585,\"start\":51575},{\"end\":52170,\"start\":52160},{\"end\":52471,\"start\":52460},{\"end\":52482,\"start\":52471},{\"end\":52492,\"start\":52482},{\"end\":52505,\"start\":52492},{\"end\":52516,\"start\":52505},{\"end\":52857,\"start\":52844},{\"end\":53115,\"start\":53104},{\"end\":53381,\"start\":53371},{\"end\":53699,\"start\":53687},{\"end\":54205,\"start\":54194},{\"end\":54218,\"start\":54205},{\"end\":54227,\"start\":54218},{\"end\":54237,\"start\":54227},{\"end\":54246,\"start\":54237},{\"end\":54674,\"start\":54665},{\"end\":54977,\"start\":54965},{\"end\":54983,\"start\":54977},{\"end\":55332,\"start\":55323},{\"end\":55343,\"start\":55332},{\"end\":55593,\"start\":55582},{\"end\":55794,\"start\":55785},{\"end\":55803,\"start\":55794},{\"end\":55815,\"start\":55803},{\"end\":55821,\"start\":55815},{\"end\":55831,\"start\":55821},{\"end\":56232,\"start\":56223},{\"end\":56240,\"start\":56232},{\"end\":56425,\"start\":56418},{\"end\":56432,\"start\":56425},{\"end\":56439,\"start\":56432},{\"end\":56446,\"start\":56439},{\"end\":56458,\"start\":56446},{\"end\":56471,\"start\":56458},{\"end\":56721,\"start\":56709},{\"end\":57000,\"start\":56988},{\"end\":57011,\"start\":57000},{\"end\":57024,\"start\":57011},{\"end\":57338,\"start\":57330},{\"end\":57588,\"start\":57575},{\"end\":57601,\"start\":57588},{\"end\":57612,\"start\":57601},{\"end\":58010,\"start\":57994},{\"end\":58018,\"start\":58010},{\"end\":58025,\"start\":58018},{\"end\":58368,\"start\":58356},{\"end\":58376,\"start\":58368},{\"end\":58383,\"start\":58376},{\"end\":58686,\"start\":58676},{\"end\":58696,\"start\":58686},{\"end\":58705,\"start\":58696}]", "bib_venue": "[{\"end\":44033,\"start\":43968},{\"end\":48712,\"start\":48663},{\"end\":53763,\"start\":53750},{\"end\":54384,\"start\":54319},{\"end\":55059,\"start\":55041},{\"end\":55949,\"start\":55894},{\"end\":57732,\"start\":57676},{\"end\":58131,\"start\":58082},{\"end\":58489,\"start\":58440},{\"end\":40671,\"start\":40660},{\"end\":41034,\"start\":41023},{\"end\":41400,\"start\":41389},{\"end\":41733,\"start\":41713},{\"end\":42044,\"start\":42033},{\"end\":42394,\"start\":42379},{\"end\":42736,\"start\":42721},{\"end\":43028,\"start\":42990},{\"end\":43296,\"start\":43279},{\"end\":43598,\"start\":43543},{\"end\":43966,\"start\":43895},{\"end\":44348,\"start\":44332},{\"end\":44592,\"start\":44586},{\"end\":44853,\"start\":44845},{\"end\":45194,\"start\":45182},{\"end\":45531,\"start\":45521},{\"end\":45825,\"start\":45801},{\"end\":46133,\"start\":46117},{\"end\":46464,\"start\":46440},{\"end\":46770,\"start\":46751},{\"end\":47110,\"start\":46917},{\"end\":47507,\"start\":47491},{\"end\":47809,\"start\":47797},{\"end\":48094,\"start\":48079},{\"end\":48369,\"start\":48327},{\"end\":48661,\"start\":48606},{\"end\":49021,\"start\":48988},{\"end\":49314,\"start\":49289},{\"end\":49590,\"start\":49559},{\"end\":49893,\"start\":49869},{\"end\":50195,\"start\":50179},{\"end\":50492,\"start\":50477},{\"end\":50865,\"start\":50839},{\"end\":51242,\"start\":51227},{\"end\":51590,\"start\":51585},{\"end\":51877,\"start\":51849},{\"end\":52186,\"start\":52170},{\"end\":52540,\"start\":52516},{\"end\":52882,\"start\":52857},{\"end\":53132,\"start\":53115},{\"end\":53389,\"start\":53381},{\"end\":53748,\"start\":53712},{\"end\":54317,\"start\":54246},{\"end\":54744,\"start\":54674},{\"end\":55039,\"start\":54983},{\"end\":55392,\"start\":55343},{\"end\":55620,\"start\":55593},{\"end\":55892,\"start\":55831},{\"end\":56221,\"start\":56139},{\"end\":56520,\"start\":56471},{\"end\":56757,\"start\":56721},{\"end\":56986,\"start\":56893},{\"end\":57346,\"start\":57338},{\"end\":57674,\"start\":57612},{\"end\":58080,\"start\":58025},{\"end\":58438,\"start\":58383},{\"end\":58771,\"start\":58721}]"}}}, "year": 2023, "month": 12, "day": 17}