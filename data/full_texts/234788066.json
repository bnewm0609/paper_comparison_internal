{"id": 234788066, "updated": "2022-01-12 17:43:04.049", "metadata": {"title": "KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild", "authors": "[{\"middle\":[],\"last\":\"G\u00f6tz-Hahn\",\"first\":\"Franz\"},{\"middle\":[],\"last\":\"Hosu\",\"first\":\"Vlad\"},{\"middle\":[],\"last\":\"Lin\",\"first\":\"Hanhe\"},{\"middle\":[],\"last\":\"Saupe\",\"first\":\"Dietmar\"}]", "venue": "IEEE Access", "journal": "IEEE Access", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/access/Gotz-HahnHLS21", "doi": "10.1109/access.2021.3077642"}}, "content": {"source": {"pdf_hash": "0518d5abe3926ca8d6bd097c04c246e69f7d2da5", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "https://doi.org/10.1109/access.2021.3077642", "status": "GOLD"}}, "grobid": {"id": "c2b33e3b12322421ba4c437fbbacbdbb5355bc5a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0518d5abe3926ca8d6bd097c04c246e69f7d2da5.txt", "contents": "\nSPECIAL SECTION ON DEEP LEARNING TECHNOLOGIES FOR INTERNET OF VIDEO THINGS KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild\n\n\nFranz G\u00f6tz-Hahn \nDepartment of Computer Science\nUniversity of Konstanz\n78464KonstanzGermany\n\nVlad Hosu \nDepartment of Computer Science\nUniversity of Konstanz\n78464KonstanzGermany\n\nANDHanhe Lin \nDepartment of Computer Science\nUniversity of Konstanz\n78464KonstanzGermany\n\nDietmar Saupe \nDepartment of Computer Science\nUniversity of Konstanz\n78464KonstanzGermany\n\nFranz G\u00f6tz-Hahn \nDepartment of Computer Science\nUniversity of Konstanz\n78464KonstanzGermany\n\nSPECIAL SECTION ON DEEP LEARNING TECHNOLOGIES FOR INTERNET OF VIDEO THINGS KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild\n10.1109/ACCESS.2021.3077642Received March 5, 2021, accepted April 12, 2021, date of publication May 5, 2021, date of current version May 20, 2021.Corresponding author: This work was supported by the Deutsche Forschungsgemeinschaft (DFG), German Research Foundation through the TRR 161 (Project A05) under Project-ID 251654672.\nVideo quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deepfeatures (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.INDEX TERMS Datasets, deep transfer learning, multi-level spatially-pooled features, video quality assessment, video quality dataset.\n\nI. INTRODUCTION\n\nVideos have become a central medium for business marketing [1], with over 81% of businesses using video as a marketing tool. Additionally, over 40% of businesses have adopted live video formats such as Facebook Live for marketing and user connection purposes [2]. For consumers, video is the primary source of media entertainment; for example the average US consumer spends 38 hours per week watching video content [3] and it is projected that online videos will make up more than 82% of all consumer internet traffic by 2022 [4]. Streaming platforms such as YouTube report that more than a billion hours of video are watched every day [5]. The success of online videos is due in part to the consumer belief that traditional TV offers an inferior quality [3]. Additionally, increased accessibility to video content The associate editor coordinating the review of this manuscript and approving it for publication was Zhang Lu. acquisition hardware, as well as improvements in overall image quality, are a central aspect in smartphone technology advancement. Similarly, user-generated content is produced at an increasing rate, but the resulting videos often suffer from quality defects.\n\nTherefore, a wide range of video producers and consumers should be able to get automated feedback on video quality. For example, user-generated video distribution platforms like YouTube or Vimeo may want to analyze new videos according to quality to separate professional from the amateur video content, instead of only indexing by video playback resolution. Additionally, with an automated video quality assessment (VQA) system, video streaming services can adjust video encoding parameters to minimize bandwidth requirements while ensuring the delivery of satisfactory video quality.\n\nA critical emerging challenge for VQA is to handle ecologically valid in-the-wild videos. In environmental psychology, VOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ ecological validity is defined as ''the applicability of the results of laboratory analogues to non-laboratory, real life settings'' [6]. In our case the term can be understood as a measure for the extent to which the data represented in a dataset can be generalized to data that would be naturally encountered in the use of a technology. Concretely, this would refer to the types and degree of distortions in visual media contents of internet videos, such as those consumed on YouTube, Flickr, or Vimeo. The term in-the-wild refers to datasets that are ''not constructed and designed with research questions in mind'' [7]. In the case of VQA this would mean datasets that are not recorded or altered with a specific research purpose in mind, such as artificially distorting videos at variable degrees. It comes as no surprise that no-reference VQA (NR-VQA), in particular, has been a field of intensive research in the past few years achieving significant performance gains [8]- [19]. However, state-of-the-art NR-VQA algorithms perform worse on in-the-wild videos than on synthetically distorted ones. These methods aggregate individual video frame quality characteristics that are engineered for specific purposes, such as detecting particular compression artifacts. Often, these features are a balance between precision and computational efficiency. Furthermore, since there is a lack of large-scale in-the-wild video quality datasets with authentic distortions, a thorough evaluation of NR-VQA methods is difficult. Most existing databases are intended as benchmarks for the detection of those specific artificial distortions that NR-VQA algorithms have classically been designed to detect.\n\nGiven the previous challenges, our first contribution is the creation of a large ecologically valid dataset, KonVid-150k. Similar to the dataset KoNViD-1k [20], the ecological validity of KonVid-150k stems from its size, content diversity, as well as naturally occurring, and thus representative degradations. However, being two orders of magnitude larger than existing datasets, it poses new challenges to VQA methods, requiring to train across a vast amount of content and a wide span of authentic distortions. Moreover, since a fixed budget usually constrains the development of a dataset, we needed to ensure a minimum level of annotation quality. Therefore, a part of KonVid-150k consists of 153,841 five seconds long videos that are annotated by five subjective opinions each. This set, from here on called KonVid-150k-A, is over 125 times larger than existing VQA datasets in terms of number of videos and with close to one million subjective ratings over eight times larger in number of annotations [20]- [23]. The dataset is accompanied by a benchmark set of nearly 1,600 videos (KonVid-150k-B) from the same source with a minimum of 89 opinion scores each. This presents a unique opportunity to analyze the trade-off between the number of training videos and the annotation noise/precision, in terms of the performance on the KonVid-150k-B benchmark dataset.\n\nThis new dataset exacerbates two problems of classical NR-VQA methods. First, the computational costs of hand-crafted feature-based approaches are increased through the sheer number of videos. Second, since hand-crafted features handle in-the-wild videos worse than conventional databases, this dataset is very challenging for classical NR-VQA methods. An alternative to hand-crafted features comes with the rise of deep convolutional neural networks (DCNNs), where stacked layers of increasingly complex feature detectors are learned directly from observations of input images. These features are often relatively generic and have been proven to transfer well to similar tasks that are not too different from the source domain [24], [25]. This suggests considering a DCNN as a feature extractor with a benefit over hand-crafted features in that the features are entirely learned from data.\n\nAs a second contribution, we propose to use a new way of extracting video features by aggregating activations of all layers of DCNNs, pre-trained for classification. We adopt a strategy similar to Hosu et al. [26] and extract narrow multi-level spatially pooled (MLSP) features of video frames from an InceptionResNet-v2 [27] architecture to learn VQA. By global average pooling the outputs of inception module activation blocks, we obtain fixed sized feature representations of the frames. We showcase the scalability of this approach by comparing it to the baseline of freezing the weights of the feature extraction network and training a new head, which is a technique that is commonly used in transfer learning.\n\nThe third contribution of this paper consists of two network variants trained on the frame feature vectors that surpass state-of-the-art NR-VQA methods on in-the-wild datasets and train at a rate that is able to scale to hundreds of thousands of videos. In a short ablation study we investigate the impact of architectural and hyperparameter choices of both models. Both approaches are then evaluated on existing VQA datasets consisting of authentic videos as well as those containing artificially degraded videos and show that on in-the-wild videos the proposed method outperforms classical methods based on hand-crafted features. In particular, training and testing on KoNViD-1k improves the state-of-the-art 0.80 to 0.82 SRCC. Finally, we show that training our proposed model on the new dataset we achieve a 0.83 SRCC when cross-testing on KoNViD-1k. This outperforms state-of-the-art intra-dataset test scenarios, where training and testing is performed on the same dataset. It is surprising, as intra-dataset tests have the benefit of not being affected by any domain shift [28].\n\nIn summary, our main contributions are:\n\n\u2022 KonVid-150k, an ecologically valid in-the-wild video quality assessment database, two orders of magnitude larger than existing ones.\n\n\u2022 The successful application of deep multi-layer spatially pooled features for video quality assessment, which allows training of state-of-the-art models at scale on conventional hardware.\n\n\u2022 Three deep neural network models (MLSP-VQA-FF, -RN, and -HYB). They surpass the intra-dataset stateof-the-art performance on KoNViD-1k with 0.82 SRCC versus the best existing 0.80 SRCC, and show excellent generalization in inter-dataset tests when trained on KonVid-150k, surpassing even the intra-dataset tests with 0.83 SRCC.\n\n\nII. RELATED WORK\n\nThis paper contributes to datasets and methods for video quality assessment. In this section we summarize related work in both fields as well as research that uses deep features that was influential for our work.\n\n\nA. VQA DATASETS\n\nThere are a few distinguishing characteristics that divide the field of VQA datasets which are usually governed by decisions made by their creators. We will cover the characteristics differentiating the wide variety of relevant related works separately.\n\n\n1) VIDEO SOURCES\n\nThe first distinguishing factor that heavily influences the use of a dataset is the source of stimuli. The early works in the field of VQA datasets stem from 2009 to 2011. EPFL-PoliMI [29], [30], LIVE-VQA [31], [32], CSIQ [33], VQEG-HD [34], and IVP [35] were mostly concerned with particular compression or transmission distortions. Consequently, these early datasets contain few source videos that were degraded artificially to cover the different distortion domains. From today's standpoint the induced degradations lack ecological validity when compared to degradations observed in new videos in-the-wild. Overall, the focus of VQA datasets has been shifting away from both transmission artifacts, as transmission networks have become much more stable over the last decades, and artificial introduction of distortions. Instead, a primary concern has been covering more contents and in-the-wild distortions.\n\nRecently designed VQA datasets from 2014 to 2019 (CVD2014 [21], LIVE-Qualcomm [22], KoNViD-1k [20], and LIVE-VQC [23]) have taken the first steps towards improving ecological validity. CVD2014 contains videos which were degraded with realistic video capture related artifacts. Videos in LIVE-Qualcomm, LIVE-VQC, and KoNViD-1k were either self-recorded or crawled from public domain video sharing platforms without any directed alteration of the content. In this paper we make the distinction between synthetic and in-the-wild datasets, where the former includes videos that have been either altered after recording or recorded in a specific way to contain particular distortions, and the latter represents sets of videos that have been gathered from auxiliary sources with minimal alteration, in order to represent content commonly consumed by internet users. Both CVD2014 and LIVE-Qualcomm fall into the synthetic category, while we categorize LIVQ-VQC and KoNViD-1k as in-the-wild.\n\nAn additional side-effect of the above-mentioned change in dataset paradigms are differences in numbers of devices and formats represented in modern datasets. Synthetic datasets commonly include fewer capturing devices, are usually recorded in the same format, and often depict fewer scenes. In-the-wild datasets, on the other hand, include more unique contents and capturing devices, as the data is gathered from external sources without control over the recording process. This is also reflected in the datasets we reference:\n\n\u2022 CVD2014 considers videos taken by 78 different cameras with different levels of quality from low-quality camera phones to high-quality digital single-lens reflex cameras. The video sequences were captured one at a time from different scenes using different devices. They captured a total of 234 videos, three from each camera, with a mixture of in-capture distortions. While each stimulus in CVD2014 is a unique video rather than an alteration of a source video, the dataset only covers five unique scenes, which is the smallest number of unique scenes among all VQA datasets.\n\n\u2022 LIVE-Qualcomm contains videos recorded using eight different mobile cameras at 54 scenes. Dominant frequently occurring distortion types such as insufficient color representation, over/under-exposure, auto-focus related distortions, blurriness, and stabilization related distortions were introduced during video capturing. In total, the 208 videos cover six types of authentic distortions, but there is no quantification as to how common these distortions are for videos in-the-wild.\n\n\u2022 LIVE-VQC contains videos captured by 80 na\u00efve mobile camera users, totaling 585 unique video scenes at various resolutions and orientations.\n\n\u2022 KoNViD-1k contains 1,200 unique videos sampled from YFCC100m. It is hard to quantify the number of devices covered, but in terms of content and distortion variety, it is the largest existing collection of videos. The videos in KoNViD-1k have been reproduced from Flickr, based on the highest quality download option; however, they are not the raw versions originally uploaded by users. The videos show compression artifacts, having been reencoded to reduce bandwidth requirements. For KonVid-150k we are employing a strategy similar to KoNViD-1k in that we download them from Flickr, however we obtained the originally uploaded versions of the videos to re-encode them at a higher quality. We aim to reduce the number of encoding artifacts while keeping the file size manageable for distribution in a crowdsourcing study with an average of 1.23 megabytes per video.\n\n\n2) SUBJECTIVE ASSESSMENT\n\nThe second distinguishing factor is the choice of subjective assessment environment. VQA has been a field of research since before the time when video could easily and reliably be transmitted over the Internet. Consequently, early datasets have all been annotated by participants in a lab environment. This allows for assessment of quality under strictly-controlled conditions with reliable raters, giving an upper bound to discriminability. With dataset sizes increasing, due to a push for more content diversity and transmission rates improving, crowdsourcing has become an affordable and fast way of annotating multimedia datasets with subjective opinions. In a lab setup it is practically infeasible to handle annotation of tens of thousands of items. The downside of crowdsourcing is a reduced level of control over the environment, resulting in potentially lower quality of annotation. However, with careful quality control considerations a crowdsourcing setup can achieve an annotation quality comparable to lab setups [36]. Concretely, CVD2014 and LIVE-Qualcomm are annotated in a lab environment, while KoNViD-1k and LIVE-VQC are both annotated using crowdsourcing. Considering the sheer size of our dataset, we also employed a crowdsourcing campaign with rigorous quality control in the form of an initial quiz and interspersed test questions to ensure a good annotation quality.\n\n\n3) NUMBER OF OBSERVERS\n\nA third factor that has been insufficiently studied thus far is the choice of numbers of ratings per video. With a few exceptions, early works in lab environments ensured at least 25 raters per stimulus. Additionally, it has been a common approach that all participants rated all stimuli.\n\nRecent works [23] have increased the number of ratings per stimulus to above 200 to ensure very high quality annotation. However, given a fixed, affordable budget of annotations, one must consider the trade-off between the benefit of slightly more accurate quality scores for a small number of stimuli and the potential increase in generalizability when annotating more stimuli with fewer votes. The 8-fold increase in numbers of ratings per stimulus when going from the generally accepted 25 to 200 ratings could just as well be invested in an 8-fold increase of numbers of stimuli, each rated 25 times. The increase of the precision of the experimental MOS suffers from diminishing returns as the number of raters increases. Since the precision gain per vote is highest at none or few ratings, careful considerations have to be made with respect to the distribution of annotation budgets across an unlabeled dataset. This is especially true in the wake of deep learning approaches outperforming classical methods in many computer vision tasks, as deep learning models are known to be robust to noisy labels [37] but also hungry for input data. Figure 1 shows a comparison of relevant VQA datasets on some of these characteristics. There is an evident progression to a wider variety of contents in the last few years. We are attempting to push this boundary much further by exploring the trade-off between the number of ratings per video and the total annotated stimuli.\n\n\nB. IQA USING DEEP FEATURES\n\nThere have been several recent works that inspired our approach for feature extraction. TL-Xception [38] was an initial work that utilized deep-features to predict image quality in a transfer learning setting. Using an Xception-net [39] as a base-model, they added two 1 \u00d7 1 convolutional layers on top, followed by both a global average pooling layer and a global maximum pooling in parallel. The outputs of the pooling served as an input to a small fully connected head which was topped off with a 5-neuron output layer that represents the opinion score as a distribution. Using this approach, the authors achieved state-of-the-art performance. 1 Recently, two related works [42], [43] extracted features from pre-trained networks, before feeding them into neural networks for quality predictions of much smaller size. Both of these approaches perform the extraction only at the heads of the feature extraction networks, which typically model higher-level semantic structures. In the case of VSFA [42] a ResNet-50 model was used, where features were extracted from the 'res5c' layer near the top of the network and subsequently pooled. The prediction network is a recurrent network using a gated recurrent unit capable of modeling temporal dependencies in the features. PVQ [43] on the other hand use both 2D features extracted from a PaQ-2-PiQ [44] model, as well as 3D features extracted using a 3D-ResNet-18 [45] model. The features are pooled independently and ultimately fed into an InceptionTime [46] network for the prediction task.\n\nThe BLINDER framework [24] improved upon the approach of feature extraction at the head of a pre-trained network by using multiple layers of the base-model to extract deep features. They resized images to 224\u00d7224 and extracted a feature vector from each layer of a pre-trained VGG-net. Each of these features vectors was then fed into separate SVR heads and trained, such that the average layer-wise scores predict the quality of an image. BLINDER was evaluated on a variety of IQA datasets and reported an improvement of the state-of-the-art.\n\nReference [26] went a step further by utilizing deeper architectures to extract features, such as Inception-v3 and InceptionResNet-v2. Furthermore, features were aggregated from multiple levels and extracted from images at their original size. This retained detailed information that would have been lost by down-sizing the inputs. Moreover, it allowed linking information coming from early levels (image dependent) and general category-related information from the latter levels in the network. This approach has since been further elaborated on with DeepFL [47], which incorporated a supervised fine-tuning step prior to feature extraction to drastically improve stateof-the-art NR-IQA performance on the complex artificially degraded KDID-10k dataset.\n\nWe use the same approach as presented in [26] to extract sets of features of video frames. The layers of the DNNs are a basic measure for the level of complexity that the feature can represent. For example, first layer features resemble Gabor filters or color blobs, while features in higher levels correspond to semantic entities such as circular objects with a particular texture or even faces. Changes in the response of different features can, therefore, encode temporal information. For example, it is reasonable to assume that a change in the overall response of low-level Gabor-like features can indicate the rapid movement of an object. Consequently, learning from frame-level features allows to learn the effect of temporal degradations on video quality indirectly.\n\nIn [48] a similar approach was used for the purpose of NR-VQA. The method extracted features for intra-frames, averaging them along the temporal domain to obtain a videolevel feature vector. The final video quality prediction is done by an SVR. In our approach we go beyond this by considering both an average feature vector with our MLSP-VQA-FF architecture, as well as an LSTM model that takes a set of consecutive features of frames as input, leveraging temporal information of feature activations.\n\n\nC. NR-VQA\n\nExisting NR-VQA methods can be differentiated based on whether they are based solely on spatial image-level features or also explicitly account for temporal information. In general, however, all recently developed models are learning-based.\n\nImage-based NR-VQA methods are mostly based on theories of human perception, with natural scene statistics (NSS) [49] being the predominant hypothesis used in several works, such as the naturalness image quality evaluator (NIQE) [50], blind/referenceless image spatial quality evaluator (BRISQUE) [51], feature-map-based referenceless image quality evaluation engine (FRIQUEE) [52] and high dynamic-range image gradient-based evaluator (HIGRADE) [53]. NSS hypothesizes that certain statistical distributions govern how the human visual system processes particular characteristics of natural images. Image quality can be derived by measuring the perturbations of these statistics. The approaches above have been extended to videos by evaluating them on a representative sample of frames and aggregating the features by averaging.\n\nApproaches that consider temporal features, so-called general-purpose VQA methods, are less numerous and more particular in their approach. In [11], the authors extended an image-based metric by incorporating time-frequency characteristics and temporal motion information of a given video using a motion coherence tensor that summarizes the predominant motion directions over local neighborhoods. The resulting approach, coined V-BLIINDS, has been the de facto standard that new NR-VQA methods are compared with.\n\nApart from V-BLIINDS, several other machine-learningbased models for NR-VQA have been proposed. Regrettably, most have only been evaluated on older datasets such as LIVE-VQA, making comparisons across multiple datasets difficult. Moreover, their codes are not publicly available, further exacerbating this issue. The three most notable examples are the following. V-CORNIA [52] is an unsupervised frame-base feature-learning approach that uses Support Vector Regression (SVR) to predict framelevel quality. Temporal pooling is then applied to obtain the final video quality. SACONVA [54] extracts feature descriptors using a 3D shearlet transform of multiple frames of a video, which are then passed to a 1D CNN to extract spatiotemporal quality features. COME [55] separated the problem of extracting spatio-temporal quality features into two parts. By fine-tuning AlexNet on the CSIQ dataset, spatial quality features are extracted for each frame by both max pooling and computing the standard deviation of activations in the last layer. Additionally, temporal quality features are extracted as standard deviations of motion vectors in the video. Then, two SVR models are used in conjunction with a Bayes classifier to predict the quality score.\n\nTLVQM [19] and 3D-CNN + LSTM [56] are recently published approaches in blind VQA which claim state-ofthe-art performance. The former is a hierarchical approach for feature extraction. It computes two types of features: low complexity features characterizing temporal aspects of the video for all video frames, and high complexity features representing spatial aspects. High complexity features relating to spatial activity, exposure, or sharpness, are extracted from a small representative subset of frames. TLVQM achieves the best performance on LIVE-Qualcomm. 3D-CNN + LSTM is an end-to-end DNN approach, where 32 groups of 16 224 \u00d7 224 crops of frames are extracted from the original video and individually fed into a 3D-CNN architecture that outputs a scalar frame-group quality. This is then subsequently passed to an LSTM that predicts the overall video quality. This approach sets the state-of-the-art for KoNViD-1k, besting TLVQM slightly.\n\nState-of-the-art for CVD2014 is achieved by VSFA [42], which is an approach that leverages feature extraction at the head of a ResNet-50 model for each frame of a video. For each video, all frame features are fed into a recurrent neural network, with the aim of modeling temporal dependencies in the frame-wise features. The approach was designed specifically for quality assessment of in-the-wild videos.\n\nFinally, PVQ [43] is the most recent approach to blind VQA that marks state-of-the-art performance on the LIVE-VQC dataset. It combines frame-level feature extraction using PaQ-2-PiQ [44] with spatio-temporal feature extraction on patches of frame stacks using a 3D ResNet-18 [45] pretrained on the Kinetics dataset [57]. Both the frame-level VOLUME 9, 2021 features as well as the 3D features are pooled twice independently, before being fed into the InceptionTime [46] model that is used to predict the quality of a given video.\n\nThere has been a body of work by another author on NR-VQA [48], [58], [59]. However, there are concerns about the validity of the published performance values [41]. Specifically, it has been shown that the performance values reported in both [58] and [59] were obtained with implementations containing some forms of data leakage. In both cases, the fine-tuning stage of the two-stage process embedded information about the test sets into the model used for feature extraction. Furthermore, in [41] it was shown that finetuning prior to feature extraction had much less impact on the final performance than claimed. Since [48] is using a similar two-stage approach involving fine-tuning and feature extraction, and there is a substantial improvement in performance from the non-fine-tuned to the fine-tuned implementation, we hold some reservations as to the validity of the reported performance values.\n\n\nIII. DATASET IMPLEMENTATION DETAILS\n\nIn this section, we introduce the video dataset in two parts. First, we discuss the design choices and gathering of the data in Section III-A alongside an evaluation of the diversity captured by the dataset in relation to existing work in Section III-B. Then, Section III-C follows up with details regarding the crowdsourcing experiment to annotate the dataset. Finally, in Section III-D we analyze the quality of annotations according to the SOS hypothesis.\n\n\nA. VIDEO DATASET\n\nOur main objective was to create a video dataset that covers a wide variety of contents and quality-levels as commonly available on video sharing websites. For this reason, we took a similar approach to collect our data as was done for KoNViD-1k, with an additional step to improve the quality of the videos. In KoNViD-1k all collected videos had been transcoded by Flickr, to reduce their bandwidth requirements and standardizing them for playback. Consequently, noticeable degradation was introduced relative to the original uploads. Flickr allows the uploading of video files of most codec and container combinations, resolutions, and durations. However, they re-encode the uploaded videos to common resolutions such as HD, Full HD, strongly compressing them.\n\nThe Flickr API allows access to metadata that links to the original, raw uploads. As these raw uploads are often very large and come in many different formats, they cannot directly be used for crowdsourcing. Therefore, we proceeded as follows. We downloaded authentic raw videos that had an aspect ratio of 16:9 and resolution higher than 960 \u00d7 540 pixels. Then we rescaled them to 960 \u00d7 540, if necessary, and extracted the middle five seconds.\n\nOur choice of a playback duration of five seconds was grounded in several considerations. First, videos with longer playback durations may bias the subjective evaluation procedure due to the presence of a temporal hysteresis effect [60], which is a lingering negative impact on the subjective quality perception after a subject observed a degradation. The longer a video playback duration, the more likely this effect can take place. Moreover, from a practical perspective, since we tied the payment of crowd workers participating in our study to the playback duration, reducing it would yield more total annotations. As a final point, shorter videos are less likely to be affected by buffering events and the total individual file size is reduced.\n\nWe re-encoded the videos using FFmpeg at a constant rate factor of 23, which balances visual quality and file size. The resulting files have an average size of 1.23 megabytes.  For each video, we extracted meta-information that identifies the original encoding, including the codec and the bit-rate. Furthermore, we collected social-network attributes such as the number of views and likes and publication dates that indicate the popularity of videos. In total, the collection amounts to 153,841 videos.\n\nWe believe that all the additional measures we have taken to refine our dataset significantly improved its ecological validity, and thus the performance of VQA methods trained on it in the future.\n\n\nB. DATASET EVALUATION\n\nIn order to evaluate the diversity of KonVid-150k, which is our main objective with this dataset, we will now demonstrate that it is not only the largest annotated VQA dataset in terms of video items, but also the most diverse in terms of content. First, we need a measure for content diversity. For this purpose we extract the activations at the top of an Inception-ResNet-v2 model pre-trained on ImageNet for each frame. To represent a given video, we spatially average the activations of the last four Inception modules over all frames and subsequently concatenate them to obtain a 1792-dimensional content feature. A similar approach has been used in the image quality domain before to create a subset of data that is diverse in content [61]. Figure 3 is an illustration of the usefulness of these content features to assess content similarity. Given a query video taken from KoNViD-1k on the left we compute the Euclidean distance in content feature space to all other videos in the dataset. On the right we show still frames from the three videos with smallest distance to the query. We can see that close proximity in content feature space seems to correspond to semantically similar video content. The images in the first row show flying objects in a blue sky, where the color of the object as well as the color of the sky seem to influence the distance in content feature space. In the second row we can see that crowds in front of a stage are located in close proximity in content feature space. Images in the third row show that videos containing heads, but especially babies are encoded similarly in the 1792-d content feature vectors. Light shows and underwater videos, as seen in the fourth and fifth rows, can also be retrieved by querying nearest neighbours of an appropriate video. It is to be noted that the closest videos for rows one, two and four are near duplicates. The recordings seem to be from different periods of time of the same scene.\n\nTherefore, the extracted features are useful as an information retrieval tool, and we make use of it to quantify the degree by which a video dataset covers the content of competing datasets. For this purpose we represent a video dataset by its corresponding set of content feature vectors,\nX = {x i | i = 1, . . . , N }, where N is the number of videos in the dataset. We consider the Euclidean distance of a point x in feature space to a (finite) point set Y , d(x, Y ) = min{d(x, y) | y \u2208 Y }.\nFor two finite point sets X = {x 1 , . . . , x n }, Y = {y 1 , . . . , y m } and any given distance s \u2265 0, we define the fraction or ratio of the first dataset X , that is covered by the dataset Y at distance s as\nC Y ,s (X ) = |{x \u2208 X | d(x, Y ) \u2264 s}| |X |\nwhere |A| denotes the cardinality of a set A. For example, if X \u2286 Y , then Y covers X perfectly at distance zero, i.e., C Y ,0 (X ) = 1. Or, if C Y ,1 (X ) = 0.8, then this means that the union of all balls of radius 1 centered at the points VOLUME 9, 2021 of the set Y contain 80% of the points in X . The function s \u2192 C Y ,s (X ) thus comprises the cumulative histogram of the individual distances d(x, Y ) for all x \u2208 X .\n\nWhen comparing the coverage two datasets with respect to each other, we check the corresponding cumulative histograms showing the coverage of one dataset by the other. The dataset with the topmost cumulative histogram then can be considered to be the dominant one that covers the competing one.\n\nTo compare the diversity of content for several given datasets X 1 , . . . , X K , let us form their union Z = X 1 \u222a \u00b7 \u00b7 \u00b7 \u222a, X k and consider how well each dataset X k covers all the others, i.e., the complement X c k = Z \\X k . For this purpose we compute the cumulative histograms C X k ,s (X c k ) for k = 1, . . . , K . Figure 4 shows the result for the five datasets KonVid-150k, KoNViD-1k, VQC, Qualcomm, and CVD 2014. Here, KonVid-150k clearly has the best coverage of contents present in the other datasets, as it has the largest area under the curve.\n\nTo summarize the coverage of one dataset X by another, Y , by a single number rather than the curves of the cumulative histogram of distances, we define the one-sided distance of X from Y as\nd(X , Y ) = f (d(x 1 , Y ), d(x 2 , Y ), . . . , d(x n , Y ))\nwhere f is a scalar, non-negative function. For example, if f is the maximum function, then d(X , Y ) is known as the onesided Hausdorff distance. For our purpose, the median is better suited as it is less sensitive to outliers. The distance d(X , Y ) can be understood as a simplified indicator for the coverage of X by Y . These medians are shown in Figure 4 by the bullet dots at the coverage ratio of 0.5.  X ) for all x c \u2208 X c , where X c is the complement to X , i.e., the union of the other datasets. The green, red, blue, yellow, and cyan lines refer to X being KonVid-150k, KoNViD-1k, VQC, Qualcomm, and CVD 2014, respectively. KonVid-150k covers the other datasets the best, as the green plot has the largest area under the curve and it has the smallest median distance of approximately 2.3 at coverage ratio 0.5. This means that for half of the videos in all other datasets, there is a similar video in KonVid-150k that has a distance in content feature space of at most 2.3.  contents of competing datasets the best, as the green curves are strictly above the cumulative histograms for the other datasets. Moreover, the other datasets cover the content space of KonVid-150k the worst, as the solid lines depicting the coverage of KoNViD-1k, CVD 2014, Qualcomm, and VQC of KonVid-150k are generally to the right of the other three for the respective dataset.\n\nThese findings are an indication that our proposed dataset KonVid-150k is comprised of a large variety of contents with good coverage of the contents contained in existing works.\n\n\nC. VIDEO ANNOTATION\n\nWe annotated all 153,841 videos for quality in a crowdsourced setting on Figure Eight. 2 First, each participant was presented with instructions according to VQEG recommendations [62], which were modified to our requirements. Here, participants were introduced to the task and provided with information about types of degradation, e.g., poor levels of detail, inconsistencies in color and brightness, or imperfections in motion. Next, we provided examples of videos of a variety of quality levels with a brief description of identifiable flaws and instructed the reader on the workflow of rating videos, which is illustrated in Figure 6. Finally, we informed participants about ongoing hidden test questions that were presented throughout the experiment, as well as the minimum resolution requirement that enabled them to continue participating in the experiment. This was checked before the playback of any video.\n\nDuring the actual annotation procedure, for each stimulus, workers were first presented with a white-box of the size of the video that also functioned as a play button. Then, the video was shown in its place with the playback controls hidden and deactivated. After playback finished, it was hidden, and the rating scale was revealed below it. This setup ensured that neither the first nor the last still frame of the video were FIGURE 6. Illustration of the crowdsourcing video playback workflow. A worker is first presented with a white box of 960 \u00d7 540 pixels. Upon clicking the box, the video plays in its place. Playback controls are disabled and hidden. Upon finishing, the video is hidden and replaced with a white box that informs the participant to rate the quality on the Absolute Category Rating (ACR) scale shown below. The rating scale is only shown upon completion of video playback.\n\ninfluencing the worker's rating which could be another source for the temporal hysteresis effect [60], and no preemptive rating could be performed before the entirety of the video had been seen. An option to replay the video was also not provided. These choices are a deviation from the VQEG recommendations, and might be perceived overly restrictive and annoying by a crowd worker. However, feedback from pilot studies for the interface design did not reflect this. Moreover, this approach improves attentiveness and ensures that the obtained score is the intuitive response from the worker. Additionally, playback of any other video on the page was disabled until the currently playing video was finished, in order to better control viewing behavior and discourage unreliable or random answers.\n\nAccording to Figure Eight's design concept, crowd workers submit batches of multiple ratings in so-called pages. Each page has a fixed batch size of rows, where each row conventionally represents a single item. Due to constraints on the number of rows allowed per study, we grouped 15 stimuli by random selection into each row, with a page size of ten rows per page, totaling to 150 videos per batch, respectively page.\n\nMoreover, the design concept intends a two-stage testing process, where workers are first presented with a quiz of test questions followed by subsequent pages where test questions are randomly inserted into the data acquisition process. Test questions are not distinguishable from conventional annotation items.\n\nIn our implementation, illustrated in Figure 7, we interspersed three test videos with twelve videos randomly sampled from the dataset in each row with test questions. The test videos were sampled from hand-picked set of videos, which in one part was made up of very high-quality videos obtained from Pixabay 3 and in another of heavily degraded versions of them. Therefore, we defined the ground truth quality of each test video as either excellent or bad, respectively. We performed a confirmation study to ensure that the perceived quality of these videos was rated at the very top or bottom ends of the 5-point ACR scale.\n\nIn the second stage, after the quiz, consisting of only test rows, workers annotated 150 videos in 10 rows per page. 3 http://pixabay.com On each page, we included one further test row at a random position. Participants had to retain at least 70% accuracy on test questions throughout the experiment. Data entered from workers that dropped below this threshold were removed from our study, and the corresponding videos were scheduled for re-annotation.\n\nWhen running a study on Figure Eight, the experimenter decides the number of ratings per data row, as well as the pay per page. The latter was set such that with eight seconds per video, including five seconds for viewing and three seconds for making the decision, a worker would be paid USD 3 per hour. We had compiled 10,368 data rows of 15 data videos each. These data rows were presented to five workers each, yielding 155,520 annotated video clips. From these, 152,265 were valid 4 and were retained, forming our larger dataset, called KonVid-150k-A.\n\nEach of the 10,368 data rows was presented to five workers. There were altogether 133 test rows for presentation to all crowd workers. However, each crowd worker could annotate any given test row at most once. Since 12 of the 15 videos in a test row were sampled from the set of data videos, we thus obtained far more than five ratings for each of these individual videos. In total, 1,596 data videos were used in the 133 test rows and were rated between 89 and 175 times, due to randomness in test question distribution. We separated 1,575 valid 4 videos of this very extensively annotated set in a new dataset and call it KonVid-150k-B. As a random subset of the entirety of our videos selected from Flickr, it is ecologically valid and from the same domain as the other data videos. This dataset will be used as a test set for the evaluation of our models trained on KonVid-150k-A.\n\nThe choice for five individual ratings per data row was based on a small scale pilot study with a subset of 600 randomly sampled videos. For this subset we obtained two sets of 50 opinion scores for each video with a similar experimental setup as described above. We then evaluated the SRCC between a MOS comprised of a random sample of n votes from one set to the MOS of the other set. At 5 votes this SRCC reached 0.8, which we considered to be a good threshold. For reference, the SRCC between the two independent samplings of 50 votes settled at 0.9. Further investigation of the feasibility of our choice of 5 ratings is contained in more detail in Section V-E.\n\n\nD. ANNOTATION QUALITY\n\nAnother common characteristic to compare the annotation quality of different studies is by evaluating the standard deviation of opinion scores (SOS) as a function of MOS. It follows the basic idea that in quality controlled experimental studies subjective opinions will vary only to a certain extent, as the experimental setup ensures similar test conditions. In the case of the 5-point scale we used in our experimental setup, the maximum SOS is expected near a MOS of 3, while the minimum will be attained near the extremes of the rating scale (i.e., 1 and 5). Therefore, computing the average SOS over all videos is not an unbiased indicator, as common datasets have differing distributions of MOS values. Instead, the variance \u03c3 2 is modelled as a quadratic function of the MOS [63], which in the case of a 5-point scale is described as:\nSOS(MOS) 2 = a(\u2212x 2 + 6x \u2212 5),(1)\nand the SOS parameter a is a better indicator the variance of subjective opinions for any particular experimental study. Moreover, the SOS parameter has been shown to correlate with task difficulty and can be used to characterise application categories [64]. For VQA the SOS parameter has been reported in the range a \u2208 [0.11, 0.21], with a KoNViD-1k = 0.14 and a CVD2014 = 0.17. In the case of LIVE-Qualcomm and LIVE-VQC, no SOS parameter has been reported and the publicly available annotation data does not allow for such an analysis, as only the MOS values for videos in these specific datasets are available.\n\nWe computed and visualized the SOS parameter for KonVid-150k-B as well, see Figure 8. For the case of the larger KonVid-150k-A set, we have 5 ratings per stimulus Considering the similarities between KoNViD-1k and KonVid-150k, the difference in a seems surprisingly large at first. However, some differences in the design choices of the subjective annotation process can be identified as potential causes for the larger SOS parameter for KonVid-150k.\n\nVideos from KoNViD-1k and KonVid-150k are both sampled from Flickr.com. However, their compression settings are different. While the videos in KonViD-1k are heavily compressed, those in KonVid-150k are representative of the originals as uploaded by their respective authors. This means that KonVid-150k videos are more diverse in terms of distortion types, as heavy compression can have a strong masking effect. A wider variety of distortions is expected to cause a higher disagreement between raters, and thus a higher variance of their answers.\n\nMoreover, the sources for the test videos in each dataset used during the crowdsourcing experiment are different. KoNViD-1k test videos were sampled from the same source and with ground truth annotations from a prior study, while the test videos in KonVid-150k are sampled from another source, and involve artificial distortions.\n\nOn the one hand, the choice of test videos for KoNViD-1k can cause workers to pay more attention, and agree better, however, at the cost of having more biased answers. First, the test and data videos are impossible to distinguish at a glance. This means that crowd-workers need to constantly pay attention to all work items, and not just to those that are easy to identify as test items. Second, the test videos have similar levels and types of distortions. There are no other items to anchor user opinions at the extreme of the quality scale. This means that the range of the quality scale may not be used well. The downside of this choice is that the accepted answers for the test videos are derived from a pilot study, and this can introduce a bias towards the opinions expressed in that study. Extraction of multi-level spatially-pooled (MLSP) features from a video frame, using an InceptionResNet-v2 model pre-trained on ImageNet. The features encode quality-related information: earlier layers describe low-level image details, e.g. image sharpness or noise, and later layers function as object detectors or encode visual appearance information. Global Average Pooling (GAP) is applied to the activations resulting from the Stem, each Inception-module, as well as the Reduction-modules, and finally concatenated to form MLSP features. For more information regarding the individual blocks please refer to the original paper [27].\n\nOn the other hand, KonVid-150k uses pristine quality videos from a different source (Pixabay.com), alongside highly degraded variants of the same videos. These videos are easier to distinguish from data videos. Consequently, workers are not forced to pay attention to all items the same, they can theoretically put more thought in answering test videos than they do for data videos. The tests in this case are more lenient, as they are selected to represent the extremes of the quality range (both highest and lower quality). However, they also serve as anchors for the quality scale, which are not available for KoNViD-1k. The approach is less biased, but can result in more disagreement between annotators, which in turn leads to a higher variance of the answers. It is preferable to have less bias rather than a higher agreement on the wrong ratings.\n\n\nIV. VIDEO QUALITY PREDICTION\n\nIn this section, we illustrate our approach to video quality prediction. We provide a brief description of the way we perform feature extraction in Section IV-A, followed by details regarding the models we evaluate in Section IV-B. Finally, in Section IV-C we provide a comparison of our two-stage approach of feature extraction followed by training with different fine-tuning approaches that are common for transfer learning approaches.\n\n\nA. FEATURE EXTRACTION\n\nThe na\u00efve way to perform transfer learning for tasks related to visual features with small sets of data is removing the head of a pre-trained base-model and replacing it with a small fully connected head. By freezing the layers in the base-model it's predictive power can be used to perform well on the new task. After training this new header, it is not uncommon to unfreeze all layers and fine-tuning the entire trained network with a low learning rate to improve predictive power even more. However, this approach has three important downsides.\n\n1) First, the new task is trained based on the highest level features in the base-model. These features are particularly tuned to detecting high-level semantic features that are useful in the detection of objects present in the image. However, for tasks such as quality, low-level features with a small receptive field are arguably more important.\n\n2) Secondly, for each forward and backward pass the entire base-model has to be present in memory, which contain many more weights than the header network that is being trained. Consequently, training is slowed down a lot. 3) Finally, the last fine-tuning step is prone to overfitting, as the high capacity of the base-model alone allows the network to memorize training data rather than extracting useful general features. Careful hyperparameter tuning is therefore required, to ensure this step is successful in improving performance. Instead of performing fine-tuning, we trained our models on features extracted from pre-trained DCNNs. The procedure is an expansion of what we described earlier for the comparison of content diversity, except we extracted features of all Inception modules of the network. The approach is inspired by [26], namely we extracted narrow multilevel spatially-pooled (MLSP) features, but for individual frames of videos, as shown in Fig. 9. In principle, this general approach of extracting activations from individual layers of a network can be applied to any popular architecture. Related work has shown that this approach works with an Inception-ResNet-v2 network as a feature extractor in the IQA domain [47], [61]. For the extraction process we, therefore, passed individual video frames to an InceptionResNet-v2 network, pre-trained on ImageNet [27]. We then performed global average pooling on the activation maps of all kernels in the stem of the network, as well as on each of the 40 Inception-ResNet modules and the two reduction modules. Concatenating the results yielded our MLSP feature vector consisting of average activation levels for 16,928 kernels of the InceptionResNet-v2 network. These MLSP feature vectors were extracted for all frames of all videos. Figure 10 shows a visualization of parts of the MLSP feature vector for multiple consecutive frames. Table 1 gives an overview of some hyperparameter settings used in the training of our MLSP-based models for the compared datasets. Mean square error (MSE) was used as a loss function for a duration of 250 epochs, stopping early if the validation loss did not improve in the most recent 25 epochs at an initial learning rate of 10 \u22124 . By default, the MLSP-VQA-FF model was trained with a learning rate   \n\n\nB. MODEL IMPLEMENTATION DETAILS\n\nDifferent learning-based regression models, such as Support Vector Regression (SVR) or Random Forest Regression (RFR), have been employed to predict subjective quality scores from frame features, with SVR yielding generally better results [19]. However, most existing works only extract a few dozen to a few hundred features. Since SVR is suboptimal when applied to very large dimensional features like our MLSP feature, we instead train three small-capacity DNNs ( Figure 11):\n\n\u2022 MLSP-VQA-FF, a feed-forward DNN where the average feature vector is the input of three blocks of fully connected layers with ReLU activations, followed by batch normalization and dropout layers.\n\n\u2022 MLSP-VQA-RN, a deep Long Short-Term Memory (LSTM) architecture, where each LSTM layer receives the feature vector or the hidden state of the lower LSTM layer as an input and outputs its hidden state. This stacking of layers allows for the simultaneous representation of input series at different time scales [65]. The bottom LSTM layer can be understood as a selective memory of past feature vectors. In contrast, each additional LSTM layer represents a selective memory of past hidden states of the previous layer.\n\n\u2022 MLSP-VQA-HYB, a two-channel hybrid of both the FF and RN variants. The temporal channel is a copy of the RN model's architecture, while the second channel is a mirror of the FF network scaled up to match the number of kernels in the temporal branch in the last layer. The outputs of the two channels are concatenated and a small 32 kernel fully connected layer feeds into the last prediction layer.\n\nOur tests showed that employing dropout of any kind within the recurrent networks, such as input/output dropout or recurrent dropout, resulted in reduced performance. We therefore do not employ any dropout in these architectures.\n\n\nC. TRANSFER LEARNING COMPARISON\n\nAs mentioned before, this two-step strategy of feature extraction followed by training a regressor is much faster than transfer learning and fine-tuning an Inception-style network. It's difficult to fairly assess the difference, as a lot of factors play a role. For example, when fine-tuning an Inception-net, the speed at which the videos are read from the hard-drive can become a bottle-neck, if a very powerful GPU is performing the training procedure. Our proposed approach with an Inception-ResNet-v2 as a feature extraction network has a benefit for this scenario. Since the input data for each frame is fixed at 16,928 floating point values, the requirements for hard-drive reading speed are not exacerbated when using datasets with larger resolution videos. In contrast, if the GPU used to perform the training is not as powerful, it itself can become a bottle-neck of the system. In this case, our proposed approach has the alternative benefit that the small network size allows for much larger batches and quicker forward and backward passes. In order to quantify the difference, we compare different setups of transfer learning and fine-tuning to our proposed two-step MLSP feature-based training procedure on a machine that reads from an NVMe connected SSD and trains the networks using Tensorflow 2.4.1 on an NVIDIA A100 with 40GB of VRAM. To simplify the setup, we are evaluating only the MLSP-VQA-FF model on the preextracted first frames of KonVid-150k-B. One might argue that the first frame is not as representative of the opinion scores, but our aim is to investigate the differences in training speed, rather than an exhaustive performance evaluation. The transfer learning scenarios are all performed using an Inception-ResNet-v2 base-model with our FF model sitting on top for 40 epochs. However, we compare four slightly different scenarios:\n\n\u2022 Koncept: The FF model takes the last layer of the basemodel as an input, much like the Koncept model proposed in [61]. The weights of the base-model are not frozen, so the entire model is fine-tuned over the course of the training. We employ two training stages, one with a learning rate of 1\u00d710 \u22123 , and the second with a learning rate of 1 \u00d7 10 \u22125 .\n\n\u2022 IRNV2: Instead of fine-tuning the entire model throughout both stages, we freeze the layers of the Inception-ResNet-v2 base-model for the first stage, so as to avoid the large update steps caused by the random initialisation of the header network to destroy the useful features in it. For the second stage we unfreeze the weights in all layers.\n\n\u2022 IRNV2-MLSP: As stated before, one downside of the above approaches lies in the circumstance that the header network relies only on the top level features as inputs. For the third comparison we concatenate the activation layers of all Inception-modules and feed that as an input to the header network. Here, we also freeze the base-model weights for the first stage, and unfreeze all weights for the second stage.\n\n\u2022 MLSP: The final item in the comparison takes the MLSP features described above as an input. This means, the model is much smaller, as the base-model does not need to be loaded. However, the model cannot leverage the spatial information about the activations to make it's prediction. No explicit weight freezing is performed in this scenario. These different cases are compared in Figure 12. The green graph, corresponding to the Koncept model, takes the longest to train in total and achieves the worst validation performance at the end of the 80 epochs. The reason for the slow training in the first stage is that none of the weights are frozen and the backpropagation step therefore takes additional time. Both the orange IRNV2 and blue IRNV2-MLSP models train faster by approximately 22%, as the weights are frozen in the first stage. However, they differ in that the inclusion of all Inception-modules in the concatenation layer for the latter increases performance significantly. Finally, the red graph, representing the MLSP-VQA-FF model trained on extracted MLSP features achieves the best performance while surpassing the IRNV2-MLSP model in terms of speed by a factor of 74. Moreover, peak performance is achieved much earlier, as the second training stage is not required, raising the speed-up to factor 171.\n\nHowever, feature extraction has to be performed once as well, which for the first frames of KonVid-150k-B took 38 seconds. Including this time in the comparison still renders the MLSP-VQA-FF model faster by factor 36, when considering both training stages. This factor is dependant on input resolutions, however with videos increasing in resolution the speed-up will only change in favor of the MLSP-based model, as its training speed will not change, while the training speed of the fine-tuning approach is inversely correlated with input resolution. This shows the power of using pre-extracted MLSP features.\n\nFurthermore, we have observed the success of fine-tuning an Inception-style network in this manner is very sensitive to hyperparameters, while training the small FF network on MLSP features is fairly robust.\n\n\nV. MODEL EVALUATION\n\nIn this section, we provide several performance evaluations of our proposed models as well as related works on our proposed dataset. First, in Section V-A we give some context to performance evaluations of modern VQA approaches of different kinds of datasets. Section V-B then compares the MLSP-VQA models on existing datasets, validating their usefulness as VQA models. A performance comparison of different VQA methods on the KonVid-150k-B set is provided in Section V-C, validating the utility of our proposed dataset. Section V-D then investigates inter-dataset performance of our proposed models when trained on our proposed dataset. Finally, in Section V-E we explore more elaborate training schemes for the MLSP-VQA-FF model which consider different numbers of vote budget distributions.\n\n\nA. INTRODUCTION\n\nOur proposed NR-VQA approach of extracting features from a pre-trained classification network and training DNN architectures on them have been designed to predict video quality in-the-wild. We evaluate the potential of the MLSP features when used for training the shallow feed-forward and recurrent networks by measuring their performance on four widely used datasets (KoNViD-1k, LIVE-VQC, CVD2014, and LIVE-Qualcomm) and our newly established dataset KonVid-150k. We consider two basic scenarios, namely (1) intra-dataset, i.e. training and testing on the same dataset, and (2) inter-dataset, i.e., training (and validating) on our large dataset KonVid-150k and testing on another.\n\nThere are two fundamental limitations in these datasets that affect the performance of our approach. The first one relates to the video content, in the form of domain shifts between ImageNet and the videos in the datasets. The other one is due to the different types of subjective video quality ratings (labels) in the datasets, that may affect the crosstesting performance.\n\nFirst, the features in the pre-trained network have been learnt from images in ImageNet. There are situations when the information in the MLSP features may not transfer well to video quality assessment:\n\n\u2022 Some artifacts are unique to video recordings; this is the case of temporal degradations such as camera shake, which does not apply to photos.\n\n\u2022 Compression methods are different for videos in comparison to images. Thus, the individual frames may show encoding-specific artifacts that are not within the domain of artifacts present in ImageNet.\n\n\u2022 In-the-wild videos have different types and magnitudes of degradations compared to photos. For example, motion blur degradations can be more prevalent and of a higher magnitude in videos compared to photos. This could affect how well MLSP features from networks pretrained on ImageNet transfer to VQA. Secondly, concerning the subjective video quality ratings to be predicted when cross-testing, while there are similarities between the rating scales used in the subjective studies corresponding to each dataset, the ratings themselves may suffer from a presentation bias. For example, in the case of a dataset with highly similar scenes, but minuscule differences in degradation levels, as is the case for LIVE-Qualcomm and CVD2014, a human observer may become very sensitive to particular degradations. Conversely, video content becomes less critical for quality judgments. The attention of the human observer is diverted to parts in the video he might otherwise not have looked at, had he not seen the same or a very similar scene many times before. Whether the resulting subjective judgments can be regarded as fair quality values is arguable. A human observer would rarely watch a scene multiple times before rating the quality. This bias of subjective opinions may greatly influence how the quality predictions trained in one setting generalize to others. Similarly, quality scores obtained in a lab environment will be much more sensitive to differences in technical quality than a worker in a crowdsourcing experiment might be able to pick up. Therefore, it may be challenging to generalize from one experimental setup to another. While consumption of ecologically valid video content happens in a variety of environments and on a multitude of devices, it is arguable whether one experimental setup is superior.\n\n\nB. MODEL PERFORMANCE COMPARISONS\n\nWe first evaluate the performance of the proposed model on four existing video datasets. KoNViD-1k and LIVE-VQC both pose the unique challenge that they are in-the-wild video datasets, containing authentic distortions that are common to videos hosted on Flickr. LIVE-Qualcomm contains selfrecorded scenes of different mobile phone cameras that were aimed at inducing common distortions. CVD2014 differs from the previous two, in that it is a dataset with artificially introduced acquisition-time distortions. It also contains only five unique scenes depicting people. Finally, LIVE-VQC was a collaborative effort of friends and family of the LIVE research group that were asked to submit video files of a variety of contents to capture diversity in capturing equipment and distortions.\n\nWe are comparing our proposed DNN models against published results for other methods that have been thoroughly evaluated on these datasets using SVR and RFR. Detailed information regarding the experimental evaluation and results of the classical methods can be found in [19].\n\nWe adopt a similar testing protocol by training 100 different random splits with 60% of the data used for training, 20% used for validation, and 20% for testing in each split. Table 2 summarizes the SRCC w.r.t. the ground-truth for the predictions of the classical methods (taken from [19]) alongside several recent approaches based on deep features and our own DNN-based approach. It is to be noted that the random splits for the classical methods are equal, whereas the test setups used for VSFA, PVQ and 3D-CNN + LSTM are slightly different. Moreover, the splits we used for our evaluations of the MLSP-VQA models are different from the ones used to evaluate the classical methods in [19], but we put an emphasis on emulating the test setup. For brevity, we are only reporting the results for classical methods obtained using SVR, although four individual results are slightly improved using RFR.\n\nThe FF network outperforms the existing works on KoNViD-1k, improving state-of-the-art SRCC from 0.80 to 0.82, while the RN and HYB models remain competitive with an SRCC of 0.78 and 0.79, respectively. This shows that the proposed approaches are performing close to state-ofthe-art on authentic videos with some encoding degradations. Since the feature extraction network is trained on images with natural image distortions, some of the extracted features are likely indicative of these distortions, which are not unlike the video encoding artifacts introduced by Flickr.\n\nExisting methods had not been evaluated exhaustively on LIVE-VQC at the time of writing. Our recurrent networks achieve 0.70 (RN) and 0.69 (HYB) SRCC, while the FF model performs at 0.72 SRCC. Recent articles on arXiv have pushed the state-of-the-art to 0.83 SRCC [43]. One of the difficulties inherent to VQC with respect to our models is the circumstance, that it is comprised of videos of various resolutions and aspect ratios. An evaluation of the performance of the models with respect to the video resolutions can be found in the top part of Figure 13. Since 1080p, 720p, and 404p in portrait orientation are the predominant resolutions with 110, 316, and 119 videos, respectively, we grouped the other resolutions into the other category. We can see that both the FF and RN models perform worse on the 1080p and 720p videos, whereas the HYB model performs better on the higher resolution videos.\n\nIn the case of LIVE-Qualcomm our best performance of 0.75 SRCC of the hybrid model is surpassed only by TLVQM with 0.78. Since the dataset is comprised of videos containing six different distortion types, we also evaluated the performance of the models according to each degradation, as depicted in the middle plot of Figure 13. Here, we show the deviation of the RMSE of each model for each distortion type from the average performance in percent. Little deviation between all three models is observed for both Exposure and Stabilization type distortions. However, for Artifacts and Color the RN model deviates from the other two drastically, performing worse on the former and better on the latter. Videos in the focus degradation class show auto-focus related distortions where parts of the video are intermittently blurry or sharp over time and are overall the biggest challenge for our recurrent models, that both perform over 20% worse on them than average. Finally, the Sharpness distortion is best predicted by the recurrent networks, with the hybrid model outperforming the pure LSTM network.\n\nOn CVD2014, our proposed models with SRCCs of 0.77, 0.75, and 0.79 for the FF, RN and HYB models, respectively, are outperformed by both FRIQUEE and TLVQM at 0.82 and 0.83 SRCC and far outperformed by VSFA at 0.88 SRCC. CVD2014 is a dataset of videos of two different resolutions, with artificially introduced capturing distortions and only five unique scenes of humans and human faces. The magnitude of the artifacts is at a level that is not commonly seen in videos in-the-wild, and the types of defects are also not within the domain of distortions present in ImageNet. Therefore, this is the most challenging dataset for our approach and, consequently, the relative performance of our approach is worse. CVD2014 is split into six subsets with partially overlapping scenes but distinct capturing cameras. The bottom part of Figure 13 shows the relative deviation of the RMSE from the mean performance for each of these test setups. The first two setups include videos at 640 \u00d7 480 pixels resolution, which are generally rated with a lower MOS than videos in the other test setups, which could both be an important factor in our models' increased performance here. Although all setups include scenes 2 and 3, scene 1 is only included in test setups 1 and 2, scene 4 is only included in test setups 3 and 4, and scene 5 is solely included in test setups 5 and 6. Since the features we use are tuned to identify content, as we showed in Section III-B, inclusion or exclusion of particular scenes can have an impact on the performance of our method. Moreover, since each test setup contains videos taken from different cameras than the rest, it is possible that the in-capture distortions caused by particular cameras in any individual test setup may be closer to the types of distortions present in ImageNet.\n\n\nC. EVALUATION OF KONVID-150K-B\n\nWe now consider the performance evaluation when training and testing on our new dataset, KonVid-150k-B of 1,596 videos, each with at least 89 ratings comprising the quality score. We separate these tests from the previous ones because, in this case, we have the option to train the networks on the additional 150k videos in KonVid-150k-A that stem from the same domain. From the previous experiments, it is evident that TLVQM is the best performing classical metric on the similar domain, given by KoNViD-1k, by a large margin. Therefore, we compare our MLSP-VQA models only against TLVQM and the standard V-BLIINDS. Furthermore, since the authors of VSFA has made code available to train their model from scratch, we also evaluate this DNN-based method. For both PVQ and 3D-CNN + LSTM functional implementations to train a model from scratch was not available at the time of writing. Table 3 summarizes the performance results. Compared to the performance on KoNViD-1k, V-BLIINDS (row 1) improves slightly, while TLVQM (row 2) performs significantly worse. In the case of VSFA the performance on KonVid-150k-B is only slightly worse. Since the main difference between KoNViD-1k and this dataset is the reduced re-encoding degradations, it appears as though the classical methods over-emphasize their prediction on these artifacts. The fourth through sixth rows list the performance of our models, which outperform the other compared methods, beating VSFA's 0.72 SRCC with 0.81 (FF), 0.78 (RN) and 0.75 (HYB) when trained and tested on the B variant exclusively.\n\nFinally, the last three rows show the results from training on the large dataset, KonVid-150k-A, with 150k videos. For these last three evaluations a random subset of 50% of KonVid-150k-B was used for validation during training. The remaining part of KonVid-150k-B was used for testing. We note an additional substantial performance increase for our networks. The FF model's performance increases from 0.81 SRCC to 0.83, while the RN model improves from 0.78 SRCC to 0.81. The largest performance gain can be observed for the HYB network, as it improves from 0.75 SRCC to 0.81 SRCC as well. This demonstrates, for the first time, the enormous potential gains that can be achieved TABLE 3. Results of NR-VQA metrics tested on KonVid-150k-B. The first six rows are all intra-dataset performance results, meaning that the metrics were trained and tested on KonVid-150k-B. The bottom three rows denoted by ''(Full)'' describe the performance when training on the entirety of KonVid-150k-A, using half of KonVid-150k-B as a validation set, and the other as a test set. by vast training datasets for VQA. Although KonVid-150k-A only has MOS scores comprised of five individual votes, by training on them and validating on the target dataset we drastically improve performance. It is to be noted as well that the test sets in this scenario are larger than when training and testing solely on KonVid-150k-B. This renders the test performance to be even more representative. However, the change in variance of the resulting correlation coefficients cannot directly be attributed to the increase in training dataset size. The difference likely arises from the fact that the models trained using KonVid-150k-A have the same training data, and are therefore more likely to learn similar features. Nonetheless, this effect should be investigated further.\n\n\nD. INTER-DATASET PERFORMANCE\n\nConsidering the diversity in content and distortions in KonVid-150k we highlight the power of KonVid-150k in combination with our MLSP-VQA models in inter-dataset testing scenarios. At the time of writing, LIVE-VQC has not been considered in any performance evaluations across datasets. The previously best reported cross-test performances between the other three legacy datasets are three different combinations of NR-VQA methods and training datasets. 5 Specifically, TLVQM trained on CVD2014 performs best on KoNViD-1k cross-testing with 0.54 SRCC. V-BLIINDS trained on KoNViD-1k is the best combination for cross-testing on LIVE-Qualcomm with 0.49 SRCC. Finally, FRIQUEE trained on KoNViD-1k performs best when cross-testing on CVD2014 with 0.62 SRCC. It is apparent from these results that no single NR-VQA and dataset combination generally outperforms in inter-dataset testing scenarios.\n\nWe evaluate the performance of our models when cross-testing on other datasets, trained on KonVid-150k-A and validated and tested on each 50% of KonVid-150k-B. The average SRCC performances of 10 models are reported in Table 4. For ease of comparison we also include the best within-dataset performance in the first row, as well as the previous best cross-dataset test performances as taken from [18] in the second row of the table. Although the performances between our different models do not vary much, the results reveal some interesting findings.\n\npower of data, even if it is annotated with lower precision. Although KonVid-150k does not have the Flickr video encoding artifacts present, it can predict the distorted videos of KoNViD-1k better than training on videos taken from the same dataset.\n\n\u2022 On LIVE-Qualcomm the cross-dataset test performances of all our models are slightly better than V-BLIINDS (0.60), when it is trained and tested on LIVE-Qualcomm. Since V-BLIINDS has been the de facto baseline method, this is a remarkable result. Additionally, for a cross-dataset test our proposed KonVid-150k dataset shows the best generalization to LIVE-Qualcomm, improving the previous best 0.49 SRCC to 0.64.\n\n\u2022 Next, our models struggle with CVD2014, as none of them beat even the most dated classical models trained and tested on CVD2014 itself. This may be in part due to the nature of the degradations induced in the creation of the dataset, which are not native to the videos present in KonVid-150k. Moreover, the domain shift between KonVid-150k and CVD2014 seems to be larger than to the other datasets, as the previous best cross-dataset performance is also not achieved.\n\nThe cross-test performance drops notably when testing on synthetic video datasets. This has already been observed in the IQA domain [47], where training and testing on the same domain resulted in much higher performance than when the source and target domains were different. The types of distortions in individual frames of videos from two different domains result in different characteristics of the activations of Inception-net features, resulting in reduced performance.\n\n\nE. EVALUATION OF TRAINING SCHEMES\n\nAs described in Section II-A, the choice of the number of ratings per video is a distinguishing, yet so far unexplored factor in the design of VQA datasets in the context of optimizing model training performance. In order to study the effect of varying the number of ratings per video, we trained a large set of corresponding models in two experiments. In the first one, we increased the number of ratings to reduce the level of noise in the training set. In the second one, we additionally introduced the natural constraint of a vote budget, limiting the total number of ratings to a constant.\n\nIt is common to use an equal number of votes for each stimulus so that the MOS of the training, validation, and test sets have the same reliability, respectively, the same level of noise. Deep learning is known to be robust to label noise [37], however, this has been only studied when the same amount of noise is present for all items in all parts of the dataset (train/test/validation). Thus, the first question we investigate is:\n\n\u2022 What impact do different noise levels in the training and validation sets have on test set prediction performance?\n\nMore precisely, we are interested to know the change in prediction performance when fewer votes are used for training and validating deep learning models, compared to the number of votes used for test items.\n\nIn order to answer this question, we randomly sampled v = 1, 2, 4, 7, 14, 26, and 50 votes five times for each video within KonVid-150k-B and computed the corresponding MOS values (7 \u00d7 5 MOS per video). We then trained our MLSP-VQA-FF model by varying both training set, and validation set MOS vote counts while keeping the test set MOS vote count at 50. For each pair of training and validation MOS, we considered twenty random splits with 60% of the data for training, 20% for validation, and 20% for testing, with the above mentioned five versions of the MOS each. Therefore, we trained 5 \u00d7 20 \u00d7 7 \u00d7 7 = 4900 models in total. The graph in Figure 14 depicts the mean SRCC between the models' predictions and the ground truth MOS of the test sets. Each line in this graph represents a different number of votes comprising the validation MOS, whereas the x-axis indicates the number of votes comprising the training MOS. Note that the x-axis is scaled logarithmically for better visualization. There are three key observations concerning the prediction performance:\n\n\u2022 The prediction performance improves as the number of votes comprising the training MOS increases, regardless of the number of votes used for validation.\n\n\u2022 The performance improvements scale approximately logarithmically with the number of votes comprising the training MOS.\n\n\u2022 The test set performance varies less due to changes in the number of votes used for validation than it does due to the number of votes for items in the training set. The fact that performance improves with lower training label noise is not surprising. Nonetheless, the gentler slope for the performance curves beyond four votes comprising the training MOS is an indicator that the common policy to gather 25 votes for all stimuli in a dataset may be sub-optimal, due to diminishing returns. In fact, at approximately five votes (1/10th of the analysed budget) the model bridges more than 66% of the performance gap between the minimal performance at 0.55 SRCC and best performance at around .73 SRCC, suggesting it to be a good trade-off between precision and cost.\n\nThe comparison between data splits in this experiment is not balanced, because the data points in the graphs of Figure 14 correspond to different vote budgets, ranging from 1 rating per video in one instance on the left up to 50 per video on the right. The annotation of datasets in the lab and also in the crowd usually is constrained by a budget in terms of total hours of testing or overall cost of crowdsourcing. This translates to a maximum number of votes that can be attained for a given dataset. Therefore, the second question we investigate is:\n\n\u2022 Given a fixed vote budget, how does the allocation of votes on the training set affect test performance?\n\nIn other words, is it better to collect more votes for fewer stimuli, or less votes for more videos?\n\nIn order to answer this question, we first divided KonVid-150k-B into five disjoint test sets (each with 20% of all videos) and sampled the same number of videos from the remaining set of KonVid-150k-B for validation. We then considered three levels of precision at 100, 5, and 1 votes comprising the MOS of videos used in training, as well as six vote budgets of 100,000, 25,000, 10,000, 2,500, and 1,000 votes. We built the training sets accordingly, sampling from the remaining videos in KonVid-150k-B first, and then adding in videos from KonVid-150k-A, if needed, such that the smaller sets are proper subsets of the larger variants. For the vote budget of 100,000 votes we consequently created three training sets of 1,000, 20,000, and 100,000 videos at training MOS precision levels of 100, 5 and 1 vote(s), respectively. It is to be noted that the overlap between the different samples of the same sets increases as the set size increases, as the whole KonVid-150k-B set is only comprised of \u2248150,000 videos, which in turn has an effect on the standard deviation of the predictions.\n\nWe trained both MLSP-VQA-FF and MLSP-VQA-RN on the five different splits for all three vote budget distributions and reported the results in Table 5. We give the average SRCC, PLCC, and RMSE between the models' predicted scores and the MOS computed by using all available votes. There are few key takeaways from these results:\n\n\u2022 As one would suspect, the performance drops as the total vote budget decreases.\n\n\u2022 Surprisingly, however, the performance appears to be stable across the different distribution strategies for budgets of more than 1,000 votes.\n\n\u2022 For smaller vote budgets a middle ground choice between MOS precision and numbers of videos seems to be favorable, as indicated by the 5 vote MOS distribution strategy outperforming the more and less precise extreme strategies. This suggests that for very small vote budgets in particular the focus should be on fewer than the commonly suggested 30 rating MOS recommendations that are found in literature.\n\n\nVI. CONCLUSION\n\nWe introduced a large-scale in-the-wild dataset KonVid-150k for video quality assessment (VQA), as well as three novel state-of-the-art no-reference VQA methods for videos in-thewild. Our learning approach (MLSP-VQA) outperforms the best existing VQA methods trained end-to-end on several datasets, and is substantially faster to train without sacrificing any predictive power. The large size of the database and efficiency of the learning approach have enabled us to study the effect of different levels of label-noise and how the vote budget (total number of collected scores from users) affects model performance. We were able to study the effect of different vote budget distribution strategies, meaning that the number of annotated videos was adjusted according to the desired MOS precision. Under a fixed budget, we found that in most cases the number of votes allocated to each video is not important for the final model performance when using our MLSP-VQA approach and other feature-based approaches. KonVid-150k takes a novel approach to VQA, going far beyond the usual in the VQA community. The database is two orders of magnitude larger than previous published datasets, and it is more authentic both in terms of variety of content types and distortions, but also due to the compression settings of the videos. We retrieved the original video files uploaded by users from Flickr, without the default re-encoding that is generally applied by any video sharing platform to reduce playback bandwidth costs. We encoded the raw video files ourselves at a high enough quality to ensure the right balance between quality and size constraints for crowdsourcing.\n\nThe main novelty of the proposed MLSP-VQA-HYB method is the two-channel architecture. By global average pooling the activation maps of all kernels in the Inception modules of an InceptionResNet-v2 network trained on Ima-geNet, we extract a wide variety of features, ranging from detections of oriented edges to more abstract ones related to object category. These features are input to the partially recurrent DNN architectures, which on the one hand makes use of the temporal sequence of the frame features, while on the other also considering the individual frame features as well.\n\nWe have trained and validated the proposed methods on the four most relevant VQA datasets, improving state-of-the-art performance on KoNViD-1k. Our models fall short on LIVE-VQC, which we assume is cause by the many different types of resolutions present in the dataset. While a few works outperform our proposed method on the LIVE-Qualcomm and CVD2014, this is likely due to the artificial nature of degradations in these datasets that our feature extraction network is not trained on. We also show that our proposed method outperforms the current state-of-the-art on KonVid-150k-B, the set of 1,596 accurately labeled videos that are part of our proposed dataset. Additionally, by training our method on the entirety of the proposed noisily annotated dataset, we can improve the inter-dataset test performance on KoNViD-1k and LIVE-Qualcomm and are competitive in an inter-dataset setup on LIVE-VQC. Moreover, we surpass even the intradataset performance on the KoNViD-1k dataset by training on KonVid-150k. CVD2014 appears to be a tough challenge for our approach, both when trained in within-dataset and crossdataset scenarios.\n\nSome of our findings open up avenues for interesting future investigations. The overall very high performance of our MLSP-VQA-FF model suggests that recurrent neural networks pose difficulties for the purpose of modeling video quality which has also been reflected in recent related work [43]. Further investigations are required to understand the more nuanced reasons for this beyond the well-established challenge of vanishing gradients within recurrent networks. Moreover, it is likely that a more elaborate pooling scheme which accounts for temporal hysteresis could be beneficial for the performance of the FF model. Recent efforts in the field show promising results by investigating more elaborate temporal pooling strategies [42], [43]. Combining our efforts of extracting features from all levels of a pre-trained network with pooling strategies that account for particular temporal effects is a key challenge in further improving quality prediction of videos in-the-wild. His work focuses on image and video quality assessment using deep learning and the subjective annotation of databases for this purpose using crowdsourcing. As a member of the Multimedia Signal Processing Group, Konstanz, he coauthored multiple video quality datasets. As part of the SFB-TRR 161 Quantitative Methods for Visual Computing, he established and co-organized the crowdsourcing workshop and has given recurring lectures on the topic at the University of Konstanz.\n\nVLAD HOSU received the Ph.D. degree from the National University of Singapore, in 2014. He was a Research Fellow with NUS. He has been holding a postdoctoral position with the Department of Computer and Information Science, University of Konstanz, Germany, since 2016. His research interests include visual quality assessment, image enhancement, crowdsourcing strategies, understanding, and modeling human visual perception via machine learning.\n\nHANHE LIN received the Ph.D. degree from the Department of Information Science, University of Otago, New Zealand, in 2016. He is currently a Postdoctoral Researcher with the Department of Computer and Information Science, University of Konstanz, Germany. His research interests include machine learning and deep learningbased application, visual quality assessment, and crowdsourcing.\n\nDIETMAR SAUPE was born in Bremen, Germany, in 1954. He received the Dr. rer. nat. degree in mathematics from the University of Bremen, Germany, in 1982. From 1985 to 1993, he was an Assistant Professor with the Departments of Mathematics, first at the University of California, Santa Cruz, USA, and then at the University of Bremen, resulting in his habilitation. From 1993 to 1998, he was a Professor of computer science with the University of Freiburg, Germany, the University of Leipzig, Germany, until 2002, and since then, the University of Konstanz, Germany. He is the coauthor of the book Chaos and Fractals, which won the Association of American Publishers Award for Best Mathematics Book of the Year, in 1992, and well over 100 research articles. His research interests include image and video processing, computer graphics, scientific visualisation, dynamical systems, and sport informatics.\n\nFIGURE 1 .\n1Comparison of size characteristics of current VQA datasets. Our proposed datasets, KonVid-150k-A and KonVid-150k-B are represented by the two right most bars of the histograms. Note the logarithmic scale.\n\nFIGURE 2 .\n2Comparison of the quality of the original (center) to the version Flickr provides (right) and our transcoded version (left).\n\nFigure 2\n2is a visual comparison of the different video versions, showing a small crop of a frame of the originally uploaded video together with the two re-encodings offered by Flickr and our own version. Compression artifacts are clearly visible in the Flickr re-encoded version, whereas our re-encoding is very similar to the original.\n\nFIGURE 3 .\n3Still images from videos closest to the query video on the left as measured by the Euclidean distance d in the feature space of top-layer features from Inception-ResNet-v2. This shows the utility of activations of layers from pre-trained DCNNs for usage in a content similarity measure. Even though only the 1792 activations of the last layer were used, which are commonly understood to focus on semantic entities more so than low level structures, these features encode useful information.\n\nFIGURE 4 .\n4This figure shows how well a video dataset covers all others together. The curves are the empirical cumulative histograms of Euclidean distances d (x c ,\n\nFigure 5\n5then shows d(X , Y ) for the competing dataset pairs individually. It can be seen that KonVid-150k covers the\n\nFIGURE 5 .\n5Pairwise comparison of content coverage. Empirical cumulative histograms of d (x, Y ) for all x \u2208 X . The green, red, blue, yellow, and cyan line colors refer to the covering set Y and the different line styles refer to X being KonVid-150k, KoNViD-1k, CVD 2014, Qualcomm, and VQC, respectively. As expected from the previous figure, KonVid-150k covers the other datasets the best, indicated by the four green plots consistently falling to the left of their counterparts.\n\nFIGURE 7 .\n7Simplified work flow diagram of the experiment. A worker is first presented with a quiz page of test rows (TR, in yellow) with three test videos and twelve data videos each. Upon passing the quiz with \u226570% accuracy they proceed to answer data pages with one test row per page. Data rows (DR, in white) contain 15 data videos. Data rows are annotated by five unique participants. Test rows can be answered once by each worker.\n\nFIGURE 8 .\n8Comparison of the SOS hypothesis[63] of KoNViD-1k, CVD2014, and KonVid-150k-B. The SOS parameter for the three datasets are a = 0.14, a = 0.17, and a = 0.21, respectively. For VQA the typical range is a \u2208 [0.11, 0.21], which shows that KonVid-150k can be considered a typical example in terms of annotation quality.which allows only for 21 different MOS values, and therefore we did not include it in the figure. Nonetheless, KonVid-150k-B is a good estimation of what can be expected in terms of annotation quality of KonVid-150k as a whole. The figure shows the comparison between KoNViD-1k, CVD2014, and KonVid-150k-B, where the latter has an SOS parameter of a KonVid-150k-B = 0.21, which lies within the typical range for VQA experiments.\n\nFIGURE 9 .\n9FIGURE 9. Extraction of multi-level spatially-pooled (MLSP) features from a video frame, using an InceptionResNet-v2 model pre-trained on ImageNet. The features encode quality-related information: earlier layers describe low-level image details, e.g. image sharpness or noise, and later layers function as object detectors or encode visual appearance information. Global Average Pooling (GAP) is applied to the activations resulting from the Stem, each Inception-module, as well as the Reduction-modules, and finally concatenated to form MLSP features. For more information regarding the individual blocks please refer to the original paper [27].\n\nFIGURE 10 .\n10Visualization of the variation of activation levels of MLSP features over the course of KonVid-150k videos. In the center, the median level of activation for each of the 43 blocks from the Inception-ResNet-v2 network is displayed for 3 sample videos. The black whiskers indicate the 50% confidence interval on the level of activation. For the first block (Stem), the whiskers extend to 0.7. The left and right plots show the activation of 1/8th of the first and last blocks' features over time.\n\n\nof 10 \u22122 , and both the MLSP-VQA-RN and the MLSP-VQA-HYB models were trained with a learning rate of 10 \u22124 .\n\nFIGURE 11 .\n11Left: The MLSP-VQA-FF model, that relies on average frame MLSP features and a densely connected feed forward network. Middle: The MLSP-VQA-RN recurrent model, implementing a stacked long short-term memory network. Right: The hybrid MLSP-VQA-HYB dual channel model, that has a bigger variant of the FF network on the left and the recurrent part of the RN network on the right. Both channels output activations at each timestep and are merged along the feature dimension, before feeding into a small prediction head. Both the RN and HYB models take corresponding frame features at each time step as an input to the network.\n\nFIGURE 12 .\n12A visualization of the convergence of different transfer learning techniques along with information about the training times. The solid lines depict the first training stage of 40 epochs, where the IRNV2 (orange) and IRNV2-MLSP (blue) architectures have their weights frozen. The dashed lines represent the second training stage of 40 epochs where all models had their weights unfrozen.For the second stage we start from the best performing model according to validation loss from the previous stage. This is the reason for the discontinuities between the graphs. Koncept (green) and IRNV2 connect the last layer to the small header network, while IRNV2-MLSP concatenates all individual Inception-module outputs to feed into the head. Finally, MLSP-VQA-FF works off of extracted MLSP features, which for this scenario took 38 seconds.\n\nFIGURE 13 .\n13Percent deviation of the mean RMSE of the proposed models on each of the six degradation types present in LIVE-Qualcomm (top), each of the six test scenarios in CVD2014 (middle), and the different resolutions in LIVE-VQC (bottom).\n\nFIGURE 14 .\n14This plot summarizes the evaluation of MLSP-VQA-FF models trained on KonVid-150k-B using different numbers of votes comprising the training or validation MOS, indicated by the x axis and the color of the graphs, respectively. The y-axis shows the average of 20 models' SRCC between the predicted MOS values on the test set and the ground truth data, which is comprised of 50 votes.\n\nFRANZ\nG\u00d6TZ-HAHN was born in Zierenberg, Germany, in 1987. He received the B.S. degree in knowledge engineering and the M.S. degree in artificial intelligence from Maastricht University, in 2011 and 2015, respectively. He is currently pursuing the Ph.D. degree in computer science with the University of Konstanz.\n\nTABLE 1 .\n1Training settings and parameters.\n\nTABLE 2 .\n2Results of different NR-VQA metrics on different authentic VQA datasets. Top performance of each dataset is highlighted.\n\nTABLE 4 .\n4Inter-dataset test performance comparison of our three models averaged over 10 splits trained on the entirety of KonVid-150k-A when compared with previous best results (See the table notes for the sources of the performance numbers.). The first row additionally contains the best intra-dataset performance. The different splits only affect the validation and test sets, as all videos of KonVid-150k-A are used for training.\n\nTABLE 5 .\n5Performance of our FF model at a fixed vote budget of 100,000, 25,000, 10,000, 2,500, and 1,000 votes.\nThe paper also references DeepRN[40] as a better model, however the results of DeepRN for KonIQ-10k have since been shown to be incorrect[41] \nhttp://www.figure-eight.com/ (now https://appen.com/) 72146 VOLUME 9, 2021\nIn some rare (\u2264 1%) cases users bypassed our restrictions by disabling javascript and were able to proceed without actually rating the videos. In that case the required 5 votes were not met, and we had to discard this video. Additionally, not all videos were readable by the Python libraries we used as feature extractors. Those videos were also removed.VOLUME 9, 2021   \n\u2022 The cross-dataset test performance of the FF model on KoNViD-1k of 0.83 SRCC is higher than all other within-dataset test performances and especially any cross-test setups. This again underlines the potential5 These results are taken from[18].VOLUME 9, 2021   \n\nWyzowl State of Video Marketing Statistics. Wyzowl, Wyzowl. (2019). Wyzowl State of Video Marketing Statistics 2019. Accessed: Nov. 15, 2019. [Online]. Available: https://info. wyzowl.com/state-of-video-marketing-2019-report\n\nState of Social. Buffer. (2019). State of Social 2019 Report. Accessed: Nov. 15, 2019. [Online]. Available: https://buffer.com/state-of-social-2019\n\nK Westcott, J Loucks, K Downs, J Watson, Digital Media Trends Survey. Hermitage, TN, USADeloitte12th edK. Westcott, J. Loucks, K. Downs, and J. Watson, Digital Media Trends Survey, 12th ed. Hermitage, TN, USA: Deloitte, 2018.\n\nCisco visual networking index: Forecast and trends. ; Cisco, Cisco, San Vni, Jose, Usa Ca, White Paper, 1Cisco, ''Cisco visual networking index: Forecast and trends, 2017-2022,'' Cisco, VNI, San Jose, CA, USA, White Paper, vol. 1, 2018.\n\nYou Know What's Cool? A Billion Hours. C Goodrow, C. Goodrow. (2017). You Know What's Cool? A Billion Hours. Accessed: Nov. 15, 2019. [Online]. Available: https://youtube. googleblog.com/2017/02/you-know-whats-cool-billion-hours.html\n\nSimulation techniques in environmental psychology. G E Mckechnie, SpringerBoston, MA, USAG. E. McKechnie, ''Simulation techniques in environmental psychol- ogy,'' in Perspectives on Environment and Behavior. Boston, MA, USA: Springer, 1977, pp. 169-189.\n\nC S Ang, A Bobrowicz, D J Schiano, B Nardi, Some reflections,'' Interactions. 20C. S. Ang, A. Bobrowicz, D. J. Schiano, and B. Nardi, ''Data in the wild: Some reflections,'' Interactions, vol. 20, no. 2, pp. 39-43, Mar. 2013.\n\nNo-reference video quality assessment for SD and HD H.264/AVC sequences based on continuous estimates of packet loss visibility. S Argyropoulos, A Raake, M.-N Garcia, P List, Proc. 3rd Int. Workshop Qual. Multimedia Exper. 3rd Int. Workshop Qual. Multimedia ExperS. Argyropoulos, A. Raake, M.-N. Garcia, and P. List, ''No-reference video quality assessment for SD and HD H.264/AVC sequences based on continuous estimates of packet loss visibility,'' in Proc. 3rd Int. Workshop Qual. Multimedia Exper., Sep. 2011, pp. 31-36.\n\nPrediction of transmission distortion for wireless video communication: Analysis. Z Chen, D Wu, IEEE Trans. Image Process. 213Z. Chen and D. Wu, ''Prediction of transmission distortion for wireless video communication: Analysis,'' IEEE Trans. Image Process., vol. 21, no. 3, pp. 1123-1137, Mar. 2012.\n\nNo-reference pixel video quality monitoring of channel-induced distortion. G Valenzise, S Magni, M Tagliasacchi, S Tubaro, IEEE Trans. Circuits Syst. Video Technol. 224G. Valenzise, S. Magni, M. Tagliasacchi, and S. Tubaro, ''No-reference pixel video quality monitoring of channel-induced distortion,'' IEEE Trans. Circuits Syst. Video Technol., vol. 22, no. 4, pp. 605-618, Apr. 2012.\n\nBlind prediction of natural video quality. M A Saad, A C Bovik, C Charrier, IEEE Trans. Image Process. 233M. A. Saad, A. C. Bovik, and C. Charrier, ''Blind prediction of natural video quality,'' IEEE Trans. Image Process., vol. 23, no. 3, pp. 1352-1365, Mar. 2014.\n\nA noreference bitstream-based perceptual model for video quality estimation of videos affected by coding artifacts and packet losses. K Pandremmenou, M Shahid, L P Kondi, B L\u00f6vstr\u00f6m, Proc. 20th. 20thK. Pandremmenou, M. Shahid, L. P. Kondi, and B. L\u00f6vstr\u00f6m, ''A no- reference bitstream-based perceptual model for video quality estimation of videos affected by coding artifacts and packet losses,'' in Proc. 20th\n\n. International Society for Optics and Photonics. 9394Hum. Vis. Electron. Imag.. Art. no. 93941FHum. Vis. Electron. Imag., vol. 9394. International Society for Optics and Photonics, 2015, Art. no. 93941F.\n\nDesign of noreference video quality metrics with multiway partial least squares regression. C Keimel, J Habigt, M Klimpke, K Diepold, Proc. 3rd Int. Workshop Qual. Multimedia Exper. 3rd Int. Workshop Qual. Multimedia ExperC. Keimel, J. Habigt, M. Klimpke, and K. Diepold, ''Design of no- reference video quality metrics with multiway partial least squares regression,'' in Proc. 3rd Int. Workshop Qual. Multimedia Exper., 2011, pp. 49-54.\n\nNo-reference video quality assessment based on artifact measurement and statistical analysis. K Zhu, C Li, V Asari, D Saupe, IEEE Trans. Circuits Syst. Video Technol. 254K. Zhu, C. Li, V. Asari, and D. Saupe, ''No-reference video quality assessment based on artifact measurement and statistical analysis,'' IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 4, pp. 533-546, Apr. 2015.\n\nNo-reference video quality assessment using codec analysis. J Sogaard, S Forchhammer, J Korhonen, IEEE Trans. Circuits Syst. Video Technol. 2510J. Sogaard, S. Forchhammer, and J. Korhonen, ''No-reference video quality assessment using codec analysis,'' IEEE Trans. Circuits Syst. Video Tech- nol., vol. 25, no. 10, pp. 1637-1650, Oct. 2015.\n\nA completely blind video integrity oracle. A Mittal, M A Saad, A C Bovik, IEEE Trans. Image Process. 251A. Mittal, M. A. Saad, and A. C. Bovik, ''A completely blind video integrity oracle,'' IEEE Trans. Image Process., vol. 25, no. 1, pp. 289-300, Jan. 2016.\n\nPredictive noreference assessment of video quality. M T Vega, D C Mocanu, S Stavrou, A Liotta, Image Commun. 52Signal Process.M. T. Vega, D. C. Mocanu, S. Stavrou, and A. Liotta, ''Predictive no- reference assessment of video quality,'' Signal Process., Image Commun., vol. 52, pp. 20-32, Mar. 2017.\n\nLearning-based prediction of packet loss artifact visibility in networked video. J Korhonen, Proc. 10th Int. Conf. Qual. Multimedia Exper. 10th Int. Conf. Qual. Multimedia ExperJ. Korhonen, ''Learning-based prediction of packet loss artifact visibility in networked video,'' in Proc. 10th Int. Conf. Qual. Multimedia Exper., 2018, pp. 1-6.\n\nTwo-level approach for no-reference consumer video quality assessment. J Korhonen, IEEE Trans. Image Process. 2812J. Korhonen, ''Two-level approach for no-reference consumer video quality assessment,'' IEEE Trans. Image Process., vol. 28, no. 12, pp. 5923-5938, Dec. 2019.\n\nV Hosu, F Hahn, M Jenadeleh, H Lin, H Men, T Szir\u00e1nyi, S Li, D Saupe, The konstanz natural video database (KoNViD-1k),'' in Proc. 9th Int. Conf. Qual. Multimedia Exper. (QoMEX). V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Szir\u00e1nyi, S. Li, and D. Saupe, ''The konstanz natural video database (KoNViD-1k),'' in Proc. 9th Int. Conf. Qual. Multimedia Exper. (QoMEX), May 2017, pp. 1-6.\n\nCVD2014-A database for evaluating no-reference video quality assessment algorithms. M Nuutinen, T Virtanen, M Vaahteranoksa, T Vuori, P Oittinen, J H\u00e4kkinen, IEEE Trans. Image Process. 257M. Nuutinen, T. Virtanen, M. Vaahteranoksa, T. Vuori, P. Oittinen, and J. H\u00e4kkinen, ''CVD2014-A database for evaluating no-reference video quality assessment algorithms,'' IEEE Trans. Image Process., vol. 25, no. 7, pp. 3073-3086, Jul. 2016.\n\nIn-capture mobile video distortions: A study of subjective behavior and objective algorithms. D Ghadiyaram, J Pan, A C Bovik, A K Moorthy, P Panda, K.-C Yang, IEEE Trans. Circuits Syst. Video Technol. 289D. Ghadiyaram, J. Pan, A. C. Bovik, A. K. Moorthy, P. Panda, and K.-C. Yang, ''In-capture mobile video distortions: A study of subjective behavior and objective algorithms,'' IEEE Trans. Circuits Syst. Video Technol., vol. 28, no. 9, pp. 2061-2077, Sep. 2018.\n\nLarge-scale study of perceptual video quality. Z Sinno, A C Bovik, IEEE Trans. Image Process. 282Z. Sinno and A. C. Bovik, ''Large-scale study of perceptual video quality,'' IEEE Trans. Image Process., vol. 28, no. 2, pp. 612-627, Feb. 2019.\n\nBlind image quality prediction by exploiting multi-level deep representations. F Gao, J Yu, S Zhu, Q Huang, Q Tian, Pattern Recognit. 81F. Gao, J. Yu, S. Zhu, Q. Huang, and Q. Tian, ''Blind image quality pre- diction by exploiting multi-level deep representations,'' Pattern Recognit., vol. 81, pp. 432-442, Sep. 2018.\n\nThe unreasonable effectiveness of deep features as a perceptual metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitR. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ''The unreasonable effectiveness of deep features as a perceptual metric,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 586-595.\n\nEffective aesthetics prediction with multi-level spatially pooled features. V Hosu, B Goldlucke, D Saupe, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)V. Hosu, B. Goldlucke, and D. Saupe, ''Effective aesthetics prediction with multi-level spatially pooled features,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 9375-9383.\n\nInception-v4, inception-resnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, A A , Proc. 31st AAAI Conf. Artif. Intell. 31st AAAI Conf. Artif. IntellC. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, ''Inception-v4, inception-resnet and the impact of residual connections on learning,'' in Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 4278-4284.\n\nAnalysis of representations for domain adaptation,'' in Proc. S Ben-David, J Blitzer, K Crammer, F Pereira, Adv. Neural Inf. Process. Syst. S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, ''Analysis of repre- sentations for domain adaptation,'' in Proc. Adv. Neural Inf. Process. Syst., 2007, pp. 137-144.\n\nSubjective assessment of H.264/AVC video sequences transmitted over a noisy channel. F. De Simone, M Naccari, M Tagliasacchi, F Dufaux, S Tubaro, T Ebrahimi, Proc. Int. Workshop Qual. Multimedia Exper. Int. Workshop Qual. Multimedia ExperF. De Simone, M. Naccari, M. Tagliasacchi, F. Dufaux, S. Tubaro, and T. Ebrahimi, ''Subjective assessment of H.264/AVC video sequences transmitted over a noisy channel,'' in Proc. Int. Workshop Qual. Multime- dia Exper., Jul. 2009, pp. 204-209.\n\nA H. 264/AVC video database for the evaluation of quality metrics,'' in Proc. F. De Simone, M Tagliasacchi, M Naccari, S Tubaro, T Ebrahimi, IEEE Int. Conf. Acoust., Speech Signal Process. F. De Simone, M. Tagliasacchi, M. Naccari, S. Tubaro, and T. Ebrahimi, ''A H. 264/AVC video database for the evaluation of quality metrics,'' in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., Mar. 2010, pp. 2430-2433.\n\nStudy of subjective and objective quality assessment of video. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, IEEE Trans. Image Process. 196K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack, ''Study of subjective and objective quality assessment of video,'' IEEE Trans. Image Process., vol. 19, no. 6, pp. 1427-1441, Jun. 2010.\n\nA subjective study to evaluate video quality assessment algorithms. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, Proc. 15th Hum. Vis. Electron. Imag. 15th Hum. Vis. Electron. Imag7527Art. no. 75270HK. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack, ''A subjective study to evaluate video quality assessment algorithms,'' in Proc. 15th Hum. Vis. Electron. Imag., vol. 7527. International Society for Optics and Photonics, 2010, Art. no. 75270H.\n\nMost apparent distortion: Full-reference image quality assessment and the role of strategy. D M Chandler, J. Electron. Imag. 191Art. no. 011006D. M. Chandler, ''Most apparent distortion: Full-reference image quality assessment and the role of strategy,'' J. Electron. Imag., vol. 19, no. 1, Jan. 2010, Art. no. 011006.\n\nReport on the Validation of Video Quality Models for High Definition Video Content. Video Quality Experts Group.Video Quality Experts Group. (2010). Report on the Validation of Video Quality Models for High Definition Video Content. [Online]. Available: http://www.its.bldrdoc.gov/media/4212/vqeg_hdtv_final_report_version_ 2.0.zip\n\nIVP subjective quality video database. F Zhang, S Li, L Ma, Y C Wong, K N Ngan, Hong KongChin. Univ. Hong KongTech. Rep.F. Zhang, S. Li, L. Ma, Y. C. Wong, and K. N. Ngan, ''IVP subjective quality video database,'' Chin. Univ. Hong Kong, Hong Kong, Tech. Rep., 2011. [Online]. Available: http://ivp.ee.cuhk.educ. hk/research/database/subjective\n\nCrowd workers proven useful: A comparative study of subjective video quality assessment. D Saupe, F Hahn, V Hosu, I Zingman, M Rana, S Li, Proc. Int. Conf. Qual. Multimedia Exper. (QoMEX). Int. Conf. Qual. Multimedia Exper. (QoMEX)D. Saupe, F. Hahn, V. Hosu, I. Zingman, M. Rana, and S. Li, ''Crowd workers proven useful: A comparative study of subjective video qual- ity assessment,'' in Proc. Int. Conf. Qual. Multimedia Exper. (QoMEX), 2016, pp. 1-2.\n\nDeep learning is robust to massive label noise. D Rolnick, A Veit, S Belongie, N Shavit, arXiv:1705.10694D. Rolnick, A. Veit, S. Belongie, and N. Shavit, ''Deep learning is robust to massive label noise,'' 2017, arXiv:1705.10694. [Online]. Available: http://arxiv.org/abs/1705.10694\n\nNo-reference image quality assessment using transfer learning. H Otroshi-Shahreza, A Amini, H Behroozi, Proc. 9th Int. Symp. Telecommun. (IST). 9th Int. Symp. Telecommun. (IST)H. Otroshi-Shahreza, A. Amini, and H. Behroozi, ''No-reference image quality assessment using transfer learning,'' in Proc. 9th Int. Symp. Telecommun. (IST), Dec. 2018, pp. 637-640.\n\nXception: Deep learning with depthwise separable convolutions. F Chollet, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)F. Chollet, ''Xception: Deep learning with depthwise separable convo- lutions,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 1251-1258.\n\nDeepRN: A content preserving deep architecture for blind image quality assessment. D Varga, D Saupe, T Szir\u00e1nyi, Proc. IEEE Int. Conf. Multimedia Expo (ICME). IEEE Int. Conf. Multimedia Expo (ICME)D. Varga, D. Saupe, and T. Szir\u00e1nyi, ''DeepRN: A content preserving deep architecture for blind image quality assessment,'' in Proc. IEEE Int. Conf. Multimedia Expo (ICME), Jul. 2018, pp. 1-6.\n\nCritical analysis on the reproducibility of visual quality assessment using deep features. F G\u00f6tz-Hahn, V Hosu, D Saupe, arXiv:2009.053692020F. G\u00f6tz-Hahn, V. Hosu, and D. Saupe, ''Critical analysis on the repro- ducibility of visual quality assessment using deep features,'' 2020, arXiv:2009.05369. [Online]. Available: http://arxiv.org/abs/2009.05369\n\nQuality assessment of in-the-wild videos. D Li, T Jiang, M Jiang, Proc. 27th ACM Int. Conf. Multimedia. 27th ACM Int. Conf. MultimediaD. Li, T. Jiang, and M. Jiang, ''Quality assessment of in-the-wild videos,'' in Proc. 27th ACM Int. Conf. Multimedia, Oct. 2019, pp. 2351-2359.\n\nPatch-VQ: 'Patching Up' the video quality problem. Z Ying, M Mandal, D Ghadiyaram, A Bovik, arXiv:2011.135442020Z. Ying, M. Mandal, D. Ghadiyaram, and A. Bovik, ''Patch-VQ: 'Patching Up' the video quality problem,'' 2020, arXiv:2011.13544. [Online]. Avail- able: http://arxiv.org/abs/2011.13544\n\nFrom patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality. Z Ying, H Niu, P Gupta, D Mahajan, D Ghadiyaram, A Bovik, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)Z. Ying, H. Niu, P. Gupta, D. Mahajan, D. Ghadiyaram, and A. Bovik, ''From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 3575-3585.\n\nLearning spatio-temporal features with 3D residual networks for action recognition. K Hara, H Kataoka, Y Satoh, Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW). IEEE Int. Conf. Comput. Vis. Workshops (ICCVW)K. Hara, H. Kataoka, and Y. Satoh, ''Learning spatio-temporal features with 3D residual networks for action recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW), Oct. 2017, pp. 3154-3160.\n\nIncep-tionTime: Finding AlexNet for time series classification. H Fawaz, B Lucas, G Forestier, C Pelletier, D F Schmidt, J Weber, G I Webb, L Idoumghar, P.-A M\u00fcller, F Petitjean, Data Mining Knowl. Discovery. 346H. Ismail Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber, G. I. Webb, L. Idoumghar, P.-A. M\u00fcller, and F. Petitjean, ''Incep- tionTime: Finding AlexNet for time series classification,'' Data Mining Knowl. Discovery, vol. 34, no. 6, pp. 1936-1962, Nov. 2020.\n\nDeepFL-IQA: Weak supervision for deep IQA feature learning. H Lin, V Hosu, D Saupe, arXiv:2001.081132020H. Lin, V. Hosu, and D. Saupe, ''DeepFL-IQA: Weak supervision for deep IQA feature learning,'' 2020, arXiv:2001.08113. [Online]. Available: http://arxiv.org/abs/2001.08113\n\nMulti-pooled inception features for no-reference video quality assessment. D Varga, Proc. VISIGRAPP (4: VISAPP). VISIGRAPP (4: VISAPP)D. Varga, ''Multi-pooled inception features for no-reference video quality assessment,'' in Proc. VISIGRAPP (4: VISAPP), 2020, pp. 338-347.\n\nOn advances in statistical modeling of natural images. A Srivastava, A B Lee, E P Simoncelli, S.-C Zhu, J. Math. Imag. Vis. 181A. Srivastava, A. B. Lee, E. P. Simoncelli, and S.-C. Zhu, ''On advances in statistical modeling of natural images,'' J. Math. Imag. Vis., vol. 18, no. 1, pp. 17-33, 2003.\n\nMaking a 'completely blind' image quality analyzer. A Mittal, R Soundararajan, A C Bovik, IEEE Signal Process. Lett. 203A. Mittal, R. Soundararajan, and A. C. Bovik, ''Making a 'completely blind' image quality analyzer,'' IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209-212, Nov. 2012.\n\nNo-reference image quality assessment in the spatial domain. A Mittal, A K Moorthy, A C Bovik, IEEE Trans. Image Process. 2112A. Mittal, A. K. Moorthy, and A. C. Bovik, ''No-reference image quality assessment in the spatial domain,'' IEEE Trans. Image Process., vol. 21, no. 12, pp. 4695-4708, Dec. 2012.\n\nNo-reference video quality assessment via feature learning. J Xu, P Ye, Y Liu, D Doermann, Proc. IEEE Int. Conf. Image Process. IEEE Int. Conf. Image essJ. Xu, P. Ye, Y. Liu, and D. Doermann, ''No-reference video quality assessment via feature learning,'' in Proc. IEEE Int. Conf. Image Process., Oct. 2014, pp. 491-495.\n\nNo-reference quality assessment of tone-mapped HDR pictures. D Kundu, D Ghadiyaram, A C Bovik, B L Evans, IEEE Trans. Image Process. 266D. Kundu, D. Ghadiyaram, A. C. Bovik, and B. L. Evans, ''No-reference quality assessment of tone-mapped HDR pictures,'' IEEE Trans. Image Process., vol. 26, no. 6, pp. 2957-2971, Jun. 2017.\n\nNo-reference video quality assessment with 3D shearlet transform and convolutional neural networks. Y Li, L.-M Po, C.-H Cheung, X Xu, L Feng, F Yuan, K.-W Cheung, IEEE Trans. Circuits Syst. Video Technol. 266Y. Li, L.-M. Po, C.-H. Cheung, X. Xu, L. Feng, F. Yuan, and K.-W. Cheung, ''No-reference video quality assessment with 3D shearlet transform and convolutional neural networks,'' IEEE Trans. Circuits Syst. Video Technol., vol. 26, no. 6, pp. 1044-1057, Jun. 2016.\n\nCOME for no-reference video quality assessment. C Wang, L Su, W Zhang, Proc. IEEE Conf. Multimedia Inf. Process. Retr. (MIPR). IEEE Conf. Multimedia Inf. ess. Retr. (MIPR)C. Wang, L. Su, and W. Zhang, ''COME for no-reference video quality assessment,'' in Proc. IEEE Conf. Multimedia Inf. Process. Retr. (MIPR), Apr. 2018, pp. 232-237.\n\nDeep neural networks for no-reference video quality assessment. J You, J Korhonen, Proc. IEEE Int. Conf. Image Process. (ICIP). IEEE Int. Conf. Image ess. (ICIP)J. You and J. Korhonen, ''Deep neural networks for no-reference video quality assessment,'' in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2019, pp. 2349-2353.\n\nThe kinetics human action video dataset. W Kay, J Carreira, K Simonyan, B Zhang, C Hillier, S Vijayanarasimhan, F Viola, T Green, T Back, P Natsev, M Suleyman, A Zisserman, arXiv:1705.06950W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman, ''The kinetics human action video dataset,'' 2017, arXiv:1705.06950. [Online]. Available: http://arxiv.org/abs/1705.06950\n\nNo-reference video quality assessment based on the temporal pooling of deep features. D Varga, Neural Process. Lett. 503D. Varga, ''No-reference video quality assessment based on the tem- poral pooling of deep features,'' Neural Process. Lett., vol. 50, no. 3, pp. 2595-2608, Dec. 2019.\n\nNo-reference video quality assessment via pretrained CNN and LSTM networks. D Varga, T Szir\u00e1nyi, Image Video Process. 138SignalD. Varga and T. Szir\u00e1nyi, ''No-reference video quality assessment via pretrained CNN and LSTM networks,'' Signal, Image Video Process., vol. 13, no. 8, pp. 1569-1576, Nov. 2019.\n\nTemporal hysteresis model of time varying subjective video quality. K Seshadrinathan, A C Bovik, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)K. Seshadrinathan and A. C. Bovik, ''Temporal hysteresis model of time varying subjective video quality,'' in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2011, pp. 1153-1156.\n\nKonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment. V Hosu, H Lin, T Sziranyi, D Saupe, IEEE Trans. Image Process. 29V. Hosu, H. Lin, T. Sziranyi, and D. Saupe, ''KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment,'' IEEE Trans. Image Process., vol. 29, pp. 4041-4056, 2020.\n\nObjective Perceptual Assessment of Video Quality: Full Reference Television, document Tutorial. ITU-T Telecommunication Standardization BureauObjective Perceptual Assessment of Video Quality: Full Reference Tele- vision, document Tutorial, ITU-T Telecommunication Standardization Bureau, 2004.\n\nSOS: The MOS is not enough!. T Ho\u00dffeld, R Schatz, S Egger, Proc. 3rd Int. Workshop Qual. Multimedia Exper. 3rd Int. Workshop Qual. Multimedia ExperT. Ho\u00dffeld, R. Schatz, and S. Egger, ''SOS: The MOS is not enough!'' in Proc. 3rd Int. Workshop Qual. Multimedia Exper., Sep. 2011, pp. 131-136.\n\nThe accuracy of subjects in a quality experiment: A theoretical subject model. L Janowski, M Pinson, IEEE Trans. Multimedia. 1712L. Janowski and M. Pinson, ''The accuracy of subjects in a quality exper- iment: A theoretical subject model,'' IEEE Trans. Multimedia, vol. 17, no. 12, pp. 2210-2224, Dec. 2015.\n\nTraining and analysing deep recurrent neural networks,'' in Proc. M Hermans, B Schrauwen, Adv. Neural Inf. Process. Syst. M. Hermans and B. Schrauwen, ''Training and analysing deep recur- rent neural networks,'' in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 190-198.\n", "annotations": {"author": "[{\"start\":\"165\",\"end\":\"257\"},{\"start\":\"258\",\"end\":\"344\"},{\"start\":\"345\",\"end\":\"434\"},{\"start\":\"435\",\"end\":\"525\"},{\"start\":\"526\",\"end\":\"618\"}]", "publisher": null, "author_last_name": "[{\"start\":\"171\",\"end\":\"180\"},{\"start\":\"263\",\"end\":\"267\"},{\"start\":\"354\",\"end\":\"357\"},{\"start\":\"443\",\"end\":\"448\"},{\"start\":\"532\",\"end\":\"541\"}]", "author_first_name": "[{\"start\":\"165\",\"end\":\"170\"},{\"start\":\"258\",\"end\":\"262\"},{\"start\":\"348\",\"end\":\"353\"},{\"start\":\"435\",\"end\":\"442\"},{\"start\":\"526\",\"end\":\"531\"}]", "author_affiliation": "[{\"start\":\"182\",\"end\":\"256\"},{\"start\":\"269\",\"end\":\"343\"},{\"start\":\"359\",\"end\":\"433\"},{\"start\":\"450\",\"end\":\"524\"},{\"start\":\"543\",\"end\":\"617\"}]", "title": "[{\"start\":\"1\",\"end\":\"162\"},{\"start\":\"619\",\"end\":\"780\"}]", "venue": null, "abstract": "[{\"start\":\"1108\",\"end\":\"2873\"}]", "bib_ref": "[{\"start\":\"2951\",\"end\":\"2954\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"3151\",\"end\":\"3154\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"3307\",\"end\":\"3310\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3418\",\"end\":\"3421\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3528\",\"end\":\"3531\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3647\",\"end\":\"3650\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4785\",\"end\":\"4799\"},{\"start\":\"5076\",\"end\":\"5079\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"5562\",\"end\":\"5565\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"5918\",\"end\":\"5921\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5923\",\"end\":\"5927\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"6795\",\"end\":\"6799\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7647\",\"end\":\"7651\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7653\",\"end\":\"7657\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"8738\",\"end\":\"8742\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"8744\",\"end\":\"8748\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"9111\",\"end\":\"9115\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"9223\",\"end\":\"9227\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"10699\",\"end\":\"10703\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"12113\",\"end\":\"12117\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"12119\",\"end\":\"12123\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"12134\",\"end\":\"12138\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"12140\",\"end\":\"12144\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"12151\",\"end\":\"12155\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"12165\",\"end\":\"12169\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"12179\",\"end\":\"12183\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"12899\",\"end\":\"12903\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"12919\",\"end\":\"12923\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"12935\",\"end\":\"12939\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"12954\",\"end\":\"12958\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"17488\",\"end\":\"17492\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"18181\",\"end\":\"18185\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"19277\",\"end\":\"19281\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"19770\",\"end\":\"19774\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"19902\",\"end\":\"19906\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"20317\",\"end\":\"20318\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"20347\",\"end\":\"20351\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"20353\",\"end\":\"20357\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"20669\",\"end\":\"20673\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"20946\",\"end\":\"20950\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"21017\",\"end\":\"21021\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"21083\",\"end\":\"21087\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"21174\",\"end\":\"21178\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"21235\",\"end\":\"21239\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"21768\",\"end\":\"21772\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"22317\",\"end\":\"22321\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"22555\",\"end\":\"22559\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"23293\",\"end\":\"23297\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"24160\",\"end\":\"24164\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"24276\",\"end\":\"24280\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"24344\",\"end\":\"24348\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"24424\",\"end\":\"24428\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"24493\",\"end\":\"24497\",\"attributes\":{\"ref_id\":\"b53\"}},{\"start\":\"25020\",\"end\":\"25024\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"25764\",\"end\":\"25768\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"25974\",\"end\":\"25978\",\"attributes\":{\"ref_id\":\"b54\"}},{\"start\":\"26152\",\"end\":\"26156\",\"attributes\":{\"ref_id\":\"b55\"}},{\"start\":\"26646\",\"end\":\"26650\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"26669\",\"end\":\"26673\",\"attributes\":{\"ref_id\":\"b56\"}},{\"start\":\"27638\",\"end\":\"27642\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"28009\",\"end\":\"28013\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"28179\",\"end\":\"28183\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"28272\",\"end\":\"28276\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"28312\",\"end\":\"28316\",\"attributes\":{\"ref_id\":\"b57\"}},{\"start\":\"28339\",\"end\":\"28353\"},{\"start\":\"28462\",\"end\":\"28466\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"28586\",\"end\":\"28590\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"28592\",\"end\":\"28596\",\"attributes\":{\"ref_id\":\"b58\"}},{\"start\":\"28598\",\"end\":\"28602\",\"attributes\":{\"ref_id\":\"b59\"}},{\"start\":\"28687\",\"end\":\"28691\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"28770\",\"end\":\"28774\",\"attributes\":{\"ref_id\":\"b58\"}},{\"start\":\"28779\",\"end\":\"28783\",\"attributes\":{\"ref_id\":\"b59\"}},{\"start\":\"29021\",\"end\":\"29025\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"29149\",\"end\":\"29153\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"31392\",\"end\":\"31396\",\"attributes\":{\"ref_id\":\"b60\"}},{\"start\":\"33378\",\"end\":\"33382\",\"attributes\":{\"ref_id\":\"b61\"}},{\"start\":\"38647\",\"end\":\"38651\",\"attributes\":{\"ref_id\":\"b62\"}},{\"start\":\"40379\",\"end\":\"40383\",\"attributes\":{\"ref_id\":\"b60\"}},{\"start\":\"42558\",\"end\":\"42559\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"45812\",\"end\":\"45816\",\"attributes\":{\"ref_id\":\"b63\"}},{\"start\":\"46159\",\"end\":\"46163\",\"attributes\":{\"ref_id\":\"b64\"}},{\"start\":\"49281\",\"end\":\"49285\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"52373\",\"end\":\"52377\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"52775\",\"end\":\"52779\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"52781\",\"end\":\"52785\",\"attributes\":{\"ref_id\":\"b61\"}},{\"start\":\"52918\",\"end\":\"52922\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"54120\",\"end\":\"54124\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"54868\",\"end\":\"54872\",\"attributes\":{\"ref_id\":\"b65\"}},{\"start\":\"57725\",\"end\":\"57729\",\"attributes\":{\"ref_id\":\"b61\"}},{\"start\":\"66236\",\"end\":\"66240\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"66528\",\"end\":\"66532\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"66930\",\"end\":\"66934\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"67982\",\"end\":\"67986\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"75460\",\"end\":\"75461\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"76297\",\"end\":\"76301\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"77724\",\"end\":\"77728\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"78939\",\"end\":\"78943\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"88087\",\"end\":\"88091\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"88532\",\"end\":\"88536\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"88538\",\"end\":\"88542\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"93446\",\"end\":\"93450\",\"attributes\":{\"ref_id\":\"b63\"}},{\"start\":\"98645\",\"end\":\"98649\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"98750\",\"end\":\"98754\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"99413\",\"end\":\"99414\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"99443\",\"end\":\"99447\",\"attributes\":{\"ref_id\":\"b18\"}}]", "figure": "[{\"start\":\"90991\",\"end\":\"91208\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"91209\",\"end\":\"91346\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"91347\",\"end\":\"91685\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"91686\",\"end\":\"92189\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"92190\",\"end\":\"92356\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"92357\",\"end\":\"92477\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"92478\",\"end\":\"92961\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"92962\",\"end\":\"93400\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"93401\",\"end\":\"94157\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"94158\",\"end\":\"94817\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"94818\",\"end\":\"95327\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"95328\",\"end\":\"95438\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"95439\",\"end\":\"96075\",\"attributes\":{\"id\":\"fig_12\"}},{\"start\":\"96076\",\"end\":\"96925\",\"attributes\":{\"id\":\"fig_13\"}},{\"start\":\"96926\",\"end\":\"97171\",\"attributes\":{\"id\":\"fig_14\"}},{\"start\":\"97172\",\"end\":\"97568\",\"attributes\":{\"id\":\"fig_15\"}},{\"start\":\"97569\",\"end\":\"97882\",\"attributes\":{\"id\":\"fig_16\"}},{\"start\":\"97883\",\"end\":\"97928\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"97929\",\"end\":\"98061\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"98062\",\"end\":\"98497\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"98498\",\"end\":\"98612\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2892\",\"end\":\"4077\"},{\"start\":\"4079\",\"end\":\"4664\"},{\"start\":\"4666\",\"end\":\"6638\"},{\"start\":\"6640\",\"end\":\"8008\"},{\"start\":\"8010\",\"end\":\"8900\"},{\"start\":\"8902\",\"end\":\"9617\"},{\"start\":\"9619\",\"end\":\"10704\"},{\"start\":\"10706\",\"end\":\"10745\"},{\"start\":\"10747\",\"end\":\"10881\"},{\"start\":\"10883\",\"end\":\"11071\"},{\"start\":\"11073\",\"end\":\"11402\"},{\"start\":\"11423\",\"end\":\"11635\"},{\"start\":\"11655\",\"end\":\"11908\"},{\"start\":\"11929\",\"end\":\"12839\"},{\"start\":\"12841\",\"end\":\"13824\"},{\"start\":\"13826\",\"end\":\"14353\"},{\"start\":\"14355\",\"end\":\"14933\"},{\"start\":\"14935\",\"end\":\"15420\"},{\"start\":\"15422\",\"end\":\"15564\"},{\"start\":\"15566\",\"end\":\"16433\"},{\"start\":\"16462\",\"end\":\"17851\"},{\"start\":\"17878\",\"end\":\"18166\"},{\"start\":\"18168\",\"end\":\"19639\"},{\"start\":\"19670\",\"end\":\"21211\"},{\"start\":\"21213\",\"end\":\"21756\"},{\"start\":\"21758\",\"end\":\"22512\"},{\"start\":\"22514\",\"end\":\"23288\"},{\"start\":\"23290\",\"end\":\"23791\"},{\"start\":\"23805\",\"end\":\"24045\"},{\"start\":\"24047\",\"end\":\"24875\"},{\"start\":\"24877\",\"end\":\"25389\"},{\"start\":\"25391\",\"end\":\"26638\"},{\"start\":\"26640\",\"end\":\"27587\"},{\"start\":\"27589\",\"end\":\"27994\"},{\"start\":\"27996\",\"end\":\"28526\"},{\"start\":\"28528\",\"end\":\"29430\"},{\"start\":\"29470\",\"end\":\"29928\"},{\"start\":\"29949\",\"end\":\"30711\"},{\"start\":\"30713\",\"end\":\"31158\"},{\"start\":\"31160\",\"end\":\"31908\"},{\"start\":\"31910\",\"end\":\"32413\"},{\"start\":\"32415\",\"end\":\"32611\"},{\"start\":\"32637\",\"end\":\"34601\"},{\"start\":\"34603\",\"end\":\"34892\"},{\"start\":\"35099\",\"end\":\"35312\"},{\"start\":\"35357\",\"end\":\"35781\"},{\"start\":\"35783\",\"end\":\"36077\"},{\"start\":\"36079\",\"end\":\"36639\"},{\"start\":\"36641\",\"end\":\"36831\"},{\"start\":\"36894\",\"end\":\"38264\"},{\"start\":\"38266\",\"end\":\"38444\"},{\"start\":\"38468\",\"end\":\"39382\"},{\"start\":\"39384\",\"end\":\"40280\"},{\"start\":\"40282\",\"end\":\"41078\"},{\"start\":\"41080\",\"end\":\"41499\"},{\"start\":\"41501\",\"end\":\"41812\"},{\"start\":\"41814\",\"end\":\"42439\"},{\"start\":\"42441\",\"end\":\"42893\"},{\"start\":\"42895\",\"end\":\"43450\"},{\"start\":\"43452\",\"end\":\"44336\"},{\"start\":\"44338\",\"end\":\"45004\"},{\"start\":\"45030\",\"end\":\"45871\"},{\"start\":\"45906\",\"end\":\"46519\"},{\"start\":\"46521\",\"end\":\"46971\"},{\"start\":\"46973\",\"end\":\"47519\"},{\"start\":\"47521\",\"end\":\"47850\"},{\"start\":\"47852\",\"end\":\"49286\"},{\"start\":\"49288\",\"end\":\"50141\"},{\"start\":\"50174\",\"end\":\"50611\"},{\"start\":\"50637\",\"end\":\"51184\"},{\"start\":\"51186\",\"end\":\"51533\"},{\"start\":\"51535\",\"end\":\"53845\"},{\"start\":\"53881\",\"end\":\"54358\"},{\"start\":\"54360\",\"end\":\"54556\"},{\"start\":\"54558\",\"end\":\"55075\"},{\"start\":\"55077\",\"end\":\"55477\"},{\"start\":\"55479\",\"end\":\"55708\"},{\"start\":\"55744\",\"end\":\"57608\"},{\"start\":\"57610\",\"end\":\"57963\"},{\"start\":\"57965\",\"end\":\"58311\"},{\"start\":\"58313\",\"end\":\"58727\"},{\"start\":\"58729\",\"end\":\"60049\"},{\"start\":\"60051\",\"end\":\"60661\"},{\"start\":\"60663\",\"end\":\"60870\"},{\"start\":\"60894\",\"end\":\"61688\"},{\"start\":\"61708\",\"end\":\"62390\"},{\"start\":\"62392\",\"end\":\"62766\"},{\"start\":\"62768\",\"end\":\"62970\"},{\"start\":\"62972\",\"end\":\"63116\"},{\"start\":\"63118\",\"end\":\"63319\"},{\"start\":\"63321\",\"end\":\"65142\"},{\"start\":\"65179\",\"end\":\"65964\"},{\"start\":\"65966\",\"end\":\"66241\"},{\"start\":\"66243\",\"end\":\"67142\"},{\"start\":\"67144\",\"end\":\"67716\"},{\"start\":\"67718\",\"end\":\"68620\"},{\"start\":\"68622\",\"end\":\"69723\"},{\"start\":\"69725\",\"end\":\"71533\"},{\"start\":\"71568\",\"end\":\"73130\"},{\"start\":\"73132\",\"end\":\"74973\"},{\"start\":\"75006\",\"end\":\"75899\"},{\"start\":\"75901\",\"end\":\"76452\"},{\"start\":\"76454\",\"end\":\"76703\"},{\"start\":\"76705\",\"end\":\"77119\"},{\"start\":\"77121\",\"end\":\"77590\"},{\"start\":\"77592\",\"end\":\"78066\"},{\"start\":\"78104\",\"end\":\"78698\"},{\"start\":\"78700\",\"end\":\"79132\"},{\"start\":\"79134\",\"end\":\"79250\"},{\"start\":\"79252\",\"end\":\"79459\"},{\"start\":\"79461\",\"end\":\"80526\"},{\"start\":\"80528\",\"end\":\"80682\"},{\"start\":\"80684\",\"end\":\"80804\"},{\"start\":\"80806\",\"end\":\"81573\"},{\"start\":\"81575\",\"end\":\"82128\"},{\"start\":\"82130\",\"end\":\"82236\"},{\"start\":\"82238\",\"end\":\"82338\"},{\"start\":\"82340\",\"end\":\"83430\"},{\"start\":\"83432\",\"end\":\"83758\"},{\"start\":\"83760\",\"end\":\"83841\"},{\"start\":\"83843\",\"end\":\"83987\"},{\"start\":\"83989\",\"end\":\"84396\"},{\"start\":\"84415\",\"end\":\"86079\"},{\"start\":\"86081\",\"end\":\"86664\"},{\"start\":\"86666\",\"end\":\"87797\"},{\"start\":\"87799\",\"end\":\"89254\"},{\"start\":\"89256\",\"end\":\"89701\"},{\"start\":\"89703\",\"end\":\"90087\"},{\"start\":\"90089\",\"end\":\"90990\"}]", "formula": "[{\"start\":\"34893\",\"end\":\"35098\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"35313\",\"end\":\"35356\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"36832\",\"end\":\"36893\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"45872\",\"end\":\"45905\",\"attributes\":{\"id\":\"formula_3\"}}]", "table_ref": "[{\"start\":\"53441\",\"end\":\"53448\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"66419\",\"end\":\"66426\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"72453\",\"end\":\"72460\"},{\"start\":\"76120\",\"end\":\"76127\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"83573\",\"end\":\"83580\",\"attributes\":{\"ref_id\":\"tab_3\"}}]", "section_header": "[{\"start\":\"2875\",\"end\":\"2890\"},{\"start\":\"11405\",\"end\":\"11421\"},{\"start\":\"11638\",\"end\":\"11653\"},{\"start\":\"11911\",\"end\":\"11927\"},{\"start\":\"16436\",\"end\":\"16460\"},{\"start\":\"17854\",\"end\":\"17876\"},{\"start\":\"19642\",\"end\":\"19668\"},{\"start\":\"23794\",\"end\":\"23803\"},{\"start\":\"29433\",\"end\":\"29468\"},{\"start\":\"29931\",\"end\":\"29947\"},{\"start\":\"32614\",\"end\":\"32635\"},{\"start\":\"38447\",\"end\":\"38466\"},{\"start\":\"45007\",\"end\":\"45028\"},{\"start\":\"50144\",\"end\":\"50172\"},{\"start\":\"50614\",\"end\":\"50635\"},{\"start\":\"53848\",\"end\":\"53879\"},{\"start\":\"55711\",\"end\":\"55742\"},{\"start\":\"60873\",\"end\":\"60892\"},{\"start\":\"61691\",\"end\":\"61706\"},{\"start\":\"65145\",\"end\":\"65177\"},{\"start\":\"71536\",\"end\":\"71566\"},{\"start\":\"74976\",\"end\":\"75004\"},{\"start\":\"78069\",\"end\":\"78102\"},{\"start\":\"84399\",\"end\":\"84413\"},{\"start\":\"90992\",\"end\":\"91002\"},{\"start\":\"91210\",\"end\":\"91220\"},{\"start\":\"91348\",\"end\":\"91356\"},{\"start\":\"91687\",\"end\":\"91697\"},{\"start\":\"92191\",\"end\":\"92201\"},{\"start\":\"92358\",\"end\":\"92366\"},{\"start\":\"92479\",\"end\":\"92489\"},{\"start\":\"92963\",\"end\":\"92973\"},{\"start\":\"93402\",\"end\":\"93412\"},{\"start\":\"94159\",\"end\":\"94169\"},{\"start\":\"94819\",\"end\":\"94830\"},{\"start\":\"95440\",\"end\":\"95451\"},{\"start\":\"96077\",\"end\":\"96088\"},{\"start\":\"96927\",\"end\":\"96938\"},{\"start\":\"97173\",\"end\":\"97184\"},{\"start\":\"97570\",\"end\":\"97575\"},{\"start\":\"97884\",\"end\":\"97893\"},{\"start\":\"97930\",\"end\":\"97939\"},{\"start\":\"98063\",\"end\":\"98072\"},{\"start\":\"98499\",\"end\":\"98508\"}]", "table": null, "figure_caption": "[{\"start\":\"91004\",\"end\":\"91208\"},{\"start\":\"91222\",\"end\":\"91346\"},{\"start\":\"91358\",\"end\":\"91685\"},{\"start\":\"91699\",\"end\":\"92189\"},{\"start\":\"92203\",\"end\":\"92356\"},{\"start\":\"92368\",\"end\":\"92477\"},{\"start\":\"92491\",\"end\":\"92961\"},{\"start\":\"92975\",\"end\":\"93400\"},{\"start\":\"93414\",\"end\":\"94157\"},{\"start\":\"94171\",\"end\":\"94817\"},{\"start\":\"94833\",\"end\":\"95327\"},{\"start\":\"95330\",\"end\":\"95438\"},{\"start\":\"95454\",\"end\":\"96075\"},{\"start\":\"96091\",\"end\":\"96925\"},{\"start\":\"96941\",\"end\":\"97171\"},{\"start\":\"97187\",\"end\":\"97568\"},{\"start\":\"97576\",\"end\":\"97882\"},{\"start\":\"97895\",\"end\":\"97928\"},{\"start\":\"97941\",\"end\":\"98061\"},{\"start\":\"98074\",\"end\":\"98497\"},{\"start\":\"98510\",\"end\":\"98612\"}]", "figure_ref": "[{\"start\":\"19314\",\"end\":\"19322\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"33384\",\"end\":\"33392\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"36404\",\"end\":\"36412\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"37246\",\"end\":\"37254\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"37305\",\"end\":\"37308\"},{\"start\":\"39096\",\"end\":\"39104\"},{\"start\":\"41093\",\"end\":\"41105\"},{\"start\":\"41852\",\"end\":\"41860\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"42919\",\"end\":\"42931\"},{\"start\":\"46597\",\"end\":\"46605\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"52500\",\"end\":\"52506\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"53340\",\"end\":\"53349\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"54347\",\"end\":\"54356\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"59111\",\"end\":\"59120\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"68266\",\"end\":\"68275\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"68940\",\"end\":\"68949\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"70552\",\"end\":\"70561\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"80103\",\"end\":\"80112\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"81687\",\"end\":\"81696\",\"attributes\":{\"ref_id\":\"fig_0\"}}]", "bib_author_first_name": "[{\"start\":\"99842\",\"end\":\"99843\"},{\"start\":\"99854\",\"end\":\"99855\"},{\"start\":\"99864\",\"end\":\"99865\"},{\"start\":\"99873\",\"end\":\"99874\"},{\"start\":\"100121\",\"end\":\"100122\"},{\"start\":\"100137\",\"end\":\"100140\"},{\"start\":\"100152\",\"end\":\"100155\"},{\"start\":\"100160\",\"end\":\"100165\"},{\"start\":\"100346\",\"end\":\"100347\"},{\"start\":\"100593\",\"end\":\"100594\"},{\"start\":\"100595\",\"end\":\"100596\"},{\"start\":\"100797\",\"end\":\"100798\"},{\"start\":\"100799\",\"end\":\"100800\"},{\"start\":\"100806\",\"end\":\"100807\"},{\"start\":\"100819\",\"end\":\"100820\"},{\"start\":\"100821\",\"end\":\"100822\"},{\"start\":\"100832\",\"end\":\"100833\"},{\"start\":\"101153\",\"end\":\"101154\"},{\"start\":\"101169\",\"end\":\"101170\"},{\"start\":\"101178\",\"end\":\"101182\"},{\"start\":\"101191\",\"end\":\"101192\"},{\"start\":\"101631\",\"end\":\"101632\"},{\"start\":\"101639\",\"end\":\"101640\"},{\"start\":\"101926\",\"end\":\"101927\"},{\"start\":\"101939\",\"end\":\"101940\"},{\"start\":\"101948\",\"end\":\"101949\"},{\"start\":\"101964\",\"end\":\"101965\"},{\"start\":\"102281\",\"end\":\"102282\"},{\"start\":\"102283\",\"end\":\"102284\"},{\"start\":\"102291\",\"end\":\"102292\"},{\"start\":\"102293\",\"end\":\"102294\"},{\"start\":\"102302\",\"end\":\"102303\"},{\"start\":\"102638\",\"end\":\"102639\"},{\"start\":\"102654\",\"end\":\"102655\"},{\"start\":\"102664\",\"end\":\"102665\"},{\"start\":\"102666\",\"end\":\"102667\"},{\"start\":\"102675\",\"end\":\"102676\"},{\"start\":\"103214\",\"end\":\"103215\"},{\"start\":\"103224\",\"end\":\"103225\"},{\"start\":\"103234\",\"end\":\"103235\"},{\"start\":\"103245\",\"end\":\"103246\"},{\"start\":\"103656\",\"end\":\"103657\"},{\"start\":\"103663\",\"end\":\"103664\"},{\"start\":\"103669\",\"end\":\"103670\"},{\"start\":\"103678\",\"end\":\"103679\"},{\"start\":\"104013\",\"end\":\"104014\"},{\"start\":\"104024\",\"end\":\"104025\"},{\"start\":\"104039\",\"end\":\"104040\"},{\"start\":\"104338\",\"end\":\"104339\"},{\"start\":\"104348\",\"end\":\"104349\"},{\"start\":\"104350\",\"end\":\"104351\"},{\"start\":\"104358\",\"end\":\"104359\"},{\"start\":\"104360\",\"end\":\"104361\"},{\"start\":\"104607\",\"end\":\"104608\"},{\"start\":\"104609\",\"end\":\"104610\"},{\"start\":\"104617\",\"end\":\"104618\"},{\"start\":\"104619\",\"end\":\"104620\"},{\"start\":\"104629\",\"end\":\"104630\"},{\"start\":\"104640\",\"end\":\"104641\"},{\"start\":\"104937\",\"end\":\"104938\"},{\"start\":\"105268\",\"end\":\"105269\"},{\"start\":\"105471\",\"end\":\"105472\"},{\"start\":\"105479\",\"end\":\"105480\"},{\"start\":\"105487\",\"end\":\"105488\"},{\"start\":\"105500\",\"end\":\"105501\"},{\"start\":\"105507\",\"end\":\"105508\"},{\"start\":\"105514\",\"end\":\"105515\"},{\"start\":\"105526\",\"end\":\"105527\"},{\"start\":\"105532\",\"end\":\"105533\"},{\"start\":\"105945\",\"end\":\"105946\"},{\"start\":\"105957\",\"end\":\"105958\"},{\"start\":\"105969\",\"end\":\"105970\"},{\"start\":\"105986\",\"end\":\"105987\"},{\"start\":\"105995\",\"end\":\"105996\"},{\"start\":\"106007\",\"end\":\"106008\"},{\"start\":\"106386\",\"end\":\"106387\"},{\"start\":\"106400\",\"end\":\"106401\"},{\"start\":\"106407\",\"end\":\"106408\"},{\"start\":\"106409\",\"end\":\"106410\"},{\"start\":\"106418\",\"end\":\"106419\"},{\"start\":\"106420\",\"end\":\"106421\"},{\"start\":\"106431\",\"end\":\"106432\"},{\"start\":\"106440\",\"end\":\"106444\"},{\"start\":\"106804\",\"end\":\"106805\"},{\"start\":\"106813\",\"end\":\"106814\"},{\"start\":\"106815\",\"end\":\"106816\"},{\"start\":\"107079\",\"end\":\"107080\"},{\"start\":\"107086\",\"end\":\"107087\"},{\"start\":\"107092\",\"end\":\"107093\"},{\"start\":\"107099\",\"end\":\"107100\"},{\"start\":\"107108\",\"end\":\"107109\"},{\"start\":\"107392\",\"end\":\"107393\"},{\"start\":\"107401\",\"end\":\"107402\"},{\"start\":\"107410\",\"end\":\"107411\"},{\"start\":\"107412\",\"end\":\"107413\"},{\"start\":\"107421\",\"end\":\"107422\"},{\"start\":\"107434\",\"end\":\"107435\"},{\"start\":\"107831\",\"end\":\"107832\"},{\"start\":\"107839\",\"end\":\"107840\"},{\"start\":\"107852\",\"end\":\"107853\"},{\"start\":\"108263\",\"end\":\"108264\"},{\"start\":\"108274\",\"end\":\"108275\"},{\"start\":\"108283\",\"end\":\"108284\"},{\"start\":\"108296\",\"end\":\"108297\"},{\"start\":\"108298\",\"end\":\"108299\"},{\"start\":\"108633\",\"end\":\"108634\"},{\"start\":\"108646\",\"end\":\"108647\"},{\"start\":\"108657\",\"end\":\"108658\"},{\"start\":\"108668\",\"end\":\"108669\"},{\"start\":\"108969\",\"end\":\"108974\"},{\"start\":\"108983\",\"end\":\"108984\"},{\"start\":\"108994\",\"end\":\"108995\"},{\"start\":\"109010\",\"end\":\"109011\"},{\"start\":\"109020\",\"end\":\"109021\"},{\"start\":\"109030\",\"end\":\"109031\"},{\"start\":\"109446\",\"end\":\"109451\"},{\"start\":\"109460\",\"end\":\"109461\"},{\"start\":\"109476\",\"end\":\"109477\"},{\"start\":\"109487\",\"end\":\"109488\"},{\"start\":\"109497\",\"end\":\"109498\"},{\"start\":\"109847\",\"end\":\"109848\"},{\"start\":\"109865\",\"end\":\"109866\"},{\"start\":\"109882\",\"end\":\"109883\"},{\"start\":\"109884\",\"end\":\"109885\"},{\"start\":\"109893\",\"end\":\"109894\"},{\"start\":\"109895\",\"end\":\"109896\"},{\"start\":\"110211\",\"end\":\"110212\"},{\"start\":\"110229\",\"end\":\"110230\"},{\"start\":\"110246\",\"end\":\"110247\"},{\"start\":\"110248\",\"end\":\"110249\"},{\"start\":\"110257\",\"end\":\"110258\"},{\"start\":\"110259\",\"end\":\"110260\"},{\"start\":\"110712\",\"end\":\"110713\"},{\"start\":\"110714\",\"end\":\"110715\"},{\"start\":\"111312\",\"end\":\"111313\"},{\"start\":\"111321\",\"end\":\"111322\"},{\"start\":\"111327\",\"end\":\"111328\"},{\"start\":\"111333\",\"end\":\"111334\"},{\"start\":\"111335\",\"end\":\"111336\"},{\"start\":\"111343\",\"end\":\"111344\"},{\"start\":\"111345\",\"end\":\"111346\"},{\"start\":\"111708\",\"end\":\"111709\"},{\"start\":\"111717\",\"end\":\"111718\"},{\"start\":\"111725\",\"end\":\"111726\"},{\"start\":\"111733\",\"end\":\"111734\"},{\"start\":\"111744\",\"end\":\"111745\"},{\"start\":\"111752\",\"end\":\"111753\"},{\"start\":\"112122\",\"end\":\"112123\"},{\"start\":\"112133\",\"end\":\"112134\"},{\"start\":\"112141\",\"end\":\"112142\"},{\"start\":\"112153\",\"end\":\"112154\"},{\"start\":\"112421\",\"end\":\"112422\"},{\"start\":\"112441\",\"end\":\"112442\"},{\"start\":\"112450\",\"end\":\"112451\"},{\"start\":\"112780\",\"end\":\"112781\"},{\"start\":\"113145\",\"end\":\"113146\"},{\"start\":\"113154\",\"end\":\"113155\"},{\"start\":\"113163\",\"end\":\"113164\"},{\"start\":\"113544\",\"end\":\"113545\"},{\"start\":\"113557\",\"end\":\"113558\"},{\"start\":\"113565\",\"end\":\"113566\"},{\"start\":\"113848\",\"end\":\"113849\"},{\"start\":\"113854\",\"end\":\"113855\"},{\"start\":\"113863\",\"end\":\"113864\"},{\"start\":\"114136\",\"end\":\"114137\"},{\"start\":\"114144\",\"end\":\"114145\"},{\"start\":\"114154\",\"end\":\"114155\"},{\"start\":\"114168\",\"end\":\"114169\"},{\"start\":\"114468\",\"end\":\"114469\"},{\"start\":\"114476\",\"end\":\"114477\"},{\"start\":\"114483\",\"end\":\"114484\"},{\"start\":\"114492\",\"end\":\"114493\"},{\"start\":\"114503\",\"end\":\"114504\"},{\"start\":\"114517\",\"end\":\"114518\"},{\"start\":\"114971\",\"end\":\"114972\"},{\"start\":\"114979\",\"end\":\"114980\"},{\"start\":\"114990\",\"end\":\"114991\"},{\"start\":\"115370\",\"end\":\"115371\"},{\"start\":\"115379\",\"end\":\"115380\"},{\"start\":\"115388\",\"end\":\"115389\"},{\"start\":\"115401\",\"end\":\"115402\"},{\"start\":\"115414\",\"end\":\"115415\"},{\"start\":\"115416\",\"end\":\"115417\"},{\"start\":\"115427\",\"end\":\"115428\"},{\"start\":\"115436\",\"end\":\"115437\"},{\"start\":\"115438\",\"end\":\"115439\"},{\"start\":\"115446\",\"end\":\"115447\"},{\"start\":\"115459\",\"end\":\"115463\"},{\"start\":\"115472\",\"end\":\"115473\"},{\"start\":\"115858\",\"end\":\"115859\"},{\"start\":\"115865\",\"end\":\"115866\"},{\"start\":\"115873\",\"end\":\"115874\"},{\"start\":\"116150\",\"end\":\"116151\"},{\"start\":\"116405\",\"end\":\"116406\"},{\"start\":\"116419\",\"end\":\"116420\"},{\"start\":\"116421\",\"end\":\"116422\"},{\"start\":\"116428\",\"end\":\"116429\"},{\"start\":\"116430\",\"end\":\"116431\"},{\"start\":\"116444\",\"end\":\"116448\"},{\"start\":\"116702\",\"end\":\"116703\"},{\"start\":\"116712\",\"end\":\"116713\"},{\"start\":\"116729\",\"end\":\"116730\"},{\"start\":\"116731\",\"end\":\"116732\"},{\"start\":\"117002\",\"end\":\"117003\"},{\"start\":\"117012\",\"end\":\"117013\"},{\"start\":\"117014\",\"end\":\"117015\"},{\"start\":\"117025\",\"end\":\"117026\"},{\"start\":\"117027\",\"end\":\"117028\"},{\"start\":\"117307\",\"end\":\"117308\"},{\"start\":\"117313\",\"end\":\"117314\"},{\"start\":\"117319\",\"end\":\"117320\"},{\"start\":\"117326\",\"end\":\"117327\"},{\"start\":\"117630\",\"end\":\"117631\"},{\"start\":\"117639\",\"end\":\"117640\"},{\"start\":\"117653\",\"end\":\"117654\"},{\"start\":\"117655\",\"end\":\"117656\"},{\"start\":\"117664\",\"end\":\"117665\"},{\"start\":\"117666\",\"end\":\"117667\"},{\"start\":\"117996\",\"end\":\"117997\"},{\"start\":\"118002\",\"end\":\"118006\"},{\"start\":\"118011\",\"end\":\"118015\"},{\"start\":\"118024\",\"end\":\"118025\"},{\"start\":\"118030\",\"end\":\"118031\"},{\"start\":\"118038\",\"end\":\"118039\"},{\"start\":\"118046\",\"end\":\"118050\"},{\"start\":\"118416\",\"end\":\"118417\"},{\"start\":\"118424\",\"end\":\"118425\"},{\"start\":\"118430\",\"end\":\"118431\"},{\"start\":\"118769\",\"end\":\"118770\"},{\"start\":\"118776\",\"end\":\"118777\"},{\"start\":\"119074\",\"end\":\"119075\"},{\"start\":\"119081\",\"end\":\"119082\"},{\"start\":\"119093\",\"end\":\"119094\"},{\"start\":\"119105\",\"end\":\"119106\"},{\"start\":\"119114\",\"end\":\"119115\"},{\"start\":\"119125\",\"end\":\"119126\"},{\"start\":\"119145\",\"end\":\"119146\"},{\"start\":\"119154\",\"end\":\"119155\"},{\"start\":\"119163\",\"end\":\"119164\"},{\"start\":\"119171\",\"end\":\"119172\"},{\"start\":\"119181\",\"end\":\"119182\"},{\"start\":\"119193\",\"end\":\"119194\"},{\"start\":\"119579\",\"end\":\"119580\"},{\"start\":\"119857\",\"end\":\"119858\"},{\"start\":\"119866\",\"end\":\"119867\"},{\"start\":\"120155\",\"end\":\"120156\"},{\"start\":\"120173\",\"end\":\"120174\"},{\"start\":\"120175\",\"end\":\"120176\"},{\"start\":\"120595\",\"end\":\"120596\"},{\"start\":\"120603\",\"end\":\"120604\"},{\"start\":\"120610\",\"end\":\"120611\"},{\"start\":\"120622\",\"end\":\"120623\"},{\"start\":\"121186\",\"end\":\"121187\"},{\"start\":\"121197\",\"end\":\"121198\"},{\"start\":\"121207\",\"end\":\"121208\"},{\"start\":\"121529\",\"end\":\"121530\"},{\"start\":\"121541\",\"end\":\"121542\"},{\"start\":\"121825\",\"end\":\"121826\"},{\"start\":\"121836\",\"end\":\"121837\"}]", "bib_author_last_name": "[{\"start\":\"99511\",\"end\":\"99517\"},{\"start\":\"99844\",\"end\":\"99852\"},{\"start\":\"99856\",\"end\":\"99862\"},{\"start\":\"99866\",\"end\":\"99871\"},{\"start\":\"99875\",\"end\":\"99881\"},{\"start\":\"100123\",\"end\":\"100128\"},{\"start\":\"100130\",\"end\":\"100135\"},{\"start\":\"100141\",\"end\":\"100144\"},{\"start\":\"100146\",\"end\":\"100150\"},{\"start\":\"100156\",\"end\":\"100158\"},{\"start\":\"100166\",\"end\":\"100171\"},{\"start\":\"100348\",\"end\":\"100355\"},{\"start\":\"100597\",\"end\":\"100606\"},{\"start\":\"100801\",\"end\":\"100804\"},{\"start\":\"100808\",\"end\":\"100817\"},{\"start\":\"100823\",\"end\":\"100830\"},{\"start\":\"100834\",\"end\":\"100839\"},{\"start\":\"101155\",\"end\":\"101167\"},{\"start\":\"101171\",\"end\":\"101176\"},{\"start\":\"101183\",\"end\":\"101189\"},{\"start\":\"101193\",\"end\":\"101197\"},{\"start\":\"101633\",\"end\":\"101637\"},{\"start\":\"101641\",\"end\":\"101643\"},{\"start\":\"101928\",\"end\":\"101937\"},{\"start\":\"101941\",\"end\":\"101946\"},{\"start\":\"101950\",\"end\":\"101962\"},{\"start\":\"101966\",\"end\":\"101972\"},{\"start\":\"102285\",\"end\":\"102289\"},{\"start\":\"102295\",\"end\":\"102300\"},{\"start\":\"102304\",\"end\":\"102312\"},{\"start\":\"102640\",\"end\":\"102652\"},{\"start\":\"102656\",\"end\":\"102662\"},{\"start\":\"102668\",\"end\":\"102673\"},{\"start\":\"102677\",\"end\":\"102685\"},{\"start\":\"103216\",\"end\":\"103222\"},{\"start\":\"103226\",\"end\":\"103232\"},{\"start\":\"103236\",\"end\":\"103243\"},{\"start\":\"103247\",\"end\":\"103254\"},{\"start\":\"103658\",\"end\":\"103661\"},{\"start\":\"103665\",\"end\":\"103667\"},{\"start\":\"103671\",\"end\":\"103676\"},{\"start\":\"103680\",\"end\":\"103685\"},{\"start\":\"104015\",\"end\":\"104022\"},{\"start\":\"104026\",\"end\":\"104037\"},{\"start\":\"104041\",\"end\":\"104049\"},{\"start\":\"104340\",\"end\":\"104346\"},{\"start\":\"104352\",\"end\":\"104356\"},{\"start\":\"104362\",\"end\":\"104367\"},{\"start\":\"104611\",\"end\":\"104615\"},{\"start\":\"104621\",\"end\":\"104627\"},{\"start\":\"104631\",\"end\":\"104638\"},{\"start\":\"104642\",\"end\":\"104648\"},{\"start\":\"104939\",\"end\":\"104947\"},{\"start\":\"105270\",\"end\":\"105278\"},{\"start\":\"105473\",\"end\":\"105477\"},{\"start\":\"105481\",\"end\":\"105485\"},{\"start\":\"105489\",\"end\":\"105498\"},{\"start\":\"105502\",\"end\":\"105505\"},{\"start\":\"105509\",\"end\":\"105512\"},{\"start\":\"105516\",\"end\":\"105524\"},{\"start\":\"105528\",\"end\":\"105530\"},{\"start\":\"105534\",\"end\":\"105539\"},{\"start\":\"105947\",\"end\":\"105955\"},{\"start\":\"105959\",\"end\":\"105967\"},{\"start\":\"105971\",\"end\":\"105984\"},{\"start\":\"105988\",\"end\":\"105993\"},{\"start\":\"105997\",\"end\":\"106005\"},{\"start\":\"106009\",\"end\":\"106017\"},{\"start\":\"106388\",\"end\":\"106398\"},{\"start\":\"106402\",\"end\":\"106405\"},{\"start\":\"106411\",\"end\":\"106416\"},{\"start\":\"106422\",\"end\":\"106429\"},{\"start\":\"106433\",\"end\":\"106438\"},{\"start\":\"106445\",\"end\":\"106449\"},{\"start\":\"106806\",\"end\":\"106811\"},{\"start\":\"106817\",\"end\":\"106822\"},{\"start\":\"107081\",\"end\":\"107084\"},{\"start\":\"107088\",\"end\":\"107090\"},{\"start\":\"107094\",\"end\":\"107097\"},{\"start\":\"107101\",\"end\":\"107106\"},{\"start\":\"107110\",\"end\":\"107114\"},{\"start\":\"107394\",\"end\":\"107399\"},{\"start\":\"107403\",\"end\":\"107408\"},{\"start\":\"107414\",\"end\":\"107419\"},{\"start\":\"107423\",\"end\":\"107432\"},{\"start\":\"107436\",\"end\":\"107440\"},{\"start\":\"107833\",\"end\":\"107837\"},{\"start\":\"107841\",\"end\":\"107850\"},{\"start\":\"107854\",\"end\":\"107859\"},{\"start\":\"108265\",\"end\":\"108272\"},{\"start\":\"108276\",\"end\":\"108281\"},{\"start\":\"108285\",\"end\":\"108294\"},{\"start\":\"108635\",\"end\":\"108644\"},{\"start\":\"108648\",\"end\":\"108655\"},{\"start\":\"108659\",\"end\":\"108666\"},{\"start\":\"108670\",\"end\":\"108677\"},{\"start\":\"108975\",\"end\":\"108981\"},{\"start\":\"108985\",\"end\":\"108992\"},{\"start\":\"108996\",\"end\":\"109008\"},{\"start\":\"109012\",\"end\":\"109018\"},{\"start\":\"109022\",\"end\":\"109028\"},{\"start\":\"109032\",\"end\":\"109040\"},{\"start\":\"109452\",\"end\":\"109458\"},{\"start\":\"109462\",\"end\":\"109474\"},{\"start\":\"109478\",\"end\":\"109485\"},{\"start\":\"109489\",\"end\":\"109495\"},{\"start\":\"109499\",\"end\":\"109507\"},{\"start\":\"109849\",\"end\":\"109863\"},{\"start\":\"109867\",\"end\":\"109880\"},{\"start\":\"109886\",\"end\":\"109891\"},{\"start\":\"109897\",\"end\":\"109904\"},{\"start\":\"110213\",\"end\":\"110227\"},{\"start\":\"110231\",\"end\":\"110244\"},{\"start\":\"110250\",\"end\":\"110255\"},{\"start\":\"110261\",\"end\":\"110268\"},{\"start\":\"110716\",\"end\":\"110724\"},{\"start\":\"111314\",\"end\":\"111319\"},{\"start\":\"111323\",\"end\":\"111325\"},{\"start\":\"111329\",\"end\":\"111331\"},{\"start\":\"111337\",\"end\":\"111341\"},{\"start\":\"111347\",\"end\":\"111351\"},{\"start\":\"111710\",\"end\":\"111715\"},{\"start\":\"111719\",\"end\":\"111723\"},{\"start\":\"111727\",\"end\":\"111731\"},{\"start\":\"111735\",\"end\":\"111742\"},{\"start\":\"111746\",\"end\":\"111750\"},{\"start\":\"111754\",\"end\":\"111756\"},{\"start\":\"112124\",\"end\":\"112131\"},{\"start\":\"112135\",\"end\":\"112139\"},{\"start\":\"112143\",\"end\":\"112151\"},{\"start\":\"112155\",\"end\":\"112161\"},{\"start\":\"112423\",\"end\":\"112439\"},{\"start\":\"112443\",\"end\":\"112448\"},{\"start\":\"112452\",\"end\":\"112460\"},{\"start\":\"112782\",\"end\":\"112789\"},{\"start\":\"113147\",\"end\":\"113152\"},{\"start\":\"113156\",\"end\":\"113161\"},{\"start\":\"113165\",\"end\":\"113173\"},{\"start\":\"113546\",\"end\":\"113555\"},{\"start\":\"113559\",\"end\":\"113563\"},{\"start\":\"113567\",\"end\":\"113572\"},{\"start\":\"113850\",\"end\":\"113852\"},{\"start\":\"113856\",\"end\":\"113861\"},{\"start\":\"113865\",\"end\":\"113870\"},{\"start\":\"114138\",\"end\":\"114142\"},{\"start\":\"114146\",\"end\":\"114152\"},{\"start\":\"114156\",\"end\":\"114166\"},{\"start\":\"114170\",\"end\":\"114175\"},{\"start\":\"114470\",\"end\":\"114474\"},{\"start\":\"114478\",\"end\":\"114481\"},{\"start\":\"114485\",\"end\":\"114490\"},{\"start\":\"114494\",\"end\":\"114501\"},{\"start\":\"114505\",\"end\":\"114515\"},{\"start\":\"114519\",\"end\":\"114524\"},{\"start\":\"114973\",\"end\":\"114977\"},{\"start\":\"114981\",\"end\":\"114988\"},{\"start\":\"114992\",\"end\":\"114997\"},{\"start\":\"115372\",\"end\":\"115377\"},{\"start\":\"115381\",\"end\":\"115386\"},{\"start\":\"115390\",\"end\":\"115399\"},{\"start\":\"115403\",\"end\":\"115412\"},{\"start\":\"115418\",\"end\":\"115425\"},{\"start\":\"115429\",\"end\":\"115434\"},{\"start\":\"115440\",\"end\":\"115444\"},{\"start\":\"115448\",\"end\":\"115457\"},{\"start\":\"115464\",\"end\":\"115470\"},{\"start\":\"115474\",\"end\":\"115483\"},{\"start\":\"115860\",\"end\":\"115863\"},{\"start\":\"115867\",\"end\":\"115871\"},{\"start\":\"115875\",\"end\":\"115880\"},{\"start\":\"116152\",\"end\":\"116157\"},{\"start\":\"116407\",\"end\":\"116417\"},{\"start\":\"116423\",\"end\":\"116426\"},{\"start\":\"116432\",\"end\":\"116442\"},{\"start\":\"116449\",\"end\":\"116452\"},{\"start\":\"116704\",\"end\":\"116710\"},{\"start\":\"116714\",\"end\":\"116727\"},{\"start\":\"116733\",\"end\":\"116738\"},{\"start\":\"117004\",\"end\":\"117010\"},{\"start\":\"117016\",\"end\":\"117023\"},{\"start\":\"117029\",\"end\":\"117034\"},{\"start\":\"117309\",\"end\":\"117311\"},{\"start\":\"117315\",\"end\":\"117317\"},{\"start\":\"117321\",\"end\":\"117324\"},{\"start\":\"117328\",\"end\":\"117336\"},{\"start\":\"117632\",\"end\":\"117637\"},{\"start\":\"117641\",\"end\":\"117651\"},{\"start\":\"117657\",\"end\":\"117662\"},{\"start\":\"117668\",\"end\":\"117673\"},{\"start\":\"117998\",\"end\":\"118000\"},{\"start\":\"118007\",\"end\":\"118009\"},{\"start\":\"118016\",\"end\":\"118022\"},{\"start\":\"118026\",\"end\":\"118028\"},{\"start\":\"118032\",\"end\":\"118036\"},{\"start\":\"118040\",\"end\":\"118044\"},{\"start\":\"118051\",\"end\":\"118057\"},{\"start\":\"118418\",\"end\":\"118422\"},{\"start\":\"118426\",\"end\":\"118428\"},{\"start\":\"118432\",\"end\":\"118437\"},{\"start\":\"118771\",\"end\":\"118774\"},{\"start\":\"118778\",\"end\":\"118786\"},{\"start\":\"119076\",\"end\":\"119079\"},{\"start\":\"119083\",\"end\":\"119091\"},{\"start\":\"119095\",\"end\":\"119103\"},{\"start\":\"119107\",\"end\":\"119112\"},{\"start\":\"119116\",\"end\":\"119123\"},{\"start\":\"119127\",\"end\":\"119143\"},{\"start\":\"119147\",\"end\":\"119152\"},{\"start\":\"119156\",\"end\":\"119161\"},{\"start\":\"119165\",\"end\":\"119169\"},{\"start\":\"119173\",\"end\":\"119179\"},{\"start\":\"119183\",\"end\":\"119191\"},{\"start\":\"119195\",\"end\":\"119204\"},{\"start\":\"119581\",\"end\":\"119586\"},{\"start\":\"119859\",\"end\":\"119864\"},{\"start\":\"119868\",\"end\":\"119876\"},{\"start\":\"120157\",\"end\":\"120171\"},{\"start\":\"120177\",\"end\":\"120182\"},{\"start\":\"120597\",\"end\":\"120601\"},{\"start\":\"120605\",\"end\":\"120608\"},{\"start\":\"120612\",\"end\":\"120620\"},{\"start\":\"120624\",\"end\":\"120629\"},{\"start\":\"121188\",\"end\":\"121195\"},{\"start\":\"121199\",\"end\":\"121205\"},{\"start\":\"121209\",\"end\":\"121214\"},{\"start\":\"121531\",\"end\":\"121539\"},{\"start\":\"121543\",\"end\":\"121549\"},{\"start\":\"121827\",\"end\":\"121834\"},{\"start\":\"121838\",\"end\":\"121847\"}]", "bib_entry": "[{\"start\":\"99467\",\"end\":\"99691\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"99693\",\"end\":\"99840\",\"attributes\":{\"id\":\"b1\"}},{\"start\":\"99842\",\"end\":\"100067\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"100069\",\"end\":\"100305\",\"attributes\":{\"id\":\"b3\"}},{\"start\":\"100307\",\"end\":\"100540\",\"attributes\":{\"id\":\"b4\"}},{\"start\":\"100542\",\"end\":\"100795\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"100797\",\"end\":\"101022\",\"attributes\":{\"id\":\"b6\"}},{\"start\":\"101024\",\"end\":\"101547\",\"attributes\":{\"matched_paper_id\":\"2902438\",\"id\":\"b7\"}},{\"start\":\"101549\",\"end\":\"101849\",\"attributes\":{\"matched_paper_id\":\"2181695\",\"id\":\"b8\"}},{\"start\":\"101851\",\"end\":\"102236\",\"attributes\":{\"matched_paper_id\":\"1188252\",\"id\":\"b9\"}},{\"start\":\"102238\",\"end\":\"102502\",\"attributes\":{\"matched_paper_id\":\"14314450\",\"id\":\"b10\"}},{\"start\":\"102504\",\"end\":\"102914\",\"attributes\":{\"matched_paper_id\":\"6664478\",\"id\":\"b11\"}},{\"start\":\"102916\",\"end\":\"103120\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"103122\",\"end\":\"103560\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"103562\",\"end\":\"103951\",\"attributes\":{\"matched_paper_id\":\"8892096\",\"id\":\"b14\"}},{\"start\":\"103953\",\"end\":\"104293\",\"attributes\":{\"matched_paper_id\":\"11290054\",\"id\":\"b15\"}},{\"start\":\"104295\",\"end\":\"104553\",\"attributes\":{\"matched_paper_id\":\"11198468\",\"id\":\"b16\"}},{\"start\":\"104555\",\"end\":\"104854\",\"attributes\":{\"matched_paper_id\":\"16991648\",\"id\":\"b17\"}},{\"start\":\"104856\",\"end\":\"105195\",\"attributes\":{\"matched_paper_id\":\"52290553\",\"id\":\"b18\"}},{\"start\":\"105197\",\"end\":\"105469\",\"attributes\":{\"matched_paper_id\":\"195760206\",\"id\":\"b19\"}},{\"start\":\"105471\",\"end\":\"105859\",\"attributes\":{\"id\":\"b20\"}},{\"start\":\"105861\",\"end\":\"106290\",\"attributes\":{\"matched_paper_id\":\"14124129\",\"id\":\"b21\"}},{\"start\":\"106292\",\"end\":\"106755\",\"attributes\":{\"matched_paper_id\":\"2727938\",\"id\":\"b22\"}},{\"start\":\"106757\",\"end\":\"106998\",\"attributes\":{\"matched_paper_id\":\"52285071\",\"id\":\"b23\"}},{\"start\":\"107000\",\"end\":\"107318\",\"attributes\":{\"matched_paper_id\":\"49301984\",\"id\":\"b24\"}},{\"start\":\"107320\",\"end\":\"107753\",\"attributes\":{\"matched_paper_id\":\"4766599\",\"id\":\"b25\"}},{\"start\":\"107755\",\"end\":\"108178\",\"attributes\":{\"matched_paper_id\":\"91184647\",\"id\":\"b26\"}},{\"start\":\"108180\",\"end\":\"108569\",\"attributes\":{\"matched_paper_id\":\"1023605\",\"id\":\"b27\"}},{\"start\":\"108571\",\"end\":\"108882\",\"attributes\":{\"matched_paper_id\":\"10908021\",\"id\":\"b28\"}},{\"start\":\"108884\",\"end\":\"109366\",\"attributes\":{\"matched_paper_id\":\"6369031\",\"id\":\"b29\"}},{\"start\":\"109368\",\"end\":\"109782\",\"attributes\":{\"id\":\"b30\"}},{\"start\":\"109784\",\"end\":\"110141\",\"attributes\":{\"matched_paper_id\":\"206724285\",\"id\":\"b31\"}},{\"start\":\"110143\",\"end\":\"110618\",\"attributes\":{\"matched_paper_id\":\"16565968\",\"id\":\"b32\"}},{\"start\":\"110620\",\"end\":\"110938\",\"attributes\":{\"matched_paper_id\":\"9341989\",\"id\":\"b33\"}},{\"start\":\"110940\",\"end\":\"111271\",\"attributes\":{\"id\":\"b34\"}},{\"start\":\"111273\",\"end\":\"111617\",\"attributes\":{\"id\":\"b35\"}},{\"start\":\"111619\",\"end\":\"112072\",\"attributes\":{\"matched_paper_id\":\"15139961\",\"id\":\"b36\"}},{\"start\":\"112074\",\"end\":\"112356\",\"attributes\":{\"id\":\"b37\",\"doi\":\"arXiv:1705.10694\"}},{\"start\":\"112358\",\"end\":\"112715\",\"attributes\":{\"matched_paper_id\":\"71151826\",\"id\":\"b38\"}},{\"start\":\"112717\",\"end\":\"113060\",\"attributes\":{\"matched_paper_id\":\"2375110\",\"id\":\"b39\"}},{\"start\":\"113062\",\"end\":\"113451\",\"attributes\":{\"matched_paper_id\":\"43963496\",\"id\":\"b40\"}},{\"start\":\"113453\",\"end\":\"113804\",\"attributes\":{\"id\":\"b41\",\"doi\":\"arXiv:2009.05369\"}},{\"start\":\"113806\",\"end\":\"114083\",\"attributes\":{\"matched_paper_id\":\"199064618\",\"id\":\"b42\"}},{\"start\":\"114085\",\"end\":\"114379\",\"attributes\":{\"id\":\"b43\",\"doi\":\"arXiv:2011.13544\"}},{\"start\":\"114381\",\"end\":\"114885\",\"attributes\":{\"matched_paper_id\":\"209444425\",\"id\":\"b44\"}},{\"start\":\"114887\",\"end\":\"115304\",\"attributes\":{\"matched_paper_id\":\"400882\",\"id\":\"b45\"}},{\"start\":\"115306\",\"end\":\"115796\",\"attributes\":{\"id\":\"b46\"}},{\"start\":\"115798\",\"end\":\"116073\",\"attributes\":{\"id\":\"b47\",\"doi\":\"arXiv:2001.08113\"}},{\"start\":\"116075\",\"end\":\"116348\",\"attributes\":{\"matched_paper_id\":\"215791804\",\"id\":\"b48\"}},{\"start\":\"116350\",\"end\":\"116648\",\"attributes\":{\"matched_paper_id\":\"5080495\",\"id\":\"b49\"}},{\"start\":\"116650\",\"end\":\"116939\",\"attributes\":{\"matched_paper_id\":\"16892725\",\"id\":\"b50\"}},{\"start\":\"116941\",\"end\":\"117245\",\"attributes\":{\"matched_paper_id\":\"2927709\",\"id\":\"b51\"}},{\"start\":\"117247\",\"end\":\"117567\",\"attributes\":{\"matched_paper_id\":\"1531407\",\"id\":\"b52\"}},{\"start\":\"117569\",\"end\":\"117894\",\"attributes\":{\"matched_paper_id\":\"8344371\",\"id\":\"b53\"}},{\"start\":\"117896\",\"end\":\"118366\",\"attributes\":{\"matched_paper_id\":\"188770\",\"id\":\"b54\"}},{\"start\":\"118368\",\"end\":\"118703\",\"attributes\":{\"matched_paper_id\":\"49538884\",\"id\":\"b55\"}},{\"start\":\"118705\",\"end\":\"119031\",\"attributes\":{\"matched_paper_id\":\"202784253\",\"id\":\"b56\"}},{\"start\":\"119033\",\"end\":\"119491\",\"attributes\":{\"id\":\"b57\",\"doi\":\"arXiv:1705.06950\"}},{\"start\":\"119493\",\"end\":\"119779\",\"attributes\":{\"matched_paper_id\":\"145942923\",\"id\":\"b58\"}},{\"start\":\"119781\",\"end\":\"120085\",\"attributes\":{\"matched_paper_id\":\"195069600\",\"id\":\"b59\"}},{\"start\":\"120087\",\"end\":\"120498\",\"attributes\":{\"matched_paper_id\":\"2754510\",\"id\":\"b60\"}},{\"start\":\"120500\",\"end\":\"120860\",\"attributes\":{\"matched_paper_id\":\"204509303\",\"id\":\"b61\"}},{\"start\":\"120862\",\"end\":\"121155\",\"attributes\":{\"id\":\"b62\"}},{\"start\":\"121157\",\"end\":\"121448\",\"attributes\":{\"matched_paper_id\":\"11598691\",\"id\":\"b63\"}},{\"start\":\"121450\",\"end\":\"121757\",\"attributes\":{\"matched_paper_id\":\"22343847\",\"id\":\"b64\"}},{\"start\":\"121759\",\"end\":\"122031\",\"attributes\":{\"matched_paper_id\":\"20071973\",\"id\":\"b65\"}}]", "bib_title": "[{\"start\":\"101024\",\"end\":\"101151\"},{\"start\":\"101549\",\"end\":\"101629\"},{\"start\":\"101851\",\"end\":\"101924\"},{\"start\":\"102238\",\"end\":\"102279\"},{\"start\":\"102504\",\"end\":\"102636\"},{\"start\":\"103122\",\"end\":\"103212\"},{\"start\":\"103562\",\"end\":\"103654\"},{\"start\":\"103953\",\"end\":\"104011\"},{\"start\":\"104295\",\"end\":\"104336\"},{\"start\":\"104555\",\"end\":\"104605\"},{\"start\":\"104856\",\"end\":\"104935\"},{\"start\":\"105197\",\"end\":\"105266\"},{\"start\":\"105861\",\"end\":\"105943\"},{\"start\":\"106292\",\"end\":\"106384\"},{\"start\":\"106757\",\"end\":\"106802\"},{\"start\":\"107000\",\"end\":\"107077\"},{\"start\":\"107320\",\"end\":\"107390\"},{\"start\":\"107755\",\"end\":\"107829\"},{\"start\":\"108180\",\"end\":\"108261\"},{\"start\":\"108571\",\"end\":\"108631\"},{\"start\":\"108884\",\"end\":\"108967\"},{\"start\":\"109368\",\"end\":\"109444\"},{\"start\":\"109784\",\"end\":\"109845\"},{\"start\":\"110143\",\"end\":\"110209\"},{\"start\":\"110620\",\"end\":\"110710\"},{\"start\":\"111619\",\"end\":\"111706\"},{\"start\":\"112358\",\"end\":\"112419\"},{\"start\":\"112717\",\"end\":\"112778\"},{\"start\":\"113062\",\"end\":\"113143\"},{\"start\":\"113806\",\"end\":\"113846\"},{\"start\":\"114381\",\"end\":\"114466\"},{\"start\":\"114887\",\"end\":\"114969\"},{\"start\":\"115306\",\"end\":\"115368\"},{\"start\":\"116075\",\"end\":\"116148\"},{\"start\":\"116350\",\"end\":\"116403\"},{\"start\":\"116650\",\"end\":\"116700\"},{\"start\":\"116941\",\"end\":\"117000\"},{\"start\":\"117247\",\"end\":\"117305\"},{\"start\":\"117569\",\"end\":\"117628\"},{\"start\":\"117896\",\"end\":\"117994\"},{\"start\":\"118368\",\"end\":\"118414\"},{\"start\":\"118705\",\"end\":\"118767\"},{\"start\":\"119493\",\"end\":\"119577\"},{\"start\":\"119781\",\"end\":\"119855\"},{\"start\":\"120087\",\"end\":\"120153\"},{\"start\":\"120500\",\"end\":\"120593\"},{\"start\":\"121157\",\"end\":\"121184\"},{\"start\":\"121450\",\"end\":\"121527\"},{\"start\":\"121759\",\"end\":\"121823\"}]", "bib_author": "[{\"start\":\"99511\",\"end\":\"99519\"},{\"start\":\"99842\",\"end\":\"99854\"},{\"start\":\"99854\",\"end\":\"99864\"},{\"start\":\"99864\",\"end\":\"99873\"},{\"start\":\"99873\",\"end\":\"99883\"},{\"start\":\"100121\",\"end\":\"100130\"},{\"start\":\"100130\",\"end\":\"100137\"},{\"start\":\"100137\",\"end\":\"100146\"},{\"start\":\"100146\",\"end\":\"100152\"},{\"start\":\"100152\",\"end\":\"100160\"},{\"start\":\"100160\",\"end\":\"100173\"},{\"start\":\"100346\",\"end\":\"100357\"},{\"start\":\"100593\",\"end\":\"100608\"},{\"start\":\"100797\",\"end\":\"100806\"},{\"start\":\"100806\",\"end\":\"100819\"},{\"start\":\"100819\",\"end\":\"100832\"},{\"start\":\"100832\",\"end\":\"100841\"},{\"start\":\"101153\",\"end\":\"101169\"},{\"start\":\"101169\",\"end\":\"101178\"},{\"start\":\"101178\",\"end\":\"101191\"},{\"start\":\"101191\",\"end\":\"101199\"},{\"start\":\"101631\",\"end\":\"101639\"},{\"start\":\"101639\",\"end\":\"101645\"},{\"start\":\"101926\",\"end\":\"101939\"},{\"start\":\"101939\",\"end\":\"101948\"},{\"start\":\"101948\",\"end\":\"101964\"},{\"start\":\"101964\",\"end\":\"101974\"},{\"start\":\"102281\",\"end\":\"102291\"},{\"start\":\"102291\",\"end\":\"102302\"},{\"start\":\"102302\",\"end\":\"102314\"},{\"start\":\"102638\",\"end\":\"102654\"},{\"start\":\"102654\",\"end\":\"102664\"},{\"start\":\"102664\",\"end\":\"102675\"},{\"start\":\"102675\",\"end\":\"102687\"},{\"start\":\"103214\",\"end\":\"103224\"},{\"start\":\"103224\",\"end\":\"103234\"},{\"start\":\"103234\",\"end\":\"103245\"},{\"start\":\"103245\",\"end\":\"103256\"},{\"start\":\"103656\",\"end\":\"103663\"},{\"start\":\"103663\",\"end\":\"103669\"},{\"start\":\"103669\",\"end\":\"103678\"},{\"start\":\"103678\",\"end\":\"103687\"},{\"start\":\"104013\",\"end\":\"104024\"},{\"start\":\"104024\",\"end\":\"104039\"},{\"start\":\"104039\",\"end\":\"104051\"},{\"start\":\"104338\",\"end\":\"104348\"},{\"start\":\"104348\",\"end\":\"104358\"},{\"start\":\"104358\",\"end\":\"104369\"},{\"start\":\"104607\",\"end\":\"104617\"},{\"start\":\"104617\",\"end\":\"104629\"},{\"start\":\"104629\",\"end\":\"104640\"},{\"start\":\"104640\",\"end\":\"104650\"},{\"start\":\"104937\",\"end\":\"104949\"},{\"start\":\"105268\",\"end\":\"105280\"},{\"start\":\"105471\",\"end\":\"105479\"},{\"start\":\"105479\",\"end\":\"105487\"},{\"start\":\"105487\",\"end\":\"105500\"},{\"start\":\"105500\",\"end\":\"105507\"},{\"start\":\"105507\",\"end\":\"105514\"},{\"start\":\"105514\",\"end\":\"105526\"},{\"start\":\"105526\",\"end\":\"105532\"},{\"start\":\"105532\",\"end\":\"105541\"},{\"start\":\"105945\",\"end\":\"105957\"},{\"start\":\"105957\",\"end\":\"105969\"},{\"start\":\"105969\",\"end\":\"105986\"},{\"start\":\"105986\",\"end\":\"105995\"},{\"start\":\"105995\",\"end\":\"106007\"},{\"start\":\"106007\",\"end\":\"106019\"},{\"start\":\"106386\",\"end\":\"106400\"},{\"start\":\"106400\",\"end\":\"106407\"},{\"start\":\"106407\",\"end\":\"106418\"},{\"start\":\"106418\",\"end\":\"106431\"},{\"start\":\"106431\",\"end\":\"106440\"},{\"start\":\"106440\",\"end\":\"106451\"},{\"start\":\"106804\",\"end\":\"106813\"},{\"start\":\"106813\",\"end\":\"106824\"},{\"start\":\"107079\",\"end\":\"107086\"},{\"start\":\"107086\",\"end\":\"107092\"},{\"start\":\"107092\",\"end\":\"107099\"},{\"start\":\"107099\",\"end\":\"107108\"},{\"start\":\"107108\",\"end\":\"107116\"},{\"start\":\"107392\",\"end\":\"107401\"},{\"start\":\"107401\",\"end\":\"107410\"},{\"start\":\"107410\",\"end\":\"107421\"},{\"start\":\"107421\",\"end\":\"107434\"},{\"start\":\"107434\",\"end\":\"107442\"},{\"start\":\"107831\",\"end\":\"107839\"},{\"start\":\"107839\",\"end\":\"107852\"},{\"start\":\"107852\",\"end\":\"107861\"},{\"start\":\"108263\",\"end\":\"108274\"},{\"start\":\"108274\",\"end\":\"108283\"},{\"start\":\"108283\",\"end\":\"108296\"},{\"start\":\"108296\",\"end\":\"108302\"},{\"start\":\"108633\",\"end\":\"108646\"},{\"start\":\"108646\",\"end\":\"108657\"},{\"start\":\"108657\",\"end\":\"108668\"},{\"start\":\"108668\",\"end\":\"108679\"},{\"start\":\"108969\",\"end\":\"108983\"},{\"start\":\"108983\",\"end\":\"108994\"},{\"start\":\"108994\",\"end\":\"109010\"},{\"start\":\"109010\",\"end\":\"109020\"},{\"start\":\"109020\",\"end\":\"109030\"},{\"start\":\"109030\",\"end\":\"109042\"},{\"start\":\"109446\",\"end\":\"109460\"},{\"start\":\"109460\",\"end\":\"109476\"},{\"start\":\"109476\",\"end\":\"109487\"},{\"start\":\"109487\",\"end\":\"109497\"},{\"start\":\"109497\",\"end\":\"109509\"},{\"start\":\"109847\",\"end\":\"109865\"},{\"start\":\"109865\",\"end\":\"109882\"},{\"start\":\"109882\",\"end\":\"109893\"},{\"start\":\"109893\",\"end\":\"109906\"},{\"start\":\"110211\",\"end\":\"110229\"},{\"start\":\"110229\",\"end\":\"110246\"},{\"start\":\"110246\",\"end\":\"110257\"},{\"start\":\"110257\",\"end\":\"110270\"},{\"start\":\"110712\",\"end\":\"110726\"},{\"start\":\"111312\",\"end\":\"111321\"},{\"start\":\"111321\",\"end\":\"111327\"},{\"start\":\"111327\",\"end\":\"111333\"},{\"start\":\"111333\",\"end\":\"111343\"},{\"start\":\"111343\",\"end\":\"111353\"},{\"start\":\"111708\",\"end\":\"111717\"},{\"start\":\"111717\",\"end\":\"111725\"},{\"start\":\"111725\",\"end\":\"111733\"},{\"start\":\"111733\",\"end\":\"111744\"},{\"start\":\"111744\",\"end\":\"111752\"},{\"start\":\"111752\",\"end\":\"111758\"},{\"start\":\"112122\",\"end\":\"112133\"},{\"start\":\"112133\",\"end\":\"112141\"},{\"start\":\"112141\",\"end\":\"112153\"},{\"start\":\"112153\",\"end\":\"112163\"},{\"start\":\"112421\",\"end\":\"112441\"},{\"start\":\"112441\",\"end\":\"112450\"},{\"start\":\"112450\",\"end\":\"112462\"},{\"start\":\"112780\",\"end\":\"112791\"},{\"start\":\"113145\",\"end\":\"113154\"},{\"start\":\"113154\",\"end\":\"113163\"},{\"start\":\"113163\",\"end\":\"113175\"},{\"start\":\"113544\",\"end\":\"113557\"},{\"start\":\"113557\",\"end\":\"113565\"},{\"start\":\"113565\",\"end\":\"113574\"},{\"start\":\"113848\",\"end\":\"113854\"},{\"start\":\"113854\",\"end\":\"113863\"},{\"start\":\"113863\",\"end\":\"113872\"},{\"start\":\"114136\",\"end\":\"114144\"},{\"start\":\"114144\",\"end\":\"114154\"},{\"start\":\"114154\",\"end\":\"114168\"},{\"start\":\"114168\",\"end\":\"114177\"},{\"start\":\"114468\",\"end\":\"114476\"},{\"start\":\"114476\",\"end\":\"114483\"},{\"start\":\"114483\",\"end\":\"114492\"},{\"start\":\"114492\",\"end\":\"114503\"},{\"start\":\"114503\",\"end\":\"114517\"},{\"start\":\"114517\",\"end\":\"114526\"},{\"start\":\"114971\",\"end\":\"114979\"},{\"start\":\"114979\",\"end\":\"114990\"},{\"start\":\"114990\",\"end\":\"114999\"},{\"start\":\"115370\",\"end\":\"115379\"},{\"start\":\"115379\",\"end\":\"115388\"},{\"start\":\"115388\",\"end\":\"115401\"},{\"start\":\"115401\",\"end\":\"115414\"},{\"start\":\"115414\",\"end\":\"115427\"},{\"start\":\"115427\",\"end\":\"115436\"},{\"start\":\"115436\",\"end\":\"115446\"},{\"start\":\"115446\",\"end\":\"115459\"},{\"start\":\"115459\",\"end\":\"115472\"},{\"start\":\"115472\",\"end\":\"115485\"},{\"start\":\"115858\",\"end\":\"115865\"},{\"start\":\"115865\",\"end\":\"115873\"},{\"start\":\"115873\",\"end\":\"115882\"},{\"start\":\"116150\",\"end\":\"116159\"},{\"start\":\"116405\",\"end\":\"116419\"},{\"start\":\"116419\",\"end\":\"116428\"},{\"start\":\"116428\",\"end\":\"116444\"},{\"start\":\"116444\",\"end\":\"116454\"},{\"start\":\"116702\",\"end\":\"116712\"},{\"start\":\"116712\",\"end\":\"116729\"},{\"start\":\"116729\",\"end\":\"116740\"},{\"start\":\"117002\",\"end\":\"117012\"},{\"start\":\"117012\",\"end\":\"117025\"},{\"start\":\"117025\",\"end\":\"117036\"},{\"start\":\"117307\",\"end\":\"117313\"},{\"start\":\"117313\",\"end\":\"117319\"},{\"start\":\"117319\",\"end\":\"117326\"},{\"start\":\"117326\",\"end\":\"117338\"},{\"start\":\"117630\",\"end\":\"117639\"},{\"start\":\"117639\",\"end\":\"117653\"},{\"start\":\"117653\",\"end\":\"117664\"},{\"start\":\"117664\",\"end\":\"117675\"},{\"start\":\"117996\",\"end\":\"118002\"},{\"start\":\"118002\",\"end\":\"118011\"},{\"start\":\"118011\",\"end\":\"118024\"},{\"start\":\"118024\",\"end\":\"118030\"},{\"start\":\"118030\",\"end\":\"118038\"},{\"start\":\"118038\",\"end\":\"118046\"},{\"start\":\"118046\",\"end\":\"118059\"},{\"start\":\"118416\",\"end\":\"118424\"},{\"start\":\"118424\",\"end\":\"118430\"},{\"start\":\"118430\",\"end\":\"118439\"},{\"start\":\"118769\",\"end\":\"118776\"},{\"start\":\"118776\",\"end\":\"118788\"},{\"start\":\"119074\",\"end\":\"119081\"},{\"start\":\"119081\",\"end\":\"119093\"},{\"start\":\"119093\",\"end\":\"119105\"},{\"start\":\"119105\",\"end\":\"119114\"},{\"start\":\"119114\",\"end\":\"119125\"},{\"start\":\"119125\",\"end\":\"119145\"},{\"start\":\"119145\",\"end\":\"119154\"},{\"start\":\"119154\",\"end\":\"119163\"},{\"start\":\"119163\",\"end\":\"119171\"},{\"start\":\"119171\",\"end\":\"119181\"},{\"start\":\"119181\",\"end\":\"119193\"},{\"start\":\"119193\",\"end\":\"119206\"},{\"start\":\"119579\",\"end\":\"119588\"},{\"start\":\"119857\",\"end\":\"119866\"},{\"start\":\"119866\",\"end\":\"119878\"},{\"start\":\"120155\",\"end\":\"120173\"},{\"start\":\"120173\",\"end\":\"120184\"},{\"start\":\"120595\",\"end\":\"120603\"},{\"start\":\"120603\",\"end\":\"120610\"},{\"start\":\"120610\",\"end\":\"120622\"},{\"start\":\"120622\",\"end\":\"120631\"},{\"start\":\"121186\",\"end\":\"121197\"},{\"start\":\"121197\",\"end\":\"121207\"},{\"start\":\"121207\",\"end\":\"121216\"},{\"start\":\"121529\",\"end\":\"121541\"},{\"start\":\"121541\",\"end\":\"121551\"},{\"start\":\"121825\",\"end\":\"121836\"},{\"start\":\"121836\",\"end\":\"121849\"}]", "bib_venue": "[{\"start\":\"99912\",\"end\":\"99930\"},{\"start\":\"101247\",\"end\":\"101287\"},{\"start\":\"102699\",\"end\":\"102703\"},{\"start\":\"103304\",\"end\":\"103344\"},{\"start\":\"104995\",\"end\":\"105033\"},{\"start\":\"107494\",\"end\":\"107538\"},{\"start\":\"107921\",\"end\":\"107973\"},{\"start\":\"108339\",\"end\":\"108368\"},{\"start\":\"109086\",\"end\":\"109122\"},{\"start\":\"110307\",\"end\":\"110336\"},{\"start\":\"111808\",\"end\":\"111850\"},{\"start\":\"112502\",\"end\":\"112534\"},{\"start\":\"112847\",\"end\":\"112895\"},{\"start\":\"113221\",\"end\":\"113259\"},{\"start\":\"113910\",\"end\":\"113940\"},{\"start\":\"114586\",\"end\":\"114638\"},{\"start\":\"115053\",\"end\":\"115099\"},{\"start\":\"116188\",\"end\":\"116209\"},{\"start\":\"117375\",\"end\":\"117400\"},{\"start\":\"118495\",\"end\":\"118539\"},{\"start\":\"118833\",\"end\":\"118866\"},{\"start\":\"120248\",\"end\":\"120300\"},{\"start\":\"121264\",\"end\":\"121304\"},{\"start\":\"99467\",\"end\":\"99509\"},{\"start\":\"99693\",\"end\":\"99708\"},{\"start\":\"99883\",\"end\":\"99910\"},{\"start\":\"100069\",\"end\":\"100119\"},{\"start\":\"100307\",\"end\":\"100344\"},{\"start\":\"100542\",\"end\":\"100591\"},{\"start\":\"100841\",\"end\":\"100873\"},{\"start\":\"101199\",\"end\":\"101245\"},{\"start\":\"101645\",\"end\":\"101670\"},{\"start\":\"101974\",\"end\":\"102014\"},{\"start\":\"102314\",\"end\":\"102339\"},{\"start\":\"102687\",\"end\":\"102697\"},{\"start\":\"102918\",\"end\":\"102964\"},{\"start\":\"103256\",\"end\":\"103302\"},{\"start\":\"103687\",\"end\":\"103727\"},{\"start\":\"104051\",\"end\":\"104091\"},{\"start\":\"104369\",\"end\":\"104394\"},{\"start\":\"104650\",\"end\":\"104662\"},{\"start\":\"104949\",\"end\":\"104993\"},{\"start\":\"105280\",\"end\":\"105305\"},{\"start\":\"105541\",\"end\":\"105647\"},{\"start\":\"106019\",\"end\":\"106044\"},{\"start\":\"106451\",\"end\":\"106491\"},{\"start\":\"106824\",\"end\":\"106849\"},{\"start\":\"107116\",\"end\":\"107132\"},{\"start\":\"107442\",\"end\":\"107492\"},{\"start\":\"107861\",\"end\":\"107919\"},{\"start\":\"108302\",\"end\":\"108337\"},{\"start\":\"108679\",\"end\":\"108709\"},{\"start\":\"109042\",\"end\":\"109084\"},{\"start\":\"109509\",\"end\":\"109555\"},{\"start\":\"109906\",\"end\":\"109931\"},{\"start\":\"110270\",\"end\":\"110305\"},{\"start\":\"110726\",\"end\":\"110743\"},{\"start\":\"110940\",\"end\":\"111022\"},{\"start\":\"111273\",\"end\":\"111310\"},{\"start\":\"111758\",\"end\":\"111806\"},{\"start\":\"112074\",\"end\":\"112120\"},{\"start\":\"112462\",\"end\":\"112500\"},{\"start\":\"112791\",\"end\":\"112845\"},{\"start\":\"113175\",\"end\":\"113219\"},{\"start\":\"113453\",\"end\":\"113542\"},{\"start\":\"113872\",\"end\":\"113908\"},{\"start\":\"114085\",\"end\":\"114134\"},{\"start\":\"114526\",\"end\":\"114584\"},{\"start\":\"114999\",\"end\":\"115051\"},{\"start\":\"115485\",\"end\":\"115513\"},{\"start\":\"115798\",\"end\":\"115856\"},{\"start\":\"116159\",\"end\":\"116186\"},{\"start\":\"116454\",\"end\":\"116472\"},{\"start\":\"116740\",\"end\":\"116765\"},{\"start\":\"117036\",\"end\":\"117061\"},{\"start\":\"117338\",\"end\":\"117373\"},{\"start\":\"117675\",\"end\":\"117700\"},{\"start\":\"118059\",\"end\":\"118099\"},{\"start\":\"118439\",\"end\":\"118493\"},{\"start\":\"118788\",\"end\":\"118831\"},{\"start\":\"119033\",\"end\":\"119072\"},{\"start\":\"119588\",\"end\":\"119608\"},{\"start\":\"119878\",\"end\":\"119897\"},{\"start\":\"120184\",\"end\":\"120246\"},{\"start\":\"120631\",\"end\":\"120656\"},{\"start\":\"120862\",\"end\":\"120956\"},{\"start\":\"121216\",\"end\":\"121262\"},{\"start\":\"121551\",\"end\":\"121573\"},{\"start\":\"121849\",\"end\":\"121879\"}]"}}}, "year": 2023, "month": 12, "day": 17}