{"id": 235765577, "updated": "2023-10-06 01:24:27.517", "metadata": {"title": "CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation", "authors": "[{\"first\":\"Yusuke\",\"last\":\"Tashiro\",\"middle\":[]},{\"first\":\"Jiaming\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Stefano\",\"last\":\"Ermon\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "24804-24816", "publication_date": {"year": 2021, "month": 7, "day": 7}, "abstract": "The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2107.03502", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/TashiroSSE21", "doi": null}}, "content": {"source": {"pdf_hash": "8982bb695dcebdacbfd079c62cd7acca8a8b48dc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.03502v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "05c0ef941a76bf2ba02aa6e0d9a9c4a238e1e7c1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8982bb695dcebdacbfd079c62cd7acca8a8b48dc.txt", "contents": "\nCSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation\n\n\nYusuke Tashiro ytashiro@cs.stanford.edu \nJiaming Song tsong@cs.stanford.edu \nDepartment of Computer Science\nStanford University\nStanfordCAUSA\n\nYang Song songyang@cs.stanford.edu \nDepartment of Computer Science\nStanford University\nStanfordCAUSA\n\nStefano Ermon ermon@cs.stanford.edu \nDepartment of Computer Science\nStanford University\nStanfordCAUSA\n\n\nMitsubishi UFJ Trust Investment Technology Institute\nTokyoJapan\n\n\nJapan Digital Design\nTokyoJapan\n\nCSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation\n\nThe imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.\n\nIntroduction\n\nMultivariate time series are abundant in real world applications such as finance, meteorology and healthcare. These time series data often contain missing values due to various reasons, including device failures and human errors [1][2][3]. Since missing values can hamper the interpretation of a time series, many studies have addressed the task of imputing missing values using machine learning techniques [4][5][6]. In the past few years, imputation methods based on deep neural networks have shown great success for both deterministic imputation [7][8][9] and probabilistic imputation [10]. These imputation methods typically utilize autoregressive models to deal with time series.\n\nScore-based diffusion models -a class of deep generative models and generate samples by gradually converting noise into a plausible data sample through denoising -have recently achieved state-ofthe-art sample quality in many tasks such as image generation [11,12] and audio synthesis [13,14], outperforming counterparts including autoregressive models. Diffusion models can also be used to impute missing values by approximating the scores of the posterior distribution obtained from the prior by conditioning on the observed values [12,15,16]. While these approximations may work well in practice, they do not correspond to the exact conditional distribution.\n\nIn this paper, we propose CSDI, a novel probabilistic imputation method that directly learns the conditional distribution with conditional score-based diffusion models. Unlike existing score-based approaches, the conditional diffusion model is designed for imputation and can exploit useful information in observed values. We illustrate the procedure of time series imputation with CSDI in 35th Conference on Neural Information Processing Systems (NeurIPS 2021).  Figure 1. We start imputation from random noise on the left of the figure and gradually convert the noise into plausible time series through the reverse process p \u03b8 of the conditional diffusion model. At each step t, the reverse process removes noise from the output of the previous step (t + 1). Unlike existing score-based diffusion models, the reverse process can take observations (on the top left of the figure) as a conditional input, allowing the model to exploit information in the observations for denoising. We utilize an attention mechanism to capture the temporal and feature dependencies of time series.\n\nFor training the conditional diffusion model, we need observed values (i.e., conditional information) and ground-truth missing values (i.e., imputation targets). However, in practice we do not know the ground-truth missing values, or training data may not contain missing values at all. Then, inspired by masked language modeling, we develop a self-supervised training method that separates observed values into conditional information and imputation targets. We note that CSDI is formulated for general imputation tasks, and is not restricted to time series imputation.\n\nOur main contributions are as follows:\n\n\u2022 We propose conditional score-based diffusion models for probabilistic imputation (CSDI), and implement CSDI for time series imputation. To train the conditional diffusion model, we develop a self-supervised training method.\n\n\u2022 We empirically show that CSDI improves the continuous ranked probability score (CRPS) by 40-65% over existing probabilistic methods on healthcare and environmental data. Moreover, deterministic imputation with CSDI decreases the mean absolute error (MAE) by 5-20% compared to the state-of-the-art methods developed for deterministic imputation.\n\n\u2022 We demonstrate that CSDI can also be applied to time series interpolations and probabilistic forecasting, and is competitive with existing baselines designed for these tasks.\n\n\nRelated works\n\nTime series imputations with deep learning Previous studies have shown deep learning models can capture the temporal dependency of time series and give more accurate imputation than statistical methods. A popular approach using deep learning is to use RNNs, including LSTMs and GRUs, for sequence modeling [17,8,7]. Subsequent studies combined RNNs with other methods to improve imputation performance, such as GANs [9,18,19] and self-training [20]. Among them, the combination of RNNs with attention mechanisms is particularly successful for imputation and interpolation of time series [21,22]. While these methods focused on deterministic imputation, GP-VAE [10] has been recently developed as a probabilistic imputation method.\n\nScore-based generative models Score-based generative models, including score matching with Langevin dynamics [23] and denoising diffusion probabilistic models [11], have outperformed existing methods with other deep generative models in many domains, such as images [23,11], audio [13,14], and graphs [24]. Most recently, TimeGrad [25] utilized diffusion probabilistic models for probabilistic time series forecasting. While the method has shown state-of-the-art performance, it cannot be applied to time series imputation due to the use of RNNs to handle past time series.\n\n\nBackground\n\n\nMultivariate time series imputation\n\nWe consider N multivariate time series with missing values. Let us denote the values of each time series as X = {x 1:K,1:L } \u2208 R K\u00d7L where K is the number of features and L is the length of time series. While the length L can be different for each time series, we treat the length of all time series as the same for simplicity, unless otherwise stated. We also denote an observation mask as\nM = {m 1:K,1:L } \u2208 {0, 1} K\u00d7L where m k,l = 0 if x k,l is missing, and m k,l = 1 if x k,l is observed.\nWe assume time intervals between two consecutive data entries can be different, and define the timestamps of the time series as s = {s 1:L } \u2208 R L . In summary, each time series is expressed as {X, M, s}.\n\nProbabilistic time series imputation is the task of estimating the distribution of the missing values of X by exploiting the observed values of X. We note that this definition of imputation includes other related tasks, such as interpolation, which imputes all features at target time points, and forecasting, which imputes all features at future time points.\n\n\nDenoising diffusion probabilistic models\n\nLet us consider learning a model distribution p \u03b8 (x 0 ) that approximates a data distribution q(x 0 ). Let x t for t = 1, . . . , T be a sequence of latent variables in the same sample space as x 0 , which is denoted as X . Diffusion probabilistic models [26] are latent variable models that are composed of two processes: the forward process and the reverse process. The forward process is defined by the following Markov chain:\nq(x 1:T | x 0 ) := T t=1 q(x t | x t\u22121 ) where q(x t | x t\u22121 ) := N 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I(1)\nand \u03b2 t is a small positive constant that represents a noise level. Sampling of x t has the closed-form written as q(\nx t | x 0 ) = N (x t ; \u221a \u03b1 t x 0 , (1 \u2212 \u03b1 t )I) where\u03b1 t := 1 \u2212 \u03b2 t and \u03b1 t := t i=1\u03b1 i . Then, x t can be expressed as x t = \u221a \u03b1 t x 0 + (1 \u2212 \u03b1 t ) where \u223c N (0, I).\nOn the other hand, the reverse process denoises x t to recover x 0 , and is defined by the following Markov chain:\np \u03b8 (x 0:T ) := p(x T ) T t=1 p \u03b8 (x t\u22121 | x t ), x T \u223c N (0, I), p \u03b8 (x t\u22121 | x t ) := N (x t\u22121 ; \u00b5 \u03b8 (x t , t), \u03c3 \u03b8 (x t , t)I).(2)\nHo et al. [11] has recently proposed denoising diffusion probabilistic models (DDPM), which considers the following specific parameterization of p \u03b8 (x t\u22121 | x t ):\n\u00b5 \u03b8 (x t , t) = 1 \u03b1 t x t \u2212 \u03b2 t \u221a 1 \u2212 \u03b1 t \u03b8 (x t , t) , \u03c3 \u03b8 (x t , t) =\u03b2 1/2 t where\u03b2 t = 1\u2212\u03b1t\u22121 1\u2212\u03b1t \u03b2 t t > 1 \u03b2 1 t = 1(3)\nwhere \u03b8 is a trainable denoising function. We denote \u00b5 \u03b8 (x t , t) and \u03c3 \u03b8 (x t , t) in Eq. (3) as \u00b5 DDPM (x t , t, \u03b8 (x t , t)) and \u03c3 DDPM (x t , t), respectively. The denoising function in Eq. (3) also corresponds to a rescaled score model for score-based generative models [23]. Under this parameterization, Ho et al. [11] have shown that the reverse process can be trained by solving the following optimization problem: min\n\u03b8 L(\u03b8) := min \u03b8 E x0\u223cq(x0), \u223cN (0,I),t || \u2212 \u03b8 (x t , t)|| 2 2 where x t = \u221a \u03b1 t x 0 + (1 \u2212 \u03b1 t ) .(4)\nThe denoising function \u03b8 estimates the noise vector that was added to its noisy input x t . This training objective also be viewed as a weighted combination of denoising score matching used for training score-based generative models [23,27,12]. Once trained, we can sample x 0 from Eq. (2). We provide the details of DDPM in Appendix A.\n\n\nImputation with diffusion models\n\nHere, we focus on general imputation tasks that are not restricted to time series imputation. Let us consider the following imputation problem: given a sample x 0 which contains missing values, we generate imputation targets x ta 0 \u2208 X ta by exploiting conditional observations x co 0 \u2208 X co , where X ta and X co are a part of the sample space X and vary per sample. Then, the goal of probabilistic imputation is to estimate the true conditional data distribution q(x ta 0 | x co 0 ) with a model distribution p \u03b8 (x ta 0 | x co 0 ). We typically impute all missing values using all observed values, and set all observed values as x co 0 and all missing values as x ta 0 , respectively. Note that time series imputation in Section 3.1 can be considered as a special case of this task.\n\nLet us consider modeling p \u03b8 (x ta 0 | x co 0 ) with a diffusion model. In the unconditional case, the reverse process p \u03b8 (x 0:T ) is used to define the final data model p \u03b8 (x 0 ). Then, a natural approach is to extend the reverse process in Eq. (2) to a conditional one:\np \u03b8 (x ta 0:T | x co 0 ) := p(x ta T ) T t=1 p \u03b8 (x ta t\u22121 | x ta t , x co 0 ), x ta T \u223c N (0, I), p \u03b8 (x ta t\u22121 | x ta t , x co 0 ) := N (x ta t\u22121 ; \u00b5 \u03b8 (x ta t , t | x co 0 ), \u03c3 \u03b8 (x ta t , t | x co 0 )I).(5)\nHowever, existing diffusion models are generally designed for data generation and do not take conditional observations x co 0 as inputs. To utilize diffusion models for imputation, previous studies [12,15,16] approximated the conditional reverse process p \u03b8 (x ta t\u22121 | x ta t , x co 0 ) with the reverse process in Eq. (2). With this approximation, in the reverse process they add noise to both the target and the conditional observations x co 0 . While this approach can impute missing values, the added noise can harm useful information in the observations. This suggests that modeling p \u03b8 (x ta t\u22121 | x ta t , x co 0 ) without approximations can improve the imputation quality. Hereafter, we call the model defined in Section 3.2 as the unconditional diffusion model.\n\n\nConditional score-based diffusion model for imputation (CSDI)\n\nIn this section, we propose CSDI, a novel imputation method based on a conditional score-based diffusion model. The conditional diffusion model allows us to exploit useful information in observed values for accurate imputation. We provide the reverse process of the conditional diffusion model, and then develop a self-supervised training method. We note that CSDI is not restricted to time series.\n\n\nImputation with CSDI\n\nWe focus on the conditional diffusion model with the reverse process in Eq. (5) and aim to model the conditional distribution p(x ta t\u22121 | x ta t , x co 0 ) without approximations. Specifically, we extend the parameterization of DDPM in Eq. (3) to the conditional case. We define a conditional denoising function \u03b8 : (X ta \u00d7 R | X co ) \u2192 X ta , which takes conditional observations x co 0 as inputs. Then, we consider the following parameterization with \u03b8 :\n\u00b5 \u03b8 (x ta t , t | x co 0 ) = \u00b5 DDPM (x ta t , t, \u03b8 (x ta t , t | x co 0 )), \u03c3 \u03b8 (x ta t , t | x co 0 ) = \u03c3 DDPM (x ta t , t) (6)\nwhere \u00b5 DDPM and \u03c3 DDPM are the functions defined in Section 3.2. Given the function \u03b8 and data x 0 , we can sample x ta 0 using the reverse process in Eq. (5) and Eq. (6). For the sampling, we set all observed values of x 0 as conditional observations x co 0 and all missing values as imputation targets x ta 0 . Note that the conditional model is reduced to the unconditional one under no conditional observations and can also be used for data generation.\n\n\nTraining of CSDI\n\nSince Eq. (6) uses the same parameterization as Eq. (3) and the difference between Eq. (3) and Eq. (6) is only the form of \u03b8 , we can follow the training procedure for the unconditional model in Section 3.2. Namely, given conditional observations x co 0 and imputation targets x ta 0 , we sample noisy targets x ta t = \u221a \u03b1 t x ta 0 + (1 \u2212 \u03b1 t ) , and train \u03b8 by minimizing the following loss function:  where the dimension of corresponds to that of the imputation targets x ta 0 . However, this training procedure has an issue. Since we do not know the ground-truth missing values in practice, it is not clear how to select x co 0 and x ta 0 from a training sample x 0 . To address this issue, we develop a self-supervised learning method inspired by masked language modeling [28]. We illustrate the training procedure in Figure 2. Given a sample x 0 , we separate observed values of x 0 into two parts, and set one of them as imputation targets x ta 0 and the other as conditional observations x co 0 . We choose the targets x ta 0 through a target choice strategy, which is discussed in Section 4.3. Then, we sample noisy targets x ta t and train \u03b8 by solving Eq. (7). We summarize how we set x co 0 and x ta 0 for training and sampling in Table 1. We also provide the algorithm of training and sampling in Appendix B.1.\n\n\nChoice of imputation targets in self-supervised learning\n\nIn the proposed self-supervised learning, the choice of imputation targets is important. We provide four target choice strategies depending on what is known about the missing patterns in the test dataset. We describe the algorithm for these strategies in Appendix B.2.\n\n(1) Random strategy : this strategy is used when we do not know about missing patterns, and randomly chooses a certain percentage of observed values as imputation targets. The percentage is sampled from [0%, 100%] to adapt to various missing ratios in the test dataset.\n\n(2) Historical strategy: this strategy exploits missing patterns in the training dataset. Given a training sample x 0 , we randomly draw another samplex 0 from the training dataset. Then, we set the intersection of the observed indices of x 0 and the missing indices ofx 0 as imputation targets. The motivation of this strategy comes from structured missing patterns in the real world. For example, missing values often appear consecutively in time series data. When missing patterns in the training and test dataset are highly correlated, this strategy helps the model learn a good conditional distribution.\n\n(3) Mix strategy: this strategy is the mix of the above two strategies. The historical strategy may lead to overfitting to missing patterns in the training dataset. The Mix strategy can benefit from generalization by the random strategy and structured missing patterns by the historical strategy.\n\n(4) Test pattern strategy: when we know the missing patterns in the test dataset, we just set the patterns as imputation targets. For example, this strategy is used for time series forecasting, since the missing patterns in the test dataset are fixed to given future time points.\n\n5 Implementation of CSDI for time series imputation Figure 3: The architecture of 2D attention. Given a tensor with K features, L length, and C channels, the temporal Transformer layer takes tensors with (1, L, C) shape as inputs and learns temporal dependency. The feature Transformer layer takes tensors with (K, 1, C) shape as inputs and learns feature dependency. The output shape of each layer is the same as the input shape.\n\nIn this section, we implement CSDI for time series imputation. For the implementation, we need the inputs and the architecture of \u03b8 .\n\nFirst, we describe how we process time series data as inputs for CSDI. As defined in Section 3.1, a time series is denoted as {X, M, s}, and the sample space X of X is R K\u00d7L . We want to handle X in the sample space R K\u00d7L for learning dependencies in a time series using a neural network, but the conditional denoising function \u03b8 takes inputs x ta t and x co 0 in varying sample spaces that are a part of X as shown in white areas of x ta t and x co 0 in Figure 2. To address this issue, we adjust the conditional denoising function \u03b8 to inputs in the fixed sample space R K\u00d7L . Concretely, we fix the shape of the inputs x ta t and x co 0 to (K \u00d7 L) by applying zero padding to x ta t and x co 0 . In other words, we set zero values to white areas for x ta t and x co 0 in Figure 2. To indicate which indices are padded, we introduce the conditional mask m co \u2208 {0, 1} K\u00d7L as an additional input to \u03b8 , which corresponds to x co 0 and takes value 1 for indices of conditional observations. For ease of handling, we also fix the output shape in the sample space R K\u00d7L by applying zero padding. Then, the conditional denoising function \u03b8 (x ta t , t | x co 0 , m co ) can be written as \u03b8 :\n(R K\u00d7L \u00d7 R | R K\u00d7L \u00d7 {0, 1} K\u00d7L ) \u2192 R K\u00d7L .\nWe discuss the effect of this adjustment on training and sampling in Appendix D.\n\nUnder the adjustment, we set conditional observations x co 0 and imputation targets x ta 0 for time series imputation by following Table 1. At sampling time, since conditional observations x co 0 are all observed values, we set m co = M and x co 0 = m co X where represents element-wise products. For training, we sample x ta 0 and x co 0 through a target choice strategy, and set the indices of x co 0 as m co . Then, x co 0 is written as x co 0 = m co X and x ta 0 is obtained as x ta 0 = (M \u2212 m co ) X. Next, we describe the architecture of \u03b8 . We adopt the architecture in DiffWave [13] as the base, which is composed of multiple residual layers with residual channel C. We refine this architecture for time series imputation. We set the diffusion step T = 50. We discuss the main differences from DiffWave (see Appendix E.1 for the whole architecture and details).\n\nAttention mechanism To capture temporal and feature dependencies of multivariate time series, we utilize a two dimensional attention mechanism in each residual layer instead of a convolution architecture. As shown in Figure 3, we introduce temporal Transformer layer and a feature Transformer layer, which are 1-layer Transformer encoders. The temporal Transformer layer takes tensors for each feature as inputs to learn temporal dependency, whereas the feature Transformer layer takes tensors for each time point as inputs to learn temporal dependency.\n\nNote that while the length L can be different for each time series as mentioned in Section 3.1, the attention mechanism allows the model to handle various lengths. For batch training, we apply zero padding to each sequence so that the lengths of the sequences are the same.\n\nSide information In addition to the arguments of \u03b8 , we provide some side information as additional inputs to the model. First, we use time embedding of s = {s 1:L } to learn the temporal dependency. Following previous studies [29,30], we use 128-dimensions temporal embedding. Second, we exploit categorical feature embedding for K features, where the dimension is 16.\n\n\nExperimental results\n\nIn this section, we demonstrate the effectiveness of CSDI for time series imputation. Since CSDI can be applied to other related tasks such as interpolation and forecasting, we also evaluate CSDI for these tasks to show the flexibility of CSDI. Due to the page limitation, we provide the detailed setup for experiments including train/validation/test splits and hyperparameters in Appendix E.2.\n\n\nTime series imputation\n\nDataset and experiment settings We run experiments for two datasets. The first one is the healthcare dataset in PhysioNet Challenge 2012 [1], which consists of 4000 clinical time series with 35 variables for 48 hours from intensive care unit (ICU). Following previous studies [7,8], we process the dataset to hourly time series with 48 time steps. The processed dataset contains around 80% missing values. Since the dataset has no ground-truth, we randomly choose 10/50/90% of observed values as ground-truth on the test data.\n\nThe second one is the air quality dataset [2]. Following previous studies [7,21], we use hourly sampled PM2.5 measurements from 36 stations in Beijing for 12 months and set 36 consecutive time steps as one time series. There are around 13% missing values and the missing patterns are not random. The dataset contains artificial ground-truth, whose missing patterns are also structured.\n\nFor both dataset, we run each experiment five times. As the target choice strategy for training, we adopt the random strategy for the healthcare dataset and the mix of the random and historical strategy for the air quality dataset, based on the missing patterns of each dataset.\n\nResults of probabilistic imputation CSDI is compared with three baselines. 1) Multitask GP [31]: the method learns the covariance between timepoints and features simultaneously. 2) GP-VAE [10]: the method showed the state-of-the-art results for probabilistic imputation. 3) V-RIN [32]: a deterministic imputation method that uses the uncertainty quantified by VAE to improve imputation. For V-RIN, we regard the quantified uncertainty as probabilistic imputation. In addition, we compare CSDI with imputation using the unconditional diffusion model in order to show the effectiveness of the conditional one (see Appendix C for training and imputation with the unconditional diffusion model).\n\nWe first show quantitative results. We adopt the continuous ranked probability score (CRPS) [33] as the metric, which is freuquently used for evaluating probabilistic time series forecasting and measures the compatibility of an estimated probability distribution with an observation. We generate 100 samples to approximate the probability distribution over missing values and report the normalized average of CRPS for all missing values following previous studies [34] (see Appendix E.3 for details of the computation).   Table 2 represents CRPS for each method. CSDI reduces CRPS by 40-65% compared to the existing baselines for both datasets. This indicates that CSDI generates more realistic distributions than other methods. We also observe that the imputation with CSDI outperforms that with the unconditional model. This suggests CSDI benefits from explicitly modeling the conditional distribution.\n\nWe provide imputation examples in Figure 4. For the air quality dataset, CSDI (green solid line) provides accurate imputations with high confidence, while those by GP-VAE (gray dashed line) are far from ground-truth. CSDI also gives reasonable imputations for the healthcare dataset. These results indicate that CSDI exploits temporal and feature dependencies to provide accurate imputations. We give more examples in Appendix G. Results of deterministic imputation We demonstrate that CSDI also provides accurate deterministic imputations, which are obtained as the median of 100 generated samples. We compare CSDI with four baselines developed for deterministic imputation including GLIMA [21], which combined recurrent imputations with an attention mechanism to capture temporal and feature dependencies and showed the state-of-the-art performance. These methods are based on autoregressive models. We use the original implementations except RDIS.\n\nWe evaluate each method by the mean absolute error (MAE). In Table 3, CSDI improves MAE by 5-20% compared to the baselines. This suggests that the conditional diffusion model is effective to learn temporal and feature dependencies for imputation. For the healthcare dataset, the gap between the baselines and CSDI is particularly significant when the missing ratio is small, because more observed values help CSDI capture dependencies. Dataset and experiment settings We use the same healthcare dataset as the previous section, but process the dataset as irregularly sampled time series, following previous studies [22,35]. Since the dataset has no ground-truth, we randomly choose 10/50/90% of time points and use observed values at these time points as ground-truth on the test data. As the target choice strategy for training, we adopt the random strategy, which is adjusted for interpolation so that some time points are sampled.\n\n\nResults\n\nWe compare CSDI with two baselines including mTANs [22], which utilized an attention mechanism and showed state-of-the-art results for the interpolation of irregularly sampled time series. We generate 100 samples to approximate the probability distribution as with the previous section. The result is shown in Table 4. CSDI outperforms the baselines for all cases. Table 5: Comparing probabilistic forecasting methods with CSDI. We report the mean and the standard error of CRPS-sum for three trials. The baseline results are cited from the original paper. 'TransMAF' is the abbreviation for 'Transformer MAF'. \n\n\nTime series Forecasting\n\nDataset and Experiment settings We use five datasets that are commonly used for evaluating probabilistic time series forecasting. Each dataset is composed of around 100 to 2000 features. We predict all features at future time steps using past time series. We use the same prediction steps as previous studies [34,37]. For the target choice strategy, we adopt the Test pattern strategy.\n\n\nResults\n\nWe compare CSDI with four baselines. Specifically, TimeGrad [25] combined the diffusion model with a RNN-based encoder. We evaluate each method for CRPS-sum, which is CRPS for the distribution of the sum of all time series across K features and accounts for joint effect (see Appendix E.3 for details).\n\nIn Table 5, CSDI outperforms the baselines for electricity and traffic datasets, and is competitive with the baselines as a whole. The advantage of CSDI over baselines for forecasting is smaller than that for imputation in Section 6.1. We hypothesize it is because the datasets for forecasting seldom contains missing values and are suitable for existing encoders including RNNs. For imputation, it is relatively difficult for RNNs to handle time series due to missing values.\n\n\nConclusion\n\nIn this paper, we have proposed CSDI, a novel approach to impute multivariate time series with conditional diffusion models. We have shown that CSDI outperforms the existing probabilistic and deterministic imputation methods.\n\nThere are some interesting directions for future work. One direction is to improve the computation efficiency. While diffusion models generate plausible samples, sampling is generally slower than other generative models. To mitigate the issue, several recent studies leverage an ODE solver to accelerate the sampling procedure [12,38,13]. Combining our method with these approaches would likely improve the sampling efficiency.\n\nAnother direction is to extend CSDI to downstream tasks such as classifications. Many previous studies have shown that accurate imputation improves the performance on downstream tasks [7,18,22]. Since conditional diffusion models can learn temporal and feature dependencies with uncertainty, joint training of imputations and downstream tasks using conditional diffusion models would be helpful to improve the performance of the downstream tasks.\n\nFinally, although our focus was on time series, it would be interesting to explore CSDI as imputation technique on other modalities.\n\n\nA Details of denoising diffusion probabilistic models\n\nIn this section, we describe the details of denoising diffusion probabilistic models in Section 3.2.\n\nDiffusion probabilistic models [26] are latent variable models that are composed of two processes: the forward process and the reverse process. The forward process and the reverse process are defined by Eq. (1) and 2, respectively. Then, the parameters \u03b8 are learned by maximizing variational lower bound (ELBO) of likelihood p \u03b8 (x 0:T ):\nE q(x0) [log p \u03b8 (x 0 )] \u2265 E q(x0,x1,...,x T ) [log p \u03b8 (x 0:T ) \u2212 log q(x 1:T | x 0 )] := ELBO.(8)\nTo analyse this ELBO, Ho et al. [11] proposed denoising diffusion probabilistic models (DDPM), which considered the parameterization given by Eq. (3). Under the parameterization, Ho et al. [11] showed ELBO satisfies the following equation:\n\u2212ELBO = c + T t=1 \u03ba t E x0\u223cq(x0), \u223cN (0,I) || \u2212 \u03b8 ( \u221a \u03b1 t x 0 + (1 \u2212 \u03b1 t ) , t)|| 2 2(9)\nwhere c is a constant and {\u03ba 1:T } are positive coefficients depending on \u03b1 1:T and \u03b2 1:T . The diffusion process can be trained by minimizing Eq. (9). In addition, Ho et al. [11] found that minimizing the following unweighted version of ELBO leads to good sample quality:\nmin \u03b8 L(\u03b8) := min \u03b8 E x0\u223cq(x0), \u223cN (0,I),t || \u2212 \u03b8 ( \u221a \u03b1 t x 0 + (1 \u2212 \u03b1 t ) , t)|| 2 2 .(10)\nThe function \u03b8 estimates noise in the noisy input. Once trained, we can sample x 0 from Eq. (2).\n\n\nB Algorithms B.1 Algorithm for training and sampling of CSDI\n\nWe provide the training procedure of CSDI in Algorithm 1 and the imputation (sampling) procedure with CSDI in Algorithm 2, which are described in Section 4. Calculate noisy targets\nx ta t = \u221a \u03b1 t x ta 0 + (1 \u2212 \u03b1 t ) 8:\nTake gradient step on \u2207 \u03b8 ||( \u2212 \u03b8 (x ta t , t | x co 0 ))|| 2 2 according to Eq. (7) Algorithm 2 Imputation (Sampling) with CSDI 1: Input: a data sample x 0 , trained denoising function \u03b8 2: Output: Imputed missing values x ta  \n\n\nB.2 Target choice strategies for self-supervised training\n\nWe describe the target choice strategies for self-supervised training of CSDI, which is discussed in Section 4.3. We give the algorithm of the random strategy in Algorithm 3 and that of the historical  \n\n\nD CSDI for implementation of time series imputation\n\nIn this section, we discuss the effect of adjusting the function \u03b8 , described in Section 5. First, let us consider the effect of the adjustment on sampling. The adjustment does not essentially affect the model at sampling time, because all values of data are either conditional observations or imputation targets as shown in Table 1 and the model can distinguish the type of each value through the mask m co . Since the output shape is adjusted, we need to recover the shape by extracting the indices of the imputation targets from the output, so that we substitute the outputs into Eq. (6).\n\nNext, we focus on the effect of the adjustment on training. Unlike sampling, the model at training time cannot distinguish imputation targets and missing values since we ignore missing values during training as shown in Table 1. In order to handle the missing values, we need to modify the inputs to \u03b8 . Here, we use a similar approach to the training procedure with the unconditional model in Section C.2. Namely, we treat the missing indices like a part of imputation targets. We illustrate the extended training procedure in Figure 5. First, we set zeros to the missing indices as dummy values. We denote the extended imputation targets as x ta 0 . Then, we sample noisy targets x ta t = \u221a \u03b1 t x ta 0 + (1 \u2212 \u03b1 t ) ta , where ta is masked noise and is given by ta := (1 \u2212 m co )\n\n, as shown in Figure 5. We denoise the noisy targets for training. We only estimate the noise for the original imputation targets, since the dummy values contain no information about the data distribution. In other words, we train \u03b8 by solving the following optimization problem:\nmin \u03b8 L(\u03b8) := min \u03b8 E x0\u223cq(x0), \u223cN (0,I),t ||( \u2212 \u03b8 (x ta t , t | x co 0 , m co )) m ta || 2 2(12)\nwhere m ta is a mask which corresponds to x ta 0 and takes value 1 for the original imputation targets.\n\n\nE Details of architectures and experiment settings E.1 Details of implementation of CSDI\n\nWe describe the details of architectures and hyperparameters for the conditional diffusion model described in Section 5. First, we provide the whole architecture of CSDI in Figure 6. Since the architecture in Figure 6 is based on DiffWave [13], we mainly explain the difference from DiffWave.\n\nOn the top of the figure, the models take x co 0 and x ta t as inputs since \u03b8 is the conditional denoising function. For the diffusion step t, we use the following 128-dimensions embedding following previous works [29,13]: As for Transformer layers, we used 1-layer TransformerEncoder implemented in PyTorch [39], which is composed of a multi-head attention layer, fully-connected layers and layer normalization. Only for forecasting tasks, we adopted the \"linear attention transformer\" package [40] to improve computational efficiency, since the forecasting datasets we used contained many features and long sequences. The package implements an efficient attention mechanism [41], and we only used global attention in the package.\n\n\nE.2 Details of experiment settings in Section 6\n\nIn this section, we provide the details of the experiment settings in Section 6. When we evaluated baseline methods with the original implementation in each section, we used their original hyperparameters and model size. Although we also ran experiments under the same model size as our model, the performance did not improve in more than half of the cases and did not outperform our model in all cases.\n\n\nE.2.1 Experiment settings for imputation in Section 6.1\n\nFirst, we explain additional information for the air quality dataset. The dataset is composed of air quality data in Beijing from 2014/05/01 to 2015/04/30. The dataset contains artificial ground-truth, whose missing patterns are created based on those in the next month.\n\nNext, we describe data splits. For the healthcare dataset, we randomly divided the dataset into five parts and used one of them as test data for each run. We also randomly split the remaining data into train and validation data with a ratio of 7:1. For the air quality dataset, following [2], we used the 3rd, 6th, 9th and 12th months as test data. To avoid evaluating imputation for each missing value multiple times, we separated the test data of each month every 36 consecutive time steps without overlap. When the length of a monthly data was not divisible by 36, we allowed the last sequence to overlap with the previous one and did not aggregate the result for the overlapping parts. For each run, we selected a month as validation data and used the rest as training data. We note that we excluded the 4th, 7th, 10th, and 1st months from missing pattern dataset for the historical strategy, because these months were used for creating missing patterns of the artificial ground-truth.\n\nOn the healthcare dataset, due to the different scales of features, we evaluate the performance on normalized data following previous studies [7]. For training of all tasks, we normalize each feature to have zero mean and unit variance.\n\nAs for hyperparameters, we set the batch size as 16 and the number of epochs as 200. We used Adam optimizer with learning rate 0.001 that is decayed to 0.0001 and 0.00001 at 75% and 90% of the total epochs, respectively. As for the model, we set the number of residual layers as 4, residual channels as 64, and attention heads as 8. We followed DiffWave [13] for the number of channels and decided the number of layers based on the validation loss and the parameter size. The number of the parameter in the model is about 415,000.\n\nWe also provide hyperparameters for the diffusion model as follows. We set the number of the diffusion step T = 50, the minimum noise level \u03b2 1 = 0.0001, and the maximum noise level \u03b2 T = 0.5. Since recent studies [38,42] reported that gentle decay of \u03b1 t could improve the sample quality, we adopted the following quadratic schedule for other noise levels:\n\u03b2 t = T \u2212 t T \u2212 1 \u03b2 1 + t \u2212 1 T \u2212 1 \u03b2 T 2 .(15)\nWith regard to the baselines for probabilistic imputation, we used their original implementations for GP-VAE and V-RIN. For Multitask GP, we utilized GPyTorch [43] for the implementation. We used RBF kernel for the covariance between timepoints and low-rank IndexKernel with rank = 10 for that between features.\n\nFinally, we describe the baselines for deterministic imputation, which were used for comparison. 1) BRITS [7]: the method utilizes a bi-directional recurrent neural network to handle multiple correlated missing values. 2) V-RIN [32]: the method utilizes the uncertainty learned with VAE to improve recurrent imputation. 3) GLIMA [21]: the method combines recurrent imputations with an attention mechanism to capture cross-time and cross-feature dependencies and shows the state-of-the-art performance. 4) RDIS [20]: the method applies random drops to given training data for self-training. We used the original implementation for BRITS and V-RIN. For RDIS, we set the number of models as 8, hidden units as 108, drop rate as 30%, threshold as 0.1, update epoch as 200, and total epochs as 1000.\n\n\nE.2.2 Experiment settings for interpolation in Section 6.2\n\nFirst, we explain how we process the dataset. We processed the healthcare dataset as irregularly sampled time series. Following previous studies [22,35], we rounded observation times to the nearest minute. Then, there are 48 \u00d7 60 possible measurement times per time series, and the lengths of time series samples can be different each other.\n\nWe used almost the same experiment settings as those for imputation in Section E.2.1. Since the length of each irregularly sampled time series is different, we applied zero padding to each time series in order to fix the length for each batch. The padding does not affect the result since the attention mechanisms in the implementation of CSDI can deal with the padding by using padding masks.\n\nWe describe the baselines which were used for comparison. We used the original implementation. 1) Latent ODE [35]: the method consists of an ODE-RNN model as the encoder and a neural ODE model as the decoder. 2) mTANs [22]: the method utilized an attention mechanism and showed state-of-the-art results for the interpolation of irregularly sampled time series.  solar  137  10392  168  24  7  50  electricity  370  5833  168  24  7  100  traffic  963  7009  168  24  7  200  taxi  1214  1488  48  24  56  300  wiki  2000  792  90  30 5 300\n\n\nE.2.3 Datasets and Experiment settings for forecasting in Section 6.3\n\nFirst we describe the datasets we used. We used five open datasets that are commonly used for evaluating probabilistic time series forecasting. The datasets were preprocessed in Salinas et al. [34] and provided in GluonTS 1 [44]:\n\n\u2022 solar [45]: hourly solar power production records of 137 stations in Alabama State. We summarize the characteristics of each dataset in Table 6. The task for these datasets is to predict the future L 2 steps by exploiting the latest L 1 steps where L 1 and L 2 depend on datasets as shown in Table 6. We set L 1 and L 2 referring to previous studies [37]. For training, we randomly selected L 1 + L 2 consecutive time steps as one time series and set the last L 2 steps as imputation targets. We followed the train/test split in previous studies. We used the last five samples of training data as validation data.\n\nAs for experiment settings, since we basically followed the setting for time series imputation in Section E.2.1, we only describe the difference from it. We ran each experiment three times with different random seeds. We set batch size as 8 because of longer sequence length, and utilized an efficient Transformer as mentioned in Section E.1.\n\nSince the number of features K is large, we adopted subset sampling of features for training. For each time series in a training batch, we randomly chose a subset of features and only used the subset for the batch. The attention mechanism allows the model to take varying length inputs. We set the subset size as 64. Due to the subset sampling, we need large epochs when the number of features K is large. Therefore, we set training epochs based on the number of features and the validation loss. We provide the epochs in Table 6.\n\nFinally, we describe the baselines which were used for comparison. 1) GP-copula [34]: the method combines a RNN-based model with a Gaussian copula process to model time-varying correlations. 2) Transformer MAF [36]: the method uses Transformer to learn temporal dynamics and a conditioned normalizing flow to capture feature dependencies. 3) TLAE [37]: the method combines a RNN-based model with autoencoders to learn latent temporal patterns. 4) TimeGrad [25]: the method has shown the state-of-the-art results for probabilistic forecasting by combining a RNN-based model with diffusion models.\n\n\nE.3 Computations of CRPS\n\nWe describe the definition and computation of the CRPS metric.\n\nThe continuous ranked probability score (CRPS) [33] measures the compatibility of an estimated probability distribution F with an observation x, and can be defined as the integral of the quantile loss \u039b \u03b1 (q, z) := (\u03b1 \u2212 1l z<q )(z \u2212 q) for all quantile levels \u03b1 \u2208 [0, 1]:\nCRPS(F \u22121 , x) = 1 0 2\u039b \u03b1 (F \u22121 (\u03b1), x)d\u03b1(16)\nwhere 1l is the indicator function. We generated 100 samples to approximate the distribution F over each missing value. We computed quantile losses for discretized quantile levels with 0.05 ticks. Namely, we approximated CRPS with\nCRPS(F \u22121 , x) 19 i=1 2\u039b i * 0.05 (F \u22121 (i * 0.05), x)/19.(17)\nThen, we evaluated the following normalized average of CRPS for all features and time steps:\nk,l CRPS(F \u22121 k,l , x k,l ) k,l |x k,l |(18)\nwhere k and l indicates features and time steps of imputation targets, respectively.\n\nFor probabilistic forecasting, we evaluated CRPS-sum. CRPS-sum is CRPS for the distribution F of the sum of all K features and is computed by the following equation:\nl CRPS(F \u22121 , k x k,l ) k,l |x k,l |(19)\nwhere k x k,l is the sum of forecasting targets for all features at time point l. \n\n\nF Additional results and experiments\n\n\nF.1 Effectiveness of two dimensional attention mechanism\n\nIn this paper, we utilized a two dimensional attention mechanism to learn temporal and feature dependencies. To show the effectiveness of the attention mechanism, we demonstrate an ablation study. We replace the attention mechanism with the following architecture baselines and compare the performance:\n\n\u2022 no temporal: remove temporal attention layers \u2022 no feature: remove feature attention layers \u2022 flatten: flatten 2D tensor (K features x L length) to 1D, and input the 1D vector to transformer layers\n\n\u2022 Bi-RNN: replace the attention mechanism with Bi-directional RNN which is a popular architecture for multivariate time series imputation \u2022 dilated conv: replace temporal and feature attention layers with 1D dilated convolution layers, respectively. The dilated convolution was used in previous studies for diffusion models [13,25] We set hyperparameters of each architecture so that the number of parameters is almost the same as our attention mechanism. We show the result in Table 7. Our attention mechanism outperforms all of the other architectures. The comparison with \"no temporal\" and \"no feature\" shows that both temporal and feature correlations are important for accurate imputation. The comparison with \"flatten\", \"Bi-RNN\", and \"dilated conv\" shows that our attention mechanism is effective to learn temporal and feature dependency compared with existing methods. In summary, the result of the ablation indicates the proposed attention mechanism plays a key role in improving the imputation performance by a large margin. The negative log likelihood (NLL) is a popular metric for evaluating probabilistic methods and ELBO is often utilized to estimate NLL. A reason why we mainly focused on other metrics is that ELBO is sometimes far from NLL and uncorrelated with the quality of generated samples. Specifically, in the proposed method, the choice of the noise schedule highly affects the ELBO while it has little effect on the sample quality.\n\nTo demonstrate this, we performed an experiment. We chose the following three noise schedules for CSDI and calculated NLL and CRPS for each schedule.\n\n\u2022 quadratic (used in the paper): quadratic spaced schedule between \u03b2 min = 0.0001 and \u03b2 max = 0.5 \u2022 linear: linear spaced schedule with the same \u03b2 min and \u03b2 max as those in the paper \u2022 quadratic (large minimum noise): quadratic schedule with large minimum noise level \u03b2 min = 0.001, which makes the model ignore small noise\n\nWe also calculated the metrics for GP-VAE. The result is shown in Table 8. While CRPS by the proposed method is almost independent from the choice of schedules, NLL significantly depends on the schedule. This phenomenon happens because time series data is generally noisy and it is difficult to denoise small noise during imputation. Estimated scores by the model could be inaccurate when inputs to the model (i.e. imputation targets) only contain small noise. These inaccurate scores could make the estimated ELBO loose, whereas small noise does not affect the sample quality. When the minimum noise level \u03b2 min is large, since the model does not denoise small noise in sampling steps, ELBO by the proposed method is tightly estimated and smaller than that by GP-VAE. Therefore, ELBO is not suitable for evaluating the sample quality and we adopted other metrics such as CRPS and MAE.\n\n\nF.3 Experimental results for other metrics in Section 6\n\nWe show the experimental results in Section 6 for different metrics in Table 9 to 12. Table 9 evaluates RMSE for deterministic imputation methods. We added SSGAN [19] as an additional baseline, which has shown the state-of-the-art performance for RMSE in the healthcare dataset. We can confirm that CSDI outperforms all baselines for RMSE. The advantage of CSDI is particularly large when the missing ratio is low. This result is consistent with that in Section 6.1.  Table 4. Table 11 and 12 report CRPS and MSE for probabilistic forecasting methods, respectively. We exclude TimeGrad [25] from the baselines, as they did not report these metrics. We can see that CSDI is competitive with baselines for these metrics as with CRPS-sum.\n\n\nF.4 Effect of the number of generated samples\n\nFor the experiments in Section 6, we generated 100 samples to estimate the distribution of imputation. We demonstrate the relationship between the number of samples and the performance in Figure 7.\n\nWe can see that five or ten samples are enough to estimate good distributions and outperform the baselines. Increasing the number of samples further improves the performance, and the improvement becomes marginal over 50 samples.    Table 5. We report the mean and the standard error for three trials. The results for baseline methods are cited from their paper. 'TransMAF' is the abbreviation for 'Transformer MAF'.  6.8e2(7.5e1) 2.0e5(9.2e4) 4.0e-4(2.9e-6) 2.6e1(8.1e-1) 3.8e7(4.2e4) CSDI (proposed) 9.0e2(6.1e1) 1.1e5(2.8e3) 3.5e-4(7.0e-7) 1.7e1(6.8e-2) 3.5e7(4.4e4) Table 13: The effect of the target choice strategy for the air quality dataset. We report the mean and the standard error for five trials.  Table 2 and the second row shows the effect on deterministic imputation in Table 3.\n\n\nF.5 Effect of target choice strategy\n\nIn the experiment for the air quality dataset in Section 6.1, we adopted the mix strategy for the target choice. Here, we provide the result for other strategies and show the effect of the target choice strategy on imputation quality. In Table 13, the performances of the mix strategy and the random strategy are almost the same, and the performance of the historical strategy is slightly worse than that of the other strategies. This means that the historical strategy is not effective for the air quality dataset even though the dataset contains structured missing patterns. This is due to the difference of missing patterns between training dataset and test dataset. Note that all strategies outperform the baselines in Table 2 and Table 3.\n\n\nG Additional examples of probabilistic imputation\n\nIn this section, we illustrate various imputation examples to show the characteristic of imputed samples. We pick a multivariate time series from the results of each experiment in Section 6.1 and show imputation results for all features of each time series in Figure 8 to 11. We compare CSDI with GP-VAE in Figure 8 to 11. Note that the scales of the y axis depend on the features. For the healthcare dataset with 90% missing ratio in Figure 10, while GP-VAE fails to learn the distribution, CSDI gives reasonable probabilistic imputation for most of the features. For the air quality dataset in Figure 11, CSDI learns the dependency between features and provides more accurate imputation than GP-VAE. In Figure 12 to 15, we compare CSDI with the unconditional diffusion model. In all figures, CSDI tends to provide tighter uncertainty than the unconditional diffusion model. We hypothesize that it is due to the approximation discussed in Section 3.3. Since the unconditional model approximates the conditional distribution by using noisy observed values, the estimated imputation become less confident than that with the conditional model.\n\n\nH Potential negative societal impacts\n\nSince score-based diffusion models are generative models, our proposed model has negative impacts as well as other generative models. For example, the model can potentially memorize private information and be used to generate fake data.        \n\nFigure 1 :\n1The procedure of time series imputation with CSDI. The reverse process p \u03b8 gradually converts random noise into plausible time series, conditioned on observed values x co 0 . Dashed lines in each box represent observed values, which are plotted in order to show the relationship with generated imputation and not included in each x ta t .\n\nFigure 2 :\n2min \u03b8 L(\u03b8) := min \u03b8 E x0\u223cq(x0), \u223cN (0,I),t ||( \u2212 \u03b8 (x ta t , t | x co 0 ))The self-supervised training procedure of CSDI. On the middle left rectangle, the green and white areas represent observed and missing values, respectively. The observed values are separated into red imputation targets x ta 0 and blue conditional observations x co 0 , and used for training of \u03b8 . The colored areas in each rectangle mean the existence of values.\n\nFigure 4 :\n4Examples of probabilistic time series imputation for the healthcare dataset with 50% missing (left) and the air quality dataset (right). The red crosses show the observed values and the blue circles show the ground-truth imputation targets. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\n\n0.020) 0.021(0.001) 0.044(0.006) 0.114(0.020) 0.049(0.002) CSDI (proposed) 0.298(0.004) 0.017(0.000) 0.020(0.001) 0.123(0.003) 0.047(0.003)\n\n\nInput: distribution of training data q(x 0 ), a target choice strategy T , the number of iteration N iter , the sequence of noise levels {\u03b1 t } 2: Output: Trained denoising function \u03b8 3: for i = 1 to N iter do 4: t \u223c Uniform({1, . . . , T }), x 0 \u223c q(x 0 ) 5: Separate observed values of x 0 into conditional information x co 0 and imputation targets x ta 0 by the target choice strategy T 6: \u223c N (0, I) where the dimension of corresponds to x\n\n0 3 :\n3Denote observed values of x 0 as x co 0 4: x ta T \u223c N (0, I) where the dimension of x ta T corresponds to the missing indices of x 0 5: for t = T to 1 do\n\n\nchoose r% of the observed values of x 0 and denote the chosen observations as x ta 0 , and denote the remaining observations as x co 0 Algorithm 4 Target choice with the historical strategy 1: Input: a training sample x 0 , missing pattern dataset D miss 2: Output: conditional information x co 0 , imputation targets x ta 0 3: Draw a data samplex 0 from D miss 4: Denote the indices of observed values of x 0 as J 5: Denote the indices of missing values ofx 0 asJ 6: Take the intersection of J andJ, and denote values of x 0 for the intersection as x ta 0 7: Set the remaining observations of x 0 as x co 0 strategy in Algorithm 4. On the historical strategy, we use the training dataset as missing pattern dataset D miss , unless otherwise stated. The mix strategy draws one of the two strategies with a ratio of 1:1 for each training sample. The test pattern strategy just uses the fixed missing pattern in the test dataset to choose imputation targets.\n\nFigure 5 :\n5The self-supervised training procedure of CSDI for implementation of time series imputation. The colored areas in each rectangle represent the existence of values. The green and white areas represent observed and missing values, respectively, and white areas are padded with zeros to fix the shape of the inputs. Zero padding is also applied to all white areas. As withFigure 2, the observed values are separated into red imputation targets x ta 0 and blue conditional observations x co 0 . For the extended targets x ta 0 , the area of value 0 shows dummy values.\n\nFigure 6 :\n6Architecture of \u03b8 in CSDI for multivariate time series imputation. Similarly, we utilize time embedding of s = {s 1:L } as a side information. We use 128-dimensions temporal embedding following previous studies [29, 30]: s embedding (s l ) = sin(s l /\u03c4 0/64 ), . . . , sin(s l /\u03c4 63/64 ), cos(s l /\u03c4 0/64 ), . . . , cos(s l /\u03c4 63/64 ) (14) where \u03c4 = 10000. On the top right of the figure, we expand each side information and concatenate all the side information. On the bottom right of the figure, we multiply the output by a mask (1 \u2212 m co ) in order to mask the indices of the conditional observations of the output.\n\n\u2022 electricity 2 :\n2hourly electricity consumption of 370 customers. \u2022 traffic 3 : hourly occupancy rate of 963 San Fancisco freeway car lanes. \u2022 taxi 4 : half hourly traffic time series of New York taxi rides taken at 1214 locations in the months of January 2015 for training and January 2016 for test. \u2022 wiki: daily page views of 2000 Wikipedia pages.\n\n\n0.025) 0.058(0.002) 0.097(0.001) 0.369(0.006) 0.298(0.001) CSDI (proposed) 0.338(0.012) 0.041(0.000) 0.073(0.000) 0.271(0.001) 0.207(0.002)\n\nFigure 7 :\n7.108(0.001)9.58(0.08) historical 0.113(0.001) 10.12(0.05) mix 0.108(0.001) 9.60(0.04) The effect of the number of generated samples. The first row shows the effect on probabilistic imputation in\n\nFigure 8 :\n8Comparison of imputation between GP-VAE and CSDI for the healthcare dataset (10% missing). The result is for a time series sample with all 35 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDI and GP-VAE, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 9 :\n9Comparison of imputation between GP-VAE and CSDI for the healthcare dataset (50% missing). The result is for a time series sample with all 35 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDIand GP-VAE, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 10 :\n10Comparison of imputation between GP-VAE and CSDI for the healthcare dataset (90% missing). The result is for a time series sample with all 35 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDI and GP-VAE, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 11 :\n11Comparison of imputation between GP-VAE and CSDI for the air quality dataset. The result is for a time series sample with all 36 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDI and GP-VAE, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 12 :\n12Comparison of imputation between the unconditional diffusion model and CSDI for the healthcare dataset (10% missing). The result is for a time series sample with all 35 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDI and the unconditional model, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 13 :\n13Comparison of imputation between the unconditional diffusion model and CSDI for the healthcare dataset (50% missing). The result is for a time series sample with all 35 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDIand the unconditional model, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 14 :\n14Comparison of imputation between the unconditional diffusion model and CSDI for the healthcare dataset (90% missing). The result is for a time series sample with all 35 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDI and the unconditional model, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nFigure 15 :\n15Comparison of imputation between the unconditional diffusion model and CSDI for the air quality dataset. The result is for a time series sample with all 36 features. The red crosses show observed values and the blue circles show ground-truth imputation targets. Green and gray colors correspond to CSDI and the unconditional model, respectively. For each method, median values of imputations are shown as the line and 5% and 95% quantiles are shown as the shade.\n\nTable 1 :\n1Imputation targets x ta \n0 and conditional observations x co \n0 for CSDI at training and sampling. \n\nimputation targets x ta \n\n0 \n\nconditional observations x co \n\n0 \n\nsampling (imputation) \nall missing values \nall observed values \n\ntraining \na subset of the observed values \n(sampled by a target choice strategy) \n\nthe remaining \nobserved values \n\n\n\nTable 2 :\n2Comparing CRPS for probabilistic imputation baselines and CSDI (lower is better). We report the mean and the standard error of CRPS for five trials.healthcare \nair quality \n\n10% missing \n50% missing \n90% missing \n\nMultitask GP [31] 0.489(0.005) \n0.581(0.003) \n0.942(0.010) \n0.301(0.003) \nGP-VAE [10] \n0.574(0.003) \n0.774(0.004) \n0.998(0.001) \n0.397(0.009) \nV-RIN [32] \n0.808(0.008) \n0.831(0.005) \n0.922(0.003) \n0.526(0.025) \nunconditional \n0.360(0.007) \n0.458(0.008) \n0.671(0.007) \n0.135(0.001) \nCSDI (proposed) \n0.238(0.001) 0.330(0.002) 0.522(0.002) 0.108(0.001) \n\n\nTable 3 :\n3Comparing MAE for deterministic imputation methods and CSDI. We report the mean and the standard error for five trials. The asterisks mean the results of the method are cited from the original paper.healthcare \nair quality \n\n10% missing \n50% missing \n90% missing \n\nV-RIN [32] \n0.271(0.001) \n0.365(0.002) \n0.606(0.006) \n25.4(0.62) \nBRITS [7] \n0.284(0.001) \n0.368(0.002) \n0.517(0.002) 14.11(0.26) \nBRITS [7] (*) \n0.278 \n\u2212 \n\u2212 \n11.56 \nGLIMA [21] (*) \n0.265 \n\u2212 \n\u2212 \n10.54 \nRDIS [20] \n0.319(0.002) \n0.419(0.002) \n0.631(0.002) 22.11(0.35) \nunconditional \n0.326(0.008) \n0.417(0.010) \n0.625(0.010) 12.13(0.07) \nCSDI (proposed) 0.217(0.001) 0.301(0.002) 0.481(0.003) 9.60(0.04) \n\n\n\nTable 4 :\n4Comparing the state-of-the-art interpolation methods with CSDI for the healthcare dataset. We report the mean and the standard error of CRPS for five trials.10% missing \n50% missing \n90% missing \n\nLatent ODE [35] \n0.700(0.002) \n0.676(0.003) \n0.761(0.010) \nmTANs [22] \n0.526(0.004) \n0.567(0.003) \n0.689(0.015) \nCSDI (proposed) 0.380(0.002) 0.418(0.001) 0.556(0.003) \n\n6.2 Interpolation of irregularly sampled time series \n\n\n\nTable 6 :\n6Description of datasets for time series forecasting.feature K \ntotal \ntime step \n\nhistory \nsteps L 1 \n\nprediction \nsteps L 2 \n\ntest \nsample \nepochs \n\n\n\nTable 7 :\n7Comparing the two dimension attention mechanism of various architectures. For ablations, \nwe report the mean and the standard error for three trials. \n\nhealthcare (10% missing) \nair quality \nMAE \nCRPS \nMAE \nCRPS \n\nno-temporal \n0.439(0.004) \n0.475(0.001) 26.63(0.23) 0.292(0.002) \nno-feature \n0.352(0.001) \n0.386(0.002) 14.44(0.11) 0.162(0.001) \nflatten \n0.383(0.002) \n0.418(0.002) 12.26(0.09) 0.139(0.001) \nBi-RNN \n0.272(0.001) \n0.301(0.001) 12.56(0.26) 0.142(0.003) \ndilated conv \n0.279(0.002) \n0.305(0.002) 11.67(0.11) 0.130(0.001) \n2D attention (proposed) 0.217(0.001) 0.238(0.001) 9.60(0.04) 0.108(0.001) \n\n\n\nTable 8 :\n8Comparison of the negative log likelihood (NLL) and CRPS for various schedules. We report the mean for three trials.healthcare (10% missing) \nair quality \nmethod \nschedule \nNLL \nCRPS \nNLL \nCRPS \n\nGP-VAE \u2212 \n< 1.22 \n0.574 \n< 1.09 \n0.397 \nproposed quad. (in paper) \n< 1.63 \n0.238 \n< 0.97 \n0.108 \nproposed linear \n< 29.70 \n0.240 \n< 18.55 \n0.110 \nproposed quad. (large min. noise) < 0.07 \n0.239 \n< \u22120.70 0.109 \n\nF.2 Comparison of negative log likelihood for probabilistic imputation \n\n\n\nTable 10 evaluates\n10MAE and RMSE for interpolation methods. The result is consistent with\n\nTable 9 :\n9Comparing deterministic imputation methods with CSDI for RMSE. The results correspond toTable 3. We report the mean and the standard error for five trials. The asterisk means the values are cited from the original paper.healthcare \nair quality \n\n10% missing \n50% missing \n90% missing \n\nV-RIN [32] \n0.628(0.025) \n0.693(0.022) \n0.928(0.013) \n40.11(1.14) \nBRITS [7] \n0.619(0.022) \n0.693(0.023) \n0.836(0.015) \n24.47(0.73) \nRDIS [20] \n0.633(0.021) \n0.741(0.018) \n0.934(0.013) \n37.49(0.28) \nSSGAN [19] (*) \n0.598 \n0.762 \n0.818 \n\u2212 \nunconditional \n0.621(0.020) \n0.734(0.024) \n0.940(0.018) \n22.58(0.23) \nCSDI (proposed) 0.498(0.020) 0.614(0.017) 0.803(0.012) 19.30(0.13) \n\n\n\nTable 10 :\n10Comparing the state-of-the-art interpolation method with CSDI for MAE and RMSE. The results correspond toTable 4. We report the mean and the standard error for five trials. CSDI (proposed) 0.722(0.043) 0.700(0.013) 0.839(0.009)10% missing \n50% missing \n90% missing \n\nMAE \n\nLatent ODE [35] \n0.522(0.002) \n0.506(0.003) \n0.578(0.009) \nmTANs [22] \n0.389(0.003) \n0.422(0.003) \n0.533(0.005) \nCSDI (proposed) 0.362(0.001) 0.394(0.002) 0.518(0.003) \n\nRMSE \n\nLatent ODE [35] \n0.799(0.012) \n0.783(0.012) \n0.865(0.017) \nmTANs [22] \n0.749(0.037) \n0.721(0.014) \n0.836(0.018) \n\n\nTable 11 :\n11Comparing probabilistic forecasting methods with CSDI for CRPS. The results correspond to\n\nTable 12 :\n12Comparing probabilistic forecasting methods with CSDI for MSE. The results correspond toTable 5. We report the mean and the standard error for three trials. The results for baseline methods are cited from their paper. 'TransMAF' is the abbreviation for 'Transformer MAF'. 'TransMAF' did not report the standard error.solar \nelectricity \ntraffic \ntaxi \nwiki \n\nGP-copula [34] 9.8e2(5.2e1) 2.4e5(5.5e4) 6.9e-4(2.2e-5) 3.1e1(1.4e0) 4.0e7(1.6e9) \nTransMAF [36] \n9.3e2 \n2.0e5 \n5.0e-4 \n4.5e1 \n3.1e7 \nTLAE [37] \n\nt embedding (t) = sin(10 0\u00b74/63 t), . . . , sin(10 63\u00b74/63 t), cos(10 0\u00b74/63 t), . . . , cos(10 63\u00b74/63 t) .(13) \nhttps://github.com/awslabs/gluon-ts 2 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 3 https://archive.ics.uci.edu/ml/datasets/PEMS-SF 4 https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data\nAcknowledgements and Disclosure of FundingC Training and imputation for unconditional diffusion model C.1 Imputation with unconditional diffusion modelWe describe the imputation method with the unconditional diffusion model used for the experiments in Section 6.1. We followed the method described in previous studies[12]. To utilize unconditional diffusion models for imputation, they approximated the conditional reverse process p \u03b8 (x ta t\u22121 | x ta t , x co 0 ) in Eq. (5) with the unconditional reverse process in Eq.(2). Given a test sample x 0 , they set all observed values as conditional observations x co 0 and all missing values as imputation targets x ta 0 . Then, instead of conditional observations x co 0 , they considered noisy conditional observationst ; x ta t ] combines x co t and x ta t to create a sample in X . Using this approximation, we can sampleand obtain x ta t\u22121 by extracting target indices from x t\u22121 . By repeating the sampling procedure from t = T to t = 1, we can generate imputation targets x ta 0 .C.2 Training procedure of unconditional diffusion models for time series imputationIn Section 3.\nPredicting inhospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. Ikaro Silva, George Moody, J Daniel, Leo A Scott, Roger G Celi, Mark, Computing in Cardiology. IEEEIkaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting in- hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In Computing in Cardiology, pages 245-248. IEEE, 2012.\n\nST-MVL: filling missing values in geo-sensory time series data. Xiuwen Yi, Yu Zheng, Junbo Zhang, Tianrui Li, Proceedings of International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial IntelligenceXiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. ST-MVL: filling missing values in geo-sensory time series data. In Proceedings of International Joint Conference on Artificial Intelligence, pages 2704-2710, 2016.\n\nA tensor-based method for missing traffic data completion. Huachun Tan, Guangdong Feng, Jianshuai Feng, Wuhong Wang, Yu-Jin Zhang, Feng Li, Transportation Research Part C: Emerging Technologies. 28Huachun Tan, Guangdong Feng, Jianshuai Feng, Wuhong Wang, Yu-Jin Zhang, and Feng Li. A tensor-based method for missing traffic data completion. Transportation Research Part C: Emerging Technologies, 28:15-27, 2013.\n\nMissing data: A comparison of neural network and expectation maximization techniques. Shakir Fulufhelo V Nelwamondo, Tshilidzi Mohamed, Marwala, Current Science. Fulufhelo V Nelwamondo, Shakir Mohamed, and Tshilidzi Marwala. Missing data: A com- parison of neural network and expectation maximization techniques. Current Science, pages 1514-1521, 2007.\n\nNearest neighbor imputation of species-level, plot-scale forest structure attributes from lidar data. Remote Sensing of Environment. T Andrew, Nicholas L Hudak, Jeffrey S Crookston, David E Evans, Michael J Hall, Falkowski, 112Andrew T Hudak, Nicholas L Crookston, Jeffrey S Evans, David E Hall, and Michael J Falkowski. Nearest neighbor imputation of species-level, plot-scale forest structure attributes from lidar data. Remote Sensing of Environment, 112(5):2232-2245, 2008.\n\nMICE: Multivariate imputation by chained equations in r. Karin S Van Buuren, Groothuis-Oudshoorn, Journal of statistical software. S van Buuren and Karin Groothuis-Oudshoorn. MICE: Multivariate imputation by chained equations in r. Journal of statistical software, pages 1-68, 2010.\n\nBRITS: Bidirectional recurrent imputation for time series. Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, Yitan Li, Advances in Neural Information Processing Systems. Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. BRITS: Bidirectional recurrent imputation for time series. In Advances in Neural Information Processing Systems, 2018.\n\nRecurrent neural networks for multivariate time series with missing values. Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu, Scientific reports. 81Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12, 2018.\n\nMultivariate time series imputation with generative adversarial networks. Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, Xiaojie Yuan, Advances in Neural Information Processing Systems. Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, and Xiaojie Yuan. Multivariate time series imputation with generative adversarial networks. In Advances in Neural Information Processing Systems, pages 1603-1614, 2018.\n\nGP-VAE: Deep probabilistic time series imputation. Dmitry Vincent Fortuin, Gunnar Baranchuk, Stephan R\u00e4tsch, Mandt, International Conference on Artificial Intelligence and Statistics. Vincent Fortuin, Dmitry Baranchuk, Gunnar R\u00e4tsch, and Stephan Mandt. GP-VAE: Deep probabilistic time series imputation. In International Conference on Artificial Intelligence and Statistics, 2020.\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.\n\nScore-based generative modeling through stochastic differential equations. Yang Song, Jascha Sohl-Dickstein, P Diederik, Abhishek Kingma, Stefano Kumar, Ben Ermon, Poole, International Conference on Learning Representations. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.\n\nDiffWave: A versatile diffusion model for audio synthesis. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro, International Conference on Learning Representations. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.\n\nWaveGrad: Estimating gradients for waveform generation. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, William Chan, International Conference on Learning Representations. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveGrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2021.\n\nSolving linear inverse problems using the prior implicit in a denoiser. Zahra Kadkhodaie, P Eero, Simoncelli, arXiv:2007.13640arXiv preprintZahra Kadkhodaie and Eero P Simoncelli. Solving linear inverse problems using the prior implicit in a denoiser. arXiv preprint arXiv:2007.13640, 2020.\n\nSymbolic music generation with diffusion models. Gautam Mittal, Jesse Engel, Hawthorne Curtis, Ian Simon, arXiv:2103.16091arXiv preprintGautam Mittal, Jesse Engel, Hawthorne Curtis, and Ian Simon. Symbolic music generation with diffusion models. arXiv preprint arXiv:2103.16091, 2021.\n\nEstimating missing data in temporal data streams using multi-directional recurrent neural networks. Jinsung Yoon, Mihaela William R Zame, Van Der Schaar, IEEE Transactions on Biomedical Engineering. 665Jinsung Yoon, William R Zame, and Mihaela van der Schaar. Estimating missing data in temporal data streams using multi-directional recurrent neural networks. IEEE Transactions on Biomedical Engineering, 66(5):1477-1490, 2018.\n\nE2GAN: End-to-end generative adversarial network for multivariate time series imputation. Yonghong Luo, Ying Zhang, Xiangrui Cai, Xiaojie Yuan, Proceedings of International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial IntelligenceYonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. E2GAN: End-to-end generative adversarial network for multivariate time series imputation. In Proceedings of International Joint Conference on Artificial Intelligence, pages 3094-3100, 2019.\n\nGenerative semi-supervised learning for multivariate time series imputation. Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, Jianwei Yin, The Thirty-Fifth AAAI Conference on Artificial Intelligence. Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, and Jianwei Yin. Generative semi-supervised learning for multivariate time series imputation. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021.\n\nRDIS: Random drop imputation with selftraining for incomplete time series data. Tae-Min Choi, Ji-Su Kang, Jong-Hwan Kim, The Thirty-Fifth AAAI Conference on Artificial Intelligence. Tae-Min Choi, Ji-Su Kang, and Jong-Hwan Kim. RDIS: Random drop imputation with self- training for incomplete time series data. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021.\n\nGLIMA: Global and local time series imputation with multi-directional attention learning. Qiuling Suo, Weida Zhong, Guangxu Xun, Jianhui Sun, Changyou Chen, Aidong Zhang, 2020 IEEE International Conference on Big Data (Big Data). IEEEQiuling Suo, Weida Zhong, Guangxu Xun, Jianhui Sun, Changyou Chen, and Aidong Zhang. GLIMA: Global and local time series imputation with multi-directional attention learning. In 2020 IEEE International Conference on Big Data (Big Data), pages 798-807. IEEE, 2020.\n\nMulti-time attention networks for irregularly sampled time series. Satya Narayan Shukla, Benjamin M Marlin, International Conference on Learning Representations. Satya Narayan Shukla and Benjamin M Marlin. Multi-time attention networks for irregularly sampled time series. In International Conference on Learning Representations, 2021.\n\nGenerative modeling by estimating gradients of the data distribution. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, 2019.\n\nPermutation invariant graph generation via score-based generative modeling. Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, Stefano Ermon, International Conference on Artificial Intelligence and Statistics. PMLRChenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, pages 4474-4484. PMLR, 2020.\n\nAutoregressive denoising diffusion models for multivariate probabilistic time series forecasting. Kashif Rasul, Calvin Seward, Ingmar Schuster, Roland Vollgraf, arXiv:2101.12072arXiv preprintKashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denois- ing diffusion models for multivariate probabilistic time series forecasting. arXiv preprint arXiv:2101.12072, 2021.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International Conference on Machine Learning. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper- vised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015.\n\nImproved techniques for training score-based generative models. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems, 2020.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017.\n\nTransformer hawkes process. Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha, International Conference on Machine Learning. Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer hawkes process. In International Conference on Machine Learning, 2020.\n\nMulti-task gaussian process prediction. V Edwin, Kian Bonilla, A Ming, Chai, K I Christopher, Williams, Advances in Neural Information Processing Systems. Edwin V Bonilla, Kian Ming A Chai, and Christopher KI Williams. Multi-task gaussian process prediction. In Advances in Neural Information Processing Systems, 2008.\n\nUncertainty-aware variational-recurrent imputation network for clinical time series. Ahmad Wisnu Mulyadi, Eunji Jun, Heung-Il Suk, IEEE Transactions on Cybernetics. 2021to appearAhmad Wisnu Mulyadi, Eunji Jun, and Heung-Il Suk. Uncertainty-aware variational-recurrent imputation network for clinical time series. IEEE Transactions on Cybernetics, 2021. to appear.\n\nScoring rules for continuous probability distributions. E James, Robert L Matheson, Winkler, Management science. 2210James E Matheson and Robert L Winkler. Scoring rules for continuous probability distributions. Management science, 22(10):1087-1096, 1976.\n\nHigh-dimensional multivariate forecasting with low-rank gaussian copula processes. David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, Jan Gasthaus, Advances in Neural Information Processing Systems. David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. High-dimensional multivariate forecasting with low-rank gaussian copula processes. In Ad- vances in Neural Information Processing Systems, 2019.\n\nLatent ordinary differential equations for irregularly-sampled time series. Yulia Rubanova, T Q Ricky, David Chen, Duvenaud, Advances in Neural Information Processing Systems. Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, 2019.\n\nMulti-variate probabilistic time series forecasting via conditioned normalizing flows. Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, Roland Vollgraf, International Conference on Learning Representations. Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf. Multi-variate probabilistic time series forecasting via conditioned normalizing flows. In Inter- national Conference on Learning Representations, 2021.\n\nTemporal latent auto-encoder: A method for probabilistic multivariate time series forecasting. Nam Nguyen, Brian Quanz, The Thirty-Fifth AAAI Conference on Artificial Intelligence. Nam Nguyen and Brian Quanz. Temporal latent auto-encoder: A method for probabilistic mul- tivariate time series forecasting. In The Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021.\n\nDenoising diffusion implicit models. Jiaming Song, Chenlin Meng, Stefano Ermon, International Conference on Learning Representations. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in Neural Information Processing Systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, 2019.\n\nLinear attention transformer. Phil Wang, Phil Wang. Linear attention transformer. https://github.com/lucidrains/ linear-attention-transformer, 2020.\n\nEfficient attention: Attention with linear complexities. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Hongsheng Li, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3531-3539, 2021.\n\nImproved denoising diffusion probabilistic models. Alex Nichol, Prafulla Dhariwal, arXiv:2102.09672arXiv preprintAlex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021.\n\nGpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. R Jacob, Geoff Gardner, David Pleiss, Bindel, Q Kilian, Andrew Gordon Weinberger, Wilson, Advances in Neural Information Processing Systems. Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances in Neural Information Processing Systems, 2018.\n\nGluonTS: Probabilistic and Neural Time Series Modeling in Python. Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner T\u00fcrkmen, Yuyang Wang, Journal of Machine Learning Research. 21116Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner T\u00fcrkmen, and Yuyang Wang. GluonTS: Probabilistic and Neural Time Series Modeling in Python. Journal of Machine Learning Research, 21(116):1-6, 2020.\n\nModeling long-and short-term temporal patterns with deep neural networks. Guokun Lai, Wei-Cheng Chang, Yiming Yang, Hanxiao Liu, The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 95-104, 2018.\n", "annotations": {"author": "[{\"end\":131,\"start\":91},{\"end\":233,\"start\":132},{\"end\":335,\"start\":234},{\"end\":438,\"start\":336},{\"end\":504,\"start\":439},{\"end\":538,\"start\":505}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":98},{\"end\":144,\"start\":140},{\"end\":243,\"start\":239},{\"end\":349,\"start\":344}]", "author_first_name": "[{\"end\":97,\"start\":91},{\"end\":139,\"start\":132},{\"end\":238,\"start\":234},{\"end\":343,\"start\":336}]", "author_affiliation": "[{\"end\":232,\"start\":168},{\"end\":334,\"start\":270},{\"end\":437,\"start\":373},{\"end\":503,\"start\":440},{\"end\":537,\"start\":506}]", "title": "[{\"end\":88,\"start\":1},{\"end\":626,\"start\":539}]", "venue": null, "abstract": "[{\"end\":1872,\"start\":628}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2120,\"start\":2117},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2123,\"start\":2120},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2126,\"start\":2123},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2298,\"start\":2295},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2301,\"start\":2298},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2304,\"start\":2301},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2440,\"start\":2437},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2443,\"start\":2440},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2446,\"start\":2443},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2480,\"start\":2476},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2834,\"start\":2830},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2837,\"start\":2834},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2862,\"start\":2858},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2865,\"start\":2862},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3111,\"start\":3107},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3114,\"start\":3111},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3117,\"start\":3114},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6009,\"start\":6005},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6011,\"start\":6009},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6013,\"start\":6011},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6118,\"start\":6115},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6121,\"start\":6118},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6124,\"start\":6121},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6147,\"start\":6143},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6290,\"start\":6286},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6293,\"start\":6290},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6363,\"start\":6359},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6544,\"start\":6540},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6594,\"start\":6590},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6701,\"start\":6697},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6704,\"start\":6701},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6716,\"start\":6712},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6719,\"start\":6716},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6736,\"start\":6732},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6766,\"start\":6762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8421,\"start\":8417},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9233,\"start\":9229},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9789,\"start\":9785},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9834,\"start\":9830},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10276,\"start\":10272},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10279,\"start\":10276},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10282,\"start\":10279},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11886,\"start\":11882},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11889,\"start\":11886},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11892,\"start\":11889},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12007,\"start\":12004},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13702,\"start\":13699},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14789,\"start\":14785},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19594,\"start\":19590},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20936,\"start\":20932},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20939,\"start\":20936},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21660,\"start\":21657},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21799,\"start\":21796},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21801,\"start\":21799},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22093,\"start\":22090},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22125,\"start\":22122},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22128,\"start\":22125},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22810,\"start\":22806},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22907,\"start\":22903},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22999,\"start\":22995},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23504,\"start\":23500},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23876,\"start\":23872},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25009,\"start\":25005},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25885,\"start\":25881},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25888,\"start\":25885},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26266,\"start\":26262},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27163,\"start\":27159},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27166,\"start\":27163},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27311,\"start\":27307},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28600,\"start\":28596},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28603,\"start\":28600},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28606,\"start\":28603},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28885,\"start\":28882},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28888,\"start\":28885},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28891,\"start\":28888},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29473,\"start\":29469},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29914,\"start\":29910},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30071,\"start\":30067},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30386,\"start\":30382},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33693,\"start\":33689},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":33962,\"start\":33958},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33965,\"start\":33962},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34056,\"start\":34052},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34243,\"start\":34239},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34424,\"start\":34420},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35553,\"start\":35550},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36398,\"start\":36395},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36849,\"start\":36845},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37241,\"start\":37237},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37244,\"start\":37241},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37592,\"start\":37588},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37851,\"start\":37848},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37974,\"start\":37970},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38075,\"start\":38071},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38256,\"start\":38252},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38748,\"start\":38744},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38751,\"start\":38748},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":39450,\"start\":39446},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39559,\"start\":39555},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40147,\"start\":40143},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40178,\"start\":40174},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40193,\"start\":40189},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40537,\"start\":40533},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":41758,\"start\":41754},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41888,\"start\":41884},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":42025,\"start\":42021},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":42134,\"start\":42130},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42413,\"start\":42409},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":44420,\"start\":44416},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":44423,\"start\":44420},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":47137,\"start\":47133},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47561,\"start\":47557},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":55627,\"start\":55626},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":64949,\"start\":64945}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51364,\"start\":51013},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51815,\"start\":51365},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52186,\"start\":51816},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52328,\"start\":52187},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52774,\"start\":52329},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52936,\"start\":52775},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53895,\"start\":52937},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54473,\"start\":53896},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55105,\"start\":54474},{\"attributes\":{\"id\":\"fig_10\"},\"end\":55459,\"start\":55106},{\"attributes\":{\"id\":\"fig_11\"},\"end\":55601,\"start\":55460},{\"attributes\":{\"id\":\"fig_12\"},\"end\":55809,\"start\":55602},{\"attributes\":{\"id\":\"fig_13\"},\"end\":56254,\"start\":55810},{\"attributes\":{\"id\":\"fig_14\"},\"end\":56698,\"start\":56255},{\"attributes\":{\"id\":\"fig_15\"},\"end\":57145,\"start\":56699},{\"attributes\":{\"id\":\"fig_16\"},\"end\":57579,\"start\":57146},{\"attributes\":{\"id\":\"fig_17\"},\"end\":58070,\"start\":57580},{\"attributes\":{\"id\":\"fig_18\"},\"end\":58560,\"start\":58071},{\"attributes\":{\"id\":\"fig_19\"},\"end\":59051,\"start\":58561},{\"attributes\":{\"id\":\"fig_20\"},\"end\":59529,\"start\":59052},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":59890,\"start\":59530},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":60469,\"start\":59891},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":61151,\"start\":60470},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61586,\"start\":61152},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61749,\"start\":61587},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":62373,\"start\":61750},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":62866,\"start\":62374},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":62958,\"start\":62867},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":63635,\"start\":62959},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64213,\"start\":63636},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":64317,\"start\":64214},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":64836,\"start\":64318}]", "paragraph": "[{\"end\":2572,\"start\":1888},{\"end\":3234,\"start\":2574},{\"end\":4316,\"start\":3236},{\"end\":4888,\"start\":4318},{\"end\":4928,\"start\":4890},{\"end\":5155,\"start\":4930},{\"end\":5503,\"start\":5157},{\"end\":5681,\"start\":5505},{\"end\":6429,\"start\":5699},{\"end\":7004,\"start\":6431},{\"end\":7447,\"start\":7057},{\"end\":7755,\"start\":7551},{\"end\":8116,\"start\":7757},{\"end\":8591,\"start\":8161},{\"end\":8802,\"start\":8685},{\"end\":9084,\"start\":8970},{\"end\":9383,\"start\":9219},{\"end\":9936,\"start\":9509},{\"end\":10375,\"start\":10039},{\"end\":11197,\"start\":10412},{\"end\":11472,\"start\":11199},{\"end\":12455,\"start\":11684},{\"end\":12919,\"start\":12521},{\"end\":13401,\"start\":12944},{\"end\":13988,\"start\":13531},{\"end\":15331,\"start\":14009},{\"end\":15660,\"start\":15392},{\"end\":15931,\"start\":15662},{\"end\":16541,\"start\":15933},{\"end\":16839,\"start\":16543},{\"end\":17120,\"start\":16841},{\"end\":17552,\"start\":17122},{\"end\":17687,\"start\":17554},{\"end\":18877,\"start\":17689},{\"end\":19002,\"start\":18922},{\"end\":19873,\"start\":19004},{\"end\":20428,\"start\":19875},{\"end\":20703,\"start\":20430},{\"end\":21074,\"start\":20705},{\"end\":21493,\"start\":21099},{\"end\":22046,\"start\":21520},{\"end\":22433,\"start\":22048},{\"end\":22713,\"start\":22435},{\"end\":23406,\"start\":22715},{\"end\":24312,\"start\":23408},{\"end\":25264,\"start\":24314},{\"end\":26199,\"start\":25266},{\"end\":26822,\"start\":26211},{\"end\":27235,\"start\":26850},{\"end\":27549,\"start\":27247},{\"end\":28027,\"start\":27551},{\"end\":28267,\"start\":28042},{\"end\":28696,\"start\":28269},{\"end\":29144,\"start\":28698},{\"end\":29278,\"start\":29146},{\"end\":29436,\"start\":29336},{\"end\":29777,\"start\":29438},{\"end\":30117,\"start\":29878},{\"end\":30479,\"start\":30207},{\"end\":30668,\"start\":30572},{\"end\":30913,\"start\":30733},{\"end\":31180,\"start\":30952},{\"end\":31444,\"start\":31242},{\"end\":32092,\"start\":31500},{\"end\":32874,\"start\":32094},{\"end\":33155,\"start\":32876},{\"end\":33357,\"start\":33254},{\"end\":33742,\"start\":33450},{\"end\":34475,\"start\":33744},{\"end\":34930,\"start\":34527},{\"end\":35260,\"start\":34990},{\"end\":36251,\"start\":35262},{\"end\":36489,\"start\":36253},{\"end\":37021,\"start\":36491},{\"end\":37380,\"start\":37023},{\"end\":37740,\"start\":37429},{\"end\":38536,\"start\":37742},{\"end\":38940,\"start\":38599},{\"end\":39335,\"start\":38942},{\"end\":39876,\"start\":39337},{\"end\":40179,\"start\":39950},{\"end\":40796,\"start\":40181},{\"end\":41140,\"start\":40798},{\"end\":41672,\"start\":41142},{\"end\":42269,\"start\":41674},{\"end\":42360,\"start\":42298},{\"end\":42633,\"start\":42362},{\"end\":42910,\"start\":42680},{\"end\":43066,\"start\":42974},{\"end\":43196,\"start\":43112},{\"end\":43363,\"start\":43198},{\"end\":43487,\"start\":43405},{\"end\":43889,\"start\":43587},{\"end\":44090,\"start\":43891},{\"end\":45548,\"start\":44092},{\"end\":45699,\"start\":45550},{\"end\":46024,\"start\":45701},{\"end\":46911,\"start\":46026},{\"end\":47706,\"start\":46971},{\"end\":47953,\"start\":47756},{\"end\":48747,\"start\":47955},{\"end\":49531,\"start\":48788},{\"end\":50726,\"start\":49585},{\"end\":51012,\"start\":50768}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7550,\"start\":7448},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8684,\"start\":8592},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8969,\"start\":8803},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9218,\"start\":9085},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9508,\"start\":9384},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10038,\"start\":9937},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11683,\"start\":11473},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13530,\"start\":13402},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18921,\"start\":18878},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29877,\"start\":29778},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30206,\"start\":30118},{\"attributes\":{\"id\":\"formula_11\"},\"end\":30571,\"start\":30480},{\"attributes\":{\"id\":\"formula_12\"},\"end\":30951,\"start\":30914},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33253,\"start\":33156},{\"attributes\":{\"id\":\"formula_14\"},\"end\":37428,\"start\":37381},{\"attributes\":{\"id\":\"formula_15\"},\"end\":42679,\"start\":42634},{\"attributes\":{\"id\":\"formula_16\"},\"end\":42973,\"start\":42911},{\"attributes\":{\"id\":\"formula_17\"},\"end\":43111,\"start\":43067},{\"attributes\":{\"id\":\"formula_18\"},\"end\":43404,\"start\":43364}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15258,\"start\":15251},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19142,\"start\":19135},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23937,\"start\":23930},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25334,\"start\":25327},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26528,\"start\":26521},{\"end\":26583,\"start\":26576},{\"end\":27561,\"start\":27554},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31833,\"start\":31826},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32321,\"start\":32314},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":39870,\"start\":39699},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40326,\"start\":40319},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40482,\"start\":40475},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41671,\"start\":41664},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":44577,\"start\":44570},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":46099,\"start\":46092},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":47049,\"start\":47042},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":47064,\"start\":47057},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":47446,\"start\":47439},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":47456,\"start\":47448},{\"end\":48194,\"start\":48187},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":48532,\"start\":48524},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48671,\"start\":48664},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":48746,\"start\":48739},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":49034,\"start\":49026},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49530,\"start\":49511}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1886,\"start\":1874},{\"attributes\":{\"n\":\"2\"},\"end\":5697,\"start\":5684},{\"attributes\":{\"n\":\"3\"},\"end\":7017,\"start\":7007},{\"attributes\":{\"n\":\"3.1\"},\"end\":7055,\"start\":7020},{\"attributes\":{\"n\":\"3.2\"},\"end\":8159,\"start\":8119},{\"attributes\":{\"n\":\"3.3\"},\"end\":10410,\"start\":10378},{\"attributes\":{\"n\":\"4\"},\"end\":12519,\"start\":12458},{\"attributes\":{\"n\":\"4.1\"},\"end\":12942,\"start\":12922},{\"attributes\":{\"n\":\"4.2\"},\"end\":14007,\"start\":13991},{\"attributes\":{\"n\":\"4.3\"},\"end\":15390,\"start\":15334},{\"attributes\":{\"n\":\"6\"},\"end\":21097,\"start\":21077},{\"attributes\":{\"n\":\"6.1\"},\"end\":21518,\"start\":21496},{\"end\":26209,\"start\":26202},{\"attributes\":{\"n\":\"6.3\"},\"end\":26848,\"start\":26825},{\"end\":27245,\"start\":27238},{\"attributes\":{\"n\":\"7\"},\"end\":28040,\"start\":28030},{\"end\":29334,\"start\":29281},{\"end\":30731,\"start\":30671},{\"end\":31240,\"start\":31183},{\"end\":31498,\"start\":31447},{\"end\":33448,\"start\":33360},{\"end\":34525,\"start\":34478},{\"end\":34988,\"start\":34933},{\"end\":38597,\"start\":38539},{\"end\":39948,\"start\":39879},{\"end\":42296,\"start\":42272},{\"end\":43526,\"start\":43490},{\"end\":43585,\"start\":43529},{\"end\":46969,\"start\":46914},{\"end\":47754,\"start\":47709},{\"end\":48786,\"start\":48750},{\"end\":49583,\"start\":49534},{\"end\":50766,\"start\":50729},{\"end\":51024,\"start\":51014},{\"end\":51376,\"start\":51366},{\"end\":51827,\"start\":51817},{\"end\":52781,\"start\":52776},{\"end\":53907,\"start\":53897},{\"end\":54485,\"start\":54475},{\"end\":55124,\"start\":55107},{\"end\":55613,\"start\":55603},{\"end\":55821,\"start\":55811},{\"end\":56266,\"start\":56256},{\"end\":56711,\"start\":56700},{\"end\":57158,\"start\":57147},{\"end\":57592,\"start\":57581},{\"end\":58083,\"start\":58072},{\"end\":58573,\"start\":58562},{\"end\":59064,\"start\":59053},{\"end\":59540,\"start\":59531},{\"end\":59901,\"start\":59892},{\"end\":60480,\"start\":60471},{\"end\":61162,\"start\":61153},{\"end\":61597,\"start\":61588},{\"end\":61760,\"start\":61751},{\"end\":62384,\"start\":62375},{\"end\":62886,\"start\":62868},{\"end\":62969,\"start\":62960},{\"end\":63647,\"start\":63637},{\"end\":64225,\"start\":64215},{\"end\":64329,\"start\":64319}]", "table": "[{\"end\":59890,\"start\":59542},{\"end\":60469,\"start\":60051},{\"end\":61151,\"start\":60681},{\"end\":61586,\"start\":61321},{\"end\":61749,\"start\":61651},{\"end\":62373,\"start\":61762},{\"end\":62866,\"start\":62502},{\"end\":63635,\"start\":63191},{\"end\":64213,\"start\":63877},{\"end\":64836,\"start\":64649}]", "figure_caption": "[{\"end\":51364,\"start\":51026},{\"end\":51815,\"start\":51378},{\"end\":52186,\"start\":51829},{\"end\":52328,\"start\":52189},{\"end\":52774,\"start\":52331},{\"end\":52936,\"start\":52783},{\"end\":53895,\"start\":52939},{\"end\":54473,\"start\":53909},{\"end\":55105,\"start\":54487},{\"end\":55459,\"start\":55126},{\"end\":55601,\"start\":55462},{\"end\":55809,\"start\":55615},{\"end\":56254,\"start\":55823},{\"end\":56698,\"start\":56268},{\"end\":57145,\"start\":56714},{\"end\":57579,\"start\":57161},{\"end\":58070,\"start\":57595},{\"end\":58560,\"start\":58086},{\"end\":59051,\"start\":58576},{\"end\":59529,\"start\":59067},{\"end\":60051,\"start\":59903},{\"end\":60681,\"start\":60482},{\"end\":61321,\"start\":61164},{\"end\":61651,\"start\":61599},{\"end\":62502,\"start\":62386},{\"end\":62958,\"start\":62889},{\"end\":63191,\"start\":62971},{\"end\":63877,\"start\":63650},{\"end\":64317,\"start\":64228},{\"end\":64649,\"start\":64332}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3708,\"start\":3700},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14839,\"start\":14831},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17182,\"start\":17174},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18152,\"start\":18144},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18471,\"start\":18463},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20100,\"start\":20092},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24356,\"start\":24348},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30027,\"start\":30024},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32630,\"start\":32622},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32898,\"start\":32890},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33631,\"start\":33623},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33667,\"start\":33659},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":47952,\"start\":47944},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":49853,\"start\":49845},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":49900,\"start\":49892},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50029,\"start\":50020},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50190,\"start\":50181},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50299,\"start\":50290}]", "bib_author_first_name": "[{\"end\":66409,\"start\":66404},{\"end\":66423,\"start\":66417},{\"end\":66432,\"start\":66431},{\"end\":66444,\"start\":66441},{\"end\":66446,\"start\":66445},{\"end\":66461,\"start\":66454},{\"end\":66807,\"start\":66801},{\"end\":66814,\"start\":66812},{\"end\":66827,\"start\":66822},{\"end\":66842,\"start\":66835},{\"end\":67259,\"start\":67252},{\"end\":67274,\"start\":67265},{\"end\":67290,\"start\":67281},{\"end\":67303,\"start\":67297},{\"end\":67316,\"start\":67310},{\"end\":67328,\"start\":67324},{\"end\":67698,\"start\":67692},{\"end\":67732,\"start\":67723},{\"end\":68094,\"start\":68093},{\"end\":68111,\"start\":68103},{\"end\":68113,\"start\":68112},{\"end\":68128,\"start\":68121},{\"end\":68130,\"start\":68129},{\"end\":68147,\"start\":68142},{\"end\":68149,\"start\":68148},{\"end\":68164,\"start\":68157},{\"end\":68166,\"start\":68165},{\"end\":68501,\"start\":68496},{\"end\":68785,\"start\":68782},{\"end\":68795,\"start\":68791},{\"end\":68806,\"start\":68802},{\"end\":68814,\"start\":68811},{\"end\":68824,\"start\":68821},{\"end\":68834,\"start\":68829},{\"end\":69156,\"start\":69147},{\"end\":69168,\"start\":69162},{\"end\":69191,\"start\":69182},{\"end\":69202,\"start\":69197},{\"end\":69214,\"start\":69211},{\"end\":69515,\"start\":69507},{\"end\":69529,\"start\":69521},{\"end\":69539,\"start\":69535},{\"end\":69550,\"start\":69547},{\"end\":69562,\"start\":69555},{\"end\":69895,\"start\":69889},{\"end\":69919,\"start\":69913},{\"end\":69938,\"start\":69931},{\"end\":70270,\"start\":70262},{\"end\":70279,\"start\":70275},{\"end\":70292,\"start\":70286},{\"end\":70577,\"start\":70573},{\"end\":70590,\"start\":70584},{\"end\":70608,\"start\":70607},{\"end\":70627,\"start\":70619},{\"end\":70643,\"start\":70636},{\"end\":70654,\"start\":70651},{\"end\":71027,\"start\":71020},{\"end\":71037,\"start\":71034},{\"end\":71049,\"start\":71044},{\"end\":71062,\"start\":71057},{\"end\":71074,\"start\":71069},{\"end\":71395,\"start\":71389},{\"end\":71404,\"start\":71402},{\"end\":71417,\"start\":71412},{\"end\":71426,\"start\":71423},{\"end\":71428,\"start\":71427},{\"end\":71444,\"start\":71436},{\"end\":71461,\"start\":71454},{\"end\":71802,\"start\":71797},{\"end\":71816,\"start\":71815},{\"end\":72072,\"start\":72066},{\"end\":72086,\"start\":72081},{\"end\":72103,\"start\":72094},{\"end\":72115,\"start\":72112},{\"end\":72410,\"start\":72403},{\"end\":72424,\"start\":72417},{\"end\":72830,\"start\":72822},{\"end\":72840,\"start\":72836},{\"end\":72856,\"start\":72848},{\"end\":72869,\"start\":72862},{\"end\":73339,\"start\":73333},{\"end\":73354,\"start\":73346},{\"end\":73362,\"start\":73359},{\"end\":73375,\"start\":73369},{\"end\":73387,\"start\":73381},{\"end\":73400,\"start\":73393},{\"end\":73779,\"start\":73772},{\"end\":73791,\"start\":73786},{\"end\":73807,\"start\":73798},{\"end\":74169,\"start\":74162},{\"end\":74180,\"start\":74175},{\"end\":74195,\"start\":74188},{\"end\":74208,\"start\":74201},{\"end\":74222,\"start\":74214},{\"end\":74235,\"start\":74229},{\"end\":74982,\"start\":74978},{\"end\":74996,\"start\":74989},{\"end\":75298,\"start\":75291},{\"end\":75308,\"start\":75304},{\"end\":75322,\"start\":75315},{\"end\":75337,\"start\":75329},{\"end\":75350,\"start\":75344},{\"end\":75366,\"start\":75359},{\"end\":75814,\"start\":75808},{\"end\":75828,\"start\":75822},{\"end\":75843,\"start\":75837},{\"end\":75860,\"start\":75854},{\"end\":76178,\"start\":76172},{\"end\":76199,\"start\":76195},{\"end\":76211,\"start\":76207},{\"end\":76234,\"start\":76229},{\"end\":76556,\"start\":76552},{\"end\":76570,\"start\":76563},{\"end\":76870,\"start\":76865},{\"end\":76887,\"start\":76879},{\"end\":76901,\"start\":76895},{\"end\":76915,\"start\":76907},{\"end\":77585,\"start\":77579},{\"end\":77599,\"start\":77595},{\"end\":77613,\"start\":77609},{\"end\":77627,\"start\":77622},{\"end\":77644,\"start\":77639},{\"end\":77657,\"start\":77652},{\"end\":77659,\"start\":77658},{\"end\":77673,\"start\":77667},{\"end\":77687,\"start\":77682},{\"end\":78000,\"start\":77994},{\"end\":78013,\"start\":78006},{\"end\":78028,\"start\":78021},{\"end\":78036,\"start\":78033},{\"end\":78051,\"start\":78043},{\"end\":78295,\"start\":78294},{\"end\":78307,\"start\":78303},{\"end\":78318,\"start\":78317},{\"end\":78332,\"start\":78331},{\"end\":78334,\"start\":78333},{\"end\":78664,\"start\":78659},{\"end\":78685,\"start\":78680},{\"end\":78699,\"start\":78691},{\"end\":78996,\"start\":78995},{\"end\":79012,\"start\":79004},{\"end\":79284,\"start\":79279},{\"end\":79301,\"start\":79294},{\"end\":79327,\"start\":79320},{\"end\":79343,\"start\":79336},{\"end\":79355,\"start\":79352},{\"end\":79735,\"start\":79730},{\"end\":79747,\"start\":79746},{\"end\":79749,\"start\":79748},{\"end\":79762,\"start\":79757},{\"end\":80111,\"start\":80105},{\"end\":80131,\"start\":80119},{\"end\":80146,\"start\":80140},{\"end\":80160,\"start\":80157},{\"end\":80177,\"start\":80171},{\"end\":80580,\"start\":80577},{\"end\":80594,\"start\":80589},{\"end\":80903,\"start\":80896},{\"end\":80917,\"start\":80910},{\"end\":80931,\"start\":80924},{\"end\":81215,\"start\":81211},{\"end\":81227,\"start\":81224},{\"end\":81244,\"start\":81235},{\"end\":81256,\"start\":81252},{\"end\":81269,\"start\":81264},{\"end\":81287,\"start\":81280},{\"end\":81302,\"start\":81296},{\"end\":81318,\"start\":81312},{\"end\":81331,\"start\":81324},{\"end\":81348,\"start\":81344},{\"end\":81726,\"start\":81722},{\"end\":81906,\"start\":81899},{\"end\":81921,\"start\":81913},{\"end\":81934,\"start\":81929},{\"end\":81946,\"start\":81941},{\"end\":81960,\"start\":81951},{\"end\":82403,\"start\":82399},{\"end\":82420,\"start\":82412},{\"end\":82671,\"start\":82670},{\"end\":82684,\"start\":82679},{\"end\":82699,\"start\":82694},{\"end\":82717,\"start\":82716},{\"end\":82732,\"start\":82726},{\"end\":82739,\"start\":82733},{\"end\":83122,\"start\":83113},{\"end\":83147,\"start\":83135},{\"end\":83164,\"start\":83157},{\"end\":83191,\"start\":83183},{\"end\":83205,\"start\":83202},{\"end\":83219,\"start\":83216},{\"end\":83242,\"start\":83234},{\"end\":83244,\"start\":83243},{\"end\":83258,\"start\":83253},{\"end\":83276,\"start\":83271},{\"end\":83292,\"start\":83286},{\"end\":83308,\"start\":83301},{\"end\":83320,\"start\":83317},{\"end\":83342,\"start\":83336},{\"end\":83837,\"start\":83831},{\"end\":83852,\"start\":83843},{\"end\":83866,\"start\":83860},{\"end\":83880,\"start\":83873}]", "bib_author_last_name": "[{\"end\":66415,\"start\":66410},{\"end\":66429,\"start\":66424},{\"end\":66439,\"start\":66433},{\"end\":66452,\"start\":66447},{\"end\":66466,\"start\":66462},{\"end\":66472,\"start\":66468},{\"end\":66810,\"start\":66808},{\"end\":66820,\"start\":66815},{\"end\":66833,\"start\":66828},{\"end\":66845,\"start\":66843},{\"end\":67263,\"start\":67260},{\"end\":67279,\"start\":67275},{\"end\":67295,\"start\":67291},{\"end\":67308,\"start\":67304},{\"end\":67322,\"start\":67317},{\"end\":67331,\"start\":67329},{\"end\":67721,\"start\":67699},{\"end\":67740,\"start\":67733},{\"end\":67749,\"start\":67742},{\"end\":68101,\"start\":68095},{\"end\":68119,\"start\":68114},{\"end\":68140,\"start\":68131},{\"end\":68155,\"start\":68150},{\"end\":68171,\"start\":68167},{\"end\":68182,\"start\":68173},{\"end\":68514,\"start\":68502},{\"end\":68535,\"start\":68516},{\"end\":68789,\"start\":68786},{\"end\":68800,\"start\":68796},{\"end\":68809,\"start\":68807},{\"end\":68819,\"start\":68815},{\"end\":68827,\"start\":68825},{\"end\":68837,\"start\":68835},{\"end\":69160,\"start\":69157},{\"end\":69180,\"start\":69169},{\"end\":69195,\"start\":69192},{\"end\":69209,\"start\":69203},{\"end\":69218,\"start\":69215},{\"end\":69519,\"start\":69516},{\"end\":69533,\"start\":69530},{\"end\":69545,\"start\":69540},{\"end\":69553,\"start\":69551},{\"end\":69567,\"start\":69563},{\"end\":69911,\"start\":69896},{\"end\":69929,\"start\":69920},{\"end\":69945,\"start\":69939},{\"end\":69952,\"start\":69947},{\"end\":70273,\"start\":70271},{\"end\":70284,\"start\":70280},{\"end\":70299,\"start\":70293},{\"end\":70582,\"start\":70578},{\"end\":70605,\"start\":70591},{\"end\":70617,\"start\":70609},{\"end\":70634,\"start\":70628},{\"end\":70649,\"start\":70644},{\"end\":70660,\"start\":70655},{\"end\":70667,\"start\":70662},{\"end\":71032,\"start\":71028},{\"end\":71042,\"start\":71038},{\"end\":71055,\"start\":71050},{\"end\":71067,\"start\":71063},{\"end\":71084,\"start\":71075},{\"end\":71400,\"start\":71396},{\"end\":71410,\"start\":71405},{\"end\":71421,\"start\":71418},{\"end\":71434,\"start\":71429},{\"end\":71452,\"start\":71445},{\"end\":71466,\"start\":71462},{\"end\":71813,\"start\":71803},{\"end\":71821,\"start\":71817},{\"end\":71833,\"start\":71823},{\"end\":72079,\"start\":72073},{\"end\":72092,\"start\":72087},{\"end\":72110,\"start\":72104},{\"end\":72121,\"start\":72116},{\"end\":72415,\"start\":72411},{\"end\":72439,\"start\":72425},{\"end\":72455,\"start\":72441},{\"end\":72834,\"start\":72831},{\"end\":72846,\"start\":72841},{\"end\":72860,\"start\":72857},{\"end\":72874,\"start\":72870},{\"end\":73344,\"start\":73340},{\"end\":73357,\"start\":73355},{\"end\":73367,\"start\":73363},{\"end\":73379,\"start\":73376},{\"end\":73391,\"start\":73388},{\"end\":73404,\"start\":73401},{\"end\":73784,\"start\":73780},{\"end\":73796,\"start\":73792},{\"end\":73811,\"start\":73808},{\"end\":74173,\"start\":74170},{\"end\":74186,\"start\":74181},{\"end\":74199,\"start\":74196},{\"end\":74212,\"start\":74209},{\"end\":74227,\"start\":74223},{\"end\":74241,\"start\":74236},{\"end\":74658,\"start\":74638},{\"end\":74677,\"start\":74660},{\"end\":74987,\"start\":74983},{\"end\":75002,\"start\":74997},{\"end\":75302,\"start\":75299},{\"end\":75313,\"start\":75309},{\"end\":75327,\"start\":75323},{\"end\":75342,\"start\":75338},{\"end\":75357,\"start\":75351},{\"end\":75372,\"start\":75367},{\"end\":75820,\"start\":75815},{\"end\":75835,\"start\":75829},{\"end\":75852,\"start\":75844},{\"end\":75869,\"start\":75861},{\"end\":76193,\"start\":76179},{\"end\":76205,\"start\":76200},{\"end\":76227,\"start\":76212},{\"end\":76242,\"start\":76235},{\"end\":76561,\"start\":76557},{\"end\":76576,\"start\":76571},{\"end\":76877,\"start\":76871},{\"end\":76893,\"start\":76888},{\"end\":76905,\"start\":76902},{\"end\":76925,\"start\":76916},{\"end\":77593,\"start\":77586},{\"end\":77607,\"start\":77600},{\"end\":77620,\"start\":77614},{\"end\":77637,\"start\":77628},{\"end\":77650,\"start\":77645},{\"end\":77665,\"start\":77660},{\"end\":77680,\"start\":77674},{\"end\":77698,\"start\":77688},{\"end\":78004,\"start\":78001},{\"end\":78019,\"start\":78014},{\"end\":78031,\"start\":78029},{\"end\":78041,\"start\":78037},{\"end\":78055,\"start\":78052},{\"end\":78301,\"start\":78296},{\"end\":78315,\"start\":78308},{\"end\":78323,\"start\":78319},{\"end\":78329,\"start\":78325},{\"end\":78346,\"start\":78335},{\"end\":78356,\"start\":78348},{\"end\":78678,\"start\":78665},{\"end\":78689,\"start\":78686},{\"end\":78703,\"start\":78700},{\"end\":79002,\"start\":78997},{\"end\":79021,\"start\":79013},{\"end\":79030,\"start\":79023},{\"end\":79292,\"start\":79285},{\"end\":79318,\"start\":79302},{\"end\":79334,\"start\":79328},{\"end\":79350,\"start\":79344},{\"end\":79364,\"start\":79356},{\"end\":79744,\"start\":79736},{\"end\":79755,\"start\":79750},{\"end\":79767,\"start\":79763},{\"end\":79777,\"start\":79769},{\"end\":80117,\"start\":80112},{\"end\":80138,\"start\":80132},{\"end\":80155,\"start\":80147},{\"end\":80169,\"start\":80161},{\"end\":80186,\"start\":80178},{\"end\":80587,\"start\":80581},{\"end\":80600,\"start\":80595},{\"end\":80908,\"start\":80904},{\"end\":80922,\"start\":80918},{\"end\":80937,\"start\":80932},{\"end\":81222,\"start\":81216},{\"end\":81233,\"start\":81228},{\"end\":81250,\"start\":81245},{\"end\":81262,\"start\":81257},{\"end\":81278,\"start\":81270},{\"end\":81294,\"start\":81288},{\"end\":81310,\"start\":81303},{\"end\":81322,\"start\":81319},{\"end\":81342,\"start\":81332},{\"end\":81355,\"start\":81349},{\"end\":81731,\"start\":81727},{\"end\":81911,\"start\":81907},{\"end\":81927,\"start\":81922},{\"end\":81939,\"start\":81935},{\"end\":81949,\"start\":81947},{\"end\":81963,\"start\":81961},{\"end\":82410,\"start\":82404},{\"end\":82429,\"start\":82421},{\"end\":82677,\"start\":82672},{\"end\":82692,\"start\":82685},{\"end\":82706,\"start\":82700},{\"end\":82714,\"start\":82708},{\"end\":82724,\"start\":82718},{\"end\":82750,\"start\":82740},{\"end\":82758,\"start\":82752},{\"end\":83133,\"start\":83123},{\"end\":83155,\"start\":83148},{\"end\":83181,\"start\":83165},{\"end\":83200,\"start\":83192},{\"end\":83214,\"start\":83206},{\"end\":83232,\"start\":83220},{\"end\":83251,\"start\":83245},{\"end\":83269,\"start\":83259},{\"end\":83284,\"start\":83277},{\"end\":83299,\"start\":83293},{\"end\":83315,\"start\":83309},{\"end\":83334,\"start\":83321},{\"end\":83347,\"start\":83343},{\"end\":83841,\"start\":83838},{\"end\":83858,\"start\":83853},{\"end\":83871,\"start\":83867},{\"end\":83884,\"start\":83881}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8678934},\"end\":66735,\"start\":66301},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9465743},\"end\":67191,\"start\":66737},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":197404444},\"end\":67604,\"start\":67193},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":83358528},\"end\":67958,\"start\":67606},{\"attributes\":{\"id\":\"b4\"},\"end\":68437,\"start\":67960},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16120223},\"end\":68721,\"start\":68439},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":44119917},\"end\":69069,\"start\":68723},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4900015},\"end\":69431,\"start\":69071},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":54024655},\"end\":69836,\"start\":69433},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209955774},\"end\":70218,\"start\":69838},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":219955663},\"end\":70496,\"start\":70220},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":227209335},\"end\":70959,\"start\":70498},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":221818900},\"end\":71331,\"start\":70961},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":221447287},\"end\":71723,\"start\":71333},{\"attributes\":{\"doi\":\"arXiv:2007.13640\",\"id\":\"b14\"},\"end\":72015,\"start\":71725},{\"attributes\":{\"doi\":\"arXiv:2103.16091\",\"id\":\"b15\"},\"end\":72301,\"start\":72017},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":24870358},\"end\":72730,\"start\":72303},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":199466115},\"end\":73254,\"start\":72732},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":235349067},\"end\":73690,\"start\":73256},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":224803084},\"end\":74070,\"start\":73692},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":232374310},\"end\":74569,\"start\":74072},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":221508448},\"end\":74906,\"start\":74571},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":196470871},\"end\":75213,\"start\":74908},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":211677799},\"end\":75708,\"start\":75215},{\"attributes\":{\"doi\":\"arXiv:2101.12072\",\"id\":\"b24\"},\"end\":76106,\"start\":75710},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14888175},\"end\":76486,\"start\":76108},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":219708245},\"end\":76781,\"start\":76488},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52967399},\"end\":77550,\"start\":76783},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":77964,\"start\":77552},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":211252840},\"end\":78252,\"start\":77966},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10790217},\"end\":78572,\"start\":78254},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":211677631},\"end\":78937,\"start\":78574},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":119590882},\"end\":79194,\"start\":78939},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202766654},\"end\":79652,\"start\":79196},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":202784022},\"end\":80016,\"start\":79654},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":211126472},\"end\":80480,\"start\":80018},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":231709435},\"end\":80857,\"start\":80482},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":222140788},\"end\":81139,\"start\":80859},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":202786778},\"end\":81690,\"start\":81141},{\"attributes\":{\"id\":\"b39\"},\"end\":81840,\"start\":81692},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":215999966},\"end\":82346,\"start\":81842},{\"attributes\":{\"doi\":\"arXiv:2102.09672\",\"id\":\"b41\"},\"end\":82585,\"start\":82348},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52890394},\"end\":83045,\"start\":82587},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":226717462},\"end\":83755,\"start\":83047},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4922476},\"end\":84233,\"start\":83757}]", "bib_title": "[{\"end\":66402,\"start\":66301},{\"end\":66799,\"start\":66737},{\"end\":67250,\"start\":67193},{\"end\":67690,\"start\":67606},{\"end\":68494,\"start\":68439},{\"end\":68780,\"start\":68723},{\"end\":69145,\"start\":69071},{\"end\":69505,\"start\":69433},{\"end\":69887,\"start\":69838},{\"end\":70260,\"start\":70220},{\"end\":70571,\"start\":70498},{\"end\":71018,\"start\":70961},{\"end\":71387,\"start\":71333},{\"end\":72401,\"start\":72303},{\"end\":72820,\"start\":72732},{\"end\":73331,\"start\":73256},{\"end\":73770,\"start\":73692},{\"end\":74160,\"start\":74072},{\"end\":74636,\"start\":74571},{\"end\":74976,\"start\":74908},{\"end\":75289,\"start\":75215},{\"end\":76170,\"start\":76108},{\"end\":76550,\"start\":76488},{\"end\":76863,\"start\":76783},{\"end\":77577,\"start\":77552},{\"end\":77992,\"start\":77966},{\"end\":78292,\"start\":78254},{\"end\":78657,\"start\":78574},{\"end\":78993,\"start\":78939},{\"end\":79277,\"start\":79196},{\"end\":79728,\"start\":79654},{\"end\":80103,\"start\":80018},{\"end\":80575,\"start\":80482},{\"end\":80894,\"start\":80859},{\"end\":81209,\"start\":81141},{\"end\":81897,\"start\":81842},{\"end\":82668,\"start\":82587},{\"end\":83111,\"start\":83047},{\"end\":83829,\"start\":83757}]", "bib_author": "[{\"end\":66417,\"start\":66404},{\"end\":66431,\"start\":66417},{\"end\":66441,\"start\":66431},{\"end\":66454,\"start\":66441},{\"end\":66468,\"start\":66454},{\"end\":66474,\"start\":66468},{\"end\":66812,\"start\":66801},{\"end\":66822,\"start\":66812},{\"end\":66835,\"start\":66822},{\"end\":66847,\"start\":66835},{\"end\":67265,\"start\":67252},{\"end\":67281,\"start\":67265},{\"end\":67297,\"start\":67281},{\"end\":67310,\"start\":67297},{\"end\":67324,\"start\":67310},{\"end\":67333,\"start\":67324},{\"end\":67723,\"start\":67692},{\"end\":67742,\"start\":67723},{\"end\":67751,\"start\":67742},{\"end\":68103,\"start\":68093},{\"end\":68121,\"start\":68103},{\"end\":68142,\"start\":68121},{\"end\":68157,\"start\":68142},{\"end\":68173,\"start\":68157},{\"end\":68184,\"start\":68173},{\"end\":68516,\"start\":68496},{\"end\":68537,\"start\":68516},{\"end\":68791,\"start\":68782},{\"end\":68802,\"start\":68791},{\"end\":68811,\"start\":68802},{\"end\":68821,\"start\":68811},{\"end\":68829,\"start\":68821},{\"end\":68839,\"start\":68829},{\"end\":69162,\"start\":69147},{\"end\":69182,\"start\":69162},{\"end\":69197,\"start\":69182},{\"end\":69211,\"start\":69197},{\"end\":69220,\"start\":69211},{\"end\":69521,\"start\":69507},{\"end\":69535,\"start\":69521},{\"end\":69547,\"start\":69535},{\"end\":69555,\"start\":69547},{\"end\":69569,\"start\":69555},{\"end\":69913,\"start\":69889},{\"end\":69931,\"start\":69913},{\"end\":69947,\"start\":69931},{\"end\":69954,\"start\":69947},{\"end\":70275,\"start\":70262},{\"end\":70286,\"start\":70275},{\"end\":70301,\"start\":70286},{\"end\":70584,\"start\":70573},{\"end\":70607,\"start\":70584},{\"end\":70619,\"start\":70607},{\"end\":70636,\"start\":70619},{\"end\":70651,\"start\":70636},{\"end\":70662,\"start\":70651},{\"end\":70669,\"start\":70662},{\"end\":71034,\"start\":71020},{\"end\":71044,\"start\":71034},{\"end\":71057,\"start\":71044},{\"end\":71069,\"start\":71057},{\"end\":71086,\"start\":71069},{\"end\":71402,\"start\":71389},{\"end\":71412,\"start\":71402},{\"end\":71423,\"start\":71412},{\"end\":71436,\"start\":71423},{\"end\":71454,\"start\":71436},{\"end\":71468,\"start\":71454},{\"end\":71815,\"start\":71797},{\"end\":71823,\"start\":71815},{\"end\":71835,\"start\":71823},{\"end\":72081,\"start\":72066},{\"end\":72094,\"start\":72081},{\"end\":72112,\"start\":72094},{\"end\":72123,\"start\":72112},{\"end\":72417,\"start\":72403},{\"end\":72441,\"start\":72417},{\"end\":72457,\"start\":72441},{\"end\":72836,\"start\":72822},{\"end\":72848,\"start\":72836},{\"end\":72862,\"start\":72848},{\"end\":72876,\"start\":72862},{\"end\":73346,\"start\":73333},{\"end\":73359,\"start\":73346},{\"end\":73369,\"start\":73359},{\"end\":73381,\"start\":73369},{\"end\":73393,\"start\":73381},{\"end\":73406,\"start\":73393},{\"end\":73786,\"start\":73772},{\"end\":73798,\"start\":73786},{\"end\":73813,\"start\":73798},{\"end\":74175,\"start\":74162},{\"end\":74188,\"start\":74175},{\"end\":74201,\"start\":74188},{\"end\":74214,\"start\":74201},{\"end\":74229,\"start\":74214},{\"end\":74243,\"start\":74229},{\"end\":74660,\"start\":74638},{\"end\":74679,\"start\":74660},{\"end\":74989,\"start\":74978},{\"end\":75004,\"start\":74989},{\"end\":75304,\"start\":75291},{\"end\":75315,\"start\":75304},{\"end\":75329,\"start\":75315},{\"end\":75344,\"start\":75329},{\"end\":75359,\"start\":75344},{\"end\":75374,\"start\":75359},{\"end\":75822,\"start\":75808},{\"end\":75837,\"start\":75822},{\"end\":75854,\"start\":75837},{\"end\":75871,\"start\":75854},{\"end\":76195,\"start\":76172},{\"end\":76207,\"start\":76195},{\"end\":76229,\"start\":76207},{\"end\":76244,\"start\":76229},{\"end\":76563,\"start\":76552},{\"end\":76578,\"start\":76563},{\"end\":76879,\"start\":76865},{\"end\":76895,\"start\":76879},{\"end\":76907,\"start\":76895},{\"end\":76927,\"start\":76907},{\"end\":77595,\"start\":77579},{\"end\":77609,\"start\":77595},{\"end\":77622,\"start\":77609},{\"end\":77639,\"start\":77622},{\"end\":77652,\"start\":77639},{\"end\":77667,\"start\":77652},{\"end\":77682,\"start\":77667},{\"end\":77700,\"start\":77682},{\"end\":78006,\"start\":77994},{\"end\":78021,\"start\":78006},{\"end\":78033,\"start\":78021},{\"end\":78043,\"start\":78033},{\"end\":78057,\"start\":78043},{\"end\":78303,\"start\":78294},{\"end\":78317,\"start\":78303},{\"end\":78325,\"start\":78317},{\"end\":78331,\"start\":78325},{\"end\":78348,\"start\":78331},{\"end\":78358,\"start\":78348},{\"end\":78680,\"start\":78659},{\"end\":78691,\"start\":78680},{\"end\":78705,\"start\":78691},{\"end\":79004,\"start\":78995},{\"end\":79023,\"start\":79004},{\"end\":79032,\"start\":79023},{\"end\":79294,\"start\":79279},{\"end\":79320,\"start\":79294},{\"end\":79336,\"start\":79320},{\"end\":79352,\"start\":79336},{\"end\":79366,\"start\":79352},{\"end\":79746,\"start\":79730},{\"end\":79757,\"start\":79746},{\"end\":79769,\"start\":79757},{\"end\":79779,\"start\":79769},{\"end\":80119,\"start\":80105},{\"end\":80140,\"start\":80119},{\"end\":80157,\"start\":80140},{\"end\":80171,\"start\":80157},{\"end\":80188,\"start\":80171},{\"end\":80589,\"start\":80577},{\"end\":80602,\"start\":80589},{\"end\":80910,\"start\":80896},{\"end\":80924,\"start\":80910},{\"end\":80939,\"start\":80924},{\"end\":81224,\"start\":81211},{\"end\":81235,\"start\":81224},{\"end\":81252,\"start\":81235},{\"end\":81264,\"start\":81252},{\"end\":81280,\"start\":81264},{\"end\":81296,\"start\":81280},{\"end\":81312,\"start\":81296},{\"end\":81324,\"start\":81312},{\"end\":81344,\"start\":81324},{\"end\":81357,\"start\":81344},{\"end\":81733,\"start\":81722},{\"end\":81913,\"start\":81899},{\"end\":81929,\"start\":81913},{\"end\":81941,\"start\":81929},{\"end\":81951,\"start\":81941},{\"end\":81965,\"start\":81951},{\"end\":82412,\"start\":82399},{\"end\":82431,\"start\":82412},{\"end\":82679,\"start\":82670},{\"end\":82694,\"start\":82679},{\"end\":82708,\"start\":82694},{\"end\":82716,\"start\":82708},{\"end\":82726,\"start\":82716},{\"end\":82752,\"start\":82726},{\"end\":82760,\"start\":82752},{\"end\":83135,\"start\":83113},{\"end\":83157,\"start\":83135},{\"end\":83183,\"start\":83157},{\"end\":83202,\"start\":83183},{\"end\":83216,\"start\":83202},{\"end\":83234,\"start\":83216},{\"end\":83253,\"start\":83234},{\"end\":83271,\"start\":83253},{\"end\":83286,\"start\":83271},{\"end\":83301,\"start\":83286},{\"end\":83317,\"start\":83301},{\"end\":83336,\"start\":83317},{\"end\":83349,\"start\":83336},{\"end\":83843,\"start\":83831},{\"end\":83860,\"start\":83843},{\"end\":83873,\"start\":83860},{\"end\":83886,\"start\":83873}]", "bib_venue": "[{\"end\":66978,\"start\":66921},{\"end\":73007,\"start\":72950},{\"end\":77198,\"start\":77071},{\"end\":82112,\"start\":82047},{\"end\":66497,\"start\":66474},{\"end\":66919,\"start\":66847},{\"end\":67386,\"start\":67333},{\"end\":67766,\"start\":67751},{\"end\":68091,\"start\":67960},{\"end\":68568,\"start\":68537},{\"end\":68888,\"start\":68839},{\"end\":69238,\"start\":69220},{\"end\":69618,\"start\":69569},{\"end\":70020,\"start\":69954},{\"end\":70350,\"start\":70301},{\"end\":70721,\"start\":70669},{\"end\":71138,\"start\":71086},{\"end\":71520,\"start\":71468},{\"end\":71795,\"start\":71725},{\"end\":72064,\"start\":72017},{\"end\":72500,\"start\":72457},{\"end\":72948,\"start\":72876},{\"end\":73465,\"start\":73406},{\"end\":73872,\"start\":73813},{\"end\":74300,\"start\":74243},{\"end\":74731,\"start\":74679},{\"end\":75053,\"start\":75004},{\"end\":75440,\"start\":75374},{\"end\":75806,\"start\":75710},{\"end\":76288,\"start\":76244},{\"end\":76627,\"start\":76578},{\"end\":77069,\"start\":76927},{\"end\":77749,\"start\":77700},{\"end\":78101,\"start\":78057},{\"end\":78407,\"start\":78358},{\"end\":78737,\"start\":78705},{\"end\":79050,\"start\":79032},{\"end\":79415,\"start\":79366},{\"end\":79828,\"start\":79779},{\"end\":80240,\"start\":80188},{\"end\":80661,\"start\":80602},{\"end\":80991,\"start\":80939},{\"end\":81406,\"start\":81357},{\"end\":81720,\"start\":81692},{\"end\":82045,\"start\":81965},{\"end\":82397,\"start\":82348},{\"end\":82809,\"start\":82760},{\"end\":83385,\"start\":83349},{\"end\":83980,\"start\":83886}]"}}}, "year": 2023, "month": 12, "day": 17}