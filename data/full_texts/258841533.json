{"id": 258841533, "updated": "2023-10-05 09:03:21.913", "metadata": {"title": "Exploring Train and Test-Time Augmentations for Audio-Language Learning", "authors": "[{\"first\":\"Eungbeom\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jinhee\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Yoori\",\"last\":\"Oh\",\"middle\":[]},{\"first\":\"Kyungsu\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Minju\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Jaeheon\",\"last\":\"Sim\",\"middle\":[]},{\"first\":\"Jinwoo\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Kyogu\",\"last\":\"Lee\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this paper, we aim to unveil the impact of data augmentation in audio-language multi-modal learning, which has not been explored despite its importance. We explore various augmentation methods at not only train-time but also test-time and find out that proper data augmentation can lead to substantial improvements. Specifically, applying our proposed audio-language paired augmentation PairMix, which is the first multi-modal audio-language augmentation method, outperforms the baselines for both automated audio captioning and audio-text retrieval tasks. To fully take advantage of data augmentation, we also present multi-level test-time augmentation (Multi-TTA) for the test-time. We successfully incorporate the two proposed methods and uni-modal augmentations and achieve 47.5 SPIDEr on audio captioning, which is an 18.2% relative increase over the baseline. In audio-text retrieval, the proposed methods also show an improvement in performance as well.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2210.17143", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "28f22b296d81f6db3cd537dfe835c329207eb4e1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.17143v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "edbfb7a9a621a9b301b92eda0cbcc4c0c7be9bb3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/28f22b296d81f6db3cd537dfe835c329207eb4e1.txt", "contents": "\nExploring Train and Test-Time Augmentations for Audio-Language Learning\n\n\nEungbeom Kim eb.kim@snu.ac.kr \nInterdisciplinary Program in Artificial Intelligence\n\n\nJinhee Kim \nDepartment of Intelligence and Information\n\n\nYoori Oh \nDepartment of Intelligence and Information\n\n\nKyungsu Kim \nMinju Park \nDepartment of Intelligence and Information\n\n\nJaeheon Sim \nInterdisciplinary Program in Artificial Intelligence\n\n\nDepartment of Intelligence and Information\n\n\nJinwoo Lee \nDepartment of Intelligence and Information\n\n\nKyogu Lee \nInterdisciplinary Program in Artificial Intelligence\n\n\nDepartment of Intelligence and Information\n\n\nArtificial Intelligence Institute\nSeoul National University\nRepublic of Korea\n\nExploring Train and Test-Time Augmentations for Audio-Language Learning\nIndex Terms: Data augmentationAudio-language learningAutomated audio captioningAudio-text retrieval\nIn this paper, we aim to unveil the impact of data augmentation in audio-language multi-modal learning, which has not been explored despite its importance. We explore various augmentation methods at not only train-time but also test-time and find out that proper data augmentation can lead to substantial improvements. Specifically, applying our proposed audiolanguage paired augmentation PairMix, which is the first multimodal audio-language augmentation method, outperforms the baselines for both automated audio captioning and audio-text retrieval tasks. To fully take advantage of data augmentation, we also present multi-level test-time augmentation (Multi-TTA) for the test-time. We successfully incorporate the two proposed methods and uni-modal augmentations and achieve 47.5 SPI-DEr on audio captioning, which is an 18.2% relative increase over the baseline. In audio-text retrieval, the proposed methods also show an improvement in performance as well.\n\nIntroduction\n\nData augmentation has shown meaningful performance gain in various domains. Multi-modal learning is one of the areas where data augmentation is prospective and various augmentation strategies are applicable. We can apply different augmentations on each modality such as audio augmentations [1,2] and text augmentations [3,4]. We can also augment multimodalities jointly. MixGen [5], for instance, is a simple way to generate image-text pairs by applying mixup [6] on images and concatenating each corresponding text.\n\nHowever, data augmentation on audio-language learning remains poorly understood. Even worse, AudioCaps [7], the largest public audio-language dataset, only includes about 40K audio clips and 46K captions in opposition to the MS COCO Captions [8] which is a widely known vision-language dataset containing 330K images and 1.5M captions. For this reason, we delve into data augmentation on audio-language learning in this study.\n\nWe first argue that paired multi-modal data augmentation takes an important role in audio-language learning. Therefore, as a train-time augmentation, we propose PairMix which is a simple and effective audio-text paired augmentation. Pair-Mix generates audio-text pairs by randomly selecting either waveform-level or mel spectrogram-level mixup with a certain probability for each generated sample and concatenating their corresponding text captions, as illustrated in Figure 1. To support our argument, we first analyze the uni-modal augmenta-\u22c6 Equal contribution. tions for audio-language learning and the proposed multi-modal augmentation method, PairMix. As a result, we observe the importance of multi-modal augmentation in modeling the audiolanguage relationship. PairMix not only outperforms other data augmentation methods but also helps uni-modal data augmentation when uni-modal augmentation and PairMix are applied at the same time. Since the audio modality has different design choices compared to the image, we also compared Pair-Mix with other audio augmentation design choices. PairMix surpasses other designs for multi-modal augmentation such as waveform-level mixup, mel spectrogram-level mixup, and audio concatenation.\n\nTest-time augmentation (TTA) is another data augmentation approach that is shown to increase performances across diverse tasks [9,10]. However, TTA is disregarded in most audiolanguage learning studies. In this paper, we investigate the effect of TTA on audio-language learning. The problem with TTA is that it has bounded performance, and fail to scale up with the size of augmentation. To address this issue, we approach TTA from a new perspective, the place of aggregation. We present a novel multi-level test-time augmentation (Multi-TTA) methodology, which is a generalization of TTA over multiple layers. Multi-TTA successfully outperforms the baselines and we discover that there is still room for improvement using augmentations at test-time.\n\nWe evaluate the proposed methods across automated audio captioning (AAC) and audio-text retrieval on AudioCaps dataset. In AAC, PairMix with audio augmentations achieves 46.6 SPIDEr, which is a 15.9% over the baseline without modifying its architecture, and outperforms the state-of-theart method with a 5x smaller model. Applying PairMix and Multi-TTA results in an extra improvement and attains 47.5 SPIDEr. PairMix and Multi-TTA are also effective in audio-text retrieval. Compared to the baseline, recall at rank 10 (R@10) increases from 82.7% to 87.2% for audio-to-text retrieval, and from 81.3% to 83.2% for text-to-audio retrieval.\n\n\nPairMix\n\nIn this section, we introduce PairMix, the first audio-text paired augmentation, which is a simple but effective augmentation method that generates audio-text pairs. Previous visionlanguage augmentation [5] simply mixes randomly selected raw images and concatenates their corresponding texts. We adapt this approach to the audio-language domain to generate audiotext pairs while preserving the multi-modal relationship.\n\nPairMix first randomly selects N audio-text pairs {(ai, ti)} N i=1 containing audio ai and text ti and then mixes each modality separately to generate the new audio-text pair. Unlike vision-language MixGen which simply implements mixup for images, PairMix leverages the characteristic of audio modality by probabilistically implementing either waveformlevel or mel spectrogram-level audio mixup to generate new mel spectrogram\u015d. Finally, PairMix generates a new audio-text pair (\u015d,t) by concatenating text. It is formally written as:\nsw = M ( N i=1 \u03bbiai) sm = N i=1 \u03bbiM (ai) s = \u03b3\u015dw + (1 \u2212 \u03b3)\u015dm t = Concat({ti} N i=1 ),(1)\nwhere \u03bbi \u2208 [0, 1] for i = 1, ..., N is a hyperparameter such that\nN i=1 \u03bbi = 1, M (\u00b7)\nis a mel spectrogram transformation from waveform, \u03b3 is randomly sampled from Bernoulli distribution with probability 0.5, and Concat(\u00b7) is a concatenation operator which concatenates text inputs.\u015dw and\u015dw denote mel spectrogram which is generated from waveform-level mixup and mel spectrogram-level mixup, respectively. Probabilistically applying two types of mixup allows us to generate more diverse samples, and we observe that this randomness contributes to stable improvements over hyperparameters as shown in Figure 4. Since PairMix can be easily applied with any other uni-modal augmentations, it is possible to drastically scale up the size of the dataset with a simple variant.\n\n\nMulti-level Test-time Augmentation\n\nTest-time augmentation (TTA) contributes to generalizing models by making multiple predictions from augmented inputs and averaging the predictions. TTA has no additional training cost because TTA is only implemented at test-time as opposed to traditional augmentations which mainly focus on train-time. In this paper, we discuss the effects of conventional TTA for audiolanguage learning and present the generalized TTA method. As shown in Figure 2, conventional TTA only averages outputs from augmented inputs. Although this can contribute to performance, conventional TTA has limited performance improvement bound as shown in Figure 4. To fully leverage TTA, we approach this problem with the view of the place of aggregation, unlike previous research [11,12] that tends to focus on a selection of augmentation types and a method of aggregation. We present middle-level TTA (Mid-TTA) that averages intermediate representations and outputs a single prediction. Finally, we propose a novel multi-level TTA (Multi-TTA) methodology by generalizing conventional TTA and Mid-TTA. Unlike a single layer-based conventional TTA, Multi-TTA selects multiple layers for TTA so that augmentations are aggregated at various layers as shown in Figure 2 (c).\n\nWe define a Multi-TTA strategy S = ({P h } H h=1 , \u03c4 ) given a model f containing H layers where \u03c4 is the total number of input augmentations as follows. Let P h be a partition of {1, ..., |P h\u22121 |}. Each element of P h contains indices of the previous layer's outputs that should be aggregated together. We define P h [i] as an i-th element of P h for simplicity. For clarity, |P h | decides the number of h-th layer's outputs, and |P h [i]| decides the number of required h-th layer's inputs for an i-th output of h-th layer. A strategy S satisfies |PH | = 1 to output a single prediction and \u03c4 = \u03a0 H h=1 |P h |. We define an i-th output of a h-th layer o h,i for i = 1, ..., |P h | as follows:\no h,i = 1 |P h [i]| j\u2208P h [i] f h (o h\u22121,j ).(2)\nFrom the notations defined above, conventional TTA becomes a special case of Multi-TTA when |P h | = \u03c4 for 1 \u2264 h \u2264 H \u2212 1 and |PH | = 1. Mid-TTA is also a special case of Multi-TTA when |P h | = \u03c4 for 1 \u2264 h \u2264 h \u2032 and |P h | = 1 for h > h \u2032 .   \n\n\nExperiments\n\n\nDataset\n\nAudioCaps is the largest audio-text paired dataset containing approximately 46K audio clips of 10 seconds extracted from AudioSet [15] and their corresponding text descriptions. We investigate the capability of our proposed methods on Audio-Caps for automated audio captioning and audio-text retrieval. We follow the train, validation, and test splits stated in [7] and reproduce the baselines without unavailable audio clips.\n\n\nAudio-Language Learning\n\n\nAutomated Audio Captioning\n\nAudio Captioning Transformer (ACT) [13] has an encoderdecoder structure from Transformer [16]. The encoder block of ACT is initialized with DeiT [17] which was trained for image classification tasks and then pre-trained on an audio tagging task using AudioSet. We utilize the pre-trained ACT encoder 1 and randomly initialize the ACT-m decoder for a baseline of AAC, resulting in 108M parameters. For evaluation, we employ BLEU [18], METEOR [19], and ROUGE-L [20] from machine translation metrics. We also report CIDEr [21], SPICE [22], and SPIDEr [23]. Captions are decoded with beam search up to size 3.\n\n\nAudio-Text Retrieval\n\nPre-trained encoders are employed and trained using contrastive loss for audio-text retrieval, following the previous study [14].  We choose PANNs [24] as the audio encoder, which has ResNet38 architecture trained on the audio tagging task using AudioSet. For the text encoder, we initialize our model with pre-trained BERT [25] and extract a text representation using the [CLS] token. The final model has 187M trainable parameters. Retrieval models are evaluated by Recall at rank k (R@k) and mean rank (meanR).\n\n\nExperimental Setup\n\nWe experiment PairMix with different ratios K \u2208 {0.125, 0.25, 0.5, 0.6} where K is a ratio of #(generated samples) in a mini-batch to a size of mini-batch. Also, we compare a fixed mixup ratio \u03bb = 0.5 to \u03bb \u223c Beta(0.1, 0.1). As models with K = 0.25 and \u03bb \u223c Beta(0.1, 0.1) show high and stable performance, we use this setting for later experiments.\n\nAdditionally, we explore other design choices such as audio concatenation, waveform-level-only mixup, and mel spectrogram-level-only mixup for mixing audio in PairMix. We mix two audio-text pairs and load additional audio-text pairs only for PairMix to avoid duplication within a mini-batch. To validate the effectiveness and compatibility of PairMix, we compared PairMix with other uni-modal augmentations. Gaussian noise and reverberation are employed as waveform-level audio augmentation, with a probability of 0.5. SpecAugment [1], which is already applied in the baseline model, was applied as a spectrogram-level audio augmentation. For text augmentation, easy data augmentation (EDA) [3] was used. Note that text augmentation is not applied in audio captioning as it can harm the ground-truth captions.\n\nWe validate Multi-TTA strategies where TTA operation is implemented at an intermediate layer hE and an output layer H. We set hE as the last layer of the encoder for AAC and the last layer of the PANN audio encoder for audio-text retrieval. We compare the results of conventional TTA, Mid-TTA, and Multi-TTA with \u03c4 \u2208 {10, 25, 50, 100}. For Multi-TTA, we employ the strategy S satisfying (\u03c4, |P h E [i], |PH [j]|) \u2208 Figure 3: Examples of generated captions in automated audio captioning. PairMix correctly captures the overlapping events in a single audio clip, unlike the others.\n\n{(10, 2, 5), (25,5,5), (50, 5, 10), (100, 5, 20)} for \u2200i, j. As we already mentioned, |P l [k]| denotes the number of required l-th layer's inputs for a single k-th output of layer l. Note that we set |P l [k]| for \u2200k as identical value for simplicity of experiments and S satisfies \u03c4 = |P h E [i] * |PH [j]|. We stabilize a prediction by averaging the output at which no augmentation is applied and the output of TTA. Types of data augmentation on test-time are consistent with train-time. Multi-modal augmentations are not included because generated samples are needless for evaluation. In testtime, we halve the maximum width and height of SpecAugment masking, reverberation degree, and the probability for applying audio augmentations compared to train-time.\n\nAll models are trained for 30 epochs using AdamW optimizer of learning rate 10 \u22124 with the weight decay of 10 \u22126 . SpecAugment is applied to every model. Other hyperparameters follow [13] for AAC and [14] for audio-text retrieval. Single NVIDIA GeForce RTX 3090 was used to train every model, taking 10 hours and 9 hours to train the AAC and audio-text retrieval models, respectively. Table 1 and 2 show an overview of the result. In AAC, using PairMix with the uni-modal audio augmentations achieves 46.6 SPIDEr, which outperforms the previous state-of-the-art performance with a 5x smaller model. In audio-text retrieval, applying PairMix increases R@10 to 86.8% for audio-to-text retrieval and 83.1% for text-to-audio retrieval. Furthermore, attaching Multi-TTA to AAC achieves 47.5 SPIDEr, which is an +18.2% relative to the baseline. Adding Multi-TTA gives additional gain to audio retrieval as well, R@10 of audio-to-text retrieval and text-to-audio retrieval is 87.2% and 83.2%, respectively.\n\n\nResults\n\n\nPairMix\n\nIn Table 3, we compare the proposed PairMix to existing unimodal augmentations. As shown in the table, PairMix performs better than all the other uni-modal augmentations in both AAC and audio-text retrieval. The superiority of our PairMix demonstrates that paired audio-text data augmentation is critical to audio-language learning. Furthermore, applying PairMix with uni-modal augmentations leads to additional improvement.\n\nAlso, we explore other design choices of PairMix. Concat PairMix concatenates audio clips instead of mixing, and waveform PairMix and spectrogram PairMix only augments at waveform-and mel spectrogram-level, respectively. Table 4 demonstrates that our PairMix with probabilistic waveform-and spectrogram-level audio mixup is the best strategy. We investigate the generated captions of the baseline, PairMix, and concat PairMix to illustrate the difference. In failure cases, the baseline and concat PairMix miss or generate inaccurate explanations as shown in Figure 3. However, PairMix generates robust captions with samples containing overlapped events. Figure 4 demonstrates the relationship between the number of input augmentations and the output performance. Only Multi-TTA enhances as the number of input augmentations \u03c4 increases, even though they seem to be similar with a small number of augmentations. This indicates that aggregating augmentations in multiple layers enhances the bounded performance of conventional single-layer TTA, allowing the model to fully use the benefit of augmented samples. As a result, applying Multi-TTA to model with \u03c4 = 100, |P h E [i]| = 5, and |PH [j]| = 20 successfully improves every evaluation metric in both AAC and audio-text retrieval.\n\n\nMulti-Level Test-Time Augmentation\n\nThere could be other hyperparameters to change like the selection of middle layers. Especially, unlike AAC where we can easily select intermediate layers to apply TTA as it has an encoder-decoder structure, audio-text retrieval models have more freedom to select any layers inside the encoder. Future research on automated layer search could further improve current results.\n\n\nConclusions\n\nWe demonstrate that appropriate augmentations improve audiolanguage learning performance. Specifically, experimental results show that PairMix simply generates new audio-text pairs effectively. Also, we generalize TTA and propose Multi-TTA which enhances the efficiency of augmentations. Our proposed methods surpass the state-of-the-art method in AAC and show competitive results in audio-text retrieval. Since PairMix and Multi-TTA are flexible, they can be simply incorporated into other architectures.\n\nFigure 1 :\n1Illustration of PairMix. PairMix mixup audio clips and concatenates the corresponding text. Audio mixup can be implemented at waveform-level or mel spectrogram-level.\n\nFigure 2 :\n2Frameworks for test-time augmentation (TTA). With the augmented inputs at test-time, (a) Conventional TTA averages the final predictions, (b) Mid-TTA averages the intermediate representations, and (c) Multi-TTA averages the outputs of multiple layers at different levels.\n\nFigure 4 :\n4Experiment on TTA with various \u03c4 in automated audio captioning. Multi-TTA strategies are describes in the form of |P h E [i]| \u00d7 |PH [j]|. Overall, Multi-TTA surpasses conventional TTA.\n\nTable 1 :\n1Evaluation of the model performance in automated audio captioning.Method \nBLEU1 BLEU2 BLEU3 BLEU4 METEOR ROUGEL CIDEr SPICE SPIDEr \nBaseline [13] \n65.4 \n47.5 \n33.6 \n23.4 \n22.4 \n47.1 \n63.5 \n16.8 \n40.2 \n+ PairMix \n69.3 \n52.9 \n38.9 \n28.3 \n24.1 \n49.9 \n75.5 \n17.7 \n46.6 \n+ Multi-TTA \n70.0 \n53.4 \n39.5 \n28.9 \n24.2 \n50.2 \n76.9 \n18.1 \n47.5 \n\n\n\nTable 2 :\n2Evaluation of the model performance in audio-text retrieval.Method \ntext-to-audio \naudio-to-text \nR@1 R@5 R@10 R@50 meanR R@1 R@5 R@10 R@50 meanR \nBaseline [14] \n33.0 \n67.9 \n81.3 \n96.5 \n10.0 \n37.9 \n71.0 \n82.7 \n97.4 \n8.8 \n+ PairMix \n34.4 \n69.6 \n83.1 \n97.5 \n8.2 \n40.0 \n73.1 \n86.8 \n97.3 \n6.6 \n+ Multi-TTA \n34.7 \n70.3 \n83.2 \n97.5 \n8.0 \n40.2 \n74.0 \n87.2 \n97.6 \n6.3 \n\n\n\nTable 3 :\n3Comparisonbetween uni-modal and multi-modal data \naugmentation. In modality, A means Audio augmentations and \nT means text augmentations. Augmentation All denotes using \nevery audio augmentation with PairMix. \n\nAugmentation \nModality SPIDEr \nT\u2192 A A\u2192 T \nR@10 R@10 \n-\n-\n41.6 \n78.9 \n81.9 \nNoise \nA \n40.9 \n79.1 \n81.1 \nReverb \nA \n41.1 \n80.4 \n81.0 \nSpecAugment [1] \nA \n40.2 \n81.3 \n82.7 \nEDA [3] \nT \n-\n79.3 \n81.3 \nPairMix \nAT \n43.0 \n82.2 \n84.9 \nAll \nAT \n46.6 \n83.1 \n86.8 \n\n\n\nTable 4 :\n4Experiment with different augmentation design choices for audio mixing in multi-modal augmentation.Design \nSPIDEr \nT\u2192 A A\u2192 T \nR@10 R@10 \nconcat \n45.0 \n81.9 \n80.8 \nwaveform-only \n43.7 \n80.5 \n84.1 \nspectrogram-only \n44.6 \n82.3 \n83.3 \nPairMix \n45.8 \n82.9 \n84.7 \n\n\n\nSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. D S Park, W Chan, Y Zhang, C.-C Chiu, B Zoph, E D Cubuk, Q V Le, Proc. Interspeech. InterspeechD. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,\" in Proc. Interspeech 2019, 2019, pp. 2613-2617.\n\nSpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification. H Wang, Y Zou, W Wang, Proc. Interspeech. InterspeechH. Wang, Y. Zou, and W. Wang, \"SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classifica- tion,\" in Proc. Interspeech 2021, 2021, pp. 551-555.\n\nEda: Easy data augmentation techniques for boosting performance on text classification tasks. J Wei, K Zou, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingJ. Wei and K. Zou, \"Eda: Easy data augmentation techniques for boosting performance on text classification tasks,\" in Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 6382-6388.\n\nUnderstanding back-translation at scale. S Edunov, M Ott, M Auli, D Grangier, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingS. Edunov, M. Ott, M. Auli, and D. Grangier, \"Understanding back-translation at scale,\" in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 489-500.\n\nMixgen: A new multi-modal data augmentation. X Hao, Y Zhu, S Appalaraju, A Zhang, W Zhang, B Li, M Li, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionX. Hao, Y. Zhu, S. Appalaraju, A. Zhang, W. Zhang, B. Li, and M. Li, \"Mixgen: A new multi-modal data augmentation,\" in Pro- ceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 379-389.\n\nmixup: Beyond empirical risk minimization. H Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, International Conference on Learning Representations. H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \"mixup: Beyond empirical risk minimization,\" in International Conference on Learning Representations, 2018. [Online].\n\nAudiocaps: Generating captions for audios in the wild. C D Kim, B Kim, H Lee, G Kim, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesC. D. Kim, B. Kim, H. Lee, and G. Kim, \"Audiocaps: Generat- ing captions for audios in the wild,\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), 2019, pp. 119-132.\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerProceedings, Part V 13T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft coco: Common objects in context,\" in Computer Vision-ECCV 2014: 13th European Con- ference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer, 2014, pp. 740-755.\n\nTest-time augmentation for deep learning-based cell segmentation on microscopy images. N Moshkov, B Mathe, A Kertesz-Farkas, R Hollandi, P Horvath, Scientific reports. 101N. Moshkov, B. Mathe, A. Kertesz-Farkas, R. Hollandi, and P. Horvath, \"Test-time augmentation for deep learning-based cell segmentation on microscopy images,\" Scientific reports, vol. 10, no. 1, pp. 1-7, 2020.\n\nGreedy policy search: A simple baseline for learnable test-time augmentation. A Lyzhov, Y Molchanova, A Ashukha, D Molchanov, D Vetrov, Conference on Uncertainty in Artificial Intelligence. PMLR, 2020. A. Lyzhov, Y. Molchanova, A. Ashukha, D. Molchanov, and D. Vetrov, \"Greedy policy search: A simple baseline for learnable test-time augmentation,\" in Conference on Uncertainty in Artifi- cial Intelligence. PMLR, 2020, pp. 1308-1317.\n\nLearning loss for test-time augmentation. I Kim, Y Kim, S Kim, Advances in Neural Information Processing Systems. 33I. Kim, Y. Kim, and S. Kim, \"Learning loss for test-time aug- mentation,\" Advances in Neural Information Processing Systems, vol. 33, pp. 4163-4174, 2020.\n\nBetter aggregation in test-time augmentation. D Shanmugam, D Blalock, G Balakrishnan, J Guttag, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionD. Shanmugam, D. Blalock, G. Balakrishnan, and J. Guttag, \"Bet- ter aggregation in test-time augmentation,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1214-1223.\n\nAudio captioning transformer. X Mei, X Liu, Q Huang, M D Plumbley, W Wang, Detection and Classification of Acoustic Scenes and Events-DCASE 2021. X. Mei, X. Liu, Q. Huang, M. D. Plumbley, and W. Wang, \"Audio captioning transformer,\" in Detection and Classification of Acous- tic Scenes and Events-DCASE 2021, 2021.\n\nOn Metric Learning for Audio-Text Cross-Modal Retrieval. X Mei, X Liu, J Sun, M Plumbley, W Wang, Proc. Interspeech. InterspeechX. Mei, X. Liu, J. Sun, M. Plumbley, and W. Wang, \"On Metric Learning for Audio-Text Cross-Modal Retrieval,\" in Proc. Inter- speech 2022, 2022, pp. 4142-4146.\n\nAudio set: An ontology and human-labeled dataset for audio events. J F Gemmeke, D P Ellis, D Freedman, A Jansen, W Lawrence, R C Moore, M Plakal, M Ritter, 2017 IEEE international conference on acoustics, speech and signal processing. IEEEICASSPJ. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \"Audio set: An ontology and human-labeled dataset for audio events,\" in 2017 IEEE inter- national conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, pp. 776-780.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017.\n\nTraining data-efficient image transformers & distillation through attention. H Touvron, M Cord, M Douze, F Massa, A Sablayrolles, H J\u00e9gou, International Conference on Machine Learning. PMLR, 2021. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \"Training data-efficient image transformers & distilla- tion through attention,\" in International Conference on Machine Learning. PMLR, 2021, pp. 10 347-10 357.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational LinguisticsK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \"Bleu: a method for automatic evaluation of machine translation,\" in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311-318.\n\nMeteor: An automatic metric for mt evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarizationS. Banerjee and A. Lavie, \"Meteor: An automatic metric for mt evaluation with improved correlation with human judgments,\" in Proceedings of the acl workshop on intrinsic and extrinsic eval- uation measures for machine translation and/or summarization, 2005, pp. 65-72.\n\nRouge: A package for automatic evaluation of summaries. C.-Y. Lin, Text summarization branches out. C.-Y. Lin, \"Rouge: A package for automatic evaluation of sum- maries,\" in Text summarization branches out, 2004, pp. 74-81.\n\nCider: Consensus-based image description evaluation. R Vedantam, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionR. Vedantam, C. Lawrence Zitnick, and D. Parikh, \"Cider: Consensus-based image description evaluation,\" in Proceedings of the IEEE conference on computer vision and pattern recogni- tion, 2015, pp. 4566-4575.\n\nSpice: Semantic propositional image caption evaluation. P Anderson, B Fernando, M Johnson, S Gould, SpringerP. Anderson, B. Fernando, M. Johnson, and S. Gould, \"Spice: Se- mantic propositional image caption evaluation,\" in European con- ference on computer vision. Springer, 2016, pp. 382-398.\n\nImproved image captioning via policy gradient optimization of spider. S Liu, Z Zhu, N Ye, S Guadarrama, K Murphy, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionS. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, \"Improved image captioning via policy gradient optimization of spider,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 873-881.\n\nPanns: Large-scale pretrained audio neural networks for audio pattern recognition. Q Kong, Y Cao, T Iqbal, Y Wang, W Wang, M D Plumbley, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 28Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumb- ley, \"Panns: Large-scale pretrained audio neural networks for audio pattern recognition,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880-2894, 2020.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of NAACL-HLT. NAACL-HLTJ. D. M.-W. C. Kenton and L. K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" in Proceedings of NAACL-HLT, 2019, pp. 4171-4186.\n", "annotations": {"author": "[{\"end\":160,\"start\":75},{\"end\":217,\"start\":161},{\"end\":272,\"start\":218},{\"end\":285,\"start\":273},{\"end\":342,\"start\":286},{\"end\":455,\"start\":343},{\"end\":512,\"start\":456},{\"end\":702,\"start\":513}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":171,\"start\":168},{\"end\":226,\"start\":224},{\"end\":284,\"start\":281},{\"end\":296,\"start\":292},{\"end\":354,\"start\":351},{\"end\":466,\"start\":463},{\"end\":522,\"start\":519}]", "author_first_name": "[{\"end\":83,\"start\":75},{\"end\":167,\"start\":161},{\"end\":223,\"start\":218},{\"end\":280,\"start\":273},{\"end\":291,\"start\":286},{\"end\":350,\"start\":343},{\"end\":462,\"start\":456},{\"end\":518,\"start\":513}]", "author_affiliation": "[{\"end\":159,\"start\":106},{\"end\":216,\"start\":173},{\"end\":271,\"start\":228},{\"end\":341,\"start\":298},{\"end\":409,\"start\":356},{\"end\":454,\"start\":411},{\"end\":511,\"start\":468},{\"end\":577,\"start\":524},{\"end\":622,\"start\":579},{\"end\":701,\"start\":624}]", "title": "[{\"end\":72,\"start\":1},{\"end\":774,\"start\":703}]", "venue": null, "abstract": "[{\"end\":1837,\"start\":875}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2146,\"start\":2143},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2148,\"start\":2146},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2175,\"start\":2172},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2177,\"start\":2175},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2234,\"start\":2231},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2316,\"start\":2313},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2477,\"start\":2474},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2616,\"start\":2613},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4167,\"start\":4164},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4170,\"start\":4167},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5645,\"start\":5642},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8051,\"start\":8047},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8054,\"start\":8051},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9688,\"start\":9684},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9919,\"start\":9916},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10076,\"start\":10072},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10130,\"start\":10126},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10186,\"start\":10182},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10469,\"start\":10465},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10482,\"start\":10478},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10500,\"start\":10496},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10560,\"start\":10556},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10572,\"start\":10568},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10589,\"start\":10585},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10795,\"start\":10791},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10818,\"start\":10814},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10995,\"start\":10991},{\"end\":11045,\"start\":11040},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12085,\"start\":12082},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12245,\"start\":12242},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12960,\"start\":12956},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12962,\"start\":12960},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12964,\"start\":12962},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13894,\"start\":13890},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13911,\"start\":13907}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17551,\"start\":17372},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17836,\"start\":17552},{\"attributes\":{\"id\":\"fig_2\"},\"end\":18034,\"start\":17837},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":18381,\"start\":18035},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":18756,\"start\":18382},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":19235,\"start\":18757},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":19508,\"start\":19236}]", "paragraph": "[{\"end\":2369,\"start\":1853},{\"end\":2797,\"start\":2371},{\"end\":4035,\"start\":2799},{\"end\":4787,\"start\":4037},{\"end\":5427,\"start\":4789},{\"end\":5858,\"start\":5439},{\"end\":6393,\"start\":5860},{\"end\":6548,\"start\":6483},{\"end\":7254,\"start\":6569},{\"end\":8537,\"start\":7293},{\"end\":9235,\"start\":8539},{\"end\":9528,\"start\":9285},{\"end\":9980,\"start\":9554},{\"end\":10642,\"start\":10037},{\"end\":11179,\"start\":10667},{\"end\":11549,\"start\":11202},{\"end\":12360,\"start\":11551},{\"end\":12941,\"start\":12362},{\"end\":13705,\"start\":12943},{\"end\":14706,\"start\":13707},{\"end\":15152,\"start\":14728},{\"end\":16437,\"start\":15154},{\"end\":16850,\"start\":16476},{\"end\":17371,\"start\":16866}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6482,\"start\":6394},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6568,\"start\":6549},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9284,\"start\":9236}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14099,\"start\":14092},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14738,\"start\":14731},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15382,\"start\":15375}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1851,\"start\":1839},{\"attributes\":{\"n\":\"2.\"},\"end\":5437,\"start\":5430},{\"attributes\":{\"n\":\"3.\"},\"end\":7291,\"start\":7257},{\"attributes\":{\"n\":\"4.\"},\"end\":9542,\"start\":9531},{\"attributes\":{\"n\":\"4.1.\"},\"end\":9552,\"start\":9545},{\"attributes\":{\"n\":\"4.2.\"},\"end\":10006,\"start\":9983},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":10035,\"start\":10009},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":10665,\"start\":10645},{\"attributes\":{\"n\":\"4.3.\"},\"end\":11200,\"start\":11182},{\"attributes\":{\"n\":\"5.\"},\"end\":14716,\"start\":14709},{\"attributes\":{\"n\":\"5.1.\"},\"end\":14726,\"start\":14719},{\"attributes\":{\"n\":\"5.2.\"},\"end\":16474,\"start\":16440},{\"attributes\":{\"n\":\"6.\"},\"end\":16864,\"start\":16853},{\"end\":17383,\"start\":17373},{\"end\":17563,\"start\":17553},{\"end\":17848,\"start\":17838},{\"end\":18045,\"start\":18036},{\"end\":18392,\"start\":18383},{\"end\":18767,\"start\":18758},{\"end\":19246,\"start\":19237}]", "table": "[{\"end\":18381,\"start\":18113},{\"end\":18756,\"start\":18454},{\"end\":19235,\"start\":18779},{\"end\":19508,\"start\":19347}]", "figure_caption": "[{\"end\":17551,\"start\":17385},{\"end\":17836,\"start\":17565},{\"end\":18034,\"start\":17850},{\"end\":18113,\"start\":18047},{\"end\":18454,\"start\":18394},{\"end\":18779,\"start\":18769},{\"end\":19347,\"start\":19248}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3275,\"start\":3267},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7091,\"start\":7083},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7741,\"start\":7733},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7929,\"start\":7921},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8532,\"start\":8524},{\"end\":12785,\"start\":12777},{\"end\":15721,\"start\":15713},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15817,\"start\":15809}]", "bib_author_first_name": "[{\"end\":19592,\"start\":19591},{\"end\":19594,\"start\":19593},{\"end\":19602,\"start\":19601},{\"end\":19610,\"start\":19609},{\"end\":19622,\"start\":19618},{\"end\":19630,\"start\":19629},{\"end\":19638,\"start\":19637},{\"end\":19640,\"start\":19639},{\"end\":19649,\"start\":19648},{\"end\":19651,\"start\":19650},{\"end\":19988,\"start\":19987},{\"end\":19996,\"start\":19995},{\"end\":20003,\"start\":20002},{\"end\":20306,\"start\":20305},{\"end\":20313,\"start\":20312},{\"end\":20987,\"start\":20986},{\"end\":20997,\"start\":20996},{\"end\":21004,\"start\":21003},{\"end\":21012,\"start\":21011},{\"end\":21427,\"start\":21426},{\"end\":21434,\"start\":21433},{\"end\":21441,\"start\":21440},{\"end\":21455,\"start\":21454},{\"end\":21464,\"start\":21463},{\"end\":21473,\"start\":21472},{\"end\":21479,\"start\":21478},{\"end\":21898,\"start\":21897},{\"end\":21907,\"start\":21906},{\"end\":21916,\"start\":21915},{\"end\":21918,\"start\":21917},{\"end\":21929,\"start\":21928},{\"end\":22223,\"start\":22222},{\"end\":22225,\"start\":22224},{\"end\":22232,\"start\":22231},{\"end\":22239,\"start\":22238},{\"end\":22246,\"start\":22245},{\"end\":22871,\"start\":22867},{\"end\":22878,\"start\":22877},{\"end\":22887,\"start\":22886},{\"end\":22899,\"start\":22898},{\"end\":22907,\"start\":22906},{\"end\":22917,\"start\":22916},{\"end\":22928,\"start\":22927},{\"end\":22938,\"start\":22937},{\"end\":22940,\"start\":22939},{\"end\":23436,\"start\":23435},{\"end\":23447,\"start\":23446},{\"end\":23456,\"start\":23455},{\"end\":23474,\"start\":23473},{\"end\":23486,\"start\":23485},{\"end\":23809,\"start\":23808},{\"end\":23819,\"start\":23818},{\"end\":23833,\"start\":23832},{\"end\":23844,\"start\":23843},{\"end\":23857,\"start\":23856},{\"end\":24209,\"start\":24208},{\"end\":24216,\"start\":24215},{\"end\":24223,\"start\":24222},{\"end\":24485,\"start\":24484},{\"end\":24498,\"start\":24497},{\"end\":24509,\"start\":24508},{\"end\":24525,\"start\":24524},{\"end\":24900,\"start\":24899},{\"end\":24907,\"start\":24906},{\"end\":24914,\"start\":24913},{\"end\":24923,\"start\":24922},{\"end\":24925,\"start\":24924},{\"end\":24937,\"start\":24936},{\"end\":25243,\"start\":25242},{\"end\":25250,\"start\":25249},{\"end\":25257,\"start\":25256},{\"end\":25264,\"start\":25263},{\"end\":25276,\"start\":25275},{\"end\":25541,\"start\":25540},{\"end\":25543,\"start\":25542},{\"end\":25554,\"start\":25553},{\"end\":25556,\"start\":25555},{\"end\":25565,\"start\":25564},{\"end\":25577,\"start\":25576},{\"end\":25587,\"start\":25586},{\"end\":25599,\"start\":25598},{\"end\":25601,\"start\":25600},{\"end\":25610,\"start\":25609},{\"end\":25620,\"start\":25619},{\"end\":26038,\"start\":26037},{\"end\":26049,\"start\":26048},{\"end\":26060,\"start\":26059},{\"end\":26070,\"start\":26069},{\"end\":26083,\"start\":26082},{\"end\":26092,\"start\":26091},{\"end\":26094,\"start\":26093},{\"end\":26103,\"start\":26102},{\"end\":26113,\"start\":26112},{\"end\":26455,\"start\":26454},{\"end\":26466,\"start\":26465},{\"end\":26474,\"start\":26473},{\"end\":26483,\"start\":26482},{\"end\":26492,\"start\":26491},{\"end\":26508,\"start\":26507},{\"end\":26873,\"start\":26872},{\"end\":26885,\"start\":26884},{\"end\":26895,\"start\":26894},{\"end\":26906,\"start\":26902},{\"end\":27394,\"start\":27393},{\"end\":27406,\"start\":27405},{\"end\":27978,\"start\":27973},{\"end\":28196,\"start\":28195},{\"end\":28208,\"start\":28207},{\"end\":28217,\"start\":28209},{\"end\":28228,\"start\":28227},{\"end\":28645,\"start\":28644},{\"end\":28657,\"start\":28656},{\"end\":28669,\"start\":28668},{\"end\":28680,\"start\":28679},{\"end\":28954,\"start\":28953},{\"end\":28961,\"start\":28960},{\"end\":28968,\"start\":28967},{\"end\":28974,\"start\":28973},{\"end\":28988,\"start\":28987},{\"end\":29419,\"start\":29418},{\"end\":29427,\"start\":29426},{\"end\":29434,\"start\":29433},{\"end\":29443,\"start\":29442},{\"end\":29451,\"start\":29450},{\"end\":29459,\"start\":29458},{\"end\":29461,\"start\":29460},{\"end\":29870,\"start\":29869},{\"end\":29872,\"start\":29871},{\"end\":29879,\"start\":29875},{\"end\":29881,\"start\":29880},{\"end\":29891,\"start\":29890},{\"end\":29893,\"start\":29892}]", "bib_author_last_name": "[{\"end\":19599,\"start\":19595},{\"end\":19607,\"start\":19603},{\"end\":19616,\"start\":19611},{\"end\":19627,\"start\":19623},{\"end\":19635,\"start\":19631},{\"end\":19646,\"start\":19641},{\"end\":19654,\"start\":19652},{\"end\":19993,\"start\":19989},{\"end\":20000,\"start\":19997},{\"end\":20008,\"start\":20004},{\"end\":20310,\"start\":20307},{\"end\":20317,\"start\":20314},{\"end\":20994,\"start\":20988},{\"end\":21001,\"start\":20998},{\"end\":21009,\"start\":21005},{\"end\":21021,\"start\":21013},{\"end\":21431,\"start\":21428},{\"end\":21438,\"start\":21435},{\"end\":21452,\"start\":21442},{\"end\":21461,\"start\":21456},{\"end\":21470,\"start\":21465},{\"end\":21476,\"start\":21474},{\"end\":21482,\"start\":21480},{\"end\":21904,\"start\":21899},{\"end\":21913,\"start\":21908},{\"end\":21926,\"start\":21919},{\"end\":21939,\"start\":21930},{\"end\":22229,\"start\":22226},{\"end\":22236,\"start\":22233},{\"end\":22243,\"start\":22240},{\"end\":22250,\"start\":22247},{\"end\":22875,\"start\":22872},{\"end\":22884,\"start\":22879},{\"end\":22896,\"start\":22888},{\"end\":22904,\"start\":22900},{\"end\":22914,\"start\":22908},{\"end\":22925,\"start\":22918},{\"end\":22935,\"start\":22929},{\"end\":22948,\"start\":22941},{\"end\":23444,\"start\":23437},{\"end\":23453,\"start\":23448},{\"end\":23471,\"start\":23457},{\"end\":23483,\"start\":23475},{\"end\":23494,\"start\":23487},{\"end\":23816,\"start\":23810},{\"end\":23830,\"start\":23820},{\"end\":23841,\"start\":23834},{\"end\":23854,\"start\":23845},{\"end\":23864,\"start\":23858},{\"end\":24213,\"start\":24210},{\"end\":24220,\"start\":24217},{\"end\":24227,\"start\":24224},{\"end\":24495,\"start\":24486},{\"end\":24506,\"start\":24499},{\"end\":24522,\"start\":24510},{\"end\":24532,\"start\":24526},{\"end\":24904,\"start\":24901},{\"end\":24911,\"start\":24908},{\"end\":24920,\"start\":24915},{\"end\":24934,\"start\":24926},{\"end\":24942,\"start\":24938},{\"end\":25247,\"start\":25244},{\"end\":25254,\"start\":25251},{\"end\":25261,\"start\":25258},{\"end\":25273,\"start\":25265},{\"end\":25281,\"start\":25277},{\"end\":25551,\"start\":25544},{\"end\":25562,\"start\":25557},{\"end\":25574,\"start\":25566},{\"end\":25584,\"start\":25578},{\"end\":25596,\"start\":25588},{\"end\":25607,\"start\":25602},{\"end\":25617,\"start\":25611},{\"end\":25627,\"start\":25621},{\"end\":26046,\"start\":26039},{\"end\":26057,\"start\":26050},{\"end\":26067,\"start\":26061},{\"end\":26080,\"start\":26071},{\"end\":26089,\"start\":26084},{\"end\":26100,\"start\":26095},{\"end\":26110,\"start\":26104},{\"end\":26124,\"start\":26114},{\"end\":26463,\"start\":26456},{\"end\":26471,\"start\":26467},{\"end\":26480,\"start\":26475},{\"end\":26489,\"start\":26484},{\"end\":26505,\"start\":26493},{\"end\":26514,\"start\":26509},{\"end\":26882,\"start\":26874},{\"end\":26892,\"start\":26886},{\"end\":26900,\"start\":26896},{\"end\":26910,\"start\":26907},{\"end\":27403,\"start\":27395},{\"end\":27412,\"start\":27407},{\"end\":27982,\"start\":27979},{\"end\":28205,\"start\":28197},{\"end\":28225,\"start\":28218},{\"end\":28235,\"start\":28229},{\"end\":28654,\"start\":28646},{\"end\":28666,\"start\":28658},{\"end\":28677,\"start\":28670},{\"end\":28686,\"start\":28681},{\"end\":28958,\"start\":28955},{\"end\":28965,\"start\":28962},{\"end\":28971,\"start\":28969},{\"end\":28985,\"start\":28975},{\"end\":28995,\"start\":28989},{\"end\":29424,\"start\":29420},{\"end\":29431,\"start\":29428},{\"end\":29440,\"start\":29435},{\"end\":29448,\"start\":29444},{\"end\":29456,\"start\":29452},{\"end\":29470,\"start\":29462},{\"end\":29888,\"start\":29882},{\"end\":29903,\"start\":29894}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":121321299},\"end\":19895,\"start\":19510},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":232428343},\"end\":20209,\"start\":19897},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59523656},\"end\":20943,\"start\":20211},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52113461},\"end\":21379,\"start\":20945},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":249712192},\"end\":21852,\"start\":21381},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3162051},\"end\":22165,\"start\":21854},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":174799768},\"end\":22822,\"start\":22167},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14113767},\"end\":23346,\"start\":22824},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":213175363},\"end\":23728,\"start\":23348},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211252848},\"end\":24164,\"start\":23730},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225039917},\"end\":24436,\"start\":24166},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":238634826},\"end\":24867,\"start\":24438},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":236154948},\"end\":25183,\"start\":24869},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":247779240},\"end\":25471,\"start\":25185},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21519176},\"end\":26008,\"start\":25473},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13756489},\"end\":26375,\"start\":26010},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":229363322},\"end\":26806,\"start\":26377},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11080756},\"end\":27297,\"start\":26808},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7164502},\"end\":27915,\"start\":27299},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":964287},\"end\":28140,\"start\":27917},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9026666},\"end\":28586,\"start\":28142},{\"attributes\":{\"id\":\"b21\"},\"end\":28881,\"start\":28588},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3873857},\"end\":29333,\"start\":28883},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":209444382},\"end\":29785,\"start\":29335},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52967399},\"end\":30116,\"start\":29787}]", "bib_title": "[{\"end\":19589,\"start\":19510},{\"end\":19985,\"start\":19897},{\"end\":20303,\"start\":20211},{\"end\":20984,\"start\":20945},{\"end\":21424,\"start\":21381},{\"end\":21895,\"start\":21854},{\"end\":22220,\"start\":22167},{\"end\":22865,\"start\":22824},{\"end\":23433,\"start\":23348},{\"end\":23806,\"start\":23730},{\"end\":24206,\"start\":24166},{\"end\":24482,\"start\":24438},{\"end\":24897,\"start\":24869},{\"end\":25240,\"start\":25185},{\"end\":25538,\"start\":25473},{\"end\":26035,\"start\":26010},{\"end\":26452,\"start\":26377},{\"end\":26870,\"start\":26808},{\"end\":27391,\"start\":27299},{\"end\":27971,\"start\":27917},{\"end\":28193,\"start\":28142},{\"end\":28951,\"start\":28883},{\"end\":29416,\"start\":29335},{\"end\":29867,\"start\":29787}]", "bib_author": "[{\"end\":19601,\"start\":19591},{\"end\":19609,\"start\":19601},{\"end\":19618,\"start\":19609},{\"end\":19629,\"start\":19618},{\"end\":19637,\"start\":19629},{\"end\":19648,\"start\":19637},{\"end\":19656,\"start\":19648},{\"end\":19995,\"start\":19987},{\"end\":20002,\"start\":19995},{\"end\":20010,\"start\":20002},{\"end\":20312,\"start\":20305},{\"end\":20319,\"start\":20312},{\"end\":20996,\"start\":20986},{\"end\":21003,\"start\":20996},{\"end\":21011,\"start\":21003},{\"end\":21023,\"start\":21011},{\"end\":21433,\"start\":21426},{\"end\":21440,\"start\":21433},{\"end\":21454,\"start\":21440},{\"end\":21463,\"start\":21454},{\"end\":21472,\"start\":21463},{\"end\":21478,\"start\":21472},{\"end\":21484,\"start\":21478},{\"end\":21906,\"start\":21897},{\"end\":21915,\"start\":21906},{\"end\":21928,\"start\":21915},{\"end\":21941,\"start\":21928},{\"end\":22231,\"start\":22222},{\"end\":22238,\"start\":22231},{\"end\":22245,\"start\":22238},{\"end\":22252,\"start\":22245},{\"end\":22877,\"start\":22867},{\"end\":22886,\"start\":22877},{\"end\":22898,\"start\":22886},{\"end\":22906,\"start\":22898},{\"end\":22916,\"start\":22906},{\"end\":22927,\"start\":22916},{\"end\":22937,\"start\":22927},{\"end\":22950,\"start\":22937},{\"end\":23446,\"start\":23435},{\"end\":23455,\"start\":23446},{\"end\":23473,\"start\":23455},{\"end\":23485,\"start\":23473},{\"end\":23496,\"start\":23485},{\"end\":23818,\"start\":23808},{\"end\":23832,\"start\":23818},{\"end\":23843,\"start\":23832},{\"end\":23856,\"start\":23843},{\"end\":23866,\"start\":23856},{\"end\":24215,\"start\":24208},{\"end\":24222,\"start\":24215},{\"end\":24229,\"start\":24222},{\"end\":24497,\"start\":24484},{\"end\":24508,\"start\":24497},{\"end\":24524,\"start\":24508},{\"end\":24534,\"start\":24524},{\"end\":24906,\"start\":24899},{\"end\":24913,\"start\":24906},{\"end\":24922,\"start\":24913},{\"end\":24936,\"start\":24922},{\"end\":24944,\"start\":24936},{\"end\":25249,\"start\":25242},{\"end\":25256,\"start\":25249},{\"end\":25263,\"start\":25256},{\"end\":25275,\"start\":25263},{\"end\":25283,\"start\":25275},{\"end\":25553,\"start\":25540},{\"end\":25564,\"start\":25553},{\"end\":25576,\"start\":25564},{\"end\":25586,\"start\":25576},{\"end\":25598,\"start\":25586},{\"end\":25609,\"start\":25598},{\"end\":25619,\"start\":25609},{\"end\":25629,\"start\":25619},{\"end\":26048,\"start\":26037},{\"end\":26059,\"start\":26048},{\"end\":26069,\"start\":26059},{\"end\":26082,\"start\":26069},{\"end\":26091,\"start\":26082},{\"end\":26102,\"start\":26091},{\"end\":26112,\"start\":26102},{\"end\":26126,\"start\":26112},{\"end\":26465,\"start\":26454},{\"end\":26473,\"start\":26465},{\"end\":26482,\"start\":26473},{\"end\":26491,\"start\":26482},{\"end\":26507,\"start\":26491},{\"end\":26516,\"start\":26507},{\"end\":26884,\"start\":26872},{\"end\":26894,\"start\":26884},{\"end\":26902,\"start\":26894},{\"end\":26912,\"start\":26902},{\"end\":27405,\"start\":27393},{\"end\":27414,\"start\":27405},{\"end\":27984,\"start\":27973},{\"end\":28207,\"start\":28195},{\"end\":28227,\"start\":28207},{\"end\":28237,\"start\":28227},{\"end\":28656,\"start\":28644},{\"end\":28668,\"start\":28656},{\"end\":28679,\"start\":28668},{\"end\":28688,\"start\":28679},{\"end\":28960,\"start\":28953},{\"end\":28967,\"start\":28960},{\"end\":28973,\"start\":28967},{\"end\":28987,\"start\":28973},{\"end\":28997,\"start\":28987},{\"end\":29426,\"start\":29418},{\"end\":29433,\"start\":29426},{\"end\":29442,\"start\":29433},{\"end\":29450,\"start\":29442},{\"end\":29458,\"start\":29450},{\"end\":29472,\"start\":29458},{\"end\":29875,\"start\":29869},{\"end\":29890,\"start\":29875},{\"end\":29905,\"start\":29890}]", "bib_venue": "[{\"end\":19673,\"start\":19656},{\"end\":20027,\"start\":20010},{\"end\":20479,\"start\":20319},{\"end\":21109,\"start\":21023},{\"end\":21564,\"start\":21484},{\"end\":21993,\"start\":21941},{\"end\":22394,\"start\":22252},{\"end\":23001,\"start\":22950},{\"end\":23514,\"start\":23496},{\"end\":23930,\"start\":23866},{\"end\":24278,\"start\":24229},{\"end\":24605,\"start\":24534},{\"end\":25013,\"start\":24944},{\"end\":25300,\"start\":25283},{\"end\":25706,\"start\":25629},{\"end\":26175,\"start\":26126},{\"end\":26572,\"start\":26516},{\"end\":26999,\"start\":26912},{\"end\":27537,\"start\":27414},{\"end\":28015,\"start\":27984},{\"end\":28314,\"start\":28237},{\"end\":28642,\"start\":28588},{\"end\":29064,\"start\":28997},{\"end\":29535,\"start\":29472},{\"end\":29929,\"start\":29905},{\"end\":19686,\"start\":19675},{\"end\":20040,\"start\":20029},{\"end\":20626,\"start\":20481},{\"end\":21182,\"start\":21111},{\"end\":21631,\"start\":21566},{\"end\":22523,\"start\":22396},{\"end\":23022,\"start\":23003},{\"end\":24663,\"start\":24607},{\"end\":25313,\"start\":25302},{\"end\":27073,\"start\":27001},{\"end\":27647,\"start\":27539},{\"end\":28378,\"start\":28316},{\"end\":29118,\"start\":29066},{\"end\":29940,\"start\":29931}]"}}}, "year": 2023, "month": 12, "day": 17}