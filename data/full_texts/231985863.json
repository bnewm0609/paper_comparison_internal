{"id": 231985863, "updated": "2023-11-08 02:42:33.904", "metadata": {"title": "LogME: Practical Assessment of Pre-trained Models for Transfer Learning", "authors": "[{\"first\":\"Kaichao\",\"last\":\"You\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jianmin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Mingsheng\",\"last\":\"Long\",\"middle\":[]}]", "venue": "ICML", "journal": "12133-12143", "publication_date": {"year": 2021, "month": 2, "day": 22}, "abstract": "This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo \\emph{without fine-tuning}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is \\emph{immune to over-fitting}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is \\emph{fast, accurate, and general}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most $3000\\times$ speedup in wall-clock time and requires only $1\\%$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: \\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.11005", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/YouLWL21", "doi": null}}, "content": {"source": {"pdf_hash": "54a0d678fa5cacc4fe033d70b86ca8b89977142e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.11005v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "422cf6224d6d3a13a32e7c5dc62e8aee00cd2984", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/54a0d678fa5cacc4fe033d70b86ca8b89977142e.txt", "contents": "\nLogME: Practical Assessment of Pre-trained Models for Transfer Learning\n\n\nKaichao You <youkaichao@gmail.com> \nEqual contribution\n\n\nSchool of Software\nTsinghua University\n100084BNRist, BeijingChina\n\nYong Liu \nEqual contribution\n\n\nSchool of Software\nTsinghua University\n100084BNRist, BeijingChina\n\nJianmin Wang \nSchool of Software\nTsinghua University\n100084BNRist, BeijingChina\n\nMingsheng Long <mingsheng@tsinghua.edu.cn>. \nSchool of Software\nTsinghua University\n100084BNRist, BeijingChina\n\nLogME: Practical Assessment of Pre-trained Models for Transfer Learning\nB90A4135431B75293346267EF57749D1\nThis paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo without fine-tuning.A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks.In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models.Unlike the maximum likelihood, the maximum evidence is immune to over-fitting, while its expensive computation can be dramatically reduced by our carefully designed algorithm.The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance.LogME is fast, accurate, and general, characterizing itself as the first practical method for assessing pre-trained models.Compared with bruteforce fine-tuning, LogME brings at most 3000\u00d7 speedup in wall-clock time and requires only 1% memory footprint.It outperforms prior methods by a large margin in their setting and is applicable to new settings.It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language).Code is available at this repository: https://github.com/thuml/LogME.\n\nIntroduction\n\nHuman performance on many recognition tasks has been surpassed by deep neural networks (He et al., 2015;2016) Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021.Copyright 2021 by the author(s).\n\ntrained with large-scale supervised data (Deng et al., 2009;Russakovsky et al., 2015) and specialized computational devices (Jouppi et al., 2017).These trained neural networks, also known as pre-trained models, not only work well on tasks they are intended for but also produce generic representations (Donahue et al., 2014) that benefit downstream tasks such as object detection (Girshick et al., 2014).\n\nApart from serving as fixed feature extractors, pre-trained models can be fine-tuned (Yosinski et al., 2014;He et al., 2019) to serve downstream tasks better.The transfer learning paradigm \"pre-training \u2192 fine-tuning\" enjoys tremendous success in both vision (Kornblith et al., 2019) and language (Devlin et al., 2019) communities, and continues to expand to communities like geometric learning (Hu et al., 2020).Transfer of pre-trained models has become one of the cornerstones of deep learning.\n\nNowadays, there are numerous public pre-trained models offered by PyTorch (Benoit et al., 2019), TensorFlow (Abadi et al., 2016) and third-party libraries like HuggingFace Transformers (Wolf et al., 2020).When a practitioner wants to employ transfer learning to solve a specific task, the first problem is to select a good pre-trained model to start from.The problem is non-trivial and task adaptive, considering that different tasks favor different pre-trained models.The problem challenges researchers to develop a practical assessment method that is fast, accurate and general.It should be fast enough compared to brute-force fine-tuning all available pre-trained models (Zamir et al., 2018), should be accurate enough so that potentially best models can be identified, and should be general enough to tackle a wide variety of common learning scenarios.\n\nDespite its practical significance, there is limited guidance arXiv: 2102.11005v3 [cs.LG] 23 Jun 2021 on task adaptive pre-trained model selection.Based on NCE (Tran et al., 2019), Nguyen et al. (2020) recently studied the problem when both the pre-train task and the downstream task are classification.They construct an empirical predictor by estimating the joint distribution over the pretrained and target label spaces and take the performance of the empirical predictor (LEEP) to assess pre-trained models.Though being fast, prior methods are not accurate and are specialized for transferring supervised pre-trained models to classification.They cannot apply to either contrastive pre-trained models (He et al., 2020;Chen et al., 2020a), unsupervised pre-trained language models (Devlin et al., 2019;Liu et al., 2019), or regression tasks.\n\nTable 1 shows the applicability of pre-trained model selection methods.Prior to this paper, for most (4 out of 5) transfer learning settings, task adaptive pre-trained model selection does not have a decent solution.\n\nTo provide a general method for pre-trained model selection in various settings, we consider the features extracted by pre-trained models, thus being agnostic to how models are pre-trained.The maximum value of label evidence (marginalized likelihood) given extracted features is calculated, providing a general probabilistic approach that is applicable to both classification and regression tasks.Finally, the logarithm of maximum evidence (LogME) is used to assess pre-trained models for transfer learning.The maximum evidence is less prone to over-fitting (Bishop, 2006), and its humongous computational cost is dramatically reduced by our carefully designed algorithm.\n\nThe contributions of this paper are two-fold:\n\n\u2022 We propose LogME for task adaptive pre-trained model selection, and develop a fast algorithm to accelerate the computation.LogME is easy to interpret and is extremely efficient.It brings at most 3000\u00d7 speedup in wall-clock time and requires just 1% memory footprint, characterizing itself as the first practical method for assessing pre-trained models in various transfer learning settings.\n\n\u2022 We extensively validate the generality and superior performance of LogME on 22 pre-trained models and 17 downstream tasks, covering various pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language).\n\n\nRelated Works\n\n\nTransfer learning\n\nTransfer learning (Thrun & Pratt, 1998) is a broad research area containing transductive transfer, inductive transfer, task transfer learning, and so on.Transductive transfer is commonly known as domain adaptation (Quionero-Candela et al., 2009;Ganin & Lempitsky, 2015;Long et al., 2015), with the focus on eliminating domain shifts between two domains.Inductive transfer, or fine-tuning (Erhan et al., 2010;Yosinski et al., 2014), leverages an inductive bias (a pretrained model) to improve the performance on a target task and is extremely popular in deep learning.In task transfer learning (Zamir et al., 2018), researchers investigate how to transfer between tasks rather than pre-trained models.They aim to discover the relationship among tasks (Ben-David & Schuller, 2003) and to exploit the relationship for further development.In the context of deep learning, transfer learning usually refers to inductive transfer, the topic we are concerned about in this paper.\n\nBesides the na\u00efve fine-tuning where pre-trained models only serve as good initializations, there are sophisticated finetuning techniques like regularization (Li et al., 2018), additional supervision (You et al., 2020), specially designed architecture (Kou et al., 2020), and intermediate-task training which continues to pre-train on an intermediate task (Gururangan et al., 2020;Pruksachatkun et al., 2020;Garg et al., 2020).They can improve transfer learning performance especially when the amount of target data is small, but in general, they do not change the ranking of pre-trained models in downstream tasks.If pre-trained model A is better than pre-trained model B in a task with vanilla fine-tuning, typically A is still better than B when those sophisticated techniques are turned on.For example, on three datasets and four sampling rates from Table 2 in You et al. (2020), better fine-tuning performance mostly indicates better Co-Tuning (their proposed method) performance.Therefore we focus on vanilla fine-tuning rather than these techniques in the rest of the paper, but practitioners are encouraged to adopt them for further improvement after selecting a pre-trained model.\n\n\nPre-trained models\n\nPre-trained models are neural networks trained on largescale datasets and can be transferred to downstream tasks.Popular pre-trained models are reviewed in the following.\n\nSupervised pre-trained models.ImageNet is the most famous dataset for supervised pre-training.In the Ima-geNet classification challenge, He et al. (2015) developed the first deep neural network that surpassed human performance.InceptionNet (Szegedy et al., 2015) is another family of deep neural networks with parallel convolution filters.ResNet (He et al., 2016) introduces skip connections to ease the training and becomes much deeper with better performance.DenseNet (Huang et al., 2017) has carefully designed densely-connected blocks.MobileNet (Sandler et al., 2018) pays attention to mobile-friendly network structures, and the structure can be further optimized by network architecture search (Tan et al., 2019).\n\nContrastive pre-trained models.Although ImageNet pretraining is popular, the labeling cost of ImageNet is very high.Given the large amount of unlabeled data on the Internet, unsupervised pre-training has gained much attention in the past year.By exploiting self-supervised learning (Jing & Tian, 2020) on unlabeled data (Mahajan et al., 2018) with contrastive loss (Gutmann & Hyv\u00e4rinen, 2010), unsupervised contrastive pre-training produces a family of pre-trained models besides supervised pre-trained models.He et al. (2020) proposed Momentum Contrast with a queue structure to fully exploit unlabeled data and obtained representations on par with supervised pre-training in terms of quality.Chen et al. (2020a) greatly improved the performance by exploring data augmentation, multi-layer projection head and many empirical design choices.How to design better contrastive pre-training strategies is still under active research (Tian et al., 2020).\n\nPre-trained language models.In the language community, unsupervised pre-training has been well established by training masked language models (Devlin et al., 2019) or autoregressive language models (Yang et al., 2019) on a large unlabeled corpus.Liu et al. (2019) explored many practical details on how to improve the training of these models.Because pre-trained language models are very large, Sanh et al. (2019) proposed distillation to get smaller and faster models.These pre-trained language models become an indispensable component in winning submissions on common benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and have profound industrial influence.\n\nPre-trained models are hosted in model zoos like TorchVision and HuggingFace.There are so many pre-trained models, but no one can overwhelmingly outperform the rest in all downstream tasks.The best model for a downstream task depends on the characteristic of both the task and the pre-trained model, thus being task adaptive.Practitioners can have a hard time choosing which pre-trained model to use for transfer learning, calling for a practical method to assess pre-trained models without brute-force fine-tuning.\n\n\nAssessing transferablitiy of pre-trained models\n\nAssessing transferability of pre-trained models has a great significance to guide common practice.Yosinski et al. (2014) studied which layer of a pre-trained model can be transferred while Kornblith et al. (2019) studied a wide variety of modern pre-trained models in computer vision.These papers aim for a deeper understanding of transfer learning (Neyshabur et al., 2020).Nonetheless, they draw conclusions by expensive and exhaustive fine-tuning with humongous computation cost (Section 5.5) which is hard for practitioners to afford.\n\nTo efficiently assess the transferability of pre-trained models, Nguyen et al. (2020) pioneered to develop LEEP with a focus on supervised pre-trained models transferred to classification tasks.The joint distribution over pre-trained labels and the target labels is estimated to construct an empirical predictor.The log expectation of the empirical predictor (LEEP) is used as a transferability measure.The LEEP method is closely related to Negative Conditional Entropy (NCE) proposed by Tran et al. (2019), an informationtheoretic quantity (Cover, 1999) to study the transferability and hardness between classification tasks.\n\nLEEP (Nguyen et al., 2020) and NCE (Tran et al., 2019), the only two prior methods for pre-trained model selection, shed light on this problem but leave plenty of room for further performance improvement.In addition, they can only handle classification tasks with supervised pre-trained models.Since contrastive pre-training and language modeling tasks do not have categorical labels, prior methods cannot deal with these increasingly popular models.To promote pre-trained model selection, we propose LogME which is broadly applicable to various pre-trained models, downstream tasks, and even data modalities.\n\n\nProblem setup\n\nIn task adaptive pre-trained model selection, we are given M pre-trained models {\u03c6 m } M m=1 and a target dataset D = {(x i , y i )} n i=1 with n labeled data points.The dataset has an evaluation metric (accuracy, MAP, MSE etc.) to measure the ground-truth transfer performance T m of fine-tuning \u03c6 m with proper hyper-parameter tuning.A practical assessment method should produce a score S m for each pre-trained model \u03c6 m (ideally without fine-tuning \u03c6 m on D), and the scores {S m } M m=1 should well correlate with {T m } M m=1 so that top performing pre-trained models can be selected by simply evaluating the scores.\n\nHow to measure the performance of pre-trained model assessing methods.A perfect pre-trained model assessing method would output {S m } M m=1 with exactly the same order as {T m } M m=1 .To measure the deviation from the perfect method, we can use simple metrics like top-1 accuracy or top-k accuracy (whether top-k in {S m } M m=1 are also topk in {T m } M m=1 ).But top-1 accuracy is too conservative and top-k accuracy is not comparable across different val-ues of M .Therefore we turn to rank correlation (Fagin et al., 2003) to directly measure the correlation between {S m } M m=1 and {T m } M m=1 .The prior work (Nguyen et al., 2020) adopted Pearson's linear correlation coefficient, but neither Pearson's linear correlation nor its variant (Spearman's rank correlation) has a simple interpretation (see the interpretation of \u03c4 below).\n\nSince the purpose of assessment is to choose a good pretrained model, we hope T i is better than T j if S i is better than S j , which can be well captured by Kendall's \u03c4 coefficient (Kendall, 1938) as described in the following.\n\nTo simplify the discussion, assume larger value of transfer performance T and score S are preferred (e.g.accuracy).If this is not the case (e.g.transfer performance is measured by mean square error), the negation can be considered.For a pair of measures (T i , S i ) and (T j , S j ), the pair is concordant if T i < T j \u2227 S i < S j or T i > T j \u2227 S i > S j (concisely speaking, sgn(T i \u2212 T j )sgn(S i \u2212 S j ) = 1).The Kendall's \u03c4 coefficient is defined by the following equation, which enumerates all M 2 pairs and counts the number of concordant pairs minus the number of discordant pairs.\n\u03c4 = 2 M (M \u2212 1) 1\u2264i<j\u2264M sgn(T i \u2212 T j )sgn(S i \u2212 S j )\nHow to interpret \u03c4 (Fagin et al., 2003).The range of \u03c4 is [\u22121, 1].\u03c4 = 1 means T and S are perfectly correlated (S i > S j \u21d0\u21d2 T i > T j ), and \u03c4 = \u22121 means T and S are reversely correlated (S i > S j \u21d0\u21d2 T i < T j ).If T and S have correlation of \u03c4 , the probability of T i > T j is \u03c4 +1 2 when S i > S j .\n\nPay attention to top performing models.Since a major application of assessing pre-trained models is to select top performing pre-trained models, discordant / concordant pairs should be weighted more if T i , T j , S i , S j are larger.This can be taken care of by \u03c4 w (Vigna, 2015).The details of calculating \u03c4 w can be found in implementation from SciPy (Virtanen et al., 2020).\n\nIn short, we measure the correlation between {S m } M m=1 and {T m } M m=1 by the weighted variant \u03c4 w (Vigna, 2015).Larger \u03c4 w indicates better correlation and better assessment.\n\nNote that how to measure the performance of pre-trained model assessing methods is neither the focus nor the claimed novelty of this paper.We use weighted Kendall's \u03c4 because it is easy to interpret, but any proper rank correlation metric (such as Pearson's linear correlation and Spearman's rank correlation) can be adopted and should yield similar conclusions on superiority of our proposed method.\n\n\nThe LogME approach\n\nFor each pre-trained model \u03c6 m , the algorithm should produce a score S m independent from the rest of pre-trained models.We thus drop the subscript m in this section.\n\nTo be fast, we try to avoid gradient optimization.The pretrained model \u03c6 serves as a fixed feature extractor.Features {f i = \u03c6(x i )} n i=1 and labels {y i } n i=1 are used to assess pre-trained models.Note that Nguyen et al. ( 2020) used a pre-trained classification head h besides the pre-trained representation model \u03c6, limiting their method to supervised pre-trained models.In contrast, we only use the pre-trained representation model \u03c6 so that the proposed method can be applied to any pre-trained model (whether supervised pre-trained or unsupervised pre-trained).\n\nWithout gradient optimization, the problem is cast into estimating the compatibility of features {f i = \u03c6(x i )} n i=1 and labels {y i } n i=1 , which is discussed in the rest of this section.\n\n\nEvidence calculation\n\nWe first consider a simple case, with features f i \u2208 R D and scalar labels y i \u2208 R. The feature matrix F \u2208 R n\u00d7D contains all the features and y \u2208 R n denotes all the labels.\n\nA direct measurement of the compatibility between features F and labels y is the probability density p(y|F ), which is intractable without a parametrized model.Since the rule-ofthumb transfer learning practice is to add a fully-connected layer on top of the pre-trained model, we use a linear model upon features parametrized by w.\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" W 4 N s Z 3 U d M d 3 J S q H J 0 n b S Z y X s A D w = \" > A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R 9 R j 0 4 j G C e U C y h N 7 J b D J m d m a Z m R V C y D 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F q e D G + v 6 3 t 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 4 b R m W a s j p V Q u l W h I Y J L l n d c i t Y K 9 U M k 0 i w Z j S 8 n f r N J 6 Y N V / L B j l I W J t i X P O Y U r Z M a H R T p A L u l s l / x Z y D L J M h J G X L U u q W v T k / R L G H S U o H G t A M / t e E Y t e V U s E m x k x m W I h 1 i n 7 U d l Z g w E 4 5 n 1 0 7 I q V N 6 J F b a l b R k p v 6 e G G N i z C i J X G e C d m A W v a n 4 n 9 f O b H w d j r l M M 8 s k n S + K M 0 G s I t P X S Y 9 r R q 0 Y O Y J U c 3 c r o Q P U S K 0 L q O h C C B Z f X i a N 8 0 p w W f H v L 8 r V m z y O A h z D C Z x B A F d Q h T u o Q R 0 o P M I z v M K b p 7 w X 7 9 3 7 m L e u e P n M E f y B 9 / k D j S e P H Q = = < / l a t e x i t > \u21b5 < l a t e x i t s h a 1 _ b a s e 6 4 = \" Z O A R U C S 6 W T M k f n w p Q 7 R 3 1 h f C N O k = \" > A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I e i w K 4 r E F + w F t K J v t p F 2 7 2 Y T d j V B C f 4 E X D 4 p 4 9 S d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X J I J r 4 7 r f z s r q 2 v r G Z m G r u L 2 z u 7 d f O j h s 6 j h V D B s s F r F q B 1 S j 4 B I b h h u B 7 U Q h j Q K B r W B 0 O / V b T 6 g 0 j + W D G S f o R 3 Q g e c g Z N V a q 3 / V K Z b f i z k C W i Z e T M u S o 9 U p f 3 X 7 M 0 g i l Y Y J q 3 f H c x P g Z V Y Y z g Z N i N 9 W Y U D a i A + x Y K m m E 2 s 9 m h 0 7 I q V X 6 J I y V L W n I T P 0 9 k d F I 6 3 E U 2 M 6 I m q F e 9 K b i f 1 4 n N e G 1 n 3 G Z p A Y l m y 8 K U 0 F M T K Z f k z 5 X y I w Y W 0 K Z 4 v Z W w o Z U U W Z s N k U b g r f 4 8 j J p n l e 8 y 4 p b v y h X b / I 4 C n A M J 3 A G H l x B F e 6 h B g 1 g g P A M r / D m P D o v z r v z M W 9 d c f K Z I / g D 5 / M H n B e M z w = = < / l a t e x i t > F < l a t e x i t s h a 1 _ b a s e 6 4 = \" b G D T O K y V 7 G D R Z r D o b 8 G d o O 8 h X f o = \" > A A A C E n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V o Q U s i o i 6 L b l x J h b 6 g S c N k O m m H T h 7 M T J Q Q 8 g 1 u / B U 3 L h R x 6 8 q d f + O k 7 U J b D 1 w 4 n H M v 9 9 7 j R o w K a R j f W m F p e W V 1 r b h e 2 t j c 2 t 7 R d / f a I o w 5 J i 0 c s p B 3 X S Q I o w F p S S o Z 6 U a c I N 9 l p O O O r 3 O / c 0 + 4 o G H Q l E l E b B 8 N A + p R j K S S H L 2 a O B R a g v r Q 8 p E c Y c T S 2 6 z y 0 G 9 C z 6 H H 0 H K J R P 3 0 x M y q j l 4 2 a s Y E c J G Y M 1 I G M z Q c / c s a h D j 2 S S A x Q 0 L 0 T C O S d o q 4 p J i R r G T F g k Q I j 9 G Q 9 B Q N k E + E n U 5 e y u C R U g b Q C 7 m q Q M K J + n s i R b 4 Q i e + q z v x u M e / l 4 n 9 e L 5 b e p Z 3 S I I o l C f B 0 k R c z K E O Y 5 w M H l B M s W a I I w p y q W y E e I Y 6 w V C m W V A j m / M u L p H 1 a M 8 9 r x t 1 Z u X 4 1 i 6 M I D s A h q A A T X I A 6 u A E N 0 A I Y P I J n 8 A r e t C f t R X v X P q a t B W 0 2 s w / + Q P v 8 A W 4 m n K w = < / l a t e x i t > y i \u21e0 N (w T f i , 1 ) < l a t e x i t s h a 1 _ b a s e 6 4 = \" B 7 f U q C A d l Q p 1 h / X 7 N S f 6 f q 9 t v I A = \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c K p i 2 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 k m m G P o s E Y n q h F S j 4 B J 9 w 4 3 A T q q Q x q H A d j i + m / n t J 1 S a J / L R T F I M Y j q U P O K M G i v 5 v R A N 7 V d r b t 2 d g 6 w S r y A 1 K N D s V 7 9 6 g 4 R l M U r D B N W 6 6 7 m p C X K q D G c C p 5 V e p j G l b E y H 2 L V U 0 h h 1 k M + P n Z I z q w x I l C h b 0 p C 5 + n s i p 7 H W k z i 0 n T E 1 I 7 3 s z c T / v G 5 m o p s g 5 z L N D E q 2 W B R l g p i E z D 4 n A 6 6 Q G T G x h D L F 7 a 2 E j a i i z N h 8 K j Y E b / n l V d K 6 q H t X d f fJ Y u o H V f X 9 m v I = \" > A A A B 6 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o m I e i x 6 8 d i C / Y A 2 l M 1 2 0 q 7 d b M L u R i m h v 8 C L B 0 W 8 + p O 8 + W / c t j l o 6 4 O B x 3 s z z M w L E s G 1 c d 1 v Z 2 V 1 b X 1 j s 7 B V 3 N 7 Z 3 d s v H R w 2 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n f q t R 1 S a x / L e j B P 0 I z q Q P O S M G i v V n 3 q l s l t x Z y D L x M t J G X L U e q W v b j 9 m a Y T S M E G 1 7 n h u Y v y M K s O Z w E m x m 2 p M K B v R A X Y s l T R C 7 W e z Q y f k 1 C p 9 E s b K l j R k p v 6 e y G i k 9 T g K b G d E z V A v e l P x P 6 + T m v D a z 7 h M U o O S z R e F q S A m J t O v S Z 8 r Z E a M L a F M c X s r Y U O q K D M 2 m 6 I N w V t 8 e Z k 0 z y v e Z c W t X 5 S r N 3 k c B T i G E z g D D 6 6 g C n d Q g w Y w Q H i G V 3 h z H p w X 5 9 3 5 m L e u O P n M E f y B 8 / k D 5 l u N A A = = < / l a t e x i t > w < l a t e x i t s h a 1 _ b a s e 6 4 = \" k q a W h W k G p N + G G G d 2 t v e N u 9 R 1 h T 0 = \" > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I o / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H k y X o R 3 Q o e c g Z N V Z q Z P 1 y x a 2 6 c 5 B V 4 u W k A j n q / f J X b x C z N E J p m K B a d z 0 3 M f 6 E K s O Z w G m p l 2 p M K B v T I X Y t l T R C 7 U / m h 0 7 J m V U G J I y V L W n I X P 0 9 M a G R 1 l k U 2 M 6 I m p F e 9 m b i f 1 4 3 N e G N P + E y S Q 1 K t l g U p o K Y m M y + J g O u k B m R W U K Z 4 v Z W w k Z U U W Z s N i U b g r f 8 8 i p p X V S 9 q 6 r b u K z U b v M 4 i n A C p 3 A O H l x D D e 6 h D k 1 g g P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D 6 W O N A g = = < / l a t e x i t > y f 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" y X R D 1 n N z v O E X T 3 Q L / r 3 a n 1 h 8 l P Y = \" > A A A B 6 n i c b V D L S g N B E O y N r x h f U Y + K D A b B U 9 g V R Y 9 B L x 4 T N A 9 I l j A 7 m U 2 G z M w u M 7 N C W H L 0 6 M W D I l 7 9 i H y H N 7 / B n 3 D y O G h i Q U N R 1 U 1 3 V x B z p o 3 r f j m Z p e W V 1 b X s e m 5 j c 2 t 7 J 7 + 7 V 9 N R o g i t k o h H q h F g T T m T t G q Y 4 b Q R K 4 p F w G k 9 6 N + M / f o D V Z p F 8 t 4 M Y u o L 3 J U s Z A Q b K 9 2 F b a + d L 7 h F d w K 0 S L w Z K Z Q O R 5 X v x 6 N R u Z 3 / b H U i k g g q D e F Y 6 6 b n x s Z P s T K M c D r M t R J N Y 0 z 6 u E u b l k o s q P b T y a l D d G K V D g o j Z U s a N F F / T 6 R Y a D 0 Q g e 0 U 2 P T 0 v D c W / / O a i Q m v / J T J O D F U k u m i M O H I R G j 8 N + o w R Y n h A 0 s w U c z e i k g P K 0 y M T S d n Q / D m X 1 4 k t b O i d 1 6 8 q N g 0 r m G K L B z A M Z y C B 5 d Q g l s o Q x U I d O E J X u D V 4 c 6 z 8 + a 8 T 1 s z z m x m H / 7 A + f g B 3 8 y R P Q = = < / l a t e x i t > f i < l a t e x i t s h a 1 _ b a s e 6 4 = \" k u 6 6 B b F S g T N H N i M 6 G n r H v 8 z P K F M = \" > A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 l G R w S B 4 C r u i 6 D H o x W M C 5 g H J E m Y n s 8 m Q m d l l Z l Y I S 4 5 e v X h Q x K v / k O / w 5 j f 4 E 8 4 m O W h i Q U N R 1 U 1 3 V x B z p o 3 r f j l L y y u r a + u 5 j f z m 1 v b O b m F v v 6 6 j R B F a I x G P V D P A m n I m a c 0 w w 2 k z V h S L g N N G M L j N / M Y D V Z p F 8 t 4 M Y + o L 3 J M s Z A S b T A o 7 L N 8 p F N 2 S O w F a J N 6 M F M t H 4 + r 3 4 / G 4 0 i l 8 t r s R S Q S V h n C s d c t z Y + O n W B l G O B 3 l 2 4 m m M S Y D 3 K M t S y U W V P v p 5 N Y R O r V K F 4 W R s i U N m q i / J 1 I s t B 6 K w H Y K b P p 6 3 s v E / 7 x W Y s J r P 2 U y T g y V Z L o o T D g y E c o e R 1 2 m K D F 8 a A k m i t l b E e l j h Y m x 8 W Q h e P M v L 5 L 6 e c m 7 K F 1 W b R o 3 M E U O D u E E z s C D K y j D H V S g B g T 6 8 A Q v 8 O o I 5 9 l 5 c 9 6 n r U v O b O Y A / s D 5 + A F p + Z G J < / l a t e x i t > y i\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" / q 3 y w K t y q r X 0      U 7 E j S E M g Q + r S t q Q 8 e l U 4 8 P i b B O 1 r p 4 l 4 g 9 P M V H q s / J 2 L w p B x 5 r k 6 m O 8 t p L x X / 8 9 q R 6 p 0 6 M f P D S F G f T D 7 q R R y r A K f N 4 C 4 T l C g + 0 g S I Y H p X T A Y g g C j d X 0 G X Y E 2 f / J c 0 D i r W c e X w + q h U P c v q y K M t t I 3 K y E I n q I o u U A 3 V E U E P 6 A m 9 o F f j 0 X g 2 3 o z 3 S T R n Z D O b 6 B e M j 2 + i C Z o V < / l a t e x i t > w \u21e0 N (0, \u21b5 1 I)\nY d j + R n x 0 X M H 2 G K c = \" > A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 l G R w S B 4 C r u i 6 D H o x W M C 5 g H J E m Y n s 8 m Q m d l l Z l Y I S 4 5 e v X h Q x K v / k O / w 5 j f 4 E 8 4 m O W h i Q U N R 1 U 1 3 V x B z p o 3 r f j l L y y u r a + u 5 j f z m 1 v b O b m F v v 6 6 j R B F a I x G P V D P A m n I m a c 0 w w 2 k z V h S L g N N G M L j N / M Y D V Z p F 8 t 4 M Y + o L 3 J M s Z A S b T B p 2 W L 5 T K L o l d w K 0 S L w Z K Z a P x t X v x + N x p V P 4 b H c j k g g q D e F Y 6 5 b n x s Z P s T K M c D r K t x N N Y 0 w G u E d b l k o s q P b T y a 0 j d G q V L g o j Z U s a N F F / T 6 R Y a D 0 U g e 0 U 2 P T 1 v J e J / 3 m t x I T X f s p k n B g q y X R R m H B k I p Q 9 j r p M U W L 4 0 B J M F L O 3 I t L H C h N j 4 8 l C 8 O Z f X i T 1 8 5 J 3 U b q s 2 j R u Y I o c H M I J n I E H V 1 C G O 6 h A D Q j 0 4 Q l e 4 N Uk z V h S L g N N G M L j N / M Y D V Z p F 8 t 4 M Y + o L 3 J M s Z A S b T B p 2 v H y n U H R L 7 g R o k X g z U i w f j a v f j 8 f j S q f w 2 e 5 G J B F U G s K x 1 i 3 P j Y 2 f Y m U Y 4 X S U b y e a x p g M c I + 2 L J V Y U O 2 n k 1 t H 6 N Q q X R R G y p Y 0 a K L + n k i x 0 H o o A t s p s O n r e S 8 T / / N a i Q m v / Z T J O D F U k u m i M O H I R C h 7 H H W Z o s T w o S W Y K G Z v R a S P F S b G x p O F 4 M 2 / v E j q 5 y X v o n R Z t W n c w B Q 5 O I Q T O A M P r q A M d 1 C B G h D o w x O 8 w K s j n G f n z X m f t iF 6 x C z h f k S H S o S C U b T S Q 9 Z X / X L F r b p z k F X i 5 a Q C O R r 9 8 l d v E L M 0 4 g q Z p M Z 0 P T d B f 0 I 1 C i b 5 t N R L D U 8 o G 9 M h 7 1 q q a M S N P 5 m f O i V n V h m Q M N a 2 F J K 5 + n t i Q i N j s i i w n R H F k V n 2 Z u J / X j f F 8 N q f C J W k y B V b L A p T S T A m s 7 / J Q G j O U G a W U K a F v Z W w E d W U o U 2 n Z E P w l l 9 e J a 2 L q l e r X t 7 X K v W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R= \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 E P Z l v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k t Z F 1 a t V L + 9 r l f p N H k c R T u A U z s G D K 6 j D H T S g C Q y G 8 A y v 8 O Y I 5 8 V 5 d z 4 W r Q U nL 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / lL 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t > ... < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t > ... < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 8 L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M 4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x\nFigure 2. The directed graphical model for calculating evidence.\n\nA na\u00efve approach to deal with the linear model is to find the best w * by logistic / linear regression and to assess pretrained models by likelihood p(y|F, w * ).However, it is well-known that likelihood is prone to over-fitting (Bishop, 2006), which is experimentally observed in Supplementary B. A better approach is to use the evidence (marginalized likelihood) p(y|F ) = p(w)p(y|F, w)dw, which integrates over all possible values of w and is better than simply using one optimal value w * .This evidence-based approach is an elegant model selection approach and has a rigorous theoretical foundation (Knuth et al., 2015).For p(w) and p(y|F, w), we use the commonly adopted graphical model (Figure 2) specified by two positive parameters \u03b1 and \u03b2: the prior distribution of the weight is an isotropic multivariate Gaussian w \u223c N (0, \u03b1 \u22121 I), and the distribution of each observation is a one-dimensional normal distribution p(y i |f i , w, \u03b2) = N (y i |w T f i , \u03b2 \u22121 ).\n\nAccording to the causal structure in Figure 2 and the basic principles in graphical models (Koller & Friedman, 2009), the evidence can be calculated analytically as Eq. 1.\n\np(y|F, \u03b1, \u03b2) = p(w|\u03b1)p(y|F, w, \u03b2)dw\n= p(w|\u03b1) n i=1 p(y i |f i , w, \u03b2)dw = ( \u03b2 2\u03c0 ) n 2 ( \u03b1 2\u03c0 ) D 2 e \u2212 \u03b1 2 w T w\u2212 \u03b2 2 ||F w\u2212y|| 2 dw (1) As e \u2212 1 2 (w T Aw+b T w+c) dw = (2\u03c0) D |A| e \u2212 1 2 c+ 1 8 b T A \u22121 b\nwhen A is positive definite, Eq. 1 can be simplified.By taking the logarithm to make the equation simple, Eq. 2 shows the logarithm of the evidence as a function of \u03b1, \u03b2, where\nA = \u03b1I + \u03b2F T F, m = \u03b2A \u22121 F T y. L(\u03b1, \u03b2) = log p(y|F, \u03b1, \u03b2) = n 2 log \u03b2 + D 2 log \u03b1 \u2212 n 2 log 2\u03c0 \u2212 \u03b2 2 ||F m \u2212 y|| 2 2 \u2212 \u03b1 2 m T m \u2212 1 2 log |A| (2)\n\nEvidence maximization and LogME\n\nA remaining issue of Eq. 2 is how to determine \u03b1, \u03b2.Gull (1989) suggested that we should choose \u03b1, \u03b2 to maximize the evidence, i.e. use (\u03b1 * , \u03b2 * ) = arg max \u03b1,\u03b2 L(\u03b1, \u03b2).Because m and A are coupled, maximizing L(\u03b1, \u03b2) is generally a difficult problem.However, this form of maximization can be achieved by alternating between evaluating m, \u03b3 and maximizing \u03b1, \u03b2 with m, \u03b3 fixed (Gull, 1989), resulting the following formula, where \u03c3 i 's are singular values of F T F .\nA = \u03b1I + \u03b2F T F, m = \u03b2A \u22121 F T y, \u03b3 = D i=1 \u03b2\u03c3 i \u03b1 + \u03b2\u03c3 i \u03b1 \u2190 \u03b3 m T m , \u03b2 \u2190 n \u2212 \u03b3 ||F m \u2212 y|| 2 2\nWhen the fixed-point iteration converges (empirically it converges with no more than three iterations), the logarithm maximum evidence L(\u03b1 * , \u03b2 * ) is used to evaluate the compatibility between features and labels.Because L(\u03b1 * , \u03b2 * ) scales linearly with n, we normalize it by L(\u03b1 * ,\u03b2 * ) n and term it LogME (logarithm of of maximum evidence).It can be intuitively interpreted as the average maximum log evidence of labels given the pre-trained features.\n\nExtending LogME to complex cases.The LogME approach described above starts from a single-target regression.If the target problem is a multivariate-regression task, i.e. y \u2208 R n\u00d7K , we can calculate LogME for each dimension k (1 \u2264 k \u2264 K) and average them over the K dimension.If the target problem is a classification task with K classes, Eq. 1 cannot be calculated analytically (Daunizeau, 2017) with a categorical prior distribution, but we can convert the labels to one-hot labels and treat the problem as multivariate regression.Therefore, LogME can be used in both classification and regression tasks.The overall algorithm of LogME is described in Algorithm 1.\nAlgorithm 1 LogME 1: Input: Pre-trained model \u03c6 Target dataset D = {(x i , y i )} n i=1\n2: Output: logarithm of maximum evidence (LogME)\n\n3: Extract features using pre-trained model \u03c6:\nF \u2208 R n\u00d7D , f i = \u03c6(x i ), Y \u2208 R n\u00d7K 4: Compute SVD F T F = V diag{\u03c3}V T 5: for k = 1 to K do 6: Let y = Y (k) \u2208 R n , initialize \u03b1 = 1, \u03b2 = 1 7: while \u03b1, \u03b2 not converge do 8: Compute \u03b3 = D i=1 \u03b2\u03c3i \u03b1+\u03b2\u03c3i , \u039b = diag{(\u03b1 + \u03b2\u03c3)} 9: Na\u00efve: A = \u03b1I + \u03b2F T F, m = \u03b2A \u22121 F T y 10: Optimized: m = \u03b2(V (\u039b \u22121 (V T (F T y)))) 11: Update \u03b1 \u2190 \u03b3 m T m , \u03b2 \u2190 n\u2212\u03b3 ||F m\u2212y|| 2 2 12:\nend while 13:\n\nCompute L k = 1 n L(\u03b1, \u03b2) using Eq. 2 14: end for 15: Return LogME 1 K K k=1 L k\n\n\nComputational speedup\n\nAlthough the Bayesian approach of maximum evidence has many nice properties (Knuth et al., 2015), it inherits the common drawback of Bayesian methods with high computational complexity.The na\u00efve implementation of Algorithm 1 has a complexity of O(KD 3 + nKD 2 ).For typical usage with D \u2248 10 3 , n \u2248 10 4 , K \u2248 10 3 , the computational cost is 10 13 , making the wall-clock time comparable to fine-tuning the pre-trained model \u03c6.\n\nNotice that the most expensive operations are Line 9 with matrix inversion A \u22121 and matrix multiplication A \u22121 F T .These expensive operations, however, can be avoided by exploiting the decomposition of F T F , which is readily accessible from Line 4.\n\nTo avoid matrix inversion A \u22121 , we exploit the decomposition F T F = V diag{\u03c3}V T (V is an orthogonal matrix).Let \u039b = diag{(\u03b1+\u03b2\u03c3)}, then A = \u03b1I +\u03b2F T F = V \u039bV T , and A \u22121 = V \u039b \u22121 V T .To avoid the matrix-matrix multiplication A \u22121 F T , we notice that y is a column vector and the associative law admits a fast computation A \u22121 F T y = (V (\u039b \u22121 (V T (F T y)))).In each for-loop, we only need to update \u039b rather than the expensive A \u22121 .In this way, all matrix-matrix multiplications are reduced to matrix-vector product, and the matrix inversion is avoided, as described in Line 10. Table 2 analyzes the complexity in detail.The optimized algorithm makes a time-consuming Bayesian approach fast enough, reducing the wall-clock time by the order of 10 2 (see Section 5.5).\nO(D 3 + nD 2 ) O(KD 3 + nKD 2 ) optimized O(D 2 + nD) O(KD 2 + nKD + D 3 + nD 2 )\nThe proposed LogME is easy to interpret, has a solid theoretical foundation, and is applicable to various settings.Its computational cost is dramatically reduced by our optimized implementation.\n\n\nExperiments\n\nWe first present the illustration of LogME on toy problems, and then focus on task adaptive pre-trained model selection.\n\nOriginal data are available in Supplementary C.\n\nIllustration with toy data.To give readers an intuitive sense of how LogME works, we generate features with increasing noise to mimic the features extracted by pretrained models with decreasing transferability and to check if LogME can measure the quality of features.For regression (Figure 3 bottom), x is uniformly distributed and the output y = 2x + with observation error \u223c N (0, 0.1 2 ).By adding noise to the feature x = x + N (0, t 2 ), the quality of feature x becomes worse and it is harder to predict y from x .With larger t (the standard deviation of noise), LogME becomes smaller as expected.\n\nThese toy experiments on synthesized data shows that LogME is a good measure of the feature quality, and therefore can provide a general assessment of pre-trained models for transfer learning.\n\n\nTransferring supervised pre-trained models to classification tasks\n\nWe use 10 ImageNet pre-trained models available from PyTorch: Inception V1 (Szegedy et al., 2015), Inception V3 (Szegedy et al., 2016), ResNet 50 (He et al., 2016), ResNet 101 (He et al., 2016), ResNet 152 (He et al., 2016), DenseNet 121 (Huang et al., 2017), DenseNet 169 (Huang et al., 2017), DenseNet 201 (Huang et al., 2017), Mo-bileNet V2 (Sandler et al., 2018), and NASNet-A Mobile (Tan et al., 2019).These pre-trained models cover most of the supervised pre-trained models in transfer learning that practitioners frequently use.\n\nFor downstream classification tasks, we take 9 commonly used datasets: Aircraft (Maji et al., 2013), Birdsnap (Berg et al., 2014), Caltech (Fei-Fei et al., 2004), Cars (Krause et al., 2013), CIFAR10 (Krizhevsky & Hinton, 2009), CI-FAR100 (Krizhevsky & Hinton, 2009), DTD (Cimpoi et al., 2014), Pets (Parkhi et al., 2012), and SUN (Xiao et al., 2010).Due to space limit, we leave the description of each dataset and data statistics in Supplementary A.\n\nTo compute the value of transfer performance {T m } M m=1 (M = 10), we carefully fine-tune pre-trained models with grid-search of hyper-parameters.As pointed out by Li et al. (2020), learning rates and weight decays are the two most important hyper-parameters.Hence we grid search learning rates and weight decays (7 learning rates from 10 \u22121 to 10 \u22124 , 7 weight decays from 10 \u22126 to 10 \u22123 , all logarithmically spaced) to select the best hyper-parameter on the validation set and compute the accuracy on the test set.It is noteworthy that LogME requires neither fine-tuning nor grid search.\n\nHere we fine-tune pre-trained models to evaluate LogME itself, but practitioners can straightforwardly use LogME to evaluate pre-trained models without fine-tuning.\n\nWe compare LogME against LEEP (Nguyen et al., 2020) and NCE (Tran et al., 2019).Prior to this paper, LEEP    4. Correlation (\u03c4w) between fine-tuned accuracy (X-axis) and three methods for pre-trained model selection on 9 datasets with 10 pre-trained models.One row for each method, one column for each dataset (with \u03c4w in the bracket near the dataset name), and one marker for each pre-trained model.The best \u03c4w in each dataset is marked in bold.\n\nand NCE are the only two methods for pre-trained model selection without fine-tuning, and they are dedicated to transferring supervised pre-trained models to classification tasks.We use LEEP, NCE and LogME to compute scores {S m } M m=1 by applying 10 pre-trained models to the datasets.The correlation \u03c4 w between scores and fine-tuned accuracies are presented in Figure 4.\n\nWe can find that LogME has consistently better correlation than LEEP, and outperforms NCE on most datasets (7 datasets out of 9 datasets).Note that LEEP and NCE even show a negative correlation in DTD (Cimpoi et al., 2014), because they rely on the relationship between classes of the pre-trained task and the target task while DTD classes are very different from ImageNet categories.In contrast, LogME still performs reasonably well for DTD.\n\nThe smallest \u03c4 w of LogME in Figure 4 is around 0.5, so the probability of a pre-trained model \u03c6 1 transferring better than \u03c6 2 is least 75% if \u03c6 1 has a larger LogME.For most tasks \u03c4 w of LogME is 0.7 or 0.8, so the probability of correct selection is 85% or 90%, sufficient for practical usage.\n\n\nTransferring supervised pre-trained models to a regression task\n\nBesides extensive classification tasks considered above, this section shows how LogME can be used to assess pre-trained models for a regression task, while prior methods (LEEP and NCE) cannot.\n\nThe regression task we use is dSprites (Matthey et al., 2017) from VTAB (Zhai et al., 2020)\n\n\nTransferring contrastive pre-trained models to downstream tasks\n\nThe recently emerging unsupervised pre-trained models (He et al., 2020) have a projection head with continuous output.However, LEEP and NCE cannot be extended to deal with the projection head of contrastive-based unsupervised pretrained models because they rely on the relationship between pre-training categories and target categories.6. Correlation (\u03c4w) between fine-tuned accuracy (X-axis) and LogME in 7 GLUE tasks with 8 popular pre-trained language models.One column for each task (with \u03c4w in the bracket near the task name), and one marker for each pre-trained model.\n\nSince LogME only requires features extracted from pretrained models, it can be applied to contrastive pre-trained models.To demonstrate this, we use three popular models pre-trained with various training scheme: MoCo V1 (He et al., 2020) with momentum contrast, MoCo V2 (Chen et al., 2020b) with an MLP projection head and strong data augmentation, MoCo 800 trained with 800 epochs as suggested by Chen et al. (2020a), and SimCLR (Chen et al., 2020a) with carefully designed implementation.\n\nAircraft (Maji et al., 2013), the first dataset (alphabetically) in Section 5.1 is used as the classification task, and dSprites (Matthey et al., 2017) is used as the regression task.\n\nResults are shown in Table 3. SimCLR on dSprites is not reported because it does not converge after several trials.LogME gives the perfect order of both transferred accuracy and MSE.Note that the order in Aircraft (MoCo V1 < MoCo V2 < MoCo 800) is different from the order in dSprites (MoCo V1 < MoCo 800 < MoCo V2), so the transfer learning performance depends on both the pre-trained model and the target data, emphasizing the importance of task adaptive pre-trained model selection.We also observe that LogME values of unsupervised pre-trained models are similar, mainly because unsupervised features are not very discriminative.Here we take an alternative approach of evaluating the transfer performance {T m } M m=1 .We do not fine-tune pre-trained models ourselves, but directly use accuracies tuned by others, and check if LogME can correlate well with the results.The HuggingFace Model Hub generously provides lots of pre-trained language models and even provides carefully tuned transfer learning results in some GLUE (Wang et al., 2018) tasks for some models.We take out pre-trained models that have GLUE performance tuned by the Hugging-Face organization, and select the top 8 downloaded models: RoBERTa (Liu et al., 2019), RoBERTa-D, uncased BERT-D, cased BERT-D, ALBERT-v1 (Lan et al., 2020), ALBERT-v2 (Lan et al., 2020), ELECTRA-base (Clark et al., 2020), and ELECTRA-small (Clark et al., 2020) (\"D\" means distilled version).The LogME on seven GLUE classification tasks together with fine-tuned accuracy are plotted in Figure 6.models only have results for certain tasks and we keep them as they are.Even though these accuracy numbers are tuned by the HuggingFace organization, LogME perfectly estimates the ranking of transfer performance for 3 tasks (with \u03c4 w = 1), showing the surprising effectiveness of LogME in pre-trained model selection.\n\n\nEfficiency of LogME\n\nLogME is a practical method to assess pre-trained models for transfer learning because it is general, accurate, and efficient.Section 4 shows the generality of LogME by considering features and labels in the general form.Results in this section validates the strong correlation between LogME and ground-truth transfer learning performance, demonstrating that LogME is accurate.Next we quantitatively measure the efficiency of LogME compared to brute-force fine-tuning.The algorithmic complexity is presented in Section 4.3, thus we focus on wall-clock time and memory footprint here.\n\nResults are shown in Table 4. ResNet 50 on Aircraft is used for computer vision, and RoBERTa-D on MNLI task is used for NLP.Both wall-clock time and memory footprint is reported.The cost of computing ground-truth transferability T m (fine-tuning with hyper-parameter search) serves as the upper bound of pre-trained model assessment.We also list the cost of extracting features by pre-trained models as a reference, which is the lower bound of pre-trained model as-\n\n\nConclusion\n\nA fast, accurate, and general assessment of pre-trained models for transfer learning has great practical significance.This paper takes a probabilistic approach and proposes logarithm of maximum evidence (LogME) to tackle the task adaptive pre-trained model selection problem.The expensive computation of maximizing the marginalized likelihood is optimized by careful implementation, leading to over 3000\u00d7 speedup compared to vanilla fine-tuning.LogME is applicable to vast transfer learning settings with supervised pre-trained models and unsupervised pre-trained models, downstream classification and regression tasks, vision and language modalities.The impressive generality of LogME and its substantially better performance over prior methods can be interesting to many practitioners.\n\nThis paper measures the quality of pre-trained by their static representations (i.e.representations before finetuning).It is interesting to consider the dynamic representa-tions (i.e.representations after fine-tuning) of pre-trained models to account for the change of pre-trained models during fine-tuning.We leave it as a future work.\n\n\nA. Dataset description and statistics\n\nAircraft: The dataset contains fine-grained classification of 10,000 aircraft pictures which belongs to 100 classes, with 100 images per class.\n\nBirdsnap: The dataset contains 49,829 images of 500 species of North American birds.CIFAR 10: The dataset consists of 60,000 32x32 colorful images in 10 classes, with 6,000 images per class.There are 50,000 training images and 10,000 test images.\n\nCIFAR 100: The dataset is just like the CIFAR 10, except it has 100 classes containing 600 images each.\n\n\nDTD:\n\nThe dataset contains a collection of 5,640 textural images in the wild, annotated with a series of human-centric attributes.It has 47 classes and 120 images per class.\n\nPets: The dataset contains 7,049 images of cat and dog species which belongs to 47 classes, with around 200 images per class.\n\n\nSUN:\n\nThe dataset contains 39,700 scenery pictures with 397 classes and 100 samples per class.\n\nFor all the datasets we use, we respect the official train / val / test splits if they exist, otherwise we use 60% data for training, 20% data for validation (hyper-parameter tuning) and 20% data for testing.\n\n\nB. Comparing LogME to re-training head\n\nA na\u00efve way to measure the relationship between features and labels is to train a classification / regression head for the downstream task, and to use the head's performance as an assessment (sometimes it is called \"linear probing\" or \"linear protocol evaluation\").Actually we have considered this idea but find that it works not as well as expected.\n\nThe issues of re-training head are studied by researchers in visual representation learning, too.Kolesnikov et al. (2019) found that (1) re-training head by second-order optimization is impractical; (2) first-order optimization with gradients is sensitive to the learning rate schedule and takes a long time to converge.\n\nApart from issues discussed by Kolesnikov et al. (2019), Kornblith et al. (2019) also note that hyper-parameter of logistic regression (strength of L2 regularization) should be tuned extensively, making head re-training inefficient.\n\nOur empirical experiments agree with the above concerns with re-training head, and also find that re-training head does not work as well as expected.In the Caltech dataset, we extract features from 10 pre-trained models, train softmax regressors with tuned hyper-parameters (the L2 regularization strength), and plot the correlation between the best head accuracy and the transfer performance w.r.t. the number of hyper-parameter trials in As a side issue, even if we re-train a head for the downstream task, it is unclear what quantity of the head should be used to measure pre-trained models.Since the performance of downstream tasks are evaluated by accuracy and MSE in transfer learning, it may somewhat cause over-fitting if we use the accuracy and MSE of the re-trained head.Indeed, in Figure 7, when the number of hyper-parameter trials increases, the correlation can even go down, showing the effect of somewhat over-fitting.\n\nTherefore, re-training head is neither efficient nor effective as LogME.\n\n\nC. Original Results in Figures\n\nOriginal results in figures are shown in the Table 5, Table 6, and Table 7.\n\nFigure 1 .\n1\nFigure 1.Illustration of task adaptive pre-trained model selection.\n\n\n\n\nh s t a 4 L e I o w w m c w j l 4 c A 0 N u I c m + M C A w z O 8 w p s j n R f n 3 f l Y t J a c Y u Y Y / s D 5 / A H F f I 6 p < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I K y P j L K y e 3 6 w 4 5 l\n\n\n\n\nR z r P z 5 r x P W 5 e c 2 c w B / I H z 8 Q O G / p G c < / l a t e x i t > y 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" R D 1 x n 0 4 M C 7 W l x q B n + M 6 h 5 p M V 2 R 8 = \" > A A A B 6 3 i c b V D L S g N B E O z 1 G e M r 6 l G R w S B 4 C r u i 6 D H o x W M C 5 g H J E m Y n s 8 m Q m d l l Z l Y I S 4 5 e v X h Q x K v / k O / w 5 j f 4 E 8 4 m O W h i Q U N R 1 U 1 3 V x B z p o 3 r f j l L y y u r a + u 5 j f z m 1 v b O b m F v v 6 6 j R B F a I x G P V D P A m n I m a c 0 w w 2\n\n\n\n\n4 5 s 5 k D + A P n 4 w c x 5 p F k < / l a t e x i t > y n < l a t e x i t s h a 1 _ b a s e 6 4 = \" f 5 W w f C o o o v R b r / S + 1 E h R e 7 / x 4 o k = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o s e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I o / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b\n\n\n\n\nl e 4 c 2 R z o v z 7 n w s W g t O P n M M f + B 8 / g B s Q o 3 m < / l a t e x i t > f n < l a t e x i t s h a 1 _ b a s e 6 4 = \" b 5 2 U V N S P / m 4 x B y F N e r w I t v P L s M 0\n\n\n\n\nn z m G P 3 A + f w B P U I 3 T < / l a t e x i t > ... < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 8L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t > ... < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 8\n\n\n\n\na t e x i t > ... < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 8L 7 Y W u J H Z K G a Y K T 1 U A o v U k S Y V U = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k o s e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A q u j e d 9 O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 1 E m m G D Z Y I h L V D q l G w S U 2 D D c C 2 6 l C G o c C W + H o d u a 3 n l B p n s h H M 0 4 x i O l A 8 o g z a q z 0 4 L p u r 1 z x X G 8 O s k r 8 n F Q g R 7 1 X / u r 2 E 5 b F K A 0 T V O u O 7 6 U m m F B l O B M4 L X U z j S l l I z r A j q W S x q i D y f z U K T m z S p 9 E i b I l D Z m r v y c m N N Z 6 H I e 2 M 6 Z m q J e 9 m f i f 1 8 l M d B 1 M u E w z g 5 I t F k W Z I C Y h s 7 9 J n y t k R o w t o U x x e y t h Q 6 o o M z a d k g 3 B X 3 5 5 l T Q v X L / q X t 5 X K 7 W b P I 4 i n M A p n I M P V 1 C D O 6 h D A x g M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t > ... < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 8\n\n\n\n\ng M 4 B l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g B O f I 0 q < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g y T 4 t 2 m R u z 3 c 6 w t s 8 u u 8 4 k 4Y z + o = \" > A A A C D H i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S J U 0 D K j o i 6 L b n Q j F e w D O m O 5 k 6 Z t a O Z B k l H K M B / g x l 9 x 4 0 I R t 3 6 A O / / G T D s L r R 4 I H M 4 5 l 9 x 7 3 J A z q U z z y 8 j N z M 7 N L + Q X C 0 v L K 6 t r x f W N h g w i Q W i d B D w Q L Rc k 5 c y n d c U U p 6 1 Q U P B c T p v u 8 D z 1 m 3 d U S B b 4 N 2 o U U s e D v s 9 6 j I D S U q d Y u s e 2 Z B 6 2 P V A D A j y + S s r m H r a B h w O 4 j f e t 5 H J X p 8 y K O Q b + S 6 y M l F C G W q f 4 a X c D E n n U V 4 S D l G 3 L D J U T g 1 C M c J o\n\n\nFigure 3 .\n3\nFigure 3. Illustration of LogME with toy data.It is clear that LogME decreases with decreasing feature quality.For classification (Figure 3 top), three clusters in 2-D plane\n\n\nFigure\n\nFigure4.Correlation (\u03c4w)  between fine-tuned accuracy (X-axis) and three methods for pre-trained model selection on 9 datasets with 10 pre-trained models.One row for each method, one column for each dataset (with \u03c4w in the bracket near the dataset name), and one marker for each pre-trained model.The best \u03c4w in each dataset is marked in bold.\n\n\nFigure 5 .\n5\nFigure 5. Supervised pre-trained models transferred to dSprites.\n\n\nFigure\n\nFigure6.Correlation (\u03c4w)  between fine-tuned accuracy (X-axis) and LogME in 7 GLUE tasks with 8 popular pre-trained language models.One column for each task (with \u03c4w in the bracket near the task name), and one marker for each pre-trained model.\n\n\nCaltech:\n\nThe dataset contains 9,144 pictures of objects belonging to 101 categories.There are about 40 to 800 images per category.Most categories have about 50 images.Cars: The dataset contains 16,185 images of 196 classes of cars.The data is split into 8,144 training images and 8,041 testing images.\n\n\nFigure 7 .\n7\nFigure 7.The correlation of re-training head w.r.t. the number of hyper-parameter trials.It is clear that re-training head is much worse than LogME.\n\n\nTable 1 .\n1\nApplicability of prior methods and the proposed LogME.\"LM\" means language modeling.\nModalityPre-trainTargetLEEP NCE LogMEclassification classificationvisionclassification contrastiveregression classificationcontrastiveregressionlanguageLMclassification\n\nTable 2 .\n2\nComputational complexity of Algorithm 1.\nComplexity per for-loopOverall complexityna\u00efve\n\n\n\nwhich commonly used for evaluating the quality of learned representations.The input is an image containing a sprite (heart, square, and ellipse) with varying scale, orientation, and position.Pretrained models are transferred to predict four scalars (scale, orientation, and (x, y) positions) together, and mean square error (MSE) on the test data is reported.The supervised pre-trained models are the same as Section 5.1 and hyperparameter tuning scheme follows.Results are plotted in Figure5.It is clear that LogME and MSE are well correlated and the correlation coefficient \u03c4 w = 0.84 is very large: if a pre-trained model \u03c6 1 has larger LogME than \u03c6 2 , with 92% probability \u03c6 1 is better (has smaller MSE) than \u03c6 2 after actually fine-tuning.\n1.7ResNet-50DenseNet-201ResNet-101Inception v11.6ResNet-152Incetpion v3DenseNet-121MobileNet v21.5DenseNet-169NASNet-A MobileLogME1.41.31.20.0250.0300.0350.0400.045MSE\n\nTable 3 .\n3\nUse LogME to assess unsupervised pre-trained models.\nPre-trained NetworkAircraftdSpritesAccuracy (%) LogME MSE LogMEMoCo V181.680.9340.0691.52MoCo V284.160.9410.0471.64MoCo 80086.990.9460.0501.58SimCLR88.100.950--\u03c4 w : 1.0\u03c4 w : 1.05.4. Transferring pre-trained language models to theGLUE benchmark\nTo further demonstrate the generality of LogME, we show how LogME can work for pre-trained language models.Again prior works (LEEP and NCE) cannot deal with these pre-trained language models.\n\n\nTable 4 .\n4\nEfficiency of LogME.It is clear that brute-force fine-tuning is computationally expensive, requiring about a day for one dataset with one pre-trained model.Selecting the best pre-trained model out of 10 models would cost 10 days.Extracting features is very cheap and costs much less.In computer vision, the wall-clock time of LogME is reduced dramatically to 0.31 of fine-tuning, bringing over 3000\u00d7 speedup while requiring 120\u00d7 less memory footprint.In the NLP domain, feature extraction is much slower and therefore the wallclock time speedup is not as striking as computer vision, but still reaching 86\u00d7 speedup.In all cases, LogME costs almost the same as the lower bound (feature extraction), meaning that LogME makes practical assessment possible with minimal additional cost.\nwall-clock timememory footprintfine-tune (upper bound) 161000s fine-tune (upper bound)6.3 GBComputer Visionextract feature (lower bound) 37s extract feature (lower bound) 43 MB LogME 50s LogME 53 MBbenefit fine-tune (upper bound) 100200s fine-tune (upper bound) 3200 \u2191 benefit120 \u2191 88 GBNatural Language Processingextract feature (lower bound) 1130s extract feature (lower bound) 1.2 GB LogME 1157s LogME 1.2 GBbenefit86 \u2191 benefit73 \u2191sessment. The cost for the rest models and datasets vary, butthe proportion is similar. Note that, because carelessly tunedhyper-parameters cannot tell good models from bad mod-els, it is necessary to attribute the cost of hyper-parametersearch to brute-force fine-tuning while LogME does notneed hyper-parameter tuning.\nAcknowledgementsWe would like to thank Ximei Wang, Xinyang Chen, Yang Shu, and Yonglong Tian for helpful discussions.This work was supported by the National Key R&D Program of China (2020AAA0109201), NSFC grants (62022050, 62021002, 61772299), Beijing Nova Program (Z201100006820041), and MOE Innovation Plan of China.\nTensorflow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, OSDI. 2016\n\nExploiting task relatedness for multiple task learning. S Ben-David, R Schuller, COLT. 2003\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. S Benoit, D Zachary, C Soumith, G Sam, P Adam, M Francisco, L Adam, C Gregory, L Zeming, Y Edward, D Alban, T Alykhan, K Andreas, B James, A Luca, R Martin, G Natalia, C Sasank, K Trevor, F Lu, B Junjie, NeurIPS. 2019\n\nLarge-scale fine-grained visual categorization of birds. T Berg, J Liu, S Woo Lee, M L Alexander, D W Jacobs, P N Belhumeur, Birdsnap, CVPR. 2014\n\nPattern recognition and machine learning. C M Bishop, 2006\n\nA Simple Framework for Contrastive Learning of Visual Representations. T Chen, S Kornblith, M Norouzi, G Hinton, ICML. 2020a\n\nX Chen, H Fan, R Girshick, K He, arXiv:2003.04297Improved Baselines with Momentum Contrastive Learning. 2020b\n\nDescribing textures in the wild. M Cimpoi, S Maji, I Kokkinos, S Mohamed, A Vedaldi, CVPR. 2014\n\nK Clark, M.-T Luong, Q V Le, C D Manning, Elec-Tra, Pre-training Text Encoders as Discriminators Rather Than Generators. In ICLR. 2020\n\nLogme, Practical Assessment of Pre-trained Models for Transfer Learning Cover, T. M. Elements of information theory. 1999\n\nSemi-analytical approximations to statistical moments of sigmoid and softmax mappings of normal variables. J Daunizeau, arXiv:1703.000912017arXiv preprint\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. 2009\n\nPre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, NAACL. 2019\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, ICML. 2014\n\nWhy does unsupervised pre-training help deep learning. D Erhan, A Courville, Y Bengio, P Vincent, AISTATS. 2010\n\nComparing top k lists. R Fagin, R Kumar, D Sivakumar, SODA. 2003\n\nLearning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. L Fei-Fei, R Fergus, P Perona, Y Ganin, V Lempitsky, CVPR Workshops. 2004. 2015ICML\n\nTransfer and adapt pre-trained transformer models for answer sentence selection. S Garg, T Vu, A Moschitti, Tanda, AAAI. 2020\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, CVPR. 2014\n\nDevelopments in maximum entropy data analysis. S F Gull, Maximum entropy and Bayesian methods. 1989\n\nDon't Stop Pretraining: Adapt Language Models to Domains and Tasks. S Gururangan, A Marasovi\u0107, S Swayamdipta, K Lo, I Beltagy, D Downey, N A Smith, ACL. 2020\n\nNoise-contrastive estimation: A new estimation principle for unnormalized statistical models. M Gutmann, A Hyv\u00e4rinen, AISTATS. 2010\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, ICCV. 2015\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 2016\n\nRethinking imagenet pretraining. K He, R Girshick, P Doll\u00e1r, ICCV. 2019\n\nMomentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, CVPR. 2020\n\nStrategies for Pre-training Graph Neural Networks. W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, J Leskovec, ICLR2020\n\nDensely connected convolutional networks. G Huang, Z Liu, K Q Weinberger, L Van Der Maaten, CVPR. 2017\n\nSelf-supervised visual feature learning with deep neural networks: A survey. L Jing, Y Tian, 2020TPAMI\n\nIn-datacenter performance analysis of a tensor processing unit. N P Jouppi, C Young, N Patil, D Patterson, G Agrawal, R Bajwa, S Bates, S Bhatia, N Boden, A Borchers, ISCA. 2017\n\nA new measure of rank correlation. M G Kendall, Biometrika. 1938\n\nBayesian Evidence and Model Selection. K H Knuth, M Habeck, N K Malakar, A M Mubeen, B Placek, Digital Signal Processing. 2015\n\nRevisiting Self-Supervised Visual Representation Learning. A Kolesnikov, X Zhai, L Beyer, CVPR. 2019\n\nProbabilistic graphical models: principles and techniques. D Koller, N Friedman, 2009\n\nDo better imagenet models transfer better. S Kornblith, J Shlens, Q V Le, CVPR. 2019\n\n. Z Kou, K You, M Long, Wang , J. Stochastic Normalization. In NeurIPS. 2020\n\nCollecting a largescale dataset of fine-grained cars. J Krause, J Deng, M Stark, L Fei-Fei, 2013\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, 2009Technical report\n\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, ICLR2020\n\nRethinking the Hyperparameters for Fine-tuning. H Li, P Chaudhari, H Yang, M Lam, A Ravichandran, R Bhotika, S Soatto, ICLR. 2020\n\nExplicit Inductive Bias for Transfer Learning with Convolutional Networks. X Li, Y Grandvalet, F Davoine, ICML. 2018\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019arXiv preprint\n\nLearning Transferable Features with Deep Adaptation Networks. M Long, Y Cao, J Wang, M Jordan, ICML. 2015\n\nExploring the limits of weakly supervised pretraining. D Mahajan, R Girshick, V Ramanathan, K He, M Paluri, Y Li, A Bharambe, L Van Der Maaten, ECCV. 2018\n\nS Maji, E Rahtu, J Kannala, M Blaschko, A Vedaldi, Fine, arXiv:1306.5151Grained Visual Classification of Aircraft. 2013\n\nL Matthey, I Higgins, D Hassabis, Lerchner, A. dsprites: Disentanglement testing sprites dataset. 2017\n\nWhat is being transferred in transfer learning? In NeurIPS. B Neyshabur, H Sedghi, C Zhang, 2020\n\nLEEP: A New Measure to Evaluate Transferability of Learned Representations. C Nguyen, T Hassner, M Seeger, C Archambeau, ICML. 2020\n\nCats and dogs. O M Parkhi, A Vedaldi, A Zisserman, C V Jawahar, CVPR. 2012\n\nIntermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work. Y Pruksachatkun, J Phang, H Liu, P M Htut, X Zhang, R Y Pang, C Vania, K Kann, S R Bowman, ACL. 2020\n\nDataset shift in machine learning. J Quionero-Candela, M Sugiyama, A Schwaighofer, N D Lawrence, 2009\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, 2015IJCV\n\nMobileNetV2: Inverted Residuals and Linear Bottlenecks. M Sandler, A Howard, M Zhu, A Zhmoginov, L.-C Chen, CVPR. 2018\n\nV Sanh, L Debut, J Chaumond, T Wolf, Distilbert, arXiv:1910.01108a distilled version of BERT: smaller, faster, cheaper and lighter. 2019arXiv preprint\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, 2015\n\nRethinking the Inception Architecture for Computer Vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. 2016\n\nMnasnet: Platform-aware neural architecture search for mobile. M Tan, B Chen, R Pang, V Vasudevan, M Sandler, A Howard, Q V Le, CVPR. 2019\n\nLearning to Learn: Introduction and Overview. S Thrun, L Pratt, Learning to Learn. 1998\n\nWhat Makes for Good Views for Contrastive Learning? In NeurIPS. Y Tian, C Sun, B Poole, D Krishnan, C Schmid, P Isola, 2020\n\nTransferability and hardness of supervised classification tasks. A T Tran, C V Nguyen, T Hassner, ICCV. 2019\n\nA Weighted Correlation Index for Rankings with Ties. S Vigna, WWW. 2015\n\nP Virtanen, R Gommers, T E Oliphant, M Haberland, T Reddy, D Cournapeau, E Burovski, P Peterson, W Weckesser, J Bright, S J Van Der Walt, M Brett, J Wilson, K J Millman, N Mayorov, A R J Nelson, E Jones, R Kern, E Larson, C J Carey, \u0130 Polat, Y Feng, E W Moore, J Vander-Plas, D Laxalde, J Perktold, R Cimrman, I Henriksen, E A Quintero, C R Harris, A M Archibald, A H Ribeiro, F Pedregosa, Van Mulbregt, 10.1038/s41592-019-0686-2and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. 202017\n\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, EMNLP. 2018\n\nTransformers: State-of-the-art natural language processing. T Wolf, J Chaumond, L Debut, V Sanh, C Delangue, A Moi, P Cistac, M Funtowicz, J Davison, S Shleifer, EMNLP. 2020\n\nSun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, CVPR. 2010\n\nXlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, NeurIPS2019\n\nJ Yosinski, J Clune, Y Bengio, H Lipson, How transferable are features in deep neural networks? In NeurIPS. 2014\n\nCo-Tuning for Transfer Learning. K You, Z Kou, M Long, J Wang, NeurIPS2020\n\nDisentangling Task Transfer Learning. A R Zamir, A Sax, W Shen, L J Guibas, J Malik, S Savarese, Taskonomy, CVPR. 2018\n\nA Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark. X Zhai, J Puigcerver, A Kolesnikov, P Ruyssen, C Riquelme, M Lucic, J Djolonga, A S Pinto, M Neumann, A Dosovitskiy, L Beyer, O Bachem, M Tschannen, M Michalski, O Bousquet, S Gelly, N Houlsby, arXiv:1910.048672020cs, stat\n", "annotations": {"author": "[{\"end\":198,\"start\":75},{\"end\":296,\"start\":199},{\"end\":377,\"start\":297},{\"end\":489,\"start\":378}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":83},{\"end\":207,\"start\":204},{\"end\":309,\"start\":305},{\"end\":392,\"start\":388}]", "author_first_name": "[{\"end\":82,\"start\":75},{\"end\":203,\"start\":199},{\"end\":304,\"start\":297},{\"end\":387,\"start\":378}]", "author_affiliation": "[{\"end\":130,\"start\":111},{\"end\":197,\"start\":132},{\"end\":228,\"start\":209},{\"end\":295,\"start\":230},{\"end\":376,\"start\":311},{\"end\":488,\"start\":423}]", "title": "[{\"end\":72,\"start\":1},{\"end\":561,\"start\":490}]", "venue": null, "abstract": "[{\"end\":2123,\"start\":595}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2243,\"start\":2226},{\"end\":2248,\"start\":2243},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2429,\"start\":2410},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2454,\"start\":2429},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2514,\"start\":2493},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2693,\"start\":2671},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2772,\"start\":2749},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2883,\"start\":2860},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2899,\"start\":2883},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3058,\"start\":3034},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3093,\"start\":3072},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3187,\"start\":3170},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3368,\"start\":3347},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3401,\"start\":3381},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":3477,\"start\":3458},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":3967,\"start\":3947},{\"end\":4217,\"start\":4200},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4310,\"start\":4291},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4332,\"start\":4312},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4852,\"start\":4835},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4871,\"start\":4852},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4935,\"start\":4914},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4952,\"start\":4935},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5766,\"start\":5752},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6688,\"start\":6667},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6894,\"start\":6863},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6918,\"start\":6894},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6936,\"start\":6918},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7057,\"start\":7037},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7079,\"start\":7057},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":7262,\"start\":7242},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7427,\"start\":7399},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7796,\"start\":7779},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7839,\"start\":7821},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7891,\"start\":7873},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8002,\"start\":7977},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8029,\"start\":8002},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8047,\"start\":8029},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8503,\"start\":8486},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9158,\"start\":9142},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9267,\"start\":9245},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9368,\"start\":9351},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9495,\"start\":9475},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9576,\"start\":9554},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9723,\"start\":9705},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10027,\"start\":10008},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10068,\"start\":10046},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10118,\"start\":10091},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10252,\"start\":10236},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10439,\"start\":10420},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10674,\"start\":10655},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10840,\"start\":10819},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10894,\"start\":10875},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10940,\"start\":10923},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11090,\"start\":11072},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11287,\"start\":11268},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11322,\"start\":11298},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12052,\"start\":12030},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12144,\"start\":12121},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12305,\"start\":12281},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12556,\"start\":12536},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12977,\"start\":12959},{\"end\":13025,\"start\":13012},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13125,\"start\":13104},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":13153,\"start\":13134},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14878,\"start\":14858},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14989,\"start\":14969},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15392,\"start\":15377},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16111,\"start\":16091},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":16659,\"start\":16646},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":16756,\"start\":16733},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":16875,\"start\":16862},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34334,\"start\":34320},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34715,\"start\":34695},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":35181,\"start\":35156},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35870,\"start\":35859},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36197,\"start\":36185},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37230,\"start\":37213},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38266,\"start\":38246},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":41058,\"start\":41036},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":41095,\"start\":41073},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":41124,\"start\":41107},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":41154,\"start\":41137},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":41184,\"start\":41167},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41219,\"start\":41199},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41254,\"start\":41234},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41289,\"start\":41269},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":41327,\"start\":41305},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":41367,\"start\":41349},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":41597,\"start\":41578},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41626,\"start\":41608},{\"end\":41659,\"start\":41626},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41687,\"start\":41666},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41724,\"start\":41697},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41763,\"start\":41736},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41790,\"start\":41769},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41818,\"start\":41797},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":41847,\"start\":41828},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42131,\"start\":42115},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":42760,\"start\":42739},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":42788,\"start\":42769},{\"end\":42837,\"start\":42818},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43755,\"start\":43734},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44596,\"start\":44574},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":44626,\"start\":44607},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":44765,\"start\":44748},{\"end\":45049,\"start\":45030},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":45507,\"start\":45490},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":45560,\"start\":45540},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45687,\"start\":45668},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45720,\"start\":45700},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":45790,\"start\":45771},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":45913,\"start\":45891},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":46993,\"start\":46974},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":47180,\"start\":47162},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":47251,\"start\":47233},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":47281,\"start\":47263},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47316,\"start\":47296},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47356,\"start\":47336},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":51685,\"start\":51661},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":51941,\"start\":51917},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":51966,\"start\":51943}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53320,\"start\":53238},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53546,\"start\":53321},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54050,\"start\":53547},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54564,\"start\":54051},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54756,\"start\":54565},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55874,\"start\":54757},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56954,\"start\":55875},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57720,\"start\":56955},{\"attributes\":{\"id\":\"fig_8\"},\"end\":57909,\"start\":57721},{\"attributes\":{\"id\":\"fig_10\"},\"end\":58263,\"start\":57910},{\"attributes\":{\"id\":\"fig_11\"},\"end\":58343,\"start\":58264},{\"attributes\":{\"id\":\"fig_13\"},\"end\":58598,\"start\":58344},{\"attributes\":{\"id\":\"fig_14\"},\"end\":58903,\"start\":58599},{\"attributes\":{\"id\":\"fig_15\"},\"end\":59067,\"start\":58904},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":59333,\"start\":59068},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59434,\"start\":59334},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60352,\"start\":59435},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":60856,\"start\":60353},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":62407,\"start\":60857}]", "paragraph": "[{\"end\":2367,\"start\":2139},{\"end\":2773,\"start\":2369},{\"end\":3271,\"start\":2775},{\"end\":4129,\"start\":3273},{\"end\":4974,\"start\":4131},{\"end\":5192,\"start\":4976},{\"end\":5865,\"start\":5194},{\"end\":5912,\"start\":5867},{\"end\":6306,\"start\":5914},{\"end\":6611,\"start\":6308},{\"end\":7620,\"start\":6649},{\"end\":8810,\"start\":7622},{\"end\":9003,\"start\":8833},{\"end\":9724,\"start\":9005},{\"end\":10675,\"start\":9726},{\"end\":11363,\"start\":10677},{\"end\":11880,\"start\":11365},{\"end\":12469,\"start\":11932},{\"end\":13097,\"start\":12471},{\"end\":13708,\"start\":13099},{\"end\":14348,\"start\":13726},{\"end\":15192,\"start\":14350},{\"end\":15423,\"start\":15194},{\"end\":16016,\"start\":15425},{\"end\":16376,\"start\":16072},{\"end\":16757,\"start\":16378},{\"end\":16938,\"start\":16759},{\"end\":17340,\"start\":16940},{\"end\":17530,\"start\":17363},{\"end\":18103,\"start\":17532},{\"end\":18297,\"start\":18105},{\"end\":18496,\"start\":18322},{\"end\":18829,\"start\":18498},{\"end\":27399,\"start\":26890},{\"end\":34089,\"start\":34025},{\"end\":35063,\"start\":34091},{\"end\":35236,\"start\":35065},{\"end\":35273,\"start\":35238},{\"end\":35622,\"start\":35446},{\"end\":36275,\"start\":35807},{\"end\":36833,\"start\":36374},{\"end\":37499,\"start\":36835},{\"end\":37636,\"start\":37588},{\"end\":37684,\"start\":37638},{\"end\":38062,\"start\":38049},{\"end\":38144,\"start\":38064},{\"end\":38599,\"start\":38170},{\"end\":38852,\"start\":38601},{\"end\":39628,\"start\":38854},{\"end\":39905,\"start\":39711},{\"end\":40041,\"start\":39921},{\"end\":40090,\"start\":40043},{\"end\":40696,\"start\":40092},{\"end\":40890,\"start\":40698},{\"end\":41496,\"start\":40961},{\"end\":41948,\"start\":41498},{\"end\":42541,\"start\":41950},{\"end\":42707,\"start\":42543},{\"end\":43155,\"start\":42709},{\"end\":43531,\"start\":43157},{\"end\":43975,\"start\":43533},{\"end\":44273,\"start\":43977},{\"end\":44533,\"start\":44341},{\"end\":44626,\"start\":44535},{\"end\":45268,\"start\":44694},{\"end\":45760,\"start\":45270},{\"end\":45945,\"start\":45762},{\"end\":47807,\"start\":45947},{\"end\":48414,\"start\":47831},{\"end\":48881,\"start\":48416},{\"end\":49683,\"start\":48896},{\"end\":50021,\"start\":49685},{\"end\":50206,\"start\":50063},{\"end\":50454,\"start\":50208},{\"end\":50559,\"start\":50456},{\"end\":50735,\"start\":50568},{\"end\":50862,\"start\":50737},{\"end\":50959,\"start\":50871},{\"end\":51169,\"start\":50961},{\"end\":51562,\"start\":51212},{\"end\":51884,\"start\":51564},{\"end\":52118,\"start\":51886},{\"end\":53053,\"start\":52120},{\"end\":53127,\"start\":53055},{\"end\":53237,\"start\":53162},{\"end\":53319,\"start\":53252},{\"end\":53545,\"start\":53324},{\"end\":54049,\"start\":53550},{\"end\":54563,\"start\":54054},{\"end\":54755,\"start\":54568},{\"end\":55873,\"start\":54760},{\"end\":56953,\"start\":55878},{\"end\":57719,\"start\":56958},{\"end\":57908,\"start\":57735},{\"end\":58262,\"start\":57919},{\"end\":58342,\"start\":58278},{\"end\":58597,\"start\":58353},{\"end\":58902,\"start\":58610},{\"end\":59066,\"start\":58918},{\"end\":59164,\"start\":59081},{\"end\":59387,\"start\":59347},{\"end\":60184,\"start\":59438},{\"end\":60418,\"start\":60366},{\"end\":60855,\"start\":60664},{\"end\":61652,\"start\":60870}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16071,\"start\":16017},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22850,\"start\":18830},{\"attributes\":{\"id\":\"formula_2\"},\"end\":26889,\"start\":22850},{\"attributes\":{\"id\":\"formula_3\"},\"end\":28289,\"start\":27400},{\"attributes\":{\"id\":\"formula_4\"},\"end\":28848,\"start\":28289},{\"attributes\":{\"id\":\"formula_5\"},\"end\":29319,\"start\":28848},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30180,\"start\":29319},{\"attributes\":{\"id\":\"formula_7\"},\"end\":31129,\"start\":30180},{\"attributes\":{\"id\":\"formula_8\"},\"end\":34024,\"start\":31129},{\"attributes\":{\"id\":\"formula_9\"},\"end\":35445,\"start\":35274},{\"attributes\":{\"id\":\"formula_10\"},\"end\":35772,\"start\":35623},{\"attributes\":{\"id\":\"formula_11\"},\"end\":36373,\"start\":36276},{\"attributes\":{\"id\":\"formula_12\"},\"end\":37587,\"start\":37500},{\"attributes\":{\"id\":\"formula_13\"},\"end\":38048,\"start\":37685},{\"attributes\":{\"id\":\"formula_14\"},\"end\":39710,\"start\":39629}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4983,\"start\":4982},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8482,\"start\":8481},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39447,\"start\":39446},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":45975,\"start\":45974},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48444,\"start\":48443},{\"end\":53214,\"start\":53213},{\"end\":53223,\"start\":53222},{\"end\":53236,\"start\":53235}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2137,\"start\":2125},{\"attributes\":{\"n\":\"2.\"},\"end\":6627,\"start\":6614},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6647,\"start\":6630},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8831,\"start\":8813},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11930,\"start\":11883},{\"attributes\":{\"n\":\"3.\"},\"end\":13724,\"start\":13711},{\"attributes\":{\"n\":\"4.\"},\"end\":17361,\"start\":17343},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18320,\"start\":18300},{\"attributes\":{\"n\":\"4.2.\"},\"end\":35805,\"start\":35774},{\"attributes\":{\"n\":\"4.3.\"},\"end\":38168,\"start\":38147},{\"attributes\":{\"n\":\"5.\"},\"end\":39919,\"start\":39908},{\"attributes\":{\"n\":\"5.1.\"},\"end\":40959,\"start\":40893},{\"attributes\":{\"n\":\"5.2.\"},\"end\":44339,\"start\":44276},{\"attributes\":{\"n\":\"5.3.\"},\"end\":44692,\"start\":44629},{\"attributes\":{\"n\":\"5.5.\"},\"end\":47829,\"start\":47810},{\"attributes\":{\"n\":\"6.\"},\"end\":48894,\"start\":48884},{\"end\":50061,\"start\":50024},{\"end\":50566,\"start\":50562},{\"end\":50869,\"start\":50865},{\"end\":51210,\"start\":51172},{\"end\":53160,\"start\":53130},{\"end\":53249,\"start\":53239},{\"end\":57732,\"start\":57722},{\"end\":57917,\"start\":57911},{\"end\":58275,\"start\":58265},{\"end\":58351,\"start\":58345},{\"end\":58608,\"start\":58600},{\"end\":58915,\"start\":58905},{\"end\":59078,\"start\":59069},{\"end\":59344,\"start\":59335},{\"end\":60363,\"start\":60354},{\"end\":60867,\"start\":60858}]", "table": "[{\"end\":59333,\"start\":59165},{\"end\":59434,\"start\":59388},{\"end\":60352,\"start\":60185},{\"end\":60663,\"start\":60419},{\"end\":62407,\"start\":61653}]", "figure_caption": "[{\"end\":53320,\"start\":53251},{\"end\":53546,\"start\":53323},{\"end\":54050,\"start\":53549},{\"end\":54564,\"start\":54053},{\"end\":54756,\"start\":54567},{\"end\":55874,\"start\":54759},{\"end\":56954,\"start\":55877},{\"end\":57720,\"start\":56957},{\"end\":57909,\"start\":57734},{\"end\":58263,\"start\":57918},{\"end\":58343,\"start\":58277},{\"end\":58598,\"start\":58352},{\"end\":58903,\"start\":58609},{\"end\":59067,\"start\":58917},{\"end\":59165,\"start\":59080},{\"end\":59388,\"start\":59346},{\"end\":60185,\"start\":59437},{\"end\":60419,\"start\":60365},{\"end\":61653,\"start\":60869}]", "figure_ref": "[{\"end\":34033,\"start\":34032},{\"end\":34793,\"start\":34792},{\"end\":35110,\"start\":35109},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40384,\"start\":40383},{\"end\":43530,\"start\":43529},{\"end\":44014,\"start\":44013},{\"end\":47489,\"start\":47488},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":52920,\"start\":52919}]", "bib_author_first_name": "[{\"end\":62783,\"start\":62782},{\"end\":62792,\"start\":62791},{\"end\":62802,\"start\":62801},{\"end\":62810,\"start\":62809},{\"end\":62818,\"start\":62817},{\"end\":62827,\"start\":62826},{\"end\":62835,\"start\":62834},{\"end\":62844,\"start\":62843},{\"end\":62856,\"start\":62855},{\"end\":62866,\"start\":62865},{\"end\":62943,\"start\":62942},{\"end\":62956,\"start\":62955},{\"end\":63050,\"start\":63049},{\"end\":63060,\"start\":63059},{\"end\":63071,\"start\":63070},{\"end\":63082,\"start\":63081},{\"end\":63089,\"start\":63088},{\"end\":63097,\"start\":63096},{\"end\":63110,\"start\":63109},{\"end\":63118,\"start\":63117},{\"end\":63129,\"start\":63128},{\"end\":63139,\"start\":63138},{\"end\":63149,\"start\":63148},{\"end\":63158,\"start\":63157},{\"end\":63169,\"start\":63168},{\"end\":63180,\"start\":63179},{\"end\":63189,\"start\":63188},{\"end\":63197,\"start\":63196},{\"end\":63207,\"start\":63206},{\"end\":63218,\"start\":63217},{\"end\":63228,\"start\":63227},{\"end\":63238,\"start\":63237},{\"end\":63244,\"start\":63243},{\"end\":63326,\"start\":63325},{\"end\":63334,\"start\":63333},{\"end\":63341,\"start\":63340},{\"end\":63352,\"start\":63351},{\"end\":63354,\"start\":63353},{\"end\":63367,\"start\":63366},{\"end\":63369,\"start\":63368},{\"end\":63379,\"start\":63378},{\"end\":63381,\"start\":63380},{\"end\":63458,\"start\":63457},{\"end\":63460,\"start\":63459},{\"end\":63547,\"start\":63546},{\"end\":63555,\"start\":63554},{\"end\":63568,\"start\":63567},{\"end\":63579,\"start\":63578},{\"end\":63602,\"start\":63601},{\"end\":63610,\"start\":63609},{\"end\":63617,\"start\":63616},{\"end\":63629,\"start\":63628},{\"end\":63746,\"start\":63745},{\"end\":63756,\"start\":63755},{\"end\":63764,\"start\":63763},{\"end\":63776,\"start\":63775},{\"end\":63787,\"start\":63786},{\"end\":63810,\"start\":63809},{\"end\":63822,\"start\":63818},{\"end\":63831,\"start\":63830},{\"end\":63833,\"start\":63832},{\"end\":63839,\"start\":63838},{\"end\":63841,\"start\":63840},{\"end\":64176,\"start\":64175},{\"end\":64278,\"start\":64277},{\"end\":64286,\"start\":64285},{\"end\":64294,\"start\":64293},{\"end\":64307,\"start\":64303},{\"end\":64313,\"start\":64312},{\"end\":64319,\"start\":64318},{\"end\":64418,\"start\":64417},{\"end\":64431,\"start\":64427},{\"end\":64440,\"start\":64439},{\"end\":64447,\"start\":64446},{\"end\":64558,\"start\":64557},{\"end\":64569,\"start\":64568},{\"end\":64576,\"start\":64575},{\"end\":64587,\"start\":64586},{\"end\":64598,\"start\":64597},{\"end\":64607,\"start\":64606},{\"end\":64616,\"start\":64615},{\"end\":64694,\"start\":64693},{\"end\":64703,\"start\":64702},{\"end\":64716,\"start\":64715},{\"end\":64726,\"start\":64725},{\"end\":64775,\"start\":64774},{\"end\":64784,\"start\":64783},{\"end\":64793,\"start\":64792},{\"end\":64946,\"start\":64945},{\"end\":64957,\"start\":64956},{\"end\":64967,\"start\":64966},{\"end\":64977,\"start\":64976},{\"end\":64986,\"start\":64985},{\"end\":65112,\"start\":65111},{\"end\":65120,\"start\":65119},{\"end\":65126,\"start\":65125},{\"end\":65240,\"start\":65239},{\"end\":65252,\"start\":65251},{\"end\":65263,\"start\":65262},{\"end\":65274,\"start\":65273},{\"end\":65342,\"start\":65341},{\"end\":65344,\"start\":65343},{\"end\":65464,\"start\":65463},{\"end\":65478,\"start\":65477},{\"end\":65491,\"start\":65490},{\"end\":65506,\"start\":65505},{\"end\":65512,\"start\":65511},{\"end\":65523,\"start\":65522},{\"end\":65533,\"start\":65532},{\"end\":65535,\"start\":65534},{\"end\":65649,\"start\":65648},{\"end\":65660,\"start\":65659},{\"end\":65781,\"start\":65780},{\"end\":65787,\"start\":65786},{\"end\":65796,\"start\":65795},{\"end\":65803,\"start\":65802},{\"end\":65868,\"start\":65867},{\"end\":65874,\"start\":65873},{\"end\":65883,\"start\":65882},{\"end\":65890,\"start\":65889},{\"end\":65942,\"start\":65941},{\"end\":65948,\"start\":65947},{\"end\":65960,\"start\":65959},{\"end\":66049,\"start\":66048},{\"end\":66055,\"start\":66054},{\"end\":66062,\"start\":66061},{\"end\":66068,\"start\":66067},{\"end\":66075,\"start\":66074},{\"end\":66150,\"start\":66149},{\"end\":66156,\"start\":66155},{\"end\":66163,\"start\":66162},{\"end\":66172,\"start\":66171},{\"end\":66182,\"start\":66181},{\"end\":66191,\"start\":66190},{\"end\":66200,\"start\":66199},{\"end\":66264,\"start\":66263},{\"end\":66273,\"start\":66272},{\"end\":66280,\"start\":66279},{\"end\":66282,\"start\":66281},{\"end\":66296,\"start\":66295},{\"end\":66403,\"start\":66402},{\"end\":66411,\"start\":66410},{\"end\":66494,\"start\":66493},{\"end\":66496,\"start\":66495},{\"end\":66506,\"start\":66505},{\"end\":66515,\"start\":66514},{\"end\":66524,\"start\":66523},{\"end\":66537,\"start\":66536},{\"end\":66548,\"start\":66547},{\"end\":66557,\"start\":66556},{\"end\":66566,\"start\":66565},{\"end\":66576,\"start\":66575},{\"end\":66585,\"start\":66584},{\"end\":66644,\"start\":66643},{\"end\":66646,\"start\":66645},{\"end\":66714,\"start\":66713},{\"end\":66716,\"start\":66715},{\"end\":66725,\"start\":66724},{\"end\":66735,\"start\":66734},{\"end\":66737,\"start\":66736},{\"end\":66748,\"start\":66747},{\"end\":66750,\"start\":66749},{\"end\":66760,\"start\":66759},{\"end\":66862,\"start\":66861},{\"end\":66876,\"start\":66875},{\"end\":66884,\"start\":66883},{\"end\":66964,\"start\":66963},{\"end\":66974,\"start\":66973},{\"end\":67035,\"start\":67034},{\"end\":67048,\"start\":67047},{\"end\":67058,\"start\":67057},{\"end\":67060,\"start\":67059},{\"end\":67080,\"start\":67079},{\"end\":67087,\"start\":67086},{\"end\":67094,\"start\":67093},{\"end\":67105,\"start\":67101},{\"end\":67210,\"start\":67209},{\"end\":67220,\"start\":67219},{\"end\":67228,\"start\":67227},{\"end\":67237,\"start\":67236},{\"end\":67309,\"start\":67308},{\"end\":67323,\"start\":67322},{\"end\":67433,\"start\":67432},{\"end\":67440,\"start\":67439},{\"end\":67448,\"start\":67447},{\"end\":67459,\"start\":67458},{\"end\":67469,\"start\":67468},{\"end\":67479,\"start\":67478},{\"end\":67548,\"start\":67547},{\"end\":67554,\"start\":67553},{\"end\":67567,\"start\":67566},{\"end\":67575,\"start\":67574},{\"end\":67582,\"start\":67581},{\"end\":67598,\"start\":67597},{\"end\":67609,\"start\":67608},{\"end\":67706,\"start\":67705},{\"end\":67712,\"start\":67711},{\"end\":67726,\"start\":67725},{\"end\":67749,\"start\":67748},{\"end\":67756,\"start\":67755},{\"end\":67763,\"start\":67762},{\"end\":67772,\"start\":67771},{\"end\":67778,\"start\":67777},{\"end\":67787,\"start\":67786},{\"end\":67795,\"start\":67794},{\"end\":67803,\"start\":67802},{\"end\":67812,\"start\":67811},{\"end\":67827,\"start\":67826},{\"end\":67994,\"start\":67993},{\"end\":68002,\"start\":68001},{\"end\":68009,\"start\":68008},{\"end\":68017,\"start\":68016},{\"end\":68094,\"start\":68093},{\"end\":68105,\"start\":68104},{\"end\":68117,\"start\":68116},{\"end\":68131,\"start\":68130},{\"end\":68137,\"start\":68136},{\"end\":68147,\"start\":68146},{\"end\":68153,\"start\":68152},{\"end\":68165,\"start\":68164},{\"end\":68195,\"start\":68194},{\"end\":68203,\"start\":68202},{\"end\":68212,\"start\":68211},{\"end\":68223,\"start\":68222},{\"end\":68235,\"start\":68234},{\"end\":68316,\"start\":68315},{\"end\":68327,\"start\":68326},{\"end\":68338,\"start\":68337},{\"end\":68480,\"start\":68479},{\"end\":68493,\"start\":68492},{\"end\":68503,\"start\":68502},{\"end\":68594,\"start\":68593},{\"end\":68604,\"start\":68603},{\"end\":68615,\"start\":68614},{\"end\":68625,\"start\":68624},{\"end\":68666,\"start\":68665},{\"end\":68668,\"start\":68667},{\"end\":68678,\"start\":68677},{\"end\":68689,\"start\":68688},{\"end\":68702,\"start\":68701},{\"end\":68704,\"start\":68703},{\"end\":68823,\"start\":68822},{\"end\":68840,\"start\":68839},{\"end\":68849,\"start\":68848},{\"end\":68856,\"start\":68855},{\"end\":68858,\"start\":68857},{\"end\":68866,\"start\":68865},{\"end\":68875,\"start\":68874},{\"end\":68877,\"start\":68876},{\"end\":68885,\"start\":68884},{\"end\":68894,\"start\":68893},{\"end\":68902,\"start\":68901},{\"end\":68904,\"start\":68903},{\"end\":68960,\"start\":68959},{\"end\":68980,\"start\":68979},{\"end\":68992,\"start\":68991},{\"end\":69008,\"start\":69007},{\"end\":69010,\"start\":69009},{\"end\":69089,\"start\":69088},{\"end\":69102,\"start\":69101},{\"end\":69111,\"start\":69110},{\"end\":69122,\"start\":69121},{\"end\":69347,\"start\":69346},{\"end\":69362,\"start\":69361},{\"end\":69370,\"start\":69369},{\"end\":69376,\"start\":69375},{\"end\":69386,\"start\":69385},{\"end\":69398,\"start\":69397},{\"end\":69404,\"start\":69403},{\"end\":69413,\"start\":69412},{\"end\":69425,\"start\":69424},{\"end\":69435,\"start\":69434},{\"end\":69514,\"start\":69513},{\"end\":69525,\"start\":69524},{\"end\":69535,\"start\":69534},{\"end\":69542,\"start\":69541},{\"end\":69558,\"start\":69554},{\"end\":69578,\"start\":69577},{\"end\":69586,\"start\":69585},{\"end\":69595,\"start\":69594},{\"end\":69607,\"start\":69606},{\"end\":69762,\"start\":69761},{\"end\":69773,\"start\":69772},{\"end\":69780,\"start\":69779},{\"end\":69787,\"start\":69786},{\"end\":69799,\"start\":69798},{\"end\":69807,\"start\":69806},{\"end\":69819,\"start\":69818},{\"end\":69828,\"start\":69827},{\"end\":69841,\"start\":69840},{\"end\":69920,\"start\":69919},{\"end\":69931,\"start\":69930},{\"end\":69944,\"start\":69943},{\"end\":69953,\"start\":69952},{\"end\":69963,\"start\":69962},{\"end\":70047,\"start\":70046},{\"end\":70054,\"start\":70053},{\"end\":70062,\"start\":70061},{\"end\":70070,\"start\":70069},{\"end\":70083,\"start\":70082},{\"end\":70094,\"start\":70093},{\"end\":70104,\"start\":70103},{\"end\":70106,\"start\":70105},{\"end\":70170,\"start\":70169},{\"end\":70179,\"start\":70178},{\"end\":70277,\"start\":70276},{\"end\":70285,\"start\":70284},{\"end\":70292,\"start\":70291},{\"end\":70301,\"start\":70300},{\"end\":70313,\"start\":70312},{\"end\":70323,\"start\":70322},{\"end\":70403,\"start\":70402},{\"end\":70405,\"start\":70404},{\"end\":70413,\"start\":70412},{\"end\":70415,\"start\":70414},{\"end\":70425,\"start\":70424},{\"end\":70501,\"start\":70500},{\"end\":70521,\"start\":70520},{\"end\":70533,\"start\":70532},{\"end\":70544,\"start\":70543},{\"end\":70546,\"start\":70545},{\"end\":70558,\"start\":70557},{\"end\":70571,\"start\":70570},{\"end\":70580,\"start\":70579},{\"end\":70594,\"start\":70593},{\"end\":70606,\"start\":70605},{\"end\":70618,\"start\":70617},{\"end\":70631,\"start\":70630},{\"end\":70641,\"start\":70640},{\"end\":70643,\"start\":70642},{\"end\":70659,\"start\":70658},{\"end\":70668,\"start\":70667},{\"end\":70678,\"start\":70677},{\"end\":70680,\"start\":70679},{\"end\":70691,\"start\":70690},{\"end\":70702,\"start\":70701},{\"end\":70706,\"start\":70703},{\"end\":70716,\"start\":70715},{\"end\":70725,\"start\":70724},{\"end\":70733,\"start\":70732},{\"end\":70743,\"start\":70742},{\"end\":70745,\"start\":70744},{\"end\":70754,\"start\":70753},{\"end\":70763,\"start\":70762},{\"end\":70771,\"start\":70770},{\"end\":70773,\"start\":70772},{\"end\":70782,\"start\":70781},{\"end\":70797,\"start\":70796},{\"end\":70808,\"start\":70807},{\"end\":70820,\"start\":70819},{\"end\":70831,\"start\":70830},{\"end\":70844,\"start\":70843},{\"end\":70846,\"start\":70845},{\"end\":70858,\"start\":70857},{\"end\":70860,\"start\":70859},{\"end\":70870,\"start\":70869},{\"end\":70872,\"start\":70871},{\"end\":70885,\"start\":70884},{\"end\":70887,\"start\":70886},{\"end\":70898,\"start\":70897},{\"end\":71143,\"start\":71142},{\"end\":71151,\"start\":71150},{\"end\":71160,\"start\":71159},{\"end\":71171,\"start\":71170},{\"end\":71179,\"start\":71178},{\"end\":71187,\"start\":71186},{\"end\":71189,\"start\":71188},{\"end\":71272,\"start\":71271},{\"end\":71280,\"start\":71279},{\"end\":71292,\"start\":71291},{\"end\":71301,\"start\":71300},{\"end\":71309,\"start\":71308},{\"end\":71321,\"start\":71320},{\"end\":71328,\"start\":71327},{\"end\":71338,\"start\":71337},{\"end\":71351,\"start\":71350},{\"end\":71362,\"start\":71361},{\"end\":71450,\"start\":71449},{\"end\":71458,\"start\":71457},{\"end\":71466,\"start\":71465},{\"end\":71468,\"start\":71467},{\"end\":71479,\"start\":71478},{\"end\":71488,\"start\":71487},{\"end\":71586,\"start\":71585},{\"end\":71594,\"start\":71593},{\"end\":71601,\"start\":71600},{\"end\":71609,\"start\":71608},{\"end\":71622,\"start\":71621},{\"end\":71624,\"start\":71623},{\"end\":71641,\"start\":71640},{\"end\":71643,\"start\":71642},{\"end\":71662,\"start\":71661},{\"end\":71674,\"start\":71673},{\"end\":71683,\"start\":71682},{\"end\":71693,\"start\":71692},{\"end\":71809,\"start\":71808},{\"end\":71816,\"start\":71815},{\"end\":71823,\"start\":71822},{\"end\":71831,\"start\":71830},{\"end\":71890,\"start\":71889},{\"end\":71892,\"start\":71891},{\"end\":71901,\"start\":71900},{\"end\":71908,\"start\":71907},{\"end\":71916,\"start\":71915},{\"end\":71918,\"start\":71917},{\"end\":71928,\"start\":71927},{\"end\":71937,\"start\":71936},{\"end\":72062,\"start\":72061},{\"end\":72070,\"start\":72069},{\"end\":72084,\"start\":72083},{\"end\":72098,\"start\":72097},{\"end\":72109,\"start\":72108},{\"end\":72121,\"start\":72120},{\"end\":72130,\"start\":72129},{\"end\":72142,\"start\":72141},{\"end\":72144,\"start\":72143},{\"end\":72153,\"start\":72152},{\"end\":72164,\"start\":72163},{\"end\":72179,\"start\":72178},{\"end\":72188,\"start\":72187},{\"end\":72198,\"start\":72197},{\"end\":72211,\"start\":72210},{\"end\":72224,\"start\":72223},{\"end\":72236,\"start\":72235},{\"end\":72245,\"start\":72244}]", "bib_author_last_name": "[{\"end\":62789,\"start\":62784},{\"end\":62799,\"start\":62793},{\"end\":62807,\"start\":62803},{\"end\":62815,\"start\":62811},{\"end\":62824,\"start\":62819},{\"end\":62832,\"start\":62828},{\"end\":62841,\"start\":62836},{\"end\":62853,\"start\":62845},{\"end\":62863,\"start\":62857},{\"end\":62872,\"start\":62867},{\"end\":62953,\"start\":62944},{\"end\":62965,\"start\":62957},{\"end\":63057,\"start\":63051},{\"end\":63068,\"start\":63061},{\"end\":63079,\"start\":63072},{\"end\":63086,\"start\":63083},{\"end\":63094,\"start\":63090},{\"end\":63107,\"start\":63098},{\"end\":63115,\"start\":63111},{\"end\":63126,\"start\":63119},{\"end\":63136,\"start\":63130},{\"end\":63146,\"start\":63140},{\"end\":63155,\"start\":63150},{\"end\":63166,\"start\":63159},{\"end\":63177,\"start\":63170},{\"end\":63186,\"start\":63181},{\"end\":63194,\"start\":63190},{\"end\":63204,\"start\":63198},{\"end\":63215,\"start\":63208},{\"end\":63225,\"start\":63219},{\"end\":63235,\"start\":63229},{\"end\":63241,\"start\":63239},{\"end\":63251,\"start\":63245},{\"end\":63331,\"start\":63327},{\"end\":63338,\"start\":63335},{\"end\":63349,\"start\":63342},{\"end\":63364,\"start\":63355},{\"end\":63376,\"start\":63370},{\"end\":63391,\"start\":63382},{\"end\":63401,\"start\":63393},{\"end\":63467,\"start\":63461},{\"end\":63552,\"start\":63548},{\"end\":63565,\"start\":63556},{\"end\":63576,\"start\":63569},{\"end\":63586,\"start\":63580},{\"end\":63607,\"start\":63603},{\"end\":63614,\"start\":63611},{\"end\":63626,\"start\":63618},{\"end\":63632,\"start\":63630},{\"end\":63753,\"start\":63747},{\"end\":63761,\"start\":63757},{\"end\":63773,\"start\":63765},{\"end\":63784,\"start\":63777},{\"end\":63795,\"start\":63788},{\"end\":63816,\"start\":63811},{\"end\":63828,\"start\":63823},{\"end\":63836,\"start\":63834},{\"end\":63849,\"start\":63842},{\"end\":63859,\"start\":63851},{\"end\":63950,\"start\":63945},{\"end\":64186,\"start\":64177},{\"end\":64283,\"start\":64279},{\"end\":64291,\"start\":64287},{\"end\":64301,\"start\":64295},{\"end\":64310,\"start\":64308},{\"end\":64316,\"start\":64314},{\"end\":64327,\"start\":64320},{\"end\":64425,\"start\":64419},{\"end\":64437,\"start\":64432},{\"end\":64444,\"start\":64441},{\"end\":64457,\"start\":64448},{\"end\":64463,\"start\":64459},{\"end\":64566,\"start\":64559},{\"end\":64573,\"start\":64570},{\"end\":64584,\"start\":64577},{\"end\":64595,\"start\":64588},{\"end\":64604,\"start\":64599},{\"end\":64613,\"start\":64608},{\"end\":64624,\"start\":64617},{\"end\":64700,\"start\":64695},{\"end\":64713,\"start\":64704},{\"end\":64723,\"start\":64717},{\"end\":64734,\"start\":64727},{\"end\":64781,\"start\":64776},{\"end\":64790,\"start\":64785},{\"end\":64803,\"start\":64794},{\"end\":64954,\"start\":64947},{\"end\":64964,\"start\":64958},{\"end\":64974,\"start\":64968},{\"end\":64983,\"start\":64978},{\"end\":64996,\"start\":64987},{\"end\":65117,\"start\":65113},{\"end\":65123,\"start\":65121},{\"end\":65136,\"start\":65127},{\"end\":65143,\"start\":65138},{\"end\":65249,\"start\":65241},{\"end\":65260,\"start\":65253},{\"end\":65271,\"start\":65264},{\"end\":65280,\"start\":65275},{\"end\":65349,\"start\":65345},{\"end\":65475,\"start\":65465},{\"end\":65488,\"start\":65479},{\"end\":65503,\"start\":65492},{\"end\":65509,\"start\":65507},{\"end\":65520,\"start\":65513},{\"end\":65530,\"start\":65524},{\"end\":65541,\"start\":65536},{\"end\":65657,\"start\":65650},{\"end\":65670,\"start\":65661},{\"end\":65784,\"start\":65782},{\"end\":65793,\"start\":65788},{\"end\":65800,\"start\":65797},{\"end\":65807,\"start\":65804},{\"end\":65871,\"start\":65869},{\"end\":65880,\"start\":65875},{\"end\":65887,\"start\":65884},{\"end\":65894,\"start\":65891},{\"end\":65945,\"start\":65943},{\"end\":65957,\"start\":65949},{\"end\":65967,\"start\":65961},{\"end\":66052,\"start\":66050},{\"end\":66059,\"start\":66056},{\"end\":66065,\"start\":66063},{\"end\":66072,\"start\":66069},{\"end\":66084,\"start\":66076},{\"end\":66153,\"start\":66151},{\"end\":66160,\"start\":66157},{\"end\":66169,\"start\":66164},{\"end\":66179,\"start\":66173},{\"end\":66188,\"start\":66183},{\"end\":66197,\"start\":66192},{\"end\":66209,\"start\":66201},{\"end\":66270,\"start\":66265},{\"end\":66277,\"start\":66274},{\"end\":66293,\"start\":66283},{\"end\":66311,\"start\":66297},{\"end\":66408,\"start\":66404},{\"end\":66416,\"start\":66412},{\"end\":66503,\"start\":66497},{\"end\":66512,\"start\":66507},{\"end\":66521,\"start\":66516},{\"end\":66534,\"start\":66525},{\"end\":66545,\"start\":66538},{\"end\":66554,\"start\":66549},{\"end\":66563,\"start\":66558},{\"end\":66573,\"start\":66567},{\"end\":66582,\"start\":66577},{\"end\":66594,\"start\":66586},{\"end\":66654,\"start\":66647},{\"end\":66722,\"start\":66717},{\"end\":66732,\"start\":66726},{\"end\":66745,\"start\":66738},{\"end\":66757,\"start\":66751},{\"end\":66767,\"start\":66761},{\"end\":66873,\"start\":66863},{\"end\":66881,\"start\":66877},{\"end\":66890,\"start\":66885},{\"end\":66971,\"start\":66965},{\"end\":66983,\"start\":66975},{\"end\":67045,\"start\":67036},{\"end\":67055,\"start\":67049},{\"end\":67063,\"start\":67061},{\"end\":67084,\"start\":67081},{\"end\":67091,\"start\":67088},{\"end\":67099,\"start\":67095},{\"end\":67217,\"start\":67211},{\"end\":67225,\"start\":67221},{\"end\":67234,\"start\":67229},{\"end\":67245,\"start\":67238},{\"end\":67320,\"start\":67310},{\"end\":67330,\"start\":67324},{\"end\":67437,\"start\":67434},{\"end\":67445,\"start\":67441},{\"end\":67456,\"start\":67449},{\"end\":67466,\"start\":67460},{\"end\":67476,\"start\":67470},{\"end\":67487,\"start\":67480},{\"end\":67551,\"start\":67549},{\"end\":67564,\"start\":67555},{\"end\":67572,\"start\":67568},{\"end\":67579,\"start\":67576},{\"end\":67595,\"start\":67583},{\"end\":67606,\"start\":67599},{\"end\":67616,\"start\":67610},{\"end\":67709,\"start\":67707},{\"end\":67723,\"start\":67713},{\"end\":67734,\"start\":67727},{\"end\":67753,\"start\":67750},{\"end\":67760,\"start\":67757},{\"end\":67769,\"start\":67764},{\"end\":67775,\"start\":67773},{\"end\":67784,\"start\":67779},{\"end\":67792,\"start\":67788},{\"end\":67800,\"start\":67796},{\"end\":67809,\"start\":67804},{\"end\":67824,\"start\":67813},{\"end\":67836,\"start\":67828},{\"end\":67845,\"start\":67838},{\"end\":67999,\"start\":67995},{\"end\":68006,\"start\":68003},{\"end\":68014,\"start\":68010},{\"end\":68024,\"start\":68018},{\"end\":68102,\"start\":68095},{\"end\":68114,\"start\":68106},{\"end\":68128,\"start\":68118},{\"end\":68134,\"start\":68132},{\"end\":68144,\"start\":68138},{\"end\":68150,\"start\":68148},{\"end\":68162,\"start\":68154},{\"end\":68180,\"start\":68166},{\"end\":68200,\"start\":68196},{\"end\":68209,\"start\":68204},{\"end\":68220,\"start\":68213},{\"end\":68232,\"start\":68224},{\"end\":68243,\"start\":68236},{\"end\":68249,\"start\":68245},{\"end\":68324,\"start\":68317},{\"end\":68335,\"start\":68328},{\"end\":68347,\"start\":68339},{\"end\":68357,\"start\":68349},{\"end\":68490,\"start\":68481},{\"end\":68500,\"start\":68494},{\"end\":68509,\"start\":68504},{\"end\":68601,\"start\":68595},{\"end\":68612,\"start\":68605},{\"end\":68622,\"start\":68616},{\"end\":68636,\"start\":68626},{\"end\":68675,\"start\":68669},{\"end\":68686,\"start\":68679},{\"end\":68699,\"start\":68690},{\"end\":68712,\"start\":68705},{\"end\":68837,\"start\":68824},{\"end\":68846,\"start\":68841},{\"end\":68853,\"start\":68850},{\"end\":68863,\"start\":68859},{\"end\":68872,\"start\":68867},{\"end\":68882,\"start\":68878},{\"end\":68891,\"start\":68886},{\"end\":68899,\"start\":68895},{\"end\":68911,\"start\":68905},{\"end\":68977,\"start\":68961},{\"end\":68989,\"start\":68981},{\"end\":69005,\"start\":68993},{\"end\":69019,\"start\":69011},{\"end\":69099,\"start\":69090},{\"end\":69108,\"start\":69103},{\"end\":69119,\"start\":69112},{\"end\":69128,\"start\":69123},{\"end\":69359,\"start\":69348},{\"end\":69367,\"start\":69363},{\"end\":69373,\"start\":69371},{\"end\":69383,\"start\":69377},{\"end\":69395,\"start\":69387},{\"end\":69401,\"start\":69399},{\"end\":69410,\"start\":69405},{\"end\":69422,\"start\":69414},{\"end\":69432,\"start\":69426},{\"end\":69445,\"start\":69436},{\"end\":69522,\"start\":69515},{\"end\":69532,\"start\":69526},{\"end\":69539,\"start\":69536},{\"end\":69552,\"start\":69543},{\"end\":69563,\"start\":69559},{\"end\":69583,\"start\":69579},{\"end\":69592,\"start\":69587},{\"end\":69604,\"start\":69596},{\"end\":69612,\"start\":69608},{\"end\":69624,\"start\":69614},{\"end\":69770,\"start\":69763},{\"end\":69777,\"start\":69774},{\"end\":69784,\"start\":69781},{\"end\":69796,\"start\":69788},{\"end\":69804,\"start\":69800},{\"end\":69816,\"start\":69808},{\"end\":69825,\"start\":69820},{\"end\":69838,\"start\":69829},{\"end\":69852,\"start\":69842},{\"end\":69928,\"start\":69921},{\"end\":69941,\"start\":69932},{\"end\":69950,\"start\":69945},{\"end\":69960,\"start\":69954},{\"end\":69969,\"start\":69964},{\"end\":70051,\"start\":70048},{\"end\":70059,\"start\":70055},{\"end\":70067,\"start\":70063},{\"end\":70080,\"start\":70071},{\"end\":70091,\"start\":70084},{\"end\":70101,\"start\":70095},{\"end\":70109,\"start\":70107},{\"end\":70176,\"start\":70171},{\"end\":70185,\"start\":70180},{\"end\":70282,\"start\":70278},{\"end\":70289,\"start\":70286},{\"end\":70298,\"start\":70293},{\"end\":70310,\"start\":70302},{\"end\":70320,\"start\":70314},{\"end\":70329,\"start\":70324},{\"end\":70410,\"start\":70406},{\"end\":70422,\"start\":70416},{\"end\":70433,\"start\":70426},{\"end\":70507,\"start\":70502},{\"end\":70530,\"start\":70522},{\"end\":70541,\"start\":70534},{\"end\":70555,\"start\":70547},{\"end\":70568,\"start\":70559},{\"end\":70577,\"start\":70572},{\"end\":70591,\"start\":70581},{\"end\":70603,\"start\":70595},{\"end\":70615,\"start\":70607},{\"end\":70628,\"start\":70619},{\"end\":70638,\"start\":70632},{\"end\":70656,\"start\":70644},{\"end\":70665,\"start\":70660},{\"end\":70675,\"start\":70669},{\"end\":70688,\"start\":70681},{\"end\":70699,\"start\":70692},{\"end\":70713,\"start\":70707},{\"end\":70722,\"start\":70717},{\"end\":70730,\"start\":70726},{\"end\":70740,\"start\":70734},{\"end\":70751,\"start\":70746},{\"end\":70760,\"start\":70755},{\"end\":70768,\"start\":70764},{\"end\":70779,\"start\":70774},{\"end\":70794,\"start\":70783},{\"end\":70805,\"start\":70798},{\"end\":70817,\"start\":70809},{\"end\":70828,\"start\":70821},{\"end\":70841,\"start\":70832},{\"end\":70855,\"start\":70847},{\"end\":70867,\"start\":70861},{\"end\":70882,\"start\":70873},{\"end\":70895,\"start\":70888},{\"end\":70908,\"start\":70899},{\"end\":70922,\"start\":70910},{\"end\":71148,\"start\":71144},{\"end\":71157,\"start\":71152},{\"end\":71168,\"start\":71161},{\"end\":71176,\"start\":71172},{\"end\":71184,\"start\":71180},{\"end\":71196,\"start\":71190},{\"end\":71277,\"start\":71273},{\"end\":71289,\"start\":71281},{\"end\":71298,\"start\":71293},{\"end\":71306,\"start\":71302},{\"end\":71318,\"start\":71310},{\"end\":71325,\"start\":71322},{\"end\":71335,\"start\":71329},{\"end\":71348,\"start\":71339},{\"end\":71359,\"start\":71352},{\"end\":71371,\"start\":71363},{\"end\":71455,\"start\":71451},{\"end\":71463,\"start\":71459},{\"end\":71476,\"start\":71469},{\"end\":71485,\"start\":71480},{\"end\":71497,\"start\":71489},{\"end\":71591,\"start\":71587},{\"end\":71598,\"start\":71595},{\"end\":71606,\"start\":71602},{\"end\":71619,\"start\":71610},{\"end\":71638,\"start\":71625},{\"end\":71646,\"start\":71644},{\"end\":71671,\"start\":71663},{\"end\":71680,\"start\":71675},{\"end\":71690,\"start\":71684},{\"end\":71700,\"start\":71694},{\"end\":71813,\"start\":71810},{\"end\":71820,\"start\":71817},{\"end\":71828,\"start\":71824},{\"end\":71836,\"start\":71832},{\"end\":71898,\"start\":71893},{\"end\":71905,\"start\":71902},{\"end\":71913,\"start\":71909},{\"end\":71925,\"start\":71919},{\"end\":71934,\"start\":71929},{\"end\":71946,\"start\":71938},{\"end\":71957,\"start\":71948},{\"end\":72067,\"start\":72063},{\"end\":72081,\"start\":72071},{\"end\":72095,\"start\":72085},{\"end\":72106,\"start\":72099},{\"end\":72118,\"start\":72110},{\"end\":72127,\"start\":72122},{\"end\":72139,\"start\":72131},{\"end\":72150,\"start\":72145},{\"end\":72161,\"start\":72154},{\"end\":72176,\"start\":72165},{\"end\":72185,\"start\":72180},{\"end\":72195,\"start\":72189},{\"end\":72208,\"start\":72199},{\"end\":72221,\"start\":72212},{\"end\":72233,\"start\":72225},{\"end\":72242,\"start\":72237},{\"end\":72253,\"start\":72246}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":62884,\"start\":62727},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13967968},\"end\":62977,\"start\":62886},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202786778},\"end\":63266,\"start\":62979},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10860374},\"end\":63413,\"start\":63268},{\"attributes\":{\"id\":\"b4\"},\"end\":63473,\"start\":63415},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211096730},\"end\":63599,\"start\":63475},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b6\"},\"end\":63710,\"start\":63601},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4309276},\"end\":63807,\"start\":63712},{\"attributes\":{\"id\":\"b8\"},\"end\":63943,\"start\":63809},{\"attributes\":{\"id\":\"b9\"},\"end\":64066,\"start\":63945},{\"attributes\":{\"doi\":\"arXiv:1703.00091\",\"id\":\"b10\"},\"end\":64222,\"start\":64068},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57246310},\"end\":64339,\"start\":64224},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":226096901},\"end\":64476,\"start\":64341},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6161478},\"end\":64636,\"start\":64478},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15796526},\"end\":64749,\"start\":64638},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6249357},\"end\":64815,\"start\":64751},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2156851},\"end\":65028,\"start\":64817},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207853043},\"end\":65155,\"start\":65030},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215827080},\"end\":65292,\"start\":65157},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":118754484},\"end\":65393,\"start\":65294},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":216080466},\"end\":65552,\"start\":65395},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15816723},\"end\":65685,\"start\":65554},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13740328},\"end\":65819,\"start\":65687},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":206594692},\"end\":65906,\"start\":65821},{\"attributes\":{\"id\":\"b24\"},\"end\":65979,\"start\":65908},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":207930212},\"end\":66096,\"start\":65981},{\"attributes\":{\"id\":\"b26\"},\"end\":66219,\"start\":66098},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9433631},\"end\":66323,\"start\":66221},{\"attributes\":{\"id\":\"b28\"},\"end\":66427,\"start\":66325},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4202768},\"end\":66606,\"start\":66429},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":120478295},\"end\":66672,\"start\":66608},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1868455},\"end\":66800,\"start\":66674},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":59292019},\"end\":66902,\"start\":66802},{\"attributes\":{\"id\":\"b33\"},\"end\":66989,\"start\":66904},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":43928547},\"end\":67075,\"start\":66991},{\"attributes\":{\"id\":\"b35\"},\"end\":67153,\"start\":67077},{\"attributes\":{\"id\":\"b36\"},\"end\":67251,\"start\":67155},{\"attributes\":{\"id\":\"b37\"},\"end\":67352,\"start\":67253},{\"attributes\":{\"id\":\"b38\"},\"end\":67497,\"start\":67354},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":211532533},\"end\":67628,\"start\":67499},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3603048},\"end\":67746,\"start\":67630},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b41\"},\"end\":67929,\"start\":67748},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":556999},\"end\":68036,\"start\":67931},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":13751202},\"end\":68192,\"start\":68038},{\"attributes\":{\"doi\":\"arXiv:1306.5151\",\"id\":\"b44\"},\"end\":68313,\"start\":68194},{\"attributes\":{\"id\":\"b45\"},\"end\":68417,\"start\":68315},{\"attributes\":{\"id\":\"b46\"},\"end\":68515,\"start\":68419},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":211572839},\"end\":68648,\"start\":68517},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":383200},\"end\":68724,\"start\":68650},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":220045835},\"end\":68922,\"start\":68726},{\"attributes\":{\"id\":\"b50\"},\"end\":69025,\"start\":68924},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":11816014},\"end\":69293,\"start\":69027},{\"attributes\":{\"id\":\"b52\"},\"end\":69455,\"start\":69295},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":4555207},\"end\":69575,\"start\":69457},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b54\"},\"end\":69727,\"start\":69577},{\"attributes\":{\"id\":\"b55\"},\"end\":69858,\"start\":69729},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":206593880},\"end\":69981,\"start\":69860},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":51891697},\"end\":70121,\"start\":69983},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":60987823},\"end\":70210,\"start\":70123},{\"attributes\":{\"id\":\"b59\"},\"end\":70335,\"start\":70212},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":201303557},\"end\":70445,\"start\":70337},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":8008817},\"end\":70518,\"start\":70447},{\"attributes\":{\"doi\":\"10.1038/s41592-019-0686-2\",\"id\":\"b62\"},\"end\":71053,\"start\":70520},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":5034059},\"end\":71209,\"start\":71055},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":208117506},\"end\":71384,\"start\":71211},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":1309931},\"end\":71509,\"start\":71386},{\"attributes\":{\"id\":\"b66\"},\"end\":71659,\"start\":71511},{\"attributes\":{\"id\":\"b67\"},\"end\":71773,\"start\":71661},{\"attributes\":{\"id\":\"b68\"},\"end\":71849,\"start\":71775},{\"attributes\":{\"id\":\"b69\"},\"end\":71969,\"start\":71851},{\"attributes\":{\"doi\":\"arXiv:1910.04867\",\"id\":\"b70\"},\"end\":72283,\"start\":71971}]", "bib_title": "[{\"end\":62780,\"start\":62727},{\"end\":62940,\"start\":62886},{\"end\":63047,\"start\":62979},{\"end\":63323,\"start\":63268},{\"end\":63544,\"start\":63475},{\"end\":63743,\"start\":63712},{\"end\":64275,\"start\":64224},{\"end\":64415,\"start\":64341},{\"end\":64555,\"start\":64478},{\"end\":64691,\"start\":64638},{\"end\":64772,\"start\":64751},{\"end\":64943,\"start\":64817},{\"end\":65109,\"start\":65030},{\"end\":65237,\"start\":65157},{\"end\":65339,\"start\":65294},{\"end\":65461,\"start\":65395},{\"end\":65646,\"start\":65554},{\"end\":65778,\"start\":65687},{\"end\":65865,\"start\":65821},{\"end\":65939,\"start\":65908},{\"end\":66046,\"start\":65981},{\"end\":66261,\"start\":66221},{\"end\":66491,\"start\":66429},{\"end\":66641,\"start\":66608},{\"end\":66711,\"start\":66674},{\"end\":66859,\"start\":66802},{\"end\":67032,\"start\":66991},{\"end\":67545,\"start\":67499},{\"end\":67703,\"start\":67630},{\"end\":67991,\"start\":67931},{\"end\":68091,\"start\":68038},{\"end\":68591,\"start\":68517},{\"end\":68663,\"start\":68650},{\"end\":68820,\"start\":68726},{\"end\":69086,\"start\":69027},{\"end\":69511,\"start\":69457},{\"end\":69917,\"start\":69860},{\"end\":70044,\"start\":69983},{\"end\":70167,\"start\":70123},{\"end\":70400,\"start\":70337},{\"end\":70498,\"start\":70447},{\"end\":71140,\"start\":71055},{\"end\":71269,\"start\":71211},{\"end\":71447,\"start\":71386},{\"end\":71887,\"start\":71851}]", "bib_author": "[{\"end\":62791,\"start\":62782},{\"end\":62801,\"start\":62791},{\"end\":62809,\"start\":62801},{\"end\":62817,\"start\":62809},{\"end\":62826,\"start\":62817},{\"end\":62834,\"start\":62826},{\"end\":62843,\"start\":62834},{\"end\":62855,\"start\":62843},{\"end\":62865,\"start\":62855},{\"end\":62874,\"start\":62865},{\"end\":62955,\"start\":62942},{\"end\":62967,\"start\":62955},{\"end\":63059,\"start\":63049},{\"end\":63070,\"start\":63059},{\"end\":63081,\"start\":63070},{\"end\":63088,\"start\":63081},{\"end\":63096,\"start\":63088},{\"end\":63109,\"start\":63096},{\"end\":63117,\"start\":63109},{\"end\":63128,\"start\":63117},{\"end\":63138,\"start\":63128},{\"end\":63148,\"start\":63138},{\"end\":63157,\"start\":63148},{\"end\":63168,\"start\":63157},{\"end\":63179,\"start\":63168},{\"end\":63188,\"start\":63179},{\"end\":63196,\"start\":63188},{\"end\":63206,\"start\":63196},{\"end\":63217,\"start\":63206},{\"end\":63227,\"start\":63217},{\"end\":63237,\"start\":63227},{\"end\":63243,\"start\":63237},{\"end\":63253,\"start\":63243},{\"end\":63333,\"start\":63325},{\"end\":63340,\"start\":63333},{\"end\":63351,\"start\":63340},{\"end\":63366,\"start\":63351},{\"end\":63378,\"start\":63366},{\"end\":63393,\"start\":63378},{\"end\":63403,\"start\":63393},{\"end\":63469,\"start\":63457},{\"end\":63554,\"start\":63546},{\"end\":63567,\"start\":63554},{\"end\":63578,\"start\":63567},{\"end\":63588,\"start\":63578},{\"end\":63609,\"start\":63601},{\"end\":63616,\"start\":63609},{\"end\":63628,\"start\":63616},{\"end\":63634,\"start\":63628},{\"end\":63755,\"start\":63745},{\"end\":63763,\"start\":63755},{\"end\":63775,\"start\":63763},{\"end\":63786,\"start\":63775},{\"end\":63797,\"start\":63786},{\"end\":63818,\"start\":63809},{\"end\":63830,\"start\":63818},{\"end\":63838,\"start\":63830},{\"end\":63851,\"start\":63838},{\"end\":63861,\"start\":63851},{\"end\":63952,\"start\":63945},{\"end\":64188,\"start\":64175},{\"end\":64285,\"start\":64277},{\"end\":64293,\"start\":64285},{\"end\":64303,\"start\":64293},{\"end\":64312,\"start\":64303},{\"end\":64318,\"start\":64312},{\"end\":64329,\"start\":64318},{\"end\":64427,\"start\":64417},{\"end\":64439,\"start\":64427},{\"end\":64446,\"start\":64439},{\"end\":64459,\"start\":64446},{\"end\":64465,\"start\":64459},{\"end\":64568,\"start\":64557},{\"end\":64575,\"start\":64568},{\"end\":64586,\"start\":64575},{\"end\":64597,\"start\":64586},{\"end\":64606,\"start\":64597},{\"end\":64615,\"start\":64606},{\"end\":64626,\"start\":64615},{\"end\":64702,\"start\":64693},{\"end\":64715,\"start\":64702},{\"end\":64725,\"start\":64715},{\"end\":64736,\"start\":64725},{\"end\":64783,\"start\":64774},{\"end\":64792,\"start\":64783},{\"end\":64805,\"start\":64792},{\"end\":64956,\"start\":64945},{\"end\":64966,\"start\":64956},{\"end\":64976,\"start\":64966},{\"end\":64985,\"start\":64976},{\"end\":64998,\"start\":64985},{\"end\":65119,\"start\":65111},{\"end\":65125,\"start\":65119},{\"end\":65138,\"start\":65125},{\"end\":65145,\"start\":65138},{\"end\":65251,\"start\":65239},{\"end\":65262,\"start\":65251},{\"end\":65273,\"start\":65262},{\"end\":65282,\"start\":65273},{\"end\":65351,\"start\":65341},{\"end\":65477,\"start\":65463},{\"end\":65490,\"start\":65477},{\"end\":65505,\"start\":65490},{\"end\":65511,\"start\":65505},{\"end\":65522,\"start\":65511},{\"end\":65532,\"start\":65522},{\"end\":65543,\"start\":65532},{\"end\":65659,\"start\":65648},{\"end\":65672,\"start\":65659},{\"end\":65786,\"start\":65780},{\"end\":65795,\"start\":65786},{\"end\":65802,\"start\":65795},{\"end\":65809,\"start\":65802},{\"end\":65873,\"start\":65867},{\"end\":65882,\"start\":65873},{\"end\":65889,\"start\":65882},{\"end\":65896,\"start\":65889},{\"end\":65947,\"start\":65941},{\"end\":65959,\"start\":65947},{\"end\":65969,\"start\":65959},{\"end\":66054,\"start\":66048},{\"end\":66061,\"start\":66054},{\"end\":66067,\"start\":66061},{\"end\":66074,\"start\":66067},{\"end\":66086,\"start\":66074},{\"end\":66155,\"start\":66149},{\"end\":66162,\"start\":66155},{\"end\":66171,\"start\":66162},{\"end\":66181,\"start\":66171},{\"end\":66190,\"start\":66181},{\"end\":66199,\"start\":66190},{\"end\":66211,\"start\":66199},{\"end\":66272,\"start\":66263},{\"end\":66279,\"start\":66272},{\"end\":66295,\"start\":66279},{\"end\":66313,\"start\":66295},{\"end\":66410,\"start\":66402},{\"end\":66418,\"start\":66410},{\"end\":66505,\"start\":66493},{\"end\":66514,\"start\":66505},{\"end\":66523,\"start\":66514},{\"end\":66536,\"start\":66523},{\"end\":66547,\"start\":66536},{\"end\":66556,\"start\":66547},{\"end\":66565,\"start\":66556},{\"end\":66575,\"start\":66565},{\"end\":66584,\"start\":66575},{\"end\":66596,\"start\":66584},{\"end\":66656,\"start\":66643},{\"end\":66724,\"start\":66713},{\"end\":66734,\"start\":66724},{\"end\":66747,\"start\":66734},{\"end\":66759,\"start\":66747},{\"end\":66769,\"start\":66759},{\"end\":66875,\"start\":66861},{\"end\":66883,\"start\":66875},{\"end\":66892,\"start\":66883},{\"end\":66973,\"start\":66963},{\"end\":66985,\"start\":66973},{\"end\":67047,\"start\":67034},{\"end\":67057,\"start\":67047},{\"end\":67065,\"start\":67057},{\"end\":67086,\"start\":67079},{\"end\":67093,\"start\":67086},{\"end\":67101,\"start\":67093},{\"end\":67108,\"start\":67101},{\"end\":67219,\"start\":67209},{\"end\":67227,\"start\":67219},{\"end\":67236,\"start\":67227},{\"end\":67247,\"start\":67236},{\"end\":67322,\"start\":67308},{\"end\":67332,\"start\":67322},{\"end\":67439,\"start\":67432},{\"end\":67447,\"start\":67439},{\"end\":67458,\"start\":67447},{\"end\":67468,\"start\":67458},{\"end\":67478,\"start\":67468},{\"end\":67489,\"start\":67478},{\"end\":67553,\"start\":67547},{\"end\":67566,\"start\":67553},{\"end\":67574,\"start\":67566},{\"end\":67581,\"start\":67574},{\"end\":67597,\"start\":67581},{\"end\":67608,\"start\":67597},{\"end\":67618,\"start\":67608},{\"end\":67711,\"start\":67705},{\"end\":67725,\"start\":67711},{\"end\":67736,\"start\":67725},{\"end\":67755,\"start\":67748},{\"end\":67762,\"start\":67755},{\"end\":67771,\"start\":67762},{\"end\":67777,\"start\":67771},{\"end\":67786,\"start\":67777},{\"end\":67794,\"start\":67786},{\"end\":67802,\"start\":67794},{\"end\":67811,\"start\":67802},{\"end\":67826,\"start\":67811},{\"end\":67838,\"start\":67826},{\"end\":67847,\"start\":67838},{\"end\":68001,\"start\":67993},{\"end\":68008,\"start\":68001},{\"end\":68016,\"start\":68008},{\"end\":68026,\"start\":68016},{\"end\":68104,\"start\":68093},{\"end\":68116,\"start\":68104},{\"end\":68130,\"start\":68116},{\"end\":68136,\"start\":68130},{\"end\":68146,\"start\":68136},{\"end\":68152,\"start\":68146},{\"end\":68164,\"start\":68152},{\"end\":68182,\"start\":68164},{\"end\":68202,\"start\":68194},{\"end\":68211,\"start\":68202},{\"end\":68222,\"start\":68211},{\"end\":68234,\"start\":68222},{\"end\":68245,\"start\":68234},{\"end\":68251,\"start\":68245},{\"end\":68326,\"start\":68315},{\"end\":68337,\"start\":68326},{\"end\":68349,\"start\":68337},{\"end\":68359,\"start\":68349},{\"end\":68492,\"start\":68479},{\"end\":68502,\"start\":68492},{\"end\":68511,\"start\":68502},{\"end\":68603,\"start\":68593},{\"end\":68614,\"start\":68603},{\"end\":68624,\"start\":68614},{\"end\":68638,\"start\":68624},{\"end\":68677,\"start\":68665},{\"end\":68688,\"start\":68677},{\"end\":68701,\"start\":68688},{\"end\":68714,\"start\":68701},{\"end\":68839,\"start\":68822},{\"end\":68848,\"start\":68839},{\"end\":68855,\"start\":68848},{\"end\":68865,\"start\":68855},{\"end\":68874,\"start\":68865},{\"end\":68884,\"start\":68874},{\"end\":68893,\"start\":68884},{\"end\":68901,\"start\":68893},{\"end\":68913,\"start\":68901},{\"end\":68979,\"start\":68959},{\"end\":68991,\"start\":68979},{\"end\":69007,\"start\":68991},{\"end\":69021,\"start\":69007},{\"end\":69101,\"start\":69088},{\"end\":69110,\"start\":69101},{\"end\":69121,\"start\":69110},{\"end\":69130,\"start\":69121},{\"end\":69361,\"start\":69346},{\"end\":69369,\"start\":69361},{\"end\":69375,\"start\":69369},{\"end\":69385,\"start\":69375},{\"end\":69397,\"start\":69385},{\"end\":69403,\"start\":69397},{\"end\":69412,\"start\":69403},{\"end\":69424,\"start\":69412},{\"end\":69434,\"start\":69424},{\"end\":69447,\"start\":69434},{\"end\":69524,\"start\":69513},{\"end\":69534,\"start\":69524},{\"end\":69541,\"start\":69534},{\"end\":69554,\"start\":69541},{\"end\":69565,\"start\":69554},{\"end\":69585,\"start\":69577},{\"end\":69594,\"start\":69585},{\"end\":69606,\"start\":69594},{\"end\":69614,\"start\":69606},{\"end\":69626,\"start\":69614},{\"end\":69772,\"start\":69761},{\"end\":69779,\"start\":69772},{\"end\":69786,\"start\":69779},{\"end\":69798,\"start\":69786},{\"end\":69806,\"start\":69798},{\"end\":69818,\"start\":69806},{\"end\":69827,\"start\":69818},{\"end\":69840,\"start\":69827},{\"end\":69854,\"start\":69840},{\"end\":69930,\"start\":69919},{\"end\":69943,\"start\":69930},{\"end\":69952,\"start\":69943},{\"end\":69962,\"start\":69952},{\"end\":69971,\"start\":69962},{\"end\":70053,\"start\":70046},{\"end\":70061,\"start\":70053},{\"end\":70069,\"start\":70061},{\"end\":70082,\"start\":70069},{\"end\":70093,\"start\":70082},{\"end\":70103,\"start\":70093},{\"end\":70111,\"start\":70103},{\"end\":70178,\"start\":70169},{\"end\":70187,\"start\":70178},{\"end\":70284,\"start\":70276},{\"end\":70291,\"start\":70284},{\"end\":70300,\"start\":70291},{\"end\":70312,\"start\":70300},{\"end\":70322,\"start\":70312},{\"end\":70331,\"start\":70322},{\"end\":70412,\"start\":70402},{\"end\":70424,\"start\":70412},{\"end\":70435,\"start\":70424},{\"end\":70509,\"start\":70500},{\"end\":70532,\"start\":70520},{\"end\":70543,\"start\":70532},{\"end\":70557,\"start\":70543},{\"end\":70570,\"start\":70557},{\"end\":70579,\"start\":70570},{\"end\":70593,\"start\":70579},{\"end\":70605,\"start\":70593},{\"end\":70617,\"start\":70605},{\"end\":70630,\"start\":70617},{\"end\":70640,\"start\":70630},{\"end\":70658,\"start\":70640},{\"end\":70667,\"start\":70658},{\"end\":70677,\"start\":70667},{\"end\":70690,\"start\":70677},{\"end\":70701,\"start\":70690},{\"end\":70715,\"start\":70701},{\"end\":70724,\"start\":70715},{\"end\":70732,\"start\":70724},{\"end\":70742,\"start\":70732},{\"end\":70753,\"start\":70742},{\"end\":70762,\"start\":70753},{\"end\":70770,\"start\":70762},{\"end\":70781,\"start\":70770},{\"end\":70796,\"start\":70781},{\"end\":70807,\"start\":70796},{\"end\":70819,\"start\":70807},{\"end\":70830,\"start\":70819},{\"end\":70843,\"start\":70830},{\"end\":70857,\"start\":70843},{\"end\":70869,\"start\":70857},{\"end\":70884,\"start\":70869},{\"end\":70897,\"start\":70884},{\"end\":70910,\"start\":70897},{\"end\":70924,\"start\":70910},{\"end\":71150,\"start\":71142},{\"end\":71159,\"start\":71150},{\"end\":71170,\"start\":71159},{\"end\":71178,\"start\":71170},{\"end\":71186,\"start\":71178},{\"end\":71198,\"start\":71186},{\"end\":71279,\"start\":71271},{\"end\":71291,\"start\":71279},{\"end\":71300,\"start\":71291},{\"end\":71308,\"start\":71300},{\"end\":71320,\"start\":71308},{\"end\":71327,\"start\":71320},{\"end\":71337,\"start\":71327},{\"end\":71350,\"start\":71337},{\"end\":71361,\"start\":71350},{\"end\":71373,\"start\":71361},{\"end\":71457,\"start\":71449},{\"end\":71465,\"start\":71457},{\"end\":71478,\"start\":71465},{\"end\":71487,\"start\":71478},{\"end\":71499,\"start\":71487},{\"end\":71593,\"start\":71585},{\"end\":71600,\"start\":71593},{\"end\":71608,\"start\":71600},{\"end\":71621,\"start\":71608},{\"end\":71640,\"start\":71621},{\"end\":71648,\"start\":71640},{\"end\":71673,\"start\":71661},{\"end\":71682,\"start\":71673},{\"end\":71692,\"start\":71682},{\"end\":71702,\"start\":71692},{\"end\":71815,\"start\":71808},{\"end\":71822,\"start\":71815},{\"end\":71830,\"start\":71822},{\"end\":71838,\"start\":71830},{\"end\":71900,\"start\":71889},{\"end\":71907,\"start\":71900},{\"end\":71915,\"start\":71907},{\"end\":71927,\"start\":71915},{\"end\":71936,\"start\":71927},{\"end\":71948,\"start\":71936},{\"end\":71959,\"start\":71948},{\"end\":72069,\"start\":72061},{\"end\":72083,\"start\":72069},{\"end\":72097,\"start\":72083},{\"end\":72108,\"start\":72097},{\"end\":72120,\"start\":72108},{\"end\":72129,\"start\":72120},{\"end\":72141,\"start\":72129},{\"end\":72152,\"start\":72141},{\"end\":72163,\"start\":72152},{\"end\":72178,\"start\":72163},{\"end\":72187,\"start\":72178},{\"end\":72197,\"start\":72187},{\"end\":72210,\"start\":72197},{\"end\":72223,\"start\":72210},{\"end\":72235,\"start\":72223},{\"end\":72244,\"start\":72235},{\"end\":72255,\"start\":72244}]", "bib_venue": "[{\"end\":62878,\"start\":62874},{\"end\":62971,\"start\":62967},{\"end\":63260,\"start\":63253},{\"end\":63407,\"start\":63403},{\"end\":63455,\"start\":63415},{\"end\":63592,\"start\":63588},{\"end\":63703,\"start\":63650},{\"end\":63801,\"start\":63797},{\"end\":63937,\"start\":63861},{\"end\":64060,\"start\":63952},{\"end\":64173,\"start\":64068},{\"end\":64333,\"start\":64329},{\"end\":64470,\"start\":64465},{\"end\":64630,\"start\":64626},{\"end\":64743,\"start\":64736},{\"end\":64809,\"start\":64805},{\"end\":65012,\"start\":64998},{\"end\":65149,\"start\":65145},{\"end\":65286,\"start\":65282},{\"end\":65387,\"start\":65351},{\"end\":65546,\"start\":65543},{\"end\":65679,\"start\":65672},{\"end\":65813,\"start\":65809},{\"end\":65900,\"start\":65896},{\"end\":65973,\"start\":65969},{\"end\":66090,\"start\":66086},{\"end\":66147,\"start\":66098},{\"end\":66317,\"start\":66313},{\"end\":66400,\"start\":66325},{\"end\":66600,\"start\":66596},{\"end\":66666,\"start\":66656},{\"end\":66794,\"start\":66769},{\"end\":66896,\"start\":66892},{\"end\":66961,\"start\":66904},{\"end\":67069,\"start\":67065},{\"end\":67147,\"start\":67108},{\"end\":67207,\"start\":67155},{\"end\":67306,\"start\":67253},{\"end\":67430,\"start\":67354},{\"end\":67622,\"start\":67618},{\"end\":67740,\"start\":67736},{\"end\":67909,\"start\":67863},{\"end\":68030,\"start\":68026},{\"end\":68186,\"start\":68182},{\"end\":68307,\"start\":68266},{\"end\":68411,\"start\":68359},{\"end\":68477,\"start\":68419},{\"end\":68642,\"start\":68638},{\"end\":68718,\"start\":68714},{\"end\":68916,\"start\":68913},{\"end\":68957,\"start\":68924},{\"end\":69216,\"start\":69130},{\"end\":69344,\"start\":69295},{\"end\":69569,\"start\":69565},{\"end\":69707,\"start\":69642},{\"end\":69759,\"start\":69729},{\"end\":69975,\"start\":69971},{\"end\":70115,\"start\":70111},{\"end\":70204,\"start\":70187},{\"end\":70274,\"start\":70212},{\"end\":70439,\"start\":70435},{\"end\":70512,\"start\":70509},{\"end\":71045,\"start\":70949},{\"end\":71203,\"start\":71198},{\"end\":71378,\"start\":71373},{\"end\":71503,\"start\":71499},{\"end\":71583,\"start\":71511},{\"end\":71767,\"start\":71702},{\"end\":71806,\"start\":71775},{\"end\":71963,\"start\":71959},{\"end\":72059,\"start\":71971},{\"end\":69289,\"start\":69218}]"}}}, "year": 2023, "month": 12, "day": 17}