{"id": 237532437, "updated": "2023-10-05 23:07:56.861", "metadata": {"title": "Context-aware Entity Typing in Knowledge Graphs", "authors": "[{\"first\":\"Weiran\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Xian-Ling\",\"last\":\"Mao\",\"middle\":[]}]", "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021", "journal": "Findings of the Association for Computational Linguistics: EMNLP 2021", "publication_date": {"year": 2021, "month": 9, "day": 16}, "abstract": "Knowledge graph entity typing aims to infer entities' missing types in knowledge graphs which is an important but under-explored issue. This paper proposes a novel method for this task by utilizing entities' contextual information. Specifically, we design two inference mechanisms: i) N2T: independently use each neighbor of an entity to infer its type; ii) Agg2T: aggregate the neighbors of an entity to infer its type. Those mechanisms will produce multiple inference results, and an exponentially weighted pooling method is used to generate the final inference result. Furthermore, we propose a novel loss function to alleviate the false-negative problem during training. Experiments on two real-world KGs demonstrate the effectiveness of our method. The source code and data of this paper can be obtained from https://github.com/CCIIPLab/CET.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.07990", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/Pan0M21", "doi": "10.18653/v1/2021.findings-emnlp.193"}}, "content": {"source": {"pdf_hash": "d3f68d15985ae85fb7b2d8ecb2085763f6ef4f00", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.07990v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cb52b05453b7e29e6f17a48c3168c2765da86f35", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d3f68d15985ae85fb7b2d8ecb2085763f6ef4f00.txt", "contents": "\nContext-aware Entity Typing in Knowledge Graphs\n\n\nWeiran Pan \nSchool of Computer Science and Technology\nCognitive Computing and Intelligent Information Processing Laboratory\nHuazhong University of Science and Technology\n\n\nWei Wei weiw@hust.edu.cn \nSchool of Computer Science and Technology\nCognitive Computing and Intelligent Information Processing Laboratory\nHuazhong University of Science and Technology\n\n\nXian-Ling Mao \nDepartment of Computer Science and Technology\nBeijing Institute of Technology\n\n\nContext-aware Entity Typing in Knowledge Graphs\n\nKnowledge graph entity typing aims to infer entities' missing types in knowledge graphs which is an important but under-explored issue. This paper proposes a novel method for this task by utilizing entities' contextual information. Specifically, we design two inference mechanisms: i) N2T: independently use each neighbor of an entity to infer its type; ii) Agg2T: aggregate the neighbors of an entity to infer its type. Those mechanisms will produce multiple inference results, and an exponentially weighted pooling method is used to generate the final inference result. Furthermore, we propose a novel loss function to alleviate the false-negative problem during training. Experiments on two real-world KGs demonstrate the effectiveness of our method. The source code and data of this paper can be obtained from https://github.com/ CCIIPLab/CET.\n\nIntroduction\n\nKnowledge graphs (KGs) store world knowledge in a structured way. They consist of collections of triples in the form of (head entity, relation, tail entity), and entities are labeled with types (see Figure 1). The entity type information on knowledge graph has applications in many NLP tasks including entity linking (Gupta et al., 2017), question answering (Bordes et al., 2014) and fine-grained entity typing in text (Ling and Weld, 2012, Choi et al., 2018, Zhou et al., 2018. An entity can have multiple types, and the entity type information on the knowledge graph is usually incomplete. In this paper, we focus on Knowledge Graph Entity Typing (KGET), which aims to infer entities' missing types in knowledge graphs.\n\nExisting methods for the KGET task can be divided into embedding-based methods and graph convolutional networks (GCNs) for the multirelational graph. Knowledge graph embedding (KGE) is representative of embedding-based methods. Treating entities' known types as special triples with a unique relation \"has type\", e.g., (Barack Obama, has type, person), the KGET task can be understood as a subtask of knowledge graph completion. Consequently, KGE methods can infer entities' missing types by completing (entity, has type, ?). Recently, two embedding-based KGET models based on KGE have been proposed: ETE (Moon et al., 2017b) and ConnectE (Zhao et al., 2020). They first obtain entity embeddings from KGE methods, then use them to infer entities' missing types. GCNs for the multi-relational graph can aggregate the rich information in entities' neighbors to infer entities' missing types.\n\nExisting methods usually encode all attributes of an entity into one embedding, then use this representation to conduct inference. However, when judging whether an entity has a particular type, only some attributes of this entity may be helpful while the others remain useless. For example, in Figure 1, only the neighbor (graduate from, Cornell University) can indicate the central entity Steven Weinberg have type Cornell University alumni. We argue that always considering all attributes of an entity during inference may introduce irrelevant information as noise and ultimately reduce the accuracy of entity typing.\n\nBesides the above-mentioned shortcoming, ETE and ConnectE also ignore entities' known type information when training entity embeddings, which is important for entity typing. For instance, in Figure 1 there are no known triples which can indicate entity Steven Weinberg has type Jewish physicists. In this case, the model needs to utilize the known type information. Steven Weinberg has type Jewish Scientists, and in the known triples exists (Steven Weinberg, has won prize, Nobel Prize in Physics). Combining the two, we can infer Steven Weinberg has type Jewish physicists. In short, these two KGET models have difficult using the entities' known types to infer the missing ones. In the experiment, we found this seriously affect their performance.\n\nTo overcome those shortcomings in existing methods, we propose a novel method for the KGET task, called CET (Context-aware Entity Typing). Specifically, CET contains two inference mechanisms: i) N2T: independently use each neighbor of an entity to infer its type; ii) Agg2T: aggregate the neighbors of an entity to infer its type. According to our observation, one neighbor usually represents a specific attribute of the central entity. Thus, the N2T mechanism allows CET to consider each attribute of an entity during inference individually. In contrast, previous works mix various attributes of an entity into one embedding for inference. Therefore, we believe CET can produce more accurate entity typing results than existing methods. Moreover, some complex types like 21st-century American novelists involve multiple semantic aspects of an entity. It's difficult to infer those types only using a single neighbor. Therefore, we further propose the Agg2T mechanism, which simultaneously considers multiple attributes of the central entity during inference by aggregating neighbors. We also treat the known types of the central entity as its neighbors to use them to infer the missing types. To aggregate the inference results generated by N2T and Agg2T mechanism, we adopt a carefully designed pooling method similar to softpool (Stergiou et al., 2021). Experiments show that this pooling method can produce stable and interpretable inference results.\n\nIn addition, we face serious false-negative problem during training. Some (entity, type) pairs are valid but happen to be missing in current knowledge graphs. Treating them as negative samples will seriously affect model performance. We propose a novel loss function to alleviate this. To sum up, our contributions are three-fold:\n\n\u2022 We propose CET, a novel and flexible method for inferring entities' missing types in knowledge graphs, which fully utilize the neighbor information in an independent-based mechanism and aggregated-based mechanism.\n\n\u2022 We design a novel loss function to alleviate the false-negative problem during training.\n\n\u2022 Experiments on two real-world knowledge graphs demonstrate the superiority of our proposed method over other state-of-the-art algorithms, and the inference process of our method is interpretable.\n\n\nRelated Work\n\nEmbedding-based methods. Moon et al. (2017b) propose to learn type embedding for knowledge graph entity typing and build two methodologies: i) Synchronous training: Adding entities' known types to knowledge graphs in the form of triples with a unique relation \"has type\", e.g., (Barack Obama, has type, person), Knowledge Graph Embedding (KGE) methods (Nickel et al., 2011, Bordes et al., 2013, Nickel et al., 2016 can learn the embeddings of entities and types simultaneously. KGE methods can infer entities' missing types by completing (entity, has type, ?). ii) Asynchronous training: The model first obtains entities' embeddings from KGE methods, then minimizes the L1 distance between the entities' and their corresponding types' embeddings while keeping the entities' embeddings fixed. During inference, the smaller L1 distance between an entity and a specific type means the entity is more likely to have this type. Moon et al. (2017b) observe that there will be only one type of relation associate with types in synchronous training. They claim this lack of diversity of relations means that synchronous training methods have difficulty solving the KGET task. So they proposed a model called ETE, which follows the asynchronous training strategy and uses CONTE (Moon et al., 2017a) to obtain entity embeddings. Zhao et al. (2020) propose ConnectE, a more advanced KGET model which contains two inference mechanisms. One is called E2T, which uses a linear transformation to project the entities' embeddings into type embedding space. Another is called TRT, which uses the neighbors' types to infer the central entities' missing types. TRT is based on the assumption that the relationship can remain unchanged when replacing the entities in the triple to their corresponding types. For instance, if triple (Barack Obama, born in, Honolulu) holds, a new triple (person, born in, location) should also hold. ConnectE also follows the asynchronous training strategy, which first uses TransE to obtain entities' embedding then fixes them to train E2T and TRT.\n\nETE and ConnectE do not consider entities' known types when training entities' embeddings, which means they do not encode the known type information into entities' embeddings. Therefore, both of them have difficulty using entities' known types to infer the missing ones, which seriously affects their performance. Our experiments support this claim.\n\nGCNs for Multi-Relational Graph. The TRT mechanism in ConnectE attempts to use entities' neighbors to infer entities' missing types. However, TRT only utilizes the neighbors' types. To fully utilize the information in entities' neighbors, GCNs for multi-relational graphs can be used to encode entities' neighbors. Schlichtkrull et al. (2018) proposed R-GCN, an extension of GCNs for relational graphs. R-GCN aggregate the information in neighbors using the relation-specific filter. Weighted Graph Convolutional Network (Shang et al., 2019) utilizes learnable relational specific scalar weights to aggregate neighbors. Vashishth et al. (2020) proposed a more generalized framework by leverage composition operators from KGE techniques during GCN aggregation. In the KGET task, the entities' missing types can be inferred by performing multi-label classification on entities' embeddings obtained by GCNs.\n\nExisting methods usually encode all attributes of an entity into one embedding during inference. We argue this will introduce noise as sometimes only part of attributes of an entity is helpful for the KGET task while the others may be useless. To overcome this shortcoming, we propose the N2T mechanism. By independently uses each neighbor of an entity to infer its missing types, the N2T mechanism allows our model to consider each attribute of an entity during inference individually. This can reduce the impact of irrelevant information on entity typing. Also, we treat entities' known types as neighbors which means our model can use them to infer entities' missing types.\n\nOthers. Note that embedding knowledge graphs containing concepts (ontologies) and modeling the relationship between concepts (Lv et al., 2018, Hao et al., 2019 are not the goal of this paper. We concentrate on inferring entities' missing types. Some other works on KGET (Neelakantan andChang, 2015, Jin et al., 2018) mainly focus on how to combine additional information, such as the text description of the entities, to infer the missing types. Our work only uses the information on the knowledge graphs to infer the missing types of entities, which is more universal.\n\n\nMethod\n\nIn this section, we introduce our proposed method in detail. We first introduce the notations used in this paper. Afterward, we introduce two inference mechanisms used in our method. Finally, we introduce a novel loss function that can alleviate the false negative problem during training.\n\n\nNotations\nLet G = {(s, r, o)} \u2286 E \u00d7 R \u00d7 E be a knowledge\ngraph where E and R are the entity set and the relation set, respectively. The known type information on the knowledge graph is represented as\nI = {(e, t)} \u2286 E \u00d7 T .\nLet L be the number of types. We number the type from 1 to L and use type i to refer to the i-th type.\n\nWe add the known type information to the knowledge graphs. If an entity e has type t, we add an edge (e, has type, t) to KG, where has type is a newly added relationship. For the convenience of discussion, if edge (s, r, o) exists in KG, we add its inverted edge (o, r \u22121 , s) to KG where r \u22121 is the reverse relation of r. Let G be the KG after adding known type information and inverted edges. After adding those inverted edges, we only consider outgoing edges when discussing entities' neighbors. The neighbors set of u can be represented as N (u) = {(n r , n e )|(u, n r , n e ) \u2208 G }. We use bold n r , n e to represent the embedding of neighbor relation and neighbor entity, respectively. Let k be the dimension of the embeddings. The neighbors mentioned later all refer to those neighbors on G which include the neighbors in the knowledge graph and the entities' know types.\n\n\nProposed Method\n\nOur proposed method contains two inference mechanisms. One is to use each neighbor to infer the central entity's type independently, called N2T. Another is to aggregate neighbor information then  Figure 2: The overall architecture of CET. The N2T mechanism independently uses each neighbor to infer entities' missing types. The Agg2T mechanism aggregates neighbors' information then conducts inference. The final inference result is generated by an exponentially weighted pooling method.\n\nconduct inference, called Agg2T. And we use an exponentially weighted pooling method to generate the final inference result. The overall architecture is shown in Figure 2.\n\nN2T mechanism. We observe a strong correlation between the neighbors and the central entity's type. For instance, the neighbor (is affiliated to, Los Angeles Lakers) can indicate the central entity has type Los Angeles Lakers player. Meanwhile, different neighbors may correspond to different types. Therefore, we propose the N2T mechanism that independently uses each neighbor to infer the missing types of central entities. It's worth noting that when judging whether an entity has a particular type, sometimes only a few neighbors are helpful while the others remain useless. The N2T mechanism focuses on a single neighbor during inference, reducing the interference of irrelevant information on entity typing. In practice, CET follows the translating assumption in TransE to obtain the neighbor embedding 1 , then conducts non-linear activation 2 on neighbor embedding and sent it to a linear layer:\nR N 2T (nr,ne) = WRelu(n e \u2212 n r ) + b,(1)\nwhere W \u2208 R L\u00d7k , b \u2208 R L are the learning parameters and R N 2T (nr,ne) \u2208 R L is the relevance score calculated by the N2T mechanism, where the ith entry represents the relevance score between neighbor (n e , n r ) and type i. The higher R N 2T (nr,ne),i means the neighbor (n e , n r ) is more relevant to 1 The original relation r and its reversed relation r \u22121 share the same set of parameters and their embeddings satisfy r = \u2212r \u22121 .\n\n2 Non-linear activation is not necessary, but we found that adding it can achieve better results. type i, which indicates the central entity is more likely to have type i.\n\nAgg2T mechanism. It's difficult to infer some complex types like 21st-century American novelists and Film directors from New York City from a single neighbor. Therefore, we further propose the Agg2T mechanism which aggregate entities' neighbors to infer entities' missing types:\nh u = 1 |N (u)| (nr,ne)\u2208N (u) (n e \u2212 n r ), (2) R Agg2T u = WRelu(h u ) + b,(3)\nwhere h u \u2208 R k is the aggregated representation of u's neighbors and R Agg2T u \u2208 R L stores the relevance scores with all types. Here we chose a simple non-parameterized mean aggregation operation to verify the effectiveness of our method. Actually, CET is a highly flexible method that can work with the existing GCN-based method by replacing the aggregation operation in the Agg2T mechanism. We leave those analyses as future work.\n\nPooling approach. The N2T mechanism and the Agg2T mechanism will generate multiple entity typing results for every entity. To generate the final entity typing result, a pooling method is needed. Mean-pooling is not recommended as some types can only be indicated by a few neighbors. Maxpooling seems to be a suitable choice. However, we find its performance is not ideal. Choosing the max value as the final result makes only a small part of the input get the gradient which means some embeddings may not be sufficiently trained. As a result, the model may fail to represent every attribute of an entity accurately. In practice, we adopt an exponentially weighted pooling method similar to softpool (Stergiou et al., 2021):\nR u,i = pool({R Agg2T u,i , R N 2T\n(nr,ne),i | \u2200(n e , n r ) \u2208 N (u)}), f or i \u2208 1, 2, . . . , L, (4)\npool({x 1 , x 2 , ..., x n }) = n i=1 w i x i ,(5)w i = exp \u03b1x i n k=1 exp \u03b1x k ,(6)\nwhere R u,i \u2208 R is the relevance score between entity u and type i. \u03b1 \u2208 R + is a hyperparameter that controls the temperature of the pooling process. The higher R u,i means entity u is more likely to have type i. This pooling method has a similar effect to max-pooling but can generate a gradient for every input which ensures every embedding gets sufficient training.\n\nNeighbor sampling. If we use all the neighbors during training, the model may learn to use available type information to infer themselves, e.g., using neighbor (has type, person) to infer the entity has type person. The model can perfectly fit the training set in this way, result in a severe overfitting problem. One solution is to perform the following mask operation before the equation (6):\nR N 2T (has type,i),i = \u2212\u221e, i \u2208 1, 2, . . . , L R Agg2T u,i = \u2212\u221e, if (u, i) \u2208 I(7)\nAnother solution is to perform neighbor sampling: dynamically sample entities' neighbors with replacement during training. We find both methods have similar performance while performing neighbor sampling can significantly save training time, so we finally settle with neighbor sampling. Sampling fewer neighbors can lead to faster training speed, but at the expense of performance degradation. In practice, we find that sampling ten neighbors can usually achieve a good balance between speed and performance. We only conduct neighbor sampling during training; all neighbors of the entity are used during inference.\n\n\nOptimization\n\nWe hope that R u,i as high as possible if entity u has type i (positive samples), while R u,i as low as possible if entity u does not has type i (negative samples). The known (entity, type) pairs in I can be used as positive samples. To gather the negative samples, a simple choice is to treat all the nonexistent (entity, type) pairs in I as negative samples. Then we can use the binary cross-entropy (BCE) as loss function:\np u,i = \u03c3(R u,i ),(8)L = \u2212 (u,i)\u2208I log p u,i \u2212 (u,i) / \u2208I log(1 \u2212 p u,i ). (9)\nHowever, some (entity, type) pairs are valid but happen to be missing in current knowledge graphs. Actually, the entities' missing types which we want to infer belongs to this category. This brings serious false negative problems during training. To overcome this, we propose the following false-negative aware (FNA) loss function:\nL = \u2212 (u,i) / \u2208I \u03b2(p u,i \u2212 p 2 u,i ) log(1 \u2212 p u,i ) \u2212 (u,i)\u2208I log p u,i ,(10)\nwhere \u03b2 is a hyper-parameter used to control the overall weight of negative samples. The FNA loss function will assign lower weight to those negative samples with too large or too small relevance scores. Those negative samples with too large relevance scores are possibly false negative samples, and those with too small relevance scores are easy ones. These two kinds of negative samples do not provide helpful information, so we give them a lower weight. \n\n\nExperiment\n\nIn this section, we evaluate and analyze the proposed method on two real-world KGs. We introduce datasets and experiment settings in Section 4.1, present the main result in Section 4.2. The ablation study can be found in Section 4.3. Section 4.4 provides some cases to further analyze our method.\n\n\nDatasets and Experiment Setup\n\nDatasets. We conduct experiments on two realworld knowledge graphs, i.e., FB15k (Bordes et al., 2013), YAGO43k (Moon et al., 2017b) which are subsets of Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007) We also tried to adjust the batch size but this had no impact so we fixed the batch size to 128. The embeddings of entities, relations, and types are uniformly initialized, using a uniform distribution:[\u221210/k, 10/k] (k is the dimension of embeddings). The best model was selected by early stopping using the MRR on validation sets, computed every 25 epochs with a maximum of 1000 epochs. The optima configurations are: {k = 100, \u03b1 = 0.5, \u03b2 = 4.0, lr = 0.001} on FB15kET; {k = 100, \u03b1 = 0.5, \u03b2 = 2.0, lr = 0.001} on YAGO43kET.\n\nEvaluation Protocol. For each test sample (e, t) in test set. We first calculate the relevance score between e and every type and then rank all the types in descending order of relevance score. Following (Bordes et al., 2013), we evaluate model performance in the filtered setting: All the known types of e in the training, validation, and test sets are removed from the ranking. Finally, we can obtain the exact rank of the correct type t in all types. We use Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at 1/3/10 as evaluation metrics.\n\nBaselines. We compare our model with six stateof-the-art models, which can be divided into three groups. Models in the first group are KGE models which treat the KGET task as a special sub-task of knowledge graph completion, including TarnsE (Bordes et al., 2013), ComplEx (Trouillon et al., 2016) and RotatE . The second group are recently proposed KGET models including ETE (Moon et al., 2017b) and ConnectE (Zhao et al., 2020). And for GCNs for multi-relational graph we choose R- GCN (Schlichtkrull et al., 2018) as baseline. To make a fair comparison, R-GCN has similar experiment settings with CET: treating entities' known types as neighbors and performing the neighbor sampling during training. Hyperparameter settings of those baselines can be found in Appendix A.\n\nImplementation. All the KGE baselines in this paper are implemented using DGL-KE (Zheng et al., 2020). Our model and R-GCN are implemented using the DGL framework (PyTorch as backends).\n\nAll the experiments were run on a single 1080Ti system with 32GB RAM.\n\n\nMain Results\n\nTable 2 summarizes our result on FB15kET and YAGO43kET. We implement TransE, ComplEx, and RotatE using the self-adversarial negative sampling  and L3 regularization (Lacroix et al., 2018), which leads to better results than the reported results in the previous paper. We can see our model outperforms all baselines on almost all metrics. Meanwhile, after using the FNA loss, the performance significantly improved. Not only our model, but R-GCN can also benefit from this, which further proved the effectiveness of FNA loss. The performance of TransE, ComplEx, and Ro-tatE is limited by their entity representation strategy. These KGE methods encode all attributes of an entity into one embedding for inference. However, when judging whether an entity has a particular type, the irrelevant attributes may interfere with the inference result. CET overcomes this shortcoming by using the N2T mechanism and achieves better performance.\n\nSimilar to the KGE methods, R-GCN also suffers from the noise introduced by irrelevant information. R-GCN aggregates entities' neighbors to infer entities' missing types. However, sometimes a type can only be indicated by a few neighbors. This kind of rare information is easily overwhelmed by other irrelevant information during   R-GCN's aggregation process. This phenomenon is rarely observed on FB15kET but is common on YAGO43kET. This can explain why R-GCN outperforms other baselines on FB15kET but has a poor performance on YAGO43kET. ETE and ConnectE are largely left behind, especially on YAGO43kET. This is because these two methods have difficulty using entities' known types to infer the missing ones. Compared with FB15k, YAGO43k has a sparser graph structure and fewer types of relations (see Table 1). Therefore, in YAGO43kET, ignoring entities' known types and only using the (entity, relation, entity) triples to train entity embeddings can hardly fully model various attributes of each entity. As a result, the performance gap between ETE/ConnectE and other methods is more pronounced on YAGO43kET. This result demonstrates that using entities' known types to infer the missing ones is crucial in the KGET task. We will further illustrate this in Section 4.3.\n\n\nAblation Study\n\nOur model includes two inference mechanisms: N2T and Agg2T. Treating entities' known types as neighbors (TAN, short for types as neighbors) and the false negative aware loss function (FNA) can also improve the performance. To understand   to infer those complex types. Types like 21stcentury American novelists that involve multiple attributes of entities and require joint inference by multiple neighbors almost only appear in YAGO43kET. So it is natural that the Agg2T mechanism has little effect on FB15kET but improves model performance on YAGO43kET.\n\nImpact of FNA. The false-negative aware loss function can bring significant performance improvement, which proves its effectiveness.\n\nWe also compared several pooling methods on YAGO43kET. The result is summarized in Table 4. mean, max, ewp stand for mean pooling, maximum pooling and exponential weighted pooling, respectively. We can see that exponentially weighted pooling outperforms other pooling methods, which is consistent with our previous analysis.\n\n\nCase Study\n\nIn Tabel 5, we select three representative inferences made by our model. These examples show how CET used the N2T and Agg2T mechanisms to infer entities' missing types, and the inference process is interpretable.\n\nIn the first example, our model mainly uses the neighbor (has won prize, Pulitzer Prize) to conduct inference. This is intuitive because the correlation between other neighbors and the candidate type Pulitzer Prize winners is indeed not strong. In the second example, our model uses several entities' known types to conduct inference. This inference process is reasonable and can be described in natural language: Ian Fleming is an English short story writer, so he is also an English writer. In the first two examples, the model mainly uses the N2T mechanism. However, in the last example, the type American male television actors involves multiple attributes of the entity, which the N2T mechanism cannot infer. Therefore, we can see our model uses the aggregation of neighbors to complete the inference, which is consistent with our previous analysis.\n\nIn addition, we provide some N2T examples in Table 6 to show the mapping from neighbors to types, and the results are intuitive.  \n\n\nConclusion\n\nThis paper describes a novel knowledge graph entity typing method called CET, which utilizes the entities' contextual information to infer entities' missing types. We design two inference mechanisms, one is to independently use each neighbor of an entity to infer its types, another is to aggre-gate entities' neighbors than conduct inference. In addition, we propose a novel loss function to alleviate the false negative problem during training. Our method is highly flexible, and we are considering introducing advanced graph convolutional network technology into our method.\n\n\nNeighbor on KGKnown Type \n\nCentral Entity \nN2T \n\nAgg2T \n\n(nr 1 , ne 1 ) \n\n(has type, type i ) \n\n(nr m , ne m ) \n\n(has type, type k ) \n\nRelation \n\nKnown Type \n\nEntity \n\nType \n\nVector Output \n\nPooling \n\n\n\nTable 2 :\n2Results of several models on FB15kET and YAGO43kET datasets. Best results are in bold.[*]: Results are taken from original papers. ConnectE has three different training settings, here we report the best one.Model \nFB15kET \nYAGO43kET \n\nN2T TAN Agg2T FNA MRR MR Hit@1 Hit@3 Hit@10 MRR MR Hit@1 Hit@3 Hit@10 \n\n0.697 19 \n0.613 \n0.745 \n0.856 \n0.503 250 0.398 \n0.567 \n0.696 \n0.682 19 \n0.593 \n0.733 \n0.852 \n0.472 239 0.362 \n0.540 \n0.669 \n0.679 19 \n0.591 \n0.730 \n0.850 \n0.460 272 0.348 \n0.528 \n0.664 \n0.663 21 \n0.575 \n0.710 \n0.836 \n0.431 505 0.331 \n0.491 \n0.615 \n\n\n\nTable 3 :\n3Results of ablation study. Models without FNA loss function use BCE loss function instead.\n\n\nModel MRR MR Hit@1 Hit@3 Hit@10mean 0.396 338 0.300 \n0.440 \n0.578 \nmax \n0.462 327 0.366 \n0.517 \n0.636 \newp \n0.503 250 0.398 \n0.567 \n0.696 \n\n\n\nTable 4 :\n4Comparison of different pooling methods.each component's effect on the model, we conduct the ablation study on FB15kET and YAGO43kET datasets. The result is reported inTable 3. We can see the full model (the first row) outperforms all the ablated models on almost all metrics, illustrating every component's effectiveness in our model. Impact of N2T. Only using the N2T mechanism, our model still achieves competitive results against other state-of-the-art baselines. This indicates that independently considering entities' different attributes during inference can reduce the noise and produce accurate entity typing results.Impact of TAN. Treating entities' known types as neighbors allows CET utilize entities' known type to infer the missing ones. This strategy is especially effective on datasets containing rich entity-type information such as YAGO43kET.Impact of Agg2T. Agg2T mechanism is designedInference \n\nTop 3 Relevant Information Source \n\nEntity \nType \nInformation Source \nRelevance Score \n\nBob Dylan Pulitzer Prize winners \n\n(has won prize, Pulitzer Prize) \n6.93 \n(has won prize, Quill Award) \n-0.44 \n(has type, American poets) \n-0.72 \n\nIan Fleming \nEnglish writers \n\n(has type, English short story writers) \n3.36 \n(has type, English novelists) \n2.96 \n(has type, English spy fiction writers) \n2.15 \n\nBen Gazzara \nAmerican male \ntelevision actors \n\nAggregation \n3.04 \n(has type, American male film actors) \n-0.53 \n(has type, American television actors) \n-0.61 \n\n\n\nTable 5 :\n5Representative entity typing examples. We present the top 3 relevant information sources for entity typing and their relevance scores. Aggregation stands for the aggregation of neighbors, the information source used in the Agg2T mechanism.\n\n\nNeighbors Top 3 Relevant Types TypeRelevance Score (plays for, A.C. Milan)A.C Milan players \n6.32 \nSerie A footballers \n4.68 \nLiving people \n2.71 \n\n(has won prize, \nNobel Prize in Chemistry) \n\nNobel laureates in Chemistry \n3.33 \nscientist \n2.35 \n20th-century chemists \n1.54 \n\n(type, American rock singers) \n\nAmerican rock singers \n6.37 \nAmerican singers \n3.87 \nrock singers \n1.89 \n\n\n\nTable 6 :\n6Three most relevant types with a particular neighbor.\nwe exclude the data which contains unseen types in the training set from validation set and test set.\nAcknowledgmentsWe would like to thank all the anonymous reviewers for their insightful and valuable suggestions, which help improve this paper's quality. This work is supported by Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology.A Hyper-parameter Settings R-GCN. We use one layer R-GCN with 100dimension embeddings in our experiment. The hyper-parameters are tuned according to the MRR on the validation set. The search space for the grid search are set as follows: learning rate lr \u2208 {0.001, 0.005, 0.01}, activation function \u03d5 \u2208 none, relu, tanh, and the weight of negative samples in FNA loss \u03b2 \u2208 {1, 2, 3, 4}. The input embedding is randomly initialized with a uniform distribution [-0.1, 0.1], and the training batch size is fixed to 128. Using basisor block-diagonaldecomposition do not improve results but removing the self-loop improve performance.Table 7summarizes the best configuration.  KGE methods. For KGE methods, we use 200dimension embeddings in our experiment. We use random search to tune the hyper-parameters for KGE methods.Table 8summarizes the search space. neg_num is the number of negative samples for every positive sample; \u03b1 is the temperature in the self-adversarial negative sampling; lr is the learning rate; \u03bb is the regularization coefficient in L3 regularization; \u03b3 is a fixed margin in logsigmoid loss function and it also controls the initialization of the embeddings. We fix the training batch size to 1024.  We run 100 trails for each model, and every trial runs 50000 steps. The best configuration was selected according to the MRR on the validation set.Table 9summarizes the best configuration for each model.\nFreebase: a collaboratively created graph database for structuring human knowledge. Kurt D Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, 10.1145/1376616.1376746Proceedings of the ACM SIG-MOD International Conference on Management of Data. the ACM SIG-MOD International Conference on Management of DataVancouver, BC, CanadaACMKurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collab- oratively created graph database for structuring hu- man knowledge. In Proceedings of the ACM SIG- MOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008, pages 1247-1250. ACM.\n\nQuestion answering with subgraph embeddings. Antoine Bordes, Sumit Chopra, Jason Weston, 10.3115/v1/d14-1067Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACLA meeting of SIGDAT, a Special Interest Group of the ACLAntoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embed- dings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 615-620. ACL.\n\nTranslating embeddings for modeling multirelational data. Antoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, Oksana Yakhnenko, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, United StatesProceedings of a meeting heldAntoine Bordes, Nicolas Usunier, Alberto Garc\u00eda- Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Pro- ceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2787- 2795.\n\nUltra-fine entity typing. Eunsol Choi, Omer Levy, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P18-1009Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018Melbourne, AustraliaAssociation for Computational Linguistics1Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle- moyer. 2018. Ultra-fine entity typing. In Proceed- ings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics, ACL 2018, Mel- bourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 87-96. Association for Computational Linguistics.\n\nEntity linking via joint encoding of types, descriptions, and context. Nitish Gupta, Sameer Singh, Dan Roth, 10.18653/v1/d17-1284Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsNitish Gupta, Sameer Singh, and Dan Roth. 2017. En- tity linking via joint encoding of types, descriptions, and context. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, EMNLP 2017, Copenhagen, Denmark, Septem- ber 9-11, 2017, pages 2681-2690. Association for Computational Linguistics.\n\nUniversal representation learning of knowledge bases by jointly embedding instances and ontological concepts. Junheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun, Wei Wang, 10.1145/3292500.3330838Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningAnchorage, AK, USAACMJunheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun, and Wei Wang. 2019. Universal representa- tion learning of knowledge bases by jointly embed- ding instances and ontological concepts. In Proceed- ings of the 25th ACM SIGKDD International Confer- ence on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 1709-1719. ACM.\n\nAttributed and predictive entity embedding for finegrained entity typing in knowledge bases. Hailong Jin, Lei Hou, Juanzi Li, Tiansi Dong, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsHailong Jin, Lei Hou, Juanzi Li, and Tiansi Dong. 2018. Attributed and predictive entity embedding for fine- grained entity typing in knowledge bases. In Pro- ceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26, 2018, pages 282-292. Association for Computational Linguis- tics.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nCanonical tensor decomposition for knowledge base completion. Timoth\u00e9e Lacroix, Nicolas Usunier, Guillaume Obozinski, PMLRProceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, Sweden80Timoth\u00e9e Lacroix, Nicolas Usunier, and Guillaume Obozinski. 2018. Canonical tensor decomposition for knowledge base completion. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stock- holm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2869-2878. PMLR.\n\nFine-grained entity recognition. Xiao Ling, Daniel S Weld, Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence. the Twenty-Sixth AAAI Conference on Artificial IntelligenceToronto, Ontario, CanadaAAAI PressXiao Ling and Daniel S. Weld. 2012. Fine-grained en- tity recognition. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22- 26, 2012, Toronto, Ontario, Canada. AAAI Press.\n\nDifferentiating concepts and instances for knowledge graph embedding. Xin Lv, Lei Hou, Juanzi Li, Zhiyuan Liu, 10.18653/v1/d18-1222Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsXin Lv, Lei Hou, Juanzi Li, and Zhiyuan Liu. 2018. Differentiating concepts and instances for knowl- edge graph embedding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1971-1979. Association for Computational Linguistics.\n\nLearning contextual embeddings for knowledge graph completion. Changsung Moon, Steve Harenberg, John Slankas, Nagiza F Samatova, 21st Pacific Asia Conference on Information Systems. Langkawi, Malaysia248Changsung Moon, Steve Harenberg, John Slankas, and Nagiza F. Samatova. 2017a. Learning contextual embeddings for knowledge graph completion. In 21st Pacific Asia Conference on Information Sys- tems, PACIS 2017, Langkawi, Malaysia, July 16-20, 2017, page 248.\n\nLearning entity type embeddings for knowledge graph completion. Changsung Moon, Paul Jones, Nagiza F Samatova, 10.1145/3132847.3133095Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge ManagementSingaporeACMChangsung Moon, Paul Jones, and Nagiza F. Samatova. 2017b. Learning entity type embeddings for knowl- edge graph completion. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017, Singapore, November 06 -10, 2017, pages 2215-2218. ACM.\n\nInferring missing entity type instances for knowledge base completion: New dataset and methods. Arvind Neelakantan, Ming-Wei Chang, 10.3115/v1/n15-1054NAACL HLT 2015. Arvind Neelakantan and Ming-Wei Chang. 2015. In- ferring missing entity type instances for knowledge base completion: New dataset and methods. In NAACL HLT 2015, The 2015 Conference of the\n\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies. Denver, Colorado, USAThe Association for Computational LinguisticsNorth American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, Denver, Colorado, USA, May 31 -June 5, 2015, pages 515-525. The Association for Computational Linguistics.\n\nHolographic embeddings of knowledge graphs. Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. the Thirtieth AAAI Conference on Artificial IntelligencePhoenix, Arizona, USAAAAI PressMaximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. 2016. Holographic embeddings of knowl- edge graphs. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12- 17, 2016, Phoenix, Arizona, USA, pages 1955-1961. AAAI Press.\n\nA three-way model for collective learning on multi-relational data. Maximilian Nickel, Hans-Peter Volker Tresp, Kriegel, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningBellevue, Washington, USAOmnipressMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 -July 2, 2011, pages 809-816. Omnipress.\n\nModeling relational data with graph convolutional networks. Michael Sejr, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, 10.1007/978-3-319-93417-4_38The Semantic Web -15th International Conference. Heraklion, Crete, GreeceSpringer10843ProceedingsMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In The Semantic Web -15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings, volume 10843 of Lecture Notes in Computer Science, pages 593-607. Springer.\n\nEnd-to-end structure-aware convolutional networks for knowledge base completion. Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, Bowen Zhou, 10.1609/aaai.v33i01.33013060The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. Honolulu, Hawaii, USAAAAI Press2019Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xi- aodong He, and Bowen Zhou. 2019. End-to-end structure-aware convolutional networks for knowl- edge base completion. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pages 3060-3067. AAAI Press.\n\nRefining activation downsampling with softpool. Alexandros Stergiou, Ronald Poppe, Grigorios Kalliatakis, abs/2101.00440CoRRAlexandros Stergiou, Ronald Poppe, and Grigorios Kalliatakis. 2021. Refining activation downsam- pling with softpool. CoRR, abs/2101.00440.\n\nYago: a core of semantic knowledge. Fabian M Suchanek, Gjergji Kasneci, Gerhard Weikum, 10.1145/1242572.1242667Proceedings of the 16th International Conference on World Wide Web. the 16th International Conference on World Wide WebBanff, Alberta, CanadaACMFabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007, pages 697-706. ACM.\n\nRotate: Knowledge graph embedding by relational rotation in complex space. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAOpenReview.netZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowledge graph embedding by relational rotation in complex space. In 7th Inter- national Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\n\nComplex embeddings for simple link prediction. Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, Guillaume Bouchard, Proceedings of the 33nd International Conference on Machine Learning. the 33nd International Conference on Machine LearningNew York City, NY, USAJMLR.org48Workshop and Conference ProceedingsTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. 2016. Com- plex embeddings for simple link prediction. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 2071- 2080. JMLR.org.\n\nComposition-based multirelational graph convolutional networks. Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Partha P Talukdar, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. 2020. Composition-based multi- relational graph convolutional networks. In 8th International Conference on Learning Representa- tions, ICLR 2020, Addis Ababa, Ethiopia, April 26- 30, 2020. OpenReview.net.\n\nConnecting embeddings for knowledge graph entity typing. Yu Zhao, Anxiang Zhang, Ruobing Xie, Kang Liu, Xiaojie Wang, 10.18653/v1/2020.acl-main.572Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, OnlineAssociation for Computational LinguisticsYu Zhao, Anxiang Zhang, Ruobing Xie, Kang Liu, and Xiaojie Wang. 2020. Connecting embeddings for knowledge graph entity typing. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6419-6428. Association for Computa- tional Linguistics.\n\nDgl-ke: Training knowledge graph embeddings at scale. Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, George Karypis, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20New York, NY, USAAssociation for Computing MachineryDa Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, and George Karypis. 2020. Dgl-ke: Training knowledge graph embeddings at scale. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 739-748, New York, NY, USA. Associa- tion for Computing Machinery.\n\nZero-shot open entity typing as typecompatible grounding. Ben Zhou, Daniel Khashabi, Chen-Tse Tsai, Dan Roth, 10.18653/v1/d18-1231Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsBen Zhou, Daniel Khashabi, Chen-Tse Tsai, and Dan Roth. 2018. Zero-shot open entity typing as type- compatible grounding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2065-2076. Association for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":222,\"start\":51},{\"end\":408,\"start\":223},{\"end\":503,\"start\":409}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":58},{\"end\":230,\"start\":227},{\"end\":422,\"start\":419}]", "author_first_name": "[{\"end\":57,\"start\":51},{\"end\":226,\"start\":223},{\"end\":418,\"start\":409}]", "author_affiliation": "[{\"end\":221,\"start\":63},{\"end\":407,\"start\":249},{\"end\":502,\"start\":424}]", "title": "[{\"end\":48,\"start\":1},{\"end\":551,\"start\":504}]", "venue": null, "abstract": "[{\"end\":1400,\"start\":553}]", "bib_ref": "[{\"end\":1624,\"start\":1615},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1753,\"start\":1733},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1795,\"start\":1774},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1855,\"start\":1835},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1874,\"start\":1855},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1893,\"start\":1874},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2764,\"start\":2744},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2797,\"start\":2778},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5758,\"start\":5735},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6758,\"start\":6739},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7086,\"start\":7066},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7107,\"start\":7086},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7128,\"start\":7107},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7656,\"start\":7637},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8003,\"start\":7983},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8051,\"start\":8033},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9669,\"start\":9649},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9771,\"start\":9748},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10853,\"start\":10837},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10871,\"start\":10853},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10998,\"start\":10982},{\"end\":11028,\"start\":10998},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16551,\"start\":16528},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20038,\"start\":20017},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20068,\"start\":20048},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20123,\"start\":20099},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20156,\"start\":20133},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20908,\"start\":20887},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21494,\"start\":21473},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21528,\"start\":21504},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21627,\"start\":21607},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21660,\"start\":21641},{\"end\":21747,\"start\":21715},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22107,\"start\":22087},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22466,\"start\":22444}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27534,\"start\":27331},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28103,\"start\":27535},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28206,\"start\":28104},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":28349,\"start\":28207},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29837,\"start\":28350},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":30089,\"start\":29838},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":30474,\"start\":30090},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":30540,\"start\":30475}]", "paragraph": "[{\"end\":2137,\"start\":1416},{\"end\":3028,\"start\":2139},{\"end\":3649,\"start\":3030},{\"end\":4401,\"start\":3651},{\"end\":5857,\"start\":4403},{\"end\":6189,\"start\":5859},{\"end\":6406,\"start\":6191},{\"end\":6498,\"start\":6408},{\"end\":6697,\"start\":6500},{\"end\":8775,\"start\":6714},{\"end\":9126,\"start\":8777},{\"end\":10032,\"start\":9128},{\"end\":10710,\"start\":10034},{\"end\":11281,\"start\":10712},{\"end\":11581,\"start\":11292},{\"end\":11783,\"start\":11641},{\"end\":11909,\"start\":11807},{\"end\":12792,\"start\":11911},{\"end\":13299,\"start\":12812},{\"end\":13472,\"start\":13301},{\"end\":14377,\"start\":13474},{\"end\":14859,\"start\":14421},{\"end\":15032,\"start\":14861},{\"end\":15312,\"start\":15034},{\"end\":15827,\"start\":15393},{\"end\":16552,\"start\":15829},{\"end\":16654,\"start\":16588},{\"end\":17108,\"start\":16740},{\"end\":17504,\"start\":17110},{\"end\":18202,\"start\":17588},{\"end\":18644,\"start\":18219},{\"end\":19055,\"start\":18724},{\"end\":19592,\"start\":19135},{\"end\":19903,\"start\":19607},{\"end\":20681,\"start\":19937},{\"end\":21229,\"start\":20683},{\"end\":22004,\"start\":21231},{\"end\":22191,\"start\":22006},{\"end\":22262,\"start\":22193},{\"end\":23211,\"start\":22279},{\"end\":24490,\"start\":23213},{\"end\":25063,\"start\":24509},{\"end\":25197,\"start\":25065},{\"end\":25523,\"start\":25199},{\"end\":25750,\"start\":25538},{\"end\":26606,\"start\":25752},{\"end\":26738,\"start\":26608},{\"end\":27330,\"start\":26753}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11640,\"start\":11594},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11806,\"start\":11784},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14420,\"start\":14378},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15392,\"start\":15313},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16587,\"start\":16553},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16705,\"start\":16655},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16739,\"start\":16705},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17587,\"start\":17505},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18666,\"start\":18645},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18723,\"start\":18666},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19134,\"start\":19056}]", "table_ref": "[{\"end\":24027,\"start\":24020},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25289,\"start\":25282},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":26660,\"start\":26653}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1414,\"start\":1402},{\"attributes\":{\"n\":\"2\"},\"end\":6712,\"start\":6700},{\"attributes\":{\"n\":\"3\"},\"end\":11290,\"start\":11284},{\"attributes\":{\"n\":\"3.1\"},\"end\":11593,\"start\":11584},{\"attributes\":{\"n\":\"3.2\"},\"end\":12810,\"start\":12795},{\"attributes\":{\"n\":\"3.3\"},\"end\":18217,\"start\":18205},{\"attributes\":{\"n\":\"4\"},\"end\":19605,\"start\":19595},{\"attributes\":{\"n\":\"4.1\"},\"end\":19935,\"start\":19906},{\"attributes\":{\"n\":\"4.2\"},\"end\":22277,\"start\":22265},{\"attributes\":{\"n\":\"4.3\"},\"end\":24507,\"start\":24493},{\"attributes\":{\"n\":\"4.4\"},\"end\":25536,\"start\":25526},{\"attributes\":{\"n\":\"5\"},\"end\":26751,\"start\":26741},{\"end\":27545,\"start\":27536},{\"end\":28114,\"start\":28105},{\"end\":28360,\"start\":28351},{\"end\":29848,\"start\":29839},{\"end\":30485,\"start\":30476}]", "table": "[{\"end\":27534,\"start\":27347},{\"end\":28103,\"start\":27754},{\"end\":28349,\"start\":28240},{\"end\":29837,\"start\":29266},{\"end\":30474,\"start\":30166}]", "figure_caption": "[{\"end\":27347,\"start\":27333},{\"end\":27754,\"start\":27547},{\"end\":28206,\"start\":28116},{\"end\":28240,\"start\":28209},{\"end\":29266,\"start\":28362},{\"end\":30089,\"start\":29850},{\"end\":30166,\"start\":30092},{\"end\":30540,\"start\":30487}]", "figure_ref": "[{\"end\":3332,\"start\":3324},{\"end\":3850,\"start\":3842},{\"end\":13016,\"start\":13008},{\"end\":13471,\"start\":13463}]", "bib_author_first_name": "[{\"end\":32499,\"start\":32495},{\"end\":32501,\"start\":32500},{\"end\":32518,\"start\":32513},{\"end\":32533,\"start\":32526},{\"end\":32547,\"start\":32544},{\"end\":32561,\"start\":32556},{\"end\":33144,\"start\":33137},{\"end\":33158,\"start\":33153},{\"end\":33172,\"start\":33167},{\"end\":33811,\"start\":33804},{\"end\":33827,\"start\":33820},{\"end\":33844,\"start\":33837},{\"end\":33864,\"start\":33859},{\"end\":33879,\"start\":33873},{\"end\":34495,\"start\":34489},{\"end\":34506,\"start\":34502},{\"end\":34518,\"start\":34513},{\"end\":34529,\"start\":34525},{\"end\":35203,\"start\":35197},{\"end\":35217,\"start\":35211},{\"end\":35228,\"start\":35225},{\"end\":35923,\"start\":35916},{\"end\":35934,\"start\":35929},{\"end\":35948,\"start\":35941},{\"end\":35959,\"start\":35953},{\"end\":35968,\"start\":35965},{\"end\":36657,\"start\":36650},{\"end\":36666,\"start\":36663},{\"end\":36678,\"start\":36672},{\"end\":36689,\"start\":36683},{\"end\":37304,\"start\":37303},{\"end\":37320,\"start\":37315},{\"end\":37730,\"start\":37722},{\"end\":37747,\"start\":37740},{\"end\":37766,\"start\":37757},{\"end\":38327,\"start\":38323},{\"end\":38340,\"start\":38334},{\"end\":38342,\"start\":38341},{\"end\":38799,\"start\":38796},{\"end\":38807,\"start\":38804},{\"end\":38819,\"start\":38813},{\"end\":38831,\"start\":38824},{\"end\":39473,\"start\":39464},{\"end\":39485,\"start\":39480},{\"end\":39501,\"start\":39497},{\"end\":39517,\"start\":39511},{\"end\":39519,\"start\":39518},{\"end\":39937,\"start\":39928},{\"end\":39948,\"start\":39944},{\"end\":39962,\"start\":39956},{\"end\":39964,\"start\":39963},{\"end\":40540,\"start\":40534},{\"end\":40562,\"start\":40554},{\"end\":41231,\"start\":41221},{\"end\":41247,\"start\":41240},{\"end\":41263,\"start\":41257},{\"end\":41265,\"start\":41264},{\"end\":41774,\"start\":41764},{\"end\":41793,\"start\":41783},{\"end\":42346,\"start\":42340},{\"end\":42348,\"start\":42347},{\"end\":42369,\"start\":42364},{\"end\":42382,\"start\":42376},{\"end\":42403,\"start\":42399},{\"end\":42413,\"start\":42410},{\"end\":43008,\"start\":43004},{\"end\":43019,\"start\":43016},{\"end\":43030,\"start\":43026},{\"end\":43043,\"start\":43038},{\"end\":43056,\"start\":43048},{\"end\":43066,\"start\":43061},{\"end\":43935,\"start\":43925},{\"end\":43952,\"start\":43946},{\"end\":43969,\"start\":43960},{\"end\":44184,\"start\":44178},{\"end\":44186,\"start\":44185},{\"end\":44204,\"start\":44197},{\"end\":44221,\"start\":44214},{\"end\":44720,\"start\":44713},{\"end\":44734,\"start\":44726},{\"end\":44749,\"start\":44741},{\"end\":44759,\"start\":44755},{\"end\":45186,\"start\":45182},{\"end\":45206,\"start\":45198},{\"end\":45223,\"start\":45214},{\"end\":45236,\"start\":45232},{\"end\":45256,\"start\":45247},{\"end\":45882,\"start\":45875},{\"end\":45900,\"start\":45894},{\"end\":45915,\"start\":45909},{\"end\":45929,\"start\":45923},{\"end\":45931,\"start\":45930},{\"end\":46376,\"start\":46374},{\"end\":46390,\"start\":46383},{\"end\":46405,\"start\":46398},{\"end\":46415,\"start\":46411},{\"end\":46428,\"start\":46421},{\"end\":47078,\"start\":47076},{\"end\":47091,\"start\":47086},{\"end\":47102,\"start\":47098},{\"end\":47113,\"start\":47107},{\"end\":47124,\"start\":47119},{\"end\":47132,\"start\":47129},{\"end\":47142,\"start\":47139},{\"end\":47155,\"start\":47150},{\"end\":47169,\"start\":47163},{\"end\":47890,\"start\":47887},{\"end\":47903,\"start\":47897},{\"end\":47922,\"start\":47914},{\"end\":47932,\"start\":47929}]", "bib_author_last_name": "[{\"end\":32511,\"start\":32502},{\"end\":32524,\"start\":32519},{\"end\":32542,\"start\":32534},{\"end\":32554,\"start\":32548},{\"end\":32568,\"start\":32562},{\"end\":33151,\"start\":33145},{\"end\":33165,\"start\":33159},{\"end\":33179,\"start\":33173},{\"end\":33818,\"start\":33812},{\"end\":33835,\"start\":33828},{\"end\":33857,\"start\":33845},{\"end\":33871,\"start\":33865},{\"end\":33889,\"start\":33880},{\"end\":34500,\"start\":34496},{\"end\":34511,\"start\":34507},{\"end\":34523,\"start\":34519},{\"end\":34541,\"start\":34530},{\"end\":35209,\"start\":35204},{\"end\":35223,\"start\":35218},{\"end\":35233,\"start\":35229},{\"end\":35927,\"start\":35924},{\"end\":35939,\"start\":35935},{\"end\":35951,\"start\":35949},{\"end\":35963,\"start\":35960},{\"end\":35973,\"start\":35969},{\"end\":36661,\"start\":36658},{\"end\":36670,\"start\":36667},{\"end\":36681,\"start\":36679},{\"end\":36694,\"start\":36690},{\"end\":37313,\"start\":37305},{\"end\":37327,\"start\":37321},{\"end\":37331,\"start\":37329},{\"end\":37738,\"start\":37731},{\"end\":37755,\"start\":37748},{\"end\":37776,\"start\":37767},{\"end\":38332,\"start\":38328},{\"end\":38347,\"start\":38343},{\"end\":38802,\"start\":38800},{\"end\":38811,\"start\":38808},{\"end\":38822,\"start\":38820},{\"end\":38835,\"start\":38832},{\"end\":39478,\"start\":39474},{\"end\":39495,\"start\":39486},{\"end\":39509,\"start\":39502},{\"end\":39528,\"start\":39520},{\"end\":39942,\"start\":39938},{\"end\":39954,\"start\":39949},{\"end\":39973,\"start\":39965},{\"end\":40552,\"start\":40541},{\"end\":40568,\"start\":40563},{\"end\":41238,\"start\":41232},{\"end\":41255,\"start\":41248},{\"end\":41272,\"start\":41266},{\"end\":41781,\"start\":41775},{\"end\":41806,\"start\":41794},{\"end\":41815,\"start\":41808},{\"end\":42338,\"start\":42326},{\"end\":42362,\"start\":42349},{\"end\":42374,\"start\":42370},{\"end\":42388,\"start\":42383},{\"end\":42397,\"start\":42390},{\"end\":42408,\"start\":42404},{\"end\":42419,\"start\":42414},{\"end\":42428,\"start\":42421},{\"end\":43014,\"start\":43009},{\"end\":43024,\"start\":43020},{\"end\":43036,\"start\":43031},{\"end\":43046,\"start\":43044},{\"end\":43059,\"start\":43057},{\"end\":43071,\"start\":43067},{\"end\":43944,\"start\":43936},{\"end\":43958,\"start\":43953},{\"end\":43981,\"start\":43970},{\"end\":44195,\"start\":44187},{\"end\":44212,\"start\":44205},{\"end\":44228,\"start\":44222},{\"end\":44724,\"start\":44721},{\"end\":44739,\"start\":44735},{\"end\":44753,\"start\":44750},{\"end\":44764,\"start\":44760},{\"end\":45196,\"start\":45187},{\"end\":45212,\"start\":45207},{\"end\":45230,\"start\":45224},{\"end\":45245,\"start\":45237},{\"end\":45265,\"start\":45257},{\"end\":45892,\"start\":45883},{\"end\":45907,\"start\":45901},{\"end\":45921,\"start\":45916},{\"end\":45940,\"start\":45932},{\"end\":46381,\"start\":46377},{\"end\":46396,\"start\":46391},{\"end\":46409,\"start\":46406},{\"end\":46419,\"start\":46416},{\"end\":46433,\"start\":46429},{\"end\":47084,\"start\":47079},{\"end\":47096,\"start\":47092},{\"end\":47105,\"start\":47103},{\"end\":47117,\"start\":47114},{\"end\":47127,\"start\":47125},{\"end\":47137,\"start\":47133},{\"end\":47148,\"start\":47143},{\"end\":47161,\"start\":47156},{\"end\":47177,\"start\":47170},{\"end\":47895,\"start\":47891},{\"end\":47912,\"start\":47904},{\"end\":47927,\"start\":47923},{\"end\":47937,\"start\":47933}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/1376616.1376746\",\"id\":\"b0\",\"matched_paper_id\":207167677},\"end\":33090,\"start\":32411},{\"attributes\":{\"doi\":\"10.3115/v1/d14-1067\",\"id\":\"b1\",\"matched_paper_id\":12938495},\"end\":33744,\"start\":33092},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14941970},\"end\":34461,\"start\":33746},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1009\",\"id\":\"b3\",\"matched_paper_id\":49212016},\"end\":35124,\"start\":34463},{\"attributes\":{\"doi\":\"10.18653/v1/d17-1284\",\"id\":\"b4\",\"matched_paper_id\":28784495},\"end\":35804,\"start\":35126},{\"attributes\":{\"doi\":\"10.1145/3292500.3330838\",\"id\":\"b5\",\"matched_paper_id\":196187271},\"end\":36555,\"start\":35806},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52012875},\"end\":37257,\"start\":36557},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6628106},\"end\":37658,\"start\":37259},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":49310354},\"end\":38288,\"start\":37660},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9345159},\"end\":38724,\"start\":38290},{\"attributes\":{\"doi\":\"10.18653/v1/d18-1222\",\"id\":\"b10\",\"matched_paper_id\":53080423},\"end\":39399,\"start\":38726},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":29231838},\"end\":39862,\"start\":39401},{\"attributes\":{\"doi\":\"10.1145/3132847.3133095\",\"id\":\"b12\",\"matched_paper_id\":28006229},\"end\":40436,\"start\":39864},{\"attributes\":{\"doi\":\"10.3115/v1/n15-1054\",\"id\":\"b13\",\"matched_paper_id\":298145},\"end\":40793,\"start\":40438},{\"attributes\":{\"id\":\"b14\"},\"end\":41175,\"start\":40795},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6071257},\"end\":41694,\"start\":41177},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1157792},\"end\":42264,\"start\":41696},{\"attributes\":{\"doi\":\"10.1007/978-3-319-93417-4_38\",\"id\":\"b17\",\"matched_paper_id\":5458500},\"end\":42921,\"start\":42266},{\"attributes\":{\"doi\":\"10.1609/aaai.v33i01.33013060\",\"id\":\"b18\",\"matched_paper_id\":53286980},\"end\":43875,\"start\":42923},{\"attributes\":{\"doi\":\"abs/2101.00440\",\"id\":\"b19\"},\"end\":44140,\"start\":43877},{\"attributes\":{\"doi\":\"10.1145/1242572.1242667\",\"id\":\"b20\",\"matched_paper_id\":207163173},\"end\":44636,\"start\":44142},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":67855617},\"end\":45133,\"start\":44638},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15150247},\"end\":45809,\"start\":45135},{\"attributes\":{\"id\":\"b23\"},\"end\":46315,\"start\":45811},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.572\",\"id\":\"b24\",\"matched_paper_id\":220046437},\"end\":47020,\"start\":46317},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":215828509},\"end\":47827,\"start\":47022},{\"attributes\":{\"doi\":\"10.18653/v1/d18-1231\",\"id\":\"b26\",\"matched_paper_id\":52247458},\"end\":48500,\"start\":47829}]", "bib_title": "[{\"end\":32493,\"start\":32411},{\"end\":33135,\"start\":33092},{\"end\":33802,\"start\":33746},{\"end\":34487,\"start\":34463},{\"end\":35195,\"start\":35126},{\"end\":35914,\"start\":35806},{\"end\":36648,\"start\":36557},{\"end\":37301,\"start\":37259},{\"end\":37720,\"start\":37660},{\"end\":38321,\"start\":38290},{\"end\":38794,\"start\":38726},{\"end\":39462,\"start\":39401},{\"end\":39926,\"start\":39864},{\"end\":40532,\"start\":40438},{\"end\":41219,\"start\":41177},{\"end\":41762,\"start\":41696},{\"end\":42324,\"start\":42266},{\"end\":43002,\"start\":42923},{\"end\":44176,\"start\":44142},{\"end\":44711,\"start\":44638},{\"end\":45180,\"start\":45135},{\"end\":45873,\"start\":45811},{\"end\":46372,\"start\":46317},{\"end\":47074,\"start\":47022},{\"end\":47885,\"start\":47829}]", "bib_author": "[{\"end\":32513,\"start\":32495},{\"end\":32526,\"start\":32513},{\"end\":32544,\"start\":32526},{\"end\":32556,\"start\":32544},{\"end\":32570,\"start\":32556},{\"end\":33153,\"start\":33137},{\"end\":33167,\"start\":33153},{\"end\":33181,\"start\":33167},{\"end\":33820,\"start\":33804},{\"end\":33837,\"start\":33820},{\"end\":33859,\"start\":33837},{\"end\":33873,\"start\":33859},{\"end\":33891,\"start\":33873},{\"end\":34502,\"start\":34489},{\"end\":34513,\"start\":34502},{\"end\":34525,\"start\":34513},{\"end\":34543,\"start\":34525},{\"end\":35211,\"start\":35197},{\"end\":35225,\"start\":35211},{\"end\":35235,\"start\":35225},{\"end\":35929,\"start\":35916},{\"end\":35941,\"start\":35929},{\"end\":35953,\"start\":35941},{\"end\":35965,\"start\":35953},{\"end\":35975,\"start\":35965},{\"end\":36663,\"start\":36650},{\"end\":36672,\"start\":36663},{\"end\":36683,\"start\":36672},{\"end\":36696,\"start\":36683},{\"end\":37315,\"start\":37303},{\"end\":37329,\"start\":37315},{\"end\":37333,\"start\":37329},{\"end\":37740,\"start\":37722},{\"end\":37757,\"start\":37740},{\"end\":37778,\"start\":37757},{\"end\":38334,\"start\":38323},{\"end\":38349,\"start\":38334},{\"end\":38804,\"start\":38796},{\"end\":38813,\"start\":38804},{\"end\":38824,\"start\":38813},{\"end\":38837,\"start\":38824},{\"end\":39480,\"start\":39464},{\"end\":39497,\"start\":39480},{\"end\":39511,\"start\":39497},{\"end\":39530,\"start\":39511},{\"end\":39944,\"start\":39928},{\"end\":39956,\"start\":39944},{\"end\":39975,\"start\":39956},{\"end\":40554,\"start\":40534},{\"end\":40570,\"start\":40554},{\"end\":41240,\"start\":41221},{\"end\":41257,\"start\":41240},{\"end\":41274,\"start\":41257},{\"end\":41783,\"start\":41764},{\"end\":41808,\"start\":41783},{\"end\":41817,\"start\":41808},{\"end\":42340,\"start\":42326},{\"end\":42364,\"start\":42340},{\"end\":42376,\"start\":42364},{\"end\":42390,\"start\":42376},{\"end\":42399,\"start\":42390},{\"end\":42410,\"start\":42399},{\"end\":42421,\"start\":42410},{\"end\":42430,\"start\":42421},{\"end\":43016,\"start\":43004},{\"end\":43026,\"start\":43016},{\"end\":43038,\"start\":43026},{\"end\":43048,\"start\":43038},{\"end\":43061,\"start\":43048},{\"end\":43073,\"start\":43061},{\"end\":43946,\"start\":43925},{\"end\":43960,\"start\":43946},{\"end\":43983,\"start\":43960},{\"end\":44197,\"start\":44178},{\"end\":44214,\"start\":44197},{\"end\":44230,\"start\":44214},{\"end\":44726,\"start\":44713},{\"end\":44741,\"start\":44726},{\"end\":44755,\"start\":44741},{\"end\":44766,\"start\":44755},{\"end\":45198,\"start\":45182},{\"end\":45214,\"start\":45198},{\"end\":45232,\"start\":45214},{\"end\":45247,\"start\":45232},{\"end\":45267,\"start\":45247},{\"end\":45894,\"start\":45875},{\"end\":45909,\"start\":45894},{\"end\":45923,\"start\":45909},{\"end\":45942,\"start\":45923},{\"end\":46383,\"start\":46374},{\"end\":46398,\"start\":46383},{\"end\":46411,\"start\":46398},{\"end\":46421,\"start\":46411},{\"end\":46435,\"start\":46421},{\"end\":47086,\"start\":47076},{\"end\":47098,\"start\":47086},{\"end\":47107,\"start\":47098},{\"end\":47119,\"start\":47107},{\"end\":47129,\"start\":47119},{\"end\":47139,\"start\":47129},{\"end\":47150,\"start\":47139},{\"end\":47163,\"start\":47150},{\"end\":47179,\"start\":47163},{\"end\":47897,\"start\":47887},{\"end\":47914,\"start\":47897},{\"end\":47929,\"start\":47914},{\"end\":47939,\"start\":47929}]", "bib_venue": "[{\"end\":32755,\"start\":32672},{\"end\":33370,\"start\":33288},{\"end\":34043,\"start\":34010},{\"end\":34764,\"start\":34662},{\"end\":35433,\"start\":35343},{\"end\":36195,\"start\":36096},{\"end\":36862,\"start\":36775},{\"end\":37409,\"start\":37391},{\"end\":37940,\"start\":37852},{\"end\":38508,\"start\":38425},{\"end\":39033,\"start\":38945},{\"end\":39601,\"start\":39583},{\"end\":40156,\"start\":40081},{\"end\":40918,\"start\":40897},{\"end\":41424,\"start\":41347},{\"end\":41965,\"start\":41887},{\"end\":42531,\"start\":42507},{\"end\":43362,\"start\":43341},{\"end\":44394,\"start\":44321},{\"end\":44855,\"start\":44835},{\"end\":45412,\"start\":45337},{\"end\":46021,\"start\":46000},{\"end\":46661,\"start\":46571},{\"end\":47427,\"start\":47303},{\"end\":48135,\"start\":48047},{\"end\":32670,\"start\":32593},{\"end\":33286,\"start\":33200},{\"end\":34008,\"start\":33891},{\"end\":34660,\"start\":34563},{\"end\":35341,\"start\":35255},{\"end\":36094,\"start\":35998},{\"end\":36773,\"start\":36696},{\"end\":37389,\"start\":37333},{\"end\":37850,\"start\":37782},{\"end\":38423,\"start\":38349},{\"end\":38943,\"start\":38857},{\"end\":39581,\"start\":39530},{\"end\":40079,\"start\":39998},{\"end\":40603,\"start\":40589},{\"end\":40895,\"start\":40795},{\"end\":41345,\"start\":41274},{\"end\":41885,\"start\":41817},{\"end\":42505,\"start\":42458},{\"end\":43339,\"start\":43101},{\"end\":43923,\"start\":43877},{\"end\":44319,\"start\":44253},{\"end\":44833,\"start\":44766},{\"end\":45335,\"start\":45267},{\"end\":45998,\"start\":45942},{\"end\":46569,\"start\":46464},{\"end\":47301,\"start\":47179},{\"end\":48045,\"start\":47959}]"}}}, "year": 2023, "month": 12, "day": 17}