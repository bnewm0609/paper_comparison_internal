{"id": 226202021, "updated": "2023-09-27 21:43:47.058", "metadata": {"title": "Improved RawNet with Feature Map Scaling for Text-independent Speaker Verification using Raw Waveforms", "authors": "[{\"first\":\"Jee-weon\",\"last\":\"Jung\",\"middle\":[]},{\"first\":\"Seung-bin\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Hye-jin\",\"last\":\"Shim\",\"middle\":[]},{\"first\":\"Ju-ho\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Ha-Jin\",\"last\":\"Yu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Interspeech 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Recent advances in deep learning have facilitated the design of speaker verification systems that directly input raw waveforms. For example, RawNet [1] extracts speaker embeddings from raw waveforms, which simplifies the process pipeline and demonstrates competitive performance. In this study, we improve RawNet by scaling feature maps using various methods. The proposed mechanism utilizes a scale vector that adopts a sigmoid non-linear function. It refers to a vector with dimensionality equal to the number of filters in a given feature map. Using a scale vector, we propose to scale the feature map multiplicatively, additively, or both. In addition, we investigate replacing the first convolution layer with the sinc-convolution layer of SincNet. Experiments performed on the VoxCeleb1 evaluation dataset demonstrate the effectiveness of the proposed methods, and the best performing system reduces the equal error rate by half compared to the original RawNet. Expanded evaluation results obtained using the VoxCeleb1-E and VoxCeleb-H protocols marginally outperform existing state-ofthe-art systems.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2004.00526", "mag": "3096084197", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/JungKSKY20", "doi": "10.21437/interspeech.2020-1011"}}, "content": {"source": {"pdf_hash": "d56fc27e55d94aba6538f93772433e01e1daec11", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2004.00526", "status": "GREEN"}}, "grobid": {"id": "d69d7a2755cfb74a002ff0837b2d1288a2770609", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d56fc27e55d94aba6538f93772433e01e1daec11.txt", "contents": "\nImproved RawNet with Feature Map Scaling for Text-independent Speaker Verification using Raw Waveforms\n\n\nJee-Weon Jung \nSchool of Computer Science\nUniversity of Seoul\nRepublic of Korea\n\nSeung-Bin Kim \nSchool of Computer Science\nUniversity of Seoul\nRepublic of Korea\n\nHye-Jin Shim \nSchool of Computer Science\nUniversity of Seoul\nRepublic of Korea\n\nJu-Ho Kim \nSchool of Computer Science\nUniversity of Seoul\nRepublic of Korea\n\nHa-Jin Yu hjyu@uos.ac.kr \nSchool of Computer Science\nUniversity of Seoul\nRepublic of Korea\n\nImproved RawNet with Feature Map Scaling for Text-independent Speaker Verification using Raw Waveforms\nIndex Terms: raw waveformspeaker verificationtext- independentattentiondeep neural networks\nRecent advances in deep learning have facilitated the design of speaker verification systems that directly input raw waveforms. For example, RawNet [1] extracts speaker embeddings from raw waveforms, which simplifies the process pipeline and demonstrates competitive performance. In this study, we improve RawNet by scaling feature maps using various methods. The proposed mechanism utilizes a scale vector that adopts a sigmoid non-linear function. It refers to a vector with dimensionality equal to the number of filters in a given feature map. Using a scale vector, we propose to scale the feature map multiplicatively, additively, or both. In addition, we investigate replacing the first convolution layer with the sinc-convolution layer of SincNet. Experiments performed on the VoxCeleb1 evaluation dataset demonstrate the effectiveness of the proposed methods, and the best performing system reduces the equal error rate by half compared to the original RawNet. Expanded evaluation results obtained using the VoxCeleb1-E and VoxCeleb-H protocols marginally outperform existing state-ofthe-art systems.\n\nIntroduction\n\nWith the recent advances in deep learning, many speaker verification studies have replaced the acoustic feature extraction process with deep neural networks (DNNs) [1][2][3][4]. In the preliminary stage of utilizing DNNs for speaker verification, acoustic features, e.g., Mel-frequency cepstral coefficients and Melfilterbank energy features, were utilized as input to DNNs [5][6][7][8]. In contrast, many recent studies have also used less processed features, e.g., spectrograms and raw waveforms [9][10][11][12], hypothesizing that the usage of such less processed features as input allows data-driven approaches with DNNs to yield better discriminative representations compared to using knowledgebased acoustic features. Following this trend, many recent systems, e.g., RawNet [1], have demonstrated competitive results using raw waveforms as input for speaker verification.\n\nThe attention mechanism was initially designed to emphasize more important elements in sequence-to-sequence processing [13][14][15][16][17]. It has been adopted to several tasks, including speaker verification. Among various methods, self-attentive pooling has been applied to speaker verification to aggregate frame-level representations into a single utterance-level representation [16]. Here, the term \"self\" refers to a property of the attention mechanism that no external data, e.g., phoneme labels [18], are used. Compared to conventional global average pooling, self-attentive pooling involves assigning a weight to * Equal contribution \u2020 Corresponding author each frame and conducting weighted summation. Recent attention mechanisms, e.g., multi-head self-attentive pooling, have also been investigated [17], and such methods have demonstrated further performance improvements. However, applying an attention mechanism to speaker verification is more focused on attentive pooling than applying such mechanisms to intermediate feature maps in image domain tasks [7,16,17].\n\nIn this study, we propose to scale the filter axis of feature maps using a sigmoid-based mechanism, which we refer to as feature map scaling (FMS). The FMS uses a scale vector whose dimension is identical to the number of filters, where each value is between 0 and 1, similar to an attention map used for an attention mechanism, with the exception that a sigmoid activation function is employed rather than a softmax function. The underlying hypothesis of using sigmoid activation functions (rather than the softmax function) to independently perform scaling is that, differing from few other tasks, an attention mechanism that exclusively selects only a few filters may remove an excessive amount of discriminative information. In addition, in light of the recent successes of attentive pooling mechanisms in speaker verification tasks, we investigate replacing the gated recurrent unit (GRU) layer of RawNet, which performs aggregation of frame-level representations, with the self-attentive pooling and self-multi-head-attentive pooling mechanisms.\n\nSpecifically, we propose to apply the FMS to feature maps by either multiplying, adding, or performing both sequentially, as shown in Figure 1. By multiplicatively scaling a feature map, we expect to emphasize each filter of a feature map independently. In addition, by applying an FMS through adding, we expect to provide small perturbations that lead to increased discriminative power. This is inspired by a previous study [19] that showed analyzing small alterations in high-dimensional space can drastically change discriminative power. By hypothesizing that these two approaches function in a complementary manner, we also propose to apply both approaches in sequence. In experiments, the proposed methods were applied to the output feature maps of each residual block following the literature [20,21]. In addition, we investigated replacing RawNet's first convolutional layer with a sinc-convolution layer [4], which has been reported to better capture aggregated frequency responses than the conventional convolutional layer.\n\nThe remainder of this paper is organized as follows. Section 2 describes the RawNet system, which we use as a baseline with several modifications. In Section 3, we introduce the proposed FMS. Section 4 discusses experimentation and presents an analysis of the experimental results. Finally, conclusions are presented in Section 5. Table 1: DNN architecture of the proposed System (referred to as RawNet2 for brevity). An output layer conducts speaker identification in the training phase, and is removed after the training. BN and LeakyReLU at the beginning of the first block are omitted following [22]. Numbers denoted in Conv and Sincconv refers to filter length, stride, and number of filters. This architecture can also be used to represent the baseline (\"#3-ours\" in Table 2) by removing FMS operations and using a convolutional layer instead of a sinc-conv layer. \n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 BN LeakyReLU Conv(3,1,128) BN LeakyReLU Conv(3,1,128) MaxPool(3) FMS \uf8fc \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8fe \u00d76 (27, 256) GRU GRU(1024) (1024,)\nSpeaker FC(1024) (1024,) embedding puts speaker embeddings designed for speaker verification [1]. The underlying assumption behind using a DNN is that speaker embeddings extracted directly from raw waveforms by replacing an acoustic feature extraction with more hidden layers are expected to yield more discriminative representations as the amount of available data increases. RawNet adopts a convolutional neural network-gated recurrent unit (CNN-GRU) architecture, in which the first CNN layer has stride size identical to the filter length. The front CNN layers comprise residual blocks followed by a max-pooling layer, and extracts framelevel representations. Then a uni-directional GRU layer aggregates frame-level features into an utterance-level representation, which is the final timestep of the GRU's output. The GRU layer is then connected to a fully-connected layer, where its output is used as the speaker embedding. Finally, the output layer receives a speaker embedding and performs identification in the training phase.\n\nTo construct the baseline used in this study, we implemented several modifications to the original RawNet. First, we modified the structure of the residual blocks to a pre-activation structure [22]. Second, we simplified the loss functions from using categorical cross-entropy (CCE), center [23], and speaker basis loss [24] to using only CCE loss based on empirical experiments. Third, we omitted a CNN pretraining scheme. Fourth, we modified the training dataset from VoxCeleb1 to VoxCeleb2 to utilize recently expanded evaluation protocols that consider the entire VoxCeleb1 dataset. Finally, we applied a test time augmentation (TTA) method in the evaluation phase [25] to extract multiple speaker embeddings from a single utterance by cropping with overlaps where the duration is identical to that in the training phase. Then, the average of the speaker embeddings is used as the final speaker embedding. Through these modifications, we achieve a relative error reduction (RER) of Here, s f is broadcasted to perform element-wise calculations with c f . 37.50 %. Throughout this study, we use cosine similarity for back-end classification to measure the improvement in speaker embedding, because various back-end classifiers have been already explored in [1].\n\n\nFilter-wise feature map scaling\n\nWe propose to independently scale each filter of a feature map using a filter-wise feature map scaling (FMS) technique. The FMS uses a scale vector whose dimension is identical to the number of filters with values between 0 and 1 derived using sigmoid activation. Its purpose is to independently modify the scale of a given feature map (i.e., the output of a residual block) to derive more discriminative representations. We also propose various methods to utilize the FMS to scale given feature maps (i.e., multiplication, addition, and applying both). Note that these proposed approaches do not require additional hyperparameters.\n\nHere, let c = [c1, c2, \u00b7 \u00b7 \u00b7 , cF ] be a feature map of a residual block, c f \u2208 R T , where T is the sequence length in time, and F is the number of filters. We derive a scale vector to conduct FMS by first performing global average pooling on the time axis, and then feed-forwarding through a fully-connected layer followed by sigmoid activation. By expressing a scale vector as s = [s1, s2, \u00b7 \u00b7 \u00b7 , sF ], s f \u2208 R 1 , we first propose to derive a scaled feature map c = [c 1 , c 2 , \u00b7 \u00b7 \u00b7 , c F ], c f \u2208 R T , to scale the feature map in an additive method expressed as follows:\nc f = c f + s f ,(1)\nwhere s f is broadcasted to perform element-wise calculation.\n\nWe also propose to scale the feature map in a multiplicative method:\nc f = c f \u00b7 s f .(2)\nThese two methods can be applied sequentially, where either method can be performed first, expressed as follows:\nc f = (c f + s f ) \u00b7 s f ,(3)c f = c f \u00b7 s f + s f .(4)\nWe also propose to use two individual scale vectors, one for addition, and the other for multiplication, for (4), because it can be interpreted as c f = (c f + 1) \u00d7 s f . Figure 1 shows the proposed methods using FMS to scale feature maps. Here, we applied the propose methods using the FMS to the outputs of residual blocks in the baseline system following the literature [20,21]. The proposed method using multiplicative FMS for scaling has commonality with the widely used attention mechanism [13][14][15] applied in the filter domain, which exclusively emphasizes a given feature map using a softmax activation. This can be interpreted as using the recently proposed multi-head attention mechanism [14] in the filter domain, where the number of the heads is equal to the number of filters. We apply scaling using a sigmoid function rather than exclusively performing scaling using a softmax function because information might be removed excessively when a conventional softmax-based attention mechanism. In translation or image classification tasks, performing exclusive concentration is reasonable; however, we hypothesize that different filters would yield complementary features for speaker verification, making independent scaling more adequate.\n\nThe proposed method with additive FMS for filter-wise scaling adds a value between 0 and 1 to a given feature map. The purpose is to apply data-driven perturbation to a feature map with a relatively small value. Here, it is assumed that this may increase the discriminative power of the feature maps. This concept is inspired by a phenomenon demonstrated by Zhang et. al. [19], where the discriminative power of a DNN's highdimensional intermediate representation can differ significantly with small perturbations. In addition, we assume that applying an additive FMS combined with a multiplicative FMS will lead to further improvements.\n\nWe also investigated replacing the RawNet's first convolutional layer with a sinc-convolution (sinc-conv) layer, which was first proposed to process raw waveforms by performing time-domain convolutions [4,12]. It is a type of a bandpass filter, where cut-off frequencies are set as parameters that are optimized with other DNN parameters. With fewer parameters, 2 \u00d7 #f ilter, the sinc-conv layer is frequently employed in DNNs that directly input a raw waveform. Table 1 details the overall architecture of the proposed system.\n\n\nExperiments and result analysis\n\nAll experiments reported in this paper were conducted using Py-Torch [28], and the code is available at https://github. com/Jungjee/RawNet.\n\n\nDataset\n\nWe used the VoxCeleb2 dataset [25] for training, and we used the VoxCeleb1 dataset [29] to evaluate various protocols. The VoxCeleb2 dataset contains over one million utterances from 6112 speakers, and the VoxCeleb1 dataset contains approxi-  Table 4: Experimental results of replacing the first strided convolution layer with varying sinc-conv layer length proposed in SincNet [4,12]. Applied to System #11-mul-add of Table 3.\n\n\nSystem\n\nSinc-conv length EER RER \n\n\nExperimental configurations\n\nWe used raw waveforms with pre-emphasis applied as input to the DNNs [1,3,11]. For the experiments in which the first convolutional layer was replaced with sinc-conv layer, we followed the literature [4]. This study did not apply pre-emphasis but performed layer normalization [30] to raw waveform. Here, we modified the duration of the input waveforms to 59049 samples (\u2248 3.69 s) in the training phase to construction mini-batches. In the testing phasing, we applied TTA with a 20 % overlap. We used Leaky ReLU activation functions [31] with a negative slope of 0.3 following [32]. The dimension of the speaker embedding is 1024. The AMSGrad optimizer [33] with a learning rate of 0.001 was used. A weight decay with \u03bb = 1e \u22124 was applied. We used CCE loss as the objective function. The other parameters related to the system architecture are described in Table 1 and the literature [1]. Table 2 shows the performance according to the modifications made to the RawNet system (Section 2) using the original Vox-Celeb1 evaluation set. Here, the top three rows describe existing systems using the same dataset for comparison. The results  Table 3 show the results obtained by applying various related methods. Systems #4 and #5 show the results obtained using the attention and multi-head attention mechanisms using the softmax-based exclusive attention map on the filter domain. Attention demonstrated marginal improvement, and multi-head attention reduced the performance matching the hypothesis discussed in Section 3. Systems #6 and #7 describe the results of applying squeeze-excitation [21] and convolutional block attention module [20] to the baseline. In addition, the experimental results obtained by replacing the GRU layer with self-attentive pooling or self-multi-headattentive pooling reduced performance. These results demonstrate that, in the case of RawNet, the GRU better aggregates frame-level representations into an utterance-level representation. Among application of various related methods, System #6 demonstrated the best result.\n\n\nResults analysis\n\nSystems #8 through #12 of Table 3 show the results of proposed FMS method with different configurations. Here, the \"Mechanism\" column shows how we performed the proposed FMS method. System #8 and System #9 applied the two proposed methods, and #10 and #11 applied both methods at the same time in different order. The results show that the proposed methods yielded improvements with RERs of 6.00 % and 11.33 %. Applying both methods simultaneously further improved the performance, and System #11 demonstrated an EER of 2.56 %. System #12 shows the result obtained using separate scale vectors for additive and multiplicative FMS. As shown, additional improvements were not observed. Table 4 shows the results obtained by replacing the RawNet's first convolutional layer with the sinc-conv layer of SincNet. Here, we used System #11 to perform these comparative experiments. The result demonstrates that it provides 3.12 % additional improvement in System #15. However, the performance was easily affected by the length of the sinc-conv layer, where setting an overly long filter length reduced performance. In the following, we refer to System #15 that demonstrates the best performance as 'RawNet2' for brevity. RawNet2 demonstrates an RER of 48.33 % compared to the original RawNet (System #1), thereby nearly halving the EER.\n\nFinally, Table 5 compares the results obtained in various recent studies using the expanded evaluation protocols, VoxCeleb1-E and VoxCeleb1-H, which utilize more than 1000 speakers and 500000 trials compared to 40 speakers and 38000 trials in the original evaluation protocol. 1 The results show that the proposed RawNet2 marginally outperformed the state-ofthe-art performance, i.e., EER of 2.87 % for the VoxCeleb-E protocol and 4.89 % for the VoxCeleb-H protocol. From the various experimental results given through Tables 2 to 5, we conclude that the proposed RawNet2 using the FMS demonstrates competitive performance despite its simple process pipeline of inputting raw waveforms to a DNN and measuring cosine similarity using the output speaker embeddings.\n\n\nConclusion\n\nIn this paper, we have proposed various FMS-based methods to improve the existing RawNet system, which is neural speaker embedding extractor in which speaker embeddings are extracted directly from raw waveforms. The FMS uses a scale vector to perform scaling, where the dimension of the scale vector is identical to the number of filters. The FMS-based methods scale filters in feature maps to construct improved feature maps that focus on more important features in the frame-level feature map through addition, multiplication, or both. We applied various FMS-based methods to the output of each residual block. In addition, by replacing the first convolution layer with a sincconv layer, we achieved further improvements. The results of the evaluation performed using the original VoxCeleb1 protocol demonstrate an EER of 2.48 %, while the original RawNet reported EER of 4.80 %. In an evaluation using recently expanded evaluation protocols, the proposed method marginally outperformed the current state-of-the-art methods.\n\n\nAcknowledgements\n\nFigure 1 :\n1Illustration of the four methods using the proposed FMS.\n\nTable 2 :\n2Performancecomparison according to modifications of \nthe baseline construction (  *  : data augmentation). Equal error \nrate (EER) is reported using the original VoxCeleb1 evaluation \ndataset. Ours shows the results of applying identity mapping \n[22], modifying the dimensionality of the code representation, \nand increasing the training set. \n\nSystem \nTrained on TTA EER RER \n\ni-vector [26] \nVox1 \n\u00d7 \n5.40 \n-\nspecCNN* [9] \nVox1 \n\u00d7 \n4.30 \n-\nx-vector* [6] \nVox2 \n\u00d7 \n3.10 \n-\n\n#1-RawNet [1] \nVox1 \n\u00d7 \n4.80 \n-\n#2-Ours \nVox2 \n\u00d7 \n3.52 26.67 \n#3-Ours \nVox2 \n\u2022 \n3.00 37.50 \n\n\n\nTable 3 :\n3Various applications of the proposed FMS. Baseline refers to the modified RawNet(Table 2). Mechanism addresses variations of applying the proposed method. c is the feature map, and s is the scale vector used to conduct FMS derived from c. \"sep\" indicates using separate scale vectors for additive and multiplicative scaling. Performance is reported in terms of EER and RER.System \nMechanism EER RER \n\nBaseline \n-\n3.00 \n-\n\n#4-att \n-\n2.89 \n3.67 \n#5-multi-att \n-\n3.42 \n-\n#6-SE \n-\n2.65 11.67 \n#7-CBAM \n-\n2.89 \n3.67 \n\n#8-add \nc + r \n2.82 \n6.00 \n#9-mul \nc \u00d7 r \n2.66 11.33 \n#10-add-mul \n(c + r) \u00d7 r 2.60 13.33 \n#11-mul-add \nc \u00d7 r + r \n2.56 14.67 \n\n#12-mul-add-sep c \u00d7 r1 + r2 2.57 14.33 \n\n\n\nTable 5 :\n5Results of comparison to state-of-the-art systems on expanded VoxCeleb1-E and VoxCeleb-H evaluation protocols.Input Feature \nFront-end \nAggregation \nLoss \nDims EER (%) \n\nVoxCeleb1-E \n\nChung et. al. [25] \nSpectrogram \nResNet-50 \nTAP \nSoftmax+Contrastive \n512 \n4.42 \nXie et. al. [27] \nSpectrogram \nThin ResNet-34 GhostVLAD \nSoftmax \n512 \n3.13 \nNagrani et al. [10] \nSpectrogram \nThin-ResNet-34 GhostVLAD \nSoftmax \n512 \n2.95 \nOurs \nRaw waveform \nRawNet2 \nGRU \nSoftmax \n1024 \n2.57 \nVoxCeleb1-H \n\nChung et. al. [25] \nSpectrogram \nResNet-50 \nTAP \nSoftmax+Contrastive \n512 \n7.33 \nXie et. al. [27] \nSpectrogram \nThin ResNet-34 GhostVLAD \nSoftmax \n512 \n5.06 \nNagrani et al. [10] \nSpectrogram \nThin-ResNet-34 GhostVLAD \nSoftmax \n512 \n4.93 \nOurs \nRaw waveform \nRawNet2 \nGRU \nSoftmax \n1024 \n4.89 \n\nindicate that the original RawNet demonstrates competitive per-\nformance. For x-vectors, we show the results for an improved \nversion reported in the literature [10]. Here, System #1 de-\nscribes the performance of RawNet [1], and System #2 shows \nthe result of changing the DNN architecture and expanding the \ntraining set to the VoxCeleb2 dataset from System #1. System \n#3 shows the results obtained by applying TTA to System #2. \nAs can be seen, the results demonstrate that the applied changes \nwere effective and resulted in RER of 37.5 % compared to the \noriginal RawNet. Note that we used System #3 as the baseline \nin all experiments. \nSystems #4 through #7 of \nWe report all performance values using the cleaned protocol.\n\nRawnet: Advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification. J Jung, H.-S Heo, J Kim, H Shim, H.-J Yu, Proc. Interspeech 2019. Interspeech 2019J.-w. Jung, H.-S. Heo, J.-h. Kim, H.-j. Shim, and H.-J. Yu, \"Rawnet: Advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification,\" Proc. In- terspeech 2019, pp. 1268-1272, 2019.\n\nTowards directly modeling raw speech signal for speaker verification using cnns. H Muckenhirn, M Doss, S Marcell, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. H. Muckenhirn, M. Doss, and S. Marcell, \"Towards directly mod- eling raw speech signal for speaker verification using cnns,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4884-4888.\n\nA complete endto-end speaker verification system using deep neural networks: From raw signals to verification result. J Jung, H Heo, I Yang, H Shim, H Yu, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPJ. Jung, H. Heo, I. Yang, H. Shim, and H. Yu, \"A complete end- to-end speaker verification system using deep neural networks: From raw signals to verification result,\" in 2018 IEEE Interna- tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5349-5353.\n\nSpeaker recognition from raw waveform with sincnet. M Ravanelli, Y Bengio, arXiv:1808.00158arXiv preprintM. Ravanelli and Y. Bengio, \"Speaker recognition from raw wave- form with sincnet,\" arXiv preprint arXiv:1808.00158, 2018.\n\nDeep neural networks for small footprint textdependent speaker verification. E Variani, X Lei, E Mcdermott, I L Moreno, J Gonzalez-Dominguez, International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPE. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez- Dominguez, \"Deep neural networks for small footprint text- dependent speaker verification,\" in International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4052-4056.\n\nX-vectors: Robust dnn embeddings for speaker recognition. D Snyder, D Garcia-Romero, G Sell, D Povey, S Khudanpur, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan- pur, \"X-vectors: Robust dnn embeddings for speaker recognition,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5329-5333.\n\nAttentive statistics pooling for deep speaker embedding. K Okabe, T Koshinaka, K Shinoda, arXiv:1803.10963arXiv preprintK. Okabe, T. Koshinaka, and K. Shinoda, \"Attentive statis- tics pooling for deep speaker embedding,\" arXiv preprint arXiv:1803.10963, 2018.\n\nSpatial pyramid encoding with convex length normalization for text-independent speaker verification. Y Jung, Y Kim, H Lim, Y Choi, H Kim, Proc. Interspeech. InterspeechY. Jung, Y. Kim, H. Lim, Y. Choi, and H. Kim, \"Spatial pyramid encoding with convex length normalization for text-independent speaker verification,\" Proc. Interspeech 2019, pp. 4030-4034, 2019.\n\nUnified hypersphere embedding for speaker recognition. M Hajibabaei, D Dai, arXiv:1807.08312arXiv preprintM. Hajibabaei and D. Dai, \"Unified hypersphere embedding for speaker recognition,\" arXiv preprint arXiv:1807.08312, 2018.\n\nVoxceleb: Large-scale speaker verification in the wild. A Nagrani, J S Chung, W Xie, A Zisserman, Computer Speech & Language. 602020A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, \"Voxceleb: Large-scale speaker verification in the wild,\" Computer Speech & Language, vol. 60, 2020.\n\nAvoiding speaker overfitting in end-to-end dnns using raw waveform for text-independent speaker verification. J Jung, H Heo, I Yang, H Shim, H Yu, Proc. Interspeech. InterspeechJ. Jung, H. Heo, I. Yang, H. Shim, and H. Yu, \"Avoiding speaker overfitting in end-to-end dnns using raw waveform for text-independent speaker verification,\" in Proc. Interspeech 2018, 2018, pp. 3583-3587.\n\nLearning speaker representations with mutual information. M Ravanelli, Y Bengio, InterspeechM. Ravanelli and Y. Bengio, \"Learning speaker representations with mutual information,\" in Interspeech, 2019.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473arXiv preprintD. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine trans- lation by jointly learning to align and translate,\" arXiv preprint arXiv:1409.0473, 2014.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\nListen, attend and spell: A neural network for large vocabulary conversational speech recognition. W Chan, N Jaitly, Q Le, O Vinyals, 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEW. Chan, N. Jaitly, Q. Le, and O. Vinyals, \"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960-4964.\n\nSelf-attentive speaker embeddings for text-independent speaker verification. Y Zhu, T Ko, D Snyder, B Mak, D Povey, in Interspeech. Y. Zhu, T. Ko, D. Snyder, B. Mak, and D. Povey, \"Self-attentive speaker embeddings for text-independent speaker verification.\" in Interspeech, 2018, pp. 3573-3577.\n\nSelf multi-head attention for speaker recognition. P Safari, J Hernando, Proc. Interspeech. InterspeechP. Safari and J. Hernando, \"Self multi-head attention for speaker recognition,\" Proc. Interspeech 2019, pp. 4305-4309, 2019.\n\nCnn with phonetic attention for text-independent speaker verification. T Zhou, Y Zhao, J Li, Y Gong, J Wu, 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEET. Zhou, Y. Zhao, J. Li, Y. Gong, and J. Wu, \"Cnn with pho- netic attention for text-independent speaker verification,\" in 2019 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU). IEEE, 2019, pp. 718-725.\n\nI-vector transformation using conditional generative adversarial networks for short utterance speaker verification. J Zhang, N Inoue, K Shinoda, Proceedings of INTERSPEECH, Hyderabad. INTERSPEECH, HyderabadIndiaJ. Zhang, N. Inoue, and K. Shinoda, \"I-vector transformation us- ing conditional generative adversarial networks for short utter- ance speaker verification,\" Proceedings of INTERSPEECH, Hy- derabad, India, 2018.\n\nCbam: Convolutional block attention module. S Woo, J Park, J.-Y. Lee, I So Kweon, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, \"Cbam: Convolu- tional block attention module,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 3-19.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132-7141.\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J Sun, SpringerK. He, X. Zhang, S. Ren, and J. Sun, \"Identity mappings in deep residual networks,\" in European conference on computer vision. Springer, 2016, pp. 630-645.\n\nA discriminative feature learning approach for deep face recognition. Y Wen, K Zhang, Z Li, Y Qiao, Springerin European conference on computer visionY. Wen, K. Zhang, Z. Li, and Y. Qiao, \"A discriminative feature learning approach for deep face recognition,\" in European confer- ence on computer vision. Springer, 2016, pp. 499-515.\n\nEnd-to-end losses based on speaker basis vectors and all-speaker hard negative mining for speaker verification. H.-S Heo, J Jung, I.-H Yang, S.-H Yoon, H Shim, H.-J Yu, arXiv:1902.02455arXiv preprintH.-S. Heo, J.-w. Jung, I.-H. Yang, S.-H. Yoon, H.-j. Shim, and H.-J. Yu, \"End-to-end losses based on speaker basis vectors and all-speaker hard negative mining for speaker verification,\" arXiv preprint arXiv:1902.02455, 2019.\n\nVoxceleb2: Deep speaker recognition. J S Chung, A Nagrani, A Zisserman, InterspeechJ. S. Chung, A. Nagrani, and A. Zisserman, \"Voxceleb2: Deep speaker recognition,\" in Interspeech, 2018.\n\nFrame-level speaker embeddings for text-independent speaker recognition and analysis of end-toend model. S Shon, H Tang, J Glass, arXiv:1809.04437arXiv preprintS. Shon, H. Tang, and J. Glass, \"Frame-level speaker embeddings for text-independent speaker recognition and analysis of end-to- end model,\" arXiv preprint arXiv:1809.04437, 2018.\n\nUtterancelevel aggregation for speaker recognition in the wild. W Xie, A Nagrani, J S Chung, A Zisserman, ICASSP. W. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, \"Utterance- level aggregation for speaker recognition in the wild,\" in ICASSP, 2019.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in Neural Information Processing Systems. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \"Pytorch: An imperative style, high-performance deep learning library,\" in Advances in Neural Information Processing Systems, 2019, pp. 8024-8035.\n\nVoxceleb: a largescale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, InterspeechA. Nagrani, J. S. Chung, and A. Zisserman, \"Voxceleb: a large- scale speaker identification dataset,\" in Interspeech, 2017.\n\nLayer normalization. J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450arXiv preprintJ. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv preprint arXiv:1607.06450, 2016.\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Y Ng, Proc. icml. icml303A. L. Maas, A. Y. Hannun, and A. Y. Ng, \"Rectifier nonlinearities improve neural network acoustic models,\" in Proc. icml, vol. 30, no. 1, 2013, p. 3.\n\nKeras. F Chollet, F. Chollet et al., \"Keras,\" https://github.com/keras-team/keras, 2015.\n\nOn the convergence of adam and beyond. S J Reddi, S Kale, S Kumar, arXiv:1904.09237arXiv preprintS. J. Reddi, S. Kale, and S. Kumar, \"On the convergence of adam and beyond,\" arXiv preprint arXiv:1904.09237, 2019.\n", "annotations": {"author": "[{\"end\":186,\"start\":106},{\"end\":267,\"start\":187},{\"end\":347,\"start\":268},{\"end\":424,\"start\":348},{\"end\":516,\"start\":425}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":115},{\"end\":200,\"start\":197},{\"end\":280,\"start\":276},{\"end\":357,\"start\":354},{\"end\":434,\"start\":432}]", "author_first_name": "[{\"end\":114,\"start\":106},{\"end\":196,\"start\":187},{\"end\":275,\"start\":268},{\"end\":353,\"start\":348},{\"end\":431,\"start\":425}]", "author_affiliation": "[{\"end\":185,\"start\":121},{\"end\":266,\"start\":202},{\"end\":346,\"start\":282},{\"end\":423,\"start\":359},{\"end\":515,\"start\":451}]", "title": "[{\"end\":103,\"start\":1},{\"end\":619,\"start\":517}]", "venue": null, "abstract": "[{\"end\":1819,\"start\":712}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2002,\"start\":1999},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2005,\"start\":2002},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2008,\"start\":2005},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2011,\"start\":2008},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2212,\"start\":2209},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2215,\"start\":2212},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2218,\"start\":2215},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2221,\"start\":2218},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2336,\"start\":2333},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2340,\"start\":2336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2344,\"start\":2340},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2348,\"start\":2344},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2618,\"start\":2615},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2837,\"start\":2833},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2841,\"start\":2837},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2845,\"start\":2841},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2849,\"start\":2845},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2853,\"start\":2849},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3102,\"start\":3098},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3222,\"start\":3218},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3529,\"start\":3525},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3786,\"start\":3783},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3789,\"start\":3786},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3792,\"start\":3789},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5277,\"start\":5273},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5651,\"start\":5647},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5654,\"start\":5651},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5763,\"start\":5760},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6485,\"start\":6481},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7038,\"start\":7035},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8175,\"start\":8171},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8273,\"start\":8269},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8302,\"start\":8298},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8651,\"start\":8647},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9241,\"start\":9238},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11212,\"start\":11208},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11215,\"start\":11212},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11335,\"start\":11331},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11339,\"start\":11335},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11343,\"start\":11339},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11541,\"start\":11537},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12466,\"start\":12462},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12934,\"start\":12931},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12937,\"start\":12934},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13365,\"start\":13361},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13477,\"start\":13473},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13530,\"start\":13526},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13824,\"start\":13821},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13827,\"start\":13824},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14010,\"start\":14007},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14012,\"start\":14010},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14015,\"start\":14012},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14141,\"start\":14138},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14219,\"start\":14215},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14475,\"start\":14471},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14519,\"start\":14515},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14595,\"start\":14591},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14826,\"start\":14823},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15533,\"start\":15529},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15579,\"start\":15575},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17620,\"start\":17619}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19235,\"start\":19166},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19815,\"start\":19236},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20510,\"start\":19816},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":21977,\"start\":20511}]", "paragraph": "[{\"end\":2712,\"start\":1835},{\"end\":3793,\"start\":2714},{\"end\":4846,\"start\":3795},{\"end\":5880,\"start\":4848},{\"end\":6753,\"start\":5882},{\"end\":7976,\"start\":6942},{\"end\":9242,\"start\":7978},{\"end\":9910,\"start\":9278},{\"end\":10491,\"start\":9912},{\"end\":10574,\"start\":10513},{\"end\":10644,\"start\":10576},{\"end\":10778,\"start\":10666},{\"end\":12088,\"start\":10835},{\"end\":12727,\"start\":12090},{\"end\":13256,\"start\":12729},{\"end\":13431,\"start\":13292},{\"end\":13870,\"start\":13443},{\"end\":13906,\"start\":13881},{\"end\":15990,\"start\":13938},{\"end\":17340,\"start\":16011},{\"end\":18105,\"start\":17342},{\"end\":19146,\"start\":18120}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6941,\"start\":6754},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10512,\"start\":10492},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10665,\"start\":10645},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10808,\"start\":10779},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10834,\"start\":10808}]", "table_ref": "[{\"end\":6220,\"start\":6213},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":6662,\"start\":6655},{\"end\":13199,\"start\":13192},{\"end\":13693,\"start\":13686},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13869,\"start\":13862},{\"end\":14803,\"start\":14796},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14835,\"start\":14828},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15083,\"start\":15076},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16044,\"start\":16037},{\"end\":16702,\"start\":16695},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17358,\"start\":17351}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1833,\"start\":1821},{\"attributes\":{\"n\":\"3.\"},\"end\":9276,\"start\":9245},{\"attributes\":{\"n\":\"4.\"},\"end\":13290,\"start\":13259},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13441,\"start\":13434},{\"end\":13879,\"start\":13873},{\"attributes\":{\"n\":\"4.2.\"},\"end\":13936,\"start\":13909},{\"attributes\":{\"n\":\"4.3.\"},\"end\":16009,\"start\":15993},{\"attributes\":{\"n\":\"5.\"},\"end\":18118,\"start\":18108},{\"attributes\":{\"n\":\"6.\"},\"end\":19165,\"start\":19149},{\"end\":19177,\"start\":19167},{\"end\":19246,\"start\":19237},{\"end\":19826,\"start\":19817},{\"end\":20521,\"start\":20512}]", "table": "[{\"end\":19815,\"start\":19259},{\"end\":20510,\"start\":20201},{\"end\":21977,\"start\":20633}]", "figure_caption": "[{\"end\":19235,\"start\":19179},{\"end\":19259,\"start\":19248},{\"end\":20201,\"start\":19828},{\"end\":20633,\"start\":20523}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4990,\"start\":4982},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11014,\"start\":11006}]", "bib_author_first_name": "[{\"end\":22152,\"start\":22151},{\"end\":22163,\"start\":22159},{\"end\":22170,\"start\":22169},{\"end\":22177,\"start\":22176},{\"end\":22188,\"start\":22184},{\"end\":22536,\"start\":22535},{\"end\":22550,\"start\":22549},{\"end\":22558,\"start\":22557},{\"end\":23010,\"start\":23009},{\"end\":23018,\"start\":23017},{\"end\":23025,\"start\":23024},{\"end\":23033,\"start\":23032},{\"end\":23041,\"start\":23040},{\"end\":23477,\"start\":23476},{\"end\":23490,\"start\":23489},{\"end\":23731,\"start\":23730},{\"end\":23742,\"start\":23741},{\"end\":23749,\"start\":23748},{\"end\":23762,\"start\":23761},{\"end\":23764,\"start\":23763},{\"end\":23774,\"start\":23773},{\"end\":24199,\"start\":24198},{\"end\":24209,\"start\":24208},{\"end\":24226,\"start\":24225},{\"end\":24234,\"start\":24233},{\"end\":24243,\"start\":24242},{\"end\":24639,\"start\":24638},{\"end\":24648,\"start\":24647},{\"end\":24661,\"start\":24660},{\"end\":24944,\"start\":24943},{\"end\":24952,\"start\":24951},{\"end\":24959,\"start\":24958},{\"end\":24966,\"start\":24965},{\"end\":24974,\"start\":24973},{\"end\":25261,\"start\":25260},{\"end\":25275,\"start\":25274},{\"end\":25491,\"start\":25490},{\"end\":25502,\"start\":25501},{\"end\":25504,\"start\":25503},{\"end\":25513,\"start\":25512},{\"end\":25520,\"start\":25519},{\"end\":25830,\"start\":25829},{\"end\":25838,\"start\":25837},{\"end\":25845,\"start\":25844},{\"end\":25853,\"start\":25852},{\"end\":25861,\"start\":25860},{\"end\":26162,\"start\":26161},{\"end\":26175,\"start\":26174},{\"end\":26378,\"start\":26377},{\"end\":26390,\"start\":26389},{\"end\":26397,\"start\":26396},{\"end\":26613,\"start\":26612},{\"end\":26624,\"start\":26623},{\"end\":26635,\"start\":26634},{\"end\":26645,\"start\":26644},{\"end\":26658,\"start\":26657},{\"end\":26667,\"start\":26666},{\"end\":26669,\"start\":26668},{\"end\":26678,\"start\":26677},{\"end\":26688,\"start\":26687},{\"end\":27059,\"start\":27058},{\"end\":27067,\"start\":27066},{\"end\":27077,\"start\":27076},{\"end\":27083,\"start\":27082},{\"end\":27526,\"start\":27525},{\"end\":27533,\"start\":27532},{\"end\":27539,\"start\":27538},{\"end\":27549,\"start\":27548},{\"end\":27556,\"start\":27555},{\"end\":27797,\"start\":27796},{\"end\":27807,\"start\":27806},{\"end\":28046,\"start\":28045},{\"end\":28054,\"start\":28053},{\"end\":28062,\"start\":28061},{\"end\":28068,\"start\":28067},{\"end\":28076,\"start\":28075},{\"end\":28501,\"start\":28500},{\"end\":28510,\"start\":28509},{\"end\":28519,\"start\":28518},{\"end\":28853,\"start\":28852},{\"end\":28860,\"start\":28859},{\"end\":28872,\"start\":28867},{\"end\":28879,\"start\":28878},{\"end\":29218,\"start\":29217},{\"end\":29224,\"start\":29223},{\"end\":29232,\"start\":29231},{\"end\":29592,\"start\":29591},{\"end\":29598,\"start\":29597},{\"end\":29607,\"start\":29606},{\"end\":29614,\"start\":29613},{\"end\":29856,\"start\":29855},{\"end\":29863,\"start\":29862},{\"end\":29872,\"start\":29871},{\"end\":29878,\"start\":29877},{\"end\":30235,\"start\":30231},{\"end\":30242,\"start\":30241},{\"end\":30253,\"start\":30249},{\"end\":30264,\"start\":30260},{\"end\":30272,\"start\":30271},{\"end\":30283,\"start\":30279},{\"end\":30583,\"start\":30582},{\"end\":30585,\"start\":30584},{\"end\":30594,\"start\":30593},{\"end\":30605,\"start\":30604},{\"end\":30839,\"start\":30838},{\"end\":30847,\"start\":30846},{\"end\":30855,\"start\":30854},{\"end\":31139,\"start\":31138},{\"end\":31146,\"start\":31145},{\"end\":31157,\"start\":31156},{\"end\":31159,\"start\":31158},{\"end\":31168,\"start\":31167},{\"end\":31396,\"start\":31395},{\"end\":31406,\"start\":31405},{\"end\":31415,\"start\":31414},{\"end\":31424,\"start\":31423},{\"end\":31433,\"start\":31432},{\"end\":31445,\"start\":31444},{\"end\":31455,\"start\":31454},{\"end\":31466,\"start\":31465},{\"end\":31473,\"start\":31472},{\"end\":31487,\"start\":31486},{\"end\":31869,\"start\":31868},{\"end\":31880,\"start\":31879},{\"end\":31882,\"start\":31881},{\"end\":31891,\"start\":31890},{\"end\":32061,\"start\":32060},{\"end\":32063,\"start\":32062},{\"end\":32069,\"start\":32068},{\"end\":32071,\"start\":32070},{\"end\":32080,\"start\":32079},{\"end\":32082,\"start\":32081},{\"end\":32291,\"start\":32290},{\"end\":32293,\"start\":32292},{\"end\":32301,\"start\":32300},{\"end\":32303,\"start\":32302},{\"end\":32313,\"start\":32312},{\"end\":32315,\"start\":32314},{\"end\":32498,\"start\":32497},{\"end\":32620,\"start\":32619},{\"end\":32622,\"start\":32621},{\"end\":32631,\"start\":32630},{\"end\":32639,\"start\":32638}]", "bib_author_last_name": "[{\"end\":22157,\"start\":22153},{\"end\":22167,\"start\":22164},{\"end\":22174,\"start\":22171},{\"end\":22182,\"start\":22178},{\"end\":22191,\"start\":22189},{\"end\":22547,\"start\":22537},{\"end\":22555,\"start\":22551},{\"end\":22566,\"start\":22559},{\"end\":23015,\"start\":23011},{\"end\":23022,\"start\":23019},{\"end\":23030,\"start\":23026},{\"end\":23038,\"start\":23034},{\"end\":23044,\"start\":23042},{\"end\":23487,\"start\":23478},{\"end\":23497,\"start\":23491},{\"end\":23739,\"start\":23732},{\"end\":23746,\"start\":23743},{\"end\":23759,\"start\":23750},{\"end\":23771,\"start\":23765},{\"end\":23793,\"start\":23775},{\"end\":24206,\"start\":24200},{\"end\":24223,\"start\":24210},{\"end\":24231,\"start\":24227},{\"end\":24240,\"start\":24235},{\"end\":24253,\"start\":24244},{\"end\":24645,\"start\":24640},{\"end\":24658,\"start\":24649},{\"end\":24669,\"start\":24662},{\"end\":24949,\"start\":24945},{\"end\":24956,\"start\":24953},{\"end\":24963,\"start\":24960},{\"end\":24971,\"start\":24967},{\"end\":24978,\"start\":24975},{\"end\":25272,\"start\":25262},{\"end\":25279,\"start\":25276},{\"end\":25499,\"start\":25492},{\"end\":25510,\"start\":25505},{\"end\":25517,\"start\":25514},{\"end\":25530,\"start\":25521},{\"end\":25835,\"start\":25831},{\"end\":25842,\"start\":25839},{\"end\":25850,\"start\":25846},{\"end\":25858,\"start\":25854},{\"end\":25864,\"start\":25862},{\"end\":26172,\"start\":26163},{\"end\":26182,\"start\":26176},{\"end\":26387,\"start\":26379},{\"end\":26394,\"start\":26391},{\"end\":26404,\"start\":26398},{\"end\":26621,\"start\":26614},{\"end\":26632,\"start\":26625},{\"end\":26642,\"start\":26636},{\"end\":26655,\"start\":26646},{\"end\":26664,\"start\":26659},{\"end\":26675,\"start\":26670},{\"end\":26685,\"start\":26679},{\"end\":26699,\"start\":26689},{\"end\":27064,\"start\":27060},{\"end\":27074,\"start\":27068},{\"end\":27080,\"start\":27078},{\"end\":27091,\"start\":27084},{\"end\":27530,\"start\":27527},{\"end\":27536,\"start\":27534},{\"end\":27546,\"start\":27540},{\"end\":27553,\"start\":27550},{\"end\":27562,\"start\":27557},{\"end\":27804,\"start\":27798},{\"end\":27816,\"start\":27808},{\"end\":28051,\"start\":28047},{\"end\":28059,\"start\":28055},{\"end\":28065,\"start\":28063},{\"end\":28073,\"start\":28069},{\"end\":28079,\"start\":28077},{\"end\":28507,\"start\":28502},{\"end\":28516,\"start\":28511},{\"end\":28527,\"start\":28520},{\"end\":28857,\"start\":28854},{\"end\":28865,\"start\":28861},{\"end\":28876,\"start\":28873},{\"end\":28888,\"start\":28880},{\"end\":29221,\"start\":29219},{\"end\":29229,\"start\":29225},{\"end\":29236,\"start\":29233},{\"end\":29595,\"start\":29593},{\"end\":29604,\"start\":29599},{\"end\":29611,\"start\":29608},{\"end\":29618,\"start\":29615},{\"end\":29860,\"start\":29857},{\"end\":29869,\"start\":29864},{\"end\":29875,\"start\":29873},{\"end\":29883,\"start\":29879},{\"end\":30239,\"start\":30236},{\"end\":30247,\"start\":30243},{\"end\":30258,\"start\":30254},{\"end\":30269,\"start\":30265},{\"end\":30277,\"start\":30273},{\"end\":30286,\"start\":30284},{\"end\":30591,\"start\":30586},{\"end\":30602,\"start\":30595},{\"end\":30615,\"start\":30606},{\"end\":30844,\"start\":30840},{\"end\":30852,\"start\":30848},{\"end\":30861,\"start\":30856},{\"end\":31143,\"start\":31140},{\"end\":31154,\"start\":31147},{\"end\":31165,\"start\":31160},{\"end\":31178,\"start\":31169},{\"end\":31403,\"start\":31397},{\"end\":31412,\"start\":31407},{\"end\":31421,\"start\":31416},{\"end\":31430,\"start\":31425},{\"end\":31442,\"start\":31434},{\"end\":31452,\"start\":31446},{\"end\":31463,\"start\":31456},{\"end\":31470,\"start\":31467},{\"end\":31484,\"start\":31474},{\"end\":31494,\"start\":31488},{\"end\":31877,\"start\":31870},{\"end\":31888,\"start\":31883},{\"end\":31901,\"start\":31892},{\"end\":32066,\"start\":32064},{\"end\":32077,\"start\":32072},{\"end\":32089,\"start\":32083},{\"end\":32298,\"start\":32294},{\"end\":32310,\"start\":32304},{\"end\":32318,\"start\":32316},{\"end\":32506,\"start\":32499},{\"end\":32628,\"start\":32623},{\"end\":32636,\"start\":32632},{\"end\":32645,\"start\":32640}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":119114892},\"end\":22452,\"start\":22040},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3536404},\"end\":22889,\"start\":22454},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52290675},\"end\":23422,\"start\":22891},{\"attributes\":{\"doi\":\"arXiv:1808.00158\",\"id\":\"b3\"},\"end\":23651,\"start\":23424},{\"attributes\":{\"id\":\"b4\"},\"end\":24138,\"start\":23653},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":46954166},\"end\":24579,\"start\":24140},{\"attributes\":{\"doi\":\"arXiv:1803.10963\",\"id\":\"b6\"},\"end\":24840,\"start\":24581},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195218675},\"end\":25203,\"start\":24842},{\"attributes\":{\"doi\":\"arXiv:1807.08312\",\"id\":\"b8\"},\"end\":25432,\"start\":25205},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":208114131},\"end\":25717,\"start\":25434},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52187936},\"end\":26101,\"start\":25719},{\"attributes\":{\"id\":\"b11\"},\"end\":26304,\"start\":26103},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b12\"},\"end\":26583,\"start\":26306},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13756489},\"end\":26957,\"start\":26585},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18165915},\"end\":27446,\"start\":26959},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52190317},\"end\":27743,\"start\":27448},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195345194},\"end\":27972,\"start\":27745},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":211243247},\"end\":28382,\"start\":27974},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4570732},\"end\":28806,\"start\":28384},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49867180},\"end\":29182,\"start\":28808},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":140309863},\"end\":29544,\"start\":29184},{\"attributes\":{\"id\":\"b21\"},\"end\":29783,\"start\":29546},{\"attributes\":{\"id\":\"b22\"},\"end\":30117,\"start\":29785},{\"attributes\":{\"doi\":\"arXiv:1902.02455\",\"id\":\"b23\"},\"end\":30543,\"start\":30119},{\"attributes\":{\"id\":\"b24\"},\"end\":30731,\"start\":30545},{\"attributes\":{\"doi\":\"arXiv:1809.04437\",\"id\":\"b25\"},\"end\":31072,\"start\":30733},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":67856245},\"end\":31323,\"start\":31074},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202786778},\"end\":31811,\"start\":31325},{\"attributes\":{\"id\":\"b28\"},\"end\":32037,\"start\":31813},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b29\"},\"end\":32223,\"start\":32039},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16489696},\"end\":32488,\"start\":32225},{\"attributes\":{\"id\":\"b31\"},\"end\":32578,\"start\":32490},{\"attributes\":{\"doi\":\"arXiv:1904.09237\",\"id\":\"b32\"},\"end\":32792,\"start\":32580}]", "bib_title": "[{\"end\":22149,\"start\":22040},{\"end\":22533,\"start\":22454},{\"end\":23007,\"start\":22891},{\"end\":23728,\"start\":23653},{\"end\":24196,\"start\":24140},{\"end\":24941,\"start\":24842},{\"end\":25488,\"start\":25434},{\"end\":25827,\"start\":25719},{\"end\":26610,\"start\":26585},{\"end\":27056,\"start\":26959},{\"end\":27523,\"start\":27448},{\"end\":27794,\"start\":27745},{\"end\":28043,\"start\":27974},{\"end\":28498,\"start\":28384},{\"end\":28850,\"start\":28808},{\"end\":29215,\"start\":29184},{\"end\":31136,\"start\":31074},{\"end\":31393,\"start\":31325},{\"end\":32288,\"start\":32225}]", "bib_author": "[{\"end\":22159,\"start\":22151},{\"end\":22169,\"start\":22159},{\"end\":22176,\"start\":22169},{\"end\":22184,\"start\":22176},{\"end\":22193,\"start\":22184},{\"end\":22549,\"start\":22535},{\"end\":22557,\"start\":22549},{\"end\":22568,\"start\":22557},{\"end\":23017,\"start\":23009},{\"end\":23024,\"start\":23017},{\"end\":23032,\"start\":23024},{\"end\":23040,\"start\":23032},{\"end\":23046,\"start\":23040},{\"end\":23489,\"start\":23476},{\"end\":23499,\"start\":23489},{\"end\":23741,\"start\":23730},{\"end\":23748,\"start\":23741},{\"end\":23761,\"start\":23748},{\"end\":23773,\"start\":23761},{\"end\":23795,\"start\":23773},{\"end\":24208,\"start\":24198},{\"end\":24225,\"start\":24208},{\"end\":24233,\"start\":24225},{\"end\":24242,\"start\":24233},{\"end\":24255,\"start\":24242},{\"end\":24647,\"start\":24638},{\"end\":24660,\"start\":24647},{\"end\":24671,\"start\":24660},{\"end\":24951,\"start\":24943},{\"end\":24958,\"start\":24951},{\"end\":24965,\"start\":24958},{\"end\":24973,\"start\":24965},{\"end\":24980,\"start\":24973},{\"end\":25274,\"start\":25260},{\"end\":25281,\"start\":25274},{\"end\":25501,\"start\":25490},{\"end\":25512,\"start\":25501},{\"end\":25519,\"start\":25512},{\"end\":25532,\"start\":25519},{\"end\":25837,\"start\":25829},{\"end\":25844,\"start\":25837},{\"end\":25852,\"start\":25844},{\"end\":25860,\"start\":25852},{\"end\":25866,\"start\":25860},{\"end\":26174,\"start\":26161},{\"end\":26184,\"start\":26174},{\"end\":26389,\"start\":26377},{\"end\":26396,\"start\":26389},{\"end\":26406,\"start\":26396},{\"end\":26623,\"start\":26612},{\"end\":26634,\"start\":26623},{\"end\":26644,\"start\":26634},{\"end\":26657,\"start\":26644},{\"end\":26666,\"start\":26657},{\"end\":26677,\"start\":26666},{\"end\":26687,\"start\":26677},{\"end\":26701,\"start\":26687},{\"end\":27066,\"start\":27058},{\"end\":27076,\"start\":27066},{\"end\":27082,\"start\":27076},{\"end\":27093,\"start\":27082},{\"end\":27532,\"start\":27525},{\"end\":27538,\"start\":27532},{\"end\":27548,\"start\":27538},{\"end\":27555,\"start\":27548},{\"end\":27564,\"start\":27555},{\"end\":27806,\"start\":27796},{\"end\":27818,\"start\":27806},{\"end\":28053,\"start\":28045},{\"end\":28061,\"start\":28053},{\"end\":28067,\"start\":28061},{\"end\":28075,\"start\":28067},{\"end\":28081,\"start\":28075},{\"end\":28509,\"start\":28500},{\"end\":28518,\"start\":28509},{\"end\":28529,\"start\":28518},{\"end\":28859,\"start\":28852},{\"end\":28867,\"start\":28859},{\"end\":28878,\"start\":28867},{\"end\":28890,\"start\":28878},{\"end\":29223,\"start\":29217},{\"end\":29231,\"start\":29223},{\"end\":29238,\"start\":29231},{\"end\":29597,\"start\":29591},{\"end\":29606,\"start\":29597},{\"end\":29613,\"start\":29606},{\"end\":29620,\"start\":29613},{\"end\":29862,\"start\":29855},{\"end\":29871,\"start\":29862},{\"end\":29877,\"start\":29871},{\"end\":29885,\"start\":29877},{\"end\":30241,\"start\":30231},{\"end\":30249,\"start\":30241},{\"end\":30260,\"start\":30249},{\"end\":30271,\"start\":30260},{\"end\":30279,\"start\":30271},{\"end\":30288,\"start\":30279},{\"end\":30593,\"start\":30582},{\"end\":30604,\"start\":30593},{\"end\":30617,\"start\":30604},{\"end\":30846,\"start\":30838},{\"end\":30854,\"start\":30846},{\"end\":30863,\"start\":30854},{\"end\":31145,\"start\":31138},{\"end\":31156,\"start\":31145},{\"end\":31167,\"start\":31156},{\"end\":31180,\"start\":31167},{\"end\":31405,\"start\":31395},{\"end\":31414,\"start\":31405},{\"end\":31423,\"start\":31414},{\"end\":31432,\"start\":31423},{\"end\":31444,\"start\":31432},{\"end\":31454,\"start\":31444},{\"end\":31465,\"start\":31454},{\"end\":31472,\"start\":31465},{\"end\":31486,\"start\":31472},{\"end\":31496,\"start\":31486},{\"end\":31879,\"start\":31868},{\"end\":31890,\"start\":31879},{\"end\":31903,\"start\":31890},{\"end\":32068,\"start\":32060},{\"end\":32079,\"start\":32068},{\"end\":32091,\"start\":32079},{\"end\":32300,\"start\":32290},{\"end\":32312,\"start\":32300},{\"end\":32320,\"start\":32312},{\"end\":32508,\"start\":32497},{\"end\":32630,\"start\":32619},{\"end\":32638,\"start\":32630},{\"end\":32647,\"start\":32638}]", "bib_venue": "[{\"end\":22233,\"start\":22217},{\"end\":25010,\"start\":24999},{\"end\":25896,\"start\":25885},{\"end\":27848,\"start\":27837},{\"end\":28595,\"start\":28568},{\"end\":29005,\"start\":28956},{\"end\":29379,\"start\":29317},{\"end\":32336,\"start\":32332},{\"end\":22215,\"start\":22193},{\"end\":22645,\"start\":22568},{\"end\":23123,\"start\":23046},{\"end\":23474,\"start\":23424},{\"end\":23862,\"start\":23795},{\"end\":24332,\"start\":24255},{\"end\":24636,\"start\":24581},{\"end\":24997,\"start\":24980},{\"end\":25258,\"start\":25205},{\"end\":25558,\"start\":25532},{\"end\":25883,\"start\":25866},{\"end\":26159,\"start\":26103},{\"end\":26375,\"start\":26306},{\"end\":26750,\"start\":26701},{\"end\":27179,\"start\":27093},{\"end\":27578,\"start\":27564},{\"end\":27835,\"start\":27818},{\"end\":28153,\"start\":28081},{\"end\":28566,\"start\":28529},{\"end\":28954,\"start\":28890},{\"end\":29315,\"start\":29238},{\"end\":29589,\"start\":29546},{\"end\":29853,\"start\":29785},{\"end\":30229,\"start\":30119},{\"end\":30580,\"start\":30545},{\"end\":30836,\"start\":30733},{\"end\":31186,\"start\":31180},{\"end\":31545,\"start\":31496},{\"end\":31866,\"start\":31813},{\"end\":32058,\"start\":32039},{\"end\":32330,\"start\":32320},{\"end\":32495,\"start\":32490},{\"end\":32617,\"start\":32580}]"}}}, "year": 2023, "month": 12, "day": 17}