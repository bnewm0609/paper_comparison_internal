{"id": 13188174, "updated": "2023-09-29 12:20:56.225", "metadata": {"title": "Deep Learning with Coherent Nanophotonic Circuits", "authors": "[{\"first\":\"Yichen\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Nicholas\",\"last\":\"Harris\",\"middle\":[\"C.\"]},{\"first\":\"Scott\",\"last\":\"Skirlo\",\"middle\":[]},{\"first\":\"Mihika\",\"last\":\"Prabhu\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Baehr-Jones\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Hochberg\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Shijie\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Hugo\",\"last\":\"Larochelle\",\"middle\":[]},{\"first\":\"Dirk\",\"last\":\"Englund\",\"middle\":[]},{\"first\":\"Marin\",\"last\":\"Soljacic\",\"middle\":[]}]", "venue": "Nature Photonics", "journal": "Nature Photonics", "publication_date": {"year": 2016, "month": 10, "day": 7}, "abstract": "Artificial Neural Networks are computational network models inspired by signal processing in the brain. These models have dramatically improved the performance of many learning tasks, including speech and object recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made to develop electronic architectures tuned to implement artificial neural networks that improve upon both computational speed and energy efficiency. Here, we propose a new architecture for a fully-optical neural network that, using unique advantages of optics, promises a computational speed enhancement of at least two orders of magnitude over the state-of-the-art and three orders of magnitude in power efficiency for conventional learning tasks. We experimentally demonstrate essential parts of our architecture using a programmable nanophotonic processor.", "fields_of_study": "[\"Physics\"]", "external_ids": {"arxiv": "1610.02365", "mag": "2752849906", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1038/nphoton.2017.93"}}, "content": {"source": {"pdf_hash": "9da92d480225865099d4ac2fc10396da0b989f1a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1610.02365v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1610.02365", "status": "GREEN"}}, "grobid": {"id": "beda44cb7c96d56a02987ce5fa5ad6955d5f8588", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9da92d480225865099d4ac2fc10396da0b989f1a.txt", "contents": "\nDeep Learning with Coherent Nanophotonic Circuits\n7 Oct 2016\n\nYichen Shen \nResearch Laboratory of Electronics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nNicholas C Harris \nResearch Laboratory of Electronics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nScott Skirlo \nResearch Laboratory of Electronics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nMihika Prabhu \nResearch Laboratory of Electronics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nTom Baehr-Jones \nCoriant Advanced Technology\n171 Madison Avenue, Suite 110010016New YorkNYUSA\n\nMichael Hochberg \nCoriant Advanced Technology\n171 Madison Avenue, Suite 110010016New YorkNYUSA\n\nXin Sun \nDepartment of Mathematics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nShijie Zhao \nDepartment of Biology\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nHugo Larochelle \nTwitter Inc\n141 Portland St02139CambridgeMAUSA\n\nDirk Englund \nResearch Laboratory of Electronics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nMarin Solja\u010di\u0107 \nResearch Laboratory of Electronics\nMassachusetts Institute of Technology\n02139CambridgeMAUSA\n\nDeep Learning with Coherent Nanophotonic Circuits\n7 Oct 2016\nThese authors contributed equally to this work.Artificial Neural Networks are computational network models inspired by signal processing in the brain. These models have dramatically improved the performance of many learning tasks, including speech and object recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made to develop electronic architectures tuned to implement artificial neural networks that improve upon both computational speed and energy efficiency. Here, we propose a new architecture for a fully-optical neural network that, using unique advantages of optics, promises a computational speed enhancement of at least two orders of magnitude over the state-of-the-art and three orders of magnitude in power efficiency for conventional learning tasks. We experimentally demonstrate essential parts of our architecture using a programmable nanophotonic processor.\n\nModern computers based on the von Neumann architecture are far more power-hungry and less effective than their biological counterparts -central nervous systemsfor a wide range of tasks including perception, communication, learning, and decision making. With the increasing data volume associated with processing big data, developing computers that learn, combine, and analyze vast amounts of information quickly and efficiently is becoming increasingly important. For example, speech recognition software (e.g., Apple's Siri) is typically executed in the cloud since these computations are too taxing for mobile hardware; realtime image processing is an even more demanding task [1]. To address the shortcomings of von Neumann computing architectures for neural networks, much recent work has focused on increasing artificial neural network computing speed and power efficiency by developing electronic architectures (such as ASIC and FPGA chips) specifically tailored to a task [2][3][4][5]. Recent demonstrations of electronic neuromorphic hardware architectures have reported improved computational performance [6]. Hybrid optical-electronic systems that implement spike processing [7][8][9] and reservoir computing [10,11] have also been investigated recently. However, the computational speed and power efficiency achieved with these hardware architectures are still limited by electronic clock rates and ohmic losses.\n\nFully-optical neural networks offer a promising alternative approach to microelectronic and hybrid optical-electronic implementations. Linear transformations (and certain nonlinear transformations) can be performed at the speed of light and detected at rates exceeding 100 GHz [12] in photonic networks, and in some cases, with minimal power consumption [13]. For example, it is well known that a common lens performs Fourier transform without any power consumption, and that certain matrix operations can also be performed optically without consuming power. However, implementing such transformations with bulk optical components (such as fibers and lenses) has been a major barrier because of the need for phase stability and large neuron counts. Integrated photonics solves this problem by providing a scalable solution to large, phase-stable optical transformations [14].\n\nHere, we experimentally demonstrate on-chip, coherent, optical neuromorphic computing on a vowel recognition dataset. We achieve a level of accuracy comparable to a conventional digital computer using a fully connected neural network algorithm. We show that, under certain conditions, the optical neural network architecture can be at least two orders of magnitude faster for forward propagation while providing linear scaling of neuron number versus power consumption. This feature is enabled largely by the fact that photonics can perform matrix multiplications, a major part of nerual network algorithms, with extreme energy efficiency. While implementing scalable von Neumann optical computers has proven challenging, artificial neural networks implemented in optics can leverage inherent properties, such their weak requirements on nonlinearities, to enable a practical, all-optical computing application. An optical neural network architecture can be substantially more energy efficient than conventional artificial neural networks implemented on current electronic computers. connected to at least one hidden layer and an output layer.\nX 1 X 2 X 3 X 4 h 1 (1) h 2 (1) h 3 (1) h 4 (1) Z (1) = W 0 X h 1 (i) h 2 (i) h 3 (i) h 4 (i) h 1 (n) h 2 (n) h 3 (n) h 4 (n) Y 1 Y 2 Y 3 Y 4 h (i) = f (Z (i) ) Y = W n h (n) Input\nIn each layer (depicted in Fig. 1(b)), information propagates by linear combination (e.g. matrix multiplication) followed by the application of a nonlinear activation function. ANNs can be trained by feeding training data into the input layer and then computing the output by forward propagation; weighting parameters in each matrix are subsequently optimized using back propagation [16]. The Optical Neural Network (ONN) architecture is depicted in Fig. 1 (b,c). As shown in Fig. 1(c), signals are encoded in the amplitude of optical pulses propagating in integrated photonic waveguides where they pass through an optical interference unit (OIU) and finally an optical nonlinearity unit (ONU). Optical matrix multiplication is implemented with an OIU and nonlinear activation is realized with an ONU.\n\nTo realize an OIU that can implement any real-valued matrix, we use the singular value decomposition (SVD) [17] since a general, real-valued matrix (M) may be decomposed as M = U\u03a3V * , where U is an m \u00d7 m unitary matrix, \u03a3 is a m \u00d7 n diagonal matrix with non-negative real numbers on the diagonal, and V * is the complex conjugate of the n \u00d7 n unitary matrix V. It was theoretically shown that any unitary transformations U, V * can be implemented with optical beamsplitters and phase shifters [18,19]. Matrix multipli-cation implemented in this manner consumes, in principle, no power. The fact that a major part of ANN calculations involves matrix products enables the extreme energy efficiency of the ONN architecture presented here. Finally, \u03a3 can be implemented using optical attenuators; optical amplification materials such as semiconductors or dyes could also be used [20].\n\nThe ONU can be implemented using optical nonlinearities such as saturable absorption [21][22][23] and bistability [24][25][26][27][28] that have been demonstrated seperately in photonic circuits. For an input intensity I in , the optical output intensity is thus given by a nonlinear function I out = f (I in ) [29].\n\n\nEXPERIMENT\n\nFor an experimental demonstration of our ONN architecture, we implement a two layer, fully connected neural network with the OIU shown in Fig. 2 and use it to perform vowel recognition. To prepare the training and testing dataset, we use 360 datapoints that each consist of four log area ratio coefficients [30] of one phoneme. The log area ratio coefficients, or feature vectors, represent the power contained in different logarithmically-spaced frequency bands and are derived by computing the Fourier transform of the voice signal multiplied by a Hamming window function. The 360 datapoints were generated by 90 different people speaking 4 different vowel phonemes [31]. We use half of these datapoints for training and the remaining half to test the performance of the trained ONN. We train the matrix parameters used in the ONN with the standard back propagation algorithm using stochastic gradient descent method [1], on a conventional computer. Further details on the dataset and backpropagation procedure are included in Supplemental Information Section 3.\n\nThe coherent ONN is realized with a programmable nanophotonic processor [14] composed of an array of 56 Mach-Zehnder interferometers (MZIs) and 213 phase shifting elements, as shown in Fig. 2. Each interferometer is composed of two evanescent-mode waveguide couplers sandwiching an internal thermo-optic phase shifter [32] to control the splitting ratio of the output modes, followed by a second modulator to control the relative phase of the output modes. By controlling the phase imparted by these two phase shifters, these MZIs perform all rotations in the SU(2) Lie group given a controlled incident phase on the two electromagnetic input modes of the MZI. The nanophotonic processor was fabricated in a silicon-on-insulator photonics platform with the OPSIS Foundry [33].\n\nTo experimentally realize arbitrary matrices by SVD, we programmed an SU(4) core [18,34] and a non-unitary diagonal matrix multiplication core (DMMC) into the nanophotonic processor [14,32], as shown in Fig. 2 (b). The SU(4) core implements operators U and V by a Givens rotations algorithm [18,34] that decomposes unitary matrices into sets of phase shifters and beam splitters, while the DMMC implements \u03a3 by controlling the splitting ratios of the DMMC interferometers to add or remove light from the optical mode relative to a baseline amplitude. The measured fidelity for the 720 OIU and DMMC cores used in the experiment was 99.8 \u00b1 0.003 %; see methods for further detail.\n\nIn this analog computer, fidelity is limited by practical non-idealities such as (1) finite precision with which an op-tical phase can be set using our custom 240-channel voltage supply with 16-bit voltage resolution per channel (2) photodetection noise, and (3) thermal cross-talk between phase shifters which effectively reduces the number of bits of resolution for setting phases. As with digital floating-point computations, values are represented to some number of bits of precision, the finite dynamic range and noise in the optical intensities causes effective truncation errors. A detailed analysis of finite precision and low-flux photon shot noise is presented in Supplement Section 1.\n\nIn this proof-of-concept demonstration, we implement the nonlinear transformation I out = f (I in ) in the electronic domain, by measuring optical mode output intensities on a photodetector array and injecting signals I out into the next stage of OIU. Here, f models the mathematical function associated with a realistic saturable absorber (such as a dye, semiconductor or graphene saturable absorber or saturable amplifier) that could, in future implementations, be directly integrated into waveguides after each OIU stage of the circuit. For example, graphene layers integrated on nanophotonic waveguides have already been demonstrated as saturable absorbers [35]. Saturable absorption is modeled as [21] (Supplement Section 2),\n\u03c3\u03c4 s I 0 = 1 2 ln(T m /T 0 ) 1 \u2212 T m ,(1)\nwhere \u03c3 is the absorption cross section, \u03c4 s is the radiative lifetime of the absorber material, T 0 is the initial transmittance (a constant that only depends on the design of saturable absorbers), I 0 is the incident intensity, and T m is the transmittance of the absorber. Given an input intensity I 0 , one can solve for T m (I 0 ) from Eqn. 1, and the output intensity can be calculated as I out = I 0 \u00b7 T m (I 0 ). A plot of the saturable absorber's response function I out (I in ) is shown in supplement Section 2.\n\nAfter programming the nanophotonic processor to implement our ONN architecture, which consist of 4 layers of OIUs with 4 neurons on each layer (which requires training a total of 4 \u00b7 6 \u00b7 2 = 48 phase shifter settings), we evaluated it on the vowel recognition test set. Our ONN correctly identified 138/180 cases (76.7%) [36] compared to a simulated correctness of 165/180 (91.7%).\n\nSince our ONN processes information in the analog signal domain, the architecture can be vulnerable to computational errors. Photodetection and phase encoding are the dominant sources of error in the ONN presented here (as discussed above). To understand the role of phase encoding noise and photodection noise in our ONN hardware architecture and to develop a model for its accuracy, we numerically simulate the performance of our trained matrices with varying degrees of phase encoding noise (\u03c3 \u03a6 ) and photodection noise (\u03c3 D ) (detailed simulation steps can be found in methods section). The distribution of correctness percentage vs \u03c3 \u03a6 and \u03c3 D is shown in Fig. 3 (a), which serves as a guide to understanding experimental performance of the ONN. Improvements to the control and readout hardware, includ-ing implementing higher precision analog-to-digital converters in the photodetection array and voltage controller, are practical avenues towards approaching the performance of digital computers. Well-known techniques can be applied to engineer the photodiode array to achieve significantly higher dynamic range; for example, using logarithmic or multi-stage gain amplifiers. Addressing these managable engineering problems can further enhance the correctness performance of the ONN to ultimately achieve correctness percentages approaching those of error-corrected digital computers. In addition, ANN parameters trained by conventional back propagation algorithm can become suboptimal when encoding errors are encountered. In such a case, robust simulated annealing algorithms [37] can be used to train ANN parameters which is error-tolerant, hence when encoded in the ONN, will have better performance.\n\n\nDISCUSSION\n\nProcessing big data at high speeds and with low power is a central challenge in the field of computer science, and, in fact, a majority of the power and processors in data centers are spent on doing forward propagation (test-time prediction). Furthermore, low forward propagation speeds limit applications of ANNs in many fields including self-driving cars which require high speed and parallel image recognition.\n\nOur optical neural network architecture takes advantage of high detection rate, high-sensitivity photon detectors to enable high-speed, energy-efficient neural networks compared to state-of-the-art electronic computer architectures. Once all parameters have been trained and programmed on the nanophotonic processor, forward propagation computing is performed optically on a passive system. In our implementation, maintaining the phase modulator settings requires some (small) power of \u223c10 mW per modulator on average. However, in future implementations, the phases could be set with nonvolatile phase-change materials [38], which would require no power to maintain. With this change, the total power consumption is limited only by the physical size, the spectral bandwidth of dispersive components (THz), and the photo-detection rate (100GHz). In principle, such a system can be at least 2 orders of magnitude faster than electronic neural networks (which are restricted at GHz clock rate). Assuming our ONN has N nodes, implementing m layers of N\u00d7N matrix multiplication and operating at a typical 100 GHz photo-detection rate, the number of operations per second of our system would be R = 2m \u00b7 N 2 \u00b7 10 11 operations/s ONN power consumption during computation is dominated by the optical power necessary to trigger an optical nonlinearity and achieve a sufficiently high signal-to-noise ratio (SNR) at the photodetectors (assuming shot-noise limited detection on n photons per pulse, SNR \u221a 1/n). We assume a saturable absorber threshold of p 1 MW/cm 2valid for many dyes, semiconductors, and graphene [21,22]. Since the cross section for the waveguide is A = 0.2\u00b5m \u00d70.5\u00b5m, the total power needed to run the system is therefore estimated to be: P \u2248 N mW. Therefore, the energy per operation of ONN will scale as R/P = 2m \u00b7 N \u00b7 10 14 operations/J (or P/R = 5 mN fJ/operation). Almost the same energy performance and speed can be obtained if optical bistability [24,27,39] is used instead of saturable absorption as the enabling nonlinear phenomenon. Even for very small neural networks, the above power efficiency is already at least 3 orders of magnitude better than that in conventional electronic CPUs and GPUs, where P/R \u2248 1pJ/operation (not including the power spent on data movement) [40], while conventional image recognition tasks require tens of millions of training parameters and thousands of neurons (mN \u2248 10 5 ) [41]. These considerations suggest that the optical NN approach may be tens of millions times more efficient than conventional computers for standard problem sizes. In fact, the larger the neural network, the bigger the advantage of using optics is: this comes from the fact that evaluating an N \u00d7 N matrix in electronics requires O(N 2 ) energy, while in optics, it requires in principle no energy. Further details on power efficiency calculation can be found in the Supplementary information section 3.\n\nONNs enable new ways to train ANN parameters. On a conventional computer, parameters are trained with back propagation and gradient descent. However, for certain ANNs where the effective number of parameters substantially exceeds the number of distinct parameters (includ-ing recurrent neural networks (RNN) and convolutional neural networks(CNN)), training using back propagation is notoriously inefficient. Specifically the recurrent nature of RNNs gives them effectively an extremely deep ANN (depth=sequence length), while in CNNs the same weight parameters are used repeatedly in different parts of an image for extracting features. Here we propose an alternative approach to directly obtain the gradient of each distinct parameter without back propagation, using forward propagation on ONN and the finite difference method. It is well known that the gradient for a particular distinct weight parameter \u2206W ij in ANN can be obtained with two forward propagation steps that compute J(W ij ) and J(W ij + \u03b4 ij ), followed by the evaluation of \u2206W ij = J(W ij +\u03b4 ij )\u2212J(W ij ) \u03b4 ij (this step only takes two operations). On a conventional computer, this scheme is not favored because forward propagation (evaluating J(W)) is computationally expensive. In an ONN, each forward propagation step is computed in constant time (limited by the photodetection rate which can exceed 100 GHz [12]), with power consumption that is only proportional to the number of neurons-making the scheme above tractable. Furthermore, with this on-chip training scheme, one can readily parametrize and train unitary matrices-an approach known to be particularly useful for deep neural networks [42]. As a proof of concept, we carry out the unitary-matrix-on-chip training scheme for our vowel recognition problem (see Supplementary Information Section 4).\n\nRegarding the physical size of the proposed ONN, current technologies are capable of realizing ONNs exceeding the 1000 neuron regime -photonic circuits with up to 4096 optical components have been demonstrated [43]. 3-D photonic integration could enable even larger ONNs by adding another spatial degree of freedom [44]. Furthermore, by feeding in input signals (e.g. an image) via multiple patches over time (instead of all at once) -an algorithm that has been increasingly adopted by deep learning community [45] -the ONN should be able to realize much bigger effective neural networks with relatively small number of physical neurons.\n\n\nCONCLUSION\n\nThe proposed architecture could be applied to other artificial neural network algorithms where matrix multiplications and nonlinear activations are heavily used, including convolutional neural networks and recurrent neural networks. Further, the superior forward propagation speed and power efficiency of our ONN can potentially enable training the neural network on the photonics chip directly, using only forward propagation. Finally, it needs to be emphasized that another major portion of power dissipation in current NN architectures is associated with data movement-an outstanding challenge that remains to be addressed. However, recent dramatic improvements in optical interconnects using integrated photonics technology has the potential to significantly reduce data-movement energy cost [46]. Further integration of optical interconnects and optical computing units need to be explored to realize the full advantage of all-optical computing.\n\n\nMETHODS\n\n\nFidelity Analysis\n\nWe evaluated the performance of the SU(4) core with the fidelity metric f = \u2211 i \u221a p i q i where p i , q i are experimental and simulated normalized (\u2211 i x i = 1 where x \u2208 {p, q}) optical intensity distributions across the waveguide modes, respectively. Simulation Method for Noise in ONN We carry out the following steps to numerically simulate the performance of our trained matrices with varying degrees of phase encoding (\u03c3 \u03a6 ) and detection (\u03c3 D ) noise.\n\n1. For each of the four trained 4 \u00d7 4 unitary matrices U k , we calculate a set of {\u03b8 k i , \u03c6 k i } that encode the matrix.\n\n2. We add a set of random phase encoding errors, {\u03b4\u03b8 k i , \u03b4\u03c6 k i } to the old calculated phases {\u03b8 k i , \u03c6 k i }, where we assume each \u03b4\u03b8 k i and \u03b4\u03c6 k i is a random variable sampled from a Gaussian distribution G(\u00b5, \u03c3) with \u00b5 = 0 and \u03c3 = \u03c3 \u03a6 . We obtain a new set of perturbed phases {\u03b8 k i , \u03c6 k i } = {\u03b8 k i + \u03b4\u03b8 k i , \u03c6 k i + \u03b4\u03c6 k i }.\n\n3. We encode the four perturbed 4 \u00d7 4 unitary matrices U k based on the new perturbed phases {\u03b8 k i , \u03c6 k i }. 4. We carry out the forward propagation algorithm based on the perturbed matrices U k with our test data set. During the forward propagation, every time when a matrix multiplication is performed (let's say when we compute \u2212 \u2192 v = U k \u00b7 \u2212 \u2192 u ), we add a set of random photo-detection errors \u2212 \u2192 \u03b4v to the resulting \u2212 \u2192 v , where we assume each entry of \u2212 \u2192 \u03b4v is a random variable sampled from a Gaussian distribution G(\u00b5, \u03c3) with \u00b5 = 0 and \u03c3 = \u03c3 D \u00b7 | \u2212 \u2192 v |. We obtain the perturbed output vector \u2212 \u2192 v = \u2212 \u2192 v + \u2212 \u2192 \u03b4v.\n\n\n5.\n\nWith the modified forward propagation scheme above, we calculate the correctness percentage for the perturbed ONN.\n\n6. Steps 2)-5) are repeated 50 times to obtain the distribution of correctness percentage for each phase encoding noise (\u03c3 \u03a6 ) and photodetection noise (\u03c3 D ).\n\nFIG. 1 .\n1General Architecture of Optical Neural Network a. General artificial neural network architecture composed of an input layer, a number of hidden layers, and an output layer. b. Decomposition of the general neural network into individual layers. c. Optical interference and nonlinearity units that compose each layer of the artificial neural network.\n\nFIG. 2 .\n2Illustration of Optical Interference Unit a. Optical micrograph of an experimentally fabricated 22-mode on-chip optical interference unit; the physical region where the optical neural network program exists is highlighted in grey. The system acts as an optical field-programmable gate array-a test bed for optical experiments. b. Schematic illustration of the optical neural network program demonstrated here which realizes both matrix multiplication and amplification fully optically. c. Schematic illustration of a single phase shifter in the Mach-Zehnder Interferometer (MZI) and the transmission curve for tuning the internal phase shifter of the MZI.\n\nFIG. 3 .\n3Vowel recognition. (a) Correct rate for vowel recognition problem with various phase encoding error (\u03c3 \u03a6 ) and photodetection error (\u03c3 D ), the definition of these two variables can be found in method section. The solid lines are the contours for different level correctness percentage. (b-e) Simulated and experimental vowel recognition results for an error-free training matrix where (b) vowel A was spoken, (c) vowel B was spoken, (d) vowel C was spoken, and (e) vowel D was spoken.\nOPTICAL NEURAL NETWORK DEVICE ARCHITECTUREAn artificial neural network (ANN)[15]consists of a set of input artificial neurons (represented as circles inFig. 1\nReducing the dimensionality of data with neural networks. G E Hinton, R R Salakhutdinov, Science. 313Hinton, G. E. & Salakhutdinov, R. R. Reducing the di- mensionality of data with neural networks. Science 313, 504-507 (2006).\n\nNeuromorphic electronic systems. C Mead, Proceedings of the IEEE. 78Mead, C. Neuromorphic electronic systems. Proceedings of the IEEE 78, 1629-1636 (1990).\n\nNeuromorphic silicon neurons and large-scale neural networks: challenges and opportunities. C.-S Poon, K Zhou, http:/www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2011.00108/fullFrontiers in Neuroscience. 5Poon, C.-S. & Zhou, K. Neuromorphic silicon neu- rons and large-scale neural networks: challenges and opportunities. Frontiers in Neuroscience 5 (2011). URL http://www.frontiersin.org/neuromorphic_ engineering/10.3389/fnins.2011.00108/full.\n\nIsaac: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. A Shafiee, Proc. ISCA. ISCAShafiee, A. et al. Isaac: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. In Proc. ISCA (2016).\n\nArtificial neural networks in hardware: A survey of two decades of progress. J Misra, I Saha, Neurocomputing. 74Misra, J. & Saha, I. Artificial neural networks in hardware: A survey of two decades of progress. Neurocomputing 74, 239-255 (2010).\n\nMastering the game of go with deep neural networks and tree search. D Silver, Nature. 529Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484-489 (2016).\n\nPhotonic neuromorphic signal processing and computing. A N Tait, M A Nahmias, Y Tian, B J Shastri, P R Prucnal, Nanophotonic Information Physics. SpringerTait, A. N., Nahmias, M. A., Tian, Y., Shastri, B. J. & Prucnal, P. R. Photonic neuromorphic signal processing and computing. In Nanophotonic Information Physics, 183-222 (Springer, 2014).\n\nBroadcast and weight: an integrated network for scalable photonic spike processing. A N Tait, M A Nahmias, B J Shastri, P R Prucnal, Journal of Lightwave Technology. 32Tait, A. N., Nahmias, M. A., Shastri, B. J. & Prucnal, P. R. Broadcast and weight: an integrated network for scalable photonic spike processing. Journal of Lightwave Technology 32, 3427-3439 (2014).\n\nRecent progress in semiconductor excitable lasers for photonic spike processing. P R Prucnal, B J Shastri, T F De Lima, M A Nahmias, A N Tait, Advances in Optics and Photonics. 8Prucnal, P. R., Shastri, B. J., de Lima, T. F., Nahmias, M. A. & Tait, A. N. Recent progress in semiconductor excitable lasers for photonic spike processing. Advances in Optics and Photonics 8, 228-299 (2016).\n\nExperimental demonstration of reservoir computing on a silicon photonics chip. K Vandoorne, Nature communications. 5Vandoorne, K. et al. Experimental demonstration of reser- voir computing on a silicon photonics chip. Nature commu- nications 5 (2014).\n\nInformation processing using a single dynamical node as complex system. L Appeltant, Nature communications. 2468Appeltant, L. et al. Information processing using a single dynamical node as complex system. Nature communications 2, 468 (2011).\n\nZero-bias 40gbit/s germanium waveguide photodetector on silicon. L Vivien, Opt. Express. 20Vivien, L. et al. Zero-bias 40gbit/s germanium waveg- uide photodetector on silicon. Opt. Express 20, 1096- 1101 (2012).\n\nLow loss etchless silicon photonic waveguides. J Cardenas, 10.1364/OE.17.004752Opt. Express. 174752Cardenas, J. et al. Low loss etchless silicon photonic waveguides. Opt. Express 17, 4752 (2009). URL http: //dx.doi.org/10.1364/OE.17.004752.\n\nBosonic transport simulations in a large-scale programmable nanophotonic processor. N C Harris, arXiv:1507.03406Harris, N. C. et al. Bosonic transport simulations in a large-scale programmable nanophotonic processor. arXiv:1507.03406 (2015).\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, 10.1038/nature14539Nature. 521LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015). URL http://dx.doi.org/10.1038/ nature14539.\n\nDeep learning in neural networks: An overview. J Schmidhuber, Neural Networks. 61Schmidhuber, J. Deep learning in neural networks: An overview. Neural Networks 61, 85-117 (2015).\n\nSolving least squares problems. C L Lawson, R J Hanson, SIAM15Lawson, C. L. & Hanson, R. J. Solving least squares prob- lems, vol. 15 (SIAM, 1995).\n\nExperimental realization of any discrete unitary operator. M Reck, A Zeilinger, H J Bernstein, P Bertani, http:/link.aps.org/doi/10.1103/PhysRevLett.73.58Phys. Rev. Lett. 73Reck, M., Zeilinger, A., Bernstein, H. J. & Bertani, P. Experimental realization of any discrete unitary operator. Phys. Rev. Lett. 73, 58-61 (1994). URL http://link. aps.org/doi/10.1103/PhysRevLett.73.58.\n\nPerfect optics with imperfect components. D A B Miller, 2Miller, D. A. B. Perfect optics with imperfect components. Optica 2, 747-750 (2015). URL http://www.osapublishing.org/optica/abstract. cfm?URI=optica-2-8-747.\n\nSemiconductor optical amplifiers. M J Connelly, Springer Science & Business MediaConnelly, M. J. Semiconductor optical amplifiers (Springer Science & Business Media, 2007).\n\nPulse transmission through a saturable absorber. A Selden, British Journal of Applied Physics. 18743Selden, A. Pulse transmission through a saturable absorber. British Journal of Applied Physics 18, 743 (1967).\n\nMonolayer graphene as a saturable absorber in a mode-locked laser. Q Bao, 10.1007/s12274-010-0082-9Nano Res. 4Bao, Q. et al. Monolayer graphene as a saturable absorber in a mode-locked laser. Nano Res. 4, 297-307 (2010). URL http://dx.doi.org/10.1007/s12274-010-0082-9.\n\nNonlinear mirror based on two-photon absorption. R W Schirmer, A L Gaeta, JOSA B. 14Schirmer, R. W. & Gaeta, A. L. Nonlinear mirror based on two-photon absorption. JOSA B 14, 2865-2868 (1997).\n\nOptimal bistable switching in nonlinear photonic crystals. M Solja\u010di\u0107, M Ibanescu, S G Johnson, Y Fink, J Joannopoulos, Physical Review E. 6655601Solja\u010di\u0107, M., Ibanescu, M., Johnson, S. G., Fink, Y. & Joannopoulos, J. Optimal bistable switching in nonlinear photonic crystals. Physical Review E 66, 055601 (2002).\n\nExperimental observations of bistability and instability in a two-dimensional nonlinear optical superlattice. B Xu, N.-B Ming, http:/link.aps.org/doi/10.1103/PhysRevLett.71.3959Phys. Rev. Lett. 71Xu, B. & Ming, N.-B. Experimental observations of bista- bility and instability in a two-dimensional nonlinear opti- cal superlattice. Phys. Rev. Lett. 71, 3959-3962 (1993). URL http://link.aps.org/doi/10.1103/PhysRevLett. 71.3959.\n\nOptical bistability in finite-size nonlinear bidimensional photonic crystals doped by a microcavity. E Centeno, D Felbacq, http:/link.aps.org/doi/10.1103/PhysRevB.62.R7683Phys. Rev. B. 62Centeno, E. & Felbacq, D. Optical bistability in finite-size nonlinear bidimensional photonic crystals doped by a micro- cavity. Phys. Rev. B 62, R7683-R7686 (2000). URL http: //link.aps.org/doi/10.1103/PhysRevB.62.R7683.\n\nSub-femtojoule all-optical switching using a photonic-crystal nanocavity. K Nozaki, 10.1038/nphoton.2010.89Nature Photonics. 4Nozaki, K. et al. Sub-femtojoule all-optical switching using a photonic-crystal nanocavity. Nature Photonics 4, 477- 483 (2010). URL http://dx.doi.org/10.1038/nphoton. 2010.89.\n\nIntegrated all-photonic non-volatile multilevel memory. C R\u00edos, 10.1038/nphoton.2015.182Nature Photonics. 9R\u00edos, C. et al. Integrated all-photonic non-volatile multi- level memory. Nature Photonics 9, 725-732 (2015). URL http://dx.doi.org/10.1038/nphoton.2015.182.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. Pereira, F., Burges, C. J. C., Bottou, L. & Weinberger, K. Q.Curran Associates, Inc25Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Pereira, F., Burges, C. J. C., Bottou, L. & Wein- berger, K. Q. (eds.) Advances in Neural Information Processing Systems 25, 1097-1105 (Curran Associates, Inc., 2012).\n\nSpeaker identification based on log area ratio and gaussian mixture models in narrow-band speech. D Chow, W H Abdulla, PRICAI 2004: Trends in Artificial Intelligence. SpringerChow, D. & Abdulla, W. H. Speaker identification based on log area ratio and gaussian mixture models in narrow-band speech. In PRICAI 2004: Trends in Artificial Intelligence, 901-908 (Springer, 2004).\n\nSpeaker normalisation for automatic speech recognition. D H Deterding, University of CambridgePh.D. thesisDeterding, D. H. Speaker normalisation for automatic speech recognition. Ph.D. thesis, University of Cambridge (1990).\n\nEfficient, compact and low loss thermooptic phase shifter in silicon. N C Harris, 10.1364/OE.22.010487Optics Express. 22Harris, N. C. et al. Efficient, compact and low loss thermo- optic phase shifter in silicon. Optics Express 22 (2014). URL http://dx.doi.org/10.1364/OE.22.010487.\n\nA 25 Gb/s Silicon Photonics Platform. T Baehr-Jones, ArXiv e-printsBaehr-Jones, T. et al. A 25 Gb/s Silicon Photonics Platform. ArXiv e-prints (2012). URL http://adsabs. harvard.edu/abs/2012arXiv1203.0767B. 1203.0767.\n\nSelf-configuring universal linear optical component. D A B Miller, invitedMiller, D. A. B. Self-configuring universal linear optical component [invited].\n\n. 10.1364/PRJ.1.000001Photonics Research. 1Photonics Research 1 (2013). URL http://dx.doi.org/10.1364/PRJ.1.000001.\n\nIn-plane optical absorption and free carrier absorption in graphene-on-silicon waveguides. Z Cheng, H K Tsang, X Wang, K Xu, J.-B Xu, IEEE Journal of Selected Topics in Quantum Electronics. 20Cheng, Z., Tsang, H. K., Wang, X., Xu, K. & Xu, J.-B. In-plane optical absorption and free carrier absorption in graphene-on-silicon waveguides. IEEE Journal of Selected Topics in Quantum Electronics 20, 43-48 (2014).\n\nOn the repeatability of this experiment: we have carried out the entire testing run 3 times. The reported result (76.7% correctness percentage) is associated with the best calibrated run (highest measured fidelity). The other two less calibrated runs exceeded 70% correctness percentage. For further discussion on enhancing the correctness percentage, see S.I. (Section 6On the repeatability of this experiment: we have carried out the entire testing run 3 times. The reported result (76.7% correctness percentage) is associated with the best cali- brated run (highest measured fidelity). The other two less calibrated runs exceeded 70% correctness percentage. For further discussion on enhancing the correctness percentage, see S.I. (Section 6).\n\nRobust optimization with simulated annealing. D Bertsimas, O Nohadani, Journal of Global Optimization. 48Bertsimas, D. & Nohadani, O. Robust optimization with simulated annealing. Journal of Global Optimization 48, 323-334 (2010).\n\nIntegrated all-photonic non-volatile multilevel memory. C R\u00edos, Nature Photonics. 9R\u00edos, C. et al. Integrated all-photonic non-volatile multi- level memory. Nature Photonics 9, 725-732 (2015).\n\nFast bistable all-optical switch and memory on a silicon photonic crystal on-chip. T Tanabe, M Notomi, S Mitsugi, A Shinya, E Kuramochi, Opt. Lett. 30Tanabe, T., Notomi, M., Mitsugi, S., Shinya, A. & Ku- ramochi, E. Fast bistable all-optical switch and memory on a silicon photonic crystal on-chip. Opt. Lett. 30, 2575- 2577 (2005). URL http://ol.osa.org/abstract.cfm? URI=ol-30-19-2575.\n\n1.1 computing's energy problem (and what we can do about it). M Horowitz, 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC). IEEEHorowitz, M. 1.1 computing's energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 10-14 (IEEE, 2014).\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097- 1105 (2012).\n\nUnitary evolution recurrent neural networks. M Arjovsky, A Shah, Y Bengio, arXiv:1511.06464arXiv preprintArjovsky, M., Shah, A. & Bengio, Y. Unitary evolution recurrent neural networks. arXiv preprint arXiv:1511.06464 (2015).\n\nLarge-scale nanophotonic phased array. J Sun, E Timurdogan, A Yaacobi, E S Hosseini, M R Watts, 10.1038/nature11727Nature. 493Sun, J., Timurdogan, E., Yaacobi, A., Hosseini, E. S. & Watts, M. R. Large-scale nanophotonic phased array. Na- ture 493, 195-199 (2013). URL http://dx.doi.org/10. 1038/nature11727.\n\nPhotonic floquet topological insulators. M C Rechtsman, Nature. 496Rechtsman, M. C. et al. Photonic floquet topological insu- lators. Nature 496, 196-200 (2013).\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, http:/doi.acm.org/10.1145/2647868.2654889Proceedings of the 22Nd ACM International Conference on Multimedia, MM '14. the 22Nd ACM International Conference on Multimedia, MM '14New York, NY, USAACMJia, Y. et al. Caffe: Convolutional architecture for fast fea- ture embedding. In Proceedings of the 22Nd ACM Interna- tional Conference on Multimedia, MM '14, 675-678 (ACM, New York, NY, USA, 2014). URL http://doi.acm.org/ 10.1145/2647868.2654889.\n\nSingle-chip microprocessor that communicates directly using light. C Sun, 10.1038/nature16454Nature. 528Sun, C. et al. Single-chip microprocessor that communicates directly using light. Nature 528, 534-538 (2015). URL http://dx.doi.org/10.1038/nature16454.\n", "annotations": {"author": "[{\"end\":169,\"start\":63},{\"end\":282,\"start\":170},{\"end\":390,\"start\":283},{\"end\":499,\"start\":391},{\"end\":594,\"start\":500},{\"end\":690,\"start\":595},{\"end\":784,\"start\":691},{\"end\":878,\"start\":785},{\"end\":943,\"start\":879},{\"end\":1051,\"start\":944},{\"end\":1161,\"start\":1052}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":187,\"start\":181},{\"end\":295,\"start\":289},{\"end\":404,\"start\":398},{\"end\":515,\"start\":504},{\"end\":611,\"start\":603},{\"end\":698,\"start\":695},{\"end\":796,\"start\":792},{\"end\":894,\"start\":884},{\"end\":956,\"start\":949},{\"end\":1066,\"start\":1058}]", "author_first_name": "[{\"end\":69,\"start\":63},{\"end\":178,\"start\":170},{\"end\":180,\"start\":179},{\"end\":288,\"start\":283},{\"end\":397,\"start\":391},{\"end\":503,\"start\":500},{\"end\":602,\"start\":595},{\"end\":694,\"start\":691},{\"end\":791,\"start\":785},{\"end\":883,\"start\":879},{\"end\":948,\"start\":944},{\"end\":1057,\"start\":1052}]", "author_affiliation": "[{\"end\":168,\"start\":76},{\"end\":281,\"start\":189},{\"end\":389,\"start\":297},{\"end\":498,\"start\":406},{\"end\":593,\"start\":517},{\"end\":689,\"start\":613},{\"end\":783,\"start\":700},{\"end\":877,\"start\":798},{\"end\":942,\"start\":896},{\"end\":1050,\"start\":958},{\"end\":1160,\"start\":1068}]", "title": "[{\"end\":50,\"start\":1},{\"end\":1211,\"start\":1162}]", "venue": null, "abstract": "[{\"end\":2251,\"start\":1223}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2935,\"start\":2932},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3235,\"start\":3232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3238,\"start\":3235},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3241,\"start\":3238},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3244,\"start\":3241},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3370,\"start\":3367},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3441,\"start\":3438},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3444,\"start\":3441},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3447,\"start\":3444},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3476,\"start\":3472},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3479,\"start\":3476},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3959,\"start\":3955},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4036,\"start\":4032},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4552,\"start\":4548},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6266,\"start\":6262},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6793,\"start\":6789},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7180,\"start\":7176},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7183,\"start\":7180},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7562,\"start\":7558},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7654,\"start\":7650},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7658,\"start\":7654},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7662,\"start\":7658},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7683,\"start\":7679},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7687,\"start\":7683},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7691,\"start\":7687},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7695,\"start\":7691},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7699,\"start\":7695},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7880,\"start\":7876},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8207,\"start\":8203},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8568,\"start\":8564},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8818,\"start\":8815},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9038,\"start\":9034},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9284,\"start\":9280},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9737,\"start\":9733},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9825,\"start\":9821},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9828,\"start\":9825},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9926,\"start\":9922},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9929,\"start\":9926},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10035,\"start\":10031},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10038,\"start\":10035},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11782,\"start\":11778},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11823,\"start\":11819},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12738,\"start\":12734},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14386,\"start\":14382},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15561,\"start\":15557},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16547,\"start\":16543},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16550,\"start\":16547},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16905,\"start\":16901},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16908,\"start\":16905},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16911,\"start\":16908},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17234,\"start\":17230},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17369,\"start\":17365},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19258,\"start\":19254},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19546,\"start\":19542},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19919,\"start\":19915},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20024,\"start\":20020},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20219,\"start\":20215},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21157,\"start\":21153},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22377,\"start\":22376}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23541,\"start\":23182},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24208,\"start\":23542},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24705,\"start\":24209}]", "paragraph": "[{\"end\":3676,\"start\":2253},{\"end\":4553,\"start\":3678},{\"end\":5697,\"start\":4555},{\"end\":6680,\"start\":5879},{\"end\":7563,\"start\":6682},{\"end\":7881,\"start\":7565},{\"end\":8960,\"start\":7896},{\"end\":9738,\"start\":8962},{\"end\":10418,\"start\":9740},{\"end\":11115,\"start\":10420},{\"end\":11847,\"start\":11117},{\"end\":12411,\"start\":11890},{\"end\":12794,\"start\":12413},{\"end\":14508,\"start\":12796},{\"end\":14936,\"start\":14523},{\"end\":17869,\"start\":14938},{\"end\":19703,\"start\":17871},{\"end\":20342,\"start\":19705},{\"end\":21307,\"start\":20357},{\"end\":21797,\"start\":21339},{\"end\":21922,\"start\":21799},{\"end\":22263,\"start\":21924},{\"end\":22899,\"start\":22265},{\"end\":23020,\"start\":22906},{\"end\":23181,\"start\":23022}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5878,\"start\":5698},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11889,\"start\":11848}]", "table_ref": null, "section_header": "[{\"end\":7894,\"start\":7884},{\"end\":14521,\"start\":14511},{\"end\":20355,\"start\":20345},{\"end\":21317,\"start\":21310},{\"end\":21337,\"start\":21320},{\"end\":22904,\"start\":22902},{\"end\":23191,\"start\":23183},{\"end\":23551,\"start\":23543},{\"end\":24218,\"start\":24210}]", "table": null, "figure_caption": "[{\"end\":23541,\"start\":23193},{\"end\":24208,\"start\":23553},{\"end\":24705,\"start\":24220}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5912,\"start\":5906},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6341,\"start\":6329},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6364,\"start\":6355},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8040,\"start\":8034},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9153,\"start\":9147},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9953,\"start\":9943},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13468,\"start\":13458}]", "bib_author_first_name": "[{\"end\":24924,\"start\":24923},{\"end\":24926,\"start\":24925},{\"end\":24936,\"start\":24935},{\"end\":24938,\"start\":24937},{\"end\":25127,\"start\":25126},{\"end\":25346,\"start\":25342},{\"end\":25354,\"start\":25353},{\"end\":25807,\"start\":25806},{\"end\":26048,\"start\":26047},{\"end\":26057,\"start\":26056},{\"end\":26285,\"start\":26284},{\"end\":26476,\"start\":26475},{\"end\":26478,\"start\":26477},{\"end\":26486,\"start\":26485},{\"end\":26488,\"start\":26487},{\"end\":26499,\"start\":26498},{\"end\":26507,\"start\":26506},{\"end\":26509,\"start\":26508},{\"end\":26520,\"start\":26519},{\"end\":26522,\"start\":26521},{\"end\":26849,\"start\":26848},{\"end\":26851,\"start\":26850},{\"end\":26859,\"start\":26858},{\"end\":26861,\"start\":26860},{\"end\":26872,\"start\":26871},{\"end\":26874,\"start\":26873},{\"end\":26885,\"start\":26884},{\"end\":26887,\"start\":26886},{\"end\":27214,\"start\":27213},{\"end\":27216,\"start\":27215},{\"end\":27227,\"start\":27226},{\"end\":27229,\"start\":27228},{\"end\":27240,\"start\":27239},{\"end\":27242,\"start\":27241},{\"end\":27253,\"start\":27252},{\"end\":27255,\"start\":27254},{\"end\":27266,\"start\":27265},{\"end\":27268,\"start\":27267},{\"end\":27601,\"start\":27600},{\"end\":27847,\"start\":27846},{\"end\":28083,\"start\":28082},{\"end\":28278,\"start\":28277},{\"end\":28557,\"start\":28556},{\"end\":28559,\"start\":28558},{\"end\":28731,\"start\":28730},{\"end\":28740,\"start\":28739},{\"end\":28750,\"start\":28749},{\"end\":28960,\"start\":28959},{\"end\":29125,\"start\":29124},{\"end\":29127,\"start\":29126},{\"end\":29137,\"start\":29136},{\"end\":29139,\"start\":29138},{\"end\":29301,\"start\":29300},{\"end\":29309,\"start\":29308},{\"end\":29322,\"start\":29321},{\"end\":29324,\"start\":29323},{\"end\":29337,\"start\":29336},{\"end\":29664,\"start\":29663},{\"end\":29668,\"start\":29665},{\"end\":29873,\"start\":29872},{\"end\":29875,\"start\":29874},{\"end\":30062,\"start\":30061},{\"end\":30292,\"start\":30291},{\"end\":30545,\"start\":30544},{\"end\":30547,\"start\":30546},{\"end\":30559,\"start\":30558},{\"end\":30561,\"start\":30560},{\"end\":30749,\"start\":30748},{\"end\":30761,\"start\":30760},{\"end\":30773,\"start\":30772},{\"end\":30775,\"start\":30774},{\"end\":30786,\"start\":30785},{\"end\":30794,\"start\":30793},{\"end\":31115,\"start\":31114},{\"end\":31124,\"start\":31120},{\"end\":31535,\"start\":31534},{\"end\":31546,\"start\":31545},{\"end\":31918,\"start\":31917},{\"end\":32204,\"start\":32203},{\"end\":32479,\"start\":32478},{\"end\":32493,\"start\":32492},{\"end\":32506,\"start\":32505},{\"end\":32508,\"start\":32507},{\"end\":33035,\"start\":33034},{\"end\":33043,\"start\":33042},{\"end\":33045,\"start\":33044},{\"end\":33370,\"start\":33369},{\"end\":33372,\"start\":33371},{\"end\":33610,\"start\":33609},{\"end\":33612,\"start\":33611},{\"end\":33862,\"start\":33861},{\"end\":34096,\"start\":34095},{\"end\":34100,\"start\":34097},{\"end\":34406,\"start\":34405},{\"end\":34415,\"start\":34414},{\"end\":34417,\"start\":34416},{\"end\":34426,\"start\":34425},{\"end\":34434,\"start\":34433},{\"end\":34443,\"start\":34439},{\"end\":35520,\"start\":35519},{\"end\":35533,\"start\":35532},{\"end\":35762,\"start\":35761},{\"end\":35983,\"start\":35982},{\"end\":35993,\"start\":35992},{\"end\":36003,\"start\":36002},{\"end\":36014,\"start\":36013},{\"end\":36024,\"start\":36023},{\"end\":36351,\"start\":36350},{\"end\":36715,\"start\":36714},{\"end\":36729,\"start\":36728},{\"end\":36742,\"start\":36741},{\"end\":36744,\"start\":36743},{\"end\":37035,\"start\":37034},{\"end\":37047,\"start\":37046},{\"end\":37055,\"start\":37054},{\"end\":37256,\"start\":37255},{\"end\":37263,\"start\":37262},{\"end\":37277,\"start\":37276},{\"end\":37288,\"start\":37287},{\"end\":37290,\"start\":37289},{\"end\":37302,\"start\":37301},{\"end\":37304,\"start\":37303},{\"end\":37567,\"start\":37566},{\"end\":37569,\"start\":37568},{\"end\":37751,\"start\":37750},{\"end\":38271,\"start\":38270}]", "bib_author_last_name": "[{\"end\":24933,\"start\":24927},{\"end\":24952,\"start\":24939},{\"end\":25132,\"start\":25128},{\"end\":25351,\"start\":25347},{\"end\":25359,\"start\":25355},{\"end\":25815,\"start\":25808},{\"end\":26054,\"start\":26049},{\"end\":26062,\"start\":26058},{\"end\":26292,\"start\":26286},{\"end\":26483,\"start\":26479},{\"end\":26496,\"start\":26489},{\"end\":26504,\"start\":26500},{\"end\":26517,\"start\":26510},{\"end\":26530,\"start\":26523},{\"end\":26856,\"start\":26852},{\"end\":26869,\"start\":26862},{\"end\":26882,\"start\":26875},{\"end\":26895,\"start\":26888},{\"end\":27224,\"start\":27217},{\"end\":27237,\"start\":27230},{\"end\":27250,\"start\":27243},{\"end\":27263,\"start\":27256},{\"end\":27273,\"start\":27269},{\"end\":27611,\"start\":27602},{\"end\":27857,\"start\":27848},{\"end\":28090,\"start\":28084},{\"end\":28287,\"start\":28279},{\"end\":28566,\"start\":28560},{\"end\":28737,\"start\":28732},{\"end\":28747,\"start\":28741},{\"end\":28757,\"start\":28751},{\"end\":28972,\"start\":28961},{\"end\":29134,\"start\":29128},{\"end\":29146,\"start\":29140},{\"end\":29306,\"start\":29302},{\"end\":29319,\"start\":29310},{\"end\":29334,\"start\":29325},{\"end\":29345,\"start\":29338},{\"end\":29675,\"start\":29669},{\"end\":29884,\"start\":29876},{\"end\":30069,\"start\":30063},{\"end\":30296,\"start\":30293},{\"end\":30556,\"start\":30548},{\"end\":30567,\"start\":30562},{\"end\":30758,\"start\":30750},{\"end\":30770,\"start\":30762},{\"end\":30783,\"start\":30776},{\"end\":30791,\"start\":30787},{\"end\":30807,\"start\":30795},{\"end\":31118,\"start\":31116},{\"end\":31129,\"start\":31125},{\"end\":31543,\"start\":31536},{\"end\":31554,\"start\":31547},{\"end\":31925,\"start\":31919},{\"end\":32209,\"start\":32205},{\"end\":32490,\"start\":32480},{\"end\":32503,\"start\":32494},{\"end\":32515,\"start\":32509},{\"end\":33040,\"start\":33036},{\"end\":33053,\"start\":33046},{\"end\":33382,\"start\":33373},{\"end\":33619,\"start\":33613},{\"end\":33874,\"start\":33863},{\"end\":34107,\"start\":34101},{\"end\":34412,\"start\":34407},{\"end\":34423,\"start\":34418},{\"end\":34431,\"start\":34427},{\"end\":34437,\"start\":34435},{\"end\":34446,\"start\":34444},{\"end\":35530,\"start\":35521},{\"end\":35542,\"start\":35534},{\"end\":35767,\"start\":35763},{\"end\":35990,\"start\":35984},{\"end\":36000,\"start\":35994},{\"end\":36011,\"start\":36004},{\"end\":36021,\"start\":36015},{\"end\":36034,\"start\":36025},{\"end\":36360,\"start\":36352},{\"end\":36726,\"start\":36716},{\"end\":36739,\"start\":36730},{\"end\":36751,\"start\":36745},{\"end\":37044,\"start\":37036},{\"end\":37052,\"start\":37048},{\"end\":37062,\"start\":37056},{\"end\":37260,\"start\":37257},{\"end\":37274,\"start\":37264},{\"end\":37285,\"start\":37278},{\"end\":37299,\"start\":37291},{\"end\":37310,\"start\":37305},{\"end\":37579,\"start\":37570},{\"end\":37755,\"start\":37752},{\"end\":38275,\"start\":38272}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1658773},\"end\":25091,\"start\":24865},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1169506},\"end\":25248,\"start\":25093},{\"attributes\":{\"doi\":\"http:/www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2011.00108/full\",\"id\":\"b2\",\"matched_paper_id\":7007433},\"end\":25709,\"start\":25250},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6329628},\"end\":25968,\"start\":25711},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2522635},\"end\":26214,\"start\":25970},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":515925},\"end\":26418,\"start\":26216},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10933166},\"end\":26762,\"start\":26420},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":147513},\"end\":27130,\"start\":26764},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":46580816},\"end\":27519,\"start\":27132},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":205324293},\"end\":27772,\"start\":27521},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7571855},\"end\":28015,\"start\":27774},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":34926828},\"end\":28228,\"start\":28017},{\"attributes\":{\"doi\":\"10.1364/OE.17.004752\",\"id\":\"b12\",\"matched_paper_id\":31680098},\"end\":28470,\"start\":28230},{\"attributes\":{\"doi\":\"arXiv:1507.03406\",\"id\":\"b13\"},\"end\":28713,\"start\":28472},{\"attributes\":{\"doi\":\"10.1038/nature14539\",\"id\":\"b14\",\"matched_paper_id\":1779661},\"end\":28910,\"start\":28715},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11715509},\"end\":29090,\"start\":28912},{\"attributes\":{\"id\":\"b16\"},\"end\":29239,\"start\":29092},{\"attributes\":{\"doi\":\"http:/link.aps.org/doi/10.1103/PhysRevLett.73.58\",\"id\":\"b17\",\"matched_paper_id\":6566281},\"end\":29619,\"start\":29241},{\"attributes\":{\"id\":\"b18\"},\"end\":29836,\"start\":29621},{\"attributes\":{\"id\":\"b19\"},\"end\":30010,\"start\":29838},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":93467945},\"end\":30222,\"start\":30012},{\"attributes\":{\"doi\":\"10.1007/s12274-010-0082-9\",\"id\":\"b21\",\"matched_paper_id\":119261196},\"end\":30493,\"start\":30224},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":120943479},\"end\":30687,\"start\":30495},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7773074},\"end\":31002,\"start\":30689},{\"attributes\":{\"doi\":\"http:/link.aps.org/doi/10.1103/PhysRevLett.71.3959\",\"id\":\"b24\",\"matched_paper_id\":46564789},\"end\":31431,\"start\":31004},{\"attributes\":{\"doi\":\"http:/link.aps.org/doi/10.1103/PhysRevB.62.R7683\",\"id\":\"b25\",\"matched_paper_id\":121200245},\"end\":31841,\"start\":31433},{\"attributes\":{\"doi\":\"10.1038/nphoton.2010.89\",\"id\":\"b26\",\"matched_paper_id\":122876809},\"end\":32145,\"start\":31843},{\"attributes\":{\"doi\":\"10.1038/nphoton.2015.182\",\"id\":\"b27\",\"matched_paper_id\":124787396},\"end\":32411,\"start\":32147},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":195908774},\"end\":32934,\"start\":32413},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1914021},\"end\":33311,\"start\":32936},{\"attributes\":{\"id\":\"b30\"},\"end\":33537,\"start\":33313},{\"attributes\":{\"doi\":\"10.1364/OE.22.010487\",\"id\":\"b31\",\"matched_paper_id\":7702548},\"end\":33821,\"start\":33539},{\"attributes\":{\"id\":\"b32\"},\"end\":34040,\"start\":33823},{\"attributes\":{\"id\":\"b33\"},\"end\":34195,\"start\":34042},{\"attributes\":{\"doi\":\"10.1364/PRJ.1.000001\",\"id\":\"b34\"},\"end\":34312,\"start\":34197},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":26362335},\"end\":34723,\"start\":34314},{\"attributes\":{\"id\":\"b36\"},\"end\":35471,\"start\":34725},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6991467},\"end\":35703,\"start\":35473},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":124787396},\"end\":35897,\"start\":35705},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":43099422},\"end\":36286,\"start\":35899},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":22028726},\"end\":36647,\"start\":36288},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":195908774},\"end\":36987,\"start\":36649},{\"attributes\":{\"doi\":\"arXiv:1511.06464\",\"id\":\"b42\"},\"end\":37214,\"start\":36989},{\"attributes\":{\"doi\":\"10.1038/nature11727\",\"id\":\"b43\",\"matched_paper_id\":205231845},\"end\":37523,\"start\":37216},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4349770},\"end\":37686,\"start\":37525},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/2647868.2654889\",\"id\":\"b45\",\"matched_paper_id\":1799558},\"end\":38201,\"start\":37688},{\"attributes\":{\"doi\":\"10.1038/nature16454\",\"id\":\"b46\",\"matched_paper_id\":205247044},\"end\":38459,\"start\":38203}]", "bib_title": "[{\"end\":24921,\"start\":24865},{\"end\":25124,\"start\":25093},{\"end\":25340,\"start\":25250},{\"end\":25804,\"start\":25711},{\"end\":26045,\"start\":25970},{\"end\":26282,\"start\":26216},{\"end\":26473,\"start\":26420},{\"end\":26846,\"start\":26764},{\"end\":27211,\"start\":27132},{\"end\":27598,\"start\":27521},{\"end\":27844,\"start\":27774},{\"end\":28080,\"start\":28017},{\"end\":28275,\"start\":28230},{\"end\":28728,\"start\":28715},{\"end\":28957,\"start\":28912},{\"end\":29298,\"start\":29241},{\"end\":30059,\"start\":30012},{\"end\":30289,\"start\":30224},{\"end\":30542,\"start\":30495},{\"end\":30746,\"start\":30689},{\"end\":31112,\"start\":31004},{\"end\":31532,\"start\":31433},{\"end\":31915,\"start\":31843},{\"end\":32201,\"start\":32147},{\"end\":32476,\"start\":32413},{\"end\":33032,\"start\":32936},{\"end\":33607,\"start\":33539},{\"end\":34403,\"start\":34314},{\"end\":35517,\"start\":35473},{\"end\":35759,\"start\":35705},{\"end\":35980,\"start\":35899},{\"end\":36348,\"start\":36288},{\"end\":36712,\"start\":36649},{\"end\":37253,\"start\":37216},{\"end\":37564,\"start\":37525},{\"end\":37748,\"start\":37688},{\"end\":38268,\"start\":38203}]", "bib_author": "[{\"end\":24935,\"start\":24923},{\"end\":24954,\"start\":24935},{\"end\":25134,\"start\":25126},{\"end\":25353,\"start\":25342},{\"end\":25361,\"start\":25353},{\"end\":25817,\"start\":25806},{\"end\":26056,\"start\":26047},{\"end\":26064,\"start\":26056},{\"end\":26294,\"start\":26284},{\"end\":26485,\"start\":26475},{\"end\":26498,\"start\":26485},{\"end\":26506,\"start\":26498},{\"end\":26519,\"start\":26506},{\"end\":26532,\"start\":26519},{\"end\":26858,\"start\":26848},{\"end\":26871,\"start\":26858},{\"end\":26884,\"start\":26871},{\"end\":26897,\"start\":26884},{\"end\":27226,\"start\":27213},{\"end\":27239,\"start\":27226},{\"end\":27252,\"start\":27239},{\"end\":27265,\"start\":27252},{\"end\":27275,\"start\":27265},{\"end\":27613,\"start\":27600},{\"end\":27859,\"start\":27846},{\"end\":28092,\"start\":28082},{\"end\":28289,\"start\":28277},{\"end\":28568,\"start\":28556},{\"end\":28739,\"start\":28730},{\"end\":28749,\"start\":28739},{\"end\":28759,\"start\":28749},{\"end\":28974,\"start\":28959},{\"end\":29136,\"start\":29124},{\"end\":29148,\"start\":29136},{\"end\":29308,\"start\":29300},{\"end\":29321,\"start\":29308},{\"end\":29336,\"start\":29321},{\"end\":29347,\"start\":29336},{\"end\":29677,\"start\":29663},{\"end\":29886,\"start\":29872},{\"end\":30071,\"start\":30061},{\"end\":30298,\"start\":30291},{\"end\":30558,\"start\":30544},{\"end\":30569,\"start\":30558},{\"end\":30760,\"start\":30748},{\"end\":30772,\"start\":30760},{\"end\":30785,\"start\":30772},{\"end\":30793,\"start\":30785},{\"end\":30809,\"start\":30793},{\"end\":31120,\"start\":31114},{\"end\":31131,\"start\":31120},{\"end\":31545,\"start\":31534},{\"end\":31556,\"start\":31545},{\"end\":31927,\"start\":31917},{\"end\":32211,\"start\":32203},{\"end\":32492,\"start\":32478},{\"end\":32505,\"start\":32492},{\"end\":32517,\"start\":32505},{\"end\":33042,\"start\":33034},{\"end\":33055,\"start\":33042},{\"end\":33384,\"start\":33369},{\"end\":33621,\"start\":33609},{\"end\":33876,\"start\":33861},{\"end\":34109,\"start\":34095},{\"end\":34414,\"start\":34405},{\"end\":34425,\"start\":34414},{\"end\":34433,\"start\":34425},{\"end\":34439,\"start\":34433},{\"end\":34448,\"start\":34439},{\"end\":35532,\"start\":35519},{\"end\":35544,\"start\":35532},{\"end\":35769,\"start\":35761},{\"end\":35992,\"start\":35982},{\"end\":36002,\"start\":35992},{\"end\":36013,\"start\":36002},{\"end\":36023,\"start\":36013},{\"end\":36036,\"start\":36023},{\"end\":36362,\"start\":36350},{\"end\":36728,\"start\":36714},{\"end\":36741,\"start\":36728},{\"end\":36753,\"start\":36741},{\"end\":37046,\"start\":37034},{\"end\":37054,\"start\":37046},{\"end\":37064,\"start\":37054},{\"end\":37262,\"start\":37255},{\"end\":37276,\"start\":37262},{\"end\":37287,\"start\":37276},{\"end\":37301,\"start\":37287},{\"end\":37312,\"start\":37301},{\"end\":37581,\"start\":37566},{\"end\":37757,\"start\":37750},{\"end\":38277,\"start\":38270}]", "bib_venue": "[{\"end\":24961,\"start\":24954},{\"end\":25157,\"start\":25134},{\"end\":25466,\"start\":25441},{\"end\":25827,\"start\":25817},{\"end\":26078,\"start\":26064},{\"end\":26300,\"start\":26294},{\"end\":26564,\"start\":26532},{\"end\":26928,\"start\":26897},{\"end\":27307,\"start\":27275},{\"end\":27634,\"start\":27613},{\"end\":27880,\"start\":27859},{\"end\":28104,\"start\":28092},{\"end\":28321,\"start\":28309},{\"end\":28554,\"start\":28472},{\"end\":28784,\"start\":28778},{\"end\":28989,\"start\":28974},{\"end\":29122,\"start\":29092},{\"end\":29410,\"start\":29395},{\"end\":29661,\"start\":29621},{\"end\":29870,\"start\":29838},{\"end\":30105,\"start\":30071},{\"end\":30331,\"start\":30323},{\"end\":30575,\"start\":30569},{\"end\":30826,\"start\":30809},{\"end\":31196,\"start\":31181},{\"end\":31616,\"start\":31604},{\"end\":31966,\"start\":31950},{\"end\":32251,\"start\":32235},{\"end\":32566,\"start\":32517},{\"end\":33101,\"start\":33055},{\"end\":33367,\"start\":33313},{\"end\":33655,\"start\":33641},{\"end\":33859,\"start\":33823},{\"end\":34093,\"start\":34042},{\"end\":34237,\"start\":34219},{\"end\":34502,\"start\":34448},{\"end\":35011,\"start\":34725},{\"end\":35574,\"start\":35544},{\"end\":35785,\"start\":35769},{\"end\":36045,\"start\":36036},{\"end\":36452,\"start\":36362},{\"end\":36802,\"start\":36753},{\"end\":37032,\"start\":36989},{\"end\":37337,\"start\":37331},{\"end\":37587,\"start\":37581},{\"end\":37872,\"start\":37798},{\"end\":38302,\"start\":38296},{\"end\":25833,\"start\":25829},{\"end\":37950,\"start\":37874}]"}}}, "year": 2023, "month": 12, "day": 17}