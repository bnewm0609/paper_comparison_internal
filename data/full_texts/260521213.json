{"id": 260521213, "updated": "2023-10-06 14:41:05.015", "metadata": {"title": "Image Super-Resolution With Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining", "authors": "[{\"first\":\"Yiqun\",\"last\":\"Mei\",\"middle\":[]},{\"first\":\"Yuchen\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Yuqian\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Lichao\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Huang\",\"middle\":[\"S.\"]},{\"first\":\"Honghui\",\"last\":\"Shi\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range feature-wise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However, none of the current deep models have studied another inherent property of images: cross-scale feature correlation. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new state-of-the-arts on multiple SISR benchmarks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2006.01424", "mag": "3035280441", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/MeiFZHHS20", "doi": "10.1109/cvpr42600.2020.00573"}}, "content": {"source": {"pdf_hash": "54ff45c493e5d789f840df627beb79a163f6a666", "pdf_src": "Arxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2006.01424", "status": "GREEN"}}, "grobid": {"id": "1c5c11a84f6fb44ed7a0eb0a22ea4c9b4b829baa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/54ff45c493e5d789f840df627beb79a163f6a666.txt", "contents": "\nImage Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining\n\n\nYiqun Mei \nIFP Group\nUIUC\n\n\nYuchen Fan \nIFP Group\nUIUC\n\n\nYuqian Zhou \nIFP Group\nUIUC\n\n\nLichao Huang \nHorizon Robotics\n\n\nThomas S Huang \nIFP Group\nUIUC\n\n\nHumphrey Shi \nIFP Group\nUIUC\n\n\nUniversity of Oregon\n\n\nImage Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining\nbe available at: https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention\nDeep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range featurewise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However, none of the current deep models have studied another inherent property of images: cross-scale feature correlation. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new stateof-the-arts on multiple SISR benchmarks. Our code will be available at: https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention\n\nIntroduction\n\nSingle image super resolution (SISR) aims at recovering a high-resolution (HR) image from its low-resolution (LR) counterpart. SISR has numerous applications in the areas of satellite imaging, medical imaging, surveillance monitoring and high-definition display and imaging etc [3,32,40,41,43]. The mapping between LR and HR image is not bijective, which yields more possibilities for a faithful and high-quality HR recovery. Due to this ill-posed nature, SISR remains challenging in the past decades.\n\nEarly efforts in traditional methods provide good practices for resolving SISR. By fully using the intrinsic property of the LR images, they mostly focus on local prior and non-local prior for patch matching and reconstruction. Specifically, local prior based methods, like bilinear or bicubic interpolation, reconstruct pixels merely by the weighted sum of neighbour ones. To go beyond the local limitation, methods based on non-local mean filtering [24,35] start to globally search similar patches over the whole LR image.\n\nThe non-local search for self-similarity can be further extended to cross-scale cues. It has been verified that cross-scale patch similarity widely exists in natural images [9,42]. Intuitively, in addition to non-local pixel-to-pixel matching, pixels can also be matched with larger image patches. The natural cross-scale feature correspondence makes us search high-frequency details directly from LR images, leading to more faithful, accurate and high-quality reconstructions.\n\nSince the first deep learning-based method [4] was proposed, discriminative learning based methods make it possible to use large-scale external image priors for SISR. Compared with traditional methods, they tend to have better feature representation ability, faster inference speed, endto-end trainable paradigm [10,16], and significant performance improvement. To further take the advantages of deep SISR, for several years, efforts [5,14,19,37,39,33,6] have been made on increasing the depth or width of the networks to increase the receptive field or improve the feature representation. However, the essence of the solutions was not changed, but locally finding external similar patches. It yields great limitations of deep SISR. SISR performance was boosted right after the non-local attention modules [2,20,38] were proposed. They explored non-local selfsimilarity property and embedded the non-local modules into the deep network.\n\nWhat should be the next progress for deep SISR? One intuitive idea is following the traditional methods to explore the non-local cross-scale self-similarity in deep networks. Recently, Shocher et al. [25] proposed a zero-shot superresolution (ZSSR) network to learn the high-frequency details from a pair of down-sampled LR and LR itself using one single test LR image. The essence of ZSSR is an implicit cross-scale patch matching approach using a lightweight network. However, inferring with ZSSR requires additional training time for each new LR image, which is not elegant and efficient enough for practical applications.\n\nFollowing the successful path of non-local attention modules, in this paper, we are seeking ways of incorporating cross-scale non-local attention scheme into the deep SR network. Specifically, we propose a novel Cross-Scale Non-Local (CS-NL) attention module, learning to mine long-range dependencies between LR features to largerscale HR patches within the same feature map, as shown in Figure 1. After that, we integrate the previous local prior, In-Scale Non-Local (IS-NL) prior and the proposed Cross-Scale Non-Local prior into a Self-Exemplars Mining (SEM) module, and fuse them with multi-branch mutualprojection. Finally, we embed the SEM module into a recurrent framework for image super-resolution task.\n\nIn summary, the main contributions of this paper are three-fold:\n\n\u2022 The core contribution of the paper is to propose the first Cross-Scale Non-Local (CS-NL) attention module in deep networks for SISR task. We explicitly formulate the pixel-to-patch and patch-to-patch similarities inside the image, and demonstrate that additionally mining cross-scale self-similarities greatly improves the SISR performance.\n\n\u2022 We then propose a powerful Self-Exemplar Mining (SEM) cell to fuse information recurrently. Inside the cell, we exhaustively mine all the possible intrinsic priors by combining local, in-scale non-local, and the proposed cross-scale non-local feature correlations, and embrace rich external statistics learned by the network.\n\n\u2022 The newly proposed recurrent SR network achieves the state-of-the-art performance on multiple image benchmarks. Extensive ablation experiments further verify the effectiveness of the novel network.\n\n\nRelated Works\n\nSelf-Similarity in Image SR The fact that small patches tend to recur within and across scale of a same image has been verified for most natural images [9,42]. Since then, a category of self-similarity based approaches has been extensively developed and achieves promising results. Such algorithms utilize the cross-scale information redundancy of a given image as a unique source for reconstruction without relying on any external examples [7,8,9,13,23,26,31]. In the pioneering work, Glasner et al. [9] proposed to jointly exploit repeating patches within and across image scales by integrating the idea of multiple image SR and examplebased SR into a unified framework. Furthermore, Freedman et al. [7] effectively assumed that similar patches exist in an extremely localized region and thus can greatly reduce computation time. Following this fashion, Yang et al. [31] proposed a very fast regression model that focused on only in-place cross-scale similarity. To handle appearance variations in the scene, Huang et al. [13] enlarged the internal dictionary by modeling geometric transformations. The idea of internal data repetition has also been applied to solve SR with blur and noisy images [23,26].\n\nDeep CNNs for Image SR The first work that introduced CNN to solve image SR was proposed by [4], where they interpret the three consecutive convolution layers as corresponding extraction, non-linear mapping and reconstruction step in sparse coding. Kim et al. [14] proposed a very deep model VDSR with more than 16 convolution layers benefiting from effective residual learning. To further unleash the power of deep CNNs, Lim et al. [19] integrated residual blocks into the SR framework to form a very wide model (EDSR) and a very deep model (MDSR). As the network goes as deep as hundreds of layers, Zhang et al. [39] utilized densely connected blocks with global feature fusion to effectively exploit hierarchical features from all intermediate layers. Besides extensive efforts spent on designing wider and deeper structures, algorithms with attention modules [2,20,37,38] were proposed to further enhance representation power of deep CNNs by exploring feature correlations along either spatial or channel dimension.\n\n\nNon-Local Attention in Deep Networks\n\nIn recent years, there is an emerging trend of applying non-local attention mechanism to solve various computer vision problems. In general, non-local attention in deep CNNs allows the network to concentrate more on informative areas. Wang et al. [29] initially proposed non-local neural network to seek semantic relationships for high-level tasks, such as image classification and object detection. On the contrary, nonlocal attention for image restoration is based on non-local similarities prior. Methods, such as NRLN [20], RNAN [37] and SAN [2], incorporate non-local operation into their networks in order to make better use of image structural cues, by considering long-range feature correlations. As such, they achieved considerable performance gain. However, existing non-local approaches for image restoration only explored feature similarities at the same scale, while ignoring abundant internal LR-HR exemplars across scales, leading to relatively low performance. It is known that the internal HR correspondences contain more relevant high-frequency information and stronger predictive power. To this end, we propose Cross-Scale Non-Local (CS-NL) attention by exploring cross-scale feature correlations.\n\n\nCross-Scale Non-Local (CS-NL) Attention\n\nIn this section, we formulate the proposed cross-scale non-local attention, and compare it with the existing inscale non-local attention.\n\nIn-Scale Non-Local (IS-NL) Attention Non-local attention can explore self-exemplars by summarizing related features from the whole images. Formally, given image feature map X, the non-local attention is defined as\nZ i,j = g,h exp(\u03c6(X i,j , X g,h )) u,v exp(\u03c6(X i,j , X u,v )) \u03c8(X g,h ), (1)\nwhere (i, j), (g, h) and (u, v) are pairs of coordinates of X. \u03c8(\u00b7) is feature transformation function, and \u03c6(\u00b7, \u00b7) is correlation function to measure similarity that is defined as\n\u03c6(X i,j , X g,h ) = \u03b8(X i,j ) T \u03b4(X g,h ),(2)\nwhere \u03b8(\u00b7) and \u03b4(\u00b7) are feature transformations. Note that the pixel-wise correlation is measured in the same scale.\n\n\nCross-Scale Non-Local (CS-NL) Attention\n\nThe above formulation can be easily extended to a cross-scale version referring to Figure 2. Instead of measuring the pixel-wise mutual correlation as the in-scale non-local module, the proposed cross-scale attention is designed to measure the correlation between low-resolution pixels and larger-scale patches in the LR image. To super-resolve the LR image, the Cross-Scale Non-Local (CS-NL) attention directly utilizes the patches matched to each pixel within this LR image.\n\nHence, for super-resolution purposes, cross-scale nonlocal attention is built upon in-scale attention by finding candidates in features Y = X \u2193 s downsampled by scaling factor s. The reason to do so is because directly matching pixels with patches using common similarity measurement is infeasible due to spatial dimension difference. So we simply downsample the features to represent the patch as pixel and measure the affinity. Downsampling operation in this paper is bilinear interpolation.\n\nSuppose the input feature map is X (W \u00d7H), to compute pixel-patch similarity, we need to first downsample X to Y ( W s \u00d7 H s ) and find pixel-wise similarity between X and Y , and finally use corresponding s \u00d7 s patches in X to superresolve pixels in X, thus the output Z will be sW \u00d7 sH. Cross-scale attention can be adapted from Eq.1 as\nZ s\u00d7s si,sj = g,h exp(\u03c6(X i,j , Y g,h )) u,v exp (\u03c6(X i,j , Y u,v )) \u03c8(X s\u00d7s sg,sh ),(3)\nwhere Z s\u00d7s si,sj now is the feature patch of size s \u00d7 s located at (si, sj). We obtain the weighted-averaged features Z s\u00d7s si,sj directly from the feature patches X s\u00d7s sg,sh extracted from the input feature maps. Intuitively, with the cross-scale attention, we can mine more faithful and richer high-frequency details from the original intrinsic image resources.\n\n\nPatch-Based\n\nCross-Scale Non-Local Attention Feature-wise affinity measurement can be problematic. First, high-level features are robust to transformations and distortions, that is rotated/distorted low-level patches may yield same high-level features. Take the average pooling as an example, an original region representing a HR window and its flipped version have exactly the same high-level features. Therefore, it is likely that many erroneous matches will be synthesized to HR tensors. Besides, adjacent target regions (e.g. Z s\u00d7s si,sj and Z s\u00d7s s(i+1),s(j) ) are generated in a non-overlapping fashion, possibly creating discontinuous region boundaries artifacts.\n\n\nCross-Scale NL Attention\n\nIn-Scale NL Attention\n\n\nMutual-Projected Fusion\n\nLocal Branch Based on the above analysis, we generalize to empirically implement our Cross-Scale Non-Local (CS-NL) attention using another patch-wise matching. Therefore, Eq.3 is generalized to,\n! \" ! \"#$ % \" Conv ReLU Stride Conv Deconv Self-Exemplars Mining Cell % \" % & % $ Concat SEM ! $ SEM ! \" SEMZ sp\u00d7sp si,sj = g,h exp \u03c6(X p\u00d7p i,j , Y p\u00d7p g,h ) u,v exp \u03c6(X p\u00d7p i,j , Y p\u00d7p u,v ) \u03c8(X sp\u00d7sp sg,sh ),(4)\nand Eq.4 will be identical to Eq.3 if p = 1. The measured correlations are efficiently extended to patch-level, and regions in the output feature map Z are now densely overlapped due to patch-based matching.\n\n\nMethodology\n\nThe proposed network architecture is shown in Figure  3. It is basically a recurrent neural network, with each recurrent cell called Self-Exemplars Mining (SEM) fully integrating local, in-scale non-local, and a newly proposed Cross-Scale Non-Local (CS-NL) priors. In this section, we introduce them in a bottom-up manner. Figure 2 illustrates the newly-proposed Cross-Scale Non-Local (CS-NL) attention module embedded into the deep networks. As formulated in section 3, we apply a patch-level cross-scale similarity-matching in the CS-NL attention module. Specifically, suppose we are conducting an s-scale super-resolution with the module, given a feature map X of spatial size (W, H), we first bilinearly downsample it to Y with scale s, and match the p \u00d7 p patches in X with the downsampled p \u00d7 p candidates in Y to obtain the softmax matching score. Finally, we conduct deconvolution on the score by weighted adding the patches of size (sp, sp) extracted from X. The obtained Z of size (sW, sH), will be s times super-resolved than X.\n\n\nCS-NL Attention Module\n\n\nSelf-Exemplars Mining (SEM) Cell\n\nMulti-Branch Exemplars Inside the Self-Exemplars Mining (SEM) cell, we exhaustively mine all the possible intrinsic priors, and embrace rich external image priors. Specifically, we mine the image self-similarities and learn the new information using a multi-branch structure, including the conventional Local (L) and In-Scale Non-Local (IS-NL) branches, and also the newly proposed CS-NL branch.\n\nThe local branch, in Figure 3, is a simple identical pathway connecting the convolutional features to the fusion structure. For the IS-NL branch, it contains a non-local attention module adopted from [2] and a deconvolution layer for upscaling the module outputs. The IS-NL module is region-based in this paper. As in [2], we divide the feature maps into region grids, where the inter-dependencies are captured independently in each grid. This reduces the computation burden.\n\nMutual-Projected Fusion While three-branch structure in SEM generates three feature maps by independently exploiting each information sources from LR images, how to fuse these separate tensors into a comprehensive feature map remains unclear. One possible solution is simply adding or concatenating them together, as widely used in previous works [19,20,38,39]. In this paper, we proposed a mutual-projected fusion to progressively combine features together. The algorithm procedure is illustrated in Figure 4.\n\nTo allow the network to concentrate on more informative features, we first compute the residual R IC between two features from IS-NL F I and CS-NL F C branch, and then after a single convolution layer conv on R IC , the features are added back to F I to obtain F IC .\nR IC = F I \u2212 F C ,(5)F IC = conv(R IC ) + F I .(6)\nIntuitively, the residual feature R IC represents the details existing in one source while missing in the other. Such inter-residual projection allows the network to focus on only the distinct information between sources while bypassing the common knowledge, thus improves the discriminative ability of the network.\n\nMotivated by the traditional Image SR and recent DBPN [11], we adopt the back-projection approach to incorporate local information to regularize the feature and correct reconstruction errors. Following [11], the final fused feature H is computed by,\ne = F L \u2212 downsample(F IC ),(7)H = upsample(e) + F IC ,(8)\nwhere F L is the feature maps of the Local branch, downsample is a stride convolution to down-sample F IC , and upsample is a stride deconvolution to upscale the feature maps. The mutual-projected operation guarantees a residual learning while fusing different feature sources, enabling a more discriminative feature learning compared with trivial adding or concatenating.\n\n\nRecurrent Framework\n\nThe repeated SEM cells are embedded into a recurrent framework, as shown in Figure 3. At each iteration i, the hidden unit H i of SEM is directly the fused feature map H, and the output unit L i is the computed by H i going through a two-layer CNN. Note that the initial features L 0 are directly computed by the LR image I LR through only two convolutional layers.\n\nLater on, the extracted deep SR features H i from each iteration i are concatenated together into a wide tensor and mapped to the SR image I SR via one single convolution operation. The network is trained solely with L 1 reconstruction loss.\n\n\nExperiments\n\n\nDatasets and Evaluation Metrics\n\nFollowing [19,38,39], we use 800 images from DIV2K [28] dataset to train our models. For testing, we report the performance on five standard benchmark datasets: Set5 [1], Set14 [34], B100 [21], Urban100 [13] and Manga109 [22]. For evaluation, all the SR results are first transformed into YCbCr space and evaluated by PSNR and SSIM [30] metrics on Y channel only.\n\n\nImplementation details\n\nWe set the recurrence number of SEM as 12 following [20]. For the Cross-Scale Non-Local (CS-NL) attention in SEM, we set patch size p = 3 and stride s = 1 for dense sampling. We use 3 \u00d7 3 as filter size for all convolution layers except for those in attention blocks where the kernel size is 1 \u00d7 1. The filter size for stride convolution and deconvolution in SEM are set to be equal at each scale, e.g., 6 \u00d7 6, 9 \u00d7 9 and 8 \u00d7 8 for scale factor 2, 3, 4, respectively. All intermediate features have channel C = 128 except for those embedded features in attention module, where C = 64. The last convolution layer in SEM has 3 convolution filters that transfer a deep SR feature to an RGB image.\n\nDuring training, we crop 16 images with patch size 48 \u00d7 48 to form a input batch. The training examples are augmented by random rotating 90 \u2022 , 180 \u2022 , 270 \u2022 and horizontal flipping. To optimize our model, we use ADAM optimizer [15] with \u03b2 1 = 0.9, \u03b2 2 = 0.999, and =1e-8. The initial learning rate is set to 1e-4 and reduced to half every 150 epochs. The training stops at 500 epochs. We implement the model using PyTorch, and train it on Nvidia V100 GPUs.\n\n\nComparisons with State-of-the-arts\n\nTo verify the effectiveness of the proposed model, we compare our approach with 11 state-of-the-art methods, which are LapSRN [17], SRMDNF [36], MemNet [27], EDSR [19], DBPN [11], RDN [39], RCAN [37], NLRN [20], SRFBN [18], OISR [12] and SAN [2]. Table 1, We report the quantitative comparisons for scale factor \u00d72, \u00d73 and \u00d74. Compared with other methods, our CS-NL-embedded recurrent model achieved the best performance on multiple benchmarks for almost all scaling factors. It worth noting that our model significantly outperforms NLRN, which is the first proposed in-scale non-local network for image restoration. Our method has better performance when the scaling factor is larger. For \u00d74 settings, our CS-NL embedded model achieves the state-of-the-art PSNR for all the testing benchmarks. In particular, for Urban100 and Manga109 dataset, our model outperforms previous state-of-the-art approaches by 0.4 dB and 0.2 dB, respectively. These datasets contains abundant repeated patterns, such as edges and small corners. Therefore, the superior performance demonstrates the effectiveness of our attention in exploiting internal HR hints. We claim that cross-scale intrinsic priors are indeed effective for a more faithful reconstruction.\n\n\nQuantitative Evaluations In\n\nQualitative Evaluations The qualitative evaluations on Urban100 dataset are shown in Figure 5. The proposed model is proven to be more effective for images with repeated high-frequency features like windows, lines, squares, etc. For example, in the figure of building, LR image contains plenty of window features covering long-range of spatial-frequency. Directly utilizing those cross-scale self-exemplars from the images will be significantly better than searching for in-scale features or external patches in the training set. For all the shown examples, our method perceptually out-performs other state-of-the-arts by a large margin.  which only needs 20% parameters of RCAN and SAN, but achieves the second best result. Therefore, our CSNLN obtains better parameter efficiency in comparison with other methods, by effectively mining internal HR hints.\n\n\nAblation Study\n\nCross-Scale v.s. In-Scale Attention The key difference between our cross-scale non-local attention and the in-scale one is to allow network to benefit from abundant internal HR hints with different scales. To verify it, we visualize its correlation maps on 6 images from Urban100 [13], and compare it with in-scale non-local attention. As shown in Figure 6, these images contain extensive recurrences of small patterns both within scale and across scale. It is interesting to point out that once the image contains repeated edges, such redundant recurrences are not limited to where high scale patterns appear, but also can be found in-place or even in the area that pattern tends to slightly shrink. For example, the HR appearance of a small corner can be simply found by properly zooming out. All these recurrences contain valuable high frequency information for reconstruction. As shown in Figure 6, the in-scale attention only focuses on pixels with similar intensity. In contrast, our cross-scale non-local attention is able to utilize the abundant repeating structures in the images, demonstrating its effectiveness for exploiting internal HR information.\n\n\nSelf-Exemplars Mining Module\n\nTo demonstrate the effectiveness of our proposed Self-Exemplars Mining (SEM) module, we construct a baseline model by removing all branches, resulting in a fully convolutional recurrent network (RNN). To keep the total parameters same as other variants, we set 10 convolution layers in the recurrent block. As shown in Table 3, the basic RNN achieves 33.32 dB on Set14 (\u00d72). Results in first 4 columns demonstrate the effectiveness of individual branch, as each of them brings improvement over the baseline. Furthermore, from last 4 columns, we find that combining these branches achieves the best performance. For example, when cross-scale nonlocal branch is added, the performance is improved from Figure 6. Comparisons of correlation maps of CS-NL attention and IS-NL attention. For each group of three columns, the left one is the input image, the middle one shows the in-scale attention, and the right one depicts the cross-scale attention. one can see that the in-scale attention only focuses on pixels with similar intensity. In contrast, our cross-scale non-local attention is able to utilize the abundant repeated structures in the images, demonstrating its effectiveness for exploiting internal HR information.  33.47 dB to 33.64 dB. When both local branch and nonlocal branch are added to the network, the best performance is achieved by further adding cross-scale non-local branch, resulting in a improvement from 33.62 dB to 33.74 dB.\n\nThese facts indicate that the cross-scale correlations learned by our attention can not be captured by either simple convolution or previous in-scale attention module, demonstrating that our CS-NL attention is of crucial importance for fully exploiting information from LR images.\n\nPatch-Based Matching v.s. Pixel-Based Matching In practical implementation, we compute patch-wise correlation rather than pixel-wise correlation. Here we investigate the influence of patch size p in CS-NL attention. We compare the patch size of 1 \u00d7 1, 3 \u00d7 3 and 5 \u00d7 5, where 1 \u00d7 1 is equivalent to pixel-wise matching. As shown in Table 4, the performance peak is at 3 \u00d7 3, which is higher than pixel based matching, indicating that a small patch can serve as a better region descriptor. However, when using a larger patch size, the performance is worse than the pixel-based matching. This is mainly because larger patches mean additional constraint on the content when evaluating similarity, and therefore it becomes harder to find well-matched cor-respondences. All these results show that it is necessary to choose a proper patch size for effectively computing correlations in CS-NL attention.\n\n\nMutual-Projected Fusion\n\nWe show the effectiveness of our mutual-projected fusion by comparing it with other commonly used fusion strategies, e.g., feature addition and concatenation. As shown in Table 5, it can be found that our projection based fusion obtains the best result. By replacing the addition and concatenation with mutual projection, the performance improves about 0.05 dB and 0.12 dB. These results demonstrate the effectiveness of our fusion module in progressively aggregating information.\n\n\nConclusion\n\nIn this paper, we proposed the first Cross-Scale Non-Local (CS-NL) attention module for image super-resolution deep networks. With the novel module, we are able to sufficiently discover the widely existing cross-scale feature similarities in natural images. Further integrating it with local and the previous in-scale non-local priors, while embracing the abundant external information learned by the network, our recurrent model achieved state-of-the-art performance for multiple benchmarks. Our experiments suggest that exploring cross-scale long-range dependencies will greatly benefit single image super-resolution (SISR) task, and possibly is also promising for general image restoration task.\n\nFigure 1 .\n1Visualization of most engaged patches captured by our Cross-Scale Non-Local (CS-NL) attention. Cross-scale similarities widely exist in natural images. Multiple high-resolution (HR) patches from the low-resolution (LR) image itself significantly improve target patch super-resolution.\n\nFigure 2 .\n2The proposed Cross-Scale Non-Local (CS-NL) attention module. The bottom green box is for patch-level cross-scale similarity-matching. The upper branch shows extracting the original HR patches in LR image.\n\nFigure 3 .\n3The recurrent architecture with the proposed Self-Exemplars Mining (SEM) cell. Inside SEM, it fuses features learned from a proposed Cross-Scale Non-Local (CS-NL) attention, with others from the In-Scale Non-Local (IS-NL) and the local paths.\n\nFigure 4 .\n4Mutual-projected fusion. Downsample and upsample operations are implemented using stride convolution and stride deconvolution, respectively.\n\nFigure 5 .\n5Visual comparison for 4\u00d7 SR on Urban100 dataset. For all the shown examples, especially the images with repeated edges or structures, our method perceptually out-performs other state-of-the-arts by a large margin.\n\nTable 1 .\n1Quantitative results on benchmark datasets. Best and second best results are colored with red and blue.Method \nScale \nSet5 \nSet14 \nB100 \nUrban100 \nManga109 \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \nPSNR \nSSIM \n\nLapSRN [17] \n\u00d72 \n37.52 \n0.9591 \n33.08 \n0.9130 \n31.08 \n0.8950 \n30.41 \n0.9101 \n37.27 \n0.9740 \nMemNet [27] \n\u00d72 \n37.78 \n0.9597 \n33.28 \n0.9142 \n32.08 \n0.8978 \n31.31 \n0.9195 \n37.72 \n0.9740 \nEDSR [19] \n\u00d72 \n38.11 \n0.9602 \n33.92 \n0.9195 \n32.32 \n0.9013 \n32.93 \n0.9351 \n39.10 \n0.9773 \nSRMDNF [36] \n\u00d72 \n37.79 \n0.9601 \n33.32 \n0.9159 \n32.05 \n0.8985 \n31.33 \n0.9204 \n38.07 \n0.9761 \nDBPN [11] \n\u00d72 \n38.09 \n0.9600 \n33.85 \n0.9190 \n32.27 \n0.9000 \n32.55 \n0.9324 \n38.89 \n0.9775 \nRDN [39] \n\u00d72 \n38.24 \n0.9614 \n34.01 \n0.9212 \n32.34 \n0.9017 \n32.89 \n0.9353 \n39.18 \n0.9780 \nRCAN [37] \n\u00d72 \n38.27 \n0.9614 \n34.12 \n0.9216 \n32.41 \n0.9027 \n33.34 \n0.9384 \n39.44 \n0.9786 \nNLRN [20] \n\u00d72 \n38.00 \n0.9603 \n33.46 \n0.9159 \n32.19 \n0.8992 \n31.81 \n0.9249 \n-\n-\nSRFBN [18] \n\u00d72 \n38.11 \n0.9609 \n33.82 \n0.9196 \n32.29 \n0.9010 \n32.62 \n0.9328 \n39.08 \n0.9779 \nOISR [12] \n\u00d72 \n38.21 \n0.9612 \n33.94 \n0.9206 \n32.36 \n0.9019 \n33.03 \n0.9365 \n-\n-\nSAN [2] \n\u00d72 \n38.31 \n0.9620 \n34.07 \n0.9213 \n32.42 \n0.9028 \n33.10 \n0.9370 \n39.32 \n0.9792 \nCSNLN (ours) \n\u00d72 \n38.28 \n0.9616 \n34.12 \n0.9223 \n32.40 \n0.9024 \n33.25 \n0.9386 \n39.37 \n0.9785 \n\nLapSRN [17] \n\u00d73 \n33.82 \n0.9227 \n29.87 \n0.8320 \n28.82 \n0.7980 \n27.07 \n0.8280 \n32.21 \n0.9350 \nMemNet [27] \n\u00d73 \n34.09 \n0.9248 \n30.00 \n0.8350 \n28.96 \n0.8001 \n27.56 \n0.8376 \n32.51 \n0.9369 \nEDSR [19] \n\u00d73 \n34.65 \n0.9280 \n30.52 \n0.8462 \n29.25 \n0.8093 \n28.80 \n0.8653 \n34.17 \n0.9476 \nSRMDNF [36] \n\u00d73 \n34.12 \n0.9254 \n30.04 \n0.8382 \n28.97 \n0.8025 \n27.57 \n0.8398 \n33.00 \n0.9403 \nRDN [39] \n\u00d73 \n34.71 \n0.9296 \n30.57 \n0.8468 \n29.26 \n0.8093 \n28.80 \n0.8653 \n34.13 \n0.9484 \nRCAN [37] \n\u00d73 \n34.74 \n0.9299 \n30.65 \n0.8482 \n29.32 \n0.8111 \n29.09 \n0.8702 \n34.44 \n0.9499 \nNLRN [20] \n\u00d73 \n34.27 \n0.9266 \n30.16 \n0.8374 \n29.06 \n0.8026 \n27.93 \n0.8453 \n-\n-\nSRFBN [18] \n\u00d73 \n34.70 \n0.9292 \n30.51 \n0.8461 \n29.24 \n0.8084 \n28.73 \n0.8641 \n34.18 \n0.9481 \nOISR [12] \n\u00d73 \n34.72 \n0.9297 \n30.57 \n0.8470 \n29.29 \n0.8103 \n28.95 \n0.8680 \n-\n-\nSAN [2] \n\u00d73 \n34.75 \n0.9300 \n30.59 \n0.8476 \n29.33 \n0.8112 \n28.93 \n0.8671 \n34.30 \n0.9494 \nCSNLN (ours) \n\u00d73 \n34.74 \n0.9300 \n30.66 \n0.8482 \n29.33 \n0.8105 \n29.13 \n0.8712 \n34.45 \n0.9502 \n\nLapSRN [17] \n\u00d74 \n31.54 \n0.8850 \n28.19 \n0.7720 \n27.32 \n0.7270 \n25.21 \n0.7560 \n29.09 \n0.8900 \nMemNet [27] \n\u00d74 \n31.74 \n0.8893 \n28.26 \n0.7723 \n27.40 \n0.7281 \n25.50 \n0.7630 \n29.42 \n0.8942 \nEDSR [19] \n\u00d74 \n32.46 \n0.8968 \n28.80 \n0.7876 \n27.71 \n0.7420 \n26.64 \n0.8033 \n31.02 \n0.9148 \nSRMDNF [36] \n\u00d74 \n31.96 \n0.8925 \n28.35 \n0.7787 \n27.49 \n0.7337 \n25.68 \n0.7731 \n30.09 \n0.9024 \nDBPN [11] \n\u00d74 \n32.47 \n0.8980 \n28.82 \n0.7860 \n27.72 \n0.7400 \n26.38 \n0.7946 \n30.91 \n0.9137 \nRDN [39] \n\u00d74 \n32.47 \n0.8990 \n28.81 \n0.7871 \n27.72 \n0.7419 \n26.61 \n0.8028 \n31.00 \n0.9151 \nRCAN [37] \n\u00d74 \n32.63 \n0.9002 \n28.87 \n0.7889 \n27.77 \n0.7436 \n26.82 \n0.8087 \n31.22 \n0.9173 \nNLRN [20] \n\u00d74 \n31.92 \n0.8916 \n28.36 \n0.7745 \n27.48 \n0.7306 \n25.79 \n0.7729 \n-\n-\nSRFBN [18] \n\u00d74 \n32.47 \n0.8983 \n28.81 \n0.7868 \n27.72 \n0.7409 \n26.60 \n0.8015 \n31.15 \n0.9160 \nOISR [12] \n\u00d74 \n32.53 \n0.8992 \n28.86 \n0.7878 \n27.75 \n0.7428 \n26.79 \n0.8068 \n-\n-\nSAN [2] \n\u00d74 \n32.64 \n0.9003 \n28.92 \n0.7888 \n27.78 \n0.7436 \n26.79 \n0.8068 \n31.18 \n0.9169 \nCSNLN (ours) \n\u00d74 \n32.68 \n0.9004 \n28.95 \n0.7888 \n27.80 \n0.7439 \n27.22 \n0.8168 \n31.43 \n0.9201 \n\n\n\n\nLocal (L) branchIn-Scale Non-Local (IS-NL) Branch Cross-Scale Non-Local (CS-NL) branchTable 3. Ablation study on the branch features in SEM. We report the PSNR results on Set14 (2\u00d7) after 200 epochs. With an additional CS-NL branch, the performance becomes 33.74dB compared with the one without CS-NL, 33.62dB.Table 4. Effects of patch size for matching.Table 5. Comparison of fusion operators.PSNR \n33.32 \n33.47 \n33.52 \n33.51 \n33.62 \n33.64 \n33.57 \n33.74 \n\nAttention Patch Size \n1\u00d71 \n3\u00d73 \n5\u00d75 \nPSNR \n33.67 \n33.74 \n33.61 \n\nFusion \naddition \nconcatenation \nMutual \nProjection \nPSNR \n33.69 \n33.62 \n33.74 \n\n\nAcknowledgments This work is in part supported by IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network.\nChristine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. Marco Bevilacqua, Aline Roumy, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceMarco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In Proceedings of the British Machine Vision Conference, 2012. 5\n\nSecond-order attention network for single image super-resolution. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition67Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single im- age super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11065- 11074, 2019. 2, 3, 4, 5, 6, 7\n\nDiscrete wavelet transform-based satellite image resolution enhancement. Hasan Demirel, Gholamreza Anbarjafari, IEEE transactions on geoscience and remote sensing. 49Hasan Demirel and Gholamreza Anbarjafari. Discrete wavelet transform-based satellite image resolution enhance- ment. IEEE transactions on geoscience and remote sensing, 49(6):1997-2004, 2011. 1\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, European conference on computer vision. Springer1Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In European conference on computer vi- sion, pages 184-199. Springer, 2014. 1, 2\n\nBalanced two-stage residual networks for image super-resolution. Yuchen Fan, Honghui Shi, Jiahui Yu, Ding Liu, Wei Han, Haichao Yu, Zhangyang Wang, Xinchao Wang, Thomas S Huang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsYuchen Fan, Honghui Shi, Jiahui Yu, Ding Liu, Wei Han, Haichao Yu, Zhangyang Wang, Xinchao Wang, and Thomas S Huang. Balanced two-stage residual networks for image super-resolution. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition Work- shops, pages 161-168, 2017. 2\n\nScale-wise convolution for image restoration. Yuchen Fan, Jiahui Yu, Ding Liu, Thomas S Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020Yuchen Fan, Jiahui Yu, Ding Liu, and Thomas S Huang. Scale-wise convolution for image restoration. In Proceed- ings of the AAAI Conference on Artificial Intelligence, 2020. 2\n\nImage and video upscaling from local self-examples. Gilad Freedman, Raanan Fattal, ACM Transactions on Graphics (TOG). 30212Gilad Freedman and Raanan Fattal. Image and video upscal- ing from local self-examples. ACM Transactions on Graph- ics (TOG), 30(2):12, 2011. 2\n\nExample-based super-resolution. William T Freeman, R Thouis, Egon C Jones, Pasztor, IEEE Computer graphics and Applications. 222William T Freeman, Thouis R Jones, and Egon C Pasztor. Example-based super-resolution. IEEE Computer graphics and Applications, 22(2):56-65, 2002. 2\n\nSuperresolution from a single image. Daniel Glasner, Shai Bagon, Michal Irani, Proceedings of the IEEE 12th International Conference on Computer Vision. the IEEE 12th International Conference on Computer Vision1Daniel Glasner, Shai Bagon, and Michal Irani. Super- resolution from a single image. In Proceedings of the IEEE 12th International Conference on Computer Vision, pages 349-356, 2009. 1, 2\n\nDeep learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT pressIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. 2\n\nDeep back-projection networks for super-resolution. Muhammad Haris, Gregory Shakhnarovich, Norimichi Ukita, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition67Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks for super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1664-1673, 2018. 5, 6, 7\n\nOde-inspired network design for single image super-resolution. Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu, Mingyuan Yang, Jian Cheng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition67Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu, Mingyuan Yang, and Jian Cheng. Ode-inspired network design for sin- gle image super-resolution. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 1732-1741, 2019. 5, 6, 7\n\nSingle image super-resolution from transformed self-exemplars. Jia-Bin Huang, Abhishek Singh, Narendra Ahuja, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition57Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Sin- gle image super-resolution from transformed self-exemplars. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5197-5206, 2015. 2, 5, 7\n\nAccurate image super-resolution using very deep convolutional networks. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1646-1654, 2016. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097-1105, 2012. 2\n\nDeep laplacian pyramid networks for fast and accurate super-resolution. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition67Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming- Hsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 624-632, 2017. 5, 6, 7\n\nFeedback network for image superresolution. Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, Wei Wu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition56Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwang- gil Jeon, and Wei Wu. Feedback network for image super- resolution. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 3867-3876, 2019. 5, 6\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshops67Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition workshops, pages 136-144, 2017. 2, 4, 5, 6, 7\n\nNon-local recurrent network for image restoration. Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S Huang, Advances in Neural Information Processing Systems. 56Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network for image restoration. In Advances in Neural Information Processing Systems, pages 1673-1682, 2018. 2, 3, 4, 5, 6\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. D Martin, D Fowlkes, J Tal, Malik, Proceedings Eighth IEEE International Conference on Computer Vision. ICCV. Eighth IEEE International Conference on Computer Vision. ICCVIEEE2D Martin, C Fowlkes, D Tal, and J Malik. A database of hu- man segmented natural images and its application to evaluat- ing segmentation algorithms and measuring ecological statis- tics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416-423. IEEE, 2001. 5\n\nSketch-based manga retrieval using manga109 dataset. Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, Kiyoharu Aizawa, Multimedia Tools and Applications. 76Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Mul- timedia Tools and Applications, 76(20):21811-21838, 2017. 5\n\nNonparametric blind super-resolution. Tomer Michaeli, Michal Irani, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionTomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In Proceedings of the IEEE International Conference on Computer Vision, pages 945-952, 2013. 2\n\nGeneralizing the nonlocal-means to superresolution reconstruction. Matan Protter, Michael Elad, Hiroyuki Takeda, Peyman Milanfar, IEEE Transactions on image processing. 181Matan Protter, Michael Elad, Hiroyuki Takeda, and Pey- man Milanfar. Generalizing the nonlocal-means to super- resolution reconstruction. IEEE Transactions on image pro- cessing, 18(1):36-51, 2008. 1\n\nzero-shot super-resolution using deep internal learning. Assaf Shocher, Nadav Cohen, Michal Irani, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAssaf Shocher, Nadav Cohen, and Michal Irani. zero-shot super-resolution using deep internal learning. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3118-3126, 2018. 2\n\nSuper-resolution using sub-band self-similarity. Abhishek Singh, Narendra Ahuja, Asian Conference on Computer Vision. SpringerAbhishek Singh and Narendra Ahuja. Super-resolution using sub-band self-similarity. In Asian Conference on Computer Vision, pages 552-568. Springer, 2014. 2\n\nMemnet: A persistent memory network for image restoration. Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision56Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem- net: A persistent memory network for image restoration. In Proceedings of the IEEE international conference on com- puter vision, pages 4539-4547, 2017. 5, 6\n\nNtire 2017 challenge on single image super-resolution: Methods and results. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsRadu Timofte, Eirikur Agustsson, Luc Van Gool, Ming- Hsuan Yang, and Lei Zhang. Ntire 2017 challenge on single image super-resolution: Methods and results. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 114-125, 2017. 5\n\nNon-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 7794-7803, 2018. 3\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Sheikh, P Eero, Simoncelli, IEEE transactions on image processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simon- celli, et al. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. 5\n\nFast image superresolution based on in-place example regression. Jianchao Yang, Zhe Lin, Scott Cohen, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJianchao Yang, Zhe Lin, and Scott Cohen. Fast image super- resolution based on in-place example regression. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 1059-1066, 2013. 2\n\nComputed tomography super-resolution using convolutional neural networks. Haichao Yu, Ding Liu, Honghui Shi, Hanchao Yu, Zhangyang Wang, Xinchao Wang, Brent Cross, Matthew Bramler, Thomas S Huang, 2017 IEEE International Conference on Image Processing (ICIP). IEEEHaichao Yu, Ding Liu, Honghui Shi, Hanchao Yu, Zhangyang Wang, Xinchao Wang, Brent Cross, Matthew Bramler, and Thomas S Huang. Computed tomography super-resolution using convolutional neural networks. In 2017 IEEE International Conference on Image Processing (ICIP), pages 3944-3948. IEEE, 2017. 1\n\nWide activation for efficient image and video super-resolution. Jiahui Yu, Yuchen Fan, Thomas Huang, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceJiahui Yu, Yuchen Fan, and Thomas Huang. Wide activation for efficient image and video super-resolution. In Proceed- ings of the British Machine Vision Conference, 2019. 2\n\nOn single image scale-up using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, International conference on curves and surfaces. SpringerRoman Zeyde, Michael Elad, and Matan Protter. On sin- gle image scale-up using sparse-representations. In Interna- tional conference on curves and surfaces, pages 711-730. Springer, 2010. 5\n\nSingle image super-resolution with non-local means and steering kernel regression. Kaibing Zhang, Xinbo Gao, Dacheng Tao, Xuelong Li, IEEE Transactions on Image Processing. 2111Kaibing Zhang, Xinbo Gao, Dacheng Tao, and Xuelong Li. Single image super-resolution with non-local means and steering kernel regression. IEEE Transactions on Image Pro- cessing, 21(11):4544-4556, 2012. 1\n\nLearning a single convolutional super-resolution network for multiple degradations. Kai Zhang, Wangmeng Zuo, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition56Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a single convolutional super-resolution network for multi- ple degradations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3262- 3271, 2018. 5, 6\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)67Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 286-301, 2018. 2, 3, 5, 6, 7\n\nResidual non-local attention networks for image restoration. Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, Yun Fu, arXiv:1903.100825arXiv preprintYulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local attention networks for image restora- tion. arXiv preprint arXiv:1903.10082, 2019. 2, 4, 5\n\nResidual dense network for image super-resolution. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition67Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2472-2481, 2018. 2, 4, 5, 6, 7\n\nSurvey of face detection on low-quality images. Yuqian Zhou, Ding Liu, Thomas Huang, 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018). IEEEYuqian Zhou, Ding Liu, and Thomas Huang. Survey of face detection on low-quality images. In 2018 13th IEEE Interna- tional Conference on Automatic Face & Gesture Recognition (FG 2018), pages 769-773. IEEE, 2018. 1\n\nImage restoration for under-display camera. Yuqian Zhou, David Ren, Neil Emerton, Sehoon Lim, Timothy Large, arXiv:2003.04857arXiv preprintYuqian Zhou, David Ren, Neil Emerton, Sehoon Lim, and Timothy Large. Image restoration for under-display camera. arXiv preprint arXiv:2003.04857, 2020. 1\n\nInternal statistics of a single natural image. Maria Zontak, Michal Irani, CVPR 2011. IEEE1Maria Zontak and Michal Irani. Internal statistics of a single natural image. In CVPR 2011, pages 977-984. IEEE, 2011. 1, 2\n\nVery low resolution face recognition problem. W W Wilman, Zou, C Pong, Yuen, IEEE Transactions on image processing. 211Wilman WW Zou and Pong C Yuen. Very low resolution face recognition problem. IEEE Transactions on image pro- cessing, 21(1):327-340, 2011. 1\n", "annotations": {"author": "[{\"end\":127,\"start\":100},{\"end\":156,\"start\":128},{\"end\":186,\"start\":157},{\"end\":219,\"start\":187},{\"end\":252,\"start\":220},{\"end\":306,\"start\":253},{\"end\":127,\"start\":100},{\"end\":156,\"start\":128},{\"end\":186,\"start\":157},{\"end\":219,\"start\":187},{\"end\":252,\"start\":220},{\"end\":306,\"start\":253}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":106},{\"end\":138,\"start\":135},{\"end\":168,\"start\":164},{\"end\":199,\"start\":194},{\"end\":234,\"start\":229},{\"end\":265,\"start\":262},{\"end\":109,\"start\":106},{\"end\":138,\"start\":135},{\"end\":168,\"start\":164},{\"end\":199,\"start\":194},{\"end\":234,\"start\":229},{\"end\":265,\"start\":262}]", "author_first_name": "[{\"end\":105,\"start\":100},{\"end\":134,\"start\":128},{\"end\":163,\"start\":157},{\"end\":193,\"start\":187},{\"end\":226,\"start\":220},{\"end\":228,\"start\":227},{\"end\":261,\"start\":253},{\"end\":105,\"start\":100},{\"end\":134,\"start\":128},{\"end\":163,\"start\":157},{\"end\":193,\"start\":187},{\"end\":226,\"start\":220},{\"end\":228,\"start\":227},{\"end\":261,\"start\":253}]", "author_affiliation": "[{\"end\":126,\"start\":111},{\"end\":155,\"start\":140},{\"end\":185,\"start\":170},{\"end\":218,\"start\":201},{\"end\":251,\"start\":236},{\"end\":282,\"start\":267},{\"end\":305,\"start\":284},{\"end\":126,\"start\":111},{\"end\":155,\"start\":140},{\"end\":185,\"start\":170},{\"end\":218,\"start\":201},{\"end\":251,\"start\":236},{\"end\":282,\"start\":267},{\"end\":305,\"start\":284}]", "title": "[{\"end\":97,\"start\":1},{\"end\":403,\"start\":307},{\"end\":97,\"start\":1},{\"end\":403,\"start\":307}]", "venue": null, "abstract": "[{\"end\":1656,\"start\":481},{\"end\":1656,\"start\":481}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1953,\"start\":1950},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1956,\"start\":1953},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1959,\"start\":1956},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1962,\"start\":1959},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1965,\"start\":1962},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2630,\"start\":2626},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2633,\"start\":2630},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2877,\"start\":2874},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2880,\"start\":2877},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3226,\"start\":3223},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3496,\"start\":3492},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3617,\"start\":3614},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3620,\"start\":3617},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3623,\"start\":3620},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3626,\"start\":3623},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3629,\"start\":3626},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3632,\"start\":3629},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3634,\"start\":3632},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3989,\"start\":3986},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3992,\"start\":3989},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3995,\"start\":3992},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4322,\"start\":4318},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6570,\"start\":6567},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6573,\"start\":6570},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6859,\"start\":6856},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6861,\"start\":6859},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6863,\"start\":6861},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6866,\"start\":6863},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6869,\"start\":6866},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6872,\"start\":6869},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6919,\"start\":6916},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7287,\"start\":7283},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7443,\"start\":7439},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7618,\"start\":7614},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7621,\"start\":7618},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7719,\"start\":7716},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7888,\"start\":7884},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8061,\"start\":8057},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8242,\"start\":8238},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8490,\"start\":8487},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8493,\"start\":8490},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8496,\"start\":8493},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8499,\"start\":8496},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8935,\"start\":8931},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9210,\"start\":9206},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9221,\"start\":9217},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9233,\"start\":9230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15611,\"start\":15608},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15729,\"start\":15726},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16236,\"start\":16232},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16239,\"start\":16236},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16242,\"start\":16239},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16245,\"start\":16242},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17091,\"start\":17087},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17239,\"start\":17235},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18410,\"start\":18406},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18413,\"start\":18410},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18416,\"start\":18413},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18451,\"start\":18447},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18565,\"start\":18562},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18577,\"start\":18573},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18588,\"start\":18584},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18603,\"start\":18599},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18621,\"start\":18617},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18732,\"start\":18728},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18842,\"start\":18838},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19712,\"start\":19708},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20106,\"start\":20102},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20119,\"start\":20115},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20132,\"start\":20128},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20143,\"start\":20139},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20154,\"start\":20150},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20164,\"start\":20160},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20175,\"start\":20171},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20186,\"start\":20182},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20198,\"start\":20194},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20209,\"start\":20205},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20221,\"start\":20218},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22408,\"start\":22404},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24542,\"start\":24540},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1953,\"start\":1950},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1956,\"start\":1953},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1959,\"start\":1956},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1962,\"start\":1959},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1965,\"start\":1962},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2630,\"start\":2626},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2633,\"start\":2630},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2877,\"start\":2874},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2880,\"start\":2877},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3226,\"start\":3223},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3496,\"start\":3492},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3617,\"start\":3614},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3620,\"start\":3617},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3623,\"start\":3620},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3626,\"start\":3623},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3629,\"start\":3626},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3632,\"start\":3629},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3634,\"start\":3632},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3989,\"start\":3986},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3992,\"start\":3989},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3995,\"start\":3992},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4322,\"start\":4318},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6570,\"start\":6567},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6573,\"start\":6570},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6859,\"start\":6856},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6861,\"start\":6859},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6863,\"start\":6861},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6866,\"start\":6863},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6869,\"start\":6866},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6872,\"start\":6869},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6919,\"start\":6916},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7287,\"start\":7283},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7443,\"start\":7439},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7618,\"start\":7614},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7621,\"start\":7618},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7719,\"start\":7716},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7888,\"start\":7884},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8061,\"start\":8057},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8242,\"start\":8238},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8490,\"start\":8487},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8493,\"start\":8490},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8496,\"start\":8493},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8499,\"start\":8496},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8935,\"start\":8931},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9210,\"start\":9206},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9221,\"start\":9217},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9233,\"start\":9230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15611,\"start\":15608},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15729,\"start\":15726},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16236,\"start\":16232},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16239,\"start\":16236},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16242,\"start\":16239},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16245,\"start\":16242},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17091,\"start\":17087},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17239,\"start\":17235},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18410,\"start\":18406},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18413,\"start\":18410},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18416,\"start\":18413},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18451,\"start\":18447},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18565,\"start\":18562},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18577,\"start\":18573},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18588,\"start\":18584},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18603,\"start\":18599},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18621,\"start\":18617},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18732,\"start\":18728},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18842,\"start\":18838},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19712,\"start\":19708},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20106,\"start\":20102},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20119,\"start\":20115},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20132,\"start\":20128},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20143,\"start\":20139},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20154,\"start\":20150},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20164,\"start\":20160},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20175,\"start\":20171},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20186,\"start\":20182},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20198,\"start\":20194},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20209,\"start\":20205},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20221,\"start\":20218},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22408,\"start\":22404},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24542,\"start\":24540}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27464,\"start\":27167},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27682,\"start\":27465},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27938,\"start\":27683},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28092,\"start\":27939},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28319,\"start\":28093},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31658,\"start\":28320},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32264,\"start\":31659},{\"attributes\":{\"id\":\"fig_0\"},\"end\":27464,\"start\":27167},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27682,\"start\":27465},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27938,\"start\":27683},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28092,\"start\":27939},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28319,\"start\":28093},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31658,\"start\":28320},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32264,\"start\":31659}]", "paragraph": "[{\"end\":2173,\"start\":1672},{\"end\":2699,\"start\":2175},{\"end\":3178,\"start\":2701},{\"end\":4116,\"start\":3180},{\"end\":4743,\"start\":4118},{\"end\":5457,\"start\":4745},{\"end\":5523,\"start\":5459},{\"end\":5867,\"start\":5525},{\"end\":6196,\"start\":5869},{\"end\":6397,\"start\":6198},{\"end\":7622,\"start\":6415},{\"end\":8643,\"start\":7624},{\"end\":9900,\"start\":8684},{\"end\":10081,\"start\":9944},{\"end\":10296,\"start\":10083},{\"end\":10554,\"start\":10374},{\"end\":10717,\"start\":10601},{\"end\":11237,\"start\":10761},{\"end\":11732,\"start\":11239},{\"end\":12072,\"start\":11734},{\"end\":12527,\"start\":12162},{\"end\":13200,\"start\":12543},{\"end\":13250,\"start\":13229},{\"end\":13472,\"start\":13278},{\"end\":13894,\"start\":13687},{\"end\":14949,\"start\":13910},{\"end\":15406,\"start\":15011},{\"end\":15883,\"start\":15408},{\"end\":16395,\"start\":15885},{\"end\":16664,\"start\":16397},{\"end\":17031,\"start\":16716},{\"end\":17282,\"start\":17033},{\"end\":17714,\"start\":17342},{\"end\":18103,\"start\":17738},{\"end\":18346,\"start\":18105},{\"end\":18759,\"start\":18396},{\"end\":19478,\"start\":18786},{\"end\":19937,\"start\":19480},{\"end\":21217,\"start\":19976},{\"end\":22105,\"start\":21249},{\"end\":23285,\"start\":22124},{\"end\":24765,\"start\":23318},{\"end\":25047,\"start\":24767},{\"end\":25945,\"start\":25049},{\"end\":26453,\"start\":25973},{\"end\":27166,\"start\":26468},{\"end\":2173,\"start\":1672},{\"end\":2699,\"start\":2175},{\"end\":3178,\"start\":2701},{\"end\":4116,\"start\":3180},{\"end\":4743,\"start\":4118},{\"end\":5457,\"start\":4745},{\"end\":5523,\"start\":5459},{\"end\":5867,\"start\":5525},{\"end\":6196,\"start\":5869},{\"end\":6397,\"start\":6198},{\"end\":7622,\"start\":6415},{\"end\":8643,\"start\":7624},{\"end\":9900,\"start\":8684},{\"end\":10081,\"start\":9944},{\"end\":10296,\"start\":10083},{\"end\":10554,\"start\":10374},{\"end\":10717,\"start\":10601},{\"end\":11237,\"start\":10761},{\"end\":11732,\"start\":11239},{\"end\":12072,\"start\":11734},{\"end\":12527,\"start\":12162},{\"end\":13200,\"start\":12543},{\"end\":13250,\"start\":13229},{\"end\":13472,\"start\":13278},{\"end\":13894,\"start\":13687},{\"end\":14949,\"start\":13910},{\"end\":15406,\"start\":15011},{\"end\":15883,\"start\":15408},{\"end\":16395,\"start\":15885},{\"end\":16664,\"start\":16397},{\"end\":17031,\"start\":16716},{\"end\":17282,\"start\":17033},{\"end\":17714,\"start\":17342},{\"end\":18103,\"start\":17738},{\"end\":18346,\"start\":18105},{\"end\":18759,\"start\":18396},{\"end\":19478,\"start\":18786},{\"end\":19937,\"start\":19480},{\"end\":21217,\"start\":19976},{\"end\":22105,\"start\":21249},{\"end\":23285,\"start\":22124},{\"end\":24765,\"start\":23318},{\"end\":25047,\"start\":24767},{\"end\":25945,\"start\":25049},{\"end\":26453,\"start\":25973},{\"end\":27166,\"start\":26468}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10373,\"start\":10297},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10600,\"start\":10555},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12161,\"start\":12073},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13581,\"start\":13473},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13686,\"start\":13581},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16686,\"start\":16665},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16715,\"start\":16686},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17314,\"start\":17283},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17341,\"start\":17314},{\"attributes\":{\"id\":\"formula_0\"},\"end\":10373,\"start\":10297},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10600,\"start\":10555},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12161,\"start\":12073},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13581,\"start\":13473},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13686,\"start\":13581},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16686,\"start\":16665},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16715,\"start\":16686},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17314,\"start\":17283},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17341,\"start\":17314}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20230,\"start\":20223},{\"end\":23644,\"start\":23637},{\"end\":25387,\"start\":25380},{\"end\":26151,\"start\":26144},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20230,\"start\":20223},{\"end\":23644,\"start\":23637},{\"end\":25387,\"start\":25380},{\"end\":26151,\"start\":26144}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1670,\"start\":1658},{\"attributes\":{\"n\":\"2.\"},\"end\":6413,\"start\":6400},{\"end\":8682,\"start\":8646},{\"attributes\":{\"n\":\"3.\"},\"end\":9942,\"start\":9903},{\"end\":10759,\"start\":10720},{\"end\":12541,\"start\":12530},{\"end\":13227,\"start\":13203},{\"end\":13276,\"start\":13253},{\"attributes\":{\"n\":\"4.\"},\"end\":13908,\"start\":13897},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14974,\"start\":14952},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15009,\"start\":14977},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17736,\"start\":17717},{\"attributes\":{\"n\":\"5.\"},\"end\":18360,\"start\":18349},{\"attributes\":{\"n\":\"5.1.\"},\"end\":18394,\"start\":18363},{\"attributes\":{\"n\":\"5.2.\"},\"end\":18784,\"start\":18762},{\"attributes\":{\"n\":\"5.3.\"},\"end\":19974,\"start\":19940},{\"end\":21247,\"start\":21220},{\"attributes\":{\"n\":\"5.4.\"},\"end\":22122,\"start\":22108},{\"end\":23316,\"start\":23288},{\"end\":25971,\"start\":25948},{\"attributes\":{\"n\":\"6.\"},\"end\":26466,\"start\":26456},{\"end\":27178,\"start\":27168},{\"end\":27476,\"start\":27466},{\"end\":27694,\"start\":27684},{\"end\":27950,\"start\":27940},{\"end\":28104,\"start\":28094},{\"end\":28330,\"start\":28321},{\"attributes\":{\"n\":\"1.\"},\"end\":1670,\"start\":1658},{\"attributes\":{\"n\":\"2.\"},\"end\":6413,\"start\":6400},{\"end\":8682,\"start\":8646},{\"attributes\":{\"n\":\"3.\"},\"end\":9942,\"start\":9903},{\"end\":10759,\"start\":10720},{\"end\":12541,\"start\":12530},{\"end\":13227,\"start\":13203},{\"end\":13276,\"start\":13253},{\"attributes\":{\"n\":\"4.\"},\"end\":13908,\"start\":13897},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14974,\"start\":14952},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15009,\"start\":14977},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17736,\"start\":17717},{\"attributes\":{\"n\":\"5.\"},\"end\":18360,\"start\":18349},{\"attributes\":{\"n\":\"5.1.\"},\"end\":18394,\"start\":18363},{\"attributes\":{\"n\":\"5.2.\"},\"end\":18784,\"start\":18762},{\"attributes\":{\"n\":\"5.3.\"},\"end\":19974,\"start\":19940},{\"end\":21247,\"start\":21220},{\"attributes\":{\"n\":\"5.4.\"},\"end\":22122,\"start\":22108},{\"end\":23316,\"start\":23288},{\"end\":25971,\"start\":25948},{\"attributes\":{\"n\":\"6.\"},\"end\":26466,\"start\":26456},{\"end\":27178,\"start\":27168},{\"end\":27476,\"start\":27466},{\"end\":27694,\"start\":27684},{\"end\":27950,\"start\":27940},{\"end\":28104,\"start\":28094},{\"end\":28330,\"start\":28321}]", "table": "[{\"end\":31658,\"start\":28435},{\"end\":32264,\"start\":32055},{\"end\":31658,\"start\":28435},{\"end\":32264,\"start\":32055}]", "figure_caption": "[{\"end\":27464,\"start\":27180},{\"end\":27682,\"start\":27478},{\"end\":27938,\"start\":27696},{\"end\":28092,\"start\":27952},{\"end\":28319,\"start\":28106},{\"end\":28435,\"start\":28332},{\"end\":32055,\"start\":31661},{\"end\":27464,\"start\":27180},{\"end\":27682,\"start\":27478},{\"end\":27938,\"start\":27696},{\"end\":28092,\"start\":27952},{\"end\":28319,\"start\":28106},{\"end\":28435,\"start\":28332},{\"end\":32055,\"start\":31661}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5141,\"start\":5133},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10852,\"start\":10844},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13965,\"start\":13956},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14241,\"start\":14233},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15437,\"start\":15429},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16394,\"start\":16386},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17822,\"start\":17814},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21342,\"start\":21334},{\"end\":22480,\"start\":22472},{\"end\":23025,\"start\":23017},{\"end\":24026,\"start\":24018},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5141,\"start\":5133},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10852,\"start\":10844},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13965,\"start\":13956},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14241,\"start\":14233},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15437,\"start\":15429},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16394,\"start\":16386},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17822,\"start\":17814},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21342,\"start\":21334},{\"end\":22480,\"start\":22472},{\"end\":23025,\"start\":23017},{\"end\":24026,\"start\":24018}]", "bib_author_first_name": "[{\"end\":32590,\"start\":32585},{\"end\":32608,\"start\":32603},{\"end\":33009,\"start\":33006},{\"end\":33022,\"start\":33015},{\"end\":33036,\"start\":33028},{\"end\":33051,\"start\":33044},{\"end\":33060,\"start\":33057},{\"end\":33549,\"start\":33544},{\"end\":33569,\"start\":33559},{\"end\":33902,\"start\":33898},{\"end\":33913,\"start\":33909},{\"end\":33920,\"start\":33914},{\"end\":33933,\"start\":33926},{\"end\":33944,\"start\":33938},{\"end\":34276,\"start\":34270},{\"end\":34289,\"start\":34282},{\"end\":34301,\"start\":34295},{\"end\":34310,\"start\":34306},{\"end\":34319,\"start\":34316},{\"end\":34332,\"start\":34325},{\"end\":34346,\"start\":34337},{\"end\":34360,\"start\":34353},{\"end\":34375,\"start\":34367},{\"end\":34898,\"start\":34892},{\"end\":34910,\"start\":34904},{\"end\":34919,\"start\":34915},{\"end\":34933,\"start\":34925},{\"end\":35287,\"start\":35282},{\"end\":35304,\"start\":35298},{\"end\":35551,\"start\":35550},{\"end\":35566,\"start\":35560},{\"end\":35820,\"start\":35814},{\"end\":35834,\"start\":35830},{\"end\":35848,\"start\":35842},{\"end\":36195,\"start\":36192},{\"end\":36214,\"start\":36208},{\"end\":36228,\"start\":36223},{\"end\":36396,\"start\":36388},{\"end\":36411,\"start\":36404},{\"end\":36436,\"start\":36427},{\"end\":36883,\"start\":36876},{\"end\":36893,\"start\":36888},{\"end\":36905,\"start\":36898},{\"end\":36916,\"start\":36912},{\"end\":36930,\"start\":36922},{\"end\":36941,\"start\":36937},{\"end\":37420,\"start\":37413},{\"end\":37436,\"start\":37428},{\"end\":37452,\"start\":37444},{\"end\":37910,\"start\":37905},{\"end\":37920,\"start\":37916},{\"end\":37925,\"start\":37921},{\"end\":37940,\"start\":37931},{\"end\":38359,\"start\":38358},{\"end\":38375,\"start\":38370},{\"end\":38603,\"start\":38599},{\"end\":38620,\"start\":38616},{\"end\":38640,\"start\":38632},{\"end\":38642,\"start\":38641},{\"end\":38988,\"start\":38979},{\"end\":39001,\"start\":38994},{\"end\":39017,\"start\":39009},{\"end\":39035,\"start\":39025},{\"end\":39487,\"start\":39483},{\"end\":39499,\"start\":39492},{\"end\":39511,\"start\":39506},{\"end\":39524,\"start\":39517},{\"end\":39539,\"start\":39531},{\"end\":39549,\"start\":39546},{\"end\":40003,\"start\":40000},{\"end\":40017,\"start\":40009},{\"end\":40029,\"start\":40023},{\"end\":40043,\"start\":40035},{\"end\":40058,\"start\":40049},{\"end\":40547,\"start\":40543},{\"end\":40558,\"start\":40553},{\"end\":40570,\"start\":40564},{\"end\":40580,\"start\":40576},{\"end\":40587,\"start\":40581},{\"end\":40601,\"start\":40593},{\"end\":41016,\"start\":41015},{\"end\":41026,\"start\":41025},{\"end\":41037,\"start\":41036},{\"end\":41560,\"start\":41554},{\"end\":41573,\"start\":41569},{\"end\":41583,\"start\":41579},{\"end\":41598,\"start\":41593},{\"end\":41613,\"start\":41609},{\"end\":41630,\"start\":41621},{\"end\":41649,\"start\":41641},{\"end\":41965,\"start\":41960},{\"end\":41982,\"start\":41976},{\"end\":42350,\"start\":42345},{\"end\":42367,\"start\":42360},{\"end\":42382,\"start\":42374},{\"end\":42397,\"start\":42391},{\"end\":42713,\"start\":42708},{\"end\":42728,\"start\":42723},{\"end\":42742,\"start\":42736},{\"end\":43161,\"start\":43153},{\"end\":43177,\"start\":43169},{\"end\":43451,\"start\":43447},{\"end\":43461,\"start\":43457},{\"end\":43476,\"start\":43468},{\"end\":43489,\"start\":43482},{\"end\":43912,\"start\":43908},{\"end\":43929,\"start\":43922},{\"end\":43944,\"start\":43941},{\"end\":43965,\"start\":43955},{\"end\":43975,\"start\":43972},{\"end\":44453,\"start\":44445},{\"end\":44464,\"start\":44460},{\"end\":44482,\"start\":44475},{\"end\":44497,\"start\":44490},{\"end\":44921,\"start\":44917},{\"end\":44932,\"start\":44928},{\"end\":44934,\"start\":44933},{\"end\":44943,\"start\":44942},{\"end\":44960,\"start\":44959},{\"end\":45300,\"start\":45292},{\"end\":45310,\"start\":45307},{\"end\":45321,\"start\":45316},{\"end\":45769,\"start\":45762},{\"end\":45778,\"start\":45774},{\"end\":45791,\"start\":45784},{\"end\":45804,\"start\":45797},{\"end\":45818,\"start\":45809},{\"end\":45832,\"start\":45825},{\"end\":45844,\"start\":45839},{\"end\":45859,\"start\":45852},{\"end\":45877,\"start\":45869},{\"end\":46321,\"start\":46315},{\"end\":46332,\"start\":46326},{\"end\":46344,\"start\":46338},{\"end\":46676,\"start\":46671},{\"end\":46691,\"start\":46684},{\"end\":46703,\"start\":46698},{\"end\":47051,\"start\":47044},{\"end\":47064,\"start\":47059},{\"end\":47077,\"start\":47070},{\"end\":47090,\"start\":47083},{\"end\":47431,\"start\":47428},{\"end\":47447,\"start\":47439},{\"end\":47456,\"start\":47453},{\"end\":47926,\"start\":47921},{\"end\":47941,\"start\":47934},{\"end\":47949,\"start\":47946},{\"end\":47960,\"start\":47954},{\"end\":47973,\"start\":47967},{\"end\":47984,\"start\":47981},{\"end\":48425,\"start\":48420},{\"end\":48440,\"start\":48433},{\"end\":48448,\"start\":48445},{\"end\":48459,\"start\":48453},{\"end\":48470,\"start\":48467},{\"end\":48732,\"start\":48727},{\"end\":48746,\"start\":48740},{\"end\":48755,\"start\":48753},{\"end\":48768,\"start\":48762},{\"end\":48779,\"start\":48776},{\"end\":49213,\"start\":49207},{\"end\":49224,\"start\":49220},{\"end\":49236,\"start\":49230},{\"end\":49599,\"start\":49593},{\"end\":49611,\"start\":49606},{\"end\":49621,\"start\":49617},{\"end\":49637,\"start\":49631},{\"end\":49650,\"start\":49643},{\"end\":49895,\"start\":49890},{\"end\":49910,\"start\":49904},{\"end\":50106,\"start\":50105},{\"end\":50108,\"start\":50107},{\"end\":50123,\"start\":50122},{\"end\":32590,\"start\":32585},{\"end\":32608,\"start\":32603},{\"end\":33009,\"start\":33006},{\"end\":33022,\"start\":33015},{\"end\":33036,\"start\":33028},{\"end\":33051,\"start\":33044},{\"end\":33060,\"start\":33057},{\"end\":33549,\"start\":33544},{\"end\":33569,\"start\":33559},{\"end\":33902,\"start\":33898},{\"end\":33913,\"start\":33909},{\"end\":33920,\"start\":33914},{\"end\":33933,\"start\":33926},{\"end\":33944,\"start\":33938},{\"end\":34276,\"start\":34270},{\"end\":34289,\"start\":34282},{\"end\":34301,\"start\":34295},{\"end\":34310,\"start\":34306},{\"end\":34319,\"start\":34316},{\"end\":34332,\"start\":34325},{\"end\":34346,\"start\":34337},{\"end\":34360,\"start\":34353},{\"end\":34375,\"start\":34367},{\"end\":34898,\"start\":34892},{\"end\":34910,\"start\":34904},{\"end\":34919,\"start\":34915},{\"end\":34933,\"start\":34925},{\"end\":35287,\"start\":35282},{\"end\":35304,\"start\":35298},{\"end\":35551,\"start\":35550},{\"end\":35566,\"start\":35560},{\"end\":35820,\"start\":35814},{\"end\":35834,\"start\":35830},{\"end\":35848,\"start\":35842},{\"end\":36195,\"start\":36192},{\"end\":36214,\"start\":36208},{\"end\":36228,\"start\":36223},{\"end\":36396,\"start\":36388},{\"end\":36411,\"start\":36404},{\"end\":36436,\"start\":36427},{\"end\":36883,\"start\":36876},{\"end\":36893,\"start\":36888},{\"end\":36905,\"start\":36898},{\"end\":36916,\"start\":36912},{\"end\":36930,\"start\":36922},{\"end\":36941,\"start\":36937},{\"end\":37420,\"start\":37413},{\"end\":37436,\"start\":37428},{\"end\":37452,\"start\":37444},{\"end\":37910,\"start\":37905},{\"end\":37920,\"start\":37916},{\"end\":37925,\"start\":37921},{\"end\":37940,\"start\":37931},{\"end\":38359,\"start\":38358},{\"end\":38375,\"start\":38370},{\"end\":38603,\"start\":38599},{\"end\":38620,\"start\":38616},{\"end\":38640,\"start\":38632},{\"end\":38642,\"start\":38641},{\"end\":38988,\"start\":38979},{\"end\":39001,\"start\":38994},{\"end\":39017,\"start\":39009},{\"end\":39035,\"start\":39025},{\"end\":39487,\"start\":39483},{\"end\":39499,\"start\":39492},{\"end\":39511,\"start\":39506},{\"end\":39524,\"start\":39517},{\"end\":39539,\"start\":39531},{\"end\":39549,\"start\":39546},{\"end\":40003,\"start\":40000},{\"end\":40017,\"start\":40009},{\"end\":40029,\"start\":40023},{\"end\":40043,\"start\":40035},{\"end\":40058,\"start\":40049},{\"end\":40547,\"start\":40543},{\"end\":40558,\"start\":40553},{\"end\":40570,\"start\":40564},{\"end\":40580,\"start\":40576},{\"end\":40587,\"start\":40581},{\"end\":40601,\"start\":40593},{\"end\":41016,\"start\":41015},{\"end\":41026,\"start\":41025},{\"end\":41037,\"start\":41036},{\"end\":41560,\"start\":41554},{\"end\":41573,\"start\":41569},{\"end\":41583,\"start\":41579},{\"end\":41598,\"start\":41593},{\"end\":41613,\"start\":41609},{\"end\":41630,\"start\":41621},{\"end\":41649,\"start\":41641},{\"end\":41965,\"start\":41960},{\"end\":41982,\"start\":41976},{\"end\":42350,\"start\":42345},{\"end\":42367,\"start\":42360},{\"end\":42382,\"start\":42374},{\"end\":42397,\"start\":42391},{\"end\":42713,\"start\":42708},{\"end\":42728,\"start\":42723},{\"end\":42742,\"start\":42736},{\"end\":43161,\"start\":43153},{\"end\":43177,\"start\":43169},{\"end\":43451,\"start\":43447},{\"end\":43461,\"start\":43457},{\"end\":43476,\"start\":43468},{\"end\":43489,\"start\":43482},{\"end\":43912,\"start\":43908},{\"end\":43929,\"start\":43922},{\"end\":43944,\"start\":43941},{\"end\":43965,\"start\":43955},{\"end\":43975,\"start\":43972},{\"end\":44453,\"start\":44445},{\"end\":44464,\"start\":44460},{\"end\":44482,\"start\":44475},{\"end\":44497,\"start\":44490},{\"end\":44921,\"start\":44917},{\"end\":44932,\"start\":44928},{\"end\":44934,\"start\":44933},{\"end\":44943,\"start\":44942},{\"end\":44960,\"start\":44959},{\"end\":45300,\"start\":45292},{\"end\":45310,\"start\":45307},{\"end\":45321,\"start\":45316},{\"end\":45769,\"start\":45762},{\"end\":45778,\"start\":45774},{\"end\":45791,\"start\":45784},{\"end\":45804,\"start\":45797},{\"end\":45818,\"start\":45809},{\"end\":45832,\"start\":45825},{\"end\":45844,\"start\":45839},{\"end\":45859,\"start\":45852},{\"end\":45877,\"start\":45869},{\"end\":46321,\"start\":46315},{\"end\":46332,\"start\":46326},{\"end\":46344,\"start\":46338},{\"end\":46676,\"start\":46671},{\"end\":46691,\"start\":46684},{\"end\":46703,\"start\":46698},{\"end\":47051,\"start\":47044},{\"end\":47064,\"start\":47059},{\"end\":47077,\"start\":47070},{\"end\":47090,\"start\":47083},{\"end\":47431,\"start\":47428},{\"end\":47447,\"start\":47439},{\"end\":47456,\"start\":47453},{\"end\":47926,\"start\":47921},{\"end\":47941,\"start\":47934},{\"end\":47949,\"start\":47946},{\"end\":47960,\"start\":47954},{\"end\":47973,\"start\":47967},{\"end\":47984,\"start\":47981},{\"end\":48425,\"start\":48420},{\"end\":48440,\"start\":48433},{\"end\":48448,\"start\":48445},{\"end\":48459,\"start\":48453},{\"end\":48470,\"start\":48467},{\"end\":48732,\"start\":48727},{\"end\":48746,\"start\":48740},{\"end\":48755,\"start\":48753},{\"end\":48768,\"start\":48762},{\"end\":48779,\"start\":48776},{\"end\":49213,\"start\":49207},{\"end\":49224,\"start\":49220},{\"end\":49236,\"start\":49230},{\"end\":49599,\"start\":49593},{\"end\":49611,\"start\":49606},{\"end\":49621,\"start\":49617},{\"end\":49637,\"start\":49631},{\"end\":49650,\"start\":49643},{\"end\":49895,\"start\":49890},{\"end\":49910,\"start\":49904},{\"end\":50106,\"start\":50105},{\"end\":50108,\"start\":50107},{\"end\":50123,\"start\":50122}]", "bib_author_last_name": "[{\"end\":32601,\"start\":32591},{\"end\":32614,\"start\":32609},{\"end\":33013,\"start\":33010},{\"end\":33026,\"start\":33023},{\"end\":33042,\"start\":33037},{\"end\":33055,\"start\":33052},{\"end\":33066,\"start\":33061},{\"end\":33557,\"start\":33550},{\"end\":33581,\"start\":33570},{\"end\":33907,\"start\":33903},{\"end\":33924,\"start\":33921},{\"end\":33936,\"start\":33934},{\"end\":33949,\"start\":33945},{\"end\":34280,\"start\":34277},{\"end\":34293,\"start\":34290},{\"end\":34304,\"start\":34302},{\"end\":34314,\"start\":34311},{\"end\":34323,\"start\":34320},{\"end\":34335,\"start\":34333},{\"end\":34351,\"start\":34347},{\"end\":34365,\"start\":34361},{\"end\":34381,\"start\":34376},{\"end\":34902,\"start\":34899},{\"end\":34913,\"start\":34911},{\"end\":34923,\"start\":34920},{\"end\":34939,\"start\":34934},{\"end\":35296,\"start\":35288},{\"end\":35311,\"start\":35305},{\"end\":35548,\"start\":35531},{\"end\":35558,\"start\":35552},{\"end\":35572,\"start\":35567},{\"end\":35581,\"start\":35574},{\"end\":35828,\"start\":35821},{\"end\":35840,\"start\":35835},{\"end\":35854,\"start\":35849},{\"end\":36206,\"start\":36196},{\"end\":36221,\"start\":36215},{\"end\":36238,\"start\":36229},{\"end\":36402,\"start\":36397},{\"end\":36425,\"start\":36412},{\"end\":36442,\"start\":36437},{\"end\":36886,\"start\":36884},{\"end\":36896,\"start\":36894},{\"end\":36910,\"start\":36906},{\"end\":36920,\"start\":36917},{\"end\":36935,\"start\":36931},{\"end\":36947,\"start\":36942},{\"end\":37426,\"start\":37421},{\"end\":37442,\"start\":37437},{\"end\":37458,\"start\":37453},{\"end\":37914,\"start\":37911},{\"end\":37929,\"start\":37926},{\"end\":37944,\"start\":37941},{\"end\":38368,\"start\":38360},{\"end\":38382,\"start\":38376},{\"end\":38386,\"start\":38384},{\"end\":38614,\"start\":38604},{\"end\":38630,\"start\":38621},{\"end\":38649,\"start\":38643},{\"end\":38992,\"start\":38989},{\"end\":39007,\"start\":39002},{\"end\":39023,\"start\":39018},{\"end\":39040,\"start\":39036},{\"end\":39490,\"start\":39488},{\"end\":39504,\"start\":39500},{\"end\":39515,\"start\":39512},{\"end\":39529,\"start\":39525},{\"end\":39544,\"start\":39540},{\"end\":39552,\"start\":39550},{\"end\":40007,\"start\":40004},{\"end\":40021,\"start\":40018},{\"end\":40033,\"start\":40030},{\"end\":40047,\"start\":40044},{\"end\":40062,\"start\":40059},{\"end\":40551,\"start\":40548},{\"end\":40562,\"start\":40559},{\"end\":40574,\"start\":40571},{\"end\":40591,\"start\":40588},{\"end\":40607,\"start\":40602},{\"end\":41023,\"start\":41017},{\"end\":41034,\"start\":41027},{\"end\":41041,\"start\":41038},{\"end\":41048,\"start\":41043},{\"end\":41567,\"start\":41561},{\"end\":41577,\"start\":41574},{\"end\":41591,\"start\":41584},{\"end\":41607,\"start\":41599},{\"end\":41619,\"start\":41614},{\"end\":41639,\"start\":41631},{\"end\":41656,\"start\":41650},{\"end\":41974,\"start\":41966},{\"end\":41988,\"start\":41983},{\"end\":42358,\"start\":42351},{\"end\":42372,\"start\":42368},{\"end\":42389,\"start\":42383},{\"end\":42406,\"start\":42398},{\"end\":42721,\"start\":42714},{\"end\":42734,\"start\":42729},{\"end\":42748,\"start\":42743},{\"end\":43167,\"start\":43162},{\"end\":43183,\"start\":43178},{\"end\":43455,\"start\":43452},{\"end\":43466,\"start\":43462},{\"end\":43480,\"start\":43477},{\"end\":43492,\"start\":43490},{\"end\":43920,\"start\":43913},{\"end\":43939,\"start\":43930},{\"end\":43953,\"start\":43945},{\"end\":43970,\"start\":43966},{\"end\":43981,\"start\":43976},{\"end\":44458,\"start\":44454},{\"end\":44473,\"start\":44465},{\"end\":44488,\"start\":44483},{\"end\":44500,\"start\":44498},{\"end\":44926,\"start\":44922},{\"end\":44940,\"start\":44935},{\"end\":44949,\"start\":44944},{\"end\":44957,\"start\":44951},{\"end\":44965,\"start\":44961},{\"end\":44977,\"start\":44967},{\"end\":45305,\"start\":45301},{\"end\":45314,\"start\":45311},{\"end\":45327,\"start\":45322},{\"end\":45772,\"start\":45770},{\"end\":45782,\"start\":45779},{\"end\":45795,\"start\":45792},{\"end\":45807,\"start\":45805},{\"end\":45823,\"start\":45819},{\"end\":45837,\"start\":45833},{\"end\":45850,\"start\":45845},{\"end\":45867,\"start\":45860},{\"end\":45883,\"start\":45878},{\"end\":46324,\"start\":46322},{\"end\":46336,\"start\":46333},{\"end\":46350,\"start\":46345},{\"end\":46682,\"start\":46677},{\"end\":46696,\"start\":46692},{\"end\":46711,\"start\":46704},{\"end\":47057,\"start\":47052},{\"end\":47068,\"start\":47065},{\"end\":47081,\"start\":47078},{\"end\":47093,\"start\":47091},{\"end\":47437,\"start\":47432},{\"end\":47451,\"start\":47448},{\"end\":47462,\"start\":47457},{\"end\":47932,\"start\":47927},{\"end\":47944,\"start\":47942},{\"end\":47952,\"start\":47950},{\"end\":47965,\"start\":47961},{\"end\":47979,\"start\":47974},{\"end\":47987,\"start\":47985},{\"end\":48431,\"start\":48426},{\"end\":48443,\"start\":48441},{\"end\":48451,\"start\":48449},{\"end\":48465,\"start\":48460},{\"end\":48473,\"start\":48471},{\"end\":48738,\"start\":48733},{\"end\":48751,\"start\":48747},{\"end\":48760,\"start\":48756},{\"end\":48774,\"start\":48769},{\"end\":48782,\"start\":48780},{\"end\":49218,\"start\":49214},{\"end\":49228,\"start\":49225},{\"end\":49242,\"start\":49237},{\"end\":49604,\"start\":49600},{\"end\":49615,\"start\":49612},{\"end\":49629,\"start\":49622},{\"end\":49641,\"start\":49638},{\"end\":49656,\"start\":49651},{\"end\":49902,\"start\":49896},{\"end\":49916,\"start\":49911},{\"end\":50115,\"start\":50109},{\"end\":50120,\"start\":50117},{\"end\":50128,\"start\":50124},{\"end\":50134,\"start\":50130},{\"end\":32601,\"start\":32591},{\"end\":32614,\"start\":32609},{\"end\":33013,\"start\":33010},{\"end\":33026,\"start\":33023},{\"end\":33042,\"start\":33037},{\"end\":33055,\"start\":33052},{\"end\":33066,\"start\":33061},{\"end\":33557,\"start\":33550},{\"end\":33581,\"start\":33570},{\"end\":33907,\"start\":33903},{\"end\":33924,\"start\":33921},{\"end\":33936,\"start\":33934},{\"end\":33949,\"start\":33945},{\"end\":34280,\"start\":34277},{\"end\":34293,\"start\":34290},{\"end\":34304,\"start\":34302},{\"end\":34314,\"start\":34311},{\"end\":34323,\"start\":34320},{\"end\":34335,\"start\":34333},{\"end\":34351,\"start\":34347},{\"end\":34365,\"start\":34361},{\"end\":34381,\"start\":34376},{\"end\":34902,\"start\":34899},{\"end\":34913,\"start\":34911},{\"end\":34923,\"start\":34920},{\"end\":34939,\"start\":34934},{\"end\":35296,\"start\":35288},{\"end\":35311,\"start\":35305},{\"end\":35548,\"start\":35531},{\"end\":35558,\"start\":35552},{\"end\":35572,\"start\":35567},{\"end\":35581,\"start\":35574},{\"end\":35828,\"start\":35821},{\"end\":35840,\"start\":35835},{\"end\":35854,\"start\":35849},{\"end\":36206,\"start\":36196},{\"end\":36221,\"start\":36215},{\"end\":36238,\"start\":36229},{\"end\":36402,\"start\":36397},{\"end\":36425,\"start\":36412},{\"end\":36442,\"start\":36437},{\"end\":36886,\"start\":36884},{\"end\":36896,\"start\":36894},{\"end\":36910,\"start\":36906},{\"end\":36920,\"start\":36917},{\"end\":36935,\"start\":36931},{\"end\":36947,\"start\":36942},{\"end\":37426,\"start\":37421},{\"end\":37442,\"start\":37437},{\"end\":37458,\"start\":37453},{\"end\":37914,\"start\":37911},{\"end\":37929,\"start\":37926},{\"end\":37944,\"start\":37941},{\"end\":38368,\"start\":38360},{\"end\":38382,\"start\":38376},{\"end\":38386,\"start\":38384},{\"end\":38614,\"start\":38604},{\"end\":38630,\"start\":38621},{\"end\":38649,\"start\":38643},{\"end\":38992,\"start\":38989},{\"end\":39007,\"start\":39002},{\"end\":39023,\"start\":39018},{\"end\":39040,\"start\":39036},{\"end\":39490,\"start\":39488},{\"end\":39504,\"start\":39500},{\"end\":39515,\"start\":39512},{\"end\":39529,\"start\":39525},{\"end\":39544,\"start\":39540},{\"end\":39552,\"start\":39550},{\"end\":40007,\"start\":40004},{\"end\":40021,\"start\":40018},{\"end\":40033,\"start\":40030},{\"end\":40047,\"start\":40044},{\"end\":40062,\"start\":40059},{\"end\":40551,\"start\":40548},{\"end\":40562,\"start\":40559},{\"end\":40574,\"start\":40571},{\"end\":40591,\"start\":40588},{\"end\":40607,\"start\":40602},{\"end\":41023,\"start\":41017},{\"end\":41034,\"start\":41027},{\"end\":41041,\"start\":41038},{\"end\":41048,\"start\":41043},{\"end\":41567,\"start\":41561},{\"end\":41577,\"start\":41574},{\"end\":41591,\"start\":41584},{\"end\":41607,\"start\":41599},{\"end\":41619,\"start\":41614},{\"end\":41639,\"start\":41631},{\"end\":41656,\"start\":41650},{\"end\":41974,\"start\":41966},{\"end\":41988,\"start\":41983},{\"end\":42358,\"start\":42351},{\"end\":42372,\"start\":42368},{\"end\":42389,\"start\":42383},{\"end\":42406,\"start\":42398},{\"end\":42721,\"start\":42714},{\"end\":42734,\"start\":42729},{\"end\":42748,\"start\":42743},{\"end\":43167,\"start\":43162},{\"end\":43183,\"start\":43178},{\"end\":43455,\"start\":43452},{\"end\":43466,\"start\":43462},{\"end\":43480,\"start\":43477},{\"end\":43492,\"start\":43490},{\"end\":43920,\"start\":43913},{\"end\":43939,\"start\":43930},{\"end\":43953,\"start\":43945},{\"end\":43970,\"start\":43966},{\"end\":43981,\"start\":43976},{\"end\":44458,\"start\":44454},{\"end\":44473,\"start\":44465},{\"end\":44488,\"start\":44483},{\"end\":44500,\"start\":44498},{\"end\":44926,\"start\":44922},{\"end\":44940,\"start\":44935},{\"end\":44949,\"start\":44944},{\"end\":44957,\"start\":44951},{\"end\":44965,\"start\":44961},{\"end\":44977,\"start\":44967},{\"end\":45305,\"start\":45301},{\"end\":45314,\"start\":45311},{\"end\":45327,\"start\":45322},{\"end\":45772,\"start\":45770},{\"end\":45782,\"start\":45779},{\"end\":45795,\"start\":45792},{\"end\":45807,\"start\":45805},{\"end\":45823,\"start\":45819},{\"end\":45837,\"start\":45833},{\"end\":45850,\"start\":45845},{\"end\":45867,\"start\":45860},{\"end\":45883,\"start\":45878},{\"end\":46324,\"start\":46322},{\"end\":46336,\"start\":46333},{\"end\":46350,\"start\":46345},{\"end\":46682,\"start\":46677},{\"end\":46696,\"start\":46692},{\"end\":46711,\"start\":46704},{\"end\":47057,\"start\":47052},{\"end\":47068,\"start\":47065},{\"end\":47081,\"start\":47078},{\"end\":47093,\"start\":47091},{\"end\":47437,\"start\":47432},{\"end\":47451,\"start\":47448},{\"end\":47462,\"start\":47457},{\"end\":47932,\"start\":47927},{\"end\":47944,\"start\":47942},{\"end\":47952,\"start\":47950},{\"end\":47965,\"start\":47961},{\"end\":47979,\"start\":47974},{\"end\":47987,\"start\":47985},{\"end\":48431,\"start\":48426},{\"end\":48443,\"start\":48441},{\"end\":48451,\"start\":48449},{\"end\":48465,\"start\":48460},{\"end\":48473,\"start\":48471},{\"end\":48738,\"start\":48733},{\"end\":48751,\"start\":48747},{\"end\":48760,\"start\":48756},{\"end\":48774,\"start\":48769},{\"end\":48782,\"start\":48780},{\"end\":49218,\"start\":49214},{\"end\":49228,\"start\":49225},{\"end\":49242,\"start\":49237},{\"end\":49604,\"start\":49600},{\"end\":49615,\"start\":49612},{\"end\":49629,\"start\":49622},{\"end\":49641,\"start\":49638},{\"end\":49656,\"start\":49651},{\"end\":49902,\"start\":49896},{\"end\":49916,\"start\":49911},{\"end\":50115,\"start\":50109},{\"end\":50120,\"start\":50117},{\"end\":50128,\"start\":50124},{\"end\":50134,\"start\":50130}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5250573},\"end\":32938,\"start\":32449},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":174788791},\"end\":33469,\"start\":32940},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16794075},\"end\":33830,\"start\":33471},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18874645},\"end\":34203,\"start\":33832},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10183447},\"end\":34844,\"start\":34205},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209415058},\"end\":35228,\"start\":34846},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1781727},\"end\":35497,\"start\":35230},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6775458},\"end\":35775,\"start\":35499},{\"attributes\":{\"id\":\"b8\"},\"end\":36175,\"start\":35777},{\"attributes\":{\"id\":\"b9\"},\"end\":36334,\"start\":36177},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3739626},\"end\":36811,\"start\":36336},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":192571596},\"end\":37348,\"start\":36813},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8282555},\"end\":37831,\"start\":37350},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9971732},\"end\":38312,\"start\":37833},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":38532,\"start\":38314},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195908774},\"end\":38905,\"start\":38534},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1543021},\"end\":39437,\"start\":38907},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":85496615},\"end\":39931,\"start\":39439},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6540453},\"end\":40490,\"start\":39933},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":47007607},\"end\":40873,\"start\":40492},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":64193},\"end\":41499,\"start\":40875},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8887614},\"end\":41920,\"start\":41501},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7044126},\"end\":42276,\"start\":41922},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2142115},\"end\":42649,\"start\":42278},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215825382},\"end\":43102,\"start\":42651},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15444534},\"end\":43386,\"start\":43104},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8550762},\"end\":43830,\"start\":43388},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":484327},\"end\":44416,\"start\":43832},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4852647},\"end\":44841,\"start\":44418},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":207761262},\"end\":45225,\"start\":44843},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11988955},\"end\":45686,\"start\":45227},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3451078},\"end\":46249,\"start\":45688},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":211114571},\"end\":46614,\"start\":46251},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2356330},\"end\":46959,\"start\":46616},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12822252},\"end\":47342,\"start\":46961},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2141622},\"end\":47843,\"start\":47344},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":49657846},\"end\":48357,\"start\":47845},{\"attributes\":{\"doi\":\"arXiv:1903.10082\",\"id\":\"b37\"},\"end\":48674,\"start\":48359},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3619954},\"end\":49157,\"start\":48676},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5032773},\"end\":49547,\"start\":49159},{\"attributes\":{\"doi\":\"arXiv:2003.04857\",\"id\":\"b40\"},\"end\":49841,\"start\":49549},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14152023},\"end\":50057,\"start\":49843},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":8876346},\"end\":50318,\"start\":50059},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5250573},\"end\":32938,\"start\":32449},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":174788791},\"end\":33469,\"start\":32940},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16794075},\"end\":33830,\"start\":33471},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18874645},\"end\":34203,\"start\":33832},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10183447},\"end\":34844,\"start\":34205},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209415058},\"end\":35228,\"start\":34846},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1781727},\"end\":35497,\"start\":35230},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6775458},\"end\":35775,\"start\":35499},{\"attributes\":{\"id\":\"b8\"},\"end\":36175,\"start\":35777},{\"attributes\":{\"id\":\"b9\"},\"end\":36334,\"start\":36177},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3739626},\"end\":36811,\"start\":36336},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":192571596},\"end\":37348,\"start\":36813},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8282555},\"end\":37831,\"start\":37350},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9971732},\"end\":38312,\"start\":37833},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":38532,\"start\":38314},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195908774},\"end\":38905,\"start\":38534},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1543021},\"end\":39437,\"start\":38907},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":85496615},\"end\":39931,\"start\":39439},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6540453},\"end\":40490,\"start\":39933},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":47007607},\"end\":40873,\"start\":40492},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":64193},\"end\":41499,\"start\":40875},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8887614},\"end\":41920,\"start\":41501},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7044126},\"end\":42276,\"start\":41922},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2142115},\"end\":42649,\"start\":42278},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":215825382},\"end\":43102,\"start\":42651},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15444534},\"end\":43386,\"start\":43104},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8550762},\"end\":43830,\"start\":43388},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":484327},\"end\":44416,\"start\":43832},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4852647},\"end\":44841,\"start\":44418},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":207761262},\"end\":45225,\"start\":44843},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11988955},\"end\":45686,\"start\":45227},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3451078},\"end\":46249,\"start\":45688},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":211114571},\"end\":46614,\"start\":46251},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2356330},\"end\":46959,\"start\":46616},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12822252},\"end\":47342,\"start\":46961},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2141622},\"end\":47843,\"start\":47344},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":49657846},\"end\":48357,\"start\":47845},{\"attributes\":{\"doi\":\"arXiv:1903.10082\",\"id\":\"b37\"},\"end\":48674,\"start\":48359},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3619954},\"end\":49157,\"start\":48676},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5032773},\"end\":49547,\"start\":49159},{\"attributes\":{\"doi\":\"arXiv:2003.04857\",\"id\":\"b40\"},\"end\":49841,\"start\":49549},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14152023},\"end\":50057,\"start\":49843},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":8876346},\"end\":50318,\"start\":50059}]", "bib_title": "[{\"end\":32583,\"start\":32449},{\"end\":33004,\"start\":32940},{\"end\":33542,\"start\":33471},{\"end\":33896,\"start\":33832},{\"end\":34268,\"start\":34205},{\"end\":34890,\"start\":34846},{\"end\":35280,\"start\":35230},{\"end\":35529,\"start\":35499},{\"end\":35812,\"start\":35777},{\"end\":36386,\"start\":36336},{\"end\":36874,\"start\":36813},{\"end\":37411,\"start\":37350},{\"end\":37903,\"start\":37833},{\"end\":38597,\"start\":38534},{\"end\":38977,\"start\":38907},{\"end\":39481,\"start\":39439},{\"end\":39998,\"start\":39933},{\"end\":40541,\"start\":40492},{\"end\":41013,\"start\":40875},{\"end\":41552,\"start\":41501},{\"end\":41958,\"start\":41922},{\"end\":42343,\"start\":42278},{\"end\":42706,\"start\":42651},{\"end\":43151,\"start\":43104},{\"end\":43445,\"start\":43388},{\"end\":43906,\"start\":43832},{\"end\":44443,\"start\":44418},{\"end\":44915,\"start\":44843},{\"end\":45290,\"start\":45227},{\"end\":45760,\"start\":45688},{\"end\":46313,\"start\":46251},{\"end\":46669,\"start\":46616},{\"end\":47042,\"start\":46961},{\"end\":47426,\"start\":47344},{\"end\":47919,\"start\":47845},{\"end\":48725,\"start\":48676},{\"end\":49205,\"start\":49159},{\"end\":49888,\"start\":49843},{\"end\":50103,\"start\":50059},{\"end\":32583,\"start\":32449},{\"end\":33004,\"start\":32940},{\"end\":33542,\"start\":33471},{\"end\":33896,\"start\":33832},{\"end\":34268,\"start\":34205},{\"end\":34890,\"start\":34846},{\"end\":35280,\"start\":35230},{\"end\":35529,\"start\":35499},{\"end\":35812,\"start\":35777},{\"end\":36386,\"start\":36336},{\"end\":36874,\"start\":36813},{\"end\":37411,\"start\":37350},{\"end\":37903,\"start\":37833},{\"end\":38597,\"start\":38534},{\"end\":38977,\"start\":38907},{\"end\":39481,\"start\":39439},{\"end\":39998,\"start\":39933},{\"end\":40541,\"start\":40492},{\"end\":41013,\"start\":40875},{\"end\":41552,\"start\":41501},{\"end\":41958,\"start\":41922},{\"end\":42343,\"start\":42278},{\"end\":42706,\"start\":42651},{\"end\":43151,\"start\":43104},{\"end\":43445,\"start\":43388},{\"end\":43906,\"start\":43832},{\"end\":44443,\"start\":44418},{\"end\":44915,\"start\":44843},{\"end\":45290,\"start\":45227},{\"end\":45760,\"start\":45688},{\"end\":46313,\"start\":46251},{\"end\":46669,\"start\":46616},{\"end\":47042,\"start\":46961},{\"end\":47426,\"start\":47344},{\"end\":47919,\"start\":47845},{\"end\":48725,\"start\":48676},{\"end\":49205,\"start\":49159},{\"end\":49888,\"start\":49843},{\"end\":50103,\"start\":50059}]", "bib_author": "[{\"end\":32603,\"start\":32585},{\"end\":32616,\"start\":32603},{\"end\":33015,\"start\":33006},{\"end\":33028,\"start\":33015},{\"end\":33044,\"start\":33028},{\"end\":33057,\"start\":33044},{\"end\":33068,\"start\":33057},{\"end\":33559,\"start\":33544},{\"end\":33583,\"start\":33559},{\"end\":33909,\"start\":33898},{\"end\":33926,\"start\":33909},{\"end\":33938,\"start\":33926},{\"end\":33951,\"start\":33938},{\"end\":34282,\"start\":34270},{\"end\":34295,\"start\":34282},{\"end\":34306,\"start\":34295},{\"end\":34316,\"start\":34306},{\"end\":34325,\"start\":34316},{\"end\":34337,\"start\":34325},{\"end\":34353,\"start\":34337},{\"end\":34367,\"start\":34353},{\"end\":34383,\"start\":34367},{\"end\":34904,\"start\":34892},{\"end\":34915,\"start\":34904},{\"end\":34925,\"start\":34915},{\"end\":34941,\"start\":34925},{\"end\":35298,\"start\":35282},{\"end\":35313,\"start\":35298},{\"end\":35550,\"start\":35531},{\"end\":35560,\"start\":35550},{\"end\":35574,\"start\":35560},{\"end\":35583,\"start\":35574},{\"end\":35830,\"start\":35814},{\"end\":35842,\"start\":35830},{\"end\":35856,\"start\":35842},{\"end\":36208,\"start\":36192},{\"end\":36223,\"start\":36208},{\"end\":36240,\"start\":36223},{\"end\":36404,\"start\":36388},{\"end\":36427,\"start\":36404},{\"end\":36444,\"start\":36427},{\"end\":36888,\"start\":36876},{\"end\":36898,\"start\":36888},{\"end\":36912,\"start\":36898},{\"end\":36922,\"start\":36912},{\"end\":36937,\"start\":36922},{\"end\":36949,\"start\":36937},{\"end\":37428,\"start\":37413},{\"end\":37444,\"start\":37428},{\"end\":37460,\"start\":37444},{\"end\":37916,\"start\":37905},{\"end\":37931,\"start\":37916},{\"end\":37946,\"start\":37931},{\"end\":38370,\"start\":38358},{\"end\":38384,\"start\":38370},{\"end\":38388,\"start\":38384},{\"end\":38616,\"start\":38599},{\"end\":38632,\"start\":38616},{\"end\":38651,\"start\":38632},{\"end\":38994,\"start\":38979},{\"end\":39009,\"start\":38994},{\"end\":39025,\"start\":39009},{\"end\":39042,\"start\":39025},{\"end\":39492,\"start\":39483},{\"end\":39506,\"start\":39492},{\"end\":39517,\"start\":39506},{\"end\":39531,\"start\":39517},{\"end\":39546,\"start\":39531},{\"end\":39554,\"start\":39546},{\"end\":40009,\"start\":40000},{\"end\":40023,\"start\":40009},{\"end\":40035,\"start\":40023},{\"end\":40049,\"start\":40035},{\"end\":40064,\"start\":40049},{\"end\":40553,\"start\":40543},{\"end\":40564,\"start\":40553},{\"end\":40576,\"start\":40564},{\"end\":40593,\"start\":40576},{\"end\":40609,\"start\":40593},{\"end\":41025,\"start\":41015},{\"end\":41036,\"start\":41025},{\"end\":41043,\"start\":41036},{\"end\":41050,\"start\":41043},{\"end\":41569,\"start\":41554},{\"end\":41579,\"start\":41569},{\"end\":41593,\"start\":41579},{\"end\":41609,\"start\":41593},{\"end\":41621,\"start\":41609},{\"end\":41641,\"start\":41621},{\"end\":41658,\"start\":41641},{\"end\":41976,\"start\":41960},{\"end\":41990,\"start\":41976},{\"end\":42360,\"start\":42345},{\"end\":42374,\"start\":42360},{\"end\":42391,\"start\":42374},{\"end\":42408,\"start\":42391},{\"end\":42723,\"start\":42708},{\"end\":42736,\"start\":42723},{\"end\":42750,\"start\":42736},{\"end\":43169,\"start\":43153},{\"end\":43185,\"start\":43169},{\"end\":43457,\"start\":43447},{\"end\":43468,\"start\":43457},{\"end\":43482,\"start\":43468},{\"end\":43494,\"start\":43482},{\"end\":43922,\"start\":43908},{\"end\":43941,\"start\":43922},{\"end\":43955,\"start\":43941},{\"end\":43972,\"start\":43955},{\"end\":43983,\"start\":43972},{\"end\":44460,\"start\":44445},{\"end\":44475,\"start\":44460},{\"end\":44490,\"start\":44475},{\"end\":44502,\"start\":44490},{\"end\":44928,\"start\":44917},{\"end\":44942,\"start\":44928},{\"end\":44951,\"start\":44942},{\"end\":44959,\"start\":44951},{\"end\":44967,\"start\":44959},{\"end\":44979,\"start\":44967},{\"end\":45307,\"start\":45292},{\"end\":45316,\"start\":45307},{\"end\":45329,\"start\":45316},{\"end\":45774,\"start\":45762},{\"end\":45784,\"start\":45774},{\"end\":45797,\"start\":45784},{\"end\":45809,\"start\":45797},{\"end\":45825,\"start\":45809},{\"end\":45839,\"start\":45825},{\"end\":45852,\"start\":45839},{\"end\":45869,\"start\":45852},{\"end\":45885,\"start\":45869},{\"end\":46326,\"start\":46315},{\"end\":46338,\"start\":46326},{\"end\":46352,\"start\":46338},{\"end\":46684,\"start\":46671},{\"end\":46698,\"start\":46684},{\"end\":46713,\"start\":46698},{\"end\":47059,\"start\":47044},{\"end\":47070,\"start\":47059},{\"end\":47083,\"start\":47070},{\"end\":47095,\"start\":47083},{\"end\":47439,\"start\":47428},{\"end\":47453,\"start\":47439},{\"end\":47464,\"start\":47453},{\"end\":47934,\"start\":47921},{\"end\":47946,\"start\":47934},{\"end\":47954,\"start\":47946},{\"end\":47967,\"start\":47954},{\"end\":47981,\"start\":47967},{\"end\":47989,\"start\":47981},{\"end\":48433,\"start\":48420},{\"end\":48445,\"start\":48433},{\"end\":48453,\"start\":48445},{\"end\":48467,\"start\":48453},{\"end\":48475,\"start\":48467},{\"end\":48740,\"start\":48727},{\"end\":48753,\"start\":48740},{\"end\":48762,\"start\":48753},{\"end\":48776,\"start\":48762},{\"end\":48784,\"start\":48776},{\"end\":49220,\"start\":49207},{\"end\":49230,\"start\":49220},{\"end\":49244,\"start\":49230},{\"end\":49606,\"start\":49593},{\"end\":49617,\"start\":49606},{\"end\":49631,\"start\":49617},{\"end\":49643,\"start\":49631},{\"end\":49658,\"start\":49643},{\"end\":49904,\"start\":49890},{\"end\":49918,\"start\":49904},{\"end\":50117,\"start\":50105},{\"end\":50122,\"start\":50117},{\"end\":50130,\"start\":50122},{\"end\":50136,\"start\":50130},{\"end\":32603,\"start\":32585},{\"end\":32616,\"start\":32603},{\"end\":33015,\"start\":33006},{\"end\":33028,\"start\":33015},{\"end\":33044,\"start\":33028},{\"end\":33057,\"start\":33044},{\"end\":33068,\"start\":33057},{\"end\":33559,\"start\":33544},{\"end\":33583,\"start\":33559},{\"end\":33909,\"start\":33898},{\"end\":33926,\"start\":33909},{\"end\":33938,\"start\":33926},{\"end\":33951,\"start\":33938},{\"end\":34282,\"start\":34270},{\"end\":34295,\"start\":34282},{\"end\":34306,\"start\":34295},{\"end\":34316,\"start\":34306},{\"end\":34325,\"start\":34316},{\"end\":34337,\"start\":34325},{\"end\":34353,\"start\":34337},{\"end\":34367,\"start\":34353},{\"end\":34383,\"start\":34367},{\"end\":34904,\"start\":34892},{\"end\":34915,\"start\":34904},{\"end\":34925,\"start\":34915},{\"end\":34941,\"start\":34925},{\"end\":35298,\"start\":35282},{\"end\":35313,\"start\":35298},{\"end\":35550,\"start\":35531},{\"end\":35560,\"start\":35550},{\"end\":35574,\"start\":35560},{\"end\":35583,\"start\":35574},{\"end\":35830,\"start\":35814},{\"end\":35842,\"start\":35830},{\"end\":35856,\"start\":35842},{\"end\":36208,\"start\":36192},{\"end\":36223,\"start\":36208},{\"end\":36240,\"start\":36223},{\"end\":36404,\"start\":36388},{\"end\":36427,\"start\":36404},{\"end\":36444,\"start\":36427},{\"end\":36888,\"start\":36876},{\"end\":36898,\"start\":36888},{\"end\":36912,\"start\":36898},{\"end\":36922,\"start\":36912},{\"end\":36937,\"start\":36922},{\"end\":36949,\"start\":36937},{\"end\":37428,\"start\":37413},{\"end\":37444,\"start\":37428},{\"end\":37460,\"start\":37444},{\"end\":37916,\"start\":37905},{\"end\":37931,\"start\":37916},{\"end\":37946,\"start\":37931},{\"end\":38370,\"start\":38358},{\"end\":38384,\"start\":38370},{\"end\":38388,\"start\":38384},{\"end\":38616,\"start\":38599},{\"end\":38632,\"start\":38616},{\"end\":38651,\"start\":38632},{\"end\":38994,\"start\":38979},{\"end\":39009,\"start\":38994},{\"end\":39025,\"start\":39009},{\"end\":39042,\"start\":39025},{\"end\":39492,\"start\":39483},{\"end\":39506,\"start\":39492},{\"end\":39517,\"start\":39506},{\"end\":39531,\"start\":39517},{\"end\":39546,\"start\":39531},{\"end\":39554,\"start\":39546},{\"end\":40009,\"start\":40000},{\"end\":40023,\"start\":40009},{\"end\":40035,\"start\":40023},{\"end\":40049,\"start\":40035},{\"end\":40064,\"start\":40049},{\"end\":40553,\"start\":40543},{\"end\":40564,\"start\":40553},{\"end\":40576,\"start\":40564},{\"end\":40593,\"start\":40576},{\"end\":40609,\"start\":40593},{\"end\":41025,\"start\":41015},{\"end\":41036,\"start\":41025},{\"end\":41043,\"start\":41036},{\"end\":41050,\"start\":41043},{\"end\":41569,\"start\":41554},{\"end\":41579,\"start\":41569},{\"end\":41593,\"start\":41579},{\"end\":41609,\"start\":41593},{\"end\":41621,\"start\":41609},{\"end\":41641,\"start\":41621},{\"end\":41658,\"start\":41641},{\"end\":41976,\"start\":41960},{\"end\":41990,\"start\":41976},{\"end\":42360,\"start\":42345},{\"end\":42374,\"start\":42360},{\"end\":42391,\"start\":42374},{\"end\":42408,\"start\":42391},{\"end\":42723,\"start\":42708},{\"end\":42736,\"start\":42723},{\"end\":42750,\"start\":42736},{\"end\":43169,\"start\":43153},{\"end\":43185,\"start\":43169},{\"end\":43457,\"start\":43447},{\"end\":43468,\"start\":43457},{\"end\":43482,\"start\":43468},{\"end\":43494,\"start\":43482},{\"end\":43922,\"start\":43908},{\"end\":43941,\"start\":43922},{\"end\":43955,\"start\":43941},{\"end\":43972,\"start\":43955},{\"end\":43983,\"start\":43972},{\"end\":44460,\"start\":44445},{\"end\":44475,\"start\":44460},{\"end\":44490,\"start\":44475},{\"end\":44502,\"start\":44490},{\"end\":44928,\"start\":44917},{\"end\":44942,\"start\":44928},{\"end\":44951,\"start\":44942},{\"end\":44959,\"start\":44951},{\"end\":44967,\"start\":44959},{\"end\":44979,\"start\":44967},{\"end\":45307,\"start\":45292},{\"end\":45316,\"start\":45307},{\"end\":45329,\"start\":45316},{\"end\":45774,\"start\":45762},{\"end\":45784,\"start\":45774},{\"end\":45797,\"start\":45784},{\"end\":45809,\"start\":45797},{\"end\":45825,\"start\":45809},{\"end\":45839,\"start\":45825},{\"end\":45852,\"start\":45839},{\"end\":45869,\"start\":45852},{\"end\":45885,\"start\":45869},{\"end\":46326,\"start\":46315},{\"end\":46338,\"start\":46326},{\"end\":46352,\"start\":46338},{\"end\":46684,\"start\":46671},{\"end\":46698,\"start\":46684},{\"end\":46713,\"start\":46698},{\"end\":47059,\"start\":47044},{\"end\":47070,\"start\":47059},{\"end\":47083,\"start\":47070},{\"end\":47095,\"start\":47083},{\"end\":47439,\"start\":47428},{\"end\":47453,\"start\":47439},{\"end\":47464,\"start\":47453},{\"end\":47934,\"start\":47921},{\"end\":47946,\"start\":47934},{\"end\":47954,\"start\":47946},{\"end\":47967,\"start\":47954},{\"end\":47981,\"start\":47967},{\"end\":47989,\"start\":47981},{\"end\":48433,\"start\":48420},{\"end\":48445,\"start\":48433},{\"end\":48453,\"start\":48445},{\"end\":48467,\"start\":48453},{\"end\":48475,\"start\":48467},{\"end\":48740,\"start\":48727},{\"end\":48753,\"start\":48740},{\"end\":48762,\"start\":48753},{\"end\":48776,\"start\":48762},{\"end\":48784,\"start\":48776},{\"end\":49220,\"start\":49207},{\"end\":49230,\"start\":49220},{\"end\":49244,\"start\":49230},{\"end\":49606,\"start\":49593},{\"end\":49617,\"start\":49606},{\"end\":49631,\"start\":49617},{\"end\":49643,\"start\":49631},{\"end\":49658,\"start\":49643},{\"end\":49904,\"start\":49890},{\"end\":49918,\"start\":49904},{\"end\":50117,\"start\":50105},{\"end\":50122,\"start\":50117},{\"end\":50130,\"start\":50122},{\"end\":50136,\"start\":50130}]", "bib_venue": "[{\"end\":32668,\"start\":32616},{\"end\":33145,\"start\":33068},{\"end\":33633,\"start\":33583},{\"end\":33989,\"start\":33951},{\"end\":34470,\"start\":34383},{\"end\":35002,\"start\":34941},{\"end\":35347,\"start\":35313},{\"end\":35622,\"start\":35583},{\"end\":35928,\"start\":35856},{\"end\":36190,\"start\":36177},{\"end\":36521,\"start\":36444},{\"end\":37026,\"start\":36949},{\"end\":37537,\"start\":37460},{\"end\":38023,\"start\":37946},{\"end\":38356,\"start\":38314},{\"end\":38700,\"start\":38651},{\"end\":39119,\"start\":39042},{\"end\":39631,\"start\":39554},{\"end\":40151,\"start\":40064},{\"end\":40658,\"start\":40609},{\"end\":41123,\"start\":41050},{\"end\":41691,\"start\":41658},{\"end\":42057,\"start\":41990},{\"end\":42445,\"start\":42408},{\"end\":42827,\"start\":42750},{\"end\":43220,\"start\":43185},{\"end\":43561,\"start\":43494},{\"end\":44070,\"start\":43983},{\"end\":44579,\"start\":44502},{\"end\":45016,\"start\":44979},{\"end\":45406,\"start\":45329},{\"end\":45946,\"start\":45885},{\"end\":46404,\"start\":46352},{\"end\":46760,\"start\":46713},{\"end\":47132,\"start\":47095},{\"end\":47541,\"start\":47464},{\"end\":48053,\"start\":47989},{\"end\":48418,\"start\":48359},{\"end\":48861,\"start\":48784},{\"end\":49328,\"start\":49244},{\"end\":49591,\"start\":49549},{\"end\":49927,\"start\":49918},{\"end\":50173,\"start\":50136},{\"end\":32668,\"start\":32616},{\"end\":33145,\"start\":33068},{\"end\":33633,\"start\":33583},{\"end\":33989,\"start\":33951},{\"end\":34470,\"start\":34383},{\"end\":35002,\"start\":34941},{\"end\":35347,\"start\":35313},{\"end\":35622,\"start\":35583},{\"end\":35928,\"start\":35856},{\"end\":36190,\"start\":36177},{\"end\":36521,\"start\":36444},{\"end\":37026,\"start\":36949},{\"end\":37537,\"start\":37460},{\"end\":38023,\"start\":37946},{\"end\":38356,\"start\":38314},{\"end\":38700,\"start\":38651},{\"end\":39119,\"start\":39042},{\"end\":39631,\"start\":39554},{\"end\":40151,\"start\":40064},{\"end\":40658,\"start\":40609},{\"end\":41123,\"start\":41050},{\"end\":41691,\"start\":41658},{\"end\":42057,\"start\":41990},{\"end\":42445,\"start\":42408},{\"end\":42827,\"start\":42750},{\"end\":43220,\"start\":43185},{\"end\":43561,\"start\":43494},{\"end\":44070,\"start\":43983},{\"end\":44579,\"start\":44502},{\"end\":45016,\"start\":44979},{\"end\":45406,\"start\":45329},{\"end\":45946,\"start\":45885},{\"end\":46404,\"start\":46352},{\"end\":46760,\"start\":46713},{\"end\":47132,\"start\":47095},{\"end\":47541,\"start\":47464},{\"end\":48053,\"start\":47989},{\"end\":48418,\"start\":48359},{\"end\":48861,\"start\":48784},{\"end\":49328,\"start\":49244},{\"end\":49591,\"start\":49549},{\"end\":49927,\"start\":49918},{\"end\":50173,\"start\":50136},{\"end\":32707,\"start\":32670},{\"end\":33209,\"start\":33147},{\"end\":34544,\"start\":34472},{\"end\":35050,\"start\":35004},{\"end\":35987,\"start\":35930},{\"end\":36585,\"start\":36523},{\"end\":37090,\"start\":37028},{\"end\":37601,\"start\":37539},{\"end\":38087,\"start\":38025},{\"end\":39183,\"start\":39121},{\"end\":39695,\"start\":39633},{\"end\":40225,\"start\":40153},{\"end\":41186,\"start\":41125},{\"end\":42111,\"start\":42059},{\"end\":42891,\"start\":42829},{\"end\":43615,\"start\":43563},{\"end\":44144,\"start\":44072},{\"end\":44643,\"start\":44581},{\"end\":45470,\"start\":45408},{\"end\":46443,\"start\":46406},{\"end\":47605,\"start\":47543},{\"end\":48104,\"start\":48055},{\"end\":48925,\"start\":48863},{\"end\":32707,\"start\":32670},{\"end\":33209,\"start\":33147},{\"end\":34544,\"start\":34472},{\"end\":35050,\"start\":35004},{\"end\":35987,\"start\":35930},{\"end\":36585,\"start\":36523},{\"end\":37090,\"start\":37028},{\"end\":37601,\"start\":37539},{\"end\":38087,\"start\":38025},{\"end\":39183,\"start\":39121},{\"end\":39695,\"start\":39633},{\"end\":40225,\"start\":40153},{\"end\":41186,\"start\":41125},{\"end\":42111,\"start\":42059},{\"end\":42891,\"start\":42829},{\"end\":43615,\"start\":43563},{\"end\":44144,\"start\":44072},{\"end\":44643,\"start\":44581},{\"end\":45470,\"start\":45408},{\"end\":46443,\"start\":46406},{\"end\":47605,\"start\":47543},{\"end\":48104,\"start\":48055},{\"end\":48925,\"start\":48863}]"}}}, "year": 2023, "month": 12, "day": 17}