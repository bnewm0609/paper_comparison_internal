{"id": 212908749, "updated": "2023-11-07 18:50:49.617", "metadata": {"title": "Improved Knowledge Distillation via Teacher Assistant", "authors": "[{\"first\":\"Seyed-Iman\",\"last\":\"Mirzadeh\",\"middle\":[]},{\"first\":\"Mehrdad\",\"last\":\"Farajtabar\",\"middle\":[]},{\"first\":\"Ang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Nir\",\"last\":\"Levine\",\"middle\":[]},{\"first\":\"Akihiro\",\"last\":\"Matsukawa\",\"middle\":[]},{\"first\":\"Hassan\",\"last\":\"Ghasemzadeh\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a \ufb01xed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1902.03393", "mag": "2996569511", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/MirzadehFLLMG20", "doi": "10.1609/aaai.v34i04.5963"}}, "content": {"source": {"pdf_hash": "c7132dca62b58c808befd6bd5228f317129dc785", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/5963/5819\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/5963/5819", "status": "GOLD"}}, "grobid": {"id": "5ddfa8389f158ef4eb45385cf54a495f7b74eb80", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c7132dca62b58c808befd6bd5228f317129dc785.txt", "contents": "\nImproved Knowledge Distillation via Teacher Assistant\n\n\nIman Seyed \nMirzadeh \nWashington State University\nWAUSA\n\nMehrdad Farajtabar \nDeepMind\nCAUSA\n\nAng Li \nDeepMind\nCAUSA\n\nNir Levine nirlevine@google.com3akihiro.matsukawa@gmail.com \nDeepMind\nCAUSA\n\nAkihiro Matsukawa \nHassan Ghasemzadeh hassan.ghasemzadeh@wsu.edu2farajtabar \nWashington State University\nWAUSA\n\nNY, USAD E Shaw \nWashington State University\nWAUSA\n\nImproved Knowledge Distillation via Teacher Assistant\n\nDespite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.\n\nIntroduction\n\nDeep neural networks have achieved state of the art results in a variety of applications such as computer vision (Huang et al. 2017;Hu, Shen, and Sun 2018), speech recognition ) and natural language processing (Devlin et al. 2018). Although it is established that introducing more layers and more parameters often improves the accuracy of a model, big models are computationally too expensive to be deployed on devices with limited computation power such as mobile phones and embedded sensors. Model compression techniques have emerged to address such issues, e.g., parameter pruning and sharing (Han, Mao, and Dally 2016), low-rank factorization (Tai et al. 2015) and knowledge distillation (Bucila, Caruana, and Niculescu-Mizil 2006;Hinton, Vinyals, and Dean 2015). Among these approaches, knowledge distillation has proven a promising way to obtain a small model that retains the accuracy of a large one. It works Figure 1: TA fills the gap between student & teacher by adding a term to the usual classification loss that encourages the student to mimic the teacher's behavior.\n\nHowever, we argue that knowledge distillation is not always effective, especially when the gap (in size) between teacher and student is large. To illustrate, we ran experiments that show surprisingly a student model distilled from a teacher with more parameters(and better accuracy) performs worse than the same one distilled from a smaller teacher with a smaller capacity. Such scenarios seem to impact the efficacy of knowledge distillation where one is given a small student network and a pre-trained large one as a teacher, both fixed and (wrongly) presumed to form a perfect transfer pair.\n\nInspired by this observation, we propose a new distillation framework called Teacher Assistant Knowledge Distillation (TAKD), which introduces intermediate models as teacher assistants (TAs) between the teacher and the student to fill in their gap (Figure 1). TA models are distilled from the teacher, and the student is then only distilled from the TAs.\n\nOur contributions are: (1) We show that the size (capacity) gap between teacher and student is important. To the best of our knowledge, we are the first to study this gap and verify that the distillation performance is not at its top with the largest teacher; (2) We propose a teacher assistant based knowledge distillation approach to improve the accuracy of student network in the case of extreme compression; (3) We extend this framework to include a chain of multiple TAs from teacher to student to further improve the knowledge transfer and provided some insights to find the best one; (4) Through extensive empirical evaluations and a theoretical justification, we show that introducing intermediary TA networks improves the distillation performance.\n\n\nRelated Work\n\nWe discuss in this section related literature in knowledge distillation and neural network compression.\n\nModel Compression. Since our goal is to train a small, yet accurate network, this work is related to model compression. There has been an interesting line of research that compresses a large network by reducing the connections based on weight magnitudes (Han, Mao, and Dally 2016;Li et al. 2016) or importance scores (Yu et al. 2018). The reduced network is fine-tuned on the same dataset to retain its accuracy. Another line of research focuses on distilling the original (large) network to a smaller network (Polino, Pascanu, and Alistarh 2018;Wang et al. 2018a), in which case the smaller network is more flexible in its architecture design does not have to be a sub-graph of the original network.\n\nKnowledge Distillation. Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) and popularized by Hinton, Vinyals, and Dean (2015) knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Since then, knowledge distillation has been widely adopted in a variety of learning tasks (Yim et al. 2017;Yu et al. 2017;Schmitt et al. 2018;Chen et al. 2017). Adversarial methods also have been utilized for modeling knowledge transfer between teacher and student (Heo et al. 2018;Xu, Hsu, and Huang 2018;Wang et al. 2018b;2018c).\n\nThere have been works studying variants of model distillation that involve multiple networks learning at the same time. Romero et al. (2014) proposed to transfer the knowledge using not only the logit layer but earlier ones too. To cope with the difference in width, they suggested a regressor to connect teacher and student's intermediate layers.\n\nUnfortunately, there is not a principled way to do this. To solve this issue, Yim et al.;Yu et al. (2017; used a shared representation of layers, however, it's not straightforward to choose the appropriate layer to be matched. Czarnecki et al. (2017) minimized the difference between teacher and student derivatives of the loss combined with the divergence from teacher predictions while Tarvainen and Valpola (2017) uses averaging model weights instead of target predictions. Urban et al. (2017) trained a network consisting of an ensemble of 16 convolutional neural networks and compresses the learned function into shallow multilayer perceptrons. To improve the student performance, Sau and Balasubramanian (2016) injected noise into teacher logits to make the student more robust. Utilizing multiple teachers were always a way to increase robustness. Zhang et al. (2017) proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. KL divergences between pairs of students are added into the loss function to enforce the knowledge transfer among peers. You et al. (2017) proposed a voting strategy to unify multiple relative dissimilarity information provided by multiple teacher networks. Anil et al. (2018) introduced an efficient distributed online distil-lation framework called co-distillation and argue that distillation can even work when the teacher and student are made by the same network architecture. The idea is to train multiple models in parallel and use distillation loss when they are not converged, in which case the model training is faster and the model quality is also improved.\n\nHowever, the effectiveness of distilling a large model to a small model has not yet been well studied. Our work differs from existing approaches in that we study how to improve the student performance given fixed student and teacher network sizes, and introduces intermediate networks with a moderate capacity to improve distillation performance. Moreover, our work can be seen as a complement that can be combined with them and improve their performance.\n\nDistillation Theory. Despite its huge popularity, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training. The so-called dark knowledge transferred in the process helps the student learn the finer structure of teacher network. Hinton, Vinyals, and Dean (2015) argues that the success of knowledge distillation is attributed to the logit distribution of the incorrect outputs, which provides information on the similarity between output categories. Furlanello et al. (2018) investigated the success of knowledge distillation via gradients of the loss where the soft-target part acts as an importance sampling weight based on the teachers confidence in its maximum value. Zhang et al. (2017) analyzed knowledge distillation from the posterior entropy viewpoint claiming that soft-targets bring robustness by regularizing a much more informed choice of alternatives than blind entropy regularization. Last but not least, Lopez-Paz et al. (2015) studied the effectiveness of knowledge distillation from the perspective of learning theory (Vapnik 1998) by studying the estimation error in empirical risk minimization framework.\n\nIn this paper, we take this last approach to support our claim on the effectiveness of introducing an intermediate network between student and teacher. Moreover, we empirically analyze it via visualizing the loss function.\n\n\nAssistant based Knowledge Distillation Background and Notations\n\nThe idea behind knowledge distillation is to have the student network (S) be trained not only via the information provided by true labels but also by observing how the teacher network (T) represents and works with the data. The teacher network is sometimes deeper and wider (Hinton, Vinyals, and Dean 2015), of similar size (Anil et al. 2018;Zhang et al. 2017), or shallower but wider (Romero et al. 2014).\n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss\nL SL = H(softmax(a s ), y r ).\n(1)\n\nIn knowledge distillation, originally proposed by Bucila, Caruana, and Niculescu-Mizil;Ba and Caruana (2006; and popularized by Hinton, Vinyals, and Dean (2015) one also tries to match the softened outputs of student y s = softmax(a s /\u03c4 ) and teacher y t =softmax(a t /\u03c4 ) via a KLdivergence loss\nL KD = \u03c4 2 KL(y s , y t )(2)\nHyperparameter \u03c4 referred to temperature is introduced to put additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss function:\nL student = (1 \u2212 \u03bb)L SL + \u03bbL KD (3)\nwhere \u03bb is a second hyperparameter controlling the trade-off between the two losses. We refer to this approach as Baseline Knowledge Distillation (BLKD) through the paper.\n\n\nThe Gap Between Student and Teacher\n\nGiven a fixed student network, e.g., a Convolutional Neural Network (CNN) with 2 layers to be deployed on a small embedded device, and a pool of larger pre-trained CNNs, which one should be selected as the teacher in the knowledge distillation framework? The first answer is to pick the strongest which is the biggest one. However, this is not what we observed empirically as showing in Figure 2. Here, a plain CNN student with 2 convolutional layers is being trained via distillation with similar but larger teachers of size 4, 6, 8, and 10 on both CIFAR-10 and CIFAR-100 datasets. By size, we mean the number of convolutional layers in the CNN. This number is roughly proportional to the actual size or number of parameters of the neural network and proxy its capacity. Note that they are usually followed by max-pooling or fully connected layers too. We defer the full details on experimental setup to experiments section. With increasing teacher size, its own (test) accuracy increases (plotted in red on the right axis). However, the trained student accuracy first increases and then decreases (depicted in blue on the left axis). To explain this phenomenon, we can name a few factors that are competing against each other when enlarging the teacher: 1. Teacher's performance increases, thus it provides better supervision for the student by being a better predictor. 2. The teacher is becoming so complex that the student does not have the sufficient capacity or mechanics to mimic her behavior despite receiving hints. 3. Teacher's certainty about data increases, thus making its logits (soft targets) less soft. This weakens the knowledge transfer which is done via matching the soft targets. a) CIFAR-10 b) CIFAR-100 Figure 3: Percentage of distilled student performance increase over the performance when it learns from scratch with varying student size. The teacher has 10 layers.\n\nFactor 1 is in favor of increasing the distillation performance while factors 2 and 3 are against it. Initially, as the teacher size increases, factor 1 prevails; as it grows larger, factors 2 and 3 dominate.\n\nSimilarly, imagine the dual problem. We are given a large teacher network to be used for training smaller students, and we are interested in knowing for what student size this teacher is most beneficial in the sense of boosting the accuracy against the same student learned from scratch. As expected and illustrated in Figure 3, by decreasing student size, factor 1 causes an increase in the student's performance boost while gradually factors 2 and 3 prevail and worsen the performance gain.\n\n\nTeacher Assistant Knowledge Distillation (TAKD)\n\nImagine a real-world scenario where a pre-trained large network is given, and we are asked to distill its knowledge to a fixed and very small student network. The gap discussed in the previous subsection makes the knowledge distillation less effective than it could be. Note that we cannot select the teacher size or the student size to maximize the performance. Both are fixed and given.\n\nIn this paper, we propose to use intermediate-size networks to fill in the gap between them. The teacher assistant (TA) lies somewhere in between teacher and student in terms of size or capacity. First, the TA network is distilled from the teacher. Then, the TA plays the role of a teacher and trains the student via distillation. This strategy will alleviate factor 2 in the previous subsection by being closer to the student than the teacher. Therefore, the student is able to fit TA's logit distribution more effectively than that of the teacher's. It also alleviates factor 3 by allowing softer (and maybe) less confident targets. In terms of factor 1, a TA may degrade the performance, however, as we will see in experiments and theoretical analysis sections, both empirical results and theoretical analyses substantiate the effectiveness (improved performance) of TAKD. This happens because encouraging positively correlated factors (like 2 and 3) outweighs the performance loss due to negative ones (like 1).\n\nIt will be demonstrated in experiments that TA with any intermediate size always improves the knowledge distillation performance. However, one might ask what the optimal TA size for the highest performance gain is? If one TA improves the distillation result, why not also train this TA via another distilled TA? Or would a TA trained from scratch be as effective as our approach? In the following sections, we try to study and answer these questions from empirical perspectives complemented with some theoretical intuitions.\n\n\nExperimental Setup\n\nWe describe in this section the settings of our experiments. Datasets. We perform a set of experiments on two standard datasets CIFAR-10 and CIFAR-100 and one experiment on the large-scale ImageNet dataset. The datasets consist of 32 \u00d7 32 RGB images. The task for all of them is to classify images into image categories. CIFAR-10, CIFAR-100 and ImageNet contain 10 and 100 and 1000 classes, respectively.\n\nImplementation. We used PyTorch (Paszke et al. 2017) framework for the implementation 1 and as a preprocessing step, we transformed images to ones with zero mean and standard deviation of 0.5. For optimization, we used stochastic gradient descent with Nesterov momentum of 0.9 and learning rate of 0.1 for 150 epochs. For experiments on plain CNN networks, we used the same learning rate, while for ResNet training we decrease learning rate to 0.01 on epoch 80 and 0.001 on epoch 120. We also used weight decay with the value of 0.0001 for training ResNets. To attain reliable results, we performed all the experiments with a hyperparameter optimization toolkit (Microsoft-Research 2018) which uses a tree-structured Parzen estimator to tune hyperparameters as explained in (Bergstra et al. 2011). Hyperparameters include distillation trade-off \u03bb and temperature \u03c4 explained in the previous section. It's notable that all the accuracy results reported in this paper, are the top-1 test accuracy reached by the hyper-parameter optimizer after running each experiment for 120 trials.\n\nNetwork Architectures. We evaluate the performance of the proposed method on two architectures. The first one is a VGG like architecture (plain CNN) consists of convolutional cells (usually followed by max pooling and/or batch normalization) ended with fully connected layer. We take the number of convolutional cells as a proxy for size or capacity of the network. The full details of each plain CNN network  is provided in the appendix 2 . We also used ResNet as a more advanced CNN architecture with skip connections. We used the structures proposed in the original paper (He et al. 2016). The number of blocks in the ResNet architecture is served as a proxy for the size or flexibility of the network.\n\n\nResults and Analysis\n\nIn this section, we evaluate our proposed Teacher Assistant Knowledge Distillation (TAKD) and investigate several important questions related to this approach. Throughout this section, we use S=i to represent the student network of size i, T=j to represent a teacher network of size j and TA=k to represent a teacher assistant network of size k. As a reminder by size we mean the number of convolutional layers for plain CNN and ResNet blocks for the case of ResNet. These serve as a proxy for the size or the number of parameters or capacity of the network.\n\n\nWill TA Improve Knowledge Distillation?\n\nFirst of all, we compare the performance of our Teacher Assistant based method (TAKD) with the baseline knowledge distillation (BLKD) and with training normally without any distillation (NOKD) for the three datasets and two architectures. Table 1 shows the results. It is seen the proposed method outperforms both the baseline knowledge distillation and the normal training of neural networks by a reasonable margin. We include ImageNet dataset only for this experiment to demonstrate TAKD works for the web-scale data too. For the rest of the paper we work with CIFAR10 and CIFAR100.\n\nWhat is the Best TA Size?\n\nThe benefits of having a teacher assistant as an intermediary network for transferring knowledge comes with an essential burden -selecting the proper TA size. We evaluate the student's accuracy given varied TA sizes for plain CNN in Table 2 and for ResNet in 3, respectively. The first observation is that having a TA (of any size) improves the result compared to BLKD and NOKD reported in Table 1. Another observation is that for the case of CNN, TA=4 performs better than TA=6 or TA=8. One might naturally ask why 4 is the best while 6 seems to be better bridge as it is exactly lies between 2 and 10? Alternatively, we note that for both CIFAR-10 and CIFAR-100, the optimal TA size (4) is actually placed close to the middle in terms of average accuracy rather than the average of size. Figure 4-a,b depicts the accuracy of a trained neural network with no distillation in blue while the mean accuracy between S=2 and T=10 is depicted in red dashed line. The figure shows that for both of them, size 4 is closer to the mean value compared to 6 or 8. For ResNet in Table 3 for CIFAR-10, TA=14 is the optimum, while, for CIFAR-100, TA=20 is the best. Interestingly, Figure 4-c,d confirms that for CIFAR-10, TA=14 is closer to the mean performance of size 8 and 110 while TA=20 is so for CIFAR-100. Incorporating a TA with size close to the average performance of teacher and student seems to be a reasonable heuristic to find the optimal TA size, however, more systematic theoretical and empirical investigation remains an interesting venue for future work.\n\nWhy Limiting to 1-step TA?\n\nWe have seen that for CNN networks on CIFAR-100, incorporating a TA=4 between S=2 and T=10 improves the student. However, to train TA=4 via distillation from T=10, one may propose to put another TA (Say TA =6) in between to enhance the TA training via another distillation. Using a simplified notation we represent the above sequential distillation process by the distillation path 10 \u2192 6 \u2192 4 \u2192 2.\n\nEven, one could go further and do a distillation via the path 10 \u2192 8 \u2192 6 \u2192 4 \u2192 2.\n\nTo investigate this extension we evaluate all the possible distillation paths and show their outcomes in a single graph in Figure 5. To simplify the presentation we only include networks with even numbers of layers. The numbers in each oval are the accuracy on CIFAR-100 trained on CNN network using the corresponding distillation paths. A benefit of this visualization is not only that we can study the transfer results to S=2, but also for intermediate sizes. Given n possible intermediate networks, there are 2 n possible paths. For example, the 4 possible paths to transfer from T=10 to S=4 are shown in column associated to size 4. For better comparison, the direct transfer (associated to BLKD) are colored in green while the performance without distillation (NOKD) is shown in the last row.\n\nBy studying this figure we get interesting insights. Firstly, it is clear that, for all the student sizes (S=2,4,6), TAKD works better than BLKD or NOKD. No matter how many TAs are included in the distillation path, one can obtain better students compared to BLKD and NOKD. Secondly, the column associated with size 2 reveals that all multistep TAKD variants work comparably good and considerably better than BLKD and NOKD. Thirdly, for S=2 and S=4, a full path going through all possible intermediate TA networks performs the best. According to these observations, one can choose a distillation path based on the time and computing resources available. Without any constraint, a full distillation path is optimal (refer to appendix for details). However, an interesting extension is to limit the number of intermediate TA networks. Can we find the best path in that setting? Given a student and teacher is there a way to automatically find the best path given the constraints? In the appendix section we provide a discussion for these problems.\n\n\nComparison with Other Distillation Methods\n\nSince the rediscovery of the basic knowledge distillation method (Hinton, Vinyals, and Dean 2015) many variants of it has been proposed. In Fig 6-right we have compared the performance of our proposed framework via a single TA with some of the most recent state-of-the-art ones reported and evaluated by Heo et al. (2018). The 'FITNET' (Romero et al. 2014) proposed to match the knowledge in the intermediate layers. The method denoted as 'AT' proposed spatial transfer between teacher and student (Z and K 2016). In the 'FSP' method, a channel-wise correlation matrix is used as the medium of knowledge transfer (Yim et al. 2017). The method 'BSS' (Heo et al. 2018) trains a student classifier based on the adversarial samples supporting the decision boundary. For these the numbers are reported from the paper (Heo et al. 2018). To make a fair comparison, we used exactly the same setting for CIFAR-10 experiments. In addition to 50K-10K training-test division, all classifiers were trained 80 epochs. Although we found that more epochs (e.g. 160) further improves our result, we followed their setting for a fair comparison. ResNet26 is the teacher and ResNet8 and ResNet14 are the students. In addition, we compared with deep mutual learning, 'MUTUAL', with our own implementation of the proposed algorithm in (Zhang et al. 2017) where the second network is the teacher network. Also, since deep mutual learning needs an initial training phase for both networks, we did this initialization phase for 40 epochs for both networks and then, trained both networks mutually for 80 epochs, equal to other modes. For our method, we used TAs ResNet20 and ResNet14 for students ResNet14 and ResNet8, respectively. It's seen that our TAtrained student outperforms all of them. Note that our proposed framework can be combined with all these variants to improve them too. Why Does Distillation with TA work?\n\nIn this section, we try to shed some light on why and how our TA based knowledge distillation is better than the baselines.\n\n\nTheoretical Analysis\n\nAccording to the VC theory (Vapnik 1998) one can decompose the classification error of a classifier f s as\nR(f s ) \u2212 R(f r ) \u2264 O |F s | C n \u03b1sr + sr ,(4)\nwhere, the O(\u00b7) and sr terms are the estimation and approximation error, respectively. The former is related to the statistical procedure for learning given the number of data points, while the latter is characterized by the capacity of the learning machine. Here, f r \u2208 F r is the real (ground truth) target function and f s \u2208 F s is the student function, R is the error, | \u00b7 | C is some function class capacity measure, n is the number of data point, and finally 1 2 \u2264 \u03b1 sr \u2264 1 is related to the learning rate acquiring small values close to 1 2 for difficult problems while being close to 1 for easier problems. Note that sr is the approximation error of the student function class F s with respect to f r \u2208 F r . Building on the top of Lopez-Paz et al. (2015), we extend their result and investigate why and when introducing a TA improves knowledge distillation. In Equation (4) student learns from scratch (NOKD). Let f t \u2208 F t be the teacher function, then\nR(f t ) \u2212 R(f r ) \u2264 O |F t | C n \u03b1tr + tr ,(5)\nwhere, \u03b1 tr and tr are correspondingly defined for teacher learning from scratch. Then, we can transfer the knowledge of the teacher directly to the student and retrieve the baseline knowledge distillation (BLKD). To simplify the argument we assume the training is done via pure distillation (\u03bb = 1):\nR(f s ) \u2212 R(f t ) \u2264 O |F s | C n \u03b1st + st ,(6)\nwhere \u03b1 st and st are associated to student learning from teacher. If we combine Equations (5) and (6) (7) to hold for BLKD to be effective. In line with our finding, but with a little different formulation, Lopez-Paz et al. (2015) pointed out |F t | C should be small, otherwise the BLKD would not outperform NOKD. We acknowledge that similar to Lopez-Paz et al. (2015), we work with the upper bounds not the actual performance and also in an asymptotic regime.\n\nHere we built on top of their result and put a (teacher) assistant between student and teacher\nR(f s ) \u2212 R(f a ) \u2264 O |F s | C n \u03b1sa + sa ,(8)\nand, then the TA itself learns from the teacher\nR(f a ) \u2212 R(f t ) \u2264 O |F a | C n \u03b1at + at ,(9)\nwhere, \u03b1 sa , sa , \u03b1 at , and at are defined accordingly. Combing Equations (5), (8), and (9) leads to the following equation that needs to be satisfied in order to TAKD outperforms BLKD and NOKD, respectively:\nO |F t | C n \u03b1tr + |F a | C n \u03b1at + |F s | C n \u03b1sa + tr + at + sa (10) \u2264 O |F t | C n \u03b1tr + |F s | C n \u03b1st + tr + st (11) \u2264 O |F s | C n \u03b1sr + sr .(12)\nWe now discuss how the first inequality (eq. (10) \u2264 eq. (11) Hinton, Vinyals, and Dean (2015) we know at + sa \u2264 S TA T R Figure 6: Left) rate of learning between different targets (longer distance means lower \u03b1 \u00b7\u00b7 ); Right) Table for Comparison of  TAKD with distillation alternatives on ResNet8 and ResNet14 as student and ResNet26  st . These two together establish eq. (10) \u2264 eq. (11), which means that the upper bound of error in TAKD is smaller than its upper bound in BLKD. Similarly, for the second inequality (eq. (11) \u2264 eq. (12)) one can use \u03b1 sr \u2264 \u03b1 st and \u03b1 sr \u2264 \u03b1 tr and tr + st \u2264 sr . Note that, these are asymptotic equations and hold when n \u2192 \u221e. In the finite sample regime, when |F t | C is very large, then the inequality eq. (11) \u2264 eq. (12) may not be valid and BLKD fails. Another failure case (in the finite sample regime) for BLKD happens when the student and teacher differ greatly in the capacity (i.e. \u03b1 st is very small and close to \u03b1 sr ). In this case, the error due to transfer from real to teacher outweigh (11) in comparison to (12) and the inequality becomes invalid. In this case TAKD turns to be the key. By injecting a TA between student and teacher we break the very small \u03b1 st to two larger components \u03b1 sa and \u03b1 at which makes the second inequality (eq. (10) \u2264 eq. (11)) a game changer for improving knowledge distillation.\n\n\nEmpirical Analysis\n\nWhether or not a smooth (or sharp) loss landscape is related to the generalization error, is under an active debate in the general machine learning community ). However, for the case of knowledge distillation it seems to have connections to better accuracy. It's believed that softened targets provide information on the similarity between output categories (Hinton, Vinyals, and Dean 2015). Furlanello et al. (2018) connected the knowledge distillation to a weighted/smoothed loss over classification labels. Importantly, Zhang et al. (2017) used posterior entropy and its flatness to make sense of the success of knowledge distillation. Supported by these prior works we propose to analyze the KD methods through loss landscape. In Figure 7, using a recent state of the art landscape visualization technique ) the loss surface of plain CNN on CIFAR-100 is plotted for student in three modes: (1) no knowledge distillation (NOKD), (2) baseline knowledge distillation (BLKD), (3) the proposed method (TAKD). It's seen that our network has a flatter surface around the local minima. This is related to robustness against noisy inputs which leads to better generalization.\n\n\nSummary\n\nWe studied an under-explored yet important property in Knowledge Distillation of neural networks. We showed that the gap between student and teacher networks is a key to the efficacy of knowledge distillation and the student network performance may decrease when the gap is larger. We proposed a framework based on Teacher Assistant knowledge Distillation to remedy this situation. We demonstrated the effectiveness of our approach in various scenarios and studied its properties both empirically and theoretically. Designing a fully data-driven automated TA selection is an interesting venue for future work. We also would like to make a call for research on deriving tighter theoretical bounds and rigorous analysis for knowledge distillation.\n\nFigure 2 :\n2Distillation performance with increasing teacher size. The number of convolutional layers in student is 2.\n\nFigure 4 :\n4Accuracy of training from scratch for different network sizes. The dashed red line shows the average performance of the teacher and student.\n\nFigure 5 :\n5Distillation paths for plain CNN on CIFAR-100 with T=10\n\nFigure 7 :\n7BLKD (10 \u2192 2) (c) TAKD (10 \u2192 4 \u2192 2) (d) NOKD (e) BLKD (110 \u2192 8) (f) TAKD (110 \u2192 20 \u2192 8)Loss landscape around local minima. Top) plain CNN for student of size 2. Bottom: ResNet for student of size 8.\n\nTable 1 :\n1Comparison on evaluation accuracy between our method (TAKD) and baselines. For CIFAR, plain (S=2, TA=4, T=10) and for ResNet (S=8, TA=20, T=110) are used. For ImageNet, ResNet (S=14, TA=20, T=50) is used. Higher numbers are better.Model \nDataset \nNOKD BLKD TAKD \n\nCNN \nCIFAR-10 \n70.16 \n72.57 \n73.51 \nCIFAR-100 \n41.09 \n44.57 \n44.92 \n\nResNet \nCIFAR-10 \n88.52 \n88.65 \n88.98 \nCIFAR-100 \n61.37 \n61.41 \n61.82 \n\nResNet \nImageNet \n65.20 \n66.60 \n67.36 \n\n\n\nTable 2 :\n2Student's accuracy given varied TA sizes for (S=2, \nT=10) \nModel \nDataset \nTA=8 TA=6 TA=4 \n\nCNN \nCIFAR-10 \n72.75 73.15 73.51 \nCIFAR-100 44.28 44.57 44.92 \n\n\n\nTable 3 :\n3Student's accuracy given varied TA sizes for (S=8, T=110)Model \nDataset \nTA=56 TA=32 TA=20 TA=14 \n\nResNet \nCIFAR-10 \n88.70 \n88.73 \n88.90 \n88.98 \nCIFAR-100 \n61.47 \n61.55 \n61.82 \n61.5 \n\n\n\n\nwe get|F s | C n \u03b1st + tr + st \u2264 O |F s | C n \u03b1sr + srO \n|F t | C \nn \u03b1tr + \n\n\n\n\n) holds which entails TAKD outperforms BLKD. To do so, first note that \u03b1 st \u2264 \u03b1 sa and \u03b1 st \u2264 \u03b1 at (the larger the gap means the lower rate of learning or smaller \u03b1 \u00b7\u00b7 ). Figure 6-left shows their differences. Student learning directly from teacher is certainly more difficult than either student learning from TA or TA learning from teacher. Therefore, asymptotically speaking, O |Fa| C n \u03b1 at + |Fs| C which in turn leads to O |Ft| C n \u03b1 tr + |Fa| C n \u03b1 at + |Fs| Cn \u03b1sa \n\n\u2264 O |Fs| C \n\nn \u03b1 st \n\nn \u03b1sa \n\n\u2264 \n\nO |Ft| C \nn \u03b1 tr + |Fs| C \nn \u03b1 st . Moreover, according to assumption \nof \nCodes and Appendix are available at the following address: https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation\nAppendix is available along with the code repository.\nAcknowledgementAuthors Mirzadeh and Ghasemzadeh were supported in part through grant CNS-1750679 from the United States National Science Foundation. The authors would like to thank Luke Metz, Rohan Anil, Sepehr Sameni, Hooman Shahrokhi, Janardhan Rao Doppa, and Hung Bui for their review and feedback.\nLarge scale distributed neural network training through online distillation. R Anil, G Pereyra, A Passos, R Orm\u00e1ndi, G Dahl, G Hinton, CoRR abs/1804.03235Anil, R.; Pereyra, G.; Passos, A.; Orm\u00e1ndi, R.; Dahl, G.; and Hinton, G. 2018. Large scale distributed neural network train- ing through online distillation. CoRR abs/1804.03235.\n\nDo deep nets really need to be deep. J Ba, R Caruana, NIPS. Ba, J., and Caruana, R. 2014. Do deep nets really need to be deep? In NIPS, 2654-2662.\n\nAlgorithms for hyper-parameter optimization. J Bergstra, R Bardenet, Y Bengio, B K\u00e9gl, NIPS. Bergstra, J.; Bardenet, R.; Bengio, Y.; and K\u00e9gl, B. 2011. Al- gorithms for hyper-parameter optimization. In NIPS.\n\nModel compression. C Bucila, R Caruana, A Niculescu-Mizil, SIGKDD. ACMBucila, C.; Caruana, R.; and Niculescu-Mizil, A. 2006. Model compression. In SIGKDD, 535-541. ACM.\n\nLearning efficient object detection models with knowledge distillation. G Chen, W Choi, X Yu, T Han, M Chandraker, NIPS. Chen, G.; Choi, W.; Yu, X.; Han, T.; and Chandraker, M. 2017. Learning efficient object detection models with knowledge dis- tillation. In NIPS, 742-751.\n\nW Czarnecki, S Osindero, M Jaderberg, G Swirszcz, R Pascanu, Sobolev training for neural networks. In NIPS. Czarnecki, W.; Osindero, S.; Jaderberg, M.; Swirszcz, G.; and Pascanu, R. 2017. Sobolev training for neural networks. In NIPS, 4278-4287.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, CoRR abs/1810.04805Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language un- derstanding. CoRR abs/1810.04805.\n\nT Furlanello, Z Lipton, M Tschannen, L Itti, A Anandkumar, arXiv:1805.04770Born again neural networks. arXiv preprintFurlanello, T.; Lipton, Z.; Tschannen, M.; Itti, L.; and Anand- kumar, A. 2018. Born again neural networks. arXiv preprint arXiv:1805.04770.\n\nThe capio 2017 conversational speech recognition system. K J Han, A Chandrashekaran, J Kim, I Lane, CoRR abs/1801.00059Han, K. J.; Chandrashekaran, A.; Kim, J.; and Lane, I. 2017. The capio 2017 conversational speech recognition system. CoRR abs/1801.00059.\n\nDeep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. S Han, H Mao, W J Dally, 4th International Conference on Learning Representations. Han, S.; Mao, H.; and Dally, W. J. 2016. Deep compression: Compressing deep neural network with pruning, trained quan- tization and huffman coding. In 4th International Conference on Learning Representations, ICLR 2016.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770-778.\n\nImproving knowledge distillation with supporting adversarial samples. B Heo, M Lee, S Yun, J Choi, arXiv:1805.05532arXiv preprintHeo, B.; Lee, M.; Yun, S.; and Choi, J. 2018. Improving knowl- edge distillation with supporting adversarial samples. arXiv preprint arXiv:1805.05532.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, NIPS Deep Learning and Representation Learning Workshop. Hinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, CVPR. Hu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation networks. In CVPR.\n\n. G Huang, Z Liu, L Maaten, K Weinberger, Huang, G.; Liu, Z.; Maaten, L.; and Weinberger, K. 2017. Densely connected convolutional networks. CVPR 2261-2269.\n\nPruning filters for efficient convnets. H Li, A Kadav, I Durdanovic, H Samet, H P Graf, CoRR abs/1608.08710Li, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P. 2016. Pruning filters for efficient convnets. CoRR abs/1608.08710.\n\nVisualizing the loss landscape of neural nets. H Li, Z Xu, G Taylor, C Studer, T Goldstein, NIPS. Li, H.; Xu, Z.; Taylor, G.; Studer, C.; and Goldstein, T. 2018. Visualizing the loss landscape of neural nets. In NIPS, 6391- 6401.\n\nD Lopez-Paz, L Bottou, B Sch\u00f6lkopf, V Vapnik, arXiv:1511.03643Unifying distillation and privileged information. arXiv preprintLopez-Paz, D.; Bottou, L.; Sch\u00f6lkopf, B.; and Vapnik, V. 2015. Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643.\n\nNeural network intelligence toolkit. Microsoft-Research, Microsoft-Research. 2018. Neural network intelligence toolkit.\n\nModel compression via distillation and quantization. A Paszke, S Gross, S Chintala, NIPS Autodiff Workshop. ICLRPaszke, A.; Gross, S.; Chintala, S.; and et. al. 2017. Automatic differentiation in pytorch. In NIPS Autodiff Workshop. Polino, A.; Pascanu, R.; and Alistarh, D. 2018. Model com- pression via distillation and quantization. In ICLR.\n\nA Romero, N Ballas, S Kahou, A Chassang, C Gatta, Y Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintRomero, A.; Ballas, N.; Kahou, S.; Chassang, A.; Gatta, C.; and Bengio, Y. 2014. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550.\n\nB Sau, V Balasubramanian, arXiv:1610.09650Deep model compression: Distilling knowledge from noisy teachers. arXiv preprintSau, B., and Balasubramanian, V. 2016. Deep model compres- sion: Distilling knowledge from noisy teachers. arXiv preprint arXiv:1610.09650.\n\nKickstarting deep reinforcement learning. S Schmitt, J Hudson, A Zidek, CoRR abs/1803.03835Schmitt, S.; Hudson, J.; Zidek, A.; and et. al. 2018. Kickstarting deep reinforcement learning. CoRR abs/1803.03835.\n\nC Tai, T Xiao, X Wang, E Weinan, CoRR abs/1511.06067Convolutional neural networks with low-rank regularization. Tai, C.; Xiao, T.; Wang, X.; and Weinan, E. 2015. Convo- lutional neural networks with low-rank regularization. CoRR abs/1511.06067.\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. A Tarvainen, H Valpola, NIPS. Tarvainen, A., and Valpola, H. 2017. Mean teachers are bet- ter role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NIPS.\n\nDo deep convolutional nets really need to be deep and convolutional?. G Urban, K J Geras, S E Kahou, 5th International Conference on Learning Representations. Conference Track ProceedingsUrban, G.; Geras, K. J.; Kahou, S. E.; and et. al. 2017. Do deep convolutional nets really need to be deep and convolutional? In 5th International Conference on Learning Representations, ICLR 2017, Conference Track Proceedings.\n\nStatistical learning theory. V Vapnik, Wiley3New YorkVapnik, V. 1998. Statistical learning theory. 1998, volume 3. Wiley, New York.\n\nPrivate model compression via knowledge distillation. J Wang, W Bao, L Sun, X Zhu, B Cao, P S Yu, CoRR abs/1811.05072Wang, J.; Bao, W.; Sun, L.; Zhu, X.; Cao, B.; and Yu, P. S. 2018a. Private model compression via knowledge distillation. CoRR abs/1811.05072.\n\nKdgan: Knowledge distillation with generative adversarial networks. X Wang, R Zhang, Y Sun, J Qi, NIPS. Wang, X.; Zhang, R.; Sun, Y.; and Qi, J. 2018b. Kdgan: Knowl- edge distillation with generative adversarial networks. In NIPS, 783-794.\n\nAdversarial learning of portable student networks. Y Wang, C Xu, C Xu, D Tao, AAAI. Wang, Y.; Xu, C.; Xu, C.; and Tao, D. 2018c. Adversarial learning of portable student networks. In AAAI.\n\nTraining shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks. Z Xu, Y Hsu, J Huang, 6th International Conference on Learning Representations. Xu, Z.; Hsu, Y.; and Huang, J. 2018. Training shallow and thin networks for acceleration via knowledge distillation with con- ditional adversarial networks. In 6th International Conference on Learning Representations, ICLR 2018.\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. J Yim, D Joo, J Bae, J Kim, CVPR. Yim, J.; Joo, D.; Bae, J.; and Kim, J. 2017. A gift from knowl- edge distillation: Fast optimization, network minimization and transfer learning. In CVPR, 7130-7138.\n\nLearning from multiple teacher networks. S You, C Xu, C Xu, D Tao, SIGKDD. ACMYou, S.; Xu, C.; Xu, C.; and Tao, D. 2017. Learning from multiple teacher networks. In SIGKDD, 1285-1294. ACM.\n\nVisual relationship detection with internal and external linguistic knowledge distillation. R Yu, A Li, V I Morariu, L Davis, ICCV. Yu, R.; Li, A.; Morariu, V. I.; and Davis, L. 2017. Visual re- lationship detection with internal and external linguistic knowl- edge distillation. In ICCV.\n\nNisp: Pruning networks using neuron importance score propagation. R Yu, A Li, C Chen, J Lai, V Morariu, X Han, M Gao, C Lin, L Davis, CVPR. Yu, R.; Li, A.; Chen, C.; Lai, J.; Morariu, V.; Han, X.; Gao, M.; Lin, C.; and Davis, L. 2018. Nisp: Pruning networks using neuron importance score propagation. In CVPR.\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Z , S , K , N Zhang, Y Xiang, T Hospedales, T Lu, H , arXiv:1612.03928arXiv preprintDeep mutual learning. CoRR abs/1706.00384Z, S., and K, N. 2016. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928. Zhang, Y.; Xiang, T.; Hospedales, T.; and Lu, H. 2017. Deep mutual learning. CoRR abs/1706.00384.\n", "annotations": {"author": "[{\"end\":68,\"start\":57},{\"end\":113,\"start\":69},{\"end\":149,\"start\":114},{\"end\":173,\"start\":150},{\"end\":250,\"start\":174},{\"end\":269,\"start\":251},{\"end\":362,\"start\":270},{\"end\":414,\"start\":363}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":62},{\"end\":77,\"start\":69},{\"end\":132,\"start\":122},{\"end\":156,\"start\":154},{\"end\":184,\"start\":178},{\"end\":268,\"start\":259},{\"end\":288,\"start\":277},{\"end\":378,\"start\":374}]", "author_first_name": "[{\"end\":61,\"start\":57},{\"end\":121,\"start\":114},{\"end\":153,\"start\":150},{\"end\":177,\"start\":174},{\"end\":258,\"start\":251},{\"end\":276,\"start\":270},{\"end\":371,\"start\":370},{\"end\":373,\"start\":372}]", "author_affiliation": "[{\"end\":112,\"start\":79},{\"end\":148,\"start\":134},{\"end\":172,\"start\":158},{\"end\":249,\"start\":235},{\"end\":361,\"start\":328},{\"end\":413,\"start\":380}]", "title": "[{\"end\":54,\"start\":1},{\"end\":468,\"start\":415}]", "venue": null, "abstract": "[{\"end\":1668,\"start\":470}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1816,\"start\":1797},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1839,\"start\":1816},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1914,\"start\":1894},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2306,\"start\":2280},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2348,\"start\":2331},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2419,\"start\":2376},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2450,\"start\":2419},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4876,\"start\":4850},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4891,\"start\":4876},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4929,\"start\":4913},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5142,\"start\":5106},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5160,\"start\":5142},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5388,\"start\":5345},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5440,\"start\":5408},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5885,\"start\":5868},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5900,\"start\":5885},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5920,\"start\":5900},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5937,\"start\":5920},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6060,\"start\":6043},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6084,\"start\":6060},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6102,\"start\":6084},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6108,\"start\":6102},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6251,\"start\":6231},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6549,\"start\":6538},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6565,\"start\":6549},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6710,\"start\":6687},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6876,\"start\":6848},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6956,\"start\":6937},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7176,\"start\":7146},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7334,\"start\":7315},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7609,\"start\":7592},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7747,\"start\":7729},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8920,\"start\":8888},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9133,\"start\":9109},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9350,\"start\":9331},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9602,\"start\":9579},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9707,\"start\":9695},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10381,\"start\":10349},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10417,\"start\":10399},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10435,\"start\":10417},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10480,\"start\":10460},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10890,\"start\":10853},{\"end\":10911,\"start\":10890},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16658,\"start\":16639},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17402,\"start\":17381},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18280,\"start\":18265},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23693,\"start\":23661},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23917,\"start\":23900},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23952,\"start\":23932},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24225,\"start\":24209},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24261,\"start\":24245},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24424,\"start\":24408},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24929,\"start\":24910},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25685,\"start\":25673},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26563,\"start\":26540},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28315,\"start\":28283},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29995,\"start\":29963},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30021,\"start\":29997}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31652,\"start\":31533},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31806,\"start\":31653},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31875,\"start\":31807},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32087,\"start\":31876},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32545,\"start\":32088},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32714,\"start\":32546},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32911,\"start\":32715},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32991,\"start\":32912},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33577,\"start\":32992}]", "paragraph": "[{\"end\":2764,\"start\":1684},{\"end\":3360,\"start\":2766},{\"end\":3716,\"start\":3362},{\"end\":4474,\"start\":3718},{\"end\":4594,\"start\":4491},{\"end\":5296,\"start\":4596},{\"end\":6109,\"start\":5298},{\"end\":6458,\"start\":6111},{\"end\":8138,\"start\":6460},{\"end\":8595,\"start\":8140},{\"end\":9783,\"start\":8597},{\"end\":10007,\"start\":9785},{\"end\":10481,\"start\":10075},{\"end\":10766,\"start\":10483},{\"end\":10801,\"start\":10798},{\"end\":11100,\"start\":10803},{\"end\":11352,\"start\":11130},{\"end\":11560,\"start\":11389},{\"end\":13491,\"start\":11600},{\"end\":13701,\"start\":13493},{\"end\":14195,\"start\":13703},{\"end\":14635,\"start\":14247},{\"end\":15652,\"start\":14637},{\"end\":16178,\"start\":15654},{\"end\":16605,\"start\":16201},{\"end\":17688,\"start\":16607},{\"end\":18395,\"start\":17690},{\"end\":18978,\"start\":18420},{\"end\":19606,\"start\":19022},{\"end\":19633,\"start\":19608},{\"end\":21193,\"start\":19635},{\"end\":21221,\"start\":21195},{\"end\":21620,\"start\":21223},{\"end\":21703,\"start\":21622},{\"end\":22502,\"start\":21705},{\"end\":23549,\"start\":22504},{\"end\":25496,\"start\":23596},{\"end\":25621,\"start\":25498},{\"end\":25752,\"start\":25646},{\"end\":26762,\"start\":25800},{\"end\":27110,\"start\":26810},{\"end\":27620,\"start\":27158},{\"end\":27716,\"start\":27622},{\"end\":27811,\"start\":27764},{\"end\":28069,\"start\":27859},{\"end\":29582,\"start\":28222},{\"end\":30775,\"start\":29605},{\"end\":31532,\"start\":30787}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10797,\"start\":10767},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11129,\"start\":11101},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11388,\"start\":11353},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25799,\"start\":25753},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26809,\"start\":26763},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27157,\"start\":27111},{\"attributes\":{\"id\":\"formula_6\"},\"end\":27763,\"start\":27717},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27858,\"start\":27812},{\"attributes\":{\"id\":\"formula_8\"},\"end\":28221,\"start\":28070}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19268,\"start\":19261},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19875,\"start\":19868},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20032,\"start\":20025},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20709,\"start\":20702},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28554,\"start\":28446}]", "section_header": "[{\"end\":1682,\"start\":1670},{\"end\":4489,\"start\":4477},{\"end\":10073,\"start\":10010},{\"end\":11598,\"start\":11563},{\"end\":14245,\"start\":14198},{\"end\":16199,\"start\":16181},{\"end\":18418,\"start\":18398},{\"end\":19020,\"start\":18981},{\"end\":23594,\"start\":23552},{\"end\":25644,\"start\":25624},{\"end\":29603,\"start\":29585},{\"end\":30785,\"start\":30778},{\"end\":31544,\"start\":31534},{\"end\":31664,\"start\":31654},{\"end\":31818,\"start\":31808},{\"end\":31887,\"start\":31877},{\"end\":32098,\"start\":32089},{\"end\":32556,\"start\":32547},{\"end\":32725,\"start\":32716}]", "table": "[{\"end\":32545,\"start\":32331},{\"end\":32714,\"start\":32558},{\"end\":32911,\"start\":32784},{\"end\":32991,\"start\":32968},{\"end\":33577,\"start\":33461}]", "figure_caption": "[{\"end\":31652,\"start\":31546},{\"end\":31806,\"start\":31666},{\"end\":31875,\"start\":31820},{\"end\":32087,\"start\":31889},{\"end\":32331,\"start\":32100},{\"end\":32784,\"start\":32727},{\"end\":32968,\"start\":32914},{\"end\":33461,\"start\":32994}]", "figure_ref": "[{\"end\":2609,\"start\":2601},{\"end\":3619,\"start\":3610},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11995,\"start\":11987},{\"end\":13334,\"start\":13326},{\"end\":14030,\"start\":14022},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20433,\"start\":20425},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20810,\"start\":20802},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21836,\"start\":21828},{\"end\":23747,\"start\":23736},{\"end\":28351,\"start\":28343},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30347,\"start\":30339}]", "bib_author_first_name": "[{\"end\":34141,\"start\":34140},{\"end\":34149,\"start\":34148},{\"end\":34160,\"start\":34159},{\"end\":34170,\"start\":34169},{\"end\":34181,\"start\":34180},{\"end\":34189,\"start\":34188},{\"end\":34435,\"start\":34434},{\"end\":34441,\"start\":34440},{\"end\":34591,\"start\":34590},{\"end\":34603,\"start\":34602},{\"end\":34615,\"start\":34614},{\"end\":34625,\"start\":34624},{\"end\":34774,\"start\":34773},{\"end\":34784,\"start\":34783},{\"end\":34795,\"start\":34794},{\"end\":34997,\"start\":34996},{\"end\":35005,\"start\":35004},{\"end\":35013,\"start\":35012},{\"end\":35019,\"start\":35018},{\"end\":35026,\"start\":35025},{\"end\":35201,\"start\":35200},{\"end\":35214,\"start\":35213},{\"end\":35226,\"start\":35225},{\"end\":35239,\"start\":35238},{\"end\":35251,\"start\":35250},{\"end\":35530,\"start\":35529},{\"end\":35540,\"start\":35539},{\"end\":35549,\"start\":35548},{\"end\":35556,\"start\":35555},{\"end\":35750,\"start\":35749},{\"end\":35764,\"start\":35763},{\"end\":35774,\"start\":35773},{\"end\":35787,\"start\":35786},{\"end\":35795,\"start\":35794},{\"end\":36066,\"start\":36065},{\"end\":36068,\"start\":36067},{\"end\":36075,\"start\":36074},{\"end\":36094,\"start\":36093},{\"end\":36101,\"start\":36100},{\"end\":36373,\"start\":36372},{\"end\":36380,\"start\":36379},{\"end\":36387,\"start\":36386},{\"end\":36389,\"start\":36388},{\"end\":36723,\"start\":36722},{\"end\":36729,\"start\":36728},{\"end\":36738,\"start\":36737},{\"end\":36745,\"start\":36744},{\"end\":36939,\"start\":36938},{\"end\":36946,\"start\":36945},{\"end\":36953,\"start\":36952},{\"end\":36960,\"start\":36959},{\"end\":37196,\"start\":37195},{\"end\":37206,\"start\":37205},{\"end\":37217,\"start\":37216},{\"end\":37466,\"start\":37465},{\"end\":37472,\"start\":37471},{\"end\":37480,\"start\":37479},{\"end\":37574,\"start\":37573},{\"end\":37583,\"start\":37582},{\"end\":37590,\"start\":37589},{\"end\":37600,\"start\":37599},{\"end\":37770,\"start\":37769},{\"end\":37776,\"start\":37775},{\"end\":37785,\"start\":37784},{\"end\":37799,\"start\":37798},{\"end\":37808,\"start\":37807},{\"end\":37810,\"start\":37809},{\"end\":38014,\"start\":38013},{\"end\":38020,\"start\":38019},{\"end\":38026,\"start\":38025},{\"end\":38036,\"start\":38035},{\"end\":38046,\"start\":38045},{\"end\":38198,\"start\":38197},{\"end\":38211,\"start\":38210},{\"end\":38221,\"start\":38220},{\"end\":38234,\"start\":38233},{\"end\":38645,\"start\":38644},{\"end\":38655,\"start\":38654},{\"end\":38664,\"start\":38663},{\"end\":38937,\"start\":38936},{\"end\":38947,\"start\":38946},{\"end\":38957,\"start\":38956},{\"end\":38966,\"start\":38965},{\"end\":38978,\"start\":38977},{\"end\":38987,\"start\":38986},{\"end\":39210,\"start\":39209},{\"end\":39217,\"start\":39216},{\"end\":39515,\"start\":39514},{\"end\":39526,\"start\":39525},{\"end\":39536,\"start\":39535},{\"end\":39682,\"start\":39681},{\"end\":39689,\"start\":39688},{\"end\":39697,\"start\":39696},{\"end\":39705,\"start\":39704},{\"end\":40049,\"start\":40048},{\"end\":40062,\"start\":40061},{\"end\":40319,\"start\":40318},{\"end\":40328,\"start\":40327},{\"end\":40330,\"start\":40329},{\"end\":40339,\"start\":40338},{\"end\":40341,\"start\":40340},{\"end\":40694,\"start\":40693},{\"end\":40852,\"start\":40851},{\"end\":40860,\"start\":40859},{\"end\":40867,\"start\":40866},{\"end\":40874,\"start\":40873},{\"end\":40881,\"start\":40880},{\"end\":40888,\"start\":40887},{\"end\":40890,\"start\":40889},{\"end\":41126,\"start\":41125},{\"end\":41134,\"start\":41133},{\"end\":41143,\"start\":41142},{\"end\":41150,\"start\":41149},{\"end\":41350,\"start\":41349},{\"end\":41358,\"start\":41357},{\"end\":41364,\"start\":41363},{\"end\":41370,\"start\":41369},{\"end\":41607,\"start\":41606},{\"end\":41613,\"start\":41612},{\"end\":41620,\"start\":41619},{\"end\":42016,\"start\":42015},{\"end\":42023,\"start\":42022},{\"end\":42030,\"start\":42029},{\"end\":42037,\"start\":42036},{\"end\":42258,\"start\":42257},{\"end\":42265,\"start\":42264},{\"end\":42271,\"start\":42270},{\"end\":42277,\"start\":42276},{\"end\":42499,\"start\":42498},{\"end\":42505,\"start\":42504},{\"end\":42511,\"start\":42510},{\"end\":42513,\"start\":42512},{\"end\":42524,\"start\":42523},{\"end\":42763,\"start\":42762},{\"end\":42769,\"start\":42768},{\"end\":42775,\"start\":42774},{\"end\":42783,\"start\":42782},{\"end\":42790,\"start\":42789},{\"end\":42801,\"start\":42800},{\"end\":42808,\"start\":42807},{\"end\":42815,\"start\":42814},{\"end\":42822,\"start\":42821},{\"end\":43127,\"start\":43126},{\"end\":43131,\"start\":43130},{\"end\":43135,\"start\":43134},{\"end\":43139,\"start\":43138},{\"end\":43148,\"start\":43147},{\"end\":43157,\"start\":43156},{\"end\":43171,\"start\":43170},{\"end\":43177,\"start\":43176}]", "bib_author_last_name": "[{\"end\":34146,\"start\":34142},{\"end\":34157,\"start\":34150},{\"end\":34167,\"start\":34161},{\"end\":34178,\"start\":34171},{\"end\":34186,\"start\":34182},{\"end\":34196,\"start\":34190},{\"end\":34438,\"start\":34436},{\"end\":34449,\"start\":34442},{\"end\":34600,\"start\":34592},{\"end\":34612,\"start\":34604},{\"end\":34622,\"start\":34616},{\"end\":34630,\"start\":34626},{\"end\":34781,\"start\":34775},{\"end\":34792,\"start\":34785},{\"end\":34811,\"start\":34796},{\"end\":35002,\"start\":34998},{\"end\":35010,\"start\":35006},{\"end\":35016,\"start\":35014},{\"end\":35023,\"start\":35020},{\"end\":35037,\"start\":35027},{\"end\":35211,\"start\":35202},{\"end\":35223,\"start\":35215},{\"end\":35236,\"start\":35227},{\"end\":35248,\"start\":35240},{\"end\":35259,\"start\":35252},{\"end\":35537,\"start\":35531},{\"end\":35546,\"start\":35541},{\"end\":35553,\"start\":35550},{\"end\":35566,\"start\":35557},{\"end\":35761,\"start\":35751},{\"end\":35771,\"start\":35765},{\"end\":35784,\"start\":35775},{\"end\":35792,\"start\":35788},{\"end\":35806,\"start\":35796},{\"end\":36072,\"start\":36069},{\"end\":36091,\"start\":36076},{\"end\":36098,\"start\":36095},{\"end\":36106,\"start\":36102},{\"end\":36377,\"start\":36374},{\"end\":36384,\"start\":36381},{\"end\":36395,\"start\":36390},{\"end\":36726,\"start\":36724},{\"end\":36735,\"start\":36730},{\"end\":36742,\"start\":36739},{\"end\":36749,\"start\":36746},{\"end\":36943,\"start\":36940},{\"end\":36950,\"start\":36947},{\"end\":36957,\"start\":36954},{\"end\":36965,\"start\":36961},{\"end\":37203,\"start\":37197},{\"end\":37214,\"start\":37207},{\"end\":37222,\"start\":37218},{\"end\":37469,\"start\":37467},{\"end\":37477,\"start\":37473},{\"end\":37484,\"start\":37481},{\"end\":37580,\"start\":37575},{\"end\":37587,\"start\":37584},{\"end\":37597,\"start\":37591},{\"end\":37611,\"start\":37601},{\"end\":37773,\"start\":37771},{\"end\":37782,\"start\":37777},{\"end\":37796,\"start\":37786},{\"end\":37805,\"start\":37800},{\"end\":37815,\"start\":37811},{\"end\":38017,\"start\":38015},{\"end\":38023,\"start\":38021},{\"end\":38033,\"start\":38027},{\"end\":38043,\"start\":38037},{\"end\":38056,\"start\":38047},{\"end\":38208,\"start\":38199},{\"end\":38218,\"start\":38212},{\"end\":38231,\"start\":38222},{\"end\":38241,\"start\":38235},{\"end\":38525,\"start\":38507},{\"end\":38652,\"start\":38646},{\"end\":38661,\"start\":38656},{\"end\":38673,\"start\":38665},{\"end\":38944,\"start\":38938},{\"end\":38954,\"start\":38948},{\"end\":38963,\"start\":38958},{\"end\":38975,\"start\":38967},{\"end\":38984,\"start\":38979},{\"end\":38994,\"start\":38988},{\"end\":39214,\"start\":39211},{\"end\":39233,\"start\":39218},{\"end\":39523,\"start\":39516},{\"end\":39533,\"start\":39527},{\"end\":39542,\"start\":39537},{\"end\":39686,\"start\":39683},{\"end\":39694,\"start\":39690},{\"end\":39702,\"start\":39698},{\"end\":39712,\"start\":39706},{\"end\":40059,\"start\":40050},{\"end\":40070,\"start\":40063},{\"end\":40325,\"start\":40320},{\"end\":40336,\"start\":40331},{\"end\":40347,\"start\":40342},{\"end\":40701,\"start\":40695},{\"end\":40857,\"start\":40853},{\"end\":40864,\"start\":40861},{\"end\":40871,\"start\":40868},{\"end\":40878,\"start\":40875},{\"end\":40885,\"start\":40882},{\"end\":40893,\"start\":40891},{\"end\":41131,\"start\":41127},{\"end\":41140,\"start\":41135},{\"end\":41147,\"start\":41144},{\"end\":41153,\"start\":41151},{\"end\":41355,\"start\":41351},{\"end\":41361,\"start\":41359},{\"end\":41367,\"start\":41365},{\"end\":41374,\"start\":41371},{\"end\":41610,\"start\":41608},{\"end\":41617,\"start\":41614},{\"end\":41626,\"start\":41621},{\"end\":42020,\"start\":42017},{\"end\":42027,\"start\":42024},{\"end\":42034,\"start\":42031},{\"end\":42041,\"start\":42038},{\"end\":42262,\"start\":42259},{\"end\":42268,\"start\":42266},{\"end\":42274,\"start\":42272},{\"end\":42281,\"start\":42278},{\"end\":42502,\"start\":42500},{\"end\":42508,\"start\":42506},{\"end\":42521,\"start\":42514},{\"end\":42530,\"start\":42525},{\"end\":42766,\"start\":42764},{\"end\":42772,\"start\":42770},{\"end\":42780,\"start\":42776},{\"end\":42787,\"start\":42784},{\"end\":42798,\"start\":42791},{\"end\":42805,\"start\":42802},{\"end\":42812,\"start\":42809},{\"end\":42819,\"start\":42816},{\"end\":42828,\"start\":42823},{\"end\":43145,\"start\":43140},{\"end\":43154,\"start\":43149},{\"end\":43168,\"start\":43158},{\"end\":43174,\"start\":43172}]", "bib_entry": "[{\"attributes\":{\"doi\":\"CoRR abs/1804.03235\",\"id\":\"b0\"},\"end\":34395,\"start\":34063},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11536917},\"end\":34543,\"start\":34397},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11688126},\"end\":34752,\"start\":34545},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11253972},\"end\":34922,\"start\":34754},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":29308926},\"end\":35198,\"start\":34924},{\"attributes\":{\"id\":\"b5\"},\"end\":35445,\"start\":35200},{\"attributes\":{\"doi\":\"CoRR abs/1810.04805\",\"id\":\"b6\"},\"end\":35747,\"start\":35447},{\"attributes\":{\"doi\":\"arXiv:1805.04770\",\"id\":\"b7\"},\"end\":36006,\"start\":35749},{\"attributes\":{\"doi\":\"CoRR abs/1801.00059\",\"id\":\"b8\"},\"end\":36265,\"start\":36008},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2134321},\"end\":36674,\"start\":36267},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":36866,\"start\":36676},{\"attributes\":{\"doi\":\"arXiv:1805.05532\",\"id\":\"b11\"},\"end\":37147,\"start\":36868},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7200347},\"end\":37430,\"start\":37149},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":140309863},\"end\":37569,\"start\":37432},{\"attributes\":{\"id\":\"b14\"},\"end\":37727,\"start\":37571},{\"attributes\":{\"doi\":\"CoRR abs/1608.08710\",\"id\":\"b15\"},\"end\":37964,\"start\":37729},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3693334},\"end\":38195,\"start\":37966},{\"attributes\":{\"doi\":\"arXiv:1511.03643\",\"id\":\"b17\"},\"end\":38468,\"start\":38197},{\"attributes\":{\"id\":\"b18\"},\"end\":38589,\"start\":38470},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3323727},\"end\":38934,\"start\":38591},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b20\"},\"end\":39207,\"start\":38936},{\"attributes\":{\"doi\":\"arXiv:1610.09650\",\"id\":\"b21\"},\"end\":39470,\"start\":39209},{\"attributes\":{\"doi\":\"CoRR abs/1803.03835\",\"id\":\"b22\"},\"end\":39679,\"start\":39472},{\"attributes\":{\"doi\":\"CoRR abs/1511.06067\",\"id\":\"b23\"},\"end\":39925,\"start\":39681},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2759724},\"end\":40246,\"start\":39927},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":16550689},\"end\":40662,\"start\":40248},{\"attributes\":{\"id\":\"b26\"},\"end\":40795,\"start\":40664},{\"attributes\":{\"doi\":\"CoRR abs/1811.05072\",\"id\":\"b27\"},\"end\":41055,\"start\":40797},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53976534},\"end\":41296,\"start\":41057},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19182852},\"end\":41486,\"start\":41298},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4916078},\"end\":41914,\"start\":41488},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206596723},\"end\":42214,\"start\":41916},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":26021416},\"end\":42404,\"start\":42216},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":28101867},\"end\":42694,\"start\":42406},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4142619},\"end\":43005,\"start\":42696},{\"attributes\":{\"doi\":\"arXiv:1612.03928\",\"id\":\"b35\"},\"end\":43523,\"start\":43007}]", "bib_title": "[{\"end\":34432,\"start\":34397},{\"end\":34588,\"start\":34545},{\"end\":34771,\"start\":34754},{\"end\":34994,\"start\":34924},{\"end\":36370,\"start\":36267},{\"end\":36720,\"start\":36676},{\"end\":37193,\"start\":37149},{\"end\":37463,\"start\":37432},{\"end\":38011,\"start\":37966},{\"end\":38642,\"start\":38591},{\"end\":40046,\"start\":39927},{\"end\":40316,\"start\":40248},{\"end\":41123,\"start\":41057},{\"end\":41347,\"start\":41298},{\"end\":41604,\"start\":41488},{\"end\":42013,\"start\":41916},{\"end\":42255,\"start\":42216},{\"end\":42496,\"start\":42406},{\"end\":42760,\"start\":42696}]", "bib_author": "[{\"end\":34148,\"start\":34140},{\"end\":34159,\"start\":34148},{\"end\":34169,\"start\":34159},{\"end\":34180,\"start\":34169},{\"end\":34188,\"start\":34180},{\"end\":34198,\"start\":34188},{\"end\":34440,\"start\":34434},{\"end\":34451,\"start\":34440},{\"end\":34602,\"start\":34590},{\"end\":34614,\"start\":34602},{\"end\":34624,\"start\":34614},{\"end\":34632,\"start\":34624},{\"end\":34783,\"start\":34773},{\"end\":34794,\"start\":34783},{\"end\":34813,\"start\":34794},{\"end\":35004,\"start\":34996},{\"end\":35012,\"start\":35004},{\"end\":35018,\"start\":35012},{\"end\":35025,\"start\":35018},{\"end\":35039,\"start\":35025},{\"end\":35213,\"start\":35200},{\"end\":35225,\"start\":35213},{\"end\":35238,\"start\":35225},{\"end\":35250,\"start\":35238},{\"end\":35261,\"start\":35250},{\"end\":35539,\"start\":35529},{\"end\":35548,\"start\":35539},{\"end\":35555,\"start\":35548},{\"end\":35568,\"start\":35555},{\"end\":35763,\"start\":35749},{\"end\":35773,\"start\":35763},{\"end\":35786,\"start\":35773},{\"end\":35794,\"start\":35786},{\"end\":35808,\"start\":35794},{\"end\":36074,\"start\":36065},{\"end\":36093,\"start\":36074},{\"end\":36100,\"start\":36093},{\"end\":36108,\"start\":36100},{\"end\":36379,\"start\":36372},{\"end\":36386,\"start\":36379},{\"end\":36397,\"start\":36386},{\"end\":36728,\"start\":36722},{\"end\":36737,\"start\":36728},{\"end\":36744,\"start\":36737},{\"end\":36751,\"start\":36744},{\"end\":36945,\"start\":36938},{\"end\":36952,\"start\":36945},{\"end\":36959,\"start\":36952},{\"end\":36967,\"start\":36959},{\"end\":37205,\"start\":37195},{\"end\":37216,\"start\":37205},{\"end\":37224,\"start\":37216},{\"end\":37471,\"start\":37465},{\"end\":37479,\"start\":37471},{\"end\":37486,\"start\":37479},{\"end\":37582,\"start\":37573},{\"end\":37589,\"start\":37582},{\"end\":37599,\"start\":37589},{\"end\":37613,\"start\":37599},{\"end\":37775,\"start\":37769},{\"end\":37784,\"start\":37775},{\"end\":37798,\"start\":37784},{\"end\":37807,\"start\":37798},{\"end\":37817,\"start\":37807},{\"end\":38019,\"start\":38013},{\"end\":38025,\"start\":38019},{\"end\":38035,\"start\":38025},{\"end\":38045,\"start\":38035},{\"end\":38058,\"start\":38045},{\"end\":38210,\"start\":38197},{\"end\":38220,\"start\":38210},{\"end\":38233,\"start\":38220},{\"end\":38243,\"start\":38233},{\"end\":38527,\"start\":38507},{\"end\":38654,\"start\":38644},{\"end\":38663,\"start\":38654},{\"end\":38675,\"start\":38663},{\"end\":38946,\"start\":38936},{\"end\":38956,\"start\":38946},{\"end\":38965,\"start\":38956},{\"end\":38977,\"start\":38965},{\"end\":38986,\"start\":38977},{\"end\":38996,\"start\":38986},{\"end\":39216,\"start\":39209},{\"end\":39235,\"start\":39216},{\"end\":39525,\"start\":39514},{\"end\":39535,\"start\":39525},{\"end\":39544,\"start\":39535},{\"end\":39688,\"start\":39681},{\"end\":39696,\"start\":39688},{\"end\":39704,\"start\":39696},{\"end\":39714,\"start\":39704},{\"end\":40061,\"start\":40048},{\"end\":40072,\"start\":40061},{\"end\":40327,\"start\":40318},{\"end\":40338,\"start\":40327},{\"end\":40349,\"start\":40338},{\"end\":40703,\"start\":40693},{\"end\":40859,\"start\":40851},{\"end\":40866,\"start\":40859},{\"end\":40873,\"start\":40866},{\"end\":40880,\"start\":40873},{\"end\":40887,\"start\":40880},{\"end\":40895,\"start\":40887},{\"end\":41133,\"start\":41125},{\"end\":41142,\"start\":41133},{\"end\":41149,\"start\":41142},{\"end\":41155,\"start\":41149},{\"end\":41357,\"start\":41349},{\"end\":41363,\"start\":41357},{\"end\":41369,\"start\":41363},{\"end\":41376,\"start\":41369},{\"end\":41612,\"start\":41606},{\"end\":41619,\"start\":41612},{\"end\":41628,\"start\":41619},{\"end\":42022,\"start\":42015},{\"end\":42029,\"start\":42022},{\"end\":42036,\"start\":42029},{\"end\":42043,\"start\":42036},{\"end\":42264,\"start\":42257},{\"end\":42270,\"start\":42264},{\"end\":42276,\"start\":42270},{\"end\":42283,\"start\":42276},{\"end\":42504,\"start\":42498},{\"end\":42510,\"start\":42504},{\"end\":42523,\"start\":42510},{\"end\":42532,\"start\":42523},{\"end\":42768,\"start\":42762},{\"end\":42774,\"start\":42768},{\"end\":42782,\"start\":42774},{\"end\":42789,\"start\":42782},{\"end\":42800,\"start\":42789},{\"end\":42807,\"start\":42800},{\"end\":42814,\"start\":42807},{\"end\":42821,\"start\":42814},{\"end\":42830,\"start\":42821},{\"end\":43130,\"start\":43126},{\"end\":43134,\"start\":43130},{\"end\":43138,\"start\":43134},{\"end\":43147,\"start\":43138},{\"end\":43156,\"start\":43147},{\"end\":43170,\"start\":43156},{\"end\":43176,\"start\":43170},{\"end\":43180,\"start\":43176}]", "bib_venue": "[{\"end\":34138,\"start\":34063},{\"end\":34455,\"start\":34451},{\"end\":34636,\"start\":34632},{\"end\":34819,\"start\":34813},{\"end\":35043,\"start\":35039},{\"end\":35306,\"start\":35261},{\"end\":35527,\"start\":35447},{\"end\":35850,\"start\":35824},{\"end\":36063,\"start\":36008},{\"end\":36453,\"start\":36397},{\"end\":36755,\"start\":36751},{\"end\":36936,\"start\":36868},{\"end\":37279,\"start\":37224},{\"end\":37490,\"start\":37486},{\"end\":37767,\"start\":37729},{\"end\":38062,\"start\":38058},{\"end\":38307,\"start\":38259},{\"end\":38505,\"start\":38470},{\"end\":38697,\"start\":38675},{\"end\":39044,\"start\":39011},{\"end\":39315,\"start\":39251},{\"end\":39512,\"start\":39472},{\"end\":39791,\"start\":39733},{\"end\":40076,\"start\":40072},{\"end\":40405,\"start\":40349},{\"end\":40691,\"start\":40664},{\"end\":40849,\"start\":40797},{\"end\":41159,\"start\":41155},{\"end\":41380,\"start\":41376},{\"end\":41684,\"start\":41628},{\"end\":42047,\"start\":42043},{\"end\":42289,\"start\":42283},{\"end\":42536,\"start\":42532},{\"end\":42834,\"start\":42830},{\"end\":43124,\"start\":43007}]"}}}, "year": 2023, "month": 12, "day": 17}