{"id": 459456, "updated": "2023-09-27 23:05:48.997", "metadata": {"title": "Deep Learning Face Attributes in the Wild", "authors": "[{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Xiaogang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xiaoou\",\"last\":\"Tang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2014, "month": 11, "day": 27}, "abstract": "Predicting face attributes from web images is challenging due to background clutters and face variations. A novel deep learning framework is proposed for face attribute prediction in the wild. It cascades two CNNs (LNet and ANet) for face localization and attribute prediction respectively. These nets are trained in a cascade manner with attribute labels, but pre-trained differently. LNet is pre-trained with massive general object categories, while ANet is pre-trained with massive face identities. This framework not only outperforms state-of-the-art with large margin, but also reveals multiple valuable facts on learning face representation as below. (1) It shows how LNet and ANet can be improved by different pre-training strategies. (2) It reveals that although filters of LNet are fine-tuned by attribute labels, their response maps over the entire image have strong indication of face's location. This fact enables training LNet for face localization with only attribute tags, but without face bounding boxes (which are required by all detection works). With a novel fast feed-forward scheme, the cascade of LNet and ANet can localize faces and recognize attributes in images with arbitrary sizes in real time. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training, and such concepts are significantly enriched after fine-tuning. Each attribute can be well explained by a sparse linear combination of these concepts. By analyzing such combinations, attributes show clear grouping patterns, which could be well interpreted semantically.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1411.7766", "mag": "2949886837", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/LiuLWT15", "doi": "10.1109/iccv.2015.425"}}, "content": {"source": {"pdf_hash": "ff7abee907aa3ae3ba00fba89c1a7e6f969ca40a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1411.7766v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1411.7766", "status": "GREEN"}}, "grobid": {"id": "731b1cbf7ee3e78fd0ccf1570d3fa73e4a3796f8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ff7abee907aa3ae3ba00fba89c1a7e6f969ca40a.txt", "contents": "\nDeep Learning Face Attributes in the Wild\n\n\nZiwei Liu \nDepartment of Information Engineering\nThe Chinese University of Hong\nKong\n\nPing Luo \nDepartment of Information Engineering\nThe Chinese University of Hong\nKong\n\nXiaogang Wang xgwang@ee.cuhk.edu.hk \nDepartment of Electronic Engineering\nThe Chinese University of Hong Kong\n\n\nXiaoou Tang xtang@ie.cuhk.edu.hk \nDepartment of Information Engineering\nThe Chinese University of Hong\nKong\n\nDeep Learning Face Attributes in the Wild\n\nPredicting face attributes from web images is challenging due to background clutters and face variations. A novel deep learning framework is proposed for face attribute prediction in the wild. It cascades two CNNs (LNet and ANet) for face localization and attribute prediction respectively. These nets are trained in a cascade manner with attribute labels, but pre-trained differently. LNet is pretrained with massive general object categories, while ANet is pre-trained with massive face identities. This framework not only outperforms state-of-the-art with large margin, but also reveals multiple valuable facts on learning face representation as below.(1) It shows how LNet and ANet can be improved by different pre-training strategies.(2) It reveals that although filters of LNet are fine-tuned by attribute labels, their response maps over the entire image have strong indication of face's location. This fact enables training LNet for face localization with only attribute tags, but without face bounding boxes (which are required by all detection works). With a novel fast feed-forward scheme, the cascade of LNet and ANet can localize faces and recognize attributes in images with arbitrary sizes in real time.(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pretraining, and such concepts are significantly enriched after fine-tuning. Each attribute can be well explained by a sparse linear combination of these concepts. By analyzing such combinations, attributes show clear grouping patterns, which could be well interpreted semantically.\n\nIntroduction\n\nFace attributes, such as expression, race, and hair style, are beneficial for many applications such as image tagging [19] and face verification [12]. Predicting face attributes * For more technical details of this work, please send email to pluo. lhi@gmail.com from images on the web is challenging, because of large background clutters and face variations, such as scale, pose, and illumination, as shown in Fig.1. Existing methods [12,2,1,15,26] for attribute recognition first detect faces and their landmarks, and then extract high-dimensional features, such as HOG [3] or LBP [17], from image patches centered on landmarks. These features are concatenated to train classifiers. Although this pipeline is suitable for controlled environment, it has drawbacks when dealing with web images. It heavily depends on the precision of face and landmark detections, which are not reliable in web images. Fig.1 (a) shows the results of state-of-the-art face detection [14] and alignment [22] and the attribute predictions of HOG (landmark)+SVM on challenging images. Most of them fail because features are extracted at wrong landmark positions. Face detection also has ambiguity. In the third image of Fig.1 (a), the face detector confuses the cat face and the human face, as they appear similarly in the HOG space.\n\nThis work proposes a novel deep learning framework for face attribute prediction in the wild, and its novelty are in three aspects. Firstly, it does not rely on face and landmark detection. Instead, it cascades two CNNs, ...  in which one (LNet) to locate face region and the other (ANet) to extract high-level face representation from the entire located face region (without landmarks) for attribute prediction. Training LNet and ANet is in a weakly supervised manner, i.e. only attribute tags of training images are provided. This is fundamentally different from training face and landmark detectors, where face bounding boxes and landmark positions are needed. It makes the preparation of training data much easier. LNet and ANet are first pretrained differently and then jointly trained with attribute labels.\n\nSecondly, different pre-training and fine-tuning strategies are designed for LNet and ANet. Different from training face detectors with positive (face) and negative (nonface) samples, LNet is pre-trained by classifying massive general object categories. Thus, its pre-trained features have good generalization capability on handling various background clutters. LNet is then fine-tuned by predicting attributes. Features learned by attribute prediction can capture rich face variations and are effective for face localization. It also can better distinguish subtle differences between human faces and analogous patterns, such as a cat face. ANet is pre-trained by classifying massive face identities, to obtain discriminative face representation. Then it is fine-tuned by the attribute prediction task.\n\nThirdly, to make face localization and attribute prediction realtime, a fast feed-forward scheme is proposed. It evaluates web image with arbitrary size. If filters are globally shared, this can be done by convolving images with filters. It becomes non-trivial if the filters are locally shared, while studies [24,23] showed that locally shared filters perform better in face related tasks. This is solved by proposing an interweaved operation.\n\nBesides proposing new methods, our framework also reveals valuable facts on learning face representation. They not only motivate this work but also benefit future research on face and deep learning.\n\n(1) It shows how supervised pre-training with massive object categories and massive identities can improve feature learning of LNet and ANet for face localization and attribute recognition, respectively.\n\n(2) It demonstrates that although filters of LNet are finetuned by attribute prediction, their response maps over the entire image have strong indication of face's location. Good features for face localization should be able to capture rich face variations, and more supervised information on these variations improves the learning process. To understand, one could consider the examples in Fig.2. If only a single detector [14,16] is used to classify all the positive and negative samples in Fig.2 (a), it is difficult to handle complex face variations. Therefore, multi-view face detectors [8] were developed in Fig.2 (b), i.e. face images in different views are handled by different detectors. View labels were used in training detectors and the whole training set is divided into subsets according to views. If views are treated as one type of face attributes, learning face representation by predicting attributes with deep models actually extends this idea to extreme. As shown in Fig.2 (c), a filter (or a group of filters) functions as a detector of an attribute. When a subset of neurons are activated, they indicate the existence of face images, which have a particular attribute configuration. The neurons at different layers can form many activation patterns, implying that the whole set of face images can be divided into many subsets based on attribute configurations, and each activation pattern corresponds to one subset (e.g. 'pointy nose', 'rosy cheek', and 'smiling'). Therefore, it is not surprising that filters learned by attribute prediction lead to effective representations for face localization. By simply averaging and thresholding response maps, good face localization is achieved.\n\n(3) This framework also discloses that the high-level hidden neurons of ANet after pre-training implicitly learn and discover sematic concepts that are related to identity, such as race, gender, and age. These concepts are significantly expanded after fine-tuning for attribute classification. This fact indicates that when a deep model is pre-trained for face recognition, it is also implicitly learning attributes. The performance of attribute prediction drops without the pretraining stage. With this strategy, each face attribute is well explained by a sparse linear combination of these sematic concepts. By analyzing the coefficients of such combinations, attributes show clear grouping patterns, which could be well interpreted semantically.\n\nThe main contributions are summarized as follows.\n\n(1) We propose a novel deep learning framework for face localization and attribute prediction. Two cascaded CNNs are trained in a weakly supervised manner, which makes it easier to prepare training data from web images. Its fast feed-forward scheme can evaluate on web image of arbitrary size. It achieves state-of-the-art attribute classification results on both the challenging CelebFaces [23] and LFW [9] datasets, improving existing methods by 8 and 13 percent, respectively. (2) Our study reveals multiple valuable facts on leaning face representation by deep models. (3) We also contribute a large facial attribute database, consisting of over two hundred thousand images, each of which is annotated with 40 attributes. It has more than eight million attribute labels and is 20 times larger than  the largest publicly available dataset.\n\n\nRelated Work\n\nAttribute Inference by Hand-crafted Features Extracting hand-crafted features, such as HOG [3], LBP [17], and GIST [18], at pre-defined landmarks is a standard step in attribute recognition. Farhadi et al. [6] combined HOG and color histogram to train logistic regression for object search and tagging based on attributes. Kumar et al. [12] extracted HOG-like features on various face regions to tackle attribute classification and face verification. To improve the discriminativeness of hand-crafted features given a specific task, Bourdev et al. [2] built a three-level SVM system to extract higher-level information. Berg et al. [1] combined various hand-crafted features to obtain an intermediate representation for a particular domain.\n\nAttribute Inference by Deep Models Recently, deep learning methods [20,5,26,25,10] achieved great success in attribute inference, due to their ability to learn compact and discriminative features. Razavian et al. [20] and Donahue et al. [5] demonstrated that off-the-shelf features learned by the Convolutional Network (CNN) of ImageNet [10] can be effectively adapted to attribute classification. Zhang et al. [26] showed that better performance can be achieved by ensembling learned features of multiple posenormalized CNNs. Specific network structures have also been designed for attribute prediction. Luo et al. [15] introduced a deep sum-product architecture to account for occlusion during attribute inference. The main drawback of the above methods is that they heavily rely on accurate landmark detection and pose estimation in both training and testing steps. Even though a recent work [25] can perform automatically part localization during testing, it still requires landmark annotations of the training data.\n\n\nOur Approach\n\nFramework Overview The process of attribute inference consists of four stages by cascading LNet o , LNet s , and ANet. In the first stage, given a face image x o with arbitrary size, x o \u2208 R m\u00d7n , LNet o calculates a response map h (5) o , which indicates a location of head-shoulder, as shown in Fig.3 (a). x o is then combined with h (5) o to crop the region of head-shoulder, denoted as x s . Fig.3 (b) presents the second stage, where LNet s utilizes x s as input and outputs a response map h (5) s , which designates a region of face. Similarly, h (5) s is combined with x s to locate the face region x f . Note that the high responses (red) in these maps of LNet o and LNet s mainly correspond to the head-shoulder and face, demonstrating that they are robust to background clutter. The above two stages are cascaded to propose face location in a coarse-to-fine manner. In the third stage shown in Fig.3 (c), ANet is applied on the face region x f to extract response map h (4) f . FC is a fully-connected layer to classify attributes y. High responses in these maps are associated to different facial components, implying that ANet is able to capture subtle face differences, such as shapes of lips and eyebrows. In the last stage as illustrated in Fig.3 (d), several candidate windows are selected to pool the feature vectors by FC. Then these features are concatenated as h a to train linear classifier for attribute recognition.\n\nMore specifically, the network structures of LNet o and LNet s are the same as shown in Fig.3 (a) and (b), which stack two max-pooling and five convolutional layers (C1 to C5) with globally shared filters. These filters are recurrently applied at every location of the image and are able to account for large face translation and scaling. ANet stacks four convolutional layers (C1 to C4), three max-pooling layers, and one fully-connected layer (FC), where the filters at C1 and C2 are globally shared, while the filters at C3 and C4 are locally shared. As shown in Fig.3 (c), the response maps at C2 and C3 are divided into grids with non-overlapping cells, each of which learns different filters.\n(a.1) (a.2) (b.1) (b.2) (b.3) (b.4)\nThe locally shared filters have been proved effective for face related problems [24,23], because they can capture different information from different face parts. The network structures are specified in Fig.3. For instance, the filters at C1 of LNet o has 96 channels and the filter size in each channel is 11 \u00d7 11 \u00d7 3, as the input image x o contains three color channels.\n\n\nCoarse-to-fine Face Localization\n\nBoth of LNet o and LNet s have five convolutional layers, each of which utilizes the output of the previous layer as input and is formulated as\nh v(l) = relu(b v(l) + u k vu(l) * h u(l\u22121) ),(1)\nwhere relu(x) = max(0, x) is the rectified linear function [10] and * denotes the convolution operator. h u(l\u22121) and h v(l) stand for the u-th input channel at the l \u2212 1 layer and the v-th output channel at the l layer, respectively. k vu(l) and b v(l) denote the filters and bias. For instance, each output channel v of h\n(5) o is achieved by h v(5) o = relu(b v(5) + 384 u=1 k vu(5) * h u(4) ), v = 1, 2, ..., 256.\nThe max-pooling operations at C1 and C2 partition the feature maps into grid with overlapping cells, which are formulated as\nh v(l) (i,j) = max \u2200(p,q)\u2208\u2126 (i,j) {h v(l) (p,q) }.(2)\nHere, \u2126 (i,j) indicates the cell with index (i, j) and (p, q) is a position index within \u2126. The maximum value is pooled over each small cell as expressed in Eqn. (2).\n\nAfter we obtain the response map, for example h   Figure 5. Illustration of the interweaved operation.\n(a) (b) (c) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )\nwe may have multiple regions with evenly high responses, as shown in Fig.4 (a.1). Therefore, we devise an fast density peak identifying technique. Similar to [21], it calculates a special geodesic distance for each position i in h (5) o :\nd i = \u03c1 2 i + \u03c3 2 i (3)\nwhere \u03c1 i is the density intensity in position i, \u03c3 i = min j:\u03c1j >\u03c1i (s ij ) and s ij is the spatial distance between position i and position j. \u03c3 i measures its distance to the nearest position which has a larger density intensity. Then the density peaks are identified by selecting extreme large d i . This process can be further accelerated, as h (5) o is sparse. Fig.4 (a.2) outlines the picks of the density map. We can propose the correct window by cropping the region with the highest density. Note that the face image x f can be cropped in a similar fashion as above.\n\n\nFeature Extraction\n\nANet utilizes the estimated face region x f as input. As shown in Fig.3 (c), the filters of C1 and C2 in ANet are globally shared and can be formulated in the same way as Eqn. (1) and (2). The locally shared filters at C3 and C4 are learned to capture different local information in the specific facial regions (cells). For example, the highlighted cells in purple and black (Fig.3 (c)) are corresponded to left eye and left mouth corner respectively. These locally shared filters can be formulated as\nh v(l) (p,q) = relu(b v(l) (p,q) + u k vu(l) (p,q) * h u(l\u22121) (p,q) ),(4)\nwhere (p, q) is the cell index. However, as shown in Fig.3 (c), the estimated face region x f is not well-aligned, because of the large variation presented in the web image. If we simply apply Eqn.(4), the subsequent face features may contain noise. A simple solution is to densely crop image patches and apply ANet on each of them, but there are redundant computations (e.g. C1 and C2). Therefore, we propose interweaved operation, which can account for misalignment without cropping multiple patches.\n\nTo better visualize the process, the network structure of C2, C3 and C4 is again illustrated in Fig.5 (a), where each filter in C3 corresponds to four local regions in C2.\n\nThese regions can be overlapped. And the same relationship applies between C4 and C3. For clarity, we consider four filters k\n(3) 1 , k (3) 2 , k(3)\n3 and k (3) 4 in C3 and one filter k (4) 1 in C4. We assume there is only one channel.\n\nAfter obtaining response map h (2) in C2, we apply each filter in C3 using Eqn.(1) to the entire response map h (2) , resulting in the response maps h \n(3) 1 , h (3) 2 , h(3)\n4 , as shown in Fig.5 (b). In the next step, we need to apply k (4) 1 to these maps. Difficulty exists because filters in C3 have spatial relationship. For instance, the response of k \n\ninter , is constructed as depicted in Fig.5 (c), where responses in the same cell are padded together.\n\nThen, the feature map of C4 is calculated as standard convolution using Eqn.(1) as h\n(4) 1 = relu(k (4) 1 * h (3) inter ).\nSimilarly, we could get feature maps h (4) i for other locally shared filters in C4.\n\nSince we assume filters in C4 have one channel, the redundant parts in h (4) i are filter responses at other possible spatial positions. To find desired position, we construct interweaved map h (4) inter of C4 to preserve geometric constraint and search for its maximum component:\nh (4) = arg max \u2126 (i,j) max \u2200(p,q)\u2208\u2126 (i,j) {h (4) inter (\u2126 (i,j) )}(5)\nThe whole process could be viewed as implicitly combining different part detectors (locally shared filters) under geometric constrains (interweaved operation) to facilitate accurate localization. Fig.4 (b.1-4) demonstrate misaligned faces could be localized. Then we properly crop and pool at different positions to generate multiple views of h (4) . It could further suppress residual misalignment and achieve size fit for fully connected layer. Feeding these multi-view response maps into FC would lead to multiview representations of face region. We concatenate all multi-view representations together to obtain the final face representation h a .\n\n\nLearning Algorithms\n\nWe introduce the pre-training and fine-tuning methods for LNet o , LNet s , and ANet, respectively. Since the convolutional structures of LNet o and LNet s are the same, their filters are initialized by a single network, denoted as LNet+, which is trained with massive general object categories. Then LNet o and LNet s are fine-tuned separately by face attributes on full image x o and head-shoulder x s , respectively. ANet is pre-trained by large amount of face identities and then fine-tuned by face attributes on x f .\n\nPre-training of LNet+ We adopt 1, 000 general object categories, following the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [4], where 1.2 million training images and 50 thousands validation images are released. Each image has image-level annotation, indicating whether an object is presented or not. All these data are employed for pre-training except one third of the validation data for choosing hyper-parameters. The convolutional structures (C1 to C5) of LNet+ is designed in the same way as LNet o and LNet s . We add two fully-connected hidden layers on top of C5 in order to improve the non-linearity for classification. This is inspired by AlexNet [10] for object recognition. As a result, LNet+ contains five convolutional layers, two fully-connected layers of 4096 dimensions, and one fully connected layer of 1000 dimensions, indicating the probabilities of the presences of object categories. Following previous works [10], we augment data by cropping ten patches from each image, including one patch in the center and four patches at the corners, and their horizontal flips. We adopt the softmax loss to pre-train LNet+, which can be optimized by stochastic gradient descent (SGD) with back-propagation [13].\n\nPre-training of ANet ANet is pre-trained using eight thousand face identities, selecting from the CelebFaces [23] dataset, where each identity has around twenty images. We have over 160 thousand images in total. We follow [23] to preprocess and augment these data. A simple way to train ANet is done by classifying eight thousand classes, using the softmax loss as above. However, this problem is challenging because the number of samples of each identity is limited to maintain the intra-class invariance. To improve intra-class invariance, we employ the similarity loss similar to [23,7]. This loss decreases the distances between samples of the same identity. We have 2 2 , where f (I i ) and f (I j ) denote the features vector of the i-th and j-th samples respectively, and y i = y j indicates the identities of these samples are the same. In summary, ANet is pre-trained by combining the softmax loss and the similarity loss.  \nL = |D| i=1,yi=yj f (I i ) \u2212 f (I j )\n\nExperiments\n\nLarge-scale Data Collection To evaluate the proposed approach, we construct two face attribute datasets, namely CelebA and LFWA, by labeling images selected from two challenging face datasets, CelebFaces [23] and LFW [9]. CelebA contains ten thousand identities, each of which has twenty images. There are two hundred thousand images in total. LFWA has the same scale of LFW, which contains 5, 749 identities with totaly 13, 233 images. Each image in CelebA and LFWA is annotated with forty face attributes and five key points by a professional labeling company. In summary, CelebA and LFWA have over eight million and five hundred thousand attribute labels respectively.\n\nWe partition CelebA into three parts, including a subset to pre-train and fine-tune ANet, a subset to train SVM for attribute prediction, and the remaining subset for testing. More precisely, the images of the first eight thousand identities (i.e. 160 thousand images) are used to pre-train and finetune ANet and the images of another one thousand identities (i.e. twenty thousand images) are employed to train SVM. The images of the remaining one thousand identities (i.e. twenty thousand images) are used for testing. For the LFWA, we partition the data into half for training and half for testing. Specifically, 6, 263 images are adopted to train SVM and the remaining images for testing. To further exam whether the proposed approach can be generalized to unseen attributes, we manually label 30 more attributes for the testing images, denoted as LFWA+. The attributes presented in LFWA+ do not appear in LFWA.\n\nMethods for Comparisons The proposed method is compared with three competitive approaches, including FaceTracer [11], PANDA-w [26], and PANDA-l [26]. The first method extracts HOG and color histogram on several important functional face regions and then trains SVM for attribute classification. To enable it to evaluate on web image, we calculate these functional regions referring to   the ground truth landmark points. The last two methods based on PANDA [26], which was proposed recently for human attribute recognition by ensembling multiple CNNs, each of which extracts features from a well-aligned human part. These features are concatenated to train SVM for attribute recognition. It is straight forward to adapt this method to face attributes, since face parts can be wellaligned by landmark points. Here, we consider two settings, PANDA-w and PANDA-l. The former one obtains the face parts by applying the state-of-the-art face detection [14] and alignment [22] on the wild image, while the latter one attains the face parts by ground truth landmark points. To allow a fair comparison, all the above methods are trained using the same data as the proposed approach.\n\n\nEffectiveness of the Framework\n\nThis section demonstrates the effectiveness of the framework. All experiments are done on CelebA.\n\n\u2022 LNet To show how pre-training improves face localization, we compare LNet with pre-training and without pretraining on the recall rates of face localization with respect to different overlapping ratios. The results are provided in Fig.7 (a), where shows that LNet (pretrain) significantly  Figure 8. The effectiveness of pre-training and fine-tuning ANet.  Figure 9. Comparisons of the accuracies of attribute recognition between different layers of ANet.\n\noutperforms LNet (without-pretrain) by 74 percent when the overlapping ratio equals 0.5. To further demonstrate its effectiveness, we compare LNet (pretrain) with two stateof-the-art face detectors, SURF [14] and DPM [16]. At the 0.5 overlapping ratio, LNet (pretrain) archives recall rate better than [14] and [16] by 23 and 11 percent, respectively. Several response maps are given in Fig.7 (b), where illustrates the benefit of pre-training. Response maps with pre-training generally are robust to background clutters, existence of body parts and face variations. We also demonstrate more response maps of LNet under different circumstances in Fig.6 (a). Despite some failure cases ( Fig.6 (b)) due to extreme pose, large occlusion, and low resolution, LNet accurately localize face regions in the wild. More examples are provided in Fig.14.\n\n\u2022 ANet We analyze ANet in three aspects.\n\n\nPre-training Discovers Semantic Concepts\n\nWe show that the pre-training of ANet can implicity discover semantic concepts, which are related to face identity. To this end, given a hidden neuron at the FC layer of ANet as shown in Fig.3 (c), we partition the face images into three groups, including the face images with high, medium, and low responses at this neuron. The face images of each group are then averaged to obtain the mean face. As illustrated in Fig.8 (a), we visualize these mean faces for serval neurons. Interestingly, these mean face changes smoothly from high response to low response, following a high-level concept. Human can easily assign each neuron a semantic concept it measures (i.e. the text in yellow). For example, the neurons in (a.1) and (a.4) correspond to 'gender' and 'race', respectively. This effect reveals that the high-level hidden neurons of ANet can implicitly learn to discover semantic concepts, even though they are only optimized for face recognition using identity information. We also observe that most of these concepts are intrinsic to face identity, such as the shape of facial components, age, gender, and race.\n\nTo better explain this phenomena, we compare accuracy of attribute prediction using features at different layers of ANet right after pre-training. They are FC, C4, and C3. The forty attributes are roughly separated to two groups, which are identity-related attributes, such as gender and age, and identity-non-related attributes, for example attributes of expressions, shapes, colors, and textures. We select some representative attributes for each group and plot the results in Fig.9, where shows that the features' performance of FC outperforms C4 and C3 in the group of identity-related attributes, but they are relatively weak when dealing with identity-non-related attributes. This is because the top layer FC learns identity features, which are insensitive to face variations.\n\nFine-tuning Expands Semantic Concepts In the above, we show that the pre-training of ANet essentially discovers semantic concepts related to identity. Here, as illustrated in Fig.8 (b), we show that after fine-tuning, ANet can expand these concepts to more attribute types. The last five columns of Fig.8 (b) visualizes the neurons at the FC layer, which are ranked by their responses in descending order with respect to several test images. Similar to the above, human can assign semantic meaning to each of these neurons. We found that large number of new concepts can be observed. Remarkably, these neurons express diverse high-level meanings and cooperate to explain the test images. The activations of all neurons are visualized in the second column of Fig.8 (b), showing that they are sparse. In some sense, the attributes presented in each test image is explained by a sparse linear combination of these Table 1. Performance Comparison of FaceTracer [11], PANDA-w [26], PANDA-l [26] and LNets+ANet on CelebA and LFWA concepts. For instance, the first image is described by \"a lady with big bang, brown hair, pale skin, narrow eyes, and high cheekbone\", which completely matches the human perception.\n\nAttribute Grouping Here we show that the weight matrix at the FC layer of ANet can implicitly capture relations between attributes. Each column vector of the weight matrix can be viewed as a decision hyperplane to partition the negatives and positive samples of an attribute. By simply applying k-means to these vectors, the clusters show clear grouping patterns, which can be interpreted semantically. As shown in Fig.10 Figure 11. Attribute-specific regions discovery.\n\ntributes capture information from different regions of face.\n\nWe show that ANet automatically learn to discover these regions. Given an attribute, by inspecting the learned weights and filters that have large influence to the firing (prediction) of it, we can locate important region of this attribute. Fig.11 shows some examples. The important regions of some attributes are locally distributed, such as 'Narrow Eyes', 'Big Nose', and 'Wearing Hat', but some are globally distributed, such as 'Attractive'.\n\n\nAttribute Prediction\n\nPerformance Comparison The classification accuracies of attribute prediction are reported in Table 1. On CelebA, the averaged classification accuracies of FaceTracer [11], PANDA-w [26], PANDA-l [26] and our LNets+ANet are 81, 79, 85 and 87 percent respectively, while the cor-60% 65% 70% 75% 80% 85% 90% 95% 100% FaceTracer [11] PANDA-w [26] PANDA-l [26] LNets+ANet Accuracy Figure 12. Performance comparison of FaceTracer [11], PANDAw [26], PANDA-l [26] and LNets+ANet on LFWA+. responding accuracies on LFWA are 74, 71, 81 and 84 percent. Our method outperforms PANDA-w by nearly 10 percent. Remarkably, even when PANDA-w is equipped with ground truth bounding box and landmark positions, our method still achieve 3 percent advantage. The strength of our method is illustrated not only on fine-grained facial traits, e.g. 'Mustache' and 'Pointy Nose', but also on globally-perceived attributes, e.g. 'Chubby' and 'Young'. Neurons with various concepts help to represent and distinguish different human-namable semantics. We also examine the case of providing ANet with the localized face region, but without pre-training. The averaged accuracies are 83 and 79 percent on CelebA and LFWA, which indicate pre-training with massive facial identities is effective.\n\nPerformance on LFWA+ This experiment shows that the proposed approach can be generalized to attributes, which are not presented in the training stage. Fig.12 reports the results. LNets+ANet outperforms the other three approaches (FaceTracer, PANDA-w and PANDA-l) by 8, 10 and 3 percent on average, respectively. It demonstrates that our method learns discriminative face representations and has good generalization ability.\n\nSize of Training Dataset We compare the attribute prediction accuracy of the proposed method with the accuracy of PANDA-l, regarding different sizes of training datasets. Fig.13 demonstrates that our method performs well when dataset size is small, but the performance of PANDA-l drops significantly.\n\nTime Complexity For a 300 * 300 image, our method takes 35ms to localize face region and 14ms to extract feature in average on GPU. It has large potential in real-world applications.\n\n\nConclusion\n\nThis paper has proposed a novel deep learning framework for face attribute prediction in the wild. With carefully designed pre-training strategies, our method is robust to background clutters and face variations. We devise a new feed forward technique for locally shared filters, which enables evaluating image with arbitrary size in realtime. We have also revealed several important facts about learning face representation, which shed a light on new directions of face detection and learning face representation.\n\nFigure 1 .\n1(a) Inaccurate detection and alignment lead to prediction errors on attributes by existing approaches. (b) Our LNet localizes face regions by averaging the response maps of attribute filters. ANet predicts attributes without face alignment.\n\n\na) single detector (b) multi-view detector (c) face localization by attributes ...\n\nFigure 2 .\n2Face localization by attributes.\n\nFigure 3 .\n3The proposed pipeline of attribute inference.\n\nFigure 4 .\n4(a) Pruning responses when multiple faces are presented. (b) Our feature extraction scheme can handle large face translation.\n\n\nissue is how to crop the patch of head-shoulder from x o . A simple solution is cropping the region with the responses in h\n\n\nthan a threshold. However, difficulty exists when multiple faces are presented, such that\n\nFine\n-tuning All the filters of LNet o and LNet s are initialized by LNet+ after pre-training. Then LNet o , LNet s , and ANet are fine-tuned by attribute classification, where LNet o adopts the full image x o as input, LNet s uses the the image of head-shoulder x s as input, while ANet employs the estimated face region x f as input. Similar to the pre-training of LNet+, we add two fully-connected layers to both LNet o and LNet s , where the weight matrices are initialized randomly. We adopt the cross-entropy loss to solve attribute classification for these nets, i.e. L = |D| i=1 (y i log p(y i |I i ) + (1 \u2212 y i ) log(1 \u2212 p(y i |I i ))), where p(y i = c|I i ) = 11+exp(\u2212f (Ii)) is the sigmoid function. It can be optimized by the SGD with back-propagation[13].\n\nFigure 6 .\n6(a) Examples of LNet response maps. (b) Some failure cases.\n\nFigure 7 .\n7(a) Comparisons of face localization performances. (b) The effectiveness of pre-training LNet.\n\nFigure 10 .\n10Attribute grouping.\n\nFigure 13 .\n13Performances of different sizes of training dataset.\n\n3 and h\nand\n\n\n, Group #1, Group #2 and Group #4 demonstrate co-occurrence relationship between attributes, e.g. 'Attractive' and 'Heavy Makeup' have high correlation. Attributes in Group #3 share similar color descriptors, while attributes in Group #6 correspond to certain texture and appearance traits.Attribute-specific Region Discovery Different at-Attractive \n\nBlond \nHair \n\nWearing \nHat \nEyeglasses \n\nGoatee \n\nSmiling \nOval \nFace \n\nBig \nNose \n\nNarrow \nEyes \n\n\n\nPoof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation. T Berg, P N Belhumeur, CVPR. T. Berg and P. N. Belhumeur. Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation. In CVPR, pages 955-962, 2013.\n\nDescribing people: A poselet-based approach to attribute classification. L Bourdev, S Maji, J Malik, ICCV. L. Bourdev, S. Maji, and J. Malik. Describing people: A poselet-based approach to attribute classification. In ICCV, pages 1543-1550, 2011.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, CVPR. 1N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, volume 1, pages 886-893, 2005.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255, 2009.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, arXiv:1310.1531arXiv preprintJ. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.\n\nDescribing objects by their attributes. A Farhadi, I Endres, D Hoiem, D Forsyth, CVPR. A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, pages 1778-1785, 2009.\n\nDimensionality reduction by learning an invariant mapping. R Hadsell, S Chopra, Y Lecun, CVPR. 2R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, volume 2, pages 1735-1742, 2006.\n\nVector boosting for rotation invariant multi-view face detection. C Huang, H Ai, Y Li, S Lao, ICCV. 1C. Huang, H. Ai, Y. Li, and S. Lao. Vector boosting for rotation invariant multi-view face detection. In ICCV, volume 1, pages 446-453, 2005.\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. G B Huang, M Ramesh, T Berg, E Learned-Miller, AmherstUniversity of MassachusettsTechnical Report 07-49G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Re- port 07-49, University of Massachusetts, Amherst, October 2007.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097-1105, 2012.\n\nFacetracer: A search engine for large collections of images with faces. N Kumar, P Belhumeur, S Nayar, ECCV. N. Kumar, P. Belhumeur, and S. Nayar. Facetracer: A search engine for large collections of images with faces. In ECCV, pages 340-353. 2008.\n\nAttribute and simile classifiers for face verification. N Kumar, A C Berg, P N Belhumeur, S K Nayar, ICCV. N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and simile classifiers for face verification. In ICCV, pages 365-372, 2009.\n\nHandwritten digit recognition with a back-propagation network. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , NIPS. Figure 14. More examples of face localization by LNetY. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a back-propagation network. In NIPS, 1990. Figure 14. More examples of face localization by LNet.\n\nLearning surf cascade for fast and accurate object detection. J Li, Y Zhang, CVPR. J. Li and Y. Zhang. Learning surf cascade for fast and accurate object detection. In CVPR, pages 3468-3475, 2013.\n\nA deep sum-product architecture for robust facial attributes analysis. P Luo, X Wang, X Tang, ICCV. P. Luo, X. Wang, and X. Tang. A deep sum-product architecture for robust facial attributes analysis. In ICCV, pages 2864-2871, 2013.\n\nFace detection without bells and whistles. M Mathias, R Benenson, M Pedersoli, L Van Gool, ECCV. M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool. Face detection without bells and whistles. In ECCV, pages 720-735. 2014.\n\nMultiresolution gray-scale and rotation invariant texture classification with local binary patterns. T Ojala, M Pietikainen, T Maenpaa, TPAMI. 247T. Ojala, M. Pietikainen, and T. Maenpaa. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. TPAMI, 24(7):971-987, 2002.\n\nModeling the shape of the scene: A holistic representation of the spatial envelope. A Oliva, A Torralba, IJCV. 423A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 42(3):145-175, 2001.\n\nRelative attributes. D Parikh, K Grauman, ICCV. D. Parikh and K. Grauman. Relative attributes. In ICCV, pages 503-510, 2011.\n\nCnn features off-the-shelf: an astounding baseline for recognition. A S Razavian, H Azizpour, J Sullivan, S Carlsson, arXiv:1403.6382arXiv preprintA. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for recog- nition. arXiv preprint arXiv:1403.6382, 2014.\n\nClustering by fast search and find of density peaks. A Rodriguez, A Laio, Science. 3446191A. Rodriguez and A. Laio. Clustering by fast search and find of density peaks. Science, 344(6191):1492-1496, 2014.\n\nDeep convolutional network cascade for facial point detection. Y Sun, X Wang, X Tang, CVPR. Y. Sun, X. Wang, and X. Tang. Deep convolutional network cascade for facial point detection. In CVPR, pages 3476- 3483, 2013.\n\nDeep learning face representation by joint identification-verification. Y Sun, X Wang, X Tang, NIPS. Y. Sun, X. Wang, and X. Tang. Deep learning face representation by joint identification-verification. In NIPS, 2014.\n\nDeepface: Closing the gap to human-level performance in face verification. Y Taigman, M Yang, M Ranzato, L Wolf, CVPR. Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verifica- tion. In CVPR, pages 1701-1708, 2014.\n\nPartbased r-cnns for fine-grained category detection. N Zhang, J Donahue, R Girshick, T Darrell, ECCV. N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part- based r-cnns for fine-grained category detection. In ECCV, pages 834-849. 2014.\n\nPanda: Pose aligned networks for deep attribute modeling. N Zhang, M Paluri, M Ranzato, T Darrell, L Bourdev, CVPR. N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev. Panda: Pose aligned networks for deep attribute modeling. In CVPR, 2014.\n", "annotations": {"author": "[{\"end\":130,\"start\":45},{\"end\":215,\"start\":131},{\"end\":327,\"start\":216},{\"end\":436,\"start\":328}]", "publisher": null, "author_last_name": "[{\"end\":54,\"start\":51},{\"end\":139,\"start\":136},{\"end\":229,\"start\":225},{\"end\":339,\"start\":335}]", "author_first_name": "[{\"end\":50,\"start\":45},{\"end\":135,\"start\":131},{\"end\":224,\"start\":216},{\"end\":334,\"start\":328}]", "author_affiliation": "[{\"end\":129,\"start\":56},{\"end\":214,\"start\":141},{\"end\":326,\"start\":253},{\"end\":435,\"start\":362}]", "title": "[{\"end\":42,\"start\":1},{\"end\":478,\"start\":437}]", "venue": null, "abstract": "[{\"end\":2095,\"start\":480}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2233,\"start\":2229},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2260,\"start\":2256},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2549,\"start\":2545},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2551,\"start\":2549},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2553,\"start\":2551},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2556,\"start\":2553},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2559,\"start\":2556},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2685,\"start\":2682},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2697,\"start\":2693},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3079,\"start\":3075},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3098,\"start\":3094},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5357,\"start\":5353},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5360,\"start\":5357},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6322,\"start\":6318},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6325,\"start\":6322},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6489,\"start\":6486},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8801,\"start\":8797},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8813,\"start\":8810},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9359,\"start\":9356},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9369,\"start\":9365},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9384,\"start\":9380},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9474,\"start\":9471},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9605,\"start\":9601},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9816,\"start\":9813},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9900,\"start\":9897},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10078,\"start\":10074},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10080,\"start\":10078},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10083,\"start\":10080},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10086,\"start\":10083},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10089,\"start\":10086},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10224,\"start\":10220},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10247,\"start\":10244},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10348,\"start\":10344},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10422,\"start\":10418},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10627,\"start\":10623},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10906,\"start\":10902},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11279,\"start\":11276},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11383,\"start\":11380},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11544,\"start\":11541},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11600,\"start\":11597},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12027,\"start\":12024},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13303,\"start\":13299},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13306,\"start\":13303},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13886,\"start\":13882},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14584,\"start\":14581},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14904,\"start\":14900},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14976,\"start\":14973},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15358,\"start\":15355},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17016,\"start\":17013},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17127,\"start\":17124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17208,\"start\":17205},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17843,\"start\":17840},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17964,\"start\":17961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18467,\"start\":18464},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19463,\"start\":19460},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20271,\"start\":20267},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20557,\"start\":20553},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20673,\"start\":20669},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20786,\"start\":20782},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21147,\"start\":21143},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21149,\"start\":21147},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21232,\"start\":21231},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21754,\"start\":21750},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21766,\"start\":21763},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23251,\"start\":23247},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23265,\"start\":23261},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23283,\"start\":23279},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23596,\"start\":23592},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24086,\"start\":24082},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24105,\"start\":24101},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25110,\"start\":25106},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25123,\"start\":25119},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25208,\"start\":25204},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25217,\"start\":25213},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28698,\"start\":28694},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28712,\"start\":28708},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28726,\"start\":28722},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30119,\"start\":30115},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30133,\"start\":30129},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30147,\"start\":30143},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30277,\"start\":30273},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30290,\"start\":30286},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30376,\"start\":30372},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30389,\"start\":30385},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30403,\"start\":30399},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34221,\"start\":34217}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32905,\"start\":32652},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32990,\"start\":32906},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33036,\"start\":32991},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33095,\"start\":33037},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33234,\"start\":33096},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33360,\"start\":33235},{\"attributes\":{\"id\":\"fig_7\"},\"end\":33452,\"start\":33361},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34222,\"start\":33453},{\"attributes\":{\"id\":\"fig_10\"},\"end\":34295,\"start\":34223},{\"attributes\":{\"id\":\"fig_12\"},\"end\":34403,\"start\":34296},{\"attributes\":{\"id\":\"fig_13\"},\"end\":34438,\"start\":34404},{\"attributes\":{\"id\":\"fig_14\"},\"end\":34506,\"start\":34439},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34519,\"start\":34507},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34973,\"start\":34520}]", "paragraph": "[{\"end\":3422,\"start\":2111},{\"end\":4237,\"start\":3424},{\"end\":5041,\"start\":4239},{\"end\":5487,\"start\":5043},{\"end\":5687,\"start\":5489},{\"end\":5892,\"start\":5689},{\"end\":7603,\"start\":5894},{\"end\":8353,\"start\":7605},{\"end\":8404,\"start\":8355},{\"end\":9248,\"start\":8406},{\"end\":10005,\"start\":9265},{\"end\":11027,\"start\":10007},{\"end\":12482,\"start\":11044},{\"end\":13182,\"start\":12484},{\"end\":13592,\"start\":13219},{\"end\":13772,\"start\":13629},{\"end\":14145,\"start\":13823},{\"end\":14364,\"start\":14240},{\"end\":14585,\"start\":14419},{\"end\":14689,\"start\":14587},{\"end\":14980,\"start\":14742},{\"end\":15580,\"start\":15005},{\"end\":16104,\"start\":15603},{\"end\":16681,\"start\":16179},{\"end\":16854,\"start\":16683},{\"end\":16981,\"start\":16856},{\"end\":17091,\"start\":17005},{\"end\":17244,\"start\":17093},{\"end\":17452,\"start\":17268},{\"end\":17556,\"start\":17454},{\"end\":17642,\"start\":17558},{\"end\":17765,\"start\":17681},{\"end\":18047,\"start\":17767},{\"end\":18769,\"start\":18119},{\"end\":19315,\"start\":18793},{\"end\":20558,\"start\":19317},{\"end\":21493,\"start\":20560},{\"end\":22217,\"start\":21546},{\"end\":23133,\"start\":22219},{\"end\":24309,\"start\":23135},{\"end\":24441,\"start\":24344},{\"end\":24900,\"start\":24443},{\"end\":25746,\"start\":24902},{\"end\":25788,\"start\":25748},{\"end\":26951,\"start\":25833},{\"end\":27735,\"start\":26953},{\"end\":28943,\"start\":27737},{\"end\":29415,\"start\":28945},{\"end\":29477,\"start\":29417},{\"end\":29924,\"start\":29479},{\"end\":31211,\"start\":29949},{\"end\":31636,\"start\":31213},{\"end\":31938,\"start\":31638},{\"end\":32122,\"start\":31940},{\"end\":32651,\"start\":32137}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13218,\"start\":13183},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13822,\"start\":13773},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14239,\"start\":14146},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14418,\"start\":14365},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14741,\"start\":14690},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15004,\"start\":14981},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16178,\"start\":16105},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17004,\"start\":16982},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17267,\"start\":17245},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17680,\"start\":17643},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18118,\"start\":18048},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21531,\"start\":21494}]", "table_ref": "[{\"end\":28655,\"start\":28648},{\"end\":30049,\"start\":30042}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2109,\"start\":2097},{\"attributes\":{\"n\":\"2.\"},\"end\":9263,\"start\":9251},{\"attributes\":{\"n\":\"3.\"},\"end\":11042,\"start\":11030},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13627,\"start\":13595},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15601,\"start\":15583},{\"attributes\":{\"n\":\"4.\"},\"end\":18791,\"start\":18772},{\"attributes\":{\"n\":\"5.\"},\"end\":21544,\"start\":21533},{\"attributes\":{\"n\":\"5.1.\"},\"end\":24342,\"start\":24312},{\"end\":25831,\"start\":25791},{\"attributes\":{\"n\":\"5.2.\"},\"end\":29947,\"start\":29927},{\"attributes\":{\"n\":\"6.\"},\"end\":32135,\"start\":32125},{\"end\":32663,\"start\":32653},{\"end\":33002,\"start\":32992},{\"end\":33048,\"start\":33038},{\"end\":33107,\"start\":33097},{\"end\":33458,\"start\":33454},{\"end\":34234,\"start\":34224},{\"end\":34307,\"start\":34297},{\"end\":34416,\"start\":34405},{\"end\":34451,\"start\":34440},{\"end\":34515,\"start\":34508}]", "table": "[{\"end\":34973,\"start\":34861}]", "figure_caption": "[{\"end\":32905,\"start\":32665},{\"end\":32990,\"start\":32908},{\"end\":33036,\"start\":33004},{\"end\":33095,\"start\":33050},{\"end\":33234,\"start\":33109},{\"end\":33360,\"start\":33237},{\"end\":33452,\"start\":33363},{\"end\":34222,\"start\":33459},{\"end\":34295,\"start\":34236},{\"end\":34403,\"start\":34309},{\"end\":34438,\"start\":34419},{\"end\":34506,\"start\":34454},{\"end\":34861,\"start\":34522}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2526,\"start\":2521},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3017,\"start\":3012},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3318,\"start\":3309},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6290,\"start\":6285},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6396,\"start\":6387},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6517,\"start\":6508},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6886,\"start\":6881},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11346,\"start\":11341},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11445,\"start\":11440},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11953,\"start\":11948},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12309,\"start\":12300},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12577,\"start\":12572},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13055,\"start\":13050},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13427,\"start\":13422},{\"end\":14645,\"start\":14637},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14822,\"start\":14811},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15381,\"start\":15372},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15678,\"start\":15669},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15988,\"start\":15978},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16241,\"start\":16232},{\"end\":16788,\"start\":16779},{\"end\":17293,\"start\":17284},{\"end\":17501,\"start\":17492},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18328,\"start\":18315},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":24685,\"start\":24676},{\"end\":24743,\"start\":24735},{\"end\":24810,\"start\":24802},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":25294,\"start\":25289},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":25558,\"start\":25549},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":25598,\"start\":25589},{\"end\":25743,\"start\":25739},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26029,\"start\":26020},{\"end\":26258,\"start\":26249},{\"end\":27437,\"start\":27432},{\"end\":27921,\"start\":27912},{\"end\":28045,\"start\":28036},{\"end\":28504,\"start\":28495},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29366,\"start\":29360},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29376,\"start\":29367},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29726,\"start\":29720},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30333,\"start\":30324},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31370,\"start\":31364},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31815,\"start\":31809}]", "bib_author_first_name": "[{\"end\":35092,\"start\":35091},{\"end\":35100,\"start\":35099},{\"end\":35102,\"start\":35101},{\"end\":35370,\"start\":35369},{\"end\":35381,\"start\":35380},{\"end\":35389,\"start\":35388},{\"end\":35599,\"start\":35598},{\"end\":35608,\"start\":35607},{\"end\":35797,\"start\":35796},{\"end\":35805,\"start\":35804},{\"end\":35813,\"start\":35812},{\"end\":35826,\"start\":35822},{\"end\":35832,\"start\":35831},{\"end\":35838,\"start\":35837},{\"end\":36081,\"start\":36080},{\"end\":36092,\"start\":36091},{\"end\":36099,\"start\":36098},{\"end\":36110,\"start\":36109},{\"end\":36121,\"start\":36120},{\"end\":36130,\"start\":36129},{\"end\":36139,\"start\":36138},{\"end\":36417,\"start\":36416},{\"end\":36428,\"start\":36427},{\"end\":36438,\"start\":36437},{\"end\":36447,\"start\":36446},{\"end\":36645,\"start\":36644},{\"end\":36656,\"start\":36655},{\"end\":36666,\"start\":36665},{\"end\":36887,\"start\":36886},{\"end\":36896,\"start\":36895},{\"end\":36902,\"start\":36901},{\"end\":36908,\"start\":36907},{\"end\":37164,\"start\":37163},{\"end\":37166,\"start\":37165},{\"end\":37175,\"start\":37174},{\"end\":37185,\"start\":37184},{\"end\":37193,\"start\":37192},{\"end\":37566,\"start\":37565},{\"end\":37580,\"start\":37579},{\"end\":37593,\"start\":37592},{\"end\":37595,\"start\":37594},{\"end\":37828,\"start\":37827},{\"end\":37837,\"start\":37836},{\"end\":37850,\"start\":37849},{\"end\":38062,\"start\":38061},{\"end\":38071,\"start\":38070},{\"end\":38073,\"start\":38072},{\"end\":38081,\"start\":38080},{\"end\":38083,\"start\":38082},{\"end\":38096,\"start\":38095},{\"end\":38098,\"start\":38097},{\"end\":38319,\"start\":38318},{\"end\":38328,\"start\":38327},{\"end\":38337,\"start\":38336},{\"end\":38339,\"start\":38338},{\"end\":38349,\"start\":38348},{\"end\":38362,\"start\":38361},{\"end\":38364,\"start\":38363},{\"end\":38374,\"start\":38373},{\"end\":38385,\"start\":38384},{\"end\":38387,\"start\":38386},{\"end\":38738,\"start\":38737},{\"end\":38744,\"start\":38743},{\"end\":38945,\"start\":38944},{\"end\":38952,\"start\":38951},{\"end\":38960,\"start\":38959},{\"end\":39151,\"start\":39150},{\"end\":39162,\"start\":39161},{\"end\":39174,\"start\":39173},{\"end\":39187,\"start\":39186},{\"end\":39436,\"start\":39435},{\"end\":39445,\"start\":39444},{\"end\":39460,\"start\":39459},{\"end\":39737,\"start\":39736},{\"end\":39746,\"start\":39745},{\"end\":39926,\"start\":39925},{\"end\":39936,\"start\":39935},{\"end\":40099,\"start\":40098},{\"end\":40101,\"start\":40100},{\"end\":40113,\"start\":40112},{\"end\":40125,\"start\":40124},{\"end\":40137,\"start\":40136},{\"end\":40399,\"start\":40398},{\"end\":40412,\"start\":40411},{\"end\":40615,\"start\":40614},{\"end\":40622,\"start\":40621},{\"end\":40630,\"start\":40629},{\"end\":40843,\"start\":40842},{\"end\":40850,\"start\":40849},{\"end\":40858,\"start\":40857},{\"end\":41065,\"start\":41064},{\"end\":41076,\"start\":41075},{\"end\":41084,\"start\":41083},{\"end\":41095,\"start\":41094},{\"end\":41319,\"start\":41318},{\"end\":41328,\"start\":41327},{\"end\":41339,\"start\":41338},{\"end\":41351,\"start\":41350},{\"end\":41564,\"start\":41563},{\"end\":41573,\"start\":41572},{\"end\":41583,\"start\":41582},{\"end\":41594,\"start\":41593},{\"end\":41605,\"start\":41604}]", "bib_author_last_name": "[{\"end\":35097,\"start\":35093},{\"end\":35112,\"start\":35103},{\"end\":35378,\"start\":35371},{\"end\":35386,\"start\":35382},{\"end\":35395,\"start\":35390},{\"end\":35605,\"start\":35600},{\"end\":35615,\"start\":35609},{\"end\":35802,\"start\":35798},{\"end\":35810,\"start\":35806},{\"end\":35820,\"start\":35814},{\"end\":35829,\"start\":35827},{\"end\":35835,\"start\":35833},{\"end\":35846,\"start\":35839},{\"end\":36089,\"start\":36082},{\"end\":36096,\"start\":36093},{\"end\":36107,\"start\":36100},{\"end\":36118,\"start\":36111},{\"end\":36127,\"start\":36122},{\"end\":36136,\"start\":36131},{\"end\":36147,\"start\":36140},{\"end\":36425,\"start\":36418},{\"end\":36435,\"start\":36429},{\"end\":36444,\"start\":36439},{\"end\":36455,\"start\":36448},{\"end\":36653,\"start\":36646},{\"end\":36663,\"start\":36657},{\"end\":36672,\"start\":36667},{\"end\":36893,\"start\":36888},{\"end\":36899,\"start\":36897},{\"end\":36905,\"start\":36903},{\"end\":36912,\"start\":36909},{\"end\":37172,\"start\":37167},{\"end\":37182,\"start\":37176},{\"end\":37190,\"start\":37186},{\"end\":37208,\"start\":37194},{\"end\":37577,\"start\":37567},{\"end\":37590,\"start\":37581},{\"end\":37602,\"start\":37596},{\"end\":37834,\"start\":37829},{\"end\":37847,\"start\":37838},{\"end\":37856,\"start\":37851},{\"end\":38068,\"start\":38063},{\"end\":38078,\"start\":38074},{\"end\":38093,\"start\":38084},{\"end\":38104,\"start\":38099},{\"end\":38325,\"start\":38320},{\"end\":38334,\"start\":38329},{\"end\":38346,\"start\":38340},{\"end\":38359,\"start\":38350},{\"end\":38371,\"start\":38365},{\"end\":38382,\"start\":38375},{\"end\":38741,\"start\":38739},{\"end\":38750,\"start\":38745},{\"end\":38949,\"start\":38946},{\"end\":38957,\"start\":38953},{\"end\":38965,\"start\":38961},{\"end\":39159,\"start\":39152},{\"end\":39171,\"start\":39163},{\"end\":39184,\"start\":39175},{\"end\":39196,\"start\":39188},{\"end\":39442,\"start\":39437},{\"end\":39457,\"start\":39446},{\"end\":39468,\"start\":39461},{\"end\":39743,\"start\":39738},{\"end\":39755,\"start\":39747},{\"end\":39933,\"start\":39927},{\"end\":39944,\"start\":39937},{\"end\":40110,\"start\":40102},{\"end\":40122,\"start\":40114},{\"end\":40134,\"start\":40126},{\"end\":40146,\"start\":40138},{\"end\":40409,\"start\":40400},{\"end\":40417,\"start\":40413},{\"end\":40619,\"start\":40616},{\"end\":40627,\"start\":40623},{\"end\":40635,\"start\":40631},{\"end\":40847,\"start\":40844},{\"end\":40855,\"start\":40851},{\"end\":40863,\"start\":40859},{\"end\":41073,\"start\":41066},{\"end\":41081,\"start\":41077},{\"end\":41092,\"start\":41085},{\"end\":41100,\"start\":41096},{\"end\":41325,\"start\":41320},{\"end\":41336,\"start\":41329},{\"end\":41348,\"start\":41340},{\"end\":41359,\"start\":41352},{\"end\":41570,\"start\":41565},{\"end\":41580,\"start\":41574},{\"end\":41591,\"start\":41584},{\"end\":41602,\"start\":41595},{\"end\":41613,\"start\":41606}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14125992},\"end\":35294,\"start\":34975},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6980765},\"end\":35542,\"start\":35296},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206590483},\"end\":35741,\"start\":35544},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57246310},\"end\":35999,\"start\":35743},{\"attributes\":{\"doi\":\"arXiv:1310.1531\",\"id\":\"b4\"},\"end\":36374,\"start\":36001},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14940757},\"end\":36583,\"start\":36376},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8281592},\"end\":36818,\"start\":36585},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14705012},\"end\":37062,\"start\":36820},{\"attributes\":{\"id\":\"b8\"},\"end\":37498,\"start\":37064},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":195908774},\"end\":37753,\"start\":37500},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":945967},\"end\":38003,\"start\":37755},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3136364},\"end\":38253,\"start\":38005},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2542741},\"end\":38673,\"start\":38255},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11123248},\"end\":38871,\"start\":38675},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1948186},\"end\":39105,\"start\":38873},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11157572},\"end\":39332,\"start\":39107},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14540685},\"end\":39650,\"start\":39334},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11664336},\"end\":39902,\"start\":39652},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2633340},\"end\":40028,\"start\":39904},{\"attributes\":{\"doi\":\"arXiv:1403.6382\",\"id\":\"b19\"},\"end\":40343,\"start\":40030},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":123905174},\"end\":40549,\"start\":40345},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6064403},\"end\":40768,\"start\":40551},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1395439},\"end\":40987,\"start\":40770},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2814088},\"end\":41262,\"start\":40989},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11710343},\"end\":41503,\"start\":41264},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6943286},\"end\":41754,\"start\":41505}]", "bib_title": "[{\"end\":35089,\"start\":34975},{\"end\":35367,\"start\":35296},{\"end\":35596,\"start\":35544},{\"end\":35794,\"start\":35743},{\"end\":36414,\"start\":36376},{\"end\":36642,\"start\":36585},{\"end\":36884,\"start\":36820},{\"end\":37563,\"start\":37500},{\"end\":37825,\"start\":37755},{\"end\":38059,\"start\":38005},{\"end\":38316,\"start\":38255},{\"end\":38735,\"start\":38675},{\"end\":38942,\"start\":38873},{\"end\":39148,\"start\":39107},{\"end\":39433,\"start\":39334},{\"end\":39734,\"start\":39652},{\"end\":39923,\"start\":39904},{\"end\":40396,\"start\":40345},{\"end\":40612,\"start\":40551},{\"end\":40840,\"start\":40770},{\"end\":41062,\"start\":40989},{\"end\":41316,\"start\":41264},{\"end\":41561,\"start\":41505}]", "bib_author": "[{\"end\":35099,\"start\":35091},{\"end\":35114,\"start\":35099},{\"end\":35380,\"start\":35369},{\"end\":35388,\"start\":35380},{\"end\":35397,\"start\":35388},{\"end\":35607,\"start\":35598},{\"end\":35617,\"start\":35607},{\"end\":35804,\"start\":35796},{\"end\":35812,\"start\":35804},{\"end\":35822,\"start\":35812},{\"end\":35831,\"start\":35822},{\"end\":35837,\"start\":35831},{\"end\":35848,\"start\":35837},{\"end\":36091,\"start\":36080},{\"end\":36098,\"start\":36091},{\"end\":36109,\"start\":36098},{\"end\":36120,\"start\":36109},{\"end\":36129,\"start\":36120},{\"end\":36138,\"start\":36129},{\"end\":36149,\"start\":36138},{\"end\":36427,\"start\":36416},{\"end\":36437,\"start\":36427},{\"end\":36446,\"start\":36437},{\"end\":36457,\"start\":36446},{\"end\":36655,\"start\":36644},{\"end\":36665,\"start\":36655},{\"end\":36674,\"start\":36665},{\"end\":36895,\"start\":36886},{\"end\":36901,\"start\":36895},{\"end\":36907,\"start\":36901},{\"end\":36914,\"start\":36907},{\"end\":37174,\"start\":37163},{\"end\":37184,\"start\":37174},{\"end\":37192,\"start\":37184},{\"end\":37210,\"start\":37192},{\"end\":37579,\"start\":37565},{\"end\":37592,\"start\":37579},{\"end\":37604,\"start\":37592},{\"end\":37836,\"start\":37827},{\"end\":37849,\"start\":37836},{\"end\":37858,\"start\":37849},{\"end\":38070,\"start\":38061},{\"end\":38080,\"start\":38070},{\"end\":38095,\"start\":38080},{\"end\":38106,\"start\":38095},{\"end\":38327,\"start\":38318},{\"end\":38336,\"start\":38327},{\"end\":38348,\"start\":38336},{\"end\":38361,\"start\":38348},{\"end\":38373,\"start\":38361},{\"end\":38384,\"start\":38373},{\"end\":38390,\"start\":38384},{\"end\":38743,\"start\":38737},{\"end\":38752,\"start\":38743},{\"end\":38951,\"start\":38944},{\"end\":38959,\"start\":38951},{\"end\":38967,\"start\":38959},{\"end\":39161,\"start\":39150},{\"end\":39173,\"start\":39161},{\"end\":39186,\"start\":39173},{\"end\":39198,\"start\":39186},{\"end\":39444,\"start\":39435},{\"end\":39459,\"start\":39444},{\"end\":39470,\"start\":39459},{\"end\":39745,\"start\":39736},{\"end\":39757,\"start\":39745},{\"end\":39935,\"start\":39925},{\"end\":39946,\"start\":39935},{\"end\":40112,\"start\":40098},{\"end\":40124,\"start\":40112},{\"end\":40136,\"start\":40124},{\"end\":40148,\"start\":40136},{\"end\":40411,\"start\":40398},{\"end\":40419,\"start\":40411},{\"end\":40621,\"start\":40614},{\"end\":40629,\"start\":40621},{\"end\":40637,\"start\":40629},{\"end\":40849,\"start\":40842},{\"end\":40857,\"start\":40849},{\"end\":40865,\"start\":40857},{\"end\":41075,\"start\":41064},{\"end\":41083,\"start\":41075},{\"end\":41094,\"start\":41083},{\"end\":41102,\"start\":41094},{\"end\":41327,\"start\":41318},{\"end\":41338,\"start\":41327},{\"end\":41350,\"start\":41338},{\"end\":41361,\"start\":41350},{\"end\":41572,\"start\":41563},{\"end\":41582,\"start\":41572},{\"end\":41593,\"start\":41582},{\"end\":41604,\"start\":41593},{\"end\":41615,\"start\":41604}]", "bib_venue": "[{\"end\":35118,\"start\":35114},{\"end\":35401,\"start\":35397},{\"end\":35621,\"start\":35617},{\"end\":35852,\"start\":35848},{\"end\":36078,\"start\":36001},{\"end\":36461,\"start\":36457},{\"end\":36678,\"start\":36674},{\"end\":36918,\"start\":36914},{\"end\":37161,\"start\":37064},{\"end\":37608,\"start\":37604},{\"end\":37862,\"start\":37858},{\"end\":38110,\"start\":38106},{\"end\":38394,\"start\":38390},{\"end\":38756,\"start\":38752},{\"end\":38971,\"start\":38967},{\"end\":39202,\"start\":39198},{\"end\":39475,\"start\":39470},{\"end\":39761,\"start\":39757},{\"end\":39950,\"start\":39946},{\"end\":40096,\"start\":40030},{\"end\":40426,\"start\":40419},{\"end\":40641,\"start\":40637},{\"end\":40869,\"start\":40865},{\"end\":41106,\"start\":41102},{\"end\":41365,\"start\":41361},{\"end\":41619,\"start\":41615}]"}}}, "year": 2023, "month": 12, "day": 17}