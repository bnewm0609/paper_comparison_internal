{"id": 4541069, "updated": "2023-09-29 13:17:37.345", "metadata": {"title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation", "authors": "[{\"first\":\"Taehwan\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Keane\",\"middle\":[]},{\"first\":\"Weiran\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Jason\",\"last\":\"Riggle\",\"middle\":[]},{\"first\":\"Gregory\",\"last\":\"Shakhnarovich\",\"middle\":[]},{\"first\":\"Diane\",\"last\":\"Brentari\",\"middle\":[]},{\"first\":\"Karen\",\"last\":\"Livescu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2016, "month": 9, "day": 26}, "abstract": "We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1609.07876", "mag": "2963408148", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/0003KWTRSBL16", "doi": "10.1016/j.csl.2017.05.009"}}, "content": {"source": {"pdf_hash": "ca007af298aea9d698542e3a20b59e64f379f63d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1609.07876v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.sciencedirect.com/science/article/am/pii/S0885230816302868", "status": "BRONZE"}}, "grobid": {"id": "011eedf0b3a60693a3d229392d2f73ea2384035d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ca007af298aea9d698542e3a20b59e64f379f63d.txt", "contents": "\nLexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation\n\n\nTaehwan Kim \nToyota Technological Institute at Chicago\n6045 S Kenwood Ave60637ChicagoILUSA\n\nJonathan Keane \nDepartment of Linguistics\nUniversity of Chicago\n1115 E. 58th Street60637ChicagoILUSA\n\nWeiran Wang \nToyota Technological Institute at Chicago\n6045 S Kenwood Ave60637ChicagoILUSA\n\nHao Tang \nToyota Technological Institute at Chicago\n6045 S Kenwood Ave60637ChicagoILUSA\n\nJason Riggle \nDepartment of Linguistics\nUniversity of Chicago\n1115 E. 58th Street60637ChicagoILUSA\n\nGregory Shakhnarovich \nToyota Technological Institute at Chicago\n6045 S Kenwood Ave60637ChicagoILUSA\n\nDiane Brentari \nDepartment of Linguistics\nUniversity of Chicago\n1115 E. 58th Street60637ChicagoILUSA\n\nKaren Livescu \nToyota Technological Institute at Chicago\n6045 S Kenwood Ave60637ChicagoILUSA\n\nLexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation\nAmerican Sign Languagefingerspelling recognitionsegmental modeldeep neural networkadaptation\nWe study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.\n\nIntroduction\n\nSign languages are the primary means of communication for millions of Deaf people in the world. In the US, there are about 350,000-500,000 people for whom American Sign Language (ASL) is the primary language [1]. While there has been extensive research over several decades on automatic recognition and analysis of spoken language, much less progress has been made for sign languages. Both signers and nonsigners would benefit from technology that improves communication between these populations and facilitates search and retrieval in sign language video.\n\nSign language recognition involves major challenges. The linguistics of sign language is less well understood than that of spoken language, hampering both scientific and technological progress. Another challenge is the high variability in the appearance of signers' bodies and the large number of degrees of freedom. The closely related  Signing in ASL (as in many other sign languages) involves the simultaneous use of handshape, location, movement, orientation, and non-manual behaviors. There has now been a significant amount of research on sign language recognition, for a number of sign languages. Prior research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on handshape. In some contexts, however, handshape carries much of the content, and it is this aspect of ASL that we study here.\n\nSign language handshape has its own phonology that has been studied and enjoys a broadly agreed-upon understanding relative to the other manual parameters of movement and place of articulation [3]. Recent linguistic work on sign language phonology has developed approaches based on articulatory features, related to motions of parts of the hand [4,2]. At the same time, computer vision research has studied pose estimation and tracking of hands [5], but usually not in the context of a grammar that constrains the motion. There is therefore a great need to better understand and model the handshape properties of sign language.\n\nThis project focuses mainly on one constrained, but very practical, component of ASL: fingerspelling. In certain contexts (e.g., when no sign exists for a word such as a name, to introduce a new word, or for emphasis), signers use fingerspelling: They spell out the word as a sequence of handshapes or hand trajectories corresponding to individual letters. Fig. 1 shows the ASL fingerspelled alphabet, and Figs. 2 show examples of real fingerspelling sequences. We will refer to the handshapes in Fig. 1 as fingerspelled letters (FS-letters), which are canonical target handshapes for each of the 26 letters, and the starred actual handshapes in Fig. 2 as peak handshapes.\n\nFingerspelled words arise naturally in the context of technical conversation or conversation about current events, such as in Deaf blogs or news sites. 1 Automatic fingerspelling recognition could add significant value to such resources. Overall, fingerspelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of the handshapes used in ASL [7]. These factors make fingerspelling an excellent testbed for handshape recognition.\n\nMost previous work on fingerspelling and handshape has focused on restricted con-gerspelling sequences, such as those in Figure 2-1, is the large amount of motion and lack of any prolonged \"steady state\" for each letter. Typically, each letter is represented by a brief \"peak of articulation\" of one or a few frames, during which the hand's motion is at a minimum and the handshape is the closest to the target handshape for the letter. This peak is surrounded by longer period of motion between the current letter and the previous/next letters. We consider signer-dependent, signer-independent, and signer-adapted recognition. We next describe the recognizers we compare, as well as the techniques we explore for signer adaptation. All of the recognizers use deep neural network (DNN) classifiers of letters or handshape features. produced by two signers. Image frames are sub-sampled at the same rate from both signers to show the true relative speeds. Asterisks indicate manually annotated peak frames for each letter. \"<s>\" and \"</s>\" denote non-signing intervals before/after signing. \n\n\nRecognizers\n\nIn designing recognizers, we keep several considerations in mind. First, the data set, while large by sign language research standards, is still quite small compared to typical speech data sets. This means that large models with many context-dependent units are infeasible to train on our data (as confirmed by our initial experiments). We therefore restrict attention here to \"mono-letter\" models, that is models in which each unit is a context-independent letter. We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41,66,10]. Second, we would like our models to be able to capture rich sign language-specific information, such as the dynamic aspects of fingerspelled letters as discussed above; this suggests the segmental models that we consider below. Finally, we would like our models to be easy to adapt to new signers. In order to enable this, all of our recognizers use independently trained deep neural network (DNN) classifiers, which can be adapted and plugged into different sequence models. Our DNNs are trained using an L2-regularized cross-entropy loss. The inputs are the image features concatenated over a multi-frame window centered at the current frame, which are fed through several fully connected layers followed by a softmax output layer with as many units as labels.\n\n\nTandem model\n\nThe first recognizer we consider is based on the popular tandem approach to speech recognition [21]. In tandem-based speech recognition, Neural Networks (NN) are trained to classify phones, and their outputs (phone posteriors) are post-processed and used as observations in a standard HMM-based recognizer with Gaussian mixture observation distributions. The post-processing may include taking the logs of the posteriors (or simply taking the linear outputs of the NNs rather than posteriors), applying principal components analysis, and/or appending acoustic features to the NN outputs.\n\nIn this work, we begin with a basic adaptation of the tandem approach, where instead of phone posteriors estimated from acoustic frames, we use letter posteriors estimated from image features. We also propose to use classifiers of phonological features of fin- Figure 2: Images and ground-truth segmentations of the fingerspelled words T-U-L-I-P and A-R-T produced by two signers in our data set. Image frames are sub-sampled at the same rate from both signers to show the true relative speeds. The starred frames indicate manually annotated peak handshapes (peak-HSs) for each FS-letter (see the text for the full definition of FS-letter and peak-HS). \"<s>\" and \"</s>\" denote non-signing intervals before/after signing. See Sec. 3, 4 for more details on data collection, annotation, and segmentation. ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8,9,10] (see Section 2 for more on related work). In such restricted settings, letter error rates (Levenshtein distances between hypothesized and true FS-letter sequences, as a proportion of the number of true FS-letters) of 10% or less have been obtained. In this work we consider lexicon-free fingerspelling sequences produced by multiple signers. This is a natural setting, since fingerspelling is often used for names and other \"new\" terms that do not appear in any closed vocabulary. Our long-term goals are to develop techniques for robust automatic detection and recognition of fingerspelled words in video, and to enable generalization across signers, styles, and recording conditions, both in controlled visual settings and \"in the wild.\" To this end, we are also interested in developing multi-signer, multi-style corpora of fingerspelling, as well as efficient annotation schemes for the new data.\n\nThe work in this paper represents our first steps toward this goal: studio collection and annotation of a new multi-signer connected fingerspelling data set (Section 3) and high-quality fingerspelling recognition in the signer-dependent and multi-signer settings (Section 5). The new data set, while small relative to typical speech data sets, comprises the largest fingerspelling video data set of which we are aware containing connected multi-signer fingerspelling that is not restricted to any particular lexicon. Next, we compare several recognition models inspired by automatic speech recognition, with some custom-made characteristics for ASL (Section 4). We begin with a tandem hidden Markov model (HMM) approach, where the features are based on pos-teriors of deep neural network (DNN) classifiers of letter and handshape phonological features. We also develop discriminative segmental models, which allow us to introduce more flexible features of fingerspelling segments. In the category of segmental models, we compare models for rescoring lattices and a first-pass segmental model, and the latter ultimately outperforms the others. Finally, we address the problem of signer variation via adaptation of the DNN classifiers, which allows us to bridge much of the gap between signer-dependent and signer-independent performance. 2 \n\n\nRelated work\n\nAutomatic sign language recognition has been approached in a variety of ways, including approaches based on computer vision techniques and ones inspired by automatic speech recognition. Thorough surveys on the topic are provided by Koller et al. [14] and Ong and Ranganath [15]. Here we focus on the most closely related work.\n\nA great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16,17,18,19,20,21,22]. This approach is very attractive for designing new communication interfaces for Deaf individuals. In many settings, however, video is more practical, and for online or archival recordings is the only choice. In this paper we restrict the discussion to the video-only setting.\n\nA number of sign language video corpora have been collected [23,24,25,26,27,28]. Some of the largest data collection efforts have been for European languages, such as the Dicta-Sign [29] and SignSpeak [30] projects. For American Sign Language, the American Sign Language Lexicon Video Dataset (ASLLVD) [27,28,31] includes recordings of almost 3,000 isolated signs and can be searched via a queryby-example interface. The National Center for Sign Language and Gesture Resources (NCSLGR) Corpus includes videos of continuous ASL signing, with over 10,000 sign tokens including about 1,500 fingerspelling sequences, annotated using the SignStream linguistic annotation tool [32,33]. The latter includes the largest previous data set of fingerspelling sequences of which we are aware. In order to explicitly study fingerspelling, however, it is helpful to have a collection that is both larger and annotated specifically with fingerspelling in mind. To our knowledge, the data collection and annotation we report in this paper (see Sec. 3) is the largest currently available for ASL fingerspelling.\n\nSign language recognition from video begins with front-end features. Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34,35,36,37], sometimes combined with appearance descriptors [38,39,40,4] and color models [41,42]. In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG) [43], which have also been used in other recent sign language recognition work [14,44].\n\nMuch prior work has used hidden Markov model (HMM)-based approaches [45,46,39,14], and this is the starting point for our work as well. Ney and colleagues have shown that it is possible to borrow many of the standard HMM-based techniques from automatic speech recognition to obtain good performance on naturalistic German Sign Language videos [39,14]. In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47,48,49].\n\nAs in acoustic and even visual speech recognition, the choice of basic linguistic unit is an important research question. For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51,52]. As the research community started to consider visual speech recognition (\"lipreading\"), analogous units have been explored: visemes, articulatory features, and automatic clusters [53,54,55]. While sign language shares some aspects of spoken language, it has some unique characteristics. A number of linguistically motivated representations of handshape and motion have been used in some prior work, e.g., [34,4,47,48,56,57,46,58,59], and multiple systems of phonological and phonetic features have been developed by linguists [60,2]. One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61,62,63,64].\n\nA subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65,66,47,18] and fingerspelling sequence recognition [8,9,10]. Letter error rates of 10% or less have been achieved when the recognition is constrained to a small (up to 100-word) lexicon of allowed sequences. Our work is the first of which we are aware to address the task of lexicon-free fingerspelling sequence recognition.\n\nThe problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14,67,68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.\n\nThe models we use and develop in this paper are related to prior work in both vision and speech (unrelated to sign language). Our HMM baselines are tandem models, analogous to ones developed for speech recognition [69,70]. The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71,72,73,74,75]. In natural language processing, semi-Markov CRFs have been used for named entity recognition [76], where the labeling is binary. Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [77,78] with a small set of possible activities to choose from, including work on spotting and recognition of a small vocabulary of (non-fingerspelled) signs in sign language video [79] or instrumented data capture [80]. One aspect that our work shares with the speech recognition work is that we have a relatively large set of labels (26 FS-letters plus non-letter \"N/A\" labels), and widely varying lengths of segments corresponding to each label (our data includes segment durations anywhere from 2 to 40 frames), which makes the search space larger and the recognition task more difficult than in the vision and text tasks. In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [71,74]. We compare this approach to an efficient first-pass segmental model [72].\n\n\nData collection and annotation\n\nWe recorded and analyzed videos of 3 native ASL signers and 1 early learner, fingerspelling a total of 3,684 word instances. We annotated the video by identifying the time of peak articulation (also known as hold, posture, or target) for each fingerspelled letter. There were 21,453 peaks in total. The following sections describe in detail the data collection and annotation process.\n\nThe data and annotations will be released publicly, in order to make it easy for others to study fingerspelling more extensively, and to replicate and compare against our results.\n\n\nVideo recording\n\nThe data was collected across several sessions, each including all of the words on one word list (the lists are described in detail below). Our use of \"words\" here includes both real words and nonsense letter sequences. The signers were presented with an isolated word on a computer screen. They were asked to fingerspell the word, and then press either a green button to advance or a red button to repeat the word if they felt they had made a mistake. For most sessions the signers were asked to fingerspell at a normal, natural speed. 3 Each session lasted 25-40 minutes, with a self-timed rest/stretch break in the middle of each session.\n\nThree word lists were created and used to collect data. The first list had 300 words: 100 names, 100 nouns, and 100 non-English words. These words were chosen to get examples of many letters in many contexts. The second list consisted of 300 mostly non-English words in an effort to get examples of each possible letter bigram. The third list had the 300 most common nouns in the CELEX corpus in order to get a list of words that are reasonably familiar to the signers. Additionally, a set of carefully articulated, isolated fingerspelled letters were also collected from each signer. A full listing of the word lists can be found in [2].\n\nThe video was recorded in a laboratory setting, with the signers wearing green or blue clothing and with the signers set against a green background. For most sessions the signers sat in a chair with an armrest that they could use if they felt the desire to. In a small number of sessions the signers were asked to stand rather than sit. Video was recorded using at least two cameras, each at an approximately 45 degree angle from a direct frontal view. Each camera recorded video at 1920x540 pixels per field 4 , 60 fields per second, interlaced, using the AVCHD format. For purposes of annotation, the video files were processed with FFMPEG to deinterlace, crop, resize, and reencode them for compatibility with the ELAN annotation software [81]. For purposes of recognition experiments, the videos were kept at full size and deinterlaced only.\n\nThe recording settings, including differences in environment and camera placement across sessions, are illustrated in Fig. 3. \n\n\nAnnotation\n\nOur annotation method is separated into two main parts: 1. a simple task to identify approximate times of each peak (peak detection) and 2. a verification task to determine precise timing for each peak (peak verification). The first is designed to be extremely quick, allowing multiple annotator judgements to be aggregated together. The second is more time-consuming and performed by a single annotator.\n\n\nPeak detection\n\n3-4 human annotators identified the peak of each letter. For this purpose, we defined a peak as the point where the articulators change direction to proceed on to the next peak (i.e. where the instantaneous velocity of the articulators is zero or at its minimum). This point is typically where the hand most closely resembles the canonical handshape, although at normal speed the peak handshape is often very different from canonical. Two FS-letters, -J-and -Z-, are not well-represented in terms of peaks, since they have movement. For these two FS-letters, annotators were asked to mark a peak at the point that they could determine that it was one of these two FS-letters. Peak detection is simple, requiring minimal training; annotators reported that this task was very intuitive. Most of the annotators at this stage had no exposure to ASL or fingerspelling.\n\nThe peak times from the multiple annotators were averaged, after aligning the annotations to minimize the mean absolute difference in time between the individual annotators' peaks. We accounted for misidentified peaks by penalizing missing or extra ones in the alignment. Using logs from the recording session, a best guess at the FS-letter corresponding to each peak was added by aligning the intended FS-letter sequence to the peak times (starting at the left edge of the word, matching each peak with a FS-letter).\n\n\nPeak verification\n\nFinally, a more experienced, second-language learner of ASL or an annotator specifically trained in fingerspelling annotation verified the location and identity of each peak from the peak detection stage. For the verification stage, we further refined the definition of peak to the point in time when the handshape configuration is closest to the canonical handshape for a given FS-letter. If a peak handshape remained stable for more than one frame, each stable frame was marked. However, for recognition experiments, only the original single peak frame was used.\n\nWhen using the data for recognizer training, the peak annotations are used to segment each word into FS-letters: The boundary between consecutive FS-letters is defined as the midpoint between their peak frames. Appendix A provides additional details about the annotation and analysis conventions, as well as detailed definitions of the fingerspelled letters.\n\nFS-letter frequencies in the collected data are given in Tab. 1. A histogram of the durations of all peaks in our data (excluding outliers) is given in Fig. 4.\n\nSeveral studies [82,83,84,2] have been conducted looking at how frequently FSletters are realized canonically (i.e. as the correct peak handshape) in our data. The frequency of the canonical peak handshape of the FS-letter depends on a number of factors (FS-letter identity, speed, signer, etc.). Some FS-letters show up nearly always as the canonical variant (e.g., -C-), some almost never (-D-), while others have 75%-85% ( -E-or -O-respectively) canonical peak handshape realizations [83]. Similarly to the phonetics of spoken language, contextual effects have an impact on the phonetic configuration of each peak handshape. For example, the pinky extension property of FS-letters has been shown to spread from peak handshapes that have an extended pinky to the ones before and after [2]. These coarticulation effects, combined with the quick motions of fingerspelling, are some of the factors that make the recognition task quite challenging.\n\n\nRecognition methods\n\nOur task is to take as input a video (a sequence of images) corresponding to a fingerspelled word, as in Fig. 2, and predict the signed FS-letters. This is a sequence prediction task analogous to connected phone or word recognition, but there are some interesting sign language-specific properties to the data domain. For example, one striking aspect of fingerspelling sequences, such as those in Fig. 2, is the large amount of motion and lack of any prolonged \"steady state\" for each FS-letter. As described in Sec. 3, each FS-letter is represented by a brief \"peak of articulation\", during which the Figure 4: Histogram of peak durations for all peaks in the corpus, excluding outliers. Any peak durations that were greater than 6 standard deviations more than the mean peak duration were excluded.\n\nhand's motion is at a minimum and the handshape is the closest to the target handshape for the FS-letter. This peak is surrounded by longer periods of motion between the current FS-letter and the previous/next FS-letters.\n\nAnother striking property of sign language is the wide variation between signers. Inspection of data such as Fig. 2 reveals some types of signer variation, including differences in speed, hand appearance, and non-signing motion before and after signing. The speed variation is large: In our data, the ratio between the average per-letter duration is about 1.8 between the fastest and slowest signers.\n\nWe consider signer-dependent, signer-independent, and signer-adapted recognition. We next describe the recognizers we compare, as well as the techniques we explore for signer adaptation. All of the recognizers use deep neural network (DNN) classifiers of FS-letters or handshape features.\n\n\nRecognizers\n\nIn designing recognizers, we keep several considerations in mind. First, the data set, while large in comparison to prior fingerspelling data sets, is still quite small compared to typical speech data sets. This means that large models with many contextdependent units are infeasible to train on our data (as confirmed by our initial experiments). We therefore restrict attention here to \"mono-letter\" models, that is models in which each unit is a context-independent FS-letter. We also consider the use of articulatory (phonological and phonetic) feature units, and there is evidence from speech recognition research that these may be useful in low-data settings [85,86]. Second, we would like our models to be able to capture detailed sign language-specific information, such as the dynamic aspects of FS-letters as discussed above; this suggests the segmental models that we consider below. Finally, we would like our models to be easy to adapt to new signers. In order to enable this, all of our recognizers use independently trained deep neural network (DNN) classifiers, which can be adapted and plugged into different sequence models. Our DNNs are trained using an L2-regularized cross-entropy loss. The inputs are the image features concatenated over a multi-frame window centered at the current frame, which are fed through several fully connected layers followed by a softmax output layer with as many units as labels. 5\n\n\nTandem model\n\nThe first recognizer we consider is a fairly typical tandem model [69,70]. Framelevel image features are fed to seven DNN classifiers, one of which predicts the frame's FS-letter label and six others which predict handshape phonological features. The phonological features are defined in Tab. 2, and example frames for values of one feature are shown in Fig. 5. The classifier outputs and image features are reduced in dimensionality via PCA and then concatenated. The concatenated features form the observations in an HMM-based recognizer with Gaussian mixture observation densities. We use a 3-state HMM for each (context-independent) FS-letter, plus one HMM each for initial and final \"silence\" (non-signing segments).\n\nIn addition, we use a bigram letter language model. In general the language model of FS-letter sequences is difficult to define or estimate, since fingerspelling does not follow the same distribution as English words and there is no large database of natural fingerspelled sequences on which to train. In addition, in our data set, the words were selected so as to maximize coverage of letter n-grams and word types rather than following a natural distribution. For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [87]. The issue of language modeling for fingerspelling deserves more attention in future work.   [3]. The first five features are properties of the active fingers (selected fingers, SF); the last feature is the state of the inactive or unselected fingers (UF). In addition to Brentari's feature values, we add a SIL (\"silence\") value to the features that do not have an N/A value. For detailed descriptions, see [3]. \nA L Y D E F\n\nSegmental CRF\n\nThe second recognizer is a segmental CRF (SCRF). SCRFs [76,71] are conditional log-linear models with feature functions that can be based on variable-length segments of input frames, allowing for great flexibility in defining feature functions. Formally, for a sequence of frames o 1 , o 2 , . . . , o T , a segmentation of size k is a sequence of time points 0 = q 0 , q 1 , . . . , q k\u22121 , q k = T used to denote time boundaries of segments. In other words, the i-th segment starts at time q i\u22121 and ends at q i . The labeling of segmentation q is a sequence of labels s 1 , s 2 , . . . , s k . We will denote the length of label sequences and segmentations |s|, |q|, respectively. For a segmentation of size k, |s| = |q| = k. A SCRF defines, over a sequence of labels s given a sequence of frames o, a probability distribution\np(s|o) = q:|q|=|s| exp |q| i=1 \u03bb f (s i\u22121 , s i , q i\u22121 , q i , o) s q :|q |=|s | exp |q | i=1 \u03bb f (s i\u22121 , s i , q i\u22121 , q i , o)\nwhere \u03bb is a weight vector, and f (s, s , q, q , o) is a feature vector. We assume s 0 is the sentence-start string, so that f (s 0 , s 1 , q 0 , q 1 ) is well-defined. In the case of sign language, it is natural to define feature functions that are sensitive to the duration and dynamics of each FS-letter segment.\n\n\nRescoring segmental CRF\n\nOne common way of using SCRFs is to rescore the outputs from a baseline HMMbased recognizer, and this is one way we apply SCRFs here. We first use the baseline recognizer, in this case the tandem HMM, to generate lattices of high-scoring segmentations and labelings, and then rescore them with our SCRFs.\n\nWe use the same feature functions as in [12], described here for completeness. Some of the feature functions are quite general to sequence recognition tasks, while some are tailored specifically to fingerspelling recognition.\n\nDNN classifier-based feature functions. The first set of feature functions measure how well the frames within a segment match the hypothesized label. For this purpose we use the same DNN classifiers as in the tandem model above.\n\nLet y be a FS-letter and v be the value of a FS-letter or linguistic feature, and g(v|o i ) the softmax output of a DNN classifier at frame i for class v. We define several feature functions based on aggregating the DNN outputs over a segment in various ways:\n\u2022 mean: f mean yv (s, s , q, q , o) = \u03b4(s = y) \u00b7 1 q \u2212q+1 q i=q g(v|o i )\n\u2022 max: f max yv (s, s , q, q , o) = \u03b4(s = y) \u00b7 max i\u2208{q,q+1,...,q } g(v|o i ) \u2022 div s : a concatenation of three mean feature functions, each computed over a third of the segment \u2022 div m : a concatenation of three max feature functions, each computed over a third of the segment\n\nThese are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88,89,72]. We tune the choice of aggregated feature functions on tuning data in our experiments.\n\nPeak detection features. A sequence of fingerspelled FS-letters yields a corresponding sequence of peak handshapes as described above. The peak frame and the frames around it for each FS-letter tend to have relatively little motion, while the transition frames between peaks have much more motion. To encourage each predicted FS-letter segment to have a single peak, we define letter-specific \"peak detection features\" based on approximate derivatives of the visual descriptors. We compute the approximate derivative as the l 2 norm of the difference between descriptors in every pair of consecutive frames, smoothed by averaging over 5-frame windows. Typically, there should be a single local minimum in this derivative over the span of the segment. We define the feature function corresponding to FS-letter y as f peak y (s, s , q, q , o) = \u03b4(s = y) \u00b7 \u03b4 peak (o, q, q ) where \u03b4 peak (o, q, q ) is 1 if there is exactly one local minimum between time point q and q and 0 otherwise.\n\nLanguage model feature. The language model feature is a bigram probability of the FS-letter pair corresponding to an edge:\nf lm (s, s , q, q , o) = p LM (s, s ).\nwhere p LM is our smoothed bigram language model. Baseline consistency feature. To take advantage of the already high-quality baseline that generated the lattices, we use a baseline feature like the one in [71], which is based on the 1-best output from the baseline tandem recognizer. The feature has value 1 when the corresponding segment spans exactly one FS-letter label in the baseline output and the label matches it:\nf bias (s, s , q, q , o) = \uf8f1 \uf8f2 \uf8f3 +1\nif C(q, q ) = 1, and B(q, q ) = s \u22121 otherwise\n\nwhere C(q, q ) is the number of distinct baseline labels in the time span from q to q , B(q, q ) is the label corresponding to time span (q, q ) when C(q, q ) = 1.\n\n\nFirst-pass segmental CRF\n\nOne of the drawbacks of a rescoring approach is that the quality of the final outputs depends both on the quality of the baseline lattices and the fit between the segmentations in the baseline lattices and those preferred by the second-pass model. We therefore also consider a first-pass segmental model, using similar features to the rescoring model. The difference between a rescoring model and a first-pass model lies in the set of segmentations and labellings to marginalize over. The rescoring model only marginalizes over the hypotheses generated by the tandem model, while the first-pass model marginalizes over all possible hypotheses. The first-pass model thus does not depend on the tandem HMM. We use a first-pass SCRF inspired by the phonetic recognizer of Tang et al. [72], and the same feature functions as in [72], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias. Recall that g(v|o i ) is the softmax output of a DNN classifier at frame i for class v. We use the same DNN frame classifiers as in the rescoring setting. Also recall that y is a FS-letter and v is the value of a FS-letter or linguistic feature. The exact definition of each feature is listed below.\n\nAverages of DNN outputs. These are the same as the mean features used in the rescoring setting. Duration. An indicator for the duration of a segment f dur yk (s, s , q, q , o) = \u03b4(q \u2212 q = k)\u03b4(s = y) for k \u2208 {1, 2, . . . , 30}.\n\n\nBias. A constant 1\n\nf bias y (s, s , q, q , o) = 1 \u00b7 \u03b4(s = y). As shown in the above definitions, all features are lexicalized with the unigram label, i.e., they are multiplied by an indicator for the hypothesized FS-letter label.\n\n\nDNN adaptation\n\nIn experiments, we will consider both signer-dependent and signer-independent recognition. In the latter case, the test signer is not seen in the training set. As we will see, there is a very large gap between signer-dependent and signer-independent recognition on our data. We also consider the case where we have some labeled data from the test signer, but not a sufficient amount for training signer-dependent models. In this case we consider adapting a signer-independent model toward the test signer. The most straightforward form of adaptation for our models is to adapt only the DNN classifiers, and then use the adapted ones in pre-trained signer-independent models. We consider adaptation with carefully annotated adaptation data, using ground-truth output .  frame-level labels, as well as the setting where only word labels are available but no frame-level alignments. In the latter case, we obtain frame labels via forced alignment using the signer-independent model.\n\nOur adaptation approaches are inspired by several DNN adaptation techniques that have been developed for speech recognition (e.g., [90,91,92,93]). We consider several approaches, shown in Fig. 6. Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94,95,96]. In these techniques most of the network parameters are fixed to the signer-independent ones, and a limited set of weights at the input and/or output layers are learned.\n\nIn the approach we refer to as LIN+UP in Fig. 6, we apply an affine transformation W LIN to the static features at each frame, as a pre-processing step before frame concatenation, and use the transformed features as input to the trained signer-independent DNNs. We simultaneously learn W LIN and fine-tune the last (softmax) layer weights by minimizing the same cross-entropy loss on the adaptation data, initialized with the signer-independent weights..\n\nThe second approach, referred to as LIN+LON in Fig. 6, uses the same adaptation layer at the input. However, instead of adapting the softmax weights at the top layer, it removes the softmax output activation and adds a new softmax output layer W LON trained for the test signer. The new input and output layers are trained jointly with the same cross-entropy loss.\n\nFinally, we also consider adaptation by fine-tuning all of the DNN weights on the adaptation data, using as initialization the signer-independent DNN weights (that is, using the signer-independent DNN as a \"warm start\").\n\n\nExperimental Results\n\nWe report on experiments using the fingerspelling data from the four ASL signers described above. We begin by describing some of the front-end details of hand segmentation and feature extraction, followed by experiments with the frame-level DNN classifiers (Sec. 5.1) and FS-letter sequence recognizers (Sec. 5.2).\n\nHand localization and segmentation As in prior work [9,11,12], we used a simple signer-dependent model for hand detection. First we manually annotated hand regions in 30 frames, and we trained a mixture of Gaussians P hand for the color of the hand pixels in L*a*b color space, and a single-Gaussian color model P x bg for every pixel x in the image excluding pixel values in or near marked hand regions. Given the color triplet c x = [l x , a x , b x ] at pixel x from a test frame, we assign the pixel to the hand if\nP hand (c x )\u03c0 hand > P x bg (c x )(1 \u2212 \u03c0 hand ),(1)\nwhere the prior \u03c0 hand for hand size is estimated from the same 30 training frames. We clean up the output of this simple model via several filtering steps. First, we suppress pixels that fall within regions detected as faces by the Viola-Jones face detector [97], since these tend to be false positives. We also suppress pixels that passed the log-odds test (Eq. 1) but have a low estimated value of P hand , since these tend to correspond to movements in the scene. Finally, we suppress pixels outside of a (generous) spatial region where the signing is expected to occur. The largest surviving connected component of the resulting binary map is treated as a mask that defines the detected hand region. Some examples of the resulting hand regions are shown in Fig. 2. Although this approach currently involves manual annotation for a small number of frames, it could be fully automated in an interactive system, and may not be required if given a larger number of training signers.\n\nHandshape descriptors We use histograms of oriented gradients (HOG [43]) as the visual descriptor (feature vector) for a given hand region. We first resize the tight bounding box of the hand region to a canonical size of 128\u00d7128 pixels, and then compute HOG features on a spatial pyramid of regions, 4\u00d74, 8\u00d78, and 16\u00d716 grids, with eight orientation bins per grid cell, resulting in 2688-dimensional descriptors. Pixels outside of the hand mask are ignored in this computation. For the HMM-based recognizers, to speed up computation, these descriptors were projected to at most 200 principal dimensions; the exact dimensionality in each experiment was tuned on a development set. For DNN frame classifiers, we found that finer grids did not improve much with increasing complexities, so we use 128-dimensional descriptors.\n\n\nDNN frame classification performance\n\nSince all of our fingerspelling recognition models use DNN frame classifiers as a building block, we first examine the performance of the frame classifiers.\n\nFor training and tuning the DNNs, we use the recognizer training data, split into 90% for DNN training and 10% for DNN tuning. The DNNs are trained for seven tasks (FS-letter classification and classification of each of the six phonological features). The input is the 128-dimensional HOG features concatenated over a 21-frame window. The DNNs have three hidden layers, each with 3000 ReLUs [98]. Network learning is done with cross-entropy training with a weight decay penalty of 10 \u22125 , via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [99] at a rate of 0.5 at each hidden layer, fixed momentum of 0.95, and initial learning rate of 0.01, which is halved when held-out accuracy stops improving. We pick the bestperforming epoch on held-out data. The network structure and hyperparameters were tuned on held-out (signer-independent) data in initial experiments.  The horizontal axis labels indicate the amount of adaptation data (0, 1, 2, 3 = none, 5%, 10%, 20% of the test signer's data, corresponding to no adaptation (signer-independent), \u223c 29, \u223c 58, and \u223c 115 words). GT = ground truth labels; FA = forced alignment labels; FT = fine-tuning. We also added trained DNN on only 20% of the test signer's data. Signer-dependent DNN uses 80% of the test signer's data.\n\nWe consider the signer-dependent setting (where the DNN is trained on data from the test signer), signer-independent setting (where the DNN is trained on data from all except the test signer), and signer-adapted setting (where the signer-independent DNNs are adapted using adaptation data from the test signer). For LIN+UP and LIN+LON, we adapt by running SGD over minibatches of 100 samples with a fixed momentum of 0.9 for up to 20 epochs, with initial learning rate of 0.02 (which is halved when accuracy stops improving on the adaptation data). For fine-tuning, we use the same SGD procedure as for the signer-independent DNNs. We pick the epoch with the highest accuracy on the adaptation data.\n\nThe frame error rates for all settings are given in Fig. 7. For the signer-adapted case, we consider DNN adaptation with different types and amounts of supervision. The types of supervision include fully labeled adaptation data (\"GT\", for \"ground truth\", in the figure), where the peak locations for all FS-letters are manually annotated; as well as adaptation data labeled only with the FS-letter sequence but not the timing information. In the latter case, we use the baseline tandem system to generate forced alignments (\"FA\" in the figure). We consider amounts of adaptation data from 5% to 20% of the test signer's full data.\n\nThese results show that among the adaptation methods, LIN+UP slightly outperforms LIN+LON, and fine-tuning outperforms both LIN+UP and LIN+LON. For FSletter sequence recognition experiments in the next section, we adapt via fine-tuning using 20% of the test signer's data.\n\nWe have analyzed the types of errors made by the DNN classifiers. One of the main effects is that all of the signer-independent classifiers have a large number of incorrect predictions of the non-signing classes (<s>, </s>). This may be due to the previously mentioned observation that non-linguistic gestures are variable and easy to confuse with signing when given a new signer's image frames. As the DNNs are adapted, this is the main type of error that is corrected. Specifically, of the frames with label <s> that are misrecognized by the signer-independent DNNs, 18.3% are corrected after adaptation; of the misrecognized frames labeled </s>, 11.2% are corrected.\n\n\nFS-letter recognition experiments\n\n\nSigner-dependent recognition\n\nOur first continuous FS-letter recognition experiments are signer-dependent; that is, we train and test on the same signer, for each of four signers. For each signer, we use a 10-fold setup: In each fold, 80% of the data is used as a training set, 10% as a development set for tuning parameters, and the remaining 10% as a final test set. We independently tune the parameters in each fold. To make the results comparable to the later adaptation results, we use 8 out of 10 folds to compute the final test results and report the average letter error rate (LER) over those 8 folds. For language models, we train letter bigram language models from large online dictionaries of varying sizes that include both English words and names [100]. We use HTK [101] to implement the baseline HMM-based recognizers and SRILM [102] to train the language models. The HMM parameters (number of Gaussians per state, size of language model vocabulary, transition penalty and language model weight), as well as the dimensionality of the HOG descriptor input and HOG depth, were tuned to minimize development set letter error rates for the baseline HMM system. The front-end and language model hyperparameters were then kept fixed for the experiments with SCRFs (in this sense the SCRFs are slightly disadvantaged). Additional parameters tuned for the SCRF rescoring models included the N-best list sizes, type of feature functions, choice of language models, and L1 and L2 regularization parameters. Finally, for the first-pass SCRF, we tuned step size, maximum length of segmentations, and number of training epochs.\n\nTab. 3 (last row) shows the signer-dependent FS-letter recognition results. SCRF rescoring improves over the tandem HMM, and the first-pass SCRF outperforms both.\n\nNote that in our experimental setup, there is some overlap of word types between training and test data. This is a realistic setup, since in real applications some of the test words will have been previously seen and some will be new. However, for comparison, we have also conducted the same experiments while keeping the training, development, and test vocabularies disjoint; in this modified setup, letter error rates increase by about 2-3% overall, but the SCRFs still outperform the other models.  Table 3: Letter error rates (%) on four test signers.\n\n\nSigner-independent recognition\n\nIn the signer-independent setting, we would like to recognize FS-letter sequences from a new signer, given a model trained only on data from other signers. For each of the four test signers, we train models on the remaining three signers, and report the performance for each test signer and averaged over the four test signers. For direct comparison with the signer-dependent experiments, each test signer's performance is itself an average over the 8 test folds for that signer.\n\nAs shown in the first line of Tab. 3, the signer-independent performance of the three types of recognizers is quite poor, with the rescoring SCRF somewhat outperforming the tandem HMM and first-pass SCRF. The poor performance is perhaps to be expected with such a small number of training signers.\n\n\nSigner-adapted recognition\n\nThe remainder of Tab. 3 (second and third rows) gives the connected FS-letter recognition performance obtained with the three types of models using DNNs adapted via fine-tuning, using different types of adaptation data (ground-truth, GT, vs. forcedaligned, FA). For all models, we do not retrain the models with the adapted DNNs, but tune hyperparameters 6 on 10% of the test signer's data. The tuned models are evaluated on an unseen 10% of the test signer's remaining data; finally, we repeat this for eight choices of tuning and test sets, covering the 80% of the test signer's data that we do not use for adaptation, and report the mean FS-letter accuracy over the test sets.\n\nAs shown in Tab. 3, adaptation improves the performance to up to 30.3% letter error rate with forced-alignment adaptation labels and up to 17.3% letter error rate with ground-truth adaptation labels. All of the adapted models improve similarly. However, interestingly, the first-pass SCRF is slightly worse than the others before adaptation but better (by 4.4% absolute) after ground-truth adaptation. One hypothesis is that the first-pass SCRF is more dependent on the DNN performance, while the tandem model uses the original image features and the rescoring SCRF uses that tandem model's hypotheses. Once the DNNs are adapted, however, the first-pass SCRF outperforms the other models. Fig. 8 shows an example fingerspelling sequence and the hypotheses of the tandem, rescoring SCRF, and first-pass SCRF.\n\n\nExtensions and analysis\n\nWe next analyze our results and consider potential extensions for improving the models. Figure 8: Sample frames from the word ROAD, with asterisks denoting the peak frame for each FS-letter and \"<s>\" and \"</s>\" denoting periods before the first FS-letter and after the last FS-letter; ground-truth labeling and segmentation based on peak annotations (GT); hypothesized output from the tandem HMM (Tandem), rescoring SCRF (re-SCRF), and first-pass SCRF (1p-SCRF).\n\n\nAnalysis: Could we do better by training entirely on adaptation data?\n\nUntil now we have considered adapting the DNNs while using sequence models (HMMs/SCRFs) trained only on signer-independent data. In this section we consider alternatives to this adaptation setting. We fix the model to a first-pass SCRF and the adaptation data to 20% of the test signer's data annotated with ground-truth labels. In this setting, we consider two alternative ways of using the adaptation data: (1) using the adaptation data from the test signer to train both the DNNs and sequence model from scratch, ignoring the signer-independent training set; and (2) training the DNNs from scratch on the adaptation data, but using the SCRF trained on the training signers. We compare these options with our best results using the signer-independent SCRF and DNNs fine-tuned on the adaptation data. The results are shown in Tab. 4.\n\nWe find that ignoring the signer-independent training set and training both DNNs and SCRFs from scratch on the test signer (option (1) above) works remarkably well, better than the signer-independent models and even better than adaptation via forced alignment (see Tab. 3). However, training the SCRF on the training signers but DNNs from scratch on the adaptation data (option (2) above) improves performance further. However, neither of these outperforms our previous best approach of signerindependent SCRFs plus DNNs fine-tuned on the adaptation data.  \n\n\nAnalysis: FS-letter vs. feature DNNs\n\nWe next compare the FS-letter DNN classifiers and the phonological feature DNN classifiers in the context of the first-pass SCRF recognizers. We also consider an alternative sub-letter feature set, in particular a set of phonetic features introduced by Keane [2], whose feature values are listed in Section Appendix A, Tab. B.9. We use the first-pass SCRF with either only FS-letter classifiers, only phonetic feature classifiers, FS-letter + phonological feature classifiers, and FS-letter + phonetic feature classifiers. We do not consider the case of phonological features alone, because they are not discrimative for some FS-letters. Fig. 9 shows the FS-letter recognition results for the signer-dependent and signer-adapted settings.\n\nWe find that using FS-letter classifiers alone outperforms the other options in the signer-dependent setting, achieving 7.7% letter error rate. For signer-adapted recognition, phonological or phonetic features are helpful in addition to FS-letters for two of the signers (signers 2 and 4) but not for the other two (signers 1,3); on average, using FS-letter classifiers alone is best in both cases, achieving 16.6% accuracy on average. In contrast, in earlier work [11] we found that phonological features outperform FSletters in the tandem HMM. However, those experiments used a tandem model with neural networks with a single hidden layer; we conjecture that with more layers, we are able to do a better job at the more complicated task of FS-letter classification.\n\n\nAnalysis: DNNs vs. CNNs\n\nIn all experiments thus far, we have used HOG image descriptors fed into fully connected feedforward DNNs. However, it has recently become common to use convolutional neural networks (CNNs) on raw image pixels without any hand-crafted image descriptors, which produces improved performance for certain visual recognition tasks (e.g., [103,104]). In our case, we have relatively little training data compared with typical benchmark visual recognition tasks, motivating our choice to preprocess with image descriptors rather than rely on networks that learn features from raw pixels. To test this assumption, we compare the performance of FS-letter and phonological feature classifiers based on DNNs and CNNs. We use a signer-dependent setting and the same 8-fold setup.\n\nFor CNN experiments, our inputs are grayscale 64 \u00d7 64 \u00d7 T pixels. T is the number of frames used in the input window, as in our DNNs. The CNNs use 32 kernels of 3 \u00d7 3 filters with stride 1 for the first and second convolutional layers. Next we add a max pooling layer with a 2 \u00d7 2 pixel window, with stride 2. For the third and fourth convolutional layer, we use 64 kernels of 3 \u00d7 3 filters with stride 1. Again, we then add a max pooling layer with a 2 \u00d7 2 pixel window, with stride 2. For all convolutional layers, ReLUs are used for the nonlinearity (as in our DNNs). Finally, two fully connected layers with 2000 ReLUs are added, followed by a softmax output layer. We use dropout to prevent overfitting, with a 0.25 dropout probability for all convolutional layers and 0.5 for fully connected layers. Training is done via stochastic gradient descent with learning rate 0.01, learning rate decay 1 \u00d7 10 \u22126 , momentum 0.9, and miini-batches of size 100. The network structure and all other settings were tuned on held-out data in preliminary experiments. T was also tuned and set to 21. We implemented the CNNs using Keras [105] with Theano [106]. Fig. 10 shows the results for the signer-dependent setting. We find that the performances of the DNNs and CNNs are comparable. Specifically, for FS-letter classification, DNNs are slightly better, while for phonological feature classification, CNNs are slightly better. However, in both cases the gaps are very small. These results provide evidence for our hypothesis that on this data set, CNNs on raw pixels do not provide a substantial benefit, and we do not use them in further experiments. \n\n\nImproving performance in the force-aligned adaptation case\n\nWe next attempt to improve the performance of adaptation in the absence of groundtruth (manually annotated) frame labels. This is an important setting, since in practice it can be very difficult to obtain ground-truth labels at the frame level. Using only the FS-letter label sequence for the adaptation data, we use the signer-independent tandem recognizer to get force-aligned frame labels. We then adapt (fine-tune) the DNNs using the force-aligned adaptation data (as before in the FA case). We then re-align the test signer's adaptation data with the adapted recognizer. Finally, we adapt the DNNs again with the re-aligned data. Throughout this experiment, we do not change the recognizer but only update the DNNs. Using this iterative realignment approach, we are able to furthur improve the recognition accuracy in the FA case by about 1.3%, as shown in Tab. 5.  \n\n\nImproving performance with segmental cascades\n\nFinally, we consider whether we can improve upon the performance of our best models, the first-pass SCRFs, by rescoring their results in a second pass with more powerful features. We follow the discriminative segmental cascades (DSC) approach of [72], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.\n\nFor these experiments we start with the most successful first-pass SCRF in the above experiments, which uses FS-letter DNNs and is adapted with 20% of the test signer's data with ground-truth peak handshape labels. For the second-pass SCRF, we use the first-pass score as a feature, and add to it two more complex features: a segmental DNN, which takes as input an entire hypothesized segment and produces posterior probabilities for all of the FS-letter classes; and the \"peak detection\" feature described in Section 4. We use the same bigram language model as in the tandem HMM and rescoring SCRF models. For the segmental DNN, the training segments are given by the ground-truth segmentations derived from manual peak annotations. The input layer consists of a concatenation of three mean HOG vectors, each averaged over one third of the segment. We use the same DNN structure and learning strategy as for the DNN frame classifiers.\n\nAs shown in Tab. 6, this approach slightly improves the average FS-letter accuracy over four signers, from 16.6% in the first pass to 16.2% in the second pass. This improvement, while small, is statistically significant at the p=0.05 level (using the MAPSSWE significance test from the NIST Scoring Toolkit [107]). These results combine our most successful ideas and form our final best results for signer-adapted recognition. For the signer-dependent setting, this approach achieves comparable performance to the first-pass SCRF.  \n\n\nConclusion\n\nThis paper tackles the problem of unconstrained fingerspelled letter sequence recognition in ASL, where the FS-letter sequences are not restricted to any closed vocabulary. This problem is challenging due to both the small amount of available training data and the significant variation between signers. Our data collection effort has thus far produced a set of carefully annotated fingerspelled letter sequences for four native ASL signers. Our recognition experiments have compared HMM-based and segmental models with features based on DNN classifiers, and have investigated a range of settings including signer-dependent, signer-independent, and signer-adapted. Our main contributions are:\n\n\u2022 We have developed an approach for quick annotation of fingerspelling data using a two-pass approach, where multiple non-expert annotators quickly find candidate peak times, which are then automatically combined and verified by an ASL expert.\n\n\u2022 Signer-dependent recognition, even with only a small amount of training data, is quite successful, reaching letter error rates below 10%. Signer-independent recognition, in contrast, is quite challenging, with letter error rates around 60%.\n\nDNN adaptation allows us to bridge a large part of the gap between signerindependent and signer-dependent performance.\n\n\u2022 Our best results are obtained using two-pass discriminative segmental cascades with features based on frame-level DNN FS-letter classifiers. This approach achieves an average letter error rate of 7.6% in the signer-dependent setting and 16.2% LER in the signer-adapted setting. The adapted models use signerindependent SCRFs and DNNs adapted by fine-tuning on the adaptation data.\n\nThe adapted results are obtained using about 115 words of adaptation data with manual annotations of FS-letter peaks.\n\n\u2022 If an even smaller amount of adaptation data is available, we can still get improvements from adaptation down to about 30 words of adaptation data.\n\n\u2022 In the absence of manual frame-level annotations, we can automatically align the adaptation data and still get a significant boost in performance from adaptation. We can also iteratively improve the performance by re-aligning the data with the adapted models. Our best adapted models using automatically aligned adaptation data achieve 27.3% LER.\n\n\u2022 The main types of errors that are addressed by adapting the DNN classifiers are confusions between signing and non-signing segments.\n\nWe are continuing to investigate additional sequence recognition approaches as well as adaptation methods, especially for the case where no manual annotations are available. Future work will also expand our data collection to a larger number of signers and to data collected \"in the wild\", such as videos from Deaf online media. Finally, future work will consider jointly detecting and recognizing fingerspelling sequences embedded within running ASL.\n\n\u2022 Peak handshapes are the phonetically realized handshapes used in a given sequence at the moment when it most closely approximates the FS-letter.\n\nOrientation -Most FS-letters are produced with the palm facing away from the signer's body. The few exceptions to this are -G-, -H-, -P-, and -Q- 7 where the palm faces the signer ( -G-and -H-; labeled side in our analysis), or faces down ( -P-and -Q-; labeled down in our analysis). Because handshape and orientation changes are not always synchronized, we have annotated handshape stability as a hold, even if the hand is continuing to undergo an orientation change. Future annotation is necessary for orientation changes in detail and determine the pattern of stability and motion that exists there.\n\nMovement -Two FS-letters are described as having movement: -J-and -Z-. -Jinvolves an orientation change, and -Z-traces the path of the letter 8 . For both of these FS-letters, again we have annotated a hold to be where the handshape is stable, regardless of orientation change, or path movement.\n\nHandshape detail -A detailed (although not exhaustive) description of handshapes is given in table A.7. This is meant to be guidance to annotators, and is intended to catch the core features for each handshape, allowing for the systematic variation known to exist in handshape. If some of these features match, but the handshape is significantly different than expected, annotators added a diacritic (+) to note a large amount of deviance. This is not intended to exhaustively mark all of the deviant handshapes, but only those that should be looked into further. There are some instances where a peak is found, but no peak was detected. Although we have not analyzed this systematically, these instances are frequently peak handshapes that are instantaneous, or peak handshapes that occur extremely close to each other. These peaks are noted with a different diacritic (*). Finally if two handshapes have compressed to form a single peak handshape, a digraph is used to annotate the combined peak handshape. Examples that we have seen so far are -GH-, -IT-, -IN-, -IO-, -IL-, and -CI-. Here the digraph is simply two FS-letters that seem to make up the single peak; for consistency they should be written in alphabetical order regardless of the orthographic order of the letters in the word being fingerspelled. See table A.8 for a description of those found so far.\n\nAppendix B. Phonetic feature definitions 7 These are the FS-letters traditionally described as having different orientation; there are other possibilities that we have found as well: -X-and -Y-. 8 This is frequently abbreviated to just a horizontal line, representing the top bar of the z.  \n\n\n1 arXiv:1609.07876v1 [cs.CL] 26 Sep 2016\n\nFigure 1 :\n1The ASL fingerspelled alphabet. Reproduced from[2]. computer vision problem of articulated pose estimation and tracking remains largely unsolved.\n\nFigure 2 - 1 :\n21Images and ground-truth segmentations of the fingerspelled word 'TULIP'\n\nFigure 2 - 2 :\n22Similar to Figure 2-1, images and ground-truth segmentations of the fingerspelled word 'ART' produced by other two signers.\n\nFigure 3 :\n3Example video frames from the four signers included in the recognition experiments.\n\nFigure 5 :\n5Example images corresponding to SF thumb = 'unopposed' (upper row) and SF thumb = 'opposed' (bottom row).\n\n\nSamples of DNN outputs. These are samples of the DNN outputs at the mid-points of three equal-sized sub-segmentsf spl yvi (s, s , q, q , o) = \u03b4(s = y)g(v|o q+(q \u2212q)i ) for i = 16%, 50%, 84%.Left boundary. Three DNN output vectors around the left boundary of the segment f l-bndry yvk (s, s , q, q , o) = \u03b4(s = y)g(v|o q+k ) for k \u2208 {\u22121, 0, 1}. Right boundary. Three DNN output vectors around the right boundary of the segment f r-bndry yvk (s, s , q, q , o) = \u03b4(s = y)g(v|o q +k ) for k \u2208 {\u22121, 0, 1}.\n\nFigure 6 :\n6Left: Unadapted DNN classifier; middle: adaptation via linear input network and output layer updating (LIN+UP); right: adaptation via linear input network and linear output network (LIN+LON).\n\nFigure 7 :\n7Frame errors with DNN classifiers, with various settings.\n\nFigure 9 :\n9(Top) Signer-dependent recognition and (bottom) signer-independent recognition with frame annotations and adaptation. We compare: FS-letter only, phonetic features only, FS-letters + phonological features[3] and FS-letters + phonetic features[2].\n\nFigure 10 :\n10Comparison between DNN and CNN frame classifiers, in signer-dependent experiments over four signers.\n\n\nare flexed, with thumb touching the radial side of the hand, or extended -B-all fingers are extended. The thumb is hyper flexed across the palm -C-all the fingers are curved. -D-the index finger is fully extended. At least the middle finger is making contact with the thumb: the ring and pinky may be either flexed, or making contact with the thumb -E-the thumb is bent and hyper flexed across the palm, the index finger is bent and may be touching the thumb. The other fingers may be bent, like the index finger, or flexed completely. -F-contact with the index and thumb. The middle, ring, and pinky are all extended -G-the index finger is fully extended. All other fingers are flexed. The thumb is either extended fully, or unextended, against the middle finger.-H-index and middle fingers are fully extended. All others are flexed. The thumb is unextended or extended -I-the pinky is fully extended, all other fingers are flexed. The thumb is either hyperflexed, or unexetended, against the radial side -J-the pinky is fully extended, all other fingers are flexed. The thumb is either hyperflexed, or unexetended, against the radial side -K-the index finger is fully extended, the middle finger is extended, but bent 90 at the joint closest to the hand -L-the index finger is fully extended, and the thumb is extended away from the hand. All other fingers are flexed -M-the index, middle, and ring fingers are closed, or flat-closed over the thumb, which is hyper flexed across the palm, possibly touching the base of the pinky and ring fingers -N-the index and middle fingers are closed, or flat-closed over the thumb, which is hyper flexed across the palm, possibly touching the base of the ring and middle fingers -O-the thumb and the index finger are touching in a curved, closed configuration. The other fingers are either in the same configuration, touching the thumb, or completely flexed. -P-the index finger is fully extended, the middle finger is extended, but bent 90 at the joint closest to the hand -Q-the index finger is fully extended. All other fingers are flexed. The thumb is either extended fully, or unextended, against the middle finger. -R-the index and middle fingers are extended and crossed over each other. All other fingers are flexed -S-all fingers are completely flexed, with thumb hyperflexed across the fist -T-the index finger is closed, or flat-closed over the thumb, which is hyper flexed across the palm, possibly touching the base of the middle and index fingers 9 -U-the index and middle fingers are completely extended, all other fingers are flexed, the thumb is hyperflexed across the palm -V-the index and middle fingers are completely extended and are spread apart, all other fingers are flexed, the thumb is hyperflexed across the palm -W-the index, middle, and ring fingers are completely extended and are spread apart, all other fingers are flexed, the thumb is hyperflexed across the palm -X-the index finger is bent similar to -E-, all other fingers are completely flexed -Y-the pinky is fully extended, the thumb is hyper extended away from the hand. All other fingers are flexed -Z-the index finger is fully extended, and all other fingers are flexed Table A.7: Description of handshapes. FS-letter description of the most canonical handshape -I-and -T-the index finger is closed, or flat-closed over the thumb, which is hyper flexed across the palm, possibly touching the base of the middle and index fingers, and the pinky is extended. This should only be used if the hand reaches this configuration at a single frame. -G-and -H-index and middle fingers as well as the thumb are extended. Similar to the CL 3 handshape. -I-and -N-index and middle fingers are (partially) flexed over the thumb, and the pinky is fully extended. -I-and -O-index, middle, and ring fingers are looped and touching the thumb, and the pinky is fully extended. -I-and -L-the index and thumb are extended (the thumb is abducted), and the pinky is fully extended. -C-and -I-the index, middle, and ring fingers as well as the thumb are partially extended (also described as curved open), and the pinky is fully extended.\n\n\nTable 1: Frequencies and counts of peaks in the corpuspeak letter \n\nfreq \nn \nE \n0.1124 2411 \nA \n0.0979 2101 \nI \n0.0788 1690 \nO \n0.0685 1470 \nN \n0.0680 1458 \nR \n0.0606 1300 \nT \n0.0581 1246 \nS \n0.0492 1055 \nL \n0.0491 1053 \nU \n0.0376 \n807 \nC \n0.0352 \n755 \nD \n0.0277 \n594 \nM \n0.0276 \n593 \nP \n0.0256 \n550 \n\npeak letter \nfreq \nn \nG \n0.0234 \n501 \nH \n0.0225 \n483 \nY \n0.0225 \n483 \nF \n0.0201 \n432 \nK \n0.0184 \n395 \nB \n0.0170 \n365 \nV \n0.0140 \n300 \nW \n0.0135 \n290 \nZ \n0.0122 \n261 \nX \n0.0114 \n245 \nJ \n0.0103 \n221 \n? \n0.0101 \n216 \nQ \n0.0083 \n178 \n\n\n\nTable 2 :\n2Definition and possible values for phonological features based on\n\nTable 4 :\n4Letter error rates (%) for different settings of SCRF and DNN training in the signer-adapted case. \nDetails are given in Section 5.3.1. \n\n\n\nTable 5 :\n5Letter error rates (%) with iterated forced-alignment (FA) adaptation. \n\n\n\n\nSigner-dependentSigner-independent Signer 1 Signer 2 Signer 3 Signer 4 Mean Signer 1 Signer 2 Signer 3 Signer 4 Mean7.2 \n6.5 \n8.1 \n8.6 \n7.6 \n13.0 \n11.2 \n21.7 \n18.8 \n16.2 \n\n\n\nTable 6 :\n6Letter error rates (%) obtained with a two-pass segmental cascade.\n\nTable A .\nA8: Description of digraphs.\nE.g., http://deafvideo.tv, http://aslized.org.\nParts of this work have appeared in our conference papers[11,12,13]. This paper includes additional model comparisons and improvements, different linguistic feature sets, and detailed presentation of the collected data and annotation.\nThe instructions, given in ASL, were to: \"proceed at normal speed and in your natural way of fingerspelling.\" 4 Through our deinterlacing process, additional lines were interpolated to give us a final result of 1920x1080 pixels per frame at 60 frames per second\nWe have also considered convolutional neural networks with raw image pixels as input, but these did not significantly outperform the DNNs on image features. See Sec. 5.3.3.\nSee[11,12,72] for details of the tuning parameters.\nAcknowledgementsWe are grateful for the work of several undergraduate assistants for annotation of the data. This research was funded by a Google Faculty Award and by NSF grants NSF-1433485 and NSF/BCS-1251807. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.Appendix A. Fingerspelled handshape definitions and annotation conventionsHandshape -We defined a handshape as stable if all of digits assumed a position and maintained it with only minor fluctuations. As soon as any digit moves, the handshape is considered to not be stable anymore. We were conservative with respect to holds, in that if a digit moves a small amount, but that movement is part of a larger movement that preceded or followed, that was not considered stable.p  180  180  90  180  90  90  90  90  0  0  90 180  m  dwn  q  180  180  90  90  90  90  90  90  0  0  90 180  m  dwn  r  180  180  180  180  90  90  90  90  -1  -45 0 180  r  for  s  90  90  90  90  90  90  90  90  0  -45 45 180  r  for  t  90  135  90  90  90  90  90  90  0  -45 90 180  m  for  u  180  180  180  180  90  90  90  90  0  -45 90 180  r  for  v  180  180  180  180  90  90  90  90  1  -45 90 180  r  for  w  180  180  180  180  180  180  90  90  1  -45 90 180  p  for  x  180  135  90  90  90  90  90  90  0  -45 45 180  m  for  y  90  90  90  90  90  90  180  180  1  90  0 180  -for  z  180  180  90  90  90  90  90  90  0  0  45 180  m  for  zz  180  180  180  180  90  90  90  90  1  0  45 180  m  for   Table B.9: Phonetic features[2]. The numerical values refer to joint angles in each finger.\nHow many people use ASL in the United States? Why estimates need updating. R E Mitchell, T A Young, B Bachleda, M A Karchmer, Sign Language Studies. 63R. E. Mitchell, T. A. Young, B. Bachleda, M. A. Karchmer, How many people use ASL in the United States? Why estimates need updating, Sign Language Studies 6 (3) (2006) 306-335.\n\nTowards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language. J Keane, University of ChicagoPh.D. thesisJ. Keane, Towards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language, Ph.D. thesis, University of Chicago (December 2014).\n\nA Prosodic Model of Sign Language Phonology. D Brentari, MIT PressD. Brentari, A Prosodic Model of Sign Language Phonology, MIT Press, 1998.\n\nModelling and recognition of the linguistic components in American Sign Language. L Ding, A M Martinez, Image and Vision Computing. 2712L. Ding, A. M. Martinez, Modelling and recognition of the linguistic compo- nents in American Sign Language, Image and Vision Computing 27 (12) (2009) 1826-1844.\n\nReal-time articulated hand pose estimation using semi-supervised transductive regression forests. D Tang, T.-H Yu, T.-K Kim, ICCVD. Tang, T.-H. Yu, T.-K. Kim, Real-time articulated hand pose estimation using semi-supervised transductive regression forests, in: ICCV, 2013.\n\nHow the alphabet came to be used in a sign language. C Padden, D C Gunsauls, Sign Language Studies. 41C. Padden, D. C. Gunsauls, How the alphabet came to be used in a sign language, Sign Language Studies 4 (1) (2003) 10-33.\n\nNative and foreign vocabulary in American Sign Language: A lexicon with multiple origins, in: Foreign vocabulary in sign languages: A cross-linguistic investigation of word formation. D Brentari, C Padden, Lawrence Erlbaum, Mahwah, NJD. Brentari, C. Padden, Native and foreign vocabulary in American Sign Lan- guage: A lexicon with multiple origins, in: Foreign vocabulary in sign lan- guages: A cross-linguistic investigation of word formation, Lawrence Erlbaum, Mahwah, NJ, 2001, pp. 87-119.\n\nDynamic fingerspelling recognition using geometric and motion features. P Goh, E.-J Holden, ICIPP. Goh, E.-J. Holden, Dynamic fingerspelling recognition using geometric and motion features, in: ICIP, 2006.\n\nAutomatic recognition of fingerspelled words in British Sign Language. S Liwicki, M Everingham, 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis. S. Liwicki, M. Everingham, Automatic recognition of fingerspelled words in British Sign Language, in: 2nd IEEE Workshop on CVPR for Human Commu- nicative Behavior Analysis, 2009.\n\nFingerspelling recognition through classification of letterto-letter transitions. S Ricco, C Tomasi, ACCVS. Ricco, C. Tomasi, Fingerspelling recognition through classification of letter- to-letter transitions, in: ACCV, 2009.\n\nAmerican Sign Language fingerspelling recognition with phonological feature-based tandem models. T Kim, K Livescu, G Shakhnarovich, Proc. IEEE Workshop on Spoken Language Technology (SLT). IEEE Workshop on Spoken Language Technology (SLT)T. Kim, K. Livescu, G. Shakhnarovich, American Sign Language fingerspelling recognition with phonological feature-based tandem models, in: Proc. IEEE Workshop on Spoken Language Technology (SLT), 2012.\n\nFingerspelling recognition with semi-Markov conditional random fields. T Kim, G Shakhnarovich, K Livescu, ICCVT. Kim, G. Shakhnarovich, K. Livescu, Fingerspelling recognition with semi- Markov conditional random fields, in: ICCV, 2013.\n\nSigner-independent fingerspelling recognition with deep neural network adaptation. T Kim, W Wang, H Tang, K Livescu, ICASSPT. Kim, W. Wang, H. Tang, K. Livescu, Signer-independent fingerspelling recog- nition with deep neural network adaptation, in: ICASSP, 2016.\n\nContinuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers, Computer Vision and Image Understanding. O Koller, J Forster, H Ney, 141O. Koller, J. Forster, H. Ney, Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers, Com- puter Vision and Image Understanding 141 (2015) 108-125.\n\nAutomatic sign language analysis: A survey and the future beyond lexical meaning. S C Ong, S Ranganath, IEEE transactions on pattern analysis and machine intelligence. 27S. C. Ong, S. Ranganath, Automatic sign language analysis: A survey and the future beyond lexical meaning, IEEE transactions on pattern analysis and ma- chine intelligence 27 (6) (2005) 873-891.\n\nRecognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove. C Oz, M C Leu, 2nd International Conference on Advances in Neural Networks. C. Oz, M. C. Leu, Recognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove, in: 2nd International Conference on Advances in Neural Networks, 2005.\n\nReal-time hand-tracking with a color glove. R Wang, J Popovic, SIG-GRAPHR. Wang, J. Popovic, Real-time hand-tracking with a color glove, in: SIG- GRAPH, 2009.\n\nN Pugeault, R Bowden, Spelling it out: Real-time ASL fingerspelling recognition, in: ICCV Workshops. N. Pugeault, R. Bowden, Spelling it out: Real-time ASL fingerspelling recogni- tion, in: ICCV Workshops, 2011.\n\nHand pose estimation and hand shape classification using multi-layered randomized decision forests. C Keskin, F K\u0131ra\u00e7, Y E Kara, L Akarun, ECCVC. Keskin, F. K\u0131ra\u00e7, Y. E. Kara, L. Akarun, Hand pose estimation and hand shape classification using multi-layered randomized decision forests, in: ECCV, 2012.\n\nCollecting and evaluating the cuny asl corpus for research on american sign language animation. P Lu, M Huenerfauth, Computer Speech and Language. 283P. Lu, M. Huenerfauth, Collecting and evaluating the cuny asl corpus for re- search on american sign language animation, Computer Speech and Language 28 (3) (2014) 812-831.\n\nHistogram of 3d facets: a depth descriptor for human action and hand gesture recognition. C Zhang, Y Tian, Computer Vision and Image Understanding. 139C. Zhang, Y. Tian, Histogram of 3d facets: a depth descriptor for human action and hand gesture recognition, Computer Vision and Image Understanding 139 (2015) 29-39.\n\nAmerican Sign Language alphabet recognition using Microsoft Kinect. C Dong, M C Lieu, Z Yin, CVPR Workshops. C. Dong, M. C. Lieu, Z. Yin, American Sign Language alphabet recognition using Microsoft Kinect, in: CVPR Workshops, 2015.\n\nPurdue ASL database for the recognition of American sign language. A M Martinez, R B Wilbur, R Shay, A C Kak, ICMIA. M. Martinez, R. B. Wilbur, R. Shay, A. C. Kak, Purdue ASL database for the recognition of American sign language, in: ICMI, 2002.\n\nBenchmark databases for video-based automatic sign language recognition. P Dreuw, C Neidle, V Athitsos, S Sclaroff, H Ney, International Conference on Language Resources and Evaluation (LREC). P. Dreuw, C. Neidle, V. Athitsos, S. Sclaroff, H. Ney, Benchmark databases for video-based automatic sign language recognition., in: International Conference on Language Resources and Evaluation (LREC), 2008.\n\nThe significance of facial features for automatic sign language recognition. U Von Agris, M Knorr, K.-F Kraiss, Proc. IEEE International Conference on Automatic Face and Gesture Recognition. IEEE International Conference on Automatic Face and Gesture RecognitionU. Von Agris, M. Knorr, K.-F. Kraiss, The significance of facial features for automatic sign language recognition, in: Proc. IEEE International Conference on Automatic Face and Gesture Recognition, 2008.\n\nSignSpeak -understanding, recognition, and translation of sign languages. P Dreuw, J Forster, Y Gweth, D Stein, H Ney, G Martinez, J V Llahi, O Crasborn, E Ormel, W Du, T Hoyoux, J Piater, J M M Lazaro, M Wheatley, Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT). Workshop on the Representation and essing of Sign Languages: Corpora and Sign Language Technologies (CSLT)P. Dreuw, J. Forster, Y. Gweth, D. Stein, H. Ney, G. Martinez, J. V. Llahi, O. Crasborn, E. Ormel, W. Du, T. Hoyoux, J. Piater, J. M. M. Lazaro, M. Wheat- ley, SignSpeak -understanding, recognition, and translation of sign languages, in: Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT), 2010.\n\nLarge lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms. V Athitsos, C Neidle, S Sclaroff, J Nash, A Stefan, A Thangali, H Wang, Q Yuan, Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT). V. Athitsos, C. Neidle, S. Sclaroff, J. Nash, A. Stefan, A. Thangali, H. Wang, Q. Yuan, Large lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms, in: Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT), 2010.\n\nC Neidle, C Vogler, A new web interface to facilitate access to corpora: Development of the ASLLRP data access interface (DAI), in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon. C. Neidle, C. Vogler, A new web interface to facilitate access to corpora: Devel- opment of the ASLLRP data access interface (DAI), in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, 2012.\n\nE Efthimiou, S.-E Fotinea, T Hanke, J Glauert, R Bowden, A Braffort, C Collet, P Maragos, F Lefebvre-Albaret, Sign language technologies and resources of the Dicta-Sign project, in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon. E. Efthimiou, S.-E. Fotinea, T. Hanke, J. Glauert, R. Bowden, A. Braffort, C. Collet, P. Maragos, F. Lefebvre-Albaret, Sign language technologies and re- sources of the Dicta-Sign project, in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, 2012.\n\nA large vocabulary sign language recognition and translation corpus. J Forster, C Schmidt, T Hoyoux, O Koller, U Zelle, J H Piater, H Ney, Rwth-Phoenix-Weather, International Conference on Language Resources and Evaluation (LREC). J. Forster, C. Schmidt, T. Hoyoux, O. Koller, U. Zelle, J. H. Piater, H. Ney, RWTH-PHOENIX-Weather: A large vocabulary sign language recognition and translation corpus., in: International Conference on Language Resources and Evaluation (LREC), 2012.\n\n. American Sign Language Lexicon Video Dataset. American Sign Language Lexicon Video Dataset. http://www.bu.edu/av/asllrp/dai-asllvd.html.\n\nNCSLGR. NCSLGR. http://www.bu.edu/asllrp/ncslgr-for-download/download-info.html.\n\nSignStream: A tool for linguistic and computer vision research on visual-gestural language data. C Neidle, S Sclaroff, V Athitsos, Behavior Research Methods. 333Instruments, & ComputersC. Neidle, S. Sclaroff, V. Athitsos, SignStream: A tool for linguistic and com- puter vision research on visual-gestural language data, Behavior Research Meth- ods, Instruments, & Computers 33 (3) (2001) 311-320.\n\nA linguistic feature vector for the visual interpretation of sign language. R Bowden, D Windridge, T Kadir, A Zisserman, M Brady, ECCVR. Bowden, D. Windridge, T. Kadir, A. Zisserman, M. Brady, A linguistic fea- ture vector for the visual interpretation of sign language, in: ECCV, 2004.\n\nDetecting coarticulation in sign language using conditional random fields. R Yang, S Sarkar, Proc. Intl. Conf. on Pattern Recognition (ICPR). Intl. Conf. on Pattern Recognition (ICPR)R. Yang, S. Sarkar, Detecting coarticulation in sign language using conditional random fields, in: Proc. Intl. Conf. on Pattern Recognition (ICPR), 2006.\n\nGeometric features for improving continuous appearance-based sign language recognition. M Zahedi, P Dreuw, D Rybach, T Deselaers, H Ney, BMVCM. Zahedi, P. Dreuw, D. Rybach, T. Deselaers, H. Ney, Geometric features for improving continuous appearance-based sign language recognition, in: BMVC, 2006.\n\nSign language recognition using a combination of new vision based features. M M Zaki, S I Shaheen, Pattern Recognition Letters. 324M. M. Zaki, S. I. Shaheen, Sign language recognition using a combination of new vision based features, Pattern Recognition Letters 32 (4) (2011) 572-577.\n\nTransfer learning in sign language. A Farhadi, D Forsyth, R White, CVPRA. Farhadi, D. Forsyth, R. White, Transfer learning in sign language, in: CVPR, 2007.\n\nSpeech recognition techniques for a sign language recognition system. P Dreuw, D Rybach, T Deselaers, M Zahedi, H Ney, Proc. Interspeech. InterspeechP. Dreuw, D. Rybach, T. Deselaers, M. Zahedi, , H. Ney, Speech recognition techniques for a sign language recognition system, in: Proc. Interspeech, 2007.\n\nAutomated extraction of signs from continuous sign language sentences using iterated conditional modes. S Nayak, S Sarkar, B Loeding, CVPRS. Nayak, S. Sarkar, B. Loeding, Automated extraction of signs from continuous sign language sentences using iterated conditional modes, in: CVPR, 2009.\n\nUpper body detection and tracking in extended signing sequences. P Buehler, M Everingham, D P Huttenlocher, A Zisserman, International Journal of Computer Vision. 952P. Buehler, M. Everingham, D. P. Huttenlocher, A. Zisserman, Upper body de- tection and tracking in extended signing sequences, International Journal of Computer Vision 95 (2) (2011) 180-197.\n\nAutomatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts. T Pfister, J Charles, M Everingham, A Zisserman, BMVCT. Pfister, J. Charles, M. Everingham, A. Zisserman, Automatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts, in: BMVC, 2012.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, CVPRN. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in: CVPR, 2005.\n\nSign language recognition using dynamic time warping and hand shape distance based on histogram of oriented gradient features. P Jangyodsuk, C Conly, V Athitsos, Proc. 7th International Conference on PErvasive Technologies Related to Assistive Environments. 7th International Conference on PErvasive Technologies Related to Assistive EnvironmentsP. Jangyodsuk, C. Conly, V. Athitsos, Sign language recognition using dynamic time warping and hand shape distance based on histogram of oriented gradi- ent features, in: Proc. 7th International Conference on PErvasive Technologies Related to Assistive Environments, 2014.\n\nReal-time american sign language recognition using desk and wearable computer based video. T Starner, J Weaver, A Pentland, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2012T. Starner, J. Weaver, A. Pentland, Real-time american sign language recognition using desk and wearable computer based video, IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (12) (1998) 1371-1375.\n\nParallel hidden Markov models for American Sign Language recognition. C Vogler, D Metaxas, ICCVC. Vogler, D. Metaxas, Parallel hidden Markov models for American Sign Lan- guage recognition, in: ICCV, 1999.\n\nExploiting phonological constraints for handshape inference in ASL video. A Thangali, J P Nash, S Sclaroff, C Neidle, CVPRA. Thangali, J. P. Nash, S. Sclaroff, C. Neidle, Exploiting phonological con- straints for handshape inference in ASL video, in: CVPR, 2011.\n\nHandshapes and movements: Multiple-channel ASL recognition. C Vogler, D Metaxas, Gesture Workshop. C. Vogler, D. Metaxas, Handshapes and movements: Multiple-channel ASL recognition, in: Gesture Workshop, 2003.\n\nProduct-HMMs for automatic sign language recognition. S Theodorakis, A Katsamanis, P Maragos, ICASSPS. Theodorakis, A. Katsamanis, P. Maragos, Product-HMMs for automatic sign language recognition, in: ICASSP, 2009.\n\nThe use of context in large vocabulary speech recognition. J Odell, University of CambridgePh.D. thesisJ. Odell, The use of context in large vocabulary speech recognition, Ph.D. thesis, University of Cambridge (March 1995).\n\nMoving beyond the 'beads-on-a-string' model of speech. M Ostendorf, Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)M. Ostendorf, Moving beyond the 'beads-on-a-string' model of speech, in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 1999.\n\nSubword modeling for automatic speech recognition: Past, present, and emerging approaches. K Livescu, E Fosler-Lussier, F Metze, IEEE Signal Processing Magazine. 296K. Livescu, E. Fosler-Lussier, F. Metze, Subword modeling for automatic speech recognition: Past, present, and emerging approaches, IEEE Signal Processing Magazine 29 (6) (2012) 44-57.\n\nAudiovisual automatic speech recognition. G Potamianos, C Neti, J Luettin, I Matthews, Audiovisual Speech Processing. Cambridge University PressG. Potamianos, C. Neti, J. Luettin, I. Matthews, Audiovisual Speech Processing, Cambridge University Press, 2015, Ch. Audiovisual automatic speech recogni- tion, pp. 193-247.\n\nA segment-based audio-visual speech recognizer: Data collection, development, and initial experiments. T J Hazen, K Saenko, C.-H La, J R Glass, ACMT. J. Hazen, K. Saenko, C.-H. La, J. R. Glass, A segment-based audio-visual speech recognizer: Data collection, development, and initial experiments, in: ICMI, ACM, 2004, pp. 235-242.\n\nMultistream articulatory featurebased models for visual speech recognition. K Saenko, K Livescu, J Glass, T Darrell, IEEE Transactions on Pattern Analysis and Machine Intelligence. 319K. Saenko, K. Livescu, J. Glass, T. Darrell, Multistream articulatory feature- based models for visual speech recognition, IEEE Transactions on Pattern Anal- ysis and Machine Intelligence 31 (9) (2009) 1700-1707.\n\nA framework for recognizing the simultaneous aspects of American Sign Language. C Vogler, D Metaxas, Computer Vision and Image Understanding. 81C. Vogler, D. Metaxas, A framework for recognizing the simultaneous aspects of American Sign Language, Computer Vision and Image Understanding 81 (2001) 358-384.\n\nToward scalability in ASL recognition: Breaking down signs into phonemes. C Vogler, D Metaxas, Gesture Workshop. C. Vogler, D. Metaxas, Toward scalability in ASL recognition: Breaking down signs into phonemes, in: Gesture Workshop, 1999.\n\nAdvances in phoneticsbased sub-unit modeling for transcription alignment and sign language recognition. V Pitsikalis, S Theodorakis, C Vogler, P Maragos, IEEE CVPR Workshop on Gesture Recognition. V. Pitsikalis, S. Theodorakis, C. Vogler, P. Maragos, Advances in phonetics- based sub-unit modeling for transcription alignment and sign language recogni- tion, in: IEEE CVPR Workshop on Gesture Recognition, 2011.\n\nModel-level data-driven sub-units for signs in videos of continuous sign language. S Theodorakis, V Pitsikalis, P Maragos, ICASSPS. Theodorakis, V. Pitsikalis, P. Maragos, Model-level data-driven sub-units for signs in videos of continuous sign language, in: ICASSP, 2010.\n\nA prosodic model of sign language phonology. D Brentari, The MIT PressD. Brentari, A prosodic model of sign language phonology, The MIT Press, 1998.\n\nAdapting hidden Markov models for ASL recognition by using three-dimensional computer vision methods. C Vogler, D Metaxas, IEEE International Conference on Systems, Man and Cybernetics. C. Vogler, D. Metaxas, Adapting hidden Markov models for ASL recognition by using three-dimensional computer vision methods, in: IEEE International Conference on Systems, Man and Cybernetics, 1997.\n\nHandling movement epenthesis and hand segmentation ambiguities in continuous sign language recognition using nested dynamic programming. R Yang, S Sarkar, B Loeding, IEEE Transactions on Pattern Analysis and Machine Intelligence. 323R. Yang, S. Sarkar, B. Loeding, Handling movement epenthesis and hand seg- mentation ambiguities in continuous sign language recognition using nested dy- namic programming, IEEE Transactions on Pattern Analysis and Machine Intel- ligence 32 (3) (2010) 462-477.\n\nAdvances in dynamic-static integration of manual cues for sign language recognition. S Theodorakis, V Pitsikalis, P Maragos, The 9th International Gesture Workshop: Gesture in Embodied Communication and Human-Computer Interaction (GW). S. Theodorakis, V. Pitsikalis, P. Maragos, Advances in dynamic-static integra- tion of manual cues for sign language recognition, in: The 9th International Ges- ture Workshop: Gesture in Embodied Communication and Human-Computer Interaction (GW), 2011.\n\nSign transition modeling and a scalable solution to continuous sign language recognition for real-world applications. K Li, Z Zhou, C.-H Lee, ACM Transactions on Accessible Computing (TACCESS). 827K. Li, Z. Zhou, C.-H. Lee, Sign transition modeling and a scalable solution to continuous sign language recognition for real-world applications, ACM Trans- actions on Accessible Computing (TACCESS) 8 (2) (2016) 7.\n\nBoostMap: A method for efficient approximate similarity rankings. V Athitsos, J Alon, S Sclaroff, G Kollios, CVPRV. Athitsos, J. Alon, S. Sclaroff, G. Kollios, BoostMap: A method for efficient approximate similarity rankings, in: CVPR, 2004.\n\nAffine-invariant modeling of shape-appearance images applied on sign language handshape classification. A Roussos, S Theodorakis, V Pitsikalis, P Maragos, ICIPA. Roussos, S. Theodorakis, V. Pitsikalis, P. Maragos, Affine-invariant modeling of shape-appearance images applied on sign language handshape classification, in: ICIP, 2010.\n\nRapid signer adaptation for continuous sign language recognition using a combined approach of eigenvoices, MLLR, and MAP. U Von Agris, C Blomer, K.-F Kraiss, Proc. Intl. Conf. on Pattern Recognition (ICPR). Intl. Conf. on Pattern Recognition (ICPR)U. Von Agris, C. Blomer, K.-F. Kraiss, Rapid signer adaptation for continuous sign language recognition using a combined approach of eigenvoices, MLLR, and MAP, in: Proc. Intl. Conf. on Pattern Recognition (ICPR), 2008.\n\nDeciphering gestures with layered meanings and signer adaptation. S C Ong, S Ranganath, Proc. IEEE International Conference on Automatic Face and Gesture Recognition. IEEE International Conference on Automatic Face and Gesture RecognitionS. C. Ong, S. Ranganath, Deciphering gestures with layered meanings and signer adaptation, in: Proc. IEEE International Conference on Automatic Face and Gesture Recognition, 2004.\n\nTandem connectionist feature extraction for conventional HMM systems. H Hermansky, D P W Ellis, S Sharma, ICASSPH. Hermansky, D. P. W. Ellis, S. Sharma, Tandem connectionist feature extrac- tion for conventional HMM systems, in: ICASSP, 2000.\n\nProbabilistic and bottle-neck features for LVCSR of meetings. F Gr\u00e9zl, M Karafi\u00e1t, S Kont\u00e1r, J Cernocky, ICASSPF. Gr\u00e9zl, M. Karafi\u00e1t, S. Kont\u00e1r, J. Cernocky, Probabilistic and bottle-neck fea- tures for LVCSR of meetings, in: ICASSP, 2007.\n\nSegmental CRF approach to large vocabulary continuous speech recognition. G Zweig, P Nguyen, Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)G. Zweig, P. Nguyen, Segmental CRF approach to large vocabulary continuous speech recognition, in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2009.\n\nDiscriminative segmental cascades for feature-rich phone recognition. H Tang, W Wang, K Gimpel, K Livescu, Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)H. Tang, W. Wang, K. Gimpel, K. Livescu, Discriminative segmental cascades for feature-rich phone recognition, in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.\n\nL Lu, L Kong, C Dyer, N A Smith, S Renals, arXiv:1603.00223Segmental recurrent neural networks for end-to-end speech recognition. arXiv preprintL. Lu, L. Kong, C. Dyer, N. A. Smith, S. Renals, Segmental recurrent neural networks for end-to-end speech recognition, arXiv preprint arXiv:1603.00223.\n\nG Zweig, P Nguyen, D Van Compernolle, K Demuynck, L Atlas, P Clark, G Sell, M Wang, F Sha, H Hermansky, Speech recognition with segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop. ICASSPG. Zweig, P. Nguyen, D. Van Compernolle, K. Demuynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha, H. Hermansky, et al., Speech recognition with seg- mental conditional random fields: A summary of the jhu clsp 2010 summer workshop, in: ICASSP, 2011.\n\nEfficient segmental conditional random fields for phone recognition. Y He, E Fosler-Lussier, Proc. Interspeech. InterspeechY. He, E. Fosler-Lussier, Efficient segmental conditional random fields for phone recognition, in: Proc. Interspeech, 2012.\n\nSemi-Markov conditional random fields for information extraction. S Sarawagi, W W Cohen, NIPSS. Sarawagi, W. W. Cohen, Semi-Markov conditional random fields for infor- mation extraction, in: NIPS, 2004.\n\nHuman action segmentation and recognition using discriminative semi-Markov models. Q Shi, L Cheng, L Wang, A Smola, International Journal of Computer Vision. 931Q. Shi, L. Cheng, L. Wang, A. Smola, Human action segmentation and recog- nition using discriminative semi-Markov models, International Journal of Com- puter Vision 93 (1).\n\nActivity recognition and abnormality detection with the switching hidden semi-Markov model. T V Duong, H H Bui, D Q Phung, S Venkatesh, CVPRT. V. Duong, H. H. Bui, D. Q. Phung, S. Venkatesh, Activity recognition and ab- normality detection with the switching hidden semi-Markov model, in: CVPR, 2005.\n\nSign language spotting based on semi-Markov conditional random field. S.-S Cho, H.-D Yang, S.-W Lee, Workshop on the Applications of Computer Vision. S.-S. Cho, H.-D. Yang, S.-W. Lee, Sign language spotting based on semi- Markov conditional random field, in: Workshop on the Applications of Com- puter Vision, 2009.\n\nTowards subject independent continuous sign language recognition: A segment and merge approach. W W Kong, S Ranganath, Pattern Recognition. 473W. W. Kong, S. Ranganath, Towards subject independent continuous sign lan- guage recognition: A segment and merge approach, Pattern Recognition 47 (3).\n\nAnnotation by category: ELAN and ISO DCR. H Sloetjes, P Wittenburg, International Conference on Language Resources and Evaluation (LREC). H. Sloetjes, P. Wittenburg, Annotation by category: ELAN and ISO DCR., in: International Conference on Language Resources and Evaluation (LREC), 2008.\n\nJ Keane, D Brentari, J Riggle, Annual Meeting of the North East Linguistic Society. Coarticulation in ASL fingerspellingJ. Keane, D. Brentari, J. Riggle, Coarticulation in ASL fingerspelling, in: An- nual Meeting of the North East Linguistic Society, 2012.\n\nHandshape and coarticulation in ASL fingerspelling, conference presentation, linguistic Society of America. J Keane, D Brentari, J Riggle, Annual Meeting. J. Keane, D. Brentari, J. Riggle, Handshape and coarticulation in ASL finger- spelling, conference presentation, linguistic Society of America 2012 Annual Meeting (January 2012).\n\nDispelling prescriptive rules in ASL fingerspelling: the case of -E-, poster, theoretical Issues in Sign Language Research 11. J Keane, D Brentari, J Riggle, London, UKJ. Keane, D. Brentari, J. Riggle, Dispelling prescriptive rules in ASL finger- spelling: the case of -E-, poster, theoretical Issues in Sign Language Research 11; London, UK (July 2013).\n\nIntegrating multilingual articulatory features into speech recognition. S St\u00fcker, F Metze, T Schultz, A Waibel, Proc. Interspeech. InterspeechS. St\u00fcker, F. Metze, T. Schultz, A. Waibel, Integrating multilingual articulatory features into speech recognition., in: Proc. Interspeech, 2003.\n\nAn articulatory feature-based tandem approach and factored observation modeling. O Etin, A Kantor, S King, C Bartels, M Magimai-Doss, J Frankel, K Livescu, ICASSPO. \u00c7 etin, A. Kantor, S. King, C. Bartels, M. Magimai-doss, J. Frankel, K. Livescu, An articulatory feature-based tandem approach and factored obser- vation modeling, in: ICASSP, 2007.\n\n. D Graff, R Rosenfeld, D Paul, Csr-Iii Text, D. Graff, R. Rosenfeld, D. Paul, CSR-III text, http://http://www.ldc. upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6 (1995).\n\nDeep segmental neural networks for speech recognition. O Hamid, L Deng, D Yu, H Jiang, Proc. Interspeech. InterspeechO. Abdel-Hamid, L. Deng, D. Yu, H. Jiang, Deep segmental neural networks for speech recognition, in: Proc. Interspeech, 2013.\n\nSegmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition. Y He, E Fosler-Lussier, Proc. Interspeech. InterspeechY. He, E. Fosler-Lussier, Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition, in: Proc. Inter- speech, 2015.\n\nH Liao, Speaker adaptation of context dependent deep neural networks, in: ICASSP. H. Liao, Speaker adaptation of context dependent deep neural networks, in: ICASSP, 2013.\n\nFast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code. O Hamid, H Jiang, ICASSPO. Abdel-Hamid, H. Jiang, Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code, in: ICASSP, 2013.\n\nLearning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models. P Swietojanski, S Renals, Proc. IEEE Workshop on Spoken Language Technology (SLT). IEEE Workshop on Spoken Language Technology (SLT)P. Swietojanski, S. Renals, Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models, in: Proc. IEEE Workshop on Spoken Language Technology (SLT), 2014.\n\nSpeaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition. R Doddipatla, M Hasan, T Hain, Proc. Interspeech. InterspeechR. Doddipatla, M. Hasan, T. Hain, Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition, in: Proc. Interspeech, 2014.\n\nRobinson, Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system. J Neto, L Almeida, M Hochberg, C Martins, L Nunes, S Renals, T , Proc. Eurospeech. EurospeechJ. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, T. Robin- son, Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system, in: Proc. Eurospeech, 1995.\n\nAdaptation of context-dependent deep neural networks for automatic speech recognition. K Yao, D Yu, F Seide, H Su, L Deng, Y Gong, Proc. IEEE Workshop on Spoken Language Technology (SLT). IEEE Workshop on Spoken Language Technology (SLT)K. Yao, D. Yu, F. Seide, H. Su, L. Deng, Y. Gong, Adaptation of context-dependent deep neural networks for automatic speech recognition, in: Proc. IEEE Workshop on Spoken Language Technology (SLT), 2012.\n\nComparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems. B Li, K C Sim, Proc. Interspeech. InterspeechB. Li, K. C. Sim, Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems, in: Proc. Interspeech, 2010.\n\nRapid object detection using a boosted cascade of simple features. P Viola, M J Jones, CVPRP. Viola, M. J. Jones, Rapid object detection using a boosted cascade of simple features, in: CVPR, 2001.\n\nOn rectified linear units for speech processing. M D Zeiler, M Ranzato, R Monga, M Mao, K Yang, Q V Le, P Nguyen, A Senior, V Vanhoucke, J Dean, G E Hinton, ICASSPM. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, G. E. Hinton, On rectified linear units for speech processing, in: ICASSP, 2013.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R R Salakhutdinov, Journal of Machine Learing Research. 15N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, Journal of Machine Learing Research 15 (2014) 1929-1958.\n\nSRILM at sixteen: update and outlook. A Stolcke, J Zheng, W Wang, V Abrash, Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)A. Stolcke, J. Zheng, W. Wang, V. Abrash, SRILM at sixteen: update and out- look, in: Proc. IEEE Workshop on Automatic Speech Recognition and Under- standing (ASRU), 2011.\n\nA Krizhevsky, I Sutskever, G E Hinton, Imagenet classification with deep convolutional neural networks. Advances in neural information processing systemsA. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Advances in neural information processing systems, 2012, pp. 1097-1105.\n\nK Simonyan, A Zisserman, arXiv:1409.1556Very deep convolutional networks for large-scale image recognition. arXiv preprintK. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556.\n\n. F Chollet, F. Chollet, keras, https://github.com/fchollet/keras (2015).\n\nTheano: A Python framework for fast computation of mathematical expressions, arXiv e-prints abs/1605.02688. Theano Development Team, Theano: A Python framework for fast computation of mathematical expressions, arXiv e-prints abs/1605.02688. URL http://arxiv.org/abs/1605.02688\n\nSCTK. SCTK. http://www.nist.gov/speech/tools/index.htm.\n", "annotations": {"author": "[{\"end\":182,\"start\":91},{\"end\":284,\"start\":183},{\"end\":376,\"start\":285},{\"end\":465,\"start\":377},{\"end\":565,\"start\":466},{\"end\":667,\"start\":566},{\"end\":769,\"start\":668},{\"end\":863,\"start\":770}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":99},{\"end\":197,\"start\":192},{\"end\":296,\"start\":292},{\"end\":385,\"start\":381},{\"end\":478,\"start\":472},{\"end\":587,\"start\":574},{\"end\":682,\"start\":674},{\"end\":783,\"start\":776}]", "author_first_name": "[{\"end\":98,\"start\":91},{\"end\":191,\"start\":183},{\"end\":291,\"start\":285},{\"end\":380,\"start\":377},{\"end\":471,\"start\":466},{\"end\":573,\"start\":566},{\"end\":673,\"start\":668},{\"end\":775,\"start\":770}]", "author_affiliation": "[{\"end\":181,\"start\":104},{\"end\":283,\"start\":199},{\"end\":375,\"start\":298},{\"end\":464,\"start\":387},{\"end\":564,\"start\":480},{\"end\":666,\"start\":589},{\"end\":768,\"start\":684},{\"end\":862,\"start\":785}]", "title": "[{\"end\":88,\"start\":1},{\"end\":951,\"start\":864}]", "venue": null, "abstract": "[{\"end\":2022,\"start\":1045}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2249,\"start\":2246},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3650,\"start\":3647},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3802,\"start\":3799},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3804,\"start\":3802},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3902,\"start\":3899},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4496,\"start\":4495},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4910,\"start\":4909},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5046,\"start\":5043},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5124,\"start\":5121},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6968,\"start\":6964},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6971,\"start\":6968},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6974,\"start\":6971},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7854,\"start\":7850},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9275,\"start\":9272},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9277,\"start\":9275},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9280,\"start\":9277},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11521,\"start\":11520},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11789,\"start\":11785},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11816,\"start\":11812},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12024,\"start\":12020},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12027,\"start\":12024},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12030,\"start\":12027},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12033,\"start\":12030},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12036,\"start\":12033},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12039,\"start\":12036},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12042,\"start\":12039},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12385,\"start\":12381},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12388,\"start\":12385},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12391,\"start\":12388},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12394,\"start\":12391},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12397,\"start\":12394},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12400,\"start\":12397},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12507,\"start\":12503},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12526,\"start\":12522},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12627,\"start\":12623},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12630,\"start\":12627},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12633,\"start\":12630},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12996,\"start\":12992},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12999,\"start\":12996},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13625,\"start\":13621},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13628,\"start\":13625},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13631,\"start\":13628},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13634,\"start\":13631},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13687,\"start\":13683},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13690,\"start\":13687},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13693,\"start\":13690},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13695,\"start\":13693},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13717,\"start\":13713},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13720,\"start\":13717},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13927,\"start\":13923},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14006,\"start\":14002},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14009,\"start\":14006},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14084,\"start\":14080},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14087,\"start\":14084},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14090,\"start\":14087},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14093,\"start\":14090},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14359,\"start\":14355},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14362,\"start\":14359},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14434,\"start\":14430},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14482,\"start\":14478},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14485,\"start\":14482},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14488,\"start\":14485},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14699,\"start\":14695},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14828,\"start\":14824},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":14831,\"start\":14828},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":15016,\"start\":15012},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15019,\"start\":15016},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":15022,\"start\":15019},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15242,\"start\":15238},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15244,\"start\":15242},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15247,\"start\":15244},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15250,\"start\":15247},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":15253,\"start\":15250},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15256,\"start\":15253},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15259,\"start\":15256},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":15262,\"start\":15259},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":15265,\"start\":15262},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":15363,\"start\":15359},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15365,\"start\":15363},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":15599,\"start\":15595},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15602,\"start\":15599},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":15605,\"start\":15602},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":15608,\"start\":15605},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":15723,\"start\":15719},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":15726,\"start\":15723},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15729,\"start\":15726},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15732,\"start\":15729},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15776,\"start\":15773},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15778,\"start\":15776},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15781,\"start\":15778},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16274,\"start\":16270},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":16277,\"start\":16274},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":16280,\"start\":16277},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":16608,\"start\":16604},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":16611,\"start\":16608},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":16793,\"start\":16789},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":16796,\"start\":16793},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":16799,\"start\":16796},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":16802,\"start\":16799},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":16805,\"start\":16802},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":16904,\"start\":16900},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":17060,\"start\":17056},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":17063,\"start\":17060},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":17241,\"start\":17237},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":17275,\"start\":17271},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":17932,\"start\":17928},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":17935,\"start\":17932},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":18009,\"start\":18005},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19168,\"start\":19167},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19910,\"start\":19907},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":20659,\"start\":20655},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":23835,\"start\":23831},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":23838,\"start\":23835},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":23841,\"start\":23838},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23843,\"start\":23841},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":24306,\"start\":24302},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24605,\"start\":24602},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":27185,\"start\":27181},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":27188,\"start\":27185},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":28034,\"start\":28030},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":28037,\"start\":28034},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":29262,\"start\":29258},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29359,\"start\":29356},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29674,\"start\":29671},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":29764,\"start\":29760},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":29767,\"start\":29764},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31359,\"start\":31355},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":32489,\"start\":32485},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":32492,\"start\":32489},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":32495,\"start\":32492},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":33940,\"start\":33936},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":35214,\"start\":35210},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":35257,\"start\":35253},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":37305,\"start\":37301},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":37308,\"start\":37305},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":37311,\"start\":37308},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":37314,\"start\":37311},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":37466,\"start\":37462},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":37469,\"start\":37466},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":37472,\"start\":37469},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39082,\"start\":39079},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39085,\"start\":39082},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39088,\"start\":39085},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":39862,\"start\":39858},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":40655,\"start\":40651},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":42000,\"start\":41996},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":42198,\"start\":42194},{\"end\":46006,\"start\":46001},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":46088,\"start\":46083},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":51319,\"start\":51318},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":52182,\"start\":52179},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":53129,\"start\":53125},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":53794,\"start\":53789},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":53798,\"start\":53794},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":55356,\"start\":55351},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":55374,\"start\":55369},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":57105,\"start\":57101},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":58508,\"start\":58503},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":61934,\"start\":61933},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":64099,\"start\":64098},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":64253,\"start\":64252},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":64455,\"start\":64452},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":65998,\"start\":65995},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":66036,\"start\":66033},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":71556,\"start\":71552},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":71559,\"start\":71556},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":71562,\"start\":71559},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":72172,\"start\":72168},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":72175,\"start\":72172},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":72178,\"start\":72175}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64391,\"start\":64349},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64550,\"start\":64392},{\"attributes\":{\"id\":\"fig_2\"},\"end\":64640,\"start\":64551},{\"attributes\":{\"id\":\"fig_3\"},\"end\":64782,\"start\":64641},{\"attributes\":{\"id\":\"fig_4\"},\"end\":64879,\"start\":64783},{\"attributes\":{\"id\":\"fig_5\"},\"end\":64998,\"start\":64880},{\"attributes\":{\"id\":\"fig_6\"},\"end\":65501,\"start\":64999},{\"attributes\":{\"id\":\"fig_7\"},\"end\":65706,\"start\":65502},{\"attributes\":{\"id\":\"fig_9\"},\"end\":65777,\"start\":65707},{\"attributes\":{\"id\":\"fig_10\"},\"end\":66037,\"start\":65778},{\"attributes\":{\"id\":\"fig_11\"},\"end\":66153,\"start\":66038},{\"attributes\":{\"id\":\"fig_12\"},\"end\":70302,\"start\":66154},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":70838,\"start\":70303},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":70916,\"start\":70839},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":71067,\"start\":70917},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":71153,\"start\":71068},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":71328,\"start\":71154},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":71407,\"start\":71329},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":71447,\"start\":71408}]", "paragraph": "[{\"end\":2595,\"start\":2038},{\"end\":3452,\"start\":2597},{\"end\":4081,\"start\":3454},{\"end\":4755,\"start\":4083},{\"end\":5207,\"start\":4757},{\"end\":6299,\"start\":5209},{\"end\":7738,\"start\":6315},{\"end\":8342,\"start\":7755},{\"end\":10181,\"start\":8344},{\"end\":11522,\"start\":10183},{\"end\":11865,\"start\":11539},{\"end\":12319,\"start\":11867},{\"end\":13415,\"start\":12321},{\"end\":14010,\"start\":13417},{\"end\":14489,\"start\":14012},{\"end\":15609,\"start\":14491},{\"end\":16046,\"start\":15611},{\"end\":16388,\"start\":16048},{\"end\":18010,\"start\":16390},{\"end\":18429,\"start\":18045},{\"end\":18610,\"start\":18431},{\"end\":19271,\"start\":18630},{\"end\":19911,\"start\":19273},{\"end\":20758,\"start\":19913},{\"end\":20886,\"start\":20760},{\"end\":21305,\"start\":20901},{\"end\":22187,\"start\":21324},{\"end\":22706,\"start\":22189},{\"end\":23292,\"start\":22728},{\"end\":23652,\"start\":23294},{\"end\":23813,\"start\":23654},{\"end\":24761,\"start\":23815},{\"end\":25585,\"start\":24785},{\"end\":25808,\"start\":25587},{\"end\":26210,\"start\":25810},{\"end\":26500,\"start\":26212},{\"end\":27947,\"start\":26516},{\"end\":28685,\"start\":27964},{\"end\":29676,\"start\":28687},{\"end\":30534,\"start\":29705},{\"end\":30981,\"start\":30666},{\"end\":31313,\"start\":31009},{\"end\":31540,\"start\":31315},{\"end\":31770,\"start\":31542},{\"end\":32031,\"start\":31772},{\"end\":32384,\"start\":32106},{\"end\":32582,\"start\":32386},{\"end\":33566,\"start\":32584},{\"end\":33690,\"start\":33568},{\"end\":34152,\"start\":33730},{\"end\":34235,\"start\":34189},{\"end\":34400,\"start\":34237},{\"end\":35709,\"start\":34429},{\"end\":35937,\"start\":35711},{\"end\":36170,\"start\":35960},{\"end\":37168,\"start\":36189},{\"end\":37642,\"start\":37170},{\"end\":38098,\"start\":37644},{\"end\":38464,\"start\":38100},{\"end\":38686,\"start\":38466},{\"end\":39025,\"start\":38711},{\"end\":39545,\"start\":39027},{\"end\":40582,\"start\":39599},{\"end\":41406,\"start\":40584},{\"end\":41603,\"start\":41447},{\"end\":42924,\"start\":41605},{\"end\":43625,\"start\":42926},{\"end\":44257,\"start\":43627},{\"end\":44531,\"start\":44259},{\"end\":45202,\"start\":44533},{\"end\":46869,\"start\":45271},{\"end\":47033,\"start\":46871},{\"end\":47590,\"start\":47035},{\"end\":48104,\"start\":47625},{\"end\":48403,\"start\":48106},{\"end\":49113,\"start\":48434},{\"end\":49922,\"start\":49115},{\"end\":50412,\"start\":49950},{\"end\":51320,\"start\":50486},{\"end\":51879,\"start\":51322},{\"end\":52658,\"start\":51920},{\"end\":53427,\"start\":52660},{\"end\":54223,\"start\":53455},{\"end\":55871,\"start\":54225},{\"end\":56805,\"start\":55934},{\"end\":57257,\"start\":56855},{\"end\":58194,\"start\":57259},{\"end\":58728,\"start\":58196},{\"end\":59435,\"start\":58743},{\"end\":59680,\"start\":59437},{\"end\":59924,\"start\":59682},{\"end\":60044,\"start\":59926},{\"end\":60428,\"start\":60046},{\"end\":60547,\"start\":60430},{\"end\":60698,\"start\":60549},{\"end\":61048,\"start\":60700},{\"end\":61184,\"start\":61050},{\"end\":61637,\"start\":61186},{\"end\":61785,\"start\":61639},{\"end\":62389,\"start\":61787},{\"end\":62686,\"start\":62391},{\"end\":64055,\"start\":62688},{\"end\":64348,\"start\":64057}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":29688,\"start\":29677},{\"attributes\":{\"id\":\"formula_1\"},\"end\":30665,\"start\":30535},{\"attributes\":{\"id\":\"formula_2\"},\"end\":32105,\"start\":32032},{\"attributes\":{\"id\":\"formula_3\"},\"end\":33729,\"start\":33691},{\"attributes\":{\"id\":\"formula_4\"},\"end\":34188,\"start\":34153},{\"attributes\":{\"id\":\"formula_5\"},\"end\":39598,\"start\":39546}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":47544,\"start\":47537},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":51591,\"start\":51587}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2036,\"start\":2024},{\"attributes\":{\"n\":\"2.1\"},\"end\":6313,\"start\":6302},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":7753,\"start\":7741},{\"attributes\":{\"n\":\"2.\"},\"end\":11537,\"start\":11525},{\"attributes\":{\"n\":\"3.\"},\"end\":18043,\"start\":18013},{\"attributes\":{\"n\":\"3.1.\"},\"end\":18628,\"start\":18613},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20899,\"start\":20889},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":21322,\"start\":21308},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":22726,\"start\":22709},{\"attributes\":{\"n\":\"4.\"},\"end\":24783,\"start\":24764},{\"attributes\":{\"n\":\"4.1.\"},\"end\":26514,\"start\":26503},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":27962,\"start\":27950},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":29703,\"start\":29690},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":31007,\"start\":30984},{\"attributes\":{\"n\":\"4.1.4.\"},\"end\":34427,\"start\":34403},{\"end\":35958,\"start\":35940},{\"attributes\":{\"n\":\"4.2.\"},\"end\":36187,\"start\":36173},{\"attributes\":{\"n\":\"5.\"},\"end\":38709,\"start\":38689},{\"attributes\":{\"n\":\"5.1.\"},\"end\":41445,\"start\":41409},{\"attributes\":{\"n\":\"5.2.\"},\"end\":45238,\"start\":45205},{\"attributes\":{\"n\":\"5.2.1.\"},\"end\":45269,\"start\":45241},{\"attributes\":{\"n\":\"5.2.2.\"},\"end\":47623,\"start\":47593},{\"attributes\":{\"n\":\"5.2.3.\"},\"end\":48432,\"start\":48406},{\"attributes\":{\"n\":\"5.3.\"},\"end\":49948,\"start\":49925},{\"attributes\":{\"n\":\"5.3.1.\"},\"end\":50484,\"start\":50415},{\"attributes\":{\"n\":\"5.3.2.\"},\"end\":51918,\"start\":51882},{\"attributes\":{\"n\":\"5.3.3.\"},\"end\":53453,\"start\":53430},{\"attributes\":{\"n\":\"5.3.4.\"},\"end\":55932,\"start\":55874},{\"attributes\":{\"n\":\"5.3.5.\"},\"end\":56853,\"start\":56808},{\"attributes\":{\"n\":\"6.\"},\"end\":58741,\"start\":58731},{\"end\":64403,\"start\":64393},{\"end\":64566,\"start\":64552},{\"end\":64656,\"start\":64642},{\"end\":64794,\"start\":64784},{\"end\":64891,\"start\":64881},{\"end\":65513,\"start\":65503},{\"end\":65718,\"start\":65708},{\"end\":65789,\"start\":65779},{\"end\":66050,\"start\":66039},{\"end\":70849,\"start\":70840},{\"end\":70927,\"start\":70918},{\"end\":71078,\"start\":71069},{\"end\":71339,\"start\":71330},{\"end\":71418,\"start\":71409}]", "table": "[{\"end\":70838,\"start\":70359},{\"end\":71067,\"start\":70929},{\"end\":71153,\"start\":71080},{\"end\":71328,\"start\":71272}]", "figure_caption": "[{\"end\":64391,\"start\":64351},{\"end\":64550,\"start\":64405},{\"end\":64640,\"start\":64569},{\"end\":64782,\"start\":64659},{\"end\":64879,\"start\":64796},{\"end\":64998,\"start\":64893},{\"end\":65501,\"start\":65001},{\"end\":65706,\"start\":65515},{\"end\":65777,\"start\":65720},{\"end\":66037,\"start\":65791},{\"end\":66153,\"start\":66053},{\"end\":70302,\"start\":66156},{\"end\":70359,\"start\":70305},{\"end\":70916,\"start\":70851},{\"end\":71272,\"start\":71156},{\"end\":71407,\"start\":71341},{\"end\":71447,\"start\":71420}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4446,\"start\":4440},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4586,\"start\":4580},{\"end\":4735,\"start\":4729},{\"end\":5338,\"start\":5330},{\"end\":8613,\"start\":8605},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20884,\"start\":20878},{\"end\":23812,\"start\":23806},{\"end\":24896,\"start\":24890},{\"end\":25188,\"start\":25182},{\"end\":25395,\"start\":25387},{\"end\":25925,\"start\":25919},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28324,\"start\":28318},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37364,\"start\":37358},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37691,\"start\":37685},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38153,\"start\":38147},{\"end\":40367,\"start\":40361},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":43685,\"start\":43679},{\"end\":44170,\"start\":44163},{\"end\":49810,\"start\":49804},{\"end\":50046,\"start\":50038},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52564,\"start\":52558},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":55383,\"start\":55376}]", "bib_author_first_name": "[{\"end\":73919,\"start\":73918},{\"end\":73921,\"start\":73920},{\"end\":73933,\"start\":73932},{\"end\":73935,\"start\":73934},{\"end\":73944,\"start\":73943},{\"end\":73956,\"start\":73955},{\"end\":73958,\"start\":73957},{\"end\":74320,\"start\":74319},{\"end\":74618,\"start\":74617},{\"end\":74797,\"start\":74796},{\"end\":74805,\"start\":74804},{\"end\":74807,\"start\":74806},{\"end\":75112,\"start\":75111},{\"end\":75123,\"start\":75119},{\"end\":75132,\"start\":75128},{\"end\":75341,\"start\":75340},{\"end\":75351,\"start\":75350},{\"end\":75353,\"start\":75352},{\"end\":75697,\"start\":75696},{\"end\":75709,\"start\":75708},{\"end\":76080,\"start\":76079},{\"end\":76090,\"start\":76086},{\"end\":76286,\"start\":76285},{\"end\":76297,\"start\":76296},{\"end\":76642,\"start\":76641},{\"end\":76651,\"start\":76650},{\"end\":76884,\"start\":76883},{\"end\":76891,\"start\":76890},{\"end\":76902,\"start\":76901},{\"end\":77299,\"start\":77298},{\"end\":77306,\"start\":77305},{\"end\":77323,\"start\":77322},{\"end\":77548,\"start\":77547},{\"end\":77555,\"start\":77554},{\"end\":77563,\"start\":77562},{\"end\":77571,\"start\":77570},{\"end\":77893,\"start\":77892},{\"end\":77903,\"start\":77902},{\"end\":77914,\"start\":77913},{\"end\":78222,\"start\":78221},{\"end\":78224,\"start\":78223},{\"end\":78231,\"start\":78230},{\"end\":78645,\"start\":78644},{\"end\":78651,\"start\":78650},{\"end\":78653,\"start\":78652},{\"end\":78994,\"start\":78993},{\"end\":79002,\"start\":79001},{\"end\":79110,\"start\":79109},{\"end\":79122,\"start\":79121},{\"end\":79423,\"start\":79422},{\"end\":79433,\"start\":79432},{\"end\":79442,\"start\":79441},{\"end\":79444,\"start\":79443},{\"end\":79452,\"start\":79451},{\"end\":79723,\"start\":79722},{\"end\":79729,\"start\":79728},{\"end\":80041,\"start\":80040},{\"end\":80050,\"start\":80049},{\"end\":80338,\"start\":80337},{\"end\":80346,\"start\":80345},{\"end\":80348,\"start\":80347},{\"end\":80356,\"start\":80355},{\"end\":80570,\"start\":80569},{\"end\":80572,\"start\":80571},{\"end\":80584,\"start\":80583},{\"end\":80586,\"start\":80585},{\"end\":80596,\"start\":80595},{\"end\":80604,\"start\":80603},{\"end\":80606,\"start\":80605},{\"end\":80824,\"start\":80823},{\"end\":80833,\"start\":80832},{\"end\":80843,\"start\":80842},{\"end\":80855,\"start\":80854},{\"end\":80867,\"start\":80866},{\"end\":81231,\"start\":81230},{\"end\":81244,\"start\":81243},{\"end\":81256,\"start\":81252},{\"end\":81695,\"start\":81694},{\"end\":81704,\"start\":81703},{\"end\":81715,\"start\":81714},{\"end\":81724,\"start\":81723},{\"end\":81733,\"start\":81732},{\"end\":81740,\"start\":81739},{\"end\":81752,\"start\":81751},{\"end\":81754,\"start\":81753},{\"end\":81763,\"start\":81762},{\"end\":81775,\"start\":81774},{\"end\":81784,\"start\":81783},{\"end\":81790,\"start\":81789},{\"end\":81800,\"start\":81799},{\"end\":81810,\"start\":81809},{\"end\":81814,\"start\":81811},{\"end\":81824,\"start\":81823},{\"end\":82531,\"start\":82530},{\"end\":82543,\"start\":82542},{\"end\":82553,\"start\":82552},{\"end\":82565,\"start\":82564},{\"end\":82573,\"start\":82572},{\"end\":82583,\"start\":82582},{\"end\":82595,\"start\":82594},{\"end\":82603,\"start\":82602},{\"end\":83042,\"start\":83041},{\"end\":83052,\"start\":83051},{\"end\":83539,\"start\":83538},{\"end\":83555,\"start\":83551},{\"end\":83566,\"start\":83565},{\"end\":83575,\"start\":83574},{\"end\":83586,\"start\":83585},{\"end\":83596,\"start\":83595},{\"end\":83608,\"start\":83607},{\"end\":83618,\"start\":83617},{\"end\":83629,\"start\":83628},{\"end\":84212,\"start\":84211},{\"end\":84223,\"start\":84222},{\"end\":84234,\"start\":84233},{\"end\":84244,\"start\":84243},{\"end\":84254,\"start\":84253},{\"end\":84263,\"start\":84262},{\"end\":84265,\"start\":84264},{\"end\":84275,\"start\":84274},{\"end\":84944,\"start\":84943},{\"end\":84954,\"start\":84953},{\"end\":84966,\"start\":84965},{\"end\":85322,\"start\":85321},{\"end\":85332,\"start\":85331},{\"end\":85345,\"start\":85344},{\"end\":85354,\"start\":85353},{\"end\":85367,\"start\":85366},{\"end\":85609,\"start\":85608},{\"end\":85617,\"start\":85616},{\"end\":85960,\"start\":85959},{\"end\":85970,\"start\":85969},{\"end\":85979,\"start\":85978},{\"end\":85989,\"start\":85988},{\"end\":86002,\"start\":86001},{\"end\":86248,\"start\":86247},{\"end\":86250,\"start\":86249},{\"end\":86258,\"start\":86257},{\"end\":86260,\"start\":86259},{\"end\":86494,\"start\":86493},{\"end\":86505,\"start\":86504},{\"end\":86516,\"start\":86515},{\"end\":86686,\"start\":86685},{\"end\":86695,\"start\":86694},{\"end\":86705,\"start\":86704},{\"end\":86718,\"start\":86717},{\"end\":86728,\"start\":86727},{\"end\":87025,\"start\":87024},{\"end\":87034,\"start\":87033},{\"end\":87044,\"start\":87043},{\"end\":87278,\"start\":87277},{\"end\":87289,\"start\":87288},{\"end\":87303,\"start\":87302},{\"end\":87305,\"start\":87304},{\"end\":87321,\"start\":87320},{\"end\":87672,\"start\":87671},{\"end\":87683,\"start\":87682},{\"end\":87694,\"start\":87693},{\"end\":87708,\"start\":87707},{\"end\":87949,\"start\":87948},{\"end\":87958,\"start\":87957},{\"end\":88191,\"start\":88190},{\"end\":88205,\"start\":88204},{\"end\":88214,\"start\":88213},{\"end\":88775,\"start\":88774},{\"end\":88786,\"start\":88785},{\"end\":88796,\"start\":88795},{\"end\":89163,\"start\":89162},{\"end\":89173,\"start\":89172},{\"end\":89374,\"start\":89373},{\"end\":89386,\"start\":89385},{\"end\":89388,\"start\":89387},{\"end\":89396,\"start\":89395},{\"end\":89408,\"start\":89407},{\"end\":89624,\"start\":89623},{\"end\":89634,\"start\":89633},{\"end\":89829,\"start\":89828},{\"end\":89844,\"start\":89843},{\"end\":89858,\"start\":89857},{\"end\":90050,\"start\":90049},{\"end\":90271,\"start\":90270},{\"end\":90681,\"start\":90680},{\"end\":90692,\"start\":90691},{\"end\":90710,\"start\":90709},{\"end\":90983,\"start\":90982},{\"end\":90997,\"start\":90996},{\"end\":91005,\"start\":91004},{\"end\":91016,\"start\":91015},{\"end\":91364,\"start\":91363},{\"end\":91366,\"start\":91365},{\"end\":91375,\"start\":91374},{\"end\":91388,\"start\":91384},{\"end\":91394,\"start\":91393},{\"end\":91396,\"start\":91395},{\"end\":91669,\"start\":91668},{\"end\":91679,\"start\":91678},{\"end\":91690,\"start\":91689},{\"end\":91699,\"start\":91698},{\"end\":92071,\"start\":92070},{\"end\":92081,\"start\":92080},{\"end\":92372,\"start\":92371},{\"end\":92382,\"start\":92381},{\"end\":92641,\"start\":92640},{\"end\":92655,\"start\":92654},{\"end\":92670,\"start\":92669},{\"end\":92680,\"start\":92679},{\"end\":93033,\"start\":93032},{\"end\":93048,\"start\":93047},{\"end\":93062,\"start\":93061},{\"end\":93269,\"start\":93268},{\"end\":93476,\"start\":93475},{\"end\":93486,\"start\":93485},{\"end\":93896,\"start\":93895},{\"end\":93904,\"start\":93903},{\"end\":93914,\"start\":93913},{\"end\":94339,\"start\":94338},{\"end\":94354,\"start\":94353},{\"end\":94368,\"start\":94367},{\"end\":94862,\"start\":94861},{\"end\":94868,\"start\":94867},{\"end\":94879,\"start\":94875},{\"end\":95222,\"start\":95221},{\"end\":95234,\"start\":95233},{\"end\":95242,\"start\":95241},{\"end\":95254,\"start\":95253},{\"end\":95503,\"start\":95502},{\"end\":95514,\"start\":95513},{\"end\":95529,\"start\":95528},{\"end\":95543,\"start\":95542},{\"end\":95856,\"start\":95855},{\"end\":95869,\"start\":95868},{\"end\":95882,\"start\":95878},{\"end\":96269,\"start\":96268},{\"end\":96271,\"start\":96270},{\"end\":96278,\"start\":96277},{\"end\":96692,\"start\":96691},{\"end\":96705,\"start\":96704},{\"end\":96709,\"start\":96706},{\"end\":96718,\"start\":96717},{\"end\":96928,\"start\":96927},{\"end\":96937,\"start\":96936},{\"end\":96949,\"start\":96948},{\"end\":96959,\"start\":96958},{\"end\":97181,\"start\":97180},{\"end\":97190,\"start\":97189},{\"end\":97602,\"start\":97601},{\"end\":97610,\"start\":97609},{\"end\":97618,\"start\":97617},{\"end\":97628,\"start\":97627},{\"end\":97987,\"start\":97986},{\"end\":97993,\"start\":97992},{\"end\":98001,\"start\":98000},{\"end\":98009,\"start\":98008},{\"end\":98011,\"start\":98010},{\"end\":98020,\"start\":98019},{\"end\":98285,\"start\":98284},{\"end\":98294,\"start\":98293},{\"end\":98304,\"start\":98303},{\"end\":98323,\"start\":98322},{\"end\":98335,\"start\":98334},{\"end\":98344,\"start\":98343},{\"end\":98353,\"start\":98352},{\"end\":98361,\"start\":98360},{\"end\":98369,\"start\":98368},{\"end\":98376,\"start\":98375},{\"end\":98825,\"start\":98824},{\"end\":98831,\"start\":98830},{\"end\":99070,\"start\":99069},{\"end\":99082,\"start\":99081},{\"end\":99084,\"start\":99083},{\"end\":99291,\"start\":99290},{\"end\":99298,\"start\":99297},{\"end\":99307,\"start\":99306},{\"end\":99315,\"start\":99314},{\"end\":99635,\"start\":99634},{\"end\":99637,\"start\":99636},{\"end\":99646,\"start\":99645},{\"end\":99648,\"start\":99647},{\"end\":99655,\"start\":99654},{\"end\":99657,\"start\":99656},{\"end\":99666,\"start\":99665},{\"end\":99918,\"start\":99914},{\"end\":99928,\"start\":99924},{\"end\":99939,\"start\":99935},{\"end\":100258,\"start\":100257},{\"end\":100260,\"start\":100259},{\"end\":100268,\"start\":100267},{\"end\":100500,\"start\":100499},{\"end\":100512,\"start\":100511},{\"end\":100748,\"start\":100747},{\"end\":100757,\"start\":100756},{\"end\":100769,\"start\":100768},{\"end\":101114,\"start\":101113},{\"end\":101123,\"start\":101122},{\"end\":101135,\"start\":101134},{\"end\":101468,\"start\":101467},{\"end\":101477,\"start\":101476},{\"end\":101489,\"start\":101488},{\"end\":101769,\"start\":101768},{\"end\":101779,\"start\":101778},{\"end\":101788,\"start\":101787},{\"end\":101799,\"start\":101798},{\"end\":102067,\"start\":102066},{\"end\":102075,\"start\":102074},{\"end\":102085,\"start\":102084},{\"end\":102093,\"start\":102092},{\"end\":102104,\"start\":102103},{\"end\":102120,\"start\":102119},{\"end\":102131,\"start\":102130},{\"end\":102336,\"start\":102335},{\"end\":102345,\"start\":102344},{\"end\":102358,\"start\":102357},{\"end\":102372,\"start\":102365},{\"end\":102567,\"start\":102566},{\"end\":102576,\"start\":102575},{\"end\":102584,\"start\":102583},{\"end\":102590,\"start\":102589},{\"end\":102870,\"start\":102869},{\"end\":102876,\"start\":102875},{\"end\":103096,\"start\":103095},{\"end\":103388,\"start\":103387},{\"end\":103397,\"start\":103396},{\"end\":103683,\"start\":103682},{\"end\":103699,\"start\":103698},{\"end\":104117,\"start\":104116},{\"end\":104131,\"start\":104130},{\"end\":104140,\"start\":104139},{\"end\":104428,\"start\":104427},{\"end\":104436,\"start\":104435},{\"end\":104447,\"start\":104446},{\"end\":104459,\"start\":104458},{\"end\":104470,\"start\":104469},{\"end\":104479,\"start\":104478},{\"end\":104489,\"start\":104488},{\"end\":104795,\"start\":104794},{\"end\":104802,\"start\":104801},{\"end\":104808,\"start\":104807},{\"end\":104817,\"start\":104816},{\"end\":104823,\"start\":104822},{\"end\":104831,\"start\":104830},{\"end\":105265,\"start\":105264},{\"end\":105271,\"start\":105270},{\"end\":105273,\"start\":105272},{\"end\":105540,\"start\":105539},{\"end\":105549,\"start\":105548},{\"end\":105551,\"start\":105550},{\"end\":105720,\"start\":105719},{\"end\":105722,\"start\":105721},{\"end\":105732,\"start\":105731},{\"end\":105743,\"start\":105742},{\"end\":105752,\"start\":105751},{\"end\":105759,\"start\":105758},{\"end\":105767,\"start\":105766},{\"end\":105769,\"start\":105768},{\"end\":105775,\"start\":105774},{\"end\":105785,\"start\":105784},{\"end\":105795,\"start\":105794},{\"end\":105808,\"start\":105807},{\"end\":105816,\"start\":105815},{\"end\":105818,\"start\":105817},{\"end\":106091,\"start\":106090},{\"end\":106105,\"start\":106104},{\"end\":106107,\"start\":106106},{\"end\":106117,\"start\":106116},{\"end\":106131,\"start\":106130},{\"end\":106144,\"start\":106143},{\"end\":106146,\"start\":106145},{\"end\":106444,\"start\":106443},{\"end\":106455,\"start\":106454},{\"end\":106464,\"start\":106463},{\"end\":106472,\"start\":106471},{\"end\":106803,\"start\":106802},{\"end\":106817,\"start\":106816},{\"end\":106830,\"start\":106829},{\"end\":106832,\"start\":106831},{\"end\":107141,\"start\":107140},{\"end\":107153,\"start\":107152},{\"end\":107393,\"start\":107392}]", "bib_author_last_name": "[{\"end\":73930,\"start\":73922},{\"end\":73941,\"start\":73936},{\"end\":73953,\"start\":73945},{\"end\":73967,\"start\":73959},{\"end\":74326,\"start\":74321},{\"end\":74627,\"start\":74619},{\"end\":74802,\"start\":74798},{\"end\":74816,\"start\":74808},{\"end\":75117,\"start\":75113},{\"end\":75126,\"start\":75124},{\"end\":75136,\"start\":75133},{\"end\":75348,\"start\":75342},{\"end\":75362,\"start\":75354},{\"end\":75706,\"start\":75698},{\"end\":75716,\"start\":75710},{\"end\":76084,\"start\":76081},{\"end\":76097,\"start\":76091},{\"end\":76294,\"start\":76287},{\"end\":76308,\"start\":76298},{\"end\":76648,\"start\":76643},{\"end\":76658,\"start\":76652},{\"end\":76888,\"start\":76885},{\"end\":76899,\"start\":76892},{\"end\":76916,\"start\":76903},{\"end\":77303,\"start\":77300},{\"end\":77320,\"start\":77307},{\"end\":77331,\"start\":77324},{\"end\":77552,\"start\":77549},{\"end\":77560,\"start\":77556},{\"end\":77568,\"start\":77564},{\"end\":77579,\"start\":77572},{\"end\":77900,\"start\":77894},{\"end\":77911,\"start\":77904},{\"end\":77918,\"start\":77915},{\"end\":78228,\"start\":78225},{\"end\":78241,\"start\":78232},{\"end\":78648,\"start\":78646},{\"end\":78657,\"start\":78654},{\"end\":78999,\"start\":78995},{\"end\":79010,\"start\":79003},{\"end\":79119,\"start\":79111},{\"end\":79129,\"start\":79123},{\"end\":79430,\"start\":79424},{\"end\":79439,\"start\":79434},{\"end\":79449,\"start\":79445},{\"end\":79459,\"start\":79453},{\"end\":79726,\"start\":79724},{\"end\":79741,\"start\":79730},{\"end\":80047,\"start\":80042},{\"end\":80055,\"start\":80051},{\"end\":80343,\"start\":80339},{\"end\":80353,\"start\":80349},{\"end\":80360,\"start\":80357},{\"end\":80581,\"start\":80573},{\"end\":80593,\"start\":80587},{\"end\":80601,\"start\":80597},{\"end\":80610,\"start\":80607},{\"end\":80830,\"start\":80825},{\"end\":80840,\"start\":80834},{\"end\":80852,\"start\":80844},{\"end\":80864,\"start\":80856},{\"end\":80871,\"start\":80868},{\"end\":81241,\"start\":81232},{\"end\":81250,\"start\":81245},{\"end\":81263,\"start\":81257},{\"end\":81701,\"start\":81696},{\"end\":81712,\"start\":81705},{\"end\":81721,\"start\":81716},{\"end\":81730,\"start\":81725},{\"end\":81737,\"start\":81734},{\"end\":81749,\"start\":81741},{\"end\":81760,\"start\":81755},{\"end\":81772,\"start\":81764},{\"end\":81781,\"start\":81776},{\"end\":81787,\"start\":81785},{\"end\":81797,\"start\":81791},{\"end\":81807,\"start\":81801},{\"end\":81821,\"start\":81815},{\"end\":81833,\"start\":81825},{\"end\":82540,\"start\":82532},{\"end\":82550,\"start\":82544},{\"end\":82562,\"start\":82554},{\"end\":82570,\"start\":82566},{\"end\":82580,\"start\":82574},{\"end\":82592,\"start\":82584},{\"end\":82600,\"start\":82596},{\"end\":82608,\"start\":82604},{\"end\":83049,\"start\":83043},{\"end\":83059,\"start\":83053},{\"end\":83549,\"start\":83540},{\"end\":83563,\"start\":83556},{\"end\":83572,\"start\":83567},{\"end\":83583,\"start\":83576},{\"end\":83593,\"start\":83587},{\"end\":83605,\"start\":83597},{\"end\":83615,\"start\":83609},{\"end\":83626,\"start\":83619},{\"end\":83646,\"start\":83630},{\"end\":84220,\"start\":84213},{\"end\":84231,\"start\":84224},{\"end\":84241,\"start\":84235},{\"end\":84251,\"start\":84245},{\"end\":84260,\"start\":84255},{\"end\":84272,\"start\":84266},{\"end\":84279,\"start\":84276},{\"end\":84301,\"start\":84281},{\"end\":84951,\"start\":84945},{\"end\":84963,\"start\":84955},{\"end\":84975,\"start\":84967},{\"end\":85329,\"start\":85323},{\"end\":85342,\"start\":85333},{\"end\":85351,\"start\":85346},{\"end\":85364,\"start\":85355},{\"end\":85373,\"start\":85368},{\"end\":85614,\"start\":85610},{\"end\":85624,\"start\":85618},{\"end\":85967,\"start\":85961},{\"end\":85976,\"start\":85971},{\"end\":85986,\"start\":85980},{\"end\":85999,\"start\":85990},{\"end\":86006,\"start\":86003},{\"end\":86255,\"start\":86251},{\"end\":86268,\"start\":86261},{\"end\":86502,\"start\":86495},{\"end\":86513,\"start\":86506},{\"end\":86522,\"start\":86517},{\"end\":86692,\"start\":86687},{\"end\":86702,\"start\":86696},{\"end\":86715,\"start\":86706},{\"end\":86725,\"start\":86719},{\"end\":86732,\"start\":86729},{\"end\":87031,\"start\":87026},{\"end\":87041,\"start\":87035},{\"end\":87052,\"start\":87045},{\"end\":87286,\"start\":87279},{\"end\":87300,\"start\":87290},{\"end\":87318,\"start\":87306},{\"end\":87331,\"start\":87322},{\"end\":87680,\"start\":87673},{\"end\":87691,\"start\":87684},{\"end\":87705,\"start\":87695},{\"end\":87718,\"start\":87709},{\"end\":87955,\"start\":87950},{\"end\":87965,\"start\":87959},{\"end\":88202,\"start\":88192},{\"end\":88211,\"start\":88206},{\"end\":88223,\"start\":88215},{\"end\":88783,\"start\":88776},{\"end\":88793,\"start\":88787},{\"end\":88805,\"start\":88797},{\"end\":89170,\"start\":89164},{\"end\":89181,\"start\":89174},{\"end\":89383,\"start\":89375},{\"end\":89393,\"start\":89389},{\"end\":89405,\"start\":89397},{\"end\":89415,\"start\":89409},{\"end\":89631,\"start\":89625},{\"end\":89642,\"start\":89635},{\"end\":89841,\"start\":89830},{\"end\":89855,\"start\":89845},{\"end\":89866,\"start\":89859},{\"end\":90056,\"start\":90051},{\"end\":90281,\"start\":90272},{\"end\":90689,\"start\":90682},{\"end\":90707,\"start\":90693},{\"end\":90716,\"start\":90711},{\"end\":90994,\"start\":90984},{\"end\":91002,\"start\":90998},{\"end\":91013,\"start\":91006},{\"end\":91025,\"start\":91017},{\"end\":91372,\"start\":91367},{\"end\":91382,\"start\":91376},{\"end\":91391,\"start\":91389},{\"end\":91402,\"start\":91397},{\"end\":91676,\"start\":91670},{\"end\":91687,\"start\":91680},{\"end\":91696,\"start\":91691},{\"end\":91707,\"start\":91700},{\"end\":92078,\"start\":92072},{\"end\":92089,\"start\":92082},{\"end\":92379,\"start\":92373},{\"end\":92390,\"start\":92383},{\"end\":92652,\"start\":92642},{\"end\":92667,\"start\":92656},{\"end\":92677,\"start\":92671},{\"end\":92688,\"start\":92681},{\"end\":93045,\"start\":93034},{\"end\":93059,\"start\":93049},{\"end\":93070,\"start\":93063},{\"end\":93278,\"start\":93270},{\"end\":93483,\"start\":93477},{\"end\":93494,\"start\":93487},{\"end\":93901,\"start\":93897},{\"end\":93911,\"start\":93905},{\"end\":93922,\"start\":93915},{\"end\":94351,\"start\":94340},{\"end\":94365,\"start\":94355},{\"end\":94376,\"start\":94369},{\"end\":94865,\"start\":94863},{\"end\":94873,\"start\":94869},{\"end\":94883,\"start\":94880},{\"end\":95231,\"start\":95223},{\"end\":95239,\"start\":95235},{\"end\":95251,\"start\":95243},{\"end\":95262,\"start\":95255},{\"end\":95511,\"start\":95504},{\"end\":95526,\"start\":95515},{\"end\":95540,\"start\":95530},{\"end\":95551,\"start\":95544},{\"end\":95866,\"start\":95857},{\"end\":95876,\"start\":95870},{\"end\":95889,\"start\":95883},{\"end\":96275,\"start\":96272},{\"end\":96288,\"start\":96279},{\"end\":96702,\"start\":96693},{\"end\":96715,\"start\":96710},{\"end\":96725,\"start\":96719},{\"end\":96934,\"start\":96929},{\"end\":96946,\"start\":96938},{\"end\":96956,\"start\":96950},{\"end\":96968,\"start\":96960},{\"end\":97187,\"start\":97182},{\"end\":97197,\"start\":97191},{\"end\":97607,\"start\":97603},{\"end\":97615,\"start\":97611},{\"end\":97625,\"start\":97619},{\"end\":97636,\"start\":97629},{\"end\":97990,\"start\":97988},{\"end\":97998,\"start\":97994},{\"end\":98006,\"start\":98002},{\"end\":98017,\"start\":98012},{\"end\":98027,\"start\":98021},{\"end\":98291,\"start\":98286},{\"end\":98301,\"start\":98295},{\"end\":98320,\"start\":98305},{\"end\":98332,\"start\":98324},{\"end\":98341,\"start\":98336},{\"end\":98350,\"start\":98345},{\"end\":98358,\"start\":98354},{\"end\":98366,\"start\":98362},{\"end\":98373,\"start\":98370},{\"end\":98386,\"start\":98377},{\"end\":98828,\"start\":98826},{\"end\":98846,\"start\":98832},{\"end\":99079,\"start\":99071},{\"end\":99090,\"start\":99085},{\"end\":99295,\"start\":99292},{\"end\":99304,\"start\":99299},{\"end\":99312,\"start\":99308},{\"end\":99321,\"start\":99316},{\"end\":99643,\"start\":99638},{\"end\":99652,\"start\":99649},{\"end\":99663,\"start\":99658},{\"end\":99676,\"start\":99667},{\"end\":99922,\"start\":99919},{\"end\":99933,\"start\":99929},{\"end\":99943,\"start\":99940},{\"end\":100265,\"start\":100261},{\"end\":100278,\"start\":100269},{\"end\":100509,\"start\":100501},{\"end\":100523,\"start\":100513},{\"end\":100754,\"start\":100749},{\"end\":100766,\"start\":100758},{\"end\":100776,\"start\":100770},{\"end\":101120,\"start\":101115},{\"end\":101132,\"start\":101124},{\"end\":101142,\"start\":101136},{\"end\":101474,\"start\":101469},{\"end\":101486,\"start\":101478},{\"end\":101496,\"start\":101490},{\"end\":101776,\"start\":101770},{\"end\":101785,\"start\":101780},{\"end\":101796,\"start\":101789},{\"end\":101806,\"start\":101800},{\"end\":102072,\"start\":102068},{\"end\":102082,\"start\":102076},{\"end\":102090,\"start\":102086},{\"end\":102101,\"start\":102094},{\"end\":102117,\"start\":102105},{\"end\":102128,\"start\":102121},{\"end\":102139,\"start\":102132},{\"end\":102342,\"start\":102337},{\"end\":102355,\"start\":102346},{\"end\":102363,\"start\":102359},{\"end\":102377,\"start\":102373},{\"end\":102573,\"start\":102568},{\"end\":102581,\"start\":102577},{\"end\":102587,\"start\":102585},{\"end\":102596,\"start\":102591},{\"end\":102873,\"start\":102871},{\"end\":102891,\"start\":102877},{\"end\":103101,\"start\":103097},{\"end\":103394,\"start\":103389},{\"end\":103403,\"start\":103398},{\"end\":103696,\"start\":103684},{\"end\":103706,\"start\":103700},{\"end\":104128,\"start\":104118},{\"end\":104137,\"start\":104132},{\"end\":104145,\"start\":104141},{\"end\":104433,\"start\":104429},{\"end\":104444,\"start\":104437},{\"end\":104456,\"start\":104448},{\"end\":104467,\"start\":104460},{\"end\":104476,\"start\":104471},{\"end\":104486,\"start\":104480},{\"end\":104799,\"start\":104796},{\"end\":104805,\"start\":104803},{\"end\":104814,\"start\":104809},{\"end\":104820,\"start\":104818},{\"end\":104828,\"start\":104824},{\"end\":104836,\"start\":104832},{\"end\":105268,\"start\":105266},{\"end\":105277,\"start\":105274},{\"end\":105546,\"start\":105541},{\"end\":105557,\"start\":105552},{\"end\":105729,\"start\":105723},{\"end\":105740,\"start\":105733},{\"end\":105749,\"start\":105744},{\"end\":105756,\"start\":105753},{\"end\":105764,\"start\":105760},{\"end\":105772,\"start\":105770},{\"end\":105782,\"start\":105776},{\"end\":105792,\"start\":105786},{\"end\":105805,\"start\":105796},{\"end\":105813,\"start\":105809},{\"end\":105825,\"start\":105819},{\"end\":106102,\"start\":106092},{\"end\":106114,\"start\":106108},{\"end\":106128,\"start\":106118},{\"end\":106141,\"start\":106132},{\"end\":106160,\"start\":106147},{\"end\":106452,\"start\":106445},{\"end\":106461,\"start\":106456},{\"end\":106469,\"start\":106465},{\"end\":106479,\"start\":106473},{\"end\":106814,\"start\":106804},{\"end\":106827,\"start\":106818},{\"end\":106839,\"start\":106833},{\"end\":107150,\"start\":107142},{\"end\":107163,\"start\":107154},{\"end\":107401,\"start\":107394}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":146557236},\"end\":74170,\"start\":73843},{\"attributes\":{\"id\":\"b1\"},\"end\":74570,\"start\":74172},{\"attributes\":{\"id\":\"b2\"},\"end\":74712,\"start\":74572},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18540653},\"end\":75011,\"start\":74714},{\"attributes\":{\"id\":\"b4\"},\"end\":75285,\"start\":75013},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18119821},\"end\":75510,\"start\":75287},{\"attributes\":{\"id\":\"b6\"},\"end\":76005,\"start\":75512},{\"attributes\":{\"id\":\"b7\"},\"end\":76212,\"start\":76007},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6509900},\"end\":76557,\"start\":76214},{\"attributes\":{\"id\":\"b9\"},\"end\":76784,\"start\":76559},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6836977},\"end\":77225,\"start\":76786},{\"attributes\":{\"id\":\"b11\"},\"end\":77462,\"start\":77227},{\"attributes\":{\"id\":\"b12\"},\"end\":77727,\"start\":77464},{\"attributes\":{\"id\":\"b13\"},\"end\":78137,\"start\":77729},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18440098},\"end\":78503,\"start\":78139},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":39346255},\"end\":78947,\"start\":78505},{\"attributes\":{\"id\":\"b16\"},\"end\":79107,\"start\":78949},{\"attributes\":{\"id\":\"b17\"},\"end\":79320,\"start\":79109},{\"attributes\":{\"id\":\"b18\"},\"end\":79624,\"start\":79322},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17481884},\"end\":79948,\"start\":79626},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":28113495},\"end\":80267,\"start\":79950},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":51684383},\"end\":80500,\"start\":80269},{\"attributes\":{\"id\":\"b22\"},\"end\":80748,\"start\":80502},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7120486},\"end\":81151,\"start\":80750},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14524787},\"end\":81618,\"start\":81153},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":193873},\"end\":82420,\"start\":81620},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9020329},\"end\":83039,\"start\":82422},{\"attributes\":{\"id\":\"b27\"},\"end\":83536,\"start\":83041},{\"attributes\":{\"id\":\"b28\"},\"end\":84140,\"start\":83538},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2516961},\"end\":84622,\"start\":84142},{\"attributes\":{\"id\":\"b30\"},\"end\":84762,\"start\":84624},{\"attributes\":{\"id\":\"b31\"},\"end\":84844,\"start\":84764},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":35714192},\"end\":85243,\"start\":84846},{\"attributes\":{\"id\":\"b33\"},\"end\":85531,\"start\":85245},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14395556},\"end\":85869,\"start\":85533},{\"attributes\":{\"id\":\"b35\"},\"end\":86169,\"start\":85871},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12004269},\"end\":86455,\"start\":86171},{\"attributes\":{\"id\":\"b37\"},\"end\":86613,\"start\":86457},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14825282},\"end\":86918,\"start\":86615},{\"attributes\":{\"id\":\"b39\"},\"end\":87210,\"start\":86920},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6148555},\"end\":87569,\"start\":87212},{\"attributes\":{\"id\":\"b41\"},\"end\":87892,\"start\":87571},{\"attributes\":{\"id\":\"b42\"},\"end\":88061,\"start\":87894},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3363350},\"end\":88681,\"start\":88063},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":13308074},\"end\":89090,\"start\":88683},{\"attributes\":{\"id\":\"b45\"},\"end\":89297,\"start\":89092},{\"attributes\":{\"id\":\"b46\"},\"end\":89561,\"start\":89299},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":8622160},\"end\":89772,\"start\":89563},{\"attributes\":{\"id\":\"b48\"},\"end\":89988,\"start\":89774},{\"attributes\":{\"id\":\"b49\"},\"end\":90213,\"start\":89990},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":55514208},\"end\":90587,\"start\":90215},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":3258736},\"end\":90938,\"start\":90589},{\"attributes\":{\"id\":\"b52\"},\"end\":91258,\"start\":90940},{\"attributes\":{\"id\":\"b53\"},\"end\":91590,\"start\":91260},{\"attributes\":{\"id\":\"b54\"},\"end\":91988,\"start\":91592},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":14191037},\"end\":92295,\"start\":91990},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":7159252},\"end\":92534,\"start\":92297},{\"attributes\":{\"id\":\"b57\"},\"end\":92947,\"start\":92536},{\"attributes\":{\"id\":\"b58\"},\"end\":93221,\"start\":92949},{\"attributes\":{\"id\":\"b59\"},\"end\":93371,\"start\":93223},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":15050923},\"end\":93756,\"start\":93373},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":15118062},\"end\":94251,\"start\":93758},{\"attributes\":{\"id\":\"b62\"},\"end\":94741,\"start\":94253},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":12143260},\"end\":95153,\"start\":94743},{\"attributes\":{\"id\":\"b64\"},\"end\":95396,\"start\":95155},{\"attributes\":{\"id\":\"b65\"},\"end\":95731,\"start\":95398},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":1599525},\"end\":96200,\"start\":95733},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":10037625},\"end\":96619,\"start\":96202},{\"attributes\":{\"id\":\"b68\"},\"end\":96863,\"start\":96621},{\"attributes\":{\"id\":\"b69\"},\"end\":97104,\"start\":96865},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":1406894},\"end\":97529,\"start\":97106},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":4525936},\"end\":97984,\"start\":97531},{\"attributes\":{\"doi\":\"arXiv:1603.00223\",\"id\":\"b72\"},\"end\":98282,\"start\":97986},{\"attributes\":{\"id\":\"b73\"},\"end\":98753,\"start\":98284},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":15918815},\"end\":99001,\"start\":98755},{\"attributes\":{\"id\":\"b75\"},\"end\":99205,\"start\":99003},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":9054863},\"end\":99540,\"start\":99207},{\"attributes\":{\"id\":\"b77\"},\"end\":99842,\"start\":99542},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":17544393},\"end\":100159,\"start\":99844},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":44773206},\"end\":100455,\"start\":100161},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":15247659},\"end\":100745,\"start\":100457},{\"attributes\":{\"id\":\"b81\"},\"end\":101003,\"start\":100747},{\"attributes\":{\"id\":\"b82\"},\"end\":101338,\"start\":101005},{\"attributes\":{\"id\":\"b83\"},\"end\":101694,\"start\":101340},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":15674418},\"end\":101983,\"start\":101696},{\"attributes\":{\"id\":\"b85\"},\"end\":102331,\"start\":101985},{\"attributes\":{\"id\":\"b86\"},\"end\":102509,\"start\":102333},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":5911399},\"end\":102753,\"start\":102511},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":18882467},\"end\":103093,\"start\":102755},{\"attributes\":{\"id\":\"b89\"},\"end\":103265,\"start\":103095},{\"attributes\":{\"id\":\"b90\"},\"end\":103574,\"start\":103267},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":2052273},\"end\":104014,\"start\":103576},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":10440969},\"end\":104339,\"start\":104016},{\"attributes\":{\"id\":\"b93\"},\"end\":104705,\"start\":104341},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":11308291},\"end\":105147,\"start\":104707},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":9315703},\"end\":105470,\"start\":105149},{\"attributes\":{\"id\":\"b96\"},\"end\":105668,\"start\":105472},{\"attributes\":{\"id\":\"b97\"},\"end\":106021,\"start\":105670},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":6844431},\"end\":106403,\"start\":106023},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":14692925},\"end\":106800,\"start\":106405},{\"attributes\":{\"id\":\"b100\"},\"end\":107138,\"start\":106802},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b101\"},\"end\":107388,\"start\":107140},{\"attributes\":{\"id\":\"b102\"},\"end\":107463,\"start\":107390},{\"attributes\":{\"id\":\"b103\"},\"end\":107741,\"start\":107465},{\"attributes\":{\"id\":\"b104\"},\"end\":107798,\"start\":107743}]", "bib_title": "[{\"end\":73916,\"start\":73843},{\"end\":74794,\"start\":74714},{\"end\":75338,\"start\":75287},{\"end\":76283,\"start\":76214},{\"end\":76881,\"start\":76786},{\"end\":78219,\"start\":78139},{\"end\":78642,\"start\":78505},{\"end\":79720,\"start\":79626},{\"end\":80038,\"start\":79950},{\"end\":80335,\"start\":80269},{\"end\":80821,\"start\":80750},{\"end\":81228,\"start\":81153},{\"end\":81692,\"start\":81620},{\"end\":82528,\"start\":82422},{\"end\":84209,\"start\":84142},{\"end\":84941,\"start\":84846},{\"end\":85606,\"start\":85533},{\"end\":86245,\"start\":86171},{\"end\":86683,\"start\":86615},{\"end\":87275,\"start\":87212},{\"end\":88188,\"start\":88063},{\"end\":88772,\"start\":88683},{\"end\":89621,\"start\":89563},{\"end\":90268,\"start\":90215},{\"end\":90678,\"start\":90589},{\"end\":90980,\"start\":90940},{\"end\":91666,\"start\":91592},{\"end\":92068,\"start\":91990},{\"end\":92369,\"start\":92297},{\"end\":92638,\"start\":92536},{\"end\":93473,\"start\":93373},{\"end\":93893,\"start\":93758},{\"end\":94336,\"start\":94253},{\"end\":94859,\"start\":94743},{\"end\":95853,\"start\":95733},{\"end\":96266,\"start\":96202},{\"end\":97178,\"start\":97106},{\"end\":97599,\"start\":97531},{\"end\":98822,\"start\":98755},{\"end\":99288,\"start\":99207},{\"end\":99912,\"start\":99844},{\"end\":100255,\"start\":100161},{\"end\":100497,\"start\":100457},{\"end\":101111,\"start\":101005},{\"end\":101766,\"start\":101696},{\"end\":102564,\"start\":102511},{\"end\":102867,\"start\":102755},{\"end\":103680,\"start\":103576},{\"end\":104114,\"start\":104016},{\"end\":104425,\"start\":104341},{\"end\":104792,\"start\":104707},{\"end\":105262,\"start\":105149},{\"end\":106088,\"start\":106023},{\"end\":106441,\"start\":106405}]", "bib_author": "[{\"end\":73932,\"start\":73918},{\"end\":73943,\"start\":73932},{\"end\":73955,\"start\":73943},{\"end\":73969,\"start\":73955},{\"end\":74328,\"start\":74319},{\"end\":74629,\"start\":74617},{\"end\":74804,\"start\":74796},{\"end\":74818,\"start\":74804},{\"end\":75119,\"start\":75111},{\"end\":75128,\"start\":75119},{\"end\":75138,\"start\":75128},{\"end\":75350,\"start\":75340},{\"end\":75364,\"start\":75350},{\"end\":75708,\"start\":75696},{\"end\":75718,\"start\":75708},{\"end\":76086,\"start\":76079},{\"end\":76099,\"start\":76086},{\"end\":76296,\"start\":76285},{\"end\":76310,\"start\":76296},{\"end\":76650,\"start\":76641},{\"end\":76660,\"start\":76650},{\"end\":76890,\"start\":76883},{\"end\":76901,\"start\":76890},{\"end\":76918,\"start\":76901},{\"end\":77305,\"start\":77298},{\"end\":77322,\"start\":77305},{\"end\":77333,\"start\":77322},{\"end\":77554,\"start\":77547},{\"end\":77562,\"start\":77554},{\"end\":77570,\"start\":77562},{\"end\":77581,\"start\":77570},{\"end\":77902,\"start\":77892},{\"end\":77913,\"start\":77902},{\"end\":77920,\"start\":77913},{\"end\":78230,\"start\":78221},{\"end\":78243,\"start\":78230},{\"end\":78650,\"start\":78644},{\"end\":78659,\"start\":78650},{\"end\":79001,\"start\":78993},{\"end\":79012,\"start\":79001},{\"end\":79121,\"start\":79109},{\"end\":79131,\"start\":79121},{\"end\":79432,\"start\":79422},{\"end\":79441,\"start\":79432},{\"end\":79451,\"start\":79441},{\"end\":79461,\"start\":79451},{\"end\":79728,\"start\":79722},{\"end\":79743,\"start\":79728},{\"end\":80049,\"start\":80040},{\"end\":80057,\"start\":80049},{\"end\":80345,\"start\":80337},{\"end\":80355,\"start\":80345},{\"end\":80362,\"start\":80355},{\"end\":80583,\"start\":80569},{\"end\":80595,\"start\":80583},{\"end\":80603,\"start\":80595},{\"end\":80612,\"start\":80603},{\"end\":80832,\"start\":80823},{\"end\":80842,\"start\":80832},{\"end\":80854,\"start\":80842},{\"end\":80866,\"start\":80854},{\"end\":80873,\"start\":80866},{\"end\":81243,\"start\":81230},{\"end\":81252,\"start\":81243},{\"end\":81265,\"start\":81252},{\"end\":81703,\"start\":81694},{\"end\":81714,\"start\":81703},{\"end\":81723,\"start\":81714},{\"end\":81732,\"start\":81723},{\"end\":81739,\"start\":81732},{\"end\":81751,\"start\":81739},{\"end\":81762,\"start\":81751},{\"end\":81774,\"start\":81762},{\"end\":81783,\"start\":81774},{\"end\":81789,\"start\":81783},{\"end\":81799,\"start\":81789},{\"end\":81809,\"start\":81799},{\"end\":81823,\"start\":81809},{\"end\":81835,\"start\":81823},{\"end\":82542,\"start\":82530},{\"end\":82552,\"start\":82542},{\"end\":82564,\"start\":82552},{\"end\":82572,\"start\":82564},{\"end\":82582,\"start\":82572},{\"end\":82594,\"start\":82582},{\"end\":82602,\"start\":82594},{\"end\":82610,\"start\":82602},{\"end\":83051,\"start\":83041},{\"end\":83061,\"start\":83051},{\"end\":83551,\"start\":83538},{\"end\":83565,\"start\":83551},{\"end\":83574,\"start\":83565},{\"end\":83585,\"start\":83574},{\"end\":83595,\"start\":83585},{\"end\":83607,\"start\":83595},{\"end\":83617,\"start\":83607},{\"end\":83628,\"start\":83617},{\"end\":83648,\"start\":83628},{\"end\":84222,\"start\":84211},{\"end\":84233,\"start\":84222},{\"end\":84243,\"start\":84233},{\"end\":84253,\"start\":84243},{\"end\":84262,\"start\":84253},{\"end\":84274,\"start\":84262},{\"end\":84281,\"start\":84274},{\"end\":84303,\"start\":84281},{\"end\":84953,\"start\":84943},{\"end\":84965,\"start\":84953},{\"end\":84977,\"start\":84965},{\"end\":85331,\"start\":85321},{\"end\":85344,\"start\":85331},{\"end\":85353,\"start\":85344},{\"end\":85366,\"start\":85353},{\"end\":85375,\"start\":85366},{\"end\":85616,\"start\":85608},{\"end\":85626,\"start\":85616},{\"end\":85969,\"start\":85959},{\"end\":85978,\"start\":85969},{\"end\":85988,\"start\":85978},{\"end\":86001,\"start\":85988},{\"end\":86008,\"start\":86001},{\"end\":86257,\"start\":86247},{\"end\":86270,\"start\":86257},{\"end\":86504,\"start\":86493},{\"end\":86515,\"start\":86504},{\"end\":86524,\"start\":86515},{\"end\":86694,\"start\":86685},{\"end\":86704,\"start\":86694},{\"end\":86717,\"start\":86704},{\"end\":86727,\"start\":86717},{\"end\":86734,\"start\":86727},{\"end\":87033,\"start\":87024},{\"end\":87043,\"start\":87033},{\"end\":87054,\"start\":87043},{\"end\":87288,\"start\":87277},{\"end\":87302,\"start\":87288},{\"end\":87320,\"start\":87302},{\"end\":87333,\"start\":87320},{\"end\":87682,\"start\":87671},{\"end\":87693,\"start\":87682},{\"end\":87707,\"start\":87693},{\"end\":87720,\"start\":87707},{\"end\":87957,\"start\":87948},{\"end\":87967,\"start\":87957},{\"end\":88204,\"start\":88190},{\"end\":88213,\"start\":88204},{\"end\":88225,\"start\":88213},{\"end\":88785,\"start\":88774},{\"end\":88795,\"start\":88785},{\"end\":88807,\"start\":88795},{\"end\":89172,\"start\":89162},{\"end\":89183,\"start\":89172},{\"end\":89385,\"start\":89373},{\"end\":89395,\"start\":89385},{\"end\":89407,\"start\":89395},{\"end\":89417,\"start\":89407},{\"end\":89633,\"start\":89623},{\"end\":89644,\"start\":89633},{\"end\":89843,\"start\":89828},{\"end\":89857,\"start\":89843},{\"end\":89868,\"start\":89857},{\"end\":90058,\"start\":90049},{\"end\":90283,\"start\":90270},{\"end\":90691,\"start\":90680},{\"end\":90709,\"start\":90691},{\"end\":90718,\"start\":90709},{\"end\":90996,\"start\":90982},{\"end\":91004,\"start\":90996},{\"end\":91015,\"start\":91004},{\"end\":91027,\"start\":91015},{\"end\":91374,\"start\":91363},{\"end\":91384,\"start\":91374},{\"end\":91393,\"start\":91384},{\"end\":91404,\"start\":91393},{\"end\":91678,\"start\":91668},{\"end\":91689,\"start\":91678},{\"end\":91698,\"start\":91689},{\"end\":91709,\"start\":91698},{\"end\":92080,\"start\":92070},{\"end\":92091,\"start\":92080},{\"end\":92381,\"start\":92371},{\"end\":92392,\"start\":92381},{\"end\":92654,\"start\":92640},{\"end\":92669,\"start\":92654},{\"end\":92679,\"start\":92669},{\"end\":92690,\"start\":92679},{\"end\":93047,\"start\":93032},{\"end\":93061,\"start\":93047},{\"end\":93072,\"start\":93061},{\"end\":93280,\"start\":93268},{\"end\":93485,\"start\":93475},{\"end\":93496,\"start\":93485},{\"end\":93903,\"start\":93895},{\"end\":93913,\"start\":93903},{\"end\":93924,\"start\":93913},{\"end\":94353,\"start\":94338},{\"end\":94367,\"start\":94353},{\"end\":94378,\"start\":94367},{\"end\":94867,\"start\":94861},{\"end\":94875,\"start\":94867},{\"end\":94885,\"start\":94875},{\"end\":95233,\"start\":95221},{\"end\":95241,\"start\":95233},{\"end\":95253,\"start\":95241},{\"end\":95264,\"start\":95253},{\"end\":95513,\"start\":95502},{\"end\":95528,\"start\":95513},{\"end\":95542,\"start\":95528},{\"end\":95553,\"start\":95542},{\"end\":95868,\"start\":95855},{\"end\":95878,\"start\":95868},{\"end\":95891,\"start\":95878},{\"end\":96277,\"start\":96268},{\"end\":96290,\"start\":96277},{\"end\":96704,\"start\":96691},{\"end\":96717,\"start\":96704},{\"end\":96727,\"start\":96717},{\"end\":96936,\"start\":96927},{\"end\":96948,\"start\":96936},{\"end\":96958,\"start\":96948},{\"end\":96970,\"start\":96958},{\"end\":97189,\"start\":97180},{\"end\":97199,\"start\":97189},{\"end\":97609,\"start\":97601},{\"end\":97617,\"start\":97609},{\"end\":97627,\"start\":97617},{\"end\":97638,\"start\":97627},{\"end\":97992,\"start\":97986},{\"end\":98000,\"start\":97992},{\"end\":98008,\"start\":98000},{\"end\":98019,\"start\":98008},{\"end\":98029,\"start\":98019},{\"end\":98293,\"start\":98284},{\"end\":98303,\"start\":98293},{\"end\":98322,\"start\":98303},{\"end\":98334,\"start\":98322},{\"end\":98343,\"start\":98334},{\"end\":98352,\"start\":98343},{\"end\":98360,\"start\":98352},{\"end\":98368,\"start\":98360},{\"end\":98375,\"start\":98368},{\"end\":98388,\"start\":98375},{\"end\":98830,\"start\":98824},{\"end\":98848,\"start\":98830},{\"end\":99081,\"start\":99069},{\"end\":99092,\"start\":99081},{\"end\":99297,\"start\":99290},{\"end\":99306,\"start\":99297},{\"end\":99314,\"start\":99306},{\"end\":99323,\"start\":99314},{\"end\":99645,\"start\":99634},{\"end\":99654,\"start\":99645},{\"end\":99665,\"start\":99654},{\"end\":99678,\"start\":99665},{\"end\":99924,\"start\":99914},{\"end\":99935,\"start\":99924},{\"end\":99945,\"start\":99935},{\"end\":100267,\"start\":100257},{\"end\":100280,\"start\":100267},{\"end\":100511,\"start\":100499},{\"end\":100525,\"start\":100511},{\"end\":100756,\"start\":100747},{\"end\":100768,\"start\":100756},{\"end\":100778,\"start\":100768},{\"end\":101122,\"start\":101113},{\"end\":101134,\"start\":101122},{\"end\":101144,\"start\":101134},{\"end\":101476,\"start\":101467},{\"end\":101488,\"start\":101476},{\"end\":101498,\"start\":101488},{\"end\":101778,\"start\":101768},{\"end\":101787,\"start\":101778},{\"end\":101798,\"start\":101787},{\"end\":101808,\"start\":101798},{\"end\":102074,\"start\":102066},{\"end\":102084,\"start\":102074},{\"end\":102092,\"start\":102084},{\"end\":102103,\"start\":102092},{\"end\":102119,\"start\":102103},{\"end\":102130,\"start\":102119},{\"end\":102141,\"start\":102130},{\"end\":102344,\"start\":102335},{\"end\":102357,\"start\":102344},{\"end\":102365,\"start\":102357},{\"end\":102379,\"start\":102365},{\"end\":102575,\"start\":102566},{\"end\":102583,\"start\":102575},{\"end\":102589,\"start\":102583},{\"end\":102598,\"start\":102589},{\"end\":102875,\"start\":102869},{\"end\":102893,\"start\":102875},{\"end\":103103,\"start\":103095},{\"end\":103396,\"start\":103387},{\"end\":103405,\"start\":103396},{\"end\":103698,\"start\":103682},{\"end\":103708,\"start\":103698},{\"end\":104130,\"start\":104116},{\"end\":104139,\"start\":104130},{\"end\":104147,\"start\":104139},{\"end\":104435,\"start\":104427},{\"end\":104446,\"start\":104435},{\"end\":104458,\"start\":104446},{\"end\":104469,\"start\":104458},{\"end\":104478,\"start\":104469},{\"end\":104488,\"start\":104478},{\"end\":104492,\"start\":104488},{\"end\":104801,\"start\":104794},{\"end\":104807,\"start\":104801},{\"end\":104816,\"start\":104807},{\"end\":104822,\"start\":104816},{\"end\":104830,\"start\":104822},{\"end\":104838,\"start\":104830},{\"end\":105270,\"start\":105264},{\"end\":105279,\"start\":105270},{\"end\":105548,\"start\":105539},{\"end\":105559,\"start\":105548},{\"end\":105731,\"start\":105719},{\"end\":105742,\"start\":105731},{\"end\":105751,\"start\":105742},{\"end\":105758,\"start\":105751},{\"end\":105766,\"start\":105758},{\"end\":105774,\"start\":105766},{\"end\":105784,\"start\":105774},{\"end\":105794,\"start\":105784},{\"end\":105807,\"start\":105794},{\"end\":105815,\"start\":105807},{\"end\":105827,\"start\":105815},{\"end\":106104,\"start\":106090},{\"end\":106116,\"start\":106104},{\"end\":106130,\"start\":106116},{\"end\":106143,\"start\":106130},{\"end\":106162,\"start\":106143},{\"end\":106454,\"start\":106443},{\"end\":106463,\"start\":106454},{\"end\":106471,\"start\":106463},{\"end\":106481,\"start\":106471},{\"end\":106816,\"start\":106802},{\"end\":106829,\"start\":106816},{\"end\":106841,\"start\":106829},{\"end\":107152,\"start\":107140},{\"end\":107165,\"start\":107152},{\"end\":107403,\"start\":107392}]", "bib_venue": "[{\"end\":73990,\"start\":73969},{\"end\":74317,\"start\":74172},{\"end\":74615,\"start\":74572},{\"end\":74844,\"start\":74818},{\"end\":75109,\"start\":75013},{\"end\":75385,\"start\":75364},{\"end\":75694,\"start\":75512},{\"end\":76077,\"start\":76007},{\"end\":76377,\"start\":76310},{\"end\":76639,\"start\":76559},{\"end\":76973,\"start\":76918},{\"end\":77296,\"start\":77227},{\"end\":77545,\"start\":77464},{\"end\":77890,\"start\":77729},{\"end\":78305,\"start\":78243},{\"end\":78718,\"start\":78659},{\"end\":78991,\"start\":78949},{\"end\":79208,\"start\":79131},{\"end\":79420,\"start\":79322},{\"end\":79771,\"start\":79743},{\"end\":80096,\"start\":80057},{\"end\":80376,\"start\":80362},{\"end\":80567,\"start\":80502},{\"end\":80941,\"start\":80873},{\"end\":81342,\"start\":81265},{\"end\":81951,\"start\":81835},{\"end\":82720,\"start\":82610},{\"end\":83282,\"start\":83061},{\"end\":83829,\"start\":83648},{\"end\":84371,\"start\":84303},{\"end\":84670,\"start\":84626},{\"end\":84770,\"start\":84764},{\"end\":85002,\"start\":84977},{\"end\":85319,\"start\":85245},{\"end\":85673,\"start\":85626},{\"end\":85957,\"start\":85871},{\"end\":86297,\"start\":86270},{\"end\":86491,\"start\":86457},{\"end\":86751,\"start\":86734},{\"end\":87022,\"start\":86920},{\"end\":87373,\"start\":87333},{\"end\":87669,\"start\":87571},{\"end\":87946,\"start\":87894},{\"end\":88319,\"start\":88225},{\"end\":88869,\"start\":88807},{\"end\":89160,\"start\":89092},{\"end\":89371,\"start\":89299},{\"end\":89660,\"start\":89644},{\"end\":89826,\"start\":89774},{\"end\":90047,\"start\":89990},{\"end\":90359,\"start\":90283},{\"end\":90749,\"start\":90718},{\"end\":91056,\"start\":91027},{\"end\":91361,\"start\":91260},{\"end\":91771,\"start\":91709},{\"end\":92130,\"start\":92091},{\"end\":92408,\"start\":92392},{\"end\":92731,\"start\":92690},{\"end\":93030,\"start\":92949},{\"end\":93266,\"start\":93223},{\"end\":93557,\"start\":93496},{\"end\":93986,\"start\":93924},{\"end\":94487,\"start\":94378},{\"end\":94935,\"start\":94885},{\"end\":95219,\"start\":95155},{\"end\":95500,\"start\":95398},{\"end\":95938,\"start\":95891},{\"end\":96367,\"start\":96290},{\"end\":96689,\"start\":96621},{\"end\":96925,\"start\":96865},{\"end\":97275,\"start\":97199},{\"end\":97714,\"start\":97638},{\"end\":98114,\"start\":98045},{\"end\":98495,\"start\":98388},{\"end\":98865,\"start\":98848},{\"end\":99067,\"start\":99003},{\"end\":99363,\"start\":99323},{\"end\":99632,\"start\":99542},{\"end\":99992,\"start\":99945},{\"end\":100299,\"start\":100280},{\"end\":100593,\"start\":100525},{\"end\":100829,\"start\":100778},{\"end\":101158,\"start\":101144},{\"end\":101465,\"start\":101340},{\"end\":101825,\"start\":101808},{\"end\":102064,\"start\":101985},{\"end\":102615,\"start\":102598},{\"end\":102910,\"start\":102893},{\"end\":103175,\"start\":103103},{\"end\":103385,\"start\":103267},{\"end\":103763,\"start\":103708},{\"end\":104164,\"start\":104147},{\"end\":104508,\"start\":104492},{\"end\":104893,\"start\":104838},{\"end\":105296,\"start\":105279},{\"end\":105537,\"start\":105472},{\"end\":105717,\"start\":105670},{\"end\":106197,\"start\":106162},{\"end\":106557,\"start\":106481},{\"end\":106904,\"start\":106841},{\"end\":107246,\"start\":107180},{\"end\":107571,\"start\":107465},{\"end\":107747,\"start\":107743},{\"end\":77024,\"start\":76975},{\"end\":81415,\"start\":81344},{\"end\":82059,\"start\":81953},{\"end\":85716,\"start\":85675},{\"end\":86764,\"start\":86753},{\"end\":88409,\"start\":88321},{\"end\":90431,\"start\":90361},{\"end\":95981,\"start\":95940},{\"end\":96440,\"start\":96369},{\"end\":97347,\"start\":97277},{\"end\":97786,\"start\":97716},{\"end\":98878,\"start\":98867},{\"end\":101838,\"start\":101827},{\"end\":102628,\"start\":102617},{\"end\":102923,\"start\":102912},{\"end\":103814,\"start\":103765},{\"end\":104177,\"start\":104166},{\"end\":104520,\"start\":104510},{\"end\":104944,\"start\":104895},{\"end\":105309,\"start\":105298},{\"end\":106629,\"start\":106559}]"}}}, "year": 2023, "month": 12, "day": 17}