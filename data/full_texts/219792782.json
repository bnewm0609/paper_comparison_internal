{"id": 219792782, "updated": "2023-10-06 14:34:50.857", "metadata": {"title": "Stochastic Bandits with Linear Constraints", "authors": "[{\"first\":\"Aldo\",\"last\":\"Pacchiano\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Ghavamzadeh\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Bartlett\",\"middle\":[]},{\"first\":\"Heinrich\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 17}, "abstract": "We study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of $T$ rounds is maximum, and each has an expected cost below a certain threshold $\\tau$. We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove an $\\widetilde{\\mathcal{O}}(\\frac{d\\sqrt{T}}{\\tau-c_0})$ bound on its $T$-round regret, where the denominator is the difference between the constraint threshold and the cost of a known feasible action. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting. We prove a regret bound of $\\widetilde{\\mathcal{O}}(\\frac{\\sqrt{KT}}{\\tau - c_0})$ for this algorithm in $K$-armed bandits, which is a $\\sqrt{K}$ improvement over the regret bound we obtain by simply casting multi-armed bandits as an instance of contextual linear bandits and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.10185", "mag": "3036130182", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aistats/PacchianoGBJ21", "doi": null}}, "content": {"source": {"pdf_hash": "d830c7a19d18d864e354427abb4e2a2e07d865e4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.10185v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "51993881dc3a36095ad4eddc088c4ecfa547769c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d830c7a19d18d864e354427abb4e2a2e07d865e4.txt", "contents": "\nStochastic Bandits with Linear Constraints\n\n\nAldo Pacchiano pacchiano@berkeley.edu \nUC Berkeley\nBerkeley\n\nMohammad Ghavamzadeh ghavamza@google.com \nUC Berkeley\nBerkeley\n\nGoogle Research \nUC Berkeley\nBerkeley\n\nPeter Bartlett \nUC Berkeley\nBerkeley\n\nHeinrich Jiang heinrichj@google.com \nUC Berkeley\nBerkeley\n\nGoogle Research \nUC Berkeley\nBerkeley\n\nStochastic Bandits with Linear Constraints\n\nWe study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of T rounds is maximum, and each has an expected cost below a certain threshold \u03c4 . We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove an O( d \u221a T \u03c4 \u2212c0 ) bound on its T -round regret, where the denominator is the difference between the constraint threshold and the cost of a known feasible action. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting. We prove a regret bound of O( \u221a KT \u03c4 \u2212c0 ) for this algorithm in K-armed bandits, which is a \u221a K improvement over the regret bound we obtain by simply casting multi-armed bandits as an instance of contextual linear bandits and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results.Preprint. Under review. We adopt the following notation. The set {1, . . . , T } is denoted by [T ]. We represent the set of distributions with support over a compact set S by \u2206 S . We denote by x, y := x y \u2208 R, the inner product of two vectors x, y \u2208 R d , and by x := \u221a x x, the 2 -norm of vector x.The setting we study in this paper is contextual linear bandit with linear constraints. In each round t, the agent is given an decision set A t \u2282 R d from which it has to choose an action x t . Upon taking action x t \u2208 A, it observes a pair (r t , c t ), where r t = x t , \u03b8 * + \u03be r t and c t = x t , \u00b5 * + \u03be c t are the reward and cost signals, respectively. In the reward and cost definitions, \u03b8 * \u2208 R d and \u00b5 * \u2208 R d are the unknown reward and cost parameters, and \u03be r t and \u03be c t are reward and cost noise, satisfying conditions that will be specified soon. The agent selects its action x t \u2208 A t in each round t according to its policy \u03c0 t \u2208 \u2206 At at that round, i.e., x t \u223c \u03c0 t . 1  In the rest of the paper, we simply refer to the T -round constrained pseudo-regret R\u03a0(T ) as T -round regret. 2  The choice of the same upper-bounds for \u03b8 * and \u00b5 * is just for simplicity. 3  In the case of x0 = 0 \u2208 R d , we define Vo as the empty subspace and V \u22a5 o as the whole R d .\n\nIntroduction\n\nA multi-armed bandit (MAB) [Lai and Robbins, 1985, Auer et al., 2002, Lattimore and Szepesv\u00e1ri, 2019 is an online learning problem in which the agent acts by pulling arms. After an arm is pulled, the agent receives its stochastic reward. The goal of the agent is to maximize its expected cumulative reward without knowledge of the arms' distributions. To achieve this goal, the agent has to balance its exploration and exploitation: to decide when to explore and learn about the arms, and when to exploit and pull the arm with the highest estimated reward thus far. A stochastic linear bandit [Dani et al., 2008, Rusmevichientong and Tsitsiklis, 2010, Abbasi-Yadkori et al., 2011 is a generalization of MAB to the setting where each of (possibly) infinitely many arms is associated with a feature vector. The mean reward of an arm is the dot product of its feature vector and an unknown parameter vector, which is shared by all the arms. This formulation contains time-varying action (arm) sets and feature vectors, and thus, includes the linear contextual bandit setting. These models capture many practical applications spanning clinical trials [Villar et al., 2015], recommendation systems [Li et al., 2010, Balakrishnan et al., 2018, wireless networks [Maghsudi and Hossain, 2016], sensors [Washburn, 2008], and strategy games [Ontan\u00f3n, 2013]. The most popular exploration strategies in stochastic bandits are optimism in the face of uncertainty (OFU) [Auer et al., 2002] and Thompson sampling (TS) [Thompson, 1933, Agrawal and Goyal, 2013a, Russo et al., 2018 that are relatively well understood in both multi-armed and linear bandits [Dani et al., 2008, Abbasi-Yadkori et al., 2011, Agrawal and Goyal, 2013b, Lattimore and Szepesv\u00e1ri, 2019.\n\nIn many practical problems, the agent requires to satisfy certain operational constraints while maximizing its cumulative reward. Depending on the form of the constraints, several constrained stochastic bandit settings have been formulated and analyzed. One such setting is what is known as knapsack bandits. In this setting, pulling each arm, in addition to producing a reward signal, results in a random consumption of a global budget, and the goal is to maximize the cumulative reward before the budget is fully consumed (e.g., Badanidiyuru et al. 2013, Agrawal and Devanur 2014, Wu et al. 2015, Agrawal and Devanur 2016. Another such setting is referred to as conservative bandits. In this setting, there is a baseline arm or policy, and the agent, in addition to maximizing its cumulative reward, should ensure that at each round, the difference between its cumulative reward and that of the baseline remains below a predefined fraction of the baseline cumulative reward [Wu et al., 2016, Kazerouni et al., 2017, Garcelon et al., 2020. In these two settings, the constraint applies to a cumulative quantity (budget consumption or reward) over the entire run of the algorithm. Thus, the set of feasible actions at each round is a function of the history of the algorithm.\n\nAnother constrained bandit setting is where each arm is associated with two (unknown) distributions, generating reward and cost signals. The goal is to maximize the cumulative reward, while making sure that with high probability, the expected cost of the arm pulled at each round is below a certain threshold. Here the constraint is stage-wise, and unlike the last two settings, is independent of the history. Amani et al. [2019] and Moradipari et al. [2019] have recently studied this setting for linear bandits and derived and analyzed explore-exploit  and Thompson sampling [Moradipari et al., 2019] algorithms for it.\n\nThis setting is the closest to the one we study in this paper. In our setting, we also assume two distributions for each arm, one for reward and for cost. At each round the agent constructs a policy according to which it takes its action. The goal of the agent is to produce a sequence of policies with maximum expected cumulative reward, while making sure that the expected cost of the constructed policy (not the pulled arm) at each round is below a certain threshold. This is a linear constraint and can be easily extended to more constraints by having more cost distributions associated to each arm, one per each constraint. Compared to the previous setting, our constraint is more relaxed (from high-probability to expectation), and as a result, it would be possible for us to obtain a solution with larger expected cumulative reward. We will have a detailed discussion on the relationship between these two settings and the similarities and differences of our results with those reported in Amani et al. [2019] and Moradipari et al. [2019] in Section 7.\n\nIn this paper, we study the above setting for contextual linear bandits. After defining the setting in Section 2, we propose an upper-confidence bound (UCB) algorithm for it, called optimistic pessimistic linear bandit (OPLB), in Section 3. We prove an O( d \u221a T \u03c4 \u2212c0 ) bound on the T -round regret of OPLB in Section 4, where d is the action dimension and \u03c4 \u2212 c 0 is the difference between the constraint threshold and the cost of a known feasible action. The action set considered in our contextual linear bandit setting is general enough to include MAB. However, in Section 5, we further specialize our results to MAB and propose a computationally efficient algorithm for this setting, called optimistic pessimistic bandit (OPB). We show that in the MAB case, there always exists a feasible optimal policy with probability mass on at most m + 1 arms, where m is the number of linear constraints. This property plays an important role in the computational efficiency of OPB. We prove a regret bound of O( \u221a KT \u03c4 \u2212c0 ) for OPB in K-armed bandits, which is a \u221a K improvement over the regret bound we obtain by simply casting MAB as an instance of contextual linear bandit and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results.\n\nThe goal of the agent is to produce a sequence of policies {\u03c0 t } T t=1 with maximum expected cumulative reward over the course of T rounds, while satisfying the linear constraint\nE x\u223c\u03c0t [ x, \u00b5 * ] \u2264 \u03c4, \u2200t \u2208 [T ],\n(\u03c4 \u2265 0 is referred to as the constraint threshold). (1) Thus, the policy \u03c0 t selected by the agent in each round t \u2208 [T ] should belong to the set of feasible policies over the action set A t , i.e.,\n\u03a0 t = {\u03c0 \u2208 \u2206 At : E x\u223c\u03c0 [ x, \u00b5 * ] \u2264 \u03c4 }.\nMaximizing the expected cumulative reward in T rounds is equivalent to minimizing the T -round constrained pseudo-regret, 1\nR \u03a0 (T ) = T t=1 E x\u223c\u03c0 * t [ x, \u03b8 * ] \u2212 E x\u223c\u03c0t [ x, \u03b8 * ],(2)\nwhere \u03c0 t , \u03c0 * t \u2208 \u03a0 t \u2200t \u2208 [T ] and \u03c0 * t is the optimal feasible policy at round t, i.e., (1) and (2) are the expected reward and cost of policy \u03c0, respectively. Thus, a feasible policy is the one whose expected cost is below the constraint threshold \u03c4 , and the optimal feasible policy is a feasible policy with maximum expected reward. We use the shorthand notations x \u03c0 := E x\u223c\u03c0 [x], r \u03c0 := E x\u223c\u03c0 [ x, \u03b8 * ] and c \u03c0 := E x\u223c\u03c0 [ x, \u00b5 * ] for the expected action, reward and cost of a policy \u03c0. With these shorthand notations, we may write the T -round pseudo-regret as R \u03a0 (T ) = T t=1 r \u03c0 * t \u2212 r \u03c0t . We make the following assumptions for our setting. The first four assumptions are standard in linear bandits. The fifth one is necessary to guarantee constraint satisfaction (safety).\n\u03c0 * t \u2208 max \u03c0\u2208\u03a0t E x\u223c\u03c0 [ x, \u03b8 * ]. The terms E x\u223c\u03c0 [ x, \u03b8 * ] and E x\u223c\u03c0 [ x, \u00b5 * ] in\nAssumption 1. For all t \u2208 [T ], the reward and cost noise random variables \u03be r t and \u03be c t are conditionally R-sub-Gaussian, i.e.,\nE[\u03be r t | F t\u22121 ] = 0, E[exp(\u03b1\u03be r t ) | F t\u22121 ] \u2264 exp(\u03b1 2 R 2 /2), \u2200\u03b1 \u2208 R, E[\u03be c t | F t\u22121 ] = 0, E[exp(\u03b1\u03be c t ) | F t\u22121 ] \u2264 exp(\u03b1 2 R 2 /2), \u2200\u03b1 \u2208 R,\nwhere F t is the filtration that includes all the events (x 1:t+1 , \u03be r 1:t , \u03be c 1:t ) until the end of round t. Assumption 2. There is a known constant S > 0, such that \u03b8 * \u2264 S and \u00b5 * \u2264 S. 2 Assumption 3. The 2 -norm of all actions is bounded, i.e., max t\u2208[T ] max x\u2208At x \u2264 L.\n\nAssumption 4. For all t \u2208 [T ] and x \u2208 A t , the mean rewards and costs are bounded, i.e., x, \u03b8 * \u2208 [0, 1] and x, \u00b5 * \u2208 [0, 1].\n\nAssumption 5. There is a known safe action x 0 \u2208 A t , \u2200t \u2208 [T ] with known cost c 0 , i.e., x 0 , \u00b5 * = c 0 < \u03c4 . We will show how the assumption of knowing c 0 can be relaxed later in the paper.\n\nNotation: We conclude this section with introducing another set of notations that will be used in the rest of the paper. We define the normalized safe action as e 0 := x 0 / x 0 and the span of the safe action as\nV o := span(x 0 ) = {\u03b7x 0 : \u03b7 \u2208 R}. We denote by V \u22a5 o , the orthogonal complement of V o , i.e., V \u22a5 o = {x \u2208 R d : x, y = 0, \u2200y \u2208 V o }. 3 We define the projection of a vector x \u2208 R d into the sub-space V o , as x o := x, e 0 e 0 , and into the sub-space V \u22a5 o , as x o,\u22a5 := x \u2212 x o . We also define the projection of a policy \u03c0 into V o and V \u22a5 o , as x o \u03c0 := E x\u223c\u03c0 [x o ] and x o,\u22a5 \u03c0 := E x\u223c\u03c0 [x o,\u22a5 ].\n\nOptimistic-Pessimistic Linear Bandit Algorithm\n\nIn this section, we propose an algorithm, called optimistic-pessimistic linear bandit (OPLB), whose pseudo-code is shown in Algorithm 1. Our OPLB algorithm balances a pessimistic assessment of the set of available policies, while acting optimistically within this set. Our principal innovation is the use of confidence intervals with asymmetric radii, proportional to \u03b1 r and \u03b1 c , for the reward and cost signals. This will prove crucial in the regret analysis of the algorithm.\n\nAlgorithm 1 Optimistic-Pessimistic Linear Bandit (OPLB) Input: Horizon T , Confidence Parameter \u03b4, Regularization Parameter \u03bb, Constants \u03b1r, \u03b1c \u2265 1 for t = 1, . . . , T do 1. Compute RLS estimates \u03b8 t and \u00b5 o,\u22a5 t (see Eqs. 3 to 5) 2. Construct sets C r t (\u03b1 r ) and C c t (\u03b1 c ) (see Eq. 7) 3. Observe A t and construct the (estimated) safe policy set \u03a0 t (see Eq. 12) 4. Compute policy (\u03c0 t , \u03b8 t ) = arg max \u03c0\u2208\u03a0t, \u03b8\u2208C r t (\u03b1r) E x\u223c\u03c0 [ x, \u03b8 ] 5. Take action x t \u223c \u03c0 t and observe reward and cost (r t , c t ) Line 1 of OPLB: At each round t \u2208 [T ], given the actions {x s } t\u22121 s=1 , rewards {r s } t\u22121 s=1 , and costs {c s } t\u22121 s=1 observed until the end of round t \u2212 1, OPLB first computes the 2 -regularized least-squares (RLS) estimates of \u03b8 * and \u00b5 o,\u22a5 * (projection of the cost parameter \u00b5 * into the sub-\nspace V \u22a5 o ) as \u03b8t = \u03a3 \u22121 t t\u22121 s=1 rsxs, \u00b5 o,\u22a5 t = (\u03a3 o,\u22a5 t ) \u22121 t\u22121 s=1 c o,\u22a5 s x o,\u22a5 s ,(3)\nwhere \u03bb > 0 is the regularization parameter, and\n\u03a3t = \u03bbI + t\u22121 s=1 xsx s , \u03a3 o,\u22a5 t = \u03bbI V \u22a5 o + t\u22121 s=1 x o,\u22a5 s (x o,\u22a5 s ) ,(4)c o,\u22a5 t = ct \u2212 xt, e0 x0 c0, I V \u22a5 o = I d\u00d7d \u2212 1 x0 2 x0x 0 .(5)\nIn (4), \u03a3 t and \u03a3 o,\u22a5 t are the Gram matrices of actions and projection of actions into the sub-space V \u22a5 o . Note that \u03a3 o,\u22a5 t is a rank deficient matrix, but with abuse of notation, we use (\u03a3 o,\u22a5 t ) \u22121 to denote its pseudo-inverse throughout the paper. In (5), I V \u22a5 o is the projection of the identity matrix, I, into V \u22a5 o , and c o,\u22a5 t is the noisy projection of the cost c t incurred by taking action\nx t into V \u22a5 o , i.e., 4 c o,\u22a5 t = x o,\u22a5 t , \u00b5 o,\u22a5 * + \u03be c t = xt, \u00b5 * \u2212 x o t , \u00b5 o * + \u03be c t = ct \u2212 x o t , \u00b5 o * = ct \u2212 xt, e0 x0 c0.(6)\nLine 2: Using the RLS estimates \u03b8 t and \u00b5 o,\u22a5 t in (3), OPLB constructs the two confidence sets\nC r t (\u03b1r) = \u03b8 \u2208 R d : \u03b8\u2212 \u03b8t \u03a3 t \u2264 \u03b1r\u03b2t(\u03b4, d) , C c t (\u03b1c) = \u00b5 \u2208 V \u22a5 o : \u00b5\u2212 \u00b5 o,\u22a5 t \u03a3 o,\u22a5 t \u2264 \u03b1c\u03b2t(\u03b4, d\u22121) ,(7)\nwhere \u03b1 r , \u03b1 c \u2265 1 and \u03b2 t (\u03b4, d) in the radii of these confidence ellipsoids is defined by the following theorem, originally proved in Abbasi-Yadkori et al. [2011].  (3) and (4), and C r t (\u00b7) and C c t (\u00b7) defined by (7). Then, for a fixed \u03b4 \u2208 (0, 1) and\n\u03b2t(\u03b4, d) = R d log 1 + (t \u2212 1)L 2 /\u03bb \u03b4 + \u221a \u03bb S,(8)\nwith probability at least 1 \u2212 \u03b4 and for all t \u2265 1, it holds that \u03b8 * \u2208 C r t (1) and \u00b5 o,\u22a5 * \u2208 C c t (1).\n\nSince \u03b1 r , \u03b1 c \u2265 1, for all rounds t \u2208 [T ], the sets C r t (\u03b1 r ) and C c t (\u03b1 c ) also contain \u03b8 * , the reward parameter, and \u00b5 o,\u22a5 * , the projection of the cost parameter into V \u22a5 o , respectively, with high probability. Given these confidence sets, we define the optimistic reward and pessimistic cost of any policy \u03c0 in round t as r\u03c0,t := max\n\u03b8\u2208C r t (\u03b1r ) Ex\u223c\u03c0[ x, \u03b8 ], c\u03c0,t := x o \u03c0 , e0 c0 x0 + max \u00b5\u2208C c t (\u03b1c) Ex\u223c\u03c0[ x, \u00b5 ].(9)\nProposition 1. We may write (9) in closed-form as (proof in Appendix A.1) 4 In the derivation of (6), we use the fact that xt, \u00b5 * =\nr\u03c0,t = x\u03c0, \u03b8t + \u03b1r\u03b2t(\u03b4, d) x\u03c0 \u03a3 \u22121 t ,(10)c\u03c0,t = x o \u03c0 , e0 c0 x0 + x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 t + \u03b1c\u03b2t(\u03b4, d \u2212 1) x o,\u22a5 \u03c0 (\u03a3 o,\u22a5 t ) \u22121 .(11)x o t + x o,\u22a5 t , \u00b5 o * + \u00b5 o,\u22a5 * = x o t , \u00b5 o * + x o,\u22a5 t , \u00b5 o,\u22a5 * .\nLine 3: After observing the action set A t , OPLB constructs its (estimated) feasible (safe) policy set\n\u03a0t = {\u03c0 \u2208 \u2206A t : c\u03c0,t \u2264 \u03c4 },(12)\nwhere c \u03c0,t is the pessimistic cost of policy \u03c0 in round t defined by (11). Note that \u03a0 t is not empty since \u03c0 0 , the policy that plays the safe action x 0 with probability (w.p.) 1, is always in \u03a0 t . This is\nbecause x o \u03c00 = x 0 , x o,\u22a5 \u03c00 = 0, and x o \u03c0 0\n,e0 c0 x0 = c 0 . In the following proposition, whose proof is reported in Appendix A.2, we prove that all policies in \u03a0 t are feasible with high probability. Proposition 2. With probability at least 1 \u2212 \u03b4, for all rounds t \u2208 [T ], all policies in \u03a0 t are feasible.\n\nLine 4: The agent computes its policy, \u03c0 t , as the one that is safe (belongs to \u03a0 t ) and attains the maximum optimistic reward. We refer to \u03b8 t as the optimistic reward parameter. Thus, we write the optimistic reward of policy \u03c0 t as r \u03c0t,t = x \u03c0t , \u03b8 t .\n\nLine 5: Finally, the agent selects an action x t \u223c \u03c0 t and observes the reward-cost pair (r t , c t ).\n\nComputational Complexity of OPLB. As shown in Line 4 of Algorithm 1 and in Proposition 1, in each round t, OPLB solves the following optimization problem:\nmax \u03c0\u2208\u2206 A t x\u03c0, \u03b8t + \u03b1r\u03b2t(\u03b4, d) x\u03c0 \u03a3 \u22121 t (13) s.t. x o \u03c0 , e0 c0 x0 + x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 t + \u03b1c\u03b2t(\u03b4, d \u2212 1) x o,\u22a5 \u03c0 (\u03a3 o,\u22a5 t ) \u22121 \u2264 \u03c4.\nHowever, solving (13) can be challenging. The bottleneck is computing the safe policy set \u03a0 t , which is the intersection between \u2206 At and the ellipsoidal constraint. Remark 1. The main challenge in obtaining a regret bound for OPLB is to ensure that optimism holds in each round t, i.e., the solution (\u03c0 t , \u03b8 t ) of (13) satisfy r \u03c0t,t = x \u03c0t , \u03b8 t \u2265 r \u03c0 * t . This is not obvious, since the (estimated) safe policy set \u03a0 t may not contain the optimal policy \u03c0 * t . Our main algorithmic innovation is the use of asymmetric confidence intervals C r t (\u03b1 r ) and C c t (\u03b1 c ) for \u03b8 * and \u00b5 o,\u22a5 * , which allows us to guarantee optimism, by appropriately selecting the ratio \u03b3 = \u03b1 r /\u03b1 c . Of course, this comes at the cost of scaling the regret by a factor \u03b3. As it will be shown in our analysis in Section 4, \u03b3 depends on the inverse gap 1/(\u03c4 \u2212 c 0 ), which indicates when \u03c4 \u2212 c 0 is small (the cost of the safe arm is close to the constraint threshold), the agent will have a difficult time to identify a safe arm and to compete against the optimal feasible policy \u03c0 * t . We will formalize this in Lemma 4. Remark 2. If the cost of the safe arm c 0 is unknown, we start by taking the safe action x 0 for T 0 rounds to produce a conservative estimate\u03b4 c of \u03c4 \u2212 c 0 that satisfies\u03b4 c \u2265 \u03c4 \u2212c0 2 . We warm start our estimators for \u03b8 * and \u00b5 * using the data collected by playing x 0 . However, instead of estimating \u00b5 o,\u22a5 * , we build an estimator for \u00b5 * over all its directions, including e 0 , similar to what OPLB does for \u03b8 * . We then set \u03b1r \u03b1c = 1/\u03b4 c and run Algorithm 1 for rounds t > T 0 (see Appendix B.4 for more details).\n\n\nRegret Analysis\n\nIn this section, we prove the following regret bound for OPLB (Algorithm 1). Theorem 2 (Regret of OPLB). Let \u03b1 c = 1 and \u03b1 r = 2+\u03c4 \u2212c0 \u03c4 \u2212c0 . Then, with probability at least 1 \u2212 2\u03b4, the regret of OPLB satisfies\nR\u03a0(T ) \u2264 2L(\u03b1r + 1)\u03b2T (\u03b4, d) \u221a \u03bb 2T log(1/\u03b4) + (\u03b1r + 1)\u03b2T (\u03b4, d) 2T d log(1 + T L 2 \u03bb ).(14)\nWe start the proof of Theorem 2, by defining the following event that holds w.p. at least 1 \u2212 \u03b4:\nE := \u03b8t \u2212 \u03b8 * \u03a3 t \u2264 \u03b2t(\u03b4, d) \u2227 \u00b5 o,\u22a5 t \u2212 \u00b5 o,\u22a5 * \u03a3 o,\u22a5 t \u2264 \u03b2t(\u03b4, d \u2212 1), \u2200t \u2208 [T ] .(15)\nThe regret R \u03a0 (T ) in (2) can be decomposed as ( r \u03c0t,t is the optimistic reward defined by Eq. 9)\nR\u03a0(T ) = T t=1 r \u03c0 * t \u2212 r\u03c0 t ,t (I) + T t=1 r\u03c0 t ,t \u2212 r\u03c0 t (II) .(16)\nWe first bound the term (II) in (16). To bound (II), we further decompose it as\n(II) = T t=1 x\u03c0 t , \u03b8t \u2212 xt, \u03b8t (III) + T t=1 xt, \u03b8t \u2212 xt, \u03b8 * (IV) + T t=1\nxt, \u03b8 * \u2212 x\u03c0 t , \u03b8 * (V) .\n\nIn the following lemmas, we first bound the sum of (III) and (V) terms, and then bound (IV). Lemma 1. On the event E defined by (15), for any \u03b3 \u2208 (0, 1), w.p. at least 1 \u2212 \u03b3, we have\n(III) + (V) \u2264 2L(\u03b1r + 1)\u03b2T (\u03b4, d) \u221a \u03bb \u00b7 2T log(1/\u03b3) . Proof. We write (III) + (V) = T t=1 x \u03c0t \u2212 x t , \u03b8 t \u2212 \u03b8 * . By Cauchy-Schwartz, we have | x \u03c0t \u2212 x t , \u03b8 t \u2212 \u03b8 * | \u2264 x \u03c0t \u2212 x t \u03a3 \u22121 t \u03b8 t \u2212 \u03b8 * \u03a3t . Since \u03b8 t \u2208 C r t (\u03b1 r ), on event E, we have \u03b8 t \u2212 \u03b8 * \u03a3t \u2264 (\u03b1 r + 1)\u03b2 t (\u03b4, d). Also from the definition of \u03a3 t , we have \u03a3 t \u03bbI, and thus, x \u03c0t \u2212 x t \u03a3 \u22121 t \u2264 x \u03c0t \u2212 x t / \u221a \u03bb \u2264 2L/ \u221a \u03bb. Therefore, Y t = t s=1 x \u03c0s \u2212 x s , \u03b8 s \u2212 \u03b8 * is a martingale sequence with |Y t \u2212 Y t\u22121 | \u2264 2L(\u03b1 r + 1)\u03b2 t (\u03b4, d)/ \u221a \u03bb, for t \u2208 [T ]\n. By the Azuma-Hoeffding inequality and since \u03b2 t is an increasing function of t, i.e.,\n\u03b2 t (\u03b4, d) \u2264 \u03b2 T (\u03b4, d), \u2200t \u2208 [T ], w.p. at least 1 \u2212 \u03b3, we have P Y T \u2265 2L(\u03b1 r + 1)\u03b2 T (\u03b4, d) 2T log(1/\u03b3)/\u03bb \u2264 \u03b3, which concludes the proof. Lemma 2. On event E, we have (IV) \u2264 (\u03b1 r + 1)\u03b2 T (\u03b4, d) 2T d log 1 + T L 2 \u03bb .\nWe report the proof of Lemma 2 in Appendix B.1. After bounding all the terms in (II), we now process the term (I) in (16). Before stating the main result for this term in Lemma 4, we need to prove the following lemma (proof in Appendix B.2). Lemma 3. For any policy \u03c0, the following inequality holds:\nx o,\u22a5 \u03c0 (\u03a3 o,\u22a5 t ) \u22121 \u2264 x\u03c0 \u03a3 \u22121 t .(18)\nIn the following lemma, we prove that by appropriately setting the parameters \u03b1 r and \u03b1 c , we can guarantee that at each round t \u2208 [T ], OPLB selects an optimistic policy, i.e., a policy \u03c0 t , whose optimistic reward, r \u03c0t,t , is larger than the reward of the optimal policy r \u03c0 * t , given the event E. This means that with our choice of parameters \u03b1 r and \u03b1 c , the term (I) in (16) is always non-positive. Lemma 4. On the event E, if we set \u03b1 r and \u03b1 c , such that \u03b1 r , \u03b1 c \u2265 1 and 1 + \u03b1 c \u2264 (\u03c4 \u2212 c 0 )(\u03b1 r \u2212 1), then for any t \u2208 [T ], we have r \u03c0t,t \u2265 r \u03c0 * t . Here we provide a proof sketch for Lemma 4. The detailed proof is reported in Appendix B.3.\n\nProof Sketch. We divide the proof into two cases, depending on whether in each round t, the optimal policy \u03c0 *\n\n\nConstrained Multi-Armed Bandits\n\nIn this section, we specialize our results for contextual linear bandits to multi-armed bandits (MAB) and show that the structure of the MAB problem allows a computationally efficient implementation of the algorithm and an improvement in the regret bound.\n\nIn the MAB setting, the action set consists of K arms A = {1, . . . , K}. Each arm a \u2208 [K] has a reward and a cost distribution with meansr a ,c a \u2208 [0, 1]. In each round t \u2208 [T ], the agent constructs a policy \u03c0 t over A, pulls an arm a t \u223c \u03c0 t , and observes a reward-cost pair (r at , c at ) sampled i.i.d. from the reward and cost distributions of arm a t . Similar to the constrained contextual linear case, the goal of the agent is to produce a sequence of policies {\u03c0 t } T t=1 with maximum expected cumulative reward over T rounds, i.e.,\nT t=1 E at\u223c\u03c0t [r at ], while satisfying the linear constraint E at\u223c\u03c0t [c at ] \u2264 \u03c4, \u2200t \u2208 [T ]\n. Moreover, arm 1 is assumed to be the known safe arm, i.e.,c 1 \u2264 \u03c4 .\nOptimistic Pessimistic Bandit (OPB) Algorithm. Let {T a (t)} K a=1 and { r a (t), c a (t)} K a=1\nbe the total number of times that arm a has been pulled and the estimated mean reward and cost of arm a up until round t. In each round t \u2208 [T ], OPB relies on the high-probability upper-bounds on the mean reward and cost of the arms, i.e., {u r\na (t), u c a (t)} K a=1 , where u r a (t) = r a (t) + \u03b1 r \u03b2 a (t), u c a (t) = c a (t) + \u03b1 c \u03b2 a (t), \u03b2 a (t) = 2 log(1/\u03b4 )/T a (t)\n, and constants \u03b1 r , \u03b1 c \u2265 1. In order to produce a feasible policy, OPB solves the following linear program (LP) in each round t \u2208 [T ]:\nmax \u03c0\u2208\u2206 K a\u2208A \u03c0a u r a (t), s.t. a\u2208A \u03c0a u c a (t) \u2264 \u03c4.(19)\nAs shown in (19), OPB selects its policy by being optimistic about reward (using an upper-bound for r) and pessimistic about cost (using an upper-bound for c). We report the details of OPB and its pseudo-code (Algorithm 2) in Appendix C.1.\n\nComputational Complexity of OPB. Unlike OPLB, whose optimization problem might be complex, OPB can be implemented extremely efficiently. Lemma 5, whose proof we report in Appendix C.2, show that (19) always has a solution (policy) with support of at most 2. This property allows us to solve (19) in closed form, without a LP solver, and implement OPB quite efficiently.\n\nLemma 5. There exists a policy that solves (19) and has at most 2 non-zero entries.\n\nRegret Analysis of OPB. We prove the following regret-bound for OPB in Appendix C.3.\n\nTheorem 3 (Regret of OPB). Let \u03b4 = 4KT \u03b4 , \u03b1 c = 1, and \u03b1 r = 1 + 2/(\u03c4 \u2212c 1 ). Then, with probability at least 1 \u2212 \u03b4, the regret of OPB satisfies\nR\u03a0(T ) \u2264 1 + 2 \u03c4 \u2212c1 \u00d7 2 2KT log(4KT /\u03b4) + 4 T log(2/\u03b4) log(4KT /\u03b4) .\nThe main component in the proof of Theorem 3 is the following lemma, whose proof is reported in Appendix C.3. This lemma is the analogous to Lemma 4 in the contextual linear bandit case.\n\nLemma 6. If we set the parameters \u03b1 r and \u03b1 c , such that \u03b1 r , \u03b1 c \u2265 1 and \u03b1 c \u2264 (\u03c4 \u2212c 1 )(\u03b1 r \u2212 1), then with high probability, for any t \u2208 \n[T ], we have E a\u223c\u03c0t [u r a (t)] \u2265 E a\u223c\u03c0 * [r a ].(i) < \u03c4 i , \u2200i \u2208 [m], where {\u03c4 i } m i=1\nare the constraint thresholds. Similar to single-constraint OPB, multi-constraint OPB is also computationally efficient. The main reason is that the LP of m-constraint OPB has a solution with at most m + 1 non-zero entries. We obtain a regret bound of O( \u221a KT mini \u03c4i\u2212c1(i) ) for m-constraint OPB in Appendix C.5. Lower-bound. We also prove a mini-max lower-bound for this constrained MAB problem that shows no algorithm can attain a regret better than O(max( \u221a KT , 1 (\u03c4 \u2212c1) 2 )). The formal statement of the lower-bound and the proof are reported in Appendix C.6.\n\n\nExperiments\n\nWe run a set of experiments to show the behavior of OPB and validate our theoretical results. We consider a K = 4-armed bandits in which the reward and cost distributions of the arms are Bernoulli with meansr = (.1, .2, .4, .7) andc = (0, .4, .5, .2). So, the cost of the safe arm isc 1 = 0. In Figures 1 to 3, we gradually reduce the constraint threshold \u03c4 , and as a result the complexity of the problem \u03c4 \u2212c 1 , and show the regret (left) and the cost (middle) and reward (right) evolution of OPB. All the results are averaged over 10 runs and the shade is the \u00b1.5 standard deviation around the regret.\n\nOur results show that the regret of OPB grows as we reduce \u03c4 (left). They also indicate that the algorithm is successful in satisfying the constraint (middle) and reaching the optimal reward/performance (right). In Figure 3, the reason that the cost evolution of OPB is the same as that of the optimal policy (middle) is that in this case, the cost of the best arm (arm 4) is equal to the constraint threshold \u03c4 = .2.   As described in Section 1, our setting is the closest to the one studied by Amani et al. [2019] and Moradipari et al. [2019]. They study a slightly different setting, in which the mean cost of the action that the agent takes should satisfy the constraint, i.e., x t , \u00b5 * \u2264 \u03c4 , not the mean cost of the policy it computes, i.e., x \u03c0t , \u00b5 * \u2264 \u03c4 , as in our case. Clearly, the setting studied in our paper is more relaxed, and thus, is expected to obtain more rewards. Moradipari et al. [2019] propose a TS algorithm for their setting and prove an O(d 3/2 \u221a T /\u03c4 ) regret bound for it. They restrict themselves to linear bandits, i.e., A t = A, \u2200t \u2208 [T ], and the safe action being the origin, i.e., x 0 = 0 and c 0 = 0. This is why c 0 does not appear in their bounds. They consider their action set to be any convex compact subset of R d that contains the origin. Although later in their proofs, to guarantee that their algorithm does not violate the constraint in the first round, they require the action set to also contain the ball with radius \u03c4 /S around the origin. Therefore, our action set is more general than theirs. Moreover, unlike us, their action set does not allows their results to be immediately applicable to MAB. Our regret bound also has a better dependence on d and log T than theirs, similar to the best regret results for UCB vs. TS. However, their algorithm is TS, and thus, is less complex than ours. Although it can be still intractable, even when A is convex. They needed to do several approximations in order to make their algorithm tractable in their experiments.\n\n\nRelated Work\n\nIn Amani et al. [2019], reward and cost have the same unknown parameter \u03b8 * , and the cost is defined as c t = x t B\u03b8 * \u2264 \u03c4 , where B is a known matrix. They derive and analyze an explore-exploit algorithm for this setting. Although our rate is better than theirs, i.e., O(T 2/3 ), our algorithm cannot immediately give a O( \u221a T ) regret for their setting, unless in special cases.\n\n\nConclusions\n\nWe derived a UCB-style algorithm for a new constrained contextual linear bandit setting, in which the goal is to produce a sequence of policies with maximum expected cumulative reward, while each policy has an expected cost below a certain threshold \u03c4 . We proved a T -round regret bound of O( d \u221a T \u03c4 \u2212c0 ) for our algorithm, which shows that the difficulty of the problem depends on the difference between the constraint threshold and the cost of a known feasible action c 0 . We further specialized our results to MAB and proposed and analyzed a computationally efficient algorithm for this setting. We also proved a lower-bound for our constrained bandit problem and provided simulations to validate our theoretical results. A future direction is to use the optimism-pessimism idea behind our algorithm in other constrained bandit settings, including deriving a UCB-style algorithm for the setting studied in Amani et al. Proof. We only prove the statement for the optimistic reward, r \u03c0,t . The proof for the pessimistic cost, c \u03c0,t , is analogous. From the definition of the confidence set C r t (\u03b1 r ) in (7), any vector \u03b8 \u2208 C r t (\u03b1 r ) can be written as \u03b8 t + v, where v satisfying v \u03a3t \u2264 \u03b1 r \u03b2 t (\u03b4, d). Thus, we may write\nr \u03c0,t = max \u03b8\u2208C r t (\u03b1r) E x\u223c\u03c0 [ x, \u03b8 ] = max \u03b8\u2208C r t (\u03b1r) x \u03c0 , \u03b8 = x \u03c0 , \u03b8 t + max v: v \u03a3 t \u2264\u03b1r\u03b2t(\u03b4,d) x \u03c0 , v (a) \u2264 x \u03c0 , \u03b8 t + \u03b1 r \u03b2 t (\u03b4, d) x \u03c0 \u03a3 \u22121 t . (a) By Cauchy-Schwartz, for all v, we have x \u03c0 , v \u2264 x \u03c0 \u03a3 \u22121 t v \u03a3t .\nThe result follows from the condition on v in the maximum, i.e., v \u03a3t \u2264 \u03b1 r \u03b2 t (\u03b4, d).\n\n\nLet us define\nv * := \u03b1r\u03b2t(\u03b4,d)\u03a3 \u22121 t x\u03c0 x\u03c0 \u03a3 \u22121 t . This value of v * is feasible because v * \u03a3t = \u03b1 r \u03b2 t (\u03b4, d) x \u03c0 \u03a3 \u22121 t x \u03c0 \u03a3 \u22121 t \u03a3 t \u03a3 \u22121 t x \u03c0 = \u03b1 r \u03b2 t (\u03b4, d) x \u03c0 \u03a3 \u22121 t x \u03c0 \u03a3 \u22121 t x \u03c0 = \u03b1 r \u03b2 t (\u03b4, d)\n.\n\nWe now show that v * also achieves the upper-bound in the above inequality resulted from Cauchy-Schwartz\nx \u03c0 , v * = \u03b1 r \u03b2 t (\u03b4, d)x \u03c0 \u03a3 \u22121 t x \u03c0 x \u03c0 \u03a3 \u22121 t = \u03b1 r \u03b2 t (\u03b4, d) x \u03c0 \u03a3 \u22121 t .\nThus, v * is the maximizer and we can write\nr \u03c0,t = x \u03c0 , \u03b8 t + x \u03c0 , v * = x \u03c0 , \u03b8 t + \u03b1 r \u03b2 t (\u03b4, d) x \u03c0 \u03a3 \u22121 t ,\nwhich concludes the proof.\n\n\nA.2 Proof of Proposition 2\n\nProof. Recall thatc \u03c0,t = x o \u03c0 ,e0 c0 x0\n+ x o,\u22a5 \u03c0 , t o,\u22a5 \u03c0 + \u03b1 c \u03b2 t (\u03b4, d \u2212 1) x o,\u22a5 \u03c0 (\u03a3 o,\u22a5 t ) \u22121 \u2264 \u03c4 .\nConditioned on the event E as defined in equation 15, it follows that:\n| x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 t \u2212 \u00b5 o,\u22a5 * | \u2264 \u00b5 o,\u22a5 * \u2212 \u00b5 o,\u22a5 t \u03a3 o,\u22a5 t x \u03c0 (\u03a3 o,\u22a5 t ) \u22121 \u2264 x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 t \u2212 \u00b5 o,\u22a5 * \u03b2 t (\u03b4, d \u2212 1) x \u03c0 (\u03a3 o,\u22a5 t ) \u22121\nAnd therefore:\n0 \u2264 x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 t \u2212 \u00b5 o,\u22a5 * + \u03b2 t (\u03b4, d \u2212 1) x \u03c0 (\u03a3 o,\u22a5 t ) \u22121(20)\nObserve that:\nc \u03c0 = x o \u03c0 , e 0 c 0 x 0 + x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 * \u2264 x o \u03c0 , e 0 c 0 x 0 + x o,\u22a5 \u03c0 , \u00b5 o,\u22a5 t + \u03b1 c \u03b2 t (\u03b4, d \u2212 1) x o,\u22a5 \u03c0 (\u03a3 o,\u22a5 t ) \u22121 I(21)\nThe last inequality holds by adding Inequality 20 to Inequality 21. Since by assumption for all \u03c0 \u2208 \u03a0 t term I \u2264 \u03c4 , we obtain that c \u03c0 \u2264 \u03c4 . The result follows.\n\n\nB Proofs of Section 4 B.1 Proof of Lemma 2\n\nWe first state the following proposition that is used in the proof of Lemma 2. This proposition is a direct consequence of Eq. 20.9 and Lemma 19.4 in Lattimore and Szepesv\u00e1ri [2019]. Similar result has also been reported in the appendix of Amani et al. [2019].\n\nProposition 3. For any sequence of actions (x 1 , . . . , x t ), let \u03a3 t be its corresponding Gram matrix defined by (4) with \u03bb \u2265 1. Then, for all t \u2208 [T ], we have\nT s=1 xs \u03a3 \u22121 s \u2264 2T d log 1 + T L 2 \u03bb .\nWe now state the proof of Lemma 2.\n\nProof of Lemma 2. We prove this lemma through the following sequence of inequalities:\nT t=1 xt, \u03b8t \u2212 xt, \u03b8 * (a) \u2264 T t=1 xt \u03a3 \u22121 t \u03b8t \u2212 \u03b8 * \u03a3 t (b) \u2264 T t=1 (1 + \u03b1r)\u03b2t(\u03b4, d) xt \u03a3 \u22121 t (c) \u2264 (1 + \u03b1r)\u03b2T (\u03b4, d) T t=1 xt \u03a3 \u22121 t (d) \u2264 (1 + \u03b1r)\u03b2T (\u03b4, d) 2T d log 1 + T L 2 \u03bb\n(a) This is by Cauchy-Schwartz.\n\n(b) This follows from the fact that \u03b8 t \u2208 C r t (\u03b1 r ) and we are on event E.\n(c) This is because \u03b2 t (\u03b4, d) is an increasing function of t, i.e., \u03b2 T (\u03b4, d) \u2265 \u03b2 t (\u03b4, d), \u2200t \u2208 [T ].\n(d) This is a direct result of Proposition 3.\n\n\nB.2 Proof of Lemma 3\n\nProof. In order to prove the desired result it is enough to show that:\nx o,\u22a5 \u03c0 \u03a3 o,\u22a5 t \u2020 x o,\u22a5 \u03c0 \u2264 x \u03c0 \u03a3 \u22121 t x \u03c0\nw.l.o.g. we can assume x o = e 1 , the first basis vector. Notice that in this case \u03a3 o,\u22a5 t can be thought of as a submatrix of \u03a3 t such that \u03a3 t [2 :, 2 :] = \u03a3 o,\u22a5 t , where \u03a3 t [2 :, 2 :] denotes the submatrix with row and column indices from 2 onwards.\n\nUsing the following formula for the inverse of a psd symmetric matrix:\nZ \u03b4 \u03b4 A = 1 D \u2212 A \u22121 \u03b4 D \u2212 \u03b4 A \u22121 D A \u22121 + A 1 \u03b4\u03b4 A \u22121 D Where D = z \u2212 \u03b4 A \u22121 \u03b4. In our case D = \u03a3 t [1, 1] \u2212 \u03a3 t [2 : d] \u03a3 o,\u22a5 t \u22121 \u03a3 t [2 : d] \u2208 R.\nObserve that since \u03a3 t is PSD, D \u2265 0. Therefore:\n\u03a3 \u22121 t = \uf8ee \uf8f0 1/D \u2212 (\u03a3 o,\u22a5 t ) \u22121 \u03a3t[2,:d] D \u2212 \u03a3 t [2:d](\u03a3 o,\u22a5 t ) \u22121 D \u03a3 o,\u22a5 t \u22121 + (\u03a3 o,\u22a5 t ) \u22121 \u03a3t[2:d]\u03a3t[2:d](\u03a3 o,\u22a5 t ) \u22121 D \uf8f9 \uf8fb\nThen:\nx \u03c0 \u03a3 \u22121 t \u22121 x \u03c0 = x \u03c0 (1) 2 \u2212 2x \u03c0 (1)\u03a3 t [2 : d] \u03a3 o,\u22a5 t \u22121 x \u03c0 [2 : d] D + x \u03c0 [2 : d] \u03a3 o,\u22a5 t \u22121 \u03a3 t [2 : d]\u03a3 t [2 : d] \u03a3 o,\u22a5 t \u22121 x \u03c0 [2 : d] D + x \u03c0 [2 : d] \u03a3 o,\u22a5 t \u22121 x \u03c0 [2 : d] \u2265 x \u03c0 [2 : d] \u03a3 o,\u22a5 t \u22121 x \u03c0 [2 : d]\nThe result follows by noting that x \u03c0 [2 : d] = x o,\u22a5 \u03c0 .\n\n\nB.3 Proof of Lemma 4\n\nProof. For any policy \u03c0, we have\nr \u03c0,t = max \u03b8\u2208C r t (\u03b1r) x \u03c0 , \u03b8 \u2265 x \u03c0 , \u03b8 * = r \u03c0 .(22)\nIf \u03c0 * t \u2208 \u03a0 t , then by the definition of \u03c0 t (Line 4 of Algorithm 1), we have r \u03c0t,t \u2265 r \u03c0 * t ,t .\n\nCombining (22) and (23), we may conclude that r \u03c0t,t \u2265 r \u03c0 * t as desired. We now focus on the case that \u03c0 * t \u2208 \u03a0 t , i.e.,\nc \u03c0 * t ,t = x o \u03c0 * t , e 0 c 0 x 0 + x o,\u22a5 \u03c0 * t , \u00b5 o,\u22a5 t + \u03b1 c \u03b2 t (\u03b4, d \u2212 1) x o,\u22a5 \u03c0 * t (\u03a3 o,\u22a5 t ) \u22121 > \u03c4.\nWe define a mixture policy \u03c0 t = \u03b7 t \u03c0 * t + (1 \u2212 \u03b7 t )\u03c0 0 , where \u03c0 0 is the policy that always selects the safe action x 0 and \u03b7 t \u2208 [0, 1] is the maximum value of \u03b7 such that \u03b7\u03c0 * t + (1 \u2212 \u03b7)\u03c0 0 \u2208 \u03a0 t . Conceptually, \u03b7 t shows how close is the optimal policy \u03c0 * t to the set of safe policies \u03a0 t . By the definition of \u03c0 t , we have\nx o \u03c0t = \u03b7 t x o \u03c0 *\nIf the cost of the safe arm c 0 is unknown, we start by taking the safe action x 0 for T 0 rounds to produce first an empirical mean estimator for\u0109 9 . Notice that for all \u03b4 \u2208 (0, 1),\u0109 0 satisfies:\nP \uf8eb \uf8ed\u0109 0 \u2264 c 0 \u2212 2 log (1/\u03b4) T 0 \uf8f6 \uf8f8 \u2264 \u03b4(27)\nLetc 0 =\u0109 0 + 2 log(1/\u03b4)\n\n\nT0\n\n. By inequality 27, it follows that with probability at least 1 \u2212 \u03b4:\nc 0 \u2265 c 0\nWe select T 0 in an adaptive way. In other words, we do the following:\n\nLet \u03b4 = 1 T 2 . And let\u0109 0 (t) be the sample mean estimator of c 0 , when using only t samples. Similarly definec 0 (t) =\u0109 0 (t) + 2 log(1/\u03b4) t Let's condition on the event E that for all t \u2208 [T ]:\n|\u0109 0 (t) \u2212 c 0 | \u2264 2 log(1/\u03b4) t By assumption P(E) \u2265 1 \u2212 T 2\u03b4 = 1 \u2212 2 T . Let T 0 be the first time thatc 0 (T 0 ) + 2 2 log(1/\u03b4) T0 \u2264 \u03c4 .\nNotice that in this case and conditioned on E and therefore onc 0 (T 0 ) \u2265 c 0 :\n2 log(1/\u03b4) T 0 \u2264 \u03c4 \u2212 c 0 2 i.e. T 0 \u2265 8 log(1/\u03b4) (\u03c4 \u2212 c 0 ) 2\nIn other words, this test does not stop until T 0 \u2265 8 log(1/\u03b4) (\u03c4 \u2212c0) 2 . Now we see it won't take much longer than that to stop:\n\nConversely, let T 0 \u2265 32 log(1/\u03b4) (\u03c4 \u2212c0) 2 . For any such T 0 we observe that by conditioning on E:\nc 0 (T 0 ) + 2 2 log(1/\u03b4) T 0 \u2264 c 0 + 4 2 log(1/\u03b4) T 0 \u2264 \u03c4\nThus conditioned on E, we conclude 8 log(1/\u03b4) (\u03c4 \u2212c0) 2 \u2264 T 0 \u2264 32 log(1/\u03b4) (\u03c4 \u2212c0) 2 . Then,\nTherefore\u03b4 c = 8 log(1/\u03b4) T0\nwould serve as a conservative estimator for \u03c4 \u2212c0 2 satisfying:\n\u03c4 \u2212 c 0 2 \u2264\u03b4 c \u2264 \u03c4 \u2212 c 0\nWe proceed by warm starting our estimators for \u03b8 * and \u00b5 * using the data collected by playing x 0 . However, instead of estimating \u00b5 o,\u22a5 * , we build an estimator for \u00b5 * over all its directions, including e 0 , similar to what OPLB does for \u03b8 * . We then set \u03b1r \u03b1c = 1/\u03b4 c and run Algorithm 1 for rounds t > T 0 . Since the scaling of \u03b1 r w.r.t. \u03b1 c is optimal up to constants, the same arguments hold.\n\n\nC Constrained Multi-Armed Bandits\n\n\nC.1 Optimism Pessimism\n\nHere we reproduce the full pseudo-code for OPB:\n\nAlgorithm 2 Optimism-Pessimism Input: Number of arms K, constants \u03b1 r , \u03b1 c \u2265 1. for t = 1, . . . , T do 1. Compute estimates {u r a (t)} a\u2208A , {u c a (t)} a\u2208A . 2. Form the approximate LP (19) using these estimates. 3. Find policy \u03c0 t by solving (19). 4. Play arm a \u223c \u03c0 t Similar to the case of OPLB, we define \u03a0 t = {\u03c0 \u2208 \u2206 A : a\u2208A \u03c0 a u c a (t) \u2264 \u03c4 }. We also define \u03b2 a (0) = 0 for all a \u2208 A.\n\n\nC.2 The LP Structure\n\nThe main purpose of this section is to prove the optimal solutions of the linear program from (19) are supported on a set of size at most 2. This structural result will prove important to develop simple efficient algorithms to solve for solving it. Let's recall the form of the Linear program in 19 is:\nmax \u03c0\u2208\u2206 K a\u2208A \u03c0 a u r a (t) s.t. a\u2208A \u03c0 a u c a (t) \u2264 \u03c4\nLet's start by observing that in the case K = 2 with A = {a 1 , a 2 } and u c a1 (t) < \u03c4 < u c a2 (t), the optimal policy \u03c0 * is a mixture policy satisfying:\n\u03c0 * a1 = u c a2 (t) \u2212 \u03c4 u c a2 (t) \u2212 u c a1 (t) \u03c0 * a2 = \u03c4 \u2212 u c a1 (t) u c a2 (t) \u2212 u c a1 (t)(28)\nThe main result in this section is the following Lemma: Lemma 7 (\u03c0 * support). If (19) is feasible, there exists an optimal solution with at most 2 non-zero entries.\n\nProof. We start by inspecting the dual problem of (19):\nmin \u03bb\u22650 max a \u03bb(\u03c4 \u2212 u c a (t)) + u r a (t)(D)\nThis formulation is easily interpretable. The quantity \u03c4 \u2212 u c a (t) measures the feasibility gap of arm a, while u r a (t) introduces a dependency on the reward signal. Let \u03bb * be the optimal value of the dual variable \u03bb. Define A * \u2286 A as A * = arg max a \u03bb * (\u03c4 \u2212 u c a (t)) + u r a (t). By complementary slackness the set of nonzero entries of \u03c0 * must be a subset of A * .\n\nIf |A * | = 1, complementary slackness immediately implies the desired result. If a 1 , a 2 are two elements of A * , it is easy to see that:\n\nu r a1 (t) \u2212 \u03bb * u c a1 (t) = u r a2 (t) \u2212 \u03bb * u c a2 (t), and thus,\n\u03bb * = u r a2 (t) \u2212 u r a1 (t) u c a2 (t) \u2212 u c a1 (t)(29)\nIf \u03bb * = 0, the optimal primal value is achieved by concentrating all mass on any of the arms in A * . Otherwise, plugging 29 back into the objective of (D) and rearranging the terms, we obtain\ns (D) = \u03bb * (\u03c4 \u2212 u c a1 (t)) + u r a1 (t) = u r a1 (t) \u03c4 \u2212 u c a1 (t) u c a2 (t) \u2212 u c a1 (t) + u r a2 (t) u c a2 (t) \u2212 \u03c4 u c a2 (t) \u2212 u c a1 (t)\n.\n\nIf u c a2 (t) \u2265 \u03c4 \u2265 u c a1 (t), we obtain a feasible value for the primal variable \u03c0 t) and zero for all other a \u2208 A\\{a 1 , a 2 }. Since we have assumed (19) to be feasible there must be either one arm a * \u2208 A * satisfying a * = arg max a\u2208A * u r a (t) and u c a * (t) \u2264 \u03c4 or two such arms a 1 and a 2 in A * that satisfy u c a2 (t) \u2265 \u03c4 \u2265 u c a1 (t), since otherwise it would be impossible to produce a feasible primal solution without having any of its supporting arms a satisfying u c a (t) \u2264 \u03c4 , there must exist an arm a \u2208 A * with u c a (t) < \u03c4 . This completes the proof.\n* a1 = \u03c4 \u2212u c a 1 (t) u c a 2 (t)\u2212u c a 1 (t) , \u03c0 * a2 = u c a 2 (t)\u2212\u03c4 u c a 2 (t)\u2212u c a 1 (\nFrom the proof of Lemma 5 we can conclude the optimal policy is either a delta mass centered at the arm with the largest reward -whenever this arm is feasible -or it is a strict mixture supported on two arms.\n\nA further consequence of Lemma 7 is that it is possible to find the optimal solution \u03c0 * to problem 19 by simply enumerating all pairs of arms (a i , a j ) and all singletons, compute their optimal policies (if feasible) using Equation 28 and their values and selecting the feasible pair (or singleton) achieving the largest value. More sophisticated methods can be developed by taking into account elimination strategies to prune out arms that can be determined in advance not to be optimal nor to belong to an optimal pair. Overall this method is more efficient than running a linear programming solver on (19).\n\nIf we had instead m constraints, a similar statement to Lemma 5 holds, namely it is possible to show the optimal policy will have support of size at most m + 1. The proof is left as an exercise for the reader.\n\n\nC.3 Regret analysis\n\nIn order to show a regret bound for Algorithm 2, we start with the following regret decomposition:\nR \u03a0 (T ) = T t=1 E a\u223c\u03c0 * [r a ] \u2212 E a\u223c\u03c0t [r a ] = T t=1 E a\u223c\u03c0 * [r a ] \u2212 E a\u223c\u03c0t [u r a (t)] (i) + T t=1 E a\u223c\u03c0t [u r a (t)] \u2212 E a\u223c\u03c0t [r a ](ii)\n.\n\nIn order to bound R \u03a0 (T ), we independently bound terms (i) and (ii).\n\nWe start by bounding term (i). We proceed by first proving an Lemma 6, the equivalent version of Lemma 4 for the multi armed bandit problem.\n\n\nC.4 Proof of Lemma 6\n\nProof. Throughout this proof we denote as \u03c0 0 to the delta function over the safe arm 1. We start by noting that under E, and because \u03b1 r , \u03b1 c \u2265 1, then:\n\n(\u03b1 r \u22121)\u03b2 a (t) \u2264 \u03be r a (t) \u2264 (\u03b1 r +1)\u03b2 a (t) \u2200a and (\u03b1 c \u22121)\u03b2 a (t) \u2264 \u03be c a (t) \u2264 (\u03b1 c +1)\u03b2 a (t) \u2200a = 0.\n\n(30) If \u03c0 * \u2208 \u03a0 t , it immediately follows that:\nE a\u223c\u03c0 * [r a ] \u2264 E a\u223c\u03c0 * [u r a (t)] \u2264 E a\u223c\u03c0t [u r a (t)] .(31)\nLet's now assume \u03c0 * \u2208 \u03a0 t , i.e., E a\u223c\u03c0 * [u c a (t)] > \u03c4 . Let \u03c0 * = \u03c1 * \u03c0 * + (1 \u2212 \u03c1)\u03c0 0 with\u03c0 * \u2208 \u2206 K [2 : K] 5 .\n\nConsider a mixture policy \u03c0 t = \u03b3 t \u03c0 * + (1 \u2212 \u03b3 t )\u03c0 0 = \u03b3 t \u03c1 * \u03c0 * + (1 \u2212 \u03b3 t \u03c1 * )\u03c0 0 , where \u03b3 t is the maximum \u03b3 t \u2208 [0, 1] such that \u03c0 t \u2208 \u03a0 t . It can be easily established that\n\u03b3 t = \u03c4 \u2212c 1 \u03c1 * E a\u223c\u03c0 * [u c a (t)] \u2212 \u03c1 * c 1 = \u03c4 \u2212c 1 E a\u223c\u03c0 * [\u03c1 * (c a + \u03be c a (t))] \u2212 \u03c1 * c 1 (i) \u2265 \u03c4 \u2212c 1 \u03c4 \u2212c 1 + \u03c1 * (1 + \u03b1 c )E a\u223c\u03c0 * [\u03b2 a (t)]\n.\n\n(i) is a consequence of (30) and of the observation that since \u03c0 * is feasible \u03c1\n* E a\u223c\u03c0 * [c a ]+(1\u2212\u03c1 * )c 1 \u2264 \u03c4 . Since \u03c0 t \u2208 \u03a0 t , we have E a\u223c\u03c0t [u r a (t)] \u2265 \u03b3 t E a\u223c\u03c0 * [u r a (t)] + (1 \u2212 \u03b3 t )u r 0 (t) E a\u223c \u03c0 t [u r a (t)] (ii) \u2265 \u03c4 \u2212c 1 \u03c4 \u2212c 1 + \u03c1 * (1 + \u03b1 c )E a\u223c\u03c0 * [\u03b2 a (t)] \u00d7 E a\u223c\u03c0 * [u r a (t)] = \u03c4 \u2212c 1 \u03c4 \u2212c 1 + \u03c1 * (1 + \u03b1 c )E a\u223c\u03c0 * [\u03b2 a (t)] \u00d7 E a\u223c\u03c0 * [r a ] + E a\u223c\u03c0 * [\u03be r a (t)] (iii) \u2265 \u03c4 \u2212c 1 \u03c4 \u2212c 1 + \u03c1 * (1 + \u03b1 c )E a\u223c\u03c0 * [\u03b2 a (t)] \u00d7 E a\u223c\u03c0 * [r a ] + (\u03b1 r \u2212 1)E a\u223c\u03c0 * [\u03b2 a (t)] (iv) \u2265 \u03c4 \u2212c 1 \u03c4 \u2212c 1 + (1 + \u03b1 c )E a\u223c\u03c0 * [\u03b2 a (t)] \u00d7 E a\u223c\u03c0 * [r a ] + (\u03b1 r \u2212 1)E a\u223c\u03c0 * [\u03b2 a (t)] C0 . (ii) holds because u r 0 (t) \u2265 0. (iii) is a consequence of (30) and (iv) follows because E a\u223c\u03c0 * [\u03b2 a (t)] = \u03c1 * E a\u223c\u03c0 * [\u03b2 a (t)] + (1 \u2212 \u03c1 * )\u03b2 0 (t) \u2265 \u03c1 * E a\u223c\u03c0 * [\u03b2 a (t)]\nsince \u03b2 a (t) \u2265 0 for all a and t.\n\nLet C 1 = E a\u223c\u03c0 * [\u03b2 a (t)]. The following holds:\nC 0 = \u03c4 \u2212c 1 \u03c4 \u2212c 1 + (1 + \u03b1 c )C 1 \u00d7 E a\u223c\u03c0 * [r a ] + (\u03b1 r \u2212 1)C 1 . Note that C 0 \u2265 E a\u223c\u03c0 * [r a ] iff: (\u03c4 \u2212c 1 )E a\u223c\u03c0 * [r a ] + (\u03c4 \u2212c 1 )(\u03b1 r \u2212 1)C 1 \u2265 (\u03c4 \u2212c 1 )E a\u223c\u03c0 * [r a ] + (1 + \u03b1 c )C 1 E a\u223c\u03c0 * [r a ] ,\nwhich holds iff:\n(\u03c4 \u2212c 1 )(\u03b1 r \u2212 1)C 1 \u2265 (1 + \u03b1 c )C 1 E a\u223c\u03c0 * [r a ].\nSince E a\u223c\u03c0 * [r a ] \u2264 1, this holds if 1 + \u03b1 c \u2264 (\u03c4 \u2212c 1 )(\u03b1 r \u2212 1).\n\nProposition 4. If \u03b4 = 4KT for \u2208 (0, 1), \u03b1 r , \u03b1 c \u2265 1 with \u03b1 c \u2264 \u03c4 (\u03b1 r \u2212 1), then with probability at least 1 \u2212 2 , we have\nT t=1 E a\u223c\u03c0 * [r a ] \u2212 E a\u223c\u03c0t [u r a (t)] \u2264 0\nProof. A simple union bound implies that P(E) \u2265 1 \u2212 2 . Combining this observation with Lemma 6 yields the result.\n\nTerm (ii) can be bound using the confidence intervals radii: Proposition 5. If \u03b4 = 4KT for an \u2208 (0, 1), then with probability at least 1 \u2212 2 , we have T t=1 E a\u223c\u03c0t [u r a (t)] \u2212 E a\u223c\u03c0t [r a ] \u2264 (\u03b1 r + 1) 2 2T K log(1/\u03b4) + 4 T log(2/ ) log(1/\u03b4)\n\nProof. Under these conditions P(E) \u2265 1 \u2212 2 . Recall u r a (t) = r a (t) + \u03b1 r \u03b2 a (t) and that conditional on E,r a \u2208 [ r a (t) \u2212 \u03b2 a (t), r a (t) + \u03b2 a (t)] for all t \u2208 [T ] and a \u2208 A. Thus, for all t, we have\nE a\u223c\u03c0t [u r a (t)] \u2212 E a\u223c\u03c0t [r a ] \u2264 (\u03b1 r + 1)E a\u223c\u03c0t [\u03b2 a (t)]\n. Let F t\u22121 be the sigma algebra defined up to the choice of \u03c0 t and a t be a random variable distributed as \u03c0 t | F t\u22121 and conditionally independent from a t , i.e., a t \u22a5 a t | F t\u22121 . Note that by definition the following equality holds:\nE a\u223c\u03c0t [\u03b2 a (t)] = E a t \u223c\u03c0t [\u03b2 a (t) | F t\u22121 ].\n\nConsider the following random variables\nA t = E a t \u223c\u03c0t [\u03b2 a t (t) | F t\u22121 ] \u2212 \u03b2 at (t). Note that M t = t i=1 A i is a martingale.\nSince |A t | \u2264 2 2 log(1/\u03b4), a simple application of Azuma-Hoeffding 6 implies:\nP \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed T t=1 E a\u223c\u03c0t [\u03b2 a (t)] \u2265 T t=1 \u03b2 at (t) + 4 T log(2/ ) log(1/\u03b4) E c A \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \u2264 /2.\nWe can now upper-bound T t=1 \u03b2 at (t). Note that T t=1 \u03b2 at (t) = a\u2208A T t=1 1{a t = a}\u03b2 a (t). We start by bounding for an action a \u2208 A: T t=1 1{a t = a}\u03b2 a (t) = 2 log(1/\u03b4)\nTa(T ) t=1 1 \u221a t \u2264 2 2T a (T ) log(1/\u03b4).\nSince a\u2208A T a (T ) = T and by concavity of \u221a \u00b7, we have a\u2208A 2 2T a (T ) log(1/\u03b4) \u2264 2 2T K log(1/\u03b4).\n\nConditioning on the event E \u2229 E A whose probability satisfies P(E \u2229 E A ) \u2265 1 \u2212 yields the result.\n\nWe can combine these two results into our main theorem: Theorem 4 (Main Theorem). If \u2208 (0, 1), \u03b1 c = 1 and \u03b1 r = 2 \u03c4 \u2212c1 + 1, then with probability at least 1 \u2212 , Algorithm 2 satisfies the following regret guarantee:\nR \u03a0 (T ) \u2264 2 \u03c4 \u2212c 1 + 1 2 2T K log(4KT / ) + 4 T log(2/ ) log(4KT / )\nProof. This result is a direct consequence of Propositions 4 and 5 by setting \u03b4 = 4KT .\n\n\nC.5 Multiple constraints\n\nWe consider the problem where the learner must satisfy M constraints with threshold values \u03c4 1 , \u00b7 \u00b7 \u00b7 , \u03c4 M . Borrowing from the notation in the previous sections, we denote by as {r a } a\u2208A the mean reward signals and {c (i) a } the mean cost signals for i = 1, \u00b7 \u00b7 \u00b7 , M . The full information optimal policy can be obtained by solving the following linear program:\nmax \u03c0\u2208\u2206 K a\u2208A \u03c0 ara , (P-M) s.t. a\u2208A \u03c0 ac (i) a \u2264 \u03c4 i for i = 1, \u00b7 \u00b7 \u00b7 , M.\nIn order to ensure the learner's ability to produce a feasible policy at all times, we make the following assumption:\n\nAssumption 6. The learner has knowledge ofc (i) 1 < \u03c4 i for all i = 1, \u00b7 \u00b7 \u00b7 , M . 6 We use the following version of Azuma-Hoeffding: if Xn, n \u2265 1 is a martingale such that |Xi\u2212Xi\u22121| \u2264 di, for 1 \u2264 i \u2264 n, then for every n \u2265 1, we have P(Xn > r) \u2264 exp \u2212\nr 2 2 n i=1 d 2 i .\nWe denote by { r a } a\u2208A and { c (i) a } a\u2208A for i = 1, \u00b7 \u00b7 \u00b7 , M the empirical means of the reward and cost signals. We call {u r a (t)} a\u2208A to the upper confidence bounds for our reward signal and {u c a (t, i)} a\u2208A for i = 1, \u00b7 \u00b7 \u00b7 , M the costs' upper confidence bounds: u r a (t) = r a (t) + \u03b1 r \u03b2 a (t), u c a (t, i) = c (i) a (t) + \u03b1 c \u03b2 a (t), where \u03b2 a (t) = 2 log(1/\u03b4)/T a (t), \u03b4 \u2208 (0, 1) as before. A straightforward extension of Algorithm 2 considers instead the following M \u2212constraints LP:\nmax \u03c0\u2208\u2206 K a\u2208A \u03c0 a u r a (t) ( P \u2212 M ) s.t.\na\u2208A \u03c0 a u c a (t, i) \u2264 \u03c4 i , for i = 1, \u00b7 \u00b7 \u00b7 , M.\n\nWe now generalize Lemma 6:\nLemma 8. Let \u03b1 r , \u03b1 c \u2265 1 satisfying \u03b1 c \u2264 min i (\u03c4 i \u2212c (i) 1 )(\u03b1 r \u2212 1). Conditioning on E a (t) ensures that with probability 1 \u2212 \u03b4: E a\u223c\u03c0t [u r a (t)] \u2265 E a\u223c\u03c0 * [r a ] .\nProof. The same argument as in the proof of Lemma 6 follows through, the main ingredient is to realize that \u03b3 t satisfies the sequence of inequalities in the lemma with \u03c4 \u2212c 1 substituted by\nmin \u03c4 i \u2212c (i) 1 .\nThe following result follows: Theorem 5 (Multiple Constraints Main Theorem). If \u2208 (0, 1), \u03b1 c = 1 and \u03b1 r = 2 mini \u03c4i\u2212c (i) 1 + 1, then with probability at least 1 \u2212 , Algorithm 2 satisfies the following regret guarantee:\nR \u03a0 (T ) \u2264 2 min i \u03c4 i \u2212c (i) 1 + 1 2 2T K log(4KT / ) + 4 T log(2/ ) log(4KT / )\nProof. The proof follows the exact same argument we used for the proof of Theorem 3 substituting \u03c4 \u2212c 1 by min i \u03c4 i \u2212c (i) 1 .\n\n\nC.6 Lower bound\n\nWe start by proving a generalized version of the divergence decomposition lemma for bandits. Lemma 9. [Divergence decomposition for constrained multi armed bandits] Let \u03bd = ((P 1 , Q 1 ), \u00b7 \u00b7 \u00b7 , (P K , Q K )) be the reward and constraint distributions associated with one instance of the single constraint multi-armed bandit, and let \u03bd = ((P 1 , Q 1 ), \u00b7 \u00b7 \u00b7 , (P K , Q K )) be the reward and constraint distributions associated with another constrained bandit instance. Fix some algorithm A and let P \u03bd = P \u03bd A and P \u03bd = P \u03bd A be the probability measures on the cannonical bandit model (See section 4.6 of Lattimore and Szepesv\u00e1ri [2019]) induced by the T round interconnection of A and \u03bd (respectively A and \u03bd ). Then:\nKL(P \u03bd , P \u03bd ) = K a=1 E \u03bd [T a (T )]KL((P a , Q a ), (P a , Q a ))\nWhere T a (T ) denotes the number of times arm a was pulled until by A and up to time T .\n\nProof. The same proof as in Lemma 15.1 from Lattimore and Szepesv\u00e1ri [2019] applies in this case.\n\nThe following two lemmas will prove useful as well: Lemma 10. [Gaussian Divergence ] The divergence between two multivariate normal distributions and means \u00b5 1 , \u00b5 2 \u2208 R d with spherical identity covariance I d equals:\nKL(N (\u00b5 1 , I d ), N (\u00b5 2 , I d )) = \u00b5 1 \u2212 \u00b5 2 2 2\nDefine the binary relative entropy to be:\nd(x, y) = x log( x y ) + (1 \u2212 x) log( 1 \u2212 x 1 \u2212 y )\nand satisfies: d(x, y) \u2265 (1/2) log(1/4y) (32) for x \u2208 [1/2, 1] and y \u2208 (0, 1). Adapted from Kaufmann et al. [2016], Lemma 1. Lemma 11. Let \u03bd, \u03bd be two constrained bandit models with K arms. Borrow the setup, definitions and notations of Lemma 9, then for any measurable event B \u2208 F T : KL(P \u03bd , P \u03bd ) = K a=1 E \u03bd [T a (T )]KL((P a , Q a ), (P a , Q a )) \u2265 d (P \u03bd \n(B), P \u03bd (B))(33)\nWe now present a worst-case lower bound for the constrained multi armed bandit problem. We restrict ourselves to Gaussian instances with mean reward and cost vectorsr,c \u2208 [0, 1] K . Let A be an algorithm for policy selection in the constrained MAB problem. For the purpose of this section we denote as R \u03a0 (T, A,r,c) as the constrained regret of algorithm A in the Gaussian instance N (r, I), N (c, I). The following theorem holds:\n\nTheorem 6. Let \u03c4,c 1 \u2208 (0, 1), K \u2265 4, and B := max 1 27 (k \u2212 1)T , 1 6(\u03c4 \u2212c1) 2 and assume 7 T \u2265 max(K \u2212 1, 24eB) and let \u03c4 be the maximum allowed cost. Then for any algorithm A there is a pair of mean vectorsr,c \u2208 [0, 1] K such that: (k \u2212 1)T , 1 6(\u03c4 \u2212c1) 2 = 1 6(\u03c4 \u2212c1) 2 . Pick any algorithm. We want to show that the algorithm's regret on some environment is as large as B. If there was an instancer,c such that R \u03a0 (T, A,r,c) > B there would be nothing to be proven. Hence without loss of generality, we can assume that the algorithm satisfies R \u03a0 (T, A,r,c) \u2264 B for allr,c \u2208 [0, 1] K and having unit variance Gaussian rewards.\n\nLet c \u2208 (0, 1) with c = \u03c4 \u2212c 1 . For the reader's convenience we will use the notation \u2206 = 1/2. By treating the rewards in a symbolic way it is easier to understand the logic of the proof argument. Let's consider the following constrained bandit instance inducing measure \u03bd:\nc 1 = (\u03c4 \u2212 c,\n\u03c4 + 2c, \u03c4 \u2212 c, \u03c4 + 2c, \u00b7 \u00b7 \u00b7 , \u03c4 + 2c) r 1 = (\u2206, 8\u2206, 0, 4\u2206, \u00b7 \u00b7 \u00b7 , 4\u2206)\n\nNotice that the optimal policy equals a mixture between arm 1 and 2, where arm 1 is chosen with probability 2/3 and arm 2 with probability 1/3. The value of this optimal policy equals 10/3\u2206.\n\nRecall we use the notationT j (t) denote the total amount of probability mass that A allocated to arm j up to time t. Notice that the expected reward of all feasible policies that do not have arm 1 in their support have a gap (w.r.t the optimal feasible policy's expected reward) of at least 2\u2206 3 . Since by assumption, A satisfies R \u03a0 (T, A,r 1 ,c 1 ) \u2264 B:\n\nB \u2265 R \u03a0 (T, A,r 1 ,c 1 ) \u2265 2\u2206 3 2 3 T \u2212 1 2 T P T 1 (T ) < T 2 = \u2206 9 T P T 1 (T ) < T 2\n\nAnd therefore:\nP T 1 (T ) \u2265 T 2 = 1 \u2212 P T 1 (T ) < T 2 \u2265 1 \u2212 9B \u2206T \u2265 1/2\nThe last inequality follows from the assumption T \u2265 max(K \u2212 1, 24eB).\n\nLet's now consider the following constrained bandit instance inducing measure \u03bd :\n\nc 2 = (\u03c4 \u2212 c, \u03c4 + 2c, 0, \u03c4 \u2212 c, \u00b7 \u00b7 \u00b7 , \u03c4 + 2c) r 2 = (\u2206, 8\u2206, 0, 4\u2206, \u00b7 \u00b7 \u00b7 , 4\u2206)\n\nIn this instance the optimal policy is to play arm 4 deterministically, which gets a reward of 4\u2206.\n\nNotice that the expected reward of any feasible policy that does not contain arm 4 in its support has a gap (w.r.t. the optimal feasible policy's expected reward) of at least 2\u2206 3 . Since by assumption, A satisfies R \u03a0 (T, A,r 2 ,c 2 ) \u2264 B:\nB \u2265 R \u03a0 (T, A,r 2 ,c 2 ) \u2265 2\u2206 3 1 2 T P T 1 (T ) \u2265 T 2 = \u2206 3 T P T 1 (T ) \u2265 T 2\nAnd therefore:\nP T 1 (T ) \u2265 T 2 \u2264 3B \u2206T \u2264 1 4e\nThe last inequality follows from the assumption T \u2265 max(K \u2212 1, 24eB). As a consequence of inequality 32, Lemma 11 and 10:\nE \u03bd [T 4 (T )]KL( \u03c4 + 2c 4\u2206 , I d ), N ( \u03c4 \u2212 c 4\u2206 , I d )) = E \u03bd [T 4 (T )]2c 2 \u2265 1 2\nAnd therefore we can conclude:\nE[T 4 (T )] = E[T 4 (T )] \u2265 1 4c 2(34)\nSince in \u03bd, any feasible policy with support in arm 4 and no support in arm 2 has a suboptimality gap of 4/3\u2206, we conclude the regret R \u03a0 (T, A,r 2 ,c 2 ) must satisfy:\n\nR \u03a0 (T, A,r 2 ,c 2 ) \u2265 \u2206 3c 2\n\nSince \u2206 = 1 2 and noting that in this case \u2206 3c 2 = B. The result follows.\n\n\nOur contextual linear bandit formulation is general enough to include MAB. The regret analysis of OPLB (Theorem 2) yields a regret bound of orderO( K \u221a T \u03c4 \u2212c1 ) for MAB. However, our OPB regret bound in Theorem 3 is of order O( \u221a KT \u03c4 \u2212c1 ), which shows a \u221a K improvement over simply casting MAB as an instance of contextual linear bandit and using the regret bound of OPLB.Extension to m Constraints. In this case, the agent receives m cost signals after pulling each arm. The cost vector of the safe arm c 1 satisfies c 1\n\nFigure 1 :\n1Constraint Threshold \u03c4 = 1.\n\nFigure 2 :\n2Constraint Threshold \u03c4 = 0.5.\n\nFigure 3 :\n3Constraint Threshold \u03c4 = 0.2. OPB. Bernoulli arms.r = (.1, .2, .4, .7),c = (0, .4, .5, .2),c1 = 0.\n\n\n[2019] and Moradipari et al. [2019]. D. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen. A tutorial on Thompson sampling. Foundations and Trends in Machine Learning, 11(1):1-96, 2018. W. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285-294, 1933. S. Villar, J. Bowden, and J. Wason. Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges. Statistical Science, 30(2):199-215, 2015. R. Washburn. Application of multi-armed bandits to sensor management. In Foundations and Applications of Sensor Management, pages 153-175. Springer, 2008. H. Wu, R. Srikant, X. Liu, and C. Jiang. Algorithms with logarithmic or sub-linear regret for constrained contextual bandits. In Advances in Neural Information Processing Systems 28, pages 433-441, 2015. Y. Wu, R. Shariff, T. Lattimore, and C. Szepesv\u00e1ri. Conservative bandits. In International Conference on Machine Learning, pages 1254-1262, 2016.\n\n\u221a\nKT , then the argument in Theorem 15.2 of Lattimore and Szepesv\u00e1ri [2019] yields the desired result by noting that the framework of constrained bandits subsumes unconstrained multi armed bandits when all costs equal zero. In this case we conclude there is an instancer,c withc a = 0 for all a \u2208 A satisfying: R \u03a0 (T, A,r,c) \u2265 1 27 (k \u2212 1)T Let's instead focus on the case where B = max 1 27\nt belongs to the (estimated) set of feasible policies \u03a0 t , or not. Case yes 1. If \u03c0 * t \u2208 \u03a0 t , then its optimistic reward is less than that of the policy \u03c0 t selected at round t (by the definition of \u03c0 t on Line 4 of Algorithm 1), i.e., r \u03c0 * t ,t \u2264 r \u03c0t,t . This together with the fact that the optimistic reward of any policy \u03c0 is larger than its expected reward, i.e., r \u03c0,t \u2265 r \u03c0 , gives us the desired result that r \u03c0t,t \u2265 r \u03c0 * t . Case 2. If \u03c0 * t \u2208 \u03a0 t , then we define a mixture policy \u03c0 t = \u03b7 t \u03c0 * t + (1 \u2212 \u03b7 t )\u03c0 0 , where \u03c0 0 is the policy that always selects the safe action x 0 and \u03b7 t \u2208 [0, 1] is the maximum value of \u03b7 for which the mixture policy belongs to the set of feasible actions, i.e., \u03c0 t \u2208 \u03a0 t . Conceptually, we can think of \u03b7 t as a measure for safety of the optimal policy \u03c0 * t . Mathematically, \u03b7 t is the value at which the pessimistic cost of the mixture policy equals to the constraint threshold, i.e., c \u03c0t,t = \u03c4 . In the rest of the proof, we first write c \u03c0t,t in terms of the pessimistic cost of the optimal policy as c \u03c0t,t = (1\u2212\u03b7 t )c 0 +\u03b7 t c \u03c0 * t ,t (c 0 is the expected cost of the safe action x 0 ), and find a lower-bound for \u03b7 t (see Eq. 25 in Appendix B.3). We then use the fact that since \u03c0 t \u2208 \u03a0 t , its optimistic reward is less than that of \u03c0 t , i.e., r \u03c0t,t \u2265 r \u03c0t,t , and obtain a lower-bound for r \u03c0t,t as a function of r \u03c0 * t (see Eq. 26 in Appendix B.3). Finally, we conclude the proof by using this lower-bound and finding the relationship between the parameters \u03b1 r and \u03b1 c for which the desired result r \u03c0t,t \u2265 r \u03c0 * t is obtained, i.e., 1 + \u03b1 c \u2264 (\u03c4 \u2212 c 0 )(\u03b1 r \u2212 1).Proof of Theorem 2. The proof follows from the fact that the term (I) is negative (Lemma 4), and by combining the upper-bounds on the term (II) from Lemmas 1 and 2, and setting \u03b3 = \u03b4.\nt + (1 \u2212 \u03b7 t )x 0 , x o,\u22a5 \u03c0t = \u03b7 t x o,\u22a5\nt , \u03b8 * = r \u03c0 * t (and as a results r \u03c0t,t \u2265 r \u03c0 * t as desired) iff:(\u03c4 \u2212 c 0 )r \u03c0 * t + (\u03c4 \u2212 c 0 )(\u03b1 r \u2212 1)C 1 \u2265 (\u03c4 \u2212 c 0 )r \u03c0 * t + (1 + \u03b1 c )C 1 r \u03c0 * t , which holds iff: (\u03c4 \u2212 c 0 )(\u03b1 r \u2212 1)C 1 \u2265 (1 + \u03b1 c )C 1 r \u03c0 * t .Since r \u03c0 * t \u2264 1 from Assumption 4, this holds iff: 1 + \u03b1 c \u2264 (\u03c4 \u2212 c 0 )(\u03b1 r \u2212 1). This concludes the proof as for both cases of \u03c0 * t \u2208 \u03a0 t and \u03c0 * t \u2208 \u03a0 t , we proved that r \u03c0t,t \u2265 r \u03c0 * t .B.4 Learning the safe policy's valueIn this section we relax Assumption 5, and instead assume we only have the knowledge of a safe arm, but not any knowledge of its value c 0 .\nIn other words, the support of\u03c0 * does not contain the safe arm 1.\nThis constraint on T translates to T \u2265 C for some constant C.\nwhich allows us to writeFrom the definition of \u03b7 t , we have c \u03c0t,t = (1\u2212\u03b7t) x0,e0 c0 x0 + \u03b7 t c \u03c0 * t ,t = \u03c4 , and thus, we may write(a) This holds becausewhere the last inequality is because we are on the event E.(b) This passage is due to the fact that the optimal policy \u03c0 * t is feasible, and thus, E x\u223c\u03c0 * t [ x, \u00b5 * ] \u2264 \u03c4 . Therefore, we may writeSince \u03c0 t \u2208 \u03a0 t , we have(a) This is because we may writewhere the last inequality is due to the fact that we are on the event E. Thus,This is a consequence of Lemma 3 stated in the paper and proved in Appendix B.2.(c) This is from the definition of \u03c0 and Eq. 24.(d) This is because \u03b7 t \u2208 [0, 1] and from Assumption 4 we have that all expected rewards are positive (belong to [0, 1]), and thus, x 0 , \u03b8 * \u2265 0.(e) This is by lower-bounding \u03b7 t from (25).Let us define the shorthand notation C 1 := \u03b2 t (\u03b4, d \u2212 1) x o,\u22a5 \u03c0 * t (\u03a3 o,\u22a5 t ) \u22121 . Thus, we may write C 0 asNote that C 0 \u2265 x \u03c0 *\nImproved algorithms for linear stochastic bandits. Y Abbasi-Yadkori, D P\u00e1l, C Szepesv\u00e1ri, Advances in Neural Information Processing Systems. 24Y. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312-2320, 2011.\n\nBandits with concave rewards and convex knapsacks. S Agrawal, N Devanur, In Proceedings of the Fifteenth ACM conference on Economics and computation. S. Agrawal and N. Devanur. Bandits with concave rewards and convex knapsacks. In Proceedings of the Fifteenth ACM conference on Economics and computation, pages 989-1006, 2014.\n\nLinear contextual bandits with knapsacks. S Agrawal, N Devanur, Advances in Neural Information Processing Systems. 29S. Agrawal and N. Devanur. Linear contextual bandits with knapsacks. In Advances in Neural Information Processing Systems 29, pages 3450-3458, 2016.\n\nFurther optimal regret bounds for Thompson sampling. S Agrawal, N Goyal, Proceedings of the 16th International Conference on Artificial Intelligence and Statistics. the 16th International Conference on Artificial Intelligence and StatisticsS. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics, pages 99-107, 2013a.\n\nThompson sampling for contextual bandits with linear payoffs. S Agrawal, N Goyal, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningS. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127-135, 2013b.\n\nLinear stochastic bandits under safety constraints. M Amani, C Alizadeh, Thrampoulidis, Advances in Neural Information Processing Systems. Amani, M. Alizadeh, and C. Thrampoulidis. Linear stochastic bandits under safety constraints. In Advances in Neural Information Processing Systems, pages 9252-9262, 2019.\n\nFinite-time analysis of the multiarmed bandit problem. N Auer, P Cesa-Bianchi, Fischer, Machine Learning. 47Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235-256, 2002.\n\nBandits with knapsacks. A Badanidiyuru, R Kleinberg, A Slivkins, IEEE 54th Annual Symposium on Foundations of Computer Science. A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with knapsacks. In IEEE 54th Annual Symposium on Foundations of Computer Science, pages 207-216, 2013.\n\nResourceful contextual bandits. A Badanidiyuru, J Langford, A Slivkins, Proceedings of The 27th Conference on Learning Theory. The 27th Conference on Learning TheoryA. Badanidiyuru, J. Langford, and A. Slivkins. Resourceful contextual bandits. In Proceedings of The 27th Conference on Learning Theory, pages 1109-1134, 2014.\n\nUsing contextual bandits with behavioral constraints for constrained online movie recommendation. A Balakrishnan, D Bouneffouf, N Mattei, F Rossi, IJCAI. A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi. Using contextual bandits with behavioral constraints for constrained online movie recommendation. In IJCAI, pages 5802-5804, 2018.\n\nStochastic linear optimization under bandit feedback. T Dani, S Hayes, Kakade, Proceedings of the 21st Annual Conference on Learning Theory. the 21st Annual Conference on Learning TheoryDani, T. Hayes, and S. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Annual Conference on Learning Theory, pages 355-366, 2008.\n\nImproved algorithms for conservative exploration in bandits. E Garcelon, M Ghavamzadeh, A Lazaric, M Pirotta, AAAI. E. Garcelon, M. Ghavamzadeh, A. Lazaric, and M. Pirotta. Improved algorithms for conservative exploration in bandits. In AAAI, 2020.\n\nOn the complexity of best-arm identification in multi-armed bandit models. Emilie Kaufmann, Olivier Capp\u00e9, Aur\u00e9lien Garivier, The Journal of Machine Learning Research. 171Emilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best-arm identification in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1-42, 2016.\n\nConservative contextual linear bandits. A Kazerouni, M Ghavamzadeh, Y Yadkori, B Van Roy, Advances in Neural Information Processing Systems. A. Kazerouni, M. Ghavamzadeh, Y. Abbasi Yadkori, and B. Van Roy. Conservative contextual linear bandits. In Advances in Neural Information Processing Systems, pages 3910-3919, 2017.\n\nAsymptotically efficient adaptive allocation rules. T Lai, H Robbins, Advances in Applied Mathematics. 61T. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4-22, 1985.\n\nBandit Algorithms. T Lattimore, C Szepesv\u00e1ri, Cambridge University PressT. Lattimore and C. Szepesv\u00e1ri. Bandit Algorithms. Cambridge University Press, 2019.\n\nA contextual-bandit approach to personalized news article recommendation. W Li, J Chu, R Langford, Schapire, WWW. Li, W. Chu, J. Langford, and R. Schapire. A contextual-bandit approach to personalized news article recommendation. In WWW, pages 661-670, 2010.\n\nMulti-armed bandits with application to 5G small cells. S Maghsudi, E Hossain, IEEE Wireless Communications23S. Maghsudi and E. Hossain. Multi-armed bandits with application to 5G small cells. IEEE Wireless Communications, 23(3):64-73, 2016.\n\nSafe linear thompson sampling with side information. A Moradipari, S Amani, M Alizadeh, C Thrampoulidis, arXiv:1911.02156preprintA. Moradipari, S. Amani, M. Alizadeh, and C. Thrampoulidis. Safe linear thompson sampling with side information. preprint arXiv:1911.02156, 2019.\n\nThe combinatorial multi-armed bandit problem and its application to real-time strategy games. S Ontan\u00f3n, Ninth Artificial Intelligence and Interactive Digital Entertainment Conference. S. Ontan\u00f3n. The combinatorial multi-armed bandit problem and its application to real-time strategy games. In Ninth Artificial Intelligence and Interactive Digital Entertainment Conference, 2013.\n\nLinearly parameterized bandits. P Rusmevichientong, J Tsitsiklis, Mathematics of Operations Research. 352P. Rusmevichientong and J. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395-411, 2010.\n", "annotations": {"author": "[{\"end\":106,\"start\":46},{\"end\":170,\"start\":107},{\"end\":209,\"start\":171},{\"end\":247,\"start\":210},{\"end\":306,\"start\":248},{\"end\":345,\"start\":307}]", "publisher": null, "author_last_name": "[{\"end\":60,\"start\":51},{\"end\":127,\"start\":116},{\"end\":186,\"start\":178},{\"end\":224,\"start\":216},{\"end\":262,\"start\":257},{\"end\":322,\"start\":314}]", "author_first_name": "[{\"end\":50,\"start\":46},{\"end\":115,\"start\":107},{\"end\":177,\"start\":171},{\"end\":215,\"start\":210},{\"end\":256,\"start\":248},{\"end\":313,\"start\":307}]", "author_affiliation": "[{\"end\":105,\"start\":85},{\"end\":169,\"start\":149},{\"end\":208,\"start\":188},{\"end\":246,\"start\":226},{\"end\":305,\"start\":285},{\"end\":344,\"start\":324}]", "title": "[{\"end\":43,\"start\":1},{\"end\":388,\"start\":346}]", "venue": null, "abstract": "[{\"end\":2721,\"start\":390}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2786,\"start\":2764},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2805,\"start\":2786},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2837,\"start\":2805},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3348,\"start\":3330},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3387,\"start\":3348},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3416,\"start\":3387},{\"end\":3905,\"start\":3884},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3946,\"start\":3930},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3973,\"start\":3946},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4021,\"start\":3993},{\"end\":4047,\"start\":4031},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4083,\"start\":4068},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4212,\"start\":4193},{\"end\":4255,\"start\":4240},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4281,\"start\":4255},{\"end\":4301,\"start\":4281},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4395,\"start\":4377},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4424,\"start\":4395},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4450,\"start\":4424},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4482,\"start\":4450},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5040,\"start\":5016},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5066,\"start\":5040},{\"end\":5082,\"start\":5066},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5108,\"start\":5082},{\"end\":5477,\"start\":5461},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5501,\"start\":5477},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5524,\"start\":5501},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6191,\"start\":6172},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6220,\"start\":6196},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6364,\"start\":6339},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7401,\"start\":7382},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7430,\"start\":7406},{\"end\":14363,\"start\":14357},{\"end\":15129,\"start\":15128},{\"end\":19200,\"start\":19197},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26136,\"start\":26117},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26165,\"start\":26141},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26532,\"start\":26508},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27671,\"start\":27652},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31122,\"start\":31091},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31200,\"start\":31181},{\"end\":38181,\"start\":38179},{\"end\":45238,\"start\":45237},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":47844,\"start\":47813},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48346,\"start\":48324},{\"end\":48594,\"start\":48590}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52540,\"start\":52014},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52581,\"start\":52541},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52624,\"start\":52582},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52736,\"start\":52625},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53757,\"start\":52737},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54151,\"start\":53758}]", "paragraph": "[{\"end\":4483,\"start\":2737},{\"end\":5760,\"start\":4485},{\"end\":6383,\"start\":5762},{\"end\":7444,\"start\":6385},{\"end\":8779,\"start\":7446},{\"end\":8960,\"start\":8781},{\"end\":9194,\"start\":8995},{\"end\":9360,\"start\":9237},{\"end\":10213,\"start\":9423},{\"end\":10430,\"start\":10300},{\"end\":10860,\"start\":10581},{\"end\":10989,\"start\":10862},{\"end\":11187,\"start\":10991},{\"end\":11401,\"start\":11189},{\"end\":12338,\"start\":11859},{\"end\":13153,\"start\":12340},{\"end\":13298,\"start\":13250},{\"end\":13849,\"start\":13442},{\"end\":14085,\"start\":13990},{\"end\":14455,\"start\":14198},{\"end\":14612,\"start\":14507},{\"end\":14964,\"start\":14614},{\"end\":15186,\"start\":15054},{\"end\":15492,\"start\":15389},{\"end\":15736,\"start\":15526},{\"end\":16051,\"start\":15786},{\"end\":16310,\"start\":16053},{\"end\":16414,\"start\":16312},{\"end\":16570,\"start\":16416},{\"end\":18338,\"start\":16704},{\"end\":18569,\"start\":18358},{\"end\":18759,\"start\":18663},{\"end\":18948,\"start\":18849},{\"end\":19099,\"start\":19020},{\"end\":19202,\"start\":19176},{\"end\":19386,\"start\":19204},{\"end\":20003,\"start\":19916},{\"end\":20524,\"start\":20224},{\"end\":21224,\"start\":20565},{\"end\":21336,\"start\":21226},{\"end\":21627,\"start\":21372},{\"end\":22174,\"start\":21629},{\"end\":22337,\"start\":22268},{\"end\":22680,\"start\":22435},{\"end\":22951,\"start\":22813},{\"end\":23250,\"start\":23011},{\"end\":23621,\"start\":23252},{\"end\":23706,\"start\":23623},{\"end\":23792,\"start\":23708},{\"end\":23939,\"start\":23794},{\"end\":24196,\"start\":24010},{\"end\":24340,\"start\":24198},{\"end\":24998,\"start\":24432},{\"end\":25619,\"start\":25014},{\"end\":27632,\"start\":25621},{\"end\":28030,\"start\":27649},{\"end\":29278,\"start\":28046},{\"end\":29596,\"start\":29509},{\"end\":29811,\"start\":29810},{\"end\":29917,\"start\":29813},{\"end\":30043,\"start\":30000},{\"end\":30142,\"start\":30116},{\"end\":30214,\"start\":30173},{\"end\":30354,\"start\":30284},{\"end\":30510,\"start\":30496},{\"end\":30595,\"start\":30582},{\"end\":30894,\"start\":30733},{\"end\":31201,\"start\":30941},{\"end\":31367,\"start\":31203},{\"end\":31443,\"start\":31409},{\"end\":31530,\"start\":31445},{\"end\":31744,\"start\":31713},{\"end\":31823,\"start\":31746},{\"end\":31974,\"start\":31929},{\"end\":32069,\"start\":31999},{\"end\":32368,\"start\":32113},{\"end\":32440,\"start\":32370},{\"end\":32639,\"start\":32591},{\"end\":32777,\"start\":32772},{\"end\":33059,\"start\":33002},{\"end\":33116,\"start\":33084},{\"end\":33275,\"start\":33174},{\"end\":33401,\"start\":33277},{\"end\":33851,\"start\":33515},{\"end\":34070,\"start\":33873},{\"end\":34140,\"start\":34116},{\"end\":34215,\"start\":34147},{\"end\":34296,\"start\":34226},{\"end\":34495,\"start\":34298},{\"end\":34715,\"start\":34635},{\"end\":34908,\"start\":34778},{\"end\":35010,\"start\":34910},{\"end\":35163,\"start\":35070},{\"end\":35256,\"start\":35193},{\"end\":35686,\"start\":35282},{\"end\":35796,\"start\":35749},{\"end\":36193,\"start\":35798},{\"end\":36520,\"start\":36218},{\"end\":36733,\"start\":36576},{\"end\":36999,\"start\":36834},{\"end\":37056,\"start\":37001},{\"end\":37479,\"start\":37103},{\"end\":37622,\"start\":37481},{\"end\":37692,\"start\":37624},{\"end\":37944,\"start\":37751},{\"end\":38092,\"start\":38091},{\"end\":38671,\"start\":38094},{\"end\":38973,\"start\":38765},{\"end\":39588,\"start\":38975},{\"end\":39799,\"start\":39590},{\"end\":39921,\"start\":39823},{\"end\":40066,\"start\":40065},{\"end\":40138,\"start\":40068},{\"end\":40280,\"start\":40140},{\"end\":40459,\"start\":40305},{\"end\":40567,\"start\":40461},{\"end\":40617,\"start\":40569},{\"end\":40799,\"start\":40682},{\"end\":40986,\"start\":40801},{\"end\":41140,\"start\":41139},{\"end\":41222,\"start\":41142},{\"end\":41952,\"start\":41918},{\"end\":42003,\"start\":41954},{\"end\":42233,\"start\":42217},{\"end\":42357,\"start\":42288},{\"end\":42483,\"start\":42359},{\"end\":42644,\"start\":42530},{\"end\":42889,\"start\":42646},{\"end\":43101,\"start\":42891},{\"end\":43406,\"start\":43165},{\"end\":43668,\"start\":43589},{\"end\":43944,\"start\":43771},{\"end\":44085,\"start\":43986},{\"end\":44185,\"start\":44087},{\"end\":44403,\"start\":44187},{\"end\":44561,\"start\":44474},{\"end\":44958,\"start\":44590},{\"end\":45152,\"start\":45035},{\"end\":45405,\"start\":45154},{\"end\":45929,\"start\":45426},{\"end\":46023,\"start\":45973},{\"end\":46051,\"start\":46025},{\"end\":46417,\"start\":46227},{\"end\":46658,\"start\":46437},{\"end\":46868,\"start\":46741},{\"end\":47609,\"start\":46888},{\"end\":47767,\"start\":47678},{\"end\":47866,\"start\":47769},{\"end\":48086,\"start\":47868},{\"end\":48179,\"start\":48138},{\"end\":48595,\"start\":48232},{\"end\":49045,\"start\":48614},{\"end\":49679,\"start\":49047},{\"end\":49955,\"start\":49681},{\"end\":50041,\"start\":49970},{\"end\":50233,\"start\":50043},{\"end\":50592,\"start\":50235},{\"end\":50681,\"start\":50594},{\"end\":50697,\"start\":50683},{\"end\":50825,\"start\":50756},{\"end\":50908,\"start\":50827},{\"end\":50990,\"start\":50910},{\"end\":51090,\"start\":50992},{\"end\":51332,\"start\":51092},{\"end\":51427,\"start\":51413},{\"end\":51581,\"start\":51460},{\"end\":51698,\"start\":51668},{\"end\":51906,\"start\":51738},{\"end\":51937,\"start\":51908},{\"end\":52013,\"start\":51939}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8994,\"start\":8961},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9236,\"start\":9195},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9422,\"start\":9361},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10299,\"start\":10214},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10580,\"start\":10431},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11809,\"start\":11402},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13249,\"start\":13154},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13377,\"start\":13299},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13441,\"start\":13377},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13989,\"start\":13850},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14197,\"start\":14086},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14506,\"start\":14456},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15053,\"start\":14965},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15229,\"start\":15187},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15317,\"start\":15229},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15388,\"start\":15317},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15525,\"start\":15493},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15785,\"start\":15737},{\"attributes\":{\"id\":\"formula_18\"},\"end\":16703,\"start\":16571},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18662,\"start\":18570},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18848,\"start\":18760},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19019,\"start\":18949},{\"attributes\":{\"id\":\"formula_22\"},\"end\":19175,\"start\":19100},{\"attributes\":{\"id\":\"formula_24\"},\"end\":19915,\"start\":19387},{\"attributes\":{\"id\":\"formula_25\"},\"end\":20223,\"start\":20004},{\"attributes\":{\"id\":\"formula_26\"},\"end\":20564,\"start\":20525},{\"attributes\":{\"id\":\"formula_27\"},\"end\":22267,\"start\":22175},{\"attributes\":{\"id\":\"formula_28\"},\"end\":22434,\"start\":22338},{\"attributes\":{\"id\":\"formula_29\"},\"end\":22812,\"start\":22681},{\"attributes\":{\"id\":\"formula_30\"},\"end\":23010,\"start\":22952},{\"attributes\":{\"id\":\"formula_31\"},\"end\":24009,\"start\":23940},{\"attributes\":{\"id\":\"formula_32\"},\"end\":24391,\"start\":24341},{\"attributes\":{\"id\":\"formula_33\"},\"end\":24431,\"start\":24391},{\"attributes\":{\"id\":\"formula_34\"},\"end\":29508,\"start\":29279},{\"attributes\":{\"id\":\"formula_35\"},\"end\":29809,\"start\":29613},{\"attributes\":{\"id\":\"formula_36\"},\"end\":29999,\"start\":29918},{\"attributes\":{\"id\":\"formula_37\"},\"end\":30115,\"start\":30044},{\"attributes\":{\"id\":\"formula_38\"},\"end\":30283,\"start\":30215},{\"attributes\":{\"id\":\"formula_39\"},\"end\":30495,\"start\":30355},{\"attributes\":{\"id\":\"formula_40\"},\"end\":30581,\"start\":30511},{\"attributes\":{\"id\":\"formula_41\"},\"end\":30732,\"start\":30596},{\"attributes\":{\"id\":\"formula_42\"},\"end\":31408,\"start\":31368},{\"attributes\":{\"id\":\"formula_43\"},\"end\":31712,\"start\":31531},{\"attributes\":{\"id\":\"formula_44\"},\"end\":31928,\"start\":31824},{\"attributes\":{\"id\":\"formula_45\"},\"end\":32112,\"start\":32070},{\"attributes\":{\"id\":\"formula_46\"},\"end\":32590,\"start\":32441},{\"attributes\":{\"id\":\"formula_47\"},\"end\":32771,\"start\":32640},{\"attributes\":{\"id\":\"formula_48\"},\"end\":33001,\"start\":32778},{\"attributes\":{\"id\":\"formula_49\"},\"end\":33173,\"start\":33117},{\"attributes\":{\"id\":\"formula_51\"},\"end\":33514,\"start\":33402},{\"attributes\":{\"id\":\"formula_52\"},\"end\":33872,\"start\":33852},{\"attributes\":{\"id\":\"formula_53\"},\"end\":34115,\"start\":34071},{\"attributes\":{\"id\":\"formula_54\"},\"end\":34225,\"start\":34216},{\"attributes\":{\"id\":\"formula_55\"},\"end\":34634,\"start\":34496},{\"attributes\":{\"id\":\"formula_56\"},\"end\":34777,\"start\":34716},{\"attributes\":{\"id\":\"formula_57\"},\"end\":35069,\"start\":35011},{\"attributes\":{\"id\":\"formula_58\"},\"end\":35192,\"start\":35164},{\"attributes\":{\"id\":\"formula_59\"},\"end\":35281,\"start\":35257},{\"attributes\":{\"id\":\"formula_60\"},\"end\":36575,\"start\":36521},{\"attributes\":{\"id\":\"formula_61\"},\"end\":36833,\"start\":36734},{\"attributes\":{\"id\":\"formula_62\"},\"end\":37102,\"start\":37057},{\"attributes\":{\"id\":\"formula_63\"},\"end\":37750,\"start\":37693},{\"attributes\":{\"id\":\"formula_64\"},\"end\":38090,\"start\":37945},{\"attributes\":{\"id\":\"formula_65\"},\"end\":38764,\"start\":38672},{\"attributes\":{\"id\":\"formula_66\"},\"end\":40064,\"start\":39922},{\"attributes\":{\"id\":\"formula_67\"},\"end\":40681,\"start\":40618},{\"attributes\":{\"id\":\"formula_68\"},\"end\":41138,\"start\":40987},{\"attributes\":{\"id\":\"formula_69\"},\"end\":41917,\"start\":41223},{\"attributes\":{\"id\":\"formula_70\"},\"end\":42216,\"start\":42004},{\"attributes\":{\"id\":\"formula_71\"},\"end\":42287,\"start\":42234},{\"attributes\":{\"id\":\"formula_72\"},\"end\":42529,\"start\":42484},{\"attributes\":{\"id\":\"formula_73\"},\"end\":43164,\"start\":43102},{\"attributes\":{\"id\":\"formula_74\"},\"end\":43455,\"start\":43407},{\"attributes\":{\"id\":\"formula_75\"},\"end\":43588,\"start\":43497},{\"attributes\":{\"id\":\"formula_76\"},\"end\":43770,\"start\":43669},{\"attributes\":{\"id\":\"formula_77\"},\"end\":43985,\"start\":43945},{\"attributes\":{\"id\":\"formula_78\"},\"end\":44473,\"start\":44404},{\"attributes\":{\"id\":\"formula_79\"},\"end\":45034,\"start\":44959},{\"attributes\":{\"id\":\"formula_80\"},\"end\":45425,\"start\":45406},{\"attributes\":{\"id\":\"formula_81\"},\"end\":45972,\"start\":45930},{\"attributes\":{\"id\":\"formula_82\"},\"end\":46226,\"start\":46052},{\"attributes\":{\"id\":\"formula_83\"},\"end\":46436,\"start\":46418},{\"attributes\":{\"id\":\"formula_84\"},\"end\":46740,\"start\":46659},{\"attributes\":{\"id\":\"formula_85\"},\"end\":47677,\"start\":47610},{\"attributes\":{\"id\":\"formula_86\"},\"end\":48137,\"start\":48087},{\"attributes\":{\"id\":\"formula_87\"},\"end\":48231,\"start\":48180},{\"attributes\":{\"id\":\"formula_88\"},\"end\":48613,\"start\":48596},{\"attributes\":{\"id\":\"formula_89\"},\"end\":49969,\"start\":49956},{\"attributes\":{\"id\":\"formula_90\"},\"end\":50755,\"start\":50698},{\"attributes\":{\"id\":\"formula_91\"},\"end\":51412,\"start\":51333},{\"attributes\":{\"id\":\"formula_92\"},\"end\":51459,\"start\":51428},{\"attributes\":{\"id\":\"formula_93\"},\"end\":51667,\"start\":51582},{\"attributes\":{\"id\":\"formula_94\"},\"end\":51737,\"start\":51699}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2735,\"start\":2723},{\"attributes\":{\"n\":\"3\"},\"end\":11857,\"start\":11811},{\"attributes\":{\"n\":\"4\"},\"end\":18356,\"start\":18341},{\"attributes\":{\"n\":\"5\"},\"end\":21370,\"start\":21339},{\"attributes\":{\"n\":\"6\"},\"end\":25012,\"start\":25001},{\"attributes\":{\"n\":\"7\"},\"end\":27647,\"start\":27635},{\"attributes\":{\"n\":\"8\"},\"end\":28044,\"start\":28033},{\"end\":29612,\"start\":29599},{\"end\":30171,\"start\":30145},{\"end\":30939,\"start\":30897},{\"end\":31997,\"start\":31977},{\"end\":33082,\"start\":33062},{\"end\":34145,\"start\":34143},{\"end\":35722,\"start\":35689},{\"end\":35747,\"start\":35725},{\"end\":36216,\"start\":36196},{\"end\":39821,\"start\":39802},{\"end\":40303,\"start\":40283},{\"end\":43496,\"start\":43457},{\"end\":44588,\"start\":44564},{\"end\":46886,\"start\":46871},{\"end\":52552,\"start\":52542},{\"end\":52593,\"start\":52583},{\"end\":52636,\"start\":52626},{\"end\":53760,\"start\":53759}]", "table": null, "figure_caption": "[{\"end\":52540,\"start\":52016},{\"end\":52581,\"start\":52554},{\"end\":52624,\"start\":52595},{\"end\":52736,\"start\":52638},{\"end\":53757,\"start\":52739},{\"end\":54151,\"start\":53761}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25323,\"start\":25309},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25844,\"start\":25836}]", "bib_author_first_name": "[{\"end\":57725,\"start\":57724},{\"end\":57743,\"start\":57742},{\"end\":57750,\"start\":57749},{\"end\":58046,\"start\":58045},{\"end\":58057,\"start\":58056},{\"end\":58365,\"start\":58364},{\"end\":58376,\"start\":58375},{\"end\":58643,\"start\":58642},{\"end\":58654,\"start\":58653},{\"end\":59087,\"start\":59086},{\"end\":59098,\"start\":59097},{\"end\":59465,\"start\":59464},{\"end\":59474,\"start\":59473},{\"end\":59779,\"start\":59778},{\"end\":59787,\"start\":59786},{\"end\":59987,\"start\":59986},{\"end\":60003,\"start\":60002},{\"end\":60016,\"start\":60015},{\"end\":60283,\"start\":60282},{\"end\":60299,\"start\":60298},{\"end\":60311,\"start\":60310},{\"end\":60675,\"start\":60674},{\"end\":60691,\"start\":60690},{\"end\":60705,\"start\":60704},{\"end\":60715,\"start\":60714},{\"end\":60974,\"start\":60973},{\"end\":60982,\"start\":60981},{\"end\":61339,\"start\":61338},{\"end\":61351,\"start\":61350},{\"end\":61366,\"start\":61365},{\"end\":61377,\"start\":61376},{\"end\":61608,\"start\":61602},{\"end\":61626,\"start\":61619},{\"end\":61642,\"start\":61634},{\"end\":61930,\"start\":61929},{\"end\":61943,\"start\":61942},{\"end\":61958,\"start\":61957},{\"end\":61969,\"start\":61968},{\"end\":62266,\"start\":62265},{\"end\":62273,\"start\":62272},{\"end\":62464,\"start\":62463},{\"end\":62477,\"start\":62476},{\"end\":62677,\"start\":62676},{\"end\":62683,\"start\":62682},{\"end\":62690,\"start\":62689},{\"end\":62919,\"start\":62918},{\"end\":62931,\"start\":62930},{\"end\":63159,\"start\":63158},{\"end\":63173,\"start\":63172},{\"end\":63182,\"start\":63181},{\"end\":63194,\"start\":63193},{\"end\":63476,\"start\":63475},{\"end\":63795,\"start\":63794},{\"end\":63815,\"start\":63814}]", "bib_author_last_name": "[{\"end\":57740,\"start\":57726},{\"end\":57747,\"start\":57744},{\"end\":57761,\"start\":57751},{\"end\":58054,\"start\":58047},{\"end\":58065,\"start\":58058},{\"end\":58373,\"start\":58366},{\"end\":58384,\"start\":58377},{\"end\":58651,\"start\":58644},{\"end\":58660,\"start\":58655},{\"end\":59095,\"start\":59088},{\"end\":59104,\"start\":59099},{\"end\":59471,\"start\":59466},{\"end\":59483,\"start\":59475},{\"end\":59498,\"start\":59485},{\"end\":59784,\"start\":59780},{\"end\":59800,\"start\":59788},{\"end\":59809,\"start\":59802},{\"end\":60000,\"start\":59988},{\"end\":60013,\"start\":60004},{\"end\":60025,\"start\":60017},{\"end\":60296,\"start\":60284},{\"end\":60308,\"start\":60300},{\"end\":60320,\"start\":60312},{\"end\":60688,\"start\":60676},{\"end\":60702,\"start\":60692},{\"end\":60712,\"start\":60706},{\"end\":60721,\"start\":60716},{\"end\":60979,\"start\":60975},{\"end\":60988,\"start\":60983},{\"end\":60996,\"start\":60990},{\"end\":61348,\"start\":61340},{\"end\":61363,\"start\":61352},{\"end\":61374,\"start\":61367},{\"end\":61385,\"start\":61378},{\"end\":61617,\"start\":61609},{\"end\":61632,\"start\":61627},{\"end\":61651,\"start\":61643},{\"end\":61940,\"start\":61931},{\"end\":61955,\"start\":61944},{\"end\":61966,\"start\":61959},{\"end\":61977,\"start\":61970},{\"end\":62270,\"start\":62267},{\"end\":62281,\"start\":62274},{\"end\":62474,\"start\":62465},{\"end\":62488,\"start\":62478},{\"end\":62680,\"start\":62678},{\"end\":62687,\"start\":62684},{\"end\":62699,\"start\":62691},{\"end\":62709,\"start\":62701},{\"end\":62928,\"start\":62920},{\"end\":62939,\"start\":62932},{\"end\":63170,\"start\":63160},{\"end\":63179,\"start\":63174},{\"end\":63191,\"start\":63183},{\"end\":63208,\"start\":63195},{\"end\":63484,\"start\":63477},{\"end\":63812,\"start\":63796},{\"end\":63826,\"start\":63816}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1713123},\"end\":57992,\"start\":57673},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11118267},\"end\":58320,\"start\":57994},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7061528},\"end\":58587,\"start\":58322},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5839932},\"end\":59022,\"start\":58589},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":96146},\"end\":59410,\"start\":59024},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":201058395},\"end\":59721,\"start\":59412},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207609497},\"end\":59960,\"start\":59723},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3698068},\"end\":60248,\"start\":59962},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8370575},\"end\":60574,\"start\":60250},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":51608105},\"end\":60917,\"start\":60576},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9134969},\"end\":61275,\"start\":60919},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211068780},\"end\":61525,\"start\":61277},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12309216},\"end\":61887,\"start\":61527},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1128850},\"end\":62211,\"start\":61889},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18456561},\"end\":62442,\"start\":62213},{\"attributes\":{\"id\":\"b15\"},\"end\":62600,\"start\":62444},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207178795},\"end\":62860,\"start\":62602},{\"attributes\":{\"id\":\"b17\"},\"end\":63103,\"start\":62862},{\"attributes\":{\"doi\":\"arXiv:1911.02156\",\"id\":\"b18\"},\"end\":63379,\"start\":63105},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7281055},\"end\":63760,\"start\":63381},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3204347},\"end\":63994,\"start\":63762}]", "bib_title": "[{\"end\":57722,\"start\":57673},{\"end\":58043,\"start\":57994},{\"end\":58362,\"start\":58322},{\"end\":58640,\"start\":58589},{\"end\":59084,\"start\":59024},{\"end\":59462,\"start\":59412},{\"end\":59776,\"start\":59723},{\"end\":59984,\"start\":59962},{\"end\":60280,\"start\":60250},{\"end\":60672,\"start\":60576},{\"end\":60971,\"start\":60919},{\"end\":61336,\"start\":61277},{\"end\":61600,\"start\":61527},{\"end\":61927,\"start\":61889},{\"end\":62263,\"start\":62213},{\"end\":62674,\"start\":62602},{\"end\":63473,\"start\":63381},{\"end\":63792,\"start\":63762}]", "bib_author": "[{\"end\":57742,\"start\":57724},{\"end\":57749,\"start\":57742},{\"end\":57763,\"start\":57749},{\"end\":58056,\"start\":58045},{\"end\":58067,\"start\":58056},{\"end\":58375,\"start\":58364},{\"end\":58386,\"start\":58375},{\"end\":58653,\"start\":58642},{\"end\":58662,\"start\":58653},{\"end\":59097,\"start\":59086},{\"end\":59106,\"start\":59097},{\"end\":59473,\"start\":59464},{\"end\":59485,\"start\":59473},{\"end\":59500,\"start\":59485},{\"end\":59786,\"start\":59778},{\"end\":59802,\"start\":59786},{\"end\":59811,\"start\":59802},{\"end\":60002,\"start\":59986},{\"end\":60015,\"start\":60002},{\"end\":60027,\"start\":60015},{\"end\":60298,\"start\":60282},{\"end\":60310,\"start\":60298},{\"end\":60322,\"start\":60310},{\"end\":60690,\"start\":60674},{\"end\":60704,\"start\":60690},{\"end\":60714,\"start\":60704},{\"end\":60723,\"start\":60714},{\"end\":60981,\"start\":60973},{\"end\":60990,\"start\":60981},{\"end\":60998,\"start\":60990},{\"end\":61350,\"start\":61338},{\"end\":61365,\"start\":61350},{\"end\":61376,\"start\":61365},{\"end\":61387,\"start\":61376},{\"end\":61619,\"start\":61602},{\"end\":61634,\"start\":61619},{\"end\":61653,\"start\":61634},{\"end\":61942,\"start\":61929},{\"end\":61957,\"start\":61942},{\"end\":61968,\"start\":61957},{\"end\":61979,\"start\":61968},{\"end\":62272,\"start\":62265},{\"end\":62283,\"start\":62272},{\"end\":62476,\"start\":62463},{\"end\":62490,\"start\":62476},{\"end\":62682,\"start\":62676},{\"end\":62689,\"start\":62682},{\"end\":62701,\"start\":62689},{\"end\":62711,\"start\":62701},{\"end\":62930,\"start\":62918},{\"end\":62941,\"start\":62930},{\"end\":63172,\"start\":63158},{\"end\":63181,\"start\":63172},{\"end\":63193,\"start\":63181},{\"end\":63210,\"start\":63193},{\"end\":63486,\"start\":63475},{\"end\":63814,\"start\":63794},{\"end\":63828,\"start\":63814}]", "bib_venue": "[{\"end\":58829,\"start\":58754},{\"end\":59229,\"start\":59176},{\"end\":60415,\"start\":60377},{\"end\":61105,\"start\":61060},{\"end\":57812,\"start\":57763},{\"end\":58142,\"start\":58067},{\"end\":58435,\"start\":58386},{\"end\":58752,\"start\":58662},{\"end\":59174,\"start\":59106},{\"end\":59549,\"start\":59500},{\"end\":59827,\"start\":59811},{\"end\":60088,\"start\":60027},{\"end\":60375,\"start\":60322},{\"end\":60728,\"start\":60723},{\"end\":61058,\"start\":60998},{\"end\":61391,\"start\":61387},{\"end\":61693,\"start\":61653},{\"end\":62028,\"start\":61979},{\"end\":62314,\"start\":62283},{\"end\":62461,\"start\":62444},{\"end\":62714,\"start\":62711},{\"end\":62916,\"start\":62862},{\"end\":63156,\"start\":63105},{\"end\":63564,\"start\":63486},{\"end\":63862,\"start\":63828}]"}}}, "year": 2023, "month": 12, "day": 17}