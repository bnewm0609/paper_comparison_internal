{"id": 6200260, "updated": "2023-09-28 21:45:16.991", "metadata": {"title": "Image-to-Image Translation with Conditional Adversarial Networks", "authors": "[{\"first\":\"Phillip\",\"last\":\"Isola\",\"middle\":[]},{\"first\":\"Jun-Yan\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Tinghui\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Alexei\",\"last\":\"Efros\",\"middle\":[\"A.\"]}]", "venue": "CVPR 2017", "journal": null, "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1611.07004", "mag": "2963073614", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/IsolaZZE16", "doi": "10.1109/cvpr.2017.632"}}, "content": {"source": {"pdf_hash": "072fd0b8d471f183da0ca9880379b3bb29031b6a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1611.07004v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1611.07004", "status": "GREEN"}}, "grobid": {"id": "33d18f44a19ecf3910704a0e1be49a4555c886e8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/072fd0b8d471f183da0ca9880379b3bb29031b6a.txt", "contents": "\nImage-to-Image Translation with Conditional Adversarial Networks\n\n\nPhillip Isola isola@eecs.berkeley.edu \n(BAIR) Laboratory\nUniversity of California\nBerkeley\n\nJun-Yan Zhu junyanz@eecs.berkeley.edu \n(BAIR) Laboratory\nUniversity of California\nBerkeley\n\nTinghui Zhou tinghuiz@eecs.berkeley.edu \n(BAIR) Laboratory\nUniversity of California\nBerkeley\n\nAlexei A Efros efros@eecs.berkeley.edu \n(BAIR) Laboratory\nUniversity of California\nBerkeley\n\nBerkeley Ai Research \n(BAIR) Laboratory\nUniversity of California\nBerkeley\n\nImage-to-Image Translation with Conditional Adversarial Networks\n\n\n\nThe community has already taken significant steps in this direction, with convolutional neural nets (CNNs) becoming the common workhorse behind a wide variety of image prediction problems. CNNs learn to minimize a loss functionan objective that scores the quality of results -and although the learning process is automatic, a lot of manual effort still goes into designing effective losses. In other words, we still have to tell the CNN what we wish it to minimize. But, just like Midas, we must be careful what we wish for! If we take a naive approach, and ask the CNN to minimize Euclidean distance between predicted and ground truth pixels, it will tend to produce blurry results [29,46]. This is because Euclidean distance is minimized by averaging all plausible outputs, which causes blurring. Coming up with loss functions that force the CNN to do what we really want -e.g., output sharp, realistic images -is an open problem and generally requires expert knowledge.\n\nIt would be highly desirable if we could instead specify only a high-level goal, like \"make the output indistinguishable from reality\", and then automatically learn a loss function appropriate for satisfying this goal. Fortunately, this is exactly what is done by the recently proposed Generative Adversarial Networks (GANs) [14,5,30,36,47]. GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss. Blurry images will not be tolerated since they look obviously fake. Because GANs learn a loss that adapts to the data, they can be applied to a multitude of tasks that traditionally would require very different kinds of loss functions.\n\nIn this paper, we explore GANs in the conditional setting. Just as GANs learn a generative model of data, conditional GANs (cGANs) learn a conditional generative model [14]. This makes cGANs suitable for image-to-image translation tasks, where we condition on an input image and generate a corresponding output image.\n\nGANs have been vigorously studied in the last two years and many of the techniques we explore in this paper have been previously proposed. Nonetheless, earlier papers have focused on specific applications, and it has remained unclear how effective image-conditional GANs can be as a general-purpose solution for image-toimage translation. Our primary contribution is to demonstrate that on a wide variety of problems, conditional GANs produce reasonable results. Our second contribution is to present a simple framework sufficient to achieve good results, and to analyze the effects of several important architectural choices. Code is available at https://github.com/phillipi/pix2pix.\n\n\nRelated work\n\nStructured losses for image modeling Image-to-image translation problems are often formulated as per-pixel clas-sification or regression [26,42,17,23,46]. These formulations treat the output space as \"unstructured\" in the sense that each output pixel is considered conditionally independent from all others given the input image. Conditional GANs instead learn a structured loss. Structured losses penalize the joint configuration of the output. A large body of literature has considered losses of this kind, with popular methods including conditional random fields [2], the SSIM metric [40], feature matching [6], nonparametric losses [24], the convolutional pseudo-prior [41], and losses based on matching covariance statistics [19]. Our conditional GAN is different in that the loss is learned, and can, in theory, penalize any possible structure that differs between output and target.\n\nConditional GANs We are not the first to apply GANs in the conditional setting. Previous works have conditioned GANs on discrete labels [28], text [32], and, indeed, images. The image-conditional models have tackled inpainting [29], image prediction from a normal map [39], image manipulation guided by user constraints [49], future frame prediction [27], future state prediction [48], product photo generation [43], and style transfer [25]. Each of these methods was tailored for a specific application. Our framework differs in that nothing is application-specific. This makes our setup considerably simpler than most others.\n\nOur method also differs from these prior works in several architectural choices for the generator and discriminator. Unlike past work, for our generator we use a \"U-Net\"based architecture [34], and for our discriminator we use a convolutional \"PatchGAN\" classifier, which only penalizes structure at the scale of image patches. A similar Patch-GAN architecture was previously proposed in [25], for the purpose of capturing local style statistics. Here we show that this approach is effective on a wider range of problems, and we investigate the effect of changing the patch size.\n\n\nMethod\n\nGANs are generative models that learn a mapping from random noise vector z to output image y: G : z \u2192 y [14]. In contrast, conditional GANs learn a mapping from observed image x and random noise vector z, to y: G : {x, z} \u2192 y. The generator G is trained to produce outputs that cannot be distinguished from \"real\" images by an adversarially trained discrimintor, D, which is trained to do as well as possible at detecting the generator's \"fakes\". This training procedure is diagrammed in Figure 2.\n\n\nObjective\n\nThe objective of a conditional GAN can be expressed as  : Training a conditional GAN to predict aerial photos from maps. The discriminator, D, learns to classify between real and synthesized pairs. The generator learns to fool the discriminator. Unlike an unconditional GAN, both the generator and discriminator observe an input image.\nL cGAN (G, D) =E x,y\u223cp data (x,y) [log D(x, y)]+ E x\u223cp data (x),z\u223cpz(z) [log(1 \u2212 D(x, G(x, z))],(1)\nwhere G tries to minimize this objective against an adversarial D that tries to maximize it, i.e.\nG * = arg min G max D L cGAN (G, D).\nTo test the importance of conditioning the discrimintor, we also compare to an unconditional variant in which the discriminator does not observe x:\nL GAN (G, D) =E y\u223cp data (y) [log D(y)]+ E x\u223cp data (x),z\u223cpz(z) [log(1 \u2212 D(G(x, z))].(2)\nPrevious approaches to conditional GANs have found it beneficial to mix the GAN objective with a more traditional loss, such as L2 distance [29]. The discriminator's job remains unchanged, but the generator is tasked to not only fool the discriminator but also to be near the ground truth output in an L2 sense. We also explore this option, using L1 distance rather than L2 as L1 encourages less blurring:\nL L1 (G) = E x,y\u223cp data (x,y),z\u223cpz(z) [ y \u2212 G(x, z) 1 ]. (3)\nOur final objective is\nG * = arg min G max D L cGAN (G, D) + \u03bbL L1 (G).(4)\nWithout z, the net could still learn a mapping from x to y, but would produce deterministic outputs, and therefore fail to match any distribution other than a delta function. Past conditional GANs have acknowledged this and provided Gaussian noise z as an input to the generator, in addition to x (e.g., [39]). In initial experiments, we did not find Encoder-decoder U-Net Figure 3: Two choices for the architecture of the generator. The \"U-Net\" [34] is an encoder-decoder with skip connections between mirrored layers in the encoder and decoder stacks.\n\nthis strategy effective -the generator simply learned to ignore the noise -which is consistent with Mathieu et al. [27]. Instead, for our final models, we provide noise only in the form of dropout, applied on several layers of our generator at both training and test time. Despite the dropout noise, we observe very minor stochasticity in the output of our nets. Designing conditional GANs that produce stochastic output, and thereby capture the full entropy of the conditional distributions they model, is an important question left open by the present work.\n\n\nNetwork architectures\n\nWe adapt our generator and discriminator architectures from those in [30]. Both generator and discriminator use modules of the form convolution-BatchNorm-ReLu [18]. Details of the architecture are provided in the appendix, with key features discussed below.\n\n\nGenerator with skips\n\nA defining feature of image-to-image translation problems is that they map a high resolution input grid to a high resolution output grid. In addition, for the problems we consider, the input and output differ in surface appearance, but both are renderings of the same underlying structure. Therefore, structure in the input is roughly aligned with structure in the output. We design the generator architecture around these considerations.\n\nMany previous solutions [29,39,19,48,43] to problems in this area have used an encoder-decoder network [16]. In such a network, the input is passed through a series of layers that progressively downsample, until a bottleneck layer, at which point the process is reversed (Figure 3). Such a network requires that all information flow pass through all the layers, including the bottleneck. For many image translation problems, there is a great deal of low-level information shared between the input and output, and it would be desirable to shuttle this information directly across the net. For example, in the case of image colorizaton, the input and output share the location of prominent edges.\n\nTo give the generator a means to circumvent the bottleneck for information like this, we add skip connections, following the general shape of a \"U-Net\" [34] (Figure 3). Specifically, we add skip connections between each layer i and layer n \u2212 i, where n is the total number of layers. Each skip connection simply concatenates all channels at layer i with those at layer n \u2212 i.\n\n\nMarkovian discriminator (PatchGAN)\n\nIt is well known that the L2 loss -and L1, see Figure 4 -produces blurry results on image generation problems [22]. Although these losses fail to encourage highfrequency crispness, in many cases they nonetheless accurately capture the low frequencies. For problems where this is the case, we do not need an entirely new framework to enforce correctness at the low frequencies. L1 will already do.\n\nThis motivates restricting the GAN discriminator to only model high-frequency structure, relying on an L1 term to force low-frequency correctness (Eqn. 4). In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture -which we term a PatchGAN -that only penalizes structure at the scale of patches. This discriminator tries to classify if each N \u00d7 N patch in an image is real or fake. We run this discriminator convolutationally across the image, averaging all responses to provide the ultimate output of D.\n\nIn Section 3.4, we demonstrate that N can be much smaller than the full size of the image and still produce high quality results. This is advantageous because a smaller PatchGAN has fewer parameters, runs faster, and can be applied on arbitrarily large images.\n\nSuch a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. This connection was previously explored in [25], and is also the common assumption in models of texture [8,12] and style [7,15,13,24]. Our PatchGAN can therefore be understood as a form of texture/style loss.\n\n\nOptimization and inference\n\nTo optimize our networks, we follow the standard approach from [14]: we alternate between one gradient descent step on D, then one step on G. We use minibatch SGD and apply the Adam solver [20].\n\nAt inference time, we run the generator net in exactly the same manner as during the training phase. This differs from the usual protocol in that we apply dropout at test time, and we apply batch normalization [18] using the statistics of the test batch, rather than aggregated statistics of the training batch. This approach to batch normalization, when the batch size is set to 1, has been termed \"instance normalization\" and has been demonstrated to be effective at image generation tasks [38]. In our experiments, we use batch size 1 for certain experiments and 4 for others, noting little difference between these two conditions.\n\n\nExperiments\n\nTo explore the generality of conditional GANs, we test the method on a variety of tasks and datasets, including both graphics tasks, like photo generation, and vision tasks, like semantic segmentation:\n\n\u2022 Semantic labels\u2194photo, trained on the Cityscapes dataset [4]. \u2022 Architectural labels\u2192photo, trained on the CMP Facades dataset [31]. \u2022 Map\u2194aerial photo, trained on data scraped from Google Maps. \u2022 BW\u2192color photos, trained on [35].\n\n\u2022 Edges\u2192photo, trained on data from [49] and [44]; binary edges generated using the HED edge detector [42] plus postprocessing. \u2022 Sketch\u2192photo: tests edges\u2192photo models on humandrawn sketches from [10]. Data requirements and speed We note that decent results can often be obtained even on small datasets. Our facade training set consists of just 400 images (see results in Figure 12), and the day to night training set consists of only 91 unique webcams (see results in Figure 13). On datasets of this size, training can be very fast: for example, the results shown in Figure 12 took less than two hours of training on a single Pascal Titan X GPU. At test time, all models run in well under a second on this GPU.\n\n\nEvaluation metrics\n\nEvaluating the quality of synthesized images is an open and difficult problem [36]. Traditional metrics such as perpixel mean-squared error do not assess joint statistics of the result, and therefore do not measure the very structure that structured losses aim to capture.\n\nIn order to more holistically evaluate the visual quality of our results, we employ two tactics. First, we run \"real vs fake\" perceptual studies on Amazon Mechanical Turk (AMT). For graphics problems like colorization and photo generation, plausibility to a human observer is often the ultimate goal. Therefore, we test our map generation, aerial photo generation, and image colorization using this approach.\n\nSecond, we measure whether or not our synthesized cityscapes are realistic enough that off-the-shelf recognition system can recognize the objects in them. This metric is similar to the \"inception score\" from [36], the object detection evaluation in [39], and the \"semantic interpretability\" measure in [46].\n\nAMT perceptual studies For our AMT experiments, we followed the protocol from [46]: Turkers were presented with a series of trials that pitted a \"real\" image against a \"fake\" image generated by our algorithm. On each trial, each image appeared for 1 second, after which the images disappeared and Turkers were given unlimited time to respond as to which was fake. The first 10 images of each session were practice and Turkers were given feedback. No feedback was provided on the 40 trials of the main experiment. Each session tested just one algorithm at a time, and Turkers were not allowed to complete more than one session. \u223c 50 Turkers evaluated each algorithm. All images were presented at 256 \u00d7 256 resolution. Unlike [46], we did not include vigilance trials. For our colorization experiments, the real and fake images were generated from the same grayscale input. For map\u2194aerial photo, the real and fake images were not generated from the same input, in order to make the task more difficult and avoid floor-level results.\n\nFCN-score While quantitative evaluation of generative models is known to be challenging, recent works [36,39,46] have tried using pre-trained semantic classifiers to measure the discriminability of the generated images as a pseudo-metric. The intuition is that if the generated images are realistic, classifiers trained on real images will be able to classify the synthesized image correctly as well. To this end, we adopt the popular FCN-8s [26] architecture for semantic segmentation, and train it on the cityscapes dataset. We then score synthesized photos by the classification accuracy against the labels these photos were synthesized from.   \n\n\nAnalysis of the objective function\n\nWhich components of the objective in Eqn. 4 are important? We run ablation studies to isolate the effect of the L1 term, the GAN term, and to compare using a discriminator conditioned on the input (cGAN, Eqn. 1) against using an unconditional discriminator (GAN, Eqn. 2). Figure 4 shows the qualitative effects of these variations on two labels\u2192photo problems. L1 alone leads to reasonable but blurry results. The cGAN alone (setting \u03bb = 0 in Eqn. 4) gives much sharper results, but results in some artifacts in facade synthesis. Adding both terms together (with \u03bb = 100) reduces these artifacts.\n\nWe quantify these observations using the FCN-score on the cityscapes labels\u2192photo task (Table 1): the GAN-based objectives achieve higher scores, indicating that the synthesized images include more recognizable structure. We also test the effect of removing conditioning from the discriminator (labeled as GAN). In this case, the loss does not penalize mismatch between the input and output; it only cares that the output look realistic. This variant results in very poor performance; examining the results reveals that the generator collapsed into producing nearly the exact same output regardless of input photograph. Clearly it is important, in this case, that the loss measure the quality of the match between input and output, and indeed cGAN performs much better than GAN. Note, however, that adding an L1 term also encourages that the output respect the input, since the L1 loss penalizes the distance between ground truth outputs, which match the input, and synthesized outputs, which may not. Correspondingly, L1+GAN is also effective at creating realistic renderings that respect the in- The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric ( Colorfulness A striking effect of conditional GANs is that they produce sharp images, hallucinating spatial structure even where it does not exist in the input label map. One might imagine cGANs have a similar effect on \"sharpening\" in the spectral dimension -i.e. making images more colorful. Just as L1 will incentivize a blur when it is uncertain where exactly to locate an edge, it will also incentivize an average, grayish color when it is uncertain which of several plausible color values a pixel should take on. Specially, L1 will be minimized by choosing the median of of the conditional probability density function over possible colors. An adversarial loss, on the other hand, can in principle become aware that grayish outputs are unrealistic, and encourage matching the true color distribution [14]. In Figure 7, we investigate if our cGANs actually achieve this effect on the Cityscapes dataset. The plots show the marginal distri-butions over output color values in Lab color space. The ground truth distributions are shown with a dotted line. It is apparent that L1 leads to a narrower distribution than the ground truth, confirming the hypothesis that L1 encourages average, grayish colors. Using a cGAN, on the other hand, pushes the output distribution closer to the ground truth.\n\n\nAnalysis of the generator architecture\n\nA U-Net architecture allows low-level information to shortcut across the network. Does this lead to better results? Figure 5 compares the U-Net against an encoder-decoder on cityscape generation U-Net. The encoder-decoder is created simply by severing the skip connections in the U-Net. The encoder-decoder is unable to learn to generate realistic images in our experiments, and indeed collapses to producing nearly identical results for each input label map. The advantages of the U-Net appear not to be specific to conditional GANs: when both U-Net and encoder-decoder are trained   Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions. L1 1x1 16x16 70x70 256x256 Figure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric ( Table 2).\n\n\nClassification\n\nOurs L2 [44] (rebal.) [44] (L1 + cGAN) Ground truth Figure 7: Colorization results of conditional GANs versus the L2 regression from [44] and the full method (classification with rebalancing) from [46]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\n\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo!labels. Figure 8 shows qualitative results, and quantitative classification accuracies are reported in Table 4. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \"labels\", which are\n\n\nInput\n\nGround truth L1 cGAN Figure 8: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects. nearly discrete, rather than \"images\", with their continuousvalued variation. Although cGANs achieve some success, they are far from the best available method for solving this problem: simply using L1 regression gets better scores than using a cGAN, as shown in Table 4. We argue that for vision problems, the goal (i.e. predicting output close to ground truth) may be less ambiguous than graphics tasks, and reconstruction losses like L1 are mostly sufficient.\n\n\nConclusion\n\nThe results in this paper suggest that conditional adversarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.   Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions. L1 1x1 16x16 70x70 256x256 Figure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric ( Table 2).\n7 log P (L) log P (a) log P (b) L a b (\n\nClassification\n\nOurs L2 [44] (rebal.) [44] (L1 + cGAN) Ground truth Figure 7: Colorization results of conditional GANs versus the L2 regression from [44] and the full method (classification with rebalancing) from [46]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\n\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo!labels. Figure 8 shows qualitative results, and quantitative classification accuracies are reported in Table 4. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \"labels\", which are\n\n\nInput\n\nGround truth L1 cGAN Figure 8: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects. nearly discrete, rather than \"images\", with their continuousvalued variation. Although cGANs achieve some success, they are far from the best available method for solving this problem: simply using L1 regression gets better scores than using a cGAN, as shown in Table 4. We argue that for vision problems, the goal (i.e. predicting output close to ground truth) may be less ambiguous than graphics tasks, and reconstruction losses like L1 are mostly sufficient.\n\n\nConclusion\n\nThe results in this paper suggest that conditional adversarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.    Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions. L1 1x1 16x16 70x70 256x256 Figure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric ( Table 2).\n\n\nClassification\n\nOurs L2 [44] (rebal.) [44] (L1 + cGAN) Ground truth Figure 7: Colorization results of conditional GANs versus the L2 regression from [44] and the full method (classification with rebalancing) from [46]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\n\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo!labels. Figure 8 shows qualitative results, and quantitative classification accuracies are reported in Table 4. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \"labels\", which are\n\n\nInput\n\nGround truth L1 cGAN Figure 8: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects. nearly discrete, rather than \"images\", with their continuousvalued variation. Although cGANs achieve some success, they are far from the best available method for solving this problem: simply using L1 regression gets better scores than using a cGAN, as shown in Table 4. We argue that for vision problems, the goal (i.e. predicting output close to ground truth) may be less ambiguous than graphics tasks, and reconstruction losses like L1 are mostly sufficient.\n\n\nConclusion\n\nThe results in this paper suggest that conditional adversarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.  Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions.\n\nwith an L1 loss, the U-Net again achieves the superior results ( Figure 5).\n\n\nFrom PixelGANs to PatchGans to ImageGANs\n\nWe test the effect of varying the patch size N of our discriminator receptive fields, from a 1 \u00d7 1 \"PixelGAN\" to a full 256 \u00d7 256 \"ImageGAN\" 1 . Figure 6 shows qualitative results of this analysis and Table 2 quantifies the effects using the FCN-score. Note that elsewhere in this paper, unless specified, all experiments use 70 \u00d7 70 PatchGANs, and for this section all experiments use an L1+cGAN loss.\n\nThe PixelGAN has no effect on spatial sharpness, but does increase the colorfulness of the results (quantified in Figure 7). For example, the bus in Figure 6 is painted gray when the net is trained with an L1 loss, but becomes red with the PixelGAN loss. Color histogram matching is a common problem in image processing [33], and PixelGANs may be a promising lightweight solution.\n\nUsing a 16\u00d716 PatchGAN is sufficient to promote sharp outputs, but also leads to tiling artifacts. The 70 \u00d7 70 Patch-GAN alleviates these artifacts. Scaling beyond this, to the full 256 \u00d7 256 ImageGAN, does not appear to improve the visual quality of the results, and in fact gets a considerably lower FCN-score (Table 2). This may be because the Im-ageGAN has many more parameters and greater depth than the 70 \u00d7 70 PatchGAN, and may be harder to train.\n\nFully-convolutional translation An advantage of the PatchGAN is that a fixed-size patch discriminator can be applied to arbitrarily large images. We may also apply the generator convolutionally, on larger images than those on which it was trained. We test this on the map\u2194aerial photo task. After training a generator on 256\u00d7256 images, we test it on 512 \u00d7 512 images. The results in Figure 8 demonstrate the effectiveness of this approach. 1 We achieve this variation in patch size by adjusting the depth of the GAN discriminator. Details of this process, and the discriminator architectures are provided in the appendix\n\n\nClassification\n\nOurs L2 [46] (rebal.) [46] (L1 + cGAN) Ground truth Figure 9: Colorization results of conditional GANs versus the L2 regression from [46] and the full method (classification with rebalancing) from [48]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\n\nPhoto \u2192 Map Map \u2192 Photo Loss % Turkers labeled real % Turkers labeled real L1\n\n2.8% \u00b1 1.0% 0.8% \u00b1 0.3% L1+cGAN\n\n6.1% \u00b1 1.3% 18.9% \u00b1 2.5% Table 3: AMT \"real vs fake\" test on maps\u2194aerial photos.\n\nMethod % Turkers labeled real L2 regression from [46] 16.3% \u00b1 2.4% Zhang et al. 2016 [46] 27.8% \u00b1 2.7% Ours 22.5% \u00b1 1.6%  \n\n\nInput\n\nGround truth L1 cGAN Figure 10: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects.\n\n\nPerceptual validation\n\nWe validate the perceptual realism of our results on the tasks of map\u2194aerial photograph and grayscale\u2192color. Results of our AMT experiment for map\u2194photo are given in Table 3. The aerial photos generated by our method fooled participants on 18.9% of trials, significantly above the L1 baseline, which produces blurry results and nearly never fooled participants. In contrast, in the photo\u2192map directionm our method only fooled participants on 6.1% of trials, and this was not significantly different than the performance of the L1 baseline (based on bootstrap test). This may be because minor structural errors are more visible in maps, which have rigid geometry, than in aerial photographs, which are more chaotic.\n\nWe trained colorization on ImageNet [35], and tested on the test split introduced by [46,23]. Our method, with L1+cGAN loss, fooled participants on 22.5% of trials (Table 4). We also tested the results of [46] and a variant of their method that used an L2 loss (see [46] for details). The conditional GAN scored similarly to the L2 variant of [46] (difference insignificant by bootstrap test), but fell short of [46]'s full method, which fooled participants on 27.8% of trials in our experiment. We note that their method was specifically engineered to do well on colorization.\n\n\nSemantic segmentation\n\nConditional GANs appear to be effective on problems where the output is highly detailed or photographic, as is common in image processing and graphics tasks. What about vision problems, like semantic segmentation, where the output is instead less complex than the input?\n\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo\u2192labels. Figure 10 shows qualitative results, and quantitative classification accuracies are reported in Table 5. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \"labels\", which are nearly discrete, rather than \"images\", with their continuousvalued variation 2 . Although cGANs achieve some success, they are far from the best available method for solving this problem: simply using L1 regression gets better scores than using a cGAN, as shown in Table 5. We argue that for vision problems, the goal (i.e. predicting output close to ground truth) may be less ambiguous than graphics tasks, and reconstruction losses like L1 are mostly sufficient.\n\n\nConclusion\n\nThe results in this paper suggest that conditional adversarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings. \n\n\nInput\n\nGround truth Output Input Ground truth Output Figure 11: Example results of our method on Cityscapes labels\u2192photo, compared to ground truth.\n\n\nInput\n\nGround truth Output Input Ground truth Output \n\n\nInput\n\nGround truth Output Input Ground truth Output Figure 14: Example results of our method on automatically detected edges\u2192handbags, compared to ground truth.\n\n\nInput\n\nGround truth Output Input Ground truth Output Figure 15: Example results of our method on automatically detected edges\u2192shoes, compared to ground truth.\n\n\nInput\n\nOutput Input Output Input Output Input Output Figure 16: Example results of the edges\u2192photo models applied to human-drawn sketches from [10]. Note that the models were trained on automatically detected edges, but generalize to human drawings \n\nFigure 2\n2Figure 2: Training a conditional GAN to predict aerial photos from maps. The discriminator, D, learns to classify between real and synthesized pairs. The generator learns to fool the discriminator. Unlike an unconditional GAN, both the generator and discriminator observe an input image.\n\n\u2022\nDay\u2192night, trained on [21]. Details of training on each of these datasets are provided in the Appendix. In all cases, the input and output are simply 1-3 channel images. Qualitative results are shown in Figures 8, 9, 10, 11, 12, 14, 15, 16, and 13. Several failure cases are highlighted in Figure 17. More comprehensive results are available at https://phillipi.github.io/pix2pix/.\n\nFigure 5 :\n5Adding skip connections to an encoder-decoder to create a \"U-Net\" results in much higher quality results.\n\nFigure 4 :Figure 6 :\n46Different losses induce different quality of results. Each column shows results trained under a different loss. Please see https://phillipi.github.io/pix2pix/ for additional examples. Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1.\n\nFigure 5 :\n5Color distribution matching property of the cGAN, tested on Cityscapes. (c.f.\n\nFigure 5 :\n5Color distribution matching property of the cGAN, tested on Cityscapes. (c.f.\n\nFigure 5 :\n5Color distribution matching property of the cGAN, tested on Cityscapes. (c.f.\n\nFigure 7 :\n7Color distribution matching property of the cGAN, tested on Cityscapes. (c.f.\n\nFigure 8 :\n8Example results on Google Maps at 512x512 resolution (model was trained on images at 256x256 resolution, and run convolutionally on the larger images at test time). Contrast adjusted for clarity.\n\nFigure 12 :Figure 13 :\n1213Example results of our method on facades labels\u2192photo, compared to ground Example results of our method on day\u2192night, compared to ground truth.\n\nFigure 17 :\n17Example failure cases. Each pair of images shows input on the left and output on the right. These examples are selected as some of the worst results on our tasks. Common failures include artifacts in regions where the input image is sparse, and difficulty in handling unusual inputs. Please see https://phillipi.github.io/pix2pix/ for more comprehensive results.\n\n\nDiscriminator receptive field Per-pixel acc. Per-class acc.Class IOU \n1\u00d71 \n0.44 \n0.14 \n0.10 \n16\u00d716 \n0.62 \n0.20 \n0.16 \n70\u00d770 \n0.63 \n0.21 \n0.16 \n256\u00d7256 \n0.47 \n0.18 \n0.13 \n\n\n\nTable 2 :\n2FCN-scores for different receptive field sizes of the discriminator, evaluated on Cityscapes labels\u2192photos.\n\nTable 2 )\n2. Please \n\n\nTable 4 :\n4AMT \"real vs fake\" test on colorization.Loss \n\nPer-pixel acc. Per-class acc. Class IOU \nL1 \n0.86 \n0.42 \n0.35 \ncGAN \n0.74 \n0.28 \n0.22 \nL1+cGAN \n0.83 \n0.36 \n0.29 \n\n\n\nTable 5 :\n5Performance of photo\u2192labels on cityscapes.\nNote that the label maps we train on are not exactly discrete valued, as they are resized from the original maps using bilinear interpolation and saved as jpeg images, with some compression artifacts.\nAppendixNetwork architecturesWe adapt our network architectures from those in[30].Code for the models is available at https://github.com/phillipi/pix2pix. Let Ck denote a Convolution-BatchNorm-ReLU layer with k filters. CDk denotes a a Convolution-BatchNorm-Dropout-ReLU layer with a dropout rate of 50%. All convolutions are 4 \u00d7 4 spatial filters applied with stride 2. Convolutions in the encoder, and in the discriminator, downsample by a factor of 2, whereas in the decoder they upsample by a factor of 2.Generator architecturesThe encoder-decoder architecture consists of: encoder:After the last layer in the decoder, a convolution is applied to map to the number of output channels (3 in general, except in colorization, where it is 2), followed by a Tanh function. As an exception to the above notation, Batch-Norm is not applied to the first C64 layer in the encoder. All ReLUs in the encoder are leaky, with slope 0.2, while ReLUs in the decoder are not leaky.The U-Net architecture is identical except with skip connections between each layer i in the encoder and layer n \u2212 i in the decoder, where n is the total number of layers. The skip connections concatenate activations from layer i to layer n \u2212 i. This changes the number of channels in the decoder:U-Net decoder:Discriminator architecturesThe 70 \u00d7 70 discriminator architecture is:After the last layer, a convolution is applied to map to a 1 dimensional output, followed by a Sigmoid function. As an exception to the above notation, BatchNorm is not applied to the first C64 layer. All ReLUs are leaky, with slope 0.2.All other discriminators follow the same basic architecture, with depth varied to modify the receptive field size:1 \u00d7 1 discriminator: C64-C128 (note, in this special case, all convolutions are 1 \u00d7 1 spatial filters) 16 \u00d7 16 discriminator:Note the the 256 \u00d7 256 discriminator has receptive fields that could cover up to 574 \u00d7 574 pixels, if they were available, but since the input images are only 256 \u00d7 256 pixels, only 256\u00d7256 pixels are seen, and so we refer to this setting as the 256 \u00d7 256 discriminator.Training detailsRandom jitter was applied by resizing the 256\u00d7256 input images to 286 \u00d7 286 and then randomly cropping back to size 256 \u00d7 256.All networks were trained from scratch. Weights were initialized from a Gaussian distribution with mean 0 and standard deviation 0.02.Semantic labels\u2192photo 2975 training images from the Cityscapes training set[4], trained for 200 epochs, batch size 1, with random jitter and mirroring. We used the Cityscapes val set for testing.Architectural labels\u2192photo 400 training images from[31], trained for 200 epochs, batch size 1, with random jitter and mirroring. Data from was split into train and test randomly.Maps\u2194aerial photograph 1096 training images scraped from Google Maps, trained for 200 epochs, batch size 1, with random jitter and mirroring. Images were sampled from in and around New York City. Data was then split into train and test about the median latitude of the sampling region (with a buffer region added to ensure that no training pixel appeared in the test set).BW\u2192color 1.2 million training images (Imagenet training set[35]), trained for \u223c 6 epochs, batch size 4, with only mirroring, no random jitter. Tested on subset of Imagenet val set, following protocol of[46]and[23].Edges\u2192shoes 50k training images from UT Zappos50K dataset[45]trained for 15 epochs, batch size 4. Data from was split into train and test randomly.Edges\u2192Handbag 137K Amazon Handbag images from[49], trained for 15 epochs, batch size 4. Data from was split into train and test randomly.Day\u2192night 17823 training images extracted from 91 webcams, from[21]trained for 17 epochs, batch size 4, with random jitter and mirroring. We use 91 webcams as training, and 10 webcams for test.\nA non-local algorithm for image denoising. A Buades, B Coll, J.-M Morel, CVPR. IEEE2A. Buades, B. Coll, and J.-M. Morel. A non-local algo- rithm for image denoising. In CVPR, volume 2, pages 60-65. IEEE, 2005. 1\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, ICLR. L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep con- volutional nets and fully connected crfs. In ICLR, 2015. 2\n\nSketch2photo: internet image montage. T Chen, M.-M Cheng, P Tan, A Shamir, S.-M Hu, ACM Transactions on Graphics (TOG). 285124T. Chen, M.-M. Cheng, P. Tan, A. Shamir, and S.-M. Hu. Sketch2photo: internet image montage. ACM Transactions on Graphics (TOG), 28(5):124, 2009. 1\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, 416M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR), 2016. 4, 16\n\nDeep generative image models using a laplacian pyramid of adversarial networks. E L Denton, S Chintala, R Fergus, NIPS. E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera- tive image models using a laplacian pyramid of adversarial networks. In NIPS, pages 1486-1494, 2015. 2\n\nGenerating images with perceptual similarity metrics based on deep networks. A Dosovitskiy, T Brox, arXiv:1602.02644arXiv preprintA. Dosovitskiy and T. Brox. Generating images with per- ceptual similarity metrics based on deep networks. arXiv preprint arXiv:1602.02644, 2016. 2\n\nImage quilting for texture synthesis and transfer. A A Efros, W T Freeman, SIGGRAPH. ACM14A. A. Efros and W. T. Freeman. Image quilting for tex- ture synthesis and transfer. In SIGGRAPH, pages 341-346. ACM, 2001. 1, 4\n\nTexture synthesis by nonparametric sampling. A A Efros, T K Leung, ICCV. IEEE2A. A. Efros and T. K. Leung. Texture synthesis by non- parametric sampling. In ICCV, volume 2, pages 1033-1038. IEEE, 1999. 4\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionD. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolu- tional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650-2658, 2015. 1\n\nHow do humans sketch objects?. M Eitz, J Hays, M Alexa, SIGGRAPH3112M. Eitz, J. Hays, and M. Alexa. How do humans sketch objects? SIGGRAPH, 31(4):44-1, 2012. 4, 12\n\nRemoving camera shake from a single photograph. R Fergus, B Singh, A Hertzmann, S T Roweis, W T Freeman, In ACM Transactions on Graphics. 251ACMR. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. T. Freeman. Removing camera shake from a single photograph. In ACM Transactions on Graphics (TOG), volume 25, pages 787-794. ACM, 2006. 1\n\nTexture synthesis and the controlled generation of natural stimuli using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, arXiv:1505.0737612arXiv preprintL. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis and the controlled generation of natural stimuli using convo- lutional neural networks. arXiv preprint arXiv:1505.07376, 12, 2015. 4\n\nL A Gatys, A S Ecker, M Bethge, Image style transfer using convolutional neural networks. CVPR. L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. CVPR, 2016. 4\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. 67I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In NIPS, 2014. 2, 4, 6, 7\n\nImage analogies. A Hertzmann, C E Jacobs, N Oliver, B Curless, D H Salesin, SIGGRAPH. A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H. Salesin. Image analogies. In SIGGRAPH, pages 327-340.\n\nACM. ACM, 2001. 1, 4\n\nReducing the dimensionality of data with neural networks. G E Hinton, R R Salakhutdinov, Science. 3135786G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504-507, 2006. 3\n\nLet there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification. S Iizuka, E Simo-Serra, H Ishikawa, ACM Transactions on Graphics (TOG). 354S. Iizuka, E. Simo-Serra, and H. Ishikawa. Let there be Color!: Joint End-to-end Learning of Global and Local Im- age Priors for Automatic Image Colorization with Simulta- neous Classification. ACM Transactions on Graphics (TOG), 35(4), 2016. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, 34S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. 2015. 3, 4\n\nPerceptual losses for real-time style transfer and super-resolution. J Johnson, A Alahi, L Fei-Fei, 23J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. 2016. 2, 3\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, ICLR. 4D. Kingma and J. Ba. Adam: A method for stochastic opti- mization. ICLR, 2015. 4\n\nTransient attributes for high-level understanding and editing of outdoor scenes. P.-Y Laffont, Z Ren, X Tao, C Qian, J Hays, ACM Transactions on Graphics (TOG). 33416P.-Y. Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays. Transient attributes for high-level understanding and editing of outdoor scenes. ACM Transactions on Graphics (TOG), 33(4):149, 2014. 1, 4, 16\n\nAutoencoding beyond pixels using a learned similarity metric. A B L Larsen, S K S\u00f8nderby, O Winther, arXiv:1512.09300arXiv preprintA. B. L. Larsen, S. K. S\u00f8nderby, and O. Winther. Autoen- coding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015. 4\n\nLearning representations for automatic colorization. G Larsson, M Maire, G Shakhnarovich, ECCV. 8216G. Larsson, M. Maire, and G. Shakhnarovich. Learning rep- resentations for automatic colorization. ECCV, 2016. 2, 8, 16\n\nCombining markov random fields and convolutional neural networks for image synthesis. C Li, M Wand, CVPR. 24C. Li and M. Wand. Combining markov random fields and convolutional neural networks for image synthesis. CVPR, 2016. 2, 4\n\nPrecomputed real-time texture synthesis with markovian generative adversarial networks. C Li, M Wand, ECCV. 24C. Li and M. Wand. Precomputed real-time texture synthe- sis with markovian generative adversarial networks. ECCV, 2016. 2, 4\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. 15J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431- 3440, 2015. 1, 2, 5\n\nDeep multi-scale video prediction beyond mean square error. M Mathieu, C Couprie, Y Lecun, ICLR. 23M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016. 2, 3\n\nM Mirza, S Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintM. Mirza and S. Osindero. Conditional generative adversar- ial nets. arXiv preprint arXiv:1411.1784, 2014. 2\n\nContext encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, T Darrell, A A Efros, CVPR. 23D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. CVPR, 2016. 2, 3\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434216arXiv preprintA. Radford, L. Metz, and S. Chintala. Unsupervised repre- sentation learning with deep convolutional generative adver- sarial networks. arXiv preprint arXiv:1511.06434, 2015. 2, 3, 16\n\nSpatial pattern templates for recognition of objects with regular structure. R \u0160 Tyle\u010dek, Proc. GCPR, Saarbrucken. GCPR, SaarbruckenGermany416R.\u0160. Radim Tyle\u010dek. Spatial pattern templates for recogni- tion of objects with regular structure. In Proc. GCPR, Saar- brucken, Germany, 2013. 4, 16\n\nS Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, arXiv:1605.05396Generative adversarial text to image synthesis. arXiv preprintS. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016. 2\n\nColor transfer between images. E Reinhard, M Ashikhmin, B Gooch, P Shirley, IEEE Computer Graphics and Applications. 217E. Reinhard, M. Ashikhmin, B. Gooch, and P. Shirley. Color transfer between images. IEEE Computer Graphics and Ap- plications, 21:34-41, 2001. 7\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MIC-CAISpringer24O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In MIC- CAI, pages 234-241. Springer, 2015. 2, 3, 4\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, IJCV. 115316O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211-252, 2015. 4, 8, 16\n\nT Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, arXiv:1606.03498Improved techniques for training gans. 5arXiv preprintT. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad- ford, and X. Chen. Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016. 2, 4, 5\n\nData-driven hallucination of different times of day from a single outdoor photo. Y Shih, S Paris, F Durand, W T Freeman, ACM Transactions on Graphics (TOG). 326200Y. Shih, S. Paris, F. Durand, and W. T. Freeman. Data-driven hallucination of different times of day from a single outdoor photo. ACM Transactions on Graphics (TOG), 32(6):200, 2013. 1\n\nInstance normalization: The missing ingredient for fast stylization. D Ulyanov, A Vedaldi, V Lempitsky, arXiv:1607.08022arXiv preprintD. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normal- ization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. 4\n\nGenerative image modeling using style and structure adversarial networks. X Wang, A Gupta, ECCV. 25X. Wang and A. Gupta. Generative image modeling using style and structure adversarial networks. ECCV, 2016. 2, 3, 5\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Transactions on Image Processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to struc- tural similarity. IEEE Transactions on Image Processing, 13(4):600-612, 2004. 2\n\nTop-down learning for structured labeling with convolutional pseudoprior. S Xie, X Huang, Z Tu, S. Xie, X. Huang, and Z. Tu. Top-down learning for struc- tured labeling with convolutional pseudoprior. 2015. 2\n\nHolistically-nested edge detection. S Xie, Z Tu, ICCV. S. Xie and Z. Tu. Holistically-nested edge detection. In ICCV, 2015. 1, 2, 4\n\nPixellevel domain transfer. D Yoo, N Kim, S Park, A S Paek, I S Kweon, ECCV. 23D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. Pixel- level domain transfer. ECCV, 2016. 2, 3\n\nFine-Grained Visual Comparisons with Local Learning. A Yu, K Grauman, CVPR. A. Yu and K. Grauman. Fine-Grained Visual Comparisons with Local Learning. In CVPR, 2014. 4\n\nFine-grained visual comparisons with local learning. A Yu, K Grauman, CVPR. 16A. Yu and K. Grauman. Fine-grained visual comparisons with local learning. In CVPR, pages 192-199, 2014. 16\n\nColorful image colorization. ECCV. R Zhang, P Isola, A A Efros, 16R. Zhang, P. Isola, and A. A. Efros. Colorful image coloriza- tion. ECCV, 2016. 1, 2, 5, 7, 8, 16\n\nEnergy-based generative adversarial network. J Zhao, M Mathieu, Y Lecun, arXiv:1609.03126arXiv preprintJ. Zhao, M. Mathieu, and Y. LeCun. Energy-based genera- tive adversarial network. arXiv preprint arXiv:1609.03126, 2016. 2\n\nLearning temporal transformations from time-lapse videos. Y Zhou, T L Berg, ECCV. 7Y. Zhou and T. L. Berg. Learning temporal transformations from time-lapse videos. In ECCV, 2016. 2, 3, 7\n\nGenerative visual manipulation on the natural image manifold. J.-Y Zhu, P Kr\u00e4henb\u00fchl, E Shechtman, A A Efros, ECCV. 16J.-Y. Zhu, P. Kr\u00e4henb\u00fchl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image mani- fold. In ECCV, 2016. 2, 4, 16\n", "annotations": {"author": "[{\"end\":159,\"start\":68},{\"end\":251,\"start\":160},{\"end\":345,\"start\":252},{\"end\":438,\"start\":346},{\"end\":513,\"start\":439}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":76},{\"end\":171,\"start\":168},{\"end\":264,\"start\":260},{\"end\":360,\"start\":355},{\"end\":459,\"start\":451}]", "author_first_name": "[{\"end\":75,\"start\":68},{\"end\":167,\"start\":160},{\"end\":259,\"start\":252},{\"end\":352,\"start\":346},{\"end\":354,\"start\":353},{\"end\":447,\"start\":439},{\"end\":450,\"start\":448}]", "author_affiliation": "[{\"end\":158,\"start\":107},{\"end\":250,\"start\":199},{\"end\":344,\"start\":293},{\"end\":437,\"start\":386},{\"end\":512,\"start\":461}]", "title": "[{\"end\":65,\"start\":1},{\"end\":578,\"start\":514}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1269,\"start\":1265},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1272,\"start\":1269},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1885,\"start\":1881},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1887,\"start\":1885},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1890,\"start\":1887},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1893,\"start\":1890},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1896,\"start\":1893},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2457,\"start\":2453},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3446,\"start\":3442},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3449,\"start\":3446},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3452,\"start\":3449},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3455,\"start\":3452},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3458,\"start\":3455},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3874,\"start\":3871},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3896,\"start\":3892},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3918,\"start\":3915},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3945,\"start\":3941},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3982,\"start\":3978},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4039,\"start\":4035},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4336,\"start\":4332},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4347,\"start\":4343},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4427,\"start\":4423},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4468,\"start\":4464},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4520,\"start\":4516},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4550,\"start\":4546},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4580,\"start\":4576},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4611,\"start\":4607},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4636,\"start\":4632},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5017,\"start\":5013},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5217,\"start\":5213},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5523,\"start\":5519},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6878,\"start\":6874},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7584,\"start\":7580},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7726,\"start\":7722},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7950,\"start\":7946},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8489,\"start\":8485},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8579,\"start\":8575},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9166,\"start\":9162},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9169,\"start\":9166},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9172,\"start\":9169},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9175,\"start\":9172},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9178,\"start\":9175},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9245,\"start\":9241},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9990,\"start\":9986},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10362,\"start\":10358},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11727,\"start\":11723},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11787,\"start\":11784},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11790,\"start\":11787},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11804,\"start\":11801},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11807,\"start\":11804},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11810,\"start\":11807},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11813,\"start\":11810},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11986,\"start\":11982},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12112,\"start\":12108},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12329,\"start\":12325},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12611,\"start\":12607},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13030,\"start\":13027},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13101,\"start\":13097},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13199,\"start\":13195},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13242,\"start\":13238},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13251,\"start\":13247},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13308,\"start\":13304},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13403,\"start\":13399},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14019,\"start\":14015},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14833,\"start\":14829},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14874,\"start\":14870},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14927,\"start\":14923},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15012,\"start\":15008},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15658,\"start\":15654},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16068,\"start\":16064},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16071,\"start\":16068},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16074,\"start\":16071},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16408,\"start\":16404},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19650,\"start\":19646},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20805,\"start\":20801},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21784,\"start\":21780},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21798,\"start\":21794},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21909,\"start\":21905},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21973,\"start\":21969},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23610,\"start\":23606},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24628,\"start\":24624},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24642,\"start\":24638},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24753,\"start\":24749},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24817,\"start\":24813},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26455,\"start\":26451},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27434,\"start\":27430},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27448,\"start\":27444},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27559,\"start\":27555},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27623,\"start\":27619},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29259,\"start\":29255},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30349,\"start\":30345},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31305,\"start\":31304},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31515,\"start\":31511},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31529,\"start\":31525},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31640,\"start\":31636},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31704,\"start\":31700},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32109,\"start\":32105},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32145,\"start\":32141},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33182,\"start\":33178},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33231,\"start\":33227},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33234,\"start\":33231},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33351,\"start\":33347},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33412,\"start\":33408},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33489,\"start\":33485},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33558,\"start\":33554},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35919,\"start\":35915}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36320,\"start\":36022},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36705,\"start\":36321},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36824,\"start\":36706},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37198,\"start\":36825},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37289,\"start\":37199},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37380,\"start\":37290},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37471,\"start\":37381},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37562,\"start\":37472},{\"attributes\":{\"id\":\"fig_9\"},\"end\":37771,\"start\":37563},{\"attributes\":{\"id\":\"fig_10\"},\"end\":37943,\"start\":37772},{\"attributes\":{\"id\":\"fig_11\"},\"end\":38321,\"start\":37944},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38495,\"start\":38322},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38615,\"start\":38496},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38638,\"start\":38616},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38813,\"start\":38639},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":38868,\"start\":38814}]", "paragraph": "[{\"end\":1554,\"start\":582},{\"end\":2283,\"start\":1556},{\"end\":2602,\"start\":2285},{\"end\":3288,\"start\":2604},{\"end\":4194,\"start\":3305},{\"end\":4823,\"start\":4196},{\"end\":5404,\"start\":4825},{\"end\":5912,\"start\":5415},{\"end\":6261,\"start\":5926},{\"end\":6459,\"start\":6362},{\"end\":6644,\"start\":6497},{\"end\":7139,\"start\":6734},{\"end\":7223,\"start\":7201},{\"end\":7829,\"start\":7276},{\"end\":8390,\"start\":7831},{\"end\":8673,\"start\":8416},{\"end\":9136,\"start\":8698},{\"end\":9832,\"start\":9138},{\"end\":10209,\"start\":9834},{\"end\":10644,\"start\":10248},{\"end\":11262,\"start\":10646},{\"end\":11524,\"start\":11264},{\"end\":11888,\"start\":11526},{\"end\":12113,\"start\":11919},{\"end\":12749,\"start\":12115},{\"end\":12966,\"start\":12765},{\"end\":13200,\"start\":12968},{\"end\":13914,\"start\":13202},{\"end\":14209,\"start\":13937},{\"end\":14619,\"start\":14211},{\"end\":14928,\"start\":14621},{\"end\":15960,\"start\":14930},{\"end\":16610,\"start\":15962},{\"end\":17245,\"start\":16649},{\"end\":20138,\"start\":17247},{\"end\":21753,\"start\":20181},{\"end\":22129,\"start\":21772},{\"end\":22543,\"start\":22131},{\"end\":23226,\"start\":22553},{\"end\":24558,\"start\":23241},{\"end\":24973,\"start\":24616},{\"end\":25387,\"start\":24975},{\"end\":26070,\"start\":25397},{\"end\":27403,\"start\":26085},{\"end\":27779,\"start\":27422},{\"end\":28193,\"start\":27781},{\"end\":28876,\"start\":28203},{\"end\":29499,\"start\":28891},{\"end\":29576,\"start\":29501},{\"end\":30023,\"start\":29621},{\"end\":30405,\"start\":30025},{\"end\":30861,\"start\":30407},{\"end\":31484,\"start\":30863},{\"end\":31860,\"start\":31503},{\"end\":31939,\"start\":31862},{\"end\":31972,\"start\":31941},{\"end\":32054,\"start\":31974},{\"end\":32178,\"start\":32056},{\"end\":32400,\"start\":32188},{\"end\":33140,\"start\":32426},{\"end\":33719,\"start\":33142},{\"end\":34015,\"start\":33745},{\"end\":34895,\"start\":34017},{\"end\":35238,\"start\":34910},{\"end\":35388,\"start\":35248},{\"end\":35444,\"start\":35398},{\"end\":35608,\"start\":35454},{\"end\":35769,\"start\":35618},{\"end\":36021,\"start\":35779}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6361,\"start\":6262},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6496,\"start\":6460},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6733,\"start\":6645},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7200,\"start\":7140},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7275,\"start\":7224},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24598,\"start\":24559}]", "table_ref": "[{\"end\":17343,\"start\":17334},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21751,\"start\":21744},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":22322,\"start\":22315},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":23034,\"start\":23027},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24556,\"start\":24549},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25166,\"start\":25159},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25878,\"start\":25871},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27401,\"start\":27394},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":27972,\"start\":27965},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28684,\"start\":28677},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29829,\"start\":29822},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30727,\"start\":30719},{\"end\":32006,\"start\":31999},{\"end\":32599,\"start\":32592},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":34209,\"start\":34202},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":34703,\"start\":34696}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3303,\"start\":3291},{\"attributes\":{\"n\":\"2.\"},\"end\":5413,\"start\":5407},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5924,\"start\":5915},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8414,\"start\":8393},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":8696,\"start\":8676},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":10246,\"start\":10212},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11917,\"start\":11891},{\"attributes\":{\"n\":\"3.\"},\"end\":12763,\"start\":12752},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13935,\"start\":13917},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16647,\"start\":16613},{\"attributes\":{\"n\":\"3.3.\"},\"end\":20179,\"start\":20141},{\"end\":21770,\"start\":21756},{\"end\":22551,\"start\":22546},{\"attributes\":{\"n\":\"4.\"},\"end\":23239,\"start\":23229},{\"end\":24614,\"start\":24600},{\"end\":25395,\"start\":25390},{\"attributes\":{\"n\":\"4.\"},\"end\":26083,\"start\":26073},{\"end\":27420,\"start\":27406},{\"end\":28201,\"start\":28196},{\"attributes\":{\"n\":\"4.\"},\"end\":28889,\"start\":28879},{\"attributes\":{\"n\":\"3.4.\"},\"end\":29619,\"start\":29579},{\"end\":31501,\"start\":31487},{\"end\":32186,\"start\":32181},{\"attributes\":{\"n\":\"3.5.\"},\"end\":32424,\"start\":32403},{\"attributes\":{\"n\":\"3.6.\"},\"end\":33743,\"start\":33722},{\"attributes\":{\"n\":\"4.\"},\"end\":34908,\"start\":34898},{\"end\":35246,\"start\":35241},{\"end\":35396,\"start\":35391},{\"end\":35452,\"start\":35447},{\"end\":35616,\"start\":35611},{\"end\":35777,\"start\":35772},{\"end\":36031,\"start\":36023},{\"end\":36323,\"start\":36322},{\"end\":36717,\"start\":36707},{\"end\":36846,\"start\":36826},{\"end\":37210,\"start\":37200},{\"end\":37301,\"start\":37291},{\"end\":37392,\"start\":37382},{\"end\":37483,\"start\":37473},{\"end\":37574,\"start\":37564},{\"end\":37795,\"start\":37773},{\"end\":37956,\"start\":37945},{\"end\":38506,\"start\":38497},{\"end\":38626,\"start\":38617},{\"end\":38649,\"start\":38640},{\"end\":38824,\"start\":38815}]", "table": "[{\"end\":38495,\"start\":38383},{\"end\":38638,\"start\":38628},{\"end\":38813,\"start\":38691}]", "figure_caption": "[{\"end\":36320,\"start\":36033},{\"end\":36705,\"start\":36324},{\"end\":36824,\"start\":36719},{\"end\":37198,\"start\":36849},{\"end\":37289,\"start\":37212},{\"end\":37380,\"start\":37303},{\"end\":37471,\"start\":37394},{\"end\":37562,\"start\":37485},{\"end\":37771,\"start\":37576},{\"end\":37943,\"start\":37800},{\"end\":38321,\"start\":37959},{\"end\":38383,\"start\":38324},{\"end\":38615,\"start\":38508},{\"end\":38691,\"start\":38651},{\"end\":38868,\"start\":38826}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5911,\"start\":5903},{\"end\":7657,\"start\":7649},{\"end\":9418,\"start\":9409},{\"end\":10000,\"start\":9991},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13584,\"start\":13575},{\"end\":13681,\"start\":13672},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13780,\"start\":13771},{\"end\":16929,\"start\":16921},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":19663,\"start\":19655},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20305,\"start\":20297},{\"end\":20774,\"start\":20766},{\"end\":21081,\"start\":21073},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":21832,\"start\":21824},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":22228,\"start\":22220},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":22582,\"start\":22574},{\"end\":23579,\"start\":23571},{\"end\":23886,\"start\":23878},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24676,\"start\":24668},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":25072,\"start\":25064},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":25426,\"start\":25418},{\"end\":26424,\"start\":26416},{\"end\":26731,\"start\":26723},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27482,\"start\":27474},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27878,\"start\":27870},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":28232,\"start\":28224},{\"end\":29228,\"start\":29220},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29574,\"start\":29566},{\"end\":29774,\"start\":29766},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30147,\"start\":30139},{\"end\":30182,\"start\":30174},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":31255,\"start\":31247},{\"end\":31563,\"start\":31555},{\"end\":32218,\"start\":32209},{\"end\":34115,\"start\":34106},{\"end\":35303,\"start\":35294},{\"end\":35509,\"start\":35500},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35673,\"start\":35664},{\"end\":35834,\"start\":35825}]", "bib_author_first_name": "[{\"end\":42922,\"start\":42921},{\"end\":42932,\"start\":42931},{\"end\":42943,\"start\":42939},{\"end\":43178,\"start\":43174},{\"end\":43186,\"start\":43185},{\"end\":43200,\"start\":43199},{\"end\":43212,\"start\":43211},{\"end\":43222,\"start\":43221},{\"end\":43224,\"start\":43223},{\"end\":43450,\"start\":43449},{\"end\":43461,\"start\":43457},{\"end\":43470,\"start\":43469},{\"end\":43477,\"start\":43476},{\"end\":43490,\"start\":43486},{\"end\":43750,\"start\":43749},{\"end\":43760,\"start\":43759},{\"end\":43769,\"start\":43768},{\"end\":43778,\"start\":43777},{\"end\":43789,\"start\":43788},{\"end\":43802,\"start\":43801},{\"end\":43814,\"start\":43813},{\"end\":43824,\"start\":43823},{\"end\":43832,\"start\":43831},{\"end\":44118,\"start\":44117},{\"end\":44120,\"start\":44119},{\"end\":44130,\"start\":44129},{\"end\":44142,\"start\":44141},{\"end\":44397,\"start\":44396},{\"end\":44412,\"start\":44411},{\"end\":44650,\"start\":44649},{\"end\":44652,\"start\":44651},{\"end\":44661,\"start\":44660},{\"end\":44663,\"start\":44662},{\"end\":44863,\"start\":44862},{\"end\":44865,\"start\":44864},{\"end\":44874,\"start\":44873},{\"end\":44876,\"start\":44875},{\"end\":45131,\"start\":45130},{\"end\":45140,\"start\":45139},{\"end\":45534,\"start\":45533},{\"end\":45542,\"start\":45541},{\"end\":45550,\"start\":45549},{\"end\":45716,\"start\":45715},{\"end\":45726,\"start\":45725},{\"end\":45735,\"start\":45734},{\"end\":45748,\"start\":45747},{\"end\":45750,\"start\":45749},{\"end\":45760,\"start\":45759},{\"end\":45762,\"start\":45761},{\"end\":46111,\"start\":46110},{\"end\":46113,\"start\":46112},{\"end\":46122,\"start\":46121},{\"end\":46124,\"start\":46123},{\"end\":46133,\"start\":46132},{\"end\":46368,\"start\":46367},{\"end\":46370,\"start\":46369},{\"end\":46379,\"start\":46378},{\"end\":46381,\"start\":46380},{\"end\":46390,\"start\":46389},{\"end\":46607,\"start\":46606},{\"end\":46621,\"start\":46620},{\"end\":46638,\"start\":46637},{\"end\":46647,\"start\":46646},{\"end\":46653,\"start\":46652},{\"end\":46669,\"start\":46668},{\"end\":46678,\"start\":46677},{\"end\":46691,\"start\":46690},{\"end\":46890,\"start\":46889},{\"end\":46903,\"start\":46902},{\"end\":46905,\"start\":46904},{\"end\":46915,\"start\":46914},{\"end\":46925,\"start\":46924},{\"end\":46936,\"start\":46935},{\"end\":46938,\"start\":46937},{\"end\":47155,\"start\":47154},{\"end\":47157,\"start\":47156},{\"end\":47167,\"start\":47166},{\"end\":47169,\"start\":47168},{\"end\":47482,\"start\":47481},{\"end\":47492,\"start\":47491},{\"end\":47506,\"start\":47505},{\"end\":47897,\"start\":47896},{\"end\":47906,\"start\":47905},{\"end\":48119,\"start\":48118},{\"end\":48130,\"start\":48129},{\"end\":48139,\"start\":48138},{\"end\":48315,\"start\":48314},{\"end\":48325,\"start\":48324},{\"end\":48504,\"start\":48500},{\"end\":48515,\"start\":48514},{\"end\":48522,\"start\":48521},{\"end\":48529,\"start\":48528},{\"end\":48537,\"start\":48536},{\"end\":48845,\"start\":48844},{\"end\":48849,\"start\":48846},{\"end\":48859,\"start\":48858},{\"end\":48861,\"start\":48860},{\"end\":48873,\"start\":48872},{\"end\":49122,\"start\":49121},{\"end\":49133,\"start\":49132},{\"end\":49142,\"start\":49141},{\"end\":49376,\"start\":49375},{\"end\":49382,\"start\":49381},{\"end\":49609,\"start\":49608},{\"end\":49615,\"start\":49614},{\"end\":49814,\"start\":49813},{\"end\":49822,\"start\":49821},{\"end\":49835,\"start\":49834},{\"end\":50051,\"start\":50050},{\"end\":50062,\"start\":50061},{\"end\":50073,\"start\":50072},{\"end\":50206,\"start\":50205},{\"end\":50215,\"start\":50214},{\"end\":50457,\"start\":50456},{\"end\":50467,\"start\":50466},{\"end\":50481,\"start\":50480},{\"end\":50492,\"start\":50491},{\"end\":50503,\"start\":50502},{\"end\":50505,\"start\":50504},{\"end\":50751,\"start\":50750},{\"end\":50762,\"start\":50761},{\"end\":50770,\"start\":50769},{\"end\":51077,\"start\":51076},{\"end\":51079,\"start\":51078},{\"end\":51293,\"start\":51292},{\"end\":51301,\"start\":51300},{\"end\":51310,\"start\":51309},{\"end\":51317,\"start\":51316},{\"end\":51331,\"start\":51330},{\"end\":51342,\"start\":51341},{\"end\":51614,\"start\":51613},{\"end\":51626,\"start\":51625},{\"end\":51639,\"start\":51638},{\"end\":51648,\"start\":51647},{\"end\":51914,\"start\":51913},{\"end\":51929,\"start\":51928},{\"end\":51940,\"start\":51939},{\"end\":52177,\"start\":52176},{\"end\":52192,\"start\":52191},{\"end\":52200,\"start\":52199},{\"end\":52206,\"start\":52205},{\"end\":52216,\"start\":52215},{\"end\":52228,\"start\":52227},{\"end\":52234,\"start\":52233},{\"end\":52243,\"start\":52242},{\"end\":52255,\"start\":52254},{\"end\":52265,\"start\":52264},{\"end\":52497,\"start\":52496},{\"end\":52509,\"start\":52508},{\"end\":52523,\"start\":52522},{\"end\":52534,\"start\":52533},{\"end\":52544,\"start\":52543},{\"end\":52555,\"start\":52554},{\"end\":52879,\"start\":52878},{\"end\":52887,\"start\":52886},{\"end\":52896,\"start\":52895},{\"end\":52906,\"start\":52905},{\"end\":52908,\"start\":52907},{\"end\":53216,\"start\":53215},{\"end\":53227,\"start\":53226},{\"end\":53238,\"start\":53237},{\"end\":53510,\"start\":53509},{\"end\":53518,\"start\":53517},{\"end\":53726,\"start\":53725},{\"end\":53734,\"start\":53733},{\"end\":53736,\"start\":53735},{\"end\":53745,\"start\":53744},{\"end\":53747,\"start\":53746},{\"end\":53757,\"start\":53756},{\"end\":53759,\"start\":53758},{\"end\":54086,\"start\":54085},{\"end\":54093,\"start\":54092},{\"end\":54102,\"start\":54101},{\"end\":54258,\"start\":54257},{\"end\":54265,\"start\":54264},{\"end\":54383,\"start\":54382},{\"end\":54390,\"start\":54389},{\"end\":54397,\"start\":54396},{\"end\":54405,\"start\":54404},{\"end\":54407,\"start\":54406},{\"end\":54415,\"start\":54414},{\"end\":54417,\"start\":54416},{\"end\":54589,\"start\":54588},{\"end\":54595,\"start\":54594},{\"end\":54758,\"start\":54757},{\"end\":54764,\"start\":54763},{\"end\":54927,\"start\":54926},{\"end\":54936,\"start\":54935},{\"end\":54945,\"start\":54944},{\"end\":54947,\"start\":54946},{\"end\":55102,\"start\":55101},{\"end\":55110,\"start\":55109},{\"end\":55121,\"start\":55120},{\"end\":55342,\"start\":55341},{\"end\":55350,\"start\":55349},{\"end\":55352,\"start\":55351},{\"end\":55538,\"start\":55534},{\"end\":55545,\"start\":55544},{\"end\":55559,\"start\":55558},{\"end\":55572,\"start\":55571},{\"end\":55574,\"start\":55573}]", "bib_author_last_name": "[{\"end\":42929,\"start\":42923},{\"end\":42937,\"start\":42933},{\"end\":42949,\"start\":42944},{\"end\":43183,\"start\":43179},{\"end\":43197,\"start\":43187},{\"end\":43209,\"start\":43201},{\"end\":43219,\"start\":43213},{\"end\":43231,\"start\":43225},{\"end\":43455,\"start\":43451},{\"end\":43467,\"start\":43462},{\"end\":43474,\"start\":43471},{\"end\":43484,\"start\":43478},{\"end\":43493,\"start\":43491},{\"end\":43757,\"start\":43751},{\"end\":43766,\"start\":43761},{\"end\":43775,\"start\":43770},{\"end\":43786,\"start\":43779},{\"end\":43799,\"start\":43790},{\"end\":43811,\"start\":43803},{\"end\":43821,\"start\":43815},{\"end\":43829,\"start\":43825},{\"end\":43840,\"start\":43833},{\"end\":44127,\"start\":44121},{\"end\":44139,\"start\":44131},{\"end\":44149,\"start\":44143},{\"end\":44409,\"start\":44398},{\"end\":44417,\"start\":44413},{\"end\":44658,\"start\":44653},{\"end\":44671,\"start\":44664},{\"end\":44871,\"start\":44866},{\"end\":44882,\"start\":44877},{\"end\":45137,\"start\":45132},{\"end\":45147,\"start\":45141},{\"end\":45539,\"start\":45535},{\"end\":45547,\"start\":45543},{\"end\":45556,\"start\":45551},{\"end\":45723,\"start\":45717},{\"end\":45732,\"start\":45727},{\"end\":45745,\"start\":45736},{\"end\":45757,\"start\":45751},{\"end\":45770,\"start\":45763},{\"end\":46119,\"start\":46114},{\"end\":46130,\"start\":46125},{\"end\":46140,\"start\":46134},{\"end\":46376,\"start\":46371},{\"end\":46387,\"start\":46382},{\"end\":46397,\"start\":46391},{\"end\":46618,\"start\":46608},{\"end\":46635,\"start\":46622},{\"end\":46644,\"start\":46639},{\"end\":46650,\"start\":46648},{\"end\":46666,\"start\":46654},{\"end\":46675,\"start\":46670},{\"end\":46688,\"start\":46679},{\"end\":46698,\"start\":46692},{\"end\":46900,\"start\":46891},{\"end\":46912,\"start\":46906},{\"end\":46922,\"start\":46916},{\"end\":46933,\"start\":46926},{\"end\":46946,\"start\":46939},{\"end\":47164,\"start\":47158},{\"end\":47183,\"start\":47170},{\"end\":47489,\"start\":47483},{\"end\":47503,\"start\":47493},{\"end\":47515,\"start\":47507},{\"end\":47903,\"start\":47898},{\"end\":47914,\"start\":47907},{\"end\":48127,\"start\":48120},{\"end\":48136,\"start\":48131},{\"end\":48147,\"start\":48140},{\"end\":48322,\"start\":48316},{\"end\":48328,\"start\":48326},{\"end\":48512,\"start\":48505},{\"end\":48519,\"start\":48516},{\"end\":48526,\"start\":48523},{\"end\":48534,\"start\":48530},{\"end\":48542,\"start\":48538},{\"end\":48856,\"start\":48850},{\"end\":48870,\"start\":48862},{\"end\":48881,\"start\":48874},{\"end\":49130,\"start\":49123},{\"end\":49139,\"start\":49134},{\"end\":49156,\"start\":49143},{\"end\":49379,\"start\":49377},{\"end\":49387,\"start\":49383},{\"end\":49612,\"start\":49610},{\"end\":49620,\"start\":49616},{\"end\":49819,\"start\":49815},{\"end\":49832,\"start\":49823},{\"end\":49843,\"start\":49836},{\"end\":50059,\"start\":50052},{\"end\":50070,\"start\":50063},{\"end\":50079,\"start\":50074},{\"end\":50212,\"start\":50207},{\"end\":50224,\"start\":50216},{\"end\":50464,\"start\":50458},{\"end\":50478,\"start\":50468},{\"end\":50489,\"start\":50482},{\"end\":50500,\"start\":50493},{\"end\":50511,\"start\":50506},{\"end\":50759,\"start\":50752},{\"end\":50767,\"start\":50763},{\"end\":50779,\"start\":50771},{\"end\":51087,\"start\":51080},{\"end\":51298,\"start\":51294},{\"end\":51307,\"start\":51302},{\"end\":51314,\"start\":51311},{\"end\":51328,\"start\":51318},{\"end\":51339,\"start\":51332},{\"end\":51346,\"start\":51343},{\"end\":51623,\"start\":51615},{\"end\":51636,\"start\":51627},{\"end\":51645,\"start\":51640},{\"end\":51656,\"start\":51649},{\"end\":51926,\"start\":51915},{\"end\":51937,\"start\":51930},{\"end\":51945,\"start\":51941},{\"end\":52189,\"start\":52178},{\"end\":52197,\"start\":52193},{\"end\":52203,\"start\":52201},{\"end\":52213,\"start\":52207},{\"end\":52225,\"start\":52217},{\"end\":52231,\"start\":52229},{\"end\":52240,\"start\":52235},{\"end\":52252,\"start\":52244},{\"end\":52262,\"start\":52256},{\"end\":52275,\"start\":52266},{\"end\":52506,\"start\":52498},{\"end\":52520,\"start\":52510},{\"end\":52531,\"start\":52524},{\"end\":52541,\"start\":52535},{\"end\":52552,\"start\":52545},{\"end\":52560,\"start\":52556},{\"end\":52884,\"start\":52880},{\"end\":52893,\"start\":52888},{\"end\":52903,\"start\":52897},{\"end\":52916,\"start\":52909},{\"end\":53224,\"start\":53217},{\"end\":53235,\"start\":53228},{\"end\":53248,\"start\":53239},{\"end\":53515,\"start\":53511},{\"end\":53524,\"start\":53519},{\"end\":53731,\"start\":53727},{\"end\":53742,\"start\":53737},{\"end\":53754,\"start\":53748},{\"end\":53770,\"start\":53760},{\"end\":54090,\"start\":54087},{\"end\":54099,\"start\":54094},{\"end\":54105,\"start\":54103},{\"end\":54262,\"start\":54259},{\"end\":54268,\"start\":54266},{\"end\":54387,\"start\":54384},{\"end\":54394,\"start\":54391},{\"end\":54402,\"start\":54398},{\"end\":54412,\"start\":54408},{\"end\":54423,\"start\":54418},{\"end\":54592,\"start\":54590},{\"end\":54603,\"start\":54596},{\"end\":54761,\"start\":54759},{\"end\":54772,\"start\":54765},{\"end\":54933,\"start\":54928},{\"end\":54942,\"start\":54937},{\"end\":54953,\"start\":54948},{\"end\":55107,\"start\":55103},{\"end\":55118,\"start\":55111},{\"end\":55127,\"start\":55122},{\"end\":55347,\"start\":55343},{\"end\":55357,\"start\":55353},{\"end\":55542,\"start\":55539},{\"end\":55556,\"start\":55546},{\"end\":55569,\"start\":55560},{\"end\":55580,\"start\":55575}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11206708},\"end\":43089,\"start\":42878},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1996665},\"end\":43409,\"start\":43091},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":11816255},\"end\":43684,\"start\":43411},{\"attributes\":{\"id\":\"b3\"},\"end\":44035,\"start\":43686},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1282515},\"end\":44317,\"start\":44037},{\"attributes\":{\"doi\":\"arXiv:1602.02644\",\"id\":\"b5\"},\"end\":44596,\"start\":44319},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9334387},\"end\":44815,\"start\":44598},{\"attributes\":{\"id\":\"b7\"},\"end\":45020,\"start\":44817},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":102496818},\"end\":45500,\"start\":45022},{\"attributes\":{\"id\":\"b9\"},\"end\":45665,\"start\":45502},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":40060575},\"end\":46004,\"start\":45667},{\"attributes\":{\"doi\":\"arXiv:1505.07376\",\"id\":\"b11\"},\"end\":46365,\"start\":46006},{\"attributes\":{\"id\":\"b12\"},\"end\":46575,\"start\":46367},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1033682},\"end\":46870,\"start\":46577},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2201072},\"end\":47072,\"start\":46872},{\"attributes\":{\"id\":\"b15\"},\"end\":47094,\"start\":47074},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1658773},\"end\":47332,\"start\":47096},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":237949161},\"end\":47800,\"start\":47334},{\"attributes\":{\"id\":\"b18\"},\"end\":48047,\"start\":47802},{\"attributes\":{\"id\":\"b19\"},\"end\":48268,\"start\":48049},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":48417,\"start\":48270},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":182946914},\"end\":48780,\"start\":48419},{\"attributes\":{\"doi\":\"arXiv:1512.09300\",\"id\":\"b22\"},\"end\":49066,\"start\":48782},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7023610},\"end\":49287,\"start\":49068},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6635779},\"end\":49518,\"start\":49289},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":18781152},\"end\":49755,\"start\":49520},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1629541},\"end\":49988,\"start\":49757},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":205514},\"end\":50203,\"start\":49990},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b28\"},\"end\":50404,\"start\":50205},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2202933},\"end\":50654,\"start\":50406},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b30\"},\"end\":50997,\"start\":50656},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6060524},\"end\":51290,\"start\":50999},{\"attributes\":{\"doi\":\"arXiv:1605.05396\",\"id\":\"b32\"},\"end\":51580,\"start\":51292},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14088925},\"end\":51846,\"start\":51582},{\"attributes\":{\"id\":\"b34\"},\"end\":52123,\"start\":51848},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2930547},\"end\":52494,\"start\":52125},{\"attributes\":{\"doi\":\"arXiv:1606.03498\",\"id\":\"b36\"},\"end\":52795,\"start\":52496},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":556757},\"end\":53144,\"start\":52797},{\"attributes\":{\"doi\":\"arXiv:1607.08022\",\"id\":\"b38\"},\"end\":53433,\"start\":53146},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1541706},\"end\":53649,\"start\":53435},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":207761262},\"end\":54009,\"start\":53651},{\"attributes\":{\"id\":\"b41\"},\"end\":54219,\"start\":54011},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":6423078},\"end\":54352,\"start\":54221},{\"attributes\":{\"id\":\"b43\"},\"end\":54533,\"start\":54354},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":7698906},\"end\":54702,\"start\":54535},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":7698906},\"end\":54889,\"start\":54704},{\"attributes\":{\"id\":\"b46\"},\"end\":55054,\"start\":54891},{\"attributes\":{\"doi\":\"arXiv:1609.03126\",\"id\":\"b47\"},\"end\":55281,\"start\":55056},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1955345},\"end\":55470,\"start\":55283},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":14924561},\"end\":55734,\"start\":55472}]", "bib_title": "[{\"end\":42919,\"start\":42878},{\"end\":43172,\"start\":43091},{\"end\":43447,\"start\":43411},{\"end\":44115,\"start\":44037},{\"end\":44647,\"start\":44598},{\"end\":44860,\"start\":44817},{\"end\":45128,\"start\":45022},{\"end\":45713,\"start\":45667},{\"end\":46604,\"start\":46577},{\"end\":46887,\"start\":46872},{\"end\":47152,\"start\":47096},{\"end\":47479,\"start\":47334},{\"end\":48312,\"start\":48270},{\"end\":48498,\"start\":48419},{\"end\":49119,\"start\":49068},{\"end\":49373,\"start\":49289},{\"end\":49606,\"start\":49520},{\"end\":49811,\"start\":49757},{\"end\":50048,\"start\":49990},{\"end\":50454,\"start\":50406},{\"end\":51074,\"start\":50999},{\"end\":51611,\"start\":51582},{\"end\":52174,\"start\":52125},{\"end\":52876,\"start\":52797},{\"end\":53507,\"start\":53435},{\"end\":53723,\"start\":53651},{\"end\":54255,\"start\":54221},{\"end\":54380,\"start\":54354},{\"end\":54586,\"start\":54535},{\"end\":54755,\"start\":54704},{\"end\":55339,\"start\":55283},{\"end\":55532,\"start\":55472}]", "bib_author": "[{\"end\":42931,\"start\":42921},{\"end\":42939,\"start\":42931},{\"end\":42951,\"start\":42939},{\"end\":43185,\"start\":43174},{\"end\":43199,\"start\":43185},{\"end\":43211,\"start\":43199},{\"end\":43221,\"start\":43211},{\"end\":43233,\"start\":43221},{\"end\":43457,\"start\":43449},{\"end\":43469,\"start\":43457},{\"end\":43476,\"start\":43469},{\"end\":43486,\"start\":43476},{\"end\":43495,\"start\":43486},{\"end\":43759,\"start\":43749},{\"end\":43768,\"start\":43759},{\"end\":43777,\"start\":43768},{\"end\":43788,\"start\":43777},{\"end\":43801,\"start\":43788},{\"end\":43813,\"start\":43801},{\"end\":43823,\"start\":43813},{\"end\":43831,\"start\":43823},{\"end\":43842,\"start\":43831},{\"end\":44129,\"start\":44117},{\"end\":44141,\"start\":44129},{\"end\":44151,\"start\":44141},{\"end\":44411,\"start\":44396},{\"end\":44419,\"start\":44411},{\"end\":44660,\"start\":44649},{\"end\":44673,\"start\":44660},{\"end\":44873,\"start\":44862},{\"end\":44884,\"start\":44873},{\"end\":45139,\"start\":45130},{\"end\":45149,\"start\":45139},{\"end\":45541,\"start\":45533},{\"end\":45549,\"start\":45541},{\"end\":45558,\"start\":45549},{\"end\":45725,\"start\":45715},{\"end\":45734,\"start\":45725},{\"end\":45747,\"start\":45734},{\"end\":45759,\"start\":45747},{\"end\":45772,\"start\":45759},{\"end\":46121,\"start\":46110},{\"end\":46132,\"start\":46121},{\"end\":46142,\"start\":46132},{\"end\":46378,\"start\":46367},{\"end\":46389,\"start\":46378},{\"end\":46399,\"start\":46389},{\"end\":46620,\"start\":46606},{\"end\":46637,\"start\":46620},{\"end\":46646,\"start\":46637},{\"end\":46652,\"start\":46646},{\"end\":46668,\"start\":46652},{\"end\":46677,\"start\":46668},{\"end\":46690,\"start\":46677},{\"end\":46700,\"start\":46690},{\"end\":46902,\"start\":46889},{\"end\":46914,\"start\":46902},{\"end\":46924,\"start\":46914},{\"end\":46935,\"start\":46924},{\"end\":46948,\"start\":46935},{\"end\":47166,\"start\":47154},{\"end\":47185,\"start\":47166},{\"end\":47491,\"start\":47481},{\"end\":47505,\"start\":47491},{\"end\":47517,\"start\":47505},{\"end\":47905,\"start\":47896},{\"end\":47916,\"start\":47905},{\"end\":48129,\"start\":48118},{\"end\":48138,\"start\":48129},{\"end\":48149,\"start\":48138},{\"end\":48324,\"start\":48314},{\"end\":48330,\"start\":48324},{\"end\":48514,\"start\":48500},{\"end\":48521,\"start\":48514},{\"end\":48528,\"start\":48521},{\"end\":48536,\"start\":48528},{\"end\":48544,\"start\":48536},{\"end\":48858,\"start\":48844},{\"end\":48872,\"start\":48858},{\"end\":48883,\"start\":48872},{\"end\":49132,\"start\":49121},{\"end\":49141,\"start\":49132},{\"end\":49158,\"start\":49141},{\"end\":49381,\"start\":49375},{\"end\":49389,\"start\":49381},{\"end\":49614,\"start\":49608},{\"end\":49622,\"start\":49614},{\"end\":49821,\"start\":49813},{\"end\":49834,\"start\":49821},{\"end\":49845,\"start\":49834},{\"end\":50061,\"start\":50050},{\"end\":50072,\"start\":50061},{\"end\":50081,\"start\":50072},{\"end\":50214,\"start\":50205},{\"end\":50226,\"start\":50214},{\"end\":50466,\"start\":50456},{\"end\":50480,\"start\":50466},{\"end\":50491,\"start\":50480},{\"end\":50502,\"start\":50491},{\"end\":50513,\"start\":50502},{\"end\":50761,\"start\":50750},{\"end\":50769,\"start\":50761},{\"end\":50781,\"start\":50769},{\"end\":51089,\"start\":51076},{\"end\":51300,\"start\":51292},{\"end\":51309,\"start\":51300},{\"end\":51316,\"start\":51309},{\"end\":51330,\"start\":51316},{\"end\":51341,\"start\":51330},{\"end\":51348,\"start\":51341},{\"end\":51625,\"start\":51613},{\"end\":51638,\"start\":51625},{\"end\":51647,\"start\":51638},{\"end\":51658,\"start\":51647},{\"end\":51928,\"start\":51913},{\"end\":51939,\"start\":51928},{\"end\":51947,\"start\":51939},{\"end\":52191,\"start\":52176},{\"end\":52199,\"start\":52191},{\"end\":52205,\"start\":52199},{\"end\":52215,\"start\":52205},{\"end\":52227,\"start\":52215},{\"end\":52233,\"start\":52227},{\"end\":52242,\"start\":52233},{\"end\":52254,\"start\":52242},{\"end\":52264,\"start\":52254},{\"end\":52277,\"start\":52264},{\"end\":52508,\"start\":52496},{\"end\":52522,\"start\":52508},{\"end\":52533,\"start\":52522},{\"end\":52543,\"start\":52533},{\"end\":52554,\"start\":52543},{\"end\":52562,\"start\":52554},{\"end\":52886,\"start\":52878},{\"end\":52895,\"start\":52886},{\"end\":52905,\"start\":52895},{\"end\":52918,\"start\":52905},{\"end\":53226,\"start\":53215},{\"end\":53237,\"start\":53226},{\"end\":53250,\"start\":53237},{\"end\":53517,\"start\":53509},{\"end\":53526,\"start\":53517},{\"end\":53733,\"start\":53725},{\"end\":53744,\"start\":53733},{\"end\":53756,\"start\":53744},{\"end\":53772,\"start\":53756},{\"end\":54092,\"start\":54085},{\"end\":54101,\"start\":54092},{\"end\":54107,\"start\":54101},{\"end\":54264,\"start\":54257},{\"end\":54270,\"start\":54264},{\"end\":54389,\"start\":54382},{\"end\":54396,\"start\":54389},{\"end\":54404,\"start\":54396},{\"end\":54414,\"start\":54404},{\"end\":54425,\"start\":54414},{\"end\":54594,\"start\":54588},{\"end\":54605,\"start\":54594},{\"end\":54763,\"start\":54757},{\"end\":54774,\"start\":54763},{\"end\":54935,\"start\":54926},{\"end\":54944,\"start\":54935},{\"end\":54955,\"start\":54944},{\"end\":55109,\"start\":55101},{\"end\":55120,\"start\":55109},{\"end\":55129,\"start\":55120},{\"end\":55349,\"start\":55341},{\"end\":55359,\"start\":55349},{\"end\":55544,\"start\":55534},{\"end\":55558,\"start\":55544},{\"end\":55571,\"start\":55558},{\"end\":55582,\"start\":55571}]", "bib_venue": "[{\"end\":45270,\"start\":45218},{\"end\":51138,\"start\":51114},{\"end\":42955,\"start\":42951},{\"end\":43237,\"start\":43233},{\"end\":43529,\"start\":43495},{\"end\":43747,\"start\":43686},{\"end\":44155,\"start\":44151},{\"end\":44394,\"start\":44319},{\"end\":44681,\"start\":44673},{\"end\":44888,\"start\":44884},{\"end\":45216,\"start\":45149},{\"end\":45531,\"start\":45502},{\"end\":45803,\"start\":45772},{\"end\":46108,\"start\":46006},{\"end\":46461,\"start\":46399},{\"end\":46704,\"start\":46700},{\"end\":46956,\"start\":46948},{\"end\":47077,\"start\":47074},{\"end\":47192,\"start\":47185},{\"end\":47551,\"start\":47517},{\"end\":47894,\"start\":47802},{\"end\":48116,\"start\":48049},{\"end\":48334,\"start\":48330},{\"end\":48578,\"start\":48544},{\"end\":48842,\"start\":48782},{\"end\":49162,\"start\":49158},{\"end\":49393,\"start\":49389},{\"end\":49626,\"start\":49622},{\"end\":49849,\"start\":49845},{\"end\":50085,\"start\":50081},{\"end\":50280,\"start\":50241},{\"end\":50517,\"start\":50513},{\"end\":50748,\"start\":50656},{\"end\":51112,\"start\":51089},{\"end\":51410,\"start\":51364},{\"end\":51697,\"start\":51658},{\"end\":51911,\"start\":51848},{\"end\":52281,\"start\":52277},{\"end\":52615,\"start\":52578},{\"end\":52952,\"start\":52918},{\"end\":53213,\"start\":53146},{\"end\":53530,\"start\":53526},{\"end\":53809,\"start\":53772},{\"end\":54083,\"start\":54011},{\"end\":54274,\"start\":54270},{\"end\":54429,\"start\":54425},{\"end\":54609,\"start\":54605},{\"end\":54778,\"start\":54774},{\"end\":54924,\"start\":54891},{\"end\":55099,\"start\":55056},{\"end\":55363,\"start\":55359},{\"end\":55586,\"start\":55582}]"}}}, "year": 2023, "month": 12, "day": 17}