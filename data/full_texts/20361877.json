{"id": 20361877, "updated": "2022-03-03 17:14:35.914", "metadata": {"title": "Beyond Success Rate: Utility as a Search Quality Metric for Online Experiments", "authors": "[{\"first\":\"Widad\",\"last\":\"Machmouchi\",\"middle\":[]},{\"first\":\"Ahmed\",\"last\":\"Awadallah\",\"middle\":[\"Hassan\"]},{\"first\":\"Imed\",\"last\":\"Zitouni\",\"middle\":[]},{\"first\":\"Georg\",\"last\":\"Buscher\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "User satisfaction metrics are an integral part of search engine development as they help system developers to understand and evaluate the quality of the user experience. Research to date has mostly focused on predicting success or frustration as a proxy for satisfaction. However, users' search experience is more complex than merely being either successful or not. As such, using success rate as a measure of satisfaction can be limiting. In this work, we propose the use of utility as a measure of searcher satisfaction. This concept represents the fulfillment a user receives from con-suming a service and explains how users aim to gain optimal overall satisfaction. Our utility metrics measure the user satisfac-tion by aggregating all their interaction with the search engine. These interactions are represented as a timeline of actions and their dwelltimes, where each action is classified as having a posi-tive or negative effect on the user. We examine sessions mined from Bing logs, with multi-point scale assessment of searcher satisfaction and show that utility is a better proxy for satisfaction compared to success. Leveraging that data, we design metrics of searcher satisfaction that assess the overall utility accumulated by a user during her search session. We use real user traffic from millions of users in an A/B setting to compare utility metrics to success rate metrics. We show that utility is a better metric for evaluating searcher satisfaction with the search engine, and a more sensitive and accurate metric when compared to predicting success. These metrics are currently adopted as the top-level met-ric for evaluating the thousands of A/B experiments that are run on Bing each year.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2748827028", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cikm/MachmouchiAZB17", "doi": "10.1145/3132847.3132850"}}, "content": {"source": {"pdf_hash": "3fafd28791b05253a035a772bfb11f9def903e50", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "432fc384cef42c8f06ea0f1e9bebe34b667f597e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3fafd28791b05253a035a772bfb11f9def903e50.txt", "contents": "\nBeyond Success Rate: Utility as a Search Quality Metric for Online Experiments\n\n\nWidad Machmouchi Microsoft widadm@microsoft.com \nAhmed Hassan hassanam@microsoft.com \nAwadallah Microsoft \nImed Zitouni Microsoft \nGeorg Buscher georgbu@fb.com \nFacebook \nBeyond Success Rate: Utility as a Search Quality Metric for Online Experiments\n10.1145/3132847.3132850*The work in this paper was done during the author's employment at Microsoft. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita-tion on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy other-wise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. CIKM'17 , November 6-10, 2017, Singapore, SingaporeCCS CONCEPTS \u2022 Information systems\u2192 Retrieval efficiency \u2022 Information systems\u2192 Task models Keywords Search satisfactionevaluationutilityeffortsession\nUser satisfaction metrics are an integral part of search engine development as they help system developers to understand and evaluate the quality of the user experience. Research to date has mostly focused on predicting success or frustration as a proxy for satisfaction. However, users' search experience is more complex than merely being either successful or not. As such, using success rate as a measure of satisfaction can be limiting. In this work, we propose the use of utility as a measure of searcher satisfaction. This concept represents the fulfillment a user receives from consuming a service and explains how users aim to gain optimal overall satisfaction. Our utility metrics measure the user satisfaction by aggregating all their interaction with the search engine. These interactions are represented as a timeline of actions and their dwelltimes, where each action is classified as having a positive or negative effect on the user. We examine sessions mined from Bing logs, with multi-point scale assessment of searcher satisfaction and show that utility is a better proxy for satisfaction compared to success. Leveraging that data, we design metrics of searcher satisfaction that assess the overall utility accumulated by a user during her search session. We use real user traffic from millions of users in an A/B setting to compare utility metrics to success rate metrics. We show that utility is a better metric for evaluating searcher satisfaction with the search engine, and a more sensitive and accurate metric when compared to predicting success. These metrics are currently adopted as the top-level metric for evaluating the thousands of A/B experiments that are run on Bing each year.\n\nINTRODUCTION\n\nSearch engine evaluation is critical for guiding product development. Web search engines have been traditionally evaluated in terms of the relevance of web pages to individual queries. These evaluation methods take on a system-oriented approach where the goal is to evaluate the quality of a list of ranked documents returned by the search engine in response to a query. Such methods require a testing set of queries and ranked list of documents. Relevance of documents to queries is labeled by third party judges. Once the labels are collected, multiple metrics are defined to quantify the engine's performance (e.g. Normalized Discounted Cumulative gain (nDCG) [14], Rank Biased Precision (RBP) [25], etc.).\n\nSystem-oriented approaches have the advantages of being simple, repeatable and are generally useful for training rankers. However, they also suffer from multiple disadvantages. For example, they only focus on document ranking ignoring other elements on the Search Results Page (SERP). Moreover, they evaluate the results from a third party judge perspective, which may be quite different from the actual user experience on the search engine, especially as results become more and more personal.\n\nOn the other hand, there has been multiple studies that developed user-oriented metrics where models of actual user behavior while interacting with the system are used to evaluate the quality of the engine [8,11]. Unlike system-oriented metrics, user-oriented metrics are not reusable and cannot be easily used for training. However, they take the activities of millions of users into account and directly model user satisfaction which is what matters most for any system. Most research on user satisfaction modeling up to date has focused on the binary prediction of multiple notions of satisfaction like success, frustration, etc. In this line of work, models of user behavior are built to predict the success or frustration of the users with their search experience [7,9]. In terms of controlled A/B experiments, this results in metrics that measure the success rate of searches across millions of users and compare the success rate of a treatment group to that of a control group.\n\nIn reality, user experience is too complex to be characterized on a binary scale as either positive or negative. For example, Jiang et al. [15] has shown that introducing multiple grades of satisfaction is a more accurate way to predict the quality of the user experience. The relation between search interaction and microeconomic theories have also been studied in [2]. This showed that the process of search can be viewed as a combination of inputs (queries and assessments) that produce outputs (relevant information). This leads to the premise that users continue searching until they get enough utility or decide to give up. This in turn suggests that utility can be used as a measure of user satisfaction.\n\nIn this paper, we present and evaluate web search metrics that use utility as a proxy to assess user satisfaction. We focus on building and testing user-oriented metrics that can be used for controlled online experiments where the behavior of millions of users is used to compare user satisfaction in the treatment vs the control groups. We experiment with utility as an online metric on various controlled experiments with hundreds of millions of users and show its power in detecting treatment effects, while reducing costs of experimentation. Overall, we make the following contributions with this research:\n\n\u2022\uf020Propose a new framework for measuring a search session utility that quantifies the user's satisfaction within the session.\n\n\u2022\uf020Build a search quality metric for assessing user satisfaction for use with online A/B experiments run at Bing.\uf020\n\n\u2022\uf020Evaluate the metric using online controlled experiments showing that the metric is both accurate and more sensitive compared to existing metrics.\n\n\nRELATED WORK\n\nThere are two main areas of work related to the research presented in this paper: (i) search satisfaction modeling and (ii) online controlled experiments. In this section, we cover them in detail and discuss how our method extends this prior work.\n\n\nSearch Satisfaction\n\nMeasuring and modeling user satisfaction has received extensive attention in the literature with multiple methods that focus on deriving indicators of satisfaction or lack thereof from online user behavior [8,11]. Satisfaction has also been widely adopted as a subjective measure of the quality of the search experience. Kelly [16] defined searcher satisfaction as follows: \"satisfaction can be understood as the fulfillment of a specified desire or goal.\". Fox et al [8] used an instrumented browser to determine whether there was an association between explicit ratings of user satisfaction and implicit measures of user interest and identified the measures that were most strongly associated with user satisfaction. They found that there was a link between user activity and satisfaction ratings, and that clickthrough, dwell time, and session termination activity combined to make good predictors of satisfaction for Web pages. For example, they found out that a short dwell time is an indicator of dissatisfaction, while long dwell time is correlated more with satisfaction. Behavioral patterns were also used to predict user satisfaction for search sessions. Hassan et al. [9,11] developed models of user behavior to accurately estimate search success on a session level, independent of the relevance of documents retrieved by the search engine. Wang et al. [28] enhanced these models using conditional random fields and structured learning. Ageev et al. [1] proposed a formalization of different types of success for informational search, and presented a scalable game-like infrastructure for crowdsourcing search behavior studies, specifically targeted towards capturing and evaluating successful search strategies on informational tasks with known intent. Feild et al [7] developed methods to predict user frustration by assigning users difficult information seeking tasks and monitoring their degree of frustration via query logs and physical sensors. Perhaps the closest work to our research is the work of Jiang et al. [15] where the authors introduced the notion of graded search satisfaction and argued that they represent searcher satisfaction better than the binary scale commonly used.\n\nThe relationship between utility and satisfaction has been extensively studied in economics. Mankiw [24] described utility as \"a person's subjective measure of well-being or satisfaction\". In Web search, similar relation between utility and satisfaction can be established. For example, Su [26] defined utility in Web search as a measure of worth of search results versus time, physical effort, and mental effort expended. This suggests that we can consider searcher satisfaction as a compound measure of multiple factors, including search outcome and search effort.\n\nOur work builds a utility framework as an alternative to success for assessing user satisfaction. We also focus on using utility as a metric in online controlled experiments and show that it yields significantly better results compared to success-based binary metrics. Additionally, we perform a large scale evaluation on online experimentation data with millions of users and evaluate multiple dimensions of the metric including accuracy and sensitivity. This lead to adopting utility metrics as the OEC when running such A/B experiments in Bing.\n\n\nOnline Controlled Experiments\n\nLarge scale controlled online experiments are becoming a standard tool for testing the impact of changes of products or services and guiding product development [19]. These experiments rely on a form of statistical hypothesis testing run with two variants, control and treatment. The two variants are identical except for one variation that might affect the way users behave, hence allowing us to establish causality. The first variant could be the currently used system (control) while the other is modified in some respect (treatment). Metrics, such as the ones described earlier in this section, are then used to compare control to treatment. Controlled online experiments have been widely adopted by major search engines to evaluate changes to the system [20,27].\n\nMany studies in the literature have looked at using controlled online experiments for evaluating different aspects of the search systems including user interface [19], ranking algorithms [21], etc. Other lines of work focused on proper design and implementation of controlled online experimentation with focus on infrastructure [21], trustworthiness of outcomes [21] and general pitfalls to avoid [3]. Yet another line of work focused on improving the sensitivity of online experiments, which leads to reductions in the experiment period and the number of users affected, which in turn reduces cost and/or increases the bandwidth for running more experiments. Simple sensitivity improvement techniques looked at experiment duration and user population [3], reducing the variance of the metrics [6] and eliminating users who were not affected by the experiment [22].\n\nIn this work, we design metrics that can be used with controlled online experimentation and show that the proposed metrics can outperform metrics based on success prediction by improving the sensitivity across several large-scale controlled online experiments run on Bing. \n\n\nDATA\n\nIn this section, we describe the data that we use to create the utility metric and to evaluate it. We rely on logs collected from actual users interacting with Bing and we collect labels using both offline judgements and by analyzing A/B experiments.\n\n\nOffline Experimentation Data\n\nOur offline labeled data consists of a sample of thousands of raw search sessions from the actual usage logs of Bing. Every log entry contained an anonymized user identifier, a timestamp, a query, and all search result clicks and their associated dwell times. Automated bot traffic, intranet, and secure URLs (https) were removed at the source prior to analysis. The log entries are segmented by time into sessions. From these logs we extracted raw search sessions. Every raw session began with a search query and could contain further queries and visits to clicked search results. We used 30 minutes of idle time to demarcate sessions, as has often been performed in related studies [8,11,17].\n\nEvery session was replayed for human annotators where they got to see the submitted queries, search result pages, clicked results, subsequent reformulations, etc. Annotators were asked to rate the overall user satisfaction using a 5-point Likert scale ranging from 1 (very unsatisfied) to 5 (very satisfied). Annotators were asked to consider the overall satisfaction of the user and not just the fact that the user was able to find what she wanted or not. They were also instructed to consider factors like how much effort did the searcher incur and whether the searcher was frustrated due to unnecessary effort or irrelevant information while assessing their satisfaction.\n\nIn total, we collected data for 2000 sessions with 7031 unique queries, resulting in an average of 3.5 queries per session. We collected three labels for every session. The final label assigned to a session was computed by averaging the score of all annotators. Annotators' ratings had a Krippendorf's alpha coefficient of 0.62, indicating both reasonable agreement between annotators and the subjectivity of the annotation task. Employing third-party annotators to annotate logged search sessions is inexpensive and allows us to label relatively large amounts of data. Additionally, many previous studies [7,9,11,12,15] have shown that reliable labeled data and solid conclusions can be reached using this data collection method. An alternative approach is to collect first-party satisfaction labels in lab studies. This method has the advantage of using first-party labels, but it is hard to perform in large scale settings and it is also difficult to simulate real search scenarios [1].\n\n\nOnline Experimentation Data\n\nA/B testing has been frequently used for evaluating the quality of online systems in general and search engines in particular. A/B testing is a form of two-sample hypothesis testing used to compare two variants (A and B) of a system that are identical in all aspects except for one variation to evaluate. One variant represents the control group and the other representing the treatment group. After the experiment is performed, a pre-defined quality metric is then calculated for all users and the relative difference of the average values is calculated. Before making any conclusions based on the difference between the average values of control and treatment, a statistical significance test is applied to check if the difference is statistically significant.\n\nWe conducted 6 A/B experiments. The experiments covered a wide variety of treatments: relevance (ranking algorithm changes) experiments, answers content and UI (changes to the user interface). Each experiment ran for either 7 or 14 days. In all experiments, the treatment was known to be an improvement, or an intentional degradation compared to control. This allows us to evaluate multiple metrics on this dataset since the outcome is known. A good metric should both align with the experiment label and be sensitive to changes introduced. A sensitive metric will do that by achieving higher level of statistical significance (e.g. higher t-statistics) or by requiring less data to reach certain level of statistical significance. We will discuss this in more detail in Section 6.\n\nTo verify the correctness of the A/B experiments, we conducted A/A tests for each experiment before users in the treatment are exposed to the change. A/A tests are control experiments that compare two identical versions of the search engine. The A/A test, sometimes also called a Null test, is intended to test the experimentation system. The Null hypothesis should be rejected no more than 5% of the time (when a 95% confidence level is used). We conducted A/A tests by randomizing users multiple times and assigning them to control/treatment and evaluating the hypothesis that they are the same. No A/A tests have failed for any of the A/B experiments in our dataset.\n\nWhile hundreds of experiments are performed on the search engine during a month, we opted to consider only few experiments that represent the various changes that can be introduced to the search engine and we deep-dived on each experiment to verify the metrics movements are expected given the change introduced. The results for each of these experiments are representative of results we observed on other experiments that affected the same aspect of the search engine. Moreover, each of these experiments were run at least twice to reproduce the movements we see in the metrics.\n\nWe collected the usage logs of users that were involved in these experiments. Like the labeled data, every log entry contained an anonymized user identifier, a timestamp, a query, and all search result clicks and their associated dwell times. We applied the same rules for filtering the data and segmenting it into sessions/tasks as described earlier.\n\n\nBENEFIT AND FRUSTRATION\n\nWe hypothesize that assessing searcher satisfaction by taking both benefit and frustration into consideration is a better way to build search quality metrics for online experimentation compared to metrics that focus only on benefit to measure success. To prove this hypothesis, we start by analyzing our offline labeled data in order to understand which signals denote user benefit and which signals denote user frustration.\n\nWe start our analysis by defining few concepts that will be frequently used throughout the rest of this paper.\n\nSatisfaction. Satisfaction is the searcher's subjective measure of utility of a search engine. The higher the system utility is for the searcher, the more satisfied the searcher is.\n\nSession 4A: Evaluation CIKM'17, November 6-10, 2017, Singapore\n\nSuccess. Success represents a notion of goal completion. Users are successful if they had accomplished the information need underlying their searches.\n\nBenefit. Benefit is the value of search results to the searcher, i.e., the quantity of useful information found [15]. Benefit and Success are closely related since success in accomplishing a goal directly depends on the benefits the searcher accumulates.\n\nFrustration. Searchers develop frustration when their search process is impeded [15]. When searchers have trouble fulfilling their information needs, they may become frustrated, even if they are ultimately successful.\n\nUtility is a measure of worth of search results versus time, physical effort, and mental effort expended [26].\n\nTo understand the relation between satisfaction, benefit and frustration, we begin by examining the relationship between satisfaction and different signals characterizing both benefit and frustration. We directly measure satisfaction using annotators' average ratings. We came up with many signals characterizing user behavior including clicks, dwell time, queries and query reformulation that can be derived from the logs that we collect from user interactions with Bing. In the rest of this section, we discuss the feature groups that correlate the most with benefit (more satisfaction) or with frustration (less satisfaction). We compute the correlation of the various signals and their with satisfaction Pearson's coefficient. We report various levels of satisfaction: Very Satisfied/Satisfied, Satisfied/Somewhat Satisfied and Somewhat Satisfied/Unsatisfied.\n\n\nBenefit Signals\n\nWe see from the Table 1 that clicks and dwell time are the signals mostly correlated with satisfaction. Although if we just consider the total number of clicks, we do not see a strong correlation with satisfaction. We think that this because the total number of clicks counts all clicks regardless of the amount of time the searcher spent on the landing page, which may result in combining multiple contradicting signals as we will see later.\n\nInstead of the total number of clicks, we focus on clicks with dwell time greater than a certain threshold. We use 30, 50 and 100 seconds as threshold values. A dwell time of 30 seconds has in particular been frequently used in the literature [8] to define clicks that are more likely to result in success. We notice stronger positive correlation with satisfaction when we introduce the dwell time thresholds. We also notice higher positive correlation with higher time thresholds. This indicates that when users spend more time on the landing page, they are likely to gain more relevant information which increases their overall satisfaction. We can also see this when we consider click dwelltime in general. We looked at the average and total time spent on the landing pages after each click for all clicks in a given session. These dwellime characteristics correlate positively with satisfaction, supporting our earlier observation on how clicks and durations after clicks are a strong indicator of users' satisfaction. \n- -0.13 - - Long Dwell Time Clicks TotalNumClicks (t>=30) 0.11 - - - TotalNumClicks (t>=50) 0.11 - - - TotalNumClicks (t>=100) 0.15 - - - Dwell Time AvgClickDwellTime 0.23 0.18 - - TotalClickDwellTime 0.18 0.15 - -\n\nFrustration Signals\n\nIn addition to the positive signals described, there are many other signals that point to user frustration. Table 2 shows a list of signals that had the strongest negative correlation with satisfaction.\n\nThe first category of signals focuses on queries. We hypothesize that the increased number of queries during a session may indicate that the searcher is struggling with finding what she wants and hence she is more frustrated and less satisfied. We see that indeed an increased number of queries is negatively correlated with dissatisfaction. However, not all queries are created equal. To differentiate between exploratory queries and struggling queries, we look at query reformulation. Query reformulation is the act of rewriting a previous query in hope of retrieving better results [10]. As a proxy for query reformulation, we use two signals that assess the similarity between queries in the session: average Levenshtein edit distance and average number of characters/words between all pairs of queries. In Table 2, we see that all these signals are indicative of user frustration. Note that edit distance is positively correlated with satisfaction while other signals are negatively correlated, since edit distance is a distance (inverse similarity) measure while all others are measures of similarity.\n\nAnother category of signals that we considered is clicks with short dwell time. We focus on clicks with dwell time below a certain threshold. We use 5, 10, 15 and 20 seconds as threshold values. We hypothesize that the user not only finds these clicks useless but also, they contribute to more searcher frustration and hence less satisfaction. The data shows us that indeed quick back clicks have negative correlation with satisfaction. Negative correlation is higher for clicks with dwell time less than 5 and 10 seconds compared to clicks with less than 15 or 20 seconds. ---0.11 -TotalNumClicks(t<20) -0.09 ---In this section, we described multiple signals that contribute to user benefit and frustration. In the next section, we describe how we combine these signals into a single metric that we can use to measure search quality in online A/B experiments.\n\n\nUTILITY METRIC\n\nMany query-level search metrics assume a user model where the user starts at the first document and works their way down the list, eventually stopping when they find what they want or become tired or bored [13,14]. Other work [16] on session-level evaluation assumes that, at each decision point, the user can decide to either stop the search or reformulate the query.\n\nOur metric is different from these metrics in three ways. First, and like some of the models discussed above, we model user satisfaction at the session level as opposed to the query or the document level. This has several benefits as discussed in [11]. Second, we do not have any relevance labels and hence we need to rely on events in the user behavior trail in our logs to assess satisfaction. Third, we need to model both the positive user value (benefit) as well as the negative user value (frustration).\n\nBuilding on previous work and taking these constraints into consideration, we assume a user model where the user accumulates both positive and negative values throughout her search session. The positive value comes from relevant document and answers while the negative value comes from wasted effort and irrelevant results. Users keep searching (examining document and reformulating queries) till they meet one of two conditions: 1) users collect enough positive value to meet their information needs and end their search, or 2) users accumulate enough negative value till they decide to give up. Regardless of whether the users' information needs were met or not, the users may have different levels of satisfaction with their experience with the search engine. We hypothesize that this satisfaction does not only depend on the positive value (benefit) but also the negative value (frustration).\n\nTo quantify the positive and negative values the user accumulates, we use time as the economic unit that the user trades on the search engine. The benefit or frustration that the user develops with every action depends on the time she spends on this action. User actions are arranged on a timeline, ordered by the timestamp where the action took place. Next, depending on whether the action would cause the user benefit or frustration, the intervals created by the timestamps are assigned a positive or negative weight. Utility increases as the user accumulates more benefit (positive actions) and decreases as the user accumulates more frustration (negative actions). In the following subsections, we describe this process in more detail.\n\n\nFormal Definition of Utility\n\nWe define a user timeline as an interval of time where the user actions on the search engine are laid out in an ascending order in terms of their timestamps as a set of events , = 1 , \u2026 , representing the user actions, such that occurs before +1 . A search session would be such a timeline.\n\nTo quantify the positive or negative effect the event has on the user's satisfaction, we define a real function that we call Weight that maps an event to a value between -1 and 1: Using the two functions, Weight and Payout, the Utility of the timeline T is given by:\n( ) = \u2211 ( ) * ( ) \u2208T .\nSince it is desirable for a satisfaction measure to be bounded and to avoid biasing the metric towards longer timelines, we compute the Utility Rate of a timeline T by normalizing the Utility value by the overall time the user spent in the timeline:\n(T) = \u2211 ( ) * ( ) \u2208T \u2211 1 * ( ) \u2208T (1)\nThe utility rate is a metric in the interval [-1, 1] (where -1 denotes the worst outcome and 1 denotes the best outcome) and is computed as a rate over time, i.e., utility per second spent searching. It estimates how much value the user derived from their interactions with the search engine per second of time. The metric trades off positive user value (benefit) against negative user value (frustration). For every user action, the weight specifies whether the type of action is creating positive user value (e.g., last clicks in the session) or negative user value (e.g., quickbacks).\n\n\nEstimating the Events Weights\n\nAn important component of the utility metric is the weight function that maps every type of event to a real value between -1 and 1. The values of these weights correlate with the user satisfaction (or dissatisfaction) level associated with the event. Hence, to estimate the weights of each event, we build a linear regression model to predict the satisfaction level of each session using the events defined in the previous subsections as actions. The model is trained using the offline data explained in Section 3.1. Once we have the feature weights for different events, we normalize them to values between -1 and 1, associating the events most correlated with satisfaction with weight 1 and those most correlated with dissatisfaction with weight -1. One point to note is that the events we study consider all the logged interactions Session 4A: Evaluation CIKM'17, November 6-10, 2017, Singapore the user has with the search engine, from clicks to queries and the time the user spent before and after those interactions. As interactions with the search engine get richer, the utility metric allows the addition of new events, in such a way that their weight can be determined by similar offline analysis or other means. Table 3 shows the most important utility events we use. They are ordered in a descending order of the absolute value of their weights and their type reflect whether they contribute positively or negatively to the overall utility of a timeline. Their order of importance is directly proportional to their normalized weight. For example, a click to an external website that has a long dwelltime has a higher positive contribution to the utility than the negative contribution of issuing a query that doesn't get reformulated. Table 3 A list of user actions that are included in utility. The row rank reflects a descending order in the absolute value of the weight associated with the events.\n\nThe weights of the utility events reflect our a-priori notion of success and failure on a search engine. External clicks with long dwelltime have been associated with user's satisfaction [8]: the longer the time the user spends on the external page, the stronger the indication that the user finds the information there useful. Hence accounting for the dwelltime in the metric computation as a payout of the click event will help further specify the satisfaction the user is getting from the page. Similarly, the time the user spends on the search result page after they issue a query is a time spent exerting an effort to examine the results and finding the document on the page that is most relevant to their search need. Hence, counting the dwelltime of the query as negative captures the effort the user spent on the page. Moreover, a reformulated query is a strong indicator of user dissatisfaction with the results that were presented [10]. This dissatisfaction was incorporated into the utility metric by assigning a more negative weight to a reformulated query compared to a non-reformulated one.\n\n\nEVALUATION\n\n\nEvaluation Setup\n\nWe performed multiple experiments to evaluate utility rate as a way of measuring user satisfaction. We evaluated the metric using real-world A/B experiments where a change was introduced to Bing. These A/B experiments affected multiple aspects of the search results page and they involved real search log data from millions of users on Bing. The online experiments data used for evaluation was described in detail in Section 3.2. Each experiment has a positive/negative label that reflects the treatment on the user's satisfaction. The label was determined by a team of analysts and feature experts: they examined the experiment data as well as the users' sessions and their behavior after they encountered the change. They also looked at basic engagement metrics and offline metrics to determine the label. Moreover, each experiment was run twice to confirm the stability of the metrics movements that informed the label.\n\n\nMetrics to Evaluate\n\nAs explained in Section 3, the users' traffic in all these experiments were split among sessions and for each session, a quality measure is computed. For each user, the quality measure is averaged across the sessions of that user. Finally, the quality measure is computed for the A and B variants separately by averaging across the users of each variant. The relative delta in the measure is then calculated and a statistical Student's t-test is performed to measure the statistical significance of the change in the quality metric. The statistical significance threshold for p-values was 0.05.\n\nTo build a binary query success prediction model that can be used in our baseline metric, we train a logistic regression model adopting features that has already been used before in the literature [10,28,26]. More specifically, we use query features (query length, number of terms in the query, lexical similarity between the current and next query and time in seconds between the current and next query), click features (number of clicks, click dwell time) and session features (time in seconds from the beginning of the session and query position in the session). All features are computed at the query-level and at the session-level to correlate the queries and their success across the session. Since this is a binary classifier, we mark a query as successful only if its average satisfaction label is greater than 0.5. We use 10-fold cross validation. For each training fold, we use grid search to optimize for the training algorithm parameters. This model has an accuracy of 67.7%, a precision of 69% and a recall of 95%.\n\nWe compute four metrics on the labeled online experiments: three metrics that implement different success predictors based query success predictor described above and the utility metric. Note that while sessions-per-user is a metric used to evaluate users' loyalty to the product in the literature, e.g. [5,4], it is not a sensitive metric and is very hard to move in search engines experiments [4]. In fact, for all the experiments we study, the sessions-per-user metric does not move in a statistically significant way.\n\n1. Utility Rate: To compute the utility of every session, we apply equation (1) in Section 5.1, where the timeline T is taken to be the user's session.\n\n2. Success Rate (Any): At least 1 successful event in the session: this metric considers a session successful if one or more positive events (see Section 5) associated with user action are present (e.g. a long dwell time click). If the session has no such positive event, it is considered unsuccessful.\n\n3. Baseline (Any): At least 1 successful query in the session: this metric considers a session successful if any query in the session is successful according to the query success predictor we described earlier.\n\n4. Baseline (Avg): Average query success in the session: this metric considers a session successful if the average query success in the session is at least 0.5, where query success is determined by predictor we described earlier.\n\n\nEvents\n\nType A click to an external page that is the last interaction in the user's session strongly positive A query issued by the user that is followed by a query that reformulates it strongly negative A click to an external page with a long dwelltime positive A click to an external page with a short dwelltime negative A query issued by the searcher that is not followed by a reformulation weakly negative Session 4A: Evaluation CIKM'17, November 6-10, 2017, Singapore\n\n\nEvaluation Criteria\n\nBefore diving into the different dimensions we compare the metrics on, it is important to explain the difference between evaluating a success predictor and an online A/B metric. As the authors explain in [23], an online A/B metric is different than a success predictor in the following aspects:\n\n\u2022\uf020While recall and precision are important evaluation criteria for a success predictor, they are not the most important when evaluating an online metric. For an online A/B metric, critical evaluation criteria are directional correctness and sensitivity, i.e., the ability to correctly identify which variant (A or B) is better, and to do so with as little experimental data (statistical power) as possible. It is acceptable that an online metric systematically errs on some instances of user sessions if it can measure the difference in user behavior induced by the A and B variants \u2022 A satisfaction predictor can depend on a host of complex features and work like a black-box. However, a desired property of an online metric is simplicity and explainability: search engine engineers want to know why the metric moved the way it did for an experiment to inform their next steps. Also, almost no metric is fail-safe, so it is critical to be able to verify that an individual experiment does not exploit a loophole in the metric due to improper instrumentation, for example.\n\nTaking the above into account, we compare the metrics on accuracy and sensitivity as used in the literature [4,23].\n\nAccuracy: One main criteria for measuring the performance of a metric is its correctness in identifying the true effect of a change on the search engine. If a metric measures the user's satisfaction, it is correct when it moves positively (or negatively) in a statistically significant way when the experiment has a positive (or negative) effect on the user. We measure accuracy as the proportion of experiments the metric has a stat-sig movement that agrees with the experiment label. Since all our metrics are measuring user satisfaction, a positive movement in the metric will reflect a positive effect on users.\n\nSensitivity: In addition to agreement with the experiments labels, we want the metric to be sensitive to changes affecting the search engine. We measure sensitivity by the level of statistical significance the metric achieves on the experiments. We do this in two different ways:\n\n1.\n\nWe compare the average of the absolute value of the t-statistic for each metric, where the average is taken across the all experiments we consider. A higher (absolute value of a) t-statistic implies that the metric can detect changes between treatment and control more confidently than one with a smaller t-statistic. The reason for that is that the metrics we consider have the same sample size since they are all computed at the session-level for each user. Hence, the variation in the t-statistic values is due to how much change is detected by the metric through its mean and standard deviation in treatment and control.\n\n2.\n\nWe compare the movement of the metrics when reducing the duration and the number of users, separately, for an individual experiment. The delta of a more sensitive metric will turn statistically significant earlier (in terms of experiment duration) than a less sensitive one. This implies that we can run our experiment for a shorter period and on a smaller subset of our users to detect the changed caused by the treatment. This is very important for search engine developers since it implies that they can run more experiments within the same experimentation budget.\n\n\nAgreement with the Experiments Labels\n\nFirst, we look at the rate of agreement of each metric with the label of an experiment. A metric movement is unclear if the statistical significance test reported a p-value above 0.05. A metric agrees (or disagrees) with the label when the sign of the statistically significant relative delta of the metric agrees (or disagrees) with the label of the experiment. Table 4 shows the agreement rate for the four metrics we are studying. The table shows that the Utility Rate had a larger number of agreements with the labels of the online experiments, compared to both Success Rate (Any) and Baseline (Any) and Baseline (Avg). This implies that the Utility Rate is more accurate than the other metrics in properly capturing changes to the search engine. \n\n\nSensitivity analysis\n\nTo measure the sensitivity of the different metrics, we compute the average of the absolute value of the t-statistic across the online experiments. A higher average t-statistic reflect a higher sensitivity since the higher the t-statistic computed for a metric on an experiment, the smaller the p-value and hence the higher confidence in the experiment's outcome. Table 5 compares the utility rate against the three metrics in terms of their sensitivity. The utility rate has a higher average t-statistic. Hence, we conclude that it is a more sensitive metric to pick up changes in Bing that effect the user's search satisfaction. 1.613\n\n\nExperiment Duration and Number of Users\n\nTo further compare the sensitivity of the different metrics, we consider one experiment out of the 6 experiments in our set and compute the metrics at (i) increasing durations and (ii) increasing number of users to identify the duration and sample size beyond which a metric's movement becomes statistically significant. A metric whose movement turns stat-sig earlier or with fewer number of users is more sensitive and hence more cost-effective since it allows experiments to be run for shorter periods of time or with smaller number of users. This increases the number of experiments that can be run on the search engine, thus leading to faster improvements.\n\n\nMetric\n\nAgree Unclear Total Utility Rate 6 (100%) 0 6 Success Rate (Any) 4 (67%) 2 6 Baseline (Any) 1(17%) 5 6 Baseline (Avg) 3(50%) 3 6\n\nSession 4A: Evaluation CIKM'17, November 6-10, 2017, Singapore\n\nThe experiment we consider introduces a positive change in the ranking algorithm of the web results shown on the search results page and all the metric deltas that are statistically significant agree with the experiment label.  Table 6 shows the p-value of the various metrics' movements when computing them on the experiment data after 1, 2, 3, 5, and 7 days from the experiment's start. The utility rate metric turns statistically significant after day 2 of the experiment, with the p-value coming close to 0 after 7 days. The other metrics turn statistically significant starting from the day 3, day 5 or not at all.\n\nSimilarly, Table 7 shows the p-value of the metrics' movements when computing them on a ranking relevance experiment with 10%, 25%, 50%, 75% and 100% of all the users in the experiment, all randomly chosen. The utility rate metric turns statistically significant after including 25% of the users in the experiment, with the p-value coming close to 0 with 100% of the users. The other metrics turn statistically significant after including 75% of the users, 100% of the users or not at all. \n\n\nUtility Rate Metric on A/A Experiments\n\nWe also test the utility metric on robustness against A/A tests. We computed the utility rate metric on a host of A/A experiments, where users in both variants of the experiment are exposed to the same experience. In such cases, a robust metric shouldn't have statistically significant movements beyond what is expected based on the p-value threshold used. For example, our p-value threshold is set at 0.05, hence we would expect the utility rate metric to have statistically significant movements in around 5% of the experiments run. We ran 514 experiments over different durations (2, 5, 6 and 14 days) with millions of users in each experiment. Out of these experiments, the utility rate metric had statistically significant movements on 16 of them, or around 3.1% of the experiments. This is less than the 5% that corresponds to our p-value threshold of 0.05. This indicates that the utility rate metric is robust against noise and is reliable to detect true treatment effects.\n\n\n7\n\n\nDISCUSSION AND IMPLICATIONS\n\nIn this paper, we have introduced and studied a novel utility framework as the basis for search engine user satisfaction metrics.\n\nThe key concept that makes the utility framework different from other metrics is that it considers the entire time the user spent during a search process, and assigns positive and negative user value to the that time, depending on observed events. In contrast, success-focused metrics are blind to the effort exerted by the user and hence ignore changes to the search engine that help achieve success with less struggling [10,18,26].\n\nWe have shown that utility yields superior results than state-ofthe-art user satisfaction metrics in terms of both accuracy, directional correctness, and sensitivity on A/B experiments affecting Bing. Accuracy and directional correctness are critical to lead engineering efforts towards improvements of the search engine. Sensitivity is important as it directly relates to agility in experimentation: metrics with higher sensitivity require less users to participate in an experiment for shorter durations to determine statistically significant movements, which means that more experiments can be conducted [21].\n\nWhile the utility metric has higher complexity than simple binary success predictors, it is a simple and debuggable metric. It has the desirable property that it is a linear combination of contributions of various types of events. Hence, an additive decomposition can be created listing in detail the exact amount each event type contributed to the final value and the A/B delta of the metric. This makes it possible to isolate the effect of individual types of interactions in an experiment (for example, long dwell time clicks vs. short dwell time clicks vs. queries), and therefore it can yield actionable insights to search engine engineers as to how to improve the treatment technique in an experiment. Interpretability and compositionality are desirable properties for online experimentation metrics [22].\n\nThe utility framework also allows for easy addition of new events, which is an important aspect given that the user interface and the type of interactions with SERPs evolve over time. Once a weight has been determined for a new event (e.g., by data analysis such as done in this paper, or by policy), it can be added to the user timeline of events considered by the metric, and the metric framework will automatically account for the new event's contribution to the final metric value.\n\nWhile we have studied the utility framework only in the context of a search engine, it can also be more generally applied to understand user satisfaction for interactive online services. The utility metric provides a general satisfaction measurement framework applicable to any online service where the user's interaction can be represented as a timeline of events. The weights of these events will be specific to the service and should be determined by an offline analysis that correlates them with the satisfaction level of the user. All is needed is to identify its weight and then place it on the user's timeline among the other interactions. The payout is defined as the economic unit that the user Session 4A: Evaluation CIKM'17, November 6-10, 2017, Singapore uses to interact with the service. For a social platform website, for example, time could be chosen as the economic unit the user exchanges with the service, and \"like\" and \"share\" actions could receive positive weights whereas \"hide\" actions could receive negative weights, etc. Generally, once the economic unit of the online service and the weights of the interactions are determined, the utility metric can be meaningfully computed on any timeline definition that is the most relevant to the service.\n\n\nCONCLUSION\n\nDeveloping accurate and sensitive search quality metrics is key to the successful development and improvement of search engines. Search engines, and many other online services, heavily depend on online controlled experiments to compare different variants of their systems and decide how to drive development toward the north star. For any such online experiment, a reliable metric is needed to assess the effect of the treatment and compare the two variants being studied. Most work in the literature has been focused on predicting success or frustration on binary level. Following that, many online experimentation metrics described in the literature focused on using signals that measure success rate at the session, task or query level. Recent work in this area has been showing that search satisfaction is a complex construct that goes beyond a binary success/failure label. We built on these finding by introducing utility rate as an online experimentation metric for measuring search quality. We combined signals of benefit and frustration to assess the overall satisfaction of the users as they interact with the search engine. We used time as the economic unit that the user trades on the search engine to quantify the positive and negative values users accumulate in their searches. We performed large scale testing on many A/B testing experiments with millions of users to show that the proposed metric is both more accurate and more sensitive than existing metrics.\n\n\nW: E \u2192 [\u22121,1] , where E is the set of all possible events representing user actions and ( ) represents our confidence in the positive or negative effect of the event on the user. The contribution of an event to the satisfaction or dissatisfaction of the user depends on the duration of time it took the user to do another action. This time duration captures the dwelltime of the event. We call this function Payout: Payout: T \u2192 \u211d , where ( ) = ( +1 ) \u2212 ( ) for all < and a constant otherwise.\n\nTable 1\n1Benefit Signals and Correlation with Satisfaction labels. \"-\" denotes values that are not statically significant.Signals \nCorrelation with Satisfaction \nlabels \nAll \nVS/S S/SS \nSS/US \nClicks \nTotalNumClicks \n\n\nTable 2\n2Frustration Signals and Correlation with Satisfaction. \"-\" denotes values that are not statically significant.Signals \nCorrelation with Satisfaction \nAll \nVS/S \nS/SS \nSS/US \nQueries \nNumQueries \n-0.24 \n-\n-0.11 \n-0.18 \n\n\n\nTable 4\n4Number of experiments each metric agrees with out of 6 experiments.\n\nTable 5\n5The average absolute t-statistic for the different metrics across the different online experiments.Metric \n( \n( \u2212 \n)) \nUtility Rate \n3.796 \nSuccess Rate (Any) \n2.176 \nBaseline (Any) \n1.203 \nBaseline (Avg) \n\n\nTable 6 P\n6-values of the deltas of the metrics computed after 1, 2, 3, 5 and 7 days from the experiment's start.Metric \n1-day \n2-day \n3-day \n5-day 7-day \nUtility \nRate \n0.480 \n0.005 \n0.001 \n0.000 \n0.000 \n\nSuccess \nRate (Any) \n0.678 \n0.259 \n0.197 \n0.019 \n0.0031 \n\nBaseline \n(Any) \n0.574 \n0.520 \n0.567 \n0.250 \n0.263 \n\nBaseline \n(Avg) \n0.827 \n0.074 \n0.01 \n0.006 \n0.011 \n\n\n\nTable 7 P\n7-values of the deltas of the metrics when computed with 10%, 25%, 50%, 75% and 100% of all the users in the experiment.Metric \n10% \n25% \n50% \n75% \n100% \nUtility Rate \n0.160 \n0.026 \n0.002 \n0.000 \n0.000 \nSuccess Rate \n(Any) \n0.495 \n0.281 \n0.127 \n0.062 \n0.031 \n\nBaseline \n(Any) \n0.724 \n0.576 \n0.429 \n0.333 \n0.263 \n\nBaseline \n(Avg) \n0.419 \n0.202 \n0.071 \n0.027 \n0.011 \n\n\nAcknowledgmentsThe authors would like to thank Nick Craswell and the reviewers for feedback on this paper.\nFind it if you can: a game for modeling different types of web search success using interaction data. M Ageev, SIGIR. 11Ageev, M et al. 2011. Find it if you can: a game for modeling different types of web search success using interaction data. In SIGIR '11: 345-354.\n\nModelling interaction with economic models of search. L Azzopardi, SIGIR. 14L. Azzopardi. 2014. Modelling interaction with economic models of search. In SIGIR'14: 3-12.\n\nSeven pitfalls to avoid when running controlled experiments on the web. T Crook, KDD'09. T. Crook et al. Seven pitfalls to avoid when running controlled experiments on the web. In KDD'09, 1105-1114, 2009.\n\nMeasuring Metrics. P Dmitriev, X Wu, CIKM'16. P. Dmitriev, and X.Wu. 2015. Measuring Metrics. In CIKM'16\n\nPractical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics. A Drutsa, A Ufliand, G Gussev, CIKM'15. A. Drutsa, A. Ufliand and G. Gussev. Practical Aspects of Sensitivity in Online Experimentation with User Engagement Metrics. In CIKM'15. 2015\n\nEngagement periodicity in search engine usage: analysis and its application to search quality evaluation. A Drutsa, G Gusev, P Serdyukov, WSDM'15. A. Drutsa, G. Gusev, and P. Serdyukov. Engagement periodicity in search en- gine usage: analysis and its application to search quality evaluation. In WSDM'15, 27-36, 2015.\n\nPredicting searcher frustration. H Feild, SIGIR. 10H. Feild et al. Predicting searcher frustration. In SIGIR'10: 34-41, 2010.\n\nEvaluating implicit measures to improve web search. S Fox, ACM TOIS. 232S. Fox et al. Evaluating implicit measures to improve web search. ACM TOIS, 23(2): 147-168, 2005.\n\nA semi-supervised approach to modeling web search satisfaction. A Hassan, SIGIR. 12A. Hassan. 2012. A semi-supervised approach to modeling web search satis- faction. In SIGIR'12: 275-284.\n\nBeyond clicks: Query reformulation as a predictor of search satisfaction. A Hassan, CIKM. 13A. Hassan et al. 2013. Beyond clicks: Query reformulation as a predictor of search satisfaction. In CIKM'13: 2019-2028.\n\nBeyond DCG: User behavior as a predictor of a successful search. A Hassan, WSDM. 10A. Hassan et al. 2010. Beyond DCG: User behavior as a predictor of a success- ful search. In WSDM'10: 221-230.\n\nHow well does result relevance predict session satisfaction?. S B Huffman, M Hochster, SIGIR'07. S. B. Huffman and M. Hochster. 2007. How well does result relevance predict session satisfaction? In SIGIR'07: 567-574.\n\nIR evaluation methods for retrieving highly relevant documents. K J\u00e4rvelin, J Kek\u00e4l\u00e4inen, SIGIR'00. K. J\u00e4rvelin and J. Kek\u00e4l\u00e4inen. 2000. IR evaluation methods for retrieving highly relevant documents. In SIGIR'00: 41-48.\n\nDiscounted cumulated gain based evaluation of multiple-query IR sessions. K J\u00e4rvelin, ECIR'08. K. J\u00e4rvelin et al. 2008. Discounted cumulated gain based evaluation of multi- ple-query IR sessions. In ECIR'08: 4-15.\n\nUnderstanding and Predicting Graded Search Satisfaction. J Jiang, WSDM. 15J. Jiang et al. 2015. Understanding and Predicting Graded Search Satisfaction. In WSDM'15: 57-66.\n\nEvaluating multi-query sessions. E Kanoulas, SIGIR. 11E. Kanoulas et al. 2011. Evaluating multi-query sessions. In SIGIR'11: 1053- 1062.\n\nMethods for evaluating interactive information retrieval systems with users. Foundation and Trends in Information Retrieval. D Kelly, 3D. Kelly. 2009. Methods for evaluating interactive information retrieval sys- tems with users. Foundation and Trends in Information Retrieval, 3(1-2): 1- 224.\n\nModeling dwell time to predict click-level satisfaction. Y Kim, WSDM. 14Y. Kim et al. 2014. Modeling dwell time to predict click-level satisfaction. In WSDM'14: 193-202.\n\nControlled experiments on the web: survey and practical guide. R Kohavi, Data Mining and Knowledge Discovery. 181R. Kohavi et al. 2009. Controlled experiments on the web: survey and practi- cal guide. Data Mining and Knowledge Discovery. 18(1): 140-181.\n\nTrustworthy online controlled experiments: Five puzzling outcomes explained. R Kohavi, KDD'12. 15R. Kohavi et al.. Trustworthy online controlled experiments: Five puzzling outcomes explained. In KDD'12, 786-794, 2012. [15]\n\nOnline controlled experiments at large scale. R Kohavi, KDD'13. R. Kohavi et al. 2013. Online controlled experiments at large scale. In KDD'13, 1168-1176.\n\nSeven rules of thumb for web site experimenters. R Kohavi, KDD'14. R. Kohavi et al. Seven rules of thumb for web site experimenters. In KDD'14, 2014.\n\nPrinciples for the design of online A/B experiments. W Machmouchi, G Buscher, SIGIR. 16W. Machmouchi and G. Buscher. 2016. Principles for the design of online A/B experiments. In SIGIR'16: 589-590\n\nPrinciples of Macroeconomics. South-Western Cengage Learning. G Mankiw, G. Mankiw. 2010. Principles of Macroeconomics. South-Western Cengage Learning.\n\nRank-biased precision for measurement of retrieval effectiveness. A Moffat, J Zobel, ACM Trans. Inf. Syst. 271A. Moffat and J. Zobel. 2008. Rank-biased precision for measurement of re- trieval effectiveness. ACM Trans. Inf. Syst. 27(1).\n\nA comprehensive and systematic model of user evaluation of Web search engines. L T Su, JASIST. 54L. T. Su. 2003. A comprehensive and systematic model of user evaluation of Web search engines. In JASIST, 54(13): 1175-1192.\n\nOverlapping Experiment Infrastructure: More, Better, Faster Experimentation. D Tang, 10D. Tang, et al. 2010. Overlapping Experiment Infrastructure: More, Better, Faster Experimentation. In KDD'10.\n\nModeling action-level satisfaction for search task satisfaction prediction. H Wang, SIGIR. 14H. Wang et al. 2014. Modeling action-level satisfaction for search task satis- faction prediction. In SIGIR'14: 123-132.\n", "annotations": {"author": "[{\"end\":130,\"start\":82},{\"end\":167,\"start\":131},{\"end\":188,\"start\":168},{\"end\":212,\"start\":189},{\"end\":242,\"start\":213},{\"end\":252,\"start\":243}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":99},{\"end\":143,\"start\":137},{\"end\":187,\"start\":178},{\"end\":211,\"start\":202},{\"end\":226,\"start\":219},{\"end\":251,\"start\":243}]", "author_first_name": "[{\"end\":87,\"start\":82},{\"end\":98,\"start\":88},{\"end\":136,\"start\":131},{\"end\":177,\"start\":168},{\"end\":193,\"start\":189},{\"end\":201,\"start\":194},{\"end\":218,\"start\":213}]", "author_affiliation": null, "title": "[{\"end\":79,\"start\":1},{\"end\":331,\"start\":253}]", "venue": null, "abstract": "[{\"end\":2931,\"start\":1223}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3614,\"start\":3610},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3648,\"start\":3644},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4363,\"start\":4360},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4366,\"start\":4363},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4926,\"start\":4923},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4928,\"start\":4926},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5283,\"start\":5279},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5509,\"start\":5506},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7350,\"start\":7347},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7353,\"start\":7350},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7472,\"start\":7468},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7612,\"start\":7609},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8323,\"start\":8320},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8326,\"start\":8323},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8509,\"start\":8505},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8605,\"start\":8602},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8921,\"start\":8918},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9176,\"start\":9172},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9449,\"start\":9445},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9639,\"start\":9635},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10659,\"start\":10655},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11257,\"start\":11253},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11260,\"start\":11257},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11429,\"start\":11425},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11454,\"start\":11450},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11595,\"start\":11591},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11629,\"start\":11625},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11663,\"start\":11660},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12018,\"start\":12015},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12060,\"start\":12057},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12127,\"start\":12123},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13382,\"start\":13379},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13385,\"start\":13382},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13388,\"start\":13385},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14676,\"start\":14673},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14678,\"start\":14676},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14681,\"start\":14678},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14684,\"start\":14681},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14687,\"start\":14684},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15055,\"start\":15052},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19319,\"start\":19315},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19543,\"start\":19539},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19787,\"start\":19783},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21363,\"start\":21360},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23171,\"start\":23167},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24780,\"start\":24776},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24783,\"start\":24780},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24800,\"start\":24796},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25191,\"start\":25187},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30714,\"start\":30711},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31469,\"start\":31465},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33405,\"start\":33401},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33408,\"start\":33405},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33411,\"start\":33408},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34540,\"start\":34537},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34542,\"start\":34540},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34631,\"start\":34628},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36361,\"start\":36357},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37634,\"start\":37631},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":37637,\"start\":37634},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":44830,\"start\":44826},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44833,\"start\":44830},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":44836,\"start\":44833},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":45450,\"start\":45446},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":46263,\"start\":46259}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50010,\"start\":49516},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":50229,\"start\":50011},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":50459,\"start\":50230},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50537,\"start\":50460},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50754,\"start\":50538},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51125,\"start\":50755},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51503,\"start\":51126}]", "paragraph": "[{\"end\":3656,\"start\":2947},{\"end\":4152,\"start\":3658},{\"end\":5138,\"start\":4154},{\"end\":5851,\"start\":5140},{\"end\":6463,\"start\":5853},{\"end\":6589,\"start\":6465},{\"end\":6704,\"start\":6591},{\"end\":6853,\"start\":6706},{\"end\":7117,\"start\":6870},{\"end\":9343,\"start\":7141},{\"end\":9911,\"start\":9345},{\"end\":10460,\"start\":9913},{\"end\":11261,\"start\":10494},{\"end\":12128,\"start\":11263},{\"end\":12403,\"start\":12130},{\"end\":12662,\"start\":12412},{\"end\":13389,\"start\":12695},{\"end\":14065,\"start\":13391},{\"end\":15056,\"start\":14067},{\"end\":15850,\"start\":15088},{\"end\":16633,\"start\":15852},{\"end\":17304,\"start\":16635},{\"end\":17885,\"start\":17306},{\"end\":18238,\"start\":17887},{\"end\":18690,\"start\":18266},{\"end\":18802,\"start\":18692},{\"end\":18985,\"start\":18804},{\"end\":19049,\"start\":18987},{\"end\":19201,\"start\":19051},{\"end\":19457,\"start\":19203},{\"end\":19676,\"start\":19459},{\"end\":19788,\"start\":19678},{\"end\":20653,\"start\":19790},{\"end\":21115,\"start\":20673},{\"end\":22140,\"start\":21117},{\"end\":22580,\"start\":22378},{\"end\":23689,\"start\":22582},{\"end\":24551,\"start\":23691},{\"end\":24938,\"start\":24570},{\"end\":25448,\"start\":24940},{\"end\":26346,\"start\":25450},{\"end\":27087,\"start\":26348},{\"end\":27410,\"start\":27120},{\"end\":27678,\"start\":27412},{\"end\":27951,\"start\":27702},{\"end\":28577,\"start\":27990},{\"end\":30522,\"start\":28611},{\"end\":31628,\"start\":30524},{\"end\":32584,\"start\":31662},{\"end\":33202,\"start\":32608},{\"end\":34231,\"start\":33204},{\"end\":34754,\"start\":34233},{\"end\":34907,\"start\":34756},{\"end\":35211,\"start\":34909},{\"end\":35423,\"start\":35213},{\"end\":35654,\"start\":35425},{\"end\":36129,\"start\":35665},{\"end\":36447,\"start\":36153},{\"end\":37521,\"start\":36449},{\"end\":37638,\"start\":37523},{\"end\":38255,\"start\":37640},{\"end\":38536,\"start\":38257},{\"end\":38540,\"start\":38538},{\"end\":39166,\"start\":38542},{\"end\":39170,\"start\":39168},{\"end\":39739,\"start\":39172},{\"end\":40532,\"start\":39781},{\"end\":41193,\"start\":40557},{\"end\":41897,\"start\":41237},{\"end\":42036,\"start\":41908},{\"end\":42100,\"start\":42038},{\"end\":42721,\"start\":42102},{\"end\":43213,\"start\":42723},{\"end\":44237,\"start\":43256},{\"end\":44402,\"start\":44273},{\"end\":44837,\"start\":44404},{\"end\":45451,\"start\":44839},{\"end\":46264,\"start\":45453},{\"end\":46751,\"start\":46266},{\"end\":48024,\"start\":46753},{\"end\":49515,\"start\":48039}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":22355,\"start\":22141},{\"attributes\":{\"id\":\"formula_1\"},\"end\":27701,\"start\":27679},{\"attributes\":{\"id\":\"formula_2\"},\"end\":27989,\"start\":27952}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20696,\"start\":20689},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22493,\"start\":22486},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23400,\"start\":23393},{\"end\":29840,\"start\":29833},{\"end\":30364,\"start\":30357},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40151,\"start\":40144},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40928,\"start\":40921},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":42337,\"start\":42330},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":42741,\"start\":42734}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2945,\"start\":2933},{\"attributes\":{\"n\":\"2\"},\"end\":6868,\"start\":6856},{\"attributes\":{\"n\":\"2.1\"},\"end\":7139,\"start\":7120},{\"attributes\":{\"n\":\"2.2\"},\"end\":10492,\"start\":10463},{\"end\":12410,\"start\":12406},{\"attributes\":{\"n\":\"3.1\"},\"end\":12693,\"start\":12665},{\"attributes\":{\"n\":\"3.2\"},\"end\":15086,\"start\":15059},{\"attributes\":{\"n\":\"4\"},\"end\":18264,\"start\":18241},{\"attributes\":{\"n\":\"4.1\"},\"end\":20671,\"start\":20656},{\"attributes\":{\"n\":\"4.2\"},\"end\":22376,\"start\":22357},{\"attributes\":{\"n\":\"5\"},\"end\":24568,\"start\":24554},{\"attributes\":{\"n\":\"5.1\"},\"end\":27118,\"start\":27090},{\"attributes\":{\"n\":\"5.2\"},\"end\":28609,\"start\":28580},{\"attributes\":{\"n\":\"6\"},\"end\":31641,\"start\":31631},{\"attributes\":{\"n\":\"6.1\"},\"end\":31660,\"start\":31644},{\"attributes\":{\"n\":\"6.2\"},\"end\":32606,\"start\":32587},{\"end\":35663,\"start\":35657},{\"attributes\":{\"n\":\"6.3\"},\"end\":36151,\"start\":36132},{\"attributes\":{\"n\":\"6.4\"},\"end\":39779,\"start\":39742},{\"attributes\":{\"n\":\"6.5\"},\"end\":40555,\"start\":40535},{\"attributes\":{\"n\":\"6.6\"},\"end\":41235,\"start\":41196},{\"end\":41906,\"start\":41900},{\"attributes\":{\"n\":\"6.7\"},\"end\":43254,\"start\":43216},{\"end\":44241,\"start\":44240},{\"end\":44271,\"start\":44244},{\"attributes\":{\"n\":\"8\"},\"end\":48037,\"start\":48027},{\"end\":50019,\"start\":50012},{\"end\":50238,\"start\":50231},{\"end\":50468,\"start\":50461},{\"end\":50546,\"start\":50539},{\"end\":50765,\"start\":50756},{\"end\":51136,\"start\":51127}]", "table": "[{\"end\":50229,\"start\":50134},{\"end\":50459,\"start\":50350},{\"end\":50754,\"start\":50647},{\"end\":51125,\"start\":50869},{\"end\":51503,\"start\":51257}]", "figure_caption": "[{\"end\":50010,\"start\":49518},{\"end\":50134,\"start\":50021},{\"end\":50350,\"start\":50240},{\"end\":50537,\"start\":50470},{\"end\":50647,\"start\":50548},{\"end\":50869,\"start\":50767},{\"end\":51257,\"start\":51138}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":51714,\"start\":51713},{\"end\":51934,\"start\":51933},{\"end\":52122,\"start\":52121},{\"end\":52275,\"start\":52274},{\"end\":52287,\"start\":52286},{\"end\":52451,\"start\":52450},{\"end\":52461,\"start\":52460},{\"end\":52472,\"start\":52471},{\"end\":52741,\"start\":52740},{\"end\":52751,\"start\":52750},{\"end\":52760,\"start\":52759},{\"end\":52988,\"start\":52987},{\"end\":53134,\"start\":53133},{\"end\":53317,\"start\":53316},{\"end\":53516,\"start\":53515},{\"end\":53720,\"start\":53719},{\"end\":53912,\"start\":53911},{\"end\":53914,\"start\":53913},{\"end\":53925,\"start\":53924},{\"end\":54132,\"start\":54131},{\"end\":54144,\"start\":54143},{\"end\":54364,\"start\":54363},{\"end\":54562,\"start\":54561},{\"end\":54711,\"start\":54710},{\"end\":54941,\"start\":54940},{\"end\":55168,\"start\":55167},{\"end\":55345,\"start\":55344},{\"end\":55614,\"start\":55613},{\"end\":55807,\"start\":55806},{\"end\":55966,\"start\":55965},{\"end\":56121,\"start\":56120},{\"end\":56135,\"start\":56134},{\"end\":56328,\"start\":56327},{\"end\":56484,\"start\":56483},{\"end\":56494,\"start\":56493},{\"end\":56735,\"start\":56734},{\"end\":56737,\"start\":56736},{\"end\":56956,\"start\":56955},{\"end\":57153,\"start\":57152}]", "bib_author_last_name": "[{\"end\":51720,\"start\":51715},{\"end\":51944,\"start\":51935},{\"end\":52128,\"start\":52123},{\"end\":52284,\"start\":52276},{\"end\":52290,\"start\":52288},{\"end\":52458,\"start\":52452},{\"end\":52469,\"start\":52462},{\"end\":52479,\"start\":52473},{\"end\":52748,\"start\":52742},{\"end\":52757,\"start\":52752},{\"end\":52770,\"start\":52761},{\"end\":52994,\"start\":52989},{\"end\":53138,\"start\":53135},{\"end\":53324,\"start\":53318},{\"end\":53523,\"start\":53517},{\"end\":53727,\"start\":53721},{\"end\":53922,\"start\":53915},{\"end\":53934,\"start\":53926},{\"end\":54141,\"start\":54133},{\"end\":54155,\"start\":54145},{\"end\":54373,\"start\":54365},{\"end\":54568,\"start\":54563},{\"end\":54720,\"start\":54712},{\"end\":54947,\"start\":54942},{\"end\":55172,\"start\":55169},{\"end\":55352,\"start\":55346},{\"end\":55621,\"start\":55615},{\"end\":55814,\"start\":55808},{\"end\":55973,\"start\":55967},{\"end\":56132,\"start\":56122},{\"end\":56143,\"start\":56136},{\"end\":56335,\"start\":56329},{\"end\":56491,\"start\":56485},{\"end\":56500,\"start\":56495},{\"end\":56740,\"start\":56738},{\"end\":56961,\"start\":56957},{\"end\":57158,\"start\":57154}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12553774},\"end\":51877,\"start\":51611},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10318024},\"end\":52047,\"start\":51879},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10796366},\"end\":52253,\"start\":52049},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16106382},\"end\":52359,\"start\":52255},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18192962},\"end\":52632,\"start\":52361},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14230703},\"end\":52952,\"start\":52634},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15107744},\"end\":53079,\"start\":52954},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207156607},\"end\":53250,\"start\":53081},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15808973},\"end\":53439,\"start\":53252},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10779547},\"end\":53652,\"start\":53441},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":768352},\"end\":53847,\"start\":53654},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12748599},\"end\":54065,\"start\":53849},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7644747},\"end\":54287,\"start\":54067},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17968318},\"end\":54502,\"start\":54289},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8079613},\"end\":54675,\"start\":54504},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15921817},\"end\":54813,\"start\":54677},{\"attributes\":{\"id\":\"b16\"},\"end\":55108,\"start\":54815},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1454605},\"end\":55279,\"start\":55110},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":17165746},\"end\":55534,\"start\":55281},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10813638},\"end\":55758,\"start\":55536},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13224883},\"end\":55914,\"start\":55760},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207214362},\"end\":56065,\"start\":55916},{\"attributes\":{\"id\":\"b22\"},\"end\":56263,\"start\":56067},{\"attributes\":{\"id\":\"b23\"},\"end\":56415,\"start\":56265},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":18532232},\"end\":56653,\"start\":56417},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2339212},\"end\":56876,\"start\":56655},{\"attributes\":{\"id\":\"b26\"},\"end\":57074,\"start\":56878},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8726945},\"end\":57289,\"start\":57076}]", "bib_title": "[{\"end\":51711,\"start\":51611},{\"end\":51931,\"start\":51879},{\"end\":52119,\"start\":52049},{\"end\":52272,\"start\":52255},{\"end\":52448,\"start\":52361},{\"end\":52738,\"start\":52634},{\"end\":52985,\"start\":52954},{\"end\":53131,\"start\":53081},{\"end\":53314,\"start\":53252},{\"end\":53513,\"start\":53441},{\"end\":53717,\"start\":53654},{\"end\":53909,\"start\":53849},{\"end\":54129,\"start\":54067},{\"end\":54361,\"start\":54289},{\"end\":54559,\"start\":54504},{\"end\":54708,\"start\":54677},{\"end\":55165,\"start\":55110},{\"end\":55342,\"start\":55281},{\"end\":55611,\"start\":55536},{\"end\":55804,\"start\":55760},{\"end\":55963,\"start\":55916},{\"end\":56118,\"start\":56067},{\"end\":56481,\"start\":56417},{\"end\":56732,\"start\":56655},{\"end\":57150,\"start\":57076}]", "bib_author": "[{\"end\":51722,\"start\":51713},{\"end\":51946,\"start\":51933},{\"end\":52130,\"start\":52121},{\"end\":52286,\"start\":52274},{\"end\":52292,\"start\":52286},{\"end\":52460,\"start\":52450},{\"end\":52471,\"start\":52460},{\"end\":52481,\"start\":52471},{\"end\":52750,\"start\":52740},{\"end\":52759,\"start\":52750},{\"end\":52772,\"start\":52759},{\"end\":52996,\"start\":52987},{\"end\":53140,\"start\":53133},{\"end\":53326,\"start\":53316},{\"end\":53525,\"start\":53515},{\"end\":53729,\"start\":53719},{\"end\":53924,\"start\":53911},{\"end\":53936,\"start\":53924},{\"end\":54143,\"start\":54131},{\"end\":54157,\"start\":54143},{\"end\":54375,\"start\":54363},{\"end\":54570,\"start\":54561},{\"end\":54722,\"start\":54710},{\"end\":54949,\"start\":54940},{\"end\":55174,\"start\":55167},{\"end\":55354,\"start\":55344},{\"end\":55623,\"start\":55613},{\"end\":55816,\"start\":55806},{\"end\":55975,\"start\":55965},{\"end\":56134,\"start\":56120},{\"end\":56145,\"start\":56134},{\"end\":56337,\"start\":56327},{\"end\":56493,\"start\":56483},{\"end\":56502,\"start\":56493},{\"end\":56742,\"start\":56734},{\"end\":56963,\"start\":56955},{\"end\":57160,\"start\":57152}]", "bib_venue": "[{\"end\":51727,\"start\":51722},{\"end\":51951,\"start\":51946},{\"end\":52136,\"start\":52130},{\"end\":52299,\"start\":52292},{\"end\":52488,\"start\":52481},{\"end\":52779,\"start\":52772},{\"end\":53001,\"start\":52996},{\"end\":53148,\"start\":53140},{\"end\":53331,\"start\":53326},{\"end\":53529,\"start\":53525},{\"end\":53733,\"start\":53729},{\"end\":53944,\"start\":53936},{\"end\":54165,\"start\":54157},{\"end\":54382,\"start\":54375},{\"end\":54574,\"start\":54570},{\"end\":54727,\"start\":54722},{\"end\":54938,\"start\":54815},{\"end\":55178,\"start\":55174},{\"end\":55389,\"start\":55354},{\"end\":55629,\"start\":55623},{\"end\":55822,\"start\":55816},{\"end\":55981,\"start\":55975},{\"end\":56150,\"start\":56145},{\"end\":56325,\"start\":56265},{\"end\":56522,\"start\":56502},{\"end\":56748,\"start\":56742},{\"end\":56953,\"start\":56878},{\"end\":57165,\"start\":57160}]"}}}, "year": 2023, "month": 12, "day": 17}