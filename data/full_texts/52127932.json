{"id": 52127932, "updated": "2023-10-03 05:50:11.106", "metadata": {"title": "Self-Attentive Sequential Recommendation", "authors": "[{\"first\":\"Wang-Cheng\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Julian\",\"last\":\"McAuley\",\"middle\":[]}]", "venue": "2018 IEEE International Conference on Data Mining (ICDM)", "journal": "2018 IEEE International Conference on Data Mining (ICDM)", "publication_date": {"year": 2018, "month": 8, "day": 20}, "abstract": "Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the `context' of users' activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user's next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. The goal of our work is to balance these two goals, by proposing a self-attention based sequential model (SASRec) that allows us to capture long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are `relevant' from a user's action history, and use them to predict the next item. Extensive empirical studies show that our method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models. Visualizations on attention weights also show how our model adaptively handles datasets with various density, and uncovers meaningful patterns in activity sequences.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1808.09781", "mag": "2963367478", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icdm/KangM18", "doi": "10.1109/icdm.2018.00035"}}, "content": {"source": {"pdf_hash": "21cdebb4449aef0708617cb240524093f53df02e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1808.09781v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1808.09781", "status": "GREEN"}}, "grobid": {"id": "2906f2c7cf52cdf78d0151c092fc7ce9ab4358b3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/21cdebb4449aef0708617cb240524093f53df02e.txt", "contents": "\nSelf-Attentive Sequential Recommendation\n\n\nWang-Cheng Kang wckang@ucsd.edu \nJulian Mcauley jmcauley@ucsd.edu \nU C San \nDiego \nSelf-Attentive Sequential Recommendation\n\nSequential dynamics are a key feature of many modern recommender systems, which seek to capture the 'context' of users' activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user's next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. The goal of our work is to balance these two goals, by proposing a self-attention based sequential model (SASRec) that allows us to capture long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are 'relevant' from a user's action history, and use them to predict the next item. Extensive empirical studies show that our method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models. Visualizations on attention weights also show how our model adaptively handles datasets with various density, and uncovers meaningful patterns in activity sequences.\n\nI. INTRODUCTION\n\nThe goal of sequential recommender systems is to combine personalized models of user behavior (based on historical activities) with some notion of 'context' on the basis of users' recent actions. Capturing useful patterns from sequential dynamics is challenging, primarily because the dimension of the input space grows exponentially with the number of past actions used as context. Research in sequential recommendation is therefore largely concerned with how to capture these highorder dynamics succinctly.\n\nMarkov Chains (MCs) are a classic example, which assume that the next action is conditioned on only the previous action (or previous few), and have been successfully adopted to characterize short-range item transitions for recommendation [1]. Another line of work uses Recurrent Neural Networks (RNNs) to summarize all previous actions via a hidden state, which is used to predict the next action [2].\n\nBoth approaches, while strong in specific cases, are somewhat limited to certain types of data. MC-based methods, by making strong simplifying assumptions, perform well in highsparsity settings, but may fail to capture the intricate dynamics of more complex scenarios. Conversely RNNs, while expressive, require large amounts of data (an in particular dense data) before they can outperform simpler baselines.  Recently, a new sequential model Transfomer achieved stateof-the-art performance and efficiency for machine translation tasks [3]. Unlike existing sequential models that use convolutional or recurrent modules, Transformer is purely based on a proposed attention mechanism called 'self-attention,' which is highly efficient and capable of uncovering syntactic and semantic patterns between words in a sentence.\n\nInspired by this method, we seek to apply self-attention mechanisms to sequential recommendation problems. Our hope is that this idea can address both of the problems outlined above, being on the one hand able to draw context from all actions in the past (like RNNs) but on the other hand being able to frame predictions in terms of just a small number of actions (like MCs). Specifically, we build a Self-Attention based Sequential Recommendation model (SASRec), which adaptively assigns weights to previous items at each time step (Figure 1).\n\nThe proposed model significantly outperforms state-of-theart MC/CNN/RNN-based sequential recommendation methods on several benchmark datasets. In particular, we examine performance as a function of dataset sparsity, where model performance aligns closely with the patterns described above. Due to the self-attention mechanism, SASRec tends to consider long-range dependencies on dense datasets, while focusing on more recent activities on sparse datasets. This proves crucial for adaptively handling datasets with varying density.\n\nFurthermore, the core component (i.e., the self-attention block) of SASRec is suitable for parallel acceleration, resulting in a model that is an order of magnitude faster than CNN/RNNbased alternatives. In addition, we analyze the complexity and scalability of SASRec, conduct a comprehensive ablation study to show the effect of key components, and visualize the attention weights to qualitatively reveal the model's behavior.\n\nII. RELATED WORK Several lines of work are closely related to ours. We first discuss general, followed by temporal, recommendation, before discussing sequential recommendation (in particular MCs and RNNs). Last we introduce the attention mechanism, especially the self-attention module which is at the core of our model.\n\n\nA. General Recommendation\n\nRecommender systems focus on modeling the compatibility between users and items, based on historical feedback (e.g. clicks, purchases, likes). User feedback can be explicit (e.g. ratings) or implicit (e.g. clicks, purchases, comments) [4], [5]. Modeling implicit feedback can be challenging due to the ambiguity of interpreting 'non-observed' (e.g. non-purchased) data. To address the problem, point-wise [4] and pairwise [5] methods are proposed to solve such challenges.\n\nMatrix Factorization (MF) methods seek to uncover latent dimensions to represent users' preferences and items' properties, and estimate interactions through the inner product between the user and item embeddings [6], [7]. In addition, another line of work is based on Item Similarity Models (ISM) and doesn't explicitly model each user with latent factors (e.g. FISM [8]). They learn an item-to-item similarity matrix, and estimate a user's preference toward an item via measuring its similarities with items that the user has interacted with before.\n\nRecently, due to their success in related problems, various deep learning techniques have been introduced for recommendation [9]. One line of work seeks to use neural networks to extract item features (e.g. images [10], [11], text [12], [13], etc.) for content-aware recommendation. Another line of work seeks to replace conventional MF. For example, NeuMF [14] estimates user preferences via Multi-Layer Perceptions (MLP), and AutoRec [15] predicts ratings using autoencoders.\n\n\nB. Temporal Recommendation\n\nDating back to the Netflix Prize, temporal recommendation has shown strong performance on various tasks by explicitly modeling the timestamp of users' activities. TimeSVD++ [16] achieved strong results by splitting time into several segments and modeling users and items separately in each. Such models are essential to understand datasets that exhibit significant (short-or long-term) temporal 'drift' (e.g. 'how have movie preferences changed in the last 10 years,' or 'what kind of businesses do users visit at 4pm?', etc.) [16]- [18]. Sequential recommendation (or next-item recommendation) differs slightly from this setting, as it only considers the order of actions, and models sequential patterns which are independent of time. Essentially, sequential models try to model the 'context' of users' actions based on their recent activities, rather than considering temporal patterns per se.\n\n\nC. Sequential Recommendation\n\nMany sequential recommender systems seek to model itemitem transition matrices as a means of capturing sequential patterns among successive items. For instance, FPMC fuses an MF term and an item-item transition term to capture longterm preferences and short-term transitions respectively [1]. Essentially, the captured transition is a first-order Markov Chain (MC), whereas higher-order MCs assume the next action is related to several previous actions. Since the last visited item is often the key factor affecting the user's next action (essentially providing 'context'), first-order MC based methods show strong performance, especially on sparse datasets [19]. There are also methods adopting high-order MCs that consider more previous items [20], [21]. In particular, Convolutional Sequence Embedding (Caser), a CNN-based method, views the embedding matrix of L previous items as an 'image' and applies convolutional operations to extract transitions [22].\n\nOther than MC-based methods, another line of work adopts RNNs to model user sequences [2], [23]- [25]. For example, GRU4Rec uses Gated Recurrent Units (GRU) to model click sequences for session-based recommendation [2], and an improved version further boosts its Top-N recommendation performance [26]. In each time step, RNNs take the state from the last step and current action as its input. These dependencies make RNNs less efficient, though techniques like 'sessionparallelism' have been proposed to improve efficiency [2].\n\n\nD. Attention Mechanisms\n\nAttention mechanisms have been shown to be effective in various tasks such as image captioning [27] and machine translation [28], among others. Essentially the idea behind such mechanisms is that sequential outputs (for example) each depend on 'relevant' parts of some input that the model should focus on successively. An additional benefit is that attention-based methods are often more interpretable. Recently, attention mechanisms have been incorporated into recommender systems [29]- [31]. For example, Attentional Factorization Machines (AFM) [30] learn the importance of each feature interaction for content-aware recommendation.\n\nHowever, the attention technique used in the above is essentially an additional component to the original model (e.g. attention+RNNs, attention+FMs, etc.). Recently, a purely attention-based sequence-to-sequence method, Transfomer [3], achieved state-of-the-art performance and efficiency on machine translation tasks which had previously been dominated by RNN/CNN-based approaches [32], [33]. The Transformer model relies heavily on the proposed 'self-attention' modules to capture complex structures in sentences, and to retrieve relevant words (in the source language) for generating the next word (in the target language). Inspired by Transformer, we seek to build a new sequential recommendation model based upon the self-attention approach, though the problem of sequential recommendation is quite different from machine translation, and requires specially designed models. \n(S u 1 , S u 2 , ..., S u |S u | ) d \u2208 N latent vector dimensionality n \u2208 N maximum sequence length b \u2208 N number of self-attention blocks M \u2208 R |I|\u00d7d item embedding matrix P \u2208 R n\u00d7d positional embedding matrix E \u2208 R n\u00d7d input embedding matrix S (b) \u2208 R n\u00d7d\nitem embeddings after the b-th self-attention layer F (b) \u2208 R n\u00d7d item embeddings after the b-th feed-forward network\n\n\nIII. METHODOLOGY\n\nIn the setting of sequential recommendation, we are given a user's action sequence S u = (S u 1 , S u 2 , . . . , S u |S u | ), and seek to predict the next item. During the training process, at time step t, the model predicts the next item depending on the previous t items. As shown in Figure 1, it will be convenient to think of the model's input as (S u 1 , S u 2 , . . . , S u |S u |\u22121 ) and its expected output as a 'shifted' version of the same sequence:\n(S u 2 , S u 3 , . . . , S u |S u | ).\nIn this section, we describe how we build a sequential recommendation model via an embedding layer, several self-attention blocks, and a prediction layer. We also analyze its complexity and further discuss how SASRec differs from related models. Our notation is summarized in Table I.\n\n\nA. Embedding Layer\n\nWe transform the training sequence (S u 1 , S u 2 , ..., S u |S u |\u22121 ) into a fixed-length sequence s = (s 1 , s 2 , . . . , s n ), where n represents the maximum length that our model can handle. If the sequence length is greater than n, we consider the most recent n actions. If the sequence length is less than n, we repeatedly add a 'padding' item to the left until the length is n. We create an item embedding matrix M \u2208 R |I|\u00d7d where d is the latent dimensionality, and retrieve the input embedding matrix E \u2208 R n\u00d7d , where E i = M si . A constant zero vector 0 is used as the embedding for the padding item.\n\nPositional Embedding: As we will see in the next section, since the self-attention model doesn't include any recurrent or convolutional module, it is not aware of the positions of previous items. Hence we inject a learnable position embedding P \u2208 R n\u00d7d into the input embedding:\nE = \uf8ee \uf8ef \uf8ef \uf8f0 M s1 + P 1 M s2 + P 2 . . . M sn + P n \uf8f9 \uf8fa \uf8fa \uf8fb (1)\nWe also tried the fixed position embedding as used in [3], but found that this led to worse performance in our case. We analyze the effect of the position embedding quantitatively and qualitatively in our experiments.\n\n\nB. Self-Attention Block\n\nThe scaled dot-product attention [3] is defined as:\nAttention(Q, K, V) = softmax QK T \u221a d V,(2)\nwhere Q represents the queries, K the keys and V the values (each row represents an item). Intuitively, the attention layer calculates a weighted sum of all values, where the weight between query i and value j relates to the interaction between query i and key j. The scale factor \u221a d is to avoid overly large values of the inner product, especially when the dimensionality is high.\n\nSelf-Attention layer: In NLP tasks such as machine translation, attention mechanisms are typically used with K = V (e.g. using an RNN encoder-decoder for translation: the encoder's hidden states are keys and values, and the decoder's hidden states are queries) [28]. Recently, a self-attention method was proposed which uses the same objects as queries, keys, and values [3]. In our case, the self-attention operation takes the embedding E as input, converts it to three matrices through linear projections, and feeds them into an attention layer:\nS = SA( E) = Attention( EW Q , EW K , EW V ),(3)\nwhere the projection matrices\nW Q , W K , W V \u2208 R d\u00d7d .\nThe projections make the model more flexible. For example, the model can learn asymmetric interactions (i.e., <query i, key j> and <query j, key i> can have different interactions). Causality: Due to the nature of sequences, the model should consider only the first t items when predicting the (t + 1)-st item. However, the t-th output of the self-attention layer (S t ) contains embeddings of subsequent items, which makes the model ill-posed. Hence, we modify the attention by forbidding all links between Q i and K j (j > i).\n\nPoint-Wise Feed-Forward Network: Though the selfattention is able to aggregate all previous items' embeddings with adaptive weights, ultimately it is still a linear model. To endow the model with nonlinearity and to consider interactions between different latent dimensions, we apply a point-wise two-layer feed-forward network to all S i identically (sharing parameters):\nF i = FFN(S i ) = ReLU(S i W (1) + b (1) )W (2) + b (2) , (4) where W (1) , W (2) are d \u00d7 d matrices and b (1) , b (2) are d- dimensional vectors.\nNote that there is no interaction between S i and S j (i = j), meaning that we still prevent information leaks (from back to front).\n\n\nC. Stacking Self-Attention Blocks\n\nAfter the first self-attention block, F i essentially aggregates all previous items' embeddings (i.e., E j , j \u2264 i). However, it might be useful to learn more complex item transitions via another self-attention block based on F. Specifically, we stack the self-attention block (i.e., a self-attention layer and a feedforward network), and the b-th (b > 1) block is defined as:\nS (b) = SA(F (b\u22121) ), F (b) i = FFN(S (b) i ), \u2200i \u2208 {1, 2, . . . , n},(5)\nand the 1-st block is defined as S (1) = S and F (1) = F.\n\nHowever, when the network goes deeper, several problems become exacerbated: 1) the increased model capacity leads to overfitting; 2) the training process becomes unstable (due to vanishing gradients etc.); and 3) models with more parameters often require more training time. Inspired by [3], We perform the following operations to alleviate these problems:\ng(x) = x + Dropout(g(LayerNorm(x))),\nwhere g(x) represents the self attention layer or the feedforward network. That is to say, for layer g in each block, we apply layer normalization on the input x before feeding into g, apply dropout on g's output, and add the input x to the final output. We introduce these operations below.\n\nResidual Connections: In some cases, multi-layer neural networks have demonstrated the ability to learn meaningful features hierarchically [34]. However, simply adding more layers did not easily correspond to better performance until residual networks were proposed [35]. The core idea behind residual networks is to propagate low-layer features to higher layers by residual connection. Hence, if low-layer features are useful, the model can easily propagate them to the final layer. Similarly, we assume residual connections are also useful in our case. For example, existing sequential recommendation methods have shown that the last visited item plays a key role on predicting the next item [1], [19], [21]. However, after several self-attention blocks, the embedding of the last visited item is entangled with all previous items; adding residual connections to propagate the last visited item's embedding to the final layer would make it much easier for the model to leverage low-layer information.\n\nLayer Normalization: Layer normalization is used to normalize the inputs across features (i.e., zero-mean and unitvariance), which is beneficial for stabilizing and accelerating neural network training [36]. Unlike batch normalization [37], the statistics used in layer normalization are independent of other samples in the same batch. Specifically, assuming the input is a vector x which contains all features of a sample, the operation is defined as:\nLayerNorm(x) = \u03b1 x \u2212 \u00b5 \u221a \u03c3 2 + + \u03b2,\nwhere is an element-wise product (i.e., the Hadamard product), \u00b5 and \u03c3 are the mean and variance of x, \u03b1 and \u03b2 are learned scaling factors and bias terms.\n\nDropout: To alleviate overfitting problems in deep neural networks, 'Dropout' regularization techniques have been shown to be effective in various neural network architectures [38]. The idea of dropout is simple: randomly 'turn off' neurons with probability p during training, and use all neurons when testing. Further analysis points out that dropout can be viewed as a form of ensemble learning which considers an enormous number of models (exponential in the number of neurons and input features) that share parameters [39]. We also apply a dropout layer on the embedding E.\n\n\nD. Prediction Layer\n\nAfter b self-attention blocks that adaptively and hierarchically extract information of previously consumed items, we predict the next item (given the first t items) based on F (b) t . Specifically, we adopt an MF layer to predict the relevance of item i:\nr i,t = F (b) t N T i ,\nwhere r i,t is the relevance of item i being the next item given the first t items (i.e., s 1 , s 2 , . . . , s t ), and N \u2208 R |I|\u00d7d is an item embedding matrix. Hence, a high interaction score r i,t means a high relevance, and we can generate recommendations by ranking the scores.\n\nShared Item Embedding: To reduce the model size and alleviate overfitting, we consider another scheme which only uses a single item embedding M:\nr i,t = F (b) t M T i .(6)\nNote that F\n(b) t\ncan be represented as a function depending on the item embedding M: F\n(b) t = f (M s1 , M s2 , . . . , M st ).\nA potential issue of using homogeneous item embeddings is that their inner products cannot represent asymmetric item transitions (e.g. item i is frequently bought after j, but not vise versa), and thus existing methods like FPMC tend to use heterogeneous item embeddings. However, our model doesn't have this issue since it learns a nonlinear transformation. For example, the feed forward network can easily achieve the asymmetry with the same item embedding:\nFFN(M i )M T j = FFN(M j )M T i .\nEmpirically, using a shared item embedding significantly improves the performance of our model.\n\nExplicit User Modeling: To provide personalized recommendations, existing methods often take one of two approaches: 1) learn an explicit user embedding representing user preferences (e.g. MF [40], FPMC [1] and Caser [22]); 2) consider the user's previous actions, and induce an implicit user embedding from embeddings of visited items (e.g. FSIM [8], Fossil [21], GRU4Rec [2]). Our method falls into the latter category, as we generate an embedding F n by considering all actions of a user. However, we can also insert an explicit user embedding at the last layer, for example via addition:\nr u,i,t = (U u + F (b) t )M T i\nwhere U is a user embedding matrix. However, we empirically find that adding an explicit user embedding doesn't improve performance (presumably because the model already considers all of the user's actions).\n\n\nE. Network Training\n\nRecall that we convert each user sequence (excluding the last action) (S u 1 , S u 2 , . . . , S u |S u |\u22121 ) to a fixed length sequence s = {s 1 , s 2 , . . . , s n } via truncation or padding items. We define o t as the expected output at time step t:\no t = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 <pad> if s t is a padding item s t+1 1 \u2264 t < n S u |S u | t = n ,\nwhere <pad> indicates a padding item. Our model takes a sequence s as input, the corresponding sequence o as expected output, and we adopt the binary cross entropy loss as the objective function:\n\u2212 S u \u2208S t\u2208[1,2,...,n] \uf8ee \uf8f0 log(\u03c3(r ot,t )) + j \u2208S u log(1 \u2212 \u03c3(r j,t )) \uf8f9 \uf8fb .\nNote that we ignore the terms where o t = <pad>.\n\nThe network is optimized by the Adam optimizer [41], which is a variant of Stochastic Gradient Descent (SGD) with adaptive moment estimation. In each epoch, we randomly generate one negative item j for each time step in each sequence. More implementation details are described later.\n\n\nF. Complexity Analysis\n\nSpace Complexity: The learned parameters in our model are from the embeddings and parameters in the self-attention layers, feed-forward networks and layer normalization. The total number of parameters is O(|I|d + nd + d 2 ), which is moderate compared to other methods (e.g. O(|U|d + |I|d) for FPMC) since it does not grow with the number of users, and d is typically small in recommendation problems.\n\nTime Complexity: The computational complexity of our model is mainly due to the self-attention layer and the feedforward network, which is O(n 2 d + nd 2 ). The dominant term is typically O(n 2 d) from the self-attention layer. However, a convenient property in our model is that the computation in each self-attention layer is fully parallelizable, which is amenable to GPU acceleration. In contrast, RNN-based methods (e.g. GRU4Rec [2]) have a dependency on time steps (i.e., computation on time step t must wait for results from time step t-1), which leads to an O(n) time on sequential operations. We empirically find our method is over ten times faster than RNN and CNN-based methods with GPUs (the result is similar to that in [3] for machine translation tasks), and the maximum length n can easily scale to a few hundred which is generally sufficient for existing benchmark datasets.\n\nDuring testing, for each user, after calculating the embedding F (b) n , the process is the same as standard MF methods. (O(d) for evaluating the preference toward an item).\n\nHanding Long Sequences: Though our experiments empirically verify the efficiency of our method, ultimately it cannot scale to very long sequences. A few options are promising to investigate in the future: 1) using restricted self-attention [42] which only attends on recent actions rather than all actions, and distant actions can be considered in higher layers; 2) splitting long sequences into short segments as in [22].\n\n\nG. Discussion\n\nWe find that SASRec can be viewed as a generalization of some classic CF models. We also discuss conceptually how our approach and existing methods handle sequence modeling.\n\nReduction to Existing Models:\n\n\u2022 Factorized Markov Chains: FMC factorizes a first-order item transition matrix, and predicts the next item j depending on the last item i:\nP (j|i) \u221d M T i N j .\nIf we set the self-attention block to zero, use unshared item embeddings, and remove the position embedding, SASRec reduces to FMC. Furthermore, SASRec is also closely related to Factorized Personalized Markov Chains (FPMC) [1], which fuse MF with FMC to capture user preferences and short-term dynamics respectively:\nP (j|u, i) \u221d [U u , M i ] N T j .\nFollowing the reduction operations above for FMC, and adding an explicit user embedding (via concatenation), SASRec is equivalent to FPMC. \u2022 Factorized Item Similarity Models [8]: FISM estimates a preference score toward item i by considering the similarities between i and items the user consumed before:\nP (j|u) \u221d 1 |S u | i\u2208S u M i N T j .\nIf we use one self-attention layer (excluding the feedforward network), set uniform attention weights (i.e., 1 |Su| ) on items, use unshared item embeddings, and remove the position embedding, SASRec is reduced to FISM. Thus our model can be viewed as an adaptive, hierarchical, sequential item similarity model for next item recommendation.\n\n\nMC-based Recommendation:\n\nMarkov Chains (MC) can effectively capture local sequential patterns, assuming that the next item is only dependent on the previous L items. Exiting MC-based sequential recommendation methods rely on either first-order MCs (e.g. FPMC [1], HRM [43], TransRec [19]) or high-order MCs (e.g. Fossil [21], Vista [20], Caser [22]). The first group of methods tend to perform best on sparse datasets. In contrast, higher-order MC based methods have two limitations: (1) The MC order L needs to be specified before training, rather than being chosen adaptively; (2) The performance and efficiency doesn't scale well with the order L, hence L is typically small (e.g. less than 5). Our method resolves the first issue, since it can adaptively attend on related previous items (e.g. focusing on just the last item on sparse dataset, and more items on dense datasets). Moreover, our model is essentially conditioned on n previous items, and can empirically scale to several hundred previous items, exhibiting performance gains with moderate increases in training time.\n\nRNN-based Recommendation: Another line of work seeks to use RNNs to model user action sequences [2], [17], [26]. RNNs are generally suitable for modeling sequences, though recent studies show CNNs and self-attention can be stronger in some sequential settings [3], [44]. Our self-attention based model can be derived from item similarity models, which are a reasonable alternative for sequence modeling for recommendation. For RNNs, other than their inefficiency in parallel computation (Section III-F), their maximum path length (from an input node to related output nodes) is O(n).\n\nIn contrast, our model has O(1) maximum path length, which can be beneficial for learning long-range dependencies [45].\n\n\nIV. EXPERIMENTS\n\nIn this section, we present our experimental setup and empirical results. Our experiments are designed to answer the following research questions: RQ1: Does SASRec outperform state-of-the-art models including CNN/RNN based methods? RQ2: What is the influence of various components in the SASRec architecture? RQ3: What is the training efficiency and scalability (regarding n) of SASRec? RQ4: Are the attention weights able to learn meaningful patterns related to positions or items' attributes?\n\n\nA. Datasets\n\nWe evaluate our methods on four datasets from three real world applications. The datasets vary significantly in domains, platforms, and sparsity: The dataset also includes rich information that might be useful in future work, like users' play hours, pricing information, media score, category, developer (etc.). \u2022 MovieLens: A widely used benchmark dataset for evaluating collaborative filtering algorithms. We use the version (MovieLens-1M) that includes 1 million user ratings.\n\nWe followed the same preprocessing procedure from [1], [19], [21]. For all datasets, we treat the presence of a review or rating as implicit feedback (i.e., the user interacted with the item) and use timestamps to determine the sequence order of actions. We discard users and items with fewer than 5 related actions. For partitioning, we split the historical sequence S u for each user u into three parts: (1) the most recent action S u |S u | for testing, (2) the second most recent action S u  Table II. We see that the two Amazon datasets have the fewest actions per user and per item (on average), Steam has a high average number of actions per item, and MovieLens-1m is the most dense dataset.\n\n\nB. Comparison Methods\n\nTo show the effectiveness of our method, we include three groups of recommendation baselines. The first group includes general recommendation methods which only consider user feedback without considering the sequence order of actions: \u2022 PopRec: This is a simple baseline that ranks items according to their popularity (i.e., number of associated actions). \u2022 Bayesian Personalized Ranking (BPR) [5]: BPR is a classic method for learning personalized rankings from implicit feedback. Biased matrix factorization is used as the underlying recommender. The second group contains sequential recommendation methods based on first order Markov chains, which consider the last visited item: FPMC uses a combination of matrix factorization and factorized first-order Markov chains as its recommender, which captures users' long-term preferences as well as item-to-item transitions. \u2022 Translation-based Recommendation (TransRec) [19]:\n\nA state-of-the-art first-order sequential recommendation method which models each user as a translation vector to capture the transition from the current item to the next. The last group contains deep-learning based sequential recommender systems, which consider several (or all) previously visited items:\n\n\u2022 GRU4Rec [2]: A seminal method that uses RNNs to model user action sequences for session-based recommendation. We treat each user's feedback sequence as a session. \u2022 GRU4Rec + [26]: An improved version of GRU4Rec, which adopts a different loss function and sampling strategy, and shows significant performance gains on Top-N recommendation. \u2022 Convolutional Sequence Embeddings (Caser) [22]: A recently proposed CNN-based method capturing highorder Markov chains by applying convolutional operations on the embedding matrix of the L most recent items, and achieves state-of-the-art sequential recommendation performance. Since other sequential recommendation methods (e.g. PRME [47], HRM [43], Fossil [21]) have been outperformed on similar datasets by baselines among those above, we omit comparison against them. We also don't include temporal recommendation methods like TimeSVD++ [16] and RRN [17], which differ in setting from what we consider here. Table III: Recommendation performance. The best performing method in each row is boldfaced, and the second best method in each row is underlined. Improvements over non-neural and neural approaches are shown in the last two columns respectively. For fair comparison, we implement BPR, FMC, FPMC, and TransRec using TemsorFlow with the Adam [41] optimizer. For GRU4Rec, GRU4Rec + , and Caser, we use code provided by the corresponding authors. For all methods except PopRec, we consider latent dimensions d from {10, 20, 30, 40, 50}. For BPR, FMC, FPMC, and TransRec, the 2 regularizer is chosen from {0.0001, 0.001, 0.01, 0.1, 1}. All other hyperparameters and initialization strategies are those suggested by the methods' authors. We tune hyper-parameters using the validation set, and terminate training if validation performance doesn't improve for 20 epochs.\n\n\nC. implementation Details\n\nFor the architecture in the default version of SASRec, we use two self-attention blocks (b = 2), and use the learned positional embedding. Item embeddings in the embedding layer and prediction layer are shared. We implement SASRec with TensorFlow. The optimizer is the Adam optimizer [41], the learning rate is set to 0.001, and the batch size is 128. The dropout rate of turning off neurons is 0.2 for MovieLens-1m and 0.5 for the other three datasets due to their sparsity. The maximum sequence length n is set to 200 for MovieLens-1m and 50 for the other three datasets, i.e., roughly proportional to the mean number of actions per user. Performance of variants and different hyper-parameters is examined below.\n\nAll code and data shall be released at publication time.\n\n\nD. Evaluation Metrics\n\nWe adopt two common Top-N metrics, Hit Rate@10 and NDCG@10, to evaluate recommendation performance [14], [19]. Hit@10 counts the fraction of times that the ground-truth next item is among the top 10 items, while NDCG@10 is a position-aware metric which assigns larger weights on higher positions. Note that since we only have one test item for each user, Hit@10 is equivalent to Recall@10, and is proportional to Precision@10.\n\nTo avoid heavy computation on all user-item pairs, we followed the strategy in [14], [48]. For each user u, we randomly sample 100 negative items, and rank these items with the ground-truth item. Based on the rankings of these 101 items, Hit@10 and NDCG@10 can be evaluated.\n\n\nE. Recommendation Performance\n\nTable III presents the recommendation performance of all methods on the four datasets (RQ1). By considering the second best methods across all datasets, a general pattern emerges with non-neural methods (i.e., (a)-(e)) performing better on sparse datasets and neural approaches (i.e., (f)-(h)) performing better on denser datasets. Presumably this owes to neural approaches having more parameters to capture high order transitions (i.e., they are expressive but easily overfit), whereas carefully designed but simpler models are more effective in high-sparsity settings.\n\nOur method SASRec outperforms all baselines on both sparse and dense datasets, and gains 6.9% Hit Rate and 9.6% NDCG improvements (on average) against the strongest baseline. One likely reason is that our model can adaptively attend items within different ranges on different datasets (e.g. only the previous item on sparse datasets and more on dense datasets). We further analyze this behavior in Section IV-H.\n\nIn Figure 2 we also analyze the effect of a key hyperparameter, the latent dimensionality d, by showing NDCG@10 of all methods with d varying from 10 to 50. We see that our model typically benefits from larger numbers of latent dimensions. For all datasets, our model achieves satisfactory performance with d \u2265 40.\n\n\nF. Ablation Study\n\nSince there are many components in our architecture, we analyze their impacts via an ablation study (RQ2). Table IV shows the performance of our default method and its 8 variants on all four dataset (with d = 50). We introduce the variants and analyze their effect respectively:    [3] found that it is useful to use 'multi-head' attention, which applies attention in h subspaces (each a d/h-dimensional space). However, performance with two heads is consistently and slightly worse than single-head attention in our case. This might owe to the small d in our problem (d = 512 in Transformer), which is not suitable for decomposition into smaller subspaces.\n\u2022 (1) Remove\n\nG. Training Efficiency & Scalability\n\nWe evaluate two aspects of the training efficiency (RQ3) of our model: Training speed (time taken for one epoch of training) and convergence time (time taken to achieve satisfactory performance). We also examine the scalability of our model in terms of the maximum length n. All experiments are conducted with a single GTX-1080 Ti GPU.\n\nTraining Efficiency: Figure 3 demonstrates the efficiency of deep learning based methods with GPU acceleration. GRU4Rec is omitted due to its inferior performance. For fair comparison, there are two training options for Caser and GRU4Rec + : using complete training data or just the most recent 200 actions (as in SASRec). For computing speed, SASRec only spends 1.7 seconds on model updates for one epoch, which is over 11 times faster than Caser (19.1s/epoch) and 18 times faster than GRU4Rec + (30.7s/epoch). We also see that SASRec converges to optimal performance within around 350 seconds on ML-1M while other models require much longer. We also find that using full data leads to better performance for Caser and GRU4Rec + .   Scalability: As with standard MF methods, SASRec scales linearly with the total number of users, items and actions. A potential scalability concern is the maximum length n, however the computation can be effectively parallelized with GPUs. Here we measure the training time and performance of SASRec with different n, empirically study its scalability, and analyze whether it can handle sequential recommendation in most cases. Table V shows the performance and efficiency of SASRec with various sequence lengths. Performance is better with larger n, up to around n = 500 at which point performance saturates (possibly because 99.8% of actions have been covered). However, even with n = 600, the model can be trained in 2,000 seconds, which is still faster than Caser and GRU4Rec + . Hence, our model can easily scale to user sequences up to a few hundred actions, which is suitable for typical review and purchase datasets. We plan to investigate approaches (discussed in Section III-F) for handling very long sequences in the future.  \n\n\nH. Visualizing Attention Weights\n\nRecall that at time step t, the self-attention mechanism in our model adaptively assigns weights on the first t items depending on their position embeddings and item embeddings. To answer RQ4, we examine all training sequences and seek to reveal meaningful patterns by showing the average attention weights on positions as well as items.\n\nAttention on Positions: Figure 4 shows four heatmaps of average attention weights on the last 15 positions at the last 15 time steps. Note that when we calculate the average weight, the denominator is the number of valid weights, so as to avoid the influence of padding items in short sequences.\n\nWe consider a few comparisons among the heatmaps:\n\n\u2022 (a) vs. (c): This comparison indicates that the model tends to attend on more recent items on the sparse dataset Beauty, and less recent items for the dense dataset ML-1M.\n\nThis is the key factor that allows our model to adaptively handle both sparse and dense datasets, whereas existing methods tend to focus on one end of the spectrum. Presumably this is because the first self-attention block already considers all previous items, and the second block does not need to consider far away positions. Overall, the visualizations show that the behavior of our self-attention mechanism is adaptive, position-aware, and hierarchical.\n\nAttention Between Items: Showing attention weights between a few cherry-picked items might not be statistically meaningful. To perform a broader comparison, using MovieLens-1M, where each movie has several categories, we randomly select two disjoint sets where each set contains 200 movies from 4 categories: Science Fiction (Sci-Fi), Romance, Animation, and Horror. The first set is used for the query and the second set as the key. Figure 5 shows a heatmap of average attention weights between the two sets. We can see the heatmap is approximately a block diagonal matrix, meaning that the attention mechanism can identify similar items (e.g. items sharing a common category) and tends to assign larger weights between them (without being aware of categories in advance).\n\n\nV. CONCLUSION\n\nIn this work, we proposed a novel self-attention based sequential model SASRec for next item recommendation. SASRec models the entire user sequence (without any recurrent or convolutional operations), and adaptively considers consumed items for prediction. Extensive empirical results on both sparse and dense datasets show that our model outperforms stateof-the-art baselines, and is an order of magnitude faster than CNN/RNN based approaches. In the future, we plan to extend the model by incorporating rich context information (e.g. dwell time, action types, locations, devices, etc.), and to investigate approaches to handle very long sequences (e.g. clicks). \n\nFigure 1 :\n1A simplified diagram showing the training process of SASRec. At each time step, the model considers all previous items, and uses attention to 'focus on' items relevant to the next action.\n\n|S u |\u2212 1\n1for validation, and (3) all remaining actions for training. Note that during testing, the input sequences contain training actions and the validation action.Data statistics are shown in\n\n\u2022\nFactorized Markov Chains (FMC): A first-order Markov chain method. FMC factorizes an item transition matrix using two item embeddings, and generates recommendations depending only on the last visited item. \u2022 Factorized Personalized Markov Chains (FPMC) [1]:\n\nFigure 2 :\n2Effect of the latent dimensionality d on ranking performance (NDCG@10).\n\nFigure 3 :\n3Training efficiency on ML-1M. SASRec is an order of magnitude faster than CNN/RNN-based recommendation methods in terms of training time per epoch and in total.\n\nFigure 4 :\n4Visualizations of average attention weights on positions at different time steps. For comparison, the heatmap of a first-order Markov chain based model would be a diagonal matrix.\n\n\n\u2022 (b) vs. (c): This comparison shows the effect of using positional embeddings (PE). Without them attention weights are essentially uniformly distributed over previous items, while the default model (c) is more sensitive in position as it is inclined to attend on recent items. \u2022 (c) vs. (d): Since our model is hierarchical, this shows how attention varies across different blocks. Apparently, attention in high layers tends to focus on more recent positions.\n\nFigure 5 :\n5Visualization of average attention between movies from four categories. This shows our model can uncover items' attributes, and assigns larger weights between similar items.\n\nTable I :\nINotation.Notation \nDescription \n\nU , I \nuser and item set \nS u \nhistorical interaction sequence for a user u: \n\n\n\n\u2022 Amazon: A series of datasets introduced in[46], comprising large corpora of product reviews crawled from Amazon.com. Top-level product categories on Amazon are treated as separate datasets. We consider two categories, 'Beauty,' and 'Games.' This dataset is notable for its high sparsity and variability. \u2022 Steam: We introduce a new dataset crawled from Steam, a large online video game distribution platform.The dataset \ncontains 2,567,538 users, 15,474 games and 7,793,069 \nEnglish reviews spanning October 2010 to January 2018. \n\n\nTable II :\nIIDataset statistics (after preprocessing)Dataset \n#users \n#items \n\navg. \nactions \n/user \n\navg. \nactions \n/item \n\n#actions \n\nAmazon Beauty \n52,024 57,289 \n7.6 \n6.9 \n0.4M \nAmazon Games \n31,013 23,715 \n9.3 \n12.1 \n0.3M \nSteam \n334,730 13,047 \n11.0 \n282.5 \n3.7M \nMovieLens-1M \n6,040 \n3,416 \n163.5 \n289.1 \n1.0M \n\n\n\nTable IV :\nIVmeaning that the hierarchical self-attention structure is helpful to learn more complex item transitions. Using three blocks achieves similar performance to the default model.\u2022 (8) Multi-head: The authors of TransformerAblation analysis (NDCG@10) on four datasets. \nPerformance better than the default version is boldfaced. '\u2193' \nindicates a severe performance drop (more than 10%). \n\nArchitecture \nBeauty \nGames \nSteam \nML-1M \n\n(0) Default \n0.3142 \n0.5360 \n0.6306 \n0.5905 \n(1) Remove PE \n0.3183 \n0.5301 \n0.6036 \n0.5772 \n(2) Unshared IE \n0.2437\u2193 0.4266\u2193 0.4472\u2193 \n0.4557\u2193 \n(3) Remove RC \n0.2591\u2193 0.4303\u2193 0.5693 \n0.5535 \n(4) Remove Dropout 0.2436\u2193 0.4375\u2193 0.5959 \n0.5801 \n(5) 0 Block (b=0) \n0.2620\u2193 0.4745\u2193 0.5588\u2193 \n0.4830\u2193 \n(6) 1 Block (b=1) \n0.3066 \n0.5408 \n0.6202 \n0.5653 \n(7) 3 Blocks (b=3) \n0.3078 \n0.5312 \n0.6275 \n0.5931 \n(8) Multi-Head \n0.3080 \n0.5311 \n0.6272 \n0.5885 \n\n\u2022 (2) Unshared IE (Item Embedding): We find that using two \nitem embeddings consistently impairs the performance, \npresumably due to overfitting. \n\u2022 (3) Remove RC (Residual Connections): Without residual \nconnections, we find that performance is significantly \nworse. Presumably this is because information in lower \nlayers (e.g. the last item's embedding and the output of \nthe first block) can not be easily propagated to the final \nlayer, and this information is highly useful for making \nrecommendations, especially on sparse datasets. \n\u2022 (4) Remove Dropout: Our results show that dropout can \neffectively regularize the model to achieve better test \nperformance, especially on sparse datasets. The results \nalso imply the overfitting problem is less severe on dense \ndatasets. \n\u2022 (5)-(7) Number of blocks: Not surprisingly, results are \ninferior with zero blocks, since the model would only \ndepend on the last item. The variant with one block \nperforms reasonably well, though using two blocks (the \ndefault model) still boosts performance especially on \ndense datasets, \n\nTable V :\nVScalability: performance and training time with different maximum length n on ML-1M.\n\nFactorizing personalized markov chains for next-basket recommendation. S Rendle, C Freudenthaler, L Schmidt-Thieme, WWWS. Rendle, C. Freudenthaler, and L. Schmidt-Thieme, \"Factorizing personalized markov chains for next-basket recommendation,\" in WWW, 2010.\n\nSession-based recommendations with recurrent neural networks. B Hidasi, A Karatzoglou, L Baltrunas, D Tikk, ICLR. B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk, \"Session-based recommendations with recurrent neural networks,\" in ICLR, 2016.\n\n. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Attention is all you need,\" in NIPSA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in NIPS, 2017.\n\nCollaborative filtering for implicit feedback datasets. Y Hu, Y Koren, C Volinsky, ICDM. Y. Hu, Y. Koren, and C. Volinsky, \"Collaborative filtering for implicit feedback datasets,\" in ICDM, 2008.\n\nBPR: bayesian personalized ranking from implicit feedback. S Rendle, C Freudenthaler, Z Gantner, L Schmidt-Thieme, UAIS. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, \"BPR: bayesian personalized ranking from implicit feedback,\" in UAI, 2009.\n\nRecommender systems handbook. F Ricci, L Rokach, B Shapira, P Kantor, Springer USF. Ricci, L. Rokach, B. Shapira, and P. Kantor, Recommender systems handbook. Springer US, 2011.\n\nAdvances in collaborative filtering. Y Koren, R Bell, Recommender Systems Handbook. SpringerY. Koren and R. Bell, \"Advances in collaborative filtering,\" in Recom- mender Systems Handbook. Springer, 2011.\n\nFism: factored item similarity models for top-n recommender systems. S Kabbur, X Ning, G Karypis, SIGKDD. S. Kabbur, X. Ning, and G. Karypis, \"Fism: factored item similarity models for top-n recommender systems,\" in SIGKDD, 2013.\n\nS Zhang, L Yao, A Sun, abs/1707.07435Deep learning based recommender system: A survey and new perspectives. arXivS. Zhang, L. Yao, and A. Sun, \"Deep learning based recommender system: A survey and new perspectives,\" arXiv, vol. abs/1707.07435, 2017.\n\nWhat your images reveal: Exploiting visual contents for point-of-interest recommendation. S Wang, Y Wang, J Tang, K Shu, S Ranganath, H Liu, WWWS. Wang, Y. Wang, J. Tang, K. Shu, S. Ranganath, and H. Liu, \"What your images reveal: Exploiting visual contents for point-of-interest recommendation,\" in WWW, 2017.\n\nVisually-aware fashion recommendation and design with generative image models. W Kang, C Fang, Z Wang, J Mcauley, ICDM. W. Kang, C. Fang, Z. Wang, and J. McAuley, \"Visually-aware fashion recommendation and design with generative image models,\" in ICDM, 2017.\n\nCollaborative deep learning for recommender systems. H Wang, N Wang, D Yeung, SIGKDD. H. Wang, N. Wang, and D. Yeung, \"Collaborative deep learning for recommender systems,\" in SIGKDD, 2015.\n\nConvolutional matrix factorization for document context-aware recommendation. D H Kim, C Park, J Oh, S Lee, H Yu, RecSysD. H. Kim, C. Park, J. Oh, S. Lee, and H. Yu, \"Convolutional matrix factorization for document context-aware recommendation,\" in RecSys, 2016.\n\nNeural collaborative filtering. X He, L Liao, H Zhang, L Nie, X Hu, T Chua, WWWX. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T. Chua, \"Neural collaborative filtering,\" in WWW, 2017.\n\nAutorec: Autoencoders meet collaborative filtering. S Sedhain, A K Menon, S Sanner, L Xie, WWWS. Sedhain, A. K. Menon, S. Sanner, and L. Xie, \"Autorec: Autoencoders meet collaborative filtering,\" in WWW, 2015.\n\nCollaborative filtering with temporal dynamics. Y Koren, Communications of the ACM. Y. Koren, \"Collaborative filtering with temporal dynamics,\" Communica- tions of the ACM, 2010.\n\n. C Wu, A Ahmed, A Beutel, A J Smola, H Jing, Recurrent recommender networks,\" in WSDMC. Wu, A. Ahmed, A. Beutel, A. J. Smola, and H. Jing, \"Recurrent recommender networks,\" in WSDM, 2017.\n\nTemporal collaborative filtering with bayesian probabilistic tensor factorization. L Xiong, X Chen, T.-K Huang, J Schneider, J G Carbonell, SDM. L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and J. G. Carbonell, \"Temporal collaborative filtering with bayesian probabilistic tensor factorization,\" in SDM, 2010.\n\nTranslation-based recommendation. R He, W Kang, J Mcauley, RecSysR. He, W. Kang, and J. McAuley, \"Translation-based recommendation,\" in RecSys, 2017.\n\nVista: A visually, socially, and temporally-aware model for artistic recommendation. R He, C Fang, Z Wang, J Mcauley, RecSysR. He, C. Fang, Z. Wang, and J. McAuley, \"Vista: A visually, socially, and temporally-aware model for artistic recommendation,\" in RecSys, 2016.\n\nFusing similarity models with markov chains for sparse sequential recommendation. R He, J Mcauley, ICDM. R. He and J. McAuley, \"Fusing similarity models with markov chains for sparse sequential recommendation,\" in ICDM, 2016.\n\nPersonalized top-n sequential recommendation via convolutional sequence embedding. J Tang, K Wang, WSDM. J. Tang and K. Wang, \"Personalized top-n sequential recommendation via convolutional sequence embedding,\" in WSDM, 2018.\n\nNeural survival recommender. H Jing, A J Smola, WSDM. H. Jing and A. J. Smola, \"Neural survival recommender,\" in WSDM, 2017.\n\nContext-aware sequential recommendation. Q Liu, S Wu, D Wang, Z Li, L Wang, ICDM. Q. Liu, S. Wu, D. Wang, Z. Li, and L. Wang, \"Context-aware sequential recommendation,\" in ICDM, 2016.\n\nLatent cross: Making use of context in recurrent recommender systems. A Beutel, P Covington, S Jain, C Xu, J Li, V Gatto, E H Chi, WSDM. A. Beutel, P. Covington, S. Jain, C. Xu, J. Li, V. Gatto, and E. H. Chi, \"Latent cross: Making use of context in recurrent recommender systems,\" in WSDM, 2018.\n\nRecurrent neural networks with top-k gains for session-based recommendations. B Hidasi, A Karatzoglou, abs/1706.03847CoRR. B. Hidasi and A. Karatzoglou, \"Recurrent neural networks with top-k gains for session-based recommendations,\" CoRR, vol. abs/1706.03847, 2017.\n\nShow, attend and tell: Neural image caption generation with visual attention. K Xu, J Ba, R Kiros, K Cho, A C Courville, R Salakhutdinov, R S Zemel, Y Bengio, ICML. K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image caption generation with visual attention,\" in ICML, 2015.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, ICLR. D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and translate,\" in ICLR, 2015.\n\nAttentive collaborative filtering: Multimedia recommendation with item-and component-level attention. J Chen, H Zhang, X He, L Nie, W Liu, T Chua, SIGIR. J. Chen, H. Zhang, X. He, L. Nie, W. Liu, and T. Chua, \"Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention,\" in SIGIR, 2017.\n\nAttentional factorization machines: Learning the weight of feature interactions via attention networks. J Xiao, H Ye, X He, H Zhang, F Wu, T Chua, IJCAI. J. Xiao, H. Ye, X. He, H. Zhang, F. Wu, and T. Chua, \"Attentional factorization machines: Learning the weight of feature interactions via attention networks,\" in IJCAI, 2017.\n\nAttentionbased transactional context embedding for next-item recommendation. S Wang, L Hu, L Cao, X Huang, D Lian, W Liu, AAAI. S. Wang, L. Hu, L. Cao, X. Huang, D. Lian, and W. Liu, \"Attention- based transactional context embedding for next-item recommendation,\" in AAAI, 2018.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, arXiv:1609.08144arXiv preprintY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., \"Google's neural machine translation system: Bridging the gap between human and machine translation,\" arXiv preprint arXiv:1609.08144, 2016.\n\nDeep recurrent models with fast-forward connections for neural machine translation. J Zhou, Y Cao, X Wang, P Li, W Xu, TACLJ. Zhou, Y. Cao, X. Wang, P. Li, and W. Xu, \"Deep recurrent models with fast-forward connections for neural machine translation,\" TACL, 2016.\n\nVisualizing and understanding convolutional networks. M D Zeiler, R Fergus, ECCV. M. D. Zeiler and R. Fergus, \"Visualizing and understanding convolutional networks,\" in ECCV, 2014.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in CVPR, 2016.\n\nLayer normalization. L J Ba, R Kiros, G E Hinton, abs/1607.06450CoRR. L. J. Ba, R. Kiros, and G. E. Hinton, \"Layer normalization,\" CoRR, vol. abs/1607.06450, 2016.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML. S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in ICML, 2015.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, JMLRN. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut- dinov, \"Dropout: a simple way to prevent neural networks from overfitting,\" JMLR, 2014.\n\nAn empirical analysis of dropout in piecewise linear networks. D Warde-Farley, I J Goodfellow, A C Courville, Y Bengio, abs/1312.6197CoRR. D. Warde-Farley, I. J. Goodfellow, A. C. Courville, and Y. Bengio, \"An empirical analysis of dropout in piecewise linear networks,\" CoRR, vol. abs/1312.6197, 2013.\n\nMatrix factorization techniques for recommender systems. Y Koren, R Bell, C Volinsky, Computer. Y. Koren, R. Bell, and C. Volinsky, \"Matrix factorization techniques for recommender systems,\" Computer, 2009.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" in ICLR, 2015.\n\nA time-restricted self-attention layer for asr. D Povey, H Hadian, P Ghahremani, K Li, S Khudanpur, ICASSP. D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur, \"A time-restricted self-attention layer for asr,\" in ICASSP, 2018.\n\nLearning hierarchical representation model for next basket recommendation. P Wang, J Guo, Y Lan, J Xu, S Wan, X Cheng, SIGIR. P. Wang, J. Guo, Y. Lan, J. Xu, S. Wan, and X. Cheng, \"Learning hierarchical representation model for next basket recommendation,\" in SIGIR, 2015.\n\nAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling. S Bai, J Z Kolter, V Koltun, abs/1803.01271CoRR. S. Bai, J. Z. Kolter, and V. Koltun, \"An empirical evaluation of generic convolutional and recurrent networks for sequence modeling,\" CoRR, vol. abs/1803.01271, 2018.\n\nGradient flow in recurrent nets: the difficulty of learning long-term dependencies. S Hochreiter, Y Bengio, P Frasconi, J Schmidhuber, S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber et al., \"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies,\" 2001.\n\nImage-based recommendations on styles and substitutes. J J Mcauley, C Targett, Q Shi, A Van Den, Hengel, SIGIR. J. J. McAuley, C. Targett, Q. Shi, and A. van den Hengel, \"Image-based recommendations on styles and substitutes,\" in SIGIR, 2015.\n\nPersonalized ranking metric embedding for next new poi recommendation. S Feng, X Li, Y Zeng, G Cong, Y M Chee, Q Yuan, IJCAI. S. Feng, X. Li, Y. Zeng, G. Cong, Y. M. Chee, and Q. Yuan, \"Personalized ranking metric embedding for next new poi recommendation,\" in IJCAI, 2015.\n\nFactorization meets the neighborhood: a multifaceted collaborative filtering model. Y Koren, SIGKDD. Y. Koren, \"Factorization meets the neighborhood: a multifaceted collab- orative filtering model,\" in SIGKDD, 2008.\n", "annotations": {"author": "[{\"end\":76,\"start\":44},{\"end\":110,\"start\":77},{\"end\":119,\"start\":111},{\"end\":126,\"start\":120}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":55},{\"end\":91,\"start\":84},{\"end\":118,\"start\":115}]", "author_first_name": "[{\"end\":54,\"start\":44},{\"end\":83,\"start\":77},{\"end\":112,\"start\":111},{\"end\":114,\"start\":113},{\"end\":125,\"start\":120}]", "author_affiliation": null, "title": "[{\"end\":41,\"start\":1},{\"end\":167,\"start\":127}]", "venue": null, "abstract": "[{\"end\":1729,\"start\":169}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2499,\"start\":2496},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2658,\"start\":2655},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3201,\"start\":3198},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5579,\"start\":5576},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5584,\"start\":5581},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5749,\"start\":5746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5766,\"start\":5763},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6030,\"start\":6027},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6035,\"start\":6032},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6185,\"start\":6182},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6495,\"start\":6492},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6585,\"start\":6581},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6591,\"start\":6587},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6602,\"start\":6598},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6608,\"start\":6604},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6728,\"start\":6724},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6807,\"start\":6803},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7052,\"start\":7048},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7406,\"start\":7402},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7412,\"start\":7408},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8094,\"start\":8091},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8465,\"start\":8461},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8552,\"start\":8548},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8762,\"start\":8758},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8854,\"start\":8851},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8860,\"start\":8856},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8866,\"start\":8862},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8983,\"start\":8980},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9065,\"start\":9061},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9291,\"start\":9288},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9419,\"start\":9415},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9448,\"start\":9444},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9807,\"start\":9803},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9813,\"start\":9809},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9873,\"start\":9869},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10192,\"start\":10189},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10344,\"start\":10340},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10350,\"start\":10346},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13058,\"start\":13055},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13282,\"start\":13279},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13991,\"start\":13987},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14100,\"start\":14097},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16399,\"start\":16396},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16939,\"start\":16935},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17066,\"start\":17062},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17493,\"start\":17490},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17499,\"start\":17495},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17505,\"start\":17501},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18006,\"start\":18002},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18039,\"start\":18035},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18625,\"start\":18621},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18971,\"start\":18967},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20697,\"start\":20693},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20707,\"start\":20704},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20722,\"start\":20718},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20851,\"start\":20848},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20864,\"start\":20860},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20877,\"start\":20874},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22066,\"start\":22062},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23165,\"start\":23162},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23464,\"start\":23461},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24039,\"start\":24035},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24216,\"start\":24212},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24830,\"start\":24827},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25133,\"start\":25130},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25905,\"start\":25902},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25915,\"start\":25911},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25930,\"start\":25926},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25967,\"start\":25963},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25979,\"start\":25975},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25991,\"start\":25987},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26826,\"start\":26823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26832,\"start\":26828},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26838,\"start\":26834},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26990,\"start\":26987},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26996,\"start\":26992},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27430,\"start\":27426},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28495,\"start\":28492},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28501,\"start\":28497},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28507,\"start\":28503},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29563,\"start\":29560},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30089,\"start\":30085},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30412,\"start\":30409},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30580,\"start\":30576},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30789,\"start\":30785},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31081,\"start\":31077},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31091,\"start\":31087},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31104,\"start\":31100},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31287,\"start\":31283},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31300,\"start\":31296},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31697,\"start\":31693},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32533,\"start\":32529},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33146,\"start\":33142},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33152,\"start\":33148},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33554,\"start\":33550},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33560,\"start\":33556},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35385,\"start\":35382},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":42668,\"start\":42664}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40932,\"start\":40732},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41130,\"start\":40933},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41391,\"start\":41131},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41476,\"start\":41392},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41650,\"start\":41477},{\"attributes\":{\"id\":\"fig_6\"},\"end\":41843,\"start\":41651},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42306,\"start\":41844},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42493,\"start\":42307},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42617,\"start\":42494},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43153,\"start\":42618},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43474,\"start\":43154},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45440,\"start\":43475},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45537,\"start\":45441}]", "paragraph": "[{\"end\":2256,\"start\":1748},{\"end\":2659,\"start\":2258},{\"end\":3481,\"start\":2661},{\"end\":4027,\"start\":3483},{\"end\":4559,\"start\":4029},{\"end\":4989,\"start\":4561},{\"end\":5311,\"start\":4991},{\"end\":5813,\"start\":5341},{\"end\":6365,\"start\":5815},{\"end\":6844,\"start\":6367},{\"end\":7770,\"start\":6875},{\"end\":8763,\"start\":7803},{\"end\":9292,\"start\":8765},{\"end\":9956,\"start\":9320},{\"end\":10838,\"start\":9958},{\"end\":11213,\"start\":11096},{\"end\":11695,\"start\":11234},{\"end\":12019,\"start\":11735},{\"end\":12657,\"start\":12042},{\"end\":12937,\"start\":12659},{\"end\":13218,\"start\":13001},{\"end\":13297,\"start\":13246},{\"end\":13724,\"start\":13342},{\"end\":14273,\"start\":13726},{\"end\":14352,\"start\":14323},{\"end\":14907,\"start\":14379},{\"end\":15281,\"start\":14909},{\"end\":15561,\"start\":15429},{\"end\":15975,\"start\":15599},{\"end\":16107,\"start\":16050},{\"end\":16465,\"start\":16109},{\"end\":16794,\"start\":16503},{\"end\":17798,\"start\":16796},{\"end\":18252,\"start\":17800},{\"end\":18443,\"start\":18289},{\"end\":19022,\"start\":18445},{\"end\":19301,\"start\":19046},{\"end\":19608,\"start\":19326},{\"end\":19754,\"start\":19610},{\"end\":19793,\"start\":19782},{\"end\":19869,\"start\":19800},{\"end\":20370,\"start\":19911},{\"end\":20500,\"start\":20405},{\"end\":21092,\"start\":20502},{\"end\":21332,\"start\":21125},{\"end\":21609,\"start\":21356},{\"end\":21887,\"start\":21692},{\"end\":22013,\"start\":21965},{\"end\":22298,\"start\":22015},{\"end\":22726,\"start\":22325},{\"end\":23618,\"start\":22728},{\"end\":23793,\"start\":23620},{\"end\":24217,\"start\":23795},{\"end\":24408,\"start\":24235},{\"end\":24439,\"start\":24410},{\"end\":24580,\"start\":24441},{\"end\":24920,\"start\":24603},{\"end\":25260,\"start\":24955},{\"end\":25639,\"start\":25298},{\"end\":26725,\"start\":25668},{\"end\":27310,\"start\":26727},{\"end\":27431,\"start\":27312},{\"end\":27945,\"start\":27451},{\"end\":28440,\"start\":27961},{\"end\":29140,\"start\":28442},{\"end\":30090,\"start\":29166},{\"end\":30397,\"start\":30092},{\"end\":32215,\"start\":30399},{\"end\":32959,\"start\":32245},{\"end\":33017,\"start\":32961},{\"end\":33469,\"start\":33043},{\"end\":33745,\"start\":33471},{\"end\":34349,\"start\":33779},{\"end\":34762,\"start\":34351},{\"end\":35078,\"start\":34764},{\"end\":35757,\"start\":35100},{\"end\":36145,\"start\":35810},{\"end\":37918,\"start\":36147},{\"end\":38292,\"start\":37955},{\"end\":38589,\"start\":38294},{\"end\":38640,\"start\":38591},{\"end\":38815,\"start\":38642},{\"end\":39274,\"start\":38817},{\"end\":40049,\"start\":39276},{\"end\":40731,\"start\":40067}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11095,\"start\":10839},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11734,\"start\":11696},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13000,\"start\":12938},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13341,\"start\":13298},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14322,\"start\":14274},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14378,\"start\":14353},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15428,\"start\":15282},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16049,\"start\":15976},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16502,\"start\":16466},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18288,\"start\":18253},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19325,\"start\":19302},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19781,\"start\":19755},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19799,\"start\":19794},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19910,\"start\":19870},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20404,\"start\":20371},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21124,\"start\":21093},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21691,\"start\":21610},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21964,\"start\":21888},{\"attributes\":{\"id\":\"formula_18\"},\"end\":24602,\"start\":24581},{\"attributes\":{\"id\":\"formula_19\"},\"end\":24954,\"start\":24921},{\"attributes\":{\"id\":\"formula_20\"},\"end\":25297,\"start\":25261},{\"attributes\":{\"id\":\"formula_21\"},\"end\":35770,\"start\":35758}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12018,\"start\":12011},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28946,\"start\":28938},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31363,\"start\":31354},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35215,\"start\":35207},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":37316,\"start\":37309}]", "section_header": "[{\"end\":1746,\"start\":1731},{\"end\":5339,\"start\":5314},{\"end\":6873,\"start\":6847},{\"end\":7801,\"start\":7773},{\"end\":9318,\"start\":9295},{\"end\":11232,\"start\":11216},{\"end\":12040,\"start\":12022},{\"end\":13244,\"start\":13221},{\"end\":15597,\"start\":15564},{\"end\":19044,\"start\":19025},{\"end\":21354,\"start\":21335},{\"end\":22323,\"start\":22301},{\"end\":24233,\"start\":24220},{\"end\":25666,\"start\":25642},{\"end\":27449,\"start\":27434},{\"end\":27959,\"start\":27948},{\"end\":29164,\"start\":29143},{\"end\":32243,\"start\":32218},{\"end\":33041,\"start\":33020},{\"end\":33777,\"start\":33748},{\"end\":35098,\"start\":35081},{\"end\":35808,\"start\":35772},{\"end\":37953,\"start\":37921},{\"end\":40065,\"start\":40052},{\"end\":40743,\"start\":40733},{\"end\":40943,\"start\":40934},{\"end\":41133,\"start\":41132},{\"end\":41403,\"start\":41393},{\"end\":41488,\"start\":41478},{\"end\":41662,\"start\":41652},{\"end\":42318,\"start\":42308},{\"end\":42504,\"start\":42495},{\"end\":43165,\"start\":43155},{\"end\":43486,\"start\":43476},{\"end\":45451,\"start\":45442}]", "table": "[{\"end\":42617,\"start\":42515},{\"end\":43153,\"start\":43030},{\"end\":43474,\"start\":43208},{\"end\":45440,\"start\":43708}]", "figure_caption": "[{\"end\":40932,\"start\":40745},{\"end\":41130,\"start\":40945},{\"end\":41391,\"start\":41134},{\"end\":41476,\"start\":41405},{\"end\":41650,\"start\":41490},{\"end\":41843,\"start\":41664},{\"end\":42306,\"start\":41846},{\"end\":42493,\"start\":42320},{\"end\":42515,\"start\":42506},{\"end\":43030,\"start\":42620},{\"end\":43208,\"start\":43168},{\"end\":43708,\"start\":43489},{\"end\":45537,\"start\":45453}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4025,\"start\":4016},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11530,\"start\":11522},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34775,\"start\":34767},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36176,\"start\":36168},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38326,\"start\":38318},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39718,\"start\":39710}]", "bib_author_first_name": "[{\"end\":45611,\"start\":45610},{\"end\":45621,\"start\":45620},{\"end\":45638,\"start\":45637},{\"end\":45861,\"start\":45860},{\"end\":45871,\"start\":45870},{\"end\":45886,\"start\":45885},{\"end\":45899,\"start\":45898},{\"end\":46049,\"start\":46048},{\"end\":46060,\"start\":46059},{\"end\":46071,\"start\":46070},{\"end\":46081,\"start\":46080},{\"end\":46094,\"start\":46093},{\"end\":46103,\"start\":46102},{\"end\":46105,\"start\":46104},{\"end\":46114,\"start\":46113},{\"end\":46124,\"start\":46123},{\"end\":46376,\"start\":46375},{\"end\":46382,\"start\":46381},{\"end\":46391,\"start\":46390},{\"end\":46576,\"start\":46575},{\"end\":46586,\"start\":46585},{\"end\":46603,\"start\":46602},{\"end\":46614,\"start\":46613},{\"end\":46805,\"start\":46804},{\"end\":46814,\"start\":46813},{\"end\":46824,\"start\":46823},{\"end\":46835,\"start\":46834},{\"end\":46991,\"start\":46990},{\"end\":47000,\"start\":46999},{\"end\":47228,\"start\":47227},{\"end\":47238,\"start\":47237},{\"end\":47246,\"start\":47245},{\"end\":47390,\"start\":47389},{\"end\":47399,\"start\":47398},{\"end\":47406,\"start\":47405},{\"end\":47731,\"start\":47730},{\"end\":47739,\"start\":47738},{\"end\":47747,\"start\":47746},{\"end\":47755,\"start\":47754},{\"end\":47762,\"start\":47761},{\"end\":47775,\"start\":47774},{\"end\":48032,\"start\":48031},{\"end\":48040,\"start\":48039},{\"end\":48048,\"start\":48047},{\"end\":48056,\"start\":48055},{\"end\":48266,\"start\":48265},{\"end\":48274,\"start\":48273},{\"end\":48282,\"start\":48281},{\"end\":48482,\"start\":48481},{\"end\":48484,\"start\":48483},{\"end\":48491,\"start\":48490},{\"end\":48499,\"start\":48498},{\"end\":48505,\"start\":48504},{\"end\":48512,\"start\":48511},{\"end\":48700,\"start\":48699},{\"end\":48706,\"start\":48705},{\"end\":48714,\"start\":48713},{\"end\":48723,\"start\":48722},{\"end\":48730,\"start\":48729},{\"end\":48736,\"start\":48735},{\"end\":48902,\"start\":48901},{\"end\":48913,\"start\":48912},{\"end\":48915,\"start\":48914},{\"end\":48924,\"start\":48923},{\"end\":48934,\"start\":48933},{\"end\":49109,\"start\":49108},{\"end\":49243,\"start\":49242},{\"end\":49249,\"start\":49248},{\"end\":49258,\"start\":49257},{\"end\":49268,\"start\":49267},{\"end\":49270,\"start\":49269},{\"end\":49279,\"start\":49278},{\"end\":49514,\"start\":49513},{\"end\":49523,\"start\":49522},{\"end\":49534,\"start\":49530},{\"end\":49543,\"start\":49542},{\"end\":49556,\"start\":49555},{\"end\":49558,\"start\":49557},{\"end\":49777,\"start\":49776},{\"end\":49783,\"start\":49782},{\"end\":49791,\"start\":49790},{\"end\":49979,\"start\":49978},{\"end\":49985,\"start\":49984},{\"end\":49993,\"start\":49992},{\"end\":50001,\"start\":50000},{\"end\":50246,\"start\":50245},{\"end\":50252,\"start\":50251},{\"end\":50474,\"start\":50473},{\"end\":50482,\"start\":50481},{\"end\":50647,\"start\":50646},{\"end\":50655,\"start\":50654},{\"end\":50657,\"start\":50656},{\"end\":50785,\"start\":50784},{\"end\":50792,\"start\":50791},{\"end\":50798,\"start\":50797},{\"end\":50806,\"start\":50805},{\"end\":50812,\"start\":50811},{\"end\":50999,\"start\":50998},{\"end\":51009,\"start\":51008},{\"end\":51022,\"start\":51021},{\"end\":51030,\"start\":51029},{\"end\":51036,\"start\":51035},{\"end\":51042,\"start\":51041},{\"end\":51051,\"start\":51050},{\"end\":51053,\"start\":51052},{\"end\":51305,\"start\":51304},{\"end\":51315,\"start\":51314},{\"end\":51572,\"start\":51571},{\"end\":51578,\"start\":51577},{\"end\":51584,\"start\":51583},{\"end\":51593,\"start\":51592},{\"end\":51600,\"start\":51599},{\"end\":51602,\"start\":51601},{\"end\":51615,\"start\":51614},{\"end\":51632,\"start\":51631},{\"end\":51634,\"start\":51633},{\"end\":51643,\"start\":51642},{\"end\":51921,\"start\":51920},{\"end\":51933,\"start\":51932},{\"end\":51940,\"start\":51939},{\"end\":52183,\"start\":52182},{\"end\":52191,\"start\":52190},{\"end\":52200,\"start\":52199},{\"end\":52206,\"start\":52205},{\"end\":52213,\"start\":52212},{\"end\":52220,\"start\":52219},{\"end\":52515,\"start\":52514},{\"end\":52523,\"start\":52522},{\"end\":52529,\"start\":52528},{\"end\":52535,\"start\":52534},{\"end\":52544,\"start\":52543},{\"end\":52550,\"start\":52549},{\"end\":52818,\"start\":52817},{\"end\":52826,\"start\":52825},{\"end\":52832,\"start\":52831},{\"end\":52839,\"start\":52838},{\"end\":52848,\"start\":52847},{\"end\":52856,\"start\":52855},{\"end\":53121,\"start\":53120},{\"end\":53127,\"start\":53126},{\"end\":53139,\"start\":53138},{\"end\":53147,\"start\":53146},{\"end\":53149,\"start\":53148},{\"end\":53155,\"start\":53154},{\"end\":53166,\"start\":53165},{\"end\":53178,\"start\":53177},{\"end\":53188,\"start\":53187},{\"end\":53195,\"start\":53194},{\"end\":53202,\"start\":53201},{\"end\":53581,\"start\":53580},{\"end\":53589,\"start\":53588},{\"end\":53596,\"start\":53595},{\"end\":53604,\"start\":53603},{\"end\":53610,\"start\":53609},{\"end\":53817,\"start\":53816},{\"end\":53819,\"start\":53818},{\"end\":53829,\"start\":53828},{\"end\":53991,\"start\":53990},{\"end\":53997,\"start\":53996},{\"end\":54006,\"start\":54005},{\"end\":54013,\"start\":54012},{\"end\":54148,\"start\":54147},{\"end\":54150,\"start\":54149},{\"end\":54156,\"start\":54155},{\"end\":54165,\"start\":54164},{\"end\":54167,\"start\":54166},{\"end\":54386,\"start\":54385},{\"end\":54395,\"start\":54394},{\"end\":54616,\"start\":54615},{\"end\":54630,\"start\":54629},{\"end\":54632,\"start\":54631},{\"end\":54642,\"start\":54641},{\"end\":54656,\"start\":54655},{\"end\":54669,\"start\":54668},{\"end\":54917,\"start\":54916},{\"end\":54933,\"start\":54932},{\"end\":54935,\"start\":54934},{\"end\":54949,\"start\":54948},{\"end\":54951,\"start\":54950},{\"end\":54964,\"start\":54963},{\"end\":55215,\"start\":55214},{\"end\":55224,\"start\":55223},{\"end\":55232,\"start\":55231},{\"end\":55410,\"start\":55409},{\"end\":55412,\"start\":55411},{\"end\":55422,\"start\":55421},{\"end\":55568,\"start\":55567},{\"end\":55577,\"start\":55576},{\"end\":55587,\"start\":55586},{\"end\":55601,\"start\":55600},{\"end\":55607,\"start\":55606},{\"end\":55832,\"start\":55831},{\"end\":55840,\"start\":55839},{\"end\":55847,\"start\":55846},{\"end\":55854,\"start\":55853},{\"end\":55860,\"start\":55859},{\"end\":55867,\"start\":55866},{\"end\":56126,\"start\":56125},{\"end\":56133,\"start\":56132},{\"end\":56135,\"start\":56134},{\"end\":56145,\"start\":56144},{\"end\":56427,\"start\":56426},{\"end\":56441,\"start\":56440},{\"end\":56451,\"start\":56450},{\"end\":56463,\"start\":56462},{\"end\":56688,\"start\":56687},{\"end\":56690,\"start\":56689},{\"end\":56701,\"start\":56700},{\"end\":56712,\"start\":56711},{\"end\":56719,\"start\":56718},{\"end\":56948,\"start\":56947},{\"end\":56956,\"start\":56955},{\"end\":56962,\"start\":56961},{\"end\":56970,\"start\":56969},{\"end\":56978,\"start\":56977},{\"end\":56980,\"start\":56979},{\"end\":56988,\"start\":56987},{\"end\":57236,\"start\":57235}]", "bib_author_last_name": "[{\"end\":45618,\"start\":45612},{\"end\":45635,\"start\":45622},{\"end\":45653,\"start\":45639},{\"end\":45868,\"start\":45862},{\"end\":45883,\"start\":45872},{\"end\":45896,\"start\":45887},{\"end\":45904,\"start\":45900},{\"end\":46057,\"start\":46050},{\"end\":46068,\"start\":46061},{\"end\":46078,\"start\":46072},{\"end\":46091,\"start\":46082},{\"end\":46100,\"start\":46095},{\"end\":46111,\"start\":46106},{\"end\":46121,\"start\":46115},{\"end\":46135,\"start\":46125},{\"end\":46379,\"start\":46377},{\"end\":46388,\"start\":46383},{\"end\":46400,\"start\":46392},{\"end\":46583,\"start\":46577},{\"end\":46600,\"start\":46587},{\"end\":46611,\"start\":46604},{\"end\":46629,\"start\":46615},{\"end\":46811,\"start\":46806},{\"end\":46821,\"start\":46815},{\"end\":46832,\"start\":46825},{\"end\":46842,\"start\":46836},{\"end\":46997,\"start\":46992},{\"end\":47005,\"start\":47001},{\"end\":47235,\"start\":47229},{\"end\":47243,\"start\":47239},{\"end\":47254,\"start\":47247},{\"end\":47396,\"start\":47391},{\"end\":47403,\"start\":47400},{\"end\":47410,\"start\":47407},{\"end\":47736,\"start\":47732},{\"end\":47744,\"start\":47740},{\"end\":47752,\"start\":47748},{\"end\":47759,\"start\":47756},{\"end\":47772,\"start\":47763},{\"end\":47779,\"start\":47776},{\"end\":48037,\"start\":48033},{\"end\":48045,\"start\":48041},{\"end\":48053,\"start\":48049},{\"end\":48064,\"start\":48057},{\"end\":48271,\"start\":48267},{\"end\":48279,\"start\":48275},{\"end\":48288,\"start\":48283},{\"end\":48488,\"start\":48485},{\"end\":48496,\"start\":48492},{\"end\":48502,\"start\":48500},{\"end\":48509,\"start\":48506},{\"end\":48515,\"start\":48513},{\"end\":48703,\"start\":48701},{\"end\":48711,\"start\":48707},{\"end\":48720,\"start\":48715},{\"end\":48727,\"start\":48724},{\"end\":48733,\"start\":48731},{\"end\":48741,\"start\":48737},{\"end\":48910,\"start\":48903},{\"end\":48921,\"start\":48916},{\"end\":48931,\"start\":48925},{\"end\":48938,\"start\":48935},{\"end\":49115,\"start\":49110},{\"end\":49246,\"start\":49244},{\"end\":49255,\"start\":49250},{\"end\":49265,\"start\":49259},{\"end\":49276,\"start\":49271},{\"end\":49284,\"start\":49280},{\"end\":49520,\"start\":49515},{\"end\":49528,\"start\":49524},{\"end\":49540,\"start\":49535},{\"end\":49553,\"start\":49544},{\"end\":49568,\"start\":49559},{\"end\":49780,\"start\":49778},{\"end\":49788,\"start\":49784},{\"end\":49799,\"start\":49792},{\"end\":49982,\"start\":49980},{\"end\":49990,\"start\":49986},{\"end\":49998,\"start\":49994},{\"end\":50009,\"start\":50002},{\"end\":50249,\"start\":50247},{\"end\":50260,\"start\":50253},{\"end\":50479,\"start\":50475},{\"end\":50487,\"start\":50483},{\"end\":50652,\"start\":50648},{\"end\":50663,\"start\":50658},{\"end\":50789,\"start\":50786},{\"end\":50795,\"start\":50793},{\"end\":50803,\"start\":50799},{\"end\":50809,\"start\":50807},{\"end\":50817,\"start\":50813},{\"end\":51006,\"start\":51000},{\"end\":51019,\"start\":51010},{\"end\":51027,\"start\":51023},{\"end\":51033,\"start\":51031},{\"end\":51039,\"start\":51037},{\"end\":51048,\"start\":51043},{\"end\":51057,\"start\":51054},{\"end\":51312,\"start\":51306},{\"end\":51327,\"start\":51316},{\"end\":51575,\"start\":51573},{\"end\":51581,\"start\":51579},{\"end\":51590,\"start\":51585},{\"end\":51597,\"start\":51594},{\"end\":51612,\"start\":51603},{\"end\":51629,\"start\":51616},{\"end\":51640,\"start\":51635},{\"end\":51650,\"start\":51644},{\"end\":51930,\"start\":51922},{\"end\":51937,\"start\":51934},{\"end\":51947,\"start\":51941},{\"end\":52188,\"start\":52184},{\"end\":52197,\"start\":52192},{\"end\":52203,\"start\":52201},{\"end\":52210,\"start\":52207},{\"end\":52217,\"start\":52214},{\"end\":52225,\"start\":52221},{\"end\":52520,\"start\":52516},{\"end\":52526,\"start\":52524},{\"end\":52532,\"start\":52530},{\"end\":52541,\"start\":52536},{\"end\":52547,\"start\":52545},{\"end\":52555,\"start\":52551},{\"end\":52823,\"start\":52819},{\"end\":52829,\"start\":52827},{\"end\":52836,\"start\":52833},{\"end\":52845,\"start\":52840},{\"end\":52853,\"start\":52849},{\"end\":52860,\"start\":52857},{\"end\":53124,\"start\":53122},{\"end\":53136,\"start\":53128},{\"end\":53144,\"start\":53140},{\"end\":53152,\"start\":53150},{\"end\":53163,\"start\":53156},{\"end\":53175,\"start\":53167},{\"end\":53185,\"start\":53179},{\"end\":53192,\"start\":53189},{\"end\":53199,\"start\":53196},{\"end\":53211,\"start\":53203},{\"end\":53586,\"start\":53582},{\"end\":53593,\"start\":53590},{\"end\":53601,\"start\":53597},{\"end\":53607,\"start\":53605},{\"end\":53613,\"start\":53611},{\"end\":53826,\"start\":53820},{\"end\":53836,\"start\":53830},{\"end\":53994,\"start\":53992},{\"end\":54003,\"start\":53998},{\"end\":54010,\"start\":54007},{\"end\":54017,\"start\":54014},{\"end\":54153,\"start\":54151},{\"end\":54162,\"start\":54157},{\"end\":54174,\"start\":54168},{\"end\":54392,\"start\":54387},{\"end\":54403,\"start\":54396},{\"end\":54627,\"start\":54617},{\"end\":54639,\"start\":54633},{\"end\":54653,\"start\":54643},{\"end\":54666,\"start\":54657},{\"end\":54683,\"start\":54670},{\"end\":54930,\"start\":54918},{\"end\":54946,\"start\":54936},{\"end\":54961,\"start\":54952},{\"end\":54971,\"start\":54965},{\"end\":55221,\"start\":55216},{\"end\":55229,\"start\":55225},{\"end\":55241,\"start\":55233},{\"end\":55419,\"start\":55413},{\"end\":55425,\"start\":55423},{\"end\":55574,\"start\":55569},{\"end\":55584,\"start\":55578},{\"end\":55598,\"start\":55588},{\"end\":55604,\"start\":55602},{\"end\":55617,\"start\":55608},{\"end\":55837,\"start\":55833},{\"end\":55844,\"start\":55841},{\"end\":55851,\"start\":55848},{\"end\":55857,\"start\":55855},{\"end\":55864,\"start\":55861},{\"end\":55873,\"start\":55868},{\"end\":56130,\"start\":56127},{\"end\":56142,\"start\":56136},{\"end\":56152,\"start\":56146},{\"end\":56438,\"start\":56428},{\"end\":56448,\"start\":56442},{\"end\":56460,\"start\":56452},{\"end\":56475,\"start\":56464},{\"end\":56698,\"start\":56691},{\"end\":56709,\"start\":56702},{\"end\":56716,\"start\":56713},{\"end\":56727,\"start\":56720},{\"end\":56735,\"start\":56729},{\"end\":56953,\"start\":56949},{\"end\":56959,\"start\":56957},{\"end\":56967,\"start\":56963},{\"end\":56975,\"start\":56971},{\"end\":56985,\"start\":56981},{\"end\":56993,\"start\":56989},{\"end\":57242,\"start\":57237}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45796,\"start\":45539},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11810482},\"end\":46044,\"start\":45798},{\"attributes\":{\"id\":\"b2\"},\"end\":46317,\"start\":46046},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10537313},\"end\":46514,\"start\":46319},{\"attributes\":{\"id\":\"b4\"},\"end\":46772,\"start\":46516},{\"attributes\":{\"id\":\"b5\"},\"end\":46951,\"start\":46774},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14698210},\"end\":47156,\"start\":46953},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207204749},\"end\":47387,\"start\":47158},{\"attributes\":{\"doi\":\"abs/1707.07435\",\"id\":\"b8\"},\"end\":47638,\"start\":47389},{\"attributes\":{\"id\":\"b9\"},\"end\":47950,\"start\":47640},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":23787675},\"end\":48210,\"start\":47952},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4833213},\"end\":48401,\"start\":48212},{\"attributes\":{\"id\":\"b12\"},\"end\":48665,\"start\":48403},{\"attributes\":{\"id\":\"b13\"},\"end\":48847,\"start\":48667},{\"attributes\":{\"id\":\"b14\"},\"end\":49058,\"start\":48849},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3022077},\"end\":49238,\"start\":49060},{\"attributes\":{\"id\":\"b16\"},\"end\":49428,\"start\":49240},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1173916},\"end\":49740,\"start\":49430},{\"attributes\":{\"id\":\"b18\"},\"end\":49891,\"start\":49742},{\"attributes\":{\"id\":\"b19\"},\"end\":50161,\"start\":49893},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9124261},\"end\":50388,\"start\":50163},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":39847715},\"end\":50615,\"start\":50390},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207246122},\"end\":50741,\"start\":50617},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":15624722},\"end\":50926,\"start\":50743},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":44051841},\"end\":51224,\"start\":50928},{\"attributes\":{\"doi\":\"abs/1706.03847\",\"id\":\"b25\",\"matched_paper_id\":1159769},\"end\":51491,\"start\":51226},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1055111},\"end\":51847,\"start\":51493},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11212020},\"end\":52078,\"start\":51849},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":20970043},\"end\":52408,\"start\":52080},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3836251},\"end\":52738,\"start\":52410},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":19100030},\"end\":53018,\"start\":52740},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b31\"},\"end\":53494,\"start\":53020},{\"attributes\":{\"id\":\"b32\"},\"end\":53760,\"start\":53496},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3960646},\"end\":53942,\"start\":53762},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206594692},\"end\":54124,\"start\":53944},{\"attributes\":{\"doi\":\"abs/1607.06450\",\"id\":\"b35\",\"matched_paper_id\":8236317},\"end\":54289,\"start\":54126},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5808102},\"end\":54546,\"start\":54291},{\"attributes\":{\"id\":\"b37\"},\"end\":54851,\"start\":54548},{\"attributes\":{\"doi\":\"abs/1312.6197\",\"id\":\"b38\",\"matched_paper_id\":14538467},\"end\":55155,\"start\":54853},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":58370896},\"end\":55363,\"start\":55157},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6628106},\"end\":55517,\"start\":55365},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":46974195},\"end\":55754,\"start\":55519},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4002880},\"end\":56028,\"start\":55756},{\"attributes\":{\"doi\":\"abs/1803.01271\",\"id\":\"b43\",\"matched_paper_id\":4747877},\"end\":56340,\"start\":56030},{\"attributes\":{\"id\":\"b44\"},\"end\":56630,\"start\":56342},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1012652},\"end\":56874,\"start\":56632},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14873341},\"end\":57149,\"start\":56876},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":207168823},\"end\":57366,\"start\":57151}]", "bib_title": "[{\"end\":45858,\"start\":45798},{\"end\":46373,\"start\":46319},{\"end\":46988,\"start\":46953},{\"end\":47225,\"start\":47158},{\"end\":48029,\"start\":47952},{\"end\":48263,\"start\":48212},{\"end\":49106,\"start\":49060},{\"end\":49511,\"start\":49430},{\"end\":50243,\"start\":50163},{\"end\":50471,\"start\":50390},{\"end\":50644,\"start\":50617},{\"end\":50782,\"start\":50743},{\"end\":50996,\"start\":50928},{\"end\":51302,\"start\":51226},{\"end\":51569,\"start\":51493},{\"end\":51918,\"start\":51849},{\"end\":52180,\"start\":52080},{\"end\":52512,\"start\":52410},{\"end\":52815,\"start\":52740},{\"end\":53814,\"start\":53762},{\"end\":53988,\"start\":53944},{\"end\":54145,\"start\":54126},{\"end\":54383,\"start\":54291},{\"end\":54914,\"start\":54853},{\"end\":55212,\"start\":55157},{\"end\":55407,\"start\":55365},{\"end\":55565,\"start\":55519},{\"end\":55829,\"start\":55756},{\"end\":56123,\"start\":56030},{\"end\":56685,\"start\":56632},{\"end\":56945,\"start\":56876},{\"end\":57233,\"start\":57151}]", "bib_author": "[{\"end\":45620,\"start\":45610},{\"end\":45637,\"start\":45620},{\"end\":45655,\"start\":45637},{\"end\":45870,\"start\":45860},{\"end\":45885,\"start\":45870},{\"end\":45898,\"start\":45885},{\"end\":45906,\"start\":45898},{\"end\":46059,\"start\":46048},{\"end\":46070,\"start\":46059},{\"end\":46080,\"start\":46070},{\"end\":46093,\"start\":46080},{\"end\":46102,\"start\":46093},{\"end\":46113,\"start\":46102},{\"end\":46123,\"start\":46113},{\"end\":46137,\"start\":46123},{\"end\":46381,\"start\":46375},{\"end\":46390,\"start\":46381},{\"end\":46402,\"start\":46390},{\"end\":46585,\"start\":46575},{\"end\":46602,\"start\":46585},{\"end\":46613,\"start\":46602},{\"end\":46631,\"start\":46613},{\"end\":46813,\"start\":46804},{\"end\":46823,\"start\":46813},{\"end\":46834,\"start\":46823},{\"end\":46844,\"start\":46834},{\"end\":46999,\"start\":46990},{\"end\":47007,\"start\":46999},{\"end\":47237,\"start\":47227},{\"end\":47245,\"start\":47237},{\"end\":47256,\"start\":47245},{\"end\":47398,\"start\":47389},{\"end\":47405,\"start\":47398},{\"end\":47412,\"start\":47405},{\"end\":47738,\"start\":47730},{\"end\":47746,\"start\":47738},{\"end\":47754,\"start\":47746},{\"end\":47761,\"start\":47754},{\"end\":47774,\"start\":47761},{\"end\":47781,\"start\":47774},{\"end\":48039,\"start\":48031},{\"end\":48047,\"start\":48039},{\"end\":48055,\"start\":48047},{\"end\":48066,\"start\":48055},{\"end\":48273,\"start\":48265},{\"end\":48281,\"start\":48273},{\"end\":48290,\"start\":48281},{\"end\":48490,\"start\":48481},{\"end\":48498,\"start\":48490},{\"end\":48504,\"start\":48498},{\"end\":48511,\"start\":48504},{\"end\":48517,\"start\":48511},{\"end\":48705,\"start\":48699},{\"end\":48713,\"start\":48705},{\"end\":48722,\"start\":48713},{\"end\":48729,\"start\":48722},{\"end\":48735,\"start\":48729},{\"end\":48743,\"start\":48735},{\"end\":48912,\"start\":48901},{\"end\":48923,\"start\":48912},{\"end\":48933,\"start\":48923},{\"end\":48940,\"start\":48933},{\"end\":49117,\"start\":49108},{\"end\":49248,\"start\":49242},{\"end\":49257,\"start\":49248},{\"end\":49267,\"start\":49257},{\"end\":49278,\"start\":49267},{\"end\":49286,\"start\":49278},{\"end\":49522,\"start\":49513},{\"end\":49530,\"start\":49522},{\"end\":49542,\"start\":49530},{\"end\":49555,\"start\":49542},{\"end\":49570,\"start\":49555},{\"end\":49782,\"start\":49776},{\"end\":49790,\"start\":49782},{\"end\":49801,\"start\":49790},{\"end\":49984,\"start\":49978},{\"end\":49992,\"start\":49984},{\"end\":50000,\"start\":49992},{\"end\":50011,\"start\":50000},{\"end\":50251,\"start\":50245},{\"end\":50262,\"start\":50251},{\"end\":50481,\"start\":50473},{\"end\":50489,\"start\":50481},{\"end\":50654,\"start\":50646},{\"end\":50665,\"start\":50654},{\"end\":50791,\"start\":50784},{\"end\":50797,\"start\":50791},{\"end\":50805,\"start\":50797},{\"end\":50811,\"start\":50805},{\"end\":50819,\"start\":50811},{\"end\":51008,\"start\":50998},{\"end\":51021,\"start\":51008},{\"end\":51029,\"start\":51021},{\"end\":51035,\"start\":51029},{\"end\":51041,\"start\":51035},{\"end\":51050,\"start\":51041},{\"end\":51059,\"start\":51050},{\"end\":51314,\"start\":51304},{\"end\":51329,\"start\":51314},{\"end\":51577,\"start\":51571},{\"end\":51583,\"start\":51577},{\"end\":51592,\"start\":51583},{\"end\":51599,\"start\":51592},{\"end\":51614,\"start\":51599},{\"end\":51631,\"start\":51614},{\"end\":51642,\"start\":51631},{\"end\":51652,\"start\":51642},{\"end\":51932,\"start\":51920},{\"end\":51939,\"start\":51932},{\"end\":51949,\"start\":51939},{\"end\":52190,\"start\":52182},{\"end\":52199,\"start\":52190},{\"end\":52205,\"start\":52199},{\"end\":52212,\"start\":52205},{\"end\":52219,\"start\":52212},{\"end\":52227,\"start\":52219},{\"end\":52522,\"start\":52514},{\"end\":52528,\"start\":52522},{\"end\":52534,\"start\":52528},{\"end\":52543,\"start\":52534},{\"end\":52549,\"start\":52543},{\"end\":52557,\"start\":52549},{\"end\":52825,\"start\":52817},{\"end\":52831,\"start\":52825},{\"end\":52838,\"start\":52831},{\"end\":52847,\"start\":52838},{\"end\":52855,\"start\":52847},{\"end\":52862,\"start\":52855},{\"end\":53126,\"start\":53120},{\"end\":53138,\"start\":53126},{\"end\":53146,\"start\":53138},{\"end\":53154,\"start\":53146},{\"end\":53165,\"start\":53154},{\"end\":53177,\"start\":53165},{\"end\":53187,\"start\":53177},{\"end\":53194,\"start\":53187},{\"end\":53201,\"start\":53194},{\"end\":53213,\"start\":53201},{\"end\":53588,\"start\":53580},{\"end\":53595,\"start\":53588},{\"end\":53603,\"start\":53595},{\"end\":53609,\"start\":53603},{\"end\":53615,\"start\":53609},{\"end\":53828,\"start\":53816},{\"end\":53838,\"start\":53828},{\"end\":53996,\"start\":53990},{\"end\":54005,\"start\":53996},{\"end\":54012,\"start\":54005},{\"end\":54019,\"start\":54012},{\"end\":54155,\"start\":54147},{\"end\":54164,\"start\":54155},{\"end\":54176,\"start\":54164},{\"end\":54394,\"start\":54385},{\"end\":54405,\"start\":54394},{\"end\":54629,\"start\":54615},{\"end\":54641,\"start\":54629},{\"end\":54655,\"start\":54641},{\"end\":54668,\"start\":54655},{\"end\":54685,\"start\":54668},{\"end\":54932,\"start\":54916},{\"end\":54948,\"start\":54932},{\"end\":54963,\"start\":54948},{\"end\":54973,\"start\":54963},{\"end\":55223,\"start\":55214},{\"end\":55231,\"start\":55223},{\"end\":55243,\"start\":55231},{\"end\":55421,\"start\":55409},{\"end\":55427,\"start\":55421},{\"end\":55576,\"start\":55567},{\"end\":55586,\"start\":55576},{\"end\":55600,\"start\":55586},{\"end\":55606,\"start\":55600},{\"end\":55619,\"start\":55606},{\"end\":55839,\"start\":55831},{\"end\":55846,\"start\":55839},{\"end\":55853,\"start\":55846},{\"end\":55859,\"start\":55853},{\"end\":55866,\"start\":55859},{\"end\":55875,\"start\":55866},{\"end\":56132,\"start\":56125},{\"end\":56144,\"start\":56132},{\"end\":56154,\"start\":56144},{\"end\":56440,\"start\":56426},{\"end\":56450,\"start\":56440},{\"end\":56462,\"start\":56450},{\"end\":56477,\"start\":56462},{\"end\":56700,\"start\":56687},{\"end\":56711,\"start\":56700},{\"end\":56718,\"start\":56711},{\"end\":56729,\"start\":56718},{\"end\":56737,\"start\":56729},{\"end\":56955,\"start\":56947},{\"end\":56961,\"start\":56955},{\"end\":56969,\"start\":56961},{\"end\":56977,\"start\":56969},{\"end\":56987,\"start\":56977},{\"end\":56995,\"start\":56987},{\"end\":57244,\"start\":57235}]", "bib_venue": "[{\"end\":45608,\"start\":45539},{\"end\":45910,\"start\":45906},{\"end\":46406,\"start\":46402},{\"end\":46573,\"start\":46516},{\"end\":46802,\"start\":46774},{\"end\":47035,\"start\":47007},{\"end\":47262,\"start\":47256},{\"end\":47495,\"start\":47426},{\"end\":47728,\"start\":47640},{\"end\":48070,\"start\":48066},{\"end\":48296,\"start\":48290},{\"end\":48479,\"start\":48403},{\"end\":48697,\"start\":48667},{\"end\":48899,\"start\":48849},{\"end\":49142,\"start\":49117},{\"end\":49573,\"start\":49570},{\"end\":49774,\"start\":49742},{\"end\":49976,\"start\":49893},{\"end\":50266,\"start\":50262},{\"end\":50493,\"start\":50489},{\"end\":50669,\"start\":50665},{\"end\":50823,\"start\":50819},{\"end\":51063,\"start\":51059},{\"end\":51347,\"start\":51343},{\"end\":51656,\"start\":51652},{\"end\":51953,\"start\":51949},{\"end\":52232,\"start\":52227},{\"end\":52562,\"start\":52557},{\"end\":52866,\"start\":52862},{\"end\":53118,\"start\":53020},{\"end\":53578,\"start\":53496},{\"end\":53842,\"start\":53838},{\"end\":54023,\"start\":54019},{\"end\":54194,\"start\":54190},{\"end\":54409,\"start\":54405},{\"end\":54613,\"start\":54548},{\"end\":54990,\"start\":54986},{\"end\":55251,\"start\":55243},{\"end\":55431,\"start\":55427},{\"end\":55625,\"start\":55619},{\"end\":55880,\"start\":55875},{\"end\":56172,\"start\":56168},{\"end\":56424,\"start\":56342},{\"end\":56742,\"start\":56737},{\"end\":57000,\"start\":56995},{\"end\":57250,\"start\":57244}]"}}}, "year": 2023, "month": 12, "day": 17}