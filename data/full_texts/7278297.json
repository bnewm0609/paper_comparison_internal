{"id": 7278297, "updated": "2023-07-19 16:44:00.162", "metadata": {"title": "DBpedia: A Nucleus for a Web of Open Data", "authors": "[{\"first\":\"S\u00f6ren\",\"last\":\"Auer\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Bizer\",\"middle\":[]},{\"first\":\"Georgi\",\"last\":\"Kobilarov\",\"middle\":[]},{\"first\":\"Jens\",\"last\":\"Lehmann\",\"middle\":[]},{\"first\":\"Richard\",\"last\":\"Cyganiak\",\"middle\":[]},{\"first\":\"Zachary\",\"last\":\"Ives\",\"middle\":[\"G.\"]}]", "venue": "ISWC/ASWC", "journal": "722-735", "publication_date": {"year": 2007, "month": null, "day": null}, "abstract": "DBpedia is a community e\ufb00ort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "102708294", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/semweb/AuerBKLCI07", "doi": "10.1007/978-3-540-76298-0_52"}}, "content": {"source": {"pdf_hash": "eb2b5a05d8f011a001d186477e2f573a3bfd601b", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://link.springer.com/content/pdf/10.1007/978-3-540-76298-0_52.pdf", "status": "BRONZE"}}, "grobid": {"id": "1652e2ea907ee7275f95b85feff3f48bb92497d7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eb2b5a05d8f011a001d186477e2f573a3bfd601b.txt", "contents": "\nDBpedia: A Nucleus for a Web of Open Data\n\n\nS\u00f6ren Auer auer@seas.upenn.edu \nDepartment of Computer Science\nUniversit\u00e4t Leipzig\nJohannisgasse 26D-04103LeipzigGermany\n\nDepartment of Computer and Information Science Philadelphia\nUniversity of Pennsylvania\n19104PAUSA\n\nChristian Bizer \nWeb-based Systems Group\nFreie Universit\u00e4t Berlin\nGarystr. 21D-14195BerlinGermany\n\nGeorgi Kobilarov georgi.kobilarov@gmx.derichard@cyganiak.de \nWeb-based Systems Group\nFreie Universit\u00e4t Berlin\nGarystr. 21D-14195BerlinGermany\n\nJens Lehmann \nDepartment of Computer Science\nUniversit\u00e4t Leipzig\nJohannisgasse 26D-04103LeipzigGermany\n\nRichard Cyganiak \nWeb-based Systems Group\nFreie Universit\u00e4t Berlin\nGarystr. 21D-14195BerlinGermany\n\nZachary Ives zives@cis.upenn.edu \nDepartment of Computer and Information Science Philadelphia\nUniversity of Pennsylvania\n19104PAUSA\n\nDBpedia: A Nucleus for a Web of Open Data\n\nDBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human-and machineconsumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.\n\nIntroduction\n\nIt is now almost universally acknowledged that stitching together the world's structured information and knowledge to answer semantically rich queries is one of the key challenges of computer science, and one that is likely to have tremendous impact on the world as a whole. This has led to almost 30 years of research into information integration [15,19] and ultimately to the Semantic Web and related technologies [1,11,13]. Such efforts have generally only gained traction in relatively small and specialized domains, where a closed ontology, vocabulary, or schema could be agreed upon. However, the broader Semantic Web vision has not yet been realized, and one of the biggest challenges facing such efforts has been how to get enough \"interesting\" and broadly useful information into the system to make it useful and accessible to a general audience.\n\nA challenge is that the traditional \"top-down\" model of designing an ontology or schema before developing the data breaks down at the scale of the Web: both data and metadata must constantly evolve, and they must serve many different communities. Hence, there has been a recent movement to build the Semantic Web grass-roots-style, using incremental and Web 2.0-inspired collaborative approaches [10,12,13]. Such a collaborative, grass-roots Semantic Web requires a new model of structured information representation and management: first and foremost, it must handle inconsistency, ambiguity, uncertainty, data provenance [3,6,8,7], and implicit knowledge in a uniform way.\n\nPerhaps the most effective way of spurring synergistic research along these directions is to provide a rich corpus of diverse data. This would enable researchers to develop, compare, and evaluate different extraction, reasoning, and uncertainty management techniques, and to deploy operational systems on the Web.\n\nThe DBpedia project has derived such a data corpus from the Wikipedia encyclopedia. Wikipedia is heavily visited and under constant revision (e.g., according to alexa.com, Wikipedia was the 9th most visited website in the third quarter of 2007). Wikipedia editions are available in over 250 languages, with the English one accounting for more than 1.95 million articles. Like many other web applications, Wikipedia has the problem that its search capabilities are limited to full-text search, which only allows very limited access to this valuable knowledge base. As has been highly publicized, Wikipedia also exhibits many of the challenging properties of collaboratively edited data: it has contradictory data, inconsistent taxonomical conventions, errors, and even spam.\n\nThe DBpedia project focuses on the task of converting Wikipedia content into structured knowledge, such that Semantic Web techniques can be employed against it -asking sophisticated queries against Wikipedia, linking it to other datasets on the Web, or creating new applications or mashups. We make the following contributions:\n\n-We develop an information extraction framework, which converts Wikipedia content to RDF. The basic components form a foundation upon which further research into information extraction, clustering, uncertainty management, and query processing may be conducted. The DBpedia datasets can be either imported into third party applications or can be accessed online using a variety of DBpedia user interfaces. Figure 1 gives an overview about the DBpedia information extraction process and shows how extracted data is published on the Web. These main DBpedia interfaces currently use Virtuoso [9] and MySQL as storage back-ends.\n\nThe paper is structured as follows: We give an overview about the DBpedia information extraction techniques in Section 2. The resulting datasets are described in Section 3. We exhibit methods for programmatic access to the DBpedia dataset in Section 4. In Sections 5 we present our vision of how the DBpedia  datasets can be a nucleus for a Web of open data. We showcase several user interfaces for accessing DBpedia in Section 6 and finally review related work in Section 7.\n\n\nExtracting Structured Information from Wikipedia\n\nWikipedia articles consist mostly of free text, but also contain different types of structured information, such as infobox templates, categorisation information, images, geo-coordinates, links to external Web pages and links across different language editions of Wikipedia. Mediawiki 4 is the software used to run Wikipedia. Due to the nature of this Wiki system, basically all editing, linking, annotating with meta-data is done inside article texts by adding special syntactic constructs. Hence, structured information can be obtained by parsing article texts for these syntactic constructs.\n\nSince MediaWiki exploits some of this information itself for rendering the user interface, some information is cached in relational database tables. Dumps of the crucial relational database tables (including the ones containing the article texts) for different Wikipedia language versions are published on the Web on a regular basis 5 . Based on these database dumps, we currently use two different methods of extracting semantic relationships: (1) We map the relationships that are already stored in relational database tables onto RDF and (2) we extract additional information directly from the article texts and infobox templates within the articles.\n\nWe illustrate the extraction of semantics from article texts with an Wikipedia infobox template example. Figure 2 shows the infobox template (encoded within a Wikipedia article) and the rendered output of the South-Korean town Busan. The infobox extraction algorithm detects such templates and recognizes their structure using pattern matching techniques. It selects significant templates, which are then parsed and transformed to RDF triples. The algorithm uses post-processing techniques to increase the quality of the extraction. Me-diaWiki links are recognized and transformed to suitable URIs, common units are detected and transformed to data types. Furthermore, the algorithm can detect lists of objects, which are transformed to RDF lists. Details about the infobox extraction algorithm (including issues like data type recognition, cleansing heuristics and identifier generation) can be found in [2]. All extraction algorithms are implemented using PHP and are available under an open-source license 6 .\n\n\nThe DBpedia Dataset\n\nThe DBpedia dataset currently provides information about more than 1.95 million \"things\", including at least 80,000 persons, 70,000 places, 35,000 music albums, 12,000 films. It contains 657,000 links to images, 1,600,000 links to relevant external web pages, 180,000 external links into other RDF datasets, 207,000 Wikipedia categories and 75,000 YAGO categories [16].\n\nDBpedia concepts are described by short and long abstracts in 13 different languages. These abstracts have been extracted from the English, German, French, Spanish, Italian, Portuguese, Polish, Swedish, Dutch, Japanese, Chinese, Russian, Finnish and Norwegian versions of Wikipedia.\n\nAltogether the DBpedia dataset consists of around 103 million RDF triples. The dataset is provided for download as a set of smaller RDF files. Table 1 gives an overview over these files.\n\n\nDataset\n\nDescription Triples Articles\n\nDescriptions of all 1.95 million concepts within the English Wikipedia including titles, short abstracts, thumbnails and links to the corresponding articles.\n\n\n7.6M\n\nExt. Abstracts Additional, extended English abstracts.\n\n\n2.1M Languages\n\nAdditional titles, short abstracts and Wikipedia article links in German, French, Spanish, Italian, Portuguese, Polish, Swedish, Dutch, Japanese, Chinese, Russian, Finnish and Norwegian.\n\n\n5.7M\n\nLang. Abstracts Extended abstracts in 13 languages.\n\n\n1.9M Infoboxes\n\nData attributes for concepts that have been extracted from Wikipedia infoboxes.\n\n\n15.5M\n\nExternal Links Links to external web pages about a concept. 1.6M Article Categories Links from concepts to categories using SKOS.\n\n\n5.2M Categories\n\nInformation which concept is a category and how categories are related.\n\n\n1M\n\nYago Types Dataset containing rdf:type Statements for all DBpedia instances using classification from YAGO [16].\n\n\nM\n\n\nPersons\n\nInformation about 80,000 persons (date and place of birth etc.) represented using the FOAF vocabulary.\n\n\n0.5M\n\n\nPage Links\n\nInternal links between DBpedia instances derived from the internal pagelinks between Wikipedia articles.\n\n\n62M\n\n\nRDF Links\n\nLinks between DBpedia and Geonames, US Census, Musicbrainz, Project Gutenberg, the DBLP bibliography and the RDF Book Mashup.\n\n180K Table 1. The DBpedia datasets.\n\nSome datasets (such as the Persons or Infoboxes datasets) are semantically rich in the sense that they contain very specific information. Others (such as the PageLinks dataset) contain meta-data (such as links between articles) without a specific semantics. However, the latter can be beneficial, e.g. for deriving measures of closeness between concepts or relevance in search results.\n\nEach of the 1.95 million resources described in the DBpedia dataset is identified by a URI reference of the form http://dbpedia.org/resource/Name , where Name is taken from the URL of the source Wikipedia article, which has the form http://en.wikipedia.org/wiki/Name . Thus, each resource is tied directly to an English-language Wikipedia article. This yields certain beneficial properties to DBpedia identifiers:\n\n-They cover a wide range of encyclopedic topics, -They are defined by community consensus, -There are clear policies in place for their management, -And an extensive textual definition of the concept is available at a wellknown web location (the Wikipedia page).\n\n\nAccessing the DBpedia Dataset on the Web\n\nWe provide three access mechanisms to the DBpedia dataset: Linked Data, the SPARQL protocol, and downloadable RDF dumps. Royalty-free access to these interfaces is granted under the terms of the GNU Free Documentation License.\n\nLinked Data. Linked Data is a method of publishing RDF data on the Web that relies on http:// URIs as resource identifiers and the HTTP protocol to retrieve resource descriptions [4,5]. The URIs are configured to return meaningful information about the resource-typically, an RDF description containing everything that is known about it. Such a description usually mentions related resources by URI, which in turn can be accessed to yield their descriptions. This forms a dense mesh of web-accessible resource descriptions that can span server and organization boundaries. DBpedia resource identifiers, such as http://dbpedia.org/resource/Busan, are set up to return RDF descriptions when accessed by Semantic Web agents, and a simple HTML view of the same information to traditional web browsers (see Figure 3). HTTP content negotiation is used to deliver the appropriate format. Web agents that can access Linked Data include: 1. Semantic Web browsers like Disco 7 , Tabulator [17] (see Figure 3), or the OpenLink Data Web Browser 8 ; 2. Semantic Web crawlers like SWSE 9 and Swoogle 10 ; 3. Semantic Web query agents like the Semantic Web Client Library 11 and the SemWeb client for SWI prolog 12 .\n\nSPARQL Endpoint. We provide a SPARQL endpoint for querying the DBpedia dataset. Client applications can send queries over the SPARQL protocol to this endpoint at http://dbpedia.org/sparql. This interface is appropriate when the client application developer knows in advance exactly what information is needed. In addition to standard SPARQL, the endpoint supports several extensions of the query language that have proved useful for developing user interfaces: full text search over selected RDF predicates, and aggregate functions, notably COUNT. To protect the service from overload, limits on query cost and result size are in place. For example, a query that asks for the store's entire contents is rejected as too costly. SELECT results are truncated at 1000 rows. The SPARQL endpoint is hosted using Virtuoso Universal Server 13 . \n\n\nInterlinking DBpedia with other Open Datasets\n\nIn order to enable DBpedia users to discover further information, the DBpedia dataset is interlinked with various other data sources on the Web using RDF links. RDF links enable web surfers to navigate from data within one data source to related data within other sources using a Semantic Web browser. RDF links can also be followed by the crawlers of Semantic Web search engines, which may provide sophisticated search and query capabilities over crawled data.\n\nThe DBpedia interlinking effort is part of the Linking Open Data community project 14 of the W3C Semantic Web Education and Outreach (SWEO) interest group. This community project is committed to make massive datasets and ontologies, such as the US Census, Geonames, MusicBrainz, the DBLP bibliography, WordNet, Cyc and many others, interoperable on the Semantic Web. DBpedia, with its broad topic coverage, intersects with practically all these datasets and therefore makes an excellent \"linking hub\" for such efforts. Figure 4 gives an overview about the datasets that are currently interlinked with DBpedia. Altogether this Web-of-Data amounts to approximately 2 billion RDF triples. Using these RDF links, surfers can for instance navigate from a computer scientist in DBpedia to her publications in the DBLP database, from a DBpedia book to reviews and sales offers for this book provided by the RDF Book Mashup, or from a band in DBpedia to a list of their songs provided by Musicbrainz or dbtune.\n\nThe example RDF link shown below connects the DBpedia URI identifying Busan with further data about the city provided by Geonames:\n\n<http://dbpedia.org/resource/Busan> Agents can follow this link, retrieve RDF from the Geonames URI, and thereby get hold of additional information about Busan as published by the Geonames server, which again contains further links deeper into the Geonames data. DBpedia URIs can also be used to express personal interests, places of residence, and similar facts within personal FOAF profiles:\n\n<http://richard.cyganiak.de/foaf.rdf#cygri> foaf:topic_interest <http://dbpedia.org/resource/Semantic_Web> ; foaf:based_near <http://dbpedia.org/resource/Berlin> .\n\nAnother use case is categorization of blog posts, news stories and other documents. The advantage of this approach is that all DBpedia URIs are backed with data and thus allow clients to retrieve more information about a topic: <http://news.cnn.com/item1143> dc:subject <http://dbpedia.org/resource/Iraq_War> .\n\n\nUser Interfaces\n\nUser interfaces for DBpedia can range from a simple table within a classic web page, over browsing interfaces to different types of query interfaces. This section gives an overview about the different user interfaces that have been implemented so far.\n\n\nSimple Integration of DBpedia Data into Web Pages\n\nDBpedia is a valuable source of general-purpose data that can be used within web pages. Therefore, if you want a table containing German state capitals, African musicians, Amiga computer games or whatever on your website, you can generate this table using a SPARQL query against the DBpedia endpoint. Wikipedia is kept up-to-date by a large community and a nice feature of such tables is that they will also stay up-to-date as Wikipedia, and thus also DBpedia, changes. Such tables can either be implemented using Javascript on the client or with a scripting language like PHP on the server. Two examples of Javascript generated tables are found on the DBpedia website 15 .\n\n\nSearch DBpedia.org\n\nSearch DBpedia.org is a sample application that allows users to explore the DBpedia dataset together with information from interlinked datasets such as Geonames, the RDF Book Mashup or the DBLP bibliography. In contrast to the keyword-based full-text search commonly found on the Web, search over structured data offers the opportunity to make productive use of the relations in the data, enabling stepwise narrowing of search results in different dimensions. This adds a browsing component to the search task and may reduce the common \"keyword-hit-or-not-hit\" problem. A Search DBpedia.org session starts with a keyword search. A first set of results is computed by direct keyword matches. Related matches are added, using the relations between entities up to a depth of two nodes. Thus, a search for the keyword \"Scorsese\" will include the director Martin Scorsese, as well as all of his films, and the actors of these films.\n\nThe next step is result ranking. Our experiments showed that important articles receive more incoming page links from other articles. We use a combination of incoming link count, relevance of the link's source, and relation depth to calculate a relevance ranking.\n\nAfter entering a search term, the user is presented with a list of ranked results, and with a tag cloud built from the classes found in the results, using a combination of the DBpedia and YAGO [16] classifications. Each class weight is calculated from the sum of associated result weights and the frequency of occurrence. The tag cloud enables the user to narrow the results to a specific type of entities, such as \"Actor\", even though a simple keyword search may not have brought up any actors.\n\nWhen a resource from the results is selected, the user is presented with a detailed view of all data that is known about the resource. Label, image and description are shown on top. Single-valued and multi-valued properties are shown separately. Data from interlinked datasets is automatically retrieved by following RDF links within the dataset and retrieved data from interlinked datasets is shown together with the DBpedia data. \n\n\nQuerying DBpedia Data\n\nCompared to most of the other Semantic Web knowledge bases currently available, for the RDF extracted from Wikipedia we have to deal with a different type of knowledge structure -we have a very large information schema and a considerable amount of data adhering to this schema. Existing tools unfortunately mostly focus on either one of both parts of a knowledge base being large, schema or data.\n\nIf we have a large data set and large data schema, elaborated RDF stores with integrated query engines alone are not very helpful. Due to the large data schema, users can hardly know which properties and identifiers are used in the knowledge base and hence can be used for querying. Consequently, users have to be guided when building queries and reasonable alternatives should be suggested.\n\nWe specifically developed a graph pattern builder for querying the extracted Wikipedia content. Users query the knowledge base by means of a graph pattern consisting of multiple triple patterns. For each triple pattern three form fields capture variables, identifiers or filters for subject, predicate and object of a triple. While users type identifier names into one of the form fields, a look-ahead search proposes suitable options. These are obtained not just by looking for matching identifiers but by executing the currently built query using a variable for the currently edited identifier and filtering the results returned for this variable for matches starting with the search string the user supplied. This method ensures, that the identifier proposed is really used in conjunction with the graph pattern under construction and that the query actually returns results. In addition, the identifier search results are ordered by usage number, showing commonly used identifiers first. All this is executed in the background, using the Web 2.0 AJAX technology and hence completely transparent for the user. Figure 6 shows a screenshot of the graph pattern builder. \n\n\nThird Party User Interfaces\n\nThe DBpedia project aims at providing a hotbed for applications and mashups based on information from Wikipedia. Although DBpedia was just recently launched, there is already a number of third party applications using the dataset. Examples include:\n\n-A SemanticMediaWiki [14,18] installation run by the University of Karlsruhe, which has imported the DBpedia dataset together with the English edition of Wikipedia. -WikiStory (see Figure 7) which enables users to browse Wikipedia articles about people on a large timeline. -The Objectsheet JavaScript visual data environment,which allows spreadsheet calculations based on DBpedia data 16 .\n\n\nRelated Work\n\nA second project that also works on extracting structured information from Wikipedia is the YAGO project [16]. YAGO extracts only 14 relationship types, such as subClassOf, type, familyNameOf, locatedIn from different sources of information in Wikipedia. One source is the Wikipedia category system (for sub-ClassOf, locatedIn, diedInYear, bornInYear ), and another one are Wikipedia redirects. YAGO does not perform an infobox extraction as in our approach. For determining (sub-)class relationships, YAGO does not use the full Wikipedia category hierarchy, but links leaf categories to the WordNet hierarchy. The Semantic MediaWiki project [14,18] also aims at enabling the reuse of information within Wikis as well as at enhancing search and browse facilities. Semantic MediaWiki is an extension of the MediaWiki software, which allows you to add structured data into Wikis using a specific syntax. Ultimately, the DBpedia and Semantic MediaWiki have similar goals. Both want to deliver the benefits of structured information in Wikipedia to the users, but use different approaches to achieve this aim. Semantic MediaWiki requires authors to deal with a new syntax and covering all structured information within Wikipedia would require to convert all information into this syntax. DBpedia exploits the structure that already exists within Wikipedia and hence does not require deep technical or methodological changes. However, DBpedia is not as tightly integrated into Wikipedia as is planned for Semantic MediaWiki and thus is limited in constraining Wikipedia authors towards syntactical and structural consistency and homogeneity.\n\nAnother interesting approach is followed by Freebase 17 . The project aims at building a huge online database which users can edit in a similar fashion as they edit Wikipedia articles today. The DBpedia community cooperates with Metaweb and we will interlink data from both sources once Freebase is public.\n\n\nFuture Work and Conclusions\n\nAs future work, we will first concentrate on improving the quality of the DBpedia dataset. We will further automate the data extraction process in order to increase the currency of the DBpedia dataset and synchronize it with changes in Wikipedia. In parallel, we will keep on exploring different types of user interfaces and use cases for the DBpedia datasets. Within the W3C Linking Open Data community project 18 , we will interlink the DBpedia dataset with further datasets as they get published as Linked Data on the Web. We also plan to exploit synergies between Wikipedia versions in different languages in order to further increase DBpedia coverage and provide quality assurance tools to the Wikipedia community. Such a tool could for instance notify a Wikipedia author about contradictions between the content of infoboxes contained in the different language versions of an article. Interlinking DBpedia with other knowledge bases such as Cyc (and their use as back-ground knowledge) could lead to further methods for (semi-) automatic consistency checks for Wikipedia content.\n\nDBpedia is a major source of open, royalty-free data on the Web. We hope that by interlinking DBpedia with further data sources, it could serve as a nucleus for the emerging Web of Data.\n\nFigure 1 .\n1Overview of the DBpedia components.\n\nFigure 2 .\n2Example of a Wikipedia template and rendered output (excerpt).\n\nFigure 3 .\n3http://dbpedia.org/resource/Busan viewed in a web browser (left) and in Tabulator (right). RDF Dumps. N-Triple serializations of the datasets are available for download at the DBpedia website and can be used by sites that are interested in larger parts of the dataset.\n\nFigure 4 .\n4Datsets that are interlinked with DBpedia. owl:sameAs <http://sws.geonames.org/1838524/> .\n\nFigure 5 .\n5Search results and details view for Busan.\n\nFigure 6 .\n6Form based query builder.\n\nFigure 7 .\n7WikiStory allows timeline browsing of biographies in Wikipedia.\n\n\n-We provide Wikipedia content as a large, multi-domain RDF dataset, which can be used in a variety of Semantic Web applications. The DBpedia dataset consists of 103 million RDF triples. -We interlink the DBpedia dataset with other open datasets. This results in a large Web of data containing altogether around 2 billion RDF triples. -We develop a series of interfaces and access modules, such that the dataset can be accessed via Web services and linked to other sites.\nhttp://www.mediawiki.org 5 http://download.wikimedia.org/\nhttp://sf.net/projects/dbpedia\nhttp://sites.wiwiss.fu-berlin.de/suhl/bizer/ng4j/disco/ 8 http://demo.openlinksw.com/DAV/JS/rdfbrowser/index.html 9 http://swse.org 10 http://swoogle.umbc.edu/ 11 http://sites.wiwiss.fu-berlin.de/suhl/bizer/ng4j/semwebclient/ 12 http://moustaki.org/swic/ 13 http://virtuoso.openlinksw.com\nhttp://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/ LinkingOpenData\nhttp://dbpedia.org\nhttp://richk.net/objectsheet/osc.html?file=sparql_query1.os\nhttp://www.freebase.com\nhttp://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/ LinkingOpenData\nAcknowledgmentsWe are grateful to the members of the growing DBpedia community, who are actively contributing to the project. In particular we would like to thank J\u00f6rg Sch\u00fcppel and the OpenLink team around Kingsley Idehen and Orri Erling.\nThe chatty web: Emergent semantics through gossiping. Karl Aberer, Philippe Cudr\u00e9-Mauroux, Manfred Hauswirth, 12th World Wide Web Conference. Karl Aberer, Philippe Cudr\u00e9-Mauroux, and Manfred Hauswirth. The chatty web: Emergent semantics through gossiping. In 12th World Wide Web Conference, 2003.\n\nWhat have innsbruck and leipzig in common? extracting semantics from wiki content. S\u00f6ren Auer, Jens Lehmann, Lecture Notes in Computer Science. Enrico Franconi, Michael Kifer, and Wolfgang May4519SpringerS\u00f6ren Auer and Jens Lehmann. What have innsbruck and leipzig in common? extracting semantics from wiki content. In Enrico Franconi, Michael Kifer, and Wolfgang May, editors, ESWC, volume 4519 of Lecture Notes in Computer Science, pages 503-517. Springer, 2007.\n\nUldbs: Databases with uncertainty and lineage. Omar Benjelloun, Anish Das Sarma, Alon Y Halevy, Jennifer Widom, VLDB. Omar Benjelloun, Anish Das Sarma, Alon Y. Halevy, and Jennifer Widom. Uldbs: Databases with uncertainty and lineage. In VLDB, 2006.\n\n. Tim Berners-Lee, Linked dataTim Berners-Lee. Linked data, 2006. http://www.w3.org/DesignIssues/ LinkedData.html.\n\nHow to publish linked data on the web. Christian Bizer, Richard Cyganiak, Tom Heath, Christian Bizer, Richard Cyganiak, and Tom Heath. How to publish linked data on the web, 2007. http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/ LinkedDataTutorial/.\n\nWhy and where: A characterization of data provenance. Peter Buneman, Sanjeev Khanna, Wang Chiew Tan, ICDT. 1973Peter Buneman, Sanjeev Khanna, and Wang Chiew Tan. Why and where: A characterization of data provenance. In ICDT, volume 1973 of Lecture Notes in Computer Science, 2001.\n\nQuality-Driven Information Filtering in the Context of Web-Based Information Systems. Christian Bizer, Freie Universit\u00e4t BerlinPhD thesisChristian Bizer. Quality-Driven Information Filtering in the Context of Web-Based Information Systems. PhD thesis, Freie Universit\u00e4t Berlin, 2007.\n\nLineage Tracing in Data Warehouses. Yingwei Cui, Stanford UniversityPhD thesisYingwei Cui. Lineage Tracing in Data Warehouses. PhD thesis, Stanford Univer- sity, 2001.\n\nRDF support in the Virtuoso DBMS. volume P-113 of GI-Edition -Lecture Notes in Informatics (LNI). Orri Erling, Ivan Mikhailov, 1617-5468Bonner K\u00f6llen VerlagOrri Erling and Ivan Mikhailov. RDF support in the Virtuoso DBMS. volume P- 113 of GI-Edition -Lecture Notes in Informatics (LNI), ISSN 1617-5468. Bonner K\u00f6llen Verlag, September 2007.\n\nCrossing the structure chasm. Alon Halevy, Oren Etzioni, Anhai Doan, Zachary Ives, Jayant Madhavan, Luke Mcdowell, CIDR. Alon Halevy, Oren Etzioni, AnHai Doan, Zachary Ives, Jayant Madhavan, and Luke McDowell. Crossing the structure chasm. In CIDR, 2003.\n\nSchema mediation in peer data management systems. Y Alon, Zachary G Halevy, Dan Ives, Igor Suciu, Tatarinov, ICDE. Alon Y. Halevy, Zachary G. Ives, Dan Suciu, and Igor Tatarinov. Schema mediation in peer data management systems. In ICDE, March 2003.\n\nOrchestra: Rapid, collaborative sharing of dynamic data. Zachary Ives, Nitin Khandelwal, Aneesh Kapur, Murat Cakir, CIDR. Zachary Ives, Nitin Khandelwal, Aneesh Kapur, and Murat Cakir. Orchestra: Rapid, collaborative sharing of dynamic data. In CIDR, January 2005.\n\nMapping data in peer-to-peer systems: Semantics and algorithmic issues. Anastasios Kementsietsidis, Marcelo Arenas, Ren\u00e9e J Miller, SIGMOD. Anastasios Kementsietsidis, Marcelo Arenas, and Ren\u00e9e J. Miller. Mapping data in peer-to-peer systems: Semantics and algorithmic issues. In SIGMOD, June 2003.\n\nWikipedia and the Semantic Web -The Missing Links. Markus Kr\u00f6tzsch, Denny Vrandecic, Max V\u00f6lkel, Proceedings of Wikimania. Jakob Voss and Andrew LihWikimaniaFrankfurt, GermanyMarkus Kr\u00f6tzsch, Denny Vrandecic, and Max V\u00f6lkel. Wikipedia and the Semantic Web -The Missing Links. In Jakob Voss and Andrew Lih, editors, Proceedings of Wikimania 2005, Frankfurt, Germany, 2005.\n\nMULTIBASE -integrating heterogeneous distributed database systems. John Miles Smith, Philip A Bernstein, Umeshwar Dayal, Nathan Goodman, Terry Landers, Ken W T Lin, Eugene Wong, Proceedings of 1981 National Computer Conference. 1981 National Computer ConferenceJohn Miles Smith, Philip A. Bernstein, Umeshwar Dayal, Nathan Goodman, Terry Landers, Ken W.T. Lin, and Eugene Wong. MULTIBASE -integrating hetero- geneous distributed database systems. In Proceedings of 1981 National Computer Conference, 1981.\n\nYago: A Core of Semantic Knowledge. Fabian M Suchanek, Gjergji Kasneci, Gerhard Weikum, 16th international World Wide Web conference. New York, NY, USAACM PressFabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: A Core of Semantic Knowledge. In 16th international World Wide Web conference (WWW 2007), New York, NY, USA, 2007. ACM Press.\n\nExploring and analyzing linked data on the semantic web. Tim Berners-Lee, Proceedings of the 3rd International Semantic Web User Interaction Workshop. the 3rd International Semantic Web User Interaction WorkshopTim Berners-Lee et al. Tabulator: Exploring and analyzing linked data on the se- mantic web. In Proceedings of the 3rd International Semantic Web User Interaction Workshop, 2006. http://swui.semanticweb.org/swui06/papers/Berners-Lee/ Berners-Lee.pdf.\n\nSemantic wikipedia. Max V\u00f6lkel, Markus Kr\u00f6tzsch, Denny Vrandecic, Heiko Haller, Rudi Studer, Proceedings of the 15th international conference on World Wide Web. Les Carr, David De Roure, Arun Iyengar, Carole A. Goble, and Michael Dahlinthe 15th international conference on World Wide WebACMMax V\u00f6lkel, Markus Kr\u00f6tzsch, Denny Vrandecic, Heiko Haller, and Rudi Studer. Semantic wikipedia. In Les Carr, David De Roure, Arun Iyengar, Carole A. Goble, and Michael Dahlin, editors, Proceedings of the 15th international conference on World Wide Web, WWW 2006, pages 585-594. ACM, 2006.\n\nIntelligent integration of information. Gio Wiederhold, SIGMOD. Gio Wiederhold. Intelligent integration of information. In SIGMOD, 1993.\n", "annotations": {"author": "[{\"end\":265,\"start\":45},{\"end\":364,\"start\":266},{\"end\":507,\"start\":365},{\"end\":611,\"start\":508},{\"end\":711,\"start\":612},{\"end\":844,\"start\":712}]", "publisher": null, "author_last_name": "[{\"end\":55,\"start\":51},{\"end\":281,\"start\":276},{\"end\":381,\"start\":372},{\"end\":520,\"start\":513},{\"end\":628,\"start\":620},{\"end\":724,\"start\":720}]", "author_first_name": "[{\"end\":50,\"start\":45},{\"end\":275,\"start\":266},{\"end\":371,\"start\":365},{\"end\":512,\"start\":508},{\"end\":619,\"start\":612},{\"end\":719,\"start\":712}]", "author_affiliation": "[{\"end\":165,\"start\":77},{\"end\":264,\"start\":167},{\"end\":363,\"start\":283},{\"end\":506,\"start\":426},{\"end\":610,\"start\":522},{\"end\":710,\"start\":630},{\"end\":843,\"start\":746}]", "title": "[{\"end\":42,\"start\":1},{\"end\":886,\"start\":845}]", "venue": null, "abstract": "[{\"end\":1634,\"start\":888}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2002,\"start\":1998},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2005,\"start\":2002},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2069,\"start\":2066},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2072,\"start\":2069},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2075,\"start\":2072},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2907,\"start\":2903},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2910,\"start\":2907},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2913,\"start\":2910},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3133,\"start\":3130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3135,\"start\":3133},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3137,\"start\":3135},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3139,\"start\":3137},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5193,\"start\":5190},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7914,\"start\":7911},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8410,\"start\":8406},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9856,\"start\":9852},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11804,\"start\":11801},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11806,\"start\":11804},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12605,\"start\":12601},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18593,\"start\":18589},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21620,\"start\":21616},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21623,\"start\":21620},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22111,\"start\":22107},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22648,\"start\":22644},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22651,\"start\":22648}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25300,\"start\":25252},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25376,\"start\":25301},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25658,\"start\":25377},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25762,\"start\":25659},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25818,\"start\":25763},{\"attributes\":{\"id\":\"fig_5\"},\"end\":25857,\"start\":25819},{\"attributes\":{\"id\":\"fig_6\"},\"end\":25934,\"start\":25858},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26407,\"start\":25935}]", "paragraph": "[{\"end\":2505,\"start\":1650},{\"end\":3181,\"start\":2507},{\"end\":3496,\"start\":3183},{\"end\":4271,\"start\":3498},{\"end\":4600,\"start\":4273},{\"end\":5225,\"start\":4602},{\"end\":5702,\"start\":5227},{\"end\":6349,\"start\":5755},{\"end\":7004,\"start\":6351},{\"end\":8018,\"start\":7006},{\"end\":8411,\"start\":8042},{\"end\":8695,\"start\":8413},{\"end\":8883,\"start\":8697},{\"end\":8923,\"start\":8895},{\"end\":9082,\"start\":8925},{\"end\":9145,\"start\":9091},{\"end\":9350,\"start\":9164},{\"end\":9410,\"start\":9359},{\"end\":9508,\"start\":9429},{\"end\":9647,\"start\":9518},{\"end\":9738,\"start\":9667},{\"end\":9857,\"start\":9745},{\"end\":9975,\"start\":9873},{\"end\":10101,\"start\":9997},{\"end\":10246,\"start\":10121},{\"end\":10283,\"start\":10248},{\"end\":10670,\"start\":10285},{\"end\":11085,\"start\":10672},{\"end\":11349,\"start\":11087},{\"end\":11620,\"start\":11394},{\"end\":12823,\"start\":11622},{\"end\":13662,\"start\":12825},{\"end\":14173,\"start\":13712},{\"end\":15177,\"start\":14175},{\"end\":15309,\"start\":15179},{\"end\":15704,\"start\":15311},{\"end\":15869,\"start\":15706},{\"end\":16181,\"start\":15871},{\"end\":16452,\"start\":16201},{\"end\":17179,\"start\":16506},{\"end\":18129,\"start\":17202},{\"end\":18394,\"start\":18131},{\"end\":18891,\"start\":18396},{\"end\":19325,\"start\":18893},{\"end\":19747,\"start\":19351},{\"end\":20140,\"start\":19749},{\"end\":21313,\"start\":20142},{\"end\":21593,\"start\":21345},{\"end\":21985,\"start\":21595},{\"end\":23638,\"start\":22002},{\"end\":23946,\"start\":23640},{\"end\":25063,\"start\":23978},{\"end\":25251,\"start\":25065}]", "formula": null, "table_ref": "[{\"end\":8847,\"start\":8840},{\"end\":10260,\"start\":10253}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1648,\"start\":1636},{\"attributes\":{\"n\":\"2\"},\"end\":5753,\"start\":5705},{\"attributes\":{\"n\":\"3\"},\"end\":8040,\"start\":8021},{\"end\":8893,\"start\":8886},{\"end\":9089,\"start\":9085},{\"end\":9162,\"start\":9148},{\"end\":9357,\"start\":9353},{\"end\":9427,\"start\":9413},{\"end\":9516,\"start\":9511},{\"end\":9665,\"start\":9650},{\"end\":9743,\"start\":9741},{\"attributes\":{\"n\":\"1.9\"},\"end\":9861,\"start\":9860},{\"end\":9871,\"start\":9864},{\"end\":9982,\"start\":9978},{\"end\":9995,\"start\":9985},{\"end\":10107,\"start\":10104},{\"end\":10119,\"start\":10110},{\"attributes\":{\"n\":\"4\"},\"end\":11392,\"start\":11352},{\"attributes\":{\"n\":\"5\"},\"end\":13710,\"start\":13665},{\"attributes\":{\"n\":\"6\"},\"end\":16199,\"start\":16184},{\"attributes\":{\"n\":\"6.1\"},\"end\":16504,\"start\":16455},{\"attributes\":{\"n\":\"6.2\"},\"end\":17200,\"start\":17182},{\"attributes\":{\"n\":\"6.3\"},\"end\":19349,\"start\":19328},{\"attributes\":{\"n\":\"6.4\"},\"end\":21343,\"start\":21316},{\"attributes\":{\"n\":\"7\"},\"end\":22000,\"start\":21988},{\"attributes\":{\"n\":\"8\"},\"end\":23976,\"start\":23949},{\"end\":25263,\"start\":25253},{\"end\":25312,\"start\":25302},{\"end\":25388,\"start\":25378},{\"end\":25670,\"start\":25660},{\"end\":25774,\"start\":25764},{\"end\":25830,\"start\":25820},{\"end\":25869,\"start\":25859}]", "table": null, "figure_caption": "[{\"end\":25300,\"start\":25265},{\"end\":25376,\"start\":25314},{\"end\":25658,\"start\":25390},{\"end\":25762,\"start\":25672},{\"end\":25818,\"start\":25776},{\"end\":25857,\"start\":25832},{\"end\":25934,\"start\":25871},{\"end\":26407,\"start\":25937}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5015,\"start\":5007},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7119,\"start\":7111},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12432,\"start\":12424},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12619,\"start\":12611},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14702,\"start\":14694},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21263,\"start\":21255},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21784,\"start\":21776}]", "bib_author_first_name": "[{\"end\":27340,\"start\":27336},{\"end\":27357,\"start\":27349},{\"end\":27380,\"start\":27373},{\"end\":27668,\"start\":27663},{\"end\":27679,\"start\":27675},{\"end\":28097,\"start\":28093},{\"end\":28119,\"start\":28110},{\"end\":28131,\"start\":28127},{\"end\":28133,\"start\":28132},{\"end\":28150,\"start\":28142},{\"end\":28302,\"start\":28299},{\"end\":28461,\"start\":28452},{\"end\":28476,\"start\":28469},{\"end\":28490,\"start\":28487},{\"end\":28723,\"start\":28718},{\"end\":28740,\"start\":28733},{\"end\":28759,\"start\":28749},{\"end\":29041,\"start\":29032},{\"end\":29274,\"start\":29267},{\"end\":29502,\"start\":29498},{\"end\":29515,\"start\":29511},{\"end\":29776,\"start\":29772},{\"end\":29789,\"start\":29785},{\"end\":29804,\"start\":29799},{\"end\":29818,\"start\":29811},{\"end\":29831,\"start\":29825},{\"end\":29846,\"start\":29842},{\"end\":30049,\"start\":30048},{\"end\":30063,\"start\":30056},{\"end\":30065,\"start\":30064},{\"end\":30077,\"start\":30074},{\"end\":30088,\"start\":30084},{\"end\":30313,\"start\":30306},{\"end\":30325,\"start\":30320},{\"end\":30344,\"start\":30338},{\"end\":30357,\"start\":30352},{\"end\":30597,\"start\":30587},{\"end\":30622,\"start\":30615},{\"end\":30636,\"start\":30631},{\"end\":30638,\"start\":30637},{\"end\":30872,\"start\":30866},{\"end\":30888,\"start\":30883},{\"end\":30903,\"start\":30900},{\"end\":31265,\"start\":31255},{\"end\":31279,\"start\":31273},{\"end\":31281,\"start\":31280},{\"end\":31301,\"start\":31293},{\"end\":31315,\"start\":31309},{\"end\":31330,\"start\":31325},{\"end\":31343,\"start\":31340},{\"end\":31347,\"start\":31344},{\"end\":31359,\"start\":31353},{\"end\":31737,\"start\":31731},{\"end\":31739,\"start\":31738},{\"end\":31757,\"start\":31750},{\"end\":31774,\"start\":31767},{\"end\":32105,\"start\":32102},{\"end\":32531,\"start\":32528},{\"end\":32546,\"start\":32540},{\"end\":32562,\"start\":32557},{\"end\":32579,\"start\":32574},{\"end\":32592,\"start\":32588},{\"end\":33132,\"start\":33129}]", "bib_author_last_name": "[{\"end\":27347,\"start\":27341},{\"end\":27371,\"start\":27358},{\"end\":27390,\"start\":27381},{\"end\":27673,\"start\":27669},{\"end\":27687,\"start\":27680},{\"end\":28108,\"start\":28098},{\"end\":28125,\"start\":28120},{\"end\":28140,\"start\":28134},{\"end\":28156,\"start\":28151},{\"end\":28314,\"start\":28303},{\"end\":28467,\"start\":28462},{\"end\":28485,\"start\":28477},{\"end\":28496,\"start\":28491},{\"end\":28731,\"start\":28724},{\"end\":28747,\"start\":28741},{\"end\":28763,\"start\":28760},{\"end\":29047,\"start\":29042},{\"end\":29278,\"start\":29275},{\"end\":29509,\"start\":29503},{\"end\":29525,\"start\":29516},{\"end\":29783,\"start\":29777},{\"end\":29797,\"start\":29790},{\"end\":29809,\"start\":29805},{\"end\":29823,\"start\":29819},{\"end\":29840,\"start\":29832},{\"end\":29855,\"start\":29847},{\"end\":30054,\"start\":30050},{\"end\":30072,\"start\":30066},{\"end\":30082,\"start\":30078},{\"end\":30094,\"start\":30089},{\"end\":30105,\"start\":30096},{\"end\":30318,\"start\":30314},{\"end\":30336,\"start\":30326},{\"end\":30350,\"start\":30345},{\"end\":30363,\"start\":30358},{\"end\":30613,\"start\":30598},{\"end\":30629,\"start\":30623},{\"end\":30645,\"start\":30639},{\"end\":30881,\"start\":30873},{\"end\":30898,\"start\":30889},{\"end\":30910,\"start\":30904},{\"end\":31271,\"start\":31266},{\"end\":31291,\"start\":31282},{\"end\":31307,\"start\":31302},{\"end\":31323,\"start\":31316},{\"end\":31338,\"start\":31331},{\"end\":31351,\"start\":31348},{\"end\":31364,\"start\":31360},{\"end\":31748,\"start\":31740},{\"end\":31765,\"start\":31758},{\"end\":31781,\"start\":31775},{\"end\":32117,\"start\":32106},{\"end\":32538,\"start\":32532},{\"end\":32555,\"start\":32547},{\"end\":32572,\"start\":32563},{\"end\":32586,\"start\":32580},{\"end\":32599,\"start\":32593},{\"end\":33143,\"start\":33133}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5927352},\"end\":27578,\"start\":27282},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1225534},\"end\":28044,\"start\":27580},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1899963},\"end\":28295,\"start\":28046},{\"attributes\":{\"id\":\"b3\"},\"end\":28411,\"start\":28297},{\"attributes\":{\"id\":\"b4\"},\"end\":28662,\"start\":28413},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13791826},\"end\":28944,\"start\":28664},{\"attributes\":{\"id\":\"b6\"},\"end\":29229,\"start\":28946},{\"attributes\":{\"id\":\"b7\"},\"end\":29398,\"start\":29231},{\"attributes\":{\"doi\":\"1617-5468\",\"id\":\"b8\"},\"end\":29740,\"start\":29400},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8374016},\"end\":29996,\"start\":29742},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":183656},\"end\":30247,\"start\":29998},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16387167},\"end\":30513,\"start\":30249},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14458854},\"end\":30813,\"start\":30515},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17623830},\"end\":31186,\"start\":30815},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15775333},\"end\":31693,\"start\":31188},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207163173},\"end\":32043,\"start\":31695},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17241604},\"end\":32506,\"start\":32045},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1094218},\"end\":33087,\"start\":32508},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1383286},\"end\":33225,\"start\":33089}]", "bib_title": "[{\"end\":27334,\"start\":27282},{\"end\":27661,\"start\":27580},{\"end\":28091,\"start\":28046},{\"end\":28716,\"start\":28664},{\"end\":29770,\"start\":29742},{\"end\":30046,\"start\":29998},{\"end\":30304,\"start\":30249},{\"end\":30585,\"start\":30515},{\"end\":30864,\"start\":30815},{\"end\":31253,\"start\":31188},{\"end\":31729,\"start\":31695},{\"end\":32100,\"start\":32045},{\"end\":32526,\"start\":32508},{\"end\":33127,\"start\":33089}]", "bib_author": "[{\"end\":27349,\"start\":27336},{\"end\":27373,\"start\":27349},{\"end\":27392,\"start\":27373},{\"end\":27675,\"start\":27663},{\"end\":27689,\"start\":27675},{\"end\":28110,\"start\":28093},{\"end\":28127,\"start\":28110},{\"end\":28142,\"start\":28127},{\"end\":28158,\"start\":28142},{\"end\":28316,\"start\":28299},{\"end\":28469,\"start\":28452},{\"end\":28487,\"start\":28469},{\"end\":28498,\"start\":28487},{\"end\":28733,\"start\":28718},{\"end\":28749,\"start\":28733},{\"end\":28765,\"start\":28749},{\"end\":29049,\"start\":29032},{\"end\":29280,\"start\":29267},{\"end\":29511,\"start\":29498},{\"end\":29527,\"start\":29511},{\"end\":29785,\"start\":29772},{\"end\":29799,\"start\":29785},{\"end\":29811,\"start\":29799},{\"end\":29825,\"start\":29811},{\"end\":29842,\"start\":29825},{\"end\":29857,\"start\":29842},{\"end\":30056,\"start\":30048},{\"end\":30074,\"start\":30056},{\"end\":30084,\"start\":30074},{\"end\":30096,\"start\":30084},{\"end\":30107,\"start\":30096},{\"end\":30320,\"start\":30306},{\"end\":30338,\"start\":30320},{\"end\":30352,\"start\":30338},{\"end\":30365,\"start\":30352},{\"end\":30615,\"start\":30587},{\"end\":30631,\"start\":30615},{\"end\":30647,\"start\":30631},{\"end\":30883,\"start\":30866},{\"end\":30900,\"start\":30883},{\"end\":30912,\"start\":30900},{\"end\":31273,\"start\":31255},{\"end\":31293,\"start\":31273},{\"end\":31309,\"start\":31293},{\"end\":31325,\"start\":31309},{\"end\":31340,\"start\":31325},{\"end\":31353,\"start\":31340},{\"end\":31366,\"start\":31353},{\"end\":31750,\"start\":31731},{\"end\":31767,\"start\":31750},{\"end\":31783,\"start\":31767},{\"end\":32119,\"start\":32102},{\"end\":32540,\"start\":32528},{\"end\":32557,\"start\":32540},{\"end\":32574,\"start\":32557},{\"end\":32588,\"start\":32574},{\"end\":32601,\"start\":32588},{\"end\":33145,\"start\":33129}]", "bib_venue": "[{\"end\":27422,\"start\":27392},{\"end\":27722,\"start\":27689},{\"end\":28162,\"start\":28158},{\"end\":28450,\"start\":28413},{\"end\":28769,\"start\":28765},{\"end\":29030,\"start\":28946},{\"end\":29265,\"start\":29231},{\"end\":29496,\"start\":29400},{\"end\":29861,\"start\":29857},{\"end\":30111,\"start\":30107},{\"end\":30369,\"start\":30365},{\"end\":30653,\"start\":30647},{\"end\":30936,\"start\":30912},{\"end\":31414,\"start\":31366},{\"end\":31827,\"start\":31783},{\"end\":32194,\"start\":32119},{\"end\":32667,\"start\":32601},{\"end\":33151,\"start\":33145},{\"end\":30990,\"start\":30963},{\"end\":31449,\"start\":31416},{\"end\":31846,\"start\":31829},{\"end\":32256,\"start\":32196},{\"end\":32795,\"start\":32744}]"}}}, "year": 2023, "month": 12, "day": 17}