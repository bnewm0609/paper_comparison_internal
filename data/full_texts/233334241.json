{"id": 233334241, "updated": "2023-10-06 05:11:44.97", "metadata": {"title": "GAN Prior Embedded Network for Blind Face Restoration in the Wild", "authors": "[{\"first\":\"Tao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Peiran\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Xuansong\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Academy\",\"middle\":[\"Zhang\",\"DAMO\"]},{\"first\":\"Alibaba\",\"last\":\"Group\",\"middle\":[]},{\"first\":\"Department\",\"last\":\"Computing\",\"middle\":[\"of\"]},{\"first\":\"The\",\"last\":\"University\",\"middle\":[\"Hong\",\"Kong\",\"Polytechnic\"]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 5, "day": 13}, "abstract": "Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.06070", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YangRX021", "doi": "10.1109/cvpr46437.2021.00073"}}, "content": {"source": {"pdf_hash": "63c778bc07ce7bcbfa8da7b45fd03948b6a663fe", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.06070v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2105.06070", "status": "GREEN"}}, "grobid": {"id": "93bda3cbbf1343117cd8a0d62fd1c4fe8b79f925", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/63c778bc07ce7bcbfa8da7b45fd03948b6a663fe.txt", "contents": "\nGAN Prior Embedded Network for Blind Face Restoration in the Wild\n\n\nTao Yang \nAlibaba Group\nDAMO Academy\n\n\nPeiran Ren peiranr@sohu.com \nAlibaba Group\nDAMO Academy\n\n\nXuansong Xie \nAlibaba Group\nDAMO Academy\n\n\nLei Zhang cslzhang@comp.polyu.edu.hk \nAlibaba Group\nDAMO Academy\n\n\nDepartment of Computing\nThe Hong Kong Polytechnic University\n\n\nGAN Prior Embedded Network for Blind Face Restoration in the Wild\n\nBlind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/ yangxy/GPEN .\n\nIntroduction\n\nFace images are among the most popular types of images in our daily life, while face images are often degraded due to the many factors such as low resolution, blur, noise, compression, etc., or the combination of them. Face image restoration has been attracting significant attentions, aiming at reproducing a clear and realistic face image from the degraded input. Traditional face image restoration methods [50,3,2,36] usually solve an inverse problem based on the degradation model and handcrafted priors, which demonstrate limited performance in practice. Recently, deep neural networks (DNNs) have shown superior results in a variety of computer vision tasks [24,48,13,25,30], and many DNN based face restoration methods [49,29,16] have also been developed and they have demonstrated much better performance than traditional ones.\n\nThough much progress has been made for face restoration, blind face restoration (BFR) remains a challenging research problem because of the unknown and complex degradation of low quality (LQ) face images in the wild. In order to recover a high-quality (HQ) face image with photo-realistic textures from an LQ face image, a number of BFR methods have been proposed by resorting to the spatial transformer networks [49], exemplar images [29,28,9], 3D facial priors [16], and facial component dictionaries [27]. Yang et al. [47] proposed a collaborative suppression and replenishment (CSR) approach to progressively replenish facial details. These methods exhibit impressive results on artificially degraded faces; however, they fail to tackle realworld LQ face images. The conditional generative adversarial network (cGAN) based methods such as Pix2Pix [18] and Pix2PixHD [43] learn a direct mapping from input image to output image. These methods achieve more realistic results but tend to over-smooth the images (see Figures 5 and 7), which is commonly blamed to the high illness of real-world BFR tasks.\n\nWith the rapid advancement of GAN techniques [21,22], recently some methods have been proposed to reconstruct faces from extremely low resolution inputs [12,34,38]. Richardson et al. [38] employed an encoder network to generate a series of style vectors before feeding them into a pre-trained generator, achieving a generic image-to-image translation framework. However, such methods can only work on non-blind image super-resolution problems. Furthermore, they kept the pre-trained GAN unchanged in training for the consistency and convenience of face manipulations. This however leads to unstable quality of restored faces when dealing with real-world LQ face images with complex background, because it is hard to accurately project a face image with limited resolution to a desired latent code (e.g., a vector of size 512 in StyleGAN [21,22]).\n\nIn this work, we revisit the problem of BFR and target at restoring HQ faces from degraded face observations in the wild. Our idea is to seamlessly integrate the advantages of GAN and DNN. We first pre-train a GAN for HQ face image generation and embed it into a DNN as a decoder prior for face restoration. The GAN prior embedded DNN is then fine-tuned by a set of synthesized LQ-HQ face image pairs, during which the DNN learns to map the input degraded image to a desired latent space so that the GAN prior network can reproduce the desired HQ face images. We carefully design the GAN blocks to make them well suited for a Ushaped DNN, where the deep features are used to generate the latent code for global face reproduction, while the shallow features are used as noise to generate local face details and keep the image background. In this way, our learned model can reconstruct HQ faces with photo-realistic details from even severely degraded face images in the wild, avoiding over-smoothed results caused by the high illness of the BFR problem. Figure 1 shows an example. One can see that our model reconstructs the face images of those great scientists with clear details from the old photo taken in 1927.\n\nThe main contributions of this work are summarized as follows:\n\n\u2022 We learn and embed a GAN prior network into a DNN, and fine-tune the GAN embedded DNN for effective BFR in the wild. It is worthy to note that previous works only transfer the pre-trained GAN into a network without fine-tuning. \u2022 The GAN blocks are designed so that they can be easily embedded into a U-shaped DNN for fine-tuning. The latent code and noise input of the GAN are respectively generated from the deep and shallow features of the DNN to reconstruct the global structure, local face details and background of the image accordingly. \u2022 Our model sets new state-of-the-art in BFR. It is capable of tackling severely degraded face images taken in real-world scenarios.\n\n\nRelated Work\n\nFace Image Restoration. As a specific but important branch of image restoration, face image restoration has been widely studied for many years. In the early stage, Zhang et al. [50] presented a joint blind image restoration and recognition method by using sparse representation to handle face recognition from LQ images. Nishiyama et al. [36] proposed to improve the recognition performance of blurry faces by using a pre-defined set of blur kernels to restore them. With the unprecedented success of DNNs in solving image restoration tasks such as denoising [13], deblur- ring [24,40], inpainting [48,31] and image super-resolution [25,30], many DNN based face image restoration methods have also been proposed [7,23,33], which advance the traditional methods by a large margin. Considering the fact that facial images have specific structures, it is interesting to investigate whether we can restore a clear face image from severely degraded ones without knowing the degradation model. The so-called blind face restoration (BFR) problem has been attracting intensive research attentions in recent years [29,9,27,47], while it is still a challenging task due to the complex image degradations in the wild.\n\nHuang et al. [17] presented a wavelet-based approach that can ultra-resolve a very low-resolution (LR) face image. Chen et al. [7] learned the facial geometry prior to recover the high-resolution (HR) faces. Ma et al. [33] performed face super-resolution with iterative collaboration between two recurrent networks on facial image recovery and landmark estimation, respectively. Li et al. [29] used a guiding image and a wrapper subnetwork to cope with appearance variations between the LR input and the HR guiding image. This work was further extended by using an unconstrained HR face image [9], multi-exemplar images [28], and multi-scale component dictionaries [27]. Hu et al. [16] explicitly incorporated 3D facial priors to grasp the sharp facial structures. A collaborative suppression and replenishment approach was proposed by Yang et al. [47] to progressively replenish facial details. Existing works have generated impressive results on artificially degraded faces, but often failed in real-world scenarios due to the complex unknown degradation. Furthermore, their performance depends heavily on the accurate facial prior knowledge which however is hard to obtain from severely degraded face images in the wild, leading to unpredictable failures.\n\nGenerative Adversarial Network (GAN). Since the seminal work by Goodfellow et al. [11], great progress has been accomplished on learning GAN models [20,4,21,22]. GAN has been widely used for various computer vision applications due to its powerful ability to generate photorealistic images. Some typical applications include image inpainting [48], super-resolution [25,44], image colorization [18,42], texture synthesis [41], etc. Particularly, to provide more user controls for image synthesis, conditional GAN (cGAN) has been proposed [35]. By feeding the generator with different conditional information [37,18,52], cGANs succeed in handling various image-to-image translation problems. Isola et al. [18] showed that the conditional adversarial networks can be used as a general-purpose solution to image-to-image translation problems. Many following works, such as unsupervised learning [52], disentangled learning [26], few-shot learning [32], high resolution image synthesis [43], multi-domain translation [8], multimodal translation [53], have been proposed to extend cGAN to different scenarios. The cGAN learns a direct mapping from the input domain to the output one. Unfortunately, the generated results by cGANs are usually over-smoothed in highly ill-posed tasks such as BFR.\n\nGAN Prior for Image Generation. Deep generative models are popular in solving many inverse problems, e.g. deblurring [24], image inpainting [48], phase retrieval [14], etc. Recently, many works have been developed for the task of GAN inversion, i.e., reversing a given image back to a latent code with a pre-trained GAN model. Existing methods either optimize the latent code [1] or learn an extra encoder to project the image space back to the latent space [12,38]. Abdal et al. [1] embedded images into an extended latent space of StyleGAN, allowing further semantic image editing operations. Gu et al. [12] employed multiple latent codes to generate multiple feature maps to output the final image. These optimization-based methods, however, are slow and improper for real-world applications. To address this issue, Pixel2Style2Pixel (pSp) [38] embeds real images into extended latent space without additional optimization, which can be used in a wide range of imageto-image translation tasks. Menon et al. [34] proposed a self-supervised approach that traverses the HR natural image manifold, searching for images that can downscale to the original LR image. GAN inversion is an important step for applying GANs to real-world applications. However, it is difficult to perfectly project the image space back to the latent space. Moreover, it is hard, if not possible, to invert a blindly degraded face into a latent space.\n\nSome works were proposed to transfer GAN priors. Wang et al. [46] applied domain adaptation to image generation with GANs. They further proposed a novel knowledge transfer method for generative models by using a knowledge mining network [45]. Fr\u00e9gier and Gouray [10] introduced a novel approach for transfer learning with GAN ar- chitecture. These works target at transferring the knowledge from the source domain to different target domains, while in our work, the source and target domains are the same. We embed the GAN prior learned for face generation into a DNN for face restoration, and jointly fine-tune the GAN prior network with the DNN so that the latent code and noise input can be well generated from the degraded face image at different network layers.\n\n\nProposed Method\n\n\nMotivation and Framework\n\nBFR is a typical ill-posed inverse problem. Denote by X the space of degraded LQ faces, and by Y the space of original HQ face images. Given an input LQ face image x \u2208 X , BFR aims to find its corresponding clear face image y \u2208 Y. Most of the DNN based methods learn a mapping function \u03a6 to achieve this goal, i.e., \u03a6(x) \u2192 y. However, this is a one-to-many inverse problem, and there are many possible face images (e.g., y 1 , y 2 , ..., y n ) in Y that can match to the input x. Existing methods [5,29,9] usually train DNNs to perform mapping between x and y using some pixel-wise loss functions. As a result, as we illustrated in Figure 2, the final solution \u03a6(x) tends to be the mean of those HQ faces, which is over-smoothed and loses details. This coincides with the visual perception global-first theory [6]. The cGAN methods [18,43] can partially dilute this issue by adversarial training to reduce the uncertainty in mapping. However, when the degradation is severe, the problem remains and cGANs can hardly generate clear face images with realistic textures and details (see Figure 5 for example).\n\nDifferent from previous methods [5,29,9,43,47], we first train a GAN prior network, and then embed it into a DNN as decoder for HQ face image restoration. We call our method GAN prior embedded network (GPEN). As illustrated in Figure 2, the first part of our GPEN is a CNN encoder, which learns to map the input degraded image x to a desired latent code z in the latent space Z of the GAN. The GAN prior network can then reproduce the desired HQ face image via G(z) \u2192 y, where G refers to the learned generator of GAN. The generation process is basically a one-to-one mapping, largely alleviating the uncertainty of one-to-many mapping in previous methods. It should be The definition of \"Mod\" and \"Demod\" can be found in [22].\n\nnoted that the GAN inversion methods [12,34,38] share a similar idea with our GPEN; however, they keep the pretrained GANs unchanged for consistent and convenient face manipulations. While in GPEN, we carefully design and pre-train the GAN blocks and fine-tune the GAN priors for effective BFR. The architectures of GPEN and GAN blocks are shown in Figure 3 and will be explained in detail in the following sections.\n\n\nNetwork Architecture\n\nThe GAN prior network. U-Net [39] has been successfully and widely used in many image restoration tasks [43,13] and demonstrated its effectiveness in preserving image details. Therefore, our GPEN overall follows a Ushaped encoder-decoder architecture (see Figure 3(c)). Accordingly, the GAN prior network should be designed to meet two requirements: 1) it is capable of generating HQ face images; and 2) it can be readily embedded into the Ushaped GPEN as a decoder. Inspired by the state-of-theart GAN architectures, e.g., StyleGAN [21,22], we use a mapping network to project latent code z into a less entangled space w \u2208 W, as illustrated in Figure 3(a). The intermediate code w is then broadcasted to each GAN block. Since the GAN prior network will be embedded into a Ushaped DNN for finetuning, we need to leave room for the skipped feature maps extracted by the encoder of the Ushaped DNN. We thus provide additional noise inputs to each GAN block.\n\nFor the structure of GAN block, there are several options. In this work, we adopt the architecture in StyleGAN v2 (see Figure 3(b)) due to its high capability to generate HQ images. (Alternative GAN architectures such as StyleGAN v1 [21], PGGAN [20] and BigGAN [4] can also be easily adopted into our GPEN.) The number of GAN blocks is equal to the number of skipped feature maps ex-tracted in the U-shaped DNN (and the number of noise inputs), which is related to the resolution of input face image. StyleGAN requires two different noise inputs in each GAN block. To enable the GAN prior network to be readily embedded into the U-shaped GPEN, different from StyleGAN, the noise inputs are reused at the same spatial resolution for all GAN blocks. Furthermore, the noise inputs are concatenated rather than added to the convolutions in StyleGAN. We empirically found that this can bring more details in the restored face image.\n\nFull network architecture. Once the GAN prior network is trained by using some dataset (e.g., the FFHQ [21] dataset), we embed it into the U-shaped DNN as a decoder, as shown in Figure 3(c). The latent code z and the noise inputs to the GAN network are replaced by the output of the fully-connected layer (i.e., deeper features) and shallower layers of the encoder of the DNN, respectively, which will control the reconstruction of global face structure, local face details, as well as the background of face image. Since the proposed model is not fully convolutional, LQ face images are first resized to the desired resolution (e.g., 1024 2 ) using simple bilinear interpolator before being input to the GPEN. After embedding, the whole GPEN will be fine-tuned so that the encoder part and decoder part can learn to adapt to each other.\n\n\nTraining Strategy\n\nWe first pre-train the GAN prior network using a dataset of HQ face images following the training strategies of Style-GAN [21,22]. The pre-trained GAN model is embedded into the proposed GPEN, and we fine-tune the whole network using a set of synthesized LQ-HQ face image pairs (the image synthesis process will be given in Section 4.2).\n\nTo fine-tune the GPEN model, we adopt three loss functions: the adversarial loss L A , the content loss L C , and the feature matching loss L F . L A is inherited from the GAN prior network:\nL A = min G max D E (X) log 1 + exp \u2212D G(X) ,(1)\nwhere X andX denote the ground-truth HQ image and the degraded LQ one, G is the generator during training, and D is the discriminator. L C is defined as the L 1 -norm distance between the final results of the generator and the corresponding ground-truth images. L F is similar to the perceptual loss [19] but it is based on the discriminator rather than the pre-trained VGG network to fit our task. It is formulated as follows:\nL F = min G E (X) T i=0 D i (X) \u2212 D i (G(X)) 2 ,(2)\nwhere T is the total number of intermediate layers used for feature extraction. D i (X) is the extracted feature at the i-th layer of discriminator D.\n\nThe final loss L is as follows:\nL = L A + \u03b1L C + \u03b2L F ,(3)\nwhere \u03b1 and \u03b2 are balancing parameters. The content loss L C enforces the fine features and preserves the original color information. By introducing the feature matching loss L F on the discriminator, the adversarial loss L A can be better balanced to recover more realistic face images with vivid details. In all the following experiments, we empirically set \u03b1 = 1 and \u03b2 = 0.02.\n\n\nExperiments\n\n\nDatasets and Evaluation Metric\n\nThe FFHQ dataset [21], which contains 70, 000 HQ face images of resolution 1024 2 , is used to train our GPEN model. We first use it to train the GAN prior network, and then synthesize LQ images from it to fine-tune the whole GPEN. To evaluate our model, we use the CelebA-HQ dataset [20] to simulate LQ face images to quantitatively compare GPEN with other state-of-the-art methods. We also collet 1, 000 real-world LQ faces (will be made publicly available) from internet to qualitatively evaluate the performance of our model in the wild. In the quantitative evaluation, the Peak Signal-to-Noise Ratio (PSNR), the Fr\u00e9chet Inception Distances (FID) [15] and the Learned Perceptual Image Patch Similarity (LPIPS) [51] indices are used. It is worth mentioning that all these indices can only be used as references for evaluation because they cannot truly reflect the performance of a BFR method, especially for BFR in the wild.\n\n\nImplementation Details\n\nWe first train the GAN prior network using the FFHQ dataset with similar settings to StyleGAN [21,22]. The pretrained GAN prior network is embedded into the GPEN to perform fine-tuning. To build LQ-HQ image pairs for finetuning, we synthesize degraded faces from the HQ images in FFHQ using the following degradation model:\nI d = ((I \u2297 k) \u2193 s +n \u03c3 ) JP EGq ,(4)\nwhere I, k, n \u03c3 , I d are respectively the input face image, the blur kernel, the Gaussian noise with standard deviation \u03c3 and the degraded image. \u2297, \u2193 s , JP EG q respectively denote the two-dimensional convolution, the standard s-fold downsampler and the JPEG compression operator with a quality factor q. The above degradation model has been used in previous methods [29,27]. In our implementation, for each image the blur kernel k is randomly selected from a set of blurring models, including Gaussian blur and motion blur with varying kernel sizes. The additive Gaussian noise n \u03c3 is sampled channel-wise from a normal distribution, and \u03c3 is chosen from [0,25]. The value of s is randomly and uniformly sampled from [10,200] (i.e., up to 200 times downscaling) and q is randomly and uniformly sampled from [5,50] (i.e., up to 95% JPEG compression) per image. By using those severely degraded images to fine-tune the model, the encoder part of our GPEN can learn to generate suitable latent code and noise inputs to the GAN prior decoder network, which is updated simultaneously to tackle severely degraded faces in real-world scenarios.\n\nDuring model updating, we adopt the Adam optimizer with a batch size of 1. The learning rate (LR) varies for different parts of GPEN, including the encoder, the decoder and the discriminator. In our implementation, we let LR encoder = 0.002, and set LR encoder : LR decoder : LR discriminator = 100 : 10 : 1. It should be noted that the discriminator part will be removed in the testing stage.\n\n\nAblation Study\n\nTo better understand the roles of different components of GPEN and the training strategy, in this section we conduct an ablation study by introducing some variants of GPEN and comparing their BFR performance. The first variant is denoted by GPEN-w/o-ft, i.e., the embedded GAN prior network is kept unchanged in the fine-tuning process. The second variant is denoted by GPEN-w/o-noise, which refers to the GPEN model without noise inputs. The third variant is denoted by GPEN-noise-add, i.e., that the noise inputs are added rather than concatenated to the convolutions.\n\nWe perform BFR on the CelebA-HQ dataset to evaluate GPEN and its three variants. The LQ images are synthesized by using the degradation model in Eq. (4) and the (c) GFRNet [29]; (d) GWAInet [9]; (e) Pix2PixHD [43]; (f) DFDNet [27]; (g) HiFaceGAN [47]; (h) GPEN; (i) Ground truth. We can see that GPEN-w/o-ft can generate clean HQ face image; however, the appearance of the face is rather different from the ground-truth, and the background of the image is totally different. This is because without fine-tuning the GAN prior, it is difficult to generate the desired latent code into the latent space Z, which coincides with the findings in many GAN inversion works [1,38]. By discarding the noise input, the result of GPEN-w/o-noise is blurrier than GPENw/o-ft, and there are some artifact generated in the boundary of the image. This implies that the noise input plays an import role in synthesizing localize details. GPEN-noiseadd achieves comparable result to GPEN but with slightly less facial details, while it generates some false details in the background of the image. Overall, GPEN shows superior performance to its variants, demonstrating the effectiveness of concatenated U-shaped architecture and our training strategy for the BFR tasks. \n\n\nExperiments on Synthetic Images\n\nTo quantitatively compare GPEN with other state-ofthe-arts, we first perform experiments on synthetic images. Considering that many face restoration methods [12,34,38] are actually designed for FSR instead of BFR, we perform experiments on BFR and FSR separately, where different competing methods are used for fair comparison.\n\nBlind Face Restoration. By using the degradation model in Eq. (4) and the same set of parameters used in Section 4.2, we synthesized a set of LQ face images on the CelebA-HQ dataset for evaluation. We compare GPEN with the latest BFR methods, including Pix2PixHD [43], Super-FAN [5], GFRNet [29], GWAInet [9], DFDNet [27], HiFaceGAN [47]. The models trained by the original authors are used in the experiments. We do not compare with those FSR methods [12,34,38] in this experiment because they assume a very simple degradation model (e.g., bicu- Table 3: Comparison (PSNR, FID and LPIPS) of various FSR methods. Since mGANprior [12] and PULSE [34] are very time-consuming, we only used the first 1, 000 images of CelebA-HQ dataset to compute their measures. \"-\" means that the result is not available. (a) Bilinear (b) Super-FAN [5] (c) GWAInet [9] (d) GFRNet [29] (e) pix2pixHD [43] (f) HiFaceGAN [47] (g) mGANprior [12] (h) PULSE [34] (i) pSp [38] (j) GPEN (k) Ground truth bic downsampling) and cannot handle this challenging BFR task. The PSNR, FID and LPIPS results are listed in Table 2. One can see that our GPEN achieves comparable PSNR index to other competing methods, but it achieves significantly better results on FID and LPIPS indices, which are better measures than PSNR for the face image perceptual quality. Figure 5 compares the BFR results on some degraded face images by the competing methods. One can see that the competing methods fail to produce reasonable face reconstructions. They tend to generate over-smoothed face images with distorted facial structures. However, our GPEN generate visually photo-realistic face images with clear hair, eye, eyebrow, tooth and mustache details. Even the background can also be partially constructed. This clearly validates the advantages of our GPEN model and the training strategy. More visual comparison results can be found in the supplementary file.\n\nFace Super-Resolution. FSR aims to generate an HR image from the input LR version. It can be considered as a special case of BFR, where the image degradation process is specified (i.e., bicubic downsampling). To validate the generality of our GPEN, we still use our model trained for BFR to perform the FSR task, and compare it with those stateof-the-art methods designed for FSR, including Super-FAN [5], GFRNet [29], GWAInet [9], DFDNet [27], HiFace-GAN [47], mGANprior [12], PULSE [34], and pSp [38]. The zooming factor ranges from 8\u00d7 to 256\u00d7, and the LR face images are simulated on the CelebA-HQ dataset.\n\nThe quantitative results are presented in Table 3. One can see that the na\u00efve bilinear interpolator achieves the best PSNR index, though it cannot restore any facial details. This actually validates that PSNR is not a suitable index to measure FSR quality. GPEN achieves the best FID and LPIPS scores under almost all the zooming factors. \n\n\nExperiments on Images in the Wild\n\nFinally, we perform experiments on real-world LQ face images, which suffer from complex unknown degradations. We collected 1, 000 LQ face images from internet for testing. The BFR methods Pix2PixHD [43], Super-FAN [5], GFRNet [29], GWAInet [9], DFDNet [27] and HiFaceGAN [47] are used in the comparison. Figure 7 shows the BFR results on three images. One can see that the competing methods fail to restore the facial details. This is mainly because they are trained on synthesized data but have limited generalization capability to the images in the wild. Our method manages to overcome this difficulty by the carefully designed GAN prior embedding and fine-tuning strategies. It not only preserves well the global structure of the face, but also generates realistic details on the face components (e.g., hair, eye, mouth, etc.). Our GPEN can also be successfully used to renovate old photos, as we demonstrated in Figure 1. Please refer to the supplementary material for more results.\n(a) (b) (c) (d) (e) (f) (g) (h)\nSince the commonly used quantitative metrics like PSNR and SSIM do not strongly correlate with human visual perception to image quality, we conduct a user study as a subjective assessment on the performance of our method and the competing methods. The BFR results of GPEN, Pix2PixHD [43], Super-FAN [5], GFRNet [29], GWAInet [9], DFDNet [27] and HiFaceGAN [47] on 113 real-world LQ face images collected from internet are presented in a random order to 17 volunteers for subjective evaluation. The volunteers are asked to rank the six BFR outputs of each input image according to their perceptual quality. Finally, we collect 1, 915 votes, and the statistics are presented in Figure 8. As can be seen, our GPEN method receives much more rank-1 votes than the other state-of-the arts.\n\n\nConclusion and Discussion\n\nWe proposed a simple yet effective GAN prior embedded network, namely GPEN, for BFR in the wild. By embedding a pre-trained GAN into a U-shaped DNN as a decoder, and fine-tuning the whole network with artificially degraded face images, our model learned to generate high quality face images from severely degraded ones. Our extensive experiments on synthetic data and real-world images demonstrated that GPEN outperforms the latest state-of-the-arts significantly, restoring clear facial details while retaining properly the image background. The proposed method can also be applied to other tasks such as face inpainting and face colorization. Some preliminary results were provided in the supplementary material.\n\nThe proposed GPEN does not allow multiple HQ images to be generated from a single LQ image in its current form. StyleGAN controls the synthesis via style mixing; however, such an operation may lead to inconsistent image background in GPEN. In the future, we will extend GPEN to allow multiple HQ outputs for a given LQ image. For example, we can use an extra HQ face image as a reference so that different HQ outputs can be generated by GPEN for different reference images.\n\nFigure 1 :\n1Restored face images from the group photo taken in the Solvay Conference, 1927. Best viewed by zooming to 200% in the screen.\n\nFigure 2 :\n2Illustration of the motivation and framework of our GAN prior embedded network (GPEN).\n\nFigure 3 :\n3The architecture of GPEN. (a) The GAN prior network; (b) detailed structures of a GAN block; and (c) the full network architecture of GPEN.\n\nFigure 4 :Figure 5 :\n45Comparisons of our variants BFR. (a) LQ input; (b) GPEN-w/o-ft; (c) GPEN-w/o-noise; (d) GPEN-noise-add; Blind face restoration results on synthsized degraded faces. (a) Degraded faces; (b) Super-FAN [5];\n\nFigure 6 :\n6Face super-resolution results by state-of-the-art methods. The input image has a resolution of 16 2 .\n\n\nFigure 6 presents a visual comparison example for zooming factor 64\u00d7. More visual comparison results can be found in the supplementary.\n\nFigure 7 :Figure 8 :\n78Blind face restoration results on real degraded faces in the wild. (a) Real degraded faces; (b) Super-FAN[5]; (c) GFRNet[29]; (d) GWAInet[9]; (e) Pix2PixHD[43]; (f) DFDNet[27]; (g) HiFaceGAN[47]; (h) GPEN. User study results of different BFR methods.\n\nTable 1 :\n1Comparison (PSNR, FID and LPIPS) of different \nvariants of GPEN. \nMethod \nPSNR\u2191 FID\u2193 LPIPS\u2193 \nGPEN-w/o-ft \n12.55 \n92.71 \n0.653 \nGPEN-w/o-noise \n13.30 \n95.62 \n0.709 \nGPEN-noise-add \n20.71 \n34.26 \n0.359 \nGPEN \n20.80 31.72 \n0.346 \n\nsame set of parameters used in Section 4.2. Table 1 lists \nthe PSNR, FID and LPIPS results. One can see that GPEN \nachieves better quantitative measures than its variants. Fig-\nure 4 shows the BSR results of the networks on an image. \n\n\nTable 2 :\n2Comparison (PSNR, FID and LPIPS) of different \nBFR methods. * \nMethod \nPSNR\u2191 FID\u2193 LPIPS\u2193 \nPix2PixHD [43] \n20.45 \n76.89 \n0.494 \nSuper-FAN [5] \n21.56 \n136.83 \n0.616 \nGFRNet [29] \n21.70 134.92 \n0.597 \nGWAInet [9] \n19.84 \n135.84 \n0.569 \nHiFaceGAN [47] \n21.33 \n56.67 \n0.392 \nGPEN \n20.80 \n31.72 \n0.346 \n\n\n* Note that the results of DFDNet[27] are not reported because it fails to recover many face images in this experiment.\n\nIm-age2stylegan: How to embed images into the stylegan latent space. Rameen Abdal, Yipeng Qin, Peter Wonka, ICCV. 36Rameen Abdal, Yipeng Qin, and Peter Wonka. Im- age2stylegan: How to embed images into the stylegan latent space? In ICCV, 2019. 3, 6\n\nHallucinating faces. S Bakerand, T Kanade, IEEE International Conference on Automatic Face and Gesture Recognition. S. Bakerand and T. Kanade. Hallucinating faces. In IEEE International Conference on Automatic Face and Gesture Recognition, 2000. 1\n\nRestoring degraded face images: A case study in matching faxed, printed, and scanned photos. Thirimachos Bourlai, Arun Ross, Anil K Jain, IEEE Transactions on Information Forensics and Security. 62Thirimachos Bourlai, Arun Ross, and Anil K. Jain. Restor- ing degraded face images: A case study in matching faxed, printed, and scanned photos. IEEE Transactions on Informa- tion Forensics and Security, 6(2):371-384, 2011. 1\n\nLarge scale GAN training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, ICLR. 34Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In ICLR, 2019. 3, 4\n\nSuper-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans. Adrian Bulat, Georgios Tzimiropoulos, CVPR. 7Adrian Bulat and Georgios Tzimiropoulos. Super-fan: In- tegrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans. In CVPR, 2018. 3, 6, 7, 8\n\nThe topological approach to perceptual organization. Lin Chen, Visual Cognition. 124Lin Chen. The topological approach to perceptual organiza- tion. Visual Cognition, 12(4):553-637, 2005. 3\n\nFsrnet: End-to-end learning face super-resolution with facial priors. Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang, CVPR. Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. Fsrnet: End-to-end learning face super-resolution with facial priors. In CVPR, 2018. 2\n\nStargan: Unified generative adversarial networks for multi-domain image-to-image translation. Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo, In CVPR. 3Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified genera- tive adversarial networks for multi-domain image-to-image translation. In CVPR, 2018. 3\n\nExemplar guided face image super-resolution without facial landmarks. Berk Dogan, Shuhang Gu, Radu Timofte, CVPRW. 7Berk Dogan, Shuhang Gu, and Radu Timofte. Exemplar guided face image super-resolution without facial landmarks. In CVPRW, 2019. 1, 2, 3, 6, 7, 8\n\nYa\u00ebl Fr\u00e9gier, Jean-Baptiste Gouray, Mind2mind : transfer learning for gans. arXivYa\u00ebl Fr\u00e9gier and Jean-Baptiste Gouray. Mind2mind : trans- fer learning for gans. arXiv, 2019. 3\n\nGenerative adversarial nets. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, page 2672-2680, 2014. 3\n\nImage processing using multi-code gan prior. ArXiv. Jinjin Gu, Yujun Shen, Bolei Zhou, 67Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. ArXiv, 2019. 1, 3, 4, 6, 7\n\nToward convolutional blind denoising of real photographs. CVPR. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. CVPR, 2019. 1, 2, 4\n\nPhase retrieval under a generative prior. Paul Hand, Oscar Leong, Vladislav Voroninski, NIPS. Paul Hand, Oscar Leong, and Vladislav Voroninski. Phase retrieval under a generative prior. In NIPS, 2018. 3\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, NIPS. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. In NIPS, 2017. 5\n\nFace super-resolution guided by 3d facial priors. Xiaobin Hu, Wenqi Ren, John Lamaster, Xiaochun Cao, Xiaoming Li, Zechao Li, Bjoern Menze, Wei Liu, ECCV. 1Xiaobin Hu, Wenqi Ren, John Lamaster, Xiaochun Cao, Xi- aoming Li, Zechao Li, Bjoern Menze, and Wei Liu. Face super-resolution guided by 3d facial priors. In ECCV, 2020. 1, 2\n\nWavelet-srnet: A wavelet-based cnn for multi-scale face super resolution. Huaibo Huang, Ran He, Zhenan Sun, Tieniu Tan, ICCV. Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: A wavelet-based cnn for multi-scale face su- per resolution. In ICCV, 2017. 2\n\nImage-to-image translation with conditional adversarial networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, 13Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adver- sarial networks. CVPR, 2017. 1, 3\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, ECCV. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 5\n\nProgressive growing of gans for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, In ICLR. 35Tero Karras, Timo Aila, and Samuli Laine. Progressive growing of gans for improved quality, stability, and varia- tion. In ICLR, 2018. 3, 4, 5\n\nA style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, ArXiv. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. ArXiv, 2018. 1, 2, 3, 4, 5\n\nAnalyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, ArXiv. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. ArXiv, 2019. 1, 2, 3, 4, 5\n\nProgressive face super-resolution via attention to facial landmark. Deokyun Kim, Minseon Kim, Gihyun Kwon, Dae-Shik Kim, ArXiv. 2Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. Progressive face super-resolution via attention to facial landmark. ArXiv, 2019. 2\n\nDeblurgan: Blind motion deblurring using conditional adversarial networks. Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Jiri Matas, 13Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. Deblurgan: Blind motion deblurring using conditional adversarial networks. ArXiv, 2017. 1, 2, 3\n\nPhoto-realistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, CVPR. 13Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 1, 2, 3\n\nDiverse imageto-image translation via disentangled representations. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang, In ECCV. 3Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Ma- neesh Kumar Singh, and Ming-Hsuan Yang. Diverse image- to-image translation via disentangled representations. In ECCV, 2018. 3\n\nBlind face restoration via deep multi-scale component dictionaries. Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, Lei Zhang, ECCV. 7Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restora- tion via deep multi-scale component dictionaries. In ECCV, 2020. 1, 2, 5, 6, 7, 8\n\nEnhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, Wangmeng Zuo, CVPR. 1Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020. 1, 2\n\nLearning warped guidance for blind face restoration. Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, Ruigang Yang, ECCV. 7Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. Learning warped guidance for blind face restoration. In ECCV, 2018. 1, 2, 3, 5, 6, 7, 8\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. 1Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 1, 2\n\nImage inpainting for irregular holes using partial convolutions. Guilin Liu, Fitsum A Reda, Kevin Shih, Ting-Chun Wang, Andrew Tao, Bryan Catanzaro, ArXiv. 2Guilin Liu, Fitsum A. Reda, Kevin Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for ir- regular holes using partial convolutions. ArXiv, 2018. 2\n\nFew-shot unsueprvised image-to-image translation. Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, Jan Kautz, Arxiv. 3Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz. Few-shot un- sueprvised image-to-image translation. Arxiv, 2019. 3\n\nDeep face super-resolution with iterative collaboration between attentive recovery and landmark estimation. Cheng Ma, Zhenyu Jiang, Yongming Rao, Jiwen Lu, Jie Zhou, CVPR. 2020Cheng Ma, Zhenyu Jiang, Yongming Rao, Jiwen Lu, and Jie Zhou. Deep face super-resolution with iterative collabora- tion between attentive recovery and landmark estimation. In CVPR, 2020. 2\n\nPulse: Self-supervised photo upsampling via latent space exploration of generative models. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin, CVPR. 67Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsam- pling via latent space exploration of generative models. In CVPR, 2020. 1, 3, 4, 6, 7\n\nConditional generative adversarial nets. ArXiv. Mehdi Mirza, Simon Osindero, Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. ArXiv, 2014. 3\n\nFacial deblur inference to improve recognition of blurred faces. Masashi Nishiyama, Hidenori Takeshima, Jamie Shotton, Tatsuo Kozakaya, Osamu Yamaguchi, CVPR. 1Masashi Nishiyama, Hidenori Takeshima, Jamie Shotton, Tatsuo Kozakaya, and Osamu Yamaguchi. Facial deblur in- ference to improve recognition of blurred faces. In CVPR, 2009. 1, 2\n\nSemantic image synthesis with spatially-adaptive normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, CVPR. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In CVPR, 2019. 3\n\nEncoding in style: a stylegan encoder for image-to-image translation. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or, Arxiv. 67Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. Arxiv, 2020. 1, 3, 4, 6, 7\n\nUnet: a convolutional network for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Arxiv. 4Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: a convolutional network for biomedical image segmentation. Arxiv, 2015. 4\n\nDeep semantic face deblurring. Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang, CVPR. Ziyi Shen, Wei-sheng Lai, Tingfa Xu, Jan Kautz, and Ming- Hsuan Yang. Deep semantic face deblurring. In CVPR, 2018. 2\n\nHigh quality facial surface and texture synthesis via generative adversarial networks. Ron Slossberg, Gil Shamai, Ron Kimmel, ICCV. Ron Slossberg, Gil Shamai, and Ron Kimmel. High quality facial surface and texture synthesis via generative adversarial networks. In ICCV, 2018. 3\n\nInfrared image colorization based on a triplet dcgan architecture. Patricia L Suarez, Angel D Sappa, Boris X Vintimilla, CVPRW. Patricia L. Suarez, Angel D. Sappa, and Boris X. Vintimilla. Infrared image colorization based on a triplet dcgan architec- ture. In CVPRW, 2017. 3\n\nHigh-resolution image synthesis and semantic manipulation with conditional gans. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro, CVPR. 7Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image syn- thesis and semantic manipulation with conditional gans. In CVPR, 2018. 1, 3, 4, 6, 7, 8\n\nEsrgan: Enhanced super-resolution generative adversarial networks. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy, In ECCVW. 3Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En- hanced super-resolution generative adversarial networks. In ECCVW, 2018. 3\n\nMinegan: effective knowledge transfer from gans to target domains with few images. Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Joost Fahad Shahbaz Khan, Van De Weijer, CVPR. Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: effective knowledge transfer from gans to target domains with few images. In CVPR, 2020. 3\n\nTransferring gans: generating images from limited data. Yaxing Wang, Chenshen Wu, Luis Herranz, Joost Van De, Abel Weijer, Bogdan Gonzalez-Garcia, Raducanu, In ECCV. 3Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In ECCV, 2018. 3\n\nHifacegan: Face renovation via collaborative suppression and replenishment. Arxiv. Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siweia Ma, Gao Wen, 7Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siweia Ma, and Gao Wen. Hifacegan: Face renova- tion via collaborative suppression and replenishment. Arxiv, 2020. 1, 2, 3, 6, 7, 8\n\n. Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S Huang, Generative image inpainting with contextual attention. ArXiv. 13Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contex- tual attention. ArXiv, 2018. 1, 2, 3\n\nHallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders. Xin Yu, Fatih Porikli, CVPR. Xin Yu and Fatih Porikli. Hallucinating very low-resolution unaligned and noisy face images by transformative discrimi- native autoencoders. In CVPR, 2017. 1\n\nClose the loop: Joint blind image restoration and recognition with sparse representation prior. Haichao Zhang, Jianchao Yang, Yanning Zhang, M Nasser, Thomas S Nasrabadi, Huang, ICCV. Haichao Zhang, Jianchao Yang, Yanning Zhang, Nasser M. Nasrabadi, and Thomas S. Huang. Close the loop: Joint blind image restoration and recognition with sparse representation prior. In ICCV, 2011. 1, 2\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, In CVPR. 5Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 5\n\nUnpaired image-to-image translation using cycleconsistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, ICCV. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In ICCV, 2017. 3\n\nToward multimodal image-to-image translation. Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, Eli Shechtman, NIPS. Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar- rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To- ward multimodal image-to-image translation. In NIPS, 2017. 3\n", "annotations": {"author": "[{\"end\":107,\"start\":69},{\"end\":165,\"start\":108},{\"end\":208,\"start\":166},{\"end\":338,\"start\":209}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":73},{\"end\":118,\"start\":115},{\"end\":178,\"start\":175},{\"end\":218,\"start\":213}]", "author_first_name": "[{\"end\":72,\"start\":69},{\"end\":114,\"start\":108},{\"end\":174,\"start\":166},{\"end\":212,\"start\":209}]", "author_affiliation": "[{\"end\":106,\"start\":79},{\"end\":164,\"start\":137},{\"end\":207,\"start\":180},{\"end\":274,\"start\":247},{\"end\":337,\"start\":276}]", "title": "[{\"end\":66,\"start\":1},{\"end\":404,\"start\":339}]", "venue": null, "abstract": "[{\"end\":1770,\"start\":406}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2199,\"start\":2195},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2201,\"start\":2199},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2203,\"start\":2201},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2206,\"start\":2203},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2454,\"start\":2450},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2457,\"start\":2454},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2460,\"start\":2457},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2463,\"start\":2460},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2516,\"start\":2512},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2519,\"start\":2516},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2522,\"start\":2519},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3040,\"start\":3036},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3062,\"start\":3058},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3065,\"start\":3062},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3067,\"start\":3065},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3090,\"start\":3086},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3130,\"start\":3126},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3148,\"start\":3144},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3478,\"start\":3474},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3497,\"start\":3493},{\"end\":3656,\"start\":3640},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3778,\"start\":3774},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3781,\"start\":3778},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3886,\"start\":3882},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3889,\"start\":3886},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3892,\"start\":3889},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3916,\"start\":3912},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4570,\"start\":4566},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4573,\"start\":4570},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6733,\"start\":6729},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6894,\"start\":6890},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7115,\"start\":7111},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7134,\"start\":7130},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7137,\"start\":7134},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7154,\"start\":7150},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7157,\"start\":7154},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7189,\"start\":7185},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7192,\"start\":7189},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7267,\"start\":7264},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7270,\"start\":7267},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7273,\"start\":7270},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7661,\"start\":7657},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7663,\"start\":7661},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7666,\"start\":7663},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7669,\"start\":7666},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7777,\"start\":7773},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7890,\"start\":7887},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7982,\"start\":7978},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8153,\"start\":8149},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8356,\"start\":8353},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8384,\"start\":8380},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8429,\"start\":8425},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8445,\"start\":8441},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8612,\"start\":8608},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9106,\"start\":9102},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9172,\"start\":9168},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9174,\"start\":9172},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9177,\"start\":9174},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9180,\"start\":9177},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9366,\"start\":9362},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9389,\"start\":9385},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9392,\"start\":9389},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9417,\"start\":9413},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9420,\"start\":9417},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9444,\"start\":9440},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9561,\"start\":9557},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9631,\"start\":9627},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9634,\"start\":9631},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9637,\"start\":9634},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9915,\"start\":9911},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9943,\"start\":9939},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9967,\"start\":9963},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10005,\"start\":10001},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10035,\"start\":10032},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10064,\"start\":10060},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10431,\"start\":10427},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10454,\"start\":10450},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10476,\"start\":10472},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10689,\"start\":10686},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10772,\"start\":10768},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10775,\"start\":10772},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10793,\"start\":10790},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10919,\"start\":10915},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11157,\"start\":11153},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11324,\"start\":11320},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11802,\"start\":11798},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11978,\"start\":11974},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12003,\"start\":11999},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13050,\"start\":13047},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13053,\"start\":13050},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13055,\"start\":13053},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13363,\"start\":13360},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13386,\"start\":13382},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13389,\"start\":13386},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13693,\"start\":13690},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13696,\"start\":13693},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13698,\"start\":13696},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13701,\"start\":13698},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13704,\"start\":13701},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14384,\"start\":14380},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14428,\"start\":14424},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14431,\"start\":14428},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14434,\"start\":14431},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14861,\"start\":14857},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14936,\"start\":14932},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14939,\"start\":14936},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15365,\"start\":15361},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15368,\"start\":15365},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16022,\"start\":16018},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16034,\"start\":16030},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16049,\"start\":16046},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16821,\"start\":16817},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17699,\"start\":17695},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17702,\"start\":17699},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18456,\"start\":18452},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19292,\"start\":19288},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19559,\"start\":19555},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19926,\"start\":19922},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19989,\"start\":19985},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20323,\"start\":20319},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20326,\"start\":20323},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20961,\"start\":20957},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20964,\"start\":20961},{\"end\":21249,\"start\":21246},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21252,\"start\":21249},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21312,\"start\":21308},{\"end\":21316,\"start\":21312},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21401,\"start\":21398},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21404,\"start\":21401},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22890,\"start\":22886},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22907,\"start\":22904},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22927,\"start\":22923},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22944,\"start\":22940},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22964,\"start\":22960},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23382,\"start\":23379},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23385,\"start\":23382},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24161,\"start\":24157},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24164,\"start\":24161},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24167,\"start\":24164},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24596,\"start\":24592},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24611,\"start\":24608},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24624,\"start\":24620},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24637,\"start\":24634},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24650,\"start\":24646},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24666,\"start\":24662},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24785,\"start\":24781},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24788,\"start\":24785},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24791,\"start\":24788},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24962,\"start\":24958},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24977,\"start\":24973},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25162,\"start\":25159},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25178,\"start\":25175},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25194,\"start\":25190},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25213,\"start\":25209},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25232,\"start\":25228},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25251,\"start\":25247},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25266,\"start\":25262},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25279,\"start\":25275},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26651,\"start\":26648},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26664,\"start\":26660},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26677,\"start\":26674},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26690,\"start\":26686},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26707,\"start\":26703},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26723,\"start\":26719},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26735,\"start\":26731},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26749,\"start\":26745},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27437,\"start\":27433},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27452,\"start\":27449},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27465,\"start\":27461},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27478,\"start\":27475},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27491,\"start\":27487},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27510,\"start\":27506},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28541,\"start\":28537},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28556,\"start\":28553},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28569,\"start\":28565},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28582,\"start\":28579},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28595,\"start\":28591},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28614,\"start\":28610},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31262,\"start\":31259},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31278,\"start\":31274},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31294,\"start\":31291},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31313,\"start\":31309},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31329,\"start\":31325},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":31348,\"start\":31344},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32229,\"start\":32225}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30395,\"start\":30257},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30495,\"start\":30396},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30648,\"start\":30496},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30876,\"start\":30649},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30991,\"start\":30877},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31129,\"start\":30992},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31404,\"start\":31130},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31880,\"start\":31405},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32191,\"start\":31881}]", "paragraph": "[{\"end\":2621,\"start\":1786},{\"end\":3727,\"start\":2623},{\"end\":4575,\"start\":3729},{\"end\":5791,\"start\":4577},{\"end\":5855,\"start\":5793},{\"end\":6535,\"start\":5857},{\"end\":7758,\"start\":6552},{\"end\":9018,\"start\":7760},{\"end\":10308,\"start\":9020},{\"end\":11735,\"start\":10310},{\"end\":12503,\"start\":11737},{\"end\":13656,\"start\":12550},{\"end\":14385,\"start\":13658},{\"end\":14803,\"start\":14387},{\"end\":15783,\"start\":14828},{\"end\":16712,\"start\":15785},{\"end\":17551,\"start\":16714},{\"end\":17910,\"start\":17573},{\"end\":18102,\"start\":17912},{\"end\":18579,\"start\":18152},{\"end\":18782,\"start\":18632},{\"end\":18815,\"start\":18784},{\"end\":19222,\"start\":18843},{\"end\":20198,\"start\":19271},{\"end\":20548,\"start\":20225},{\"end\":21728,\"start\":20587},{\"end\":22123,\"start\":21730},{\"end\":22712,\"start\":22142},{\"end\":23964,\"start\":22714},{\"end\":24327,\"start\":24000},{\"end\":26245,\"start\":24329},{\"end\":26856,\"start\":26247},{\"end\":27197,\"start\":26858},{\"end\":28221,\"start\":27235},{\"end\":29037,\"start\":28254},{\"end\":29781,\"start\":29067},{\"end\":30256,\"start\":29783}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18151,\"start\":18103},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18631,\"start\":18580},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18842,\"start\":18816},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20586,\"start\":20549},{\"attributes\":{\"id\":\"formula_4\"},\"end\":28253,\"start\":28222}]", "table_ref": "[{\"end\":24883,\"start\":24876},{\"end\":26907,\"start\":26900}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1784,\"start\":1772},{\"attributes\":{\"n\":\"2.\"},\"end\":6550,\"start\":6538},{\"attributes\":{\"n\":\"3.\"},\"end\":12521,\"start\":12506},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12548,\"start\":12524},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14826,\"start\":14806},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17571,\"start\":17554},{\"attributes\":{\"n\":\"4.\"},\"end\":19236,\"start\":19225},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19269,\"start\":19239},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20223,\"start\":20201},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22140,\"start\":22126},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23998,\"start\":23967},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27233,\"start\":27200},{\"attributes\":{\"n\":\"5.\"},\"end\":29065,\"start\":29040},{\"end\":30268,\"start\":30258},{\"end\":30407,\"start\":30397},{\"end\":30507,\"start\":30497},{\"end\":30670,\"start\":30650},{\"end\":30888,\"start\":30878},{\"end\":31151,\"start\":31131},{\"end\":31415,\"start\":31406},{\"end\":31891,\"start\":31882}]", "table": "[{\"end\":31880,\"start\":31417},{\"end\":32191,\"start\":31893}]", "figure_caption": "[{\"end\":30395,\"start\":30270},{\"end\":30495,\"start\":30409},{\"end\":30648,\"start\":30509},{\"end\":30876,\"start\":30673},{\"end\":30991,\"start\":30890},{\"end\":31129,\"start\":30994},{\"end\":31404,\"start\":31154}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5638,\"start\":5630},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13190,\"start\":13182},{\"end\":13642,\"start\":13634},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13893,\"start\":13885},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14744,\"start\":14736},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15092,\"start\":15084},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15484,\"start\":15473},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15912,\"start\":15904},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16900,\"start\":16892},{\"end\":25663,\"start\":25655},{\"end\":27547,\"start\":27539},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28159,\"start\":28151},{\"end\":28938,\"start\":28930}]", "bib_author_first_name": "[{\"end\":32388,\"start\":32382},{\"end\":32402,\"start\":32396},{\"end\":32413,\"start\":32408},{\"end\":32585,\"start\":32584},{\"end\":32597,\"start\":32596},{\"end\":32916,\"start\":32905},{\"end\":32930,\"start\":32926},{\"end\":32941,\"start\":32937},{\"end\":32943,\"start\":32942},{\"end\":33310,\"start\":33304},{\"end\":33322,\"start\":33318},{\"end\":33337,\"start\":33332},{\"end\":33636,\"start\":33630},{\"end\":33652,\"start\":33644},{\"end\":33938,\"start\":33935},{\"end\":34145,\"start\":34143},{\"end\":34156,\"start\":34152},{\"end\":34170,\"start\":34162},{\"end\":34183,\"start\":34176},{\"end\":34194,\"start\":34190},{\"end\":34457,\"start\":34451},{\"end\":34469,\"start\":34464},{\"end\":34484,\"start\":34476},{\"end\":34498,\"start\":34490},{\"end\":34510,\"start\":34503},{\"end\":34522,\"start\":34516},{\"end\":34809,\"start\":34805},{\"end\":34824,\"start\":34817},{\"end\":34833,\"start\":34829},{\"end\":35001,\"start\":34997},{\"end\":35024,\"start\":35011},{\"end\":35207,\"start\":35204},{\"end\":35209,\"start\":35208},{\"end\":35226,\"start\":35222},{\"end\":35247,\"start\":35242},{\"end\":35259,\"start\":35255},{\"end\":35269,\"start\":35264},{\"end\":35291,\"start\":35284},{\"end\":35304,\"start\":35299},{\"end\":35322,\"start\":35316},{\"end\":35590,\"start\":35584},{\"end\":35600,\"start\":35595},{\"end\":35612,\"start\":35607},{\"end\":35802,\"start\":35797},{\"end\":35815,\"start\":35812},{\"end\":35829,\"start\":35821},{\"end\":35840,\"start\":35837},{\"end\":36040,\"start\":36036},{\"end\":36052,\"start\":36047},{\"end\":36069,\"start\":36060},{\"end\":36287,\"start\":36281},{\"end\":36302,\"start\":36296},{\"end\":36319,\"start\":36313},{\"end\":36341,\"start\":36333},{\"end\":36355,\"start\":36351},{\"end\":36625,\"start\":36618},{\"end\":36635,\"start\":36630},{\"end\":36645,\"start\":36641},{\"end\":36664,\"start\":36656},{\"end\":36678,\"start\":36670},{\"end\":36689,\"start\":36683},{\"end\":36700,\"start\":36694},{\"end\":36711,\"start\":36708},{\"end\":36980,\"start\":36974},{\"end\":36991,\"start\":36988},{\"end\":37002,\"start\":36996},{\"end\":37014,\"start\":37008},{\"end\":37243,\"start\":37236},{\"end\":37258,\"start\":37251},{\"end\":37271,\"start\":37264},{\"end\":37284,\"start\":37278},{\"end\":37286,\"start\":37285},{\"end\":37519,\"start\":37513},{\"end\":37538,\"start\":37529},{\"end\":37548,\"start\":37546},{\"end\":37780,\"start\":37776},{\"end\":37793,\"start\":37789},{\"end\":37806,\"start\":37800},{\"end\":38047,\"start\":38043},{\"end\":38062,\"start\":38056},{\"end\":38074,\"start\":38070},{\"end\":38291,\"start\":38287},{\"end\":38306,\"start\":38300},{\"end\":38319,\"start\":38314},{\"end\":38334,\"start\":38329},{\"end\":38351,\"start\":38345},{\"end\":38366,\"start\":38362},{\"end\":38628,\"start\":38621},{\"end\":38641,\"start\":38634},{\"end\":38653,\"start\":38647},{\"end\":38668,\"start\":38660},{\"end\":38903,\"start\":38898},{\"end\":38920,\"start\":38911},{\"end\":38935,\"start\":38929},{\"end\":38954,\"start\":38948},{\"end\":38968,\"start\":38964},{\"end\":39252,\"start\":39243},{\"end\":39265,\"start\":39260},{\"end\":39279,\"start\":39273},{\"end\":39292,\"start\":39288},{\"end\":39310,\"start\":39304},{\"end\":39332,\"start\":39323},{\"end\":39347,\"start\":39341},{\"end\":39363,\"start\":39356},{\"end\":39380,\"start\":39372},{\"end\":39392,\"start\":39387},{\"end\":39405,\"start\":39399},{\"end\":39778,\"start\":39769},{\"end\":39791,\"start\":39784},{\"end\":39806,\"start\":39799},{\"end\":39827,\"start\":39814},{\"end\":39845,\"start\":39835},{\"end\":40115,\"start\":40107},{\"end\":40128,\"start\":40120},{\"end\":40144,\"start\":40135},{\"end\":40158,\"start\":40151},{\"end\":40172,\"start\":40164},{\"end\":40181,\"start\":40178},{\"end\":40489,\"start\":40481},{\"end\":40499,\"start\":40494},{\"end\":40511,\"start\":40504},{\"end\":40524,\"start\":40517},{\"end\":40536,\"start\":40532},{\"end\":40551,\"start\":40543},{\"end\":40822,\"start\":40814},{\"end\":40831,\"start\":40827},{\"end\":40843,\"start\":40837},{\"end\":40856,\"start\":40848},{\"end\":40867,\"start\":40862},{\"end\":40880,\"start\":40873},{\"end\":41130,\"start\":41127},{\"end\":41144,\"start\":41136},{\"end\":41156,\"start\":41150},{\"end\":41170,\"start\":41162},{\"end\":41185,\"start\":41176},{\"end\":41427,\"start\":41421},{\"end\":41439,\"start\":41433},{\"end\":41441,\"start\":41440},{\"end\":41453,\"start\":41448},{\"end\":41469,\"start\":41460},{\"end\":41482,\"start\":41476},{\"end\":41493,\"start\":41488},{\"end\":41742,\"start\":41735},{\"end\":41751,\"start\":41748},{\"end\":41763,\"start\":41759},{\"end\":41776,\"start\":41772},{\"end\":41789,\"start\":41785},{\"end\":41802,\"start\":41796},{\"end\":41816,\"start\":41813},{\"end\":42106,\"start\":42101},{\"end\":42117,\"start\":42111},{\"end\":42133,\"start\":42125},{\"end\":42144,\"start\":42139},{\"end\":42152,\"start\":42149},{\"end\":42456,\"start\":42450},{\"end\":42473,\"start\":42464},{\"end\":42488,\"start\":42482},{\"end\":42499,\"start\":42493},{\"end\":42513,\"start\":42506},{\"end\":42780,\"start\":42775},{\"end\":42793,\"start\":42788},{\"end\":42965,\"start\":42958},{\"end\":42985,\"start\":42977},{\"end\":43002,\"start\":42997},{\"end\":43018,\"start\":43012},{\"end\":43034,\"start\":43029},{\"end\":43304,\"start\":43297},{\"end\":43318,\"start\":43311},{\"end\":43333,\"start\":43324},{\"end\":43347,\"start\":43340},{\"end\":43577,\"start\":43573},{\"end\":43595,\"start\":43590},{\"end\":43606,\"start\":43604},{\"end\":43623,\"start\":43618},{\"end\":43637,\"start\":43632},{\"end\":43648,\"start\":43644},{\"end\":43664,\"start\":43658},{\"end\":43957,\"start\":43953},{\"end\":43978,\"start\":43971},{\"end\":43994,\"start\":43988},{\"end\":44177,\"start\":44173},{\"end\":44193,\"start\":44184},{\"end\":44205,\"start\":44199},{\"end\":44213,\"start\":44210},{\"end\":44231,\"start\":44221},{\"end\":44453,\"start\":44450},{\"end\":44468,\"start\":44465},{\"end\":44480,\"start\":44477},{\"end\":44718,\"start\":44710},{\"end\":44720,\"start\":44719},{\"end\":44734,\"start\":44729},{\"end\":44736,\"start\":44735},{\"end\":44749,\"start\":44744},{\"end\":44751,\"start\":44750},{\"end\":45010,\"start\":45001},{\"end\":45024,\"start\":45017},{\"end\":45037,\"start\":45030},{\"end\":45049,\"start\":45043},{\"end\":45058,\"start\":45055},{\"end\":45071,\"start\":45066},{\"end\":45365,\"start\":45359},{\"end\":45374,\"start\":45372},{\"end\":45387,\"start\":45379},{\"end\":45398,\"start\":45392},{\"end\":45408,\"start\":45403},{\"end\":45418,\"start\":45414},{\"end\":45427,\"start\":45425},{\"end\":45445,\"start\":45434},{\"end\":45735,\"start\":45729},{\"end\":45746,\"start\":45742},{\"end\":45769,\"start\":45764},{\"end\":45781,\"start\":45777},{\"end\":45796,\"start\":45791},{\"end\":46108,\"start\":46102},{\"end\":46123,\"start\":46115},{\"end\":46132,\"start\":46128},{\"end\":46147,\"start\":46142},{\"end\":46160,\"start\":46156},{\"end\":46175,\"start\":46169},{\"end\":46480,\"start\":46474},{\"end\":46492,\"start\":46487},{\"end\":46501,\"start\":46498},{\"end\":46515,\"start\":46508},{\"end\":46528,\"start\":46522},{\"end\":46540,\"start\":46534},{\"end\":46548,\"start\":46545},{\"end\":46756,\"start\":46750},{\"end\":46764,\"start\":46761},{\"end\":46775,\"start\":46770},{\"end\":46789,\"start\":46782},{\"end\":46799,\"start\":46796},{\"end\":46812,\"start\":46804},{\"end\":47153,\"start\":47150},{\"end\":47163,\"start\":47158},{\"end\":47441,\"start\":47434},{\"end\":47457,\"start\":47449},{\"end\":47471,\"start\":47464},{\"end\":47480,\"start\":47479},{\"end\":47495,\"start\":47489},{\"end\":47497,\"start\":47496},{\"end\":47805,\"start\":47798},{\"end\":47820,\"start\":47813},{\"end\":47834,\"start\":47828},{\"end\":47836,\"start\":47835},{\"end\":47847,\"start\":47844},{\"end\":47865,\"start\":47859},{\"end\":48137,\"start\":48130},{\"end\":48150,\"start\":48143},{\"end\":48164,\"start\":48157},{\"end\":48178,\"start\":48172},{\"end\":48180,\"start\":48179},{\"end\":48409,\"start\":48402},{\"end\":48422,\"start\":48415},{\"end\":48436,\"start\":48430},{\"end\":48451,\"start\":48445},{\"end\":48467,\"start\":48461},{\"end\":48469,\"start\":48468},{\"end\":48483,\"start\":48477},{\"end\":48493,\"start\":48490}]", "bib_author_last_name": "[{\"end\":32394,\"start\":32389},{\"end\":32406,\"start\":32403},{\"end\":32419,\"start\":32414},{\"end\":32594,\"start\":32586},{\"end\":32604,\"start\":32598},{\"end\":32924,\"start\":32917},{\"end\":32935,\"start\":32931},{\"end\":32948,\"start\":32944},{\"end\":33316,\"start\":33311},{\"end\":33330,\"start\":33323},{\"end\":33346,\"start\":33338},{\"end\":33642,\"start\":33637},{\"end\":33666,\"start\":33653},{\"end\":33943,\"start\":33939},{\"end\":34150,\"start\":34146},{\"end\":34160,\"start\":34157},{\"end\":34174,\"start\":34171},{\"end\":34188,\"start\":34184},{\"end\":34199,\"start\":34195},{\"end\":34462,\"start\":34458},{\"end\":34474,\"start\":34470},{\"end\":34488,\"start\":34485},{\"end\":34501,\"start\":34499},{\"end\":34514,\"start\":34511},{\"end\":34527,\"start\":34523},{\"end\":34815,\"start\":34810},{\"end\":34827,\"start\":34825},{\"end\":34841,\"start\":34834},{\"end\":35009,\"start\":35002},{\"end\":35031,\"start\":35025},{\"end\":35220,\"start\":35210},{\"end\":35240,\"start\":35227},{\"end\":35253,\"start\":35248},{\"end\":35262,\"start\":35260},{\"end\":35282,\"start\":35270},{\"end\":35297,\"start\":35292},{\"end\":35314,\"start\":35305},{\"end\":35329,\"start\":35323},{\"end\":35593,\"start\":35591},{\"end\":35605,\"start\":35601},{\"end\":35617,\"start\":35613},{\"end\":35810,\"start\":35803},{\"end\":35819,\"start\":35816},{\"end\":35835,\"start\":35830},{\"end\":35844,\"start\":35841},{\"end\":35851,\"start\":35846},{\"end\":36045,\"start\":36041},{\"end\":36058,\"start\":36053},{\"end\":36080,\"start\":36070},{\"end\":36294,\"start\":36288},{\"end\":36311,\"start\":36303},{\"end\":36331,\"start\":36320},{\"end\":36349,\"start\":36342},{\"end\":36366,\"start\":36356},{\"end\":36628,\"start\":36626},{\"end\":36639,\"start\":36636},{\"end\":36654,\"start\":36646},{\"end\":36668,\"start\":36665},{\"end\":36681,\"start\":36679},{\"end\":36692,\"start\":36690},{\"end\":36706,\"start\":36701},{\"end\":36715,\"start\":36712},{\"end\":36986,\"start\":36981},{\"end\":36994,\"start\":36992},{\"end\":37006,\"start\":37003},{\"end\":37018,\"start\":37015},{\"end\":37249,\"start\":37244},{\"end\":37262,\"start\":37259},{\"end\":37276,\"start\":37272},{\"end\":37292,\"start\":37287},{\"end\":37527,\"start\":37520},{\"end\":37544,\"start\":37539},{\"end\":37556,\"start\":37549},{\"end\":37787,\"start\":37781},{\"end\":37798,\"start\":37794},{\"end\":37812,\"start\":37807},{\"end\":38054,\"start\":38048},{\"end\":38068,\"start\":38063},{\"end\":38079,\"start\":38075},{\"end\":38298,\"start\":38292},{\"end\":38312,\"start\":38307},{\"end\":38327,\"start\":38320},{\"end\":38343,\"start\":38335},{\"end\":38360,\"start\":38352},{\"end\":38371,\"start\":38367},{\"end\":38632,\"start\":38629},{\"end\":38645,\"start\":38642},{\"end\":38658,\"start\":38654},{\"end\":38672,\"start\":38669},{\"end\":38909,\"start\":38904},{\"end\":38927,\"start\":38921},{\"end\":38946,\"start\":38936},{\"end\":38962,\"start\":38955},{\"end\":38974,\"start\":38969},{\"end\":39258,\"start\":39253},{\"end\":39271,\"start\":39266},{\"end\":39286,\"start\":39280},{\"end\":39302,\"start\":39293},{\"end\":39321,\"start\":39311},{\"end\":39339,\"start\":39333},{\"end\":39354,\"start\":39348},{\"end\":39370,\"start\":39364},{\"end\":39385,\"start\":39381},{\"end\":39397,\"start\":39393},{\"end\":39409,\"start\":39406},{\"end\":39782,\"start\":39779},{\"end\":39797,\"start\":39792},{\"end\":39812,\"start\":39807},{\"end\":39833,\"start\":39828},{\"end\":39850,\"start\":39846},{\"end\":40118,\"start\":40116},{\"end\":40133,\"start\":40129},{\"end\":40149,\"start\":40145},{\"end\":40162,\"start\":40159},{\"end\":40176,\"start\":40173},{\"end\":40187,\"start\":40182},{\"end\":40492,\"start\":40490},{\"end\":40502,\"start\":40500},{\"end\":40515,\"start\":40512},{\"end\":40530,\"start\":40525},{\"end\":40541,\"start\":40537},{\"end\":40555,\"start\":40552},{\"end\":40825,\"start\":40823},{\"end\":40835,\"start\":40832},{\"end\":40846,\"start\":40844},{\"end\":40860,\"start\":40857},{\"end\":40871,\"start\":40868},{\"end\":40885,\"start\":40881},{\"end\":41134,\"start\":41131},{\"end\":41148,\"start\":41145},{\"end\":41160,\"start\":41157},{\"end\":41174,\"start\":41171},{\"end\":41189,\"start\":41186},{\"end\":41431,\"start\":41428},{\"end\":41446,\"start\":41442},{\"end\":41458,\"start\":41454},{\"end\":41474,\"start\":41470},{\"end\":41486,\"start\":41483},{\"end\":41503,\"start\":41494},{\"end\":41746,\"start\":41743},{\"end\":41757,\"start\":41752},{\"end\":41770,\"start\":41764},{\"end\":41783,\"start\":41777},{\"end\":41794,\"start\":41790},{\"end\":41811,\"start\":41803},{\"end\":41822,\"start\":41817},{\"end\":42109,\"start\":42107},{\"end\":42123,\"start\":42118},{\"end\":42137,\"start\":42134},{\"end\":42147,\"start\":42145},{\"end\":42157,\"start\":42153},{\"end\":42462,\"start\":42457},{\"end\":42480,\"start\":42474},{\"end\":42491,\"start\":42489},{\"end\":42504,\"start\":42500},{\"end\":42519,\"start\":42514},{\"end\":42786,\"start\":42781},{\"end\":42802,\"start\":42794},{\"end\":42975,\"start\":42966},{\"end\":42995,\"start\":42986},{\"end\":43010,\"start\":43003},{\"end\":43027,\"start\":43019},{\"end\":43044,\"start\":43035},{\"end\":43309,\"start\":43305},{\"end\":43322,\"start\":43319},{\"end\":43338,\"start\":43334},{\"end\":43351,\"start\":43348},{\"end\":43588,\"start\":43578},{\"end\":43602,\"start\":43596},{\"end\":43616,\"start\":43607},{\"end\":43630,\"start\":43624},{\"end\":43642,\"start\":43638},{\"end\":43656,\"start\":43649},{\"end\":43673,\"start\":43665},{\"end\":43969,\"start\":43958},{\"end\":43986,\"start\":43979},{\"end\":43999,\"start\":43995},{\"end\":44182,\"start\":44178},{\"end\":44197,\"start\":44194},{\"end\":44208,\"start\":44206},{\"end\":44219,\"start\":44214},{\"end\":44236,\"start\":44232},{\"end\":44463,\"start\":44454},{\"end\":44475,\"start\":44469},{\"end\":44487,\"start\":44481},{\"end\":44727,\"start\":44721},{\"end\":44742,\"start\":44737},{\"end\":44762,\"start\":44752},{\"end\":45015,\"start\":45011},{\"end\":45028,\"start\":45025},{\"end\":45041,\"start\":45038},{\"end\":45053,\"start\":45050},{\"end\":45064,\"start\":45059},{\"end\":45081,\"start\":45072},{\"end\":45370,\"start\":45366},{\"end\":45377,\"start\":45375},{\"end\":45390,\"start\":45388},{\"end\":45401,\"start\":45399},{\"end\":45412,\"start\":45409},{\"end\":45423,\"start\":45419},{\"end\":45432,\"start\":45428},{\"end\":45449,\"start\":45446},{\"end\":45740,\"start\":45736},{\"end\":45762,\"start\":45747},{\"end\":45775,\"start\":45770},{\"end\":45789,\"start\":45782},{\"end\":45815,\"start\":45797},{\"end\":45830,\"start\":45817},{\"end\":46113,\"start\":46109},{\"end\":46126,\"start\":46124},{\"end\":46140,\"start\":46133},{\"end\":46154,\"start\":46148},{\"end\":46167,\"start\":46161},{\"end\":46191,\"start\":46176},{\"end\":46201,\"start\":46193},{\"end\":46485,\"start\":46481},{\"end\":46496,\"start\":46493},{\"end\":46506,\"start\":46502},{\"end\":46520,\"start\":46516},{\"end\":46532,\"start\":46529},{\"end\":46543,\"start\":46541},{\"end\":46552,\"start\":46549},{\"end\":46759,\"start\":46757},{\"end\":46768,\"start\":46765},{\"end\":46780,\"start\":46776},{\"end\":46794,\"start\":46790},{\"end\":46802,\"start\":46800},{\"end\":46818,\"start\":46813},{\"end\":47156,\"start\":47154},{\"end\":47171,\"start\":47164},{\"end\":47447,\"start\":47442},{\"end\":47462,\"start\":47458},{\"end\":47477,\"start\":47472},{\"end\":47487,\"start\":47481},{\"end\":47507,\"start\":47498},{\"end\":47514,\"start\":47509},{\"end\":47811,\"start\":47806},{\"end\":47826,\"start\":47821},{\"end\":47842,\"start\":47837},{\"end\":47857,\"start\":47848},{\"end\":47870,\"start\":47866},{\"end\":48141,\"start\":48138},{\"end\":48155,\"start\":48151},{\"end\":48170,\"start\":48165},{\"end\":48186,\"start\":48181},{\"end\":48413,\"start\":48410},{\"end\":48428,\"start\":48423},{\"end\":48443,\"start\":48437},{\"end\":48459,\"start\":48452},{\"end\":48475,\"start\":48470},{\"end\":48488,\"start\":48484},{\"end\":48503,\"start\":48494}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":102350964},\"end\":32561,\"start\":32313},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6076353},\"end\":32810,\"start\":32563},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8361163},\"end\":33234,\"start\":32812},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52889459},\"end\":33491,\"start\":33236},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4319595},\"end\":33880,\"start\":33493},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12146469},\"end\":34071,\"start\":33882},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4564405},\"end\":34355,\"start\":34073},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9417016},\"end\":34733,\"start\":34357},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":189928566},\"end\":34995,\"start\":34735},{\"attributes\":{\"id\":\"b9\"},\"end\":35173,\"start\":34997},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1033682},\"end\":35530,\"start\":35175},{\"attributes\":{\"id\":\"b11\"},\"end\":35731,\"start\":35532},{\"attributes\":{\"id\":\"b12\"},\"end\":35992,\"start\":35733},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":49665488},\"end\":36196,\"start\":35994},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":326772},\"end\":36566,\"start\":36198},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":209098048},\"end\":36898,\"start\":36568},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":22066308},\"end\":37168,\"start\":36900},{\"attributes\":{\"id\":\"b17\"},\"end\":37442,\"start\":37170},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":980236},\"end\":37698,\"start\":37444},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3568073},\"end\":37967,\"start\":37700},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":54482423},\"end\":38230,\"start\":37969},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":209202273},\"end\":38551,\"start\":38232},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":201317137},\"end\":38821,\"start\":38553},{\"attributes\":{\"id\":\"b23\"},\"end\":39155,\"start\":38823},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211227},\"end\":39699,\"start\":39157},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51904575},\"end\":40037,\"start\":39701},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220935924},\"end\":40383,\"start\":40039},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219618216},\"end\":40759,\"start\":40385},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4889032},\"end\":41058,\"start\":40761},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6540453},\"end\":41354,\"start\":41060},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5035107},\"end\":41683,\"start\":41356},{\"attributes\":{\"id\":\"b31\"},\"end\":41991,\"start\":41685},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":214714394},\"end\":42357,\"start\":41993},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":212634162},\"end\":42725,\"start\":42359},{\"attributes\":{\"id\":\"b34\"},\"end\":42891,\"start\":42727},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1520124},\"end\":43231,\"start\":42893},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":81981856},\"end\":43501,\"start\":43233},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220936362},\"end\":43886,\"start\":43503},{\"attributes\":{\"id\":\"b38\"},\"end\":44140,\"start\":43888},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3825399},\"end\":44361,\"start\":44142},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52095607},\"end\":44641,\"start\":44363},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":8979656},\"end\":44918,\"start\":44643},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":41805341},\"end\":45290,\"start\":44920},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":52154773},\"end\":45644,\"start\":45292},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":209202392},\"end\":46044,\"start\":45646},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":19108210},\"end\":46389,\"start\":46046},{\"attributes\":{\"id\":\"b46\"},\"end\":46746,\"start\":46391},{\"attributes\":{\"id\":\"b47\"},\"end\":47035,\"start\":46748},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2064528},\"end\":47336,\"start\":47037},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":7109488},\"end\":47724,\"start\":47338},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4766599},\"end\":48048,\"start\":47726},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":233404466},\"end\":48354,\"start\":48050},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":19046372},\"end\":48684,\"start\":48356}]", "bib_title": "[{\"end\":32380,\"start\":32313},{\"end\":32582,\"start\":32563},{\"end\":32903,\"start\":32812},{\"end\":33302,\"start\":33236},{\"end\":33628,\"start\":33493},{\"end\":33933,\"start\":33882},{\"end\":34141,\"start\":34073},{\"end\":34449,\"start\":34357},{\"end\":34803,\"start\":34735},{\"end\":35202,\"start\":35175},{\"end\":36034,\"start\":35994},{\"end\":36279,\"start\":36198},{\"end\":36616,\"start\":36568},{\"end\":36972,\"start\":36900},{\"end\":37511,\"start\":37444},{\"end\":37774,\"start\":37700},{\"end\":38041,\"start\":37969},{\"end\":38285,\"start\":38232},{\"end\":38619,\"start\":38553},{\"end\":39241,\"start\":39157},{\"end\":39767,\"start\":39701},{\"end\":40105,\"start\":40039},{\"end\":40479,\"start\":40385},{\"end\":40812,\"start\":40761},{\"end\":41125,\"start\":41060},{\"end\":41419,\"start\":41356},{\"end\":41733,\"start\":41685},{\"end\":42099,\"start\":41993},{\"end\":42448,\"start\":42359},{\"end\":42956,\"start\":42893},{\"end\":43295,\"start\":43233},{\"end\":43571,\"start\":43503},{\"end\":43951,\"start\":43888},{\"end\":44171,\"start\":44142},{\"end\":44448,\"start\":44363},{\"end\":44708,\"start\":44643},{\"end\":44999,\"start\":44920},{\"end\":45357,\"start\":45292},{\"end\":45727,\"start\":45646},{\"end\":46100,\"start\":46046},{\"end\":47148,\"start\":47037},{\"end\":47432,\"start\":47338},{\"end\":47796,\"start\":47726},{\"end\":48128,\"start\":48050},{\"end\":48400,\"start\":48356}]", "bib_author": "[{\"end\":32396,\"start\":32382},{\"end\":32408,\"start\":32396},{\"end\":32421,\"start\":32408},{\"end\":32596,\"start\":32584},{\"end\":32606,\"start\":32596},{\"end\":32926,\"start\":32905},{\"end\":32937,\"start\":32926},{\"end\":32950,\"start\":32937},{\"end\":33318,\"start\":33304},{\"end\":33332,\"start\":33318},{\"end\":33348,\"start\":33332},{\"end\":33644,\"start\":33630},{\"end\":33668,\"start\":33644},{\"end\":33945,\"start\":33935},{\"end\":34152,\"start\":34143},{\"end\":34162,\"start\":34152},{\"end\":34176,\"start\":34162},{\"end\":34190,\"start\":34176},{\"end\":34201,\"start\":34190},{\"end\":34464,\"start\":34451},{\"end\":34476,\"start\":34464},{\"end\":34490,\"start\":34476},{\"end\":34503,\"start\":34490},{\"end\":34516,\"start\":34503},{\"end\":34529,\"start\":34516},{\"end\":34817,\"start\":34805},{\"end\":34829,\"start\":34817},{\"end\":34843,\"start\":34829},{\"end\":35011,\"start\":34997},{\"end\":35033,\"start\":35011},{\"end\":35222,\"start\":35204},{\"end\":35242,\"start\":35222},{\"end\":35255,\"start\":35242},{\"end\":35264,\"start\":35255},{\"end\":35284,\"start\":35264},{\"end\":35299,\"start\":35284},{\"end\":35316,\"start\":35299},{\"end\":35331,\"start\":35316},{\"end\":35595,\"start\":35584},{\"end\":35607,\"start\":35595},{\"end\":35619,\"start\":35607},{\"end\":35812,\"start\":35797},{\"end\":35821,\"start\":35812},{\"end\":35837,\"start\":35821},{\"end\":35846,\"start\":35837},{\"end\":35853,\"start\":35846},{\"end\":36047,\"start\":36036},{\"end\":36060,\"start\":36047},{\"end\":36082,\"start\":36060},{\"end\":36296,\"start\":36281},{\"end\":36313,\"start\":36296},{\"end\":36333,\"start\":36313},{\"end\":36351,\"start\":36333},{\"end\":36368,\"start\":36351},{\"end\":36630,\"start\":36618},{\"end\":36641,\"start\":36630},{\"end\":36656,\"start\":36641},{\"end\":36670,\"start\":36656},{\"end\":36683,\"start\":36670},{\"end\":36694,\"start\":36683},{\"end\":36708,\"start\":36694},{\"end\":36717,\"start\":36708},{\"end\":36988,\"start\":36974},{\"end\":36996,\"start\":36988},{\"end\":37008,\"start\":36996},{\"end\":37020,\"start\":37008},{\"end\":37251,\"start\":37236},{\"end\":37264,\"start\":37251},{\"end\":37278,\"start\":37264},{\"end\":37294,\"start\":37278},{\"end\":37529,\"start\":37513},{\"end\":37546,\"start\":37529},{\"end\":37558,\"start\":37546},{\"end\":37789,\"start\":37776},{\"end\":37800,\"start\":37789},{\"end\":37814,\"start\":37800},{\"end\":38056,\"start\":38043},{\"end\":38070,\"start\":38056},{\"end\":38081,\"start\":38070},{\"end\":38300,\"start\":38287},{\"end\":38314,\"start\":38300},{\"end\":38329,\"start\":38314},{\"end\":38345,\"start\":38329},{\"end\":38362,\"start\":38345},{\"end\":38373,\"start\":38362},{\"end\":38634,\"start\":38621},{\"end\":38647,\"start\":38634},{\"end\":38660,\"start\":38647},{\"end\":38674,\"start\":38660},{\"end\":38911,\"start\":38898},{\"end\":38929,\"start\":38911},{\"end\":38948,\"start\":38929},{\"end\":38964,\"start\":38948},{\"end\":38976,\"start\":38964},{\"end\":39260,\"start\":39243},{\"end\":39273,\"start\":39260},{\"end\":39288,\"start\":39273},{\"end\":39304,\"start\":39288},{\"end\":39323,\"start\":39304},{\"end\":39341,\"start\":39323},{\"end\":39356,\"start\":39341},{\"end\":39372,\"start\":39356},{\"end\":39387,\"start\":39372},{\"end\":39399,\"start\":39387},{\"end\":39411,\"start\":39399},{\"end\":39784,\"start\":39769},{\"end\":39799,\"start\":39784},{\"end\":39814,\"start\":39799},{\"end\":39835,\"start\":39814},{\"end\":39852,\"start\":39835},{\"end\":40120,\"start\":40107},{\"end\":40135,\"start\":40120},{\"end\":40151,\"start\":40135},{\"end\":40164,\"start\":40151},{\"end\":40178,\"start\":40164},{\"end\":40189,\"start\":40178},{\"end\":40494,\"start\":40481},{\"end\":40504,\"start\":40494},{\"end\":40517,\"start\":40504},{\"end\":40532,\"start\":40517},{\"end\":40543,\"start\":40532},{\"end\":40557,\"start\":40543},{\"end\":40827,\"start\":40814},{\"end\":40837,\"start\":40827},{\"end\":40848,\"start\":40837},{\"end\":40862,\"start\":40848},{\"end\":40873,\"start\":40862},{\"end\":40887,\"start\":40873},{\"end\":41136,\"start\":41127},{\"end\":41150,\"start\":41136},{\"end\":41162,\"start\":41150},{\"end\":41176,\"start\":41162},{\"end\":41191,\"start\":41176},{\"end\":41433,\"start\":41421},{\"end\":41448,\"start\":41433},{\"end\":41460,\"start\":41448},{\"end\":41476,\"start\":41460},{\"end\":41488,\"start\":41476},{\"end\":41505,\"start\":41488},{\"end\":41748,\"start\":41735},{\"end\":41759,\"start\":41748},{\"end\":41772,\"start\":41759},{\"end\":41785,\"start\":41772},{\"end\":41796,\"start\":41785},{\"end\":41813,\"start\":41796},{\"end\":41824,\"start\":41813},{\"end\":42111,\"start\":42101},{\"end\":42125,\"start\":42111},{\"end\":42139,\"start\":42125},{\"end\":42149,\"start\":42139},{\"end\":42159,\"start\":42149},{\"end\":42464,\"start\":42450},{\"end\":42482,\"start\":42464},{\"end\":42493,\"start\":42482},{\"end\":42506,\"start\":42493},{\"end\":42521,\"start\":42506},{\"end\":42788,\"start\":42775},{\"end\":42804,\"start\":42788},{\"end\":42977,\"start\":42958},{\"end\":42997,\"start\":42977},{\"end\":43012,\"start\":42997},{\"end\":43029,\"start\":43012},{\"end\":43046,\"start\":43029},{\"end\":43311,\"start\":43297},{\"end\":43324,\"start\":43311},{\"end\":43340,\"start\":43324},{\"end\":43353,\"start\":43340},{\"end\":43590,\"start\":43573},{\"end\":43604,\"start\":43590},{\"end\":43618,\"start\":43604},{\"end\":43632,\"start\":43618},{\"end\":43644,\"start\":43632},{\"end\":43658,\"start\":43644},{\"end\":43675,\"start\":43658},{\"end\":43971,\"start\":43953},{\"end\":43988,\"start\":43971},{\"end\":44001,\"start\":43988},{\"end\":44184,\"start\":44173},{\"end\":44199,\"start\":44184},{\"end\":44210,\"start\":44199},{\"end\":44221,\"start\":44210},{\"end\":44238,\"start\":44221},{\"end\":44465,\"start\":44450},{\"end\":44477,\"start\":44465},{\"end\":44489,\"start\":44477},{\"end\":44729,\"start\":44710},{\"end\":44744,\"start\":44729},{\"end\":44764,\"start\":44744},{\"end\":45017,\"start\":45001},{\"end\":45030,\"start\":45017},{\"end\":45043,\"start\":45030},{\"end\":45055,\"start\":45043},{\"end\":45066,\"start\":45055},{\"end\":45083,\"start\":45066},{\"end\":45372,\"start\":45359},{\"end\":45379,\"start\":45372},{\"end\":45392,\"start\":45379},{\"end\":45403,\"start\":45392},{\"end\":45414,\"start\":45403},{\"end\":45425,\"start\":45414},{\"end\":45434,\"start\":45425},{\"end\":45451,\"start\":45434},{\"end\":45742,\"start\":45729},{\"end\":45764,\"start\":45742},{\"end\":45777,\"start\":45764},{\"end\":45791,\"start\":45777},{\"end\":45817,\"start\":45791},{\"end\":45832,\"start\":45817},{\"end\":46115,\"start\":46102},{\"end\":46128,\"start\":46115},{\"end\":46142,\"start\":46128},{\"end\":46156,\"start\":46142},{\"end\":46169,\"start\":46156},{\"end\":46193,\"start\":46169},{\"end\":46203,\"start\":46193},{\"end\":46487,\"start\":46474},{\"end\":46498,\"start\":46487},{\"end\":46508,\"start\":46498},{\"end\":46522,\"start\":46508},{\"end\":46534,\"start\":46522},{\"end\":46545,\"start\":46534},{\"end\":46554,\"start\":46545},{\"end\":46761,\"start\":46750},{\"end\":46770,\"start\":46761},{\"end\":46782,\"start\":46770},{\"end\":46796,\"start\":46782},{\"end\":46804,\"start\":46796},{\"end\":46820,\"start\":46804},{\"end\":47158,\"start\":47150},{\"end\":47173,\"start\":47158},{\"end\":47449,\"start\":47434},{\"end\":47464,\"start\":47449},{\"end\":47479,\"start\":47464},{\"end\":47489,\"start\":47479},{\"end\":47509,\"start\":47489},{\"end\":47516,\"start\":47509},{\"end\":47813,\"start\":47798},{\"end\":47828,\"start\":47813},{\"end\":47844,\"start\":47828},{\"end\":47859,\"start\":47844},{\"end\":47872,\"start\":47859},{\"end\":48143,\"start\":48130},{\"end\":48157,\"start\":48143},{\"end\":48172,\"start\":48157},{\"end\":48188,\"start\":48172},{\"end\":48415,\"start\":48402},{\"end\":48430,\"start\":48415},{\"end\":48445,\"start\":48430},{\"end\":48461,\"start\":48445},{\"end\":48477,\"start\":48461},{\"end\":48490,\"start\":48477},{\"end\":48505,\"start\":48490}]", "bib_venue": "[{\"end\":32425,\"start\":32421},{\"end\":32677,\"start\":32606},{\"end\":33005,\"start\":32950},{\"end\":33352,\"start\":33348},{\"end\":33672,\"start\":33668},{\"end\":33961,\"start\":33945},{\"end\":34205,\"start\":34201},{\"end\":34536,\"start\":34529},{\"end\":34848,\"start\":34843},{\"end\":35071,\"start\":35033},{\"end\":35335,\"start\":35331},{\"end\":35582,\"start\":35532},{\"end\":35795,\"start\":35733},{\"end\":36086,\"start\":36082},{\"end\":36372,\"start\":36368},{\"end\":36721,\"start\":36717},{\"end\":37024,\"start\":37020},{\"end\":37234,\"start\":37170},{\"end\":37562,\"start\":37558},{\"end\":37821,\"start\":37814},{\"end\":38086,\"start\":38081},{\"end\":38378,\"start\":38373},{\"end\":38679,\"start\":38674},{\"end\":38896,\"start\":38823},{\"end\":39415,\"start\":39411},{\"end\":39859,\"start\":39852},{\"end\":40193,\"start\":40189},{\"end\":40561,\"start\":40557},{\"end\":40891,\"start\":40887},{\"end\":41196,\"start\":41191},{\"end\":41510,\"start\":41505},{\"end\":41829,\"start\":41824},{\"end\":42163,\"start\":42159},{\"end\":42525,\"start\":42521},{\"end\":42773,\"start\":42727},{\"end\":43050,\"start\":43046},{\"end\":43357,\"start\":43353},{\"end\":43680,\"start\":43675},{\"end\":44006,\"start\":44001},{\"end\":44242,\"start\":44238},{\"end\":44493,\"start\":44489},{\"end\":44769,\"start\":44764},{\"end\":45087,\"start\":45083},{\"end\":45459,\"start\":45451},{\"end\":45836,\"start\":45832},{\"end\":46210,\"start\":46203},{\"end\":46472,\"start\":46391},{\"end\":46880,\"start\":46820},{\"end\":47177,\"start\":47173},{\"end\":47520,\"start\":47516},{\"end\":47879,\"start\":47872},{\"end\":48192,\"start\":48188},{\"end\":48509,\"start\":48505}]"}}}, "year": 2023, "month": 12, "day": 17}