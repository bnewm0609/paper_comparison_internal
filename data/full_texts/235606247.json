{"id": 235606247, "updated": "2023-10-06 01:45:41.008", "metadata": {"title": "MIMHD: Accurate and Efficient Hyperdimensional Inference Using Multi-Bit In-Memory Computing", "authors": "[{\"first\":\"Arman\",\"last\":\"Kazemi\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Sharifi\",\"middle\":[\"Mehdi\"]},{\"first\":\"Zhuowen\",\"last\":\"Zou\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Niemier\",\"middle\":[]},{\"first\":\"X.\",\"last\":\"Hu\",\"middle\":[\"Sharon\"]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]}]", "venue": "2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "journal": "2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "publication_date": {"year": 2021, "month": 6, "day": 22}, "abstract": "Hyperdimensional Computing (HDC) is an emerging computational framework that mimics important brain functions by operating over high-dimensional vectors, called hypervectors (HVs). In-memory computing implementations of HDC are desirable since they can significantly reduce data transfer overheads. All existing in-memory HDC platforms consider binary HVs where each dimension is represented with a single bit. However, utilizing multi-bit HVs allows HDC to achieve acceptable accuracies in lower dimensions which in turn leads to higher energy efficiencies. Thus, we propose a highly accurate and efficient multi-bit in-memory HDC inference platform called MIMHD. MIMHD supports multi-bit operations using ferroelectric field-effect transistor (FeFET) crossbar arrays for multiply-and-add and FeFET multi-bit content-addressable memories for associative search. We also introduce a novel hardware-aware retraining framework (HWART) that trains the HDC model to learn to work with MIMHD. For six popular datasets and 4000 dimension HVs, MIMHD using 3-bit (2-bit) precision HVs achieves (i) average accuracies of 92.6% (88.9%) which is 8.5% (4.8%) higher than binary implementations; (ii) 84.1x (78.6x) energy improvement over a GPU, and (iii) 38.4x (34.3x) speedup over a GPU, respectively. The 3-bit $\\times$ is 4.3x and 13x faster and more energy-efficient than binary HDC accelerators while achieving similar accuracies.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.12029", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/islped/KazemiSZNHI21", "doi": "10.1109/islped52811.2021.9502498"}}, "content": {"source": {"pdf_hash": "8bd63a8c9e27e9f61744833360f04c0d11a62aaa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.12029v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2106.12029", "status": "GREEN"}}, "grobid": {"id": "dc95cd63077cd84f82f9d3a163c2a37531f84195", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8bd63a8c9e27e9f61744833360f04c0d11a62aaa.txt", "contents": "\nMIMHD: Accurate and Efficient Hyperdimensional Inference Using Multi-Bit In-Memory Computing\n\n\nArman Kazemi akazemi@nd.edu \nMohammad Mehdi Sharifi \nZhuowen Zou \nUniversity of California San Diego\n\n\nMichael Niemier \nX \nSharon Hu \nMohsen Imani \nUniversity of California Irvine\n\n\n\nUniversity of Notre Dame\n\n\nMIMHD: Accurate and Efficient Hyperdimensional Inference Using Multi-Bit In-Memory Computing\n\nHyperdimensional Computing (HDC) is an emerging computational framework that mimics important brain functions by operating over high-dimensional vectors, called hypervectors (HVs). In-memory computing implementations of HDC are desirable since they can significantly reduce data transfer overheads. All existing in-memory HDC platforms consider binary HVs where each dimension is represented with a single bit. However, utilizing multi-bit HVs allows HDC to achieve acceptable accuracies in lower dimensions which in turn leads to higher energy efficiencies. Thus, we propose a highly accurate and efficient multi-bit in-memory HDC inference platform called MIMHD. MIMHD supports multi-bit operations using ferroelectric field-effect transistor (FeFET) crossbar arrays for multiply-and-add and FeFET multi-bit content-addressable memories for associative search. We also introduce a novel hardware-aware retraining framework (HWART) that trains the HDC model to learn to work with MIMHD. For six popular datasets and 4000 dimension HVs, MIMHD using 3-bit (2-bit) precision HVs achieves (i) average accuracies of 92.6% (88.9%) which is 8.5% (4.8%) higher than binary implementations; (ii) 84.1\u00d7 (78.6\u00d7) energy improvement over a GPU, and (iii) 38.4\u00d7 (34.3\u00d7) speedup over a GPU, respectively. The 3-bit MIMHD is 4.3\u00d7 and 13\u00d7 faster and more energy-efficient than binary HDC accelerators while achieving similar accuracies.\n\nI. INTRODUCTION\n\nHyperdimensional Computing (HDC) is an emerging computational framework based on the observation that the brain operates in hyperdimensional spaces over high-dimensional vectors to perform cognition and perception [1]. The computational units of HDC are high-dimensional vectors called hypervectors (HVs). HDC is applicable to a wide range of applications such as robotics [2], machine learning, and cognitive computing. HDC achieves similar or higher accuracy than convolutional neural networks (CNNs), support vector machines (SVMs), and extreme gradient boosting for applications such as distributed sensors [3], multimodal sensor fusion [4], and biomedical signal processing [5].\n\nAnother important advantage of HDC over other machine learning models such as CNNs and SVMs is the higher energy efficiency [6], [7]. HDC operates over high-dimensional HVs and can greatly benefit from the high parallelism offered by processing in-memory (PIM) architectures and offer even more energy and latency improvements [8], [9]. In addition, the inherent robustness of HDC to computational errors makes it amenable to PIM architectures based on emerging devices with variability issues. However, previous PIM HDC works in the literature has focused on binary implementations of HDC which suffer from accuracy loss compared to floating point models and require dimensions higher that 10,000 which thwarts achieving high energy efficiencies [10], [11].\n\nWe observe that using 2 or 3-bit HVs could improve HDC's inference accuracy to match that of a high precision model. This is inline with the fact that the synapses in the brain have 4.7 bits of precision [12]. Furthermore, using multi-bit HVs allows reducing the number of dimensions at iso-accuracy which in turn offers significant energy savings. One of the most prominent impediments to designing multi-bit HDC accelerators is associative search based on cosine distance. Binary HDC implementations leverage Hamming distance measurements using ternary content-addressable memories (TCAMs) to avoid cosine distance. On the other hand, multiple binary storage elements are required to store multi-bit HVs which reduces energy efficiency gains and adds more complexity.\n\nRecently, ferroelectric field-effect transistor(FeFET) multibit content-addressable memories (MCAMs) were proposed that can perform single-step associative search based on the MCAM distance metric [13]. Furthermore, multi-level FeFETs for storage and computation have been experimentally demonstrated [14]. These advances make FeFETs a suitable candidate for designing PIM based multi-bit HDC accelerators.\n\nIn this paper, we introduce MIMHD, an in-memory implementation of HDC inference. MIMHD achieves comparable accuracy as high-bit precision HDC while offering significant energy and latency reduction. MIMHD directly represents each dimension of a HV using multi-bit FeFETs and supports essential HDC inference operations, i.e., encoding and associative search. MIMHD uses FeFET crossbar arrays [14] to implement encoding, and FeFET MCAMs [13] for associative search. We propose a novel hardware-aware retraining (HWART) algorithm that trains the HDC model to compensate for possible accuracy loss due to the MCAM distance metric. We evaluate the efficiency and accuracy of MIMHD over a wide range of classification problems. For six popular datasets, MIMHD using 2 and 3-bit precisions achieves (i) average accuracies of 88.9% and 92.6% which is similar to high precision (8-bit) implementations, (ii) 84.1\u00d7 and 78.6\u00d7 energy improvement over a GPU, and (iii) 38.4\u00d7 and 34.3\u00d7 speedup over a GPU, respectively. 3-bit MIMHD is 4.3\u00d7 and 13\u00d7 faster and more energy-efficient than the binary HDC accelerators [15].\n\n\nII. BACKGROUND AND RELATED WORK\n\n\nA. Hyperdimensional Computing Basics\n\nHDC is a neuro-inspired model of computation based on the observation that the human brain operates on high-dimensional  and distributed representations of data [1] called hypervectors (HVs). Researchers have applied HDC to diverse cognitive tasks, such as robotics [2], classification [16], bio-signal processing [17], and regression [18]. Most of the previous work use binary HVs. Below, we discuss an overview of HDC.\n\n1) HDC Encoding: The first step in HDC is to map each data point into high-dimensional space. The mapping procedure is often referred to as encoding (shown in Fig. 1b).\n\nThe encoding is such that the data points different from each other in the original space are also different in the HDC space. To explain the encoding process, assume an input feature vector (an image, voice, etc.) in the original space F = {f 1 , f 2 , \u00b7 \u00b7 \u00b7 , f n } and F \u2208 R n . HDC first linearly quantizes the feature vector values into m levels, {q 1 ,q 2 , \u00b7 \u00b7 \u00b7 ,q m }, and assigns a level HV to each quantization level, { L 1 , L 2 , \u00b7 \u00b7 \u00b7 , L m }. The level HVs are generated such that L 1 and L m are orthogonal \u03b4( L 1 , L m ) 0, but the intermediate levels follow a spectrum of similarity, depending on their physical closeness toq 1 andq m . The encoding module maps F into a high-dimensional vector H \u2208 {0, 1} D , where D >> n. The following equation and Fig. 1b show an encoding method that maps an input vector into highdimensional space [6]:\nH = n k=1 L(f k ) * B k(1)\nwhere base HVs ( B k s) retain the spatial or temporal location of the features. Base HVs are randomly chosen, hence orthogonal. Base HVs are also of dimension D. Function L(f k ) assigns the corresponding level HV to the features.\n\n2) HDC Training: To find the universal property for each class in the training dataset, the trainer module linearly combines HVs belonging to each class, i.e., adding the HVs to create a single HV for each class. Once all HVs are combined, we treat per-class accumulated HVs, called class HVs, as the learned model. Fig. 1a shows HDC operation during single-pass training. Assuming a problem with k classes, M = { C 1 , C 2 , \u00b7 \u00b7 \u00b7 , C k } represents the model. For example, after encoding all of training data belonging to class/label l, the class HV C l can be obtained by bundling (adding) all H l s. Assuming there are J inputs with label l: C l = J j H l j . 3) HDC Inference: For inference, the model checks the similarity of each encoded test data with the class HVs in two steps. The first step encodes the input (the same encoding used for training) to produce a query HV H. Then, as Fig. 1a shows, we compute the similarity (\u03b4) of H and all class HVs. Query data is labeled with the label of the class HV with the highest similarity.\n\n\nB. HDC Acceleration\n\nPrior work showed how the binary HDC models could be accelerated using PIM platforms [15], [19]. For example, work in [8], [9], [15] showed that PIM architectures leveraging CAMs can accelerate HDC associative search by two orders of magnitude. CAMs enable row-parallel associative search over binary HVs with the Hamming distance metric. However, using non-binary HVs cannot readily exploit current PIM architectures to accelerate HDC inference. A multi-bit PIM structure that supports associative search could be highly desirable for accelerating accurate HDC inference.\n\n\nC. Computing with Multi-Bit FeFETs\n\nThe FeFET device is a promising candidate for PIM architectures. FeFETs are made by integrating a ferroelectric (FE) layer in the gate stack of a MOSFET [20] (Fig. 2a). The threshold voltage (V th ) of the FeFET is determined by the non-volatile polarization of the FE layer. Applying voltage pulses to the gate of the FeFET changes the polarization of the FE layer and consequently the V th of the FeFET device [14]. Fig. 2b shows the transfer characteristics of a FeFET device programmed to 8 different V th states, i.e., storing 3-bits of information [13].\n\nFeFET crossbar arrays have been used to accelerate neural network inference and training [14], [21]. Work in [13] proposed a FeFET multi-bit CAM (MCAM), shown in Fig. 2c, that can perform parallel in-memory associative search based on the MCAM distance metric. FeFET MCAM cells store different states by programming V th\u2212Hi and V th\u2212Lo and search using voltage signals on the data lines (DL and DL). Fig. 2d shows the MCAM distance metric for a single cell where the x-axis represents the distance between the stored state and the search input, and the y-axis indicates the conductance of the cell (which directly represents similarity). The conductance grows exponentially with respect to the distance and saturates towards the higher distance. This behavior results from the transfer characteristics of the FeFETs. By sensing the matchlines (ML in Fig. 2c), which are connected to all cells in a row (Fig. 2e), we can find the MCAM row with the lowest conductance (data most similar to the search query). Although the MCAM distance metric is not a previously known and commonly used metric like cosine or Euclidean, it achieves high accuracies for different applications [13]. Furthermore, technologies like resistive RAM are amenable to multi-bit crossbar arrays, but they do not implement a CAM-based multi-bit associative search with a useful distance metric.\n\n\nIII. PROPOSED MIMHD\n\nIn this section, we present MIMHD, an accurate and efficient in-memory platform for HDC inference. We exploit multi-bit FeFET devices to directly support 1, 2, and 3-bit precision implementation of HDC. We leverage PIM to implement encoding and associative search operations to achieve  high energy and latency efficiency. Furthermore, we introduce a novel hardware-aware retraining framework (HWART) that trains the HDC model to adapt to the unique features of FeFET MCAMs in MIMHD. The overall structure of MIMHD is shown in Fig. 3 which consists of an encoding and an associative search module.\n\n\nA. Encoding\n\nAs discussed in Section II-A1, the goal of encoding is to transform the data into a high-dimensional space. Using a multi-bit encoding, we increase the information representation capacity of the HVs, compared to binary implementations. The encoding module of MIMHD operates in multi-bit precision and works with multi-bit level HVs and base HVs. To generate m level HVs we generate a random D-dimensional P -bit (P \u2208 {1, 2, 3} denotes the bit precision) HV to represent level 1. To generate the HV for level 2, we randomly choose D/ m dimensions and randomly assign them values from 1 to 2 P . We continue this process until we have generated m level HVs. Generating level HVs only happens once during training when P and m are chosen. As such, f k 's in equation 1 are represented with their corresponding level HVs. Base HVs are chosen randomly (Section II-A1) and are also fixed. Fig. 3a shows the encoding module of MIMHD. We utilize FeFET crossbar arrays to store the level HVs. We need to store a total of n HVs of size m\u00d7D for a dataset with n features. We consider m = 64 to support up to 64 level HVs and minimize accuracy loss due to HV quantization. Given that D is in the order of thousands, we use D/64 crossbar arrays of size 64 \u00d7 64 for each feature since the computation is independent for each dimension. Each i \u2212 th group of crossbar arrays (i \u2208 [1, 2, . . . , n]) activates the rows containing the level HV corresponding to f i and performs a multiplication between that level HV and the i th base HV ( B i ) as its input. Source lines (SLs) add the resulting currents from these multiplications based on Kirchhoff's law and feed them to the analog-todigital converters (ADCs). This current-based addition ensures that we do not need to store intermediate values during the encoding; avoiding expensive FeFET writes. The ADCs are used to generate a P -bit D-dimensional encoded HV.  \n\n\nB. Associative Search\n\nAssociative search is the final step in HDC inference where the class HVs are compared with the query HV. Associative search is arguably the most challenging operation of multi-bit HDC to implement with PIM. TCAMs can realize associative search with Hamming distance, but they are only suitable for binary implementations. Thus, we utilize FeFET MCAMs [13] for associative search in MIMHD. Given the high dimensionality of the class HVs, we use multiple MCAM arrays of size 64 \u00d7 64. The MCAMs store the k trained class HVs in their rows (Fig. 3b). The encoded query is routed to the MCAMs as input to perform associative search with the stored class HVs based on the MCAM distance metric. We add the currents from the same rows of the different MCAMs to get the distance for D dimensions following the implementation in [9]. Then sense amplifies in [15] and used to detect the row that is the most similar to the encoded query (lowest current) and report the corresponding class as the result of the prediction.\n\nThe utilized MCAM cell (Fig. 2c) also supports Hamming distance search [22]. This allows MIMHD to support the binary implementation of HDC by programming the FeFET crossbar arrays in a binary fashion. All peripherals that support multi-bit operations in MIMHD support the binary operations as well. The only difference between the binary and multi-bit operations is using Hamming distance for associative search for the former and the MCAM distance metric for the latter.\n\n\nC. Hardware-Aware Retraining (HWART)\n\nThe HDC training method has been theoretically designed to work with the cosine distance. MIMHD uses the MCAM distance metric for associative search, which can result in a loss of classification accuracy. In order to compensate for this possible quality loss, we propose HWART, a retraining method that teaches the model to work with a new distance metric. Fig. 4 shows an overview of the proposed HWART. We store two copies of the HDC model: quantized and non- \nC l \u2190 C l + \u03b7 (\u03b4 l \u2212 \u03b4 l ) \u00d7 H C l \u2190 C l \u2212 \u03b7 (\u03b4 l \u2212 \u03b4 l ) \u00d7 H(2)\nwhere \u03b4 l = \u03b4( H, C l ) and \u03b4 l = \u03b4( H, C l ) are the similarity of the query with correct and miss-predicted classes, respectively. Thus, we ensure that we update the model based on how \"far\" a training data point is miss-classified with the current model. In case of a very far miss-prediction, \u03b4 l >> \u03b4 l , we make greater changes to the non-quantized model; while in case of a marginal miss-prediction, \u03b4 l \u03b4 l , the update makes lesser changes to the non-quantized model.\n\nAfter updating the non-quantized model over a batch of training data, we update the quantized model by mapping each model element of non-quantized model to the desired number of bits ( \u2022 E ). We continue this adaptive iterative procedure until the classification accuracy stabilizes (Fig. 5).\n\n\nIV. EVALUATION\n\n\nA. Experimental Setup\n\nWe implement MIMHD both in software and hardware simulation. In software, we implemented and verified MIMHD functionality with a Python implementation. To evaluate MIMHD accuracy, we design a cycle-accurate simulator based on our framework in PyTorch [23]. Our framework translates all PyTorch code directly to vector-based operations supported by our PIM architecture. For the hardware design, we use NeuroSim [24] to obtain energy and latency results for the  [31] crossbar arrays including all required peripherals. We use HSPICE and the FeFETs Preisach model [20] to measure the energy consumption and latency of the FeFET MCAMs and other peripherals. All the hardware components are based on a 22nm technology node. We compare the energy and latency consumption of MIMHD with a NVIDIA GTX 1080 GPU running the HDC models with 8-bit precision. The performance and energy of the GPU are measured by the nvidia-smi tool. We evaluate the accuracy and efficiency of MIMHD on six popular datasets (listed in Table I), ranging from small datasets collected in a small IoT network to large datasets which includes hundreds of thousands of images of facial data.\n\n\nB. MIMHD Accuracy\n\nMIMHD accuracy depends on three parameters: (i) the HV dimensionality that determines the HV capacity and the level of redundancy, (ii) the precision of each HV element, and (iii) the distance metric used for associative search.\n\n1) Dimensionality: Fig. 6a compares HDC classification accuracy for different dimensionalities for face detection applications. Other datasets exhibit similar trends. Our evaluation shows that increasing dimensionality results in improving the classification accuracy for all configurations. In the low dimensional space, the binary model is much less accurate than the higher precision models. This accuracy gap shrinks by scaling the dimensionality. For example, the 1-bit precision model provides 17.9% and 4.2% lower accuracy than the 8-bit model in D=1k and D=10k, respectively. Fig. 6b reports the maximum classification accuracy achieved by the HDC models using different bit precisions and the lowest dimensionality that provides such accuracy. The results are averaged over all tested datasets. Our evaluation shows that MIMHD, using multi-bit elements, reduces the number of required dimensions to achieve maximum accuracy compared to the binary model. MIMHD provides maximum classification accuracy in D=4k (D=8k) for 3-bit (2-bit) precision. The results also indicate that, regardless of dimensionality, the binary HDC model using Hamming distance cannot achieve the maximum accuracy provided by the full precision (8-bit) model. In contrast, MIMHD can provide the same accuracy as the full precision model while enabling a fully in-memory implementation.   Fig. 6. Impact of dimensionality on MIMHD accuracy.\n\n2) Hypervector Precision: Fig. 7a compares the classification accuracy of HDC models in different precisions using different search metrics/hardware. All results are reported using the same dimensionality (D=4k). MIMHD is trained with HWART. Results show that HDC classification accuracy highly depends on model precision. MIMHD in 2-bit and 3bit precisions provides, on average across all datasets, 8.3% and 4.7% higher classification accuracy as compared to the binary model. Moreover, 3-bit MIMHD can achieve the same accuracies as an 8-bit model using cosine distance.\n\n3) Distance Metric: Fig. 7b shows the quality loss of models trained with cosine distance and deployed on MIMHD, due to MIMHD's using MCAM distance metric. The results indicate that, in the same bit precision without HWART, 2 and 3-bit MIMHD provides, on average, 1.77% and 2.7% lower accuracy than the 8-bit HDC model with cosine distance. Note that the accuracy of 2-bit and 3-bit MIMHD models are still 6.5% and 3.2% higher than the binary model using the Hamming distance metric. As Fig. 7b shows, enhancing the model with HWART can compensate for the quality loss caused by the MCAM distance metric. For example, MIMHD 3-bit achieves a similar accuracy as HDC with 3 and 8-bit precisions using cosine distance.\n\n\nC. MIMHD Efficiency\n\nWe compare the performance of MIMHD with a NVIDIA GTX 1080 GPU in terms of energy and latency since GPUs are the most common computing fabrics for HDC. We compare MIMHD, which is entirely FeFET-based, in 1, 2, and 3-bit precision with the GPU to illustrate the efficiency trends of operating in different bit precisions with the same technology. We further compare the performance of MIMHD with stateof-the-art PIM-based HDC platforms: AHAM [9] that uses a Loser-Take All circuit to support Hamming distance search, and SearcHD [15] that exploits ganged-inverters and inverse CAM to support the same search functionality. Our evaluations are performed in two setups: (i) Maximum Accuracy, at maximum achievable accuracy of the designs, and (ii) Same Accuracy, when all designs achieve the same accuracy, i.e., the maximum accuracy of a binary design.\n\n\n1)\n\nEnergy: Fig. 8a shows the energy improvement results of MIMHD over the GPU in different bit precisions and for different tasks (Table I). Due to its memory-centric nature, MIMHD in all bit precisions, achieves high energy improvements over the GPU by performing computations in-memory. The difference in energy improvement of MIMHD in different bit precisions is mainly due to the overhead of the ADCs as higher precision ADCs consume higher energy. On average,  for the considered tasks, MIMHD using 1, 2, and 3 bits achieves 90\u00d7, 84\u00d7, and 78\u00d7 improvements over the GPU. Fig. 8c compares the average energy improvement of AHAM [9], SearcHD [15], and 2 and 3-bit MIMHD over the GPU. The most important factor that affects these results is the dimensionality. At Maximum Accuracy, 3-bit MIMHD operates in D=4k while AHAM and SearcHD operate in D=16k (Fig. 6b), but still MIMHD achieves a higher accuracy. Dimensionality has a linear relation with energy efficiency as it requires more resources, e.g., crossbar arrays. At Same Accuracy, the 3-bit MIMHD outperforms other implementations by a large margin as it only needs D=1k to achieve the same accuracy as binary implementations. The 3-bit MIMHD outperforms the state of the art, i.e., SearcHD, by 3.27\u00d7 and 13\u00d7 in Max Accuracy and Same Accuracy setups, respectively.\n\n2) Latency: Fig. 8b shows the speedup results of MIMHD in different bit precisions over the GPU for the tasks in Table I. MIMHD leverages highly parallel computation for encoding and associative search to achieve high speedups over the GPU. The encoding is performed in n crossbar arrays in parallel (Fig. 3a). The single-step MCAM associative search increases speedup by alleviating memory access overheads and searching across all classes in one pass. The difference in speedup of MIMHD in different bit precisions is mainly due to the overhead of the ADCs. For the considered tasks, MIMHD with 1, 2, and 3-bit precisions achieves average improvements of 44\u00d7, 38\u00d7, and 34\u00d7 over the GPU, respectively. Fig. 8d compares the average speedup of AHAM [9], SearcHD [15], and MIMHD with 2 and 3-bit precisions over the GPU. Unlike energy improvement, speedup does not scale linearly with respect to dimensionality, since a significant portion of the computation happens in parallel. However, lower dimensinality reduces some of the peripheral overhead which is reflected in the results. 3-bit MIMHD outperforms the state of the art, i.e., SearcHD, by 2.6\u00d7 and 4.3\u00d7 in Maximum Accuracy and Same Accuracy setups, respectively.\n\n\nD. MIMHD Robustness\n\nThe technological and fabrication issues in highly scaled technology nodes add a significant amount of noise to both memory and computing units [32], [33]. These issues add extra computational error sources, which degrades the quality of inference. One of the main advantages of HDC is its high robustness to noise and failure. In HDC, HVs are random and holographic with i.i.d. components. Each HV stores the information across all its components so that no component is more responsible for storing any piece of information than 1-bit MIMHD   another. This makes HDC robust against computational errors. However, HDC with floating point precision HVs has a much lower robustness to hardware noise compared to HDC with binary precision HVs. In high precision representation, a bitflip due to noise in the exponent, the most significant bits of mantissa, or the sign bit can drastically change weight values, thus changing the prediction results. Models deployed on MIMHD inherit the intrinsic robustness of HDC. Table II reports the quality loss of HDC using different bit precisions (D=4k). Our evaluation indicates that HDC quality loss increases with HV precision. For example, using a 10% random bit flip (noise) in the encoding and associative search modules, 2 and 8-bit HDC experience 4.1% and 12.4% quality loss, respectively. Thus, it is desired to operate in lower precisions. MIMHD enables highly robust HDC inference by operating in low-precisions (1, 2, and 3 bits). HDC robustness also depends on HV dimensionality. Increasing dimensionality from D=4k to D=10k leads to 3.4\u00d7 lower noise sensitivity. However, this boost in the robustness comes at the expense of lower computation efficiency. Note that HDC is significantly less sensitive than the existing learning solutions operating in floating point precision. For example, considering 2% and 10% random noise in model parameters, neural networks exhibit 9.4% and 40.0% quality loss, respectively [11].\n\n\n2-bit MIMHD 3-bit MIMHD\n\n\nM N IS T U IC H A R IS O L E T P A M A P F A C E P E C A N\n\n\nM N IS T U IC H A R IS O L E T P A M A P F A C E P E C A N\n\nV. CONCLUSION We proposed MIMHD, accurate and efficient in-memory implementation of HDC inference. MIMHD uses multi-bit values to represent HV dimensions and exploits FeFET crossbar arrays and FeFET MCAM arrays to perform in-memory encoding and associative search. We also proposed a novel HWART algorithm which iteratively retrains the HDC model to compensate for possible accuracy loss due to the MCAM distance metric. Our evaluation shows that MIMHD provides comparable accuracy to high precision models while enabling higher computation efficiency.\n\nFig. 1 .\n1(a) An overview of HDC classification, (b) HDC encoding module.\n\nFig. 2 .\n2(a) FeFET device schematic[20]; (b) Transfer characteristics of a FeFET programmed to 8 distinct states; (c) Schematic of the FeFET MCAM cell[13]; (d) MCAM distance metric from[13]; (e) MCAM array schematic.\n\nFig. 3 .\n3MIMHD supports all required operations for HDC inference: (a) the encoding module consists of n crossbar array groups with D/64 crossbar arrays of size 64 \u00d7 64; (b) the associative search module consists of MCAM blocks that store k class HVs.\n\nFig. 4 .\n4HWART framework adapts HDC models to work with the MCAM distance metric supported by MIMHD.quantized. The non-quantized model allows for a fine-grained accumulation of information and the quantized model mimics the forward pass of inference. During the following two phases, we teach the model to work with the quantization constraints: (i) feed-forward phase that checks the similarity of an encoded training data point with the quantized model using the MCAM distance metric, and (ii) model update phase that adjusts the non-quantized HDC model for each mispredicted sample.For each encoded training data point H, we check the similarity of H with all class HVs stored in the quantized model to perform the prediction ( \u2022 A ). This similarity search is based on the MCAM distance metric derived from the FeFET MCAM( \u2022 B ). Then, we examine if the model returns the correct label l for H. If the model mispredicts it as label l , the non-quantized model is updated as follows ( \u2022 C \u2022 D ).\n\nFig. 7 .\n7(a) Impact of HV precision on HDC classification accuracy, (b) MIMHD quality loss vs. cosine metric with and without HWART retraining.\n\nFig. 8 .\n8(a) and (b) show the energy improvement and speedup of MIMHD over the GPU for D = 4000; (c) and (d) compare MIMHD with state-of-the-art binary HDC accelerators in maximum accuracy and same accuracy setups in terms of energy improvement and speedup over the GPU.\n\n\narXiv:2106.12029v1 [cs.ET] 22 Jun 2021978-1-6654-3922-0/21/$31.00 \u00a92021 IEEE \n\nBase HV (B 1 ) \n\n\n\nTABLE II HDC\nIIQUALITY LOSS USING DIFFERENT HV PRECISIONS.Hardware Error \n1% \n2% \n5% \n10% \n15% \n\nMIMHD \n\n1-bit \n0.0% 0.0% 0.9% \n3.1% \n5.2% \n2-bit \n0.0% 0.3% 1.2% \n4.1% \n6.7% \n3-bit \n0.1% 0.3% 2.0% \n5.9% \n9.3% \n8-bit \n1.2% 3.7% 5.5% 12.4% 18.7% \n\n\nACKNOWLEDGMENT This work was supported in part by Semiconductor Research Corporation (SRC) Task No. 2988.001, ASCENT, Department of the Navy, Office of Naval Research, grant #N00014-21-1-2225, and a generous gift from Cisco.\nHyperdimensional computing. Cognn Comput. P Kanerva, P. Kanerva. Hyperdimensional computing. Cognn Comput., 2009.\n\nLearning sensorimotor control with neuromorphic sensors. A Mitrokhin, Science Robotics. 430A. Mitrokhin et al. Learning sensorimotor control with neuromorphic sensors. Science Robotics, 4(30), 2019.\n\nClassification and recall with binary hyperdimensional computing. D Kleyko, IEEE Trans Neural Netw Learn Syst. 99D. Kleyko, et al. Classification and recall with binary hyperdimensional computing. IEEE Trans Neural Netw Learn Syst, (99):1-19, 2018.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, IEEE Trans. Neural Netw. Learn. Syst. 99PPO. Rasanen et al. Sequence prediction with sparse distributed hyperdi- mensional coding applied to the analysis of mobile phone use patterns. IEEE Trans. Neural Netw. Learn. Syst., PP(99):1-12, 2015.\n\nEfficient biosignal processing using hyperdimensional computing. A Rahimi, Proceedings of the IEEE. the IEEE107A. Rahimi, et al. Efficient biosignal processing using hyperdimensional computing. Proceedings of the IEEE, 107(1):123-143, 2018.\n\nA framework for collaborative learning in secure highdimensional space. M Imani, IEEECLOUDM. Imani et al. A framework for collaborative learning in secure high- dimensional space. In CLOUD, pages 435-446. IEEE, 2019.\n\nRevisiting hyperdimensional learning for fpga and lowpower architectures. M Imani, HPCA. 2021M. Imani et al. Revisiting hyperdimensional learning for fpga and low- power architectures. In HPCA, 2021.\n\nHyperdimensional computing with 3d vrram in-memory kernels. H Li, IEDM. IEEE. H. Li et al. Hyperdimensional computing with 3d vrram in-memory kernels. In IEDM. IEEE, 2016.\n\nExploring hyperdimensional associative memory. M Imani, HPCA. IEEEM. Imani et al. Exploring hyperdimensional associative memory. In HPCA, pages 445-456. IEEE, 2017.\n\nQuanthd: A quantization framework for hyperdimensional computing. M Imani, TCADM. Imani et al. Quanthd: A quantization framework for hyperdimen- sional computing. TCAD, 2019.\n\nOnlinehd: Robust, efficient, and single-pass online learning using hyperdimensional system. A Hernandez-Cano, DATE. 2021A. Hernandez-Cano et al. Onlinehd: Robust, efficient, and single-pass online learning using hyperdimensional system. In DATE, 2021.\n\nNanoconnectomic upper bound on the variability of synaptic plasticity. Elife, 4:e10778. T M BartolJr, T. M. Bartol Jr, et al. Nanoconnectomic upper bound on the variability of synaptic plasticity. Elife, 4:e10778, 2015.\n\nIn-memory nearest neighbor search with fefet multi-bit content-addressable memories. A Kazemi, DATE. IEEE. A. Kazemi, et al. In-memory nearest neighbor search with fefet multi-bit content-addressable memories. In DATE. IEEE, 2021.\n\nA ferroelectric field effect transistor based synaptic weight cell. M Jerry, Journal of Physics D: Applied Physics. M. Jerry, et al. A ferroelectric field effect transistor based synaptic weight cell. Journal of Physics D: Applied Physics, 2018.\n\nSearchd: A memory-centric hyperdimensional computing with stochastic training. M Imani, TCADM. Imani et al. Searchd: A memory-centric hyperdimensional computing with stochastic training. TCAD, 2019.\n\nSynergiclearning: Neural network-based feature extraction for highly-accurate hyperdimensional learning. M Nazemi, arXivM. Nazemi, et al. Synergiclearning: Neural network-based feature extraction for highly-accurate hyperdimensional learning. arXiv, 2020.\n\nCognitive correlative encoding for genome sequence matching in hyperdimensional system. P , DAC. 2021P. Poduval et al. Cognitive correlative encoding for genome sequence matching in hyperdimensional system. In DAC, 2021.\n\nRobust and efficient regression in hyper-dimensional learning system. A H\u00e9rnandez-Cano, DAC. 2021A. H\u00e9rnandez-Cano et al. Reghd: Robust and efficient regression in hyper-dimensional learning system. In DAC, 2021.\n\nIn-memory hyperdimensional computing. G Karunaratne, Nature Electronics. G. Karunaratne et al. In-memory hyperdimensional computing. Nature Electronics, pages 1-11, 2020.\n\nA circuit compatible accurate compact model for ferroelectric-fets. K Ni, IEEE Symposium on VLSI Technology. K. Ni, et al. A circuit compatible accurate compact model for ferroelectric-fets. In IEEE Symposium on VLSI Technology, 2018.\n\nA hybrid femfet-cmos analog synapse circuit for neural network training and inference. A Kazemi, ISCAS. A. Kazemi, et al. A hybrid femfet-cmos analog synapse circuit for neural network training and inference. In ISCAS, pages 1-5, 2020.\n\nFerroelectric ternary content-addressable memory for oneshot learning. K Ni, Nature Electronics. 211K. Ni et al. Ferroelectric ternary content-addressable memory for one- shot learning. Nature Electronics, 2(11):521-529, 2019.\n\nTensorflow: Large-scale machine learning on heterogeneous distributed systems. M Abadi, arXiv:1603.04467arXiv preprintM. Abadi et al. Tensorflow: Large-scale machine learning on heteroge- neous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n\nDnn+ neurosim: An end-to-end benchmarking framework for compute-in-memory accelerators with versatile device technologies. X Peng, IEDM. IEEEX. Peng, et al. Dnn+ neurosim: An end-to-end benchmarking framework for compute-in-memory accelerators with versatile device technologies. In IEDM, pages 32-5. IEEE, 2019.\n\nGradient-based learning applied to document recognition. Y Lecun, Proceedings of the IEEE. the IEEE86Y. LeCun et al. Gradient-based learning applied to document recogni- tion. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nMulti-column deep neural networks for image classification. D Ciregan, CVPR. IEEED. Ciregan et al. Multi-column deep neural networks for image classification. In CVPR, pages 3642-3649. IEEE, 2012.\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, AAL. SpringerD. Anguita et al. Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. In AAL, pages 216-223. Springer, 2012.\n\nUCI machine learning repository. D Dua, D. Dua et al. UCI machine learning repository, 2017.\n\nIntroducing a new benchmarked dataset for activity monitoring. A Reiss, ISWC. IEEEA. Reiss et al. Introducing a new benchmarked dataset for activity monitoring. In ISWC, pages 108-109. IEEE, 2012.\n\nPruning training sets for learning of object categories. A Angelova, CVPR. IEEE. A. Angelova et al. Pruning training sets for learning of object categories. In CVPR. IEEE, 2005.\n\n. Pecan, Dataport, Pecan street dataport. https://dataport.cloud/.\n\nReliability-aware runtime power management for many-core systems in the dark silicon era. A Rahmani, TVLSI. 252A. Rahmani et al. Reliability-aware runtime power management for many-core systems in the dark silicon era. TVLSI, 25(2):427-440, 2016.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram. T Wu, ISSCC. IEEE. T. Wu et al. Brain-inspired computing exploiting carbon nanotube fets and resistive ram. In ISSCC. IEEE, 2018.\n", "annotations": {"author": "[{\"end\":124,\"start\":96},{\"end\":148,\"start\":125},{\"end\":198,\"start\":149},{\"end\":215,\"start\":199},{\"end\":218,\"start\":216},{\"end\":229,\"start\":219},{\"end\":277,\"start\":230},{\"end\":305,\"start\":278}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":102},{\"end\":147,\"start\":140},{\"end\":160,\"start\":157},{\"end\":214,\"start\":207},{\"end\":228,\"start\":226},{\"end\":242,\"start\":237}]", "author_first_name": "[{\"end\":101,\"start\":96},{\"end\":133,\"start\":125},{\"end\":139,\"start\":134},{\"end\":156,\"start\":149},{\"end\":206,\"start\":199},{\"end\":217,\"start\":216},{\"end\":225,\"start\":219},{\"end\":236,\"start\":230}]", "author_affiliation": "[{\"end\":197,\"start\":162},{\"end\":276,\"start\":244},{\"end\":304,\"start\":279}]", "title": "[{\"end\":93,\"start\":1},{\"end\":398,\"start\":306}]", "venue": null, "abstract": "[{\"end\":1820,\"start\":400}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2056,\"start\":2053},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2215,\"start\":2212},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2453,\"start\":2450},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2483,\"start\":2480},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2521,\"start\":2518},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2651,\"start\":2648},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2656,\"start\":2653},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2854,\"start\":2851},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2859,\"start\":2856},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3275,\"start\":3271},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3281,\"start\":3277},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3492,\"start\":3488},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4256,\"start\":4252},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4360,\"start\":4356},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4859,\"start\":4855},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4903,\"start\":4899},{\"end\":5340,\"start\":5333},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5568,\"start\":5564},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5808,\"start\":5805},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5913,\"start\":5910},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5934,\"start\":5930},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5962,\"start\":5958},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5983,\"start\":5979},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7093,\"start\":7090},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8511,\"start\":8507},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8517,\"start\":8513},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8543,\"start\":8540},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8548,\"start\":8545},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8554,\"start\":8550},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9190,\"start\":9186},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9449,\"start\":9445},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9591,\"start\":9587},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9687,\"start\":9683},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9693,\"start\":9689},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9707,\"start\":9703},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10771,\"start\":10767},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13879,\"start\":13875},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14346,\"start\":14343},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14376,\"start\":14372},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14611,\"start\":14607},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16644,\"start\":16640},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16804,\"start\":16800},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16855,\"start\":16851},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16956,\"start\":16952},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20979,\"start\":20976},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21067,\"start\":21063},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22023,\"start\":22020},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22037,\"start\":22033},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23464,\"start\":23461},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23478,\"start\":23474},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24104,\"start\":24100},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24110,\"start\":24106},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25925,\"start\":25921},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26745,\"start\":26741},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26860,\"start\":26856},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26895,\"start\":26891}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26703,\"start\":26629},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26922,\"start\":26704},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27176,\"start\":26923},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28177,\"start\":27177},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28323,\"start\":28178},{\"attributes\":{\"id\":\"fig_8\"},\"end\":28596,\"start\":28324},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28695,\"start\":28597},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28943,\"start\":28696}]", "paragraph": "[{\"end\":2522,\"start\":1839},{\"end\":3282,\"start\":2524},{\"end\":4053,\"start\":3284},{\"end\":4461,\"start\":4055},{\"end\":5569,\"start\":4463},{\"end\":6064,\"start\":5644},{\"end\":6234,\"start\":6066},{\"end\":7094,\"start\":6236},{\"end\":7353,\"start\":7122},{\"end\":8398,\"start\":7355},{\"end\":8994,\"start\":8422},{\"end\":9592,\"start\":9033},{\"end\":10958,\"start\":9594},{\"end\":11579,\"start\":10982},{\"end\":13497,\"start\":11595},{\"end\":14534,\"start\":13523},{\"end\":15007,\"start\":14536},{\"end\":15510,\"start\":15048},{\"end\":16052,\"start\":15576},{\"end\":16346,\"start\":16054},{\"end\":17547,\"start\":16389},{\"end\":17797,\"start\":17569},{\"end\":19220,\"start\":17799},{\"end\":19794,\"start\":19222},{\"end\":20511,\"start\":19796},{\"end\":21385,\"start\":20535},{\"end\":22711,\"start\":21392},{\"end\":23932,\"start\":22713},{\"end\":25926,\"start\":23956},{\"end\":26628,\"start\":26076}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7121,\"start\":7095},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15575,\"start\":15511}]", "table_ref": "[{\"end\":17404,\"start\":17396},{\"end\":21528,\"start\":21519},{\"end\":22833,\"start\":22826},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24977,\"start\":24969}]", "section_header": "[{\"end\":1837,\"start\":1822},{\"end\":5603,\"start\":5572},{\"end\":5642,\"start\":5606},{\"end\":8420,\"start\":8401},{\"end\":9031,\"start\":8997},{\"end\":10980,\"start\":10961},{\"end\":11593,\"start\":11582},{\"end\":13521,\"start\":13500},{\"end\":15046,\"start\":15010},{\"end\":16363,\"start\":16349},{\"end\":16387,\"start\":16366},{\"end\":17567,\"start\":17550},{\"end\":20533,\"start\":20514},{\"end\":21390,\"start\":21388},{\"end\":23954,\"start\":23935},{\"end\":25952,\"start\":25929},{\"end\":26013,\"start\":25955},{\"end\":26074,\"start\":26016},{\"end\":26638,\"start\":26630},{\"end\":26713,\"start\":26705},{\"end\":26932,\"start\":26924},{\"end\":27186,\"start\":27178},{\"end\":28187,\"start\":28179},{\"end\":28333,\"start\":28325},{\"end\":28709,\"start\":28697}]", "table": "[{\"end\":28695,\"start\":28637},{\"end\":28943,\"start\":28755}]", "figure_caption": "[{\"end\":26703,\"start\":26640},{\"end\":26922,\"start\":26715},{\"end\":27176,\"start\":26934},{\"end\":28177,\"start\":27188},{\"end\":28323,\"start\":28189},{\"end\":28596,\"start\":28335},{\"end\":28637,\"start\":28599},{\"end\":28755,\"start\":28712}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6233,\"start\":6225},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7012,\"start\":7005},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7678,\"start\":7671},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8255,\"start\":8248},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9200,\"start\":9191},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9458,\"start\":9451},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9763,\"start\":9756},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10001,\"start\":9994},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10451,\"start\":10444},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10505,\"start\":10496},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11515,\"start\":11509},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12485,\"start\":12478},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14069,\"start\":14060},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14567,\"start\":14559},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15411,\"start\":15405},{\"end\":16345,\"start\":16337},{\"end\":17825,\"start\":17818},{\"end\":18390,\"start\":18383},{\"end\":19175,\"start\":19169},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19255,\"start\":19248},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19823,\"start\":19816},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20290,\"start\":20283},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":21407,\"start\":21400},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":21971,\"start\":21964},{\"end\":22250,\"start\":22241},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":22732,\"start\":22725},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23022,\"start\":23013},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":23423,\"start\":23416}]", "bib_author_first_name": "[{\"end\":29212,\"start\":29211},{\"end\":29342,\"start\":29341},{\"end\":29551,\"start\":29550},{\"end\":29857,\"start\":29856},{\"end\":30176,\"start\":30175},{\"end\":30425,\"start\":30424},{\"end\":30645,\"start\":30644},{\"end\":30832,\"start\":30831},{\"end\":30992,\"start\":30991},{\"end\":31177,\"start\":31176},{\"end\":31379,\"start\":31378},{\"end\":31628,\"start\":31627},{\"end\":31630,\"start\":31629},{\"end\":31846,\"start\":31845},{\"end\":32061,\"start\":32060},{\"end\":32319,\"start\":32318},{\"end\":32545,\"start\":32544},{\"end\":32785,\"start\":32784},{\"end\":32989,\"start\":32988},{\"end\":33171,\"start\":33170},{\"end\":33373,\"start\":33372},{\"end\":33628,\"start\":33627},{\"end\":33849,\"start\":33848},{\"end\":34085,\"start\":34084},{\"end\":34384,\"start\":34383},{\"end\":34632,\"start\":34631},{\"end\":34861,\"start\":34860},{\"end\":35102,\"start\":35101},{\"end\":35320,\"start\":35319},{\"end\":35444,\"start\":35443},{\"end\":35636,\"start\":35635},{\"end\":35916,\"start\":35915},{\"end\":36150,\"start\":36149}]", "bib_author_last_name": "[{\"end\":29220,\"start\":29213},{\"end\":29352,\"start\":29343},{\"end\":29558,\"start\":29552},{\"end\":29865,\"start\":29858},{\"end\":30183,\"start\":30177},{\"end\":30431,\"start\":30426},{\"end\":30651,\"start\":30646},{\"end\":30835,\"start\":30833},{\"end\":30998,\"start\":30993},{\"end\":31183,\"start\":31178},{\"end\":31394,\"start\":31380},{\"end\":31637,\"start\":31631},{\"end\":31853,\"start\":31847},{\"end\":32067,\"start\":32062},{\"end\":32325,\"start\":32320},{\"end\":32552,\"start\":32546},{\"end\":33004,\"start\":32990},{\"end\":33183,\"start\":33172},{\"end\":33376,\"start\":33374},{\"end\":33635,\"start\":33629},{\"end\":33852,\"start\":33850},{\"end\":34091,\"start\":34086},{\"end\":34389,\"start\":34385},{\"end\":34638,\"start\":34633},{\"end\":34869,\"start\":34862},{\"end\":35110,\"start\":35103},{\"end\":35324,\"start\":35321},{\"end\":35450,\"start\":35445},{\"end\":35645,\"start\":35637},{\"end\":35764,\"start\":35759},{\"end\":35774,\"start\":35766},{\"end\":35924,\"start\":35917},{\"end\":36153,\"start\":36151}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":29282,\"start\":29169},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":182118830},\"end\":29482,\"start\":29284},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":51614496},\"end\":29732,\"start\":29484},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15258913},\"end\":30108,\"start\":29734},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57365377},\"end\":30350,\"start\":30110},{\"attributes\":{\"id\":\"b5\"},\"end\":30568,\"start\":30352},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":233376633},\"end\":30769,\"start\":30570},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":25209638},\"end\":30942,\"start\":30771},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1677864},\"end\":31108,\"start\":30944},{\"attributes\":{\"id\":\"b9\"},\"end\":31284,\"start\":31110},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":236151316},\"end\":31537,\"start\":31286},{\"attributes\":{\"id\":\"b11\"},\"end\":31758,\"start\":31539},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":226964775},\"end\":31990,\"start\":31760},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":117370069},\"end\":32237,\"start\":31992},{\"attributes\":{\"id\":\"b14\"},\"end\":32437,\"start\":32239},{\"attributes\":{\"id\":\"b15\"},\"end\":32694,\"start\":32439},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":243896150},\"end\":32916,\"start\":32696},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":243878454},\"end\":33130,\"start\":32918},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":174797921},\"end\":33302,\"start\":33132},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53093975},\"end\":33538,\"start\":33304},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":214774989},\"end\":33775,\"start\":33540},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":209063048},\"end\":34003,\"start\":33777},{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b22\"},\"end\":34258,\"start\":34005},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":211206012},\"end\":34572,\"start\":34260},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14542261},\"end\":34798,\"start\":34574},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2161592},\"end\":34996,\"start\":34800},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13178535},\"end\":35284,\"start\":34998},{\"attributes\":{\"id\":\"b27\"},\"end\":35378,\"start\":35286},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10337279},\"end\":35576,\"start\":35380},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14019086},\"end\":35755,\"start\":35578},{\"attributes\":{\"id\":\"b30\"},\"end\":35823,\"start\":35757},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6237770},\"end\":36071,\"start\":35825},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3869844},\"end\":36278,\"start\":36073}]", "bib_title": "[{\"end\":29339,\"start\":29284},{\"end\":29548,\"start\":29484},{\"end\":29854,\"start\":29734},{\"end\":30173,\"start\":30110},{\"end\":30642,\"start\":30570},{\"end\":30829,\"start\":30771},{\"end\":30989,\"start\":30944},{\"end\":31376,\"start\":31286},{\"end\":31843,\"start\":31760},{\"end\":32058,\"start\":31992},{\"end\":32782,\"start\":32696},{\"end\":32986,\"start\":32918},{\"end\":33168,\"start\":33132},{\"end\":33370,\"start\":33304},{\"end\":33625,\"start\":33540},{\"end\":33846,\"start\":33777},{\"end\":34381,\"start\":34260},{\"end\":34629,\"start\":34574},{\"end\":34858,\"start\":34800},{\"end\":35099,\"start\":34998},{\"end\":35441,\"start\":35380},{\"end\":35633,\"start\":35578},{\"end\":35913,\"start\":35825},{\"end\":36147,\"start\":36073}]", "bib_author": "[{\"end\":29222,\"start\":29211},{\"end\":29354,\"start\":29341},{\"end\":29560,\"start\":29550},{\"end\":29867,\"start\":29856},{\"end\":30185,\"start\":30175},{\"end\":30433,\"start\":30424},{\"end\":30653,\"start\":30644},{\"end\":30837,\"start\":30831},{\"end\":31000,\"start\":30991},{\"end\":31185,\"start\":31176},{\"end\":31396,\"start\":31378},{\"end\":31641,\"start\":31627},{\"end\":31855,\"start\":31845},{\"end\":32069,\"start\":32060},{\"end\":32327,\"start\":32318},{\"end\":32554,\"start\":32544},{\"end\":32788,\"start\":32784},{\"end\":33006,\"start\":32988},{\"end\":33185,\"start\":33170},{\"end\":33378,\"start\":33372},{\"end\":33637,\"start\":33627},{\"end\":33854,\"start\":33848},{\"end\":34093,\"start\":34084},{\"end\":34391,\"start\":34383},{\"end\":34640,\"start\":34631},{\"end\":34871,\"start\":34860},{\"end\":35112,\"start\":35101},{\"end\":35326,\"start\":35319},{\"end\":35452,\"start\":35443},{\"end\":35647,\"start\":35635},{\"end\":35766,\"start\":35759},{\"end\":35776,\"start\":35766},{\"end\":35926,\"start\":35915},{\"end\":36155,\"start\":36149}]", "bib_venue": "[{\"end\":30218,\"start\":30210},{\"end\":34673,\"start\":34665},{\"end\":29209,\"start\":29169},{\"end\":29370,\"start\":29354},{\"end\":29593,\"start\":29560},{\"end\":29903,\"start\":29867},{\"end\":30208,\"start\":30185},{\"end\":30422,\"start\":30352},{\"end\":30657,\"start\":30653},{\"end\":30847,\"start\":30837},{\"end\":31004,\"start\":31000},{\"end\":31174,\"start\":31110},{\"end\":31400,\"start\":31396},{\"end\":31625,\"start\":31539},{\"end\":31865,\"start\":31855},{\"end\":32106,\"start\":32069},{\"end\":32316,\"start\":32239},{\"end\":32542,\"start\":32439},{\"end\":32791,\"start\":32788},{\"end\":33009,\"start\":33006},{\"end\":33203,\"start\":33185},{\"end\":33411,\"start\":33378},{\"end\":33642,\"start\":33637},{\"end\":33872,\"start\":33854},{\"end\":34082,\"start\":34005},{\"end\":34395,\"start\":34391},{\"end\":34663,\"start\":34640},{\"end\":34875,\"start\":34871},{\"end\":35115,\"start\":35112},{\"end\":35317,\"start\":35286},{\"end\":35456,\"start\":35452},{\"end\":35657,\"start\":35647},{\"end\":35931,\"start\":35926},{\"end\":36166,\"start\":36155}]"}}}, "year": 2023, "month": 12, "day": 17}