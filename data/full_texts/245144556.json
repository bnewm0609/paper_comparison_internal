{"id": 245144556, "updated": "2023-10-05 18:45:12.23", "metadata": {"title": "Large Dual Encoders Are Generalizable Retrievers", "authors": "[{\"first\":\"Jianmo\",\"last\":\"Ni\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Qu\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Zhuyun\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Gustavo\",\"last\":\"Hernandez Abrego\",\"middle\":[]},{\"first\":\"Ji\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Vincent\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Luan\",\"middle\":[]},{\"first\":\"Keith\",\"last\":\"Hall\",\"middle\":[]},{\"first\":\"Ming-Wei\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Yinfei\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.07899", "mag": null, "acl": "2022.emnlp-main.669", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/Ni0LDAMZLHCY22", "doi": "10.18653/v1/2022.emnlp-main.669"}}, "content": {"source": {"pdf_hash": "e647538ebfbb4f80a94ce2951220c60bb712652c", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.emnlp-main.669.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0f6312b6f99cd22e0e75ee3b7f4390420dc290e8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e647538ebfbb4f80a94ce2951220c60bb712652c.txt", "contents": "\nLarge Dual Encoders Are Generalizable Retrievers\nDecember 7-11, 2022\n\nJianmo Ni \nChen Qu \nJing Lu \nZhuyun Dai \nGustavo Hern\u00e1ndez \u00c1brego \nVincent Y Zhao \nYi Luan \nKeith B Hall \nMing-Wei Chang \nYinfei Yang \n\u2663 \nGoogle Research \n\u2663 Apple \nLarge Dual Encoders Are Generalizable Retrievers\n\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\nthe 2022 Conference on Empirical Methods in Natural Language ProcessingDecember 7-11, 2022\nIt has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dotproduct between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for outof-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset (Thakur et al., 2021) significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.\n\nIntroduction\n\nTypical neural retrieval models follow a dual encoder paradigm (Gillick et al., 2018;Karpukhin et al., 2020). In this setup, queries and documents are encoded separately into a shared fixed-dimensional embedding space, where relevant queries and documents are represented in each other's proximity. Then, approximated nearest neighbor search (Vanderkam et al., 2013;Johnson et al., 2021) is applied to efficiently retrieve relevant documents given an encoded input query. * Correspondence to jianmon@google.com \u2020 Work done while at Google Research While dual encoders are popular neural retrievers, the expressiveness of the model is limited by a bottleneck layer consisting of only a simple dotproduct between query and passage embeddings. Lu et al. (2021); Khattab and Zaharia (2020) argued that the dot-product (or cosine similarity) between the embeddings might not be powerful enough to capture the semantic relevance. Similarly, Thakur et al. (2021) suggested that dual encoder models have \"issues for out-of-distribution data\" and models with more interactions between queries and documents have better generalization ability.\n\nIn this paper, we challenge this belief by scaling up the dual encoder model size while keeping the bottleneck as a single dot-product with a fixed size. Note that scaling up a dual encoder is different from scaling up pretrained language models, such as BERT  and T5 (Raffel et al., 2020), because of the presence of the bottleneck layer. While increasing the model size can greatly increase model capacity, for dual encoders with a fixed bottleneck embedding size, the interactions between queries and documents are still limited by a simple dot-product.\n\nTo test this hypothesis, we take advantage of the T5 architecture and checkpoints to build encoders of up to 5 billion parameters while keeping the  Figure 2: Architecture of Generalizable T5-based dense Retrievers. The research question we ask is: can scaling up dual encoder model size improve the retrieval performance while keeping the bottleneck layers as a single dot-product with a fixed size? Only the encoder is taken from the pre-trained T5 models, and the two towers of the dual encoder share parameters.\n\nbottleneck embedding dimension of 768 in all configurations, as illustrated in Figure 2. Following , we build dual encoders by taking the encoder part of T5. To effectively leverage the power of large models, we collect two billion web question-answer pairs as generic pre-training data. By combining pre-training with generic data and fine-tuning with MS Marco (Nguyen et al., 2016), we are able to train large-scale dual encoder retrieval models. We call the resulting models Generalizable T5-based dense Retrievers (GTR). We assess the zero-shot performance of GTR on the BEIR benchmark (Thakur et al., 2021), which has 18 information retrieval tasks across 9 domains. We showed that scaling up leads to better generalization despite the fixed single-dot product bottleneck. Second, pre-training on community questionanswer pairs and fine-tuning on human curated data are both important to fully utilize the power of the scaled up model. In addition, with scaling and pre-training, we found GTR to be highly data efficient in terms of human annotated queries, as it only needs to use 10% of MS Marco to match the overall out-of-domain performance.\n\n\nBackground\n\n\nDual Encoder and dense retrieval\n\nClassic retrieval models, such as BM25 (Robertson and Zaragoza, 2009), rely on lexical overlap: term frequency, inverse document frequency and document length. To allow semantic matching between queries and documents, dense retrieval models, such as dual encoders (Yih et al., 2011;Gillick et al., 2019;Karpukhin et al., 2020), are introduced, where both queries and documents are embedded into low-dimensional dense representations.\n\nA critical challenge for dual encoders is that the performance can be bounded by the dot-product similarity function. As such, there is growing interest in applying lightweight interaction layers to replace the single dot-product function. Luan et al. (2020) propose a multi-vector encoding model to represent a document as a set of vectors and calculate the relevance scores as the maximum inner product over this set. ColBERT (Khattab and Zaharia, 2020) learns embeddings for each token and then uses a \"MaxSim\" operation to select the best candidate. While these models can achieve significant improvement, dual encoder is still the most popular in practice due to its simpleness and the ability to scale. In this paper, we take a step back and show that performance of single dot-product based methods can be improved significantly.\n\n\nBEIR generalization task\n\nWe use BEIR, a heterogenous benchmark, for zeroshot retrieval evaluation. BEIR has 18 information retrieval datasets 1 across 9 domains, including Bio-Medical, Finance, News, Twitter, Wikipedia, StackExchange, Quora, Scientific, and Misc. The majority of the datasets have binary query relevance labels. The other datasets have 3-level or 5-level relevance judgements. We refer readers to BEIR (Thakur et al., 2021) for more details.\n\n3 Generalizable T5 Retriever\n\n\nT5 dual encoder\n\nWe adopt the dual encoder framework and follow prior work (Xiong et al., 2020;Hofst\u00e4tter et al.,  2021) to initialize from pre-trained language models. We choose the pre-trained T5 model family as our backbone encoder because it provides offthe-shelf pre-trained models with capacities ranging from millions to billions of parameters (Raffel et al., 2020;Xue et al., 2020Xue et al., , 2021. We illustrate the architectures of our models in Figure 2.\n\nLet paired examples D = {(q i , p + i )} be the training set, where q i is a query and p + i is a ground-truth relevant passage. Following , we encode q i and p + i into embeddings by feeding them to the T5 encoder and taking the mean pooling of the encoder output. In all experiments, we fix the size of the output embeddings to 768. We train the model using an in-batch sampled softmax loss (Henderson et al., 2017):\nL = e sim(q i ,p + i )/\u03c4 j\u2208B e sim(q i ,p + j )/\u03c4 ,(1)\nwhere sim is cosine similarity, B is a mini-batch of examples, and \u03c4 is the softmax temperature. In addition to in-batch negatives, we also support having additional negatives p \u2212 j :\nL = e sim(q i ,p + i )/\u03c4 j\u2208B e sim(q i ,p + j )/\u03c4 + e sim(q i ,p \u2212 j )/\u03c4 .(2)\nWe apply this loss function in a bi-directional way  for matchings of both question-to-document and document-to-question.\n\n\nMulti-stage training\n\nAs shown in Figure 3, we use a multi-stage training approach to achieve generalizable retrieval models.\n\nThe training process includes a pre-training stage on a web-mined corpus and a fine-tuning stage on search datasets. Although the web-mined corpus is not annotated, it contains a large amount of semistructured data pairs (e.g., question-answer pairs) and can provide rich semantic relevance information to the model in pre-training. On the other hand, the search datasets are curated and well-annotated, and thus, can benefit the fine-tuning stage.\n\nSpecifically, for dual encoder pre-training, we initialize the dual encoders from the T5 models and train on question-answer pairs collected from the Web. Recently, Sentence-T5  explored different ways to extract strong text embeddings and achieved remarkable performance on SentEval and Sentence Textual Similarity tasks. We follow their setting to encode queries and passages via mean pooling from the T5 encoders and focus on the dense retrieval tasks.\n\nFor fine-tuning, our aim is to adapt the model to retrieval using a high quality search corpus so the model can learn to better match generic queries to documents. In this paper, we consider two datasets for fine-tuning: MS Marco (Nguyen et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019). Community QA. To leverage the power of large scale models, we collect input-response pairs and question-answer pairs from online forums and QA websites, such as Reddit and StackOverflow. This results in 2 billion question-answer pairs that we use to pre-train the dual encoder.\n\nMS Marco. The MS Marco dataset (Nguyen et al., 2016) includes 532K query and document pairs, which we use as search data for fine-tuning. This dataset is sampled from Bing search logs and covers a broad range of domains and concepts.\n\nNatural Questions. In the fine-tuning stage, we also consider the Natural Questions dataset (Kwiatkowski et al., 2019) , which has been widely used in related work (Karpukhin et al., 2020;Xiong et al., 2020). This dataset consists of 130K query and passage pairs that are human-annotated.\n\n\nConfigurations\n\nWe implement GTR models in JAX 2 and train them on Cloud TPU-V3. We consider different sizes of the T5 transformer (Vaswani et al., 2017)   number of parameters are listed in Table 1. Note that we only use the encoder of the T5 models, and thus, the number of parameters are less than half of that of the full model. We take the off-the-shelf checkpoints as the initial parameters and use the same sentencepiece vocabulary model. 3 During pre-training and fine-tuning, we set the batch size to 2048 and the softmax temperature \u03c4 to 0.01. We use Adafactor optimizer (Shazeer and Stern, 2018) and set the initial learning rate to 1e-3 with a linear decay. We train the model for 800K steps for pre-training and 20K steps for fine-tuning.\n\nFor fine-tuning, we use the hard negatives from RocketQA (Qu et al., 2021) with MS Marco and the hard negatives from Lu et al. (2021) with NQ. These hard negatives were proven to lead to better retrieval performance. By default, we use the complete MS Marco or NQ datasets for fine-tuning.\n\nWhen evaluating on the BEIR benchmark, we use sequences of 64 tokens for questions and 512 for documents in all datasets but Trec-News, Robust-04, and ArguAna. In particular, we set the document length to 768 for Trec-News and Robust-04 while setting the question length to 512 for Ar-guAna, in accordance with the average query and document lengths in these datasets.\n\nOur code and models are released at https://github.com/google-research/t5x_ retrieval#released-model-checkpoints.\n\n\nModels for comparison\n\nWe consider various baselines, including sparse retrieval models: BM25, DocT5Query, and dense retrieval models: DPR, ANCE, TAS-B, and GenQ (Thakur et al., 2021). We conduct experiments on four different sizes of our GTR models (GTR-3 https://github.com/google-research/ text-to-text-transfer-transformer# released-model-checkpoints  GTR-Base outperforms BM25 on 9 tasks with larger GTR models continuing improving on these 9 tasks. GTR-XXL catches-up/surpasses BM25 on another 5 tasks and only under-performs on the remaining 5.\n\nBase, GTR-Large, GTR-XL, and GTR-XXL). We also consider three different settings for GTR to investigate the effect of scaling up for different training stages:\n\n\u2022 GTR: the full GTR models that conduct both pre-training and fine-tuning. \u2022 GTR-FT: only fine-tuned on MS Marco without pre-training. \u2022 GTR-PT: only pre-trained on CommunityQA without fine-tuning. We evaluate our models on BEIR (Thakur et al., 2021) as discussed in Section 2.2. We consider two retrieval metrics: NDCG@10 and Recall@100 following BEIR. Due to space limitations, we report the Recall@100 results in Appendix A.\n\n\nEvaluation Results\n\nWe present three groups of experiments to study a) the in-domain performance on MS Marco, b) the out-of-domain generalization performance on BEIR, and c) the data efficiency.\n\n\nResults on MS Marco\n\nAs shown in Table 3, with scaling up, the in-domain NDCG@10 scores on MS Marco achieve consistent improvement. We observe a similar effect on other evaluation metrics, including MRR@10 and Recall@1000, and reported the numbers in Table 7 of Appendix A. This shows that increasing model capacity leads to better in-domain performance.  \n\n\nResults on BEIR generalization tasks\n\nAlso as shown in Table 3, we observe a clear gain on out-of-domain (OOD) performance in terms of NDCG@10 when the model size increases. The GTR-Large model already outperforms the previous best dense retrieval model TAS-B, as well as the best sparse model DocT5Query. Scaling up to GTR-XXL leads to another jump in retrieval performance. On average, the scaling up process demonstrates an encouraging ascending trend that eventually make GTR outperform all baseline methods on all evaluation metrics. This confirms that scaling up is a valid path towards generalizability. Previously, dual encoders failed to match the performance of BM25 for tasks that require better lexical matching capabilities. Thus, we would like to investigate what kind of tasks can get improved by scaling up the model size. Figure 4 presents a detailed comparison of all sizes of GTR models against the BM25 baseline.\n\nFor tasks like NQ, where dual encoders have been previously shown to be more effective than BM25, increasing the model size continues to advance the performance of dual encoders. This suggests scaling up can further boost the head start of dense models over sparse models on these datasets.\n\nFor tasks like BioASQ and NFCorpus, where dual encoders previously struggled to match the performance of BM25 for inherent reasons, we discover scaling up consistently improves the retrieval performance. In particular, for NFCorpus, our Base  model under-performs BM25 but the XL model outperforms BM25 by 5.5% (0.343 vs. 0.325). This exciting finding verifies our assumption that scaling up can further exploit the powerful semantic matching capabilities of the dual encoder models and enable them to ultimately outperform BM25.\n\n\nData efficiency for large retrievers\n\nTo better understand the data efficiency for large dual encoders, we train GTR models using different proportions of MS Marco during fine-tuning. In particular, we sample a subset of the MS Marco training data by keeping only 10% of the training queries, as well as their relevant (positive) passages and irrelevant (hard negative) passages. As shown in Table 4, using 10% of the training data reduces the in-domain performance of GTR on MS Marco, as expected. For OOD performance, however, we find some surprising results. We see  that the Large GTR-FT (fine-tuning only) model already shows comparable performance even trained with only 10% of the data. Moreover, the full GTR models trained with less data manage to achieve comparable or even better OOD performance than those fine-tuned on the complete MS Marco dataset. This is strong evidence that using 10% of the MS Marco dataset is sufficient to conduct fine-tuning for the full GTR models. This encouraging observation suggests that GTR models enjoy the benefit of data efficiency and can achieve domain adaptation with less fine-tuning data.\n\n\nAblation Study and Analysis\n\nIn this section we present ablations and analysis to further illustrate the effects of scaling up, the impact of fine-tuning and pre-training, and the GTR model's behavior.\n\n\nScaling up in different training stages\n\nThe first ablation study aims to investigate how scaling up impacts dual encoder pre-training and fine-tuning. Results are presented in Table 5. For fine-tuning only models (GTR-FT), scaling up benefits both in-domain and OOD performance. This suggests that, even without pre-training, having larger models is still beneficial to learn the retrieval signals during fine-tuning. For pre-training only models (GTR-PT), we also observe a generally upward trend for zero-shot retrieval tasks. This indicates scaled up models are more capable of absorbing the semantic information in generic training data, and thus, can improve generalization.  Finally, with both pre-training and fine-tuning, the full GTR models consistently improve over GTR-FT of all sizes. This shows the power of combining scaling up and a generic pre-training stage.\n\n\nImportance of the fine-tuning dataset\n\nIn Table 5, we compare GTR and GTR-PT on the BEIR benchmark to understand the importance of fine-tuning on MS Marco. The table shows that there is a clear gap between GTR models before and after fine-tuning. The result demonstrates the necessity of leveraging a high quality dataset (e.g., search data) to fine-tune the dual encoders.\n\nSpecifically, we compare fine-tuning GTR on NQ vs. MS Marco. NQ only covers Wikipedia documents and is much smaller in size than MS Marco. This allows us to investigate the performance of GTR of being fine-tuned on a less generalizable dataset. Also, fine-tuning on NQ gives a fair comparison with DPR (Karpukhin et al., 2020). Table 6, the GTR-Base model finetuned on NQ outperforms the original DPR model that uses a BERT-Base model as the backbone encoder. This demonstrates the effectiveness of our pre-training on the Web dataset, as well as the hard negatives introduced from Lu et al. (2021) for NQ. Also, fine-tuning on NQ leads to inferior performance compared to fine-tuning on MS Marco, which is consistent with prior work (Thakur et al., 2021). However, the disadvantage of fine-tuning on NQ is being relieved as the model scales up. This shows that the benefit of scaling up holds for different fine-tuning datasets. Furthermore, when scaling up from Large to XL, we observe a more significant gain when fine-tuning with NQ than with MS Marco, indicating that scaling up helps more when using weaker fine-tuning data. \"CL\" denotes their approach with contrastive learning on C4 and Wiki while others denote GTR with different sizes. Note that they only report results on 15 datasets of the BEIR benchmark.\n\n\nAs shown in\n\n\nDifferent pre-training strategies\n\nConcurrently, Izacard et al. (2021) propose to conduct contrastive learning (CL) pre-training with data from C4 and Wiki datasets in an unsupervised way. In particular, their pre-training data is constructed by randomly choosing two spans from a single document and conduct word deletion or replacement to each span. In contrast, GTR uses Web-mined QA data as the pre-training data. We compare the performance of GTR to their models to gain further insights into using different pre-training data and methods for dual encoders. As shown in Figure 5, on over half of the datasets, models with our pre-training approach under-perform CL-Pretrain with the base size; while as the model size increases, GTR-Large and -XXL models show significant gains over CL-Pretrain. This demonstrates that scaling up can mitigate the disadvantage of using a potentially inferior pre-training method. Note that our pre-training is additive to CL-Pretrain and we can leverage the pre-training on C4 and Wiki to further improve the results. We leave this exploration to future work.\n\n\nDocument length vs. model capacity\n\nPreviously, Thakur et al. (2021) showed that models trained with cosine similarity prefer short documents while those trained with dot-product prefer long documents. To investigate whether scaling up affects this observation, we compute the median lengths of the top-10 retrieved documents for all queries and present the results in Figure 6.\n\nThough all GTR models are trained using cosine similarity, we found that scaling up the model size has influence over the lengths of retrieved documents. We observe an increasing trend of document length for DB-Pedia, Fever, HotpotQA, Signal-1M, Trec-News, and Web-Touche2020 with scaling up. In particular, for Web-Touche2020, the lengths of the retrieved documents grow drastically as the model scales up: The largest GTR-XXL retrieves documents that are on average twice as long compared with the smallest GTR-Base. This contributes to the performance since Thakur et al. (2021) show that the majority of relevant documents in Web-Touche2020 are longer. On the other hand, the only exception we observe is the Trec-Covid dataset, where GTR-XXL model retrieves much shorter documents than those by its smaller size counterparts. This may explain the inferior performance of GTR-XXL on Trec-Covid shown in Table 3 and Table 8. We leave it as future work to explore the effects of using dot-product as the similarity function for large dual encoders.\n\n\nScaling up with different bottleneck size\n\nThis section presents a complementary study to reveal the interaction of scaling up model capacity and increasing the bottleneck embedding size.\n\nSpecifically, we set the bottleneck embedding size to [256,768,2048,4096]. Given this significant increase in the hyperparameter space, we make two minor compromises to be frugal on computational resources: 1) we directly fine-tune on MS Marco without pre-training on the Community QA dataset; 2) we evaluate on six randomly selected OOD datasets, which are BioASQ, DBPedia-entity, NQ, HotpotQA, Fever, and SciFact.\n\nWe present results of NDCG@10 in Figure 7. We observe that, under all choices of bottleneck embedding sizes, scaling up model capacity consistently improves model performance. On the other hand, when we increase the bottleneck dimension size, the performance gain is minimum from the dimensionality of 768 to 4096, as similarly observed in (Neelakantan et al., 2022). These observations indicate that, to enhance model generalizability under the single-vector doc-product scheme, scaling up model size can be more effective than increasing the bottleneck dimensionality. We leave the exploration of using better training objectives to improve the scaling law of bottleneck dimensionality as our future work.\n\n\nRelated Work\n\nNeural information retrieval. Document retrieval is an important task in the NLP and information retrieval (IR) communities. Traditionally, lexical based approaches trying to match the queries and documents based on term overlap, such as TF-IDF and BM25 (Robertson and Zaragoza, 2009), have achieved great success in this task. Recently, neural based approaches, which go beyond the simple term matching, are being quickly adopted by the community and achieving state-of-the-art performance on multiple retrieval tasks, such as passage retrieval (Karpukhin et al., 2020), question answering (Ahmad et al., 2019), conversational question answering (Qu et al., 2020) and bitext retrieval (Feng et al., 2020).\n\nDual encoders for neural retrieval. In the line of neural retrievers, dual encoders have demonstrated great performance compared to traditional sparse models, such as BM25, on a wide range of retrieval tasks (Karpukhin et al., 2020;Gillick et al., 2018). A key to the success of dual encoders is pre-trained language models, from which the dual encoders are initialized. Other techniques, such as negative mining (Xiong et al., 2020;Lu et al., 2021;Sachan et al., 2021) and having large training batch sizes (Qu et al., 2021), are also highly effective. However, little work has been done on discussing the effect of the capacity of the backbone models.\n\nZero-shot neural retrieval. Recent works have shown great improvement under the zero-shot setting for dual encoders by leveraging distillation and synthetic data generation (Thakur et al., 2021;Hofst\u00e4tter et al., 2021;Ma et al., 2020). Both these techniques, and scaling up backbone models, are effective ways to close the gap between dual encoders and the upper bound of the singleproduct approaches with fixed-dimension embeddings. On the other hand, multi-vector approaches introduce more interactions between dense embeddings, which could also benefit from scaling up the backbone encoders. We hope that our exploration on scaling up model sizes for single dot-product based methods can lay the groundwork for multivector approaches and further push the frontier of neural information retrieval.\n\n\nInference latency\n\nA caveat of scaling up is the increase in latency overhead. Therefore, we investigate the inference speed, in microseconds (ms), for all GTR models with a batch size of 1 and an input length of 128. We found the latency increases from 17 ms, 34 ms, 96 ms to 349 ms. To put it in context, the GTR-Base model has a close latency compared to TAS-B while the largest GTR-XXL model has a similar latency with re-ranking models (Thakur et al., 2021). With the recent work towards making large models efficient with sparsification, distillation and prompttuning, we hope the inference time for large dual encoders can be significantly reduced in the future.\n\n\nConclusion\n\nThis paper presents the Generalizable T5 Retriever (GTR), a scaled-up dual encoder model with a fixed-size dot-product bottleneck layer. We show that scaling up the model size brings significant improvement on retrieval performance across the board on the BEIR zero-shot retrieval benchmark, especially for out-of-domain generalization. The GTR-XXL model performs at the level of state-ofthe-art performance on BEIR, outperforming many models that use earlier interactions between queries and documents. This sheds light on the research direction to continue enhancing the single vector representation model through better backbone encoders. Moreover, our in-depth analysis reveals the impact of scaling up under the scenarios of different training stages, pre-training strategies, fine-tuning datasets, and bottleneck sizes, as well as how scaling up influences the retrieved document lengths. Our findings can inform future work and is an integral part of the joint effort to improve dual encoder models.\n\n\nLimitations\n\nIn our work, we focus on standard dual encoder training and have not investigated other techniques such as distillation. There have been shown distillation is a strong recipe to improve the dense retrieval models on out-of-domain performance (Santhanam et al., 2021;Formal et al., 2021). We hope to investigate whether the scaling effect could also benefit distillation if we scale up the student dual encoders. In addition, we only focus on English-only corpus and we leave the exploration of scaling up dense retrievers for multi-lingual corpus to future work. Table 7 shows the comparisons of GTR models and the baselines. Note that the best RocketQA model used additional augmented data other than MS Marco to improve the model performance while all others do not. Our best GTR-XXL models outperforms RocketQA on both MRR and recall.  A.2 Recall on BEIR  \n\n\nA More results\n\n\nA.1 Comparisons on MS Marco\n\nFigure 1 :\n1Average Recall@100 and NDCG@10 on all BEIR tasks (excl. MS Marco). Scaling up consistently improves dual encoders' out-of-domain performance.\n\nFigure 3 :\n3Multi-stage training for GTR models.\n\nFigure 4 :\n4Comparison with BM25 on NDCG@10.\n\nFigure 5 :\n5Comparison with Izacard et al. (2021) on NDCG@10.\n\nFigure 6 :\n6Median lengths (in words) of top-10 retrieved documents for all queries.\n\nFigure 7 :\n7Average NDCG@10 for selected OOD datasets with different bottleneck and model sizes.\n\n\narchitecture, including Base, Large, XL and XXL. TheirGTR Models \n\nBase Large \nXL XXL \n\n#. of params \n110M 335M 1.24B \n4.8B \n\nTable 1: Number of parameters in the GTR models. \n\nModels \nDim. size \n\nColBERT \n128 \nDPR, ANCE, TAS-B, GenQ, GTR \n768 \nBM25, DocT5Query \n-\n\n\n\nTable 2 :\n2Dimension of different models. Most dual encoder models set the embedding dimension to 768.\n\n\nTAS-B GenQ ColBERT GTR-Base GTR-Large GTR-XL GTR-XXLNDCG@10 / Model \n\nLexical / Sparse \nDense \nOurs \n\nBM25 docT5query \nDPR \nANCE MS Marco \n0.228 \n0.338 \n0.177 \n0.388 \n0.408 \n0.408 \n0.401 \n0.420 \n0.430 \n0.439 \n0.442 \n\nTrec-Covid \n0.656 \n0.713 \n0.332 \n0.654 \n0.481 \n0.619 \n0.677 \n0.539 \n0.557 \n0.584 \n0.501 \nBioASQ \n0.465 \n0.431 \n0.127 \n0.306 \n0.383 \n0.398 \n0.474 \n0.271 \n0.320 \n0.317 \n0.324 \nNFCorpus \n0.325 \n0.328 \n0.189 \n0.237 \n0.319 \n0.319 \n0.305 \n0.308 \n0.329 \n0.343 \n0.342 \nNQ \n0.329 \n0.399 \n0.474 \n0.446 \n0.463 \n0.358 \n0.524 \n0.495 \n0.547 \n0.559 \n0.568 \nHotpotQA \n0.603 \n0.58 \n0.391 \n0.456 \n0.584 \n0.534 \n0.593 \n0.535 \n0.579 \n0.591 \n0.599 \nFiQA-2018 \n0.236 \n0.291 \n0.112 \n0.295 \n0.300 \n0.308 \n0.317 \n0.349 \n0.424 \n0.444 \n0.467 \nSignal-1M \n0.330 \n0.307 \n0.155 \n0.249 \n0.289 \n0.281 \n0.274 \n0.261 \n0.265 \n0.268 \n0.273 \nTrec-News \n0.398 \n0.42 \n0.161 \n0.382 \n0.377 \n0.396 \n0.393 \n0.337 \n0.343 \n0.350 \n0.346 \nRobust04 \n0.408 \n0.437 \n0.252 \n0.392 \n0.427 \n0.362 \n0.391 \n0.437 \n0.470 \n0.479 \n0.506 \nArguAna \n0.315 \n0.349 \n0.175 \n0.415 \n0.429 \n0.493 \n0.233 \n0.511 \n0.525 \n0.531 \n0.540 \nTouch\u00e9-2020 \n0.367 \n0.347 \n0.131 \n0.240 \n0.162 \n0.182 \n0.202 \n0.205 \n0.219 \n0.230 \n0.256 \nQuora \n0.789 \n0.802 \n0.248 \n0.852 \n0.835 \n0.830 \n0.854 \n0.881 \n0.890 \n0.890 \n0.892 \nDBPedia-entity \n0.313 \n0.331 \n0.263 \n0.281 \n0.384 \n0.328 \n0.392 \n0.347 \n0.391 \n0.396 \n0.408 \nSCIDOCS \n0.158 \n0.162 \n0.077 \n0.122 \n0.149 \n0.143 \n0.145 \n0.149 \n0.158 \n0.159 \n0.161 \nFever \n0.753 \n0.714 \n0.562 \n0.669 \n0.700 \n0.669 \n0.771 \n0.660 \n0.712 \n0.717 \n0.740 \nClimate-Fever \n0.213 \n0.201 \n0.148 \n0.198 \n0.228 \n0.175 \n0.184 \n0.241 \n0.262 \n0.270 \n0.267 \nSciFact \n0.665 \n0.675 \n0.318 \n0.507 \n0.643 \n0.644 \n0.671 \n0.600 \n0.639 \n0.635 \n0.662 \nCQADupStack \n0.299 \n0.325 \n0.153 \n0.296 \n0.314 \n0.347 \n0.350 \n0.357 \n0.384 \n0.388 \n0.399 \n\nAvg \n0.413 \n0.429 \n0.234 \n0.389 \n0.414 \n0.410 \n0.429 \n0.416 \n0.444 \n0.452 \n0.457 \nAvg w/o MS Marco \n0.423 \n0.434 \n0.237 \n0.389 \n0.415 \n0.410 \n0.431 \n0.416 \n0.445 \n0.453 \n0.458 \n\n\n\nTable 3 :\n3NDCG@10 BEIR. Best results are marked in bold. GTR models are pre-trained on CommunityQA dataset and the complete MS Marco dataset. GTR models achieve better NDCG when increasing size from Base to XXL, outperforming the previous best sparse model DocT5Query and dense retrieval model TAS-B.\n\nTable 4 :\n4Comparisons of NDCG@10 for GTR models fine-tuned with different amount of data.\n\n\nGTR-FT GTR-PTGTR \n\nFine-tuning \n\n\n\n\nNDCG@10 on MS Marco \n\nBase \n0.400 \nN/A \n0.420 \nLarge \n0.415 \nN/A \n0.430 \nXL \n0.418 \nN/A \n0.439 \nXXL \n0.422 \nN/A \n0.442 \n\nZero-shot average NDCG@10 w/o MS Marco \n\nBase \n0.387 \n0.295 \n0.416 \nLarge \n0.412 \n0.315 \n0.445 \nXL \n0.433 \n0.315 \n0.453 \nXXL \n0.430 \n0.332 \n0.458 \n\n\n\nTable 5 :\n5Comparisons (NDCG@10) of models trained \nwith and without pre-training and fine-tuning. GTR-PT \nis not fine-tuned, and thus, does not have in-domain per-\nformance. Notably, GTR-FT XL already outperforms \nthe previous best dual-encoder model TAS-B. \n\n\n\nTable 6 :\n6Comparisons of fine-tuning on MS Marco and NQ with average zero-shot NDCG@10.\n\nTable 7 :\n7Comparisons of different models on MS Marco. Scaling up can improve GTR models' indomain performance.\n\nTable 8\n8presents the Recall@100 of GTR models and the baselines. Similar to NDCG@10, we observe that scaling up dual encoders lead to significant gains on the BEIR benchmark in terms of recall. TAS-B GenQ ColBERT GTR-Base GTR-Large GTR-XL GTR-XXLRecall@100 / Model \n\nLexical / Sparse \nDense \nOurs \n\nBM25 docT5query \nDPR \nANCE MS Marco \n0.658 \n0.819 \n0.552 \n0.852 \n0.884 \n0.884 \n0.865 \n0.898 \n0.908 \n0.911 \n0.916 \nTrec-Covid \n0.498 \n0.541 \n0.212 \n0.457 \n0.387 \n0.456 \n0.464 \n0.411 \n0.434 \n0.457 \n0.407 \nBioASQ \n0.714 \n0.646 \n0.256 \n0.463 \n0.579 \n0.627 \n0.645 \n0.441 \n0.490 \n0.483 \n0.483 \nNFCorpus \n0.250 \n0.253 \n0.208 \n0.232 \n0.280 \n0.280 \n0.254 \n0.275 \n0.298 \n0.318 \n0.300 \nNQ \n0.760 \n0.832 \n0.880 \n0.836 \n0.903 \n0.862 \n0.912 \n0.893 \n0.930 \n0.936 \n0.946 \nHotpotQA \n0.740 \n0.709 \n0.591 \n0.578 \n0.728 \n0.673 \n0.748 \n0.676 \n0.725 \n0.739 \n0.752 \nFiQA-2018 \n0.539 \n0.598 \n0.342 \n0.581 \n0.593 \n0.618 \n0.603 \n0.670 \n0.742 \n0.755 \n0.780 \nSignal-1M \n0.370 \n0.351 \n0.162 \n0.239 \n0.304 \n0.281 \n0.283 \n0.263 \n0.261 \n0.268 \n0.268 \nTrec-News \n0.422 \n0.439 \n0.215 \n0.398 \n0.418 \n0.412 \n0.367 \n0.475 \n0.525 \n0.512 \n0.544 \nRobust04 \n0.375 \n0.357 \n0.211 \n0.274 \n0.331 \n0.298 \n0.31 \n0.324 \n0.365 \n0.364 \n0.372 \nArguAna \n0.942 \n0.972 \n0.751 \n0.937 \n0.942 \n0.978 \n0.914 \n0.974 \n0.978 \n0.980 \n0.983 \nTouch\u00e9-2020 \n0.538 \n0.557 \n0.301 \n0.458 \n0.431 \n0.451 \n0.439 \n0.281 \n0.282 \n0.297 \n0.301 \nQuora \n0.973 \n0.982 \n0.470 \n0.987 \n0.986 \n0.988 \n0.989 \n0.996 \n0.996 \n0.997 \n0.997 \nDBPedia-entity \n0.398 \n0.365 \n0.349 \n0.319 \n0.499 \n0.431 \n0.461 \n0.418 \n0.480 \n0.480 \n0.494 \nSCIDOCS \n0.356 \n0.360 \n0.219 \n0.269 \n0.335 \n0.332 \n0.344 \n0.340 \n0.358 \n0.358 \n0.366 \nFever \n0.931 \n0.916 \n0.840 \n0.900 \n0.937 \n0.928 \n0.934 \n0.923 \n0.941 \n0.944 \n0.947 \nClimate-Fever \n0.436 \n0.427 \n0.390 \n0.445 \n0.534 \n0.450 \n0.444 \n0.522 \n0.552 \n0.569 \n0.556 \nSciFact \n0.908 \n0.914 \n0.727 \n0.816 \n0.891 \n0.893 \n0.878 \n0.872 \n0.899 \n0.911 \n0.900 \nCQADupStack \n0.606 \n0.638 \n0.403 \n0.579 \n0.622 \n0.654 \n0.624 \n0.681 \n0.714 \n0.729 \n0.740 \n\nAvg \n0.601 \n0.615 \n0.425 \n0.559 \n0.610 \n0.605 \n0.604 \n0.596 \n0.625 \n0.632 \n0.634 \nAvg w/o MS Marco \n0.598 \n0.603 \n0.418 \n0.543 \n0.594 \n0.590 \n0.590 \n0.580 \n0.609 \n0.616 \n0.619 \n\n\n\nTable 8 :\n8Recall@100 on the BEIR benchmark. The best result on a given dataset is marked in bold.9855 \n\nMS Marco is excluded from the zero-shot comparison as many baseline model used it as training data.\nhttps://github.com/google/jax\nAcknowledgmentsWe thank Chris Tar and Don Metzler for feedback and suggestions. We thank all reviewers for their constructive feedback.\nReQA: An evaluation for end-to-end answer retrieval models. Amin Ahmad, Noah Constant, Yinfei Yang, Daniel Cer, Workshop on Machine Reading for Question Answering. Amin Ahmad, Noah Constant, Yinfei Yang, and Daniel Cer. 2019. ReQA: An evaluation for end-to-end answer retrieval models. In Workshop on Machine Reading for Question Answering.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In NAACL.\n\nLanguageagnostic bert sentence embedding. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang, Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Language- agnostic bert sentence embedding.\n\nSplade v2: Sparse lexical and expansion model for information retrieval. C Thibault Formal, Benjamin Lassance, St\u00e9phane Piwowarski, Clinchant, abs/2109.10086ArXiv. Thibault Formal, C. Lassance, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. Splade v2: Sparse lexical and expansion model for information re- trieval. ArXiv, abs/2109.10086.\n\nEnd-to-end retrieval in continuous space. D Gillick, A Presta, Gaurav Singh Tomar, abs/1811.08008ArXiv. D. Gillick, A. Presta, and Gaurav Singh Tomar. 2018. End-to-end retrieval in continuous space. ArXiv, abs/1811.08008.\n\nLearning dense representations for entity retrieval. Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, Diego Garcia-Olano, 10.18653/v1/K19-1049Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Hong Kong, ChinaAssociation for Computational LinguisticsDaniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. 2019. Learning dense repre- sentations for entity retrieval. In Proceedings of the 23rd Conference on Computational Natural Lan- guage Learning (CoNLL), pages 528-537, Hong Kong, China. Association for Computational Lin- guistics.\n\nEfficient natural language response suggestion for smart reply. Matthew Henderson, Rami Al-Rfou, B Strope, Yun-Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, R Kurzweil, abs/1705.00652ArXiv. Matthew Henderson, Rami Al-Rfou, B. Strope, Yun- Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Sanjiv Ku- mar, Balint Miklos, and R. Kurzweil. 2017. Effi- cient natural language response suggestion for smart reply. ArXiv, abs/1705.00652.\n\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, Allan Hanbury, arXiv:2104.06967Efficiently teaching an effective dense retriever with balanced topic aware sampling. arXiv preprintSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef- ficiently teaching an effective dense retriever with balanced topic aware sampling. arXiv preprint arXiv:2104.06967.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, arXiv:2112.09118Towards unsupervised dense information retrieval with contrastive learning. arXiv preprintGautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118.\n\nBillion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, 10.1109/TBDATA.2019.2921572IEEE Transactions on Big Data. 73Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2021. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547.\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 6769- 6781, Online. Association for Computational Lin- guistics.\n\nColbert: Efficient and effective passage search via contextualized late interaction over bert. Omar Khattab, Matei Zaharia, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information RetrievalOmar Khattab and Matei Zaharia. 2020. Colbert: Ef- ficient and effective passage search via contextual- ized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39-48.\n\nNatural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 7Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a bench- mark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.\n\nMulti-stage training with improved negative contrast for neural passage retrieval. Jing Lu, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Jianmo Ni, Yinfei Yang, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingJing Lu, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Jianmo Ni, and Yinfei Yang. 2021. Multi-stage training with im- proved negative contrast for neural passage retrieval. In Proceedings of the 2021 Conference on Empiri- cal Methods in Natural Language Processing, pages 6091-6103.\n\nYi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins, arXiv:2005.00181Sparse, dense, and attentional representations for text retrieval. arXiv preprintYi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020. Sparse, dense, and at- tentional representations for text retrieval. arXiv preprint arXiv:2005.00181.\n\nZero-shot neural retrieval via domain-targeted synthetic query generation. Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, Ryan Mcdonald, 2004arXiv e-printsJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2020. Zero-shot neural re- trieval via domain-targeted synthetic query genera- tion. arXiv e-prints, pages arXiv-2004.\n\n. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinderand Lilian Weng. 2022. Text and code embeddings by contrastive pre-training. ArXiv, abs/2201.10005Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. 2022. Text and code embeddings by contrastive pre-training. ArXiv, abs/2201.10005.\n\nMs marco: A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine read- ing comprehension dataset.\n\nSentence-t5: Scalable sentence encoders from pre-trained text-to-text models. Jianmo Ni, Gustavo Hern\u00e1ndez \u00c1brego, Noah Constant, Ji Ma, B Keith, Daniel Hall, Yinfei Cer, Yang, arXiv:2108.08877arXiv preprintJianmo Ni, Gustavo Hern\u00e1ndez \u00c1brego, Noah Con- stant, Ji Ma, Keith B Hall, Daniel Cer, and Yin- fei Yang. 2021. Sentence-t5: Scalable sentence en- coders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877.\n\nOpen-retrieval conversational question answering. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, Mohit Iyyer, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalChen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. 2020. Open-retrieval conver- sational question answering. In Proceedings of the 43rd International ACM SIGIR Conference on Re- search and Development in Information Retrieval.\n\nRocketqa: An optimized training approach to dense passage retrieval for opendomain question answering. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. Rocketqa: An optimized train- ing approach to dense passage retrieval for open- domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 5835-5847.\n\nExploring the limits of transfer learning with a unified textto-text transformer. Colin Raffel, Noam M Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W Li, Peter J Liu, 21/140Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Explor- ing the limits of transfer learning with a unified text- to-text transformer. JMLR, 21/140.\n\nThe probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, 10.1561/1500000019Found. Trends Inf. Retr. 34Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and be- yond. Found. Trends Inf. Retr., 3(4):333-389.\n\nEnd-to-end training of neural retrievers for open-domain question answering. Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L Hamilton, Bryan Catanzaro, 10.18653/v1/2021.acl-long.519Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamil- ton, and Bryan Catanzaro. 2021. End-to-end train- ing of neural retrievers for open-domain question answering. In Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 6648-6662, Online. Association for Computa- tional Linguistics.\n\n2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. Keshav Santhanam, O Khattab, Jon Saad-Falcon, Christopher Potts, Matei A Zaharia, abs/2112.01488ArXiv. Keshav Santhanam, O. Khattab, Jon Saad-Falcon, Christopher Potts, and Matei A. Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. ArXiv, abs/2112.01488.\n\nAdafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, PMLRInternational Conference on Machine Learning. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR.\n\nBEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Abhishek Srivastava, and Iryna Gurevych. Round 2Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab- hishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evalua- tion of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Sys- tems Datasets and Benchmarks Track (Round 2).\n\nNearest neighbor search in google correlate. Dan Vanderkam, Rob Schonberger, Henry Rowley, Sanjiv Kumar, GoogleTechnical reportDan Vanderkam, Rob Schonberger, Henry Rowley, and Sanjiv Kumar. 2013. Nearest neighbor search in google correlate. Technical report, Google.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30. Curran Associates, Inc.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk, arXiv:2007.00808Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprintLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor neg- ative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.\n\nByt5: Towards a token-free future with pre-trained byte-to-byte models. Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel, arXiv:2105.13626arXiv preprintLinting Xue, Aditya Barua, Noah Constant, Rami Al- Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2021. Byt5: Towards a token-free future with pre-trained byte-to-byte models. arXiv preprint arXiv:2105.13626.\n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, arXiv:2010.11934Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprintLinting Xue, Noah Constant, Adam Roberts, Mi- hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: A mas- sively multilingual pre-trained text-to-text trans- former. arXiv preprint arXiv:2010.11934.\n\nImproving multilingual sentence embedding using bidirectional dual encoder with additive margin softmax. Yinfei Yang, Gustavo Hern\u00e1ndez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil, arXiv:1902.08564arXiv preprintYinfei Yang, Gustavo Hern\u00e1ndez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2019. Im- proving multilingual sentence embedding using bi- directional dual encoder with additive margin soft- max. arXiv preprint arXiv:1902.08564.\n\nMultilingual universal sentence encoder for semantic retrieval. Yinfei Yang, Daniel Matthew Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, G \u00c1brego, Steve Yuan, C Tar, Yun-Hsuan Sung, B Strope, R Kurzweil, ACL. Yinfei Yang, Daniel Matthew Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, G. \u00c1brego, Steve Yuan, C. Tar, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2020. Multilingual universal sentence encoder for semantic retrieval. In ACL.\n\nLearning discriminative projections for text similarity measures. Kristina Wen-Tau Yih, John C Toutanova, Christopher Platt, Meek, Proceedings of the Fifteenth Conference on Computational Natural Language Learning. the Fifteenth Conference on Computational Natural Language LearningOregon, USAAssociation for Computational LinguisticsPortlandWen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceed- ings of the Fifteenth Conference on Computational Natural Language Learning, pages 247-256, Port- land, Oregon, USA. Association for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":81,\"start\":71},{\"end\":90,\"start\":82},{\"end\":99,\"start\":91},{\"end\":111,\"start\":100},{\"end\":137,\"start\":112},{\"end\":153,\"start\":138},{\"end\":162,\"start\":154},{\"end\":176,\"start\":163},{\"end\":192,\"start\":177},{\"end\":205,\"start\":193},{\"end\":208,\"start\":206},{\"end\":225,\"start\":209},{\"end\":234,\"start\":226}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":78},{\"end\":89,\"start\":87},{\"end\":98,\"start\":96},{\"end\":110,\"start\":107},{\"end\":136,\"start\":130},{\"end\":152,\"start\":148},{\"end\":161,\"start\":157},{\"end\":175,\"start\":171},{\"end\":191,\"start\":186},{\"end\":204,\"start\":200},{\"end\":224,\"start\":216},{\"end\":233,\"start\":228}]", "author_first_name": "[{\"end\":77,\"start\":71},{\"end\":86,\"start\":82},{\"end\":95,\"start\":91},{\"end\":106,\"start\":100},{\"end\":119,\"start\":112},{\"end\":129,\"start\":120},{\"end\":145,\"start\":138},{\"end\":147,\"start\":146},{\"end\":156,\"start\":154},{\"end\":168,\"start\":163},{\"end\":170,\"start\":169},{\"end\":185,\"start\":177},{\"end\":199,\"start\":193},{\"end\":207,\"start\":206},{\"end\":215,\"start\":209},{\"end\":227,\"start\":226}]", "author_affiliation": null, "title": "[{\"end\":49,\"start\":1},{\"end\":283,\"start\":235}]", "venue": "[{\"end\":371,\"start\":285}]", "abstract": "[{\"end\":1699,\"start\":463}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1800,\"start\":1778},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1823,\"start\":1800},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2081,\"start\":2057},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2102,\"start\":2081},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2472,\"start\":2456},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2500,\"start\":2474},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2670,\"start\":2650},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3139,\"start\":3118},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4308,\"start\":4287},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4536,\"start\":4515},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5194,\"start\":5164},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5407,\"start\":5389},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5428,\"start\":5407},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5451,\"start\":5428},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5818,\"start\":5800},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6015,\"start\":5988},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6840,\"start\":6819},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6986,\"start\":6966},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7004,\"start\":6986},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7263,\"start\":7242},{\"end\":7279,\"start\":7263},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7297,\"start\":7279},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7776,\"start\":7752},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9504,\"start\":9483},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9558,\"start\":9532},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9891,\"start\":9870},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10192,\"start\":10166},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10262,\"start\":10238},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10281,\"start\":10262},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10518,\"start\":10496},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10971,\"start\":10946},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11192,\"start\":11175},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11251,\"start\":11235},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12078,\"start\":12057},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12859,\"start\":12838},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18280,\"start\":18256},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18709,\"start\":18688},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19359,\"start\":19338},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20457,\"start\":20437},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21350,\"start\":21330},{\"end\":22070,\"start\":22065},{\"end\":22074,\"start\":22070},{\"end\":22079,\"start\":22074},{\"end\":22084,\"start\":22079},{\"end\":22794,\"start\":22768},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23435,\"start\":23406},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23722,\"start\":23698},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23763,\"start\":23743},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23816,\"start\":23799},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23857,\"start\":23838},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24092,\"start\":24068},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24113,\"start\":24092},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24293,\"start\":24273},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24309,\"start\":24293},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24329,\"start\":24309},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24385,\"start\":24368},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24709,\"start\":24688},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24733,\"start\":24709},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24749,\"start\":24733},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25779,\"start\":25758},{\"end\":27289,\"start\":27265},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27309,\"start\":27289}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28084,\"start\":27930},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28134,\"start\":28085},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28180,\"start\":28135},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28243,\"start\":28181},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28329,\"start\":28244},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28427,\"start\":28330},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28696,\"start\":28428},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28800,\"start\":28697},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30768,\"start\":28801},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31071,\"start\":30769},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31163,\"start\":31072},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":31471,\"start\":31164},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":31734,\"start\":31472},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":31824,\"start\":31735},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":31938,\"start\":31825},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":34103,\"start\":31939},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":34209,\"start\":34104}]", "paragraph": "[{\"end\":2848,\"start\":1715},{\"end\":3406,\"start\":2850},{\"end\":3923,\"start\":3408},{\"end\":5075,\"start\":3925},{\"end\":5558,\"start\":5125},{\"end\":6396,\"start\":5560},{\"end\":6858,\"start\":6425},{\"end\":6888,\"start\":6860},{\"end\":7357,\"start\":6908},{\"end\":7777,\"start\":7359},{\"end\":8016,\"start\":7833},{\"end\":8216,\"start\":8095},{\"end\":8344,\"start\":8241},{\"end\":8794,\"start\":8346},{\"end\":9251,\"start\":8796},{\"end\":9837,\"start\":9253},{\"end\":10072,\"start\":9839},{\"end\":10362,\"start\":10074},{\"end\":11116,\"start\":10381},{\"end\":11407,\"start\":11118},{\"end\":11777,\"start\":11409},{\"end\":11892,\"start\":11779},{\"end\":12446,\"start\":11918},{\"end\":12607,\"start\":12448},{\"end\":13036,\"start\":12609},{\"end\":13233,\"start\":13059},{\"end\":13592,\"start\":13257},{\"end\":14527,\"start\":13633},{\"end\":14819,\"start\":14529},{\"end\":15350,\"start\":14821},{\"end\":16493,\"start\":15391},{\"end\":16697,\"start\":16525},{\"end\":17576,\"start\":16741},{\"end\":17952,\"start\":17618},{\"end\":19272,\"start\":17954},{\"end\":20386,\"start\":19324},{\"end\":20767,\"start\":20425},{\"end\":21819,\"start\":20769},{\"end\":22009,\"start\":21865},{\"end\":22426,\"start\":22011},{\"end\":23135,\"start\":22428},{\"end\":23858,\"start\":23152},{\"end\":24513,\"start\":23860},{\"end\":25314,\"start\":24515},{\"end\":25986,\"start\":25336},{\"end\":27007,\"start\":26001},{\"end\":27882,\"start\":27023}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7832,\"start\":7778},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8094,\"start\":8017}]", "table_ref": "[{\"end\":10563,\"start\":10556},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":13276,\"start\":13269},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":13494,\"start\":13487},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":13657,\"start\":13650},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":15752,\"start\":15745},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":16884,\"start\":16877},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":17628,\"start\":17621},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":18289,\"start\":18282},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":21695,\"start\":21676},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":27593,\"start\":27586}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1713,\"start\":1701},{\"attributes\":{\"n\":\"2\"},\"end\":5088,\"start\":5078},{\"attributes\":{\"n\":\"2.1\"},\"end\":5123,\"start\":5091},{\"attributes\":{\"n\":\"2.2\"},\"end\":6423,\"start\":6399},{\"attributes\":{\"n\":\"3.1\"},\"end\":6906,\"start\":6891},{\"attributes\":{\"n\":\"3.2\"},\"end\":8239,\"start\":8219},{\"attributes\":{\"n\":\"4.2\"},\"end\":10379,\"start\":10365},{\"attributes\":{\"n\":\"4.3\"},\"end\":11916,\"start\":11895},{\"attributes\":{\"n\":\"5\"},\"end\":13057,\"start\":13039},{\"attributes\":{\"n\":\"5.1\"},\"end\":13255,\"start\":13236},{\"attributes\":{\"n\":\"5.2\"},\"end\":13631,\"start\":13595},{\"attributes\":{\"n\":\"5.3\"},\"end\":15389,\"start\":15353},{\"attributes\":{\"n\":\"6\"},\"end\":16523,\"start\":16496},{\"attributes\":{\"n\":\"6.1\"},\"end\":16739,\"start\":16700},{\"attributes\":{\"n\":\"6.2\"},\"end\":17616,\"start\":17579},{\"end\":19286,\"start\":19275},{\"attributes\":{\"n\":\"6.3\"},\"end\":19322,\"start\":19289},{\"attributes\":{\"n\":\"6.4\"},\"end\":20423,\"start\":20389},{\"attributes\":{\"n\":\"6.5\"},\"end\":21863,\"start\":21822},{\"attributes\":{\"n\":\"7\"},\"end\":23150,\"start\":23138},{\"attributes\":{\"n\":\"8\"},\"end\":25334,\"start\":25317},{\"attributes\":{\"n\":\"9\"},\"end\":25999,\"start\":25989},{\"attributes\":{\"n\":\"10\"},\"end\":27021,\"start\":27010},{\"end\":27899,\"start\":27885},{\"end\":27929,\"start\":27902},{\"end\":27941,\"start\":27931},{\"end\":28096,\"start\":28086},{\"end\":28146,\"start\":28136},{\"end\":28192,\"start\":28182},{\"end\":28255,\"start\":28245},{\"end\":28341,\"start\":28331},{\"end\":28707,\"start\":28698},{\"end\":30779,\"start\":30770},{\"end\":31082,\"start\":31073},{\"end\":31482,\"start\":31473},{\"end\":31745,\"start\":31736},{\"end\":31835,\"start\":31826},{\"end\":31947,\"start\":31940},{\"end\":34114,\"start\":34105}]", "table": "[{\"end\":28696,\"start\":28484},{\"end\":30768,\"start\":28855},{\"end\":31471,\"start\":31179},{\"end\":31734,\"start\":31484},{\"end\":34103,\"start\":32187},{\"end\":34209,\"start\":34203}]", "figure_caption": "[{\"end\":28084,\"start\":27943},{\"end\":28134,\"start\":28098},{\"end\":28180,\"start\":28148},{\"end\":28243,\"start\":28194},{\"end\":28329,\"start\":28257},{\"end\":28427,\"start\":28343},{\"end\":28484,\"start\":28430},{\"end\":28800,\"start\":28709},{\"end\":28855,\"start\":28803},{\"end\":31071,\"start\":30781},{\"end\":31163,\"start\":31084},{\"end\":31179,\"start\":31166},{\"end\":31824,\"start\":31747},{\"end\":31938,\"start\":31837},{\"end\":32187,\"start\":31949},{\"end\":34203,\"start\":34116}]", "figure_ref": "[{\"end\":3565,\"start\":3557},{\"end\":4012,\"start\":4004},{\"end\":7356,\"start\":7348},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8261,\"start\":8253},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14442,\"start\":14434},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19872,\"start\":19864},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20766,\"start\":20758},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22469,\"start\":22461}]", "bib_author_first_name": "[{\"end\":34540,\"start\":34536},{\"end\":34552,\"start\":34548},{\"end\":34569,\"start\":34563},{\"end\":34582,\"start\":34576},{\"end\":34905,\"start\":34900},{\"end\":34922,\"start\":34914},{\"end\":34936,\"start\":34930},{\"end\":34950,\"start\":34942},{\"end\":35188,\"start\":35178},{\"end\":35201,\"start\":35195},{\"end\":35214,\"start\":35208},{\"end\":35226,\"start\":35220},{\"end\":35243,\"start\":35240},{\"end\":35451,\"start\":35450},{\"end\":35477,\"start\":35469},{\"end\":35496,\"start\":35488},{\"end\":35764,\"start\":35763},{\"end\":35775,\"start\":35774},{\"end\":35796,\"start\":35784},{\"end\":36003,\"start\":35997},{\"end\":36019,\"start\":36013},{\"end\":36035,\"start\":36030},{\"end\":36055,\"start\":36045},{\"end\":36069,\"start\":36064},{\"end\":36087,\"start\":36081},{\"end\":36097,\"start\":36092},{\"end\":36768,\"start\":36761},{\"end\":36784,\"start\":36780},{\"end\":36795,\"start\":36794},{\"end\":36813,\"start\":36804},{\"end\":36826,\"start\":36820},{\"end\":36840,\"start\":36835},{\"end\":36852,\"start\":36846},{\"end\":36866,\"start\":36860},{\"end\":36876,\"start\":36875},{\"end\":37148,\"start\":37139},{\"end\":37172,\"start\":37161},{\"end\":37188,\"start\":37178},{\"end\":37200,\"start\":37195},{\"end\":37211,\"start\":37206},{\"end\":37558,\"start\":37551},{\"end\":37576,\"start\":37568},{\"end\":37589,\"start\":37584},{\"end\":37609,\"start\":37600},{\"end\":37623,\"start\":37618},{\"end\":37642,\"start\":37636},{\"end\":37658,\"start\":37651},{\"end\":38056,\"start\":38052},{\"end\":38074,\"start\":38066},{\"end\":38087,\"start\":38082},{\"end\":38365,\"start\":38357},{\"end\":38383,\"start\":38377},{\"end\":38395,\"start\":38390},{\"end\":38408,\"start\":38401},{\"end\":38422,\"start\":38416},{\"end\":38433,\"start\":38427},{\"end\":38447,\"start\":38442},{\"end\":38461,\"start\":38454},{\"end\":39125,\"start\":39121},{\"end\":39140,\"start\":39135},{\"end\":39693,\"start\":39690},{\"end\":39717,\"start\":39707},{\"end\":39734,\"start\":39728},{\"end\":39752,\"start\":39745},{\"end\":39767,\"start\":39762},{\"end\":39781,\"start\":39776},{\"end\":39799,\"start\":39791},{\"end\":39814,\"start\":39809},{\"end\":39832,\"start\":39827},{\"end\":39847,\"start\":39841},{\"end\":40323,\"start\":40319},{\"end\":40335,\"start\":40328},{\"end\":40345,\"start\":40336},{\"end\":40356,\"start\":40354},{\"end\":40367,\"start\":40361},{\"end\":40378,\"start\":40372},{\"end\":40818,\"start\":40816},{\"end\":40830,\"start\":40825},{\"end\":40851,\"start\":40843},{\"end\":40870,\"start\":40863},{\"end\":41231,\"start\":41229},{\"end\":41240,\"start\":41236},{\"end\":41257,\"start\":41251},{\"end\":41269,\"start\":41264},{\"end\":41280,\"start\":41276},{\"end\":41503,\"start\":41497},{\"end\":41520,\"start\":41517},{\"end\":41529,\"start\":41525},{\"end\":41540,\"start\":41536},{\"end\":41555,\"start\":41550},{\"end\":41563,\"start\":41556},{\"end\":41574,\"start\":41569},{\"end\":41589,\"start\":41583},{\"end\":41603,\"start\":41596},{\"end\":41605,\"start\":41604},{\"end\":41617,\"start\":41613},{\"end\":41622,\"start\":41618},{\"end\":41633,\"start\":41628},{\"end\":41651,\"start\":41643},{\"end\":41668,\"start\":41662},{\"end\":41681,\"start\":41676},{\"end\":42535,\"start\":42532},{\"end\":42547,\"start\":42544},{\"end\":42562,\"start\":42559},{\"end\":42577,\"start\":42569},{\"end\":42590,\"start\":42583},{\"end\":42605,\"start\":42599},{\"end\":42618,\"start\":42616},{\"end\":42882,\"start\":42876},{\"end\":42894,\"start\":42887},{\"end\":42904,\"start\":42895},{\"end\":42917,\"start\":42913},{\"end\":42930,\"start\":42928},{\"end\":42936,\"start\":42935},{\"end\":42950,\"start\":42944},{\"end\":42963,\"start\":42957},{\"end\":43285,\"start\":43281},{\"end\":43293,\"start\":43290},{\"end\":43303,\"start\":43300},{\"end\":43317,\"start\":43310},{\"end\":43324,\"start\":43323},{\"end\":43330,\"start\":43325},{\"end\":43343,\"start\":43338},{\"end\":43921,\"start\":43915},{\"end\":43932,\"start\":43926},{\"end\":43943,\"start\":43939},{\"end\":43952,\"start\":43949},{\"end\":43965,\"start\":43958},{\"end\":43976,\"start\":43971},{\"end\":43980,\"start\":43977},{\"end\":43994,\"start\":43987},{\"end\":44004,\"start\":44001},{\"end\":44016,\"start\":44009},{\"end\":44773,\"start\":44768},{\"end\":44786,\"start\":44782},{\"end\":44788,\"start\":44787},{\"end\":44802,\"start\":44798},{\"end\":44821,\"start\":44812},{\"end\":44833,\"start\":44827},{\"end\":44849,\"start\":44842},{\"end\":44863,\"start\":44858},{\"end\":44871,\"start\":44870},{\"end\":44881,\"start\":44876},{\"end\":44883,\"start\":44882},{\"end\":45193,\"start\":45186},{\"end\":45209,\"start\":45205},{\"end\":45492,\"start\":45484},{\"end\":45508,\"start\":45501},{\"end\":45526,\"start\":45518},{\"end\":45540,\"start\":45536},{\"end\":45550,\"start\":45547},{\"end\":45564,\"start\":45557},{\"end\":45566,\"start\":45565},{\"end\":45582,\"start\":45577},{\"end\":46505,\"start\":46499},{\"end\":46518,\"start\":46517},{\"end\":46531,\"start\":46528},{\"end\":46556,\"start\":46545},{\"end\":46569,\"start\":46564},{\"end\":46571,\"start\":46570},{\"end\":46866,\"start\":46862},{\"end\":46884,\"start\":46876},{\"end\":47213,\"start\":47207},{\"end\":47226,\"start\":47222},{\"end\":47243,\"start\":47236},{\"end\":47742,\"start\":47739},{\"end\":47757,\"start\":47754},{\"end\":47776,\"start\":47771},{\"end\":47791,\"start\":47785},{\"end\":47996,\"start\":47990},{\"end\":48010,\"start\":48006},{\"end\":48024,\"start\":48020},{\"end\":48038,\"start\":48033},{\"end\":48055,\"start\":48050},{\"end\":48068,\"start\":48063},{\"end\":48070,\"start\":48069},{\"end\":48083,\"start\":48078},{\"end\":48433,\"start\":48430},{\"end\":48448,\"start\":48441},{\"end\":48458,\"start\":48456},{\"end\":48472,\"start\":48463},{\"end\":48485,\"start\":48479},{\"end\":48495,\"start\":48491},{\"end\":48511,\"start\":48505},{\"end\":48525,\"start\":48519},{\"end\":48967,\"start\":48960},{\"end\":48979,\"start\":48973},{\"end\":48991,\"start\":48987},{\"end\":49006,\"start\":49002},{\"end\":49022,\"start\":49016},{\"end\":49036,\"start\":49031},{\"end\":49047,\"start\":49043},{\"end\":49062,\"start\":49057},{\"end\":49336,\"start\":49329},{\"end\":49346,\"start\":49342},{\"end\":49361,\"start\":49357},{\"end\":49376,\"start\":49371},{\"end\":49387,\"start\":49383},{\"end\":49892,\"start\":49886},{\"end\":49906,\"start\":49899},{\"end\":49916,\"start\":49907},{\"end\":49930,\"start\":49925},{\"end\":49942,\"start\":49937},{\"end\":49954,\"start\":49948},{\"end\":49967,\"start\":49961},{\"end\":49982,\"start\":49973},{\"end\":49994,\"start\":49989},{\"end\":50006,\"start\":50003},{\"end\":50403,\"start\":50397},{\"end\":50416,\"start\":50410},{\"end\":50424,\"start\":50417},{\"end\":50434,\"start\":50430},{\"end\":50447,\"start\":50442},{\"end\":50456,\"start\":50453},{\"end\":50466,\"start\":50462},{\"end\":50478,\"start\":50477},{\"end\":50492,\"start\":50487},{\"end\":50500,\"start\":50499},{\"end\":50515,\"start\":50506},{\"end\":50523,\"start\":50522},{\"end\":50533,\"start\":50532},{\"end\":50857,\"start\":50849},{\"end\":50875,\"start\":50871},{\"end\":50877,\"start\":50876},{\"end\":50900,\"start\":50889}]", "bib_author_last_name": "[{\"end\":34546,\"start\":34541},{\"end\":34561,\"start\":34553},{\"end\":34574,\"start\":34570},{\"end\":34586,\"start\":34583},{\"end\":34912,\"start\":34906},{\"end\":34928,\"start\":34923},{\"end\":34940,\"start\":34937},{\"end\":34960,\"start\":34951},{\"end\":35193,\"start\":35189},{\"end\":35206,\"start\":35202},{\"end\":35218,\"start\":35215},{\"end\":35238,\"start\":35227},{\"end\":35248,\"start\":35244},{\"end\":35467,\"start\":35452},{\"end\":35486,\"start\":35478},{\"end\":35507,\"start\":35497},{\"end\":35518,\"start\":35509},{\"end\":35772,\"start\":35765},{\"end\":35782,\"start\":35776},{\"end\":35802,\"start\":35797},{\"end\":36011,\"start\":36004},{\"end\":36028,\"start\":36020},{\"end\":36043,\"start\":36036},{\"end\":36062,\"start\":36056},{\"end\":36079,\"start\":36070},{\"end\":36090,\"start\":36088},{\"end\":36110,\"start\":36098},{\"end\":36778,\"start\":36769},{\"end\":36792,\"start\":36785},{\"end\":36802,\"start\":36796},{\"end\":36818,\"start\":36814},{\"end\":36833,\"start\":36827},{\"end\":36844,\"start\":36841},{\"end\":36858,\"start\":36853},{\"end\":36873,\"start\":36867},{\"end\":36885,\"start\":36877},{\"end\":37159,\"start\":37149},{\"end\":37176,\"start\":37173},{\"end\":37193,\"start\":37189},{\"end\":37204,\"start\":37201},{\"end\":37219,\"start\":37212},{\"end\":37566,\"start\":37559},{\"end\":37582,\"start\":37577},{\"end\":37598,\"start\":37590},{\"end\":37616,\"start\":37610},{\"end\":37634,\"start\":37624},{\"end\":37649,\"start\":37643},{\"end\":37664,\"start\":37659},{\"end\":38064,\"start\":38057},{\"end\":38080,\"start\":38075},{\"end\":38093,\"start\":38088},{\"end\":38375,\"start\":38366},{\"end\":38388,\"start\":38384},{\"end\":38399,\"start\":38396},{\"end\":38414,\"start\":38409},{\"end\":38425,\"start\":38423},{\"end\":38440,\"start\":38434},{\"end\":38452,\"start\":38448},{\"end\":38465,\"start\":38462},{\"end\":39133,\"start\":39126},{\"end\":39148,\"start\":39141},{\"end\":39705,\"start\":39694},{\"end\":39726,\"start\":39718},{\"end\":39743,\"start\":39735},{\"end\":39760,\"start\":39753},{\"end\":39774,\"start\":39768},{\"end\":39789,\"start\":39782},{\"end\":39807,\"start\":39800},{\"end\":39825,\"start\":39815},{\"end\":39839,\"start\":39833},{\"end\":39851,\"start\":39848},{\"end\":40326,\"start\":40324},{\"end\":40352,\"start\":40346},{\"end\":40359,\"start\":40357},{\"end\":40370,\"start\":40368},{\"end\":40383,\"start\":40379},{\"end\":40823,\"start\":40819},{\"end\":40841,\"start\":40831},{\"end\":40861,\"start\":40852},{\"end\":40878,\"start\":40871},{\"end\":41234,\"start\":41232},{\"end\":41249,\"start\":41241},{\"end\":41262,\"start\":41258},{\"end\":41274,\"start\":41270},{\"end\":41289,\"start\":41281},{\"end\":41515,\"start\":41504},{\"end\":41523,\"start\":41521},{\"end\":41534,\"start\":41530},{\"end\":41548,\"start\":41541},{\"end\":41567,\"start\":41564},{\"end\":41581,\"start\":41575},{\"end\":41594,\"start\":41590},{\"end\":41611,\"start\":41606},{\"end\":41626,\"start\":41623},{\"end\":41641,\"start\":41634},{\"end\":41660,\"start\":41652},{\"end\":41674,\"start\":41669},{\"end\":41687,\"start\":41682},{\"end\":42542,\"start\":42536},{\"end\":42557,\"start\":42548},{\"end\":42567,\"start\":42563},{\"end\":42581,\"start\":42578},{\"end\":42597,\"start\":42591},{\"end\":42614,\"start\":42606},{\"end\":42623,\"start\":42619},{\"end\":42885,\"start\":42883},{\"end\":42911,\"start\":42905},{\"end\":42926,\"start\":42918},{\"end\":42933,\"start\":42931},{\"end\":42942,\"start\":42937},{\"end\":42955,\"start\":42951},{\"end\":42967,\"start\":42964},{\"end\":42973,\"start\":42969},{\"end\":43288,\"start\":43286},{\"end\":43298,\"start\":43294},{\"end\":43308,\"start\":43304},{\"end\":43321,\"start\":43318},{\"end\":43336,\"start\":43331},{\"end\":43349,\"start\":43344},{\"end\":43924,\"start\":43922},{\"end\":43937,\"start\":43933},{\"end\":43947,\"start\":43944},{\"end\":43956,\"start\":43953},{\"end\":43969,\"start\":43966},{\"end\":43985,\"start\":43981},{\"end\":43999,\"start\":43995},{\"end\":44007,\"start\":44005},{\"end\":44021,\"start\":44017},{\"end\":44780,\"start\":44774},{\"end\":44796,\"start\":44789},{\"end\":44810,\"start\":44803},{\"end\":44825,\"start\":44822},{\"end\":44840,\"start\":44834},{\"end\":44856,\"start\":44850},{\"end\":44868,\"start\":44864},{\"end\":44874,\"start\":44872},{\"end\":44887,\"start\":44884},{\"end\":45203,\"start\":45194},{\"end\":45218,\"start\":45210},{\"end\":45499,\"start\":45493},{\"end\":45516,\"start\":45509},{\"end\":45534,\"start\":45527},{\"end\":45545,\"start\":45541},{\"end\":45555,\"start\":45551},{\"end\":45575,\"start\":45567},{\"end\":45592,\"start\":45583},{\"end\":46515,\"start\":46506},{\"end\":46526,\"start\":46519},{\"end\":46543,\"start\":46532},{\"end\":46562,\"start\":46557},{\"end\":46579,\"start\":46572},{\"end\":46874,\"start\":46867},{\"end\":46890,\"start\":46885},{\"end\":47220,\"start\":47214},{\"end\":47234,\"start\":47227},{\"end\":47250,\"start\":47244},{\"end\":47752,\"start\":47743},{\"end\":47769,\"start\":47758},{\"end\":47783,\"start\":47777},{\"end\":47797,\"start\":47792},{\"end\":48004,\"start\":47997},{\"end\":48018,\"start\":48011},{\"end\":48031,\"start\":48025},{\"end\":48048,\"start\":48039},{\"end\":48061,\"start\":48056},{\"end\":48076,\"start\":48071},{\"end\":48090,\"start\":48084},{\"end\":48102,\"start\":48092},{\"end\":48439,\"start\":48434},{\"end\":48454,\"start\":48449},{\"end\":48461,\"start\":48459},{\"end\":48477,\"start\":48473},{\"end\":48489,\"start\":48486},{\"end\":48503,\"start\":48496},{\"end\":48517,\"start\":48512},{\"end\":48534,\"start\":48526},{\"end\":48971,\"start\":48968},{\"end\":48985,\"start\":48980},{\"end\":49000,\"start\":48992},{\"end\":49014,\"start\":49007},{\"end\":49029,\"start\":49023},{\"end\":49041,\"start\":49037},{\"end\":49055,\"start\":49048},{\"end\":49069,\"start\":49063},{\"end\":49340,\"start\":49337},{\"end\":49355,\"start\":49347},{\"end\":49369,\"start\":49362},{\"end\":49381,\"start\":49377},{\"end\":49395,\"start\":49388},{\"end\":49897,\"start\":49893},{\"end\":49923,\"start\":49917},{\"end\":49935,\"start\":49931},{\"end\":49946,\"start\":49943},{\"end\":49959,\"start\":49955},{\"end\":49971,\"start\":49968},{\"end\":49987,\"start\":49983},{\"end\":50001,\"start\":49995},{\"end\":50015,\"start\":50007},{\"end\":50408,\"start\":50404},{\"end\":50428,\"start\":50425},{\"end\":50440,\"start\":50435},{\"end\":50451,\"start\":50448},{\"end\":50460,\"start\":50457},{\"end\":50475,\"start\":50467},{\"end\":50485,\"start\":50479},{\"end\":50497,\"start\":50493},{\"end\":50504,\"start\":50501},{\"end\":50520,\"start\":50516},{\"end\":50530,\"start\":50524},{\"end\":50542,\"start\":50534},{\"end\":50869,\"start\":50858},{\"end\":50887,\"start\":50878},{\"end\":50906,\"start\":50901},{\"end\":50912,\"start\":50908}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":195873973},\"end\":34816,\"start\":34476},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52967399},\"end\":35134,\"start\":34818},{\"attributes\":{\"id\":\"b2\"},\"end\":35375,\"start\":35136},{\"attributes\":{\"doi\":\"abs/2109.10086\",\"id\":\"b3\",\"matched_paper_id\":237581550},\"end\":35719,\"start\":35377},{\"attributes\":{\"doi\":\"abs/1811.08008\",\"id\":\"b4\",\"matched_paper_id\":53735999},\"end\":35942,\"start\":35721},{\"attributes\":{\"doi\":\"10.18653/v1/K19-1049\",\"id\":\"b5\",\"matched_paper_id\":202718954},\"end\":36695,\"start\":35944},{\"attributes\":{\"doi\":\"abs/1705.00652\",\"id\":\"b6\",\"matched_paper_id\":2449317},\"end\":37137,\"start\":36697},{\"attributes\":{\"doi\":\"arXiv:2104.06967\",\"id\":\"b7\"},\"end\":37549,\"start\":37139},{\"attributes\":{\"doi\":\"arXiv:2112.09118\",\"id\":\"b8\"},\"end\":38007,\"start\":37551},{\"attributes\":{\"doi\":\"10.1109/TBDATA.2019.2921572\",\"id\":\"b9\",\"matched_paper_id\":926364},\"end\":38295,\"start\":38009},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.550\",\"id\":\"b10\",\"matched_paper_id\":215737187},\"end\":39024,\"start\":38297},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":216553223},\"end\":39624,\"start\":39026},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":86611921},\"end\":40234,\"start\":39626},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":243865679},\"end\":40814,\"start\":40236},{\"attributes\":{\"doi\":\"arXiv:2005.00181\",\"id\":\"b14\"},\"end\":41152,\"start\":40816},{\"attributes\":{\"id\":\"b15\"},\"end\":41493,\"start\":41154},{\"attributes\":{\"id\":\"b16\"},\"end\":42463,\"start\":41495},{\"attributes\":{\"id\":\"b17\"},\"end\":42796,\"start\":42465},{\"attributes\":{\"doi\":\"arXiv:2108.08877\",\"id\":\"b18\"},\"end\":43229,\"start\":42798},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":218869571},\"end\":43810,\"start\":43231},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":231815627},\"end\":44684,\"start\":43812},{\"attributes\":{\"doi\":\"21/140\",\"id\":\"b21\"},\"end\":45128,\"start\":44686},{\"attributes\":{\"doi\":\"10.1561/1500000019\",\"id\":\"b22\",\"matched_paper_id\":207178704},\"end\":45405,\"start\":45130},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.519\",\"id\":\"b23\",\"matched_paper_id\":230437591},\"end\":46412,\"start\":45407},{\"attributes\":{\"doi\":\"abs/2112.01488\",\"id\":\"b24\",\"matched_paper_id\":244799249},\"end\":46797,\"start\":46414},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b25\",\"matched_paper_id\":4786918},\"end\":47115,\"start\":46799},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":233296016},\"end\":47692,\"start\":47117},{\"attributes\":{\"id\":\"b27\"},\"end\":47961,\"start\":47694},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":48428,\"start\":47963},{\"attributes\":{\"doi\":\"arXiv:2007.00808\",\"id\":\"b29\"},\"end\":48886,\"start\":48430},{\"attributes\":{\"doi\":\"arXiv:2105.13626\",\"id\":\"b30\"},\"end\":49327,\"start\":48888},{\"attributes\":{\"doi\":\"arXiv:2010.11934\",\"id\":\"b31\"},\"end\":49779,\"start\":49329},{\"attributes\":{\"doi\":\"arXiv:1902.08564\",\"id\":\"b32\"},\"end\":50331,\"start\":49781},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195848217},\"end\":50781,\"start\":50333},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1965270},\"end\":51438,\"start\":50783}]", "bib_title": "[{\"end\":34534,\"start\":34476},{\"end\":34898,\"start\":34818},{\"end\":35448,\"start\":35377},{\"end\":35761,\"start\":35721},{\"end\":35995,\"start\":35944},{\"end\":36759,\"start\":36697},{\"end\":38050,\"start\":38009},{\"end\":38355,\"start\":38297},{\"end\":39119,\"start\":39026},{\"end\":39688,\"start\":39626},{\"end\":40317,\"start\":40236},{\"end\":43279,\"start\":43231},{\"end\":43913,\"start\":43812},{\"end\":45184,\"start\":45130},{\"end\":45482,\"start\":45407},{\"end\":46497,\"start\":46414},{\"end\":46860,\"start\":46799},{\"end\":47205,\"start\":47117},{\"end\":47988,\"start\":47963},{\"end\":50395,\"start\":50333},{\"end\":50847,\"start\":50783}]", "bib_author": "[{\"end\":34548,\"start\":34536},{\"end\":34563,\"start\":34548},{\"end\":34576,\"start\":34563},{\"end\":34588,\"start\":34576},{\"end\":34914,\"start\":34900},{\"end\":34930,\"start\":34914},{\"end\":34942,\"start\":34930},{\"end\":34962,\"start\":34942},{\"end\":35195,\"start\":35178},{\"end\":35208,\"start\":35195},{\"end\":35220,\"start\":35208},{\"end\":35240,\"start\":35220},{\"end\":35250,\"start\":35240},{\"end\":35469,\"start\":35450},{\"end\":35488,\"start\":35469},{\"end\":35509,\"start\":35488},{\"end\":35520,\"start\":35509},{\"end\":35774,\"start\":35763},{\"end\":35784,\"start\":35774},{\"end\":35804,\"start\":35784},{\"end\":36013,\"start\":35997},{\"end\":36030,\"start\":36013},{\"end\":36045,\"start\":36030},{\"end\":36064,\"start\":36045},{\"end\":36081,\"start\":36064},{\"end\":36092,\"start\":36081},{\"end\":36112,\"start\":36092},{\"end\":36780,\"start\":36761},{\"end\":36794,\"start\":36780},{\"end\":36804,\"start\":36794},{\"end\":36820,\"start\":36804},{\"end\":36835,\"start\":36820},{\"end\":36846,\"start\":36835},{\"end\":36860,\"start\":36846},{\"end\":36875,\"start\":36860},{\"end\":36887,\"start\":36875},{\"end\":37161,\"start\":37139},{\"end\":37178,\"start\":37161},{\"end\":37195,\"start\":37178},{\"end\":37206,\"start\":37195},{\"end\":37221,\"start\":37206},{\"end\":37568,\"start\":37551},{\"end\":37584,\"start\":37568},{\"end\":37600,\"start\":37584},{\"end\":37618,\"start\":37600},{\"end\":37636,\"start\":37618},{\"end\":37651,\"start\":37636},{\"end\":37666,\"start\":37651},{\"end\":38066,\"start\":38052},{\"end\":38082,\"start\":38066},{\"end\":38095,\"start\":38082},{\"end\":38377,\"start\":38357},{\"end\":38390,\"start\":38377},{\"end\":38401,\"start\":38390},{\"end\":38416,\"start\":38401},{\"end\":38427,\"start\":38416},{\"end\":38442,\"start\":38427},{\"end\":38454,\"start\":38442},{\"end\":38467,\"start\":38454},{\"end\":39135,\"start\":39121},{\"end\":39150,\"start\":39135},{\"end\":39707,\"start\":39690},{\"end\":39728,\"start\":39707},{\"end\":39745,\"start\":39728},{\"end\":39762,\"start\":39745},{\"end\":39776,\"start\":39762},{\"end\":39791,\"start\":39776},{\"end\":39809,\"start\":39791},{\"end\":39827,\"start\":39809},{\"end\":39841,\"start\":39827},{\"end\":39853,\"start\":39841},{\"end\":40328,\"start\":40319},{\"end\":40354,\"start\":40328},{\"end\":40361,\"start\":40354},{\"end\":40372,\"start\":40361},{\"end\":40385,\"start\":40372},{\"end\":40825,\"start\":40816},{\"end\":40843,\"start\":40825},{\"end\":40863,\"start\":40843},{\"end\":40880,\"start\":40863},{\"end\":41236,\"start\":41229},{\"end\":41251,\"start\":41236},{\"end\":41264,\"start\":41251},{\"end\":41276,\"start\":41264},{\"end\":41291,\"start\":41276},{\"end\":41517,\"start\":41497},{\"end\":41525,\"start\":41517},{\"end\":41536,\"start\":41525},{\"end\":41550,\"start\":41536},{\"end\":41569,\"start\":41550},{\"end\":41583,\"start\":41569},{\"end\":41596,\"start\":41583},{\"end\":41613,\"start\":41596},{\"end\":41628,\"start\":41613},{\"end\":41643,\"start\":41628},{\"end\":41662,\"start\":41643},{\"end\":41676,\"start\":41662},{\"end\":41689,\"start\":41676},{\"end\":42544,\"start\":42532},{\"end\":42559,\"start\":42544},{\"end\":42569,\"start\":42559},{\"end\":42583,\"start\":42569},{\"end\":42599,\"start\":42583},{\"end\":42616,\"start\":42599},{\"end\":42625,\"start\":42616},{\"end\":42887,\"start\":42876},{\"end\":42913,\"start\":42887},{\"end\":42928,\"start\":42913},{\"end\":42935,\"start\":42928},{\"end\":42944,\"start\":42935},{\"end\":42957,\"start\":42944},{\"end\":42969,\"start\":42957},{\"end\":42975,\"start\":42969},{\"end\":43290,\"start\":43281},{\"end\":43300,\"start\":43290},{\"end\":43310,\"start\":43300},{\"end\":43323,\"start\":43310},{\"end\":43338,\"start\":43323},{\"end\":43351,\"start\":43338},{\"end\":43926,\"start\":43915},{\"end\":43939,\"start\":43926},{\"end\":43949,\"start\":43939},{\"end\":43958,\"start\":43949},{\"end\":43971,\"start\":43958},{\"end\":43987,\"start\":43971},{\"end\":44001,\"start\":43987},{\"end\":44009,\"start\":44001},{\"end\":44023,\"start\":44009},{\"end\":44782,\"start\":44768},{\"end\":44798,\"start\":44782},{\"end\":44812,\"start\":44798},{\"end\":44827,\"start\":44812},{\"end\":44842,\"start\":44827},{\"end\":44858,\"start\":44842},{\"end\":44870,\"start\":44858},{\"end\":44876,\"start\":44870},{\"end\":44889,\"start\":44876},{\"end\":45205,\"start\":45186},{\"end\":45220,\"start\":45205},{\"end\":45501,\"start\":45484},{\"end\":45518,\"start\":45501},{\"end\":45536,\"start\":45518},{\"end\":45547,\"start\":45536},{\"end\":45557,\"start\":45547},{\"end\":45577,\"start\":45557},{\"end\":45594,\"start\":45577},{\"end\":46517,\"start\":46499},{\"end\":46528,\"start\":46517},{\"end\":46545,\"start\":46528},{\"end\":46564,\"start\":46545},{\"end\":46581,\"start\":46564},{\"end\":46876,\"start\":46862},{\"end\":46892,\"start\":46876},{\"end\":47222,\"start\":47207},{\"end\":47236,\"start\":47222},{\"end\":47252,\"start\":47236},{\"end\":47754,\"start\":47739},{\"end\":47771,\"start\":47754},{\"end\":47785,\"start\":47771},{\"end\":47799,\"start\":47785},{\"end\":48006,\"start\":47990},{\"end\":48020,\"start\":48006},{\"end\":48033,\"start\":48020},{\"end\":48050,\"start\":48033},{\"end\":48063,\"start\":48050},{\"end\":48078,\"start\":48063},{\"end\":48092,\"start\":48078},{\"end\":48104,\"start\":48092},{\"end\":48441,\"start\":48430},{\"end\":48456,\"start\":48441},{\"end\":48463,\"start\":48456},{\"end\":48479,\"start\":48463},{\"end\":48491,\"start\":48479},{\"end\":48505,\"start\":48491},{\"end\":48519,\"start\":48505},{\"end\":48536,\"start\":48519},{\"end\":48973,\"start\":48960},{\"end\":48987,\"start\":48973},{\"end\":49002,\"start\":48987},{\"end\":49016,\"start\":49002},{\"end\":49031,\"start\":49016},{\"end\":49043,\"start\":49031},{\"end\":49057,\"start\":49043},{\"end\":49071,\"start\":49057},{\"end\":49342,\"start\":49329},{\"end\":49357,\"start\":49342},{\"end\":49371,\"start\":49357},{\"end\":49383,\"start\":49371},{\"end\":49397,\"start\":49383},{\"end\":49899,\"start\":49886},{\"end\":49925,\"start\":49899},{\"end\":49937,\"start\":49925},{\"end\":49948,\"start\":49937},{\"end\":49961,\"start\":49948},{\"end\":49973,\"start\":49961},{\"end\":49989,\"start\":49973},{\"end\":50003,\"start\":49989},{\"end\":50017,\"start\":50003},{\"end\":50410,\"start\":50397},{\"end\":50430,\"start\":50410},{\"end\":50442,\"start\":50430},{\"end\":50453,\"start\":50442},{\"end\":50462,\"start\":50453},{\"end\":50477,\"start\":50462},{\"end\":50487,\"start\":50477},{\"end\":50499,\"start\":50487},{\"end\":50506,\"start\":50499},{\"end\":50522,\"start\":50506},{\"end\":50532,\"start\":50522},{\"end\":50544,\"start\":50532},{\"end\":50871,\"start\":50849},{\"end\":50889,\"start\":50871},{\"end\":50908,\"start\":50889},{\"end\":50914,\"start\":50908}]", "bib_venue": "[{\"end\":34638,\"start\":34588},{\"end\":34967,\"start\":34962},{\"end\":35176,\"start\":35136},{\"end\":35539,\"start\":35534},{\"end\":35823,\"start\":35818},{\"end\":36217,\"start\":36132},{\"end\":36906,\"start\":36901},{\"end\":37321,\"start\":37237},{\"end\":37756,\"start\":37682},{\"end\":38151,\"start\":38122},{\"end\":38592,\"start\":38498},{\"end\":39261,\"start\":39150},{\"end\":39914,\"start\":39853},{\"end\":40471,\"start\":40385},{\"end\":40961,\"start\":40896},{\"end\":41227,\"start\":41154},{\"end\":42530,\"start\":42465},{\"end\":42874,\"start\":42798},{\"end\":43462,\"start\":43351},{\"end\":44165,\"start\":44023},{\"end\":44766,\"start\":44686},{\"end\":45261,\"start\":45238},{\"end\":45785,\"start\":45623},{\"end\":46600,\"start\":46595},{\"end\":46940,\"start\":46896},{\"end\":47346,\"start\":47252},{\"end\":47737,\"start\":47694},{\"end\":48153,\"start\":48104},{\"end\":48635,\"start\":48552},{\"end\":48958,\"start\":48888},{\"end\":49534,\"start\":49413},{\"end\":49884,\"start\":49781},{\"end\":50547,\"start\":50544},{\"end\":50996,\"start\":50914},{\"end\":36305,\"start\":36219},{\"end\":38673,\"start\":38594},{\"end\":39359,\"start\":39263},{\"end\":40544,\"start\":40473},{\"end\":43560,\"start\":43464},{\"end\":44294,\"start\":44167},{\"end\":45934,\"start\":45787},{\"end\":51076,\"start\":50998}]"}}}, "year": 2023, "month": 12, "day": 17}