{"id": 251809429, "updated": "2023-02-22 17:24:48.099", "metadata": {"title": "Cloud and snow detection of remote sensing images based on improved Unet3+", "authors": "[{\"first\":\"Meijie\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Cui\",\"last\":\"Ni\",\"middle\":[]},{\"first\":\"Weilong\",\"last\":\"Hao\",\"middle\":[]}]", "venue": "Scientific Reports", "journal": "Scientific Reports", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Cloud detection is an important step in remote sensing image processing and a prerequisite for subsequent analysis and interpretation of remote sensing images. Traditional cloud detection methods are difficult to accurately detect clouds and snow with very similar features such as color and texture. In this paper, the features of cloud and snow in remote sensing images are deeply extracted, and an accurate cloud and snow detection method is proposed based on the advantages of Unet3+\u2009network in feature fusion. Firstly, color space conversion is performed on remote sensing images, RGB images and HIS images are used as input of Unet3+\u2009network. Resnet 50 is used to replace the Unet3+\u2009feature extraction network to extract remote sensing image features at a deeper level, and add the Convolutional Block Attention Module in Resnet50 to improve the network\u2019s attention to cloud and snow. Finally, the weighted cross entropy loss is constructed to solve the problem of unbalanced sample number caused by high proportion of background area in the image. The results show that the proposed method has strong adaptability and moderate computation. The mPA value, mIoU value and mPrecision value can reach 92.76%, 81.74% and 86.49%, respectively. Compared with other algorithms, the proposed method can better eliminate all kinds of interference information in remote sensing images of different landforms and accurately detect cloud and snow in images.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "36002645", "pubmedcentral": "9402556", "dblp": null, "doi": "10.1038/s41598-022-18812-6"}}, "content": {"source": {"pdf_hash": "b437d9980e67b160dba2106fd61b5f4c4f9527c8", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.nature.com/articles/s41598-022-18812-6.pdf", "status": "GOLD"}}, "grobid": {"id": "c603f3cc47709830f85378d3ac52671e0de6d617", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b437d9980e67b160dba2106fd61b5f4c4f9527c8.txt", "contents": "\nCloud and snow detection of remote sensing images based on improved Unet3+\n0123456789. 2022\n\nMeijie Yin \nSchool of Information Science and Electrical Engineering\nShan Dong Jiao Tong University\n250357JinanChina\n\nPeng Wang \nSchool of Information Science and Electrical Engineering\nShan Dong Jiao Tong University\n250357JinanChina\n\nInstitute of Automation\nShandong Academy of Sciences\n250013JinanChina\n\nCui Ni \nSchool of Information Science and Electrical Engineering\nShan Dong Jiao Tong University\n250357JinanChina\n\nWeilong Hao \nSchool of Information Science and Electrical Engineering\nShan Dong Jiao Tong University\n250357JinanChina\n\nCui Ni \nWeilong Hao \nCloud and snow detection of remote sensing images based on improved Unet3+\n\nScientific Reports |\n12144150123456789. 202210.1038/s41598-022-18812-61 OPEN 3 These authors contributed equally:\nCloud detection is an important step in remote sensing image processing and a prerequisite for subsequent analysis and interpretation of remote sensing images. Traditional cloud detection methods are difficult to accurately detect clouds and snow with very similar features such as color and texture. In this paper, the features of cloud and snow in remote sensing images are deeply extracted, and an accurate cloud and snow detection method is proposed based on the advantages of Unet3+ network in feature fusion. Firstly, color space conversion is performed on remote sensing images, RGB images and HIS images are used as input of Unet3+ network. Resnet 50 is used to replace the Unet3+ feature extraction network to extract remote sensing image features at a deeper level, and add the Convolutional Block Attention Module in Resnet50 to improve the network's attention to cloud and snow. Finally, the weighted cross entropy loss is constructed to solve the problem of unbalanced sample number caused by high proportion of background area in the image. The results show that the proposed method has strong adaptability and moderate computation. The mPA value, mIoU value and mPrecision value can reach 92.76%, 81.74% and 86.49%, respectively. Compared with other algorithms, the proposed method can better eliminate all kinds of interference information in remote sensing images of different landforms and accurately detect cloud and snow in images.With the continuous development of remote sensing technology, satellite has become an important part of a country's economic development. According to statistics, there are more than 2,500 satellites in orbit around the world. With the return of more and more remote sensing images, these images in the investigation of land water resources, land resources, vegetation resources, environmental monitoring and other fields are playing a more and more obvious role. However, about 60% of the earth is covered by clouds on average 1 , clouds not only block ground objects, but also affect the accuracy and reliability of inversion of surface and atmospheric parameters, which is an unfavorable factor restricting the application of remote sensing images. Therefore, it is necessary to accurately detect the clouds in remote sensing images to facilitate the subsequent analysis and interpretation of remote sensing images. However, cloud and snow have similar color and texture features in remote sensing images, it is necessary to distinguish them accurately in the detection process to improve the accuracy of cloud detection.Classical cloud detection methods mainly include physical threshold method, method based on cloud texture and spatial characteristics and method based on machine learning. With the development and application of deep learning, deep learning methods based on CNN, FCN and Unet are gradually applied to remote sensing image cloud detection. The imaging sensor technology carried by early satellites is not mature, and remote sensing images had few spectral segments, so physical threshold-based methods such as OTSU 2,3 and CLAVR 4 are mostly used for cloud detection. With the development of imaging sensors, the bands and spectral segments of remote sensing images are increasing, people begin to use the physical characteristics of clouds, such as highlighting, high reflectivity and low temperature, to complete cloud detection by filtering physical radiation threshold 5-7 . The above thresholding method can detect large and thick clouds in remote sensing images, but it is easy to be disturbed by white ground objects such as ice and snow. The cloud detection method based on cloud texture 8,9 expresses the difference between cloud and surface objects in texture features through gray level co-occurrence matrix and improved fractal dimension, so as to realize remote sensing image cloud detection. The cloud detection method based on image spatial information 10 takes advantage of the fact that the radiation changes degree of cloud covered area is higher than that of other areas, and compares the change of regional radiation value of each pixel in the image with the set threshold value to achieve pixel-level cloud detection.\n\nwww.nature.com/scientificreports/ The method based on cloud texture and spatial characteristics is mainly applicable to remote sensing images with a single background such as ocean and plain, with a small scope of application and low detection accuracy for remote sensing images with complex background. With the development of machine learning, methods such as decision tree and support vector machine (SVM) 11,12 have been applied to remote sensing image cloud detection. Fmask algorithm 13,14 separates potential cloud pixels and clear sky pixels based on cloud physical properties, masks cloud generation probability on land and water through normalized temperature probability, spectral variability probability and brightness probability, and then deduces potential clouds. ACAA algorithm 15 reduces the influence of cloud variability on cloud amount by scanning remote sensing image twice, and then performs cloud detection through decision tree threshold. Reference 16 uses vector machines to create a decision boundary for cloud detection through the representation of mapping data. In addition, the elevation-assisted cloud and snow detection method 17,18 compares the elevation differences between cloud, snow and other ground objects, and uses multiple image intensive matching technology to obtain three-dimensional geometric features of cloud, so as to realize the distinction between cloud and ground. The above cloud detection method based on machine learning is highly dependent on training data and uses artificially designed features. The rationality of feature design will have a great influence on detection results, and the method has a small scope of application.\n\nWith the improvement of computer performance, deep learning algorithm develops rapidly in image segmentation, object detection and other fields. As the basis of deep neural network, CNN network has been widely used in image classification and target detection. Its advantage lies in that it can learn deeper and more abstract image features by increasing the number of convolutional layers and increasing the receptive field. Reference 19 uses a simple CNN network to complete the cloud detection task, which solves the problem of inaccurate detection results of threshold method due to few bands and limited spectral range of remote sensing images. However, the network structure used by this method is simple and the generalization ability is poor. Reference 20 proposed a method combining clustering and convolutional network to realize cloud detection. Firstly, super pixels are obtained by SLIC clustering, and then feature information of cloud is extracted by convolutional neural network, so as to accurately extract cloud boundary, but end-to-end cloud detection cannot be achieved. Reference 21 proposed a robust multi-scale segmentation method based on deep learning, which utilized spectral spatial features of the remaining convolution layer to carry out feature mapping, and then proposed a new loss function for cloud and cloud shadow target extraction. Reference 22 proposed a cloud detection method based on multi-scale characteristic convolutional neural network (MF-CNN). By stacking multi-band spectral information, high-level semantic information is combined with low-level spatial information to accurately distinguish thick cloud, thin cloud and non-cloud regions. Reference 23 proposed a dual-branch PCA network (PCANet) to detect cloud pixels by extracting high-level semantic information of remote sensing images and combining it with SVM classifier. Reference 24 introduced high-frequency feature extractors and multi-scale convolution in Unet networks to refine cloud boundaries and predict debris clouds. Reference 25 proposed a cloud and snow detection method combining Resnet50 and Deeplabv3+ with full convolutional neural network, experimental results show that this method does not have a high distinction between cloud and snow and is prone to misjudgment. Reference 26 proposed a cloud detection method based on spectral library and convolutional neural network, which can accurately detect thin cloud and broken cloud by using residual learning and one-dimensional CNN network to accurately capture spectral information, however, this method ignored spatial information and is limited in feature extraction of complex scenes with mixed spectral information of ground and thin cloud. Reference 27 proposed a cloud detection method for high-resolution remote sensing images based on convolutional neural network, which uses unsupervised learning method for pre-training to obtain cloud feature information in remote sensing images to achieve cloud detection, however, this method ignores cloud texture and other feature information, resulting in low detection accuracy. The cloud detection method based on deep learning can automatically extract cloud feature information without designing features beforehand, and cloud detection has high accuracy. However, different types of remote sensing images have great differences in color information and spatial resolution, and remote sensing images have complex landforms, including snow, ocean, plain, mountain and other landforms, which requires cloud detection methods based on deep learning to have strong generalization ability.\n\nThis paper adopts the deep learning method and takes Unet3+ as the backbone network to accurately detect clouds and snow in remote sensing images. The main contributions are as follows: (1) Carry out color space conversion for remote sensing images, and use RGB and HIS remote sensing images as inputs of Unet3+ network to improve the network's discrimination of cloud and snow features; (2) Resnet50 is used as the feature extraction network of Unet3+ to deepen the network structure and extract deeper feature information of remote sensing images, while avoiding the problems of gradient explosion and gradient dispersion, and convolutional attention mechanism is introduced in Resnet50 network to strengthen the network's attention to cloud and snow area, and reduce the unnecessary calculation overhead caused by high proportion of background area in cloud and snow detection process; (3) Weighted cross entropy loss is designed to avoid the problem that the model weight gradually deviates to the background due to the excessive background class pixels in remote sensing images during network model training, so as to further improve the efficiency and accuracy of cloud and snow detection.\n\nThe rest work of this paper is as follows: \"Correlation network model\" section introduces the structure of the network model used, including Unet3+ network and Resnet50 network; \"Methodology\" section describes many details of the proposed method, including the improvement of network structure, the optimization of network input and the design of loss function. \"Experimental results and analysis\" section is the comparison and analysis with other cloud detection methods under the same data set. The last section summarizes the work of this paper and puts forward the next research direction. nally used in medical image processing and has been widely used in remote sensing image segmentation in recent years. Unet network only fuses feature maps of the same scale in the decoder part, and lacks sufficient information to explore from the full scale, so the location and boundary of learning objectives cannot be defined. In order to remedy the above defects, Unet3+ network 28 came into being. Unet network connects encoders and decoders of the same scale, while Unet3+ network fuses feature maps of different scales through nested and dense jump connections. As shown in Fig. 1. Figure 1a is the network structure of Unet, and Fig. 1b is the network structure of Unet3+ . As shown in Fig. 1b, X1, X2, X3, X4 and X5 are feature maps generated after feature extraction network of each layer, and then feature maps of the same scale as X4 are obtained through different pooling operations. Then, through the convolution operation of 3 \u00d7 3 filters with the same number of channels as X4, the feature graph with the same number of channels as X4 is obtained, and then the fusion feature graph FX4 is obtained by splicing and fusion with X4. By analogy, fusion feature graphs FX3, FX2 and FX1 are obtained through the same operation. In Unet3+ network, feature maps of different scales describe different image feature information. The shallower the network, such as FM1, the smaller the receptive field, the stronger the geometric detail representation ability of the extracted shallow feature information, and the more able it is to capture the spatial information of the object, such as the contour and edge of the object. However, the deep network, such as FM4, has a large receptive field and strong semantic representation ability of extracted deep feature information, which can determine object attributes, but cannot highlight details such as spatial geometric features of objects. When remote sensing images enter the network, after a series of pooling operations, the spatial details of cloud and snow in the image will be gradually lost, but the semantic representation ability of cloud and snow is constantly enhanced. In this paper, Unet3+ network is adopted to fully extract multi-scale feature information of cloud, snow and other ground objects in remote sensing images, and full-scale feature fusion method is adopted to fuse shallow feature information and deep feature information, so as to avoid feature information dilution in pooling convolutional operation and improve the detection accuracy of cloud and snow.\n\nResnet50 network. Resnet50 is a deep residual network 29 , which contains 49 convolutional layers and 1 full connection layer, as shown in Fig. 2. The Resnet50 network architecture can be divided into seven parts, the first part is about convolution, regularization, activation, and maximum pooling of the input. The second, third, fourth and fifth parts all contain residual modules, which transfer the network input across layers and carry out equal mapping, and then add the results of the convolution operation. The sixth part is global average pooling, transforming the convolution calculation results of the first five parts into a feature vector. The seventh part is the full connection layer, which uses the classifier to calculate the feature vector and output the category probability.\n\nIn this paper, Resnet50 network is taken as the feature extraction network of Unet3+ , by increasing the number of convolutional network layers, the cloud and snow in remote sensing images in color, texture and other deeper feature information can be extracted. Due to the application of residual module in Resnet50 network,  www.nature.com/scientificreports/ the number of parameters is greatly reduced, and problems such as gradient explosion and gradient dispersion caused by too many network layers can be effectively avoided, thus improving the robustness of cloud detection.\n\n\nMethodology\n\nBased on Unet3+ network structure, this paper proposes an accurate cloud and snow detection method, and the process is shown in Fig. 3. Firstly, the remote sensing image is converted into color space as the input of the network. Resnet50 is taken as the feature extraction network of Unet3+ and added into CBAM to strengthen the network's attention to cloud and snow. The weighted cross entropy is constructed to increase the weight of cloud and snow in remote sensing images, so as to avoid the model bias in the training process.\n\n\nMultidimensional image input.\n\nMost semantic segmentation networks use RGB images as input to extract feature information of R, G and B channels, however, RGB color space is sometimes not a good clustering basis for image clustering, especially for remote sensing images rich in color information. In remote sensing images, cloud and snow have great similarity in color and texture, and it is difficult to distinguish cloud and snow in ordinary RGB images from the naked eye. Feeding the images into the network makes it harder to distinguish between cloud and snow features. HIS color space based on human visual system, with Hue, Intensity and Saturation to describe the image, the color information and gray information of the image are separated in order to express the color category and the degree of the same color in a deeper level. In the case of highly similar objects, such as white clouds and snow, it is possible to distinguish the types of objects in more detail. In this paper, based on RGB images, color space conversion is carried out to convert them into HIS images, and the conversion formula is shown in (1)-(3). The color space conversion results of some images are shown in Fig. 4.\n\n(1)  www.nature.com/scientificreports/ Finally, RGB images and HIS images are used as the input of Unet3+ network to extract the features of cloud, snow and other ground objects respectively, so as to improve the network's discrimination of cloud and snow features.\n\uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 H = \ufffd \u03b8 , G \u2265 B 2\u03c0 \u2212 \u03b8 , G < B , where \u03b8= cos \u22121 \ufffd (R\u2212G)+(R\u2212B) 2 \u221a (R\u2212G) 2 +(R\u2212B)(G\u2212B) \ufffd (2) S = 1 \u2212 3 min(R, G, B) R + G + B\nFeature extraction network and convolutional attention module. In this paper, Resnet50 network is used to replace the feature extraction network of Unet3+, as shown in Fig. 5. The feature extraction network of Unet3+ contains five encoders, each of which contains three effective convolutional layers and a maximum pooling layer. The improved feature extraction network contains five layers of encoders, each of which contains several convolutional layers with different convolutional kernels, regularization layers, ReLU activation layers and residual modules, five effective feature graphs FM1-FM5 are obtained through different convolutional operations. Each decoder layer contains convolution layer and activation function layer, and the effective feature images of different scales are fused by jump connection respectively to obtain fusion feature images FFM1-FFM4.\n\nDue to the wide coverage and rich color information of remote sensing images, the network will inevitably invest a lot of computing power in non-cloud and non-snow areas during feature extraction, which reduces the network work efficiency. In order to increase the network's attention to cloud and snow and extract the characteristic information of cloud and snow at a deeper level, convolutional block attention module (CBAM) 30 is added to Resnet50 network, as shown in Fig. 6, which mainly includes channel attention mechanism and spatial attention mechanism. The mechanism of channel attention is that the input feature maps are respectively processed by global maximum pooling and global average pooling based on width and length, and then processed by multi-layer perceptron (MLP). The features of MLP output are added up, and finally activated by sigmoid to generate channel attention feature maps. The channel attention feature map is multiplied with the input image as the input feature map of the spatial attention module, and the expression is shown in (4). The spatial attention mechanism is to make a global maximum pooling and global average pooling based on channel and merge the results based on channel. Then, a convolution operation is performed to reduce the dimension to one channel, and then activated by sigmoid to generate the spatial attention feature graph, as shown in (5). Finally, the final feature map is obtained by multiplying the channel attention feature map and the spatial attention feature map. By adding CBAM, the network can enhance the attention of cloud and snow in remote sensing image, and extract the features of cloud and snow with less computational overhead www.nature.com/scientificreports/ where MLP represents multi-layer perceptron, F represents feature graph, \u03c3 represents sigmoid activation function, W 1 and W 0 represent the shared weight of two inputs of multi-layer perceptron respectively, f 7\u00d77 represents the size of convolution kernel of 7 \u00d7 7.\n(3) I = R + G + B 3 (4) M c (F) = \u03c3 (MLP(AvgPool (F)) + MLP(MaxPool (F))) = \u03c3 (W 1 (W 0 (F c avg )) + W 1 (W 0 (F c max )))\nIn order to verify the effectiveness of CBAM, this paper outputs the attention map of remote sensing images including frozen soil, mountains, towns and snowfields, the results are shown in Fig. 7, and the dark red area represents the key area concerned by the network. In order to verify the effectiveness of CBAM, this paper outputs the attention map of remote sensing images including frozen soil, mountains, towns and snowfields. The results are shown in Fig. 7, and the dark red area represents the key area concerned by the network. As can be seen from the figure, after network training, for the image with relatively single texture shown in Fig. 7a and Fig. 7d, the attention of the network is mainly focused on the cloud and snow areas in the figure. For images with more complex textures as shown in Fig. 7b,c, the attention of the network covers a wider area, but the cloud and snow areas are all within the attention range of the network.\n\nWeighted cross entropy loss. Loss function is an important link in neural network, which is used to express the difference between the predicted result and the actual result. In classification networks and semantic segmentation networks, cross entropy is often used as the loss function 31 , as shown in Formula (6)  www.nature.com/scientificreports/ where c represents the predicted category, M represents the number of categories; y c represents the one-hot vector, if c is consistent with the actual type, the value is 1, otherwise, the value is 0; P c represents the probability that the predicted sample is in the c category. At present, cross entropy loss function has been widely used in semantic segmentation network models. When the cloud and snow is far less than the number of pixels in remote sensing image background pixel number, if you use the cross entropy as the loss function, in the process of network model training, the number of y c = 0 in the formula (6) will be far greater than the number of y c = 1, which results in the loss function of y c = 0 composition is dominant, weights of network model will be seriously biased towards the background pixels, It affects the accuracy of cloud and snow detection.\n\nIn order to avoid the problem that the weight of the network model gradually deviates to the background due to too many background pixels in the process of cloud and snow detection, this paper designs a new weighted cross entropy loss based on the original cross entropy loss function, as shown in Formula (7):\n\nwhere w c represents the weight coefficient of category C, and the calculation method is shown in Formula (8): where N c represents the number of pixels of category C, N represents the total number of pixels of a remote sensing image. w c represents the weight coefficient of category c, the size of the weight coefficient is related to the number of pixels of category c, and the more pixels of category c, the smaller the weight coefficient is to ensure the balance of categories. However, the weight coefficient should not be too large or too small to prevent problems such as category imbalance. Therefore, exp function is adopted in this paper to linearly normalize the weight coefficient and limit the weight coefficient within the range of (e \u22121 , 1), by controlling the size of the weight coefficient w c , prevent network model bias caused by too large or too small weight of a certain type of pixel. In addition, exp function increases monotonically in the interval of (e \u22121 , 1). When the number of class c pixels increases, \u2212 N c N decreases and the weight coefficient decreases, which can effectively weaken the influence of high frequency classes in data samples and solve the problem of unbalanced data samples. In the process of cloud and snow detection of remote sensing images, the more pixels of the background category, the smaller the weight coefficient will be, making the model weight biased towards the cloud and snow region, enabling the network to learn the characteristic information of cloud and snow better, improving the robustness of the model and improving the accuracy of cloud and snow detection.\n\n\nExperimental results and analysis\n\nConstruction of cloud and snow detection data-set. In this paper, remote sensing images taken by Gaofen-2 (GF-2) and Huanjing-1 (HJ1A) satellites are used to construct a dataset. GF-2 imagers the ground landscape by pushing and scanning, and can take panchromatic and multispectral images with high resolution, high positioning accuracy and high radiation quality. HJ1A satellite is mainly used for environment and disaster monitoring and prediction, with CCD camera and Hyperspectral imager. In order to test the effectiveness and accuracy of cloud and snow detection method proposed in this paper, remote sensing images of various landforms are selected, as shown in Fig. 8a, including ocean, plain, town, frozen soil, snow and desert, etc. When making the cloud and snow detection data set, due to the diverse shapes and irregular spatial distribution of cloud and snow in the image, in order to improve the annotation accuracy, this paper first adopts the simple linear iterative clustering method (SLIC) 32,33 to perform super-pixel segmentation on remote sensing image, as shown in Fig. 8b, and determines the contour of cloud and snow at the sub-pixel level. Then, by manual labeling, cloud labels and snow labels are respectively labeled according to the outlined cloud and snow contours, as shown in Fig. 8c, which effectively reduces labeling errors and makes the produced data set more accurate. The label diagram contains three types of labels: red for cloud, green for snow, and black for background. Finally, the completed cloud and snow detection data set is processed with data enhancement, and a total of 5000 images are obtained. Among them, 4500 images are used as training set and 500 images are used as verification set, and the ratio of training set to verification set is 9:1.\n\n\nComparison of feature extraction network experiments.\n\nResnet50 can extract deep feature information of image with high precision and moderate computation. In this paper, Resnet50 is used as the feature extraction network of Unet3+, which can accurately extract the feature information of cloud and snow from remote sensing images. In the constructed cloud and snow detection data set, the experimental comparison of VGG16, Resnet34, Resnet50 and Resnet101 is carried out respectively. Mean intersection over union (mIoU), mean pixel accuracy (mPA), mean precision (mPrecision) and Estimated Total Size are used as evaluation indexes, and the results are shown in Table 1. Among them, IoU, PA and Precision are common evaluation indexes in semantic segmentation, which are used to measure the similarity between segmentation results and real images. Their formulas are shown in (9), (10) and (11). www.nature.com/scientificreports/ In the formula, TP represents the number of correctly classified positive class pixels, FP represents the number of correctly classified negative class pixels, FN represents the number of incorrectly classified positive class pixels, TN represents the number of accurately classified negative class pixels.\n(7) L = \u2212 M c=1 w c y c log(p c ) (8) w c = exp \u2212 N C N(\nAs shown in the table, compared with other backbone networks, Resnet50 has the highest evaluation index and a moderate number of Estimated Total Size.\n\nIn this paper, the deep learning framework of PyTorch 34 is used to train and test the network model. The compilation environment is conda 4.12.0, the CPU is i7-10,700, 16 GB RAM, NVIDIA GeForce RTX 3070-8G and CUDA 11.2. In the experiment, batch-size is set to 8, initial learning rate is set to 0.01, and Adam optimizer is used to optimize the network. Adam optimizer is a first-order optimization algorithm that can replace the traditional stochastic gradient descent process. It can update the weight of neural network iteratively based on training data and adjust the gradient descent adaptively according to the size of learning rate. It has the advantages of simple implementation, high computational efficiency, less memory requirement, and the updating of parameters is not affected by the scaling transformation of gradient. Moreover, the hyperparameters have good interpretation, and usually need no adjustment or only fine tuning. To ensure good learning efficiency, every 50  www.nature.com/scientificreports/ epochs, the learning rate is divided by 10 to train a total of 100 epochs. The training process is shown in Fig. 9. It can be seen from the figure. that when the training arrived at the 70th epoch, the network model had tended to fit.\n\n\nAblation experiment.\n\nIn order to test the improvement of cloud and snow detection performance by each method described in \"Methodology\" section, ablation experiment is conducted in this paper, and the results are shown in Table 2. Multidimensional image input: in the feature extraction stage, the input of the original network is RGB image, and the feature extraction of cloud and snow is insufficient. After HIS image is added, deeper color, texture and other feature information can be extracted, which improves the final cloud and snow detection result. The experimental results are shown in Table 2. mPA value, mIoU value and mPrecision value are improved by 0.54%, 0.17% and 0.68% respectively.\n\nFull-scale feature fusion and CBAM: Due to the rich color and wide coverage area of remote sensing images, in addition to the features of clouds and snow, a lot of feature information of other ground objects can be extracted. In the process of cloud and snow detection, too much characteristic information will lead to the dilution of cloud and snow features and affect the accuracy of cloud and snow detection. Full-scale feature fusion and CBAM can effectively reduce the dilution of cloud and snow features and strengthen the network's attention to cloud and snow. As can be seen from Table 2, mPA value, mIoU value and mPrecision value increased by 1.38%, 0.84% and 0.41% on the basis of multidimensional image input.\n\nWeighted cross entropy loss: There is a lot of background information in the cloud and snow detection data set of remote sensing image, and the proportion of cloud and snow is relatively small. In the process of network model training, it is inevitable that the model will be biased to background information. The weighted cross entropy loss designed in this paper can effectively avoid this problem. When the number of pixels of a certain class is smaller, the weight coefficient will be larger, and the network will be better able to learn this class, in addition, it can also solve the problem of sample imbalance. As can be seen from Comparison and analysis of algorithms. Under the constructed cloud and snow detection data set, the proposed method is compared with other cloud detection methods based on Unet 35 , Deeplabv3+ 24 and CDUnet 23 . In this paper, intersection over union (IoU), mean intersection over union (mIoU), mean pixel accuracy (mPA), mean precision (mPrecision) and recall are selected as the measurement indexes of the experimental results, and the results are shown in Fig. 10. As can be seen from the figure, the IoU value, mIoU value, mPA value, mPrecision value and recall value of this method are 0.74, 0.82, 0.93, 0.87 and 0.93 respectively, compared  www.nature.com/scientificreports/ with other cloud detection methods, it has been improved. This is because other methods only focus on clouds in remote sensing images. For snow with very similar features to clouds, it is easy to misjudge cloud and snow due to insufficient feature information extracted. In this paper, 20 remote sensing images of different landforms including plain, snow field, ocean and desert are selected from the validation set, and mIoU values and mPA values of different methods are output respectively. The results are shown in Figs. 11 and 12. As can be seen from the figure, the mIoU value and mPA value of the proposed method are generally higher than those of other methods when tested under remote sensing images of   www.nature.com/scientificreports/ various landforms, with the mIoU value generally reaching above 0.75 and mPA value generally reaching above 0.85. There are a lot of broken clouds in 14 and 18 remote sensing images, and the cloud boundary information is complex, CDUnet network adds high-frequency feature extractor and multi-scale convolution to refine the cloud boundary, and the detection result of broken clouds is slightly better than the method proposed in this paper. As shown in Fig. 13, the proposed method is compared with the threshold segmentation method (OTSU) and the above neural network-based method, and the detection results are respectively output in eight landforms, including desert, town, ocean, plain, snow field and river. Figure 13c shows the detection results of OTSU. As www.nature.com/scientificreports/ only a single threshold value can be set, this method can detect clouds more accurately for a single remote sensing image with only clouds. However, for remote sensing images with snow and white ground objects, misdetection is serious. As shown in c3, c5 and c7, this method also misdetects white islands and snow as clouds. Figure 13d is the output result of Unet-based cloud detection method. Due to simple feature extraction, features of cloud and snow cannot be accurately distinguished, resulting in low cloud detection accuracy of this method and cloud boundary cannot be accurately located, as shown by c2. When there is interference from snow and white ground objects, it is also prone to false detection. As shown in d3, white islands are mistakenly detected as clouds, and snow in d5 is also mistakenly detected as clouds. Figure 13e shows the output of the cloud detection method based on Deeplabv3+. Due to the addition of a simple and effective decoder module, this method has a finer division of object boundary, which is greatly improved compared with Unet method. However, there will be false detection inside the cloud, and the distinction between cloud and snow is not high, as shown in e5 and e8. Figure 13f is the output of cloud detection method based on CDUnet. This method introduces high-frequency feature extractor and multi-scale convolution, which can well detect cloud and cloud shadow in the image, and has high accuracy in the distinction between cloud and snow. However, misdetection may occur at cloud junction, as shown in f4 and f5, false detection occurs at the cloud-snow junction and multi-cloud junction. Figure 13g is the output of the method in this paper, which can not only accurately detect the clouds in remote sensing images, but also distinguish the clouds and snow in images well, with few cases of missed detection and false detection.\n\n\nConclusion\n\nIn this paper, based on Unet3+ network structure, multi-dimensional remote sensing image is used as network input through color space conversion, Resnet50 is used as feature extraction network, and CBAM is added to improve the network's attention to cloud and snow in the image, and deeply extract cloud and snow features. At the same time, the weighted cross entropy loss is designed to solve the sample imbalance problem. Experimental results show that the proposed method can effectively eliminate interference information and accurately extract cloud and snow from remote sensing images of various landforms. In the next step, we will continue to enrich the cloud detection data set, further improve the anti-interference ability against snow and other white ground objects in the cloud detection process, and explore a new lightweight deep network model to reduce computing overhead and improve the speed of cloud detection.\n\n\nData availability\n\nThe datasets used or analyzed during the current study are available from the corresponding author on reasonable request.\n\nFigure 1 .\n1Schematic diagram of Unet and Unet3+ network structure.\n\nFigure 2 .\n2Schematic diagram of Resnet50 network structure. Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\nFigure 3 .\n3Flowchart of the method presented in this paper.\n\nFigure 4 .\n4Schematic diagram of color space conversion results. Among them, the first behavior RGB image, the second behavior transformed HIS image. Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\nFigure 5 .\n5Unet3+ network structure after the introduction of Resnet50. Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\nFigure 6 .\n6Schematic diagram of CBAM.\n\nFigure 7 .\n7Schematic diagram of CBAM output results. Wherein, the original remote sensing image of the previous act and the CBAM output result corresponding to the next act. The areas in dark red indicate the areas where the network is concerned. Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\n( 10 )Figure 8 .\n108PA = TP + TN TP + FP + FN + TN (11) Precision = TP TP + FP Labels of remote sensing images of different landforms obtained after superpixel segmentation. (a) is the original remote sensing image, (b) is the result of super pixel segmentation, (c) is the label image.\n\nFigure 9 .\n9Schematic diagram of network model training process used in this paper.\n\nFigure 10 .\n10Comparison of evaluation results of IoU and other 5 quantitative indicators.\n\nFigure 11 .\n11Comparison of mIoU values of different methods.\n\nFigure 12 .\n12Comparison of mPA values of different methods. Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\nFigure 13 .\n13Prediction results (a) Original images; (b) Label images; (c) OTSU; (d) Unet; (e) Deeplabv3+; (f) CDUnet; (g) proposed. Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\n\nUnet3+ network.Unet network is a deep learning model based on encoder-decoder structure, it is origi-Scientific Reports \n| \n(2022) 12:14415 | \nhttps://doi.org/10.1038/s41598-022-18812-6 \n\nwww.nature.com/scientificreports/ \n\nCorrelation network model \n\n\n\nTable 1 .\n1Comparison of experimental results of feature extraction network.Backbone network \nmPA (%) mIoU (%) mPrecision (%) Estimated total size (M) \n\nVGG16 \n86.97 \n72.94 \n78.43 \n2907.95 \n\nResnet34 \n87.45 \n71.47 \n77.37 \n1250.33 \n\nResnet101 \n89.45 \n77.63 \n84.32 \n3437.68 \n\nResnet50 \n89.70 \n78.72 \n84.59 \n2741.59 \n\n\nTable 2 ,\n2mPA value, mIoU value and mPrecision value are further improved. Finally, mPA value, mIoU value and mPrecision value reach 92.76%, 81.74% and 86.49% respectively.\n\nTable 2 .\n2Comparison of ablation results.Method \n\n\u00a9 The Author(s) 2022\nCode availabilityThe codes used during the current study are available from the corresponding author on reasonable request.Author contributionsM.Y. contributed significantly to analysis and wrote the manuscript, P.W. contributed to the conception of the study, C.N. performed the experiment. W.H. contributed to performed the data analyses and manuscript preparation. The work described has not been published before, and its publication has been approved by the responsible authorities at the institution where the work is carried out.Competing interestsThe authors declare no competing interests.Additional informationCorrespondence and requests for materials should be addressed to P.W.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 InternationalLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nAutomated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change. Z Zhu, C E Woodcock, Remote Sens. Environ. 152Zhu, Z. & Woodcock, C. E. Automated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change. Remote Sens. Environ. 152, 217-234 (2014).\n\n. P S Xiang, MODIS cloud detection algorithm based on Kmeans and Otsu. Geospat. Inf. 184Xiang, P. S. MODIS cloud detection algorithm based on Kmeans and Otsu. Geospat. Inf. 18(4), 31-33 (2020).\n\nAutomatic cloud detection algorithm based on deep confidence network OTSU hybrid model. M Qiu, J. Comput. Appl. 38116Qiu, M. et al. Automatic cloud detection algorithm based on deep confidence network OTSU hybrid model. J. Comput. Appl. 38(11), 6 (2018).\n\nLand surface temperature measurements from the split window channels of the NOAA 7 advanced very high-resolution radiometer. J C Price, J. Geophys. Res. Atmos. 89D5Price, J. C. Land surface temperature measurements from the split window channels of the NOAA 7 advanced very high-resolution radiometer. J. Geophys. Res. Atmos. 89(D5), 7231-7237 (1984).\n\nThe universal cloud detection algorithm of MODIS data. W Li, D Li, Remotely Sens. Data Inf. SPIE. 6419Li, W. & Li, D. The universal cloud detection algorithm of MODIS data. Geoinform. 2006 Remotely Sens. Data Inf. SPIE 6419, 108-113 (2006).\n\nStudy on methods of cloud identification and data recovery for MODIS data. A Comer\u00f3n, Int. Soc. Opt. Photonics. 6745Comer\u00f3n, A. et al. Study on methods of cloud identification and data recovery for MODIS data. Int. Soc. Opt. Photonics 6745, 198-205 (2007).\n\nAn effective method for the detection and removal of thin clouds from MODIS image. R Ren, S Guo, L Gu, Proceedings of SPIE the International Society for Optical Engineering. SPIE the International Society for Optical Engineering7455Ren, R., Guo, S., Gu, L. et al. An effective method for the detection and removal of thin clouds from MODIS image. In Proceedings of SPIE the International Society for Optical Engineering, vol. 7455 (2009).\n\nFeature detection for cloud classification in remote sensing images. P Chen, R Zhang, Z K Liu, J. Univ. Sci. Technol. China. 55Chen, P., Zhang, R. & Liu, Z. K. Feature detection for cloud classification in remote sensing images. J. Univ. Sci. Technol. China 5, 5 (2009).\n\nHigh accuracy cloud detection algorithm and its application. N Shan, T Y Zheng, Z S Wang, J. Remote Sens. 618Shan, N., Zheng, T. Y. & Wang, Z. S. High accuracy cloud detection algorithm and its application. J. Remote Sens. 6, 18 (2009).\n\nCorrelation based cloud-detection and an examination of the split-window method. C Solvsteen, D W Deering, P Gudmandsen, Proc. SPIE Int. Soc. Opt. Eng. 2586Solvsteen, C., Deering, D. W. & Gudmandsen, P. Correlation based cloud-detection and an examination of the split-window method. Proc. SPIE Int. Soc. Opt. Eng. 2586, 86-97 (1995).\n\nCloud detection of remote sensing images based on H-SVM with multi-feature fusion. B Zhang, Y D Hu, J Hong, J. Atmos. Environ. Opt. 1601Zhang, B., Hu, Y. D. & Hong, J. Cloud detection of remote sensing images based on H-SVM with multi-feature fusion. J. Atmos. Environ. Opt. 16(01), 58-66 (2021).\n\nApplication of support vector machines in cloud detection. Y M He, H J Wang, Z H Jiang, J. PLA Univ. Sci. Technol. (Nat. Sci. Ed.). 1002He, Y. M., Wang, H. J. & Jiang, Z. H. Application of support vector machines in cloud detection. J. PLA Univ. Sci. Technol. (Nat. Sci. Ed.) 10(02), 191-194 (2009).\n\nImprovement and expansion of the Fmask algorithm: Cloud, cloud shadow, and snow detection for Landsats 4-7, 8, and Sentinel 2 images. C E Woodcock, Remote Sens. Environ. Interdiscip. J. 159Woodcock, C. E. et al. Improvement and expansion of the Fmask algorithm: Cloud, cloud shadow, and snow detection for Landsats 4-7, 8, and Sentinel 2 images. Remote Sens. Environ. Interdiscip. J. 159, 269-277 (2015).\n\nFmask 4.0: Improved cloud and cloud shadow detection in Landsats 4-8 and Sentinel-2 imagery. S Qiu, Z Zhu, B He, Remote Sens. Environ. 231111205Qiu, S., Zhu, Z. & He, B. Fmask 4.0: Improved cloud and cloud shadow detection in Landsats 4-8 and Sentinel-2 imagery. Remote Sens. Environ. 231, 111205 (2019).\n\nCharacterization of the Landsat-7 ETM+ automated cloud-cover assessment (ACCA) algorithm. Photogramm. Eng. Remote Sens. R R Irish, 72Irish, R. R. et al. Characterization of the Landsat-7 ETM+ automated cloud-cover assessment (ACCA) algorithm. Photogramm. Eng. Remote Sens. 72, 1179-1188 (2006).\n\nCloud detection with SVM technique. C Latry, C Panem, P Dejean, IEEE International Geoscience & Remote Sensing Symposium. IEEELatry, C., Panem, C. & Dejean, P. Cloud detection with SVM technique. In IEEE International Geoscience & Remote Sensing Sym- posium 448-451 (IEEE, 2008).\n\nResearch on Cloud and Cloud Shadow Accurate Detection Algorithm of Multispectral Satellite Remote Sensing Image (University of Electronic Science and Technology of China. S Qiu, Qiu, S. Research on Cloud and Cloud Shadow Accurate Detection Algorithm of Multispectral Satellite Remote Sensing Image (Uni- versity of Electronic Science and Technology of China, 2018).\n\n. 10.1038/s41598-022-18812-6Scientific Reports |. 12Scientific Reports | (2022) 12:14415 | https://doi.org/10.1038/s41598-022-18812-6\n\nDaytime arctic cloud detection based on multi-angle satellite data with case studies. S Tao, B Yu, C Braverman, J. Am. Stat. Assoc. 103482Tao, S., Yu, B. & Braverman, C. Daytime arctic cloud detection based on multi-angle satellite data with case studies. J. Am. Stat. Assoc. 103(482), 584-593 (2008).\n\nCloud detection of ZY-3 satellite remote sensing images based on deep learning. Y Chen, Acta Optica Sinica. 3816Chen, Y. et al. Cloud detection of ZY-3 satellite remote sensing images based on deep learning. Acta Optica Sinica 38(1), 6 (2018).\n\nMultilevel cloud detection in remote sensing images based on deep learning. F Xie, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 108Xie, F. et al. Multilevel cloud detection in remote sensing images based on deep learning. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 10(8), 3631-3640 (2017).\n\nA deep learning method for near-real-time cloud and cloud shadow segmentation from Gaofen-1 images. M Khoshboresh-Masouleh, R Shah-Hosseini, Comput. Intell. Neurosci. 2020Khoshboresh-Masouleh, M. & Shah-Hosseini, R. A deep learning method for near-real-time cloud and cloud shadow segmenta- tion from Gaofen-1 images. Comput. Intell. Neurosci. 2020 (2020).\n\nCloud detection in remote sensing images based on multiscale features-convolutional neural network. Z Shao, IEEE Trans. Geosci. Remote Sens. 57Shao, Z. et al. Cloud detection in remote sensing images based on multiscale features-convolutional neural network. IEEE Trans. Geosci. Remote Sens. 57, 1-15 (2019).\n\nA cloud detection method for Landsat 8 images based on. Z Yue, F Y Xie, Z G Jiang, PCANet. Remote Sens. 106877Yue, Z., Xie, F. Y. & Jiang, Z. G. A cloud detection method for Landsat 8 images based on PCANet. Remote Sens. 10(6), 877 (2018).\n\nCDUNet: Cloud detection UNet for remote sensing imagery. M Xia, Remote Sensing. 134533Xia, M. CDUNet: Cloud detection UNet for remote sensing imagery. Remote Sensing 13, 4533 (2021).\n\nA cloud and snow detection method of TH-1 image based on combined ResNet and DeeplabV3+. K Zheng, Acta Geodaetica et Cartographica Sinica. 491011Zheng, K. et al. A cloud and snow detection method of TH-1 image based on combined ResNet and DeeplabV3+. Acta Geodaetica et Cartographica Sinica 49(10), 11 (2020).\n\nCloud detection algorithm for multi-satellite remote sensing imagery based on a spectral library and 1D convolutional neural network. Y He, Remote Sens. 133319He, Y. Cloud detection algorithm for multi-satellite remote sensing imagery based on a spectral library and 1D convolutional neural network. Remote Sens. 13, 3319 (2021).\n\nCloud detection for Chinese high resolution remote sensing imagery using combining superpixel with convolution neural network. Q H Xu, Y B Huang, Y Chen, Bull. Surv. Mapp. 16Xu, Q. H., Huang, Y. B. & Chen, Y. Cloud detection for Chinese high resolution remote sensing imagery using combining superpixel with convolution neural network. Bull. Surv. Mapp. 1, 6 (2019).\n\nUNet 3+: A full-scale connected UNet for medical image segmentation. H Huang, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Huang, H. et al. UNet 3+: A full-scale connected UNet for medical image segmentation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 1055-1059 (2020).\n\nDeep Residual Learning for Image Recognition. K He, X Zhang, S Ren, IEEEHe, K., Zhang, X., Ren, S. et al. Deep Residual Learning for Image Recognition. (IEEE, 2016).\n\nS Woo, Convolutional Block Attention Module. SpringerWoo, S. et al. CBAM: Convolutional Block Attention Module 3-19 (Springer, 2018).\n\nNeural network-based detection of self-admitted technical debt: From performance to explainability. X Ren, ACM Trans. Softw. Eng. Methodol. 283Ren, X. et al. Neural network-based detection of self-admitted technical debt: From performance to explainability. ACM Trans. Softw. Eng. Methodol. 28(3), 1-45 (2019).\n\nSLIC superpixels compared to state-of-the-art superpixel methods. R Achanta, IEEE Trans. Pattern Anal. Mach. Intell. 3411Achanta, R. et al. SLIC superpixels compared to state-of-the-art superpixel methods. IEEE Trans. Pattern Anal. Mach. Intell. 34(11), 2274-2282 (2012).\n\nA graph partitioning algorithm based on SLIC superpixels. Y Zhao, J G Peng, Y Gao, Chin. J. Eng. Math. 033005Zhao, Y., Peng, J. G. & Gao, Y. A graph partitioning algorithm based on SLIC superpixels. Chin. J. Eng. Math. 033(005), 441-449 (2016).\n\nAutomatic differentiation in pytorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, Proceedings of the NIPS 2017 Workshop Autodiff Submission. the NIPS 2017 Workshop Autodiff SubmissionPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L. & Lerer, A. Automatic dif- ferentiation in pytorch. In: Proceedings of the NIPS 2017 Workshop Autodiff Submission (2017).\n\nCloud detection for remote sensing images using improved U-Net. Y H Zhang, Bull. Surv. Mapp. 35Zhang, Y. H. et al. Cloud detection for remote sensing images using improved U-Net. Bull. Surv. Mapp. 3, 5 (2020).\n", "annotations": {"author": "[{\"end\":211,\"start\":94},{\"end\":399,\"start\":212},{\"end\":513,\"start\":400},{\"end\":632,\"start\":514},{\"end\":640,\"start\":633},{\"end\":653,\"start\":641}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":221,\"start\":217},{\"end\":406,\"start\":404},{\"end\":525,\"start\":522},{\"end\":639,\"start\":637},{\"end\":652,\"start\":649}]", "author_first_name": "[{\"end\":100,\"start\":94},{\"end\":216,\"start\":212},{\"end\":403,\"start\":400},{\"end\":521,\"start\":514},{\"end\":636,\"start\":633},{\"end\":648,\"start\":641}]", "author_affiliation": "[{\"end\":210,\"start\":106},{\"end\":327,\"start\":223},{\"end\":398,\"start\":329},{\"end\":512,\"start\":408},{\"end\":631,\"start\":527}]", "title": "[{\"end\":75,\"start\":1},{\"end\":728,\"start\":654}]", "venue": "[{\"end\":750,\"start\":730}]", "abstract": "[{\"end\":5056,\"start\":844}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5470,\"start\":5467},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5472,\"start\":5470},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5551,\"start\":5548},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5553,\"start\":5551},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5854,\"start\":5852},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6033,\"start\":6031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6220,\"start\":6217},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6222,\"start\":6220},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7183,\"start\":7181},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19530,\"start\":19528},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20499,\"start\":20496},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22470,\"start\":22468},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26405,\"start\":26402},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26407,\"start\":26405},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28091,\"start\":28087},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32176,\"start\":32174}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37235,\"start\":37167},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37379,\"start\":37236},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37441,\"start\":37380},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37674,\"start\":37442},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37830,\"start\":37675},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37870,\"start\":37831},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38201,\"start\":37871},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38489,\"start\":38202},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38574,\"start\":38490},{\"attributes\":{\"id\":\"fig_9\"},\"end\":38666,\"start\":38575},{\"attributes\":{\"id\":\"fig_10\"},\"end\":38729,\"start\":38667},{\"attributes\":{\"id\":\"fig_11\"},\"end\":38873,\"start\":38730},{\"attributes\":{\"id\":\"fig_12\"},\"end\":39090,\"start\":38874},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39345,\"start\":39091},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39661,\"start\":39346},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39836,\"start\":39662},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39888,\"start\":39837}]", "paragraph": "[{\"end\":6743,\"start\":5058},{\"end\":10357,\"start\":6745},{\"end\":11554,\"start\":10359},{\"end\":14688,\"start\":11556},{\"end\":15485,\"start\":14690},{\"end\":16067,\"start\":15487},{\"end\":16614,\"start\":16083},{\"end\":17820,\"start\":16648},{\"end\":18087,\"start\":17822},{\"end\":19099,\"start\":18228},{\"end\":21105,\"start\":19101},{\"end\":22179,\"start\":21230},{\"end\":23411,\"start\":22181},{\"end\":23723,\"start\":23413},{\"end\":25355,\"start\":23725},{\"end\":27192,\"start\":25393},{\"end\":28433,\"start\":27250},{\"end\":28641,\"start\":28491},{\"end\":29900,\"start\":28643},{\"end\":30604,\"start\":29925},{\"end\":31327,\"start\":30606},{\"end\":36079,\"start\":31329},{\"end\":37023,\"start\":36094},{\"end\":37166,\"start\":37045}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18227,\"start\":18088},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21229,\"start\":21106},{\"attributes\":{\"id\":\"formula_2\"},\"end\":28490,\"start\":28434}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27866,\"start\":27859},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30133,\"start\":30126},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30507,\"start\":30500},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31201,\"start\":31194}]", "section_header": "[{\"end\":16081,\"start\":16070},{\"end\":16646,\"start\":16617},{\"end\":25391,\"start\":25358},{\"end\":27248,\"start\":27195},{\"end\":29923,\"start\":29903},{\"end\":36092,\"start\":36082},{\"end\":37043,\"start\":37026},{\"end\":37178,\"start\":37168},{\"end\":37247,\"start\":37237},{\"end\":37391,\"start\":37381},{\"end\":37453,\"start\":37443},{\"end\":37686,\"start\":37676},{\"end\":37842,\"start\":37832},{\"end\":37882,\"start\":37872},{\"end\":38219,\"start\":38203},{\"end\":38501,\"start\":38491},{\"end\":38587,\"start\":38576},{\"end\":38679,\"start\":38668},{\"end\":38742,\"start\":38731},{\"end\":38886,\"start\":38875},{\"end\":39356,\"start\":39347},{\"end\":39672,\"start\":39663},{\"end\":39847,\"start\":39838}]", "table": "[{\"end\":39345,\"start\":39194},{\"end\":39661,\"start\":39423},{\"end\":39888,\"start\":39880}]", "figure_caption": "[{\"end\":37235,\"start\":37180},{\"end\":37379,\"start\":37249},{\"end\":37441,\"start\":37393},{\"end\":37674,\"start\":37455},{\"end\":37830,\"start\":37688},{\"end\":37870,\"start\":37844},{\"end\":38201,\"start\":37884},{\"end\":38489,\"start\":38223},{\"end\":38574,\"start\":38503},{\"end\":38666,\"start\":38590},{\"end\":38729,\"start\":38682},{\"end\":38873,\"start\":38745},{\"end\":39090,\"start\":38889},{\"end\":39194,\"start\":39093},{\"end\":39423,\"start\":39358},{\"end\":39836,\"start\":39674},{\"end\":39880,\"start\":39849}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12737,\"start\":12731},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12748,\"start\":12739},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12794,\"start\":12787},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12851,\"start\":12844},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14835,\"start\":14829},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16217,\"start\":16211},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17819,\"start\":17813},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18402,\"start\":18396},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19579,\"start\":19573},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21425,\"start\":21419},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21694,\"start\":21688},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21897,\"start\":21878},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22046,\"start\":22039},{\"end\":26069,\"start\":26062},{\"end\":26488,\"start\":26481},{\"end\":26709,\"start\":26702},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":29780,\"start\":29774},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32433,\"start\":32426},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33858,\"start\":33851},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34121,\"start\":34111},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34531,\"start\":34521},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35039,\"start\":35029},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35422,\"start\":35412},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35849,\"start\":35839}]", "bib_author_first_name": "[{\"end\":41820,\"start\":41819},{\"end\":41827,\"start\":41826},{\"end\":41829,\"start\":41828},{\"end\":42088,\"start\":42087},{\"end\":42090,\"start\":42089},{\"end\":42369,\"start\":42368},{\"end\":42662,\"start\":42661},{\"end\":42664,\"start\":42663},{\"end\":42945,\"start\":42944},{\"end\":42951,\"start\":42950},{\"end\":43207,\"start\":43206},{\"end\":43473,\"start\":43472},{\"end\":43480,\"start\":43479},{\"end\":43487,\"start\":43486},{\"end\":43899,\"start\":43898},{\"end\":43907,\"start\":43906},{\"end\":43916,\"start\":43915},{\"end\":43918,\"start\":43917},{\"end\":44163,\"start\":44162},{\"end\":44171,\"start\":44170},{\"end\":44173,\"start\":44172},{\"end\":44182,\"start\":44181},{\"end\":44184,\"start\":44183},{\"end\":44421,\"start\":44420},{\"end\":44434,\"start\":44433},{\"end\":44436,\"start\":44435},{\"end\":44447,\"start\":44446},{\"end\":44759,\"start\":44758},{\"end\":44768,\"start\":44767},{\"end\":44770,\"start\":44769},{\"end\":44776,\"start\":44775},{\"end\":45033,\"start\":45032},{\"end\":45035,\"start\":45034},{\"end\":45041,\"start\":45040},{\"end\":45043,\"start\":45042},{\"end\":45051,\"start\":45050},{\"end\":45053,\"start\":45052},{\"end\":45409,\"start\":45408},{\"end\":45411,\"start\":45410},{\"end\":45774,\"start\":45773},{\"end\":45781,\"start\":45780},{\"end\":45788,\"start\":45787},{\"end\":46107,\"start\":46106},{\"end\":46109,\"start\":46108},{\"end\":46319,\"start\":46318},{\"end\":46328,\"start\":46327},{\"end\":46337,\"start\":46336},{\"end\":46735,\"start\":46734},{\"end\":47152,\"start\":47151},{\"end\":47159,\"start\":47158},{\"end\":47165,\"start\":47164},{\"end\":47449,\"start\":47448},{\"end\":47690,\"start\":47689},{\"end\":48013,\"start\":48012},{\"end\":48037,\"start\":48036},{\"end\":48371,\"start\":48370},{\"end\":48637,\"start\":48636},{\"end\":48644,\"start\":48643},{\"end\":48646,\"start\":48645},{\"end\":48653,\"start\":48652},{\"end\":48655,\"start\":48654},{\"end\":48879,\"start\":48878},{\"end\":49095,\"start\":49094},{\"end\":49451,\"start\":49450},{\"end\":49775,\"start\":49774},{\"end\":49777,\"start\":49776},{\"end\":49783,\"start\":49782},{\"end\":49785,\"start\":49784},{\"end\":49794,\"start\":49793},{\"end\":50085,\"start\":50084},{\"end\":50447,\"start\":50446},{\"end\":50453,\"start\":50452},{\"end\":50462,\"start\":50461},{\"end\":50568,\"start\":50567},{\"end\":50803,\"start\":50802},{\"end\":51081,\"start\":51080},{\"end\":51346,\"start\":51345},{\"end\":51354,\"start\":51353},{\"end\":51356,\"start\":51355},{\"end\":51364,\"start\":51363},{\"end\":51572,\"start\":51571},{\"end\":51582,\"start\":51581},{\"end\":51591,\"start\":51590},{\"end\":51603,\"start\":51602},{\"end\":51613,\"start\":51612},{\"end\":51621,\"start\":51620},{\"end\":51631,\"start\":51630},{\"end\":51638,\"start\":51637},{\"end\":51651,\"start\":51650},{\"end\":51661,\"start\":51660},{\"end\":52064,\"start\":52063},{\"end\":52066,\"start\":52065}]", "bib_author_last_name": "[{\"end\":41824,\"start\":41821},{\"end\":41838,\"start\":41830},{\"end\":42096,\"start\":42091},{\"end\":42373,\"start\":42370},{\"end\":42670,\"start\":42665},{\"end\":42948,\"start\":42946},{\"end\":42954,\"start\":42952},{\"end\":43215,\"start\":43208},{\"end\":43477,\"start\":43474},{\"end\":43484,\"start\":43481},{\"end\":43490,\"start\":43488},{\"end\":43904,\"start\":43900},{\"end\":43913,\"start\":43908},{\"end\":43922,\"start\":43919},{\"end\":44168,\"start\":44164},{\"end\":44179,\"start\":44174},{\"end\":44189,\"start\":44185},{\"end\":44431,\"start\":44422},{\"end\":44444,\"start\":44437},{\"end\":44458,\"start\":44448},{\"end\":44765,\"start\":44760},{\"end\":44773,\"start\":44771},{\"end\":44781,\"start\":44777},{\"end\":45038,\"start\":45036},{\"end\":45048,\"start\":45044},{\"end\":45059,\"start\":45054},{\"end\":45420,\"start\":45412},{\"end\":45778,\"start\":45775},{\"end\":45785,\"start\":45782},{\"end\":45791,\"start\":45789},{\"end\":46115,\"start\":46110},{\"end\":46325,\"start\":46320},{\"end\":46334,\"start\":46329},{\"end\":46344,\"start\":46338},{\"end\":46739,\"start\":46736},{\"end\":47156,\"start\":47153},{\"end\":47162,\"start\":47160},{\"end\":47175,\"start\":47166},{\"end\":47454,\"start\":47450},{\"end\":47694,\"start\":47691},{\"end\":48034,\"start\":48014},{\"end\":48051,\"start\":48038},{\"end\":48376,\"start\":48372},{\"end\":48641,\"start\":48638},{\"end\":48650,\"start\":48647},{\"end\":48661,\"start\":48656},{\"end\":48883,\"start\":48880},{\"end\":49101,\"start\":49096},{\"end\":49454,\"start\":49452},{\"end\":49780,\"start\":49778},{\"end\":49791,\"start\":49786},{\"end\":49799,\"start\":49795},{\"end\":50091,\"start\":50086},{\"end\":50450,\"start\":50448},{\"end\":50459,\"start\":50454},{\"end\":50466,\"start\":50463},{\"end\":50572,\"start\":50569},{\"end\":50807,\"start\":50804},{\"end\":51089,\"start\":51082},{\"end\":51351,\"start\":51347},{\"end\":51361,\"start\":51357},{\"end\":51368,\"start\":51365},{\"end\":51579,\"start\":51573},{\"end\":51588,\"start\":51583},{\"end\":51600,\"start\":51592},{\"end\":51610,\"start\":51604},{\"end\":51618,\"start\":51614},{\"end\":51628,\"start\":51622},{\"end\":51635,\"start\":51632},{\"end\":51648,\"start\":51639},{\"end\":51658,\"start\":51652},{\"end\":51667,\"start\":51662},{\"end\":52072,\"start\":52067}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":128475301},\"end\":42083,\"start\":41669},{\"attributes\":{\"id\":\"b1\"},\"end\":42278,\"start\":42085},{\"attributes\":{\"id\":\"b2\"},\"end\":42534,\"start\":42280},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":129367376},\"end\":42887,\"start\":42536},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":140145610},\"end\":43129,\"start\":42889},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":129648233},\"end\":43387,\"start\":43131},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":62580492},\"end\":43827,\"start\":43389},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":63933731},\"end\":44099,\"start\":43829},{\"attributes\":{\"id\":\"b8\"},\"end\":44337,\"start\":44101},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":129038270},\"end\":44673,\"start\":44339},{\"attributes\":{\"id\":\"b10\"},\"end\":44971,\"start\":44675},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":63943847},\"end\":45272,\"start\":44973},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":129762270},\"end\":45678,\"start\":45274},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":197582301},\"end\":45984,\"start\":45680},{\"attributes\":{\"id\":\"b14\"},\"end\":46280,\"start\":45986},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":39635412},\"end\":46561,\"start\":46282},{\"attributes\":{\"id\":\"b16\"},\"end\":46928,\"start\":46563},{\"attributes\":{\"doi\":\"10.1038/s41598-022-18812-6\",\"id\":\"b17\"},\"end\":47063,\"start\":46930},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7131103},\"end\":47366,\"start\":47065},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":126314696},\"end\":47611,\"start\":47368},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":42431268},\"end\":47910,\"start\":47613},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":226290493},\"end\":48268,\"start\":47912},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":127416559},\"end\":48578,\"start\":48270},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":51957333},\"end\":48819,\"start\":48580},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":244085587},\"end\":49003,\"start\":48821},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":234615762},\"end\":49314,\"start\":49005},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":238725447},\"end\":49645,\"start\":49316},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":216685944},\"end\":50013,\"start\":49647},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":215828394},\"end\":50398,\"start\":50015},{\"attributes\":{\"id\":\"b29\"},\"end\":50565,\"start\":50400},{\"attributes\":{\"id\":\"b30\"},\"end\":50700,\"start\":50567},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":247500985},\"end\":51012,\"start\":50702},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1806278},\"end\":51285,\"start\":51014},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":67428146},\"end\":51531,\"start\":51287},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":40027675},\"end\":51997,\"start\":51533},{\"attributes\":{\"id\":\"b35\"},\"end\":52208,\"start\":51999}]", "bib_title": "[{\"end\":41817,\"start\":41669},{\"end\":42366,\"start\":42280},{\"end\":42659,\"start\":42536},{\"end\":42942,\"start\":42889},{\"end\":43204,\"start\":43131},{\"end\":43470,\"start\":43389},{\"end\":43896,\"start\":43829},{\"end\":44160,\"start\":44101},{\"end\":44418,\"start\":44339},{\"end\":44756,\"start\":44675},{\"end\":45030,\"start\":44973},{\"end\":45406,\"start\":45274},{\"end\":45771,\"start\":45680},{\"end\":46316,\"start\":46282},{\"end\":47149,\"start\":47065},{\"end\":47446,\"start\":47368},{\"end\":47687,\"start\":47613},{\"end\":48010,\"start\":47912},{\"end\":48368,\"start\":48270},{\"end\":48634,\"start\":48580},{\"end\":48876,\"start\":48821},{\"end\":49092,\"start\":49005},{\"end\":49448,\"start\":49316},{\"end\":49772,\"start\":49647},{\"end\":50082,\"start\":50015},{\"end\":50800,\"start\":50702},{\"end\":51078,\"start\":51014},{\"end\":51343,\"start\":51287},{\"end\":51569,\"start\":51533},{\"end\":52061,\"start\":51999}]", "bib_author": "[{\"end\":41826,\"start\":41819},{\"end\":41840,\"start\":41826},{\"end\":42098,\"start\":42087},{\"end\":42375,\"start\":42368},{\"end\":42672,\"start\":42661},{\"end\":42950,\"start\":42944},{\"end\":42956,\"start\":42950},{\"end\":43217,\"start\":43206},{\"end\":43479,\"start\":43472},{\"end\":43486,\"start\":43479},{\"end\":43492,\"start\":43486},{\"end\":43906,\"start\":43898},{\"end\":43915,\"start\":43906},{\"end\":43924,\"start\":43915},{\"end\":44170,\"start\":44162},{\"end\":44181,\"start\":44170},{\"end\":44191,\"start\":44181},{\"end\":44433,\"start\":44420},{\"end\":44446,\"start\":44433},{\"end\":44460,\"start\":44446},{\"end\":44767,\"start\":44758},{\"end\":44775,\"start\":44767},{\"end\":44783,\"start\":44775},{\"end\":45040,\"start\":45032},{\"end\":45050,\"start\":45040},{\"end\":45061,\"start\":45050},{\"end\":45422,\"start\":45408},{\"end\":45780,\"start\":45773},{\"end\":45787,\"start\":45780},{\"end\":45793,\"start\":45787},{\"end\":46117,\"start\":46106},{\"end\":46327,\"start\":46318},{\"end\":46336,\"start\":46327},{\"end\":46346,\"start\":46336},{\"end\":46741,\"start\":46734},{\"end\":47158,\"start\":47151},{\"end\":47164,\"start\":47158},{\"end\":47177,\"start\":47164},{\"end\":47456,\"start\":47448},{\"end\":47696,\"start\":47689},{\"end\":48036,\"start\":48012},{\"end\":48053,\"start\":48036},{\"end\":48378,\"start\":48370},{\"end\":48643,\"start\":48636},{\"end\":48652,\"start\":48643},{\"end\":48663,\"start\":48652},{\"end\":48885,\"start\":48878},{\"end\":49103,\"start\":49094},{\"end\":49456,\"start\":49450},{\"end\":49782,\"start\":49774},{\"end\":49793,\"start\":49782},{\"end\":49801,\"start\":49793},{\"end\":50093,\"start\":50084},{\"end\":50452,\"start\":50446},{\"end\":50461,\"start\":50452},{\"end\":50468,\"start\":50461},{\"end\":50574,\"start\":50567},{\"end\":50809,\"start\":50802},{\"end\":51091,\"start\":51080},{\"end\":51353,\"start\":51345},{\"end\":51363,\"start\":51353},{\"end\":51370,\"start\":51363},{\"end\":51581,\"start\":51571},{\"end\":51590,\"start\":51581},{\"end\":51602,\"start\":51590},{\"end\":51612,\"start\":51602},{\"end\":51620,\"start\":51612},{\"end\":51630,\"start\":51620},{\"end\":51637,\"start\":51630},{\"end\":51650,\"start\":51637},{\"end\":51660,\"start\":51650},{\"end\":51669,\"start\":51660},{\"end\":52074,\"start\":52063}]", "bib_venue": "[{\"end\":43617,\"start\":43563},{\"end\":51770,\"start\":51728},{\"end\":41860,\"start\":41840},{\"end\":42168,\"start\":42098},{\"end\":42390,\"start\":42375},{\"end\":42694,\"start\":42672},{\"end\":42985,\"start\":42956},{\"end\":43241,\"start\":43217},{\"end\":43561,\"start\":43492},{\"end\":43952,\"start\":43924},{\"end\":44205,\"start\":44191},{\"end\":44489,\"start\":44460},{\"end\":44805,\"start\":44783},{\"end\":45103,\"start\":45061},{\"end\":45458,\"start\":45422},{\"end\":45813,\"start\":45793},{\"end\":46104,\"start\":45986},{\"end\":46402,\"start\":46346},{\"end\":46732,\"start\":46563},{\"end\":46978,\"start\":46958},{\"end\":47195,\"start\":47177},{\"end\":47474,\"start\":47456},{\"end\":47742,\"start\":47696},{\"end\":48077,\"start\":48053},{\"end\":48409,\"start\":48378},{\"end\":48682,\"start\":48663},{\"end\":48899,\"start\":48885},{\"end\":49142,\"start\":49103},{\"end\":49467,\"start\":49456},{\"end\":49817,\"start\":49801},{\"end\":50191,\"start\":50093},{\"end\":50444,\"start\":50400},{\"end\":50610,\"start\":50574},{\"end\":50840,\"start\":50809},{\"end\":51129,\"start\":51091},{\"end\":51388,\"start\":51370},{\"end\":51726,\"start\":51669},{\"end\":52090,\"start\":52074}]"}}}, "year": 2023, "month": 12, "day": 17}