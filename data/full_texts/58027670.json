{"id": 58027670, "updated": "2022-02-27 04:57:56.185", "metadata": {"title": "FACH: FPGA-based acceleration of hyperdimensional computing by reducing computational complexity", "authors": "[{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Sahand\",\"last\":\"Salamat\",\"middle\":[]},{\"first\":\"Saransh\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Jiani\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 24th Asia and South Pacific Design Automation Conference", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Brain-inspired hyperdimensional (HD) computing explores computing with hypervectors for the emulation of cognition as an alternative to computing with numbers. In HD, input symbols are mapped to a hypervector and an associative search is performed for reasoning and classification. An associative memory, which finds the closest match between a set of learned hypervectors and a query hypervector, uses simple Hamming distance metric for similarity check. However, we observe that, in order to provide acceptable classification accuracy HD needs to store non-binarized model in associative memory and uses costly similarity metrics such as cosine to perform a reasoning task. This makes the HD computationally expensive when it is used for realistic classification problems. In this paper, we propose a FPGA-based acceleration of HD (FACH) which significantly improves the computation efficiency by removing majority of multiplications during the reasoning task. FACH identifies representative values in each class hypervector using clustering algorithm. Then, it creates a new HD model with hardware-friendly operations, and accordingly propose an FPGA-based implementation to accelerate such tasks. Our evaluations on several classification problems show that FACH can provide 5.9X energy efficiency improvement and 5.1X speedup as compared to baseline FPGA-based implementation, while ensuring the same quality of classification.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2908729710", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aspdac/ImaniSGHR19", "doi": "10.1145/3287624.3287667"}}, "content": {"source": {"pdf_hash": "df9149e6be457bd5730e8ace917fe94c3804ac4c", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3287624.3287667", "status": "BRONZE"}}, "grobid": {"id": "53d59c573624bfee82b6ede649f5a1da09613e40", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/df9149e6be457bd5730e8ace917fe94c3804ac4c.txt", "contents": "\nFACH: FPGA-based Acceleration of Hyperdimensional Computing by Reducing Computational Complexity\n\n\nMohsen Imani moimani@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa Jolla92093CAUSA\n\nSahand Salamat ssalama@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa Jolla92093CAUSA\n\nSaransh Gupta sgupta@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa Jolla92093CAUSA\n\nJiani Huang \nComputer Science and Engineering\nUC San Diego\nLa Jolla92093CAUSA\n\nTajana Rosing tajana@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa Jolla92093CAUSA\n\nFACH: FPGA-based Acceleration of Hyperdimensional Computing by Reducing Computational Complexity\n10.1145/3287624.3287667ACM Reference Format: Mohsen Imani, Sahand Salamat, Saransh Gupta, Jiani Huang, and Tajana Rosing. 2019. FACH: FPGA-based Acceleration of Hyperdimensional Com-puting by Reducing Computational Complexity. In ASPDAC '19: 24th Asia and South Pacific Design Automation Conference (ASPDAC '19), January 21-24, 2019, Tokyo, Japan. ACM, New York, NY, USA, 6 pages. https://doi.CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning approachesSupervised learningKEYWORDS Brain-inspired computing, Hyperdimensional computing, Machine learning, Energy efficiency\nBrain-inspired hyperdimensional (HD) computing explores computing with hypervectors for the emulation of cognition as an alternative to computing with numbers. In HD, input symbols are mapped to a hypervector and an assoc iative search is performed for reasoning and classification. An associative memory, which finds the closest match between a set of learned hypervectors and a query hypervector, uses simple Hamming distance metric for similarity check. However, we observe that, in order to provide acceptable classification accuracy HD needs to store non-binarized model in associative memory and uses costly similarity metrics such as cosine to perform a reasoning task. This makes the HD computationally expensive when it is used for realistic classification problems. In this paper, we propose a FPGA-based acceleration of HD (FACH) which significantly improves the computation efficiency by removing majority of multiplications during the reasoning task. FACH identifies representative values in each class hypervector using clustering algorithm. Then, it creates a new HD model with hardware-friendly operations, and accordingly propose an FPGA-based implementation to accelerate such tasks. Our evaluations on several classification problems show that FACH can provide 5.9\u00d7 energy efficiency improvement and 5.1\u00d7 speedup as compared to baseline FPGA-based implementation, while ensuring the same quality of classification.\n\nINTRODUCTION\n\nMachine learning algorithms have shown promising accuracy in many tasks including computer vision, voice recognition, natural Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPDAC '19,  language processing, and health care [1][2][3][4][5][6]. However, the existing machine learning algorithms such as deep neural networks are computationally expensive and require enormous resources to be executed [7][8][9]. From other hands, embedded devices are often constrained in terms of available processing resources and power budget. Brain-inspired hyperdimensional (HD) computing is a computational paradigm performing energy-efficient cognitive computation. HD works based on the patterns of neural activity that are not readily associated with numbers [10]. Due to the very large size of brain's circuits, such neural activity patterns can only be modeled with vectors in high-dimensional space, called hypervectors. HD computing builds upon a well-defined set of operations with random HD vectors and it is extremely robust in the presence of failures. HD offers a complete computational paradigm that is easily applied to learning problems including: analogy-based reasoning [11], latent semantic analysis [12], language recognition [13,14], prediction from multimodal sensor fusion [15], speech recognition [16,17], activity recognition [18], DNA sequencing [19], and clustering [20].\n\nHD computing is about manipulating and comparing large patterns, stored in memory as hypervectors. In contrast to existing classification algorithms, such as neural networks, which require significantly complex and costly computation during training and inference [21], HD provides a memory-centric, hardware friendly operations which can be process on light-weight embedded devices. Figure 1 shows the overview of the HD functionality in training and inference phases. In HD training, each input is mapped to a hypervector and then hypervectors for multiple inputs are combined to create class hypervectors. In inference, an associative memory checks the similarity of the input query hypervector with all prestored class hypervectors. For a simple classification task such as language or text classification, HD can use binarized class hypervectors (0 and 1) and simple Hamming distance for similarity check.\n\nIn this work, we show that in order to achieve acceptable accuracy on realistic classification problems (i.e., speech, activity, or face recognition), HD has to use class hypervectors with non-binary elements, which means that HD needs to use cosine metric to find the similarity between query and class hypervectors. The cosine can be calculated using the dot product of an input hypervector with all stored class hypervectors which involves a large number of multiplication/addition operations. This makes running HD on the general purpose processors slow and energy hungry.\n\nIn this paper, we propose a FPGA-based ACceleration of HD (FACH) which significantly reduces the computational cost by removing the majority of the multiplications. FACH employs a clustering algorithm in order to share the values in each class hypervector by taking into account the statistical properties of each operand and output within the HD class. Instead of multiplying all pairs of the query and a class hypervector, FACH adds all query elements which are going to multiply with a shared class element and finally multiplies the result of addition with the corresponding class value. This significantly accelerates HD computation by reducing the number of required multiplications. Based on this technique, we create a new HD model with hardware-friendly operations, and accordingly propose an FPGA-based acceleration for such tasks. Our evaluations on several classification problems show that FACH can provide 5.9\u00d7 energy efficiency improvement and 5.1\u00d7 speedup as compared to baseline FPGA-based implementation while ensuring the same quality of classification.\n\n\nHD COMPUTING ALGORITHM 2.1 FACH Overview\n\nHD provides a general model of computing which can apply to different type of learning problems. Classification is one of the most important supervised learning algorithm. To perform classification, HD uses two main modules: encoding and associative memory. Figure 1 shows the overall structure of the HD classification in both training and inference phases. Encoding module maps input data to a vector in high dimensional space, called hypervector. Training performs on hypervectors, by adding all hypervectors corresponding to a particular class together. In a similar way, HD creates a single hypervector for each existing class. These class hypervectors store in an associative memory. In inference, HD uses the same encoding scheme to map a query data to high-dimensional space. Finally, the associative memory performs the reasoning task by looking for a class hypervector which has the highest similarity to input hypervector. In the following, we briefly explain how each module works.\n\n\nEncoding Module\n\nThe first step in HD computing is to encode input data to highdimensional vectors. The main goal of encoding module is to map input data to hypervector with D dimensions (e.g. D = 10, 000), while keeping all information of a data point in the original space, e.g., the feature values and their indexes for feature vector. Input data can have different representations, thus there are different encoding modules to map data to high dimensional space. For example, work in [16,22] proposed encoding methods to map feature vectors to high dimensional space. Work in [23] encodes text-like data using the idea of random indexing.\n\n\nHD Model Training\n\nHD performs the training procedure on the encoded hypervectors. For all data corresponding to a particular class, HD adds all hypervectors element-wise to create a class hypervector. For example, assume Q i = {q i 1 , q i 2 , . . . , q i D } is a hypervector belongs to a class i th . The HD model can be generated by adding all hypervectors with the same tag as Each element of the class hypervector can have non-binarized value (w \u2208 N D ). This significantly increases the inference cost, as the rest of reasoning task, i.e., similarity check, needs to perform using integer rather than binary values. To reduce the computational cost, several prior works tried to binarize the class elements after training by applying a majority function on each dimension [23,24]. However, this reduces the amount of information stored in each class hypervector. Later in this section, we discuss the accuracy-efficiency trade off when using binarized or non-binarzied class hypervectors for classification.\nC i = j Q i j , where C i = {w i 1 , w i 2 , . . . ,w i D }.\n\nAssociative Memory Module\n\nAfter training, all class hypervectors are stored in an associative memory (shown in Figure 1). In inference, an input data encodes a query hypervector using the same encoding module used for training. The associative memory is responsible for comparing the similarity of the input query hypervector with all stored class hypervectors and selecting a class with the highest similarity. For all classification problems, HD uses the same associative search to perform the reasoning task, regardless of the encoding module. Associative memory can use different similarity metrics to find a class which has the most similarity to a query hypervector. For class hypervectors with binarized elements, Hamming distance is a inexpensive and suitable similarity metric, while class hypervectors with non-binarized elements need to use cosine for similarity check. Most existing HD computing techniques are using binarized class hypervectors in order to eliminate the costly cosine metric [17,24]. However, we observed that HD with binary model provides significantly low classification accuracy as compared to non-binary model. For example, for face recognition, HD using non-binarized class elements provides 57.8% higher accuracy than HD using binarized hypervectors. From other hand, HD with non-binary model involves large amount of multiplications. For example, an application with k class hypervector and D dimensions involve k \u00d7 D multiplications.\n\n\nFACH FRAMEWORK 3.1 Overview\n\nIn this section, we propose a FPGA-based acceleration of HD (FACH), which exploits the statistical characteristic of the hyperdimensional computing in order to reduce the HD computational complexity. Figure 2a shows the overview of the FACH framework consisting of three main steps: training, model refinement, and inference. As we explained in Section 2.3, HD encodes all data points to hypervectors   \n\n\nReduction in Multiplication Domain\n\nPerforming cosine similarity between two vectors involves calculating the dot product of vectors divided by the size of each vector.\n\nSince HD trains the model offline, the normalization of the class hypervectors can be performed offline. On other hand, input data is common between all class hypervectors, thus it does not need to be normalized. Therefore, cosine similarity between a query Q = {q 1 , q 2 , . . . , q D } and i th class hypervector, C i = {w i 1 , w i 2 , . . . ,w i D }, requires calculating their dot product which involves D additions and D multiplications, where D is the dimension of the hypervectors.\n\nIn this work, model refinement in FACH reduces the class span by carefully selecting a subset from the input spaces, called \"best representatives\". FACH limits the number of values that each class element can take (i.e., {w 1 , . . . ,w D } \u2208 {c 1 , . . . , c k } and k << D)). This enables us to remove the majority of cosine multiplications by factorization. In other words, instead of multiplying the D elements of query and class hypervector, we add the input data for all dimensions for which class hypervector has the same element. Finally, the result of addition is multiplied by the value of that particular class.\n\nHere we explain how FACH can limit the number of each class elements with no or minor impact of classification accuracy. To find representative class elements, the clustering algorithm is applied on the the pre-trained class hypervectors. For each class hypervector, our design identifies a specified number of clusters, say k, based on clustering algorithm. The centroids of clusters are selected as the representative weights and stored into the weight table. Assuming that the actual numerical values belong to a set \u03b8 , the objective  N ).\n{w i 1 , w i 2 , . . . ,w i D } \u2208 {c i 1 , c i 2 , .\n. . , c i k } Formally, the objective is to reduce the Within Cluster Sum of Squares (WCSS):\nmin c 1 , c 2 , ...,c k (W CSS = k j=1 \u03b8 i \u2208c j ||\u03b8 i \u2212 c j || 2 )(1)\nwhere \u03b8 i is the i th sample drawn from \u03b8 and k is the number of clusters.\n\nWe use the k-means clustering algorithm to solve the minimization objective for each HD class hypervector separately, as the distribution of values can vary across different classes. The calculation of dot product between query, Q, and a class hypervector, C i , can be simplified by adding all query elements which belong to the same cluster in class hypervector. For example, for class dimensions with c k elements, our design adds all corresponding query elements together (s k = j = q j where w j = c k ). In a similar way, our design calculates the accumulative query elements on all k cluster centroids: {s 1 , s 2 , . . . , s k } and s \u2208 N . Finally, these values multiply with each corresponding cluster values and accumulate together to generate a dot product between Q and C i hypervectors.\nQ.C i = s 1 \u00d7 c 1 + s 2 \u00d7 c 2 , + . . . s k \u00d7 c k\nThis method reduces the number of multiplications involved in dot product from D to k, where k can be about three order of magnitudes smaller than D. Figure 3 shows an example of the dot product between a class and a query vector using conventional method and clustered model. Since in conventional method the class elements can take any value, the dot product involve six multiplications (Figure 3a). FACH exploits the advantage of clustered class values in order to first add the query elements corresponding to the same centroid and then multiply the result with the centroid values (Figure 3b). This reduces the number of multiplications to two.\n\nError Estimation Sharing the elements of input and class hypervectors reduces the HD classification accuracy. After the training, our design replaces the elements of the class hypervectors with the closest representative values (cluster centroids). We estimate the error rate of the new model by cross-validating the cluster HD on a validation data, which is a part of the training data. The quality loss, \u0394E is defined as the error rate difference between the HD using original and modified models (\u0394E = E clust er ed \u2212 E or i\u0434inal ).\n\nModel Adjustment If the error rate does not satisfy the tolerance \u0394E < \u03f5, FACH adjusts the new model by retraining the network over the same training dataset. In retraining process, HD composer looks at the similarity of each input hypervector to all \nC i = C i \u2212Q & C j = C j +Q)\n. After adjusting the model over the training data, HD refinement again clusters the data in each class hypervector and estimate the classification error rate. We expect the model retrained under the modified condition to better fit with the clustered values. If an error criterion is not satisfied, we follow the same procedure until an error rate, \u03f5, is satisfied or we reach to a pre-specified number of iterations. After the iterations, the new model, which is compatible with the proposed accelerator, is used for real-time inference. Figure 4a shows the classification accuracy of applications during different retraining iterations when the class elements are clustered to 32 values. Our evaluation shows that HD refinement can compensate the quality loss due to clustering by using less than 50 iterations. All pre-processing operations in the HD refinement module are performed offline and their overhead is amortized among all future executions of FACH accelerator. Figure 4b shows the final quality loss, \u0394E, when FACH clusters the class hypervector to different different number of centroids. We consider the cluster sizes of 4, 8, 16 and 32. The results show that different applications can provide \u0394E = 0% while using different number of class clusters. For example, face recognition can achieve \u0394E = 0% when the class elements are clustered to 16 centroids, while human activity recognition (UCIHAR) achieves \u0394E = 0% using 32 cluster centroids. In Section 5, we will explain the accuracy-efficiency trade-off in FACH using different clusters.\n\n\nHD HARDWARE ACCELERATION\n\nIn this work, we implement baseline HD and FACH on a FPGA. In the following, we explain how each design can be accelerated on FPGA.\n\n\nBaseline HD Acceleration\n\nWe use FPGA to accelerate HD computing inference. Figure 5A shows that the FPGA-based implementation of the baseline HD requires D parallel multiplications to calculate the dot product between the query and class hypervectors. Then, the results of all D multiplications accumulate in a tree-based adder. However,   Figure 5B, stores all class indices which have the value as c 1 . Since each class has D dimensions, we require lo\u0434 2 D bits to store each index.\n\n\nFACH Acceleration\n\nDue to resource limitation in FPGA, we can only read d dimensions of the query hypervector at a time and process the remaining dimensions in sequential windows. However, sequentially accessing the query elements increases the number of resource requirements, since all d elements in a read window might belong to any of the clusters. In this case, each index buffer requires a tree-based adder with d inputs in order to take care of the worse case scenario, when all d query dimensions correspond to a single cluster. Instead, in this work, each read window accesses to b = d/k indices from each index buffer. This method ensures that the number of required resources to add the element of each index buffer is less than b. We define this b window size as the batch size. In order to speedup the computation, FACH stores the index buffers, which are actually a compressed/trained HD model, inside the FPGA. These buffers are implemented using distributed memory using LookUp Table (LUT) and Flip-Flop (FF) blocks.\n\nEach element of the index buffer points to one dimension of the query hypervector. In order to maximize the FPGA resource utilization, for all elements of index buffer in a batch windows, FACH pre-fetches the query elements and store them in query buffers ( \u2022 C ). Next to each query buffer, a tree-based adder accumulates all of these additions are stored in registers. Next, FPGA processes the next batch sequentially. FACH is implemented in a pipeline, where the pre-fetching of the elements to query buffer performs simultaneously with the addition of the query elements which have been pre-fetched to query buffers in last iteration. This pipeline can perform very efficiently since these two tasks require different types of FPGA resources. The indexing and pre-fetching are memory intensive tasks and mostly utilize BRAM, LUTs and FFs, while the addition of query elements mostly utilizes DSPs. After every iteration, the values corresponding to the registers are accumulated. Once FACH has processed all D dimensions of the hypervector, each register has the accumulated query elements in all the dimensions for which class hypervector has the same clustered value. For each index buffer, our design multiplies the value of the register with the corresponding cluster value. The results of multiplication for all cluster centroids are then accumulated in order to generate the final dot product ( \u2022 E ). Regardless of the method used for calculating dot product, our design needs to compare the dot products for all existing classes and select the class which has the maximum similarity with the input vector.\n\n\nRESULTS\n\n\nExperimental Setup\n\nThe proposed FACH has been implemented with software and hardware modules. For software support, we exploit Scikit-learn library [25] for clustering and C++ software implementation for the HD model training and verification. For hardware support, we use FPGA to accelerate HD computation. We fully implemented FACH inference functionality using Verilog. We verified the functionality of the design using both synthesis and real implementation of the FACH on Xilinx Vivado Design Suite [26]. The synthesis code was implemented on the Kintex-7 FPGA KC705 Evaluation Kit.\n\n\nWorkloads\n\nWe evaluate the efficiency of the proposed FACH on four popular classification applications, as listed below: Speech Recognition (ISOLET): The goal is to recognize voice audio of the 26 letters of the English alphabet. The training and testing datasets are taken from Isolet dataset [27]. Face Recognition (FACE): We exploit Caltech dataset of 10,000 web faces [28]. Negative training images, i.e., non-face images, are selected from CIFAR-100 and Pascal VOS 2012 datasets [29]. Activity Recognition (UCIHAR) [30]: The dataset includes signals collected from motion sensors for 8 subjects performing 19 different activities. The objective is to recognize the class of human activities. Physical Activity Monitoring (PAMPA) [31]: This dataset includes logs of 8 users and three 3D accelerometers positioned on arm, chest and ankle. They were collected over different human activities such as lying, walking and, ascending stairs, and each of them was corresponded to an activity ID. The goal is to recognize 12 different activities. FACH performance depends on the number of shared class elements. FACH with more number of centroids requires more FPGA resources and thus consumes higher power. However, it improves the performance of FACH by increasing the parallelism. For example, increasing the number of centroids from 4 to 32 improves the FACH performance by 2.12\u00d7 while utilizes on average 2.08\u00d7 more resources.FPGA performance does not improve linearly for a model with larger than 16 shared class elements. This is because larger FACH models, for example a model with 32 shared elements, does not fit on FPGA, therefore FPGA processes the FACH sequentially. As we discussed in section 3.1, FACH accuracy depends on the number of shared class elements. The more is the number of shared elements, the higher the accuracy FACH can provide. Our evaluations show that FACH on average can achieve 5.9\u00d7 better energy efficiency and 5.1\u00d7 faster execution as compared to the baseline FPGA-based HD implementation while providing the same quality of classification. Similarly, accepting less than 1% quality loss, FACH can provide 6.5\u00d7 energy efficiency improvement and 4.9\u00d7 speedup as compared to baseline FPGA-based implementation of HD. Figure 7 shows the performance per average utilization for FPGA while running applications with different number of clustered centroids. Using this metric, we observe that although FACH with a larger number of cluster centroids has higher performance, performance per resource utilization is higher for FACH using less number of centroids. In other words, the FPGA can better utilize the resources while running FACH with a smaller number of shared  class elements. Thus, we can maximize the FPGA efficiency by using the minimum number of cluster centroids which provide the acceptable quality of loss. Figure 8 shows the breakdown of the FPGA resources while implementing the baseline HD and FACH for UCIHAR with 8 cluster centroids. For the baseline HD, FPGA uses several multiplications to measure the similarity of a query and class hypervector. This increases the DSP utilization to 94.4% while LUT and BRAM have only 14.7% and 13.1% utilization. In fact, in this implementation the computation performance is limited by the number of DSPs available on the chip. In contrast, FACH with shared class elements can better utilize the FPGA resources by significantly reducing the number of required multiplications. FACH implementation uses DSPs in order to add the query elements which have been pre-fetched to query buffer. Although this increases the BRAM utilization, it allows FPGA to access the query values at a much faster rate in order to fully utilize the DSPs. In addition, FACH exploits the distributed memories, designed by LUT and FF in order to store the index buffer. This increases the utilization of LUT and FF to 51.4% and 6.9% respectively. Figure 9 also shows the power breakdown of FPGA while implementing baseline HD and proposed FACH. The results show that for baseline HD implementation, DSP takes 45.4% of total power consumption, while BRAM and logic together take around 18.1% of the power. In contrast, FACH implementation requires higher BRAM utilization which increases the contribution of BRAM to total power to 31.4%. Moreover, in FACH implementation, clock takes 38.0% of total power, mostly to implement distributed memory to store index buffer and perform pre-fetching.\n\n\nAccuracy-Efficiency Trade-off\n\n\nUtilization/Power Breakdown\n\n\nCONCLUSION\n\nWe propose a novel hyperdimensional computing framework, called FACH, which significantly reduces the cost of classification. The framework extracts representative operands of a trained HD model using clustering algorithm. At runtime, instead of multiplying all inputs and class elements, our design adds all the inputs belonging to the same class cluster centroid, and multiplies the result once in the end. Our evaluation over a wide range of applications shows that FACH can provide 5.1\u00d7 faster execution and 5.9\u00d7 higher energy efficiency as compared to the baseline HD.\n\nFigure 1 :\n1HD functionality in train and inference phases using encoding and associative memory modules.\n\nFigure 2 :\n2(a) FACH supporting Framework consisting of HD training to create initial class hypervectors.\n\nFigure 3 :\n3An example of dot product between the class and query vectors with six dimensions (a) using conventional method, (b) when the elements of the class vector clustered. and trains the HD model by combining data points corresponding to each class ( \u2022 1 ). HD refinement clusters the values that elements in each class hypervector can take by applying non-linear clustering on the trained class hypervectors ( \u2022 2 ). This method reduces the possible values that the elements of each class hypervector can take. Next, FACH estimates the accuracy of the new HD model on the validation data (validation is part of training data\n\nFigure 4\n4: a) The classification accuracy of applications during retraining iterations. b) Impact of number of class elements on the quality loss of different applications.of the clustering algorithm is to find a set of k cluster centroids {c 1 , c 2 , . . . , c k } that can best represent the class values (c \u2208\n\nFigure 5 :\n5The hardware support to measure dot product between a query and class hypervectors in baseline HD and FACH with clustered class elements. stored class hypervectors; (i) if an input data correctly matches with the corresponding class in associative memory, our design does not change the mode. (ii) if an input hypervector, Q, wrongly matches with the i th class hypervector (C i ) while it actually belongs to j th class (C j ), our retraining procedure subtracts the input hypervector from the i th class and add it to j th class (\n\n\nwhen D is large, FPGA does not have enough resources to perform multiplications in all dimensions in parallel ( \u2022 A ). The number of input dimensions which FPGA reads at a time depend on the number of classes, and the number of available Digital Signal Processors (DSPs) in FPGA. We implement HD on the Kintex-7 FPGA KC705 Evaluation Kit with 840 DSPs. In this case, our design sequentially reads the first d elements of the query vector and multiply it to corresponding class elements (d < D). Then, the computation on the rest of query elements are performed sequentially.\n\nFigure 5\n5illustrates the FACH architecture which supports dot product between a query and a single class hypervector. The class hypervector has k clustered values, i.e., the class elements can take one of the k cluster centroids, {c 1 , c 2 , . . . , c k }. To accelerate FACH, our design creates k index buffers, where each buffer represents one of the cluster centroids ( \u2022 B ). Each buffer stores the indices of the class elements which have clustered to the same value. For example, the first index buffer, shown in\n\nFigure 6 :\n6Execution time and average resource utilization of FPGA running FACH with different number of shared elements. d/k indices corresponding to a particular centroid ( \u2022 D ). The results\n\nFigure 7 :\n7Performance per utilization of FPGA running applications with different number of shared class elements.\n\nFigure 6\n6shows the execution time and average resource utilization of FPGA while running four applications. The resource utilization shows the average utilization of LUT, FF and DSP in the FPGA. The x-axis shows the number of shared elements (centroids) in each class hypervector. Comparing the results of baseline HD with the FACH show that FACH can significantly improve the efficiency of the HD computing by reducing the number of multiplications.\n\nFigure 9 :\n9The breakdown of dynamic power consumption of the FPGA implementing baseline HD and FACH with 8 shared class elements.\n\n\n2019, Tokyo, Japan \u00a9 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6007-4/19/01. . . $15.00 https://doi.org/10.1145/3287624.3287667\n\n\n). If the error rate is larger than a pre-defined \u03f5 value, FACH adjusts the model and again clusters all values exist in each newly trained class hypervector. This clustering gives us new centroids which better represent the distribution of the values in each class hypervector. This process continues iteratively until the convergence condition (\u0394E < \u03f5) is satisfied, or the algorithm has run for a pre-defined number of iterations ( \u2022 3 ). When the convergence condition satisfied, FACH framework sends a new HD model with the clustered class elements to inference in order to perform the rest of classification task ( \u2022 4 ). Finally, FACH uses the modified HD model with clustered class elements for inference ( \u2022 5 ). In this following, we explain the details of the FACH framework functionality.\n\n\nFigure 8: Resource utilization of FPGA running the baseline HD and proposed FACH with 8 shared class elements.6.9% \n\n4.6% \n\nLUT \n\nFF \n\nBRAM \n\nDSP \n\nIO \n\nLUT \n\nFF \n\nBRAM \n\nDSP \nIO \n\nResource Utilization (%) \n0 \n20 \n40 60 \n80 \n14.7% \n\n3.1% \n\n13.1% \n\n1.4% \n\n(a) Baseline \n(b) Proposed FACH \n\n51.4% \n\n21.9% \n\n100 \n\nResource Utilization (%) \n0 \n20 \n40 60 \n80 100 \n\n97.5% \n94.4% \n\n38.0% \n\n17.6% \n\n11.4% \n\n31.4% \n\n1.6% \n\n24.5% \n\n11.1% \n\n5.2% \n\n45.4% \n\n12.9% \n\n0.9% \n\n\nACKNOWLEDGEMENTSThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1730158 and #1527034.\nMachine learning applications in cancer prognosis and prediction. K Kourou, Computational and structural biotechnology journal. 13K. Kourou et al., \"Machine learning applications in cancer prognosis and pre- diction,\" Computational and structural biotechnology journal, vol. 13, pp. 8-17, 2015.\n\nNested gaussian process modeling for high-dimensional data imputation in healthcare systems. F Imani, IISE 2018 Conference & Expo. Orlando, FL, MayF. Imani et al., \"Nested gaussian process modeling for high-dimensional data imputation in healthcare systems,\" in IISE 2018 Conference & Expo, Orlando, FL, May, pp. 19-22, 2018.\n\nMachine learning applications in genetics and genomics. M Libbrecht, M. Libbrecht et al., \"Machine learning applications in genetics and genomics, \"\n\nMFBO-SSM: Multi-fidelity Bayesian optimization for fast inference in state-space models. M Imani, AAAIM. Imani et al., \"MFBO-SSM: Multi-fidelity Bayesian optimization for fast infer- ence in state-space models, \" in AAAI, 2019.\n\nOrchard: Visual object recognition accelerator based on approximate in-memory processing. Y Kim, ICCAD. IEEEY. Kim et al., \"Orchard: Visual object recognition accelerator based on approximate in-memory processing, \" in ICCAD, pp. 25-32, IEEE, 2017.\n\nBayesian control of large mdps with unknown dynamics in data-poor environments. M Imani, Advances in Neural Information Processing Systems. M. Imani et al., \"Bayesian control of large mdps with unknown dynamics in data-poor environments, \" in Advances in Neural Information Processing Systems, 2018.\n\nRapidnn: In-memory deep neural network acceleration framework. M Imani, arXiv:1806.05794arXiv preprintM. Imani et al., \"Rapidnn: In-memory deep neural network acceleration frame- work, \" arXiv preprint arXiv:1806.05794, 2018.\n\nImage recognition accelerator design using in-memory processing. Y Kim, IEEE Micro. Y. Kim et al., \"Image recognition accelerator design using in-memory processing, \" IEEE Micro, 2018.\n\nLooknn: Neural network with no multiplication. M S Razlighi, DATE. IEEEM. S. Razlighi et al., \"Looknn: Neural network with no multiplication, \" in DATE, pp. 1775-1780, IEEE, 2017.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nWhat we mean when we say \"what's the dollar of mexico?\": Prototypes and mapping in concept space. P Kanerva, AAAI Fall Symposium: Quantum Informatics for Cognitive, Social, and Semantic Processes. P. Kanerva, \"What we mean when we say \"what's the dollar of mexico?\": Pro- totypes and mapping in concept space,\" in AAAI Fall Symposium: Quantum Informatics for Cognitive, Social, and Semantic Processes, pp. 2-6, 2010.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, in CogSci. 1036CiteseerP. Kanerva et al., \"Random indexing of text samples for latent semantic analysis, \" in CogSci, vol. 1036, Citeseer, 2000.\n\nLanguage geometry using random indexing. A Joshi, Quantum Interaction 2016 Conference Proceedings. In pressA. Joshi et al., \"Language geometry using random indexing, \" Quantum Interaction 2016 Conference Proceedings, In press.\n\nLow-power sparse hyperdimensional encoder for language recognition. M Imani, IEEE Design & Test. 346M. Imani et al., \"Low-power sparse hyperdimensional encoder for language recognition, \" IEEE Design & Test, vol. 34, no. 6, pp. 94-101, 2017.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O R , IEEE Trans. Neural Netw. Learn. Syst. 99O. R. andothers, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns, \" IEEE Trans. Neural Netw. Learn. Syst., vol. PP, no. 99, pp. 1-12, 2015.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, ICRC. IEEEM. Imani et al., \"Voicehd: Hyperdimensional computing for efficient speech recognition, \" in ICRC, pp. 1-6, IEEE, 2017.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM108M. Imani et al., \"Hierarchical hyperdimensional computing for energy efficient classification,\" in Proceedings of the 55th Annual Design Automation Conference, p. 108, ACM, 2018.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, Proceedings of the 8th International Conference on the Internet of Things. the 8th International Conference on the Internet of ThingsACM38Y. Kim et al., \"Efficient human activity recognition using hyperdimensional computing,\" in Proceedings of the 8th International Conference on the Internet of Things, p. 38, ACM, 2018.\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, Biomedical & Health Informatics (BHI). IEEEIEEE EMBS International Conference onM. Imani et al., \"Hdna: Energy-efficient dna sequencing using hyperdimensional computing,\" in Biomedical & Health Informatics (BHI), 2018 IEEE EMBS Interna- tional Conference on, pp. 271-274, IEEE, 2018.\n\nA memory-centric acceleration of clustering using highdimensional vectors. M Imani, 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEM. Imani et al., \"A memory-centric acceleration of clustering using high- dimensional vectors, \" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE), IEEE, 2019.\n\nLearning both weights and connections for efficient neural network. S Han, Advances in neural information processing systems. S. Han et al., \"Learning both weights and connections for efficient neural network, \" in Advances in neural information processing systems, pp. 1135-1143, 2015.\n\nA binary learning framework for hyperdimensional computing. M Imani, 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEM. Imani et al., \"A binary learning framework for hyperdimensional computing, \" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE), IEEE, 2019.\n\nHigh-dimensional computing as a nanoscalable paradigm. A Rahimi, TCAS I. A. Rahimi et al., \"High-dimensional computing as a nanoscalable paradigm,\" TCAS I, 2017.\n\nExploring hyperdimensional associative memory. M Imani, HPCA. IEEEM. Imani et al., \"Exploring hyperdimensional associative memory,\" in HPCA, pp. 445-456, IEEE, 2017.\n\nScikit-learn: Machine learning in python. F Pedregosa, JMLR. 12F. Pedregosa et al., \"Scikit-learn: Machine learning in python,\" JMLR, vol. 12, pp. 2825-2830, 2011.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite, \" White Paper, vol. 5, 2012.\n\nUci machine learning repository. \"Uci machine learning repository. \" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nCaltech-256 object category dataset. G Griffin, G. Griffin et al., \"Caltech-256 object category dataset, \" 2007.\n\nThe pascal visual object classes challenge: A retrospective. M Everingham, IJCV. 1111M. Everingham et al., \"The pascal visual object classes challenge: A retrospective, \" IJCV, vol. 111, no. 1, pp. 98-136, 2015.\n\nUci machine learning repository. \"Uci machine learning repository. \" https://archive.ics.uci.edu/ml/datasets/Daily+ and+Sports+Activities.\n\nCreating and benchmarking a new dataset for physical activity monitoring. A Reiss, ACM40PETRAA. Reiss et al., \"Creating and benchmarking a new dataset for physical activity monitoring, \" in PETRA, p. 40, ACM, 2012.\n", "annotations": {"author": "[{\"end\":196,\"start\":100},{\"end\":295,\"start\":197},{\"end\":392,\"start\":296},{\"end\":471,\"start\":393},{\"end\":568,\"start\":472}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":107},{\"end\":211,\"start\":204},{\"end\":309,\"start\":304},{\"end\":404,\"start\":399},{\"end\":485,\"start\":479}]", "author_first_name": "[{\"end\":106,\"start\":100},{\"end\":203,\"start\":197},{\"end\":303,\"start\":296},{\"end\":398,\"start\":393},{\"end\":478,\"start\":472}]", "author_affiliation": "[{\"end\":195,\"start\":131},{\"end\":294,\"start\":230},{\"end\":391,\"start\":327},{\"end\":470,\"start\":406},{\"end\":567,\"start\":503}]", "title": "[{\"end\":97,\"start\":1},{\"end\":665,\"start\":569}]", "venue": null, "abstract": "[{\"end\":2678,\"start\":1245}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3407,\"start\":3403},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3449,\"start\":3446},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3452,\"start\":3449},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3455,\"start\":3452},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3458,\"start\":3455},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3461,\"start\":3458},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3464,\"start\":3461},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3624,\"start\":3621},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3627,\"start\":3624},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3630,\"start\":3627},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3975,\"start\":3971},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4400,\"start\":4396},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4431,\"start\":4427},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4458,\"start\":4454},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4461,\"start\":4458},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4508,\"start\":4504},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4533,\"start\":4529},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4536,\"start\":4533},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4563,\"start\":4559},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4584,\"start\":4580},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4605,\"start\":4601},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4876,\"start\":4872},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8703,\"start\":8699},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8706,\"start\":8703},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8795,\"start\":8791},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9639,\"start\":9635},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9642,\"start\":9639},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10943,\"start\":10939},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10946,\"start\":10943},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21311,\"start\":21307},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21667,\"start\":21663},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22047,\"start\":22043},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22125,\"start\":22121},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22237,\"start\":22233},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22273,\"start\":22269},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22487,\"start\":22483}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26960,\"start\":26854},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27067,\"start\":26961},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27700,\"start\":27068},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28015,\"start\":27701},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28561,\"start\":28016},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29138,\"start\":28562},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29660,\"start\":29139},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29856,\"start\":29661},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29974,\"start\":29857},{\"attributes\":{\"id\":\"fig_9\"},\"end\":30427,\"start\":29975},{\"attributes\":{\"id\":\"fig_10\"},\"end\":30559,\"start\":30428},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30709,\"start\":30560},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31512,\"start\":30710},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31975,\"start\":31513}]", "paragraph": "[{\"end\":4606,\"start\":2694},{\"end\":5518,\"start\":4608},{\"end\":6096,\"start\":5520},{\"end\":7170,\"start\":6098},{\"end\":8208,\"start\":7215},{\"end\":8853,\"start\":8228},{\"end\":9870,\"start\":8875},{\"end\":11405,\"start\":9960},{\"end\":11840,\"start\":11437},{\"end\":12011,\"start\":11879},{\"end\":12503,\"start\":12013},{\"end\":13127,\"start\":12505},{\"end\":13672,\"start\":13129},{\"end\":13818,\"start\":13726},{\"end\":13963,\"start\":13889},{\"end\":14765,\"start\":13965},{\"end\":15465,\"start\":14816},{\"end\":16002,\"start\":15467},{\"end\":16255,\"start\":16004},{\"end\":17842,\"start\":16285},{\"end\":18002,\"start\":17871},{\"end\":18491,\"start\":18031},{\"end\":19526,\"start\":18513},{\"end\":21145,\"start\":19528},{\"end\":21746,\"start\":21178},{\"end\":26203,\"start\":21760},{\"end\":26853,\"start\":26280}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9931,\"start\":9871},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13725,\"start\":13673},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13888,\"start\":13819},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14815,\"start\":14766},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16284,\"start\":16256}]", "table_ref": "[{\"end\":19499,\"start\":19488}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2692,\"start\":2680},{\"attributes\":{\"n\":\"2\"},\"end\":7213,\"start\":7173},{\"attributes\":{\"n\":\"2.2\"},\"end\":8226,\"start\":8211},{\"attributes\":{\"n\":\"2.3\"},\"end\":8873,\"start\":8856},{\"attributes\":{\"n\":\"2.4\"},\"end\":9958,\"start\":9933},{\"attributes\":{\"n\":\"3\"},\"end\":11435,\"start\":11408},{\"attributes\":{\"n\":\"3.2\"},\"end\":11877,\"start\":11843},{\"attributes\":{\"n\":\"4\"},\"end\":17869,\"start\":17845},{\"attributes\":{\"n\":\"4.1\"},\"end\":18029,\"start\":18005},{\"attributes\":{\"n\":\"4.2\"},\"end\":18511,\"start\":18494},{\"attributes\":{\"n\":\"5\"},\"end\":21155,\"start\":21148},{\"attributes\":{\"n\":\"5.1\"},\"end\":21176,\"start\":21158},{\"attributes\":{\"n\":\"5.2\"},\"end\":21758,\"start\":21749},{\"attributes\":{\"n\":\"5.3\"},\"end\":26235,\"start\":26206},{\"attributes\":{\"n\":\"5.4\"},\"end\":26265,\"start\":26238},{\"attributes\":{\"n\":\"6\"},\"end\":26278,\"start\":26268},{\"end\":26865,\"start\":26855},{\"end\":26972,\"start\":26962},{\"end\":27079,\"start\":27069},{\"end\":27710,\"start\":27702},{\"end\":28027,\"start\":28017},{\"end\":29148,\"start\":29140},{\"end\":29672,\"start\":29662},{\"end\":29868,\"start\":29858},{\"end\":29984,\"start\":29976},{\"end\":30439,\"start\":30429}]", "table": "[{\"end\":31975,\"start\":31625}]", "figure_caption": "[{\"end\":26960,\"start\":26867},{\"end\":27067,\"start\":26974},{\"end\":27700,\"start\":27081},{\"end\":28015,\"start\":27712},{\"end\":28561,\"start\":28029},{\"end\":29138,\"start\":28564},{\"end\":29660,\"start\":29150},{\"end\":29856,\"start\":29674},{\"end\":29974,\"start\":29870},{\"end\":30427,\"start\":29986},{\"end\":30559,\"start\":30441},{\"end\":30709,\"start\":30562},{\"end\":31512,\"start\":30712},{\"end\":31625,\"start\":31515}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5000,\"start\":4992},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7481,\"start\":7473},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10053,\"start\":10045},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11646,\"start\":11637},{\"end\":13671,\"start\":13668},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14974,\"start\":14966},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15216,\"start\":15205},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15412,\"start\":15402},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16834,\"start\":16825},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17270,\"start\":17261},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18090,\"start\":18081},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18355,\"start\":18346},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24005,\"start\":23997},{\"end\":24608,\"start\":24600},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":25667,\"start\":25659}]", "bib_author_first_name": "[{\"end\":32209,\"start\":32208},{\"end\":32532,\"start\":32531},{\"end\":32822,\"start\":32821},{\"end\":33005,\"start\":33004},{\"end\":33235,\"start\":33234},{\"end\":33475,\"start\":33474},{\"end\":33759,\"start\":33758},{\"end\":33988,\"start\":33987},{\"end\":34156,\"start\":34155},{\"end\":34158,\"start\":34157},{\"end\":34415,\"start\":34414},{\"end\":34746,\"start\":34745},{\"end\":35128,\"start\":35127},{\"end\":35326,\"start\":35325},{\"end\":35581,\"start\":35580},{\"end\":35878,\"start\":35877},{\"end\":35880,\"start\":35879},{\"end\":36209,\"start\":36208},{\"end\":36426,\"start\":36425},{\"end\":36797,\"start\":36796},{\"end\":37199,\"start\":37198},{\"end\":37568,\"start\":37567},{\"end\":37908,\"start\":37907},{\"end\":38188,\"start\":38187},{\"end\":38498,\"start\":38497},{\"end\":38653,\"start\":38652},{\"end\":38815,\"start\":38814},{\"end\":38959,\"start\":38958},{\"end\":39198,\"start\":39197},{\"end\":39336,\"start\":39335},{\"end\":39702,\"start\":39701}]", "bib_author_last_name": "[{\"end\":32216,\"start\":32210},{\"end\":32538,\"start\":32533},{\"end\":32832,\"start\":32823},{\"end\":33011,\"start\":33006},{\"end\":33239,\"start\":33236},{\"end\":33481,\"start\":33476},{\"end\":33765,\"start\":33760},{\"end\":33992,\"start\":33989},{\"end\":34167,\"start\":34159},{\"end\":34423,\"start\":34416},{\"end\":34754,\"start\":34747},{\"end\":35136,\"start\":35129},{\"end\":35332,\"start\":35327},{\"end\":35587,\"start\":35582},{\"end\":36215,\"start\":36210},{\"end\":36432,\"start\":36427},{\"end\":36801,\"start\":36798},{\"end\":37205,\"start\":37200},{\"end\":37574,\"start\":37569},{\"end\":37912,\"start\":37909},{\"end\":38194,\"start\":38189},{\"end\":38505,\"start\":38499},{\"end\":38659,\"start\":38654},{\"end\":38825,\"start\":38816},{\"end\":38965,\"start\":38960},{\"end\":39206,\"start\":39199},{\"end\":39347,\"start\":39337},{\"end\":39708,\"start\":39703}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15315839},\"end\":32436,\"start\":32142},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":68169645},\"end\":32763,\"start\":32438},{\"attributes\":{\"id\":\"b2\"},\"end\":32913,\"start\":32765},{\"attributes\":{\"id\":\"b3\"},\"end\":33142,\"start\":32915},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4701912},\"end\":33392,\"start\":33144},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53965616},\"end\":33693,\"start\":33394},{\"attributes\":{\"doi\":\"arXiv:1806.05794\",\"id\":\"b6\"},\"end\":33920,\"start\":33695},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":58244701},\"end\":34106,\"start\":33922},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":34011320},\"end\":34287,\"start\":34108},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":733980},\"end\":34645,\"start\":34289},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7149851},\"end\":35063,\"start\":34647},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":60571601},\"end\":35282,\"start\":35065},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":39020350},\"end\":35510,\"start\":35284},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8038292},\"end\":35753,\"start\":35512},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15258913},\"end\":36136,\"start\":35755},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21351739},\"end\":36346,\"start\":36138},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":49301394},\"end\":36723,\"start\":36348},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52978766},\"end\":37124,\"start\":36725},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4708051},\"end\":37490,\"start\":37126},{\"attributes\":{\"id\":\"b19\"},\"end\":37837,\"start\":37492},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2238772},\"end\":38125,\"start\":37839},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":155109576},\"end\":38440,\"start\":38127},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10569020},\"end\":38603,\"start\":38442},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1677864},\"end\":38770,\"start\":38605},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10659969},\"end\":38935,\"start\":38772},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":110511037},\"end\":39041,\"start\":38937},{\"attributes\":{\"id\":\"b26\"},\"end\":39158,\"start\":39043},{\"attributes\":{\"id\":\"b27\"},\"end\":39272,\"start\":39160},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207252270},\"end\":39485,\"start\":39274},{\"attributes\":{\"id\":\"b29\"},\"end\":39625,\"start\":39487},{\"attributes\":{\"id\":\"b30\"},\"end\":39841,\"start\":39627}]", "bib_title": "[{\"end\":32206,\"start\":32142},{\"end\":32529,\"start\":32438},{\"end\":33232,\"start\":33144},{\"end\":33472,\"start\":33394},{\"end\":33985,\"start\":33922},{\"end\":34153,\"start\":34108},{\"end\":34412,\"start\":34289},{\"end\":34743,\"start\":34647},{\"end\":35125,\"start\":35065},{\"end\":35323,\"start\":35284},{\"end\":35578,\"start\":35512},{\"end\":35875,\"start\":35755},{\"end\":36206,\"start\":36138},{\"end\":36423,\"start\":36348},{\"end\":36794,\"start\":36725},{\"end\":37196,\"start\":37126},{\"end\":37565,\"start\":37492},{\"end\":37905,\"start\":37839},{\"end\":38185,\"start\":38127},{\"end\":38495,\"start\":38442},{\"end\":38650,\"start\":38605},{\"end\":38812,\"start\":38772},{\"end\":38956,\"start\":38937},{\"end\":39333,\"start\":39274}]", "bib_author": "[{\"end\":32218,\"start\":32208},{\"end\":32540,\"start\":32531},{\"end\":32834,\"start\":32821},{\"end\":33013,\"start\":33004},{\"end\":33241,\"start\":33234},{\"end\":33483,\"start\":33474},{\"end\":33767,\"start\":33758},{\"end\":33994,\"start\":33987},{\"end\":34169,\"start\":34155},{\"end\":34425,\"start\":34414},{\"end\":34756,\"start\":34745},{\"end\":35138,\"start\":35127},{\"end\":35334,\"start\":35325},{\"end\":35589,\"start\":35580},{\"end\":35883,\"start\":35877},{\"end\":36217,\"start\":36208},{\"end\":36434,\"start\":36425},{\"end\":36803,\"start\":36796},{\"end\":37207,\"start\":37198},{\"end\":37576,\"start\":37567},{\"end\":37914,\"start\":37907},{\"end\":38196,\"start\":38187},{\"end\":38507,\"start\":38497},{\"end\":38661,\"start\":38652},{\"end\":38827,\"start\":38814},{\"end\":38967,\"start\":38958},{\"end\":39208,\"start\":39197},{\"end\":39349,\"start\":39335},{\"end\":39710,\"start\":39701}]", "bib_venue": "[{\"end\":32585,\"start\":32569},{\"end\":36539,\"start\":36495},{\"end\":36936,\"start\":36878},{\"end\":32268,\"start\":32218},{\"end\":32567,\"start\":32540},{\"end\":32819,\"start\":32765},{\"end\":33002,\"start\":32915},{\"end\":33246,\"start\":33241},{\"end\":33532,\"start\":33483},{\"end\":33756,\"start\":33695},{\"end\":34004,\"start\":33994},{\"end\":34173,\"start\":34169},{\"end\":34446,\"start\":34425},{\"end\":34842,\"start\":34756},{\"end\":35147,\"start\":35138},{\"end\":35381,\"start\":35334},{\"end\":35607,\"start\":35589},{\"end\":35919,\"start\":35883},{\"end\":36221,\"start\":36217},{\"end\":36493,\"start\":36434},{\"end\":36876,\"start\":36803},{\"end\":37244,\"start\":37207},{\"end\":37647,\"start\":37576},{\"end\":37963,\"start\":37914},{\"end\":38267,\"start\":38196},{\"end\":38513,\"start\":38507},{\"end\":38665,\"start\":38661},{\"end\":38831,\"start\":38827},{\"end\":38978,\"start\":38967},{\"end\":39074,\"start\":39043},{\"end\":39195,\"start\":39160},{\"end\":39353,\"start\":39349},{\"end\":39518,\"start\":39487},{\"end\":39699,\"start\":39627}]"}}}, "year": 2023, "month": 12, "day": 17}