{"id": 4475365, "updated": "2023-09-28 11:15:30.295", "metadata": {"title": "MoCoGAN: Decomposing Motion and Content for Video Generation", "authors": "[{\"first\":\"Sergey\",\"last\":\"Tulyakov\",\"middle\":[]},{\"first\":\"Ming-Yu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xiaodong\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jan\",\"last\":\"Kautz\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 7, "day": 17}, "abstract": "Visual information in a natural video can be decomposed into two major components: content and motion. While content encodes the objects present in the video, motion encodes the object dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video clip by sequentially mapping random noise vectors to video frames. We divide a random noise vector into content and motion parts. The content part, modeled by a Gaussian, is kept fixed when generating individual frames in a short video clip, since the content in a short clip remains largely the same. On the other hand, the motion part, modeled by a recurrent neural network, aims at representing the dynamics in a video. Despite the lack of supervision signals on the motion - content decomposition in natural videos, we show that the MoCoGAN framework can learn to decompose these two factors through a novel adversarial training scheme. Experimental results on action, facial expression, and on a Tai Chi dataset along with comparison to the state-of-the-art verify the effectiveness of the proposed framework. We further show that, by fixing the content noise while changing the motion noise, MoCoGAN learns to generate videos of different dynamics of the same object, and, by fixing the motion noise while changing the content noise, MoCoGAN learns to generate videos of the same motion from different objects. More information is available in our project page (https://github.com/sergeytulyakov/mocogan).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1707.04993", "mag": "2963092440", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Tulyakov0YK18", "doi": "10.1109/cvpr.2018.00165"}}, "content": {"source": {"pdf_hash": "ccff8b6f8e24987f77c0d0495cbfdc6ecb9bb262", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1707.04993v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1707.04993", "status": "GREEN"}}, "grobid": {"id": "8dc9bbaef27d2d99679ee1fe2803597ebe69c26c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ccff8b6f8e24987f77c0d0495cbfdc6ecb9bb262.txt", "contents": "\nMoCoGAN: Decomposing Motion and Content for Video Generation\n\n\nSergey Tulyakov sergey.tulyakov@unitn.it \nUniversity of Trento\nItaly\n\nMing-Yu Liu mingyul@nvidia.com \nUniversity of Trento\nItaly\n\nXiaodong Yang xiaodongy@nvidia.com \nUniversity of Trento\nItaly\n\nJan Kautz Nvidia \nUniversity of Trento\nItaly\n\nMoCoGAN: Decomposing Motion and Content for Video Generation\n\nVisual information in a natural video can be decomposed into two major components: content and motion. While content encodes the objects present in the video, motion encodes the object dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video clip by sequentially mapping random noise vectors to video frames. We divide a random noise vector into content and motion parts. The content part, modeled by a Gaussian, is kept fixed when generating individual frames in a short video clip, since the content in a short clip remains largely the same. On the other hand, the motion part, modeled by a recurrent neural network, aims at representing the dynamics in a video. Despite the lack of supervision signals on the motion-content decomposition in natural videos, we show that the MoCo-GAN framework can learn to decompose these two factors through a novel adversarial training scheme. Experimental results on action, facial expression, and on a Tai Chi dataset along with comparison to the state-of-the-art verify the effectiveness of the proposed framework. We further show that, by fixing the content noise while changing the motion noise, MoCoGAN learns to generate videos of different dynamics of the same object, and, by fixing the motion noise while changing the content noise, MoCoGAN learns to generate videos of the same motion from different objects. More information is available in our project page\n\nIntroduction\n\nDeep generative modeling for image generation has recently received an increasing amount of attention, not only because it provides a means to learn deep feature representations in an unsupervised manner that can potentially leverage all the unlabeled images on Internet for training, but also because it can be used to generate novel images useful for various vision applications. As steady progress toward better image generation is made, it is also important  It divides the latent space into a content subspace and a motion subspace. By sampling a point in the content subspace and sampling different motion trajectories in the motion subspace, it generates videos of the same object performing different actions. By sampling different points in the content subspace and sampling the same motion trajectory in the motion subspace, it generates videos of different objects performing the same action.\n\nto study the video generation problem. Surprisingly, the extension from generating images to generating videos turns out to be a highly challenging task, although the generated data has only one more dimension, which is time.\n\nThe video generation problem is a much harder problem for the following reasons. First, since a video is a spatio-temporal recording of visual information of objects performing various actions, a generative model needs to learn the physical constructs of objects in addition to learning their appearance models. If the learned object constructs are incorrect, the generated video may contain objects performing physically implausible motion. Second, the time dimension brings in a huge amount of variation. Just imag-ine the amount of speed variations that a person can have as performing a squat movement. Each speed pattern results in a different video, although the appearances of the human in the videos are the same. Third, but not last, as human beings have evolved to be rather sensitive to motion, motion artifacts are particularly pronounced to human eyes. It is more difficult to generate visually convincing videos.\n\nRecently, a few attempts to the video generation problem were made through generative adversarial networks (GANs) [11]. In [37], Vondrick et al. hypothesized that a video clip is a point in a latent space and proposed learning a mapping from the latent space to video clips. A similar approach was proposed in [28]. We argue that assuming a video clip is a point in the latent space unnecessarily increases the complexity of the problem, because videos of the same action with different execution speed are represented by different points in the latent space. Moreover, this assumption forces every generated video clip to have the same length, while the length of real-world video clips varies. An alternative (and likely more intuitive and efficient) approach would assume a latent space of images and consider that a video clip is generated by traversing the points in the latent space. Video clips of different lengths correspond to paths of different lengths. In addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content subspace) leads to content changes in a video clip and the deviation in the second subspace (the motion subspace) results in motions. A video clip of a person performing an action will be represented by a point in the content subspace and a trajectory in the motion subspace. Through this modeling, videos of an action with different execution speeds will only result in different traversing speeds of a trajectory in the motion space.\n\nDecomposing motion and content allows a more controlled video generation process. By changing the content representation while fixing the motion trajectory, we have videos of different objects performing the same action. By changing motion trajectories while fixing the content representation, we have videos of the same object performing different actions. Examples of such control from our experiment results are illustrated in Figure 1.\n\nBased on the intuitions discussed above, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The framework is based on the GAN framework. It generates a video clip by sequentially generating video frames. At each time step, a deep image generative network maps a random vector to an image. The random vector consists of two parts where the first part is sampled from a content space and the second part is sampled from a motion space. Since content in a short video clip usually remains largely the same, we model the sampling from the content space using a Gaussian distribution and use the same realization through generating individual frames in a video clip. On the other hand, sampling from the motion space is modeled through a recurrent neural network where the network parameters are learned during training. In spite of lacking supervision signals about the decomposition of motion and content in natural videos, we show that MoCoGAN can learn to disentangle these two factors through a novel adversarial training scheme. We conduct extensive experimental validation using action, facial expression and Tai Chi datasets. Through qualitative and quantitative experimental validations and comparison to a state-of-the-art video generation method, we verify the effectiveness of the proposed framework. The user study we performed further supports superiority of the proposed approach.\n\n\nRelated Work\n\nVideo generation is not a new problem in computer vision. Due to limitations in computation, data, and modeling tools, early video generation works focused on generating dynamic texture patterns [31,38,8]. In the recent years, with the availability of GPUs, Internet videos, and deep neural networks, we argue that we are now better positioned to tackle this intriguing problem.\n\nVarious deep generative models were recently proposed for image generation including (GANs) [11], variational autoencoders (VAEs) [18,27], moment matching networks [19], PixelCNNs [35], and Plug&Play Generative Networks [23]. In this paper, we propose the MoCoGAN framework for video generation, which is based on GANs.\n\nIn the GAN framework, the image generation is learned through solving a 2-player zero-sum game problem where the two players are an image generative network and an image discriminative network. Denton et al. [7] proposed a Laplacian pyramid implementation of GANs. Radford et al. [25] used a deeper convolution network architecture. Zhang et al. [40] stacked two generative networks to progressively render realistic images. CoupledGANs [21] learned to generate corresponding images in different domains, which can be extended to learn to translate an image in one domain to a corresponding one in a different domain in an unsupervised fashion [20]. InfoGAN [5] learned a more interpretable latent representation. Salimans et al. [29] proposed several GAN training tricks. Arjovsky et al. [3] proposed the Wasserstein GAN framework for a more stable adversarial training. The proposed MoCoGAN framework generates a video clip by sequentially generating images using an image generator. It can easily leverage the advancement in image generation in the GAN framework for improving the quality of the generated videos. As discussed in Section 1, [37,28] extended the GAN framework to the video generation problem by assuming a latent space of video clips where all the clips have the same length.\n\nThe use of recurrent neural networks for image generation were previously explored in [13,15]. Specifically, they use recurrent mechanisms to iteratively refine a generated image. Our work is different to [13,15] in that we use the recurrent mechanism to generate motion embeddings of video frames in a video clip. The image generation is achieved through a convolutional neural network.\n\nThe future frame prediction problem studied in [30,24,22,16,10,34,39,36] is closely related to the video generation problem studied in this paper. In future frame prediction, the goal is to predict future frames in a video given the previous frames in the video. Previous works on future frame prediction can be roughly divided into two categories where one focuses on generating raw pixel values in future frames based on the observed ones [30,24,22,16,39,36] while the other focuses on generating transformations for reshuffling the pixels in the previous frames to construct future frames [10,34]. The availability of previous frames makes future frame prediction a conditional image generation problem, which is different to the video generation problem where the input to the generative network is only a vector drawn from a latent space. We note that [36] used a convolutional LSTM [14] encoder to encode temporal differences between consecutive previous frames for extracting motion information and a convolutional encoder to extract content information from the current image. The concatenation of the motion and content information was then fed to a decoder to predict future frames.\n\n\nGenerative Adversarial Networks\n\nA GAN framework [11] consists of two networks: a generative network and a discriminative network. The objective of the generative network is to generate images resembling real images, while the objective of the discriminative network is to distinguish real images from generated ones. Both the generative and discriminative networks are realized as convolutional neural networks (CNNs) in practice.\n\nLet x be a natural image (a real image) drawn from an image distribution, p X , and z be a random vector in Z I \u2261 R d . Let G I and D I be the generative and discriminative networks, respectively. The generative network takes z as input and outputs an image,x = G I (z), that has the same support as x. We denote the distribution of G I (z) as p GI . The discriminative network estimates the probability that an input image is drawn from p X . Ideally, D I (x) = 1 if x \u223c p X and D I (x) = 0 ifx \u223c p GI . The GAN framework corresponds to a 2-player zero-sum game, and the generative and discriminative networks can be trained jointly via solving a minimax problem given by\nmax GI min DI F I (D I , G I )(1)\nwhere the functional F I is given by\nF I (D I , G I ) = E x\u223cp X [\u2212 log D I (x)] + E z\u223cp Z I [\u2212 log(1 \u2212 D I (G I (z)))]. (2)\nIn practice, (1) is solved by alternating the following two gradient update steps:\n\nStep 1:\n\u03b8 D t+1 I = \u03b8 D t I \u2212 \u03bb t \u2207 \u03b8 D I F I (D t I , G t I ), Step 2: \u03b8 G t+1 I = \u03b8 G t I + \u03bb t \u2207 \u03b8 G I F I (D t+1 I , G t I )\nwhere \u03b8 DI and \u03b8 GI are the parameters of D I and G I , \u03bb is the learning rate, and t is the iteration number. Goodfellow et al. [11] show that, given enough capacity to D I and G I and sufficient training iterations, the distribution, p GI , converges to p X by using the above gradient update scheme. As a result, from a random vector input z, the network G I can synthesize an image that resembles one drawn from the true distribution, p X .\n\n\nExtension to Fixed-length Video Generation\n\nRecently, [37] propose the video generative adversarial network (VGAN) framework to extend the GAN framework to video generation. Let v L = [x (1) , ..., x (L) ] be a video clip with L frames. The video generation in VGAN is achieved by replacing the vanilla CNN-based image generative and discriminative networks, i.e., G I and D I , with the spatio-temporal CNN-based video generative and discriminative networks, i.e., G V L and D V L . The video generative network G V L maps a random vector z \u2208 Z V L \u2261 R d to a video clip,\u1e7d L = [x (1) , ...,x (L) ] = G V L (z) and the video discriminative network D V L differentiates real video clips from generated ones. Ideally,\nD V L (v L ) = 1 if v L is sam- pled from p V L and D V L (\u1e7d L ) = 0 if\u1e7d L is sampled from the video generative network distribution p G V L .\nIn the VGAN framework, each point in the latent space z \u2208 Z V L represents a video clip with a fixed length L. Hence, the VGAN framework can only model videos with L frames. Moreover, for videos of a same action of a person executed in different speeds, VGAN would represent them as different points in Z V L , which is unintuitive and inefficient. It also does not provide a way to decompose motion and content information in video clips. In order to overcome these limitations, we propose the Motion and Content decomposed GAN (MoCoGAN) framework to extend the GAN framework to video generation.\n\n\nThe MoCoGAN Framework\n\nIn this section, we will first discuss our latent space representation and then present the MoCoGAN network architecture and the learning algorithm. Finally, we will extend the framework to action-conditioned video generation. \n\u2026 \u2026 \u2026 R M (1) (2) (K) z c h 0 z (1) M z (2) M z (K) M G I G I G \u0128 vx (1)x(2)x(K) S 1 S T v D I D V {fake, real} {fake, real}\n\nLatent Space Representation\n\nWe assume a latent space of images Z I \u2261 R d where each point z \u2208 Z I represents an image, and a video of K frames is represented by a path of length K in the latent space, [z (1) , ..., z (K) ]. By adopting this formulation, videos of different lengths can be generated by paths of different lengths. Moreover, videos of the same action executed with different speeds can be generated by traversing a same path in the latent space with different speeds. More importantly, the formulation allows a simple way to decompose motion and content in video clips.\n\nSpecifically, we further assume the latent space of images Z I is decomposed into the content subspace Z C and the motion subspace Z M : i.e.,\nZ I = Z C \u00d7 Z M where Z C = R dC , Z M = R dM , and d = d C + d M .\nThe content subspace models motion-independent appearance in videos, while the motion subspace models motion-dependent appearance in videos. For example, in a video of a person making a smile, content represents the identity of the person, while motion represents the changes of facial muscle configurations of the person. A pair of the person's identity and the facial muscle configuration represents a face image of the person. A particular sequence of these pairs represents a video clip of the person making a smile. By swapping the look of the person with the look of another person but using the same facial muscle configurations, a video of a different person making a smile could be represented.\n\nWe model the sampling process from the content subspace using a standard multivariate Gaussian distribution:\nz C \u223c p ZC \u2261 N (z|0, I dC ) where I dC is an identity matrix of size d C \u00d7 d C .\nBased on the observation that the content remains largely the same in a short video clip, we use the same realization z C for generating different frames in a video clip. Given a fixed z C , the motion in the video clip is modeled by a path in the motion subspace Z M . In other words, the sequence is represented by\n[z (1) , ..., z (K) ] = z C z (1) M , ..., z C z (K) M (3) where z C \u2208 Z C and z (k) M \u2208 Z M for all k's.\nSince not all paths in Z M correspond to physically plausible motion, we need to learn to generate valid paths. We model the path generation process using a recurrent neural network.\n\nLet R M to be a recurrent neural network. At each time step, it takes a vector sampled from a standard multivariate Gaussian distribution as input:\n(k) \u223c p E \u2261 N ( |0, I dE )\nwhere d E is the dimension of the vector. At each time step, it outputs a vector in Z M , which is used as the motion representation. Let R M (k) be the output of the recurrent neural network at time k. Then, z  [14] and GRUs [6]. We use GRUs in the MoCoGAN but expect the other architectures would work equally well.\n\nWe note that assuming a motion and content decomposed representation in videos does not guarantee that we can learn the true decomposition from data. Moreover, due to the absence of supervised signals, learning the decomposition is a very challenging task. In the following, we present the MoCoGAN network architecture and the learning algorithm for learning the decomposition from data.\n\n\nNetwork Architecture\n\nThe MoCoGAN framework is based on GAN, but it consists of 4 networks which are the recurrent neural network R M , the image generative network G I , the image discriminative network D I , and the video discriminative network D V . The image generative network generates a video clip by sequentially mapping vectors in Z I to images, from a sequence of vectors [ M 's are from the recurrent neural network R M . We note that the video length K can vary for each video generation.\n\nBoth the image discriminative network D I and the video discriminative network D V play the role of judge, providing critical feedbacks to the image generative network G I and the recurrent neural network R M for generating realistic videos during adversarial training. Although D I and D V have a similar function, they have different specialties. The image discriminative network is specialized in criticizing G I based on each generated image. It is trained to output 1 for a video frame sampled from a real video clip v and to output 0 for a video frame sampled from\u1e7d. On the other hand, the video discriminative network D V provides feedbacks to G I based on the generated video clip. D V takes a fixed length video clip, say T frames, as the input. It is trained to output 1 for a video clip sampled from a real video and to output 0 for a video clip sampled from\u1e7d. Different to D I which is based on vanilla CNN architecture, D V is based on spatio-temporal CNN architecture. We note that the clip length T is a hyperparameter, which is set to 16 throughout our experiments. We also note that T can be smaller than the generated video length K. A video of length K can be divided into K \u2212 T + 1 clips in a sliding-window fashion, and each of the clips can be fed into D V .\n\nThe video discriminative network D V criticizes the generated video clips. In addition to providing feedbacks based on the appearance in each image (which D I also does), it also evaluates the generated motion. Since the image generative network G I has no concept of motion 1 , the criticisms on the motion part are redirected to the recurrent neural network R M . For generating a video with realistic dynamics for fooling D V , R M has to learn to generate a sequence of motion codes [z Ideally, D V alone should be sufficient for training G I and R M , because D V provides feedback on both static image appearance and video dynamics. However, we empirically found that using D I significantly improves the convergence of the adversarial training. We suspect this is due to the difficulty in training D V . As D V has to learn to encode both spatial and temporal information, training D V is more difficult than training D I , which only needs to focus on appearance. Once D I is well-trained, G I can learn to generate realistic images based on D I 's feedback. In this stage, the function of D V becomes much simpler. It only needs to be good at providing feedback about the generated video dynamics for training R M . We note that, in a recent work [9], a GAN framework for image generation with mul-tiple discriminative networks was proposed. In the work, all the discriminative networks had the same function, and they competed with each other for catching a synthesized image. The proposed work is different to [9] in that the two discriminative networks D I and D V have different functions. The carefully staged collaboration between D I and D V enables the MoCoGAN framework to generate realistic videos.\n\n\nLearning\n\nLet p V be the distribution of video clips of various lengths. Let \u03ba be a discrete random variable denoting the length of a video clip sampled from p V . In practice, we can estimate the distribution of \u03ba, termed p K , by computing a histogram of video clip length from training data. The distribution of the generated videos using the MoCoGAN framework can be described by\np\u1e7c(\u1e7d) = \u03ba k=1 p E ( ) \u00b7 p K (\u03ba) \u00b7 p ZC (z C ).(4)\nBasically, we can generate a video by first sample a content vector z C and a length \u03ba. We then run R M for \u03ba steps and, at each time step, R M takes a random variable as the input.\n\n\nA generated video by MoCoGAN is then given b\u1ef9\nv = G I ( z C R M (1) ), ..., G I ( z C R M (\u03ba)\n) .\n\nRecall that our D I and D V take one frame and T consecutive frames in a video as input, respectively. In order to represent these mechanisms, we need to introduce two random access functions S 1 and S T . The function S 1 takes a video clip (either v \u223c p V or\u1e7d \u223c p\u1e7c) and outputs a random frame in the clip. On the other hand, the function S T takes a video clip and randomly returns T consecutive frames in the clip. With these notations, we are now ready to present the MoCoGAN learning problem, which is given by\nmax GI,RM min DI,DV F V (D I , D V , G I , R M )(6)\nwhere the objective function\nF V (D I , D V , G I , R M ) is E v\u223cpV [\u2212 log D I (S 1 (v))] + E\u1e7d \u223cp\u1e7c [\u2212 log(1 \u2212 D I (S 1 (\u1e7d)))] + E v\u223cpV [\u2212 log D V (S T (v))] + E\u1e7d \u223cp\u1e7c [\u2212 log(1 \u2212 D V (S T (\u1e7d)))].(7)\nWe note that understanding (7) is straightforward. From the point of view of a discriminative network, the first and second terms encourage D I to output 1 for a video frame sampled from a real video clip v and 0 for a video frame from a generated one\u1e7d. Similarly, the third and forth terms encourage D V to output 1 for T consecutive frames in a real video clip v and 0 for T consecutive frames in a generated one\u1e7d. From the point of view of the generative network and the recurrent neural network, the second and forth term encourage them to generate a realistic video that both a random frame or a random T-consecutive frames in the video is so realistic that no discriminators can tell apart.\n\nTo train MoCoGAN, we can extend the alternating gradient update scheme for GAN training. Although 4 networks exist in a MoCoGAN, we only need to alternate between two gradient update steps given by\n\nStep 1:\n\u03b8 D t+1 I = \u03b8 D t I \u2212 \u03bb t \u2207 \u03b8 D I F V (D t I , D t V , G t I , R t M ), \u03b8 D t+1 V = \u03b8 D t V \u2212 \u03bb t \u2207 \u03b8 D V F V (D t I , D t V , G t I , R t M ), Step 2: \u03b8 G t+1 I = \u03b8 G t I + \u03bb t \u2207 \u03b8 G I F V (D t+1 I , D t+1 V , G t I , R t M ) \u03b8 R t+1 M = \u03b8 R t M + \u03bb t \u2207 \u03b8 R M F V (D t+1 I , D t+1 V , G t I , R t M )\nbecause D I and D V are conditionally independent and G I and R M work together for generating videos. In the first step, we update D I and D V , which are in charge of criticizing the generated video clips. In the second step, we update G I and R M , which are in charge of the generation process.\n\n\nAction Conditioned Video Generation\n\nWhen action category labels are available in training data, we can utilize the labels to improve video generation results. We can also use the action labels to learn to perform action-conditioned video generation by using a condition GAN model akin to [26]. For example, we can generate face videos conditioning on the facial expression label such as smile, anger, or disgust. We note that in addition to motion in a video, an action label may also impact static appearances in a video. For example, as generating a sport video conditioning on an action label, appearances of athletics in the video also depend on the action label. For example, basketball and hockey players have very different appearances. Hence, the action category label can be in the content subspace in addition to the motion subspace.\n\nLet z A be the action category label. When considering action label as a part of content, we can represent the content vector by the concatenation of z C and z A . When considering action label as a part of motion, we can input the action label to the recurrent neural network R M . That is input to R M is given by [ , z A ]. This way the action label can influence the generated path in the motion subspace. The generated dynamics will depend on the action. Instead of inputing the action label to the discriminative networks as in [26], we empirically found that jointly training the discriminative networks to differentiate real and synthesized videos and to predict the action category in the videos rendered better video generation results.\n\n\nImplementations\n\nWe designed G I and D I based on the DCGAN architecture [25], while we designed D V based on the spatiotemporal CNN in [37]. The recurrent neural network R M was a simple one-layer GRU network [6]. We trained MoCoGANs using ADAM [17]. We set the learning rate to 0.0002 and momentums to 0.5 and 0.999, respectively, as in [25]. We utilized the one-sided label smoothing trick [29] for robust adversarial training. The training was performed on a Tesla P100 card in an NVIDIA DGX-1 machine.\n\n\nExperiments\n\nWe conducted extensive experiments to evaluate the proposed framework. In addition to comparisons to the VGAN framework [37], we also quantitatively evaluated the proposed framework on its ability in 1) generating videos of the same object performing different motion by using a fixed content vector and varying motion trajectories and 2) generating videos of different objects performing the same motion by using different content vectors with the same motion trajectory. Evaluating generative models, especially on generating visual outputs, is a challenging task, since all the popular metrics are subject to flaws [33]. Hence, we reported experiment results on the datasets where we could develop reliable performance metrics.\n\nWe used the following datasets in our experiments:\n\n\u2022 Shape motion video generation. We built a synthetic dataset of video clips of shape motion. The dataset contained two types of shapes (circles and squares) with varying sizes and colors, performing two types of motion: one moving from left to right, the other moving from top to bottom. The motion trajectories were sampled from Bezier curves. There were 4,000 videos in the dataset, where the image resolution was 64 \u00d7 64 and video length was 16. \u2022 Facial expression video generation. We used the MUG Facial Expression Database [1] for the experiment. The dataset consisted of 86 subjects (51 male and 34 female) performing various facial expressions. Each video consisted of 50 to 160 frames. We cropped the face regions and resized them to have a resolution of 96 \u00d7 96. We discarded videos containing fewer than 64 frames and used only the sequences representing one of the six prototypic facial expressions: anger, fear, disgust, happiness, sadness, and surprise. Totally, the training datasets consists of 1254 video clips. \u2022 Tai Chi video generation. To show that MoCoGAN can generate challenging video sequences we trained it on a newly collected database of 4500 Tai Chi clips. For each video clip, we applied human pose estimator [4] and cropped the clip so that the performer is in the center.   Videos were rescaled to 64 \u00d7 64 pixels. The database includes more than 300K frames. \u2022 Human action video generation. We used the Weizmann Action database [12], containing 81 videos of 9 people performing 9 actions including jumping-jack and waving-hands. We resized the videos to have a resolution of 96 \u00d7 96. Due to the small size, we did not conduct quantitative evaluation using the dataset. Instead, we provided visualization results.\n\nPerformance Metrics. We used the following performance metrics for comparing video generation methods.\n\n\u2022 Average Content Distance (ACD). We used the ACD to measure content consistency of a generated video. For shape motion, we first computed average color of the generated shape in a frame. Each frame was, hence, represented by a 3-dimensional vector, and a video was represented by a set of vectors. We then computed pairwise L2 distances between the vectors. The average of the distances, termed as the ACD, was used as the performance metric. The smaller the ACD, the more consistent the video frames were. For facial expression video generation, the average color representation would be problematic, since this was not invariant to facial expression changes. For measuring if the generated faces in a facial expression video were all from the same person, we employed OpenFace [2], which surpassed human-level performance for face verification [32]. OpenFace extracted a feature vector for a face image and had the property that vectors of the face images from the same person were similar. We used the vectors to compute the ACD to quantify whether the generated faces in a video clip corresponds to the same person. \u2022 Motion Control Score (MCS). We used the MCS to evaluate action-conditioned video generation performance. We trained a spatio-temporal CNN classifier for action recognition using the labeled training dataset. We split the training dataset into a training set and a validation set for determining the training hyperparameters. In the test time, the MoCoGAN generated a video clip based on the input action label, and we used the classifier to verify whether the generated video contained the action. The MCS was then given by the ratio of the generated videos that the classifier considered having the actions that the generative network was asked to generate. The larger the MCS, the better control of the motion the generative network has. \u2022 Content Control Score (CCS). We used the CCS to evaluate the performance of the proposed framework on action-conditioned video generation. We generated pairs of videos, using the same action label and motion trajectory but two different content vectors. We then extracted feature vectors from the frames in the generated videos (e.g. using OpenFace). The average pairwise L2 distance between vectors in different videos in a pair was computed. The CCS was given by the average of the distances over all pairs. The larger the CCS, the more difference the content in a pair of videos. The CCS measured how well the content vector impacted the generated video content.\n\nWe trained the proposed and VGAN frameworks for the shape motion and facial expression generation tasks, respectively, for comparing their video content generation consistency. For VGAN, we used the implementation provided by the authors for training and randomly generated 256 videos from the trained models to compute the ACDs. For the proposed framework, we also randomly generated 256 videos using the trained models to compute the ACDs.  The comparison results are given in Table 1. From the table, we found that the content of the videos generated by the proposed framework are more consistent. This was particularly the case when dealing with the more difficult facial expression video generation task. The face images in the generated facial expression video from the proposed framework looked more like they were from the same person. In Figure 3, we visualized the video generation results on the shape motion and human action video generation tasks and found that the proposed MoCoGAN learned to generate various shape motion and human action.\n\nIn Figure 4, we visualized the facial expression videos generated by the proposed framework. To generate the videos, we fixed the content vector z C and randomly sampled [ (1) , ..., (K) ] to generate different motion trajectories, . The concatenation of the content and motion vectors were sequentially passed to G I to generate the facial expression videos. From the figure, we found that the generated videos corresponded to the same person performing the same expression with different speeds.\n\n\nAction-Conditioned Video Generation\n\nWe trained the MoCoGAN framework for the actionconditioned facial expression video generation task and used the trained network to generate long facial expression videos (96 frames). During the generation, we changed the action category z A every 16 frames and ensured that all 6 expressions were covered. Hence, a generated video would correspond to a person performed 6 different facial expressions. To evaluate the performance, we computed the ACD of the generated videos. A smaller ACD value meant the generated faces over the 96 frames were more likely to be from the same person. Note that the ACD values reported in this subsection were generally larger than the ACD values reported in Table 1, because the generated videos in the table contained only 1 expression while the generated videos in this subsection contained 6 different expressions. We also used the CCS metric to evaluated MoCoGAN's capability in content generation control. Specifically, we generated a set of 96-frames videos with the same motion trajectory and action category sequence but different content vectors. We then computed the CCS metric to measure how difference the generated faces were across the videos. We evaluated different conditioning schemes. In one scheme, the action category label was directly fed to the image generative network, termed z A \u2192 G I . In another scheme, the action category label was fed to the recurrent neural network, termed z A \u2192 R M . In addition, we evaluated the impact of the image discriminative network D I for the action-conditioned video generation task, where we considered training the MoCoGAN framework without D I .\n\nWe reported the experiment results in Table 2. From the table, we found that the models trained with D I consistently rendered better performances on various metrics. We also found that z A \u2192 R M rendered a better performance. Figure 5 shows two videos from the best model in Table 2. We observed that by fixing the content vector but changing the expression label, it generated videos of the same person performing different expressions. And similarly, by changing the content vector and providing the same motion trajectory, we generated videos of different people showing the same expression sequence.\n\n\nMotion and Content Subspace Dimensions\n\nWe conducted an experiment to empirically determine the dimension of the content vector z C and the dimension of the motion vectors z (t) M 's, which are referred to as d C and d M . In the experiment, we fixed the sum of the dimensions to 60 (i.e., d C + d M = 60) and changed the value of d C from 10 to 50, with a step size of 10. For each combination, we trained a MoCoGAN network using the MUG Facial Expression Database [1]. The rest of the experiment setup strictly followed those described in Section 6.1 in the main paper. We evaluated these MoCoGAN network based on the Motion Control Score (MCS), the Average Content  Distance (ACD), and the Content Control Score (CCS) as described in the main paper. Figure 6 shows the results. We found that when d C was large, the trained MoCoGAN has a small ACD. This meant that a video generated by the MoCoGAN resembled to the same person performed different expressions. The generated content was uniform. We also found that a larger d C rendered a larger CCS, which meant that the MoCoGAN had a better control in the generated content. Different content vectors with the same motion vectors corresponded to different people performing the same expression.\n\nWe were expecting that a larger z M would lead to a larger MCS but found the contrary. As visualizing the generated videos, we found that when d M was large (i.e., d C was small), the MoCoGAN failed to generate recognizable faces, which resulted in a poor MCS since the calculation of the MCS was based on a facial expression recognition network. If the generated faces were not recognizable, the facial expression recognition network could only perform a random guess on the expression and score poorly. Based on the experiment results, we adopt the setting of d C = 50 and d M = 10 as the preferred setting, which was used in all the other experiments.\n\n\nUser Study\n\nWe performed user-centric comparison of the proposed MoCoGAN framework and the VGAN work [37]. We trained both MoCoGAN and VGAN and performed a user study in which we asked Amazon Mechanical Turk (AMT) workers with approval rate >95% to select a more realistic video between the two. We randomly generated 80 videos using each algorithm and randomly paired the videos for the workers to choose. This constituted 80 questions. For each question, we gathered answers from 3 different workers. This experimental evaluation was performed for facial expression and Tai Chi video clip generation datasets. Table 3 presents the preferences of the AMT workers. It shows that the majority of the workers specified the presented framework as being able to generate more realistically looking video sequences. Examples of randomly selected image pairs are given in Fig. 7. We observe that MoCoGAN generates video sequences with much higher fidelity, explaining AMT workers preferences.\n\nFigure 1 :\n1The MoCoGAN framework adopts a motion and content decomposed representation for video generation.\n\nFigure 2 :\n2The MoCoGAN framework for video generation. For a video the content vector z c is sampled once and fixed. Then, a series of random variables [ (1) , ..., (K) ] is sampled and mapped to a series of motion codes [z via the recurrent neural network R M . A generator G I produces a framex (k) using the content and the motion vectors {z C , z (k) M }. The discriminators D I and D V are trained on real and fake images and videos, respectfully, sampled from the training set v and the generated set\u1e7d.\n\nM\n= R M (k). Intuitively, the function of the recurrent neural network is to map a sequence of independent and identically distributed (i.i.d.) random variables [ (1) , (2) , ..., (K) ] to a sequence of correlated random variables [R M (1), R M (2), ..., R M (K)] representing the dynamics in a video. Various recurrent neural network implementations exist including LSTMs\n\n]\nto a sequence of images \u1e7d = [x (1) , ...,x (K) ] wherex (k) = G I (\n\nM\n] from a sequence of i.i.d. noise inputs [ (1) , ...,(K)  ] in a way such that G I can sequentially map z (k) = [z C , z (k) M ] to consecutive frames in a video.\n\nFigure 3 :\n3Video generation results. The figure is best viewed via the Acrobat Reader. Click the image to play the video clip.\n\nFigure 4 :\n4Examples of influence of motion trajectories on video generation. Two examples (disgust and anger) are shown. Every two rows share the same content representation but have different motion trajectories. Note that different motion trajectories influence the phase of the videos, i.e. the time where a facial expression starts.\n\nFigure 5 :\n5Two examples of generating face videos of different identities with changing facial expressions. We changed the expression label from smile to fear through surprise.\n\nFigure 6 :\n6Performance of action-conditioned MoCoGAN models with varying d C and d M on facial expression generation.\n\nFigure 7 :\n7Randomly selected video clips used in the user study. The figure is best viewed via the Acrobat Reader. Click each image to play the video clip.\n\nTable 1 :\n1Video content consistency comparison. A smaller \nACD means the generated frames in a video are perceptu-\nally more similar. We also compute the ACDs for the videos \nin the training datasets, which serve as the reference shown \nin the table. \n\nShape Motion Facial Expression \n\nReference \n0 \n0.116 \nVGAN [37] \n0.055 \n0.322 \nMoCoGAN \n0.049 \n0.195 \n\n\n\nTable 2 :\n2Performance on action-conditioned facial expression video generation with various MoCoGAN settings.Settings \nMCS \nACD CCS \n\nD I z A \u2192 G I \n0.472 1.115 1.123 \nD I z A \u2192 R M 0.491 1.073 1.080 \nD I z A \u2192 G I \n0.355 0.738 1.333 \nD I z A \u2192 R M 0.581 0.606 1.269 \n\n\n\nTable 3 :\n3Amazon Mechanical Turk workers preferences.Facial expressions Tai Chi \n\nVGAN [37] \n15.8% \n42.1% \nMoCoGAN \n84.2% \n57.9% \n\n\nThis is the case because G I simply maps a vector to a still image. It does not know what a video is.\nConclusionWe presented the MoCoGAN framework for motion and content decomposed video generaton. Given sufficient video training data, MoCoGAN automatically learns to disentangle motion from content in an unsupervised manner. For instance, given videos of people performing different facial expressions, MoCoGAN learns to separate a person's identity from their expression, thus allowing us to synthesize a new video of a person performing different expressions, or fixing the expression and going through various identities. This is enabled by a new generative adversarial network, which generates a video clip by sequentially generating video frames. Each video frame is generated from a random vector, which consists of two parts, one signifying content and one signifying motion. The content subspace is modeled with a Gaussian distribution, whereas the motion subspace is modeled with a recurrent neural network. We sample this space in order to synthesize each video frame. We have validated through experiments that the proposed MoCoGAN framework is superior to current state-of-theart video generation methods.\nThe mug facial expression database. N Aifanti, C Papachristou, A Delopoulos, Image Analysis for Multimedia Interactive Services (WIAMIS), 2010 11th International Workshop on. IEEE6N. Aifanti, C. Papachristou, and A. Delopoulos. The mug facial expression database. In Image Analysis for Multime- dia Interactive Services (WIAMIS), 2010 11th International Workshop on, pages 1-4. IEEE, 2010. 6, 8\n\nOpenface: A general-purpose face recognition library with mobile applications. B Amos, B Ludwiczuk, M Satyanarayanan, CMU-CS-16-118CMU School of Computer ScienceTechnical reportB. Amos, B. Ludwiczuk, and M. Satyanarayanan. Openface: A general-purpose face recognition library with mobile ap- plications. Technical report, CMU-CS-16-118, CMU School of Computer Science, 2016. 7\n\n. M Arjovsky, S Chintala, L Bottou, arXiv:1701.07875Wasserstein gan. arXiv preprintM. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017. 2\n\nRealtime multiperson 2d pose estimation using part affinity fields. Z Cao, T Simon, S.-E Wei, Y Sheikh, Computer Vision and Patter Recognition. Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi- person 2d pose estimation using part affinity fields. In Com- puter Vision and Patter Recognition, 2017. 6\n\nInfogan: Interpretable representation learning by information maximizing generative adversarial nets. X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, P Abbeel, Advances in Neural Information Processing Systems. X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, 2016. 2\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.355546arXiv preprintJ. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. 4, 6\n\nDeep generative image models using a laplacian pyramid of adversarial networks. E L Denton, S Chintala, R Fergus, Advances in Neural Information Processing Systems. E. L. Denton, S. Chintala, R. Fergus, et al. Deep genera- tive image models using a laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems, 2015. 2\n\nDynamic textures. G Doretto, A Chiuso, Y N Wu, S Soatto, International Journal of Computer Vision. 2G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic textures. International Journal of Computer Vision, 2003. 2\n\nGenerative multiadversarial networks. I Durugkar, I Gemp, S Mahadevan, International Conference on Learning Representation. I. Durugkar, I. Gemp, and S. Mahadevan. Generative multi- adversarial networks. International Conference on Learning Representation, 2017. 5\n\nUnsupervised learning for physical interaction through video prediction. C Finn, I Goodfellow, S Levine, Advances In Neural Information Processing Systems. C. Finn, I. Goodfellow, and S. Levine. Unsupervised learn- ing for physical interaction through video prediction. In Ad- vances In Neural Information Processing Systems, 2016. 3\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in Neural Information Processing Systems. 23I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In Advances in Neural Information Processing Systems, 2014. 2, 3\n\nActions as space-time shapes. L Gorelick, M Blank, E Shechtman, M Irani, R Basri, PAMI29L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. PAMI, 29(12):2247-2253, 2007. 7\n\nDraw: A recurrent neural network for image generation. K Gregor, I Danihelka, A Graves, D J Rezende, D Wierstra, International Conference on Machine Learning. K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra. Draw: A recurrent neural network for image generation. International Conference on Machine Learning, 2015. 3\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 34S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997. 3, 4\n\nD J Im, C D Kim, H Jiang, R Memisevic, arXiv:1602.05110Generating images with recurrent adversarial networks. arXiv preprintD. J. Im, C. D. Kim, H. Jiang, and R. Memisevic. Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016. 3\n\nN Kalchbrenner, A Oord, K Simonyan, I Danihelka, O Vinyals, A Graves, K Kavukcuoglu, arXiv:1610.00527Video pixel networks. arXiv preprintN. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016. 3\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, International Conference on Learning Representations. D. Kingma and J. Ba. Adam: A method for stochastic op- timization. In International Conference on Learning Repre- sentations, 2015. 6\n\nD P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. arXiv preprintD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n\nGenerative moment matching networks. Y Li, K Swersky, R S Zemel, International Conference on Machine Learning. Y. Li, K. Swersky, and R. S. Zemel. Generative moment matching networks. In International Conference on Machine Learning, 2015. 2\n\nUnsupervised image-to-image translation networks. M.-Y Liu, T Breuel, J Kautz, arXiv:1703.00848arXiv preprintM.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. arXiv preprint arXiv:1703.00848, 2017. 2\n\nCoupled generative adversarial networks. M.-Y Liu, O Tuzel, Advances in Neural Information Processing Systems. M.-Y. Liu and O. Tuzel. Coupled generative adversarial net- works. In Advances in Neural Information Processing Sys- tems, 2016. 2\n\nDeep multi-scale video prediction beyond mean square error. M Mathieu, C Couprie, Y Lecun, arXiv:1511.05440arXiv preprintM. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440, 2015. 3\n\nPlug & play generative networks: Conditional iterative generation of images in latent space. A Nguyen, J Yosinski, Y Bengio, A Dosovitskiy, J Clune, arXiv:1612.00005arXiv preprintA. Nguyen, J. Yosinski, Y. Bengio, A. Dosovitskiy, and J. Clune. Plug & play generative networks: Conditional it- erative generation of images in latent space. arXiv preprint arXiv:1612.00005, 2016. 2\n\nActionconditional video prediction using deep networks in atari games. J Oh, X Guo, H Lee, R L Lewis, S Singh, Advances in Neural Information Processing Systems. J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action- conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Sys- tems, 2015. 3\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, International Conference on Learning Representations. 26A. Radford, L. Metz, and S. Chintala. Unsupervised rep- resentation learning with deep convolutional generative ad- versarial networks. In International Conference on Learning Representations, 2016. 2, 6\n\nGenerative adversarial text to image synthesis. S Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, International Conference on Machine Learning. S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. In International Conference on Machine Learning, 2016. 6\n\nStochastic backpropagation and variational inference in deep latent gaussian models. D J Rezende, S Mohamed, D Wierstra, International Conference on Machine Learning. D. J. Rezende, S. Mohamed, and D. Wierstra. Stochas- tic backpropagation and variational inference in deep latent gaussian models. In International Conference on Machine Learning, 2014. 2\n\nM Saito, E Matsumoto, arXiv:1611.06624Temporal generative adversarial nets. arXiv preprintM. Saito and E. Matsumoto. Temporal generative adversarial nets. arXiv preprint arXiv:1611.06624, 2016. 2\n\nImproved techniques for training gans. T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, Advances in Neural Information Processing Systems. 26T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Rad- ford, and X. Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016. 2, 6\n\nUnsupervised learning of video representations using lstms. N Srivastava, E Mansimov, R Salakhutdinov, International Conference on Machine Learning. N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsu- pervised learning of video representations using lstms. In International Conference on Machine Learning, 2015. 3\n\nTemporal texture modeling. M Szummer, R W Picard, International Conference on Image Processing. M. Szummer and R. W. Picard. Temporal texture modeling. In International Conference on Image Processing, 1996. 2\n\nDeepface: Closing the gap to human-level performance in face verification. Y Taigman, M Yang, M Ranzato, L Wolf, CVPR. Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verifica- tion. In CVPR, pages 1701-1708, 2014. 7\n\nA note on the evaluation of generative models. L Theis, A V D Oord, M Bethge, International Conference on Learning Representations. L. Theis, A. v. d. Oord, and M. Bethge. A note on the eval- uation of generative models. International Conference on Learning Representations, 2016. 6\n\nTransformation-based models of video sequences. J Van Amersfoort, A Kannan, M Ranzato, A Szlam, D Tran, S Chintala, arXiv:1701.08435arXiv preprintJ. van Amersfoort, A. Kannan, M. Ranzato, A. Szlam, D. Tran, and S. Chintala. Transformation-based models of video sequences. arXiv preprint arXiv:1701.08435, 2017. 3\n\nConditional image generation with pixelcnn decoders. A Van Den Oord, N Kalchbrenner, L Espeholt, O Vinyals, A Graves, Advances in Neural Information Processing Systems. A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, et al. Conditional image generation with pixel- cnn decoders. In Advances in Neural Information Processing Systems, 2016. 2\n\nDecomposing motion and content for natural video sequence prediction. R Villegas, J Yang, S Hong, X Lin, H Lee, International Conference on Learning Representation. R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee. Decom- posing motion and content for natural video sequence pre- diction. In International Conference on Learning Represen- tation, 2017. 3\n\nGenerating videos with scene dynamics. C Vondrick, H Pirsiavash, A Torralba, Advances In Neural Information Processing Systems. 89C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In Advances In Neural Infor- mation Processing Systems, 2016. 2, 3, 6, 7, 8, 9\n\nFast texture synthesis using treestructured vector quantization. L.-Y Wei, M Levoy, SIGGRAPH Conference. L.-Y. Wei and M. Levoy. Fast texture synthesis using tree- structured vector quantization. In SIGGRAPH Conference, 2000. 2\n\nProbabilistic modeling of future frames from a single image. T Xue, J Wu, K Bouman, B Freeman, Advances In Neural Information Processing Systems. T. Xue, J. Wu, K. Bouman, and B. Freeman. Probabilistic modeling of future frames from a single image. In Advances In Neural Information Processing Systems, 2016. 3\n\nH Zhang, T Xu, H Li, S Zhang, X Huang, X Wang, D Metaxas, Stackgan, arXiv:1612.03242Text to photo-realistic image synthesis with stacked generative adversarial networks. arXiv preprintH. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. Stackgan: Text to photo-realistic image syn- thesis with stacked generative adversarial networks. arXiv preprint arXiv:1612.03242, 2016. 2\n", "annotations": {"author": "[{\"end\":133,\"start\":64},{\"end\":193,\"start\":134},{\"end\":257,\"start\":194},{\"end\":303,\"start\":258}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":71},{\"end\":145,\"start\":142},{\"end\":207,\"start\":203},{\"end\":274,\"start\":268}]", "author_first_name": "[{\"end\":70,\"start\":64},{\"end\":141,\"start\":134},{\"end\":202,\"start\":194},{\"end\":261,\"start\":258},{\"end\":267,\"start\":262}]", "author_affiliation": "[{\"end\":132,\"start\":106},{\"end\":192,\"start\":166},{\"end\":256,\"start\":230},{\"end\":302,\"start\":276}]", "title": "[{\"end\":61,\"start\":1},{\"end\":364,\"start\":304}]", "venue": null, "abstract": "[{\"end\":1906,\"start\":366}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4100,\"start\":4096},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4109,\"start\":4105},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4296,\"start\":4292},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7720,\"start\":7716},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7723,\"start\":7720},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7725,\"start\":7723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7997,\"start\":7993},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8035,\"start\":8031},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8038,\"start\":8035},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8069,\"start\":8065},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8085,\"start\":8081},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8125,\"start\":8121},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8433,\"start\":8430},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8506,\"start\":8502},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8572,\"start\":8568},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8663,\"start\":8659},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8870,\"start\":8866},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8883,\"start\":8880},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8956,\"start\":8952},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9014,\"start\":9011},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9370,\"start\":9366},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9373,\"start\":9370},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9608,\"start\":9604},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9611,\"start\":9608},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9730,\"start\":9727},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9958,\"start\":9954},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9961,\"start\":9958},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9964,\"start\":9961},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9967,\"start\":9964},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9970,\"start\":9967},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9973,\"start\":9970},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9976,\"start\":9973},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9979,\"start\":9976},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10352,\"start\":10348},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10355,\"start\":10352},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10358,\"start\":10355},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10361,\"start\":10358},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10364,\"start\":10361},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10367,\"start\":10364},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10503,\"start\":10499},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10506,\"start\":10503},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10768,\"start\":10764},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10799,\"start\":10795},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11155,\"start\":11151},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12712,\"start\":12708},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13084,\"start\":13080},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13216,\"start\":13213},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13610,\"start\":13607},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15070,\"start\":15067},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17553,\"start\":17549},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17566,\"start\":17563},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21089,\"start\":21086},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21354,\"start\":21351},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24833,\"start\":24829},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25924,\"start\":25920},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26212,\"start\":26208},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26275,\"start\":26271},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26348,\"start\":26345},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26385,\"start\":26381},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26478,\"start\":26474},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26532,\"start\":26528},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26781,\"start\":26777},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27279,\"start\":27275},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27975,\"start\":27972},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28685,\"start\":28682},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28908,\"start\":28904},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30077,\"start\":30074},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30145,\"start\":30141},{\"end\":33068,\"start\":33065},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36141,\"start\":36138},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37684,\"start\":37680}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":38676,\"start\":38566},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39187,\"start\":38677},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39561,\"start\":39188},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39632,\"start\":39562},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39798,\"start\":39633},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39927,\"start\":39799},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40266,\"start\":39928},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40445,\"start\":40267},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40565,\"start\":40446},{\"attributes\":{\"id\":\"fig_11\"},\"end\":40723,\"start\":40566},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41082,\"start\":40724},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41354,\"start\":41083},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41488,\"start\":41355}]", "paragraph": "[{\"end\":2825,\"start\":1922},{\"end\":3052,\"start\":2827},{\"end\":3980,\"start\":3054},{\"end\":5604,\"start\":3982},{\"end\":6045,\"start\":5606},{\"end\":7504,\"start\":6047},{\"end\":7899,\"start\":7521},{\"end\":8220,\"start\":7901},{\"end\":9516,\"start\":8222},{\"end\":9905,\"start\":9518},{\"end\":11099,\"start\":9907},{\"end\":11533,\"start\":11135},{\"end\":12207,\"start\":11535},{\"end\":12278,\"start\":12242},{\"end\":12448,\"start\":12366},{\"end\":12457,\"start\":12450},{\"end\":13023,\"start\":12579},{\"end\":13741,\"start\":13070},{\"end\":14482,\"start\":13885},{\"end\":14735,\"start\":14508},{\"end\":15447,\"start\":14891},{\"end\":15591,\"start\":15449},{\"end\":16363,\"start\":15660},{\"end\":16473,\"start\":16365},{\"end\":16871,\"start\":16555},{\"end\":17160,\"start\":16978},{\"end\":17309,\"start\":17162},{\"end\":17654,\"start\":17337},{\"end\":18043,\"start\":17656},{\"end\":18546,\"start\":18068},{\"end\":19828,\"start\":18548},{\"end\":21547,\"start\":19830},{\"end\":21933,\"start\":21560},{\"end\":22165,\"start\":21984},{\"end\":22265,\"start\":22262},{\"end\":22782,\"start\":22267},{\"end\":22863,\"start\":22835},{\"end\":23728,\"start\":23032},{\"end\":23927,\"start\":23730},{\"end\":23936,\"start\":23929},{\"end\":24537,\"start\":24239},{\"end\":25384,\"start\":24577},{\"end\":26132,\"start\":25386},{\"end\":26641,\"start\":26152},{\"end\":27387,\"start\":26657},{\"end\":27439,\"start\":27389},{\"end\":29188,\"start\":27441},{\"end\":29292,\"start\":29190},{\"end\":31824,\"start\":29294},{\"end\":32880,\"start\":31826},{\"end\":33379,\"start\":32882},{\"end\":35063,\"start\":33419},{\"end\":35669,\"start\":35065},{\"end\":36920,\"start\":35712},{\"end\":37576,\"start\":36922},{\"end\":38565,\"start\":37591}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12241,\"start\":12208},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12365,\"start\":12279},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12578,\"start\":12458},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13884,\"start\":13742},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14860,\"start\":14736},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15659,\"start\":15592},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16554,\"start\":16474},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16977,\"start\":16872},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17336,\"start\":17310},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21983,\"start\":21934},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22261,\"start\":22214},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22834,\"start\":22783},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23031,\"start\":22864},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24238,\"start\":23937}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32312,\"start\":32305},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34119,\"start\":34112},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35110,\"start\":35103},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35348,\"start\":35341},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":38198,\"start\":38191}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1920,\"start\":1908},{\"attributes\":{\"n\":\"2.\"},\"end\":7519,\"start\":7507},{\"attributes\":{\"n\":\"3.\"},\"end\":11133,\"start\":11102},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13068,\"start\":13026},{\"attributes\":{\"n\":\"4.\"},\"end\":14506,\"start\":14485},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14889,\"start\":14862},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18066,\"start\":18046},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21558,\"start\":21550},{\"end\":22213,\"start\":22168},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24575,\"start\":24540},{\"attributes\":{\"n\":\"5.\"},\"end\":26150,\"start\":26135},{\"attributes\":{\"n\":\"6.\"},\"end\":26655,\"start\":26644},{\"attributes\":{\"n\":\"6.1.\"},\"end\":33417,\"start\":33382},{\"attributes\":{\"n\":\"6.2.\"},\"end\":35710,\"start\":35672},{\"attributes\":{\"n\":\"6.3.\"},\"end\":37589,\"start\":37579},{\"end\":38577,\"start\":38567},{\"end\":38688,\"start\":38678},{\"end\":39190,\"start\":39189},{\"end\":39564,\"start\":39563},{\"end\":39635,\"start\":39634},{\"end\":39810,\"start\":39800},{\"end\":39939,\"start\":39929},{\"end\":40278,\"start\":40268},{\"end\":40457,\"start\":40447},{\"end\":40577,\"start\":40567},{\"end\":40734,\"start\":40725},{\"end\":41093,\"start\":41084},{\"end\":41365,\"start\":41356}]", "table": "[{\"end\":41082,\"start\":40736},{\"end\":41354,\"start\":41194},{\"end\":41488,\"start\":41410}]", "figure_caption": "[{\"end\":38676,\"start\":38579},{\"end\":39187,\"start\":38690},{\"end\":39561,\"start\":39191},{\"end\":39632,\"start\":39565},{\"end\":39798,\"start\":39636},{\"end\":39927,\"start\":39812},{\"end\":40266,\"start\":39941},{\"end\":40445,\"start\":40280},{\"end\":40565,\"start\":40459},{\"end\":40723,\"start\":40579},{\"end\":41194,\"start\":41095},{\"end\":41410,\"start\":41367}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6044,\"start\":6036},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32681,\"start\":32673},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32893,\"start\":32885},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":36433,\"start\":36425},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38451,\"start\":38445}]", "bib_author_first_name": "[{\"end\":42746,\"start\":42745},{\"end\":42757,\"start\":42756},{\"end\":42773,\"start\":42772},{\"end\":43185,\"start\":43184},{\"end\":43193,\"start\":43192},{\"end\":43206,\"start\":43205},{\"end\":43486,\"start\":43485},{\"end\":43498,\"start\":43497},{\"end\":43510,\"start\":43509},{\"end\":43735,\"start\":43734},{\"end\":43742,\"start\":43741},{\"end\":43754,\"start\":43750},{\"end\":43761,\"start\":43760},{\"end\":44081,\"start\":44080},{\"end\":44089,\"start\":44088},{\"end\":44097,\"start\":44096},{\"end\":44110,\"start\":44109},{\"end\":44122,\"start\":44121},{\"end\":44135,\"start\":44134},{\"end\":44513,\"start\":44512},{\"end\":44522,\"start\":44521},{\"end\":44534,\"start\":44533},{\"end\":44541,\"start\":44540},{\"end\":44830,\"start\":44829},{\"end\":44832,\"start\":44831},{\"end\":44842,\"start\":44841},{\"end\":44854,\"start\":44853},{\"end\":45123,\"start\":45122},{\"end\":45134,\"start\":45133},{\"end\":45144,\"start\":45143},{\"end\":45146,\"start\":45145},{\"end\":45152,\"start\":45151},{\"end\":45360,\"start\":45359},{\"end\":45372,\"start\":45371},{\"end\":45380,\"start\":45379},{\"end\":45661,\"start\":45660},{\"end\":45669,\"start\":45668},{\"end\":45683,\"start\":45682},{\"end\":45952,\"start\":45951},{\"end\":45966,\"start\":45965},{\"end\":45983,\"start\":45982},{\"end\":45992,\"start\":45991},{\"end\":45998,\"start\":45997},{\"end\":46014,\"start\":46013},{\"end\":46023,\"start\":46022},{\"end\":46036,\"start\":46035},{\"end\":46332,\"start\":46331},{\"end\":46344,\"start\":46343},{\"end\":46353,\"start\":46352},{\"end\":46366,\"start\":46365},{\"end\":46375,\"start\":46374},{\"end\":46569,\"start\":46568},{\"end\":46579,\"start\":46578},{\"end\":46592,\"start\":46591},{\"end\":46602,\"start\":46601},{\"end\":46604,\"start\":46603},{\"end\":46615,\"start\":46614},{\"end\":46875,\"start\":46874},{\"end\":46889,\"start\":46888},{\"end\":47016,\"start\":47015},{\"end\":47018,\"start\":47017},{\"end\":47024,\"start\":47023},{\"end\":47026,\"start\":47025},{\"end\":47033,\"start\":47032},{\"end\":47042,\"start\":47041},{\"end\":47286,\"start\":47285},{\"end\":47302,\"start\":47301},{\"end\":47310,\"start\":47309},{\"end\":47322,\"start\":47321},{\"end\":47335,\"start\":47334},{\"end\":47346,\"start\":47345},{\"end\":47356,\"start\":47355},{\"end\":47633,\"start\":47632},{\"end\":47643,\"start\":47642},{\"end\":47838,\"start\":47837},{\"end\":47840,\"start\":47839},{\"end\":47850,\"start\":47849},{\"end\":48063,\"start\":48062},{\"end\":48069,\"start\":48068},{\"end\":48080,\"start\":48079},{\"end\":48082,\"start\":48081},{\"end\":48321,\"start\":48317},{\"end\":48328,\"start\":48327},{\"end\":48338,\"start\":48337},{\"end\":48549,\"start\":48545},{\"end\":48556,\"start\":48555},{\"end\":48808,\"start\":48807},{\"end\":48819,\"start\":48818},{\"end\":48830,\"start\":48829},{\"end\":49102,\"start\":49101},{\"end\":49112,\"start\":49111},{\"end\":49124,\"start\":49123},{\"end\":49134,\"start\":49133},{\"end\":49149,\"start\":49148},{\"end\":49461,\"start\":49460},{\"end\":49467,\"start\":49466},{\"end\":49474,\"start\":49473},{\"end\":49481,\"start\":49480},{\"end\":49483,\"start\":49482},{\"end\":49492,\"start\":49491},{\"end\":49834,\"start\":49833},{\"end\":49845,\"start\":49844},{\"end\":49853,\"start\":49852},{\"end\":50174,\"start\":50173},{\"end\":50182,\"start\":50181},{\"end\":50191,\"start\":50190},{\"end\":50198,\"start\":50197},{\"end\":50212,\"start\":50211},{\"end\":50223,\"start\":50222},{\"end\":50533,\"start\":50532},{\"end\":50535,\"start\":50534},{\"end\":50546,\"start\":50545},{\"end\":50557,\"start\":50556},{\"end\":50804,\"start\":50803},{\"end\":50813,\"start\":50812},{\"end\":51040,\"start\":51039},{\"end\":51052,\"start\":51051},{\"end\":51066,\"start\":51065},{\"end\":51077,\"start\":51076},{\"end\":51087,\"start\":51086},{\"end\":51098,\"start\":51097},{\"end\":51402,\"start\":51401},{\"end\":51416,\"start\":51415},{\"end\":51428,\"start\":51427},{\"end\":51688,\"start\":51687},{\"end\":51699,\"start\":51698},{\"end\":51701,\"start\":51700},{\"end\":51946,\"start\":51945},{\"end\":51957,\"start\":51956},{\"end\":51965,\"start\":51964},{\"end\":51976,\"start\":51975},{\"end\":52195,\"start\":52194},{\"end\":52204,\"start\":52203},{\"end\":52208,\"start\":52205},{\"end\":52216,\"start\":52215},{\"end\":52480,\"start\":52479},{\"end\":52498,\"start\":52497},{\"end\":52508,\"start\":52507},{\"end\":52519,\"start\":52518},{\"end\":52528,\"start\":52527},{\"end\":52536,\"start\":52535},{\"end\":52799,\"start\":52798},{\"end\":52815,\"start\":52814},{\"end\":52831,\"start\":52830},{\"end\":52843,\"start\":52842},{\"end\":52854,\"start\":52853},{\"end\":53180,\"start\":53179},{\"end\":53192,\"start\":53191},{\"end\":53200,\"start\":53199},{\"end\":53208,\"start\":53207},{\"end\":53215,\"start\":53214},{\"end\":53506,\"start\":53505},{\"end\":53518,\"start\":53517},{\"end\":53532,\"start\":53531},{\"end\":53829,\"start\":53825},{\"end\":53836,\"start\":53835},{\"end\":54051,\"start\":54050},{\"end\":54058,\"start\":54057},{\"end\":54064,\"start\":54063},{\"end\":54074,\"start\":54073},{\"end\":54302,\"start\":54301},{\"end\":54311,\"start\":54310},{\"end\":54317,\"start\":54316},{\"end\":54323,\"start\":54322},{\"end\":54332,\"start\":54331},{\"end\":54341,\"start\":54340},{\"end\":54349,\"start\":54348}]", "bib_author_last_name": "[{\"end\":42754,\"start\":42747},{\"end\":42770,\"start\":42758},{\"end\":42784,\"start\":42774},{\"end\":43190,\"start\":43186},{\"end\":43203,\"start\":43194},{\"end\":43221,\"start\":43207},{\"end\":43495,\"start\":43487},{\"end\":43507,\"start\":43499},{\"end\":43517,\"start\":43511},{\"end\":43739,\"start\":43736},{\"end\":43748,\"start\":43743},{\"end\":43758,\"start\":43755},{\"end\":43768,\"start\":43762},{\"end\":44086,\"start\":44082},{\"end\":44094,\"start\":44090},{\"end\":44107,\"start\":44098},{\"end\":44119,\"start\":44111},{\"end\":44132,\"start\":44123},{\"end\":44142,\"start\":44136},{\"end\":44519,\"start\":44514},{\"end\":44531,\"start\":44523},{\"end\":44538,\"start\":44535},{\"end\":44548,\"start\":44542},{\"end\":44839,\"start\":44833},{\"end\":44851,\"start\":44843},{\"end\":44861,\"start\":44855},{\"end\":45131,\"start\":45124},{\"end\":45141,\"start\":45135},{\"end\":45149,\"start\":45147},{\"end\":45159,\"start\":45153},{\"end\":45369,\"start\":45361},{\"end\":45377,\"start\":45373},{\"end\":45390,\"start\":45381},{\"end\":45666,\"start\":45662},{\"end\":45680,\"start\":45670},{\"end\":45690,\"start\":45684},{\"end\":45963,\"start\":45953},{\"end\":45980,\"start\":45967},{\"end\":45989,\"start\":45984},{\"end\":45995,\"start\":45993},{\"end\":46011,\"start\":45999},{\"end\":46020,\"start\":46015},{\"end\":46033,\"start\":46024},{\"end\":46043,\"start\":46037},{\"end\":46341,\"start\":46333},{\"end\":46350,\"start\":46345},{\"end\":46363,\"start\":46354},{\"end\":46372,\"start\":46367},{\"end\":46381,\"start\":46376},{\"end\":46576,\"start\":46570},{\"end\":46589,\"start\":46580},{\"end\":46599,\"start\":46593},{\"end\":46612,\"start\":46605},{\"end\":46624,\"start\":46616},{\"end\":46886,\"start\":46876},{\"end\":46901,\"start\":46890},{\"end\":47021,\"start\":47019},{\"end\":47030,\"start\":47027},{\"end\":47039,\"start\":47034},{\"end\":47052,\"start\":47043},{\"end\":47299,\"start\":47287},{\"end\":47307,\"start\":47303},{\"end\":47319,\"start\":47311},{\"end\":47332,\"start\":47323},{\"end\":47343,\"start\":47336},{\"end\":47353,\"start\":47347},{\"end\":47368,\"start\":47357},{\"end\":47640,\"start\":47634},{\"end\":47646,\"start\":47644},{\"end\":47847,\"start\":47841},{\"end\":47858,\"start\":47851},{\"end\":48066,\"start\":48064},{\"end\":48077,\"start\":48070},{\"end\":48088,\"start\":48083},{\"end\":48325,\"start\":48322},{\"end\":48335,\"start\":48329},{\"end\":48344,\"start\":48339},{\"end\":48553,\"start\":48550},{\"end\":48562,\"start\":48557},{\"end\":48816,\"start\":48809},{\"end\":48827,\"start\":48820},{\"end\":48836,\"start\":48831},{\"end\":49109,\"start\":49103},{\"end\":49121,\"start\":49113},{\"end\":49131,\"start\":49125},{\"end\":49146,\"start\":49135},{\"end\":49155,\"start\":49150},{\"end\":49464,\"start\":49462},{\"end\":49471,\"start\":49468},{\"end\":49478,\"start\":49475},{\"end\":49489,\"start\":49484},{\"end\":49498,\"start\":49493},{\"end\":49842,\"start\":49835},{\"end\":49850,\"start\":49846},{\"end\":49862,\"start\":49854},{\"end\":50179,\"start\":50175},{\"end\":50188,\"start\":50183},{\"end\":50195,\"start\":50192},{\"end\":50209,\"start\":50199},{\"end\":50220,\"start\":50213},{\"end\":50227,\"start\":50224},{\"end\":50543,\"start\":50536},{\"end\":50554,\"start\":50547},{\"end\":50566,\"start\":50558},{\"end\":50810,\"start\":50805},{\"end\":50823,\"start\":50814},{\"end\":51049,\"start\":51041},{\"end\":51063,\"start\":51053},{\"end\":51074,\"start\":51067},{\"end\":51084,\"start\":51078},{\"end\":51095,\"start\":51088},{\"end\":51103,\"start\":51099},{\"end\":51413,\"start\":51403},{\"end\":51425,\"start\":51417},{\"end\":51442,\"start\":51429},{\"end\":51696,\"start\":51689},{\"end\":51708,\"start\":51702},{\"end\":51954,\"start\":51947},{\"end\":51962,\"start\":51958},{\"end\":51973,\"start\":51966},{\"end\":51981,\"start\":51977},{\"end\":52201,\"start\":52196},{\"end\":52213,\"start\":52209},{\"end\":52223,\"start\":52217},{\"end\":52495,\"start\":52481},{\"end\":52505,\"start\":52499},{\"end\":52516,\"start\":52509},{\"end\":52525,\"start\":52520},{\"end\":52533,\"start\":52529},{\"end\":52545,\"start\":52537},{\"end\":52812,\"start\":52800},{\"end\":52828,\"start\":52816},{\"end\":52840,\"start\":52832},{\"end\":52851,\"start\":52844},{\"end\":52861,\"start\":52855},{\"end\":53189,\"start\":53181},{\"end\":53197,\"start\":53193},{\"end\":53205,\"start\":53201},{\"end\":53212,\"start\":53209},{\"end\":53219,\"start\":53216},{\"end\":53515,\"start\":53507},{\"end\":53529,\"start\":53519},{\"end\":53541,\"start\":53533},{\"end\":53833,\"start\":53830},{\"end\":53842,\"start\":53837},{\"end\":54055,\"start\":54052},{\"end\":54061,\"start\":54059},{\"end\":54071,\"start\":54065},{\"end\":54082,\"start\":54075},{\"end\":54308,\"start\":54303},{\"end\":54314,\"start\":54312},{\"end\":54320,\"start\":54318},{\"end\":54329,\"start\":54324},{\"end\":54338,\"start\":54333},{\"end\":54346,\"start\":54342},{\"end\":54357,\"start\":54350},{\"end\":54367,\"start\":54359}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7124324},\"end\":43103,\"start\":42709},{\"attributes\":{\"doi\":\"CMU-CS-16-118\",\"id\":\"b1\"},\"end\":43481,\"start\":43105},{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b2\"},\"end\":43664,\"start\":43483},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16224674},\"end\":43976,\"start\":43666},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5002792},\"end\":44432,\"start\":43978},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b5\"},\"end\":44747,\"start\":44434},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1282515},\"end\":45102,\"start\":44749},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5965233},\"end\":45319,\"start\":45104},{\"attributes\":{\"id\":\"b8\"},\"end\":45585,\"start\":45321},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2659157},\"end\":45920,\"start\":45587},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1033682},\"end\":46299,\"start\":45922},{\"attributes\":{\"id\":\"b11\"},\"end\":46511,\"start\":46301},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1930231},\"end\":46848,\"start\":46513},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1915014},\"end\":47013,\"start\":46850},{\"attributes\":{\"doi\":\"arXiv:1602.05110\",\"id\":\"b14\"},\"end\":47283,\"start\":47015},{\"attributes\":{\"doi\":\"arXiv:1610.00527\",\"id\":\"b15\"},\"end\":47586,\"start\":47285},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6628106},\"end\":47835,\"start\":47588},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b17\"},\"end\":48023,\"start\":47837},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":536962},\"end\":48265,\"start\":48025},{\"attributes\":{\"doi\":\"arXiv:1703.00848\",\"id\":\"b19\"},\"end\":48502,\"start\":48267},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10627900},\"end\":48745,\"start\":48504},{\"attributes\":{\"doi\":\"arXiv:1511.05440\",\"id\":\"b21\"},\"end\":49006,\"start\":48747},{\"attributes\":{\"doi\":\"arXiv:1612.00005\",\"id\":\"b22\"},\"end\":49387,\"start\":49008},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3147510},\"end\":49737,\"start\":49389},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11758569},\"end\":50123,\"start\":49739},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1563370},\"end\":50445,\"start\":50125},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16935709},\"end\":50801,\"start\":50447},{\"attributes\":{\"doi\":\"arXiv:1611.06624\",\"id\":\"b27\"},\"end\":50998,\"start\":50803},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1687220},\"end\":51339,\"start\":51000},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11699847},\"end\":51658,\"start\":51341},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15426108},\"end\":51868,\"start\":51660},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2814088},\"end\":52145,\"start\":51870},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2187805},\"end\":52429,\"start\":52147},{\"attributes\":{\"doi\":\"arXiv:1701.08435\",\"id\":\"b33\"},\"end\":52743,\"start\":52431},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14989939},\"end\":53107,\"start\":52745},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":24069181},\"end\":53464,\"start\":53109},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9933254},\"end\":53758,\"start\":53466},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3131710},\"end\":53987,\"start\":53760},{\"attributes\":{\"id\":\"b38\"},\"end\":54299,\"start\":53989},{\"attributes\":{\"doi\":\"arXiv:1612.03242\",\"id\":\"b39\"},\"end\":54692,\"start\":54301}]", "bib_title": "[{\"end\":42743,\"start\":42709},{\"end\":43732,\"start\":43666},{\"end\":44078,\"start\":43978},{\"end\":44827,\"start\":44749},{\"end\":45120,\"start\":45104},{\"end\":45357,\"start\":45321},{\"end\":45658,\"start\":45587},{\"end\":45949,\"start\":45922},{\"end\":46566,\"start\":46513},{\"end\":46872,\"start\":46850},{\"end\":47630,\"start\":47588},{\"end\":48060,\"start\":48025},{\"end\":48543,\"start\":48504},{\"end\":49458,\"start\":49389},{\"end\":49831,\"start\":49739},{\"end\":50171,\"start\":50125},{\"end\":50530,\"start\":50447},{\"end\":51037,\"start\":51000},{\"end\":51399,\"start\":51341},{\"end\":51685,\"start\":51660},{\"end\":51943,\"start\":51870},{\"end\":52192,\"start\":52147},{\"end\":52796,\"start\":52745},{\"end\":53177,\"start\":53109},{\"end\":53503,\"start\":53466},{\"end\":53823,\"start\":53760},{\"end\":54048,\"start\":53989}]", "bib_author": "[{\"end\":42756,\"start\":42745},{\"end\":42772,\"start\":42756},{\"end\":42786,\"start\":42772},{\"end\":43192,\"start\":43184},{\"end\":43205,\"start\":43192},{\"end\":43223,\"start\":43205},{\"end\":43497,\"start\":43485},{\"end\":43509,\"start\":43497},{\"end\":43519,\"start\":43509},{\"end\":43741,\"start\":43734},{\"end\":43750,\"start\":43741},{\"end\":43760,\"start\":43750},{\"end\":43770,\"start\":43760},{\"end\":44088,\"start\":44080},{\"end\":44096,\"start\":44088},{\"end\":44109,\"start\":44096},{\"end\":44121,\"start\":44109},{\"end\":44134,\"start\":44121},{\"end\":44144,\"start\":44134},{\"end\":44521,\"start\":44512},{\"end\":44533,\"start\":44521},{\"end\":44540,\"start\":44533},{\"end\":44550,\"start\":44540},{\"end\":44841,\"start\":44829},{\"end\":44853,\"start\":44841},{\"end\":44863,\"start\":44853},{\"end\":45133,\"start\":45122},{\"end\":45143,\"start\":45133},{\"end\":45151,\"start\":45143},{\"end\":45161,\"start\":45151},{\"end\":45371,\"start\":45359},{\"end\":45379,\"start\":45371},{\"end\":45392,\"start\":45379},{\"end\":45668,\"start\":45660},{\"end\":45682,\"start\":45668},{\"end\":45692,\"start\":45682},{\"end\":45965,\"start\":45951},{\"end\":45982,\"start\":45965},{\"end\":45991,\"start\":45982},{\"end\":45997,\"start\":45991},{\"end\":46013,\"start\":45997},{\"end\":46022,\"start\":46013},{\"end\":46035,\"start\":46022},{\"end\":46045,\"start\":46035},{\"end\":46343,\"start\":46331},{\"end\":46352,\"start\":46343},{\"end\":46365,\"start\":46352},{\"end\":46374,\"start\":46365},{\"end\":46383,\"start\":46374},{\"end\":46578,\"start\":46568},{\"end\":46591,\"start\":46578},{\"end\":46601,\"start\":46591},{\"end\":46614,\"start\":46601},{\"end\":46626,\"start\":46614},{\"end\":46888,\"start\":46874},{\"end\":46903,\"start\":46888},{\"end\":47023,\"start\":47015},{\"end\":47032,\"start\":47023},{\"end\":47041,\"start\":47032},{\"end\":47054,\"start\":47041},{\"end\":47301,\"start\":47285},{\"end\":47309,\"start\":47301},{\"end\":47321,\"start\":47309},{\"end\":47334,\"start\":47321},{\"end\":47345,\"start\":47334},{\"end\":47355,\"start\":47345},{\"end\":47370,\"start\":47355},{\"end\":47642,\"start\":47632},{\"end\":47648,\"start\":47642},{\"end\":47849,\"start\":47837},{\"end\":47860,\"start\":47849},{\"end\":48068,\"start\":48062},{\"end\":48079,\"start\":48068},{\"end\":48090,\"start\":48079},{\"end\":48327,\"start\":48317},{\"end\":48337,\"start\":48327},{\"end\":48346,\"start\":48337},{\"end\":48555,\"start\":48545},{\"end\":48564,\"start\":48555},{\"end\":48818,\"start\":48807},{\"end\":48829,\"start\":48818},{\"end\":48838,\"start\":48829},{\"end\":49111,\"start\":49101},{\"end\":49123,\"start\":49111},{\"end\":49133,\"start\":49123},{\"end\":49148,\"start\":49133},{\"end\":49157,\"start\":49148},{\"end\":49466,\"start\":49460},{\"end\":49473,\"start\":49466},{\"end\":49480,\"start\":49473},{\"end\":49491,\"start\":49480},{\"end\":49500,\"start\":49491},{\"end\":49844,\"start\":49833},{\"end\":49852,\"start\":49844},{\"end\":49864,\"start\":49852},{\"end\":50181,\"start\":50173},{\"end\":50190,\"start\":50181},{\"end\":50197,\"start\":50190},{\"end\":50211,\"start\":50197},{\"end\":50222,\"start\":50211},{\"end\":50229,\"start\":50222},{\"end\":50545,\"start\":50532},{\"end\":50556,\"start\":50545},{\"end\":50568,\"start\":50556},{\"end\":50812,\"start\":50803},{\"end\":50825,\"start\":50812},{\"end\":51051,\"start\":51039},{\"end\":51065,\"start\":51051},{\"end\":51076,\"start\":51065},{\"end\":51086,\"start\":51076},{\"end\":51097,\"start\":51086},{\"end\":51105,\"start\":51097},{\"end\":51415,\"start\":51401},{\"end\":51427,\"start\":51415},{\"end\":51444,\"start\":51427},{\"end\":51698,\"start\":51687},{\"end\":51710,\"start\":51698},{\"end\":51956,\"start\":51945},{\"end\":51964,\"start\":51956},{\"end\":51975,\"start\":51964},{\"end\":51983,\"start\":51975},{\"end\":52203,\"start\":52194},{\"end\":52215,\"start\":52203},{\"end\":52225,\"start\":52215},{\"end\":52497,\"start\":52479},{\"end\":52507,\"start\":52497},{\"end\":52518,\"start\":52507},{\"end\":52527,\"start\":52518},{\"end\":52535,\"start\":52527},{\"end\":52547,\"start\":52535},{\"end\":52814,\"start\":52798},{\"end\":52830,\"start\":52814},{\"end\":52842,\"start\":52830},{\"end\":52853,\"start\":52842},{\"end\":52863,\"start\":52853},{\"end\":53191,\"start\":53179},{\"end\":53199,\"start\":53191},{\"end\":53207,\"start\":53199},{\"end\":53214,\"start\":53207},{\"end\":53221,\"start\":53214},{\"end\":53517,\"start\":53505},{\"end\":53531,\"start\":53517},{\"end\":53543,\"start\":53531},{\"end\":53835,\"start\":53825},{\"end\":53844,\"start\":53835},{\"end\":54057,\"start\":54050},{\"end\":54063,\"start\":54057},{\"end\":54073,\"start\":54063},{\"end\":54084,\"start\":54073},{\"end\":54310,\"start\":54301},{\"end\":54316,\"start\":54310},{\"end\":54322,\"start\":54316},{\"end\":54331,\"start\":54322},{\"end\":54340,\"start\":54331},{\"end\":54348,\"start\":54340},{\"end\":54359,\"start\":54348},{\"end\":54369,\"start\":54359}]", "bib_venue": "[{\"end\":42882,\"start\":42786},{\"end\":43182,\"start\":43105},{\"end\":43808,\"start\":43770},{\"end\":44193,\"start\":44144},{\"end\":44510,\"start\":44434},{\"end\":44912,\"start\":44863},{\"end\":45201,\"start\":45161},{\"end\":45443,\"start\":45392},{\"end\":45741,\"start\":45692},{\"end\":46094,\"start\":46045},{\"end\":46329,\"start\":46301},{\"end\":46670,\"start\":46626},{\"end\":46921,\"start\":46903},{\"end\":47123,\"start\":47070},{\"end\":47406,\"start\":47386},{\"end\":47700,\"start\":47648},{\"end\":47906,\"start\":47875},{\"end\":48134,\"start\":48090},{\"end\":48315,\"start\":48267},{\"end\":48613,\"start\":48564},{\"end\":48805,\"start\":48747},{\"end\":49099,\"start\":49008},{\"end\":49549,\"start\":49500},{\"end\":49916,\"start\":49864},{\"end\":50273,\"start\":50229},{\"end\":50612,\"start\":50568},{\"end\":50877,\"start\":50841},{\"end\":51154,\"start\":51105},{\"end\":51488,\"start\":51444},{\"end\":51754,\"start\":51710},{\"end\":51987,\"start\":51983},{\"end\":52277,\"start\":52225},{\"end\":52477,\"start\":52431},{\"end\":52912,\"start\":52863},{\"end\":53272,\"start\":53221},{\"end\":53592,\"start\":53543},{\"end\":53863,\"start\":53844},{\"end\":54133,\"start\":54084},{\"end\":54469,\"start\":54385}]"}}}, "year": 2023, "month": 12, "day": 17}