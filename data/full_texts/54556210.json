{"id": 54556210, "updated": "2023-03-18 13:41:50.924", "metadata": {"title": "Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images.", "authors": "[{\"first\":\"Korsuk\",\"last\":\"Sirinukunwattana\",\"middle\":[]},{\"first\":\"Shan\",\"last\":\"Ahmed Raza\",\"middle\":[\"E\"]},{\"first\":\"\",\"last\":\"Yee-Wah Tsang\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Snead\",\"middle\":[\"R\",\"J\"]},{\"first\":\"Ian\",\"last\":\"Cree\",\"middle\":[\"A\"]},{\"first\":\"Nasir\",\"last\":\"Rajpoot\",\"middle\":[\"M\"]}]", "venue": "IEEE transactions on medical imaging", "journal": "IEEE transactions on medical imaging", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2312404985", "acl": null, "pubmed": "26863654", "pubmedcentral": null, "dblp": "journals/tmi/Sirinukunwattana16", "doi": "10.1109/tmi.2016.2525803"}}, "content": {"source": {"pdf_hash": "6605814a6a0abac781bd8c15549f19a1aa958a52", "pdf_src": "Anansi", "pdf_uri": "[\"http://wrap.warwick.ac.uk/77351/7/WRAP_tmi2016_ks.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://wrap.warwick.ac.uk/77351/7/WRAP_tmi2016_ks.pdf", "status": "GREEN"}}, "grobid": {"id": "4635bf67c0a53464203e379b605b3339a1684f8b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6605814a6a0abac781bd8c15549f19a1aa958a52.txt", "contents": "\nhttp://wrap.warwick.ac.uk Original citation\n\n\nSirinukunwattana \nKorsuk \nShan-E-Ahmed Raza \nYee-Wah Tsang \nSnead \nDavid \nIan A Cree \nNasir M Rajpoot \nNasir Mahmood \nhttp://wrap.warwick.ac.uk Original citation\n10.1109/TMI.2016.2525803Permanent WRAP url: Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. Copyright \u00a9 and all moral rights to the version of the paper presented here belong to the individual author(s) and/or other copyright owners. To the extent reasonable and practicable the material made available in WRAP has been checked for eligibility before being made available. Copies of full items can be used for personal research or study, educational, or not-for profit purposes without prior permission or charge. Provided that the authors, title and full bibliographic details are credited, a hyperlink and/or URL is given for the original metadata page and the content is not changed in any way.\n2016) Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Transactions on Medical Imaging.\n\nunderstanding of tumor but also explore various options for cancer treatment. One way to explore these cell types is to use multiple protein markers which can mark different cells in cancer tissues. That will, however, require deep biological understanding of tumors to identify informative markers and is costly in terms of laboratory time and the use of tissue to study cell types [5]. An alternative and more efficient approach which is taken in this paper is to use morphological clues in local neighborhoods to develop automated cellular recognition via image analysis which can then be deployed for sophisticated tissue morphometry in future [6].\n\nThere are several factors that hinder automatic approaches for detection and classification of cell nuclei. First of all, the inferior image quality may arise due to poor fixation and poor staining during the tissue preparation process or autofocus failure during the digitization of slide. On the other hand, complex tissue architecture (Fig 1a), clutter of nuclei, and diversity of nuclear morphology pose a challenging problem. Particularly, in case of colorectal adenocarcinoma, dysplastic and malignant epithelial cells often have irregular chromatin texture and appear highly cluttered together with no clear boundary, which makes detection of individual nuclei a challenging task (Fig 1b). Variability in the appearance of the same type of nuclei, both within and across different sample, is another factor that makes classification of individual nucleus equivalently difficult.\n\nIn this paper, we present novel locality sensitive deep learning approaches to detect and classify nuclei in routine hematoxylin and eosin (H&E) stained histopathology images of colorectal adenocarcinoma, based on convolutional neural networks (CNNs). Standard CNN based methods follow a sliding window approach whereby the sliding window is centered at the pixel to be labeled or regressed. Our locality sensitive deep learning approach is based on two premises: (a) distance from the center of an object (nucleus, in this case) should be incorporated into the calculation of probability map for detecting that object, and (b) a weighted ensemble of local predictions for a class label can yield more accurate labeling of an object. For detection, we propose a spatially constrained CNN (SC-CNN), a new variant of CNN that includes parameter estimation layer and spatially constrained layer for spatial regression. SC-CNN can be trained to predict the probability of a pixel being the center of a nucleus. As opposed to other approaches [7], [8] that do not enforce the pixels close to the center of a nucleus to have a higher probability value than those further away, the predicted probability values produced by SC-CNN are topologically constrained such that high probability values are concentrated in the vicinity of the center of nuclei. For classification, we introduce neighboring ensemble predictor (NEP) to be used in conjunction with a standard softmax CNN. This predictor based on spatial ensembling leverages all relevant patch-based predictions in the local neighborhood of the nucleus to be classified, which in turn produces more accurate classification results than its single-patch based counterpart. Moreover, the proposed approaches for detection and classification do not require the difficult step of nucleus segmentation [9] which can be fairly challenging due to the reasons mentioned above. The proposed approach for detection and classification uses a sliding window to train the networks on small patches instead of the whole image. The use of small patches not only increases the amount of training data which is crucial for CNNs, but essentially also localizes our analysis to small nuclei in images [10], [11].\n\nThe organization of the paper is as follows. The review of recent literature on cell and nucleus detection and classification is given in Section II. We briefly introduce CNNs in Section III. Sections IV-VI describe the proposed approach in detail, and experimental results are presented in Section VII. Finally, we discuss some potential applications of the work, and conclude in Sections VIII and IX, respectively.\n\n\nII. RELATED WORK\n\nMost existing methods for cell and nucleus detection can be classified into one of the following categories: thresholding followed by morphological operations, region growing, level sets, k-means, and graph-cuts. Recently proposed techniques for nucleus detection in routine H&E histology images rely on morphological features such as symmetry and stability of the nuclear region to identify nuclei [12]. Cosatto et al. [13] proposed detection of cell nuclei using difference of Gaussian (DoG) followed by Hough transform to find radially symmetrical shapes. Al Kofahi et al. [14] proposed a graph cut-based method that is initialized using response of the image to Laplacian of Gaussian (LoG) filter. Kuse et al. [15] employed local phase symmetry to detect bilaterally symmetrical nuclei. Similarly, Veta et al. [16] relied on the direction of gradient to identify the center of nuclei. These methods may fail to detect spindle-like nuclei and irregular-shaped malignant epithelial nuclei. Arteta et al. [17] employed maximally stable extremal regions for detection, which is likely to fall victim to weakly stained nuclei or nuclei with irregular chromatin texture. Ali et al. [18] proposed an active contour-based approach to detect and segment overlapping nuclei based on shape models, which is highly variable in the case of tumor nuclei. In a recent study, Vink et al. [19] employed AdaBoost classifier to train two detectors, one using pixel-based features and the other based on Haar-like features, and merged the results of two detectors to detect the nuclei in immunohistochemistry stained breast tissue images. The performance of the method was found to be limited when detecting thin fibroblasts and small nuclei.\n\nCell and nucleus classification have been applied to diverse histopathology related applications. Dalle et al. [20] and Cosatto et al. [13] used shape, texture and size of nuclei for nuclear pleomorphism grading in breast cancer images. Malon et al. [21] trained CNN classifier to classify mitotic and non-mitotic cells using color, texture and shape information. Nguyen et al. [22] classified nuclei on the basis of their appearance into cancer and normal nuclei and used the location of detected nuclei to find cancer glands in prostate cancer. Yuan et al. [9] classified nuclei into cancer, lymphocyte or stromal based on the morphological features in H&E stained breast cancer images. This requires accurate segmentation of all the nuclei in the tissue including the tumor nuclei which is not straight forward due to the complex micro-architecture of tumor. Shape features have also been used in an unsupervised manifold learning framework [23] to discriminate between normal and malignant nuclei in prostate histology images. Recently, Sharma et al. [24] proposed nuclei segmentation and classification using AdaBoost classifier using intensity, morphological and texture features. The work was focused on nuclei segmentation, whereas evaluation on classification performance was limited.\n\nRecent studies have shown that deep learning methods produce promising results on a large number of histopathological image datasets. Wang et al. [25] proposed a cascaded classifier which uses a combination of hand-crafted features and features learned through CNN to detect mitotic cells. Cruz et al. [26] showed that for detection of basal-cell carcinoma, features learned using deep learning approaches produce superior and stable results compared to pre-defined bag of feature and canonical (discrete cosine transform/wavelet transform based) representations. For nucleus detection, Xu et al. [7] used stacked sparse autoencoder to learn a highlevel representation of nuclear and non-nuclear objects in an unsupervised fashion. The higher level features are classified as nuclear or non-nuclear regions using a softmax classifier. Ciresan et al. [8] utilized CNNs for mitotic figure detection in breast cancer images, where CNNs were trained to regress the probability of belonging to a mitotic figure for each pixel, taking a patch centered at the pixel as context. Xie et al. [27] recently proposed structural regression CNN capable of learning a proximity map of cell nuclei and was shown by the authors to provide more accurate detection results. Another closely related deep learning work is by Xie et al. [28], which localizes nucleus centroids through a voting scheme.\n\nIn summary, most of the cell detection methods rely on shape of nuclei and stability of features for cell detection and on texture of the nuclei for classification. Deep learning approaches have been successfully used in the past for identification of nuclei in histopathology images, mostly for binary classification using pixel based approaches. In this paper, we propose a novel deep learning approach which is sensitive to local neighborhood and is capable of detecting and assigning class labels to multiple types of nuclei.\n\nIn terms of methodology for nucleus detection, our SC-CNN is closely related to the methods proposed by Xie et al. [27], [28]. SC-CNN contains a layer that is specifically designed to predict the centroid locations of nuclei as well as the confidence of the locations whether they correspond to true centroids. These parameters are in essence similar to the voting offset vectors and voting confidence in Xie et al. [28]. However, SC-CNN generates a probability mask for spatial regression based on these parameters rather than employing them directly for detection as done by Xie et al. [28]. Although, both Xie et al. [27] and our SC-CNN share the idea of using spatial regression as a means of localizing the nucleus centers, the regression in SC-CNN is model-based, which explicitly constrains the output form of the network. Thus, high probability values are likely to be assigned to the pixel locations in the vicinity of nucleus centers.\n\n\nIII. CONVOLUTIONAL NEURAL NETWORK\n\nA convolutional neural network (CNN) f is a composition of a sequence of L functions or layers (f 1 , .., f L ) that maps an input vector x to an output vector y, i.e.,\ny = f (x; w 1 , ..., w L ) = f L ( \u00b7 ; w L ) \u2022 f L\u22121 ( \u00b7 ; w L\u22121 ) \u2022 ... \u2022 f 2 ( \u00b7 ; w 2 ) \u2022 f 1 (x; w 1 ),(1)\nwhere w l is the weight and bias vector for the lth layer f l . Conventionally, f l is defined to perform one of the following operations: a) convolution with a bank of filters; b) spatial pooling; and c) non-linear activation. Given a set of N training data {(x (i) , y (i) )} N i=1 , we can estimate the vectors w 1 , .., w L by solving the optimization problem\nargmin w1,...,w L 1 N N i=1 (f (x (i) ; w 1 , ..., w L ), y (i) ),(2)\nwhere is an appropriately defined loss function. Numerical optimization of (2) is often performed via backpropagation and stochastic gradient descent methods.\n\n\nIV. NUCLEUS DETECTION\n\n\nA. Spatially Constrained Regression\n\nIn regression analysis, given a pair of input x and output y, the task is to estimate a function g that represents the relationship between both variables. The output y, however, may not only depend on the input x alone, but also on the topological domain (time, spatial domain, etc.) on which it is residing.\n\nLet \u2126 be the spatial domain of y, and suppose that the spatially constrained regression model g is known a priori, and is of the form y = g(\u2126; \u03b8(x)) where \u03b8(x) is an unknown parameter vector. We can employ CNN to estimate \u03b8(x) by extending the standard CNN such that the last two layers (f L\u22121 , f L ) of the network are defined as\n\u03b8(x) = f L\u22121 (x L\u22122 ; w L\u22121 ), (3) y = f L (\u2126; \u03b8(x)) \u2261 g (\u2126; \u03b8(x)) ,(4)\nwhere x L\u22122 is an output of the (L\u22122)th layer of the network, (3) is the new parameter estimation layer and (4) is the layer imposing the spatial constraints.\n\n\nB. Nucleus Detection Using a Spatially Constrained CNN\n\nGiven an image patch x \u2208 R H\u00d7W \u00d7D with its height H, width W , and the number of its features D, our aim is to detect the center of each nucleus contained in x (Fig. 2a). To tackle this problem, we first define the training output y \u2208 [0, 1] H \u00d7W as a probability map of size H \u00d7 W (Fig. 2b).\n\nLet \u2126 = {1, ..., H } \u00d7 {1, ..., W } be the spatial domain of y.\n\nThe jth element of y, j = 1, ..., |\u2126|, is defined as\ny j = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 1+( zj \u2212z 0 m 2 2 )/2 if \u2200 m = m , z j \u2212 z 0 m 2 \u2264 z j \u2212 z 0 m 2 \u2264 d, 0 otherwise,(5)\nwhere z j and z 0 m denote the coordinates of y j and the coordinates of the center of the mth nucleus on \u2126, respectively, Here, F is the fully connected layer, S1 is the new parameter estimation layer, S2 is the spatially constrained layer, and L is the total number of layers in the network. and d is a constant radius. Pictorially, the probability map defined by (5) has a high peak in the vicinity of the center of each nucleus z 0 m and flat elsewhere. Next, we define the predicted output y , generated from the spatially constrained layer (S2, the Lth layer) of the network (Fig. 2c). Following the known structure of the probability map in the training output described in (5), we define the jth element of the predicted output y as\n(a) W \u2032 H \u2032 (b) (c)y j = g(z j ;\u1e91 0 1 , ...,\u1e91 0 M , h 1 , ..., h M ) = \uf8f1 \uf8f2 \uf8f3 1 1+( zj \u2212\u1e91 0 m 2 2 )/2 h m if \u2200 m = m , z j \u2212\u1e91 0 m 2 \u2264 z j \u2212\u1e91 0 m 2 \u2264 d, 0 otherwise,(6)\nwhere\u1e91 0 m \u2208 \u2126 is the estimated center and h m \u2208 [0, 1] is the height of the mth probability mask, and M denotes the maximum number of the probability masks on y . Note that y defined in this way allows the number of predicted nuclei to vary from 0 to M because of the redundancy provided by h m = 0 or\u1e91 0 m =\u1e91 0 m for m = m . In our experiments, we set d in (5) and (6) to 4 pixels as to provide enough support area to the probability mask.\n\nThe parameters\u1e91 0 m = (u m , v m ) and h m are estimated in the parameter estimation layer (S1, the (L \u2212 1)th layer, as shown in Fig. 2c). Let x L\u22122 be the output of the (L \u2212 2)th layer of the network. We define u m , v m , h m as\nu m = (H \u2212 1) \u00b7 sigm(W L\u22121,um \u00b7 x L\u22122 + b um ) + 1, (7) v m = (W \u2212 1) \u00b7 sigm(W L\u22121,vm \u00b7 x L\u22122 + b vm ) + 1, (8) h m = sigm(W L\u22121,hm \u00b7 x L\u22122 + b hm ),(9)\nwhere W L\u22121,um , W L\u22121,vm , W L\u22121,hm denote the weight vectors and b um , b vm , b hm denote the bias variables, and sigm(\u00b7) denotes the sigmoid function.\n\nTo learn all the variables (i.e., weight vectors and bias values) in the network, we solve (2) using the following loss function:\n(y, y ) = j (y j + )H(y j , y j ),(10)\nwhere H(y j , y j ) is the cross-entropy loss [29]- [31] defined by\nH(y j , y j ) = \u2212 y j log(y j ) \u2212 (1 \u2212 y j ) log(1 \u2212 y j ) ,(11)\nand is a small constant, which is set to be the ratio of the total number of non-zero probability pixels and zero probability pixels in the training output data. The first term of the product in (10) is a weight term that penalizes the loss contributed by the output pixels with small probability. This is crucial as, in the training output data, there are a large number of pixels with zero probability as compared to the non-zero probability ones.\n\nFinally, to detect the center of nuclei from a big image, we use the sliding window strategy with overlapping windows. Since we use full-patch regression, the predicted probability of being the center of a nucleus is generated for each of the extracted patches using (6). These results are then aggregated to form a probability map. That is, for each pixel location, we average the probability values from all the patches containing that pixel. The final detection is obtained from the local maxima found in the probability map. In order to avoid overdetection, a threshold defined as a fraction of the maximum probability value found on the probability map is introduced. All local maxima whose probability values are less than the threshold are not considered in the detection. In our experiments, the threshold was empirically determined from the training data set.\n\n\nV. NUCLEUS CLASSIFICATION\n\nWe treat the problem of nucleus classification as patch-based multiclass classification. Let x \u2208 R H\u00d7W \u00d7D be an input patch containing a nucleus to be classified in the proximity of its center, and let c \u2208 {1, ..., C} denote the corresponding label of patch x, in which C is the total number of classes.\n\nWe train a CNN to produce an output vector y (x) \u2208 R C in the last layer of the network such that\nc = argmax j y j (x),(12)\nwhere y j (x) denotes the jth element of y (x). The following logarithmic loss is employed to train the network via solving (2): and p j (y (x)) is a softmax function [29]- [31] defined by\n(c, y (x)) = \u2212 log (p c (y (x))) ,(13)p j (y (x)) = exp(y j (x)) k exp(y k (x)) .(14)\nWe refer to this classification CNN as softmax CNN. In this work, we employ two strategies for predicting the class label as described below.\n\n\nA. Standard Single-Patch Predictor (SSPP)\n\nPredicted class c for input patch x with corresponding output y (x) from the network is given by\nc = argmax j p j (y (x)),(15)\nwhere y j (x) denotes the jth element of y (x). See Fig. 3a for illustration.\n\n\nB. Neighboring Ensemble Predictor (NEP)\n\nLet X I \u2282 R W \u00d7H\u00d7D denote a set of all W \u00d7 H \u00d7 D patches of image I, and let \u2126 I denote a spatial domain of image I. For each patch x \u2208 X I , denote z(x) \u2208 \u2126 I as the location at which x is centered. We now define a set of neighboring patches of x as\nB(x) = x (i) \u2208 X I : z(x (i) ) \u2212 z(x) 2 \u2264 d \u03b2 ,(16)\nwhich contains all patches centered in the ball of radius d \u03b2 centered at z(x).\n\nPredicted class c for an input patch x with corresponding network output y is given by\nc = argmax j x (i) \u2208B(x) w(z(x (i) ))p j (y (x (i) )),(17)\nwhere w : \u2126 I \u2192 R is a function assigning weight to patch x (i) based on its center position z(x (i) ). Ensemble predictor (17), defined in this way, is essentially a weighted sum of all relevant predictors (Fig. 3b). It takes into account uncertainty in detection of the center location, as well as, variability in the appearance of nuclei. In our experiments (Section VII-C), the NEP (17) shows higher classification performance than that of the single-patch predictor (15).\n\nIn all experiments, we set d \u03b2 = 4 pixels in (16) so as to allow patches in B(x) to cover the majority area of a nucleus to be classified, and we set a uniform weight w(z(x (i) )) = 1/|B(x)| for all x (i) \u2208 B(x) in (17).\n\n\nVI. NETWORK ARCHITECTURE AND TRAINING DETAILS\n\n\nA. Architectures\n\nThe detailed architectures of the spatially constrained CNN (SC-CNN) for nucleus detection and the softmax CNN for nucleus classification are shown in Table I. These architectures are inspired by [8] and [32]. The networks consisted of conventional layers, including input, convolution, non-overlapping spatial max-pooling, and fully-connected layers. SC-CNN, in addition, included parameter estimation and spatially constrained layers. In both networks, a rectified linear unit (ReLU) activation function [32] was used after each convolution layer and the first two fully-connected layers (1st, 3rd, 5th and 6th layer). To avoid over-fitting, dropout [33] was implemented in the first-two fully-connected layers (5th and 6th layer, after ReLU is applied) with a dropout rate of 0.2. We implemented both networks using MatConvNet [34].\n\nThe input features were selected with respect to the task. In nucleus detection, we selected hematoxylin intensity as an input feature to SC-CNN for each patch. Because nucleic acids inside the nucleus is stained by hematoxylin, this feature is a reasonably good representation of localization of the nuclei. The hematoxylin intensity was obtained using a recently proposed color deconvolution method [35]. In classification, morphology of nuclei (shape, size, color, and texture) is necessary to distinguish between different types of them. Raw RGB color intensities which constitute the overall visual appearance of nuclei were, thus, chosen as input features to softmax CNN for each patch. For both networks, we set input patch size to W \u00d7 H = 27 \u00d7 27 since the majority of the nuclei in a dataset used in our experiments (Section VII-A) have their size within this limit. We set the output patch size for SC-CNN to W \u00d7 H = 11 \u00d7 11. Based on this output patch size, we found that the number of nuclei contained in the training output patches is mostly less than or equal to 2. Thus, the maximum number of predicted nuclei allowed in the S1 layer of SC-CNN was set to M = 1 or M = 2, accordingly.\n\n\nB. Training Data Augmentation\n\nFor both networks, we arbitrary rotated patches (0 \u2022 , 90 \u2022 , 180 \u2022 , 270 \u2022 ) and flipped them along vertical or horizontal axis to alleviate the rotation-variant problem of the input features. To make softmax CNN persist to variability in color distribution, which is commonly found in histology images, we also arbitrary perturbed the color distribution of training patches. This was accomplished in HSV space, where hue (H), saturation (S), and value (V) variables were separately multiplied by random numbers r H \u2208 [0.95, 1.05], \nInput/Output Dimensions 0 I 27 \u00d7 27 \u00d7 1 I 27 \u00d7 27 \u00d7 1 1 C 4 \u00d7 4 \u00d7 1 \u00d7 36 24 \u00d7 24 \u00d7 36 C 4 \u00d7 4 \u00d7 1 \u00d7 36 24 \u00d7 24 \u00d7 36 2 M 2 \u00d7 2 12 \u00d7 12 \u00d7 36 M 2 \u00d7 2 12 \u00d7 12 \u00d7 36 3 C 3 \u00d7 3 \u00d7 36 \u00d7 48 10 \u00d7 10 \u00d7 48 C 3 \u00d7 3 \u00d7 36 \u00d7 48 10 \u00d7 10 \u00d7 48 4 M 2 \u00d7 2 5 \u00d7 5 \u00d7 48 M 2 \u00d7 2 5 \u00d7 5 \u00d7 48 5 F 5 \u00d7 5 \u00d7 48 \u00d7 512 1 \u00d7 512 F 5 \u00d7 5 \u00d7 48 \u00d7 512 1 \u00d7 512 6 F 1 \u00d7 1 \u00d7 512 \u00d7 512 1 \u00d7 512 F 1 \u00d7 1 \u00d7 512 \u00d7 512 1 \u00d7 512 7 S1 1 \u00d7 1 \u00d7 512 \u00d7 3 1 \u00d7 3 F 1 \u00d7 1 \u00d7 512 \u00d7 4 1 \u00d7 4 8 S2 11 \u00d7 11\nr S , r V \u2208 [0.9, 1.1], respectively. In addition, we extracted multiple patches of the same nucleus at different locations to account for location-variant that could negatively affect the classification performance of softmax CNN. This over-sample also allowed us to account for the class unbalance problem inherited in the dataset.\n\n\nC. Initialization and Training of the Networks\n\nWe initialized all weights with 0 mean and 10 \u22122 standard deviation Gaussian random numbers. All biases were set to 0. The networks were trained using stochastic gradient descent with momentum 0.9 and weight decay 5 \u00d7 10 \u22124 for 120 epochs. We annealed the learning rate, starting from 10 \u22122 for the first 60 epochs, then 10 \u22123 for the next 40 epochs, and 10 \u22124 for the last 20 epochs. We used 20% of training data for validation. The optimal networks for SC-CNN and softmax CNN were selected based on the root mean square error and the classification error on the validation set, respectively.\n\n\nVII. EXPERIMENTS AND RESULTS\n\n\nA. The Dataset 1\n\nThis study involves 100 H&E stained histology images of colorectal adenocarcinomas. All images have a common size of 500 \u00d7 500 pixels, and were cropped from non-overlapping areas of 10 whole-slide images from 9 patients, at a pixel resolution of 0.55\u00b5m/pixel (20\u00d7 optical magnification). The whole-slide images were obtained using an Omnyx VL120 scanner. The cropping areas were selected to represent a variety of tissue appearance from both normal and malignant regions of the slides. This also comprises areas with artifacts, over-staining, and failed autofocussing to represent outliers normally found in real scenarios.\n\nManual annotation of nuclei were conducted mostly by an experienced pathologist (YT) and partly by a graduate student under supervision of and validation by the same pathologist. A total number of 29, 756 nuclei were marked at the center 1 The dataset is available at http://www.warwick.ac.uk/BIAlab/data/ CRChistoLabeledNucleiHE.\n\n\nEpithelial Nuclei\n\n\nInflammatory Nuclei\n\nFibroblast Nuclei for detection purposes. Out of those, there were 22, 444 nuclei that also have an associated class label, i.e. epithelial, inflammatory, fibroblast, and miscellaneous. The remaining 7, 312 nuclei were unlabeled. The types of nuclei that were labeled as inflammatory include lymphocyte plasma, neutrophil and eosinophil. The nuclei that do not fall into the first three categories (i.e., epithelial, inflammatory, and fibroblast) such as adipocyte, endothelium, mitotic figure, nucleus of necrotic (i.e., dead) cell, etc. are labeled as miscellaneous. In total, there are 7, 722 epithelial, 5, 712 fibroblast, 6, 971 inflammatory, and 2, 039 miscellaneous nuclei. Fig. 4 shows some examples of the nuclei in the dataset.\n\n\nMiscellaneous Nuclei\n\n\nB. Detection\n\nThe objective of this experiment is to detect all nuclei in an image by locating their center positions, regardless of their class labels. Details of the dataset used in this experiment are as described above in Section VII-A. 1) Evaluation: Precision, Recall, and F1 score were used to quantitatively assess the detection performance. Here, we define the region within the radius of 6 pixels from the annotated center of the nucleus as ground truth. If there are multiple detected points within the same ground truth region, only the one closest to the annotated center is considered as true positive. The statistics, including median, 1st quartile, and 3rd quartile were also calculated to summarize the positively skewed distribution of the Euclidean distance between the detected points and their nearest annotated center of nucleus.\n\n2) Other Approaches: The following nucleus detection algorithms were selected for comparison. Firstly, center-of-thepixel CNN (CP-CNN) follows conventional method for object detection, where given a patch, it calculates the probability of being the center of a nucleus for a pixel at the center of the patch. Secondly, structural regression (SR-CNN) [27] extends the concept of a single-pixel regression of CP-CNN into a full-patch regression. The architectures of both CP-CNN and SR-CNN were set to resemble that of SC-CNN, except that the parameter estimation and the spatial constrained layers were replaced by a regression layer with a single node for CP-CNN and a regression layer with the number of nodes equal to the output patch size for SR-CNN. The training output patches for SR-CNN were the same as those for SC-CNN (see Fig. 2b for an example), whereas only the center pixel of those patches were used to train CP-CNN. Thirdly, stacked sparse autoencoder (SSAE) [7] consists of two sparse autoencoder layers followed by a softmax classifier which is trained to distinguish between nuclear and non-nuclear patches. If classified as a nucleus, all pixels inside the output patch are assigned the value of 1, or 0 otherwise. We set input feature, sizes of input and output patches of SSAE to be the same as those of SC-CNN. Fourthly, local isotropic phase symmetry measure (LIPSyM) yields high response values near the center of symmetric nuclei which can be used for detection. Lastly, CRImage [9] segments all nuclei with help of thresholding, followed by morphological operation, distance transform, and watershed. A centroid of individual segmented nucleus was used as the detected point. Note that for a fair comparison the detection of the center of nuclei for CP-CNN, SR-CNN, SSAE, and LIPSyM was done in the same fashion as that of SC-CNN (see last paragraph of Section IV-B for details). We implemented CP-CNN using MatConvNet [34], while CRImage is publicly available as an R package 2 . The implementations of SR-CNN and LIPSyM were provided by the authors, and we used a set of build-in functions in Matlab to implement SSAE.\n\n\n3) Comparative Results:\n\nWe employed 2-fold crossvalidation (50 images/fold) in the experiment. Fig. 5 shows the precision-recall curve. The curve was generated by varying the threshold value applied to the predicted probability map before locating local maxima to avoid false positive detection. Table II reports the detection results when the threshold value is empirically chosen to optimize F1 score on the training fold. Detailed results are shown in Fig. SF1 and Fig. SF2 of the supplementary material. Here, we include two variants of SC-CNN; one with the maximum number of predicted nuclei in an output patch equal to 1 (M = 1) and the other equal to 2 (M = 2).  Overall, the comparison is in favor of the algorithms that learn to predict the probability of being the center of a nucleus, based on spatial context of the whole patch, i.e. SC-CNN and SR-CNN. This is consistent with the quantitative results shown in Fig. 6c and Fig. SF2. Visual inspection of the probability generated by SC-CNN ( Fig. 6b and Fig. SF1a) and SR-CNN (Fig. SF1c) revealed that the pixels with high probability values are mostly located in the vicinity of the center of nuclei. However, SC-CNN, as imposed by spatial constrained in (6), has a narrower spread of pixels with high probability values. Probability maps generated by the CP-CNN (Fig. SF1b) and SSAE (Fig. SF1d), on the other hand, exhibit a wider spread of pixels with high probabilities away from the center of nuclei. This results in SC-CNN and SR-CNN yielding a lower value of median distance between the detected points and the annotated ground truth, as compared to those values for CP-CNN and SSAE. LIPSyM heavily relies on bilateral symmetry of nuclei for detection. For this reason, it could not precisely detect spindle-like and other irregularly-shaped nuclei such as fibroblasts and malignant epithelial nuclei (see Fig. SF2e). Due to the nature of nuclei in the dataset which often appear to have a weak boundary and/or overlapping boundaries, algorithms which require segmentation to detect nuclei such as CRImage are thus likely to fail for those cases. Given that a single output patch can contain multiple nucleus centers (mostly less than or equal to 2 in our experiment), SC-CNN (M = 2) thus gives a better performance than SC-CNN (M = 1). Nonetheless, the improvement mainly comes from the reduction in the false positive rate rather than the false negative rate. One of the possible explanations is that SC-CNN (M = 1) tries to compensate the loss incurred from not being able to regress the probability of multiple nucleus centers during the training process by giving a high confidence, i.e. h m close to 1, to the estimated nuclear center. Thus, it tends to produce false detection as compared to SC-CNN (M = 2).\n\nSC-CNN and SR-CNN share a closely resembling idea of using spatial regression to generate the probability map of nucleus center. In essence, SC-CNN uses a known spatial structure for regression, whereas SR-CNN directly learns the structure from the training output data. SR-CNN, hence, is more flexible in general. However, when the structure for regression is known and governed by a small number of parameters such as in this problem, SC-CNN can provide more advantages. Firstly, it simplifies the learning process of CNN by imposing the output functional form of the network. Secondly, the regressed structure is always consistent with that of the training data. This results in SC-CNN (M = 2) yielding better performance in terms of F1 score than SR-CNN. For further discussion about the detection performance when nuclei are stratified by their class label, see Section VII-D2.\n\n\nC. Classification\n\nThe setting of this experiment is to classify patches of size 27 \u00d7 27 pixels, containing a nucleus at the center, into 4 classes: epithelial, inflammatory, fibroblast, and miscellaneous. Full details of the dataset can be found in Section VII-A.\n\n\n1) Evaluation:\n\nWe calculated the F1 score for each class of nucleus and their average weighted by the number of nucleus samples (see Section VII-A for details) to summarize the overall classification performance. We also considered an area under the receiver operating characteristic curve for multiclass classification (multiclass AUC) [36]. Multiclass AUC measures the probability that given a pair of samples with different class labels, a classifier will assign a high prediction score for class, say c, to the sample from class c, as compared to the sample from the other class. Here, the prediction score for softmax CNN is given by softmax function (14).\n\n2) Other Approaches: First, superpixel descriptor [37] combines color and texture information, cumulated in superpixels, for classifying areas with different histologic pattern. This descriptor is used in conjunction with a random forest classifier. In the experiment, we treated a patch as a single superpixel. We implemented this method in Matlab according to the details outlined in [37]. Second, CRImage 3 [9] calculates a list of statistics 4 for each segmented nucleus and uses the support vector machine with radial basis kernel as a classifier. A successive spatial density smoothing is used to correct false classification. We implemented this method in Matlab.\n\n3) Comparative Results: Fig. 7 and Table III show the comparative classification performance on 2-fold cross-validation 3 CRImage is available as an R package (http://www.bioconductor.org/ packages/CRImage/). As of Sep 1st, 2015, the package version 1.16.0 has a certain compatibility issue with package EBImage 4.10.1 and cannot reproduce the results on the test samples given in the manual of the package. We did our best to implement the method, but disclaim a perfect replication. 4 See sweave file of [9] for details.   [37] 0.687 0.853 CRImage [9] 0.488 0.684 experiment. For detailed results, see Table ST1 of the supplementary material. The comparison is in favor of softmax CNN with the NEP in every nucleus class. The better performance of NEP than that of SSPP allows us to hypothesize that NEP is more resilient to variability in the appearance of nuclei. The superpixel descriptor [37] is originally devised to distinguish area with different histologic pattern. This descriptor does not directly contain features related to visual appearance of the nucleus, and thus yielded low classification performance as compared to softmax CNN. CRImage calculates features based the segmentation of the nuclei. As previously discussed, a reliable segmentation is difficult to be achieved when the nuclear boundary is weakly stained or there are overlapping boundaries. This prevents CRImage to perform well in this dataset. Another interesting observation is that all considered approaches including ours suffer from class imbalance problem. The classification performance declines as the number of the samples decreases (see Fig. 7 and Table ST1).\n\n\nD. Combined Detection and Classification\n\nIn this experiment, we combine detection and classification as a single work flow. Nuclei are first detected and then classified. We consider the combinations of the top-performing approaches in each task for comparison. These are SC-CNN and SR-CNN for detection and softmax CNN together with SSPP and NEP for classification.\n\n\n1) Evaluation:\n\nWe calculated F1 score for combined detection and classification separately for each class label. For in- Fig. 8. Combined performance on nucleus detection and classification stratified according to class label. stance, consider class c. Precision is defined as the proportion between the number of correctly detected and classified class c nuclei and the total number of class c nuclei in the ground truth. The definition of Recall is the proportion between the number of correctly detected and classified class c nuclei and the total number of all detected objects classified as class c nuclei. The criterion for true positive for nucleus detection is outlined in Section VII-B1. Note that, in the dataset, the number of nuclei that have a class label associated with is smaller than the total number of annotated nuclei. Therefore, we only considered the labeled nuclei as ground truth and restrict the evaluation to the area covering by patches of size 41 \u00d7 41 pixels, centered at each ground truth nucleus in the image. To summarize the overall performance, we also calculated the weighted average F1 score, where the weight term for each nucleus class is defined by the number of data samples in that class (see Section VII-A for details).\n! \" #$ % && ! \" $ % && ' % && ! \" #$ % & ! \" $ % & ' % &\n2) Comparative Results: Fig. 8 and Table IV report the combined performance on detection and classification on a 2fold cross validation experiment. See also Table ST2 of the supplementary material for the detailed results. As expected from the results in Section VII-C, the combinations that employ softmax CNN with SSPP performs better than their counterparts. SC-CNN and SR-CNN when combined with softmax CNN with SSPP yield similar F1 scores across different classes of nuclei, except for epithelial (irregular-shaped) and fibroblast (elongated-shaped) where SC-CNN (M = 2) performs better. The same trend of performance can be seen for the combinations that employ softmax CNN with NEP.\n\n\nVIII. DISCUSSION\n\nHistopathological data are often incomplete and contaminated with subjectivity, which is also the case for the dataset used in this study. This is unavoidable due to the sheer number of cell nuclei and the enormous variation in morphology, making it difficult to identify all cells with certainty. In fact most pathologist rely on low power architecture to build the main picture of what is going on, using high power cellular morphological details to confirm or reject the initial impressions. Identifying individual cells on high power features alone without architectural clues will increase their misclassification. As previously mentioned, immunohistochemistry that stains a specific type of cell would provide a decisive judgement, yet it is costly and practically difficult to deal with in the laboratory, compared with H&E staining. That, nonetheless, could provide a stronger objective validation for the future work.\n\nThe architectures for CNN presented in this work were empirically chosen on the basis of resources available at hand. Finding theoretical justification for choosing an optimal network architecture is still an ongoing search. A large network architecture would allow more variation in high-level representations of objects, at the expense of training time and other resources. Yet, with random initialization of large number of network parameters, a gradient-based optimization may get stuck in a poor local minima. One could explore different strategies for training the network as described in [31]. There is also an issue of choosing network input features. For nucleus detection, we found that hematoxylin intensity could provide better results than standard RGB intensities. On the other hand, for nucleus classification, RGB did better than hematoxylin, but there is no significant difference between the results obtained using RGB and other standard color spaces such as LAB and HSV. Selecting a set of input features is task dependent, and suitable features should reduce the complexity of the task and allow better results.\n\nInspection of the results from all nucleus classification methods in Section VII-C revealed that the majority of misclassified nuclei often appeared isolated and biologically implausible, considering their spatial positions on the original images. Yuan et al. [9] proposed the use of hierarchical spatial smoothing to correct misclassification. However, it should be pointed out that this type of correction should be used with caution, as it may falsely eliminate biologically important phenomena such as tumor budding which consists of a small number of tumor nuclei [38] or isolated islands of tumor nuclei appearing at the invasive front of the tumor. In our experiments, we did not employ any spatial correction.\n\nAutomatic approaches for combined nucleus detection and classification could offer benefits to pathology practice in a number of ways. One of the potential applications is to locate and identify all tissue-constituent nuclei in whole-slide images. Fig. 9 shows the detection and classification results produced by SC-CNN and softmax CNN with NEP on a whole-slide image. This could facilitate quantitative analysis, and at the same time, remove tediousness and reduce subjectivity in pathological routine. This is an interesting prospect for future research and yet to be validated in a large-scale study.\n\nExisting distributed computing technologies such as parallel computing and graphical processing unit (GPU) are important key factors to scale up the proposed framework for whole-slide histology images. In our experiment, a whole-slide image is first divided into small tiles of size 1, 000 \u00d7 1, 000 pixels. On a single 2.5 GHz CPU, the average execution time on an individual image tile is 47.6s (preprocessing 27.8s, detection 18.4s, and classification 1.4s). For a given whole-slide image captured at 20\u00d7 optical magnification and consisting of 60, 000 \u00d7 50, 000 pixel dimensions, there are 750 tiles of size 1, 000 \u00d7 1, 000 pixels to be processed assuming only 25% of the slide contains tissue. Theoretically speaking, by using a 12-core processor machine, the average execution time of the proposed detection and classification framework is around 50 mins per slide. However, it should be noted that the execution time reported here is for a research-grade implementation of the framework which has not yet been fully optimized to increase time efficiency, nor did we employ the computational power of GPUs which can significantly speed up the execution time for CNNs.\n\n\nIX. CONCLUSIONS\n\nIn this study, we presented deep learning approaches sensitive to the local neighborhood for nucleus detection and classification in routine stained histology images of colorectal adenocarcinomas. The evaluation was conducted on a large dataset with more than 20, 000 annotated nuclei from samples of different histologic grades. The comparison is in favor of the proposed spatially-constrained CNN for nucleus detection and the softmax CNN with the proposed neighboring ensemble predictor for nucleus classification. The combination of the two could potentially offer a systematic quantitative analysis of tissue morphology, and tissue constituents, lending itself to be a useful tool for better understanding of the tumor microenvironment. \n\nFig. 1 .\n1An example of the cancer region in colorectal adenocarcinoma. (a) disarray of intestinal glandular architecture. (b) clutter of malignant epithelial nuclei due to high proliferation rate.\n\nFig. 2 .\n2An illustration of the proposed spatially constrained CNN. (a) An input patch x of size H \u00d7 W from an image. (b) A training output patch y of size H \u00d7 W from a probability map showing the probability of being the center of nuclei. (c) An illustration of the last three layers of the proposed spatially constraint CNN.\n\nFig. 3 .\n3An illustration of the prediction strategies used in conjunction with softmax CNN: (a) standard single-patch predictor; (b) neighboring ensemble predictor. An orange dot represents the center of the detected nucleus. The center of neighboring patches is represented as yellow dots.\n\nFig. 4 .\n4Example patches of different types of nuclei found in the dataset: 1st Row -Epithelial nuclei, 2nd Row -Inflammatory nuclei (from left to right, lymphocyte, plasma nucleus, neutrophil, and eosinophil), 3rd Row -Fibroblasts, 4th Row -Miscellaneous nuclei (from left to right, adipocyte, endothelial nucleus, mitotic figure, and necrotic nucleus).\n\nFig. 5 .\n5Precision-recall curve for nucleus detection. Isolines indicate regions of different F1 scores. The curve is generated by varying the value of threshold applied to the predicted probability map before locating local maxima. Note that this thresholding scheme does not apply to CRImage[9].\n\nFig. 6 .\n6Qualitative results for nucleus detection. (a) An example image. (b) Probability maps generated by SC-CNN (M = 2). The probability value in the probability map indicates the likelihood of a pixel being the center of a nucleus. Thus, one can detect the center of an individual nucleus based on the location of local maxima found in the probability map. (c) Detection results of SC-CNN (M = 2). Here, detected centers of the nuclei are shown as red dots and the ground truth areas are shown as green shaded circles. Probability maps and detection results of other methods can be seen in Fig. SF1 and Fig. SF2, respectively in supplementary material.\n\n\nFig. SF2f reflects this problem, where CRImage failed to detect malignant epithelial nuclei (top-left corner).\n\nFig. 7 .\n7Comparative results for nucleus classification stratified with respect to class label.\n\nFig. 9 .\n9Nucleus detection and classification on a whole-slide image. Detected epithelial, inflammatory and fibroblast nuclei are represented as red, green, and yellow dots, respectively. (a), (b) and (c) show the results overlaid on the image at 1\u00d7, 5\u00d7, and 20\u00d7, respectively. The blue rectangle in (a) contains the region shown in (b), and the blue rectangle in (b) contains the region shown in (c). The detection and classification were conducted at 20\u00d7 magnification using SC-CNN and softmax CNN with NEP. This figure is best viewed on screen with magnification 400%.\n\nTABLE I ARCHITECTURES\nIOF THE SPATIALLY CONSTRAINED CNN (SC-CNN) FOR NUCLEUS DETECTION AND SOFTMAX CNN FOR NUCLEUS CLASSIFICATION.THE NETWORKS CONSIST OF INPUT (I), CONVOLUTION (C), MAX-POOLING (M), FULLY-CONNECTED (F), PARAMETER ESTIMATION (S1), AND \nSPATIALLY CONSTRAINED (S2) LAYERS. \n\nLayer \nSC-CNN for detection \nsoftmax CNN for classification \n\nType \nFilter \nDimensions \n\nInput/Output \nDimensions \nType \nFilter \nDimensions \n\n\n\nTABLE II COMPARATIVE\nIIRESULTS FOR NUCLEUS DETECTION.Median, Q1 and Q3 refer to the median, 1st quartile and 3rd quartile of the distribution of the Euclidean distance between the detected points and their nearest annotated center of nucleus, respectively.Method \nPrecision Recall F1 score Median Distance (Q1, Q3) \nSC-CNN (M = 1) 0.758 0.827 0.791 \n2.236 (1.414, 5.099) \nSC-CNN (M = 2) 0.781 \n0.823 0.802 \n2.236 (1.414, 5) \nCP-CNN \n0.697 \n0.687 \n0.692 \n3.606 (2.236, 7.616) \nSR-CNN [27] \n0.783 0.804 \n0.793 \n2.236 (1.414, 5) \nSSAE [7] \n0.617 \n0.644 \n0.630 \n4.123 (2.236, 10) \nLIPSyM [15] \n0.725 \n0.517 \n0.604 \n2.236 (1.414 , 7.211) \nCRImage [9] \n0.657 \n0.461 \n0.542 \n3.071 (1.377, 9.022) \n\n\n\nTABLE III COMPARATIVE\nIIIRESULTS FOR NUCLEUS CLASSIFICATION.Method \nWeighted \nAverage F1 score \n\nMulticlass \nAUC \nsoftmax CNN + SSPP \n0.748 \n0.893 \nsoftmax CNN + NEP \n0.784 \n0.917 \nsuperpixel descriptor \n\nTABLE IV COMBINED\nIVPERFORMANCE ON NUCLEUS DETECTION AND CLASSIFICATION.Method \nWeighted \nAverage F1 score \nDetection \nClassification \n\nSC-CNN (M = 1) \nsoftmax CNN + SSPP \n0.664 \nsoftmax CNN + NEP \n0.688 \n\nSC-CNN (M = 2) \nsoftmax CNN + SSPP \n0.670 \nsoftmax CNN + NEP \n0.692 \n\nSR-CNN [27] \nsoftmax CNN + SSPP \n0.662 \nsoftmax CNN + NEP \n0.683 \n\n\nACKNOWLEDGMENTThis paper was made possible by NPRP grant number NPRP5-1345-1-228 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors. Korsuk Sirinukunwattana acknowledges the partial financial support provided by the Department of Computer Science, University of Warwick, UK. We also would like to thank the authors of[27]for sharing the implementation of SR-CNN used in our experiments.\nSingle-cell dissection of transcriptional heterogeneity in human colon tumors. P Dalerba, T Kalisky, D Sahoo, P S Rajendran, M E Rothenberg, A A Leyrat, S Sim, J Okamoto, D M Johnston, D Qian, Nature biotechnology. 2912P. Dalerba, T. Kalisky, D. Sahoo, P. S. Rajendran, M. E. Rothenberg, A. A. Leyrat, S. Sim, J. Okamoto, D. M. Johnston, D. Qian et al., \"Single-cell dissection of transcriptional heterogeneity in human colon tumors,\" Nature biotechnology, vol. 29, no. 12, pp. 1120-1127, 2011.\n\nA human colon cancer cell capable of initiating tumour growth in immunodeficient mice. C A Obrien, A Pollett, S Gallinger, J E Dick, Nature. 4457123C. A. OBrien, A. Pollett, S. Gallinger, and J. E. Dick, \"A human colon cancer cell capable of initiating tumour growth in immunodeficient mice,\" Nature, vol. 445, no. 7123, pp. 106-110, 2007.\n\nMulti-field-of-view strategy for image-based outcome prediction of multi-parametric estrogen receptorpositive breast cancer histopathology: comparison to oncotype dx. A Basavanhally, M Feldman, N Shih, C Mies, J Tomaszewski, S Ganesan, A Madabhushi, Journal of pathology informatics. 2A. Basavanhally, M. Feldman, N. Shih, C. Mies, J. Tomaszewski, S. Ganesan, and A. Madabhushi, \"Multi-field-of-view strategy for image-based outcome prediction of multi-parametric estrogen receptor- positive breast cancer histopathology: comparison to oncotype dx,\" Journal of pathology informatics, vol. 2, 2011.\n\nA quantitative histomorphometric classifier (quhbic) identifies aggressive versus indolent p16-positive oropharyngeal squamous cell carcinoma. J S LewisJr, S Ali, J Luo, W L Thorstad, A Madabhushi, The American journal of surgical pathology. 381J. S. Lewis Jr, S. Ali, J. Luo, W. L. Thorstad, and A. Madabhushi, \"A quantitative histomorphometric classifier (quhbic) identifies aggressive versus indolent p16-positive oropharyngeal squamous cell carcinoma,\" The American journal of surgical pathology, vol. 38, no. 1, pp. 128-137, 2014.\n\nCell type heterogeneity of cytokeratin expression in complex epithelia and carcinomas as demonstrated by monoclonal antibodies specific for cytokeratins nos. 4 and 13. G N Van Muijen, D J Ruiter, W W Franke, T Achtsttter, W H Haasnoot, M Ponec, S O Warnaar, Experimental Cell Research. 1621G. N. van Muijen, D. J. Ruiter, W. W. Franke, T. Achtsttter, W. H. Haasnoot, M. Ponec, and S. O. Warnaar, \"Cell type heterogeneity of cytokeratin expression in complex epithelia and carcinomas as demon- strated by monoclonal antibodies specific for cytokeratins nos. 4 and 13,\" Experimental Cell Research, vol. 162, no. 1, pp. 97 -113, 1986.\n\nMethods for nuclei detection, segmentation, and classification in digital histopathology: A review. current status and future potential. H Irshad, A Veillard, L Roux, D Racoceanu, Biomedical Engineering. 7IEEE Reviews inH. Irshad, A. Veillard, L. Roux, and D. Racoceanu, \"Methods for nuclei detection, segmentation, and classification in digital histopathology: A review. current status and future potential,\" Biomedical Engineering, IEEE Reviews in, vol. 7, pp. 97-114, 2014.\n\nStacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images. J Xu, L Xiang, Q Liu, H Gilmore, J Wu, J Tang, A Madabhushi, IEEE Transactions on. 99Medical ImagingJ. Xu, L. Xiang, Q. Liu, H. Gilmore, J. Wu, J. Tang, and A. Madab- hushi, \"Stacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images,\" Medical Imaging, IEEE Transactions on, vol. PP, no. 99, pp. 1-1, 2015.\n\nMitosis detection in breast cancer histology images with deep neural networks. D C Cire\u015fan, A Giusti, L M Gambardella, J Schmidhuber, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013. SpringerD. C. Cire\u015fan, A. Giusti, L. M. Gambardella, and J. Schmidhuber, \"Mitosis detection in breast cancer histology images with deep neu- ral networks,\" in Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013. Springer, 2013, pp. 411-418.\n\nQuantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling. Y Yuan, H Failmezger, O M Rueda, H R Ali, S Gr\u00e4f, S.-F Chin, R F Schwarz, C Curtis, M J Dunning, H Bardwell, Science translational medicine. 4157Y. Yuan, H. Failmezger, O. M. Rueda, H. R. Ali, S. Gr\u00e4f, S.-F. Chin, R. F. Schwarz, C. Curtis, M. J. Dunning, H. Bardwell et al., \"Quantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling,\" Science translational medicine, vol. 4, no. 157, pp. 157ra143-157ra143, 2012.\n\nDeep neural networks segment neuronal membranes in electron microscopy images. D Ciresan, A Giusti, L M Gambardella, J Schmidhuber, Advances in neural information processing systems. D. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber, \"Deep neural networks segment neuronal membranes in electron microscopy images,\" in Advances in neural information processing systems, 2012, pp. 2843-2851.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, arXiv:1505.04597arXiv preprintO. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" arXiv preprint arXiv:1505.04597, 2015.\n\nBreast cancer histopathology image analysis: A review. M Veta, J Pluim, P Van Diest, M Viergever, Biomedical Engineering. 615IEEE Transactions onM. Veta, J. Pluim, P. van Diest, and M. Viergever, \"Breast cancer histopathology image analysis: A review,\" Biomedical Engineering, IEEE Transactions on, vol. 61, no. 5, pp. 1400-1411, May 2014.\n\nGrading nuclear pleomorphism on histological micrographs. E Cosatto, M Miller, H P Graf, J S Meyer, Pattern Recognition, 2008. ICPR 2008. 19th International Conference on. IEEEE. Cosatto, M. Miller, H. P. Graf, and J. S. Meyer, \"Grading nuclear pleomorphism on histological micrographs,\" in Pattern Recognition, 2008. ICPR 2008. 19th International Conference on. IEEE, 2008, pp. 1-4.\n\nImproved automatic detection and segmentation of cell nuclei in histopathology images. Y Al-Kofahi, W Lassoued, W Lee, B Roysam, Biomedical Engineering. 574IEEE Transactions onY. Al-Kofahi, W. Lassoued, W. Lee, and B. Roysam, \"Improved au- tomatic detection and segmentation of cell nuclei in histopathology images,\" Biomedical Engineering, IEEE Transactions on, vol. 57, no. 4, pp. 841-852, 2010.\n\nLocal isotropic phase symmetry measure for detection of beta cells and lymphocytes. M Kuse, Y.-F Wang, V Kalasannavar, M Khan, N Rajpoot, Journal of Pathology Informatics. 22M. Kuse, Y.-F. Wang, V. Kalasannavar, M. Khan, and N. Rajpoot, \"Local isotropic phase symmetry measure for detection of beta cells and lymphocytes,\" Journal of Pathology Informatics, vol. 2, no. 2, p. 2, 2011.\n\nAutomatic nuclei segmentation in h&e stained breast cancer histopathology images. M Veta, P J Van Diest, R Kornegoor, A Huisman, M A Viergever, J P W Pluim, PLoS ONE. 872013M. Veta, P. J. van Diest, R. Kornegoor, A. Huisman, M. A. Viergever, and J. P. W. Pluim, \"Automatic nuclei segmentation in h&e stained breast cancer histopathology images,\" PLoS ONE, vol. 8, no. 7, p. e70221, 07 2013.\n\nLearning to detect cells using non-overlapping extremal regions. C Arteta, V Lempitsky, J A Noble, A Zisserman, Medical image computing and computer-assisted intervention-MICCAI 2012. SpringerC. Arteta, V. Lempitsky, J. A. Noble, and A. Zisserman, \"Learning to detect cells using non-overlapping extremal regions,\" in Medical image computing and computer-assisted intervention-MICCAI 2012. Springer, 2012, pp. 348-356.\n\nAn integrated region-, boundary-, shapebased active contour for multiple object overlap resolution in histological imagery. S Ali, A Madabhushi, IEEE Transactions On. 317Medical ImagingS. Ali and A. Madabhushi, \"An integrated region-, boundary-, shape- based active contour for multiple object overlap resolution in histological imagery,\" Medical Imaging, IEEE Transactions On, vol. 31, no. 7, pp. 1448-1460, 2012.\n\nEfficient nucleus detector in histopathology images. J P Vink, M Van Leeuwen, C Van Deurzen, G De Haan, Journal of microscopy. 2492J. P. Vink, M. Van Leeuwen, C. Van Deurzen, and G. De Haan, \"Efficient nucleus detector in histopathology images,\" Journal of microscopy, vol. 249, no. 2, pp. 124-135, 2013.\n\nNuclear pleomorphism scoring by selective cell nuclei detection. J.-R Dalle, H Li, C.-H Huang, W K Leow, D Racoceanu, T C Putti, WACV. J.-R. Dalle, H. Li, C.-H. Huang, W. K. Leow, D. Racoceanu, and T. C. Putti, \"Nuclear pleomorphism scoring by selective cell nuclei detection.\" in WACV, 2009.\n\nClassification of mitotic figures with convolutional neural networks and seeded blob features. C D Malon, E Cosatto, Journal of pathology informatics. 4C. D. Malon and E. Cosatto, \"Classification of mitotic figures with convolutional neural networks and seeded blob features,\" Journal of pathology informatics, vol. 4, 2013.\n\nProstate cancer detection: Fusion of cytological and textural features. K Nguyen, A K Jain, B Sabata, Journal of pathology informatics. 2K. Nguyen, A. K. Jain, and B. Sabata, \"Prostate cancer detection: Fusion of cytological and textural features,\" Journal of pathology informatics, vol. 2, 2011.\n\nClassification of potential nuclei in prostate histology images using shape manifold learning. M Arif, N Rajpoot, International Conference on Machine Vision (ICMV). IEEEM. Arif and N. Rajpoot, \"Classification of potential nuclei in prostate histology images using shape manifold learning,\" in International Con- ference on Machine Vision (ICMV). IEEE, 2007, pp. 113-118.\n\nA multi-resolution approach for combining visual information using nuclei segmentation and classification in histopathological images. H Sharma, N Zerbe, D Heim, S Wienert, H.-M Behrens, O Hellwich, P Hufnagl, Proceedings of the 10th International Conference on Computer Vision Theory and Applications. the 10th International Conference on Computer Vision Theory and ApplicationsH. Sharma, N. Zerbe, D. Heim, S. Wienert, H.-M. Behrens, O. Hellwich, and P. Hufnagl, \"A multi-resolution approach for combining visual information using nuclei segmentation and classification in histopatho- logical images,\" in Proceedings of the 10th International Conference on Computer Vision Theory and Applications, 2015, pp. 37-46.\n\nCascaded ensemble of convolutional neural networks and handcrafted features for mitosis detection. H Wang, A Cruz-Roa, A Basavanhally, H Gilmore, N Shih, M Feldman, J Tomaszewski, F Gonzalez, A Madabhushi, 9041H. Wang, A. Cruz-Roa, A. Basavanhally, H. Gilmore, N. Shih, M. Feld- man, J. Tomaszewski, F. Gonzalez, and A. Madabhushi, \"Cascaded ensemble of convolutional neural networks and handcrafted features for mitosis detection,\" vol. 9041, 2014, pp. 90 410B-90 410B-10.\n\nA deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection. A A Cruz-Roa, J E A Ovalle, A Madabhushi, F A G Osorio, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013. SpringerA. A. Cruz-Roa, J. E. A. Ovalle, A. Madabhushi, and F. A. G. Osorio, \"A deep learning architecture for image representation, visual interpretabil- ity and automated basal-cell carcinoma cancer detection,\" in Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013. Springer, 2013, pp. 403-410.\n\nBeyond classification: Structured regression for robust cell detection using convolutional neural network. Y Xie, F Xing, X Kong, H Su, L Yang, Medical Image Computing and Computer-Assisted InterventionMICCAI. SpringerY. Xie, F. Xing, X. Kong, H. Su, and L. Yang, \"Beyond classifica- tion: Structured regression for robust cell detection using convolutional neural network,\" in Medical Image Computing and Computer-Assisted InterventionMICCAI 2015. Springer, 2015, pp. 358-365.\n\nDeep voting: A robust approach toward nucleus localization in microscopy images. Y Xie, X Kong, F Xing, F Liu, H Su, L Yang, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015. SpringerY. Xie, X. Kong, F. Xing, F. Liu, H. Su, and L. Yang, \"Deep voting: A robust approach toward nucleus localization in microscopy images,\" in Medical Image Computing and Computer-Assisted Intervention- MICCAI 2015. Springer, 2015, pp. 374-382.\n\nPattern recognition and machine learning. C M Bishop, springerC. M. Bishop, Pattern recognition and machine learning. springer, 2006.\n\nMachine learning: a probabilistic perspective. K P Murphy, MIT pressK. P. Murphy, Machine learning: a probabilistic perspective. MIT press, 2012.\n\nExploring strategies for training deep neural networks. H Larochelle, Y Bengio, J Louradour, P Lamblin, The Journal of Machine Learning Research. 10H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin, \"Exploring strategies for training deep neural networks,\" The Journal of Machine Learning Research, vol. 10, pp. 1-40, 2009.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in neural infor- mation processing systems, 2012, pp. 1097-1105.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of Machine Learning Research. 15N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting,\" Journal of Machine Learning Research, vol. 15, pp. 1929-1958, 2014. [Online]. Available: http://jmlr.org/papers/v15/srivastava14a.html\n\nMatconvnet -convolutional neural networks for matlab. A Vedaldi, K Lenc, abs/1412.4564CoRR. A. Vedaldi and K. Lenc, \"Matconvnet -convolutional neural networks for matlab,\" CoRR, vol. abs/1412.4564, 2014.\n\nA nonlinear mapping approach to stain normalization in digital histopathology images using image-specific color deconvolution. A M Khan, N Rajpoot, D Treanor, D Magee, Biomedical Engineering. 616IEEE Transactions onA. M. Khan, N. Rajpoot, D. Treanor, and D. Magee, \"A nonlinear map- ping approach to stain normalization in digital histopathology images using image-specific color deconvolution,\" Biomedical Engineering, IEEE Transactions on, vol. 61, no. 6, pp. 1729-1738, 2014.\n\nA simple generalisation of the area under the roc curve for multiple class classification problems. D J Hand, R J Till, Machine learning. 452D. J. Hand and R. J. Till, \"A simple generalisation of the area under the roc curve for multiple class classification problems,\" Machine learning, vol. 45, no. 2, pp. 171-186, 2001.\n\nA novel texture descriptor for detection of glandular structures in colon histology images. K Sirinukunwattana, D R Snead, N M Rajpoot, SPIE Medical Imaging. International Society for Optics and Photonics. K. Sirinukunwattana, D. R. Snead, and N. M. Rajpoot, \"A novel texture descriptor for detection of glandular structures in colon histology images,\" in SPIE Medical Imaging. International Society for Optics and Photonics, 2015, pp. 94 200S-94 200S.\n\nTumor budding in colorectal carcinoma: time to take notice. B Mitrovic, D F Schaeffer, R H Riddell, R Kirsch, Modern Pathology. 2510B. Mitrovic, D. F. Schaeffer, R. H. Riddell, and R. Kirsch, \"Tumor budding in colorectal carcinoma: time to take notice,\" Modern Pathology, vol. 25, no. 10, pp. 1315-1325, 2012.\n", "annotations": {"author": "[{\"end\":64,\"start\":47},{\"end\":72,\"start\":65},{\"end\":91,\"start\":73},{\"end\":106,\"start\":92},{\"end\":113,\"start\":107},{\"end\":120,\"start\":114},{\"end\":132,\"start\":121},{\"end\":149,\"start\":133},{\"end\":164,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":63,\"start\":47},{\"end\":71,\"start\":65},{\"end\":90,\"start\":86},{\"end\":105,\"start\":100},{\"end\":112,\"start\":107},{\"end\":119,\"start\":114},{\"end\":131,\"start\":127},{\"end\":148,\"start\":141},{\"end\":163,\"start\":156}]", "author_first_name": "[{\"end\":85,\"start\":73},{\"end\":99,\"start\":92},{\"end\":124,\"start\":121},{\"end\":126,\"start\":125},{\"end\":138,\"start\":133},{\"end\":140,\"start\":139},{\"end\":155,\"start\":150}]", "author_affiliation": null, "title": "[{\"end\":44,\"start\":1},{\"end\":208,\"start\":165}]", "venue": null, "abstract": "[{\"end\":1198,\"start\":1037}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1586,\"start\":1583},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1851,\"start\":1848},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3782,\"start\":3779},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3787,\"start\":3784},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4589,\"start\":4586},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4975,\"start\":4971},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4981,\"start\":4977},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5824,\"start\":5820},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5845,\"start\":5841},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6001,\"start\":5997},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6139,\"start\":6135},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6239,\"start\":6235},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6431,\"start\":6427},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6605,\"start\":6601},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6801,\"start\":6797},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7264,\"start\":7260},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7288,\"start\":7284},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7403,\"start\":7399},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7531,\"start\":7527},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7711,\"start\":7708},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8097,\"start\":8093},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8208,\"start\":8204},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8594,\"start\":8590},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8750,\"start\":8746},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9044,\"start\":9041},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9297,\"start\":9294},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9530,\"start\":9526},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9763,\"start\":9759},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10475,\"start\":10471},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10481,\"start\":10477},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10776,\"start\":10772},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10948,\"start\":10944},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10980,\"start\":10976},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15832,\"start\":15828},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15838,\"start\":15834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16636,\"start\":16633},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17864,\"start\":17860},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17870,\"start\":17866},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19060,\"start\":19056},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19408,\"start\":19404},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19630,\"start\":19626},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19899,\"start\":19896},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19908,\"start\":19904},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20210,\"start\":20206},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20356,\"start\":20352},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20534,\"start\":20530},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20942,\"start\":20938},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24638,\"start\":24637},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26743,\"start\":26739},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27366,\"start\":27363},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27896,\"start\":27893},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28338,\"start\":28334},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32818,\"start\":32814},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33137,\"start\":33133},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33194,\"start\":33190},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33530,\"start\":33526},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33553,\"start\":33550},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33933,\"start\":33932},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34298,\"start\":34297},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34321,\"start\":34318},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34341,\"start\":34337},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34365,\"start\":34362},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34710,\"start\":34706},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":39393,\"start\":39389},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40190,\"start\":40187},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":40500,\"start\":40496},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44663,\"start\":44660}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43385,\"start\":43187},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43714,\"start\":43386},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44007,\"start\":43715},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44364,\"start\":44008},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44664,\"start\":44365},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45323,\"start\":44665},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45436,\"start\":45324},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45534,\"start\":45437},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46108,\"start\":45535},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46541,\"start\":46109},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47234,\"start\":46542},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47439,\"start\":47235},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47784,\"start\":47440}]", "paragraph": "[{\"end\":1852,\"start\":1200},{\"end\":2739,\"start\":1854},{\"end\":4982,\"start\":2741},{\"end\":5400,\"start\":4984},{\"end\":7147,\"start\":5421},{\"end\":8442,\"start\":7149},{\"end\":9823,\"start\":8444},{\"end\":10354,\"start\":9825},{\"end\":11300,\"start\":10356},{\"end\":11506,\"start\":11338},{\"end\":11981,\"start\":11618},{\"end\":12210,\"start\":12052},{\"end\":12583,\"start\":12274},{\"end\":12916,\"start\":12585},{\"end\":13147,\"start\":12989},{\"end\":13498,\"start\":13206},{\"end\":13563,\"start\":13500},{\"end\":13617,\"start\":13565},{\"end\":14462,\"start\":13722},{\"end\":15071,\"start\":14630},{\"end\":15303,\"start\":15073},{\"end\":15611,\"start\":15457},{\"end\":15742,\"start\":15613},{\"end\":15849,\"start\":15782},{\"end\":16364,\"start\":15915},{\"end\":17234,\"start\":16366},{\"end\":17567,\"start\":17264},{\"end\":17666,\"start\":17569},{\"end\":17881,\"start\":17693},{\"end\":18109,\"start\":17968},{\"end\":18251,\"start\":18155},{\"end\":18359,\"start\":18282},{\"end\":18653,\"start\":18403},{\"end\":18785,\"start\":18706},{\"end\":18873,\"start\":18787},{\"end\":19409,\"start\":18933},{\"end\":19631,\"start\":19411},{\"end\":20535,\"start\":19700},{\"end\":21735,\"start\":20537},{\"end\":22302,\"start\":21769},{\"end\":23078,\"start\":22745},{\"end\":23722,\"start\":23129},{\"end\":24397,\"start\":23774},{\"end\":24729,\"start\":24399},{\"end\":25510,\"start\":24773},{\"end\":26387,\"start\":25550},{\"end\":28535,\"start\":26389},{\"end\":31322,\"start\":28563},{\"end\":32206,\"start\":31324},{\"end\":32473,\"start\":32228},{\"end\":33138,\"start\":32492},{\"end\":33810,\"start\":33140},{\"end\":35463,\"start\":33812},{\"end\":35833,\"start\":35508},{\"end\":37097,\"start\":35852},{\"end\":37845,\"start\":37155},{\"end\":38792,\"start\":37866},{\"end\":39925,\"start\":38794},{\"end\":40644,\"start\":39927},{\"end\":41250,\"start\":40646},{\"end\":42424,\"start\":41252},{\"end\":43186,\"start\":42444}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11617,\"start\":11507},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12051,\"start\":11982},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12988,\"start\":12917},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13721,\"start\":13618},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14482,\"start\":14463},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14629,\"start\":14482},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15456,\"start\":15304},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15781,\"start\":15743},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15914,\"start\":15850},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17692,\"start\":17667},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17920,\"start\":17882},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17967,\"start\":17920},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18281,\"start\":18252},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18705,\"start\":18654},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18932,\"start\":18874},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22744,\"start\":22303},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37154,\"start\":37098}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19858,\"start\":19851},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28843,\"start\":28835},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33856,\"start\":33847},{\"end\":34425,\"start\":34416},{\"end\":35461,\"start\":35452},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":37198,\"start\":37190},{\"end\":37321,\"start\":37312}]", "section_header": "[{\"end\":5419,\"start\":5403},{\"end\":11336,\"start\":11303},{\"end\":12234,\"start\":12213},{\"end\":12272,\"start\":12237},{\"end\":13204,\"start\":13150},{\"end\":17262,\"start\":17237},{\"end\":18153,\"start\":18112},{\"end\":18401,\"start\":18362},{\"end\":19679,\"start\":19634},{\"end\":19698,\"start\":19682},{\"end\":21767,\"start\":21738},{\"end\":23127,\"start\":23081},{\"end\":23753,\"start\":23725},{\"end\":23772,\"start\":23756},{\"end\":24749,\"start\":24732},{\"end\":24771,\"start\":24752},{\"end\":25533,\"start\":25513},{\"end\":25548,\"start\":25536},{\"end\":28561,\"start\":28538},{\"end\":32226,\"start\":32209},{\"end\":32490,\"start\":32476},{\"end\":35506,\"start\":35466},{\"end\":35850,\"start\":35836},{\"end\":37864,\"start\":37848},{\"end\":42442,\"start\":42427},{\"end\":43196,\"start\":43188},{\"end\":43395,\"start\":43387},{\"end\":43724,\"start\":43716},{\"end\":44017,\"start\":44009},{\"end\":44374,\"start\":44366},{\"end\":44674,\"start\":44666},{\"end\":45446,\"start\":45438},{\"end\":45544,\"start\":45536},{\"end\":46131,\"start\":46110},{\"end\":46563,\"start\":46543},{\"end\":47257,\"start\":47236},{\"end\":47458,\"start\":47441}]", "table": "[{\"end\":46541,\"start\":46240},{\"end\":47234,\"start\":46799},{\"end\":47439,\"start\":47296},{\"end\":47784,\"start\":47513}]", "figure_caption": "[{\"end\":43385,\"start\":43198},{\"end\":43714,\"start\":43397},{\"end\":44007,\"start\":43726},{\"end\":44364,\"start\":44019},{\"end\":44664,\"start\":44376},{\"end\":45323,\"start\":44676},{\"end\":45436,\"start\":45326},{\"end\":45534,\"start\":45448},{\"end\":46108,\"start\":45546},{\"end\":46240,\"start\":46133},{\"end\":46799,\"start\":46566},{\"end\":47296,\"start\":47261},{\"end\":47513,\"start\":47461}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2200,\"start\":2192},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2549,\"start\":2541},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13375,\"start\":13366},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13497,\"start\":13488},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14311,\"start\":14303},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15210,\"start\":15202},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18341,\"start\":18334},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19149,\"start\":19140},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25460,\"start\":25454},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27228,\"start\":27221},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28640,\"start\":28634},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29015,\"start\":28994},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29482,\"start\":29462},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29564,\"start\":29543},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29588,\"start\":29577},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29876,\"start\":29865},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29897,\"start\":29886},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30423,\"start\":30414},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33842,\"start\":33836},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35447,\"start\":35441},{\"end\":35964,\"start\":35958},{\"end\":37185,\"start\":37179},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40900,\"start\":40894}]", "bib_author_first_name": "[{\"end\":48343,\"start\":48342},{\"end\":48354,\"start\":48353},{\"end\":48365,\"start\":48364},{\"end\":48374,\"start\":48373},{\"end\":48376,\"start\":48375},{\"end\":48389,\"start\":48388},{\"end\":48391,\"start\":48390},{\"end\":48405,\"start\":48404},{\"end\":48407,\"start\":48406},{\"end\":48417,\"start\":48416},{\"end\":48424,\"start\":48423},{\"end\":48435,\"start\":48434},{\"end\":48437,\"start\":48436},{\"end\":48449,\"start\":48448},{\"end\":48847,\"start\":48846},{\"end\":48849,\"start\":48848},{\"end\":48859,\"start\":48858},{\"end\":48870,\"start\":48869},{\"end\":48883,\"start\":48882},{\"end\":48885,\"start\":48884},{\"end\":49268,\"start\":49267},{\"end\":49284,\"start\":49283},{\"end\":49295,\"start\":49294},{\"end\":49303,\"start\":49302},{\"end\":49311,\"start\":49310},{\"end\":49326,\"start\":49325},{\"end\":49337,\"start\":49336},{\"end\":49843,\"start\":49842},{\"end\":49845,\"start\":49844},{\"end\":49856,\"start\":49855},{\"end\":49863,\"start\":49862},{\"end\":49870,\"start\":49869},{\"end\":49872,\"start\":49871},{\"end\":49884,\"start\":49883},{\"end\":50405,\"start\":50404},{\"end\":50407,\"start\":50406},{\"end\":50421,\"start\":50420},{\"end\":50423,\"start\":50422},{\"end\":50433,\"start\":50432},{\"end\":50435,\"start\":50434},{\"end\":50445,\"start\":50444},{\"end\":50459,\"start\":50458},{\"end\":50461,\"start\":50460},{\"end\":50473,\"start\":50472},{\"end\":50482,\"start\":50481},{\"end\":50484,\"start\":50483},{\"end\":51007,\"start\":51006},{\"end\":51017,\"start\":51016},{\"end\":51029,\"start\":51028},{\"end\":51037,\"start\":51036},{\"end\":51443,\"start\":51442},{\"end\":51449,\"start\":51448},{\"end\":51458,\"start\":51457},{\"end\":51465,\"start\":51464},{\"end\":51476,\"start\":51475},{\"end\":51482,\"start\":51481},{\"end\":51490,\"start\":51489},{\"end\":51865,\"start\":51864},{\"end\":51867,\"start\":51866},{\"end\":51878,\"start\":51877},{\"end\":51888,\"start\":51887},{\"end\":51890,\"start\":51889},{\"end\":51905,\"start\":51904},{\"end\":52355,\"start\":52354},{\"end\":52363,\"start\":52362},{\"end\":52377,\"start\":52376},{\"end\":52379,\"start\":52378},{\"end\":52388,\"start\":52387},{\"end\":52390,\"start\":52389},{\"end\":52397,\"start\":52396},{\"end\":52408,\"start\":52404},{\"end\":52416,\"start\":52415},{\"end\":52418,\"start\":52417},{\"end\":52429,\"start\":52428},{\"end\":52439,\"start\":52438},{\"end\":52441,\"start\":52440},{\"end\":52452,\"start\":52451},{\"end\":52892,\"start\":52891},{\"end\":52903,\"start\":52902},{\"end\":52913,\"start\":52912},{\"end\":52915,\"start\":52914},{\"end\":52930,\"start\":52929},{\"end\":53280,\"start\":53279},{\"end\":53295,\"start\":53294},{\"end\":53306,\"start\":53305},{\"end\":53547,\"start\":53546},{\"end\":53555,\"start\":53554},{\"end\":53564,\"start\":53563},{\"end\":53577,\"start\":53576},{\"end\":53891,\"start\":53890},{\"end\":53902,\"start\":53901},{\"end\":53912,\"start\":53911},{\"end\":53914,\"start\":53913},{\"end\":53922,\"start\":53921},{\"end\":53924,\"start\":53923},{\"end\":54305,\"start\":54304},{\"end\":54318,\"start\":54317},{\"end\":54330,\"start\":54329},{\"end\":54337,\"start\":54336},{\"end\":54701,\"start\":54700},{\"end\":54712,\"start\":54708},{\"end\":54720,\"start\":54719},{\"end\":54736,\"start\":54735},{\"end\":54744,\"start\":54743},{\"end\":55084,\"start\":55083},{\"end\":55092,\"start\":55091},{\"end\":55094,\"start\":55093},{\"end\":55107,\"start\":55106},{\"end\":55120,\"start\":55119},{\"end\":55131,\"start\":55130},{\"end\":55133,\"start\":55132},{\"end\":55146,\"start\":55145},{\"end\":55150,\"start\":55147},{\"end\":55459,\"start\":55458},{\"end\":55469,\"start\":55468},{\"end\":55482,\"start\":55481},{\"end\":55484,\"start\":55483},{\"end\":55493,\"start\":55492},{\"end\":55938,\"start\":55937},{\"end\":55945,\"start\":55944},{\"end\":56283,\"start\":56282},{\"end\":56285,\"start\":56284},{\"end\":56293,\"start\":56292},{\"end\":56308,\"start\":56307},{\"end\":56323,\"start\":56322},{\"end\":56604,\"start\":56600},{\"end\":56613,\"start\":56612},{\"end\":56622,\"start\":56618},{\"end\":56631,\"start\":56630},{\"end\":56633,\"start\":56632},{\"end\":56641,\"start\":56640},{\"end\":56654,\"start\":56653},{\"end\":56656,\"start\":56655},{\"end\":56925,\"start\":56924},{\"end\":56927,\"start\":56926},{\"end\":56936,\"start\":56935},{\"end\":57228,\"start\":57227},{\"end\":57238,\"start\":57237},{\"end\":57240,\"start\":57239},{\"end\":57248,\"start\":57247},{\"end\":57549,\"start\":57548},{\"end\":57557,\"start\":57556},{\"end\":57961,\"start\":57960},{\"end\":57971,\"start\":57970},{\"end\":57980,\"start\":57979},{\"end\":57988,\"start\":57987},{\"end\":58002,\"start\":57998},{\"end\":58013,\"start\":58012},{\"end\":58025,\"start\":58024},{\"end\":58643,\"start\":58642},{\"end\":58651,\"start\":58650},{\"end\":58663,\"start\":58662},{\"end\":58679,\"start\":58678},{\"end\":58690,\"start\":58689},{\"end\":58698,\"start\":58697},{\"end\":58709,\"start\":58708},{\"end\":58724,\"start\":58723},{\"end\":58736,\"start\":58735},{\"end\":59151,\"start\":59150},{\"end\":59153,\"start\":59152},{\"end\":59165,\"start\":59164},{\"end\":59169,\"start\":59166},{\"end\":59179,\"start\":59178},{\"end\":59193,\"start\":59192},{\"end\":59197,\"start\":59194},{\"end\":59704,\"start\":59703},{\"end\":59711,\"start\":59710},{\"end\":59719,\"start\":59718},{\"end\":59727,\"start\":59726},{\"end\":59733,\"start\":59732},{\"end\":60157,\"start\":60156},{\"end\":60164,\"start\":60163},{\"end\":60172,\"start\":60171},{\"end\":60180,\"start\":60179},{\"end\":60187,\"start\":60186},{\"end\":60193,\"start\":60192},{\"end\":60566,\"start\":60565},{\"end\":60568,\"start\":60567},{\"end\":60706,\"start\":60705},{\"end\":60708,\"start\":60707},{\"end\":60862,\"start\":60861},{\"end\":60876,\"start\":60875},{\"end\":60886,\"start\":60885},{\"end\":60899,\"start\":60898},{\"end\":61201,\"start\":61200},{\"end\":61215,\"start\":61214},{\"end\":61228,\"start\":61227},{\"end\":61230,\"start\":61229},{\"end\":61550,\"start\":61549},{\"end\":61564,\"start\":61563},{\"end\":61574,\"start\":61573},{\"end\":61588,\"start\":61587},{\"end\":61601,\"start\":61600},{\"end\":61994,\"start\":61993},{\"end\":62005,\"start\":62004},{\"end\":62272,\"start\":62271},{\"end\":62274,\"start\":62273},{\"end\":62282,\"start\":62281},{\"end\":62293,\"start\":62292},{\"end\":62304,\"start\":62303},{\"end\":62725,\"start\":62724},{\"end\":62727,\"start\":62726},{\"end\":62735,\"start\":62734},{\"end\":62737,\"start\":62736},{\"end\":63041,\"start\":63040},{\"end\":63061,\"start\":63060},{\"end\":63063,\"start\":63062},{\"end\":63072,\"start\":63071},{\"end\":63074,\"start\":63073},{\"end\":63463,\"start\":63462},{\"end\":63475,\"start\":63474},{\"end\":63477,\"start\":63476},{\"end\":63490,\"start\":63489},{\"end\":63492,\"start\":63491},{\"end\":63503,\"start\":63502}]", "bib_author_last_name": "[{\"end\":48351,\"start\":48344},{\"end\":48362,\"start\":48355},{\"end\":48371,\"start\":48366},{\"end\":48386,\"start\":48377},{\"end\":48402,\"start\":48392},{\"end\":48414,\"start\":48408},{\"end\":48421,\"start\":48418},{\"end\":48432,\"start\":48425},{\"end\":48446,\"start\":48438},{\"end\":48454,\"start\":48450},{\"end\":48856,\"start\":48850},{\"end\":48867,\"start\":48860},{\"end\":48880,\"start\":48871},{\"end\":48890,\"start\":48886},{\"end\":49281,\"start\":49269},{\"end\":49292,\"start\":49285},{\"end\":49300,\"start\":49296},{\"end\":49308,\"start\":49304},{\"end\":49323,\"start\":49312},{\"end\":49334,\"start\":49327},{\"end\":49348,\"start\":49338},{\"end\":49851,\"start\":49846},{\"end\":49860,\"start\":49857},{\"end\":49867,\"start\":49864},{\"end\":49881,\"start\":49873},{\"end\":49895,\"start\":49885},{\"end\":50418,\"start\":50408},{\"end\":50430,\"start\":50424},{\"end\":50442,\"start\":50436},{\"end\":50456,\"start\":50446},{\"end\":50470,\"start\":50462},{\"end\":50479,\"start\":50474},{\"end\":50492,\"start\":50485},{\"end\":51014,\"start\":51008},{\"end\":51026,\"start\":51018},{\"end\":51034,\"start\":51030},{\"end\":51047,\"start\":51038},{\"end\":51446,\"start\":51444},{\"end\":51455,\"start\":51450},{\"end\":51462,\"start\":51459},{\"end\":51473,\"start\":51466},{\"end\":51479,\"start\":51477},{\"end\":51487,\"start\":51483},{\"end\":51501,\"start\":51491},{\"end\":51875,\"start\":51868},{\"end\":51885,\"start\":51879},{\"end\":51902,\"start\":51891},{\"end\":51917,\"start\":51906},{\"end\":52360,\"start\":52356},{\"end\":52374,\"start\":52364},{\"end\":52385,\"start\":52380},{\"end\":52394,\"start\":52391},{\"end\":52402,\"start\":52398},{\"end\":52413,\"start\":52409},{\"end\":52426,\"start\":52419},{\"end\":52436,\"start\":52430},{\"end\":52449,\"start\":52442},{\"end\":52461,\"start\":52453},{\"end\":52900,\"start\":52893},{\"end\":52910,\"start\":52904},{\"end\":52927,\"start\":52916},{\"end\":52942,\"start\":52931},{\"end\":53292,\"start\":53281},{\"end\":53303,\"start\":53296},{\"end\":53311,\"start\":53307},{\"end\":53552,\"start\":53548},{\"end\":53561,\"start\":53556},{\"end\":53574,\"start\":53565},{\"end\":53587,\"start\":53578},{\"end\":53899,\"start\":53892},{\"end\":53909,\"start\":53903},{\"end\":53919,\"start\":53915},{\"end\":53930,\"start\":53925},{\"end\":54315,\"start\":54306},{\"end\":54327,\"start\":54319},{\"end\":54334,\"start\":54331},{\"end\":54344,\"start\":54338},{\"end\":54706,\"start\":54702},{\"end\":54717,\"start\":54713},{\"end\":54733,\"start\":54721},{\"end\":54741,\"start\":54737},{\"end\":54752,\"start\":54745},{\"end\":55089,\"start\":55085},{\"end\":55104,\"start\":55095},{\"end\":55117,\"start\":55108},{\"end\":55128,\"start\":55121},{\"end\":55143,\"start\":55134},{\"end\":55156,\"start\":55151},{\"end\":55466,\"start\":55460},{\"end\":55479,\"start\":55470},{\"end\":55490,\"start\":55485},{\"end\":55503,\"start\":55494},{\"end\":55942,\"start\":55939},{\"end\":55956,\"start\":55946},{\"end\":56290,\"start\":56286},{\"end\":56305,\"start\":56294},{\"end\":56320,\"start\":56309},{\"end\":56331,\"start\":56324},{\"end\":56610,\"start\":56605},{\"end\":56616,\"start\":56614},{\"end\":56628,\"start\":56623},{\"end\":56638,\"start\":56634},{\"end\":56651,\"start\":56642},{\"end\":56662,\"start\":56657},{\"end\":56933,\"start\":56928},{\"end\":56944,\"start\":56937},{\"end\":57235,\"start\":57229},{\"end\":57245,\"start\":57241},{\"end\":57255,\"start\":57249},{\"end\":57554,\"start\":57550},{\"end\":57565,\"start\":57558},{\"end\":57968,\"start\":57962},{\"end\":57977,\"start\":57972},{\"end\":57985,\"start\":57981},{\"end\":57996,\"start\":57989},{\"end\":58010,\"start\":58003},{\"end\":58022,\"start\":58014},{\"end\":58033,\"start\":58026},{\"end\":58648,\"start\":58644},{\"end\":58660,\"start\":58652},{\"end\":58676,\"start\":58664},{\"end\":58687,\"start\":58680},{\"end\":58695,\"start\":58691},{\"end\":58706,\"start\":58699},{\"end\":58721,\"start\":58710},{\"end\":58733,\"start\":58725},{\"end\":58747,\"start\":58737},{\"end\":59162,\"start\":59154},{\"end\":59176,\"start\":59170},{\"end\":59190,\"start\":59180},{\"end\":59204,\"start\":59198},{\"end\":59708,\"start\":59705},{\"end\":59716,\"start\":59712},{\"end\":59724,\"start\":59720},{\"end\":59730,\"start\":59728},{\"end\":59738,\"start\":59734},{\"end\":60161,\"start\":60158},{\"end\":60169,\"start\":60165},{\"end\":60177,\"start\":60173},{\"end\":60184,\"start\":60181},{\"end\":60190,\"start\":60188},{\"end\":60198,\"start\":60194},{\"end\":60575,\"start\":60569},{\"end\":60715,\"start\":60709},{\"end\":60873,\"start\":60863},{\"end\":60883,\"start\":60877},{\"end\":60896,\"start\":60887},{\"end\":60907,\"start\":60900},{\"end\":61212,\"start\":61202},{\"end\":61225,\"start\":61216},{\"end\":61237,\"start\":61231},{\"end\":61561,\"start\":61551},{\"end\":61571,\"start\":61565},{\"end\":61585,\"start\":61575},{\"end\":61598,\"start\":61589},{\"end\":61615,\"start\":61602},{\"end\":62002,\"start\":61995},{\"end\":62010,\"start\":62006},{\"end\":62279,\"start\":62275},{\"end\":62290,\"start\":62283},{\"end\":62301,\"start\":62294},{\"end\":62310,\"start\":62305},{\"end\":62732,\"start\":62728},{\"end\":62742,\"start\":62738},{\"end\":63058,\"start\":63042},{\"end\":63069,\"start\":63064},{\"end\":63082,\"start\":63075},{\"end\":63472,\"start\":63464},{\"end\":63487,\"start\":63478},{\"end\":63500,\"start\":63493},{\"end\":63510,\"start\":63504}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16651152},\"end\":48757,\"start\":48263},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4419499},\"end\":49098,\"start\":48759},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2044592},\"end\":49697,\"start\":49100},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":33096075},\"end\":50234,\"start\":49699},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":24180188},\"end\":50867,\"start\":50236},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2991432},\"end\":51345,\"start\":50869},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2612052},\"end\":51783,\"start\":51347},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15222519},\"end\":52250,\"start\":51785},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206680596},\"end\":52810,\"start\":52252},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7725346},\"end\":53212,\"start\":52812},{\"attributes\":{\"doi\":\"arXiv:1505.04597\",\"id\":\"b10\"},\"end\":53489,\"start\":53214},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2732856},\"end\":53830,\"start\":53491},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17427864},\"end\":54215,\"start\":53832},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10186071},\"end\":54614,\"start\":54217},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1419241},\"end\":54999,\"start\":54616},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2092326},\"end\":55391,\"start\":55001},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11663829},\"end\":55811,\"start\":55393},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7480362},\"end\":56227,\"start\":55813},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":21401149},\"end\":56533,\"start\":56229},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15625376},\"end\":56827,\"start\":56535},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5448638},\"end\":57153,\"start\":56829},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5803415},\"end\":57451,\"start\":57155},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10673073},\"end\":57823,\"start\":57453},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2190508},\"end\":58541,\"start\":57825},{\"attributes\":{\"id\":\"b24\"},\"end\":59016,\"start\":58543},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":16234956},\"end\":59594,\"start\":59018},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":22350096},\"end\":60073,\"start\":59596},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2011900},\"end\":60521,\"start\":60075},{\"attributes\":{\"id\":\"b28\"},\"end\":60656,\"start\":60523},{\"attributes\":{\"id\":\"b29\"},\"end\":60803,\"start\":60658},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":996073},\"end\":61133,\"start\":60805},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195908774},\"end\":61480,\"start\":61135},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6844431},\"end\":61937,\"start\":61482},{\"attributes\":{\"doi\":\"abs/1412.4564\",\"id\":\"b33\",\"matched_paper_id\":207224096},\"end\":62142,\"start\":61939},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2999984},\"end\":62622,\"start\":62144},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":43144161},\"end\":62946,\"start\":62624},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":43976032},\"end\":63400,\"start\":62948},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":26116446},\"end\":63711,\"start\":63402}]", "bib_title": "[{\"end\":48340,\"start\":48263},{\"end\":48844,\"start\":48759},{\"end\":49265,\"start\":49100},{\"end\":49840,\"start\":49699},{\"end\":50402,\"start\":50236},{\"end\":51004,\"start\":50869},{\"end\":51440,\"start\":51347},{\"end\":51862,\"start\":51785},{\"end\":52352,\"start\":52252},{\"end\":52889,\"start\":52812},{\"end\":53544,\"start\":53491},{\"end\":53888,\"start\":53832},{\"end\":54302,\"start\":54217},{\"end\":54698,\"start\":54616},{\"end\":55081,\"start\":55001},{\"end\":55456,\"start\":55393},{\"end\":55935,\"start\":55813},{\"end\":56280,\"start\":56229},{\"end\":56598,\"start\":56535},{\"end\":56922,\"start\":56829},{\"end\":57225,\"start\":57155},{\"end\":57546,\"start\":57453},{\"end\":57958,\"start\":57825},{\"end\":59148,\"start\":59018},{\"end\":59701,\"start\":59596},{\"end\":60154,\"start\":60075},{\"end\":60859,\"start\":60805},{\"end\":61198,\"start\":61135},{\"end\":61547,\"start\":61482},{\"end\":61991,\"start\":61939},{\"end\":62269,\"start\":62144},{\"end\":62722,\"start\":62624},{\"end\":63038,\"start\":62948},{\"end\":63460,\"start\":63402}]", "bib_author": "[{\"end\":48353,\"start\":48342},{\"end\":48364,\"start\":48353},{\"end\":48373,\"start\":48364},{\"end\":48388,\"start\":48373},{\"end\":48404,\"start\":48388},{\"end\":48416,\"start\":48404},{\"end\":48423,\"start\":48416},{\"end\":48434,\"start\":48423},{\"end\":48448,\"start\":48434},{\"end\":48456,\"start\":48448},{\"end\":48858,\"start\":48846},{\"end\":48869,\"start\":48858},{\"end\":48882,\"start\":48869},{\"end\":48892,\"start\":48882},{\"end\":49283,\"start\":49267},{\"end\":49294,\"start\":49283},{\"end\":49302,\"start\":49294},{\"end\":49310,\"start\":49302},{\"end\":49325,\"start\":49310},{\"end\":49336,\"start\":49325},{\"end\":49350,\"start\":49336},{\"end\":49855,\"start\":49842},{\"end\":49862,\"start\":49855},{\"end\":49869,\"start\":49862},{\"end\":49883,\"start\":49869},{\"end\":49897,\"start\":49883},{\"end\":50420,\"start\":50404},{\"end\":50432,\"start\":50420},{\"end\":50444,\"start\":50432},{\"end\":50458,\"start\":50444},{\"end\":50472,\"start\":50458},{\"end\":50481,\"start\":50472},{\"end\":50494,\"start\":50481},{\"end\":51016,\"start\":51006},{\"end\":51028,\"start\":51016},{\"end\":51036,\"start\":51028},{\"end\":51049,\"start\":51036},{\"end\":51448,\"start\":51442},{\"end\":51457,\"start\":51448},{\"end\":51464,\"start\":51457},{\"end\":51475,\"start\":51464},{\"end\":51481,\"start\":51475},{\"end\":51489,\"start\":51481},{\"end\":51503,\"start\":51489},{\"end\":51877,\"start\":51864},{\"end\":51887,\"start\":51877},{\"end\":51904,\"start\":51887},{\"end\":51919,\"start\":51904},{\"end\":52362,\"start\":52354},{\"end\":52376,\"start\":52362},{\"end\":52387,\"start\":52376},{\"end\":52396,\"start\":52387},{\"end\":52404,\"start\":52396},{\"end\":52415,\"start\":52404},{\"end\":52428,\"start\":52415},{\"end\":52438,\"start\":52428},{\"end\":52451,\"start\":52438},{\"end\":52463,\"start\":52451},{\"end\":52902,\"start\":52891},{\"end\":52912,\"start\":52902},{\"end\":52929,\"start\":52912},{\"end\":52944,\"start\":52929},{\"end\":53294,\"start\":53279},{\"end\":53305,\"start\":53294},{\"end\":53313,\"start\":53305},{\"end\":53554,\"start\":53546},{\"end\":53563,\"start\":53554},{\"end\":53576,\"start\":53563},{\"end\":53589,\"start\":53576},{\"end\":53901,\"start\":53890},{\"end\":53911,\"start\":53901},{\"end\":53921,\"start\":53911},{\"end\":53932,\"start\":53921},{\"end\":54317,\"start\":54304},{\"end\":54329,\"start\":54317},{\"end\":54336,\"start\":54329},{\"end\":54346,\"start\":54336},{\"end\":54708,\"start\":54700},{\"end\":54719,\"start\":54708},{\"end\":54735,\"start\":54719},{\"end\":54743,\"start\":54735},{\"end\":54754,\"start\":54743},{\"end\":55091,\"start\":55083},{\"end\":55106,\"start\":55091},{\"end\":55119,\"start\":55106},{\"end\":55130,\"start\":55119},{\"end\":55145,\"start\":55130},{\"end\":55158,\"start\":55145},{\"end\":55468,\"start\":55458},{\"end\":55481,\"start\":55468},{\"end\":55492,\"start\":55481},{\"end\":55505,\"start\":55492},{\"end\":55944,\"start\":55937},{\"end\":55958,\"start\":55944},{\"end\":56292,\"start\":56282},{\"end\":56307,\"start\":56292},{\"end\":56322,\"start\":56307},{\"end\":56333,\"start\":56322},{\"end\":56612,\"start\":56600},{\"end\":56618,\"start\":56612},{\"end\":56630,\"start\":56618},{\"end\":56640,\"start\":56630},{\"end\":56653,\"start\":56640},{\"end\":56664,\"start\":56653},{\"end\":56935,\"start\":56924},{\"end\":56946,\"start\":56935},{\"end\":57237,\"start\":57227},{\"end\":57247,\"start\":57237},{\"end\":57257,\"start\":57247},{\"end\":57556,\"start\":57548},{\"end\":57567,\"start\":57556},{\"end\":57970,\"start\":57960},{\"end\":57979,\"start\":57970},{\"end\":57987,\"start\":57979},{\"end\":57998,\"start\":57987},{\"end\":58012,\"start\":57998},{\"end\":58024,\"start\":58012},{\"end\":58035,\"start\":58024},{\"end\":58650,\"start\":58642},{\"end\":58662,\"start\":58650},{\"end\":58678,\"start\":58662},{\"end\":58689,\"start\":58678},{\"end\":58697,\"start\":58689},{\"end\":58708,\"start\":58697},{\"end\":58723,\"start\":58708},{\"end\":58735,\"start\":58723},{\"end\":58749,\"start\":58735},{\"end\":59164,\"start\":59150},{\"end\":59178,\"start\":59164},{\"end\":59192,\"start\":59178},{\"end\":59206,\"start\":59192},{\"end\":59710,\"start\":59703},{\"end\":59718,\"start\":59710},{\"end\":59726,\"start\":59718},{\"end\":59732,\"start\":59726},{\"end\":59740,\"start\":59732},{\"end\":60163,\"start\":60156},{\"end\":60171,\"start\":60163},{\"end\":60179,\"start\":60171},{\"end\":60186,\"start\":60179},{\"end\":60192,\"start\":60186},{\"end\":60200,\"start\":60192},{\"end\":60577,\"start\":60565},{\"end\":60717,\"start\":60705},{\"end\":60875,\"start\":60861},{\"end\":60885,\"start\":60875},{\"end\":60898,\"start\":60885},{\"end\":60909,\"start\":60898},{\"end\":61214,\"start\":61200},{\"end\":61227,\"start\":61214},{\"end\":61239,\"start\":61227},{\"end\":61563,\"start\":61549},{\"end\":61573,\"start\":61563},{\"end\":61587,\"start\":61573},{\"end\":61600,\"start\":61587},{\"end\":61617,\"start\":61600},{\"end\":62004,\"start\":61993},{\"end\":62012,\"start\":62004},{\"end\":62281,\"start\":62271},{\"end\":62292,\"start\":62281},{\"end\":62303,\"start\":62292},{\"end\":62312,\"start\":62303},{\"end\":62734,\"start\":62724},{\"end\":62744,\"start\":62734},{\"end\":63060,\"start\":63040},{\"end\":63071,\"start\":63060},{\"end\":63084,\"start\":63071},{\"end\":63474,\"start\":63462},{\"end\":63489,\"start\":63474},{\"end\":63502,\"start\":63489},{\"end\":63512,\"start\":63502}]", "bib_venue": "[{\"end\":48476,\"start\":48456},{\"end\":48898,\"start\":48892},{\"end\":49382,\"start\":49350},{\"end\":49939,\"start\":49897},{\"end\":50520,\"start\":50494},{\"end\":51071,\"start\":51049},{\"end\":51523,\"start\":51503},{\"end\":51989,\"start\":51919},{\"end\":52493,\"start\":52463},{\"end\":52993,\"start\":52944},{\"end\":53277,\"start\":53214},{\"end\":53611,\"start\":53589},{\"end\":54002,\"start\":53932},{\"end\":54368,\"start\":54346},{\"end\":54786,\"start\":54754},{\"end\":55166,\"start\":55158},{\"end\":55575,\"start\":55505},{\"end\":55978,\"start\":55958},{\"end\":56354,\"start\":56333},{\"end\":56668,\"start\":56664},{\"end\":56978,\"start\":56946},{\"end\":57289,\"start\":57257},{\"end\":57616,\"start\":57567},{\"end\":58126,\"start\":58035},{\"end\":58640,\"start\":58543},{\"end\":59276,\"start\":59206},{\"end\":59804,\"start\":59740},{\"end\":60270,\"start\":60200},{\"end\":60563,\"start\":60523},{\"end\":60703,\"start\":60658},{\"end\":60949,\"start\":60909},{\"end\":61288,\"start\":61239},{\"end\":61653,\"start\":61617},{\"end\":62029,\"start\":62025},{\"end\":62334,\"start\":62312},{\"end\":62760,\"start\":62744},{\"end\":63152,\"start\":63084},{\"end\":63528,\"start\":63512},{\"end\":58204,\"start\":58128}]"}}}, "year": 2023, "month": 12, "day": 17}