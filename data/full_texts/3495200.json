{"id": 3495200, "updated": "2023-09-29 08:32:34.089", "metadata": {"title": "Learning to Represent Programs with Graphs", "authors": "[{\"first\":\"Miltiadis\",\"last\":\"Allamanis\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Brockschmidt\",\"middle\":[]},{\"first\":\"Mahmoud\",\"last\":\"Khademi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 11, "day": 1}, "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1711.00740", "mag": "2963499994", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/AllamanisBK18", "doi": null}}, "content": {"source": {"pdf_hash": "5f1d429ba574581ac14effe3ebab654a57dc0e39", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.00740v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "43e7a87bedfb82a1a60bf20283c8885820428baa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5f1d429ba574581ac14effe3ebab654a57dc0e39.txt", "contents": "\nLEARNING TO REPRESENT PROGRAMS WITH GRAPHS\n1 Nov 2017\n\nMiltiadis Allamanis \nMicrosoft Research Cambridge\nUK\n\nMarc Brockschmidt \nMicrosoft Research Cambridge\nUK\n\nMahmoud Khademi mkhademi@sfu.ca \nSimon Fraser University\nVancouverCanada\n\nLEARNING TO REPRESENT PROGRAMS WITH GRAPHS\n1 Nov 2017\nLearning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.\n\nINTRODUCTION\n\nThe advent of large repositories of source code as well as scalable machine learning methods naturally leads to the idea of \"big code\", i.e., largely unsupervised methods that support software engineers by generalizing from existing source code (Allamanis et al., 2017). Currently, existing machine learning models of source code capture its shallow, textual structure, e.g. as a sequence of tokens (Hindle et al., 2012;Raychev et al., 2014;Allamanis et al., 2016), as parse trees (Maddison & Tarlow, 2014;, or as a flat dependency networks of variables (Raychev et al., 2015). Such models miss out on the opportunity to capitalize on the rich and well-defined semantics of source code. In this work, we take a step to alleviate this by including two additional signal sources in source code: data flow and type hierarchies. We do this by encoding programs as graphs, in which edges represent syntactic relationships (e.g. \"token before/after\") as well as semantic relationships (\"variable last used/written here\", \"formal parameter for argument is called stream\", etc.). Our key insight is that exposing these semantics explicitly as structured input to a machine learning model lessens the requirements on amounts of training data, model capacity and training regime and allows us to solve tasks that are beyond the current state of the art.\n\nWe explore two tasks to illustrate the advantages of exposing more semantic structure of programs. First, we consider the VARNAMING task (Allamanis et al., 2014;Raychev et al., 2015), in which given some source code, the \"correct\" variable name is inferred as a sequence of subtokens. This requires some understanding of how a variable is used, i.e., requires reasoning about lines of code far apart in the source file. Secondly, we introduce the variable misuse prediction task (VARMISUSE), in which the network aims to infer which variable should be used in a program location. To illustrate Figure 1: A snippet of a detected bug in ravendb, an open-source project. The code has been slightly simplified. Our model detects correctly that the variable used in the highlighted (yellow) slot is incorrect. Instead, first should have been placed at the slot.\n\nthe task, Figure 1 shows a slightly simplified snippet of a bug our model detected in a popular opensource project. Specifically, instead of the variable clazz, variable first should have been used in the yellow highlighted slot. Existing static analysis methods cannot detect such issues, even though a software engineer would easily identify this as an error from experience.\n\nTo achieve high accuracy on these tasks, we need to learn representations of program semantics. For both tasks, we need to learn the semantic role of a variable (e.g., \"is it a counter?\", \"is it a filename?\"). Additionally, for VARMISUSE, learning variable usage semantics (e.g., \"a filename is needed here\") is required. This \"fill the blank element\" task is related to methods for learning distributed representations of natural language words, such as Word2Vec (Mikolov et al., 2013) and GLoVe (Pennington et al., 2014). However, we can learn from a much richer structure such as data flow information. This work is a step towards learning program representations, and we expect them to be valuable in a wide range of other tasks, such as code completion (\"this is the variable you are looking for\") and more advanced bug finding (\"you should lock before using this object\").\n\nTo summarize, our contributions are: (i) We define the VARMISUSE task as a challenge for machine learning modeling of source code, that requires to learn (some) semantics of programs (cf. section 3). (ii) We present deep learning models for solving the VARNAMING and VARMISUSE tasks by modeling the code's graph structure and learning program representations over those graphs (cf. section 4). (iii) We evaluate our models on a large dataset of 2.9 million lines of real-world source code, showing that our best model achieves 30.7% accuracy on the VARNAMING task and 82.1% accuracy on the VARMISUSE task, beating simpler baselines (cf. section 5). (iv) We document practical relevance of VARMISUSE by summarizing some bugs that we found in mature open-source software projects (cf. subsection 5.3).\n\n\nRELATED WORK\n\nOur work builds upon the recent field of using machine learning for source code artifacts (Allamanis et al., 2017). For example, Hindle et al. (2012); Bhoopchand et al. (2016) model the code as a sequence of tokens, while Maddison & Tarlow (2014);  model the syntax tree structure of code. All works on language models of code find that predicting variable and method identifiers is one of biggest challenges in the task.\n\nClosest to our work is the work of Allamanis et al. (2015) who learn distributed representations of variables using all their usages to predict their names. However, they do not use data flow information and we are not aware of any model that does so. The work of Raychev et al. (2015) is also relevant, as it uses a CRF over the dependency network between variables -without considering data flow -to predict their types and names. However, all variable usages are deterministically known beforehand (as the code is complete and remains unmodified), as in Allamanis et al. (2014;.\n\nOur work is remotely related to work on program synthesis using sketches (Solar-Lezama, 2008) and automated code transplantation . However, these approaches require a set of specifications (e.g. input-output examples, test suites) to complete the gaps, rather than statistics learned from big code. These approaches can be thought as complementary to ours, since we learn to statistically complete the gaps without any need for specifications, by learning common dataflow structure from code.\n\nNeural networks on graphs (Gori et al., 2005;Li et al., 2015;Defferrard et al., 2016;Kipf & Welling, 2016;Gilmer et al., 2017) adapt a variety of deep learning methods to graph-structured input. They have been used in a series of applications, such as link prediction and classification (Grover & Leskovec, 2016) and semantic role labeling in NLP (Marcheggiani & Titov, 2017). Somewhat related to source code is the work of Wang et al. (2017) who learn graph-based representations of mathematical formulas for premise selection in theorem proving.\n\n\nTHE VARMISUSE TASK\n\nDetecting variable misuses in code is a task that requires understanding and reasoning about program semantics. To successfully tackle the task one needs to infer the role and function of the program elements and understand how they relate. For example, given a program such as Fig. 1, the task is to automatically detect that the marked use of clazz is a mistake and that first should be used instead. While this task resembles standard code completion, it differs significantly in its scope and purpose, by considering only variable identifiers and a mostly complete program.\n\n\nTask Description\n\nWe view a source code file as a sequence of tokens t 0 . . . t N = T , in which some tokens t \u03bb0 , t \u03bb1 . . . are variables. Furthermore, let V t \u2282 V refer to the set of all type-correct variables in scope at the location of t, i.e., those variables that can be used at t without raising a compiler error. We call the location t where we want to predict the correct variable usage a slot. We define a separate task for each slot t \u03bb : Given t 0 . . . t \u03bb\u22121 and t \u03bb+1 , . . . , \u03bb N , correctly select t \u03bb from V t \u03bb . For training and evaluation purposes, a correct solution is one that simply matches the ground truth, but note that in practice, several possible assignments could be considered correct (i.e., when several variables refer to the same value in memory).\n\n\nMODEL: PROGRAMS AS GRAPHS\n\nIn this section, we discuss how to transform program source code into program graphs and learn representations over them. These program graphs not only encode the program text but also the semantic information that can be obtained using standard compiler tools.\n\nGated Graph Neural Networks Our work builds on Gated Graph Neural Networks (Li et al., 2015) (GGNN) and we summarize them here. A graph G = (V, E, X) is composed of a set of nodes V, node features X, and a list of directed edge sets E = (E 1 , . . . , E K ) where K is the number of edge types. We annotate each v with a real-valued vector x (v) \u2208 R D representing the features of the node (e.g., the embedding of a string label of that node).\n\nWe associate every node v with a state vector h (v) , initialized from the node label x (v) . The sizes of the state vector and feature vector are typically the same, but we can use larger state vectors through padding of node features. To propagate information throughout the graph, \"messages\" of type k are sent from each v to its neighbors, where each message is computed from its current state vector as m\n(v) k = f k (h (v) )\n. Here, f k can be an arbitrary function; we choose a linear layer in our case. By computing messages for all graph edges at the same time, all states can be updated at the same time. In particular, a new state for a node v is computed by aggregating all incoming messages as\nm (v) = g({m (u)\nk | there is an edge of type k from u to v}). g is an aggregation function, which we implement as elementwise summation. Given the aggregated messagem (v) and the current state vector h (v) of node v, the state of the next time step\nh \u2032(v) is computed as h \u2032(v) = GRU(m (v) , h (v) ),\nwhere GRU is the recurrent cell function of gated recurrent unit (GRU) (Cho et al., 2014). The dynamics defined by the above equations are repeated for a fixed number of time steps. Then, we use the state vectors from the last time step as the node representations.\n\n\nProgram Graphs\n\nWe represent program source code as graphs and use different edges to model syntactic and semantic relationships between different tokens. The backbone of a program graph is the program's abstract syntax tree (AST), consisting of syntax nodes (corresponding to non-terminals   in the programming language's grammar) and syntax tokens (corresponding to terminals). We label syntax nodes with the name of the nonterminal from the program's grammar, whereas syntax tokens are labeled with the string that they represent. We use Child edges to connect nodes according to the AST. As this does not induce an order on children of a syntax node, we additionally add NextToken edges connecting each syntax token to its successor. An example of this is shown in Fig. 2a.\n\nTo capture the flow of control and data through a program, we add additional edges connecting different uses and updates of syntax tokens corresponding to variables. For such a token v, let D R (v) be the set of syntax tokens at which the variable could have been used last. This set may contain several nodes (for example, when using a variable after a conditional in which it was used in both branches), and even syntax tokens that follow in the program code (in the case of loops). Similarly, let D W (v) be the set of syntax tokens at which the variable was last written to. Using these, we add LastRead (resp. LastWrite) edges connecting v to all elements of D R (v) (resp. D W (v)). Additionally, whenever we observe an assignment v = expr , we connect v to all variable tokens occurring in expr using ComputedFrom edges. An example of such semantic edges is shown in Fig. 2b.\n\nWe extend the graph to chain all uses of the same variable using LastLexicalUse edges (inde-\npendent of data flow, i.e., in if (...) { ... v ...} else { ... v .\n..}, we link the two occurrences of v). We also connect return tokens to the method declaration using ReturnsTo edges (this creates a \"shortcut\" to its name and type). Finally, inspired by Rice et al. (2017), we connect arguments in method calls to the formal parameters that they are matched to with FormalArgName edges, i.e., if we observe a call Foo(bar) and a method declaration Foo(InputStream stream), we connect the bar token to the stream token.\n\nFinally, for all types of edges we introduce their respective backwards edges (transposing the adjacency matrix), doubling the number of edges and edge types. Backwards edges help with propagating information faster across the GGNN and make the model more expressive.\n\nLeveraging Variable Type Information We assume a statically typed language and that the source code can be compiled, and thus each variable has a (known) type \u03c4 (v). To use it, we define a learnable embedding function r(\u03c4 ) for known types and additionally define an \"UNKTYPE\" for all unknown/unrepresented types. We also leverage the rich type hierarchy that is available in many object-oriented languages. For this, we map a variable's type \u03c4 (v) to the set of its supertypes,\ni.e. \u03c4 * (v) = {\u03c4 : \u03c4 (v) implements type \u03c4 } \u222a {\u03c4 (v)}.\nWe then compute the type representation r * (v) of a variable v as the element-wise maximum of {r(\u03c4 ) : \u03c4 \u2208 \u03c4 * (v)}. We chose the maximum here, as it is a natural pooling operation for representing partial ordering relations (such as type lattices). Using all types in \u03c4 * (v) allows us to generalize to unseen types that implement common supertypes or interfaces. For example, List<K> has multiple concrete types (e.g. List<int>, List<string>). Nevertheless, these types implement a common interface (IList) and share common characteristics. During training, we randomly select a non-empty subset of \u03c4 * (v) which ensures training of all known types in the lattice. This acts both like a dropout mechanism and allows us to learn a good representation for all types in the type lattice.\n\nInitial Node Representation To compute the initial node state, we combine information from the textual representation of the token and its type. Concretely, we split the name of a node into subtokens (e.g. classTypes will be split into two subtokens class and types) on camelCase and pascal_case. We then average the embeddings of all subtokens to retrieve an embedding for the node name. Finally, we concatenate the learned type representation r * (v), computed as discussed earlier, with the node name representation. This representation is then padded with zeros to match the target hidden layer size of the GGNN.\n\nPrograms Graphs for VARNAMING Given a program and an existing variable v, we build a program graph as discussed above and then replace the variable name in all corresponding variable tokens by a special <SLOT> token. To predict a name, we use the initial node labels computed as the concatenation of learnable token embeddings and type embeddings as discussed above, run GGNN propagation for 8 time steps 1 and then compute a variable usage representation by averaging the representations for all <SLOT> tokens. This representation is then used as the initial state of a one-layer GRU, which predicts the target name as a sequence of subtokens (e.g., the name inputStreamBuffer is treated as the sequence [input, stream, buffer]). We train this graph2seq architecture using a maximum likelihood objective. In section 5, we report the accuracy for predicting the exact name and the F1 score for predicting its subtokens.\n\nProgram Graphs for VARMISUSE To model VARMISUSE with program graphs we need to modify the graph. First, to compute a context representation c(t) for a slot t where we want to predict the used variable, we insert a new node v <SLOT> at the position of t, corresponding to a \"hole\" at this point, and connect it to the remaining graph using all applicable edges that do not depend on the chosen variable at the slot (i.e., everything but LastUse, LastWrite, and LastLexicalUse edges). Then, to compute the usage representation u(t, v) of of each candidate variable v at the target slot, we insert a \"candidate\" node v t,v for all v in V t , and connect it to the graph by inserting the LastUse, LastWrite and LastLexicalUse edges that would be used if the variable were to be used at this slot. Each of these candidate nodes represents the speculative placement of the variable within the scope.\n\nUsing the initial node representations, concatenated with an extra bit that is set to one for the candidate nodes v t,v , we run GGNN propagation for 8 time steps. 1 The context and usage representation are then the final node states of the nodes, i.e., c(t) = h (v<SLOT>) and u(t, v) = h (vt,v ) . Finally, the correct variable usage at the location is computed as arg max v c(t) T u(t, v). We train using maximum likelihood.\n\n\nIMPLEMENTATION\n\nUsing GGNNs for sets of large, diverse graphs requires some engineering effort, as efficient batching is hard in the presence of diverse shapes. An important observation is that large graphs are normally very sparse, and thus a representation of edges as an adjacency list would usually be advantageous to reduce memory consumption. In our case, this can be easily implemented using a sparse tensor representation, allowing large batch sizes that exploit the parallelism of modern GPUs efficiently. A second key insight is to represent a batch of graphs as one large graph with many disconnected components. This just requires appropriate pre-processing to make node identities unique. As this makes batch construction somewhat CPU-intensive, we found it useful to prepare minibatches on a separate thread. Our TensorFlow (Abadi et al., 2016) implementation scales to 55 graphs per second during training and 219 graphs per second during test-time using a single NVidia GeForce GTX Titan X with graphs having on average 2,228 (median 936) nodes and 8,350 (median 3,274) edges and 8 GGNN unrolling iterations, all 16 edge types (forward and backward edges for 8 original edge types) and the size of the hidden layer set to 64. The number of types of edges in the GGNN contribute proportionally to the running time. For example, a GGNN run for our ablation study using only the two most common edge types (NextToken, Child) achieves 105 graphs/second during training and 419 graphs/second at test time with the same hyperparameters. We intend to release our (generic) sparse GGNN implementation soon. \n\n\nEVALUATION\n\nDataset We collected a dataset for the VARMISUSE task from open source C # projects on GitHub.\n\nTo select projects, we picked the top-starred (non-fork) projects in GitHub. We then filtered out projects that we could not (easily) compile in full using Roslyn 2 , as we require a compilation to extract precise type information for the code (including those types present in external libraries). Our final dataset contains 29 projects from a diverse set of domains (compilers, databases, . . . ) with about 2.9 million non-empty lines of code. A full table is shown in Appendix C.\n\nFor the task of detecting variable misuses, we collect data from all projects by selecting all variable usage locations, filtering out variable declarations, where at least one other type-compatible replacement variable is in scope. The task is then to infer the correct variable that originally existed in that location. Thus, by construction there is at least one type-correct replacement variable, i.e. picking it would not not raise an error during type checking. In our test datasets, at each slot there are on average 3.8 type-correct alternative variables (median 3, \u03c3 = 2.6).\n\nFrom our dataset, we selected two projects as our development set. From the rest of the projects, we selected three projects for UNSEENPROJTEST to allow testing on projects with completely unknown structure and types. We split the remaining 23 projects into train/validation/test sets in the proportion 60-10-30, splitting along files (i.e., all examples from one source file are in the same set). We call the test set obtained like this SEENPROJTEST.\n\nBaselines For VARMISUSE, we consider two bidirectional RNN-based baselines. The local model (LOC) is a simple two-layer bidirectional GRU run over the tokens before and after the target location. For this baseline, c(t) is set to the slot representation computed by the RNN, and the usage context of each variable u(t, v) is the embedding of the name and type of the variable, computed in the same way as the initial node labels in the GGNN. This baseline allows us to evaluate how important the usage context information is for this task. The flat dataflow model (AVGBIRNN) is an extension to LOC, where the usage representation u(t, v) is computed using another two-layer bidirectional RNN run over the tokens before/after each usage, and then averaging over the computed representations at the variable token v. The local context, c(t), is identical to LOC. AVGBIRNN is a significantly stronger baseline that already takes some structural information into account, as the averaging over all variables usages helps with long-range dependencies. As in GGNN, both models pick the variable that maximizes c(t) T u(t, v).\n\nFor VARNAMING, we replace LOC by AVGLBL, which uses a log-bilinear model for 4 left and 4 right context tokens of each variable usage, and then averages over these context representations (this corresponds to the model in Allamanis et al. (2015)) We also test AVGBIRNN on VARNAMING, which essentially replaces the log-bilinear context model by a bidirectional RNN. Table 1 shows the evaluation results of the models for both tasks. As LOC captures very little information, it performs relatively badly. AVGLBL and AVGBIRNN, which capture information from many variable usage sites, but do not explicitly encode the rich structure of the problem, still lag behind the GGNN by a wide margin. The performance difference is larger for VARMISUSE, since the structure and the semantics of code are far more important within this setting. Generalization to new projects Generalizing across a diverse set of source code projects with different domains is an important challenge in machine learning. We repeat the evaluation using the UNSEENPROJTEST set stemming from projects that have no files in the training set. The right side of Table 1 shows that our models still achieve good performance, although it is slightly lower compared to SEENPROJTEST. This is expected since the type lattice is mostly unknown in UNSEEN-PROJTEST.\n\n\nQUANTITATIVE EVALUATION\n\nWe believe that some of the most important issues when transferring to new domains is the fact that projects have significantly different type hierarchies and that the vocabulary used (e.g. within tokens) is very different from the training projects.\n\n\nAblation Study\n\nTo study the effect of some of the design choices for our models, we have run some additional experiments and show their results in Table 2. First, we varied the edges used in the program graph. We find that restricting the model to syntactic information has a large impact on performance on both tasks, whereas restricting it to semantic edges seems to mostly impact performance on VARMISUSE. Similarly, the ComputedFrom, FormalArgName and ReturnsTo edges give a small boost on VARMISUSE, but greatly improve performance on VARNAMING. As evidenced by the experiments with the node label representation, syntax node and token names seem to matter little for VARMISUSE, but naturally have a great impact on VARNAMING. Figure 3 illustrates a prediction that GGNN makes on a sample test snippet. The snippet recursively searches for the global directives file by gradually descending into the root folder. Reasoning about the correct variable usages is hard, even for humans, but the GGNN correctly predicts the variable usages an all locations except two (slot 1 and 8). As a software engineer is writing the code it is imaginable that she may make a mistake misusing one variable in the place of another. Since all variables are string variables, no type errors will be raised. As the probabilities in Fig. 3 suggest most potential variable misuses can be flagged by the model yielding valuable warnings to software engineers. Additional samples with comments can be found in Appendix A.\n\n\nQUALITATIVE EVALUATION\n\nFurthermore, Appendix B shows samples of pairs of code snippets that share similar representations as computed by the cosine similarity of the usage representation u(t, v) of GGNN. The reader can notice that the network learns to group variable usages that share semantic similarities together. For example, checking for null before the use of a variable yields similar distributed representations across code segments (Sample 1 in Appendix B).\n\n\nDISCOVERED VARIABLE MISUSE BUGS\n\nWe looked at the top 500 locations of a project's test files where our model was most confident about a variable other than the ground-truth. Within this set of locations we noticed some bugs. Specific information about the project is excluded to avoid breaking the double-blind review. The bugs we found appear in the released code of a well-tested popular GitHub project with over 1.5k stars, attesting to the practical applicability of our model. The bug shown in Figure 1, possibly caused by copy-pasting, is caught since the pattern of usage is highly irregular. Such a bug cannot be easily caught by traditional methods. A compiler will not warn about unused variables (since first is used) and virtually nobody would write a test testing another test. The bugs shown in Figure 4 and Fig. 5 are two other examples of issues that were caught by the VARMISUSE model. Figure 4 shows an issue that, although not critical, can help reduce memory consumption. Figure 5 shows bool TryFindGlobalDirectivesFile(string baseDirectory, string fullPath, out string path){ baseDirectory 1 = baseDirectory 2 .TrimEnd(Path.DirectorySeparatorChar); var directivesDirectory = Path.GetDirectoryName(fullPath 3 ) .TrimEnd(Path.DirectorySeparatorChar); while(directivesDirectory 4 != null && directivesDirectory 5 .Length >= baseDirectory 6 .Length){ path 7 = Path.Combine(directivesDirectory 8 , GlobalDirectivesFileName 9 ); if (File.Exists(path 10 )) return true; directivesDirectory 11 =Path.GetDirectoryName(directivesDirectory 12 ) .TrimEnd(Path.DirectorySeparatorChar); } path 13 = null; return false; } 1: path:59%, baseDirectory:35%, fullPath:6%, GlobalDirectivesFileName:1% 2: baseDirectory:92%, fullPath:5%, GlobalDirectivesFileName:2%, path:0.4% 3: fullPath:88%, baseDirectory:9%, GlobalDirectivesFileName:2%, path:1% 4: directivesDirectory:86%, path:8%, baseDirectory:2%, GlobalDirectivesFileName:1%, fullPath:0.1% 5: directivesDirectory:46%, path:24%, baseDirectory:16%, GlobalDirectivesFileName:10%, fullPath:3% 6: baseDirectory:64%, path:26%, directivesDirectory:5%, fullPath:2%, GlobalDirectivesFileName:2% 7: path:99%, directivesDirectory:1%, GlobalDirectivesFileName:0.5%, baseDirectory:7e-5, fullPath:4e-7 8: fullPath:60%, directivesDirectory:21%, baseDirectory:18%, path:1%, GlobalDirectivesFileName:4e-4 9: GlobalDirectivesFileName:61%, baseDirectory:26%, fullPath:8%, path:4%, directivesDirectory:0.5% 10: path:70%, directivesDirectory:17%, baseDirectory:10%, GlobalDirectivesFileName:1%, fullPath:0.6% 11: directivesDirectory:93%, path:5%, GlobalDirectivesFileName:1%, baseDirectory:0.1%, fullPath:4e-5% 12: directivesDirectory:65%, path:16%, baseDirectory:12%, fullPath:5%, GlobalDirectivesFileName:3% 13: path:97%, baseDirectory:2%, directivesDirectory:0.4%, fullPath:0.3%, GlobalDirectivesFileName:4e-4 another issue arising from a non-informative error message. Finding these issues in the released code suggests that our model can be quite useful during the software development process and can be complementary to classic program analysis tools.  . Although backupFilename is found to be invalid by IsValidBackup, the user is notified that backupLocation is invalid instead.\n\n\nDISCUSSION & CONCLUSIONS\n\nAlthough source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning. It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses. On the other hand, integrating this wealth of structured information poses an interesting challenge. Our VARMISUSE task exposes these opportunities, going beyond simpler tasks such as code completion. We consider it as a first proxy for the core challenge of learning the meaning of source code, as it requires to probabilistically refine standard information included in type systems.\n\n\nA VARMISUSE PREDICTION SAMPLES\n\nBelow we list a set of samples from our SEENPROJTEST projects with comments about the model performance. Code comments and formatting may have been altered for typesetting reasons. The ground truth choice is underlined. #1 n: 100%, MAXERROR: 0%, SYNC_MAXRETRIES: 0% #2 MAXERROR: 62%, SYNC_MAXRETRIES: 22%, n: 16% \u22b2 It is hard for the model to reason about conditionals, especially with rare constants as in slot #2.\n\n\nB NEAREST NEIGHBOR OF GGNN USAGE REPRESENTATIONS\n\nHere we show pairs of nearest neighbors based on the cosine similarity of the learned representations u(t, v). Each slot t is marked in dark blue and all usages of v are marked in yellow (i.e. variableName ). This is a set of hand \u22b2 Slots that follow similar API protocols have similar representations. Note that the function HasAddress is a local function, seen only in the testset.\n\n\nSimplified syntax graph for line 2 ofFig. 1, where blue rounded boxes are syntax nodes, black rectangular boxes syntax tokens, blue edges Child edges and double black edges NextToken edges.\n\n\nData flow edges for (x 1 ,y 2 ) = Foo(); while (x 3 > 0) x 4 = x 5 + y 6 (indices added for clarity), with red dotted LastUse edges, green dashed LastWrite edges and dashdotted purple ComputedFrom edges.\n\nFigure 2 :\n2Examples of graph edges used in program representation.\n\nFigure 3 :Figure 4 :\n34VARMISUSE predictions on slots within a snippet of the SEENPROJTEST set for the ServiceStack project. Additional visualizations are available in Appendix A. The underlined tokens are the correct tokens. The model has to select among a number of string variables at each slot, where all of them represent some kind of path. The GGNN accurately predicts the correct variable usage in 11 out of the 13 slots reasoning about the complex ways the variables interact among them.public ArraySegment<byte> ReadBytes(int length){ int size = Math.Min(length, _len -_pos); var buffer = EnsureTempBuffer( length ); var used = Read(buffer, 0, size); return new ArraySegment<byte>(buffer, 0, used); } A bug found (yellow) highlighted slot in an open-source project. The code unnecessarily ensures that the buffer is of size length rather than size (which our model predicts as the correct usage for the slot).\n\n\nprotected void ValidateRestorePreconditions(string backupFilename) { if (IsValidBackup(backupFilename) == false) { output(\"Error:\"+ backupLocation +\" doesn't look like a valid backup\"); output(\"Error: Restore Canceled\"); throw new InvalidOperationException( backupLocation + \" doesn't look like a valid backup\"); } ...\n\nFigure 5 :\n5A bug found (yellow) highlighted slot in an open-source project (code modified to fit in page)\n\nTable 1 :\n1Evaluation of models. UNSEENPROJTEST refers to projects that have no files in the training data, SEENPROJTEST refers to the test set containing projects that have files in the training set.SEENPROJTEST \nUNSEENPROJTEST \nLOC \nAVGLBL \nAVGBIRNN \nGGNN \nLOC \nAVGLBL \nAVGBIRNN \nGGNN \n\nVARMISUSE \nAccuracy (%) \n15.8 \n-\n73.5 \n82.1 \n13.8 \n-\n59.7 \n68.6 \nPR AUC \n0.363 \n-\n0.931 \n0.963 \n0.363 \n-\n0.891 \n0.909 \nVARNAMING \nAccuracy (%) \n-\n22.0 \n25.5 \n30.7 \n-\n15.3 \n15.9 \n19.4 \nF1 (%) \n-\n36.1 \n42.9 \n54.6 \n-\n22.7 \n23.4 \n30.5 \n\n\n\nTable 2 :\n2Ablation study for the GGNN model on SEENPROJTEST for the two tasks.Ablation Description \nVARMISUSE Accuracy (%) \nVARNAMING Accuracy (%) \n\nStandard Model (reported in Table 1) \n82.1 \n30.7 \n\nOnly NextToken, Child, LastUse, LastWrite edges \n79.0 \n15.4 \nOnly semantic edges (all but NextToken, Child) \n74.3 \n29.7 \nOnly syntax edges (NextToken, Child) \n49.6 \n20.5 \n\nNode Labels: Tokens instead of subtokens \n82.1 \n16.8 \nNode Labels: Disabled \n80.0 \n14.7 \n\n\n\n\nSample 1#1 startingFrom: 97%, endingAt: 3% #2 port: 100%, startingFrom: 0%, endingAt: 0% #3 endingAt: 100%, startingFrom: 0%, port: 0% #4 port: 100%, startingFrom: 0%, endingAt: 0% #5 port: 100%, startingFrom: 0%, endingAt: 0% #6 port: 100%, startingFrom: 0%, endingAt: 0%\u22b2 The model correctly predicts all variables in the loop. var path = CreateFileName( #1 ); bitmap.Save( #2 , ImageFormat.Png); return #3 ; #1 name: 86%, DIR_PATH: 14% #2 path: 90%, name: 8%, DIR_PATH: 2% #3 path: 76%, name: 16%, DIR_PATH: 8% \u22b2 String variables are not confused their semantic role is inferred correctly. #1 Payload: 66%, payload_: 44% \u22b2 The model is commonly confused by aliases, i.e. variables that point to the same location in memory. In this sample, either choice would have yielded identical behavior. /// <summary> /// Notifies all subscribed observers about the exception. /// </summary> /// <param name=\"error\">The exception to send to all observers.</param> public override void OnError(Exception error) #1 error: 93%, _exception: 7% #2 error: 98%, _exception: 2% #3 _gate: 100%, _observers: 0% #4 _isStopped: 86%, _isDisposed: 13%, HasObservers: 1% #5 _isStopped: 91%, _isDisposed: 9%, HasObservers: 0% #6 _exception: 100%, error: 0% #7 error: 98%, _exception: 2% #8 _exception: 99%, error: 1% \u22b2 The model predicts the correct variables from all slots apart from the last. Reasoning about the last one, requires interprocedural understanding of the code across the class file. #1 message: 100%, Response: 0%, Message: 0% #2 message: 100%, Response: 0%, Message: 0% #3 Response: 91%, Message: 9% #4 Probe: 98%, AskedForDelete: 2% #5 Response: 98%, Message: 2% \u22b2 The model predicts correctly all usages except from the one in slot #3. Reasoning about this snippet requires additional semantic information about the intent of the code. #1 httpMethod: 99%, absoluteUrl: 1%, UserName: 0%, UserAgent: 0% #2 absoluteUrl: 99%, httpMethod: 1%, UserName: 0%, UserAgent: 0% \u22b2 The model knows about selecting the correct string parameters because it matches them to the formal parameter names. throw new InvalidOperationException(Strings_Core.FAILED_CLOCK_MONITORING);for (var port = \n#1 ; \n#2 \n< \n#3 ; \n#4 ++) \n{ \nif (!activePorts.Contains( #5 )) \nreturn \n#6 ; \n} \n\nSample 2 \n\nSample 3 \n\n[global::System.Diagnostics.DebuggerNonUserCodeAttribute] \npublic void MergeFrom(pb::CodedInputStream input) { \nuint tag; \nwhile ((tag = input.ReadTag()) != 0) { \nswitch(tag) { \ndefault: \ninput.SkipLastField(); \nbreak; \ncase 10: { \n#1 .AddEntriesFrom(input, _repeated_payload_codec); \nbreak; \n} \n} \n} \n} \n\nSample 5 \n\n{ \nif ( #1 \n== null) \nthrow new ArgumentNullException(nameof( #2 )); \n\nvar os = default(IObserver<T>[]); \nlock ( #3 ) \n{ \nCheckDisposed(); \n\nif (! #4 ) \n{ \nos = _observers.Data; \n_observers = ImmutableList<IObserver<T>>.Empty; \n#5 \n= true; \n#6 \n= \n#7 ; \n} \n} \n\nif (os != null) \n{ \nforeach (var o in os) \n{ \no.OnError( #8 ); \n} \n} \n} \n\nSample 6 \n\nprivate bool BecomingCommand(object message) \n{ \nif (ReceiveCommand( #1 ) return true; \nif ( #2 .ToString() == \n#3 ) \n#4 .Tell( #5 ); \nelse return false; \nreturn true; \n} \n\nSample 7 \n\nvar response = ResultsFilter(typeof(TResponse), \n#1 , \n#2 , request); \n\nSample 8 \n\nif ( #1 \n>= \n#2 ) \n\n\n\n-picked examples showing good and bad examples. A brief description follows after each pair. \u22b2 Slots that are checked for null-ness have similar representations. ... public IActorRef ResolveActorRef(ActorPath actorPath ){ if(HasAddress( actorPath .Address)) return _local.ResolveActorRef(RootGuardian, actorPath .ElementsWithUid); ... ...Sample 1 \n\n... \npublic void MoveShapeUp(BaseShape shape ) { \nif ( shape \n!= null) { \nfor(int i=0; i < Shapes.Count -1; i++){ \nif (Shapes[i] == shape ){ \nShapes.Move(i, ++i); \nreturn; \n} \n} \n} \n} \n... \n\n... \nlock(lockObject) { \nif ( unobservableExceptionHanler != null) \nreturn false; \nunobservableExceptionHanler = handler; \n} \n... \n\nSample 2 \n\n... \nActorPath actorPath ; \nif (TryParseCachedPath(path, out actorPath)) { \nif (HasAddress( actorPath .Address)){ \nif ( actorPath .ToStringWithoutAddress().Equals(\"/\")) \nreturn RootGuarding; \n... \n} \n... \n} \n... \n\n\nWe found fewer steps to be insufficient for good results and more propagation steps to not help substantially.\nhttp://roslyn.io\n\u22b2 Adding elements to a collection-like object yields similar representations.C DATASETThe collected dataset and its characteristics are listed inTable 3. The full dataset as a set of projects and its parsed JSON will become available online.\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, arXiv:1603.04467Large-scale machine learning on heterogeneous distributed systems. arXiv preprintMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n\nLearning natural coding conventions. Miltiadis Allamanis, T Earl, Christian Barr, Charles Bird, Sutton, Foundations of Software Engineering (FSE). Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. Learning natural coding conventions. In Foundations of Software Engineering (FSE), 2014.\n\nSuggesting accurate method and class names. Miltiadis Allamanis, T Earl, Christian Barr, Charles Bird, Sutton, Foundations of Software Engineering (FSE). Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. Suggesting accurate method and class names. In Foundations of Software Engineering (FSE), 2015.\n\nA convolutional attention network for extreme summarization of source code. Miltiadis Allamanis, Hao Peng, Charles Sutton, International Conference on Machine Learning (ICML). Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention network for extreme summarization of source code. In International Conference on Machine Learning (ICML), pp. 2091-2100, 2016.\n\nA survey of machine learning for big code and naturalness. Miltiadis Allamanis, T Earl, Premkumar Barr, Charles Devanbu, Sutton, arXiv:1709.06182arXiv preprintMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine learning for big code and naturalness. arXiv preprint arXiv:1709.06182, 2017.\n\nAutomated software transplantation. T Earl, Mark Barr, Yue Harman, Alexandru Jia, Justyna Marginean, Petke, International Symposium on Software Testing and Analysis (ISSTA). Earl T Barr, Mark Harman, Yue Jia, Alexandru Marginean, and Justyna Petke. Automated software transplantation. In International Symposium on Software Testing and Analysis (ISSTA), 2015.\n\nLearning Python code suggestion with a sparse pointer network. Avishkar Bhoopchand, Tim Rockt\u00e4schel, Earl Barr, Sebastian Riedel, arXiv:1611.08307arXiv preprintAvishkar Bhoopchand, Tim Rockt\u00e4schel, Earl Barr, and Sebastian Riedel. Learning Python code suggestion with a sparse pointer network. arXiv preprint arXiv:1611.08307, 2016.\n\nPHOG: probabilistic model for code. Pavol Bielik, Veselin Raychev, Martin Vechev, International Conference on Machine Learning (ICML). Pavol Bielik, Veselin Raychev, and Martin Vechev. PHOG: probabilistic model for code. In Interna- tional Conference on Machine Learning (ICML), 2016.\n\nOn the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Dzmitry Bart Van Merri\u00ebnboer, Yoshua Bahdanau, Bengio, Syntax, Semantics and Structure in Statistical Translation. Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in Statistical Translation, 2014.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst, Neural Information Processing Systems (NIPS). Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Neural Information Processing Systems (NIPS), pp. 3844-3852, 2016.\n\nNeural message passing for quantum chemistry. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, George E Dahl, arXiv:1704.01212arXiv preprintJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\n\nA new model for learning in graph domains. Marco Gori, Gabriele Monfardini, Franco Scarselli, IEEE International Joint Conference Neural Networks (IJCNN). IEEEMarco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In IEEE International Joint Conference Neural Networks (IJCNN). IEEE, 2005.\n\nnode2vec: Scalable feature learning for networks. Aditya Grover, Jure Leskovec, International Conference on Knowledge Discovery and Data Mining (SIGKDD). ACMAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Interna- tional Conference on Knowledge Discovery and Data Mining (SIGKDD), pp. 855-864. ACM, 2016.\n\nOn the naturalness of software. Abram Hindle, T Earl, Zhendong Barr, Mark Su, Premkumar Gabel, Devanbu, International Conference on Software Engineering (ICSE). Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the natural- ness of software. In International Conference on Software Engineering (ICSE), 2012.\n\nSemi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.02907arXiv preprintThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n\nGated graph sequence neural networks. Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel, International Conference on Learning Representations (ICLR). Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations (ICLR), 2015.\n\nStructured generative models of natural source code. J Chris, Daniel Maddison, Tarlow, International Conference on Machine Learning (ICML). Chris J Maddison and Daniel Tarlow. Structured generative models of natural source code. In International Conference on Machine Learning (ICML), 2014.\n\nEncoding sentences with graph convolutional networks for semantic role labeling. Diego Marcheggiani, Ivan Titov, ACL. Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for semantic role labeling. In ACL, 2017.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Neural Information Processing Systems (NIPS). Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa- tions of words and phrases and their compositionality. In Neural Information Processing Systems (NIPS), 2013.\n\nGloVe: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.\n\nCode completion with statistical language models. Veselin Raychev, Martin Vechev, Eran Yahav, Programming Languages Design and Implementation (PLDI). Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical language models. In Programming Languages Design and Implementation (PLDI), pp. 419-428, 2014.\n\nPredicting program properties from Big Code. Veselin Raychev, Martin Vechev, Andreas Krause, Principles of Programming Languages (POPL). Veselin Raychev, Martin Vechev, and Andreas Krause. Predicting program properties from Big Code. In Principles of Programming Languages (POPL), 2015.\n\nProbabilistic model for code with decision trees. Veselin Raychev, Pavol Bielik, Martin Vechev, Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA). Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees. In Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), 2016.\n\nDetecting argument selection defects. Andrew Rice, Edward Aftandilian, Ciera Jaspan, Emily Johnston, Michael Pradel, Yulissa Arroyo-Paredes, Andrew Rice, Edward Aftandilian, Ciera Jaspan, Emily Johnston, Michael Pradel, and Yulissa Arroyo-Paredes. Detecting argument selection defects. 2017.\n\nProgram synthesis by sketching. Armando Solar-Lezama, University of California, BerkeleyArmando Solar-Lezama. Program synthesis by sketching. University of California, Berkeley, 2008.\n\nPremise selection for theorem proving by deep graph embedding. Mingzhe Wang, Yihe Tang, Jian Wang, Jia Deng, arXiv:1709.09994arXiv preprintMingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving by deep graph embedding. arXiv preprint arXiv:1709.09994, 2017.\n", "annotations": {"author": "[{\"end\":109,\"start\":56},{\"end\":161,\"start\":110},{\"end\":235,\"start\":162}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":66},{\"end\":127,\"start\":115},{\"end\":177,\"start\":170}]", "author_first_name": "[{\"end\":65,\"start\":56},{\"end\":114,\"start\":110},{\"end\":169,\"start\":162}]", "author_affiliation": "[{\"end\":108,\"start\":77},{\"end\":160,\"start\":129},{\"end\":234,\"start\":195}]", "title": "[{\"end\":43,\"start\":1},{\"end\":278,\"start\":236}]", "venue": null, "abstract": "[{\"end\":1574,\"start\":290}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1859,\"start\":1835},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2010,\"start\":1989},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2031,\"start\":2010},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2054,\"start\":2031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2096,\"start\":2071},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2166,\"start\":2144},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3096,\"start\":3072},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3117,\"start\":3096},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4658,\"start\":4636},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4694,\"start\":4669},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5982,\"start\":5958},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6017,\"start\":5997},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6043,\"start\":6019},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6349,\"start\":6326},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6576,\"start\":6555},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6871,\"start\":6848},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6967,\"start\":6947},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7413,\"start\":7394},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7429,\"start\":7413},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7453,\"start\":7429},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7474,\"start\":7453},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7494,\"start\":7474},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7680,\"start\":7655},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7743,\"start\":7715},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7810,\"start\":7792},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9689,\"start\":9672},{\"end\":10133,\"start\":10130},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11140,\"start\":11122},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13350,\"start\":13332},{\"end\":17922,\"start\":17915},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18913,\"start\":18893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22670,\"start\":22647}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31091,\"start\":30900},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31297,\"start\":31092},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31366,\"start\":31298},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32286,\"start\":31367},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32607,\"start\":32287},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32715,\"start\":32608},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33239,\"start\":32716},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33704,\"start\":33240},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36932,\"start\":33705},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37832,\"start\":36933}]", "paragraph": "[{\"end\":2933,\"start\":1590},{\"end\":3791,\"start\":2935},{\"end\":4170,\"start\":3793},{\"end\":5050,\"start\":4172},{\"end\":5851,\"start\":5052},{\"end\":6289,\"start\":5868},{\"end\":6872,\"start\":6291},{\"end\":7366,\"start\":6874},{\"end\":7915,\"start\":7368},{\"end\":8515,\"start\":7938},{\"end\":9304,\"start\":8536},{\"end\":9595,\"start\":9334},{\"end\":10040,\"start\":9597},{\"end\":10451,\"start\":10042},{\"end\":10748,\"start\":10473},{\"end\":10998,\"start\":10766},{\"end\":11316,\"start\":11051},{\"end\":12096,\"start\":11335},{\"end\":12980,\"start\":12098},{\"end\":13074,\"start\":12982},{\"end\":13596,\"start\":13143},{\"end\":13865,\"start\":13598},{\"end\":14345,\"start\":13867},{\"end\":15190,\"start\":14403},{\"end\":15808,\"start\":15192},{\"end\":16729,\"start\":15810},{\"end\":17624,\"start\":16731},{\"end\":18052,\"start\":17626},{\"end\":19670,\"start\":18071},{\"end\":19779,\"start\":19685},{\"end\":20264,\"start\":19781},{\"end\":20849,\"start\":20266},{\"end\":21302,\"start\":20851},{\"end\":22423,\"start\":21304},{\"end\":23746,\"start\":22425},{\"end\":24024,\"start\":23774},{\"end\":25529,\"start\":24043},{\"end\":26000,\"start\":25556},{\"end\":29225,\"start\":26036},{\"end\":30013,\"start\":29254},{\"end\":30463,\"start\":30048},{\"end\":30899,\"start\":30516}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10472,\"start\":10452},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10765,\"start\":10749},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11050,\"start\":10999},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13142,\"start\":13075},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14402,\"start\":14346}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22797,\"start\":22790},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23558,\"start\":23551},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24182,\"start\":24175}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1588,\"start\":1576},{\"attributes\":{\"n\":\"2\"},\"end\":5866,\"start\":5854},{\"attributes\":{\"n\":\"3\"},\"end\":7936,\"start\":7918},{\"end\":8534,\"start\":8518},{\"attributes\":{\"n\":\"4\"},\"end\":9332,\"start\":9307},{\"end\":11333,\"start\":11319},{\"attributes\":{\"n\":\"4.1\"},\"end\":18069,\"start\":18055},{\"attributes\":{\"n\":\"5\"},\"end\":19683,\"start\":19673},{\"attributes\":{\"n\":\"5.1\"},\"end\":23772,\"start\":23749},{\"end\":24041,\"start\":24027},{\"attributes\":{\"n\":\"5.2\"},\"end\":25554,\"start\":25532},{\"attributes\":{\"n\":\"5.3\"},\"end\":26034,\"start\":26003},{\"attributes\":{\"n\":\"6\"},\"end\":29252,\"start\":29228},{\"end\":30046,\"start\":30016},{\"end\":30514,\"start\":30466},{\"end\":31309,\"start\":31299},{\"end\":31388,\"start\":31368},{\"end\":32619,\"start\":32609},{\"end\":32726,\"start\":32717},{\"end\":33250,\"start\":33241}]", "table": "[{\"end\":33239,\"start\":32917},{\"end\":33704,\"start\":33320},{\"end\":36932,\"start\":35862},{\"end\":37832,\"start\":37273}]", "figure_caption": "[{\"end\":31091,\"start\":30902},{\"end\":31297,\"start\":31094},{\"end\":31366,\"start\":31311},{\"end\":32286,\"start\":31391},{\"end\":32607,\"start\":32289},{\"end\":32715,\"start\":32621},{\"end\":32917,\"start\":32728},{\"end\":33320,\"start\":33252},{\"end\":35862,\"start\":33707},{\"end\":37273,\"start\":36935}]", "figure_ref": "[{\"end\":3537,\"start\":3529},{\"end\":3811,\"start\":3803},{\"end\":8222,\"start\":8216},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12095,\"start\":12088},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12979,\"start\":12972},{\"end\":24768,\"start\":24760},{\"end\":25350,\"start\":25344},{\"end\":26511,\"start\":26503},{\"end\":26821,\"start\":26813},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26832,\"start\":26826},{\"end\":26915,\"start\":26907},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27004,\"start\":26996}]", "bib_author_first_name": "[{\"end\":38209,\"start\":38203},{\"end\":38223,\"start\":38217},{\"end\":38237,\"start\":38233},{\"end\":38252,\"start\":38246},{\"end\":38268,\"start\":38261},{\"end\":38280,\"start\":38275},{\"end\":38292,\"start\":38288},{\"end\":38294,\"start\":38293},{\"end\":38308,\"start\":38304},{\"end\":38323,\"start\":38316},{\"end\":38338,\"start\":38330},{\"end\":38758,\"start\":38749},{\"end\":38771,\"start\":38770},{\"end\":38787,\"start\":38778},{\"end\":38801,\"start\":38794},{\"end\":39072,\"start\":39063},{\"end\":39085,\"start\":39084},{\"end\":39101,\"start\":39092},{\"end\":39115,\"start\":39108},{\"end\":39425,\"start\":39416},{\"end\":39440,\"start\":39437},{\"end\":39454,\"start\":39447},{\"end\":39789,\"start\":39780},{\"end\":39802,\"start\":39801},{\"end\":39818,\"start\":39809},{\"end\":39832,\"start\":39825},{\"end\":40089,\"start\":40088},{\"end\":40100,\"start\":40096},{\"end\":40110,\"start\":40107},{\"end\":40128,\"start\":40119},{\"end\":40141,\"start\":40134},{\"end\":40484,\"start\":40476},{\"end\":40500,\"start\":40497},{\"end\":40518,\"start\":40514},{\"end\":40534,\"start\":40525},{\"end\":40788,\"start\":40783},{\"end\":40804,\"start\":40797},{\"end\":40820,\"start\":40814},{\"end\":41119,\"start\":41110},{\"end\":41132,\"start\":41125},{\"end\":41161,\"start\":41155},{\"end\":41545,\"start\":41538},{\"end\":41564,\"start\":41558},{\"end\":41580,\"start\":41574},{\"end\":41907,\"start\":41901},{\"end\":41922,\"start\":41916},{\"end\":41924,\"start\":41923},{\"end\":41944,\"start\":41937},{\"end\":41946,\"start\":41945},{\"end\":41959,\"start\":41954},{\"end\":41975,\"start\":41969},{\"end\":41977,\"start\":41976},{\"end\":42238,\"start\":42233},{\"end\":42253,\"start\":42245},{\"end\":42272,\"start\":42266},{\"end\":42580,\"start\":42574},{\"end\":42593,\"start\":42589},{\"end\":42905,\"start\":42900},{\"end\":42915,\"start\":42914},{\"end\":42930,\"start\":42922},{\"end\":42941,\"start\":42937},{\"end\":42955,\"start\":42946},{\"end\":43272,\"start\":43271},{\"end\":43284,\"start\":43281},{\"end\":43510,\"start\":43505},{\"end\":43521,\"start\":43515},{\"end\":43534,\"start\":43530},{\"end\":43556,\"start\":43549},{\"end\":43851,\"start\":43850},{\"end\":43865,\"start\":43859},{\"end\":44175,\"start\":44170},{\"end\":44194,\"start\":44190},{\"end\":44420,\"start\":44415},{\"end\":44434,\"start\":44430},{\"end\":44449,\"start\":44446},{\"end\":44460,\"start\":44456},{\"end\":44462,\"start\":44461},{\"end\":44476,\"start\":44472},{\"end\":44790,\"start\":44783},{\"end\":44810,\"start\":44803},{\"end\":44832,\"start\":44819},{\"end\":45033,\"start\":45026},{\"end\":45049,\"start\":45043},{\"end\":45062,\"start\":45058},{\"end\":45355,\"start\":45348},{\"end\":45371,\"start\":45365},{\"end\":45387,\"start\":45380},{\"end\":45648,\"start\":45641},{\"end\":45663,\"start\":45658},{\"end\":45678,\"start\":45672},{\"end\":45993,\"start\":45987},{\"end\":46006,\"start\":46000},{\"end\":46025,\"start\":46020},{\"end\":46039,\"start\":46034},{\"end\":46057,\"start\":46050},{\"end\":46073,\"start\":46066},{\"end\":46281,\"start\":46274},{\"end\":46497,\"start\":46490},{\"end\":46508,\"start\":46504},{\"end\":46519,\"start\":46515},{\"end\":46529,\"start\":46526}]", "bib_author_last_name": "[{\"end\":38215,\"start\":38210},{\"end\":38231,\"start\":38224},{\"end\":38244,\"start\":38238},{\"end\":38259,\"start\":38253},{\"end\":38273,\"start\":38269},{\"end\":38286,\"start\":38281},{\"end\":38302,\"start\":38295},{\"end\":38314,\"start\":38309},{\"end\":38328,\"start\":38324},{\"end\":38344,\"start\":38339},{\"end\":38768,\"start\":38759},{\"end\":38776,\"start\":38772},{\"end\":38792,\"start\":38788},{\"end\":38806,\"start\":38802},{\"end\":38814,\"start\":38808},{\"end\":39082,\"start\":39073},{\"end\":39090,\"start\":39086},{\"end\":39106,\"start\":39102},{\"end\":39120,\"start\":39116},{\"end\":39128,\"start\":39122},{\"end\":39435,\"start\":39426},{\"end\":39445,\"start\":39441},{\"end\":39461,\"start\":39455},{\"end\":39799,\"start\":39790},{\"end\":39807,\"start\":39803},{\"end\":39823,\"start\":39819},{\"end\":39840,\"start\":39833},{\"end\":39848,\"start\":39842},{\"end\":40094,\"start\":40090},{\"end\":40105,\"start\":40101},{\"end\":40117,\"start\":40111},{\"end\":40132,\"start\":40129},{\"end\":40151,\"start\":40142},{\"end\":40158,\"start\":40153},{\"end\":40495,\"start\":40485},{\"end\":40512,\"start\":40501},{\"end\":40523,\"start\":40519},{\"end\":40541,\"start\":40535},{\"end\":40795,\"start\":40789},{\"end\":40812,\"start\":40805},{\"end\":40827,\"start\":40821},{\"end\":41123,\"start\":41120},{\"end\":41153,\"start\":41133},{\"end\":41170,\"start\":41162},{\"end\":41178,\"start\":41172},{\"end\":41556,\"start\":41546},{\"end\":41572,\"start\":41565},{\"end\":41594,\"start\":41581},{\"end\":41914,\"start\":41908},{\"end\":41935,\"start\":41925},{\"end\":41952,\"start\":41947},{\"end\":41967,\"start\":41960},{\"end\":41982,\"start\":41978},{\"end\":42243,\"start\":42239},{\"end\":42264,\"start\":42254},{\"end\":42282,\"start\":42273},{\"end\":42587,\"start\":42581},{\"end\":42602,\"start\":42594},{\"end\":42912,\"start\":42906},{\"end\":42920,\"start\":42916},{\"end\":42935,\"start\":42931},{\"end\":42944,\"start\":42942},{\"end\":42961,\"start\":42956},{\"end\":42970,\"start\":42963},{\"end\":43279,\"start\":43273},{\"end\":43289,\"start\":43285},{\"end\":43298,\"start\":43291},{\"end\":43513,\"start\":43511},{\"end\":43528,\"start\":43522},{\"end\":43547,\"start\":43535},{\"end\":43562,\"start\":43557},{\"end\":43857,\"start\":43852},{\"end\":43874,\"start\":43866},{\"end\":43882,\"start\":43876},{\"end\":44188,\"start\":44176},{\"end\":44200,\"start\":44195},{\"end\":44428,\"start\":44421},{\"end\":44444,\"start\":44435},{\"end\":44454,\"start\":44450},{\"end\":44470,\"start\":44463},{\"end\":44481,\"start\":44477},{\"end\":44801,\"start\":44791},{\"end\":44817,\"start\":44811},{\"end\":44840,\"start\":44833},{\"end\":45041,\"start\":45034},{\"end\":45056,\"start\":45050},{\"end\":45068,\"start\":45063},{\"end\":45363,\"start\":45356},{\"end\":45378,\"start\":45372},{\"end\":45394,\"start\":45388},{\"end\":45656,\"start\":45649},{\"end\":45670,\"start\":45664},{\"end\":45685,\"start\":45679},{\"end\":45998,\"start\":45994},{\"end\":46018,\"start\":46007},{\"end\":46032,\"start\":46026},{\"end\":46048,\"start\":46040},{\"end\":46064,\"start\":46058},{\"end\":46088,\"start\":46074},{\"end\":46294,\"start\":46282},{\"end\":46502,\"start\":46498},{\"end\":46513,\"start\":46509},{\"end\":46524,\"start\":46520},{\"end\":46534,\"start\":46530}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b0\"},\"end\":38710,\"start\":38203},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2437629},\"end\":39017,\"start\":38712},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9279336},\"end\":39338,\"start\":39019},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2723946},\"end\":39719,\"start\":39340},{\"attributes\":{\"doi\":\"arXiv:1709.06182\",\"id\":\"b4\"},\"end\":40050,\"start\":39721},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13446133},\"end\":40411,\"start\":40052},{\"attributes\":{\"doi\":\"arXiv:1611.08307\",\"id\":\"b6\"},\"end\":40745,\"start\":40413},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13020969},\"end\":41031,\"start\":40747},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11336213},\"end\":41456,\"start\":41033},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3016223},\"end\":41853,\"start\":41458},{\"attributes\":{\"doi\":\"arXiv:1704.01212\",\"id\":\"b10\"},\"end\":42188,\"start\":41855},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":20480879},\"end\":42522,\"start\":42190},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207238980},\"end\":42866,\"start\":42524},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2846066},\"end\":43203,\"start\":42868},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b14\"},\"end\":43465,\"start\":43205},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8393918},\"end\":43795,\"start\":43467},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5737841},\"end\":44087,\"start\":43797},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16839291},\"end\":44336,\"start\":44089},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16447573},\"end\":44734,\"start\":44338},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1957433},\"end\":44974,\"start\":44736},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13040187},\"end\":45301,\"start\":44976},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14571254},\"end\":45589,\"start\":45303},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2658344},\"end\":45947,\"start\":45591},{\"attributes\":{\"id\":\"b23\"},\"end\":46240,\"start\":45949},{\"attributes\":{\"id\":\"b24\"},\"end\":46425,\"start\":46242},{\"attributes\":{\"doi\":\"arXiv:1709.09994\",\"id\":\"b25\"},\"end\":46717,\"start\":46427}]", "bib_title": "[{\"end\":38747,\"start\":38712},{\"end\":39061,\"start\":39019},{\"end\":39414,\"start\":39340},{\"end\":40086,\"start\":40052},{\"end\":40781,\"start\":40747},{\"end\":41108,\"start\":41033},{\"end\":41536,\"start\":41458},{\"end\":42231,\"start\":42190},{\"end\":42572,\"start\":42524},{\"end\":42898,\"start\":42868},{\"end\":43503,\"start\":43467},{\"end\":43848,\"start\":43797},{\"end\":44168,\"start\":44089},{\"end\":44413,\"start\":44338},{\"end\":44781,\"start\":44736},{\"end\":45024,\"start\":44976},{\"end\":45346,\"start\":45303},{\"end\":45639,\"start\":45591}]", "bib_author": "[{\"end\":38217,\"start\":38203},{\"end\":38233,\"start\":38217},{\"end\":38246,\"start\":38233},{\"end\":38261,\"start\":38246},{\"end\":38275,\"start\":38261},{\"end\":38288,\"start\":38275},{\"end\":38304,\"start\":38288},{\"end\":38316,\"start\":38304},{\"end\":38330,\"start\":38316},{\"end\":38346,\"start\":38330},{\"end\":38770,\"start\":38749},{\"end\":38778,\"start\":38770},{\"end\":38794,\"start\":38778},{\"end\":38808,\"start\":38794},{\"end\":38816,\"start\":38808},{\"end\":39084,\"start\":39063},{\"end\":39092,\"start\":39084},{\"end\":39108,\"start\":39092},{\"end\":39122,\"start\":39108},{\"end\":39130,\"start\":39122},{\"end\":39437,\"start\":39416},{\"end\":39447,\"start\":39437},{\"end\":39463,\"start\":39447},{\"end\":39801,\"start\":39780},{\"end\":39809,\"start\":39801},{\"end\":39825,\"start\":39809},{\"end\":39842,\"start\":39825},{\"end\":39850,\"start\":39842},{\"end\":40096,\"start\":40088},{\"end\":40107,\"start\":40096},{\"end\":40119,\"start\":40107},{\"end\":40134,\"start\":40119},{\"end\":40153,\"start\":40134},{\"end\":40160,\"start\":40153},{\"end\":40497,\"start\":40476},{\"end\":40514,\"start\":40497},{\"end\":40525,\"start\":40514},{\"end\":40543,\"start\":40525},{\"end\":40797,\"start\":40783},{\"end\":40814,\"start\":40797},{\"end\":40829,\"start\":40814},{\"end\":41125,\"start\":41110},{\"end\":41155,\"start\":41125},{\"end\":41172,\"start\":41155},{\"end\":41180,\"start\":41172},{\"end\":41558,\"start\":41538},{\"end\":41574,\"start\":41558},{\"end\":41596,\"start\":41574},{\"end\":41916,\"start\":41901},{\"end\":41937,\"start\":41916},{\"end\":41954,\"start\":41937},{\"end\":41969,\"start\":41954},{\"end\":41984,\"start\":41969},{\"end\":42245,\"start\":42233},{\"end\":42266,\"start\":42245},{\"end\":42284,\"start\":42266},{\"end\":42589,\"start\":42574},{\"end\":42604,\"start\":42589},{\"end\":42914,\"start\":42900},{\"end\":42922,\"start\":42914},{\"end\":42937,\"start\":42922},{\"end\":42946,\"start\":42937},{\"end\":42963,\"start\":42946},{\"end\":42972,\"start\":42963},{\"end\":43281,\"start\":43271},{\"end\":43291,\"start\":43281},{\"end\":43300,\"start\":43291},{\"end\":43515,\"start\":43505},{\"end\":43530,\"start\":43515},{\"end\":43549,\"start\":43530},{\"end\":43564,\"start\":43549},{\"end\":43859,\"start\":43850},{\"end\":43876,\"start\":43859},{\"end\":43884,\"start\":43876},{\"end\":44190,\"start\":44170},{\"end\":44202,\"start\":44190},{\"end\":44430,\"start\":44415},{\"end\":44446,\"start\":44430},{\"end\":44456,\"start\":44446},{\"end\":44472,\"start\":44456},{\"end\":44483,\"start\":44472},{\"end\":44803,\"start\":44783},{\"end\":44819,\"start\":44803},{\"end\":44842,\"start\":44819},{\"end\":45043,\"start\":45026},{\"end\":45058,\"start\":45043},{\"end\":45070,\"start\":45058},{\"end\":45365,\"start\":45348},{\"end\":45380,\"start\":45365},{\"end\":45396,\"start\":45380},{\"end\":45658,\"start\":45641},{\"end\":45672,\"start\":45658},{\"end\":45687,\"start\":45672},{\"end\":46000,\"start\":45987},{\"end\":46020,\"start\":46000},{\"end\":46034,\"start\":46020},{\"end\":46050,\"start\":46034},{\"end\":46066,\"start\":46050},{\"end\":46090,\"start\":46066},{\"end\":46296,\"start\":46274},{\"end\":46504,\"start\":46490},{\"end\":46515,\"start\":46504},{\"end\":46526,\"start\":46515},{\"end\":46536,\"start\":46526}]", "bib_venue": "[{\"end\":38427,\"start\":38362},{\"end\":38857,\"start\":38816},{\"end\":39171,\"start\":39130},{\"end\":39514,\"start\":39463},{\"end\":39778,\"start\":39721},{\"end\":40224,\"start\":40160},{\"end\":40474,\"start\":40413},{\"end\":40880,\"start\":40829},{\"end\":41238,\"start\":41180},{\"end\":41640,\"start\":41596},{\"end\":41899,\"start\":41855},{\"end\":42343,\"start\":42284},{\"end\":42676,\"start\":42604},{\"end\":43027,\"start\":42972},{\"end\":43269,\"start\":43205},{\"end\":43623,\"start\":43564},{\"end\":43935,\"start\":43884},{\"end\":44205,\"start\":44202},{\"end\":44527,\"start\":44483},{\"end\":44847,\"start\":44842},{\"end\":45124,\"start\":45070},{\"end\":45438,\"start\":45396},{\"end\":45761,\"start\":45687},{\"end\":45985,\"start\":45949},{\"end\":46272,\"start\":46242},{\"end\":46488,\"start\":46427}]"}}}, "year": 2023, "month": 12, "day": 17}