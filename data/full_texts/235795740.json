{"id": 235795740, "updated": "2023-10-06 01:21:33.463", "metadata": {"title": "MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding", "authors": "[{\"first\":\"Yi-Hui\",\"last\":\"Chou\",\"middle\":[]},{\"first\":\"I-Chun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Chin-Jui\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Joann\",\"last\":\"Ching\",\"middle\":[]},{\"first\":\"Yi-Hsuan\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 7, "day": 12}, "abstract": "This paper presents an attempt to employ the mask language modeling approach of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of polyphonic piano MIDI files for tackling a number of symbolic-domain discriminative music understanding tasks. These include two note-level classification tasks, i.e., melody extraction and velocity prediction, as well as two sequence-level classification tasks, i.e., composer classification and emotion classification. We find that, given a pre-trained Transformer, our models outperform recurrent neural network based baselines with less than 10 epochs of fine-tuning. Ablation studies show that the pre-training remains effective even if none of the MIDI data of the downstream tasks are seen at the pre-training stage, and that freezing the self-attention layers of the Transformer at the fine-tuning stage slightly degrades performance. All the five datasets employed in this work are publicly available, as well as checkpoints of our pre-trained and fine-tuned models. As such, our research can be taken as a benchmark for symbolic-domain music understanding.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2107.05223", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2107-05223", "doi": null}}, "content": {"source": {"pdf_hash": "15da40e406a3f59af319352bbe7b187115a7894b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.05223v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f23516b8666c3e5efbe2c45e3d35fba9ebcca9dc", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/15da40e406a3f59af319352bbe7b187115a7894b.txt", "contents": "\nMidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding\n12 Jul 2021\n\nYi-Hui Chou \nI-Chun Chen \nChin-Jui Chang \nJoann Ching \nYi-Hsuan Yang \nMidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding\n12 Jul 20210EA2FE61593D3709D875105301F7EA7BarXiv:2107.05223v1[cs.SD]Large-scale pre-trained modelTransformersymbolic-domain music understandingmelody recognitionvelocity predictioncomposer classificationemotion classification\nThis paper presents an attempt to employ the mask language modeling approach of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of polyphonic piano MIDI files for tackling a number of symbolic-domain discriminative music understanding tasks.These include two note-level classification tasks, i.e., melody extraction and velocity prediction, as well as two sequence-level classification tasks, i.e., composer classification and emotion classification.We find that, given a pretrained Transformer, our models outperform recurrent neural network based baselines with less than 10 epochs of fine-tuning.Ablation studies show that the pre-training remains effective even if none of the MIDI data of the downstream tasks are seen at the pre-training stage, and that freezing the self-attention layers of the Transformer at the fine-tuning stage slightly degrades performance.All the five datasets employed in this work are publicly available, as well as checkpoints of our pre-trained and fine-tuned models.As such, our research can be taken as a benchmark for symbolic-domain music understanding.\n\nI. INTRODUCTION\n\nMachine learning research on musical data has been predominately focusing on audio signals in the literature, mainly due to the demand for effective content-based music retrieval and organization systems started over two decades ago [1]- [4].To improve the performance of music retrieval tasks such as similarity search, semantic keyword search, and music recommendation, a variety of audio-domain music understanding tasks have been widely studied, including beat and downbeat tracking [5], [6], multi-pitch detection [7]- [9], melody extraction [10], chord recognition [11], emotion recognition [12], and genre classification [13], to name just a few.\n\nIn contrast, relatively less research has been done on music understanding technology for music in symbolic formats such as MusicXML and MIDI. 1 Existing labeled datasets for many symbolic-domain tasks remain small in size [14]- [16], making it hard to train effective supervised machine learning models. 2 It was only until recent years that we saw a growing body of research work on symbolic music, largely thanks to a surge\n\nThe first two authors contribute equally to the paper.*The authors are all affiliated with the Research Center for IT Innovation, Academia Sinica, Taiwan.YH Chou is also affiliated with National Taiwan University; IC Chen with National Tsing Hua university; and YH Yang with Taiwan AI Labs (e-mail: b06901012@ntu.edu.tw,yomai107062130@gapp.nthu.edu.tw,{csc63182, joann8512, yang}@citi.sinica.edu.tw). 1 Musical Instrument Digital Interface; https://midi.org/specifications 2 There are large-scale datasets such as the Lakh MIDI (LMD) dataset [17] and DadaGP [18], but they do not come with high-quality human labels.\n\nof renewed interest in symbolic music generation [19]- [24].However, research on discriminative tasks, such as identifying the melody notes in a MIDI file of polyphonic piano music [15], [25], remains not much explored.\n\nIn the literature on machine learning, a prominent approach to overcome the labeled data scarcity issue is to adopt \"transfer learning\" and divide the learning problem into two stages [26]: a pre-training stage that establishes a model capturing general knowledge from one or multiple source tasks, and a fine-tuning stage that transfers the captured knowledge to target tasks.Model pre-training can be done using either a labeled dataset [27]- [29] (e.g., to train a VGG-ish like model over millions of human-labeled clips of general sound events and then finetune it on instrument classification data [30]), or an unlabeled dataset using self-supervised training strategy.The latter is in particular popular in the field of natural language processing (NLP), where pre-trained models (PTMs) using Transformers [31] have achieved state-of-the-art results on almost all NLP tasks, including generative and discriminative ones [26].\n\nThis paper presents an empirical study employing PTMs to symbolic-domain music understanding tasks.In particular, being inspired by the growing trend of treating MIDI music as a \"language\" in deep generative models for symbolic music [32]- [37], we employ a Transformer based network pre-trained using a self-supervised training strategy called \"mask language modeling\" (MLM), which has been widely used in BERT-like PTMs in NLP [38]- [42].Despite the fame of BERT, we are aware of only two publications that employ BERT-like PTMs for symbolic music classification [43], [44].We discuss how our work differs from these prior arts in Section II.\n\nWe evaluate the PTM on in total four music understanding tasks.These include two note-level classification tasks, i.e., melody extraction [15], [25] and velocity prediction [45], [46], and two sequence-level classification tasks, i.e., composer classification [43], [47], [48] and emotion classification [49]- [52].We use in total five datasets in this work, amounting to 4,166 pieces of piano MIDI.We give details of these tasks in Section V and the employed datasets in Section III.\n\nAs the major contribution of the paper, we report in Section VIII a comprehensive performance study of variants of PTM for this diverse set of classification tasks, showing that the \"pre-train and fine-tune\" strategy does lead to higher accuracy than strong recurrent neural network (RNN) based baselines (described in Section VI).And, we study the effect of the way musical events are represented as tokens in Transformers (see Section IV); the corpus used in pre-training, e.g., including the MIDI (but not the labels) of the datasets of the downstream tasks in a transductive learning setting or not; and whether to  freeze the self-attention layers of the PTM at fine-tuning.\n\nAs the secondary contribution, we open-source the code and release checkpoints of the pre-trained and fine-tuned models at https://github.com/wazenmai/MIDI-BERT. Together with the fact that all the datasets employed in this work are publicly available, our research can be taken as a new testbed of PTMs in general, and the first benchmark for deep learning-based symbolic-domain music understanding.\n\n\nII. RELATED WORK\n\nTo our best knowledge, the work of Tsai and Ji [43] represents the first attempt to use PTMs for symbolic-domain music classification.They showed that either a RoBERTa-based Transformer encoder PTM [39] or a GPT2-based Transformer encoder PTM [56] outperform non-pre-trained baselines for a 9-class symbolic-domain composer classification task.Pretraining boosts the classification accuracy for the GPT2 model greatly from 46% to 70%.However, the symbolic data format considered in their work is \"sheet music image\" [43], which are images of score sheets.This data format has been much less used than MIDI in the literature.\n\nConcurrent to our work, Zeng et al. [44] presented very recently a PTM for MIDI called \"MusicBERT.\" 3 After being pre-trained on over 1 million private multi-track MIDI pieces, MusicBERT was applied on two generative music tasks, i.e., melody completion and accompaniment suggestion, and two sequence-level discriminative tasks, i.e., genre classification and style classification, showing better result than non PTMbased baselines.Our work differs from theirs in the following aspects.First, our pre-training corpus is much smaller (only 4,166 pieces) but all public, less diverse but more dedicated (to piano music).Second, we aim at establishing a benchmark for symbolic music understanding and include not only sequencelevel but also note-level tasks.Moreover, the sizes of the labeled data for our downstream tasks are all almost under 1K, while that employed in MusicBERT (i.e., the TOP-MAGD dataset [58]) contains over 20K annotated pieces, a data size rarely seen for symbolic music tasks in general.Finally, while their token representation is designed for multi-track MIDI, ours is for single-track piano scores.\n\nThere has been decades of research on different symbolicdomain music classification tasks [59], though the progress seems relatively slower than their audio-domain counterparts.We review related publications along with the specification of the tasks considered in this work in Section V.\n\n\nIII. DATASETS\n\nWe collect four existing public-domain piano MIDI dataset, and compile a new one on our own, for the study presented in this work.All the pieces are in 4/4 time signature.We list some important statistics of these five datasets in Table I, and provide their details below.\n\nFollowing [60], we differentiate two types of MIDI files, MIDI scores, which are musical scoresheets rendered directly into MIDI with no dynamics and exactly according to the written metrical grid, and MIDI performance, which are MIDI encodings of human performances of musical scoresheets.For consistency, we convert all our datasets into MIDI scores by dropping performance-related information such as velocity and tempo, except for the training and evaluation for the velocity prediction task, where we use velocity information.\n\n\u2022 The Pop1K7 dataset [36], 4 is composed of machine transcriptions of 1,747 audio recordings of piano covers of Japanese anime, Korean and Western pop music, amounting to over 100 hours worth of data.The transcription was done with the \"onsets-and-frames\" RNN-based piano transcription model [7] and the RNN-based downbeat and beat tracking model from the Madmom library [5].We discard performance-related information, treating it as a MIDI score dataset rather than a MIDI performance one.We also temporally quantize the onset time and duration of the notes, as described in Section IV.This dataset is the largest among the five, constituting half of our training data.We only use it for pre-training.\u2022 ASAP, the aligned scores & performances dataset [53], 5contains 1,068 MIDI performances of 222 Western classical music from 15 composers, along with the MIDI scores of the 222 pieces, compiled from the MAESTRO dataset [9].We consider it as an additional dataset for pretraining, using only the MIDI scores in 4/4 time signature with no time signature change at all throughout the piece.This leaves us with 65 pieces of MIDI scores, which last for 3.5 hours.Table I shows that, being the only classical dataset among the five, ASAP features shorter average note duration and larger number of notes per bar.\u2022 POP909 [54] comprises piano covers of 909 pop songs. 6t is the only dataset among the five that provides melody, non-melody labels for each note.Specifically, each note is labeled with one of the following three classes: melody (the lead melody of the piece); bridge (the secondary melody that helps enrich the complexity of the piece); and accompaniment (including arpeggios, broken chords and many other textures).As it is a MIDI performance dataset, it also comes with velocity information.Therefore, we use it for the melody classification and velocity prediction tasks.We discard pieces that are not in 4/4 time signature, ending up with 865 pieces for this dataset.and LVLA (low valence low arousal). 9The MIDI performances of these clips are machine-transcribed from the audio recordings by the model of Kong et al. [8] and then converted into MIDI scores.We use this dataset for the emotion classification task. 10V.TOKEN REPRESENTATION Similar to text, a piece of music in MIDI can be considered as a sequence of musical events, or \"tokens.\"However, what makes music different is that musical notes are associated with a temporal length (i.e., note duration), and multiple notes can be played at the same time.Therefore, to represent music, we need note-related tokens describing, for example, the pitch and duration of the notes, as well as metric-related tokens placing the notes over a time grid.\n\nIn the literature, a variety of token representations for MIDI have been proposed, differing in many aspects such as the MIDI data being considered (e.g., melody [20], lead sheet [62], piano [33], [60], or multi-track music [23], [32]), the temporal resolution of the time grid, and the way the advancement in\nPitch (60)\nDuration ( 8)\nBar (cont)\nSub-beat ( 5)\nPitch (67)\nDuration ( 8)\nBar (cont)\nSub-beat ( 9)\nPitch (62)\nDuration ( 8)\nBar (cont)\nSub-beat ( 13)\nPitch (64)\nDuration ( 8)\nBar (new) Sub-beat (1)\nPitch ( 60)\nDuration (32)\nBar (cont)\nSub-beat (1)\nPitch ( 55)\nDuration (32)\nFig. 1: An example of a piece of score encoded using the proposed simplified version of the (a) REMI [33] and (b) CP [36] representations, using only five types of tokens, Bar, Sub-beat, Pitch, Duration, and Pad (not shown here), for piano-only MIDI scores.The text inside parentheses indicates the value each token takes.While each time step corresponds to a single token in REMI, each time step would correspond to a super token that assembles four tokens in total in CP.Without such a token grouping, the sequence length (in terms of the number of time steps) of REMI is longer than that of CP (in this example, 20 vs. 6).\n\ntime is notated [33].Auxiliary tokens describing, for example, the chord progression [33] or grooving pattern [63] underlying a piece can also be added.There is no standard thus far.\n\nIn this work, we only use MIDI scores [60] of piano music for pre-training.As such, we adopt the beat-based REMI token representation [33] to place musical notes over a discrete time grid comprising 16 sub-beats per bar, but discard REMI tokens related to the performance aspects of music, such as note velocity and tempo.In addition to REMI, we experiment with the \"token grouping\" idea of the compound word (CP) representation [36], to reduce the length of the token sequences.We depict the two adopted token representations in Figure 1, and provide some details below.\n\n\nA. REMI Token Representation\n\nThe REMI representation uses Bar and Sub-beat tokens to represent the advancement in time [33].The former marks the beginning of a new bar, while the latter points to a discrete position within a bar.Specifically, as we divide a bar into 16 sub-beats, the Sub-beat tokens can take values from 1 to 16; e.g., Sub-beat(1) indicates the position corresponding to the first sub-beat in a bar, or the first beat in 4/4 time signature, whereas Sub-beat (9) indicates the third beat.We use a Sub-beat token before each musical note, which comprises two consecutive tokens of Pitch and Duration.In other words, the Sub-beat token indicates the onset time of a note played at a certain MIDI pitch (i.e., the value taken by the Pitch token), whose duration is indicated by the Duration token, in terms of the number of \"half\" sub-beats.For example, Duration(1) and Duration(32) correspond to a thirty-second note and a whole note, respectively.\n\n\nB. CP Token Representation\n\nFigure 1(a) shows that, except for Bar, the other tokens in a REMI sequence always occur consecutively in groups, in the order of Sub-beat, Pitch, Duration.We can further differentiate Bar(new) and Bar(cont), representing respectively the beginning of a new bar and a continuation of the current bar, and always have one of them before a Sub-beat token.This way, the tokens would always occur in a group of four.Instead of feeding the token embedding of each of them individually to the Transformer, we can combine the token embeddings of the four tokens in a group by concatenation, and let the Transformer model them jointly, as depicted in Figure 1(b).We can also modify the output layer of the Transformer so that it predicts the four tokens at once with different heads.These constitute the main ideas of the CP representation [36], which has at least the following two advantages over its REMI counterpart: 1) the time steps needed to represent a MIDI piece is much reduced, since the tokens are merged into a \"super token\" (a.k.a. a \"compound word\" [36]) representing four tokens at once; 2) the self-attention in Transformer is operated over the super tokens, which might be musically more meaningful as each super token jointly represents different aspects of a musical note.Therefore, we experiment with both REMI and CP in our experiments.\n\n\nC. On Zero-padding\n\nFor training Transformers, it is convenient if all the input sequences have the same length.For both REMI and CP, we divide the token sequence for each entire piece into a number of shorter sequences with equal sequence length 512, zeropadding those at the end of a piece to 512 with appropriate number of Pad tokens.Because of the token grouping, a CP sequence for the Pop1K7 dataset would cover around 25 bars on average, whereas a corresponding REMI sequence covers only 9 bars on average.\n\nOur final token vocabulary for REMI contains 16 unique Sub-beat tokens, 86 Pitch tokens, 64 Duration tokens, one Bar token, one Pad token, and one Mask token, in total 169 tokens.For CP, we do not use a Pad token but represent a zero-padded super token by Bar(Pad), Sub-beat(Pad), Pitch(Pad), and Duration(Pad).We do similarly for a masked super token (i.e., using Bar(Mask), etc).Adding that we need an additional bar-related token Bar(cont) for CP, we see that the vocabulary size for CP is 169\u22122+8+1=176.\n\n\nV. TASK SPECIFICATION\n\nThroughout this paper, we refer to note-level classification tasks as tasks that require a prediction for each individual note in a music sequence, and sequence-level tasks as tasks that require a single prediction for an entire music sequence.We consider two note-level tasks and two sequence-level tasks in our experiments, as elaborated below. 11\n\n\nA. Symbolic-domain Melody Extraction\n\nSimilar to Simonetta et al. [15], we regard melody extraction as a task that identifies the melody notes in a single-track (i.e., single-instrument) polyphonic music such as piano or guitar music. 12Specifically, using the POP909 dataset [54], we aim to build a model that classifies each Pitch event into either melody, bridge, or accompaniment, using classification accuracy (ACC) as the evaluation metric.\n\nPossibly owing to the lack of labeled data for machine learning, the most well-known approach for melody extraction by far, the \"skyline\" algorithm, remains a simple rule-based method [25].The only exception we are aware of is the recent convolutional neural network (CNN)-based model from Simonetta et al. [15], which uses the piano-roll [65], an imagelike representation, to represent MIDI.The model was trained on a collection of 121 pieces, that do not contain bridge.\n\n\nB. Symbolic-domain Velocity Prediction\n\nVelocity is an important element in music, as a variety of dynamics is often used by musicians to add excitement and emotion to songs.Given that the tokens we choose do not contain performance information, it is interesting to see how a machine model would \"perform\" a piece by deciding these volume changes, a task that is essential in performance generation [45], [46].We treat this as a note-level classification problem and aim to classify Pitch events into six classes with the POP909 dataset [54].MIDI velocity values range from 0-127, and we quantize the information into six categories, pp (0-31), p (32-47), mp (48)(49)(50)(51)(52)(53)(54)(55)(56)(57)(58)(59)(60)(61)(62)(63), mf (64-79), f (80-95), and ff (96-127), following the specification of Apple's Logic Pro 9.\n\n\nC. Symbolic-domain Composer Classification\n\nComposer classification is a finer-grained classification task than genre classification.While genre classification may be the most widely-studied symbolic-domain discriminative task [58], [59], composer classification is less studied.We could easily find out which type of music we are listening to based on the similar patterns in that genre, while needing more musical insights to recognize the composer.\n\nDeep learning-based composer classification in MIDI has been attempted by Lee et al. [47] and Kong et al. [48], both treating MIDI as images (via the piano-roll) and using CNNbased classifiers.Our work differs from theirs in that: 1) we view MIDI as a token sequence, 2) we employ PTM, and 3) we consider non-classical music pieces (i.e., those from Pianist8).\n\n\nD. Symbolic-domain Emotion Classification\n\nEmotion classification in MIDI has been approached by a few researchers, mostly using hand-crafted features and nondeep learning classifiers [49]- [52].Some researchers work on MIDI alone, while others use both audio and MIDI in multi-modal emotion classification (e.g., [51]).The only deep learning-based approach we are aware of is presented in our prior work [55], using an RNN-based classifier called \"Bi-LSTM-Attn\" [66] (which is also used as a baseline in our experiment; see Section VI), without employing PTMs.Besides the difference in PTMs, we consider only basic properties of music (e.g., pitch and duration) from the \"MIDI score\" version of the EMOPIA dataset for emotion classification here, while we used much more information from MIDI performances in the prior work [55].\n\n\nVI. BASELINE MODEL\n\nFor the note-level classification tasks, we use as our baseline an RNN model that consists of three bi-directional long shortterm memory (Bi-LSTM) layers, each with 256 neurons, and a feed-forward, dense layer for classification, as such a network has led to state-of-the-art result in many audio-domain music understanding tasks (e.g., beat tracking [5], [6] and multi-pitch estimation [7]).All of our downstream tasks can be viewed as a multi-class classification problem.Given a REMI sequence, a Bi-LSTM model makes a prediction for each Pitch token, ignoring all the other types of tokens (i.e., Bar, Sub-beat, Duration and Pad).For CP, the Bi-LSTM model simply makes a prediction for each super token, again ignoring the zero-padded ones.\n\nFor the sequence-level tasks, which requires only a prediction for an entire sequence, we follow [55] and choose as our baseline the Bi-LSTM-Attn model from Lin et al. [66], which was originally proposed for sentiment classification in NLP.The model combines LSTM with a self-attention module for temporal aggregation.Specifically, it uses a Bi-LSTM layer 13to convert the input sequence of tokens into a sequence of embeddings (which can be considered as feature representations of the tokens), and then fuses these embeddings into one sequence-level embedding according to the weights assigned by the attention module to each token-level embedding.The sequence-level embedding then goes through two dense layers for classification.We use the token-level embeddings for all the tokens here, namely not limited to Pitch tokens.\n\n\nVII. BERT PRE-TRAINING AND FINE-TUNING\n\nWe now present our PTM, coined as \"MidiBERT-Piano,\" a pre-trained Transformer encoder with 111M parameters for machine understanding of piano MIDI music.We adopt as the model backbone the BERT BASE model [38], a classic multilayer bi-directional Transformer encoder 14 with 12 layers of multi-head self-attention, each with 12 heads, and the dimension of the hidden space of the self-attention layers being 768.\n\nBelow, we first describe the pre-training strategy, and then how we fine-tune it for the downstream tasks.\n\n\nA. Pre-training\n\nFor PTMs, an unsupervised, or self-supervised, pre-training task is needed to set the objective function for learning.We employ the mask language modeling (MLM) pre-training strategy of BERT, randomly masking 15% tokens of an input sequence, and asking the Transformer to predict (reconstruct) these masked tokens from the context made available by the visible tokens, by minimizing the cross-entropy loss. 15As a self-supervised method, MLM needs no labeled data of the downstream tasks for pre-training.Following BERT, among all the masked tokens, we replace 80% by MASK tokens, 10% by a randomly chosen token, and leave the last 10% unchanged. 16or REMI, we mask the individual tokens at random.For CP, we mask the super tokens-when we mask a super token, we will have to reconstruct (by different output heads [36]) all the four tokens composing it, as shown in Figure 2(a).\n\nFigure 2(a) also depicts that, in MidiBERT-Piano, each input token X i is firstly converted into a token embedding through an embedding layer, augmented (by addition) with a relative positional encoding [67] that is related to its position (i.e., time step) i in the sequence, and then fed to the stack of 12 self-attention layers to get an \"contextualized\" representation known as a hidden vector (or hidden states) at the output of the self-attention stack.Because of the bi-directional self-attention layers, the hidden vector is contextualized in the sense that it has attended to information from all the other tokens from the same sequence.Finally, the hidden vector of a masked token at position j is fed to a dense layer to predict what the missing (super) token is.As our network structure is rather standard, we refer readers to [31], [37], [38] for details and the mathematical underpinnings due to space limits.\n\nBecause the vocabulary sizes for the four token types are different, we weight the training loss associated with tokens of different types in proportion to the corresponding vocabulary size for both REMI and CP, to facilitate model training.\n\n\nB. Fine-tuning\n\nIt has been widely shown in NLP and related fields [68]- [71] that, by storing knowledge into huge parameters and taskspecific fine-tuning, the knowledge implicitly encoded in the parameters of a PTM can be transferred to benefit a variety of downstream tasks [26].For fine-tuning MidiBERT-Piano, we extend the architecture shown in Figure 2(a) by modifying the last few layers in two different ways, one for each of the two types of downstream classification tasks.has to predict the label of an input token during fine-tuning, by learning from the labels provided in the training data of the downstream task in a supervised way.To achieve so, we feed the hidden vectors to a stack of dense layer, a ReLU activation layer and finally another dense layer for the output classification, with 10% dropout probability.We note that this classifier design is fairly simple, as we expect much knowledge regarding the downstream task can already be extracted from the preceding self-attention layers.\n\nFigure 2(c) shows the fine-tuning architecture for sequencelevel classification.Being inspired by the Bi-LSTM-Attn model [66], we employ an attention-based weighting average mechanism to convert the sequence of 512 hidden vectors for an input sequence to one single vector before feeding it to the classifier layer, which comprises two dense layers.We note that, unlike the baseline models introduced in Section VI, we do not use RNN layers in our models. 17\n\n\nC. Implementation Details\n\nOur implementation is based on the PyTorch code from the open-source library HuggingFace [72].Given a corpus of MIDI pieces for pre-training, we use 85% of them for pretraining MidiBERT-Piano as described in Section VII-A, and 17 An alternative approach is to add the CLS token to our sequences and simply use its hidden vector as the input to the classifier layer.We do not explore this alternative since we do not have CLS tokens.the rest as the validation set.We train with a batch size of 12 sequences 18 for at most 500 epochs, using the AdamW optimizer with learning rate 2e\u22125 and weight decay rate 0.01.If the validation cross-entropy loss does not improve for 30 consecutive epochs, we early stop the training process. 19We observe that pre-training using the CP representation converges in 2.5 days on four GeForce GTX 1080-Ti GPUs, which is about 2.5 times faster than the case of REMI.\n\nWe consider as the default setting using all the five datasets listed in Table I as the pre-training corpus; this includes the three datasets of the downstream tasks (i.e., Pop909 4/4 , Pi-anist8, and EMOPIA).We consider this as a valid transductive learning setting, as we do not use the labels of the downstream tasks at all.And, we report in Section VIII-B3 the cases when the MIDI pieces of the test split, or all, of the datasets of the downstream tasks are not seen during pre-training.\n\nFor fine-tuning, we create training, validation and test splits for each of the three datasets of the downstream tasks with the 8:1:1 ratio at the piece level (i.e., all the 512-token sequences from the same piece are in the same split).With the same batch size of 12, we fine-tune the pre-trained MidiBERT-Piano for each task independently for at most 10 epochs, early stopping when there is no improvement for three consecutive epochs.Compared to pre-training, fine-tuning is less computationally expensive.All the results reported in our work can be reproduced with our GPUs in 30 minutes.\n\nIn our experiments, we will use the same pre-trained model parameters to initialize the models for different downstream tasks.During fine-tuning, we fine-tune the parameters of all the layers, including the self-attention and token embedding layers.But, we also report an ablation study in Section VIII-B2 that freezes the parameters of some layers at fine-tuning.\n\n\nVIII. PERFORMANCE STUDY A. Main Result\n\nTable II lists the testing accuracy achieved by the baseline models and the proposed ones for the four downstream tasks.We see that MidiBERT-Piano outperforms the Bi-LSTM or Bi-LSTM-Attn baselines in all tasks consistently, using either the REMI or CP representation.The combination of MidiBERT-Piano and CP (denoted as MidiBERT-Piano+CP hereafter) attains the best result in all the four tasks.We also observe that MidiBERT-Piano+CP outperforms Bi-LSTM+CP with just 1 or 2 epochs of fine-tuning, validating the strength of PTMs on symbolic-domain music understanding tasks.\n\nTable II also shows that the CP token representation tends to outperform the REMI one across different tasks for both the baseline models and the PTM-based models, demonstrating the importance of token representation for music applications.\n\nWe take a closer look at the performance of the evaluated models, in particular Bi-LSTM+CP (or Bi-LSTM-Attn+CP) and MidiBERT-Piano+CP, in different tasks in what follows.\n\n1) Melody: Figure 3 shows the normalized confusion tables on melody extraction.We see that the baseline tends to confuse melody and bridge, while our model performs a lot better, boosting the overall accuracy by almost 8% (i.e., from 88.66% to 96.37%).If we simplify the task into a \"melody vs. nonmelody\" binary classification problem by merging bridge and accompaniment, the accuracy of our model would rise to 97.09%.We can compare our model with the wellknown \"skyline\" algorithm [25] with this dichotomy, since this algorithm cannot distinguish between melody and bridge-it uses the simple rule of taking always the highest pitch in a MIDI piece as the melody, while avoiding overlapping notes.It turns out that the skyline algorithm can only attain 78.54% accuracy in the \"melody vs. non-melody\" problem.\n\nFigure 5 displays the melody/non-melody prediction result of the skyline algorithm and our model for a randomly chosen testing piece from POP909 4/4 .The skyline algorithm wrongly considers that there is a melody note all the time, while the proposed model does not.\n\n2) Velocity: Table II shows that the accuracy on our 6-class velocity classification task is not high, reaching 51.63% at best.This may be due to the fact that velocity is rather subjective, meaning that musicians can perform the same music piece fairly differently.Moreover, we note that the data is highly imbalanced, with the latter three classes (mf, f, ff) takes up nearly 90% of all labeled data.The confusion tables presented in Figure 5 show that Bi-LSTM tends to classify most of the notes into f, the most popular class among the six.This is less the case for MidiBERT-Piano, but the prediction of p and pp, i.e., the two with the lowest dynamics, remains challenging.\n\n3) Composer: Table II shows that MidiBERT-Piano greatly outperforms Bi-LSTM-Attn by around 15% in REMI and 18% in CP, reaching 78.57% testing accuracy at best for this 8-class classification problem.In addition, we see large performance gap between REMI and CP in this task, the largest among the four tasks.Figure 6 further shows that, both the baseline and our model confuse composers in similar genres or styles, and that our model performs fairly well in recognizing Herbie Hancock and Ryuichi Sakamoto.\n\n\n4) Emotion:\n\nTable II shows that MidiBERT-Piano outscores Bi-LSTM-Attn by around 14% in both REMI and CP for this 4-class classification problem, reaching 67.89% testing accuracy at best.There is little performance difference between REMI and CP in this task.Figure 7 further shows that the evaluated models can fairly easily distinguish between high arousal and low arousal pieces (i.e., 'HAHV, HALV' vs. 'LALV, LAHV'), but they have a much harder time along the valence axis (e.g., 'HAHV' vs. HALV', and 'LALV' vs 'LAHV').We see less confusion from the result of MidiBERT-Piano+CP, but there is still room for improvement.\n\n\nB. Ablation Study\n\nWe also evaluate a number of ablated versions of our model, to gain insights into the effect of different design choices.\n\n1) Initialize model without pre-trained parameters: To examine the effect of pre-training, we repeat the fine-tuning procedure except that the pre-trained model parameters are not used for initialization.Namely, we train the network shown in either Figures 2(b) or 2(c) \"from scracth\" using only the dataset of a downstream task, not the pre-trained corpus.The   third row of Table III shows that, this degrades the performance greatly, especially for the two sequence-level tasks.Without the self-supervised pre-training, the performance of this Transformer-based model turns out to be even worse than the LSTM-based baselines in three out of the four tasks (i.e., except for velocity prediction, which may be partly due to the imbalanced class distribution), suggesting that a large model may not perform better if the parameters are not properly initialized.We conjecture that this is in particular the case for downstream tasks with small number of labeled data.\n\n2) Effect of Partial Freezing: Next, we explore partial freezing during fine-tuning.We consider two options: 1) freeze the parameters of the main MidiBERT-Piano Transformer structure, and only learn the weights of the classifier layer; 2) freeze only the 12 self-attention layers, so that the token embedding layer will still be fine-tuned.As shown in the last two rows of Table III, both variants degrade the accuracy of our model for all the four tasks, suggesting that the knowledge learned during the pre-training stage of our PTM may not be  generalizeable enough to permit using the PTM's parameters \"as is\" during the fine-tuning stage.However, we see the two variants with partial freezing still outperform the RNN baseline across tasks.\n\n3) Ablation for Different Pre-train Data: Finally, we report in Table IV the performance of our model with reduced pre-training corpus.Its second row corresponds to the case where we exclude the test splits of the three datasets of the downstream tasks, whereas the last row the case where we exclude all the data of those three datasets.Interestingly, we see salient performance degradation only for the composer classification task-pre-training on only Pop1K7 and ASAP drops the testing accuracy greatly from 78.57% to 58.73%.We conjecture this is because the two datasets do not cover the genres of MIDI pieces seen in the Pianist8 dataset.This is not a problem for the melody extraction and the emotion classification tasks-pre-training on only Pop1K7 and ASAP, which cover less than half of the full-data case, barely degrades the testing accuracy.\n\n\nIX. CONCLUSION\n\nIn this paper, we have presented MidiBERT-Piano, one of the first large-scale pre-trained models for musical data in the MIDI format.We employed five public-domain polyphonic piano MIDI datasets for BERT-like masking-based pre-training, and evaluated the usefulness of the pre-trained model on four challenging downstream symbolic music understanding tasks, most with less than 1K labeled MIDI pieces.Our experiments validate the effectiveness of pre-training for both note-level and sequence-level classification tasks.\n\nThis work can be extended in many ways.First, to employ other pre-training strategies or architectures [26].Second, to employ Transformer models with linear computational complexity [73], [74], so as to use the whole MIDI pieces (instead of segments) [36], [37] at pre-training.Third, to extend the corpus and the token representation from single-track piano to multi-track MIDI, like [44].Finally, to consider more downstream tasks such as symbolic-domain music segmentation [14], [75], chord recognition [16], and beat tracking [76].We have released the code publicly, which may hopefully help facilitate such endeavors.\n\n\u2022\n\nPianist8 consists of original piano music performed by eight composers that we downloaded from YouTube for the purpose of training and evaluating symbolic-domain composer classification in this paper.7They are Richard Clayderman (pop), Yiruma (pop), Herbie Hancock (jazz), Ludovico Einaudi (contemporary), Hisaishi Joe (contemporary), Ryuichi Sakamoto (contemporary), Bethel Music (religious), and Hillsong Worship (religious).The dataset contains a total of 411 pieces with the number of pieces per composer fairly balanced.Each audio file is paired with its MIDI performance, which is machine-transcribed by the piano transcription model proposed by Kong et al.[8].Similarly, we convert the pieces into MIDI scores by dropping performance-related information.\u2022EMOPIA[55] is a new dataset of pop piano music we recently collected, also from YouTube, for research on emotion-related tasks. 8It has 1,087 clips (each around 30 seconds) segmented from 387 songs, covering Japanese anime, Korean & Western pop song covers, movie soundtracks, and personal compositions.The emotion of each clip has been labeled using the following 4-class taxonomy: HVHA (high valence high arousal); HVLA (high valence low arousal); LVHA (low valence high arousal);\n\n\nFig. 2 :\n2\nFig. 2: Illustration of the (a) pre-training procedure of MidiBERT-Piano for a CP sequence, where the model learns to predict (reconstruct) randomly-picked super tokens masked in the input sequence (each consisting of four tokens, as the example one shown in the middle with time step t); and (b), (c) the fine-tuning procedure for note-level and sequence-level classification.Apart from the outputting few layers, both pre-training and fine-tuning use the same architecture.\n\n\nFigure 2 (\n2\nFigure 2(b)  shows the fine-tuning architecture for notelevel classification.While the Transformer uses the hidden vectors to recover the masked tokens during pre-training, it has to predict the label of an input token during fine-tuning, by learning from the labels provided in the training data of the downstream task in a supervised way.To achieve so, we feed the hidden vectors to a stack of dense layer, a ReLU activation layer and finally another dense layer for the output classification, with 10% dropout probability.We note that this classifier design is fairly simple, as we expect much knowledge regarding the downstream task can already be extracted from the preceding self-attention layers.Figure2(c) shows the fine-tuning architecture for sequencelevel classification.Being inspired by the Bi-LSTM-Attn model[66], we employ an attention-based weighting average mechanism to convert the sequence of 512 hidden vectors for an input sequence to one single vector before feeding it to the classifier layer, which comprises two dense layers.We note that, unlike the baseline models introduced in Section VI, we do not use RNN layers in our models.17\n\n\n\n\nII: The testing classification accuracy (in %) of different combinations of MIDI token representations and models for our four downstream tasks: melody extraction, velocity prediction, composer classification, and emotion classification.'RNN' denotes the baseline models introduced in Section VI, representing the Bi-LSTM model for the first two (note-level) tasks and the Bi-LSTM-Attn model for the last two (sequencelevel) tasks.'MidiBERT' represents the proposed pre-trained & fine-tuned approach introduced in Section VII.\n\n\nFig. 3 :\n3\nFig. 3: Confusion tables (in %) for two different models for melody extraction, calculated on the test split of POP909 4/4 .Each row represents the percentage of notes in an actual class while each column represents a predicted class.Notation-'M': melody, 'B': bridge, 'A': accompaniment.\n\n\nFig. 4 :\n4\nFig. 4: The melody/non-melody classification result for 'POP909-343.mid'by (b) \"skyline\" [25] and (c) MidiBERT-Piano.\n\n\nFig. 5 :\n5\nFig. 5: Confusion tables (in %) for velocity prediction.\n\n\nFig. 6 :\n6\nFig. 6: Confusion tables (in %) for composer classification on the test split of Pianist8.Each row shows the percentage of sequences of a class predicted as another class.Notation-'C': R. Clayderman (pop), 'Y': Yiruma (pop), 'H': H. Hancock (jazz), 'E': L. Einaudi (contemporary), 'J': H. Joe (contemporary), 'S': R. Sakamoto (contemporary), 'M': Bethel Music (religious), and 'W': Hillsong Worship (religious).\n\n\nFig. 7 :\n7\nFig. 7: Confusion tables for emotion classification; in % of sequences on the test split of EMOPIA.\n\n\nTABLE I :\nI\nDatasets we used in this work; all currently or will-be publicly available.Average note pitch is in MIDI number.\n\n\nTABLE\n\n\nTABLE III :\nIII\nTesting accuracy (in %) of MidiBERT-Piano+CP and its ablated versions without pre-training or with partial freezing for the four downstream classification tasks.\n\n\nTABLE IV :\nIV\nTesting accuracy (in %) of MidiBERT-Piano+CP in the ablation study on the pre-training corpus.'All train splits' denotes the case where we use Pop1K7, ASAP 4/4 , and the training split of the other three datasets.\n\nWe note that the name \"MusicBERT\" has been coincidentally used in an earlier short paper that aims to learn a shared multi-modal representation for text and musical audio (i.e., not symbolic music)[57].\nhttps://github.com/YatingMusic/compound-word-transformer\nhttps://github.com/fosfrancesco/asap-dataset\nhttps://github.com/music-x-lab/POP909-Dataset\nhttps://zenodo.org/record/5089279\nhttps://annahung31.github.io/EMOPIA/\nThis taxonomy is derived from the Russell's valence-arousal model of emotion[61], where valence indicates whether the emotion is positive or negative, and arousal whether it is high (e.g., angry) or low (e.g., sad).\nAs TableIshows, the average length of the pieces in the EMOPIA dataset is the shortest among the five, for they are actually clips manually selected by dedicated annotators[55] to ensure that each clip expresses a single emotion.\nAs we consider only piano MIDIs in this paper, all the music pieces in our datasets are single-track. For multi-track MIDIs, one may in addition differentiate a type of tasks called track-level classification tasks, which require a prediction for each track in a multi-track music sequence.\nThis is different from a closely related task called melody track identification, which aims to identify the track that plays the melody in a multi-track MIDI file[64]. We note that melody extraction is a note-level classification task, whereas melody track identification is a track-level task. While the latter is also an important symbolic music understanding task, we do not consider it here for we exclusively focus on piano-only MIDI data.\nWe have tried to use a three-layer Bi-LSTM instead, to be consistent with the baseline for the note-level tasks, but found the result degraded.\nIn contrast, due to the so-called \"causal masking,\" the self-attention layers in a Transformer decoder such as GPT2[56] are uni-directional.\nWe note that the original BERT paper also used another self-supervised task called \"next sentence prediction\" (NSP)[38] together with MLM for pretraining. We do not use NSP for MidiBERT-Piano since it was later shown to be not that useful[40]-[42]; moreover, NSP requires a clear definition of \"sentences\", which is not well-defined for our MIDI sequences. As a result, we do not use tokens such as CLS, SEP and EOS used by BERT for making the boundary of the sentences.\n16 This has the effect of helping mitigate the mismatch between pre-training and fine-tuning as MASK tokens do not appear at all during fine-tuning.\nThis amounts to 6,144 tokens (or super tokens) per batch, since each sequence has 512 (super) tokens. We set the batch size to 12 for it hits the limit of our GPU memory. Our pilot study shows that a smaller batch size degrades overall performance, including downstream classification accuracy.\nWhen using all the datasets for pre-training, we can reduce the validation cross-entropy loss to 0.666 for REMI and 0.939 for CP, which translate to 80.02% and 72.32% validation \"cloze\" accuracy, respectively.\nACKNOWLEDGEMENTThe authors would like to thank Shih-Lun Wu for sharing his code for pre-processing the POP909 dataset, Wen-Yi Hsiao for sharing his implementation of the skyline algorithm and CP Transformer[36], and Ching-Yu Chiu for sharing the Bi-LSTM code of her downbeat and beat tracker for audio[6], which we adapted for implementing the baseline models.\nToward intelligent music information retrieval. T Li, M Ogihara, IEEE Transactions on Multimedia. 832006\n\nContent-based music information retrieval: Current directions and future challenges. M A Casey, Proceedings of the IEEE. the IEEE200896\n\nMusic information retrieval using social tags and audio. M Levy, M Sandler, IEEE Transactions on Multimedia. 1132009\n\nA survey of audio-based music classification and annotation. Z Fu, G Lu, K M Ting, D Zhang, IEEE Transactions on Multimedia. 1322011\n\nMadmom: A new python audio and music signal processing library. S B\u00f6ck, F Korzeniowski, J Schl\u00fcter, F Krebs, G Widmer, Proc. ACM Multimedia. ACM Multimedia201611741178\n\nDrum-aware ensemble architecture for improved joint musical beat and downbeat tracking. C.-Y Chiu, A W , .-Y Su, Y.-H Yang, IEEE Signal Processing Letters. 2021accepted for publication\n\nOnsets and Frames: Dual-objective piano transcription. C Hawthorne, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2018\n\nHigh-resolution piano transcription with pedals by regressing onsets and offsets times. Q Kong, B Li, X Song, Y Wan, Y Wang, arXiv:2010.018152020arXiv preprint\n\nEnabling factorized piano music modeling and generation with the maestro dataset. C Hawthorne, Proc. Int. Conf. Learning Representations. Int. Conf. Learning Representations2019\n\nMelody extraction from polyphonic music signals: Approaches, applications, and challenges. J Salamon, IEEE Signal Processing Magazine. 3122014\n\nAutomatic chord estimation from audio: A review of the state of the art. M Mcvicar, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2222014\n\nMusic Emotion Recognition. Y.-H Yang, H H Chen, 2011CRC PressBoca Raton, Florida\n\nDeep learning for audio-based music classification and tagging: Teaching computers to distinguish rock from bach. J Nam, IEEE Signal Processing Magazine. 3612019\n\nMusical structural analysis database based on GTTM. M Hamanaka, K Hirata, S Tojo, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2014\n\nA convolutional approach to melody line identification in symbolic scores. F Simonetta, C E C Chac\u00f3n, S Ntalampiras, G Widmer, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2019\n\nThe Jazz Harmony Treebank. D Harasim, C Finkensiep, P Ericson, T J O'donnell, M Rohrmeier, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2020\n\nExtracting ground-truth information from MIDI files: A midifesto. C Raffel, D P W Ellis, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2016\n\nDadaGP: A dataset of tokenized GuitarPro songs for sequence models. P Sarmento, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2021accepted for publication\n\nDeep learning techniques for music generation-a survey. J.-P Briot, G Hadjeres, F Pachet, arXiv:1709.016202017arXiv preprint\n\nProject Magenta: Generating long-term structure in songs and stories. E Waite, Google Brain Blog. 2016\n\nMidiNet: A convolutional generative adversarial network for symbolic-domain music generation. L.-C Yang, S.-Y Chou, Y.-H Yang, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2017\n\nMusic Transformer: Generating music with longterm structure. C.-Z A Huang, Proc. Int. Conf. Learning Representations. Int. Conf. Learning Representations2019\n\nMuseNet. C M Payne, OpenAI Blog. 2019\n\nA comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations, and future directions. S Ji, J Luo, X Yang, arXiv:2011.068012020arXiv preprint\n\nMelody retrieval on the web. W Chai, B Vercoe, Multimedia Computing and Networking. 2001\n\nPre-trained models: Past, present and future. X Han, arXiv:2106.071392021arXiv preprint\n\nTransfer learning for music classification and regression tasks. K Choi, G Fazekas, M Sandler, K Cho, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2017\n\nOne deep music representation to rule them all? A comparative analysis of different representation learning strategies. J Kim, J Urbano, C Liem, A Hanjalic, Neural Computing & Appl. 2019\n\nMulti-task self-supervised pre-training for music classification. H.-H Wu, Proc. Int. Conf. Acoustics, Speech and Signal Processing. Int. Conf. Acoustics, Speech and Signal essing2021\n\nAn attention mechanism for musical instrument recognition. S Gururani, M Sharma, A Lerch, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2019\n\nAttention is all you need. A Vaswani, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing Systems2017\n\nLakhNES: Improving multi-instrumental music generation with cross-domain pre-training. C Donahue, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2019\n\nPop Music Transformer: Beat-based modeling and generation of expressive Pop piano compositions. Y.-S Huang, Y.-H Yang, Proc. ACM Multimedia. ACM Multimedia2020\n\nPopMAG: Pop music accompaniment generation. Y Ren, J He, X Tan, T Qin, Z Zhao, T.-Y Liu, Proc. ACM Multimedia. ACM Multimedia2020\n\nSongMASS: Automatic song writing with pre-training and alignment constraint. Z Sheng, K Song, X Tan, Y Ren, W Ye, S Zhang, T Qin, arXiv:2012.051682020arXiv preprint\n\nCompound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs. W.-Y Hsiao, J.-Y Liu, Y.-C Yeh, Y.-H Yang, Proc. AAAI. AAAI2021\n\nMuseMorphose: Full-song and fine-grained music style transfer with just one Transformer VAE. S.-L Wu, Y.-H Yang, arXiv:2105.040902021arXiv preprint\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proc. Conf. North American Chapter. Conf. North American Chapterthe Association for Computational Linguistics2019\n\nRoBERTa: A robustly optimized BERT pretraining approach. Y Liu, arXiv:1907.116922019arXiv preprint\n\nSpanBERT: Improving pre-training by representing and predicting spans. M Joshi, arXiv:1907.105292019arXiv preprint\n\nXLNet: Generalized autoregressive pretraining for language understanding. Z Yang, arXiv:1906.082372019arXiv preprint\n\nCross-lingual language model pretraining. G Lample, A Conneau, arXiv:1901.072912019arXiv preprint\n\nComposer style classification of piano sheet music images using language model pretraining. T Tsai, K Ji, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2020\n\nMusicBERT: Symbolic music understanding with large-scale pre-training. M Zeng, X Tan, R Wang, Z Ju, T Qin, T.-Y Liu, Proc. Annual Meeting of the Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics2021Findings paper\n\nVirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance. D Jeong, T Kwon, Y Kim, K Lee, J Nam, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2019\n\nGraph neural network for music score data and modeling expressive piano performance. D Jeong, T Kwon, Y Kim, J Nam, Proc. Int. Conf. Machine Learning. Int. Conf. Machine Learning2019\n\nDeep composer classification using symbolic representation. H Y Lee, S Kim, S Park, K Choi, J Lee, Proc. Int. Soc. Music Information Retrieval Conf., Late-breaking Demo paper. Int. Soc. Music Information Retrieval Conf., Late-breaking Demo paper2020\n\nLarge-scale MIDI-based composer classification. Q Kong, K Choi, Y Wang, arXiv:2010.148052020arXiv preprint\n\nDetecting emotions in classical music from MIDI files. J Grekow, Z W Ra\u015b, Proc. Int. Symposium on Methodologies for Intelligent Systems. Int. Symposium on Methodologies for Intelligent Systems2009\n\nExploration of music emotion recognition based on MIDI. Y Lin, X Chen, D Yang, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval Conf2013\n\nMulti-modal music emotion recognition: A new dataset, methodology and comparative analysis. R Panda, Proc. Int. Symposium on Computer Music Multidisciplinary Research. Int. Symposium on Computer Music Multidisciplinary Research2013\n\nMusical texture and expressivity features for music emotion recognition. R Panda, R Malheiro, R P Paiva, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval Conf2018\n\nASAP: a dataset of aligned scores and performances for piano transcription. F Foscarin, A Mcleod, P Rigaux, F Jacquemard, M Sakai, Proc. Int. Soc. Music Inf. Retr. Conf. 2020\n\nPOP909: A Pop-song dataset for music arrangement generation. Z Wang, K Chen, J Jiang, Y Zhang, M Xu, S Dai, X Gu, G Xia, Proc. Int. Society for Music Information Retrieval Conf. Int. Society for Music Information Retrieval Conf2020\n\nEMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation. H.-T Hung, J Ching, S Doh, N Kim, J Nam, Y.-H Yang, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2021accepted for publication\n\nLanguage models are unsupervised multitask learners. A Radford, OpenAI Blog. 2019\n\nMusicBERT: A shared multi-modal representation for music and text. F Rossetto, J Dalton, Proc. 1st Workshop on NLP for Music and Audio. 1st Workshop on NLP for Music and Audio2020\n\nOn large-scale genre classification in symbolically encoded music by automatic identification of repeating patterns. A Ferraro, K Lemstrom, Proc. Int. Conf. Digital Libraries for Musicology. Int. Conf. Digital Libraries for Musicology2018\n\nA survey on symbolic data-based music genre classification. D C Correa, F A Rodrigues, Expert Systems. 6030Oct. 2016\n\nThis time with feeling: Learning expressive musical performance. S Oore, Neural Computing and Applications. 201832\n\nA circumplex model of affect. J Russell, Journal of Personality and Social Psychology. 39December 1980\n\nThe Jazz Transformer on the front line: Exploring the shortcomings of AI-composed music through quantitative measures. S.-L Wu, Y.-H Yang, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2020\n\nAutomatic composition of guitar tabs by Transformers and groove modeling. Y.-H Chen, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2020\n\nMelody identification in standard MIDI files. Z Jiang, R B Dannenberg, Proc. Sound and Music Computing Conf. Sound and Music Computing Conf2019\n\nPypianoroll: Open source Python package for handling multitrack pianoroll. H.-W Dong, W.-Y Hsiao, Y.-H Yang, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2018late-breaking paper\n\nA structured self-attentive sentence embedding. Z Lin, Proc. Int. Conf. Learning Representations. Int. Conf. Learning Representations2017\n\nImprove transformer models with better relative position embeddings. Z Huang, D Liang, P Xu, B Xiang, arXiv:2009.136582020arXiv preprint\n\nSpeechBERT: An audio-and-text jointly learned language model for end-to-end spoken question answering. Y.-S Chuang, C.-L Liu, H.-Y Lee, L.-S Lee, Proc. INTERSPEECH. INTERSPEECH2020\n\nViLBERT: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, arXiv:1908.022652019arXiv preprint\n\nVideoBERT: A joint model for video and language representation learning. C Sun, Proc. IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer Vision2019\n\nProteinBERT: A universal deep-learning model of protein sequence and function. N Brandes, bioRxiv. 2021\n\nHuggingFace's Transformers: State-of-the-art natural language processing. T Wolf, arXiv:1910.037712019arXiv preprint\n\nRethinking attention with Performers. K Choromanski, Proc. Int. Conf. Learning Representations. Int. Conf. Learning Representations2021\n\nRelative positional encoding for Transformers with linear complexity. A Liutkus, O Cifka, S.-L Wu, U Simsekli, Y.-H Yang, G Richard, Proc. Int. Conf. Machine Learning. Int. Conf. Machine Learning2021\n\nRule mining for local boundary detection in melodies. P V Kranenburg, Proc. Int. Soc. Music Information Retrieval Conf. Int. Soc. Music Information Retrieval Conf2020\n\nBeat and downbeat tracking of symbolic music data using deep recurrent neural networks. Y.-C Chuang, L Su, Proc. Asia Pacific Signal and Information Processing Association Annual Summit and Conf. Asia Pacific Signal and Information essing Association Annual Summit and Conf2020\n", "annotations": {"author": "[{\"end\":100,\"start\":88},{\"end\":113,\"start\":101},{\"end\":129,\"start\":114},{\"end\":142,\"start\":130},{\"end\":157,\"start\":143}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":95},{\"end\":112,\"start\":108},{\"end\":128,\"start\":123},{\"end\":141,\"start\":136},{\"end\":156,\"start\":152}]", "author_first_name": "[{\"end\":94,\"start\":88},{\"end\":107,\"start\":101},{\"end\":122,\"start\":114},{\"end\":135,\"start\":130},{\"end\":151,\"start\":143}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":231,\"start\":158}]", "venue": null, "abstract": "[{\"end\":1566,\"start\":458}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1821,\"start\":1818},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1826,\"start\":1823},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2075,\"start\":2072},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2080,\"start\":2077},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2107,\"start\":2104},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2112,\"start\":2109},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2136,\"start\":2132},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2160,\"start\":2156},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2186,\"start\":2182},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2217,\"start\":2213},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2467,\"start\":2463},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2473,\"start\":2469},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3214,\"start\":3210},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3230,\"start\":3226},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3339,\"start\":3335},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3345,\"start\":3341},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3471,\"start\":3467},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3477,\"start\":3473},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3695,\"start\":3691},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3950,\"start\":3946},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3956,\"start\":3952},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4114,\"start\":4110},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4323,\"start\":4319},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4437,\"start\":4433},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4678,\"start\":4674},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4684,\"start\":4680},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4873,\"start\":4869},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4879,\"start\":4875},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5009,\"start\":5005},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5015,\"start\":5011},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5228,\"start\":5224},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5234,\"start\":5230},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5263,\"start\":5259},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5269,\"start\":5265},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5350,\"start\":5346},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5356,\"start\":5352},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5362,\"start\":5358},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5394,\"start\":5390},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5400,\"start\":5396},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6725,\"start\":6721},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6876,\"start\":6872},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6921,\"start\":6917},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7194,\"start\":7190},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7340,\"start\":7336},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8210,\"start\":8206},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8518,\"start\":8514},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9017,\"start\":9013},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9561,\"start\":9557},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9831,\"start\":9828},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9910,\"start\":9907},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10292,\"start\":10288},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10461,\"start\":10458},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10858,\"start\":10854},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11673,\"start\":11670},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12423,\"start\":12419},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":12440,\"start\":12436},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12452,\"start\":12448},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":12458,\"start\":12454},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12485,\"start\":12481},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12491,\"start\":12487},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12947,\"start\":12943},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12963,\"start\":12959},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13489,\"start\":13485},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13558,\"start\":13554},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13583,\"start\":13579},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13695,\"start\":13691},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13791,\"start\":13787},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14086,\"start\":14082},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14351,\"start\":14347},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14707,\"start\":14704},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16058,\"start\":16054},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16282,\"start\":16278},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18044,\"start\":18040},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18254,\"start\":18250},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18610,\"start\":18606},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18733,\"start\":18729},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18765,\"start\":18761},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19301,\"start\":19297},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19307,\"start\":19303},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19439,\"start\":19435},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19561,\"start\":19557},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19565,\"start\":19561},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19569,\"start\":19565},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19573,\"start\":19569},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19577,\"start\":19573},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19581,\"start\":19577},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19585,\"start\":19581},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19589,\"start\":19585},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":19593,\"start\":19589},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19597,\"start\":19593},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19601,\"start\":19597},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":19605,\"start\":19601},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19609,\"start\":19605},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19613,\"start\":19609},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":19617,\"start\":19613},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":19621,\"start\":19617},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19948,\"start\":19944},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":19954,\"start\":19950},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20259,\"start\":20255},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20280,\"start\":20276},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20721,\"start\":20717},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20727,\"start\":20723},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20851,\"start\":20847},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20942,\"start\":20938},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":21000,\"start\":20996},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":21362,\"start\":21358},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21740,\"start\":21737},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21745,\"start\":21742},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21776,\"start\":21773},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22232,\"start\":22228},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":22303,\"start\":22299},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23209,\"start\":23205},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24358,\"start\":24354},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":24627,\"start\":24623},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25263,\"start\":25259},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25269,\"start\":25265},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25275,\"start\":25271},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":25660,\"start\":25656},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":25666,\"start\":25662},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25869,\"start\":25865},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":26725,\"start\":26721},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":27181,\"start\":27177},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30959,\"start\":30955},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36726,\"start\":36722},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":36805,\"start\":36801},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":36811,\"start\":36807},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":36874,\"start\":36870},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36880,\"start\":36876},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37008,\"start\":37004},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37099,\"start\":37095},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":37105,\"start\":37101},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37129,\"start\":37125},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":37153,\"start\":37149},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37912,\"start\":37909},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":38018,\"start\":38014},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":39821,\"start\":39817},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":42473,\"start\":42469},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":42774,\"start\":42770},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":43086,\"start\":43082},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":43598,\"start\":43594},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":44140,\"start\":44136},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":44281,\"start\":44277},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":44404,\"start\":44400},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":44409,\"start\":44405},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":44635,\"start\":44633}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38491,\"start\":37242},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38980,\"start\":38492},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40154,\"start\":38981},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40685,\"start\":40155},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40987,\"start\":40686},{\"attributes\":{\"id\":\"fig_6\"},\"end\":41118,\"start\":40988},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41188,\"start\":41119},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41613,\"start\":41189},{\"attributes\":{\"id\":\"fig_9\"},\"end\":41726,\"start\":41614},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41853,\"start\":41727},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41861,\"start\":41854},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":42041,\"start\":41862},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":42271,\"start\":42042}]", "paragraph": "[{\"end\":2238,\"start\":1585},{\"end\":2666,\"start\":2240},{\"end\":3284,\"start\":2668},{\"end\":3505,\"start\":3286},{\"end\":4438,\"start\":3507},{\"end\":5084,\"start\":4440},{\"end\":5570,\"start\":5086},{\"end\":6251,\"start\":5572},{\"end\":6653,\"start\":6253},{\"end\":7298,\"start\":6674},{\"end\":8422,\"start\":7300},{\"end\":8711,\"start\":8424},{\"end\":9001,\"start\":8729},{\"end\":9534,\"start\":9003},{\"end\":12255,\"start\":9536},{\"end\":12566,\"start\":12257},{\"end\":12591,\"start\":12578},{\"end\":12616,\"start\":12603},{\"end\":12641,\"start\":12628},{\"end\":12666,\"start\":12653},{\"end\":12691,\"start\":12678},{\"end\":12717,\"start\":12703},{\"end\":12742,\"start\":12729},{\"end\":12777,\"start\":12766},{\"end\":12802,\"start\":12792},{\"end\":12827,\"start\":12816},{\"end\":13467,\"start\":12842},{\"end\":13651,\"start\":13469},{\"end\":14224,\"start\":13653},{\"end\":15191,\"start\":14257},{\"end\":16572,\"start\":15222},{\"end\":17087,\"start\":16595},{\"end\":17596,\"start\":17089},{\"end\":17971,\"start\":17622},{\"end\":18420,\"start\":18012},{\"end\":18894,\"start\":18422},{\"end\":19714,\"start\":18937},{\"end\":20168,\"start\":19761},{\"end\":20530,\"start\":20170},{\"end\":21363,\"start\":20576},{\"end\":22129,\"start\":21386},{\"end\":22958,\"start\":22131},{\"end\":23412,\"start\":23001},{\"end\":23520,\"start\":23414},{\"end\":24418,\"start\":23540},{\"end\":25343,\"start\":24420},{\"end\":25586,\"start\":25345},{\"end\":26598,\"start\":25605},{\"end\":27058,\"start\":26600},{\"end\":27984,\"start\":27088},{\"end\":28478,\"start\":27986},{\"end\":29072,\"start\":28480},{\"end\":29438,\"start\":29074},{\"end\":30055,\"start\":29481},{\"end\":30297,\"start\":30057},{\"end\":30469,\"start\":30299},{\"end\":31281,\"start\":30471},{\"end\":31549,\"start\":31283},{\"end\":32229,\"start\":31551},{\"end\":32738,\"start\":32231},{\"end\":33365,\"start\":32754},{\"end\":33508,\"start\":33387},{\"end\":34476,\"start\":33510},{\"end\":35223,\"start\":34478},{\"end\":36078,\"start\":35225},{\"end\":36617,\"start\":36097},{\"end\":37241,\"start\":36619},{\"end\":38490,\"start\":37246},{\"end\":38979,\"start\":38504},{\"end\":40153,\"start\":38995},{\"end\":40684,\"start\":40158},{\"end\":40986,\"start\":40698},{\"end\":41117,\"start\":41000},{\"end\":41187,\"start\":41131},{\"end\":41612,\"start\":41201},{\"end\":41725,\"start\":41626},{\"end\":41852,\"start\":41740},{\"end\":42040,\"start\":41879},{\"end\":42270,\"start\":42057}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12577,\"start\":12567},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12602,\"start\":12592},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12627,\"start\":12617},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12652,\"start\":12642},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12677,\"start\":12667},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12702,\"start\":12692},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12728,\"start\":12718},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12765,\"start\":12743},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12791,\"start\":12778},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12815,\"start\":12803},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12841,\"start\":12828}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8967,\"start\":8966},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10704,\"start\":10703},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28066,\"start\":28065},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30065,\"start\":30063},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31572,\"start\":31570},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32252,\"start\":32250},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32762,\"start\":32760},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33895,\"start\":33892},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34860,\"start\":34857},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":35297,\"start\":35295}]", "section_header": "[{\"end\":1583,\"start\":1568},{\"end\":6672,\"start\":6656},{\"end\":8727,\"start\":8714},{\"end\":14255,\"start\":14227},{\"end\":15220,\"start\":15194},{\"end\":16593,\"start\":16575},{\"end\":17620,\"start\":17599},{\"end\":18010,\"start\":17974},{\"end\":18935,\"start\":18897},{\"end\":19759,\"start\":19717},{\"end\":20574,\"start\":20533},{\"end\":21384,\"start\":21366},{\"end\":22999,\"start\":22961},{\"end\":23538,\"start\":23523},{\"end\":25603,\"start\":25589},{\"end\":27086,\"start\":27061},{\"end\":29479,\"start\":29441},{\"end\":32752,\"start\":32741},{\"end\":33385,\"start\":33368},{\"end\":36095,\"start\":36081},{\"end\":37244,\"start\":37243},{\"end\":38501,\"start\":38493},{\"end\":38992,\"start\":38982},{\"end\":40695,\"start\":40687},{\"end\":40997,\"start\":40989},{\"end\":41128,\"start\":41120},{\"end\":41198,\"start\":41190},{\"end\":41623,\"start\":41615},{\"end\":41737,\"start\":41728},{\"end\":41860,\"start\":41855},{\"end\":41874,\"start\":41863},{\"end\":42053,\"start\":42043}]", "table": null, "figure_caption": "[{\"end\":38491,\"start\":37245},{\"end\":38980,\"start\":38503},{\"end\":40154,\"start\":38994},{\"end\":40685,\"start\":40157},{\"end\":40987,\"start\":40697},{\"end\":41118,\"start\":40999},{\"end\":41188,\"start\":41130},{\"end\":41613,\"start\":41200},{\"end\":41726,\"start\":41625},{\"end\":41853,\"start\":41739},{\"end\":42271,\"start\":42056}]", "figure_ref": "[{\"end\":12848,\"start\":12847},{\"end\":14191,\"start\":14190},{\"end\":15230,\"start\":15229},{\"end\":15876,\"start\":15872},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24417,\"start\":24413},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24428,\"start\":24427},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25946,\"start\":25945},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26608,\"start\":26607},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30490,\"start\":30489},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31291,\"start\":31290},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31995,\"start\":31994},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":32547,\"start\":32546},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33008,\"start\":33007},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33770,\"start\":33767}]", "bib_author_first_name": "[{\"end\":45697,\"start\":45696},{\"end\":45703,\"start\":45702},{\"end\":45840,\"start\":45839},{\"end\":45842,\"start\":45841},{\"end\":45949,\"start\":45948},{\"end\":45957,\"start\":45956},{\"end\":46071,\"start\":46070},{\"end\":46077,\"start\":46076},{\"end\":46083,\"start\":46082},{\"end\":46085,\"start\":46084},{\"end\":46093,\"start\":46092},{\"end\":46208,\"start\":46207},{\"end\":46216,\"start\":46215},{\"end\":46232,\"start\":46231},{\"end\":46244,\"start\":46243},{\"end\":46253,\"start\":46252},{\"end\":46404,\"start\":46400},{\"end\":46412,\"start\":46411},{\"end\":46414,\"start\":46413},{\"end\":46420,\"start\":46417},{\"end\":46429,\"start\":46425},{\"end\":46554,\"start\":46553},{\"end\":46753,\"start\":46752},{\"end\":46761,\"start\":46760},{\"end\":46767,\"start\":46766},{\"end\":46775,\"start\":46774},{\"end\":46782,\"start\":46781},{\"end\":46908,\"start\":46907},{\"end\":47096,\"start\":47095},{\"end\":47222,\"start\":47221},{\"end\":47337,\"start\":47333},{\"end\":47345,\"start\":47344},{\"end\":47347,\"start\":47346},{\"end\":47503,\"start\":47502},{\"end\":47604,\"start\":47603},{\"end\":47616,\"start\":47615},{\"end\":47626,\"start\":47625},{\"end\":47807,\"start\":47806},{\"end\":47820,\"start\":47819},{\"end\":47824,\"start\":47821},{\"end\":47834,\"start\":47833},{\"end\":47849,\"start\":47848},{\"end\":47984,\"start\":47983},{\"end\":47995,\"start\":47994},{\"end\":48009,\"start\":48008},{\"end\":48020,\"start\":48019},{\"end\":48022,\"start\":48021},{\"end\":48035,\"start\":48034},{\"end\":48212,\"start\":48211},{\"end\":48222,\"start\":48221},{\"end\":48226,\"start\":48223},{\"end\":48401,\"start\":48400},{\"end\":48594,\"start\":48590},{\"end\":48603,\"start\":48602},{\"end\":48615,\"start\":48614},{\"end\":48731,\"start\":48730},{\"end\":48862,\"start\":48858},{\"end\":48873,\"start\":48869},{\"end\":48884,\"start\":48880},{\"end\":49054,\"start\":49050},{\"end\":49056,\"start\":49055},{\"end\":49158,\"start\":49157},{\"end\":49160,\"start\":49159},{\"end\":49314,\"start\":49313},{\"end\":49320,\"start\":49319},{\"end\":49327,\"start\":49326},{\"end\":49400,\"start\":49399},{\"end\":49408,\"start\":49407},{\"end\":49507,\"start\":49506},{\"end\":49615,\"start\":49614},{\"end\":49623,\"start\":49622},{\"end\":49634,\"start\":49633},{\"end\":49645,\"start\":49644},{\"end\":49870,\"start\":49869},{\"end\":49877,\"start\":49876},{\"end\":49887,\"start\":49886},{\"end\":49895,\"start\":49894},{\"end\":50007,\"start\":50003},{\"end\":50182,\"start\":50181},{\"end\":50194,\"start\":50193},{\"end\":50204,\"start\":50203},{\"end\":50338,\"start\":50337},{\"end\":50544,\"start\":50543},{\"end\":50752,\"start\":50748},{\"end\":50764,\"start\":50760},{\"end\":50858,\"start\":50857},{\"end\":50865,\"start\":50864},{\"end\":50871,\"start\":50870},{\"end\":50878,\"start\":50877},{\"end\":50885,\"start\":50884},{\"end\":50896,\"start\":50892},{\"end\":51022,\"start\":51021},{\"end\":51031,\"start\":51030},{\"end\":51039,\"start\":51038},{\"end\":51046,\"start\":51045},{\"end\":51053,\"start\":51052},{\"end\":51059,\"start\":51058},{\"end\":51068,\"start\":51067},{\"end\":51212,\"start\":51208},{\"end\":51224,\"start\":51220},{\"end\":51234,\"start\":51230},{\"end\":51244,\"start\":51240},{\"end\":51370,\"start\":51366},{\"end\":51379,\"start\":51375},{\"end\":51505,\"start\":51504},{\"end\":51518,\"start\":51514},{\"end\":51527,\"start\":51526},{\"end\":51534,\"start\":51533},{\"end\":51719,\"start\":51718},{\"end\":51833,\"start\":51832},{\"end\":51952,\"start\":51951},{\"end\":52038,\"start\":52037},{\"end\":52048,\"start\":52047},{\"end\":52187,\"start\":52186},{\"end\":52195,\"start\":52194},{\"end\":52370,\"start\":52369},{\"end\":52378,\"start\":52377},{\"end\":52385,\"start\":52384},{\"end\":52393,\"start\":52392},{\"end\":52399,\"start\":52398},{\"end\":52409,\"start\":52405},{\"end\":52658,\"start\":52657},{\"end\":52667,\"start\":52666},{\"end\":52675,\"start\":52674},{\"end\":52682,\"start\":52681},{\"end\":52689,\"start\":52688},{\"end\":52879,\"start\":52878},{\"end\":52888,\"start\":52887},{\"end\":52896,\"start\":52895},{\"end\":52903,\"start\":52902},{\"end\":53038,\"start\":53037},{\"end\":53040,\"start\":53039},{\"end\":53047,\"start\":53046},{\"end\":53054,\"start\":53053},{\"end\":53062,\"start\":53061},{\"end\":53070,\"start\":53069},{\"end\":53277,\"start\":53276},{\"end\":53285,\"start\":53284},{\"end\":53293,\"start\":53292},{\"end\":53392,\"start\":53391},{\"end\":53402,\"start\":53401},{\"end\":53404,\"start\":53403},{\"end\":53591,\"start\":53590},{\"end\":53598,\"start\":53597},{\"end\":53606,\"start\":53605},{\"end\":53818,\"start\":53817},{\"end\":54032,\"start\":54031},{\"end\":54041,\"start\":54040},{\"end\":54053,\"start\":54052},{\"end\":54055,\"start\":54054},{\"end\":54252,\"start\":54251},{\"end\":54264,\"start\":54263},{\"end\":54274,\"start\":54273},{\"end\":54284,\"start\":54283},{\"end\":54298,\"start\":54297},{\"end\":54413,\"start\":54412},{\"end\":54421,\"start\":54420},{\"end\":54429,\"start\":54428},{\"end\":54438,\"start\":54437},{\"end\":54447,\"start\":54446},{\"end\":54453,\"start\":54452},{\"end\":54460,\"start\":54459},{\"end\":54466,\"start\":54465},{\"end\":54688,\"start\":54684},{\"end\":54696,\"start\":54695},{\"end\":54705,\"start\":54704},{\"end\":54712,\"start\":54711},{\"end\":54719,\"start\":54718},{\"end\":54729,\"start\":54725},{\"end\":54912,\"start\":54911},{\"end\":55009,\"start\":55008},{\"end\":55021,\"start\":55020},{\"end\":55240,\"start\":55239},{\"end\":55251,\"start\":55250},{\"end\":55423,\"start\":55422},{\"end\":55425,\"start\":55424},{\"end\":55435,\"start\":55434},{\"end\":55437,\"start\":55436},{\"end\":55546,\"start\":55545},{\"end\":55627,\"start\":55626},{\"end\":55823,\"start\":55819},{\"end\":55832,\"start\":55828},{\"end\":56015,\"start\":56011},{\"end\":56167,\"start\":56166},{\"end\":56176,\"start\":56175},{\"end\":56178,\"start\":56177},{\"end\":56344,\"start\":56340},{\"end\":56355,\"start\":56351},{\"end\":56367,\"start\":56363},{\"end\":56540,\"start\":56539},{\"end\":56700,\"start\":56699},{\"end\":56709,\"start\":56708},{\"end\":56718,\"start\":56717},{\"end\":56724,\"start\":56723},{\"end\":56875,\"start\":56871},{\"end\":56888,\"start\":56884},{\"end\":56898,\"start\":56894},{\"end\":56908,\"start\":56904},{\"end\":57048,\"start\":57047},{\"end\":57054,\"start\":57053},{\"end\":57063,\"start\":57062},{\"end\":57073,\"start\":57072},{\"end\":57189,\"start\":57188},{\"end\":57393,\"start\":57392},{\"end\":57493,\"start\":57492},{\"end\":57575,\"start\":57574},{\"end\":57744,\"start\":57743},{\"end\":57755,\"start\":57754},{\"end\":57767,\"start\":57763},{\"end\":57773,\"start\":57772},{\"end\":57788,\"start\":57784},{\"end\":57796,\"start\":57795},{\"end\":57929,\"start\":57928},{\"end\":57931,\"start\":57930},{\"end\":58134,\"start\":58130},{\"end\":58144,\"start\":58143}]", "bib_author_last_name": "[{\"end\":45700,\"start\":45698},{\"end\":45711,\"start\":45704},{\"end\":45848,\"start\":45843},{\"end\":45954,\"start\":45950},{\"end\":45965,\"start\":45958},{\"end\":46074,\"start\":46072},{\"end\":46080,\"start\":46078},{\"end\":46090,\"start\":46086},{\"end\":46099,\"start\":46094},{\"end\":46213,\"start\":46209},{\"end\":46229,\"start\":46217},{\"end\":46241,\"start\":46233},{\"end\":46250,\"start\":46245},{\"end\":46260,\"start\":46254},{\"end\":46409,\"start\":46405},{\"end\":46423,\"start\":46421},{\"end\":46434,\"start\":46430},{\"end\":46564,\"start\":46555},{\"end\":46758,\"start\":46754},{\"end\":46764,\"start\":46762},{\"end\":46772,\"start\":46768},{\"end\":46779,\"start\":46776},{\"end\":46787,\"start\":46783},{\"end\":46918,\"start\":46909},{\"end\":47104,\"start\":47097},{\"end\":47230,\"start\":47223},{\"end\":47342,\"start\":47338},{\"end\":47352,\"start\":47348},{\"end\":47507,\"start\":47504},{\"end\":47613,\"start\":47605},{\"end\":47623,\"start\":47617},{\"end\":47631,\"start\":47627},{\"end\":47817,\"start\":47808},{\"end\":47831,\"start\":47825},{\"end\":47846,\"start\":47835},{\"end\":47856,\"start\":47850},{\"end\":47992,\"start\":47985},{\"end\":48006,\"start\":47996},{\"end\":48017,\"start\":48010},{\"end\":48032,\"start\":48023},{\"end\":48045,\"start\":48036},{\"end\":48219,\"start\":48213},{\"end\":48232,\"start\":48227},{\"end\":48410,\"start\":48402},{\"end\":48600,\"start\":48595},{\"end\":48612,\"start\":48604},{\"end\":48622,\"start\":48616},{\"end\":48737,\"start\":48732},{\"end\":48867,\"start\":48863},{\"end\":48878,\"start\":48874},{\"end\":48889,\"start\":48885},{\"end\":49062,\"start\":49057},{\"end\":49166,\"start\":49161},{\"end\":49317,\"start\":49315},{\"end\":49324,\"start\":49321},{\"end\":49332,\"start\":49328},{\"end\":49405,\"start\":49401},{\"end\":49415,\"start\":49409},{\"end\":49511,\"start\":49508},{\"end\":49620,\"start\":49616},{\"end\":49631,\"start\":49624},{\"end\":49642,\"start\":49635},{\"end\":49649,\"start\":49646},{\"end\":49874,\"start\":49871},{\"end\":49884,\"start\":49878},{\"end\":49892,\"start\":49888},{\"end\":49904,\"start\":49896},{\"end\":50010,\"start\":50008},{\"end\":50191,\"start\":50183},{\"end\":50201,\"start\":50195},{\"end\":50210,\"start\":50205},{\"end\":50346,\"start\":50339},{\"end\":50552,\"start\":50545},{\"end\":50758,\"start\":50753},{\"end\":50769,\"start\":50765},{\"end\":50862,\"start\":50859},{\"end\":50868,\"start\":50866},{\"end\":50875,\"start\":50872},{\"end\":50882,\"start\":50879},{\"end\":50890,\"start\":50886},{\"end\":50900,\"start\":50897},{\"end\":51028,\"start\":51023},{\"end\":51036,\"start\":51032},{\"end\":51043,\"start\":51040},{\"end\":51050,\"start\":51047},{\"end\":51056,\"start\":51054},{\"end\":51065,\"start\":51060},{\"end\":51072,\"start\":51069},{\"end\":51218,\"start\":51213},{\"end\":51228,\"start\":51225},{\"end\":51238,\"start\":51235},{\"end\":51249,\"start\":51245},{\"end\":51373,\"start\":51371},{\"end\":51384,\"start\":51380},{\"end\":51512,\"start\":51506},{\"end\":51524,\"start\":51519},{\"end\":51531,\"start\":51528},{\"end\":51544,\"start\":51535},{\"end\":51723,\"start\":51720},{\"end\":51839,\"start\":51834},{\"end\":51957,\"start\":51953},{\"end\":52045,\"start\":52039},{\"end\":52056,\"start\":52049},{\"end\":52192,\"start\":52188},{\"end\":52198,\"start\":52196},{\"end\":52375,\"start\":52371},{\"end\":52382,\"start\":52379},{\"end\":52390,\"start\":52386},{\"end\":52396,\"start\":52394},{\"end\":52403,\"start\":52400},{\"end\":52413,\"start\":52410},{\"end\":52664,\"start\":52659},{\"end\":52672,\"start\":52668},{\"end\":52679,\"start\":52676},{\"end\":52686,\"start\":52683},{\"end\":52693,\"start\":52690},{\"end\":52885,\"start\":52880},{\"end\":52893,\"start\":52889},{\"end\":52900,\"start\":52897},{\"end\":52907,\"start\":52904},{\"end\":53044,\"start\":53041},{\"end\":53051,\"start\":53048},{\"end\":53059,\"start\":53055},{\"end\":53067,\"start\":53063},{\"end\":53074,\"start\":53071},{\"end\":53282,\"start\":53278},{\"end\":53290,\"start\":53286},{\"end\":53298,\"start\":53294},{\"end\":53399,\"start\":53393},{\"end\":53408,\"start\":53405},{\"end\":53595,\"start\":53592},{\"end\":53603,\"start\":53599},{\"end\":53611,\"start\":53607},{\"end\":53824,\"start\":53819},{\"end\":54038,\"start\":54033},{\"end\":54050,\"start\":54042},{\"end\":54061,\"start\":54056},{\"end\":54261,\"start\":54253},{\"end\":54271,\"start\":54265},{\"end\":54281,\"start\":54275},{\"end\":54295,\"start\":54285},{\"end\":54304,\"start\":54299},{\"end\":54418,\"start\":54414},{\"end\":54426,\"start\":54422},{\"end\":54435,\"start\":54430},{\"end\":54444,\"start\":54439},{\"end\":54450,\"start\":54448},{\"end\":54457,\"start\":54454},{\"end\":54463,\"start\":54461},{\"end\":54470,\"start\":54467},{\"end\":54693,\"start\":54689},{\"end\":54702,\"start\":54697},{\"end\":54709,\"start\":54706},{\"end\":54716,\"start\":54713},{\"end\":54723,\"start\":54720},{\"end\":54734,\"start\":54730},{\"end\":54920,\"start\":54913},{\"end\":55018,\"start\":55010},{\"end\":55028,\"start\":55022},{\"end\":55248,\"start\":55241},{\"end\":55260,\"start\":55252},{\"end\":55432,\"start\":55426},{\"end\":55447,\"start\":55438},{\"end\":55551,\"start\":55547},{\"end\":55635,\"start\":55628},{\"end\":55826,\"start\":55824},{\"end\":55837,\"start\":55833},{\"end\":56020,\"start\":56016},{\"end\":56173,\"start\":56168},{\"end\":56189,\"start\":56179},{\"end\":56349,\"start\":56345},{\"end\":56361,\"start\":56356},{\"end\":56372,\"start\":56368},{\"end\":56544,\"start\":56541},{\"end\":56706,\"start\":56701},{\"end\":56715,\"start\":56710},{\"end\":56721,\"start\":56719},{\"end\":56730,\"start\":56725},{\"end\":56882,\"start\":56876},{\"end\":56892,\"start\":56889},{\"end\":56902,\"start\":56899},{\"end\":56912,\"start\":56909},{\"end\":57051,\"start\":57049},{\"end\":57060,\"start\":57055},{\"end\":57070,\"start\":57064},{\"end\":57077,\"start\":57074},{\"end\":57193,\"start\":57190},{\"end\":57401,\"start\":57394},{\"end\":57498,\"start\":57494},{\"end\":57587,\"start\":57576},{\"end\":57752,\"start\":57745},{\"end\":57761,\"start\":57756},{\"end\":57770,\"start\":57768},{\"end\":57782,\"start\":57774},{\"end\":57793,\"start\":57789},{\"end\":57804,\"start\":57797},{\"end\":57942,\"start\":57932},{\"end\":58141,\"start\":58135},{\"end\":58147,\"start\":58145}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":19861553},\"end\":45752,\"start\":45648},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10076932},\"end\":45889,\"start\":45754},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10882722},\"end\":46007,\"start\":45891},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":21829516},\"end\":46141,\"start\":46009},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":259414},\"end\":46310,\"start\":46143},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235406452},\"end\":46496,\"start\":46312},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":37446424},\"end\":46662,\"start\":46498},{\"attributes\":{\"doi\":\"arXiv:2010.01815\",\"id\":\"b7\"},\"end\":46823,\"start\":46664},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53094405},\"end\":47002,\"start\":46825},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8314569},\"end\":47146,\"start\":47004},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14422827},\"end\":47304,\"start\":47148},{\"attributes\":{\"id\":\"b11\"},\"end\":47386,\"start\":47306},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57192487},\"end\":47549,\"start\":47388},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18566955},\"end\":47729,\"start\":47551},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195584099},\"end\":47954,\"start\":47731},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":236096293},\"end\":48143,\"start\":47956},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":18041017},\"end\":48330,\"start\":48145},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":236635259},\"end\":48532,\"start\":48332},{\"attributes\":{\"doi\":\"arXiv:1709.01620\",\"id\":\"b18\"},\"end\":48658,\"start\":48534},{\"attributes\":{\"id\":\"b19\"},\"end\":48762,\"start\":48660},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2002865},\"end\":48987,\"start\":48764},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":54477714},\"end\":49146,\"start\":48989},{\"attributes\":{\"id\":\"b22\"},\"end\":49185,\"start\":49148},{\"attributes\":{\"doi\":\"arXiv:2011.06801\",\"id\":\"b23\"},\"end\":49368,\"start\":49187},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2899315},\"end\":49458,\"start\":49370},{\"attributes\":{\"doi\":\"arXiv:2106.07139\",\"id\":\"b25\"},\"end\":49547,\"start\":49460},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":215825159},\"end\":49747,\"start\":49549},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3639154},\"end\":49935,\"start\":49749},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231839503},\"end\":50120,\"start\":49937},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":195847866},\"end\":50308,\"start\":50122},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13756489},\"end\":50454,\"start\":50310},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195886341},\"end\":50650,\"start\":50456},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220919638},\"end\":50811,\"start\":50652},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221151030},\"end\":50942,\"start\":50813},{\"attributes\":{\"doi\":\"arXiv:2012.05168\",\"id\":\"b34\"},\"end\":51108,\"start\":50944},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":230799404},\"end\":51271,\"start\":51110},{\"attributes\":{\"doi\":\"arXiv:2105.04090\",\"id\":\"b36\"},\"end\":51420,\"start\":51273},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52967399},\"end\":51659,\"start\":51422},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b38\"},\"end\":51759,\"start\":51661},{\"attributes\":{\"doi\":\"arXiv:1907.10529\",\"id\":\"b39\"},\"end\":51875,\"start\":51761},{\"attributes\":{\"doi\":\"arXiv:1906.08237\",\"id\":\"b40\"},\"end\":51993,\"start\":51877},{\"attributes\":{\"doi\":\"arXiv:1901.07291\",\"id\":\"b41\"},\"end\":52092,\"start\":51995},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220845601},\"end\":52296,\"start\":52094},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":235391043},\"end\":52567,\"start\":52298},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":208334424},\"end\":52791,\"start\":52569},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":174800640},\"end\":52975,\"start\":52793},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":222125092},\"end\":53226,\"start\":52977},{\"attributes\":{\"doi\":\"arXiv:2010.14805\",\"id\":\"b47\"},\"end\":53334,\"start\":53228},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":19555571},\"end\":53532,\"start\":53336},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":17981614},\"end\":53723,\"start\":53534},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":163157292},\"end\":53956,\"start\":53725},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":53875359},\"end\":54173,\"start\":53958},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":221725841},\"end\":54349,\"start\":54175},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":221140193},\"end\":54582,\"start\":54351},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":236881050},\"end\":54856,\"start\":54584},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":160025533},\"end\":54939,\"start\":54858},{\"attributes\":{\"id\":\"b56\"},\"end\":55120,\"start\":54941},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":52810867},\"end\":55360,\"start\":55122},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":207600842},\"end\":55478,\"start\":55362},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":51980304},\"end\":55594,\"start\":55480},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":145278842},\"end\":55698,\"start\":55596},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":220961652},\"end\":55935,\"start\":55700},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":221042304},\"end\":56118,\"start\":55937},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":204766730},\"end\":56263,\"start\":56120},{\"attributes\":{\"id\":\"b64\"},\"end\":56489,\"start\":56265},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":15280949},\"end\":56628,\"start\":56491},{\"attributes\":{\"doi\":\"arXiv:2009.13658\",\"id\":\"b66\"},\"end\":56766,\"start\":56630},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":219473826},\"end\":56948,\"start\":56768},{\"attributes\":{\"doi\":\"arXiv:1908.02265\",\"id\":\"b68\"},\"end\":57113,\"start\":56950},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":102483628},\"end\":57311,\"start\":57115},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":235219208},\"end\":57416,\"start\":57313},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b71\"},\"end\":57534,\"start\":57418},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":222067132},\"end\":57671,\"start\":57536},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":234762885},\"end\":57872,\"start\":57673},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":236096897},\"end\":58040,\"start\":57874},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":231851428},\"end\":58319,\"start\":58042}]", "bib_title": "[{\"end\":45694,\"start\":45648},{\"end\":45837,\"start\":45754},{\"end\":45946,\"start\":45891},{\"end\":46068,\"start\":46009},{\"end\":46205,\"start\":46143},{\"end\":46398,\"start\":46312},{\"end\":46551,\"start\":46498},{\"end\":46905,\"start\":46825},{\"end\":47093,\"start\":47004},{\"end\":47219,\"start\":47148},{\"end\":47500,\"start\":47388},{\"end\":47601,\"start\":47551},{\"end\":47804,\"start\":47731},{\"end\":47981,\"start\":47956},{\"end\":48209,\"start\":48145},{\"end\":48398,\"start\":48332},{\"end\":48728,\"start\":48660},{\"end\":48856,\"start\":48764},{\"end\":49048,\"start\":48989},{\"end\":49155,\"start\":49148},{\"end\":49397,\"start\":49370},{\"end\":49612,\"start\":49549},{\"end\":49867,\"start\":49749},{\"end\":50001,\"start\":49937},{\"end\":50179,\"start\":50122},{\"end\":50335,\"start\":50310},{\"end\":50541,\"start\":50456},{\"end\":50746,\"start\":50652},{\"end\":50855,\"start\":50813},{\"end\":51206,\"start\":51110},{\"end\":51502,\"start\":51422},{\"end\":52184,\"start\":52094},{\"end\":52367,\"start\":52298},{\"end\":52655,\"start\":52569},{\"end\":52876,\"start\":52793},{\"end\":53035,\"start\":52977},{\"end\":53389,\"start\":53336},{\"end\":53588,\"start\":53534},{\"end\":53815,\"start\":53725},{\"end\":54029,\"start\":53958},{\"end\":54249,\"start\":54175},{\"end\":54410,\"start\":54351},{\"end\":54682,\"start\":54584},{\"end\":54909,\"start\":54858},{\"end\":55006,\"start\":54941},{\"end\":55237,\"start\":55122},{\"end\":55420,\"start\":55362},{\"end\":55543,\"start\":55480},{\"end\":55624,\"start\":55596},{\"end\":55817,\"start\":55700},{\"end\":56009,\"start\":55937},{\"end\":56164,\"start\":56120},{\"end\":56338,\"start\":56265},{\"end\":56537,\"start\":56491},{\"end\":56869,\"start\":56768},{\"end\":57186,\"start\":57115},{\"end\":57390,\"start\":57313},{\"end\":57572,\"start\":57536},{\"end\":57741,\"start\":57673},{\"end\":57926,\"start\":57874},{\"end\":58128,\"start\":58042}]", "bib_author": "[{\"end\":45702,\"start\":45696},{\"end\":45713,\"start\":45702},{\"end\":45850,\"start\":45839},{\"end\":45956,\"start\":45948},{\"end\":45967,\"start\":45956},{\"end\":46076,\"start\":46070},{\"end\":46082,\"start\":46076},{\"end\":46092,\"start\":46082},{\"end\":46101,\"start\":46092},{\"end\":46215,\"start\":46207},{\"end\":46231,\"start\":46215},{\"end\":46243,\"start\":46231},{\"end\":46252,\"start\":46243},{\"end\":46262,\"start\":46252},{\"end\":46411,\"start\":46400},{\"end\":46417,\"start\":46411},{\"end\":46425,\"start\":46417},{\"end\":46436,\"start\":46425},{\"end\":46566,\"start\":46553},{\"end\":46760,\"start\":46752},{\"end\":46766,\"start\":46760},{\"end\":46774,\"start\":46766},{\"end\":46781,\"start\":46774},{\"end\":46789,\"start\":46781},{\"end\":46920,\"start\":46907},{\"end\":47106,\"start\":47095},{\"end\":47232,\"start\":47221},{\"end\":47344,\"start\":47333},{\"end\":47354,\"start\":47344},{\"end\":47509,\"start\":47502},{\"end\":47615,\"start\":47603},{\"end\":47625,\"start\":47615},{\"end\":47633,\"start\":47625},{\"end\":47819,\"start\":47806},{\"end\":47833,\"start\":47819},{\"end\":47848,\"start\":47833},{\"end\":47858,\"start\":47848},{\"end\":47994,\"start\":47983},{\"end\":48008,\"start\":47994},{\"end\":48019,\"start\":48008},{\"end\":48034,\"start\":48019},{\"end\":48047,\"start\":48034},{\"end\":48221,\"start\":48211},{\"end\":48234,\"start\":48221},{\"end\":48412,\"start\":48400},{\"end\":48602,\"start\":48590},{\"end\":48614,\"start\":48602},{\"end\":48624,\"start\":48614},{\"end\":48739,\"start\":48730},{\"end\":48869,\"start\":48858},{\"end\":48880,\"start\":48869},{\"end\":48891,\"start\":48880},{\"end\":49064,\"start\":49050},{\"end\":49168,\"start\":49157},{\"end\":49319,\"start\":49313},{\"end\":49326,\"start\":49319},{\"end\":49334,\"start\":49326},{\"end\":49407,\"start\":49399},{\"end\":49417,\"start\":49407},{\"end\":49513,\"start\":49506},{\"end\":49622,\"start\":49614},{\"end\":49633,\"start\":49622},{\"end\":49644,\"start\":49633},{\"end\":49651,\"start\":49644},{\"end\":49876,\"start\":49869},{\"end\":49886,\"start\":49876},{\"end\":49894,\"start\":49886},{\"end\":49906,\"start\":49894},{\"end\":50012,\"start\":50003},{\"end\":50193,\"start\":50181},{\"end\":50203,\"start\":50193},{\"end\":50212,\"start\":50203},{\"end\":50348,\"start\":50337},{\"end\":50554,\"start\":50543},{\"end\":50760,\"start\":50748},{\"end\":50771,\"start\":50760},{\"end\":50864,\"start\":50857},{\"end\":50870,\"start\":50864},{\"end\":50877,\"start\":50870},{\"end\":50884,\"start\":50877},{\"end\":50892,\"start\":50884},{\"end\":50902,\"start\":50892},{\"end\":51030,\"start\":51021},{\"end\":51038,\"start\":51030},{\"end\":51045,\"start\":51038},{\"end\":51052,\"start\":51045},{\"end\":51058,\"start\":51052},{\"end\":51067,\"start\":51058},{\"end\":51074,\"start\":51067},{\"end\":51220,\"start\":51208},{\"end\":51230,\"start\":51220},{\"end\":51240,\"start\":51230},{\"end\":51251,\"start\":51240},{\"end\":51375,\"start\":51366},{\"end\":51386,\"start\":51375},{\"end\":51514,\"start\":51504},{\"end\":51526,\"start\":51514},{\"end\":51533,\"start\":51526},{\"end\":51546,\"start\":51533},{\"end\":51725,\"start\":51718},{\"end\":51841,\"start\":51832},{\"end\":51959,\"start\":51951},{\"end\":52047,\"start\":52037},{\"end\":52058,\"start\":52047},{\"end\":52194,\"start\":52186},{\"end\":52200,\"start\":52194},{\"end\":52377,\"start\":52369},{\"end\":52384,\"start\":52377},{\"end\":52392,\"start\":52384},{\"end\":52398,\"start\":52392},{\"end\":52405,\"start\":52398},{\"end\":52415,\"start\":52405},{\"end\":52666,\"start\":52657},{\"end\":52674,\"start\":52666},{\"end\":52681,\"start\":52674},{\"end\":52688,\"start\":52681},{\"end\":52695,\"start\":52688},{\"end\":52887,\"start\":52878},{\"end\":52895,\"start\":52887},{\"end\":52902,\"start\":52895},{\"end\":52909,\"start\":52902},{\"end\":53046,\"start\":53037},{\"end\":53053,\"start\":53046},{\"end\":53061,\"start\":53053},{\"end\":53069,\"start\":53061},{\"end\":53076,\"start\":53069},{\"end\":53284,\"start\":53276},{\"end\":53292,\"start\":53284},{\"end\":53300,\"start\":53292},{\"end\":53401,\"start\":53391},{\"end\":53410,\"start\":53401},{\"end\":53597,\"start\":53590},{\"end\":53605,\"start\":53597},{\"end\":53613,\"start\":53605},{\"end\":53826,\"start\":53817},{\"end\":54040,\"start\":54031},{\"end\":54052,\"start\":54040},{\"end\":54063,\"start\":54052},{\"end\":54263,\"start\":54251},{\"end\":54273,\"start\":54263},{\"end\":54283,\"start\":54273},{\"end\":54297,\"start\":54283},{\"end\":54306,\"start\":54297},{\"end\":54420,\"start\":54412},{\"end\":54428,\"start\":54420},{\"end\":54437,\"start\":54428},{\"end\":54446,\"start\":54437},{\"end\":54452,\"start\":54446},{\"end\":54459,\"start\":54452},{\"end\":54465,\"start\":54459},{\"end\":54472,\"start\":54465},{\"end\":54695,\"start\":54684},{\"end\":54704,\"start\":54695},{\"end\":54711,\"start\":54704},{\"end\":54718,\"start\":54711},{\"end\":54725,\"start\":54718},{\"end\":54736,\"start\":54725},{\"end\":54922,\"start\":54911},{\"end\":55020,\"start\":55008},{\"end\":55030,\"start\":55020},{\"end\":55250,\"start\":55239},{\"end\":55262,\"start\":55250},{\"end\":55434,\"start\":55422},{\"end\":55449,\"start\":55434},{\"end\":55553,\"start\":55545},{\"end\":55637,\"start\":55626},{\"end\":55828,\"start\":55819},{\"end\":55839,\"start\":55828},{\"end\":56022,\"start\":56011},{\"end\":56175,\"start\":56166},{\"end\":56191,\"start\":56175},{\"end\":56351,\"start\":56340},{\"end\":56363,\"start\":56351},{\"end\":56374,\"start\":56363},{\"end\":56546,\"start\":56539},{\"end\":56708,\"start\":56699},{\"end\":56717,\"start\":56708},{\"end\":56723,\"start\":56717},{\"end\":56732,\"start\":56723},{\"end\":56884,\"start\":56871},{\"end\":56894,\"start\":56884},{\"end\":56904,\"start\":56894},{\"end\":56914,\"start\":56904},{\"end\":57053,\"start\":57047},{\"end\":57062,\"start\":57053},{\"end\":57072,\"start\":57062},{\"end\":57079,\"start\":57072},{\"end\":57195,\"start\":57188},{\"end\":57403,\"start\":57392},{\"end\":57500,\"start\":57492},{\"end\":57589,\"start\":57574},{\"end\":57754,\"start\":57743},{\"end\":57763,\"start\":57754},{\"end\":57772,\"start\":57763},{\"end\":57784,\"start\":57772},{\"end\":57795,\"start\":57784},{\"end\":57806,\"start\":57795},{\"end\":57944,\"start\":57928},{\"end\":58143,\"start\":58130},{\"end\":58149,\"start\":58143}]", "bib_venue": "[{\"end\":45744,\"start\":45713},{\"end\":45873,\"start\":45850},{\"end\":45998,\"start\":45967},{\"end\":46132,\"start\":46101},{\"end\":46282,\"start\":46262},{\"end\":46466,\"start\":46436},{\"end\":46614,\"start\":46566},{\"end\":46750,\"start\":46664},{\"end\":46961,\"start\":46920},{\"end\":47137,\"start\":47106},{\"end\":47295,\"start\":47232},{\"end\":47331,\"start\":47306},{\"end\":47540,\"start\":47509},{\"end\":47681,\"start\":47633},{\"end\":47906,\"start\":47858},{\"end\":48095,\"start\":48047},{\"end\":48282,\"start\":48234},{\"end\":48460,\"start\":48412},{\"end\":48588,\"start\":48534},{\"end\":48756,\"start\":48739},{\"end\":48939,\"start\":48891},{\"end\":49105,\"start\":49064},{\"end\":49179,\"start\":49168},{\"end\":49311,\"start\":49187},{\"end\":49452,\"start\":49417},{\"end\":49504,\"start\":49460},{\"end\":49699,\"start\":49651},{\"end\":49929,\"start\":49906},{\"end\":50068,\"start\":50012},{\"end\":50260,\"start\":50212},{\"end\":50403,\"start\":50348},{\"end\":50602,\"start\":50554},{\"end\":50791,\"start\":50771},{\"end\":50922,\"start\":50902},{\"end\":51019,\"start\":50944},{\"end\":51261,\"start\":51251},{\"end\":51364,\"start\":51273},{\"end\":51580,\"start\":51546},{\"end\":51716,\"start\":51661},{\"end\":51830,\"start\":51761},{\"end\":51949,\"start\":51877},{\"end\":52035,\"start\":51995},{\"end\":52248,\"start\":52200},{\"end\":52484,\"start\":52415},{\"end\":52743,\"start\":52695},{\"end\":52942,\"start\":52909},{\"end\":53151,\"start\":53076},{\"end\":53274,\"start\":53228},{\"end\":53471,\"start\":53410},{\"end\":53668,\"start\":53613},{\"end\":53891,\"start\":53826},{\"end\":54118,\"start\":54063},{\"end\":54343,\"start\":54306},{\"end\":54527,\"start\":54472},{\"end\":54784,\"start\":54736},{\"end\":54933,\"start\":54922},{\"end\":55075,\"start\":55030},{\"end\":55311,\"start\":55262},{\"end\":55463,\"start\":55449},{\"end\":55586,\"start\":55553},{\"end\":55681,\"start\":55637},{\"end\":55887,\"start\":55839},{\"end\":56070,\"start\":56022},{\"end\":56227,\"start\":56191},{\"end\":56422,\"start\":56374},{\"end\":56587,\"start\":56546},{\"end\":56697,\"start\":56630},{\"end\":56931,\"start\":56914},{\"end\":57045,\"start\":56950},{\"end\":57253,\"start\":57195},{\"end\":57410,\"start\":57403},{\"end\":57490,\"start\":57418},{\"end\":57630,\"start\":57589},{\"end\":57839,\"start\":57806},{\"end\":57992,\"start\":57944},{\"end\":58236,\"start\":58149},{\"end\":45883,\"start\":45875},{\"end\":46298,\"start\":46284},{\"end\":46658,\"start\":46616},{\"end\":46998,\"start\":46963},{\"end\":47725,\"start\":47683},{\"end\":47950,\"start\":47908},{\"end\":48139,\"start\":48097},{\"end\":48326,\"start\":48284},{\"end\":48504,\"start\":48462},{\"end\":48983,\"start\":48941},{\"end\":49142,\"start\":49107},{\"end\":49743,\"start\":49701},{\"end\":50116,\"start\":50070},{\"end\":50304,\"start\":50262},{\"end\":50450,\"start\":50405},{\"end\":50646,\"start\":50604},{\"end\":50807,\"start\":50793},{\"end\":50938,\"start\":50924},{\"end\":51267,\"start\":51263},{\"end\":51610,\"start\":51582},{\"end\":52292,\"start\":52250},{\"end\":52549,\"start\":52486},{\"end\":52787,\"start\":52745},{\"end\":52971,\"start\":52944},{\"end\":53222,\"start\":53153},{\"end\":53528,\"start\":53473},{\"end\":53719,\"start\":53670},{\"end\":53952,\"start\":53893},{\"end\":54169,\"start\":54120},{\"end\":54578,\"start\":54529},{\"end\":54828,\"start\":54786},{\"end\":55116,\"start\":55077},{\"end\":55356,\"start\":55313},{\"end\":55931,\"start\":55889},{\"end\":56114,\"start\":56072},{\"end\":56259,\"start\":56229},{\"end\":56466,\"start\":56424},{\"end\":56624,\"start\":56589},{\"end\":56944,\"start\":56933},{\"end\":57307,\"start\":57255},{\"end\":57667,\"start\":57632},{\"end\":57868,\"start\":57841},{\"end\":58036,\"start\":57994},{\"end\":58315,\"start\":58238}]"}}}, "year": 2023, "month": 12, "day": 17}