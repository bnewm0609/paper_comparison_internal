{"id": 54216961, "updated": "2023-10-01 14:25:41.879", "metadata": {"title": "ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation", "authors": "[{\"first\":\"Tuan-Hung\",\"last\":\"Vu\",\"middle\":[]},{\"first\":\"Himalaya\",\"last\":\"Jain\",\"middle\":[]},{\"first\":\"Maxime\",\"last\":\"Bucher\",\"middle\":[]},{\"first\":\"Mathieu\",\"last\":\"Cord\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"P'erez\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 11, "day": 30}, "abstract": "Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging\"synthetic-2-real\"set-ups and show that the approach can also be used for detection.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1811.12833", "mag": "2963073217", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/VuJBCP19", "doi": "10.1109/cvpr.2019.00262"}}, "content": {"source": {"pdf_hash": "d737169d73f36e250c971860375cfaad5322ff35", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1811.12833v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1811.12833", "status": "GREEN"}}, "grobid": {"id": "c09cee7a1dc615729246ff216cc7b29d96c34313", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d737169d73f36e250c971860375cfaad5322ff35.txt", "contents": "\nADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation\n\n\nTuan-Hung Vu \nHimalaya Jain \nMaxime Bucher \nMatthieu Cord \nSorbonne University\nParisFrance\n\nPatrick P\u00e9rez \n\n1 valeo.aiParisFrance\n\nADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation\n\nSemantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging \"synthetic-2-real\" set-ups and show that the approach can also be used for detection.\n\nIntroduction\n\nSemantic segmentation is the task of assigning class labels to all pixels in an image. In practice, segmentation models often serve as the backbone in complex computer vision systems like autonomous vehicles, which demand high prediction accuracy in a large variety of urban environments. For example, under adverse weathers, the system must be able to recognize roads, lanes, sideways or pedestrians despite their appearances being largely different from ones in the training set. A more extreme and important example is so-called \"synthetic-2-real\" set-up [31,30] training samples are synthesized by game engines and testing samples are real scenes. Current fully-supervised approaches [23,47,2] have not yet guaranteed a good generalization to arbitrary testing cases. Thus a model trained on one domain, named as source, usually undergoes a drastic drop in performance when applied on another domain, named as target.\n\nUnsupervised domain adaptation (UDA) is the field of research aiming at learning only from source supervision a well performing model on target samples. Among the recent methods for UDA, many address the problem by reducing cross-domain discrepancy, along with the supervised training on the source domain. They approach UDA by minimizing the difference between the distribution of the intermediate features or of the final outputs for source and target data respectively. It is done at single [15,32,44] or multiple levels [24,25] using maximum mean discrepancies (MMD) or adversarial training [10,42]. Other approaches include self-training [51] to provide pseudo labels or generative networks to produce target data [14,34,43].\n\nSemi-supervised learning addresses a closely related problem of learning from the data of which only a subset is annotated. Thus, it inspires several approaches for UDA, for example, self-training, generative model or class balancing [49]. Entropy minimization is also one of the successful approaches used for semi-supervised learning [38].\n\nIn this work, we adapt the principle of entropy minimization to the UDA task in semantic segmentation. We start from a simple observation: models trained only on source domain tend to produce over-confident, i.e. low-entropy, predictions on source-like images and under-confident, i.e. high-entropy, predictions on target-like ones. Such a phenomenon is illustrated in Figure 1. Prediction entropy maps of scenes from the source domain look like edge detection results with high entropy activations only along object borders. On the other hand, predictions on target images are less certain, resulting in very noisy, high entropy outputs. We argue that one possible way to bridge the domain gap between source and target is by enforcing high prediction certainty (low-entropy) on target predictions as well. To this end, we propose two approaches: direct entropy minimization using an entropy loss and indirect entropy minimization using an adversarial loss. While the first approach imposes the low-entropy constraint on independent pixel-wise predictions, the latter aims at globally matching source and target distributions in terms of weighted self-information. 1 We summarize our contributions as follows:\n\n\u2022 For semantic segmentation UDA, we propose to leverage an entropy loss to directly penalize low-confident predictions on target domain. The use of this entropy loss adds no significant overhead to existing semantic segmentation frameworks. \u2022 We introduce a novel entropy-based adversarial training approach targeting not only the entropy minimization objective but also the structure adaptation from source domain to target domain. \u2022 To improve further the performance in specific settings, we suggest two additional practices: (i) training on specific entropy ranges and (ii) incorporating class-ratio priors. We discuss practical insights in the experiments and ablation studies.\n\nThe entropy minimization objectives push the model's decision boundaries toward low-density regions of the target domain distribution in prediction space. This results in \"cleaner\" semantic segmentation outputs, with more refined object edges as well as large ambiguous image regions being correctly recovered, as shown in Figure 1. The proposed models outperform state-of-the-art approaches on several UDA benchmarks for semantic segmentation, in particular the two main synthetic-2-real benchmarks, GTA5\u2192Cityscapes and SYNTHIA\u2192Cityscapes. 1 Connection to the entropy is discussed in Section 3.\n\n\nRelated works\n\nUnsupervised Domain Adaptation is a well researched topic for the task of classification and detection, with recent advances in semantic segmentation also. A very appealing application of domain adaptation is on using synthetic data for real world tasks. This has encouraged the development of several synthetic scene projects with associated datasets, such as Carla [8], SYNTHIA [31], and others [35,30].\n\nMain approaches for UDA include discrepancy minimization between source and target feature distributions [10,24,15,25,42], self-training with pseudo-labels [51] and generative approaches [14,34,43]. In this work, we are particularly interested in UDA for the task of semantic segmentation. Therefore, we only review the UDA approaches for semantic segmentation here (see [7] for a more general literature review).\n\nAdversarial training for UDA is the most explored approach for semantic segmentation. It involves two networks. One network predicts the segmentation maps for the input image, which could be from source or target domain, while another network acts as a discriminator which takes the feature maps from the segmentation network and tries to predict domain of the input. The segmentation network tries to fool the discriminator, thus making the features from the two domains have a similar distribution. Hoffman et al. [15] are the first to apply the adversarial approach for UDA on semantic segmentation. They also have a category specific adaptation by transferring the label statistics from the source domain. A similar approach of global and class-wise alignment is used in [5] with the class-wise alignment being done using adversarial training on grid-wise soft pseudolabels. In [4], adversarial training is used for spatial-aware adaptation along with a distillation loss to specifically address synthetic-2-real domain shift. [16] uses a residual net to make the source feature maps similar to target's ones using adversarial training, the feature maps being then used for segmentation task. In [41], the adversarial approach is used on the output space to benefit from the structural consistency across domain. [32,33] propose another interesting way of using adversarial training: They get two predictions on the target domain image, this is done either by two classifiers [33] or using dropout in the classifier [32]. Given the two predictions the classifier is trained to maximize the discrepancy between the distributions while the feature extractor part of the network is trained to minimize it.\n\nSome methods build on generative networks to generate target images conditioned on the source. Hoffman et al. [14] propose Cycle-Consistent Adversarial Domain Adaptation (CyCADA), in which they adapt at both pixel-level and feature-level representation. For pixel-level adaptation they use Cycle-GAN [48] to generate target images conditioned on the source images. In [34], a generative model is learned to reconstruct images from the feature space. Then, for domain adaptation, the feature module is trained to produce target images on source features and vice-versa using the generator module. In DCAN [43], channel-wise feature alignment is used in the generator and segmentation network. The segmentation network is learned on generated images with the content of the source and style of the target for which source segmentation map serves as the groundtruth. Authors in [50] use generative adversarial networks (GAN) [11] to align the source and target embeddings. Also, they replace the cross-entropy loss by a conservative loss (CL) that penalizes the easy and hard cases of source examples. The CL approach is orthogonal to most of the UDA methods, including ours: it could benefit any method that uses cross-entropy for source. Another approach for UDA is self-training. The idea is to use the prediction from an ensembled model or a previous state of model as pseudo-labels for the unlabeled data to train the current model. Many semi-supervised methods [20,39] use self-training. In [51], self-training is employed for UDA on semantic segmentation which is further extended with class balancing and spatial prior. Self-training has an interesting connection to the proposed entropy minimization approach as we discuss in Section 3.1.\n\nAmong some other approaches, [26] uses a combination of adversarial and generative techniques through multiple losses, [46] combines the generative approach for appearance adaptation and adversarial training for representation adaptation, and [45] propose a curriculum-style learning for UDA by enforcing the consistency on local (superpixellevel) and global label distributions.\n\nEntropy minimization has been shown to be useful for semi-supervised learning [12,38] and clustering [17,18]. It has also been recently applied on domain adaptation for classification task [25]. To our knowledge, we are first to successfully apply entropy based UDA training to obtain competitive performance on semantic segmentation task.\n\n\nApproaches\n\nIn this section, we present our two proposed approaches for entropy minimization using (i) unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. Figure 2 illustrates our architectures.\n\nOur models are trained with a supervised loss on source domain. Formally, we consider a set X s \u2282 R H\u00d7W \u00d73 of sources examples along with associated ground-truth Cclass segmentation maps, Y s \u2282 (1, C) H\u00d7W . Sample x s is a H \u00d7 W color image and entry y\n(h,w) s = y (h,w,c) s c\nof associated map y s provides the label of pixel (h, w) as one-hot vector. Let F be a semantic segmentation network which takes an image x and predicts a C-dimensional \"soft-\nsegmentation map\" F (x) = P x = P (h,w,c) x\nh,w,c . By virtue of final softmax layer, each C-dimensional pixel-wise vector P (h,w,c) x c behaves as a discrete distribution over classes. If one class stands out, the distribution is picky (low entropy), if scores are evenly spread, sign of uncertainty from the network standpoint, the entropy is large. The parameters \u03b8 F of F are learned to minimize the segmentation loss\nL seg (x s , y s ) = \u2212 H h=1 W w=1 C c=1 y (h,w,c) s log P (h,w,c) xs (1)\non source samples. In the case of training only on source domain without domain adaptation, the optimization problem simply reads:\nmin \u03b8 F 1 |X s | xs\u2208Xs L seg (x s , y s ).\n(2)\n\n\nDirect entropy minimization\n\nFor the target domain, as we do not have the annotations y t for image samples x t \u2208 X t , we cannot use (2) to learn F . Some methods use the model's prediction\u0177 t as a proxy for y t . Also, this proxy is used only for pixels where prediction is sufficiently confident. Instead of using the highconfident proxy, we propose to constrain the model such that it produces high-confident prediction. We realize this by minimizing the entropy of the prediction.\n\nWe introduce the entropy loss L ent to directly maximize prediction certainty in the target domain. In this work, we use the standard Shannon Entropy [36]. Given a target input image x t , the entropy map E xt \u2208 [0, 1] H\u00d7W is composed of the independent pixel-wise entropies normalized to [0, 1] range:\nE (h,w) xt = \u22121 log(C) C c=1 P (h,w,c) xt log P (h,w,c) xt ,(3)\nat pixel (h, w). An example of entropy map is shown in Figure 2. The entropy loss L ent is defined as the sum of all pixel-wise normalized entropies:\nL ent (x t ) = h,w E (h,w) xt .(4)\nDuring training, we jointly optimize the supervised segmentation loss L seg on source samples and the unsupervised entropy loss L ent on target samples. The final optimization problem is formulated as follows:\nmin \u03b8 F 1 |X s | xs L seg (x s , y s ) + \u03bb ent |X t | xt L ent (x t ),(5)\nwith \u03bb ent as the weighting factor of the entropy term L ent . Connection to self-training. Pseudo-labeling is a simple yet efficient approach for semi-supervised learning [21]. Recently, the approach has been applied to UDA in semantic segmentation task with an iterative self-training (ST) procedure [51]. The ST method assumes that the set K \u2282 (1, H) \u00d7 (1, W ) of high-scoring pixel-wise predictions on target samples are correct with high probability. Such an assumption allows the use of cross-entropy loss with pseudolabels on target predictions. In practice, K is constructed by selecting high-scoring pixels with a fixed or scheduled threshold. To draw a link with entropy minimization, we write the training objective of the ST approach as:\nmin \u03b8 F 1 |X s | xs L seg (x s , y s ) + \u03bb pl |X t | xt L seg (x t ,\u0177 t ),(6)\nwith:\nL seg (x t ,\u0177 t ) = \u2212 (h,w)\u2208K C c=1\u0177 (h,w,c) t log P (h,w,c) xt .(7)\nComparing equations (3-4) and (7), we note that our entropy loss L ent (x t ) can be seen as a soft-assignment version of the pseudo-label cross-entropy loss L seg (x t ,\u0177 t ). Different to ST [51], our entropy-based approach does not require a complex scheduling procedure for choosing threshold. Even contrary to ST assumption, we show in Section 4.3 that, in some cases, training on the \"hard\" or \"mostconfused\" pixels produces better performance.\n\n\nEntropy minimization with adversarial learning\n\nThe entropy loss for an input image is defined in equation (4) as the sum of independent pixel-wise prediction en-tropies. Therefore, a mere minimization of this loss neglects the structural dependencies between local semantics. As shown in [41], for UDA in semantic segmentation, adaptation on structured output space is beneficial. It is based on the fact that source and target domain share strong similarities in semantic layout.\n\nIn this part, we introduce a unified adversarial training framework which indirectly minimizes the entropy by having target's entropy distribution similar to the source. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the weighted self-information space. Figure 2 illustrates our adversarial learning procedure. Our adversarial approach is motivated by the fact that the trained model naturally produces low-entropy predictions on source-like images. By aligning weighted self-information distributions of target and source domains, we indirectly minimize the entropy of target predictions. Moreover, as the adaptation is done on the weighted self-information space, our model leverages structural information from source to target.\n\nIn detail, given a pixel-wise predicted class score P\n(h,w,c) x , the self-information or \"surprisal\" [40] is de- fined as \u2212 log P (h,w,c) x . Effectively, the entropy E (h,w) x in (3) is the expected value of the self-information E c [\u2212 log P (h,w,c) x\n]. We here perform adversarial adaptation on weighted self-information maps I x composed of pixellevel vectors I 2 Such vectors can be seen as the disentanglement of the Shannon Entropy.\n(h,w) x = \u2212P (h,w) x \u00b7 log P (h,w) x .\nWe then construct a fully-convolutional discriminator network D with parameters \u03b8 D taking I x as input and that produces domain classification outputs, i.e. two class labels 1 and 0 corresponding to the source and target domains. Similar to [11], we train the discriminator to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator. In detail, let L D the cross-entropy domain classification loss. The training objective of the discriminator is:\nmin \u03b8 D 1 |X s | xs L D (I xs , 1) + 1 |X t | xt L D (I xt , 0),(8)\nand the adversarial objective to train the segmentation network is:\nmin \u03b8 F 1 |X t | xt L D (I xt , 1).(9)\nCombining (2) and (9), we derive the optimization problem\nmin \u03b8 F 1 |X s | xs L seg (x s , y s ) + \u03bb adv |X t | xt L D (I xt , 1),(10)\nwith the weighting factor \u03bb adv for the adversarial term L D . During training, we alternatively optimize networks D and F using objectives (8) and (10).\n\n\nIncorporating class-ratio priors\n\nEntropy minimization can get biased towards some easy classes. Therefore, sometimes it is beneficial to guide the learning with some prior. To this end, we use a simple classprior based on the distribution of the classes over the source labels. We compute the class-prior vector p s as a 1 normalized histogram of number of pixels per class over the source labels. Now based on the predicted P xt , too large discrepancy between the expected probability for any class and class-prior p s is penalized, using\nL cp (x t ) = C c=1 max 0, \u00b5p (c) s \u2212 E c (P (c) xt ) ,(11)\nwhere \u00b5 \u2208 [0, 1] is used to relax the class prior constraint. This addresses the fact that class distribution on a single target image is not necessarily close to p s .\n\n\nExperiments\n\nIn this section, we present our experimental results. Section 4.1 introduces the used datasets as well as our training parameters. In Section 4.2 and Section 4.3, we show and discuss our main results. In Section 4.3, we discuss a preliminary result on entropy-based UDA for detection.\n\n\nExperimental details\n\nDatasets. To evaluate our approaches, we use the challenging synthetic-2-real unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training. To train our models, we use either GTA5 [30] or SYNTHIA [31] as source domain synthetic data, along with the training split of Cityscapes dataset [6] as target domain data. Similar set-ups have been previously used in other works [15,14,41,51]. In detail:\n\n\u2022 GTA5\u2192Cityscapes: The GTA5 dataset consists of 24, 966 synthesized frames captured from a video game. Originally, the images are provided with pixellevel semantic annotations of 33 classes. Similar to [15], we use the 19 classes in common with the Cityscapes dataset. \u2022 SYNTHIA\u2192Cityscapes: We use the SYNTHIA-RAND-CITYSCAPES set 3 with 9, 400 synthesized images for training. We train our models with 16 common classes in SYNTHIA and Cityscapes. While evaluating we compare the performance on 16-and 13class subsets following the protocol used in [51].\n\nIn both set-ups, 2, 975 unlabeled Cityscapes images are used for training. We measure segmentation performance with the standard mean-Intersection-over-Union (mIoU) metric [9]. Evaluation is done on the 500 validation images of the Cityscapes dataset.\n\nNetwork architectures. We use Deeplab-V2 [2] as the base semantic segmentation architecture F . To better capture the scene context, Atrous Spatial Pyramid Pooling (ASPP) is applied on the last layer's feature output. Sampling rates are fixed as {6, 12, 18, 24}, similar to the ASPP-L model in [2]. We experiment on the two different base deep CNN architectures: VGG-16 [37] and ResNet-101 [13]. Following [2], we modify the stride and dilation rate of the last layers to produce denser feature maps with larger field-of-views. To further improve performance on ResNet-101, we perform adaptation on multi-level outputs coming from both conv4 and conv5 features [41].\n\nThe adversarial network D introduced in Section 3.2 has the same architecture as the one used in DCGAN [28]. Weighted self-information maps I x are forwarded through 4 convolutional layers, each coupled with a leaky-ReLU layer with a fixed negative slope of 0.2. At the end, a classifier layer produces classification outputs, indicating if the inputs correspond to source or target domain.\n\nImplementation details. We employ PyTorch deep learning framework [27] in the implementations. All experi-  We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt). In each subtable, top and bottom parts correspond to VGG-16-based and ResNet-101-based models respectively. The \"Adapt-SegMap*\" denotes our retrained model of [41]. The abbreviations \"Adv\", \"ST\" and \"Ent\" stand for adversarial training, self-training and entropy minimization approaches.\n\nments are done on a single NVIDIA 1080TI GPU with 11 GB memory. Our model, except the adversarial discriminator mentioned in Section 3.2, is trained using Stochastic Gradient Descent optimizer [1] with learning rate 2.5 \u00d7 10 \u22124 , momentum 0.9 and weight decay 10 \u22124 . We use Adam optimizer [19] with learning rate 10 \u22124 to train the discriminator. To schedule the learning rate, we follow the polynomial annealing procedure mentioned in [2]. Weighting factors of entropy and adversarial losses: To set the weight for L ent , the training set performance provides important indications. If \u03bb ent is large then the entropy drops too quickly and the model is strongly biased towards a few classes. When \u03bb ent is chosen in a suitable range however, the performance is better and not sensitive to the precise value. Thus, we use the same \u03bb ent = 0.001 for all our experiments regardless of the network or the dataset. Similar arguments hold for the weight \u03bb adv in (10). We fix \u03bb adv = 0.001 in all experiments.\n\n\nResults\n\nWe present experimental results of our approaches compared to different baselines. Our models achieve state-ofthe-art performance in the two UDA benchmarks. In what follows, we show different behaviors of our approaches in different settings, i.e. training sets and base CNNs.\n\nGTA5\u2192Cityscapes: We report in Table 1-a semantic segmentation performance in terms of mIoU (%) on Cityscapes validation set. Our first approach of direct entropy minimization, termed as MinEnt in Table 1-a, achieves comparable performance to state-of-the-art baselines on both VGG-16 and ResNet-101-based CNNs. The MinEnt outperforms Self-Training (ST) approach without and with class-balancing [51]. Compared to [41], the ResNet-101-based MinEnt shows similar results but without resorting to the training of a discriminator network. The light overhead of the entropy loss makes training time much less for the MinEnt model. Another advantage of our entropy approach is the ease of training. Indeed, training adversarial networks is generally known as a difficult task due to its instability. We observed a more stable behavior training models with the entropy loss.\n\nInterestingly, we find that in some cases, only applying entropy loss on certain ranges works best. Such a phenomenon is observed with the ResNet-101-based models. Indeed, we get the best model by training on pixels having entropy values within the top 30% of each target sample. The model is termed as MinEnt+ER in Table 1-a. We achieve state-of-the-art mIoU of 43.6% using this training strategy on the GTA5\u2192Cityscapes UDA set-up. We show more details in Section 4.3.\n\nOur second approach using adversarial training on the weighted self-information space, noted as AdvEnt, shows consistent improvement to the baselines on the two base networks. In general, AdvEnt works better than Mi-nEnt. Such results confirm our intuition on the importance of structural adaptation. With the VGG-16-based network, adaptation on the weighted self-information space brings +2.8% mIoU improvement compared to the direct entropy minimization. With the ResNet-101-based network, the improvement is less, i.e. +0.4% mIoU. We conjecture that: as GTA5 semantic layouts are very similar to ones in Cityscapes, the segmentation network F with high capacity base CNN like ResNet-101 is capable of learning some spatial priors from the supervision on source samples. As for lower-capacity base model like VGG-16, an additional regularization on the structured space with adversarial training is more beneficial.\n\nBy combining results of the two models MinEnt and Ad-vEnt, we observe a decent boost in performance, compared to results of single models. The ensemble achieves 44.8% mIoU on the Cityscape validation set. Such a result indicates that complementary information are learned by the two models. Indeed, while the entropy loss penalizes independent pixel-level predictions, the adversarial loss operates more on the image-level, i.e. scene topology. Similar to [41], for a more meaningful comparison to other UDA approaches, in Table 2-a we show the performance gap between UDA models and the oracle, i.e. the model trained with full supervision on the Cityscapes training set. Compared to models trained by other methods, our single and ensemble models have smaller mIoU gaps to the oracle.\n\nIn Figure 3, we illustrate some qualitative results of our  models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like \"building\" and \"car\". Still, there exist many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence. By observation, we notice that overall, the AdvEnt model achieves lower prediction entropy compared to the MinEnt model. We provide more qualitative examples in the supplementary materials.\n\nSYNTHIA\u2192Cityscapes: Table 1-b shows results on the 16and 13-class subsets of the Cityscapes validation set. We notice that scene images in SYNTHIA cover more diverse viewpoints than the ones in GTA5 and Cityscape. This results in different behaviors of our approaches.\n\nOn the VGG-16-based network, the MinEnt model shows comparable results to state-of-the-art methods. Compared to Self-Training [51], our model achieves +3.6% and +4.7% on 16and 13class subsets respectively. However, compared to stronger baselines like the class-balanced self-training, we observe a significant drop in class \"road\". We argue that it is due to the large layout gaps between SYNTHIA and Cityscapes. To target this issue, we incorporate the class-ratio priors from source domain, as introduced in Section 3.3. By constraining target output distribution using class-ratio priors, noted as CP in Table 1-b, we improve MinEnt by +2.9% mIoU on both 16and 13class subsets. With adversarial training, we have an additional \u223c +1% mIoU. On the ResNet-101-based network, the Ad-vEnt model achieves state-of-the-art performance. Com-pared to the retrained model of [41], i.e. Adapt-SegMap*, the AdvEnt improves the mIoUs on 16and 13class subsets by +1.2% and +1.8%.\n\nConsistent to the GTA5 results above, the ensemble of the two models MinEnt and AdvEnt trained on SYNTHIA achieves the best mIoU of 41.2% and 48.0% on 16and 13class subsets respectively. According to Table 2-b, our models have the smallest mIoU gaps to the oracle.\n\n\nDiscussion\n\nThe experimental results shown in Section 4.2 have validated the advantages of our approaches. To further push the performance, we proposed two different ways to regularize the training in two particular settings. This section discusses our experimental choices and explain the intuitions behind.\n\n\nGTA5\u2192Cityscapes:\n\nTraining on specific entropy ranges. In this setup, we observe that the performance of model MinEnt using ResNet-101-based network can be improved by training on target pixels having entropy values in a specific range. Interestingly, the best MinEnt model was trained on the top 30% highest-entropy pixels on each target sample -gaining 1.3% mIoU over the vanilla model. We note that high-entropy pixels are the \"most confusing\" ones, i.e. where the segmentation model is indecisive between multiple classes. One reason is that the ResNet-101based model generalizes well in this particular setting. Accordingly, among the \"most confusing\" predictions, there is a decent amount of correct but \"low confident\" ones. Minimizing the entropy loss on such a set still drives the model toward the desirable direction. This assumption, however, does not hold for the VGG-16-based model.\n\nSYNTHIA\u2192Cityscapes: Using class-ratio prior. As discussed before, SYNTHIA has significantly different layout and viewpoints than Cityscapes. This disparity can cause very bad prediction for some classes, which is then further encouraged with minimization of entropy or their use as label proxy in self-training. Thus, it can lead to strong class biases or in extreme case, to missing some classes completely in the target domain. Adding class-ratio prior encourages the presence of all the classes and thus helps avoid such degenerate solutions. As mentioned in Sec. 3.3, we use \u00b5 to relax the source class-ratio prior, for example \u00b5 = 0 means no prior while \u00b5 = 1 implies enforcing exact class-ratio prior. Having \u00b5 = 1 is not ideal as it means that each target image should follow the class-ratio from the source domain. We choose \u00b5 = 0.5 to let the target classratio to be somewhat different from the source.\n\nApplication on UDA for object detection. The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks like object detection. We conducted experiments in  Foggy validation set. We show results of the baseline and our entropy-based approaches \"MinEnt\" and \"AdvEnt\".\n\nthe UDA object detection set-up Cityscapes\u2192Cityscapes-Foggy, similar to the one in [3]. A straight-forward application of the entropy loss and of the adversarial loss to the existing detection architecture SSD-300 [22] significantly improves detection performance over the baseline model trained only on source. In terms of mean-average-precision (mAP), compared to the baseline performance of 14.7%, the MinEnt and AdvEnt models attain mAPs of 16.9% and 26.2%. In [3], the authors report a slightly better performance of 27.6% mAP, using Faster-RCNN [29], a more complex detection architecture than ours. We note that our detection system was trained and tested on images at lower resolution, i.e. 300 \u00d7 300. Despite those unfavorable factors, our improvement gap to the baseline (+11.5% mAP using AdvEnt) is larger than the one reported in [3] (+8.8%). Such a preliminary result suggests the possibility of applying entropy-based approaches on UDA for detection. Table 3 reports the per-class results of the approaches on Cityscapes Foggy validation set. In Figure 4, we illustrate some qualitative results of our models. . More technical details are given in the Appendix A.\n\n\nConclusion\n\nIn this work, we address the task of unsupervised domain adaptation for semantic segmentation and propose two complementary entropy-based approaches. Our models achieve state-of-the-art on the two challenging \"synthetic-2-real\" benchmarks. The ensemble of the two models improves the performance further. On UDA for object detection, we show a promising result and believe that the performance can get better using more robust detection architectures.\n\nA. Entropy-based UDA for object detection Object detection framework. We use the Single Shot MultiBox Detector (SSD-300) [22] with the VGG-16 base CNN [37] as the detection backbone in our experiments. Given an input image, SSD-300 produces dense predictions from M feature maps at different resolutions. In detail, every location on each feature map m corresponds to a set of K m anchor boxes with predefined aspect ratios and scales. The detection pipeline ends with a non-maximum suppres-sion (NMS) step to post-process the predictions. Readers are referred to [22] for more details about the architecture and the training procedure. We denote the \"soft-detection map\" of the SSD model at feature map m of dimension H m \u00d7 W m as P m\n\nx \u2208 [0, 1] Hm\u00d7Wm\u00d7Km\u00d7C .\n\nDirect entropy minimization. Considering a target input image x t , the entropy map produced at a feature map m, E m xt \u2208 [0, 1] Hm\u00d7Wm\u00d7Km , is composed of the independent box-level entropies normalized to [0, 1]: \nE m(h,w,k) xt = \u22121 log(C)\nSimilar to the semantic segmentation task, we jointly optimize the supervised object detection losses on source and the unsupervised entropy loss L ent on target samples.\n\nEntropy minimization with adversarial learning. We apply the adversarial framework proposed in Section 3.2.\n\nTo this end, we first transform the soft-detection maps P m\n\nx to the weighted self-information maps I m x . We then zeropad the I m x maps at lower resolutions to match the size of the largest one. Finally, we stack all zero-padded I m x to produce I x , which serves as the input to the discriminator.  \n\nFigure 1 :\n1Proposed entropy-based unsupervised domain adaptation for semantic segmentation. The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps (see text for details).\n\nFigure 2 :\n2Approach overview. The figure shows our two approaches for UDA. First, direct entropy minimization minimizes the entropy of the target Px t , which is equivalent to minimizing the sum of weighted self-information maps Ix t . In the second, complementary approach, we use adversarial training to enforce the consistency in Ix across domains. Red arrows are used for target domain and blue arrows for source. An example of entropy map is shown for illustration.\n\n\nloss L ent is defined as the sum of normalized box entropies over all anchor boxes and all feature resolutions:L ent (x t ) =\n\nFigure 3 :\n3Qualitative results in the GTA5\u2192Cityscapes set-up (best viewed in color). Column (a) shows a input image and the corresponding semantic segmentation ground-truth. Column (b), (c) and (d) show segmentation results (bottom) along with prediction entropy maps produced by different approaches (top).\n\nFigure 4 :\n4Qualitative results for object detection. Column (a) shows the input images. Columns (b), (c) and (d) illustrate detection results of the baseline, our MinEnt and AdvEnt models. Detections of different classes are plotted in different colors. We visualize all the detections with scores greater than 0.5.\n\n( a )\naGTA5 \u2192 CityscapesModels \nAppr. \nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nlight \nsign \nveg \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmbike \nbike \nmIoU \n\nFCNs in the Wild [15] \nAdv 70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 \n7.3 \n0.0 \n3.5 \n0.0 27.1 \n\nCyCADA [14] \nAdv 83.5 38.3 76.4 20.6 16.5 22.2 26.2 21.9 80.4 28.7 65.7 49.4 4.2 74.6 16.0 26.6 2.0 \n8.0 \n0.0 34.8 \n\nAdapt-SegMap [41] \nAdv 87.3 29.8 78.6 21.1 18.2 22.5 21.5 11.0 79.7 29.6 71.3 46.8 6.5 80.1 23.0 26.9 0.0 10.6 0.3 35.0 \n\nSelf-Training [51] \nST \n83.8 17.4 72.1 14.3 2.9 16.5 16.0 6.8 81.4 24.2 47.2 40.7 7.6 71.7 10.2 7.6 \n0.5 11.1 0.9 28.1 \n\nSelf-Training + CB [51] ST \n66.7 26.8 73.7 14.8 9.5 28.3 25.9 10.1 75.5 15.7 51.6 47.2 6.2 71.9 3.7 \n2.2 \n5.4 18.9 32.4 30.9 \n\nOurs (MinEnt) \nEnt 85.1 18.9 76.3 32.4 19.7 19.9 21.0 8.9 76.3 26.2 63.1 42.8 5.9 80.8 20.2 9.8 \n0.0 14.8 0.6 32.8 \n\nOurs (AdvEnt) \nAdv 86.8 28.5 78.1 27.6 24.2 20.7 19.3 8.9 78.8 29.3 69.0 47.9 5.9 79.8 25.9 34.1 0.0 11.3 0.3 35.6 \n\nAdapt-SegMap [41] \nAdv 86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 \n\nAdapt-SegMap* \nAdv 85.5 18.4 80.8 29.1 24.6 27.9 33.1 20.9 83.8 31.2 75.0 57.5 28.6 77.3 32.3 30.9 1.1 28.7 35.9 42.2 \n\nOurs (MinEnt) \nEnt 86.2 18.6 80.3 27.2 24.0 23.4 33.5 24.7 83.3 31.0 75.6 54.6 25.6 85.2 30.0 10.9 0.1 21.9 37.1 42.3 \n\nOurs (MinEnt + ER) \nEnt 86.6 25.6 80.8 28.9 25.3 26.5 33.7 25.5 83.3 30.9 76.8 56.8 27.9 84.3 33.6 41.1 1.2 23.9 36.4 43.6 \n\nOurs (AdvEnt) \nAdv 86.9 16.6 81.5 32.1 24.5 28.8 33.7 20.4 84.1 31.1 74.4 58.1 30.0 82.8 29.9 37.6 0.5 27.3 30.7 42.7 \n\nOurs (AdvEnt+MinEnt) A+E 87.6 21.4 82.0 34.8 26.2 28.5 35.6 23.0 84.5 35.1 76.2 58.6 30.7 84.8 34.2 43.4 0.4 28.4 35.3 44.8 \n\n(b) SYNTHIA \u2192 Cityscapes \n\nModels \nAppr. \nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nlight \nsign \nveg \nsky \nperson \nrider \ncar \nbus \nmbike \nbike \nmIoU mIoU* \n\nFCNs in the Wild [15] \nAdv 11.5 19.6 30.8 4.4 \n0.0 20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.0 3.2 \n0.2 \n0.6 20.2 22.1 \n\nAdapt-SegMap [41] \nAdv 78.9 29.2 75.5 \n-\n-\n-\n0.1 \n4.8 72.6 76.7 43.4 8.8 71.1 16.0 3.6 \n8.4 \n-\n37.6 \n\nSelf-Training [51] \nST \n0.2 14.5 53.8 1.6 \n0.0 18.9 0.9 \n7.8 72.2 80.3 48.1 6.3 67.7 4.7 \n0.2 \n4.5 23.9 27.8 \n\nSelf-Training + CB [51] \nST \n69.6 28.7 69.5 12.1 0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 6.6 \n3.7 32.4 35.4 36.1 \n\nOurs (MinEnt) \nEnt \n37.8 18.2 65.8 2.0 \n0.0 15.5 0.0 \n0.0 \n76 73.9 45.7 11.3 66.6 13.3 1.5 13.1 27.5 32.5 \n\nOurs (MinEnt + CP) \nEnt \n45.9 19.6 65.8 5.3 \n0.2 20.7 2.1 \n8.2 74.4 76.7 47.5 12.2 71.1 22.8 4.5 \n9.2 30.4 35.4 \n\nOurs (AdvEnt + CP) \nAdv 67.9 29.4 71.9 6.3 \n0.3 19.9 0.6 \n2.6 74.9 74.9 35.4 9.6 67.8 21.4 4.1 15.5 31.4 36.6 \n\nAdapt-SegMap [41] \nAdv 84.3 42.7 77.5 \n-\n-\n-\n4.7 \n7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3 \n-\n46.7 \n\nAdapt-SegMap* [41] \nAdv 81.7 39.1 78.4 11.1 0.3 25.8 6.8 \n9.0 79.1 80.8 54.8 21.0 66.8 34.7 13.8 29.9 39.6 45.8 \n\nOurs (MinEnt) \nEnt \n73.5 29.2 77.1 7.7 \n0.2 27.0 7.1 11.4 76.7 82.1 57.2 21.3 69.4 29.2 12.9 27.9 38.1 44.2 \n\nOurs (AdvEnt) \nAdv 87.0 44.1 79.7 9.6 \n0.6 24.3 4.8 \n7.2 80.1 83.6 56.4 23.7 72.7 32.6 12.8 33.7 40.8 47.6 \n\nOurs (AdvEnt+MinEnt) A+E 85.6 42.2 79.7 8.7 \n0.4 25.9 5.4 \n8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2 48.0 \n\n\n\nTable 1 :\n1Semantic segmentation performance mIoU (%) on Cityscapes validation set of models trained on GTA5 (a) and SYNTHIA (b).\n\nTable 2 :\n2Performance gap between UDA models and the oracle inthe two set-ups: GTA5\u2192Cityscapes and SYNTHIA\u2192Cityscapes. \nTop and bottom parts of each table correspond to VGG-16-based \nand ResNet-101-based models respectively. \n\n\n\n\nCityscapes \u2192 Cityscapes FoggyModels \nperson \nrider \ncar \ntruck \nbus \ntrain \nmcycle \nbicycle \n\nmAP \n\nSSD-300 \n15.0 17.4 27.2 5.7 15.1 9.1 11.0 16.7 14.7 \n\nOurs (MinEnt) \n15.8 22.0 28.3 5.0 15.2 15.0 13.0 20.6 16.9 \n\nOurs (AdvEnt) \n17.6 25.0 39.6 20.0 37.1 25.9 21.3 23.1 26.2 \n\n\n\nTable 3 :\n3Object detection performance mAP (%) on Cityscapes\nAbusing notation, we denote log P (h,w) x the vector obtained by applying log to P (h,w) x component-wise.\nA split of the SYNTHIA dataset[31] using compatible labels with the Cityscapes dataset.\n\nLarge-scale machine learning with stochastic gradient descent. L Bottou, COMPSTAT. L. Bottou. Large-scale machine learning with stochastic gra- dient descent. In COMPSTAT. 2010.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, PAMIL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. PAMI, 2018.\n\nDomain adaptive faster r-cnn for object detection in the wild. Y Chen, W Li, C Sakaridis, D Dai, L Van Gool, CVPR. Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Do- main adaptive faster r-cnn for object detection in the wild. In CVPR, 2018.\n\nRoad: Reality oriented adaptation for semantic segmentation of urban scenes. Y Chen, W Li, L Van Gool, CVPR. Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented adaptation for semantic segmentation of urban scenes. In CVPR, 2018.\n\nNo more discrimination: Cross city adaptation of road scene segmenters. Y.-H Chen, W.-Y Chen, Y.-T Chen, B.-C Tsai, Y.-C F Wang, M Sun, ICCV. Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017.\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, CVPR. M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.\n\nDomain adaptation for visual applications: A comprehensive survey. G Csurka, G. Csurka. Domain adaptation for visual applications: A comprehensive survey. 2017.\n\nCARLA: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. CARLA: An open urban driving simulator. In CoRL, 2017.\n\nThe pascal visual object classes challenge: A retrospective. M Everingham, S M A Eslami, L Van Gool, C K I Williams, J Winn, A Zisserman, IJCVM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual ob- ject classes challenge: A retrospective. IJCV, 2015.\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, ICML. Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In NIPS, 2014.\n\nSemi-supervised learning by entropy minimization. Y Grandvalet, Y Bengio, NIPS. Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS, 2005.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nCyCADA: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A Efros, T Darrell, ICML. J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell. CyCADA: Cycle-consistent adver- sarial domain adaptation. In ICML, 2018.\n\nJ Hoffman, D Wang, F Yu, T Darrell, arXiv:1612.02649Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprintJ. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adapta- tion. arXiv preprint arXiv:1612.02649, 2016.\n\nConditional generative adversarial network for structured domain adaptation. W Hong, Z Wang, M Yang, J Yuan, CVPR. W. Hong, Z. Wang, M. Yang, and J. Yuan. Conditional gen- erative adversarial network for structured domain adaptation. In CVPR, 2018.\n\nSubic: A supervised, structured binary code for image search. H Jain, J Zepeda, P P\u00e9rez, R Gribonval, ICCV. H. Jain, J. Zepeda, P. P\u00e9rez, and R. Gribonval. Subic: A su- pervised, structured binary code for image search. In ICCV, 2017.\n\nLearning a complete image indexing pipeline. H Jain, J Zepeda, P P\u00e9rez, R Gribonval, CVPR. H. Jain, J. Zepeda, P. P\u00e9rez, and R. Gribonval. Learning a complete image indexing pipeline. In CVPR, 2018.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nTemporal ensembling for semisupervised learning. S Laine, T Aila, arXiv:1610.02242arXiv preprintS. Laine and T. Aila. Temporal ensembling for semi- supervised learning. arXiv preprint arXiv:1610.02242, 2016.\n\nPseudo-label: The simple and efficient semisupervised learning method for deep neural networks. D.-H Lee, ICML Workshop. D.-H. Lee. Pseudo-label: The simple and efficient semi- supervised learning method for deep neural networks. In ICML Workshop, 2013.\n\nSSD: single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S E Reed, C Fu, A C Berg, ECCV. W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg. SSD: single shot multibox detector. In ECCV, 2016.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M I Jordan, ICML. M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans- ferable features with deep adaptation networks. In ICML, 2015.\n\nUnsupervised domain adaptation with residual transfer networks. M Long, H Zhu, J Wang, M I Jordan, NIPS. M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In NIPS, 2016.\n\nImage to image translation for domain adaptation. Z Murez, S Kolouri, D Kriegman, R Ramamoorthi, K Kim, CVPR. Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim. Image to image translation for domain adaptation. In CVPR, 2018.\n\nAutomatic differentiation in pytorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z De-Vito, Z Lin, A Desmaison, L Antiga, A Lerer, NIPS Workshop. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De- Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto- matic differentiation in pytorch. In NIPS Workshop, 2017.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, ICLR. A. Radford, L. Metz, and S. Chintala. Unsupervised repre- sentation learning with deep convolutional generative adver- sarial networks. In ICLR, 2016.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPS. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015.\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, ECCV. S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, J Materzynska, D Vazquez, A M Lopez, CVPR. G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, 2016.\n\nAdversarial dropout regularization. K Saito, Y Ushiku, T Harada, K Saenko, ICLR. K. Saito, Y. Ushiku, T. Harada, and K. Saenko. Adversarial dropout regularization. In ICLR, 2018.\n\nMaximum classifier discrepancy for unsupervised domain adaptation. K Saito, K Watanabe, Y Ushiku, T Harada, CVPR. K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In CVPR, 2018.\n\nLearning from synthetic data: Addressing domain shift for semantic segmentation. S Sankaranarayanan, Y Balaji, A Jain, S Nam Lim, R Chellappa, CVPR. S. Sankaranarayanan, Y. Balaji, A. Jain, S. Nam Lim, and R. Chellappa. Learning from synthetic data: Addressing do- main shift for semantic segmentation. In CVPR, 2018.\n\nPlay and learn: Using video games to train computer vision models. A Shafaei, J J Little, M Schmidt, BMVC. A. Shafaei, J. J. Little, and M. Schmidt. Play and learn: Us- ing video games to train computer vision models. In BMVC, 2016.\n\nA mathematical theory of communication. Bell system technical journal. C E Shannon, C. E. Shannon. A mathematical theory of communication. Bell system technical journal, 1948.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n\nUnsupervised and semi-supervised learning with categorical generative adversarial networks. ICLR. J T Springenberg, J. T. Springenberg. Unsupervised and semi-supervised learn- ing with categorical generative adversarial networks. ICLR, 2016.\n\nMean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results. A Tarvainen, H Valpola, NIPS. A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi- supervised deep learning results. In NIPS, 2017.\n\nM Tribus, Thermostatics and thermodynamics. M. Tribus. Thermostatics and thermodynamics. 1970.\n\nLearning to adapt structured output space for semantic segmentation. Y.-H Tsai, W.-C Hung, S Schulter, K Sohn, M.-H Yang, M Chandraker, CVPR. Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, CVPR. E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017.\n\nDcan: Dual channel-wise alignment networks for unsupervised scene adaptation. Z Wu, X Han, Y.-L Lin, M Uzunbas, T Goldstein, S Nam Lim, L S Davis, ECCV. Z. Wu, X. Han, Y.-L. Lin, M. Gokhan Uzunbas, T. Goldstein, S. Nam Lim, and L. S. Davis. Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation. In ECCV, 2018.\n\nMind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. H Yan, Y Ding, P Li, Q Wang, Y Xu, W Zuo, CVPR. H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. In CVPR, 2017.\n\nCurriculum domain adaptation for semantic segmentation of urban scenes. Y Zhang, P David, B Gong, ICCV. Y. Zhang, P. David, and B. Gong. Curriculum domain adap- tation for semantic segmentation of urban scenes. In ICCV, 2017.\n\nFully convolutional adaptation networks for semantic segmentation. Y Zhang, Z Qiu, T Yao, D Liu, T Mei, CVPR. Y. Zhang, Z. Qiu, T. Yao, D. Liu, and T. Mei. Fully convo- lutional adaptation networks for semantic segmentation. In CVPR, 2018.\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, CVPR. H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. In CVPR, 2017.\n\nUnpaired imageto-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, ICCV. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image- to-image translation using cycle-consistent adversarial net- works. In ICCV, 2017.\n\nSemi-supervised learning literature survey. X Zhu, 1530Computer Sciences. University of Wisconsin-MadisonTechnical ReportX. Zhu. Semi-supervised learning literature survey. Tech- nical Report 1530, Computer Sciences, University of Wisconsin-Madison, 2005.\n\nPenalizing top performers: Conservative loss for semantic segmentation adaptation. X Zhu, H Zhou, C Yang, J Shi, D Lin, ECCV. X. Zhu, H. Zhou, C. Yang, J. Shi, and D. Lin. Penalizing top performers: Conservative loss for semantic segmentation adaptation. In ECCV, September 2018.\n\nUnsupervised domain adaptation for semantic segmentation via classbalanced self-training. Y Zou, Z Yu, B V Kumar, J Wang, ECCV. Y. Zou, Z. Yu, B. V. Kumar, and J. Wang. Unsupervised domain adaptation for semantic segmentation via class- balanced self-training. In ECCV, 2018.\n", "annotations": {"author": "[{\"end\":104,\"start\":91},{\"end\":119,\"start\":105},{\"end\":134,\"start\":120},{\"end\":182,\"start\":135},{\"end\":197,\"start\":183},{\"end\":221,\"start\":198}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":101},{\"end\":118,\"start\":114},{\"end\":133,\"start\":127},{\"end\":148,\"start\":144},{\"end\":196,\"start\":191}]", "author_first_name": "[{\"end\":100,\"start\":91},{\"end\":113,\"start\":105},{\"end\":126,\"start\":120},{\"end\":143,\"start\":135},{\"end\":190,\"start\":183}]", "author_affiliation": "[{\"end\":181,\"start\":150},{\"end\":220,\"start\":199}]", "title": "[{\"end\":88,\"start\":1},{\"end\":309,\"start\":222}]", "venue": null, "abstract": "[{\"end\":1192,\"start\":311}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1770,\"start\":1766},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1773,\"start\":1770},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1900,\"start\":1896},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1903,\"start\":1900},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1905,\"start\":1903},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2629,\"start\":2625},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2632,\"start\":2629},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2635,\"start\":2632},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2659,\"start\":2655},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2662,\"start\":2659},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2730,\"start\":2726},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2733,\"start\":2730},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2778,\"start\":2774},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2854,\"start\":2850},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2857,\"start\":2854},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2860,\"start\":2857},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3101,\"start\":3097},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3203,\"start\":3199},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4373,\"start\":4372},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5644,\"start\":5643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6085,\"start\":6082},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6099,\"start\":6095},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6116,\"start\":6112},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6119,\"start\":6116},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6231,\"start\":6227},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6234,\"start\":6231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6237,\"start\":6234},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6240,\"start\":6237},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6243,\"start\":6240},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6282,\"start\":6278},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6313,\"start\":6309},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6316,\"start\":6313},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6319,\"start\":6316},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6496,\"start\":6493},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7057,\"start\":7053},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7315,\"start\":7312},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7422,\"start\":7419},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7572,\"start\":7568},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7741,\"start\":7737},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7858,\"start\":7854},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7861,\"start\":7858},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8021,\"start\":8017},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8061,\"start\":8057},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8359,\"start\":8355},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8549,\"start\":8545},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8617,\"start\":8613},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8853,\"start\":8849},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9124,\"start\":9120},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9171,\"start\":9167},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9713,\"start\":9709},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9716,\"start\":9713},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9743,\"start\":9739},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10024,\"start\":10020},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10114,\"start\":10110},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10238,\"start\":10234},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10454,\"start\":10450},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10457,\"start\":10454},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10477,\"start\":10473},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10480,\"start\":10477},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10565,\"start\":10561},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12827,\"start\":12823},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13685,\"start\":13681},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13815,\"start\":13811},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14609,\"start\":14605},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15158,\"start\":15154},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16586,\"start\":16585},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16944,\"start\":16940},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19129,\"start\":19125},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19145,\"start\":19141},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19234,\"start\":19231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19319,\"start\":19315},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19322,\"start\":19319},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19325,\"start\":19322},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19328,\"start\":19325},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19548,\"start\":19544},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19894,\"start\":19890},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20072,\"start\":20069},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20194,\"start\":20191},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20447,\"start\":20444},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20524,\"start\":20520},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20544,\"start\":20540},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20559,\"start\":20556},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20815,\"start\":20811},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20925,\"start\":20921},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21280,\"start\":21276},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21595,\"start\":21591},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21917,\"start\":21914},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22015,\"start\":22011},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22161,\"start\":22158},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23416,\"start\":23412},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23434,\"start\":23430},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25736,\"start\":25732},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27123,\"start\":27119},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27865,\"start\":27861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30775,\"start\":30772},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30907,\"start\":30903},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31157,\"start\":31154},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31244,\"start\":31240},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31534,\"start\":31531},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32459,\"start\":32455},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32489,\"start\":32485},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32902,\"start\":32898},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":39721,\"start\":39717}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34392,\"start\":33923},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34865,\"start\":34393},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34993,\"start\":34866},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35303,\"start\":34994},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35621,\"start\":35304},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38875,\"start\":35622},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39006,\"start\":38876},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39236,\"start\":39007},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39516,\"start\":39237},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39579,\"start\":39517}]", "paragraph": "[{\"end\":2129,\"start\":1208},{\"end\":2861,\"start\":2131},{\"end\":3204,\"start\":2863},{\"end\":4416,\"start\":3206},{\"end\":5100,\"start\":4418},{\"end\":5697,\"start\":5102},{\"end\":6120,\"start\":5715},{\"end\":6535,\"start\":6122},{\"end\":8243,\"start\":6537},{\"end\":9989,\"start\":8245},{\"end\":10370,\"start\":9991},{\"end\":10711,\"start\":10372},{\"end\":11055,\"start\":10726},{\"end\":11309,\"start\":11057},{\"end\":11509,\"start\":11334},{\"end\":11931,\"start\":11554},{\"end\":12136,\"start\":12006},{\"end\":12183,\"start\":12180},{\"end\":12671,\"start\":12215},{\"end\":12975,\"start\":12673},{\"end\":13189,\"start\":13040},{\"end\":13434,\"start\":13225},{\"end\":14258,\"start\":13509},{\"end\":14342,\"start\":14337},{\"end\":14862,\"start\":14412},{\"end\":15346,\"start\":14913},{\"end\":16216,\"start\":15348},{\"end\":16271,\"start\":16218},{\"end\":16658,\"start\":16472},{\"end\":17227,\"start\":16698},{\"end\":17363,\"start\":17296},{\"end\":17460,\"start\":17403},{\"end\":17691,\"start\":17538},{\"end\":18235,\"start\":17728},{\"end\":18464,\"start\":18296},{\"end\":18764,\"start\":18480},{\"end\":19340,\"start\":18789},{\"end\":19895,\"start\":19342},{\"end\":20148,\"start\":19897},{\"end\":20816,\"start\":20150},{\"end\":21208,\"start\":20818},{\"end\":21719,\"start\":21210},{\"end\":22727,\"start\":21721},{\"end\":23015,\"start\":22739},{\"end\":23884,\"start\":23017},{\"end\":24355,\"start\":23886},{\"end\":25274,\"start\":24357},{\"end\":26062,\"start\":25276},{\"end\":26721,\"start\":26064},{\"end\":26991,\"start\":26723},{\"end\":27961,\"start\":26993},{\"end\":28227,\"start\":27963},{\"end\":28538,\"start\":28242},{\"end\":29437,\"start\":28559},{\"end\":30350,\"start\":29439},{\"end\":30687,\"start\":30352},{\"end\":31866,\"start\":30689},{\"end\":32332,\"start\":31881},{\"end\":33069,\"start\":32334},{\"end\":33094,\"start\":33071},{\"end\":33309,\"start\":33096},{\"end\":33506,\"start\":33336},{\"end\":33615,\"start\":33508},{\"end\":33676,\"start\":33617},{\"end\":33922,\"start\":33678}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11333,\"start\":11310},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11553,\"start\":11510},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12005,\"start\":11932},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12179,\"start\":12137},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13039,\"start\":12976},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13224,\"start\":13190},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13508,\"start\":13435},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14336,\"start\":14259},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14411,\"start\":14343},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16471,\"start\":16272},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16697,\"start\":16659},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17295,\"start\":17228},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17402,\"start\":17364},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17537,\"start\":17461},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18295,\"start\":18236},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33335,\"start\":33310}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23054,\"start\":23047},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23220,\"start\":23213},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24209,\"start\":24202},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25806,\"start\":25799},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26750,\"start\":26743},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27607,\"start\":27600},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28170,\"start\":28163},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31661,\"start\":31654}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1206,\"start\":1194},{\"attributes\":{\"n\":\"2.\"},\"end\":5713,\"start\":5700},{\"attributes\":{\"n\":\"3.\"},\"end\":10724,\"start\":10714},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12213,\"start\":12186},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14911,\"start\":14865},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17726,\"start\":17694},{\"attributes\":{\"n\":\"4.\"},\"end\":18478,\"start\":18467},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18787,\"start\":18767},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22737,\"start\":22730},{\"attributes\":{\"n\":\"4.3.\"},\"end\":28240,\"start\":28230},{\"end\":28557,\"start\":28541},{\"attributes\":{\"n\":\"5.\"},\"end\":31879,\"start\":31869},{\"end\":33934,\"start\":33924},{\"end\":34404,\"start\":34394},{\"end\":35005,\"start\":34995},{\"end\":35315,\"start\":35305},{\"end\":35628,\"start\":35623},{\"end\":38886,\"start\":38877},{\"end\":39017,\"start\":39008},{\"end\":39527,\"start\":39518}]", "table": "[{\"end\":38875,\"start\":35647},{\"end\":39236,\"start\":39071},{\"end\":39516,\"start\":39268}]", "figure_caption": "[{\"end\":34392,\"start\":33936},{\"end\":34865,\"start\":34406},{\"end\":34993,\"start\":34868},{\"end\":35303,\"start\":35007},{\"end\":35621,\"start\":35317},{\"end\":35647,\"start\":35630},{\"end\":39006,\"start\":38888},{\"end\":39071,\"start\":39019},{\"end\":39268,\"start\":39239},{\"end\":39579,\"start\":39529}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3583,\"start\":3575},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5433,\"start\":5425},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11024,\"start\":11016},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13103,\"start\":13095},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15747,\"start\":15739},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26075,\"start\":26067},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31757,\"start\":31749}]", "bib_author_first_name": "[{\"end\":39840,\"start\":39839},{\"end\":40072,\"start\":40068},{\"end\":40080,\"start\":40079},{\"end\":40094,\"start\":40093},{\"end\":40106,\"start\":40105},{\"end\":40116,\"start\":40115},{\"end\":40118,\"start\":40117},{\"end\":40392,\"start\":40391},{\"end\":40400,\"start\":40399},{\"end\":40406,\"start\":40405},{\"end\":40419,\"start\":40418},{\"end\":40426,\"start\":40425},{\"end\":40657,\"start\":40656},{\"end\":40665,\"start\":40664},{\"end\":40671,\"start\":40670},{\"end\":40890,\"start\":40886},{\"end\":40901,\"start\":40897},{\"end\":40912,\"start\":40908},{\"end\":40923,\"start\":40919},{\"end\":40934,\"start\":40930},{\"end\":40936,\"start\":40935},{\"end\":40944,\"start\":40943},{\"end\":41183,\"start\":41182},{\"end\":41193,\"start\":41192},{\"end\":41202,\"start\":41201},{\"end\":41211,\"start\":41210},{\"end\":41222,\"start\":41221},{\"end\":41235,\"start\":41234},{\"end\":41247,\"start\":41246},{\"end\":41257,\"start\":41256},{\"end\":41265,\"start\":41264},{\"end\":41534,\"start\":41533},{\"end\":41669,\"start\":41668},{\"end\":41684,\"start\":41683},{\"end\":41691,\"start\":41690},{\"end\":41704,\"start\":41703},{\"end\":41713,\"start\":41712},{\"end\":41903,\"start\":41902},{\"end\":41917,\"start\":41916},{\"end\":41921,\"start\":41918},{\"end\":41931,\"start\":41930},{\"end\":41943,\"start\":41942},{\"end\":41947,\"start\":41944},{\"end\":41959,\"start\":41958},{\"end\":41967,\"start\":41966},{\"end\":42202,\"start\":42201},{\"end\":42211,\"start\":42210},{\"end\":42353,\"start\":42352},{\"end\":42367,\"start\":42366},{\"end\":42384,\"start\":42383},{\"end\":42393,\"start\":42392},{\"end\":42399,\"start\":42398},{\"end\":42415,\"start\":42414},{\"end\":42424,\"start\":42423},{\"end\":42437,\"start\":42436},{\"end\":42656,\"start\":42655},{\"end\":42670,\"start\":42669},{\"end\":42827,\"start\":42826},{\"end\":42833,\"start\":42832},{\"end\":42842,\"start\":42841},{\"end\":42849,\"start\":42848},{\"end\":43017,\"start\":43016},{\"end\":43028,\"start\":43027},{\"end\":43037,\"start\":43036},{\"end\":43048,\"start\":43044},{\"end\":43055,\"start\":43054},{\"end\":43064,\"start\":43063},{\"end\":43074,\"start\":43073},{\"end\":43083,\"start\":43082},{\"end\":43263,\"start\":43262},{\"end\":43274,\"start\":43273},{\"end\":43282,\"start\":43281},{\"end\":43288,\"start\":43287},{\"end\":43642,\"start\":43641},{\"end\":43650,\"start\":43649},{\"end\":43658,\"start\":43657},{\"end\":43666,\"start\":43665},{\"end\":43877,\"start\":43876},{\"end\":43885,\"start\":43884},{\"end\":43895,\"start\":43894},{\"end\":43904,\"start\":43903},{\"end\":44096,\"start\":44095},{\"end\":44104,\"start\":44103},{\"end\":44114,\"start\":44113},{\"end\":44123,\"start\":44122},{\"end\":44295,\"start\":44294},{\"end\":44297,\"start\":44296},{\"end\":44307,\"start\":44306},{\"end\":44452,\"start\":44451},{\"end\":44461,\"start\":44460},{\"end\":44711,\"start\":44707},{\"end\":44903,\"start\":44902},{\"end\":44910,\"start\":44909},{\"end\":44922,\"start\":44921},{\"end\":44931,\"start\":44930},{\"end\":44942,\"start\":44941},{\"end\":44944,\"start\":44943},{\"end\":44952,\"start\":44951},{\"end\":44958,\"start\":44957},{\"end\":44960,\"start\":44959},{\"end\":45160,\"start\":45159},{\"end\":45168,\"start\":45167},{\"end\":45181,\"start\":45180},{\"end\":45371,\"start\":45370},{\"end\":45379,\"start\":45378},{\"end\":45386,\"start\":45385},{\"end\":45394,\"start\":45393},{\"end\":45396,\"start\":45395},{\"end\":45600,\"start\":45599},{\"end\":45608,\"start\":45607},{\"end\":45615,\"start\":45614},{\"end\":45623,\"start\":45622},{\"end\":45625,\"start\":45624},{\"end\":45815,\"start\":45814},{\"end\":45824,\"start\":45823},{\"end\":45835,\"start\":45834},{\"end\":45847,\"start\":45846},{\"end\":45862,\"start\":45861},{\"end\":46042,\"start\":46041},{\"end\":46052,\"start\":46051},{\"end\":46061,\"start\":46060},{\"end\":46073,\"start\":46072},{\"end\":46083,\"start\":46082},{\"end\":46091,\"start\":46090},{\"end\":46102,\"start\":46101},{\"end\":46109,\"start\":46108},{\"end\":46122,\"start\":46121},{\"end\":46132,\"start\":46131},{\"end\":46429,\"start\":46428},{\"end\":46440,\"start\":46439},{\"end\":46448,\"start\":46447},{\"end\":46698,\"start\":46697},{\"end\":46705,\"start\":46704},{\"end\":46711,\"start\":46710},{\"end\":46723,\"start\":46722},{\"end\":46924,\"start\":46923},{\"end\":46926,\"start\":46925},{\"end\":46937,\"start\":46936},{\"end\":46947,\"start\":46946},{\"end\":46955,\"start\":46954},{\"end\":47192,\"start\":47191},{\"end\":47199,\"start\":47198},{\"end\":47210,\"start\":47209},{\"end\":47225,\"start\":47224},{\"end\":47236,\"start\":47235},{\"end\":47238,\"start\":47237},{\"end\":47473,\"start\":47472},{\"end\":47482,\"start\":47481},{\"end\":47492,\"start\":47491},{\"end\":47502,\"start\":47501},{\"end\":47684,\"start\":47683},{\"end\":47693,\"start\":47692},{\"end\":47705,\"start\":47704},{\"end\":47715,\"start\":47714},{\"end\":47944,\"start\":47943},{\"end\":47964,\"start\":47963},{\"end\":47974,\"start\":47973},{\"end\":47982,\"start\":47981},{\"end\":47986,\"start\":47983},{\"end\":47993,\"start\":47992},{\"end\":48249,\"start\":48248},{\"end\":48260,\"start\":48259},{\"end\":48262,\"start\":48261},{\"end\":48272,\"start\":48271},{\"end\":48487,\"start\":48486},{\"end\":48489,\"start\":48488},{\"end\":48661,\"start\":48660},{\"end\":48673,\"start\":48672},{\"end\":48904,\"start\":48903},{\"end\":48906,\"start\":48905},{\"end\":49169,\"start\":49168},{\"end\":49182,\"start\":49181},{\"end\":49366,\"start\":49365},{\"end\":49534,\"start\":49530},{\"end\":49545,\"start\":49541},{\"end\":49553,\"start\":49552},{\"end\":49565,\"start\":49564},{\"end\":49576,\"start\":49572},{\"end\":49584,\"start\":49583},{\"end\":49812,\"start\":49811},{\"end\":49821,\"start\":49820},{\"end\":49832,\"start\":49831},{\"end\":49842,\"start\":49841},{\"end\":50048,\"start\":50047},{\"end\":50054,\"start\":50053},{\"end\":50064,\"start\":50060},{\"end\":50071,\"start\":50070},{\"end\":50082,\"start\":50081},{\"end\":50095,\"start\":50094},{\"end\":50099,\"start\":50096},{\"end\":50106,\"start\":50105},{\"end\":50108,\"start\":50107},{\"end\":50403,\"start\":50402},{\"end\":50410,\"start\":50409},{\"end\":50418,\"start\":50417},{\"end\":50424,\"start\":50423},{\"end\":50432,\"start\":50431},{\"end\":50438,\"start\":50437},{\"end\":50689,\"start\":50688},{\"end\":50698,\"start\":50697},{\"end\":50707,\"start\":50706},{\"end\":50911,\"start\":50910},{\"end\":50920,\"start\":50919},{\"end\":50927,\"start\":50926},{\"end\":50934,\"start\":50933},{\"end\":50941,\"start\":50940},{\"end\":51116,\"start\":51115},{\"end\":51124,\"start\":51123},{\"end\":51131,\"start\":51130},{\"end\":51137,\"start\":51136},{\"end\":51145,\"start\":51144},{\"end\":51333,\"start\":51329},{\"end\":51340,\"start\":51339},{\"end\":51348,\"start\":51347},{\"end\":51357,\"start\":51356},{\"end\":51359,\"start\":51358},{\"end\":51565,\"start\":51564},{\"end\":51861,\"start\":51860},{\"end\":51868,\"start\":51867},{\"end\":51876,\"start\":51875},{\"end\":51884,\"start\":51883},{\"end\":51891,\"start\":51890},{\"end\":52149,\"start\":52148},{\"end\":52156,\"start\":52155},{\"end\":52162,\"start\":52161},{\"end\":52164,\"start\":52163},{\"end\":52173,\"start\":52172}]", "bib_author_last_name": "[{\"end\":39847,\"start\":39841},{\"end\":40077,\"start\":40073},{\"end\":40091,\"start\":40081},{\"end\":40103,\"start\":40095},{\"end\":40113,\"start\":40107},{\"end\":40125,\"start\":40119},{\"end\":40397,\"start\":40393},{\"end\":40403,\"start\":40401},{\"end\":40416,\"start\":40407},{\"end\":40423,\"start\":40420},{\"end\":40435,\"start\":40427},{\"end\":40662,\"start\":40658},{\"end\":40668,\"start\":40666},{\"end\":40680,\"start\":40672},{\"end\":40895,\"start\":40891},{\"end\":40906,\"start\":40902},{\"end\":40917,\"start\":40913},{\"end\":40928,\"start\":40924},{\"end\":40941,\"start\":40937},{\"end\":40948,\"start\":40945},{\"end\":41190,\"start\":41184},{\"end\":41199,\"start\":41194},{\"end\":41208,\"start\":41203},{\"end\":41219,\"start\":41212},{\"end\":41232,\"start\":41223},{\"end\":41244,\"start\":41236},{\"end\":41254,\"start\":41248},{\"end\":41262,\"start\":41258},{\"end\":41273,\"start\":41266},{\"end\":41541,\"start\":41535},{\"end\":41681,\"start\":41670},{\"end\":41688,\"start\":41685},{\"end\":41701,\"start\":41692},{\"end\":41710,\"start\":41705},{\"end\":41720,\"start\":41714},{\"end\":41914,\"start\":41904},{\"end\":41928,\"start\":41922},{\"end\":41940,\"start\":41932},{\"end\":41956,\"start\":41948},{\"end\":41964,\"start\":41960},{\"end\":41977,\"start\":41968},{\"end\":42208,\"start\":42203},{\"end\":42221,\"start\":42212},{\"end\":42364,\"start\":42354},{\"end\":42381,\"start\":42368},{\"end\":42390,\"start\":42385},{\"end\":42396,\"start\":42394},{\"end\":42412,\"start\":42400},{\"end\":42421,\"start\":42416},{\"end\":42434,\"start\":42425},{\"end\":42444,\"start\":42438},{\"end\":42667,\"start\":42657},{\"end\":42677,\"start\":42671},{\"end\":42830,\"start\":42828},{\"end\":42839,\"start\":42834},{\"end\":42846,\"start\":42843},{\"end\":42853,\"start\":42850},{\"end\":43025,\"start\":43018},{\"end\":43034,\"start\":43029},{\"end\":43042,\"start\":43038},{\"end\":43052,\"start\":43049},{\"end\":43061,\"start\":43056},{\"end\":43071,\"start\":43065},{\"end\":43080,\"start\":43075},{\"end\":43091,\"start\":43084},{\"end\":43271,\"start\":43264},{\"end\":43279,\"start\":43275},{\"end\":43285,\"start\":43283},{\"end\":43296,\"start\":43289},{\"end\":43647,\"start\":43643},{\"end\":43655,\"start\":43651},{\"end\":43663,\"start\":43659},{\"end\":43671,\"start\":43667},{\"end\":43882,\"start\":43878},{\"end\":43892,\"start\":43886},{\"end\":43901,\"start\":43896},{\"end\":43914,\"start\":43905},{\"end\":44101,\"start\":44097},{\"end\":44111,\"start\":44105},{\"end\":44120,\"start\":44115},{\"end\":44133,\"start\":44124},{\"end\":44304,\"start\":44298},{\"end\":44310,\"start\":44308},{\"end\":44458,\"start\":44453},{\"end\":44466,\"start\":44462},{\"end\":44715,\"start\":44712},{\"end\":44907,\"start\":44904},{\"end\":44919,\"start\":44911},{\"end\":44928,\"start\":44923},{\"end\":44939,\"start\":44932},{\"end\":44949,\"start\":44945},{\"end\":44955,\"start\":44953},{\"end\":44965,\"start\":44961},{\"end\":45165,\"start\":45161},{\"end\":45178,\"start\":45169},{\"end\":45189,\"start\":45182},{\"end\":45376,\"start\":45372},{\"end\":45383,\"start\":45380},{\"end\":45391,\"start\":45387},{\"end\":45403,\"start\":45397},{\"end\":45605,\"start\":45601},{\"end\":45612,\"start\":45609},{\"end\":45620,\"start\":45616},{\"end\":45632,\"start\":45626},{\"end\":45821,\"start\":45816},{\"end\":45832,\"start\":45825},{\"end\":45844,\"start\":45836},{\"end\":45859,\"start\":45848},{\"end\":45866,\"start\":45863},{\"end\":46049,\"start\":46043},{\"end\":46058,\"start\":46053},{\"end\":46070,\"start\":46062},{\"end\":46080,\"start\":46074},{\"end\":46088,\"start\":46084},{\"end\":46099,\"start\":46092},{\"end\":46106,\"start\":46103},{\"end\":46119,\"start\":46110},{\"end\":46129,\"start\":46123},{\"end\":46138,\"start\":46133},{\"end\":46437,\"start\":46430},{\"end\":46445,\"start\":46441},{\"end\":46457,\"start\":46449},{\"end\":46702,\"start\":46699},{\"end\":46708,\"start\":46706},{\"end\":46720,\"start\":46712},{\"end\":46727,\"start\":46724},{\"end\":46934,\"start\":46927},{\"end\":46944,\"start\":46938},{\"end\":46952,\"start\":46948},{\"end\":46962,\"start\":46956},{\"end\":47196,\"start\":47193},{\"end\":47207,\"start\":47200},{\"end\":47222,\"start\":47211},{\"end\":47233,\"start\":47226},{\"end\":47244,\"start\":47239},{\"end\":47479,\"start\":47474},{\"end\":47489,\"start\":47483},{\"end\":47499,\"start\":47493},{\"end\":47509,\"start\":47503},{\"end\":47690,\"start\":47685},{\"end\":47702,\"start\":47694},{\"end\":47712,\"start\":47706},{\"end\":47722,\"start\":47716},{\"end\":47961,\"start\":47945},{\"end\":47971,\"start\":47965},{\"end\":47979,\"start\":47975},{\"end\":47990,\"start\":47987},{\"end\":48003,\"start\":47994},{\"end\":48257,\"start\":48250},{\"end\":48269,\"start\":48263},{\"end\":48280,\"start\":48273},{\"end\":48497,\"start\":48490},{\"end\":48670,\"start\":48662},{\"end\":48683,\"start\":48674},{\"end\":48919,\"start\":48907},{\"end\":49179,\"start\":49170},{\"end\":49190,\"start\":49183},{\"end\":49373,\"start\":49367},{\"end\":49539,\"start\":49535},{\"end\":49550,\"start\":49546},{\"end\":49562,\"start\":49554},{\"end\":49570,\"start\":49566},{\"end\":49581,\"start\":49577},{\"end\":49595,\"start\":49585},{\"end\":49818,\"start\":49813},{\"end\":49829,\"start\":49822},{\"end\":49839,\"start\":49833},{\"end\":49850,\"start\":49843},{\"end\":50051,\"start\":50049},{\"end\":50058,\"start\":50055},{\"end\":50068,\"start\":50065},{\"end\":50079,\"start\":50072},{\"end\":50092,\"start\":50083},{\"end\":50103,\"start\":50100},{\"end\":50114,\"start\":50109},{\"end\":50407,\"start\":50404},{\"end\":50415,\"start\":50411},{\"end\":50421,\"start\":50419},{\"end\":50429,\"start\":50425},{\"end\":50435,\"start\":50433},{\"end\":50442,\"start\":50439},{\"end\":50695,\"start\":50690},{\"end\":50704,\"start\":50699},{\"end\":50712,\"start\":50708},{\"end\":50917,\"start\":50912},{\"end\":50924,\"start\":50921},{\"end\":50931,\"start\":50928},{\"end\":50938,\"start\":50935},{\"end\":50945,\"start\":50942},{\"end\":51121,\"start\":51117},{\"end\":51128,\"start\":51125},{\"end\":51134,\"start\":51132},{\"end\":51142,\"start\":51138},{\"end\":51149,\"start\":51146},{\"end\":51337,\"start\":51334},{\"end\":51345,\"start\":51341},{\"end\":51354,\"start\":51349},{\"end\":51365,\"start\":51360},{\"end\":51569,\"start\":51566},{\"end\":51865,\"start\":51862},{\"end\":51873,\"start\":51869},{\"end\":51881,\"start\":51877},{\"end\":51888,\"start\":51885},{\"end\":51895,\"start\":51892},{\"end\":52153,\"start\":52150},{\"end\":52159,\"start\":52157},{\"end\":52170,\"start\":52165},{\"end\":52178,\"start\":52174}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":115963355},\"end\":39953,\"start\":39776},{\"attributes\":{\"id\":\"b1\"},\"end\":40326,\"start\":39955},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3780471},\"end\":40577,\"start\":40328},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4715607},\"end\":40812,\"start\":40579},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1448074},\"end\":41117,\"start\":40814},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":502946},\"end\":41464,\"start\":41119},{\"attributes\":{\"id\":\"b6\"},\"end\":41626,\"start\":41466},{\"attributes\":{\"id\":\"b7\"},\"end\":41839,\"start\":41628},{\"attributes\":{\"id\":\"b8\"},\"end\":42148,\"start\":41841},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6755881},\"end\":42321,\"start\":42150},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1033682},\"end\":42603,\"start\":42323},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7890982},\"end\":42778,\"start\":42605},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":42958,\"start\":42780},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7646250},\"end\":43260,\"start\":42960},{\"attributes\":{\"doi\":\"arXiv:1612.02649\",\"id\":\"b14\"},\"end\":43562,\"start\":43262},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49547920},\"end\":43812,\"start\":43564},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":19919965},\"end\":44048,\"start\":43814},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10146328},\"end\":44248,\"start\":44050},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6628106},\"end\":44400,\"start\":44250},{\"attributes\":{\"doi\":\"arXiv:1610.02242\",\"id\":\"b19\"},\"end\":44609,\"start\":44402},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18507866},\"end\":44864,\"start\":44611},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2141740},\"end\":45101,\"start\":44866},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1629541},\"end\":45306,\"start\":45103},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":556999},\"end\":45533,\"start\":45308},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":745350},\"end\":45762,\"start\":45535},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":13815143},\"end\":46001,\"start\":45764},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":40027675},\"end\":46332,\"start\":46003},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11758569},\"end\":46615,\"start\":46334},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10328909},\"end\":46869,\"start\":46617},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5844139},\"end\":47086,\"start\":46871},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206594095},\"end\":47434,\"start\":47088},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3687439},\"end\":47614,\"start\":47436},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4619542},\"end\":47860,\"start\":47616},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4540721},\"end\":48179,\"start\":47862},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7611267},\"end\":48413,\"start\":48181},{\"attributes\":{\"id\":\"b35\"},\"end\":48590,\"start\":48415},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14124313},\"end\":48803,\"start\":48592},{\"attributes\":{\"id\":\"b37\"},\"end\":49046,\"start\":48805},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2759724},\"end\":49363,\"start\":49048},{\"attributes\":{\"id\":\"b39\"},\"end\":49459,\"start\":49365},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3556146},\"end\":49763,\"start\":49461},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4357800},\"end\":49967,\"start\":49765},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4889062},\"end\":50302,\"start\":49969},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":24919744},\"end\":50614,\"start\":50304},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11824004},\"end\":50841,\"start\":50616},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":5093704},\"end\":51082,\"start\":50843},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":5299559},\"end\":51247,\"start\":51084},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":233404466},\"end\":51518,\"start\":51249},{\"attributes\":{\"doi\":\"1530\",\"id\":\"b48\",\"matched_paper_id\":2731141},\"end\":51775,\"start\":51520},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52155286},\"end\":52056,\"start\":51777},{\"attributes\":{\"id\":\"b50\"},\"end\":52333,\"start\":52058}]", "bib_title": "[{\"end\":39837,\"start\":39776},{\"end\":40389,\"start\":40328},{\"end\":40654,\"start\":40579},{\"end\":40884,\"start\":40814},{\"end\":41180,\"start\":41119},{\"end\":42199,\"start\":42150},{\"end\":42350,\"start\":42323},{\"end\":42653,\"start\":42605},{\"end\":42824,\"start\":42780},{\"end\":43014,\"start\":42960},{\"end\":43639,\"start\":43564},{\"end\":43874,\"start\":43814},{\"end\":44093,\"start\":44050},{\"end\":44292,\"start\":44250},{\"end\":44705,\"start\":44611},{\"end\":44900,\"start\":44866},{\"end\":45157,\"start\":45103},{\"end\":45368,\"start\":45308},{\"end\":45597,\"start\":45535},{\"end\":45812,\"start\":45764},{\"end\":46039,\"start\":46003},{\"end\":46426,\"start\":46334},{\"end\":46695,\"start\":46617},{\"end\":46921,\"start\":46871},{\"end\":47189,\"start\":47088},{\"end\":47470,\"start\":47436},{\"end\":47681,\"start\":47616},{\"end\":47941,\"start\":47862},{\"end\":48246,\"start\":48181},{\"end\":48658,\"start\":48592},{\"end\":49166,\"start\":49048},{\"end\":49528,\"start\":49461},{\"end\":49809,\"start\":49765},{\"end\":50045,\"start\":49969},{\"end\":50400,\"start\":50304},{\"end\":50686,\"start\":50616},{\"end\":50908,\"start\":50843},{\"end\":51113,\"start\":51084},{\"end\":51327,\"start\":51249},{\"end\":51562,\"start\":51520},{\"end\":51858,\"start\":51777},{\"end\":52146,\"start\":52058}]", "bib_author": "[{\"end\":39849,\"start\":39839},{\"end\":40079,\"start\":40068},{\"end\":40093,\"start\":40079},{\"end\":40105,\"start\":40093},{\"end\":40115,\"start\":40105},{\"end\":40127,\"start\":40115},{\"end\":40399,\"start\":40391},{\"end\":40405,\"start\":40399},{\"end\":40418,\"start\":40405},{\"end\":40425,\"start\":40418},{\"end\":40437,\"start\":40425},{\"end\":40664,\"start\":40656},{\"end\":40670,\"start\":40664},{\"end\":40682,\"start\":40670},{\"end\":40897,\"start\":40886},{\"end\":40908,\"start\":40897},{\"end\":40919,\"start\":40908},{\"end\":40930,\"start\":40919},{\"end\":40943,\"start\":40930},{\"end\":40950,\"start\":40943},{\"end\":41192,\"start\":41182},{\"end\":41201,\"start\":41192},{\"end\":41210,\"start\":41201},{\"end\":41221,\"start\":41210},{\"end\":41234,\"start\":41221},{\"end\":41246,\"start\":41234},{\"end\":41256,\"start\":41246},{\"end\":41264,\"start\":41256},{\"end\":41275,\"start\":41264},{\"end\":41543,\"start\":41533},{\"end\":41683,\"start\":41668},{\"end\":41690,\"start\":41683},{\"end\":41703,\"start\":41690},{\"end\":41712,\"start\":41703},{\"end\":41722,\"start\":41712},{\"end\":41916,\"start\":41902},{\"end\":41930,\"start\":41916},{\"end\":41942,\"start\":41930},{\"end\":41958,\"start\":41942},{\"end\":41966,\"start\":41958},{\"end\":41979,\"start\":41966},{\"end\":42210,\"start\":42201},{\"end\":42223,\"start\":42210},{\"end\":42366,\"start\":42352},{\"end\":42383,\"start\":42366},{\"end\":42392,\"start\":42383},{\"end\":42398,\"start\":42392},{\"end\":42414,\"start\":42398},{\"end\":42423,\"start\":42414},{\"end\":42436,\"start\":42423},{\"end\":42446,\"start\":42436},{\"end\":42669,\"start\":42655},{\"end\":42679,\"start\":42669},{\"end\":42832,\"start\":42826},{\"end\":42841,\"start\":42832},{\"end\":42848,\"start\":42841},{\"end\":42855,\"start\":42848},{\"end\":43027,\"start\":43016},{\"end\":43036,\"start\":43027},{\"end\":43044,\"start\":43036},{\"end\":43054,\"start\":43044},{\"end\":43063,\"start\":43054},{\"end\":43073,\"start\":43063},{\"end\":43082,\"start\":43073},{\"end\":43093,\"start\":43082},{\"end\":43273,\"start\":43262},{\"end\":43281,\"start\":43273},{\"end\":43287,\"start\":43281},{\"end\":43298,\"start\":43287},{\"end\":43649,\"start\":43641},{\"end\":43657,\"start\":43649},{\"end\":43665,\"start\":43657},{\"end\":43673,\"start\":43665},{\"end\":43884,\"start\":43876},{\"end\":43894,\"start\":43884},{\"end\":43903,\"start\":43894},{\"end\":43916,\"start\":43903},{\"end\":44103,\"start\":44095},{\"end\":44113,\"start\":44103},{\"end\":44122,\"start\":44113},{\"end\":44135,\"start\":44122},{\"end\":44306,\"start\":44294},{\"end\":44312,\"start\":44306},{\"end\":44460,\"start\":44451},{\"end\":44468,\"start\":44460},{\"end\":44717,\"start\":44707},{\"end\":44909,\"start\":44902},{\"end\":44921,\"start\":44909},{\"end\":44930,\"start\":44921},{\"end\":44941,\"start\":44930},{\"end\":44951,\"start\":44941},{\"end\":44957,\"start\":44951},{\"end\":44967,\"start\":44957},{\"end\":45167,\"start\":45159},{\"end\":45180,\"start\":45167},{\"end\":45191,\"start\":45180},{\"end\":45378,\"start\":45370},{\"end\":45385,\"start\":45378},{\"end\":45393,\"start\":45385},{\"end\":45405,\"start\":45393},{\"end\":45607,\"start\":45599},{\"end\":45614,\"start\":45607},{\"end\":45622,\"start\":45614},{\"end\":45634,\"start\":45622},{\"end\":45823,\"start\":45814},{\"end\":45834,\"start\":45823},{\"end\":45846,\"start\":45834},{\"end\":45861,\"start\":45846},{\"end\":45868,\"start\":45861},{\"end\":46051,\"start\":46041},{\"end\":46060,\"start\":46051},{\"end\":46072,\"start\":46060},{\"end\":46082,\"start\":46072},{\"end\":46090,\"start\":46082},{\"end\":46101,\"start\":46090},{\"end\":46108,\"start\":46101},{\"end\":46121,\"start\":46108},{\"end\":46131,\"start\":46121},{\"end\":46140,\"start\":46131},{\"end\":46439,\"start\":46428},{\"end\":46447,\"start\":46439},{\"end\":46459,\"start\":46447},{\"end\":46704,\"start\":46697},{\"end\":46710,\"start\":46704},{\"end\":46722,\"start\":46710},{\"end\":46729,\"start\":46722},{\"end\":46936,\"start\":46923},{\"end\":46946,\"start\":46936},{\"end\":46954,\"start\":46946},{\"end\":46964,\"start\":46954},{\"end\":47198,\"start\":47191},{\"end\":47209,\"start\":47198},{\"end\":47224,\"start\":47209},{\"end\":47235,\"start\":47224},{\"end\":47246,\"start\":47235},{\"end\":47481,\"start\":47472},{\"end\":47491,\"start\":47481},{\"end\":47501,\"start\":47491},{\"end\":47511,\"start\":47501},{\"end\":47692,\"start\":47683},{\"end\":47704,\"start\":47692},{\"end\":47714,\"start\":47704},{\"end\":47724,\"start\":47714},{\"end\":47963,\"start\":47943},{\"end\":47973,\"start\":47963},{\"end\":47981,\"start\":47973},{\"end\":47992,\"start\":47981},{\"end\":48005,\"start\":47992},{\"end\":48259,\"start\":48248},{\"end\":48271,\"start\":48259},{\"end\":48282,\"start\":48271},{\"end\":48499,\"start\":48486},{\"end\":48672,\"start\":48660},{\"end\":48685,\"start\":48672},{\"end\":48921,\"start\":48903},{\"end\":49181,\"start\":49168},{\"end\":49192,\"start\":49181},{\"end\":49375,\"start\":49365},{\"end\":49541,\"start\":49530},{\"end\":49552,\"start\":49541},{\"end\":49564,\"start\":49552},{\"end\":49572,\"start\":49564},{\"end\":49583,\"start\":49572},{\"end\":49597,\"start\":49583},{\"end\":49820,\"start\":49811},{\"end\":49831,\"start\":49820},{\"end\":49841,\"start\":49831},{\"end\":49852,\"start\":49841},{\"end\":50053,\"start\":50047},{\"end\":50060,\"start\":50053},{\"end\":50070,\"start\":50060},{\"end\":50081,\"start\":50070},{\"end\":50094,\"start\":50081},{\"end\":50105,\"start\":50094},{\"end\":50116,\"start\":50105},{\"end\":50409,\"start\":50402},{\"end\":50417,\"start\":50409},{\"end\":50423,\"start\":50417},{\"end\":50431,\"start\":50423},{\"end\":50437,\"start\":50431},{\"end\":50444,\"start\":50437},{\"end\":50697,\"start\":50688},{\"end\":50706,\"start\":50697},{\"end\":50714,\"start\":50706},{\"end\":50919,\"start\":50910},{\"end\":50926,\"start\":50919},{\"end\":50933,\"start\":50926},{\"end\":50940,\"start\":50933},{\"end\":50947,\"start\":50940},{\"end\":51123,\"start\":51115},{\"end\":51130,\"start\":51123},{\"end\":51136,\"start\":51130},{\"end\":51144,\"start\":51136},{\"end\":51151,\"start\":51144},{\"end\":51339,\"start\":51329},{\"end\":51347,\"start\":51339},{\"end\":51356,\"start\":51347},{\"end\":51367,\"start\":51356},{\"end\":51571,\"start\":51564},{\"end\":51867,\"start\":51860},{\"end\":51875,\"start\":51867},{\"end\":51883,\"start\":51875},{\"end\":51890,\"start\":51883},{\"end\":51897,\"start\":51890},{\"end\":52155,\"start\":52148},{\"end\":52161,\"start\":52155},{\"end\":52172,\"start\":52161},{\"end\":52180,\"start\":52172}]", "bib_venue": "[{\"end\":39857,\"start\":39849},{\"end\":40066,\"start\":39955},{\"end\":40441,\"start\":40437},{\"end\":40686,\"start\":40682},{\"end\":40954,\"start\":40950},{\"end\":41279,\"start\":41275},{\"end\":41531,\"start\":41466},{\"end\":41666,\"start\":41628},{\"end\":41900,\"start\":41841},{\"end\":42227,\"start\":42223},{\"end\":42450,\"start\":42446},{\"end\":42683,\"start\":42679},{\"end\":42859,\"start\":42855},{\"end\":43097,\"start\":43093},{\"end\":43387,\"start\":43314},{\"end\":43677,\"start\":43673},{\"end\":43920,\"start\":43916},{\"end\":44139,\"start\":44135},{\"end\":44316,\"start\":44312},{\"end\":44449,\"start\":44402},{\"end\":44730,\"start\":44717},{\"end\":44971,\"start\":44967},{\"end\":45195,\"start\":45191},{\"end\":45409,\"start\":45405},{\"end\":45638,\"start\":45634},{\"end\":45872,\"start\":45868},{\"end\":46153,\"start\":46140},{\"end\":46463,\"start\":46459},{\"end\":46733,\"start\":46729},{\"end\":46968,\"start\":46964},{\"end\":47250,\"start\":47246},{\"end\":47515,\"start\":47511},{\"end\":47728,\"start\":47724},{\"end\":48009,\"start\":48005},{\"end\":48286,\"start\":48282},{\"end\":48484,\"start\":48415},{\"end\":48689,\"start\":48685},{\"end\":48901,\"start\":48805},{\"end\":49196,\"start\":49192},{\"end\":49407,\"start\":49375},{\"end\":49601,\"start\":49597},{\"end\":49856,\"start\":49852},{\"end\":50120,\"start\":50116},{\"end\":50448,\"start\":50444},{\"end\":50718,\"start\":50714},{\"end\":50951,\"start\":50947},{\"end\":51155,\"start\":51151},{\"end\":51371,\"start\":51367},{\"end\":51592,\"start\":51575},{\"end\":51901,\"start\":51897},{\"end\":52184,\"start\":52180}]"}}}, "year": 2023, "month": 12, "day": 17}