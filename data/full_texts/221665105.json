{"id": 221665105, "updated": "2023-10-06 11:54:06.306", "metadata": {"title": "Learning to summarize from human feedback", "authors": "[{\"first\":\"Nisan\",\"last\":\"Stiennon\",\"middle\":[]},{\"first\":\"Long\",\"last\":\"Ouyang\",\"middle\":[]},{\"first\":\"Jeff\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Ziegler\",\"middle\":[\"M.\"]},{\"first\":\"Ryan\",\"last\":\"Lowe\",\"middle\":[]},{\"first\":\"Chelsea\",\"last\":\"Voss\",\"middle\":[]},{\"first\":\"Alec\",\"last\":\"Radford\",\"middle\":[]},{\"first\":\"Dario\",\"last\":\"Amodei\",\"middle\":[]},{\"first\":\"Paul\",\"last\":\"Christiano\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.01325", "mag": "3098985263", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2009-01325", "doi": null}}, "content": {"source": {"pdf_hash": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.01325v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cc3866606349736bd7dfa329b2c87cc23d7cc7cf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/053b1d7b97eb2c91fc3921d589c160b0923c70b1.txt", "contents": "\nLearning to summarize from human feedback\n\n\nNisan Stiennon \nLong Ouyang \nJeff Wu \nDaniel M Ziegler \nRyan Lowe \nChelsea Voss \nAlec Radford \nDario Amodei \nPaul Christiano \nOpenai \nLearning to summarize from human feedback\n\nAs language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about-summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-specific fine-tuning. 2 We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.3We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want. * This was a joint project of the OpenAI Reflection team. Author order was randomized amongst {LO, JW, DZ, NS}; CV and RL were full-time contributors for most of the duration. PC is the team lead.2 Samples from all of our models can be viewed on our website.3We provide inference code for our 1.3B models and baselines, as well as a model card and our human feedback dataset with over 64k summary comparisons, here.\n\nIntroduction\n\nLarge-scale language model pretraining has become increasingly prevalent for achieving high performance on a variety of natural language processing (NLP) tasks. When applying these models to a specific task, they are usually fine-tuned using supervised learning, often to maximize the log probability of a set of human demonstrations.\n\nWhile this strategy has led to markedly improved performance, there is still a misalignment between this fine-tuning objective-maximizing the likelihood of human-written text-and what we care about-generating high-quality outputs as determined by humans. This misalignment has several causes: the maximum likelihood objective has no distinction between important errors (e.g. making up facts [41]) and unimportant errors (e.g. selecting the precise word from a set of synonyms); models Figure 1: Fraction of the time humans prefer our models' summaries over the human-generated reference summaries on the TL;DR dataset. 4 Since quality judgments involve an arbitrary decision about how to trade off summary length vs. coverage within the 24-48 token limit, we also provide length-controlled graphs in Appendix F; length differences explain about a third of the gap between feedback and supervised learning at 6.7B. are incentivized to place probability mass on all human demonstrations, including those that are low-quality; and distributional shift during sampling can degrade performance [56,52]. Quality can often be improved significantly by non-uniform sampling strategies such as beam search [51], but these can lead to repetition and other undesirable artifacts [69,23]. Optimizing for quality may be a principled approach to overcoming these problems.\n\nOur goal in this paper is to advance methods for training language models on objectives that more closely capture the behavior we care about. To make short-term progress towards this goal, we focus on abstractive English text summarization, as it has a long history in the NLP community [16,8,54,59,50], and is a subjective task where we believe it is difficult to quantify summary quality without human judgments. Indeed, existing automatic metrics for evaluating summary quality, such as ROUGE [39], have received criticism for poor correlation with human judgments [55,45,6,33].\n\nWe follow the works of [3,73], who fine-tune language models from human feedback using reward learning [35]. We first collect a dataset of human preferences between pairs of summaries, then train a reward model (RM) via supervised learning to predict the human-preferred summary. Finally, we train a policy via reinforcement learning (RL) to maximize the score given by the RM; the policy generates a token of text at each 'time step', and is updated using the PPO algorithm [58] based on the RM 'reward' given to the entire generated summary. We can then gather more human data using samples from the resulting policy, and repeat the process. We follow the works of [48,4] and use large pretrained GPT-3 models with as many as 6.7 billion parameters.\n\nOur main contributions are four-fold.\n\n(1) We show that training with human feedback significantly outperforms very strong baselines on English summarization. When applying our methods on a version of the Reddit TL;DR dataset [63], we train policies via human feedback that produce better summaries than much larger policies trained via supervised learning. Summaries from our human feedback models are preferred by our labelers to the original human demonstrations in the dataset (see Figure 1).\n\n(2) We show human feedback models generalize much better to new domains than supervised models. Our Reddit-trained human feedback models also generate high-quality summaries of news articles on the CNN/DailyMail (CNN/DM) dataset without any news-specific fine-tuning, almost matching the quality of the dataset's reference summaries. We perform several checks to ensure that these human preferences reflect a real quality difference: we consistently monitor agreement rates amongst labelers and researchers, and find researcher-labeler agreement rates are nearly as high as researcher-researcher agreement rates (see Section C.2), and we verify models are not merely optimizing simple metrics like length or amount of copying (see Appendices F and G.7).\n\n(3) We conduct extensive empirical analyses of our policy and reward model. We examine the impact of model and data size ( Figure 6), study performance as we continue to optimize a given reward model (Section 4.3), and analyze reward model performance using synthetic and humanwritten perturbations of summaries (Section 4.3). We confirm that our reward model outperforms other metrics such as ROUGE at predicting human preferences, and that optimizing our reward model directly results in better summaries than optimizing ROUGE according to humans (Section 4.4).\n\n(4) We publicly release our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our evaluation data on both TL;DR (comparisons and Likert scores) and CNN/DM (Likert scores).\n\nThe methods we present in this paper are motivated in part by longer-term concerns about the misalignment of AI systems with what humans want them to do. When misaligned summarization models make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems become more powerful and are given increasingly important tasks, the mistakes they make will likely become more subtle and safety-critical, making this an important area for further research.\n\n\nRelated work\n\nMost directly related to our work is previous work using human feedback to train summarization models with RL [3,73]. Bohm et al. [3] learn a reward function from a dataset of human ratings of 2.5k CNN/DM summaries, and train a policy whose summaries are preferred to a policy optimizing ROUGE. Our work is most similar to [73], who also train Transformer models [62] to optimize human feedback across a range of tasks, including summarization on the Reddit TL;DR and CNN/DM datasets. Unlike us, they train in an online manner and find the model highly extractive. They note that their labelers prefer extractive summaries and have low agreement rates with researchers. Compared to [73], we use significantly larger models, move to the batch setting for collecting human feedback, ensure high labeler-researcher agreement, and make some algorithmic modifications, such as separating the policy and value networks.\n\nHuman feedback has also been used as a reward to train models in other domains such as dialogue [25, 68,21], translation [32,1], semantic parsing [34], story generation [72], review generation [7], and evidence extraction [46]. Our reward modeling approach was developed in prior work on learning to rank [40], which has been applied to ranking search results using either explicit feedback [2,18] or implicit feedback in the form of click-through data [29,30]. In a related line of research, human feedback has been used to train agents in simulated environments [10,24]. There is also a rich literature on using RL to optimize automatic metrics for NLP tasks, such as ROUGE for summarization [50,65,45,15,19], BLEU for translation [50,66,1,43], and other domains [61,27,26]. Finally, there has been extensive research on modifying architectures [22,59] and pre-training procedures [70,36,49,60,53,14] for improving summarization performance.\n\n3 Method and experiment details 3\n\n\n.1 High-level methodology\n\nOur approach is similar to the one outlined in [73], adapted to the batch setting. We start with an initial policy that is fine-tuned via supervised learning on the desired dataset (in our case, the Reddit TL;DR summarization dataset). The process (illustrated in Figure 2) then consists of three steps that can be repeated iteratively.\n\nStep 1: Collect samples from existing policies and send comparisons to humans. For each Reddit post, we sample summaries from several sources including the current policy, initial policy, original reference summaries and various baselines. We send a batch of pairs of summaries to our human evaluators, who are tasked with selecting the best summary of a given Reddit post.\n\nStep 2: Learn a reward model from human comparisons. Given a post and a candidate summary, we train a reward model to predict the log odds that this summary is the better one, as judged by our labelers.\n\nStep 3: Optimize a policy against the reward model. We treat the logit output of the reward model as a reward that we optimize using reinforcement learning, specifically with the PPO algorithm [58].\n\n\nCollect human feedback\n\n\"j is better than k\" \"j is better than k\"  We provide a more thorough description of our procedure, including details of the reward model and policy training and our quality control process, in the following sections. In practice, rather than precisely iterating this sequence of three steps, we updated our data collection and training procedures over the course of the project while accumulating labels (see Appendix C.6 for details).\n\n\nDatasets and task\n\nDatasets. We use the TL;DR summarization dataset [63], which contains~3 million posts from reddit.com across a variety of topics (subreddits), as well summaries of the posts written by the original poster (TL;DRs). We additionally filter this dataset (see Appendix A) to ensure quality, including using a whitelist of subreddits that are understandable to the general population. Crucially, we also filter to include only posts where the human-written summaries contain between 24 and 48 tokens, to minimize the potential effect of summary length on quality (see Section 4.1 and Appendix F). Our final filtered dataset contains 123,169 posts, and we hold out~5% as a validation set. For the remainder of this paper, we refer to this dataset simply as TL;DR.\n\nWe chose the TL;DR dataset over the more commonly used CNN/DM dataset primarily because very strong performance can be attained on CNN/DM with simple extractive baselines. We find in Section 4.2 that our labelers prefer lead-3 over the CNN/DM reference summaries, 5 and that the supervised T5 model [49] with low-temperature sampling already surpasses the reference summary quality, while copying extensively from the article. On the other hand, simple extractive baselines perform poorly on TL;DR in our human evaluations (see Appendix G.2). Instead of training on CNN/DM, we study the transfer performance of our human feedback models to CNN/DM after being trained to summarize Reddit posts.\n\nTask. We define our ground-truth task as producing a model that generates summaries fewer than 48 tokens long that are as good as possible, according to our judgments. We judge summary quality by how faithfully the summary conveys the original post to a reader who can only read the summary and not the post (see Appendix C.5 for further discussion of criteria). Since we have limited capacity to do comparisons, we hire labelers to do the comparisons for us. We rely on detailed procedures to ensure high agreement between labelers and us on the task, which we describe in the next section.\n\n[r/dating_advice] First date ever, going to the beach. Would like some tips Hey Reddit! I (20M) would like some tips, because I have my first ever date tomorrow (although I've had a gf for 3 years, but no actual dating happened), and we're going to the beach.\n\nI met this girl, we have mutual friends, at a festival a few days ago. We didn't kiss, but we talked, held hands, danced a bit. I asked her to go on a date with me, which was super hard as it is the first time I've asked this to anybody. What I mean to say is, it's not like a standard *first* date because we already spent some time together.\n\nI'm really nervous and excited. I'm going to pick her up tomorrow, we're cycling to the beach which will take 30 \n\n\nCollecting human feedback\n\nPrevious work on fine-tuning language models from human feedback [73] reported \"a mismatch between the notion of quality we wanted our model to learn, and what the humans labelers actually evaluated\", leading to model-generated summaries that were high-quality according to the labelers, but fairly low-quality according to the researchers.\n\nCompared to [73], we implement two changes to improve human data quality. First, we transition entirely to the offline setting, where we alternate between sending large batches of comparison data 6 to our human labelers and re-training our models on the cumulative collected data. Second, we maintain a hands-on relationship with labelers: 7 we on-board them with detailed instructions, answer their questions in a shared chat room, and provide regular feedback on their performance. We train all labelers to ensure high agreement with our judgments, and continuously monitor labeler-researcher agreement over the course of the project. See Appendix C.1 and C.5 for details.\n\nAs a result of our procedure, we obtained high labeler-researcher agreement: on a subset of comparison tasks, labelers agree with researchers 77% \u00b1 2% of the time, while researchers agree with each other 73% \u00b1 4% of the time. We provide more analysis of our human data quality in Appendix C.2.\n\n\nModels\n\nAll of our models are Transformer decoders [62] in the style of GPT-3 [47,4]. We conduct our human feedback experiments on models with 1.3 billion (1.3B) and 6.7 billion (6.7B) parameters.\n\nPretrained models. Similarly to [12,47], we start with models pretrained to autoregressively predict the next token in a large text corpus. As in [48,4], we use these models as 'zero-shot' baselines by padding the context with examples of high-quality summaries from the dataset. We provide details on pretraining in Appendix B, and on our zero-shot procedure in Appendix B.2.\n\nSupervised baselines. We next fine-tune these models via supervised learning to predict summaries from our filtered TL;DR dataset (see Appendix B for details). We use these supervised models to sample initial summaries for collecting comparisons, to initialize our policy and reward models, and as baselines for evaluation. In our final human evaluations, we use T=0 to sample from all models, as we found it performed better than higher temperatures or nucleus sampling (see Appendix B.1).\n\nTo validate that our supervised models are indeed strong baselines for comparison, we run our supervised fine-tuning procedure with our 6.7B model on the CNN/DM dataset, and find that we achieve slightly better ROUGE scores than SOTA models [71] from mid-2019 (see Appendix G.4).\n\nReward models. To train our reward models, we start from a supervised baseline, as described above, then add a randomly initialized linear head that outputs a scalar value. We train this model to predict which summary y \u2208 {y 0 , y 1 } is better as judged by a human, given a post x. If the summary preferred by the human is y i , we can write the RM loss as:\nloss(r \u03b8 ) = \u2212E (x,y0,y1,i)\u223cD [log(\u03c3(r \u03b8 (x, y i ) \u2212 r \u03b8 (x, y 1\u2212i )))]\nwhere r \u03b8 (x, y) is the scalar output of the reward model for post x and summary y with parameters \u03b8, and D is the dataset of human judgments. At the end of training, we normalize the reward model outputs such that the reference summaries from our dataset achieve a mean score of 0.\n\nHuman feedback policies. We want to use the reward model trained above to train a policy that generates higher-quality outputs as judged by humans. We primarily do this using reinforcement learning, by treating the output of the reward model as a reward for the entire summary that we maximize with the PPO algorithm [58], where each time step is a BPE token. 8 We initialize our policy to be the model fine-tuned on Reddit TL;DR. Importantly, we include a term in the reward that penalizes the KL divergence between the learned RL policy \u03c0 RL \u03c6 with parameters \u03c6 and this original supervised model \u03c0 SFT , as previously done in [25]. The full reward R can be written as:\nR(x, y) = r \u03b8 (x, y) \u2212 \u03b2 log[\u03c0 RL \u03c6 (y|x)/\u03c0 SFT (y|x)]\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn't learn to produce outputs that are too different from those that the reward model has seen during training.\n\nFor the PPO value function, we use a Transformer with completely separate parameters from the policy. This prevents updates to the value function from partially destroying the pretrained policy early in training (see ablation in Appendix G.1). We initialize the value function to the parameters of the reward model. In our experiments, the reward model, policy, and value function are the same size.\n\n\nResults\n\n\nSummarizing Reddit posts from human feedback\n\nPolicies trained with human feedback are preferred to much larger supervised policies. Our main results evaluating our human feedback policies on TL;DR are shown in Figure 1. We measure policy quality as the percentage of summaries generated by that policy that humans prefer over the reference summaries in the dataset. Our policies trained with human feedback significantly outperform our supervised baselines on this metric, with our 1.3B human feedback model significantly outperforming a supervised model 10\u00d7 its size (61% versus 43% raw preference score against reference summaries). Our 6.7B model in turn significantly outperforms our 1.3B model, suggesting that training with human feedback also benefits from scale. Additionally, both of our human feedback models are judged by humans to be superior to the human demonstrations used in the dataset.\n\nControlling for summary length. When judging summary quality, summary length is a confounding factor. The target length of a summary is implicitly part of the summarization task; depending on the desired trade-off between conciseness and coverage, a shorter or longer summary might be better. Since our models learned to generate longer summaries, length could account for much of our quality improvements. We find that after controlling for length (Appendix F), the preference of our human feedback models vs. reference summaries drops by~5%; even so, our 6.7B model summaries are still preferred to the reference summaries~65% of the time.\n\nHow do our policies improve over the baselines? To better understand the quality of our models' summaries compared to the reference summaries and those of our supervised baselines, we conduct an additional analysis where human labelers assess summary quality across four dimensions (or \"axes\") using a 7-point Likert scale [38]. Labelers rated summaries for coverage (how much important information from the original post is covered), accuracy (to what degree the statements in the summary are stated in the post), coherence (how easy the summary is to read on its own), and overall quality.  The results ( Figure 3) indicate that our human feedback models outperform the supervised baselines across every dimension of quality, but particularly coverage. Although our human labelers had a high bar for giving perfect overall scores, summaries from our 6.7B PPO model achieve a 7/7 overall score 45% of the time (compared to 20% and 23% for the 6.7B supervised baseline and reference summaries, respectively).\n\n\nTransfer to summarizing news articles\n\nOur human feedback models can also generate excellent summaries of CNN/DM news articles without any further training ( Figure 4). Our human feedback models significantly outperform models trained via supervised learning on TL;DR and models trained only on pretraining corpora. In fact, our 6.7B human feedback model performs almost as well as a 6.7B model that was fine-tuned on the CNN/DM reference summaries, despite generating much shorter summaries.\n\nSince our human feedback models transferred to CNN/DM have little overlap in summary length distribution with models trained on CNN/DM, with about half as many tokens on average, they are difficult to compare directly. Thus our evaluations in Figure 4 use a 7-point Likert scale on four quality dimensions, as in Section 4.1 (see Appendix C.5 for labeler instructions). In Figure 4b we show the average overall score at different summary lengths, which suggests our human feedback models would perform even better if they generated longer summaries. Qualitatively, CNN/DM summaries from our human feedback models are consistently fluent and reasonable representations of the article; we show examples on our website and in Appendix H.\n\n\nUnderstanding the reward model\n\nWhat happens as we optimize the reward model? Optimizing against our reward model is supposed to make our policy align with human preferences. But the reward model isn't a perfect representation of our labeler preferences, as it has limited capacity and only sees a small amount of comparison data from a relatively narrow distribution of summaries. While we can hope our reward model generalizes to summaries unseen during training, it's unclear how much one can optimize against the reward model until it starts giving useless evaluations.\n\nTo answer this question, we created a range of policies optimized against an earlier version of our reward model, with varying degrees of optimization strength, and asked labelers to compare samples from them to the reference summaries. Figure 5 shows the results for PPO at a range of KL penalty coefficients (\u03b2). Under light optimization, the models improve (according to labelers). However, as we optimize further, true preferences fall off compared to the prediction, and eventually the reward model becomes anti-correlated with human preferences. Though this is clearly undesirable, we note that this over-optimization also happens with ROUGE (see [45] and Appendix G.3). Similar behavior has been observed in learned reward functions in the robotics domain [5].\n\nHow does reward modeling scale with increasing model and data size? We conduct an ablation to determine how data quantity and model size affect reward modeling performance. We train 7 reward models ranging from 160M to 13B parameters, on 8k to 64k human comparisons from our dataset. We find that doubling the training data amount leads to a~1.1% increase in the reward model validation set accuracy, whereas doubling the model size leads to a~1.8% increase ( Figure 6).\n\nWhat has the reward model learned? We probe our reward model by evaluating it on several validation sets. We show the full results in Appendix G.6, and highlight them here. We find that our reward models generalize to evaluating CNN/DM summaries (Appendix G.7), agreeing with labeler preferences 62.4% and 66.5% of the time (for our 1.3B and 6.7B models, respectively). Our 6.7B reward model nearly matches the inter-labeler agreement value of 66.9%.\n\nWe also find that our reward models are sensitive to small but semantically important details in the summary. We construct an additional validation set by having labelers make minimal edits to summaries to improve them. Our RMs prefer the edited summaries almost as often (79.4% for 1.3B and 82.8% for 6.7B) as a separate set of human evaluators (84.1%). Further, when comparing the reference summaries to perturbed summaries where the participants' roles are reversed, our models reliably select the original summary (92.9% of the time for 1.3B, 97.2% for 6.7B). However, our RMs are biased towards longer summaries: our 6.7B RM prefers improving edits that make the summary shorter only 62.6% of the time (vs. 76.4% for humans).\n\n\nAnalyzing automatic metrics for summarization\n\nEvaluation. We study how well various automatic metrics act as predictors for human preferences, and compare them to our RMs. Specifically, we examine ROUGE, summary length, amount of copying from the post, 9 and log probability under our baseline supervised models. We present a full matrix of agreement rates between these metrics in Appendix G.7.\n\nWe find that our learned reward models consistently outperform other metrics, even on the CNN/DM dataset on which it was never trained. We also find that ROUGE fails to track sample quality as our Figure 7: Summary quality as a function of metric optimized and amount of optimization, using best-of-N rejection sampling. We evaluate ROUGE, our main reward models, and an earlier iteration of the 1.3B model trained on approximately 75% as much data (see Table 11 for details). ROUGE appears to peak both sooner and at a substantially lower preference rate than all reward models. Details in Appendix G.3. models improve. While ROUGE has~57% agreement with labelers when comparing samples from our supervised baseline models, this drops to~50% for samples from our human feedback model.\n\nSimilarly, log probability agreement with humans drops to \u226450% on comparisons between samples from our human feedback models, while our RMs still perform above chance (62%). Scaling up the size of the supervised model does not reliably improve log probability's agreement with labelers.\n\nOptimization. In Figure 7, we show that optimizing ROUGE using a simple optimization scheme doesn't consistently increase quality, as has been noted in [45]. Optimization against ROUGE peaks both sooner and at a substantially lower quality rate than optimization against our reward models.\n\n\nDiscussion\n\nLimitations. One limitation of our work is the time and cost required to produce our final models. Notably, fine-tuning our 6.7B model with RL required approximately 320 GPU-days. Our data collection procedure is also expensive compared to prior work -the training set took thousands of labeler hours and required significant researcher time to ensure quality. For this reason, we were unable to collect baselines such as an equivalent amount of high-quality human demonstrations for supervised baselines. See D for more discussion. We leave this ablation to future work. Nevertheless, we believe reward modeling is more likely to scale to tasks where it is extremely skill-intensive or time-consuming to provide good demonstrations.\n\nFuture directions. The methods in this paper could be applied to any task where humans can compare samples, including dialogue, machine translation, question answering, speech synthesis, and music generation. We expect this method to be particularly important for generating long samples, where the distributional shift and degeneracy of maximum likelihood samples can be problematic. It may be possible to improve sample efficiency by training to predict feedback across many tasks [42].\n\nWe are particularly interested in scaling human feedback to tasks where humans can't easily evaluate the quality of model outputs. In this setting, it is particularly challenging to identify whether an ML system is aligned with the human designer's intentions. One approach is to train ML systems to help humans perform the evaluation task quickly and accurately [9].\n\nThere is also a rich landscape of human feedback methods beyond binary comparisons that could be explored for training models [28,17,44,64]. For example, we could solicit high-quality demonstrations from labelers, have labelers edit model outputs to make them better, or have labelers provide explanations for why they preferred one model output over another. All of this feedback could be leveraged as a signal to train more capable reward models and policies.\n\nBroader impacts. The techniques we explore in this paper are generic techniques that could be used in a wide variety of machine learning applications, for any task where it is feasible for humans to evaluate the quality of model outputs. Thus, the potential implications are quite broad.\n\nOur research is primarily motivated by the potential positive effects of aligning machine learning algorithms with the designer's preferences. Many machine learning applications optimize simple metrics which are only rough proxies for what the designer intends. This can lead to problems, such as Youtube recommendations promoting click-bait [11]. In the short term, improving techniques for learning from and optimizing human preferences directly may enable these applications to be more aligned with human well-being.\n\nIn the long term, as machine learning systems become more capable it will likely become increasingly difficult to ensure that they are behaving safely: the mistakes they make might be more difficult to spot, and the consequences will be more severe. For instance, writing an inaccurate summary of a news article is both easy to notice (one simply has to read the original article) and has fairly low consequences. On the other hand, imitating human driving may be substantially less safe than driving to optimize human preferences. We believe that the techniques we explore in this paper are promising steps towards mitigating the risks from such capable systems, and better aligning them with what humans care about.\n\nUnfortunately, our techniques also enable malicious actors to more easily train models that cause societal harm. For instance, one could use human feedback to fine-tune a language model to be more persuasive and manipulate humans' beliefs, or to induce dependence of humans on the technology, or to generate large amounts of toxic or hurtful content intended to harm specific individuals. Avoiding these outcomes is a significant challenge for which there are few obvious solutions.\n\nLarge-scale models trained with human feedback could have significant impacts on many groups. Thus, it is important to be careful about how we define the 'good' model behavior that human labelers will reinforce. Deciding what makes a good summary is fairly straightforward, but doing this for tasks with more complex objectives, where different humans might disagree on the correct model behavior, will require significant care. In these cases, it is likely not appropriate to use researcher labels as the 'gold standard'; rather, individuals from groups impacted by the technology should be included in the process to define 'good' behavior, and hired as labelers to reinforce this behavior in the model.\n\nWe chose to train on the Reddit TL;DR dataset because the summarization task is significantly more challenging than on CNN/DM. However, since the dataset consists of user-submitted posts with minimal moderation, they often contain content that is offensive or reflects harmful social biases. This means our models can generate biased or offensive summaries, as they have been trained to summarize such content. For this reason, we recommend that the potential harms of our models be thoroughly studied before deploying them in user-facing applications.\n\nFinally, by improving the ability of machine learning algorithms to perform tasks that were previously only achievable by humans, we are increasing the likelihood of many jobs being automated, potentially leading to significant job loss. Without suitable policies targeted at mitigating the effects of large-scale unemployment, this could also lead to significant societal harm.   Here, we discuss the pre-processing steps that we apply to the TL;DR dataset. We first remove all duplicate posts by checking the text body, finding that there are nearly 20,000 exact duplicates. We then re-parse the TL;DR carefully using a set of heuristics, and filter to use only top-level posts (rather than comments). We also filter out any post that is from a subreddit not in our 'subreddit whitelist' (see Table 2 for the distribution over subreddits), any post where the title starts with some variant of 'Edit' or 'Update', 10 and posts that contain certain topics (such as graphic sex or suicide) using heuristics. Finally, to ensure the posts are short enough to fit into the context length of our models, we filter out any post whose body is longer than 512 tokens. This resulted in a set of 287,790 posts filtered by body but not summary, of which we hold out approximately 5% as a validation set. We used this set of posts for RL training since our RL procedure does not require reference summaries.\n\nWe next perform additional filtering on the parsed reference summaries that we use for training our supervised baselines. Specifically, we remove summaries where the TL;DR starts with variants of 'Edit', 'Update', or 'P.S.', we heuristically remove summaries with certain levels of profanity, and we remove summaries that are less than 24 tokens or more than 48 tokens. As discussed in Section 4.1, since our RL models tend to generate summaries on the upper end of the allowed length limit, this length filtering ensures that there is enough length overlap between the RL summaries and reference summaries for us to perform a length-controlled analysis. Additionally, we found that summaries shorter than 16 tokens were usually of low quality. We later verified that the summaries we filtered out were lower quality according to our reward model -more than 0.5 nats worse on average (i.e. they are predicted to be exp(0.5) \u2248 1.6 times less likely to be preferred). Our final TL;DR dataset contains 123,169 posts including summaries, again with about 5% held out as a validation set. We use 1913 of these validation articles for model selection during development; the evaluations in this paper exclude these articles.\n\nNote that, from Table 2 we can see that about two thirds of our TL;DR dataset consists of posts relating to relationships or relationship advice, which is a fairly specific domain. This raises potential concerns about the generality of our models, though their strong transfer performance on CNN/DM news articles suggests they are not unreasonably specialized to relationship advice.  Table 3: Hyperparameters for our models of various sizes. Figure 8: The sweep we conducted for determining our sampling procedure, varying the temperature and the 'top p' value for nucleus sampling. While we didn't do a large enough test to determine whether nucleus sampling is better or worse than moderate-temperature sampling, we found that very low temperature sampling is better than both on this task.\n\n\nB Further model training details B.1 Hyperparameters\n\nAll models follow the standard Transformer architecture, with 2048 learned position embeddings. All models are trained with fp16 activations and the Adam optimizer [31]. Nearly all supervised baselines, reward models, and reinforcement learning models are trained with fp32 weights; the exception is our TL;DR supervised baselines, which were trained with fp16 weights. 11 All models are trained with the same byte-pair encoding as in [48].\n\nDuring pretraining, the models were trained to predict the next token on a large text corpus consisting of Commoncrawl, Webtext [48], books, and Wikipedia. Training lasts between 1-3 epochs on each, for a total of 200-300 billion tokens. Learning rate follows a cosine schedule, with a short warmup, decaying to 10% of the maximum value. The batch size ramped up throughout training to some maximum, with each input having 2048 tokens. Hyperparameters for each model are shown in Table 3.\n\nFor supervised baselines, we initialize models from the pretrained models. We decay the learning rate with a cosine schedule, using an initial learning rate chosen from a log linear sweep of at least 7 values. This resulted in learning rates of 6.35e-5, 5.66e-5, 2.83e-5, and 2.83e-5 for our TL;DR models of size 1.3B, 3B, 6.7B, and 13B respectively, and a learning rate of 2.38e-5 for our CNN/DM 6.7B model. We use a batch size of 128, and run for a single epoch.\n\nFor reward modeling, we initialize to the supervised baseline, but with a reward head on top with weights initialized according to N (0, 1/(d model + 1)) [20]. We train for one epoch, decaying the learning rate with a cosine schedule, using an initial learning rate chosen from a log linear sweep of at least 7 values. We also sweep over between 3 and 10 seeds, and choose the reward model that performs best on the development portion of the validation set, as we find that both the data iteration order and reward head initialization affect results [13]. For our main results, the 1.3B and 6.7B reward models had learning rates of 1.5e-5 and 5e-6, respectively. We use a batch size of 64, and run for a single epoch.\n\nFor PPO, we run with separate policy and value networks, initializing our policies to the supervised baseline, and our value functions to the reward model. We set \u03b3 = 1 and \u03bb = 0.95 for the advantage estimation [57] and do 4 epochs of optimization for each batch of rollouts. We used a linear learning rate decay schedule, with initial learning rates of 1.5e-5 for the 1.3B model and 7e-6 for the 6.7B model, based on small amounts of experimentation and rough model size extrapolation. We used a KL coefficient of 0.05 for both of the main runs we report results for (except when we explicitly vary this value in the reward model optimization graphs). We use a batch size of 512 for the 1.3B model and 256 for the 6.7B model, and run for 1 million episodes.\n\n\nB.2 Input format\n\nOur model always receives a byte-pair encoded string of a fixed size. When the input is too small, we pad from the beginning of the input with a padding token, and if the input is too long we truncate the post/article field at newlines to stay under the limit.\n\nWhen sampling from models pretrained only on our pretrain mixture and not fine-tuned on TL;DR, we follow [48] and instead of padding with a padding token, we pad the beginning of the context with examples of posts/articles and high-quality summaries. We use as many examples as will fit in the token limit, with the examples formatted the same way as the main input. Table 4 documents the formats we used (with pythonic format strings).\n\n\nC Human data collection details C.1 Process for ensuring high-quality human data\n\nWe first detail the procedures we use to ensure high-quality data. While these procedures became more rigorous over the course of the project, they generally involved four steps.\n\nStep 0: Understanding the task ourselves. To understand the task, we first do many summary comparisons ourselves. We also hire a small number of human labelers 12 to do comparisons, and discuss our disagreements. We then draft instructions for a larger set of human labelers.\n\nStep 1: Labeler onboarding. Labelers are hired from Upwork, a freelancing platform, as well as two labeling services, Scale and Lionbridge. Labelers first complete a (paid) training process where they label summaries on a shared set of data. For some comparisons, labelers get immediate feedback about which summary was chosen by us, and why, to help them calibrate. We retain labelers that pass a minimum threshold for speed and agreement with us. To allow for a customizable labeler interface, we built our own website for data collection (see Appendix C.4).\n\nStep 2: Collecting comparison data. Next, we have labelers evaluate a large batch of comparisons on our website, which generates the bulk of our data. Before comparing two summaries directly, we have labelers write their 'naive interpretations' of summaries without seeing the original post. We've found this helpful for evaluating summaries, as they surface points of ambiguity in the summary that might not have been detected if the summary was read after the original post. After doing naive interpretations, labelers do comparisons by assigning a value on a 9-point scale for how confident they are that summary A is better than summary B (or the converse).\n\nStep 3: Providing labeler feedback. After collecting the comparison data, we can look at agreement rates between labelers. While most comparisons are only given to a single labeler, each labeler gets about 10-20% questions from a shared pool for calibration purposes. We can both attempt to use these statistics as crude measures of quality, and show cases of disagreements to workers to help them improve their labels.\n\nStep 4: Researcher comparison calibrations. We occasionally also do the task ourselves, to measure agreement rates between each labeler and us. This is used for quality assessment (see C.2). We also calculate per-labeler \"high confidence\" thresholds, by finding the confidence value on the Likert scale for each labeler such that we expect labels above this threshold to agree with us 80% of the time on average. For the purposes of reward model selection, we filter the validation set to contain only these higher confidence labels. For the entire process, we keep a high communication bandwidth with labelers: we use a shared chat room for labelers to ask clarifying questions and discuss difficult comparisons amongst themselves, host office hours, and occasionally have one-on-one video calls with labelers to discuss points of disagreement.\n\nWe keep good labelers throughout the lifetime of the project, while firing the lowest-performing workers.\n\n\nC.2 Assessing human feedback quality\n\nWe assess labeler accuracy by comparing the labeler's preferred summary with the summary we prefer (ignoring the confidence level). We exclude comparisons where either the labeler or researcher expresses indifference. This gives us an agreement rate, in theory ranging from 0% (perfect disagreement) to 100% (perfect agreement). For our 2-way comparisons, a random labeler would get 50% agreement.\n\nTo obtain our main number comparing labeler-researcher to researcher-researcher agreement, we restrict ourselves to comparisons between summaries from our 1.3B supervised baseline, because this subset of the data has the most researcher-labeled data. On this subset, labelers agree with researchers 77% \u00b1 2% of the time, while researchers agree with each other 73% \u00b1 4% of the time. We believe substantial noise comes from comparisons being quite difficult and subjective.\n\nIn general, agreement rates range from about 65% for the least proficient labelers and most difficult comparisons (comparing two high-temperature samples from a single RL policy) to about 85% for (a) (b) Figure 9: (a) The website we made to collect data from labelers. (b) Naive interpretations of summaries on the website. the most proficient labelers and easiest comparisons (comparing a high-temperature sample from a supervised baseline to the reference summary). Averaging over all workers, weighted by their volume, gives us an estimated agreement rate of 73% \u00b1 3% for our reward model training corpus.\n\nLabelers agree with each other 72% of the time in the training corpus. This suggests we could get more reliable labels by aggregating labels from multiple workers on the same comparison. Indeed, on the subset of the training data for which we have enough shared comparisons, taking the modal label from 3 labelers increases their agreement rate with researchers from 72% to 77%. However, we usually collect only one label per comparison, in order to maximize label throughput.\n\nOn the evaluations for Figure 1, labelers agreed with researchers 73% \u00b1 3% of the time, and labelers agreed with each other 73% \u00b1 2% of the time.\n\nAgreement rate between researchers ranged from about 65% on the most difficult comparisons (comparing two high-temperature samples from a single RL policy), to about 80% on the easiest comparisons (comparing a high-temperature sample from a supervised baseline to the human reference summary), to about 95% in cases where we discussed the comparisons with each other.\n\nOverall we believe that quality is fairly high. Our attempts to filter data generally hurt reward model accuracy. For example, using the confidence thresholds mentioned above, we found that while lower-confidence labels were less useful than high-confidence labels for improving reward model accuracy, they were still better to include than to omit. Similarly, leaving out workers with poorer agreement rates did not help.\n\n\nC.3 Labeler demographics\n\nWhen training machine learning models with human feedback, the humans providing the feedback are essential in reinforcing the desired model behavior. If we are to scale human feedback to train models on more complex tasks, where humans might disagree about what the desired model behavior should be, it's important for members of groups that will be impacted by the model to be included in the labeler population.\n\nTo provide more transparency into our labeler demographics, we provide results from a survey given to our labelers in Table 5. The survey was optional, anonymous, and it was made clear that the results would not affect hiring or firing decisions. We find that our labelers span a range of ethnicities, nationalities, ages, and genders, and educational backgrounds, but are more likely to be White and American.\n\n\nC.4 Labeler website\n\nSince we hired and trained our own set of labelers, rather than using a crowdsourcing website such as Amazon Mechanical Turk, we built our own website to allow for a standardized, customizable user interface for all labelers. Each labeler created a separate profile, allowing us to assign different sets of comparisons to different labelers. The website contains different renderers for different kinds  Table 5: Demographic data from 21 of our labelers who participated in our voluntary survey. of questions, including naive interpretations, summary comparisons, and Likert evaluations along different axes, along with room for labelers to express concerns with the question or explanations for their decision. Screenshots from the website are shown in Figure 9. Data collected from the website can be easily ported into a central database containing all of our human data.\n\n\nC.5 Instructions for labelers\n\nHere we provide more detail on the specific instructions given to labelers for comparing summaries, and for doing Likert evaluations of summaries along axes of quality. We produced separate sets of instructions for evaluating Reddit posts, and for evaluating CNN/DM news articles. For Reddit instructions, we first describe Reddit in general and provide a table that translates Reddit-specific lingo into common parlance.\n\nInstructions for comparing summaries. We show an excerpt of the instructions given to labelers for making comparisons in Table 6. In addition to these instructions, we provide an example labeled comparison between Reddit summaries, and also example naive interpretations for summaries.\n\nInstructions for evaluating summaries along axes of quality. We provide a separate set of detailed instructions for labelers for the 7-point Likert evaluations. We first introduce each of the 4 axes of quality we consider, giving an overview of coherence, accuracy, coverage, and overall score (shown in Table 7). We also provide a brief rubric for giving scores of 1, 4, and 7, along with several Reddit summaries annotated with our own judgments of quality along each of these axes (with explanations).\n\nWhat makes for a good summary? Roughly speaking, a good summary is a shorter piece of text that has the essence of the original -tries to accomplish the same purpose and conveys the same information as the original post. We would like you to consider these different dimensions of summaries:\n\nEssence: is the summary a good representation of the post?\n\nClarity: is the summary reader-friendly? Does it express ideas clearly?\n\nAccuracy: does the summary contain the same information as the longer post?\n\nPurpose: does the summary serve the same purpose as the original post?\n\nConcise: is the summary short and to-the-point?\n\nStyle: is the summary written in the same style as the original post?\n\nGenerally speaking, we give higher weight to the dimensions at the top of the list. Things are complicated though -none of these dimensions are simple yes/no matters, and there aren't hard and fast rules for trading off different dimensions. This is something you'll pick up through practice and feedback on our website. Table 6: An excerpt from the instructions we gave to labelers for doing comparisons.\n\nFinally, we provide a FAQ section that answers common questions raised by the small initial set of labelers we assigned to this task.\n\nFor CNN/DM, we provide the same set of instructions, except we add some additional clarifications for how to judge news articles. We specifically ask labelers to place less emphasis on fluidity of sentences (because the reference summaries were originally written in bullet-point form, and we didn't want labelers to penalize this), and to place less emphasis on the summary matching the intent of the article (which was important for Reddit summaries).\n\nIn terms of quality control, we conducted a smaller version of the quality control process described in Appendix C.1: we first labeled a small set of summaries ourselves along each axis to understand points of confusion, then we wrote the instructions document to provide to labelers, then we had a small number of labelers do a trial of the task to catch any remaining bugs or points of confusion, and finally we onboarded a larger set of labelers onto the task while remaining available to answer any questions.\n\n\nC.6 Composition of the labeled dataset\n\nOver the course of the project, we trained several reward models and policies. Each batch of summaries that we sent to the labelers were sampled from a variety of policies. We didn't have a systematic plan for which policies to sample from; rather, we chose what seemed best at the time in the spirit of exploratory research. Every time we trained a reward model, we trained on all labels we had collected so far. Successive models also benefited from improved hyperparameters and dataset cleaning. Our results could likely be replicated with a simpler, more systematic approach.\n\nIn general, as we hire new labelers and as existing labelers perform the task more, it is possible that there is 'labeler drift', where the set of criteria used by labelers to evaluate summaries gradually shifts over time. This could lead to a regression in labeler-researcher disagreement, or lead to some policies becoming more or less preferred over time. To help guard against this, in most batches we include comparisons between samples from our supervised baseline and reference summaries, and measure the frequency with which the workers prefer one over the other. If this number drifts over time, it's an indication that our workers' preferences are also changing. However, we generally found that this preference number stayed relatively constant, within noise. Table 8 lists the policies we trained by supervised finetuning on the TL;DR dataset, as well as the reward models, trained on successively larger datasets of human labels. Table 9 lists the RL policies.\n\n\nCoherence\n\nFor this axis, answer the question \"how coherent is the summary on its own?\" A summary is coherent if, when read by itself, it's easy to understand and free of English errors. A summary is not coherent if it's difficult to understand what the summary is trying to say. Generally, it's more important that the summary is understandable than it being free of grammar errors.\n\n\nRubric:\n\nScore of 1: The summary is impossible to understand. Score of 4: The summary has mistakes or confusing phrasing that make it a bit hard to understand. Score of 7: The summary is perfectly clear. \n\n\nAccuracy\n\n\nOverall quality\n\nFor this axis, answer the question \"how good is the summary overall at representing the post?\" This can encompass all of the above axes of quality, as well as others you feel are important. If it's hard to find ways to make the summary better, give the summary a high score. If there are lots of different ways the summary can be made better, give the summary a low score.\n\n\nRubric:\n\nScore of 1: The summary is terrible. Score of 4: The summary is an okay representation of the post, but could be significantly improved. Score of 7: The summary is an excellent representation of the post.  Table 10: Best-of-N policies. KL divergence is computed analytically as KL(boN, sup) = log N -(N-1)/N. We also explored a simple alternative to reinforcement learning: Sample N summaries from a supervised baseline at temperature 0.7, score them with a reward model, and take the summary with the highest score. This best-of-N (BoN) procedure is effectively a mildly optimized policy requiring no training. These policies are named in Table 10, and samples from them form part of the training data. Table 11 lists the source policies for the training data for each reward model. Reward model Policy0  Policy1   rm1  ref  sup1  5404  sup1  sup1  5386  rm2  ref  sup1  5404  sup2  12779  sup2 bo8 rm1  1426  sup3_6b  1424  sup1  sup1  5386   Continued on next page  Label count  Reward model Policy0  Policy1   sup4 bo128 rm3  sup4 bo128 rm3  288  sup4 bo256 rm3  582  sup4 bo128 rm3_6b sup4 bo128 rm3_6b  95  sup4 bo256 rm3_6b  203  sup4 bo512 rm3  sup4 ppo rm3 3  216  sup4_6b  60  sup4 bo64 rm3  sup4 ppo rm3 2  218  sup4_6b  55  sup4 bo8 rm3 sup4 ppo rm3 1 752 sup4 ppo rm3 1 sup4 ppo rm3 1 372 sup4 ppo rm3 2 sup4 ppo rm3 2 4256 sup4_6b 215 sup4 ppo rm3 3 sup4 ppo rm3 3 4037 sup4_6b 216 Table 11: Training data for reward models. \"ref\" refers to human reference summaries.\n\n\nLabel count\n\n\nC.7 Example comparison tasks\n\nTo give a sense of the difficulty of the comparisons task, we provide example comparisons between two summaries generated by our 6.7B human feedback model. In Table 12 we show both a random comparison drawn from the TL;DR dataset, and a cherry-picked comparison (selected from 10 comparisons where labelers disagreed) to illustrate the trade-off between accuracy in coverage that can occur when labelers conduct evaluations. I'm sure it's not of the norm for me to feel so disassociated about the whole thing, but I am. I'm suppose to go look at wedding dresses this Friday. I am feeling super anxious because I don't know if trying on wedding dresses is going to turn me into a blubbering baby about not having a mom. My future mother-in-law is suppose to come with me to help look. I worry about turning into that blubbering baby and offending her. I don't want her thinking that I don't appreciate her being there.\n\nAside from me worrying about becoming a giant baby, I've also been having issues with my bridal party. While I haven't made any official choices, I have ideas of who I want involved. That would be my best friend, my sister, and my future sister-in-law. My first choice for a MOH is my best friend. However, she lives out of state, and is in a medical program for school. So her visit time is severely limited. My sister feels entitled to be the MOH, despite the fact that we are not close at all. So getting people together to get any kind of wedding stuff done is almost impossible. Summary A: I'm having doubts about whether or not to try on wedding dresses. I am also having doubts about my bridal party's ability to get things done. Summary B: I think I'm going to turn into a blubbering baby and offend my mother-in-law. \n\n\nD Choice of baselines\n\nIn testing our human feedback techniques, we collected a large amount of high-quality data from human labelers. In order to compare fairly against supervision-based techniques, we would have needed to spend a similar amount of labeler time collecting high quality demonstrations, and used those to fine-tune a model via supervised learning. Because this is prohibitively expensive, we do not provide such a baseline.\n\nExisting prior work such as PEGASUS [70] has studied supervised methods on a dataset very similar to ours (the /r/tifu subset of TL;DR). However, they use much smaller (500M parameters) models, and report that their model outputs are worse than the human reference summaries, according to human evaluations. Thus, due to our limited labeler budget for evaluation, we decided to use our own supervised and zero-shot models as baselines (after sanity-checking the ROUGE performance of our supervised models), as well as T5 [49].\n\nT5 models [49] are pretrained and fine-tuned in a similar way to our supervised baselines, but they use an encoder-decoder architecture. We used T5 outputs which were obtained via beam search decoding, as described in [49]. We also carefully account for differences in tokenization between model outputs. 13\n\n\nE CNN/DM lead-3 vs reference summaries\n\nOn the CNN/DM dataset, our labelers significantly preferred lead-3 (a summary consisting of the first 3 sentences of the article) to reference summaries. In part this is due to longer summaries receiving higher coverage scores and lead-3 being 50% longer, as shown in Table 13.\n\nPolicy Length (stdev) Quality Quality increase / 100 char. ref 314 (119) 5.54 0.14 lead-3 475 (114) 6.23 0.34 Table 13: How length affects overall quality on CNN/DM for lead-3 and reference summaries.\n\nHowever, if we use a linear regression (similar to the procedure in Appendix F) to predict what lead-3 performance would be if its average length were reduced to 314 characters, we still find a quality of 5.68, modestly higher than the reference summaries. Moreover, for lead-3 to even achieve parity with the reference summaries seems to call into question the need for abstractive summarization or sophisticated ML methods, since a simple extractive baseline can match a perfect imitation of the reference summaries.\n\nWe wanted to understand labeler behavior on these comparisons, to ensure that it was not an error. To do this, we examined a sample of our labeler's judgments ourselves. We found that in 20/143 cases labelers preferred lead-3 by 3 points or more, and that excluding these datapoints would raise the relative score of the reference summaries by about 0.5 points. 14 We were surprised to see the reference summaries performing so poorly in a significant fraction of cases, so we looked at labeler's explanations and confirmed they made sense.\n\nWe found that two features of the reference summaries explained most of its underperformance. First, 13 of these 20 summaries omitted one of the key points from the article-the highlights are often written for a reader who had already seen the title of the article, even though the titles are not included in the CNN/DM dataset. Second, 10 of these 20 summaries actually introduced new information not present in the original article. From the perspective of labelers this information is totally confabulated and so led to lower scores. A likely explanation for these errors is that the reference summaries are extracted from \"highlights\" on the news sites rather than being a straightforward summary of the article. These failures are common enough that they significantly impact the average quality of the reference summaries, and the effects seem to be large relative to quality differences between ML models.\n\nOverall we believe that labeler judgments were reasonable in these cases, and that it is potentially problematic to treat the \"highlights\" in the CNN/DM dataset as reference summaries. You can view all of our labeler's judgments on CNN/DM at our website.  Figure 1, using the procedure described in Appendix F. Controlling for length reduces the relative preference of our human feedback models, however they are still preferred to the reference summaries. (b) Plotting model quality for different summary lengths on the TL;DR dataset. Our 6.7B human feedback model outperforms both the 6.7B supervised baseline and the reference summaries (horizontal line at 0.5) across lengths.\n\n\nF Controlling for summary length\n\nAs discussed in Section 4.1, the length of a summary is a confounding factor for evaluating summary quality; depending on the desired trade-off between conciseness and coverage, a shorter or longer summary might be better. Our models generate summaries that are longer than the reference summaries, as this led to higher labeler preference given the 24-48 token limit for our task. Here we describe the procedure we use to attempt to control for length.\n\nTo calculate a single length-controlled preference number, we train a logistic regression model to predict the human-preferred summary on our dataset of human comparisons. We provide this model with 2 features: the identity of each policy, and the log ratio of the summary lengths. To calculate the length-controlled preference value between two policies, we simply give each policy ID to our trained logistic regression model and set the log length ratio to zero (see Figure 10a). In Figure 10b we examine summary quality across a range of summary lengths on TL;DR. We find that our human feedback model outperforms the supervised baseline across all length values.\n\nFor CNN/DM, we use a similar procedure as described above to control for length, except using a linear regression model to predict the Likert rating from 1-7. We show the expected quality increase for making summaries 100 characters longer in Table 14, which suggests our human feedback models would perform better if they generated longer summaries.  G Additional results\n\n\nG.1 Value function ablation\n\nIn this section, we conduct an ablation comparing using separate parameters for the value function and policy, against using a shared network as done in [73]. The results, shown in Figure 11, clearly indicate that using separate networks outperforms the latter. On the other hand, having separate networks increases the memory requirements of running RL fine-tuning. Having separate networks also allows us to initialize the value function to be the learned reward model that is being optimized. Figure 11: Comparing the reward obtained by optimizing with separate value function and reward model parameters to shared parameters.\n\n\nG.2 Evaluating policies along axes of quality\n\nWe show the full results of the evaluations of policies on a 7-point Likert scale along different axes of quality; for TL;DR this is shown in Figure 12, and for CNN/DM this is shown in Figure 13. It is evident that on both datasets coverage correlates strongly with overall score across models, and all models achieve a high coherence score.\n\n\nG.3 Studying best-of-N optimization\n\nA natural way to evaluate an automatic evaluation metric is to see the extent to which optimizing against it leads to high performance according to humans. One way to assess this is to use best-of-N as an (inefficient) optimization technique -this has the benefits of being simple and invariant to monotonic transformations. We report results for up to best-of-2048 on ROUGE and three of our reward models in Figure 7, using samples from the 1.3B supervised baseline. The results suggest that optimizing against ROUGE significantly under-performs optimizing against our reward models. The data also suggests ROUGE degrades with too much optimization much faster than our reward models.\n\nWith increasing N, the best-of-N policies get higher average reward. Similarly, by decreasing the KL coefficient \u03b2, the PPO policies get higher average reward. We found that at a given average reward, the best-of-N and PPO policies have similar quality as judged by human labelers (not shown). However, the PPO policy is farther from the supervised baseline than best-of-N is, as measured by the KL divergence. 15\n\n\nG.4 ROUGE scores\n\nIn Figure 14a and 14b, we show the ROUGE scores of our models on the TL;DR and CNN/DM datasets, respectively. We report results with T=0, consistent with our human evaluations. We found that temperature has an (often significant) impact on ROUGE score, and we did a thorough sweep to verify that the best temperature setting is T=0.   Table 15: Comparing the ROUGE score of our 6.7B supervised model on CNN/DM to recent SOTA models from the literature. Without any summarization-specific engineering, our model achieves ROUGE scores better than SOTA models from mid-2019, indicating that it is a strong baseline for comparison. On TL;DR, we find that our human feedback models obtain a slightly lower ROUGE score than the supervised models at T = 0, further indicating that ROUGE correlates poorly with human preferences. For supervised models, lowering temperature has a larger impact than increasing model size. Interestingly, at higher temperatures, our feedback models actually outperform supervised counterparts (not shown).\n\nOn CNN/DM, ROUGE agrees with our human evaluations that our human feedback models transfer better than our supervised models. However, unsurprisingly, supervised CNN/DM models still achieve much higher ROUGE. In Table 15, we show the ROUGE results on CNN/DM for our 6.7B supervised baseline and various models from the literature. We find that our model achieves ROUGE scores less than T5 [49], but slightly greater than the CNN-2sent-hieco-RBM model from [71], which was SOTA for abstractive summarization on CNN/DM in mid-2019 according to the NLP-progress leaderboard. 16 \n\n\nG.5 Bigram overlap statistics\n\nIn Table 16, we show the bigram overlap statistics for our models on the TL;DR and CNN/DM datasets as a proxy for how much the summaries copy frmo the post. As in Section 4.4, we compute the longest common subsequence of bigrams with the original Reddit post or news article, and dividing by the number of bigrams in the summary. We find that models evaluated on CNN/DM  (whether or not they were trained on CNN/DM) generally copy more than models evaluated on TL;DR. Further, our supervised and human feedback models copy less than our pretrained models.\n\n\nG.6 Reward model validation sets\n\nIn this section, we report results evaluating our reward models on various manually constructed validation sets, shown in Tables 17 and 18. Notably, we asked our humans to produce a small dataset of edits, by having them make improvements to existing summaries (either reference summaries or supervised baseline summaries). Our 6.7B reward model prefer the improved summaries at a similar rate to humans (who do not know which summary has been edited).\n\nOur reward models are also sensitive to sentence shuffling (whereas metrics like ROUGE are largely not), and are able to detect when the roles portrayed in the summary have been switched. On the other hand, our reward models sometimes exhibit preference for poor artificial summaries, such as  Table 18: Reward model performance on various manually constructed validation sets. In all cases, Summary A is intended to be better than Summary B, and thus a higher preference % is generally better. 'rand-3' indicates a baseline where 3 random sentences are taken from the post; however these sentences are kept in the order in which they appear in the post. 'Original summary' is either the reference summary or a summary from our supervised baselines. r/tifu is a subreddit whose purpose is sharing embarrassing stories (not asking for advice).\n\nthe post title copied twice, or asking for advice at the end of the summary. In Table 19, we show examples where our model is sensitive to small, semantically meaningful changes in the summary.\n\n\nG.7 Measuring agreement between different evaluation metrics\n\nWe are interested in understanding the relationship between different metrics for evaluating summaries. To do this, we compute agreement between various metrics, including automatic metrics and humans, for different subsets of the data for which we have human evaluations. To remove policy quality as a confounding variable, all of the summary comparisons are generated by the same policy at the same temperature value. In Table 20, we use samples from our 1.3B supervised model at T=0.7 on TL;DR; Table 21 has comparisons from our 6.7B supervised model at T=0.7 on TL;DR; Table 22 has comparisons from our 6.7B human feedback model at T=0.7 on TL;DR; and Table 23 has comparisons from our 6.7B supervised baseline trained on CNN/DM. Our 6.7B reward model generally agrees with labelers as much as other labelers, although an ensemble of labelers does better. On the other hand, ROUGE generally has poor agreement, as does log probability under the supervised baselines, with simple heuristics like copying (longest common subsequence of bigrams with the article) and length often performing comparably.\n\n\nEdited summary\n\nReward \u2206 Crush on girl I haven't seen in 4 years. She doesn't like me and I don't still like her. What do?\n\n\n+0.64\n\nA girl told me she loved liked me, she ended up picking another guy over me, that guy badly influenced her, and now I'm here alone thinking what could've been.\n\n+0.82 I tried to show my friend a picture of my tarantula and she smashed my phone with all her might and now I lost a good friend phone.\n\n-0.64\n\nBoyfriend still FB stalks his high school ex girlfriend from time to time and told me when he was very drunk that she was his first love.\n\n\n+0.73\n\nI've become pathetic, pining after a guy my ex. Would like to reach state of less pathetic. If more info is necessary, please let me know.\n\n\n+0.69\n\nI have body issues (body acne/scarring and weight issues) that prevent me from having a normal life without shame and prevent me from having a better sex life with my BF.\n\n\n+1.0\n\nDo you take someone back after they've turned you down off, even if you can't see them in person or are they just not worth the risk? +0.52 Table 19: Qualitative examples showing the change in reward of the reward model on humangenerated edits to TL;DR summaries that make the summaries better. Examples are randomly selected from the set where the edit distance was less than 5 and the magnitude of change in reward was greater than 0.5. Text in strike-through was removed from the original summary in the edit, and text in bold was added. The reward model is sensitive to small but semantically meaningful changes in the summary, although it makes errors on occasion.     \n\nFigure 2 :\n2Diagram of our human feedback, reward model training, and policy training procedure.\n\nFigure 4 :\n4Transfer results on CNN/DM. (a) Overall summary quality on CNN/DM as a function of model size. Full results across axes shown in Appendix G.2. (b) Overall scores vs. length for the 6.7B TL;DR supervised baseline, the 6.7B TL;DR human feedback model, and T5 fine-tuned on CNN/DM summaries. At similar summary lengths, our 6.7B TL;DR human feedback model nearly matches T5 despite never being trained to summarize news articles.\n\nFigure 3 :\n3Evaluations of four axes of summary quality on the TL;DR dataset.\n\nFigure 5 :Figure 6 :\n56Preference scores versus degree of reward model optimization. Optimizing against the reward model initially improves summaries, but eventually overfits, giving worse summaries. This figure uses an earlier version of our reward model (see rm3 in Appendix C.6). See Appendix H.2 for samples from the KL 250 model. Reward model performance versus data size and model size. Doubling amount of training data leads to a~1.1% increase in reward model validation accuracy, whereas doubling the model size leads to a~1.8% increase. The 6.7B model trained on all data begins approaching the accuracy of a single human.\n\n\nthis axis, answer the question \"does the factual information in the summary accurately match the post?\" A summary is accurate if it doesn't say things that aren't in the article, it doesn't mix up people, and generally is not misleading. If the summary says anything at all that is not mentioned in the post or contradicts something in the post, it should be given a maximum score of 5. (If you are confused about how to use '6', see the FAQ!)Rubric:Score of 1: The summary is completely wrong, made up, or exactly contradicts what is written in the post. Score of 4: The summary says at least one substantial thing that is not mentioned in the post, or that contradicts something in the post. (Score of 5: The summary says anything, no matter how small, that is not mentioned in the post, or that contradicts something in the post.) Score of 7: The summary has no incorrect statements or misleading implications.CoverageFor this axis, answer the question \"how well does the summary cover the important information in the post?\" A summary has good coverage if it mentions the main information from the post that's important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g. to ask for advice).Rubric:Score of 1: The summary contains no information relevant to the post. Score of 4: The summary is missing at least 1 important piece of information required to understand the situation. Score of 7: The summary covers all of the important information required to understand the situation.\n\nFigure 10 :\n10(a) A length-controlled version of\n\nFigure 12 :\n12Evaluating TL;DR policies on a 7-point Likert scale along several axes of quality.\n\nFigure 13 :\n13Evaluating CNN/DM policies on a 7-point Likert scale along several axes of quality.\n\nFigure 14 :\n14ROUGE scores for our models on (a) the TL;DR dataset, and (b) the CNN/DM dataset.\n\n\nminutes, and then what? I'm a bit scared. Should I bring something (the weather, although no rain and sunny, is not super so no swimming), should we do something. I'd like all the tips I can get. Thanks! First date after 3 years in a relationship, going to the beach, terrified. What to bring with me, what to do? Going on a date with a girl I met a few days ago, going to the beach. What should I bring, what should we do? Going on my first ever date tomorrow, cycling to the beach. Would like some tips on what to do and bring. I'm a bit nervous and excited. Thanks! Table 1: Example of post and samples on the TL;DR dataset, chosen to be particularly short. For random samples (along with posts), see Appendix H and our website.Human written reference TL;DR \n6.7B supervised model \n6.7B human feedback model \n\n\n\nUifalean, Morris Stuttard, Russell Bernandez, Tasmai Dave, Rachel Wallace, Jenny Fletcher, Jian Ouyang, Justin Dill, Maria Orzek, Megan Niffenegger, William Sells, Emily Mariner, Andrew Seely, Lychelle Ignacio, Jelena Ostojic, Nhan Tran, Purev Batdelgar, Valentina Kezic, Michelle Wilkerson, Kelly Guerrero, Heather Scott, Sarah Mulligan, Gabriel Ricafrente, Kara Bell, Gabriel Perez, and Alfred Lee. [18] N. Fuhr. Optimum polynomial retrieval functions based on the probability ranking principle. ACM Transactions on Information Systems (TOIS), 7(3):183-204, 1989. [19] Y. Gao, C. M. Meyer, M. Mesgar, and I. Gurevych. Reward learning for efficient reinforcement learning in extractive document summarisation. arXiv preprint arXiv:1907.12894, 2019. [20] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256, 2010. [21] B. Hancock, A. Bordes, P.-E. Mazare, and J. Weston. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019. [22] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In Advances in neural information processing systems, pages 1693-1701, 2015. [23] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.[24] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human preferences and demonstrations in atari. In Advances in neural information processing systems, pages 8011-8023, 2018. [25] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and R. Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019. [26] N. Jaques, S. Gu, D. Bahdanau, J. M. Hern\u00e1ndez-Lobato, R. E. Turner, and D. Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In International Conference on Machine Learning, pages 1645-1654. PMLR, 2017.[27] N. Jaques, S. Gu, R. E. Turner, and D. Eck. Tuning recurrent neural networks with reinforcement learning. 2017.[28] H. J. Jeon, S. Milli, and A. D. Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. arXiv preprint arXiv:2002.04833, 2020.\n\nTable of Contents\nofA TL;DR dataset details 16 B Further model training details 17 B.1 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Input format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C Human data collection details 19 C.1 Process for ensuring high-quality human data . . . . . . . . . . . . . . . . . . . 19 C.2 Assessing human feedback quality. . . . . . . . . . . . . . .. . . . . . . . . 19 C.3 Labeler demographics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.4 Labeler website . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.5 Instructions for labelers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.6 Composition of the labeled dataset . . . . . . . . . . . . . . . . . . . . . . . . 22 C.7 Example comparison tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Value function ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 G.2 Evaluating policies along axes of quality . . . . . . . . . . . . . . . . . . . . . 31 G.3 Studying best-of-N optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 31 G.4 ROUGE scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 G.5 Bigram overlap statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.6 Reward model validation sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 G.7 Measuring agreement between different evaluation metrics . . . . . . . . . . . . 35 H Samples 38 H.1 Random samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 H.2 Overoptimized samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A TL;DR dataset detailsD Choice of baselines \n28 \n\nE CNN/DM lead-3 vs reference summaries \n29 \n\nF Controlling for summary length \n30 \n\nG Additional results \n31 \nG.1 Subreddit \n# posts % of dataset \nrelationships \n63324 \n54.25% \nAskReddit \n15440 \n13.23% \nrelationship_advice \n8691 \n7.45% \ntifu \n7685 \n6.58% \ndating_advice \n2849 \n2.44% \npersonalfinance \n2312 \n1.98% \nAdvice \n2088 \n1.79% \nlegaladvice \n1997 \n1.71% \noffmychest \n1582 \n1.36% \nloseit \n1452 \n1.24% \njobs \n1084 \n0.93% \nself \n1048 \n0.90% \nBreakUps \n838 \n0.72% \naskwomenadvice \n688 \n0.59% \ndogs \n638 \n0.55% \nrunning \n567 \n0.49% \npettyrevenge \n548 \n0.47% \nneedadvice \n528 \n0.45% \ntravel \n452 \n0.39% \nParenting \n435 \n0.37% \nweddingplanning \n433 \n0.37% \nPets \n366 \n0.31% \nDogtraining \n362 \n0.31% \ncats \n324 \n0.28% \nAskDocs \n283 \n0.24% \ncollege \n264 \n0.23% \nGetMotivated \n169 \n0.14% \nbooks \n161 \n0.14% \nCooking \n114 \n0.10% \n\n\n\nTable 2 :\n2Number of posts in the training set of our filtered Reddit TL;DR dataset by subreddit.\n\n\nFormats used for the context for each of our trained models on the TL;DR and CNN/DM datasets.Trained models \n\nFormat \nMax tokens \nTL;DR (supervised, RL) \nSUBREDDIT: r/{subreddit} \nTITLE: {title} \n512 \nPOST: {post} \nTL;DR: \nTransfer from TL;DR to \n{article} \n512 \nCNN/DM (supervised, RL) \nTL;DR: \nTL;DR (pretrained) \n{context_stuffed_with_examples} \n===== \nSubreddit: r/{subreddit} \n1999 \nTitle: {title} \n{post} \nTL;DR: \nCNN/DM (supervised) \nArticle: {article} \n1999 \nTL;DR: \nCNN/DM (pretrained) \n{context_stuffed_with_examples} \n===== \n1999 \nArticle: {article} \nTL;DR: \nTable 4: \n\nTable 7 :\n7Instructions given to labelers for evaluating summaries along four different axes of quality. Left: supervised baselines. sup4 and sup4_6b are the final supervised baselines used throughout the paper. Right: reward models. rm4 and rm4_6b are the final reward models used throughout the paper.Supervised policy name # Parameters \nsup1 \n750M \nsup2 \n1.3B \nsup3 \n1.3B \nsup3_6b \n6.7B \nsup4 \n1.3B \nsup4_6b \n6.7B \n\nReward model name # Parameters \nrm1 \n1.3B \nrm2 \n6.7B \nrm3 \n1.3B \nrm3_6b \n6.7B \nrm4 \n1.3B \nrm4_6b \n6.7B \nTable 8: RL policy name \n# Parameters Objective Initialization KL coefficient KL(ppo, sup) \nsup3 ppo rm1 \n1.3B \nrm1 \nsup3 \n0.35 \n1.8 \nsup4 ppo rm3 1 \n1.3B \nrm3 \nsup4 \n0.10 \n3.8 \nsup4 ppo rm3 2 \n1.3B \nrm3 \nsup4 \n0.07 \n9.4 \nsup4 ppo rm3 3 \n1.3B \nrm3 \nsup4 \n0.05 \n19.0 \nsup4 ppo rm4 \n1.3B \nrm4 \nsup4 \n0.05 \n18.0 \nsup4_6b ppo rm4_6b \n6.7B \nrm4_6b \nsup4_6b \n0.05 \n14.0 \nTable 9: PPO policies. sup4 ppo rm4 and sup4_6b ppo rm4_6b are the final policies used throughout \nthe paper. \n\nBoN policy name \nObjective Base policy \nN \nKL(BoN, sup) \nsup2 bo8 rm1 \nrm1 \nsup2 \n8 \n1.2 \nsup3 bo8 rm1 \nrm2 \nsup3 \n8 \n1.2 \nsup3 bo63 rm2 \nrm2 \nsup3 \n63 \n3.2 \nsup4 bo8 rm3 \nrm3 \nsup4 \n8 \n1.2 \nsup4 bo64 rm3 \nrm3 \nsup4 \n64 \n3.2 \nsup4 bo128 rm3 \nrm3 \nsup4 \n128 \n3.9 \nsup4 bo256 rm3 \nrm3 \nsup4 \n256 \n4.5 \nsup4 bo512 rm3 \nrm3 \nsup4 \n512 \n5.2 \nsup4 bo128 rm3_6b \nrm3_6b \nsup4 \n128 \n3.9 \nsup4 bo256 rm3_6b \nrm3_6b \nsup4 \n256 \n4.5 \n\n\nTable 12 :\n12Top: Example of a random comparison task on the TL;DR dataset between two summaries \nfrom our 6.7B human feedback model. Comparison chosen randomly from the validation set. Bottom: \nAn example of a difficult comparison task on the TL;DR dataset. Chosen by looking at comparisons \nbetween supervised baseline summaries with at least 4 labeler judgements and with at least 40% vote \nfor each summary. Cherry-picked out of 10 to highlight an accuracy-coverage tradeoff. Summary A \nis inaccurate since the author does not explicitly say she is having doubts about trying on wedding \ndresses. Summary B is entirely accurate but does not capture the general essence of the post. In this \ncase, 4 workers chose A and 3 workers chose B. For more comparisons, see our website. \n\n\nTable 14 :\n14How length affects overall quality on CNN/DM. We show average length and quality scores for various policies, and how much the summary quality increases on average per 100 added characters.\n\n\nHuman feedback (TL;DR) 1.3B 53.3% Human feedback (TL;DR) 6.7B 46.0% 8% Table 16: Bigram overlap statistics on the TL;DR dataset (top) and the CNN/DM dataset (bottom). Models trained on CNN/DM copy significantly more than models trained on TL;DR.Evaluated on TL;DR \nModel \nModel size Bigram overlap % \nGPT \n1.3B \n66.7% \nGPT \n3B \n72.7% \nGPT \n6.7B \n61.4% \nGPT \n13B \n75.9% \nSupervised (TL;DR) \n1.3B \n49.0% \nSupervised (TL;DR) \n3B \n48.7% \nSupervised (TL;DR) \n6.7B \n48.9% \nSupervised (TL;DR) \n13B \n48.0% \nEvaluated on CNN/DM \nModel \nModel size Bigram overlap % \nGPT \n1.3B \n76.3% \nGPT \n6.7B \n76.2% \nSupervised (TL;DR) \n1.3B \n59.5% \nSupervised (TL;DR) \n6.7B \n56.9% \nHuman feedback (TL;DR) 1.3B \n64.8% \nHuman feedback (TL;DR) 6.7B \n51.2% \nSupervised (CNN/DM) \n1.3B \n66.0% \nT5 \n11B \n68.8% \nreference \n-\n36.\n\n\nRM size Edit length RM prefers edit Human prefers edit RM, human agree Comparing reward model and human preference of summaries that were edited by humans to make them better. For each summary, the human labeler that makes the comparison is different than the labeler that wrote the edit. The agreement numbers do not include comparisons where the labeler's preference was marked as 'uncertain'.Shorter \n63.6% \n76.2% \n62.1% \n1.3B \nLonger \n86.8% \n88.6% \n79.6% \nAvg. \n81.2% \n85.6% \n75.4% \nShorter \n66.0% \n76.2% \n65.5% \n6.7B \nLonger \n89.2% \n88.6% \n80.2% \nAvg. \n83.7% \n85.6% \n76.7% \nTable 17: Preference % of Summary A \nSummary A \nSummary B \n1.3B RM \n6.7B RM \nOriginal summary \nReversed roles \n93.1% \n97.4% \nlead-3 \nShuffled lead-3 \n68.1% \n75.5% \nrand-3 \nShuffled rand-3 \n60.8% \n76.1% \nPost title \nRandom title \n97.4% \n98.5% \nPost title \nRandom title from same subreddit \n98.8% \n97.2% \nPost title \nPost title repeated twice \n84.6% \n58.4% \n(r/tifu only) Reference summary Ref + \"What should I do?\" \n34.3 % \n74.5% \nReference summary \nlead-3 \n63.0% \n56.4% \nReference summary \nlead-2 \n71.0% \n73.8% \nReference summary \nrand-3 \n69.5% \n59.5% \n\n\nTable 20 :\n20Agreement rates between humans and various automated metrics on TL;DR 1.3b supervised model at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers.TL;DR \n6.7B sup. \nT=0.7 \n\nlabeler \nlabeler \nensem-\nble \n\nlength \ncopying ROUGE 1.3B \nsup \nlogprob \n\n1.3B \nRM \n\n6.7B \nsup \nlogprob \n\n6.7B \nRM \n\nlabeler \n70.8% \n\u00b12.6% \n\n73.1% \n\u00b12.9% \n\n56.9% \n\u00b10.6% \n\n56.4% \n\u00b10.6% \n\n56.9% \n\u00b10.6% \n\n54.5% \n\u00b11.2% \n\n67.5% \n\u00b11.1% \n\n54.3% \n\u00b11.2% \n\n69.7% \n\u00b11.1% \nlabeler \nensemble \n73.1% \n\u00b12.9% \n\n-\n55.0% \n\u00b15.1% \n\n54.5% \n\u00b14.8% \n\n66.7% \n\u00b14.7% \n\n61.1% \n\u00b111.4% \n\n77.8% \n\u00b19.7% \n\n55.6% \n\u00b111.7% \n\n77.8% \n\u00b110.0% \nlength \n56.9% \n\u00b10.6% \n\n55.0% \n\u00b15.1% \n\n-\n50.5% \n\u00b10.6% \n\n60.2% \n\u00b10.6% \n\n26.9% \n\u00b11.1% \n\n59.5% \n\u00b11.2% \n\n26.4% \n\u00b11.1% \n\n60.3% \n\u00b11.1% \ncopying 56.4% \n\u00b10.6% \n\n54.5% \n\u00b14.8% \n\n50.5% \n\u00b10.6% \n\n-\n54.4% \n\u00b10.6% \n\n59.3% \n\u00b11.1% \n\n57.9% \n\u00b11.2% \n\n60.2% \n\u00b11.2% \n\n58.0% \n\u00b11.2% \nROUGE 56.9% \n\u00b10.6% \n\n66.7% \n\u00b14.7% \n\n60.2% \n\u00b10.6% \n\n54.4% \n\u00b10.6% \n\n-\n48.7% \n\u00b11.2% \n\n58.1% \n\u00b11.2% \n\n47.7% \n\u00b11.2% \n\n58.4% \n\u00b11.2% \n1.3B sup. \nlogprob \n54.5% \n\u00b11.2% \n\n61.1% \n\u00b111.4% \n\n26.9% \n\u00b11.1% \n\n59.3% \n\u00b11.1% \n\n48.7% \n\u00b11.2% \n\n-\n53.3% \n\u00b11.2% \n\n91.9% \n\u00b10.6% \n\n53.8% \n\u00b11.2% \n1.3B RM 67.5% \n\u00b11.1% \n\n77.8% \n\u00b19.7% \n\n59.5% \n\u00b11.2% \n\n57.9% \n\u00b11.2% \n\n58.1% \n\u00b11.2% \n\n53.3% \n\u00b11.2% \n\n-\n54.1% \n\u00b11.2% \n\n78.8% \n\u00b11.0% \n6.7B sup. \nlogprob \n54.3% \n\u00b11.2% \n\n55.6% \n\u00b111.7% \n\n26.4% \n\u00b11.1% \n\n60.2% \n\u00b11.2% \n\n47.7% \n\u00b11.2% \n\n91.9% \n\u00b10.6% \n\n54.1% \n\u00b11.2% \n\n-\n54.5% \n\u00b11.2% \n6.7B RM 69.7% \n\u00b11.1% \n\n77.8% \n\u00b110.0% \n\n60.3% \n\u00b11.1% \n\n58.0% \n\u00b11.2% \n\n58.4% \n\u00b11.2% \n\n53.8% \n\u00b11.2% \n\n78.8% \n\u00b11.0% \n\n54.5% \n\u00b11.2% \n\n-\n\n\n\nTable 21 :\n21Agreement rates between humans and various automated metrics on TL;DR 6.7B supervised model at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers.TL;DR \n6.7B RL \nT=0.7 \n\nlabeler \nlabeler \nensem-\nble \n\nlength \ncopying ROUGE 1.3B \nsup \nlogprob \n\n1.3B \nRM \n\n6.7B \nsup \nlogprob \n\n6.7B \nRM \n\nlabeler \n60.4% \n\u00b15.9% \n\n66.0% \n\u00b17.6% \n\n55.8% \n\u00b12.2% \n\n52.7% \n\u00b12.1% \n\n49.9% \n\u00b12.1% \n\n48.0% \n\u00b12.2% \n\n57.4% \n\u00b12.0% \n\n47.3% \n\u00b12.2% \n\n62.3% \n\u00b12.1% \nlabeler \nensemble \n66.0% \n\u00b17.6% \n\n-\n80.0% \n\u00b18.9% \n\n65.0% \n\u00b110.6% \n\n35.0% \n\u00b110.5% \n\n45.0% \n\u00b111.1% \n\n75.0% \n\u00b19.8% \n\n40.0% \n\u00b110.5% \n\n75.0% \n\u00b19.8% \nlength \n55.8% \n\u00b12.2% \n\n80.0% \n\u00b18.9% \n\n-\n48.1% \n\u00b12.2% \n\n50.3% \n\u00b12.2% \n\n30.0% \n\u00b12.1% \n\n62.0% \n\u00b12.1% \n\n30.4% \n\u00b12.0% \n\n59.8% \n\u00b12.2% \ncopying 52.7% \n\u00b12.1% \n\n65.0% \n\u00b110.6% \n\n48.1% \n\u00b12.2% \n\n-\n52.0% \n\u00b12.2% \n\n64.2% \n\u00b12.1% \n\n56.7% \n\u00b12.2% \n\n64.4% \n\u00b12.1% \n\n53.4% \n\u00b12.2% \nROUGE 49.9% \n\u00b12.1% \n\n35.0% \n\u00b110.5% \n\n50.3% \n\u00b12.2% \n\n52.0% \n\u00b12.2% \n\n-\n50.5% \n\u00b12.2% \n\n52.0% \n\u00b12.3% \n\n51.1% \n\u00b12.3% \n\n54.5% \n\u00b12.1% \n1.3B sup. \nlogprob \n48.0% \n\u00b12.2% \n\n45.0% \n\u00b111.1% \n\n30.0% \n\u00b12.1% \n\n64.2% \n\u00b12.1% \n\n50.5% \n\u00b12.2% \n\n-\n47.0% \n\u00b12.2% \n\n90.2% \n\u00b11.3% \n\n46.1% \n\u00b12.2% \n1.3B RM 57.4% \n\u00b12.0% \n\n75.0% \n\u00b19.8% \n\n62.0% \n\u00b12.1% \n\n56.7% \n\u00b12.2% \n\n52.0% \n\u00b12.3% \n\n47.0% \n\u00b12.2% \n\n-\n45.7% \n\u00b12.1% \n\n71.4% \n\u00b12.0% \n6.7B sup. \nlogprob \n47.3% \n\u00b12.2% \n\n40.0% \n\u00b110.5% \n\n30.4% \n\u00b12.0% \n\n64.4% \n\u00b12.1% \n\n51.1% \n\u00b12.3% \n\n90.2% \n\u00b11.3% \n\n45.7% \n\u00b12.1% \n\n-\n44.7% \n\u00b12.1% \n6.7B RM 62.3% \n\u00b12.1% \n\n75.0% \n\u00b19.8% \n\n59.8% \n\u00b12.2% \n\n53.4% \n\u00b12.2% \n\n54.5% \n\u00b12.1% \n\n46.1% \n\u00b12.2% \n\n71.4% \n\u00b12.0% \n\n44.7% \n\u00b12.1% \n\n-\n\n\n\nTable 22 :\n22Agreement rates between humans and various automated metrics on TL;DR 6.7B human feedback optimized model at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. All ensembles have at least 3 workers.\n\nTable 23 :\n23Agreement rates between humans and various automated metrics on CNN/DM 6.7B supervised model at T=0.3. Standard errors estimated via bootstrapping. NOTE: in the entry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude the labeler being predicted. (All ensembles have at least 3 workers)\nThroughout the paper, error bars represent 1 standard error.\nWe manually check this result in Appendix E and find we generally agree with labeler ratings.\nOur decision to collect comparisons rather than Likert scores is supported by recent work, e.g.[37].7 We recruited labelers from a freelancing platform, Upwork, and two labeling services, Scale and Lionbridge.\nNote that the reward model only gives rewards for entire summaries, and not at intermediate time steps. In RL terminology, each episode terminates when the policy outputs the EOS token, and the discount factor \u03b3 = 1.\nWe measure copying by computing the longest common subsequence of bigrams with the original Reddit post or news article, and dividing by the number of bigrams in the summary.\nThese posts are usually follow-ups of previous posts that have been posted to Reddit, and require the context of the original post to fully understand.\nThis was for a historical reason -we found that fp32 weights improved RL performance and so used it for all our RL runs. This introduces a small discrepancy, since supervised runs trained in fp32 would have performed slightly better. Unfortunately, we forgot to address this in our human evaluations. However, the effect on the supervised loss corresponds to increasing model size by less than 20%, which is small compared to effect sizes that are present in this paper (as seen inFigure 1.)\nWe pay labelers an hourly wage, regardless of the number of comparisons completed.\nSince tokenization affects capitalization and punctuation of the model outputs, we normalized all CNN/Daily Mail outputs from all models by lower-casing everything and then heuristically re-capitalizing. We verify that this normalization procedure produces identical results for reference summaries tokenized in different ways.\nThe reference summaries were preferred to lead-3 by a similar margin in only 7/143 cases.\nWe can use KL from the supervised baseline as a distance metric. Note that we can calculate the KL of a best-of-N policy analytically as log(n) \u2212 n\u22121 n .\nhttp://nlpprogress.com/english/summarization.html\nAcknowledgementsWe'd like to thank Beth Barnes for help with labeler hiring and general encouragement; Geoffrey Irving for guidance on earlier iterations of the project and inspiring conversations; Ben Mann, Tom Brown, Nick Ryder, and Melanie Subbiah for training and evaluating our pretrained models; Chris Hesse, Eric Sigler, Benjamin Chess, Christopher Berner, Clemens Winter, Mateusz Litwin, and many others for supporting us through computing infrastructure improvements and maintenance; Scott Gray for writing fast GPU kernels; Arvind Neelakantan and Wojciech Kryscinski for discussions on how to present the work, experiment design, and what datasets to use; Shan Carter for help designing the main diagram; Douwe Kiela, Zach Lipton, and Alex Irpan for providing feedback on the paper; and Gretchen Krueger for co-writing the model card accompanying the paper.Finally, we'd like to thank all of our contractors for providing the data that was essential for training the models in this paper, including: Emill Jayson Caypuno, Rachelle Froyalde, Cyra Denura, Alex Malek, Isik Agil, Reshmi Patel, William Yap, Natalie Silver, Erol Akbaba, Jennifer Brillo, AlexandraAppendixLabel countReward model Policy0  Policy1   sup2  sup2  11346  sup2 bo8 rm1  1376  sup3_6b  1383  sup2 bo8 rm1  sup3_6b  1390  rm3, rm3_6b  ref  sup1  5404  sup2  12779  sup2 bo8 rm1  1426  sup3  438  sup3 bo63 rm2  447  sup3 bo8 rm2  887  sup3 ppo rm1  884  sup3_6b  1424  sup1  sup1  5386  sup2  sup2  11346  sup2 bo8 rm1  1376  sup3_6b  1383  sup2 bo8 rm1  sup3_6b  1390  sup3  sup3 bo8 rm2  428  sup3 ppo rm1  416  sup3 bo63 rm2  sup3 bo8 rm2  432  sup3 ppo rm1  444  sup3 bo8 rm2  sup3 ppo rm1  855  rm4, rm4_6b  ref  sup1  5404  sup2  12779  sup2 bo8 rm1  1426  sup3  438  sup3 bo63 rm2  447  sup3 bo8 rm2  887  sup3 ppo rm1  884  sup3_6b  1424  sup4  1335  sup4 bo128 rm3  602  sup4 bo128 rm3_6b  203  sup4 bo256 rm3  307  sup4 bo256 rm3_6b  101  sup4 bo512 rm3  52  sup4 bo64 rm3  52landlord stubbornly postponees gettin fridge repair despite tried reasonable compromise offer??? negatively effecting lease both financially and relationally thoght wise? want change this dumbass shitty ass landlord behavior now please pls halp Girlfriend talks to ex, he's still in love with her, shes completely open about their conversations with me and has told him there is zero chance of me and her breaking up and that hes only in love with being in a relationship not with her, she says she has nothing more to say to him, yet continues to talk to him. I'm confused :( girlfriend stubbornly continue talkin with estranged ex despite tried compromise offer??? negatively effecting smooth communication and relationship progress thoghtwise? regret this dumbass behaviour on her part? need insight pls halp GF likes to cover her tracks/generally secretive when using social networks. However, nothing wrong is with our very open-to-each-other relationship so far. Should I confront her? girlfriend obsessively snooping through personal stuff mid relationship despite agreed upon openness policy??? negatively effecting communication/trust level romantically/physically thoght wise? want change this dumbass crazy policy of hers pls help Laid off almost a year ago, up to my ears in debt, and trying to figure out how to get motivated every day to do anything productive and find a job. kid stubbornly postponees pursuing job citing medical reasons despite tried reasonable compromise??? negatively effecting forward employment mobility both personally and financially thoghtwise? want change this dumbass selfish/lazy attitude now please help plsTable 29: Example of samples from our over-optimized model overfit to one of our 1.3B reward models.\nAn actor-critic algorithm for sequence prediction. D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe, J Pineau, A Courville, Y Bengio, arXiv:1607.07086arXiv preprintD. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\n\nAutomatic combination of multiple ranked retrieval systems. B T Bartell, G W Cottrell, R K Belew, SIGIR'94. SpringerB. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In SIGIR'94, pages 173-181. Springer, 1994.\n\nF B\u00f6hm, Y Gao, C M Meyer, O Shapira, I Dagan, I Gurevych, arXiv:1909.01214Better rewards yield better summaries: Learning to summarise without references. arXiv preprintF. B\u00f6hm, Y. Gao, C. M. Meyer, O. Shapira, I. Dagan, and I. Gurevych. Better rewards yield better summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214, 2019.\n\n. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Language models are few-shot learners. 2020T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. 2020.\n\nS Cabi, S Colmenarejo, A Novikov, K Konyushkova, S Reed, R Jeong, K Zolna, Y Aytar, D Budden, M Vecerik, Scaling data-driven robotics with reward sketching and batch reinforcement learning. arXiv. 1909S. Cabi, S. G\u00f3mez Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna, Y. Aytar, D. Budden, M. Vecerik, et al. Scaling data-driven robotics with reward sketching and batch reinforcement learning. arXiv, pages arXiv-1909, 2019.\n\nThe price of debiasing automatic metrics in natural language evaluation. A T Chaganty, S Mussman, P Liang, arXiv:1807.02202arXiv preprintA. T. Chaganty, S. Mussman, and P. Liang. The price of debiasing automatic metrics in natural language evaluation. arXiv preprint arXiv:1807.02202, 2018.\n\nTowards coherent and cohesive long-form text generation. W S Cho, P Zhang, Y Zhang, X Li, M Galley, C Brockett, M Wang, J Gao, arXiv:1811.00511arXiv preprintW. S. Cho, P. Zhang, Y. Zhang, X. Li, M. Galley, C. Brockett, M. Wang, and J. Gao. Towards coherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511, 2018.\n\nAbstractive sentence summarization with attentive recurrent neural networks. S Chopra, M Auli, A M Rush, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesS. Chopra, M. Auli, and A. M. Rush. Abstractive sentence summarization with attentive recurrent neural networks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 93-98, 2016.\n\nSupervising strong learners by amplifying weak experts. P Christiano, B Shlegeris, D Amodei, arXiv:1810.08575arXiv preprintP. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.\n\nDeep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Advances in Neural Information Processing Systems. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4299-4307, 2017.\n\nDeep neural networks for youtube recommendations. P Covington, J Adams, E Sargin, Proceedings of the 10th ACM conference on recommender systems. the 10th ACM conference on recommender systemsP. Covington, J. Adams, and E. Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pages 191-198, 2016.\n\nSemi-supervised sequence learning. A M Dai, Q V Le, Advances in neural information processing systems. A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079-3087, 2015.\n\nJ Dodge, G Ilharco, R Schwartz, A Farhadi, H Hajishirzi, N Smith, arXiv:2002.06305Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprintJ. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020.\n\nUnified language model pre-training for natural language understanding and generation. L Dong, N Yang, W Wang, F Wei, X Liu, Y Wang, J Gao, M Zhou, H.-W Hon, Advances in Neural Information Processing Systems. L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, 2019.\n\nY Dong, Y Shen, E Crawford, H Van Hoof, J C K Cheung, arXiv:1809.09672Banditsum: Extractive summarization as a contextual bandit. arXiv preprintY. Dong, Y. Shen, E. Crawford, H. van Hoof, and J. C. K. Cheung. Banditsum: Extractive summarization as a contextual bandit. arXiv preprint arXiv:1809.09672, 2018.\n\nHedge trimmer: A parse-and-trim approach to headline generation. B Dorr, D Zajic, R Schwartz, Proceedings of the HLT-NAACL 03 on Text summarization workshop. the HLT-NAACL 03 on Text summarization workshopAssociation for Computational Linguistics5B. Dorr, D. Zajic, and R. Schwartz. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5, pages 1-8. Association for Computational Linguistics, 2003.\n\nTeaching machines to describe images with natural language feedback. S Fidler, Advances in Neural Information Processing Systems. S. Fidler et al. Teaching machines to describe images with natural language feedback. In Advances in Neural Information Processing Systems, pages 5068-5078, 2017.\n\nOptimizing search engines using clickthrough data. T Joachims, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. the eighth ACM SIGKDD international conference on Knowledge discovery and data miningT. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133-142, 2002.\n\nAccurately interpreting clickthrough data as implicit feedback. T Joachims, L Granka, B Pan, H Hembrooke, G Gay, ACM SIGIR Forum. New York, NY, USAAcm51T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting click- through data as implicit feedback. In ACM SIGIR Forum, volume 51, pages 4-11. Acm New York, NY, USA, 2005.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nCan neural machine translation be improved with user feedback?. J Kreutzer, S Khadivi, E Matusov, S Riezler, arXiv:1804.05958arXiv preprintJ. Kreutzer, S. Khadivi, E. Matusov, and S. Riezler. Can neural machine translation be improved with user feedback? arXiv preprint arXiv:1804.05958, 2018.\n\nNeural text summarization: A critical evaluation. W Kryscinski, N S Keskar, B Mccann, C Xiong, R Socher, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)W. Kryscinski, N. S. Keskar, B. McCann, C. Xiong, and R. Socher. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540-551, 2019.\n\nImproving a neural semantic parser by counterfactual learning from human bandit feedback. C Lawrence, S Riezler, arXiv:1805.01252arXiv preprintC. Lawrence and S. Riezler. Improving a neural semantic parser by counterfactual learning from human bandit feedback. arXiv preprint arXiv:1805.01252, 2018.\n\nJ Leike, D Krueger, T Everitt, M Martic, V Maini, S Legg, arXiv:1811.07871Scalable agent alignment via reward modeling: a research direction. arXiv preprintJ. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n\nBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, arXiv:1910.13461arXiv preprintM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n\nAcute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. M Li, J Weston, S Roller, arXiv:1909.03087arXiv preprintM. Li, J. Weston, and S. Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019.\n\nA technique for the measurement of attitudes. Archives of psychology. R Likert, R. Likert. A technique for the measurement of attitudes. Archives of psychology, 1932.\n\nAutomatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. C.-Y Lin, F J Och, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. the 42nd Annual Meeting on Association for Computational LinguisticsAssociation for Computational Linguistics605C.-Y. Lin and F. J. Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 605. Association for Computational Linguistics, 2004.\n\nLearning to rank for information retrieval. T.-Y Liu, Springer Science & Business MediaT.-Y. Liu. Learning to rank for information retrieval. Springer Science & Business Media, 2011.\n\nOn faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in abstractive summarization, 2020.\n\nB Mccann, N S Keskar, C Xiong, R Socher, arXiv:1806.08730The natural language decathlon: Multitask learning as question answering. arXiv preprintB. McCann, N. S. Keskar, C. Xiong, and R. Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. K Nguyen, H Daum\u00e9, Iii , J Boyd-Graber, arXiv:1707.07402arXiv preprintK. Nguyen, H. Daum\u00e9 III, and J. Boyd-Graber. Reinforcement learning for bandit neural machine translation with simulated human feedback. arXiv preprint arXiv:1707.07402, 2017.\n\nPolite dialogue generation without parallel data. T Niu, M Bansal, Transactions of the Association for Computational Linguistics. 6T. Niu and M. Bansal. Polite dialogue generation without parallel data. Transactions of the Association for Computational Linguistics, 6:373-389, 2018.\n\nA deep reinforced model for abstractive summarization. R Paulus, C Xiong, R Socher, arXiv:1705.04304arXiv preprintR. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\n\nFinding generalizable evidence by learning to convince q&a models. E Perez, S Karamcheti, R Fergus, J Weston, D Kiela, K Cho, arXiv:1909.05863arXiv preprintE. Perez, S. Karamcheti, R. Fergus, J. Weston, D. Kiela, and K. Cho. Finding generalizable evidence by learning to convince q&a models. arXiv preprint arXiv:1909.05863, 2019.\n\nImproving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language under- standing by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI Blog. 189A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, arXiv:1910.10683arXiv preprintC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nM Ranzato, S Chopra, M Auli, W Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintM. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.\n\nSpeech understanding systems: A summary of results of the five-year research effort. department of computer science. D R Reddy, D. R. Reddy et al. Speech understanding systems: A summary of results of the five-year research effort. department of computer science, 1977.\n\nA reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsS. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635, 2011.\n\nLeveraging pre-trained checkpoints for sequence generation tasks. S Rothe, S Narayan, A Severyn, Transactions of the Association for Computational Linguistics. S. Rothe, S. Narayan, and A. Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 2020.\n\nA neural attention model for abstractive sentence summarization. A M Rush, S Chopra, J Weston, arXiv:1509.00685arXiv preprintA. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685, 2015.\n\nThe limits of automatic summarisation according to rouge. N Schluter, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European Chapter2Short PapersN. Schluter. The limits of automatic summarisation according to rouge. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 41-45, 2017.\n\nGeneralization in generation: A closer look at exposure bias. F Schmidt, arXiv:1910.00292arXiv preprintF. Schmidt. Generalization in generation: A closer look at exposure bias. arXiv preprint arXiv:1910.00292, 2019.\n\nHigh-dimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M Jordan, P Abbeel, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.\n\nJ Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nGet to the point: Summarization with pointer-generator networks. A See, P J Liu, C D Manning, arXiv:1704.04368arXiv preprintA. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.\n\nMass: Masked sequence to sequence pre-training for language generation. K Song, X Tan, T Qin, J Lu, T.-Y Liu, arXiv:1905.02450arXiv preprintK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\n\nP Tambwekar, M Dhuliawala, A Mehta, L J Martin, B Harrison, M O , arXiv:1809.10736Controllable neural story generation via reinforcement learning. arXiv preprintP. Tambwekar, M. Dhuliawala, A. Mehta, L. J. Martin, B. Harrison, and M. O. Riedl. Con- trollable neural story generation via reinforcement learning. arXiv preprint arXiv:1809.10736, 2018.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.\n\nTl; dr: Mining reddit to learn automatic summarization. M V\u00f6lske, M Potthast, S Syed, B Stein, Proceedings of the Workshop on New Frontiers in Summarization. the Workshop on New Frontiers in SummarizationM. V\u00f6lske, M. Potthast, S. Syed, and B. Stein. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, 2017.\n\nS Welleck, I Kulikov, S Roller, E Dinan, K Cho, J Weston, arXiv:1908.04319Neural text generation with unlikelihood training. arXiv preprintS. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\n\nLearning to extract coherent summary via deep reinforcement learning. Y Wu, B Hu, Thirty-Second AAAI Conference on Artificial Intelligence. Y. Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, arXiv:1609.08144arXiv preprintY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\nProphetnet: Predicting future n-gram for sequence-to-sequence pre-training. Y Yan, W Qi, Y Gong, D Liu, N Duan, J Chen, R Zhang, M Zhou, arXiv:2001.04063arXiv preprintY. Yan, W. Qi, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou. Prophetnet: Pre- dicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063, 2020.\n\nTowards coherent and engaging spoken dialog response generation using automatic conversation evaluators. S Yi, R Goel, C Khatri, A Cervone, T Chung, B Hedayatnia, A Venkatesh, R Gabriel, D Hakkani-Tur, arXiv:1904.13015arXiv preprintS. Yi, R. Goel, C. Khatri, A. Cervone, T. Chung, B. Hedayatnia, A. Venkatesh, R. Gabriel, and D. Hakkani-Tur. Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. arXiv preprint arXiv:1904.13015, 2019.\n\nTrading off diversity and quality in natural language generation. H Zhang, D Duckworth, D Ippolito, A Neelakantan, arXiv:2004.10450arXiv preprintH. Zhang, D. Duckworth, D. Ippolito, and A. Neelakantan. Trading off diversity and quality in natural language generation. arXiv preprint arXiv:2004.10450, 2020.\n\nPegasus: Pre-training with extracted gap-sentences for abstractive summarization. J Zhang, Y Zhao, M Saleh, P J Liu, arXiv:1912.08777arXiv preprintJ. Zhang, Y. Zhao, M. Saleh, and P. J. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. arXiv preprint arXiv:1912.08777, 2019.\n\nAbstract text summarization with a convolutional seq2seq model. Y Zhang, D Li, Y Wang, Y Fang, W Xiao, Applied Sciences. 981665Y. Zhang, D. Li, Y. Wang, Y. Fang, and W. Xiao. Abstract text summarization with a convolu- tional seq2seq model. Applied Sciences, 9(8):1665, 2019.\n\nLearning to compare for better training and evaluation of open domain natural language generation models. W Zhou, K Xu, arXiv:2002.05058arXiv preprintW. Zhou and K. Xu. Learning to compare for better training and evaluation of open domain natural language generation models. arXiv preprint arXiv:2002.05058, 2020.\n\nD M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. arXiv preprintD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irv- ing. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n\nHere we provide non-cherry-picked samples and human evaluations for various models. Tables 25-26 we show samples on the TL;DR dataset, and in Tables 27-28 we show samples on the CNN/DM dataset (where we truncate the article for brevity. See our website for more uncurated policy samplesHere we provide non-cherry-picked samples and human evaluations for various models. In Tables 25- 26 we show samples on the TL;DR dataset, and in Tables 27-28 we show samples on the CNN/DM dataset (where we truncate the article for brevity). See our website for more uncurated policy samples.\n\nOveroptimized samples. H.2 Overoptimized samples\n\nWe show examples of samples from a policy overoptimized to rm3. The summaries, while clearly long, low quality, and full of idiosyncrasies. do still reflect the rough gist of the postWe show examples of samples from a policy overoptimized to rm3. The summaries, while clearly long, low quality, and full of idiosyncrasies, do still reflect the rough gist of the post.\n\nwhat should I do? I'm 20 f, my boss is around 50 years old. POST Subreddit: r/relationships TITLE: Screwed up with boss. POST Subreddit: r/relationships TITLE: Screwed up with boss... what should I do? I'm 20 f, my boss is around 50 years old, also f.\n\nOne of my jobs I have had for three years, the other one I have had for a month and a bit. I forgot to give my schedule from one job to my boss at my other job, and so I was not scheduled for this week. I didn't realize why I had not been put on the schedule until now. My question is, since I royally screwed up, what can I do to redeem myself? I don't want to call my boss today because it is a Sunday and she has the day off. Mistakes aren't easily forgiven where I work, as far as I can tell, and the boss often makes comments about how the employees should be scared of her. I have screwed up at previous jobs (little things) but my boss was less intimidating than my current one, so I am not sure how to handle this situation. Policy: Human written reference TL;DR Summary: screwed up at work by not giving the boss my schedule from my other job, am not scheduled this week, what should I say in order to apologize to my (scary/intimidating) boss?. Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6 Policy: Title (extractive baseline) Summary: Screwed up with boss... what should I do? Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3 Policy: Lead-2 (extractive baseline) Summary: I'm 20 f. So I have two jobs, and the schedules for both jobs are made on a weekly basis. my boss is around 50 years old, also fSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs I have had for three years, the other one I have had for a month and a bit. I forgot to give my schedule from one job to my boss at my other job, and so I was not scheduled for this week. I didn't realize why I had not been put on the schedule until now. My question is, since I royally screwed up, what can I do to redeem myself? I don't want to call my boss today because it is a Sunday and she has the day off. Mistakes aren't easily forgiven where I work, as far as I can tell, and the boss often makes comments about how the employees should be scared of her. I have screwed up at previous jobs (little things) but my boss was less intimidating than my current one, so I am not sure how to handle this situation. Policy: Human written reference TL;DR Summary: screwed up at work by not giving the boss my schedule from my other job, am not scheduled this week, what should I say in order to apologize to my (scary/intimidating) boss? Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6 Policy: Title (extractive baseline) Summary: Screwed up with boss... what should I do? Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3 Policy: Lead-2 (extractive baseline) Summary: I'm 20 f, my boss is around 50 years old, also f.\n\nWhat can I do to make things right again? Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B human-feedback model Summary: I forgot to give my boss my weekly schedule for one of my jobs, and so I was not scheduled this week. I royally screwed up. What can I do to redeem myself? Overall score: 5 Accuracy: 7 Coherence: 6 Coverage: 6 Policy: 6.7B supervised model Summary: I forgot to give my boss my schedule for one of my jobs, and now I have been scheduled for the wrong week. So I have two jobs, and the schedules for both jobs are made on a weekly basis. 3Coverage: 4 Policy: 1.3B human-feedback model Summary: I screwed up by not giving my schedule to my boss at my other job, and now I don't know what to do to redeem myself. What do I do? Overall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3 Policy: 6.7B pretrained model Summary: I royally screwed up and I don't know what to do. Overall score: 2 Accuracy: 7 Coherence: 7 CoverageSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs I have had for three years, the other one I have had for a month and a bit. Overall score: 3 Accuracy: 7 Coherence: 7 Coverage: 4 Policy: 1.3B human-feedback model Summary: I screwed up by not giving my schedule to my boss at my other job, and now I don't know what to do to redeem myself. What can I do to make things right again? Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B human-feedback model Summary: I forgot to give my boss my weekly schedule for one of my jobs, and so I was not scheduled this week. I royally screwed up. What can I do to redeem myself? Overall score: 5 Accuracy: 7 Coherence: 6 Coverage: 6 Policy: 6.7B supervised model Summary: I forgot to give my boss my schedule for one of my jobs, and now I have been scheduled for the wrong week. What do I do? Overall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3 Policy: 6.7B pretrained model Summary: I royally screwed up and I don't know what to do. Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\n\nFor more random examples of evaluations, see our website. POST Subreddit: r/relationships TITLE: I am a [18 M] she is a [17 F] and I don't know how to read this relationship? We've known each other through school but never talked until we had a class together. I asked her out after about a week, we went out for food, laughed, flirted etc etc. I asked her out again same situation, everything going great. Now it's three weeks later (midst of exams) and I'm starting to get the feeling that she's not thinking of this as a \"relationship. Table 24: Examples of evaluations on the TL;DR dataset. in the conventional bf/gf sense. I'm new to the whole dating game and wondering a few thingsTable 24: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations, see our website. POST Subreddit: r/relationships TITLE: I am a [18 M] she is a [17 F] and I don't know how to read this relationship? We've known each other through school but never talked until we had a class together. I asked her out after about a week, we went out for food, laughed, flirted etc etc. I asked her out again same situation, everything going great. Now it's three weeks later (midst of exams) and I'm starting to get the feeling that she's not thinking of this as a \"relationship\" in the conventional bf/gf sense. I'm new to the whole dating game and wondering a few things.\n\nOh and finally, to add to my lengthy situation, On our third \"date\" (studying for an exam) I drove her home and attempted to kiss her but got the infamous cheek. I didn't make a big deal about it I just moved past said \"got your things?\" and politely said goodbye preceding to wait for her to get inside her house. I've been told I shouldn't fret about ONE kiss rejection cause there could be multiple reasons for it but at the same time that made me think more about the friend zone possibility. Do I need to ask if we're together as bf/gf or is that implied by asking her out multiple times? Should I straight up ask if she likes me the way I like her? I know what the friend zone is and I'm not feeling there, yet, but we. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B supervised model Summary: I'm a. 18 M] she's a [17 F] and I don't know how to read this relationship? Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2 Policy: 6.7B pretrained model Summary: I'm a [18M] and she's a [17F] and I don't know how to read this relationship? Overall score: 2 Accuracy: 7 Coherence: 7 CoverageDo I need to ask if we're together as bf/gf or is that implied by asking her out multiple times? Should I straight up ask if she likes me the way I like her? I know what the friend zone is and I'm not feeling there, yet, but we're having a movie night tomorrow just her and I. I plan on seeing what's what then by trying to get physical, not hardcore, just simple stuff like leg touches, cuddling etc. Oh and finally, to add to my lengthy situation, On our third \"date\" (studying for an exam) I drove her home and attempted to kiss her but got the infamous cheek. I didn't make a big deal about it I just moved past said \"got your things?\" and politely said goodbye preceding to wait for her to get inside her house. I've been told I shouldn't fret about ONE kiss rejection cause there could be multiple reasons for it but at the same time that made me think more about the friend zone possibility. Any advice or similar situations and how you solved the problem would be smashing! Thanks in advance. Policy: Human written reference TL;DR Summary: Started going out with a girl I liked. Feelings were obviously mutual. Took it slow due to exams. Not sure now if she is feeling the same way I am. Overall score: 6 Accuracy: 4 Coherence: 7 Coverage: 7 Policy: Title (extractive baseline) Summary: I am a [18 M] she is a [17 F] and I don't know how to read this relationship? Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2 Policy: Lead-2 (extractive baseline) Summary: We've known each other through school but never talked until we had a class together. I asked her out after about a week, we went out for food, laughed, flirted etc etc. Overall score: 1 Accuracy: 7 Coherence: 7 Coverage: 1 Policy: 1.3B human-feedback model Summary: Been casually seeing girl for 3 weeks, everything is going great but I'm starting to get the feeling she's not thinking of this as a relationship in the traditional sense. Should I ask if we're together as bf/gf? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B human-feedback model Summary: Been on three dates with a girl, getting the feeling she's not interested in a relationship in the traditional sense. Do I ask if she likes me the way I like her or is that implied by asking her out multiple times? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B supervised model Summary: I'm a [18 M] she's a [17 F] and I don't know how to read this relationship? Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2 Policy: 6.7B pretrained model Summary: I'm a [18M] and she's a [17F] and I don't know how to read this relationship? Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\n\nExamples of evaluations on the TL;DR dataset. For further random examples of evaluations, see our website. POST Subreddit: r/relationships TITLE: Help! My. 2524F] boyfriend [28M] is neglecting his studies and our relationship.... Because of a video gameTable 25: Examples of evaluations on the TL;DR dataset. For further random examples of evaluations, see our website. POST Subreddit: r/relationships TITLE: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship.... Because of a video game.\n\nMy boyfriend and I have been dating for a year and it has been great. Except for one thing. Hi Reddit, Hi Reddit. My boyfriend and I have been dating for a year and it has been great. Except for one thing.\n\nEven saying this game is making me rethink our relationship! He says he understands, but isn't proving it through his actions! Reddit, what should I do? I really care about this guy and besides this gaming habit, our relationship is great! Policy: Human written reference TL;DR Summary: my boyfriend games whenever he can! How can I get him to stop gaming so much and focus more on school and our relationship?. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B human-feedback model Summary: My boyfriend is neglecting his studies and our relationship because of his excessive gaming of a video game. What can I do to get him to stop? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B supervised model Summary: Boyfriend plays video games for hours on end, neglecting his studies and our relationship. What can I do?. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B pretrained model Summary: My [24F] boyfriend [28M] is neglecting his studies and our relationship because of a video game. Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 5DOTA. He plays it at least 2 hours a day, and on the weekends, all day if he can. If you know that game you know it takes at least an hour on average per game and is one that you can't really pause. Sometimes it is fine that he plays, I don't mind sitting on my phone, browsing Reddit. However, lately it seems like it has interfered in our relationship as well as his studies. He is doing his PH.D. in Physics. He is obviously very swamped with schoolwork that he seems to put on the back burner for this game. The other day, on a Saturday, I was over and he was playing a game. I thought it would just be one but instead he proceeded to play for three hours as I just sat there. What can I do? I have tried talking to him... Even saying this game is making me rethink our relationship! He says he understands, but isn't proving it through his actions! Reddit, what should I do? I really care about this guy and besides this gaming habit, our relationship is great! Policy: Human written reference TL;DR Summary: my boyfriend games whenever he can! How can I get him to stop gaming so much and focus more on school and our relationship? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: Title (extractive baseline) Summary: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship.... Because of a video game. Overall score: 6 Accuracy: 7 Coherence: 6 Coverage: 6 Policy: Lead-2 (extractive baseline) Summary: Hi Reddit. My boyfriend and I have been dating for a year and it has been great. Overall score: 2 Accuracy: 7 Coherence: 7 Coverage: 1 Policy: 1.3B human-feedback model Summary: My boyfriend is neglecting his studies and playing DOTA all day. It is interfering with his schoolwork and our relationship. What can I do to fix this? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B human-feedback model Summary: My boyfriend is neglecting his studies and our relationship because of his excessive gaming of a video game. What can I do to get him to stop? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B supervised model Summary: Boyfriend plays video games for hours on end, neglecting his studies and our relationship. What can I do? Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 6.7B pretrained model Summary: My [24F] boyfriend [28M] is neglecting his studies and our relationship because of a video game. Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 5\n\nFor more random examples of evaluations, see our website. ARTICLE Site: dailymail TITLE: Tech firm uses pole dancing ROBOTS to attract people in Shoreditch Technology shows such as the annual E3 games conference in Los Angeles have become known for their scantily clad. Table 26: Examples of evaluations on the TL;DR dataset. booth babes' designed to attract people to standsTable 26: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations, see our website. ARTICLE Site: dailymail TITLE: Tech firm uses pole dancing ROBOTS to attract people in Shoreditch Technology shows such as the annual E3 games conference in Los Angeles have become known for their scantily clad 'booth babes' designed to attract people to stands.\n\nTransip hired a pair of pole dancing robots for an event in Shoreditch. The dancers, which cost around \u00a32,500 to hire, were made out of old car parts and were designed by British artist Giles Walker. The robots will be pole dancing for the remaining four days of the event, along with presentations from leading figures in the technology industry. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 1.3B human-feedback model (transfer) Summary: Dutch firm transip hired a pair of pole dancing robots for an event in London to attract technology enthusiasts to stand. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B human-feedback model (transfer) Summary: Dutch hosting company transip hired pole dancing robots to entertain Tech enthusiasts at its London event. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B supervised TL;DR model (transfer) Summary: Dutch firm transip hired pole dancing robots to entertain its London event. Overall score: 4 Accuracy: 6 Coherence: 5 Coverage: 5 Policy: 6.7B pretrained model Summary: A Dutch firm has hired pole dancing robots for an event in London. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B supervised CNN/DM. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Table 27: Examples of evaluations on the CNN/DM dataset. For further random examples of evaluations. see our website. ARTICLE Site: dailymail TITLE: Crying toddler removed from Transavia Airlines flight and met by police A crying 19-month-old girl was removed from a plane and met by police after she was accused of causing a security breachHowever, a Dutch firm has taken a slightly different approach for its London event. TransIP hired a pair of pole dancing robots for an event in Shoreditch. [truncated for brevity] LINK: https://www.dailymail.co.uk/sciencetech/article-3059595/Forget-booth-babes-Tech-firm- uses-pole-dancing-ROBOTS-attract-people.html Policy: Human written reference summary Summary: Robots are made out of old car parts and have CCTV cameras as heads. Originally created by artist Giles Walker for a show called 'peepshow'. Overall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3 Policy: Lead-3 (extractive baseline) Summary: Technology shows such as the annual E3 games conference in Los Angeles have become known for their scantily clad 'booth babes' designed to attract people to stands. However, a Dutch firm has taken a slightly different approach for its London event. Transip hired a pair of pole dancing robots for an event in Shoreditch. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 1.3B human-feedback model (transfer) Summary: Dutch firm transip hired a pair of pole dancing robots for an event in London to attract technology enthusiasts to stand. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B human-feedback model (transfer) Summary: Dutch hosting company transip hired pole dancing robots to entertain Tech enthusiasts at its London event. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B supervised TL;DR model (transfer) Summary: Dutch firm transip hired pole dancing robots to entertain its London event. Overall score: 4 Accuracy: 6 Coherence: 5 Coverage: 5 Policy: 6.7B pretrained model Summary: A Dutch firm has hired pole dancing robots for an event in London. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B supervised CNN/DM model Summary: Robots were originally created by Walker for a show called 'peepshow' the dancers, which cost around \u00a32,500 to hire, were made out of old car parts. Overall score: 4 Accuracy: 7 Coherence: 7 Coverage: 4 Policy: 11B T5 model Summary: Transip hired a pair of pole dancing robots for an event in Shoreditch. The dancers, which cost around \u00a32,500 to hire, were made out of old car parts and were designed by British artist Giles Walker. The robots will be pole dancing for the remaining four days of the event, along with presentations from leading figures in the technology industry. Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Table 27: Examples of evaluations on the CNN/DM dataset. For further random examples of evaluations, see our website. ARTICLE Site: dailymail TITLE: Crying toddler removed from Transavia Airlines flight and met by police A crying 19-month-old girl was removed from a plane and met by police after she was accused of causing a security breach.\n\nAs the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton. Sarina Aziz became agitated after cabin crew asked that she sit on her parents' lapAs the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton, Sarina Aziz became agitated after cabin crew asked that she sit on her parents' lap.\n\nwho live in north London, struggled to keep their daughter still after strapping her in the connector belt required for children under the age of two. Ariella , Mark Aziz, Ariella and Mark Aziz, who live in north London, struggled to keep their daughter still after strapping her in the connector belt required for children under the age of two.\n\nAs the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton, Sarina Aziz became agitated after cabin crew asked that she sit on her parents' lap. Ariella and Mark Aziz, who live in north London, struggled to keep their daughter still after strapping her in the connector belt required for children under the age of two. Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6 Policy: 1.3B human-feedback model (transfer) Summary: 19-month-old girl was removed from a Transavia Airlines flight after cabin crew accused her of causing a security breach. The incident was later determined to have been justified. Policy: Human written reference summary Summary: Sarina Aziz was flying back from Israel with parents Mark and ariella Aziz. But girl became agitated after being placed on the parents' lap. model Summary: A family was kicked off a flight after their son refused to wear a seatbelt, a passenger claims. The family were told they could not fly without a doctor's note clearing them to fly Overall score: 2 Accuracy: 6 Coherence: 7 Coverage: 3 Policy: 6.7B supervised CNN/DM model Summary: Sarina Aziz became agitated after being asked to sit on her parents' lap. The 19-month. But she became frustrated and unhappy after being moved from her seat. Transavia Airlines staff alerted the captain who halted the flight. The family were removed from the aircraft and met by police at the AirportOne-year-old Sarina Aziz became agitated on a flight from Israel to Luton that led to her being ejected from the plane [truncated for brevity] LINK: https://www.dailymail.co.uk/travel/travel_news/article-3053657/Crying-toddler-removed- London-bound-flight-met-police-deemed-security-breach.html Policy: Human written reference summary Summary: Sarina Aziz was flying back from Israel with parents Mark and ariella Aziz. But girl became agitated after being placed on the parents' lap. Pilot turned plane around at Ben Gurion, and armed police ejected family. Father Mark Aziz insists family were being compliant and asking for help . Mother speaks of her disbelief at how the incident was handled by staff . Overall score: 3 Accuracy: 4 Coherence: 7 Coverage: 6 Policy: Lead-3 (extractive baseline) Summary: A crying 19-month-old girl was removed from a plane and met by police after she was accused of causing a security breach. As the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton, Sarina Aziz became agitated after cabin crew asked that she sit on her parents' lap. Ariella and Mark Aziz, who live in north London, struggled to keep their daughter still after strapping her in the connector belt required for children under the age of two. Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6 Policy: 1.3B human-feedback model (transfer) Summary: 19-month-old girl was removed from a Transavia Airlines flight after cabin crew accused her of causing a security breach. The incident was later determined to have been justified. Overall score: 3 Accuracy: 4 Coherence: 7 Coverage: 4 Policy: 6.7B human-feedback model (transfer) Summary: Family were removed from a plane after a crying 19-month-old girl was accused of causing a security breach. Family say they were simply trying to calm their child down. Transavia Airlines have said the action was justified. Overall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6 Policy: 6.7B supervised TL;DR model (transfer) Summary: Family were removed from a plane after a one-year-old girl became unhappy after being moved from her seat to their laps. Overall score: 4 Accuracy: 7 Coherence: 7 Coverage: 5 Policy: 6.7B pretrained model Summary: A family was kicked off a flight after their son refused to wear a seatbelt, a passenger claims. The family were told they could not fly without a doctor's note clearing them to fly Overall score: 2 Accuracy: 6 Coherence: 7 Coverage: 3 Policy: 6.7B supervised CNN/DM model Summary: Sarina Aziz became agitated after being asked to sit on her parents' lap. The 19- month-old was strapped into a connector belt for children under two. But she became frustrated and unhappy after being moved from her seat. Transavia Airlines staff alerted the captain who halted the flight. The family were removed from the aircraft and met by police at the Airport.\n\nOne-year-old Sarina Aziz became agitated on a flight from Israel to Luton. Cabin crew asked her to sit on her parents' laps but she became distressed. Transavia Airlines said the action was justified as the child was not 'following safety instructions' Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 11B T5 model Summary. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 6 Table 28: Examples of evaluations on the CNN/DM dataset. For further random examples of evaluations. see our website. Reference summaryTransavia Airlines said the action was justified as the child was not 'following safety instructions' Overall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7 Policy: 11B T5 model Summary: One-year-old Sarina Aziz became agitated on a flight from Israel to Luton. Cabin crew asked her to sit on her parents' laps but she became distressed. Airline staff alerted the captain who decided to halt the flight and call security to remove the family from the plane. Overall score: 5 Accuracy: 7 Coherence: 7 Coverage: 6 Table 28: Examples of evaluations on the CNN/DM dataset. For further random examples of evaluations, see our website. Reference summary\n", "annotations": {"author": "[{\"end\":60,\"start\":45},{\"end\":73,\"start\":61},{\"end\":82,\"start\":74},{\"end\":100,\"start\":83},{\"end\":111,\"start\":101},{\"end\":125,\"start\":112},{\"end\":139,\"start\":126},{\"end\":153,\"start\":140},{\"end\":170,\"start\":154},{\"end\":178,\"start\":171}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":51},{\"end\":72,\"start\":66},{\"end\":81,\"start\":79},{\"end\":99,\"start\":92},{\"end\":110,\"start\":106},{\"end\":124,\"start\":120},{\"end\":138,\"start\":131},{\"end\":152,\"start\":146},{\"end\":169,\"start\":159},{\"end\":177,\"start\":171}]", "author_first_name": "[{\"end\":50,\"start\":45},{\"end\":65,\"start\":61},{\"end\":78,\"start\":74},{\"end\":89,\"start\":83},{\"end\":91,\"start\":90},{\"end\":105,\"start\":101},{\"end\":119,\"start\":112},{\"end\":130,\"start\":126},{\"end\":145,\"start\":140},{\"end\":158,\"start\":154}]", "author_affiliation": null, "title": "[{\"end\":42,\"start\":1},{\"end\":220,\"start\":179}]", "venue": null, "abstract": "[{\"end\":2185,\"start\":222}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2933,\"start\":2929},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3158,\"start\":3157},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3631,\"start\":3627},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3634,\"start\":3631},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3810,\"start\":3806},{\"end\":3813,\"start\":3810},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4189,\"start\":4185},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4191,\"start\":4189},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4194,\"start\":4191},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4197,\"start\":4194},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4200,\"start\":4197},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4398,\"start\":4394},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4470,\"start\":4466},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4473,\"start\":4470},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4475,\"start\":4473},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4478,\"start\":4475},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4507,\"start\":4504},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":4510,\"start\":4507},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4588,\"start\":4584},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4960,\"start\":4956},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5152,\"start\":5148},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5154,\"start\":5152},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5464,\"start\":5460},{\"end\":6700,\"start\":6687},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7904,\"start\":7901},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7907,\"start\":7904},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7924,\"start\":7921},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8118,\"start\":8114},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8158,\"start\":8154},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8477,\"start\":8473},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8810,\"start\":8807},{\"end\":8813,\"start\":8810},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8831,\"start\":8827},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8833,\"start\":8831},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8856,\"start\":8852},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8879,\"start\":8875},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8902,\"start\":8899},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8932,\"start\":8928},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9015,\"start\":9011},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9100,\"start\":9097},{\"end\":9103,\"start\":9100},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9163,\"start\":9159},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9166,\"start\":9163},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9274,\"start\":9270},{\"end\":9277,\"start\":9274},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9404,\"start\":9400},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9407,\"start\":9404},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9410,\"start\":9407},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9413,\"start\":9410},{\"end\":9416,\"start\":9413},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9443,\"start\":9439},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9446,\"start\":9443},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9448,\"start\":9446},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9451,\"start\":9448},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9475,\"start\":9471},{\"end\":9478,\"start\":9475},{\"end\":9481,\"start\":9478},{\"end\":9557,\"start\":9553},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9560,\"start\":9557},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9593,\"start\":9589},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9596,\"start\":9593},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9599,\"start\":9596},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9602,\"start\":9599},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9605,\"start\":9602},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9608,\"start\":9605},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9684,\"start\":9683},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9765,\"start\":9761},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10828,\"start\":10824},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11367,\"start\":11363},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12338,\"start\":12337},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12376,\"start\":12372},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14079,\"start\":14077},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14179,\"start\":14175},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":14468,\"start\":14464},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15479,\"start\":15475},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15506,\"start\":15502},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15508,\"start\":15506},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15658,\"start\":15654},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15661,\"start\":15658},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15772,\"start\":15768},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15774,\"start\":15772},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":16737,\"start\":16733},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17809,\"start\":17805},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17849,\"start\":17848},{\"end\":18121,\"start\":18117},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20804,\"start\":20800},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23951,\"start\":23947},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24060,\"start\":24057},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25975,\"start\":25974},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27349,\"start\":27345},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28719,\"start\":28715},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29088,\"start\":29085},{\"end\":29221,\"start\":29217},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29224,\"start\":29221},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29227,\"start\":29224},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":29230,\"start\":29227},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30189,\"start\":30185},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33745,\"start\":33743},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36463,\"start\":36459},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36667,\"start\":36665},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36734,\"start\":36730},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36869,\"start\":36865},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38248,\"start\":38244},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38628,\"start\":38624},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":39563,\"start\":39559},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":57539,\"start\":57535},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":58024,\"start\":58020},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":58041,\"start\":58037},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":58249,\"start\":58245},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":59742,\"start\":59740},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":63235,\"start\":63231},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":66683,\"start\":66679},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":66750,\"start\":66746},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":66864,\"start\":66862},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":90486,\"start\":90482},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":90488,\"start\":90487}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":71838,\"start\":71741},{\"attributes\":{\"id\":\"fig_1\"},\"end\":72278,\"start\":71839},{\"attributes\":{\"id\":\"fig_2\"},\"end\":72357,\"start\":72279},{\"attributes\":{\"id\":\"fig_3\"},\"end\":72990,\"start\":72358},{\"attributes\":{\"id\":\"fig_4\"},\"end\":74720,\"start\":72991},{\"attributes\":{\"id\":\"fig_5\"},\"end\":74770,\"start\":74721},{\"attributes\":{\"id\":\"fig_6\"},\"end\":74868,\"start\":74771},{\"attributes\":{\"id\":\"fig_7\"},\"end\":74967,\"start\":74869},{\"attributes\":{\"id\":\"fig_8\"},\"end\":75064,\"start\":74968},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":75879,\"start\":75065},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":78357,\"start\":75880},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":80985,\"start\":78358},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":81084,\"start\":80986},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":81666,\"start\":81085},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":83091,\"start\":81667},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":83875,\"start\":83092},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":84079,\"start\":83876},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":84878,\"start\":84080},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":86013,\"start\":84879},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":87743,\"start\":86014},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":89473,\"start\":87744},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":89858,\"start\":89474},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":90231,\"start\":89859}]", "paragraph": "[{\"end\":2535,\"start\":2201},{\"end\":3896,\"start\":2537},{\"end\":4479,\"start\":3898},{\"end\":5232,\"start\":4481},{\"end\":5271,\"start\":5234},{\"end\":5730,\"start\":5273},{\"end\":6485,\"start\":5732},{\"end\":7050,\"start\":6487},{\"end\":7299,\"start\":7052},{\"end\":7774,\"start\":7301},{\"end\":8704,\"start\":7791},{\"end\":9649,\"start\":8706},{\"end\":9684,\"start\":9651},{\"end\":10050,\"start\":9714},{\"end\":10425,\"start\":10052},{\"end\":10629,\"start\":10427},{\"end\":10829,\"start\":10631},{\"end\":11292,\"start\":10856},{\"end\":12071,\"start\":11314},{\"end\":12766,\"start\":12073},{\"end\":13359,\"start\":12768},{\"end\":13620,\"start\":13361},{\"end\":13965,\"start\":13622},{\"end\":14080,\"start\":13967},{\"end\":14450,\"start\":14110},{\"end\":15126,\"start\":14452},{\"end\":15421,\"start\":15128},{\"end\":15620,\"start\":15432},{\"end\":15998,\"start\":15622},{\"end\":16490,\"start\":16000},{\"end\":16771,\"start\":16492},{\"end\":17131,\"start\":16773},{\"end\":17486,\"start\":17204},{\"end\":18159,\"start\":17488},{\"end\":18514,\"start\":18215},{\"end\":18915,\"start\":18516},{\"end\":19832,\"start\":18974},{\"end\":20475,\"start\":19834},{\"end\":21485,\"start\":20477},{\"end\":21980,\"start\":21527},{\"end\":22716,\"start\":21982},{\"end\":23292,\"start\":22751},{\"end\":24061,\"start\":23294},{\"end\":24533,\"start\":24063},{\"end\":24985,\"start\":24535},{\"end\":25717,\"start\":24987},{\"end\":26116,\"start\":25767},{\"end\":26903,\"start\":26118},{\"end\":27191,\"start\":26905},{\"end\":27482,\"start\":27193},{\"end\":28230,\"start\":27497},{\"end\":28720,\"start\":28232},{\"end\":29089,\"start\":28722},{\"end\":29552,\"start\":29091},{\"end\":29841,\"start\":29554},{\"end\":30362,\"start\":29843},{\"end\":31081,\"start\":30364},{\"end\":31565,\"start\":31083},{\"end\":32272,\"start\":31567},{\"end\":32826,\"start\":32274},{\"end\":34223,\"start\":32828},{\"end\":35443,\"start\":34225},{\"end\":36238,\"start\":35445},{\"end\":36735,\"start\":36295},{\"end\":37225,\"start\":36737},{\"end\":37691,\"start\":37227},{\"end\":38411,\"start\":37693},{\"end\":39171,\"start\":38413},{\"end\":39452,\"start\":39192},{\"end\":39890,\"start\":39454},{\"end\":40153,\"start\":39975},{\"end\":40430,\"start\":40155},{\"end\":40992,\"start\":40432},{\"end\":41655,\"start\":40994},{\"end\":42076,\"start\":41657},{\"end\":42923,\"start\":42078},{\"end\":43030,\"start\":42925},{\"end\":43468,\"start\":43071},{\"end\":43942,\"start\":43470},{\"end\":44552,\"start\":43944},{\"end\":45030,\"start\":44554},{\"end\":45177,\"start\":45032},{\"end\":45546,\"start\":45179},{\"end\":45970,\"start\":45548},{\"end\":46412,\"start\":45999},{\"end\":46824,\"start\":46414},{\"end\":47722,\"start\":46848},{\"end\":48177,\"start\":47756},{\"end\":48464,\"start\":48179},{\"end\":48970,\"start\":48466},{\"end\":49263,\"start\":48972},{\"end\":49323,\"start\":49265},{\"end\":49396,\"start\":49325},{\"end\":49473,\"start\":49398},{\"end\":49545,\"start\":49475},{\"end\":49594,\"start\":49547},{\"end\":49665,\"start\":49596},{\"end\":50072,\"start\":49667},{\"end\":50207,\"start\":50074},{\"end\":50662,\"start\":50209},{\"end\":51177,\"start\":50664},{\"end\":51799,\"start\":51220},{\"end\":52774,\"start\":51801},{\"end\":53160,\"start\":52788},{\"end\":53367,\"start\":53172},{\"end\":53770,\"start\":53398},{\"end\":55263,\"start\":53782},{\"end\":56227,\"start\":55310},{\"end\":57055,\"start\":56229},{\"end\":57497,\"start\":57081},{\"end\":58025,\"start\":57499},{\"end\":58334,\"start\":58027},{\"end\":58654,\"start\":58377},{\"end\":58856,\"start\":58656},{\"end\":59376,\"start\":58858},{\"end\":59918,\"start\":59378},{\"end\":60832,\"start\":59920},{\"end\":61514,\"start\":60834},{\"end\":62004,\"start\":61551},{\"end\":62672,\"start\":62006},{\"end\":63046,\"start\":62674},{\"end\":63707,\"start\":63078},{\"end\":64098,\"start\":63757},{\"end\":64823,\"start\":64138},{\"end\":65238,\"start\":64825},{\"end\":66288,\"start\":65259},{\"end\":66865,\"start\":66290},{\"end\":67454,\"start\":66899},{\"end\":67943,\"start\":67491},{\"end\":68787,\"start\":67945},{\"end\":68982,\"start\":68789},{\"end\":70150,\"start\":69047},{\"end\":70275,\"start\":70169},{\"end\":70444,\"start\":70285},{\"end\":70583,\"start\":70446},{\"end\":70590,\"start\":70585},{\"end\":70729,\"start\":70592},{\"end\":70877,\"start\":70739},{\"end\":71057,\"start\":70887},{\"end\":71740,\"start\":71066}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17203,\"start\":17132},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18214,\"start\":18160}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":13551,\"start\":13441},{\"end\":26580,\"start\":26572},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33630,\"start\":33623},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35468,\"start\":35461},{\"end\":35837,\"start\":35830},{\"end\":37224,\"start\":37217},{\"end\":39828,\"start\":39821},{\"end\":46539,\"start\":46532},{\"end\":47259,\"start\":47252},{\"end\":48307,\"start\":48300},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":48777,\"start\":48770},{\"end\":49995,\"start\":49988},{\"end\":52579,\"start\":52572},{\"end\":52751,\"start\":52744},{\"end\":53996,\"start\":53988},{\"end\":54430,\"start\":54422},{\"end\":54494,\"start\":54486},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":55030,\"start\":54566},{\"end\":55186,\"start\":55178},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":55477,\"start\":55469},{\"end\":58653,\"start\":58645},{\"end\":58774,\"start\":58766},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":62925,\"start\":62917},{\"end\":65602,\"start\":65594},{\"end\":66510,\"start\":66502},{\"end\":66910,\"start\":66902},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":67629,\"start\":67613},{\"end\":68247,\"start\":68239},{\"end\":68877,\"start\":68869},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":69478,\"start\":69470},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":69553,\"start\":69545},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":69628,\"start\":69620},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":69711,\"start\":69703},{\"end\":71214,\"start\":71206}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2199,\"start\":2187},{\"attributes\":{\"n\":\"2\"},\"end\":7789,\"start\":7777},{\"end\":9712,\"start\":9687},{\"attributes\":{\"n\":\"1\"},\"end\":10854,\"start\":10832},{\"attributes\":{\"n\":\"3.2\"},\"end\":11312,\"start\":11295},{\"attributes\":{\"n\":\"3.3\"},\"end\":14108,\"start\":14083},{\"attributes\":{\"n\":\"3.4\"},\"end\":15430,\"start\":15424},{\"attributes\":{\"n\":\"4\"},\"end\":18925,\"start\":18918},{\"attributes\":{\"n\":\"4.1\"},\"end\":18972,\"start\":18928},{\"attributes\":{\"n\":\"4.2\"},\"end\":21525,\"start\":21488},{\"attributes\":{\"n\":\"4.3\"},\"end\":22749,\"start\":22719},{\"attributes\":{\"n\":\"4.4\"},\"end\":25765,\"start\":25720},{\"attributes\":{\"n\":\"5\"},\"end\":27495,\"start\":27485},{\"end\":36293,\"start\":36241},{\"end\":39190,\"start\":39174},{\"end\":39973,\"start\":39893},{\"end\":43069,\"start\":43033},{\"end\":45997,\"start\":45973},{\"end\":46846,\"start\":46827},{\"end\":47754,\"start\":47725},{\"end\":51218,\"start\":51180},{\"end\":52786,\"start\":52777},{\"end\":53170,\"start\":53163},{\"end\":53378,\"start\":53370},{\"end\":53396,\"start\":53381},{\"end\":53780,\"start\":53773},{\"end\":55277,\"start\":55266},{\"end\":55308,\"start\":55280},{\"end\":57079,\"start\":57058},{\"end\":58375,\"start\":58337},{\"end\":61549,\"start\":61517},{\"end\":63076,\"start\":63049},{\"end\":63755,\"start\":63710},{\"end\":64136,\"start\":64101},{\"end\":65257,\"start\":65241},{\"end\":66897,\"start\":66868},{\"end\":67489,\"start\":67457},{\"end\":69045,\"start\":68985},{\"end\":70167,\"start\":70153},{\"end\":70283,\"start\":70278},{\"end\":70737,\"start\":70732},{\"end\":70885,\"start\":70880},{\"end\":71064,\"start\":71060},{\"end\":71752,\"start\":71742},{\"end\":71850,\"start\":71840},{\"end\":72290,\"start\":72280},{\"end\":72379,\"start\":72359},{\"end\":74733,\"start\":74722},{\"end\":74783,\"start\":74772},{\"end\":74881,\"start\":74870},{\"end\":74980,\"start\":74969},{\"end\":78376,\"start\":78359},{\"end\":80996,\"start\":80987},{\"end\":81677,\"start\":81668},{\"end\":83103,\"start\":83093},{\"end\":83887,\"start\":83877},{\"end\":86025,\"start\":86015},{\"end\":87755,\"start\":87745},{\"end\":89485,\"start\":89475},{\"end\":89870,\"start\":89860}]", "table": "[{\"end\":75879,\"start\":75798},{\"end\":80985,\"start\":80131},{\"end\":81666,\"start\":81180},{\"end\":83091,\"start\":81971},{\"end\":83875,\"start\":83106},{\"end\":84878,\"start\":84327},{\"end\":86013,\"start\":85276},{\"end\":87743,\"start\":86384},{\"end\":89473,\"start\":88114}]", "figure_caption": "[{\"end\":71838,\"start\":71754},{\"end\":72278,\"start\":71852},{\"end\":72357,\"start\":72292},{\"end\":72990,\"start\":72382},{\"end\":74720,\"start\":72993},{\"end\":74770,\"start\":74736},{\"end\":74868,\"start\":74786},{\"end\":74967,\"start\":74884},{\"end\":75064,\"start\":74983},{\"end\":75798,\"start\":75067},{\"end\":78357,\"start\":75882},{\"end\":80131,\"start\":78379},{\"end\":81084,\"start\":80998},{\"end\":81180,\"start\":81087},{\"end\":81971,\"start\":81679},{\"end\":84079,\"start\":83890},{\"end\":84327,\"start\":84082},{\"end\":85276,\"start\":84881},{\"end\":86384,\"start\":86028},{\"end\":88114,\"start\":87758},{\"end\":89858,\"start\":89488},{\"end\":90231,\"start\":89873}]", "figure_ref": "[{\"end\":3031,\"start\":3023},{\"end\":5728,\"start\":5720},{\"end\":6618,\"start\":6610},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9986,\"start\":9978},{\"end\":19147,\"start\":19139},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21092,\"start\":21084},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21654,\"start\":21646},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22233,\"start\":22225},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22364,\"start\":22355},{\"end\":23539,\"start\":23531},{\"end\":24531,\"start\":24523},{\"end\":26323,\"start\":26315},{\"end\":27218,\"start\":27210},{\"end\":35896,\"start\":35888},{\"end\":44156,\"start\":44148},{\"end\":45063,\"start\":45055},{\"end\":47610,\"start\":47602},{\"end\":61098,\"start\":61090},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":62485,\"start\":62475},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":62501,\"start\":62491},{\"end\":63268,\"start\":63259},{\"end\":63583,\"start\":63574},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":63908,\"start\":63899},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":63951,\"start\":63942},{\"end\":64555,\"start\":64547},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":65272,\"start\":65262}]", "bib_author_first_name": "[{\"end\":96109,\"start\":96108},{\"end\":96121,\"start\":96120},{\"end\":96131,\"start\":96130},{\"end\":96137,\"start\":96136},{\"end\":96146,\"start\":96145},{\"end\":96154,\"start\":96153},{\"end\":96164,\"start\":96163},{\"end\":96177,\"start\":96176},{\"end\":96458,\"start\":96457},{\"end\":96460,\"start\":96459},{\"end\":96471,\"start\":96470},{\"end\":96473,\"start\":96472},{\"end\":96485,\"start\":96484},{\"end\":96487,\"start\":96486},{\"end\":96667,\"start\":96666},{\"end\":96675,\"start\":96674},{\"end\":96682,\"start\":96681},{\"end\":96684,\"start\":96683},{\"end\":96693,\"start\":96692},{\"end\":96704,\"start\":96703},{\"end\":96713,\"start\":96712},{\"end\":97028,\"start\":97027},{\"end\":97030,\"start\":97029},{\"end\":97039,\"start\":97038},{\"end\":97047,\"start\":97046},{\"end\":97056,\"start\":97055},{\"end\":97067,\"start\":97066},{\"end\":97077,\"start\":97076},{\"end\":97089,\"start\":97088},{\"end\":97104,\"start\":97103},{\"end\":97113,\"start\":97112},{\"end\":97123,\"start\":97122},{\"end\":97133,\"start\":97132},{\"end\":97144,\"start\":97143},{\"end\":97160,\"start\":97159},{\"end\":97171,\"start\":97170},{\"end\":97183,\"start\":97182},{\"end\":97192,\"start\":97191},{\"end\":97202,\"start\":97201},{\"end\":97204,\"start\":97203},{\"end\":97215,\"start\":97214},{\"end\":97221,\"start\":97220},{\"end\":97231,\"start\":97230},{\"end\":97240,\"start\":97239},{\"end\":97248,\"start\":97247},{\"end\":97258,\"start\":97257},{\"end\":97268,\"start\":97267},{\"end\":97276,\"start\":97275},{\"end\":97285,\"start\":97284},{\"end\":97294,\"start\":97293},{\"end\":97304,\"start\":97303},{\"end\":97318,\"start\":97317},{\"end\":97329,\"start\":97328},{\"end\":97342,\"start\":97341},{\"end\":97802,\"start\":97801},{\"end\":97810,\"start\":97809},{\"end\":97825,\"start\":97824},{\"end\":97836,\"start\":97835},{\"end\":97851,\"start\":97850},{\"end\":97859,\"start\":97858},{\"end\":97868,\"start\":97867},{\"end\":97877,\"start\":97876},{\"end\":97886,\"start\":97885},{\"end\":97896,\"start\":97895},{\"end\":98321,\"start\":98320},{\"end\":98323,\"start\":98322},{\"end\":98335,\"start\":98334},{\"end\":98346,\"start\":98345},{\"end\":98597,\"start\":98596},{\"end\":98599,\"start\":98598},{\"end\":98606,\"start\":98605},{\"end\":98615,\"start\":98614},{\"end\":98624,\"start\":98623},{\"end\":98630,\"start\":98629},{\"end\":98640,\"start\":98639},{\"end\":98652,\"start\":98651},{\"end\":98660,\"start\":98659},{\"end\":98954,\"start\":98953},{\"end\":98964,\"start\":98963},{\"end\":98972,\"start\":98971},{\"end\":98974,\"start\":98973},{\"end\":99589,\"start\":99588},{\"end\":99603,\"start\":99602},{\"end\":99616,\"start\":99615},{\"end\":99848,\"start\":99847},{\"end\":99850,\"start\":99849},{\"end\":99864,\"start\":99863},{\"end\":99873,\"start\":99872},{\"end\":99882,\"start\":99881},{\"end\":99892,\"start\":99891},{\"end\":99900,\"start\":99899},{\"end\":100214,\"start\":100213},{\"end\":100227,\"start\":100226},{\"end\":100236,\"start\":100235},{\"end\":100567,\"start\":100566},{\"end\":100569,\"start\":100568},{\"end\":100576,\"start\":100575},{\"end\":100578,\"start\":100577},{\"end\":100772,\"start\":100771},{\"end\":100781,\"start\":100780},{\"end\":100792,\"start\":100791},{\"end\":100804,\"start\":100803},{\"end\":100815,\"start\":100814},{\"end\":100829,\"start\":100828},{\"end\":101265,\"start\":101264},{\"end\":101273,\"start\":101272},{\"end\":101281,\"start\":101280},{\"end\":101289,\"start\":101288},{\"end\":101296,\"start\":101295},{\"end\":101303,\"start\":101302},{\"end\":101311,\"start\":101310},{\"end\":101318,\"start\":101317},{\"end\":101329,\"start\":101325},{\"end\":101619,\"start\":101618},{\"end\":101627,\"start\":101626},{\"end\":101635,\"start\":101634},{\"end\":101647,\"start\":101646},{\"end\":101659,\"start\":101658},{\"end\":101663,\"start\":101660},{\"end\":101993,\"start\":101992},{\"end\":102001,\"start\":102000},{\"end\":102010,\"start\":102009},{\"end\":102482,\"start\":102481},{\"end\":102758,\"start\":102757},{\"end\":103212,\"start\":103211},{\"end\":103224,\"start\":103223},{\"end\":103234,\"start\":103233},{\"end\":103241,\"start\":103240},{\"end\":103254,\"start\":103253},{\"end\":103541,\"start\":103540},{\"end\":103543,\"start\":103542},{\"end\":103553,\"start\":103552},{\"end\":103759,\"start\":103758},{\"end\":103771,\"start\":103770},{\"end\":103782,\"start\":103781},{\"end\":103793,\"start\":103792},{\"end\":104040,\"start\":104039},{\"end\":104054,\"start\":104053},{\"end\":104056,\"start\":104055},{\"end\":104066,\"start\":104065},{\"end\":104076,\"start\":104075},{\"end\":104085,\"start\":104084},{\"end\":104841,\"start\":104840},{\"end\":104853,\"start\":104852},{\"end\":105052,\"start\":105051},{\"end\":105061,\"start\":105060},{\"end\":105072,\"start\":105071},{\"end\":105083,\"start\":105082},{\"end\":105093,\"start\":105092},{\"end\":105102,\"start\":105101},{\"end\":105499,\"start\":105498},{\"end\":105508,\"start\":105507},{\"end\":105515,\"start\":105514},{\"end\":105524,\"start\":105523},{\"end\":105541,\"start\":105540},{\"end\":105552,\"start\":105551},{\"end\":105560,\"start\":105559},{\"end\":105572,\"start\":105571},{\"end\":105966,\"start\":105965},{\"end\":105972,\"start\":105971},{\"end\":105982,\"start\":105981},{\"end\":106259,\"start\":106258},{\"end\":106473,\"start\":106469},{\"end\":106480,\"start\":106479},{\"end\":106482,\"start\":106481},{\"end\":107019,\"start\":107015},{\"end\":107217,\"start\":107216},{\"end\":107227,\"start\":107226},{\"end\":107238,\"start\":107237},{\"end\":107248,\"start\":107247},{\"end\":107379,\"start\":107378},{\"end\":107389,\"start\":107388},{\"end\":107391,\"start\":107390},{\"end\":107401,\"start\":107400},{\"end\":107410,\"start\":107409},{\"end\":107780,\"start\":107779},{\"end\":107790,\"start\":107789},{\"end\":107801,\"start\":107798},{\"end\":107805,\"start\":107804},{\"end\":108077,\"start\":108076},{\"end\":108084,\"start\":108083},{\"end\":108366,\"start\":108365},{\"end\":108376,\"start\":108375},{\"end\":108385,\"start\":108384},{\"end\":108623,\"start\":108622},{\"end\":108632,\"start\":108631},{\"end\":108646,\"start\":108645},{\"end\":108656,\"start\":108655},{\"end\":108666,\"start\":108665},{\"end\":108675,\"start\":108674},{\"end\":108949,\"start\":108948},{\"end\":108960,\"start\":108959},{\"end\":108974,\"start\":108973},{\"end\":108986,\"start\":108985},{\"end\":109307,\"start\":109306},{\"end\":109318,\"start\":109317},{\"end\":109324,\"start\":109323},{\"end\":109333,\"start\":109332},{\"end\":109341,\"start\":109340},{\"end\":109351,\"start\":109350},{\"end\":109611,\"start\":109610},{\"end\":109621,\"start\":109620},{\"end\":109632,\"start\":109631},{\"end\":109643,\"start\":109642},{\"end\":109650,\"start\":109649},{\"end\":109660,\"start\":109659},{\"end\":109670,\"start\":109669},{\"end\":109678,\"start\":109677},{\"end\":109684,\"start\":109683},{\"end\":109686,\"start\":109685},{\"end\":109942,\"start\":109941},{\"end\":109953,\"start\":109952},{\"end\":109963,\"start\":109962},{\"end\":109971,\"start\":109970},{\"end\":110329,\"start\":110328},{\"end\":110331,\"start\":110330},{\"end\":110573,\"start\":110572},{\"end\":110581,\"start\":110580},{\"end\":110591,\"start\":110590},{\"end\":111096,\"start\":111095},{\"end\":111105,\"start\":111104},{\"end\":111116,\"start\":111115},{\"end\":111429,\"start\":111428},{\"end\":111431,\"start\":111430},{\"end\":111439,\"start\":111438},{\"end\":111449,\"start\":111448},{\"end\":111690,\"start\":111689},{\"end\":112107,\"start\":112106},{\"end\":112338,\"start\":112337},{\"end\":112350,\"start\":112349},{\"end\":112360,\"start\":112359},{\"end\":112370,\"start\":112369},{\"end\":112380,\"start\":112379},{\"end\":112760,\"start\":112759},{\"end\":112772,\"start\":112771},{\"end\":112782,\"start\":112781},{\"end\":112794,\"start\":112793},{\"end\":112805,\"start\":112804},{\"end\":113096,\"start\":113095},{\"end\":113103,\"start\":113102},{\"end\":113105,\"start\":113104},{\"end\":113112,\"start\":113111},{\"end\":113114,\"start\":113113},{\"end\":113370,\"start\":113369},{\"end\":113378,\"start\":113377},{\"end\":113385,\"start\":113384},{\"end\":113392,\"start\":113391},{\"end\":113401,\"start\":113397},{\"end\":113597,\"start\":113596},{\"end\":113610,\"start\":113609},{\"end\":113624,\"start\":113623},{\"end\":113633,\"start\":113632},{\"end\":113635,\"start\":113634},{\"end\":113645,\"start\":113644},{\"end\":113657,\"start\":113656},{\"end\":113659,\"start\":113658},{\"end\":113975,\"start\":113974},{\"end\":113986,\"start\":113985},{\"end\":113997,\"start\":113996},{\"end\":114007,\"start\":114006},{\"end\":114020,\"start\":114019},{\"end\":114029,\"start\":114028},{\"end\":114031,\"start\":114030},{\"end\":114040,\"start\":114039},{\"end\":114050,\"start\":114049},{\"end\":114378,\"start\":114377},{\"end\":114388,\"start\":114387},{\"end\":114400,\"start\":114399},{\"end\":114408,\"start\":114407},{\"end\":114715,\"start\":114714},{\"end\":114726,\"start\":114725},{\"end\":114737,\"start\":114736},{\"end\":114747,\"start\":114746},{\"end\":114756,\"start\":114755},{\"end\":114763,\"start\":114762},{\"end\":115083,\"start\":115082},{\"end\":115089,\"start\":115088},{\"end\":115408,\"start\":115407},{\"end\":115414,\"start\":115413},{\"end\":115426,\"start\":115425},{\"end\":115434,\"start\":115433},{\"end\":115436,\"start\":115435},{\"end\":115442,\"start\":115441},{\"end\":115453,\"start\":115452},{\"end\":115465,\"start\":115464},{\"end\":115475,\"start\":115474},{\"end\":115482,\"start\":115481},{\"end\":115489,\"start\":115488},{\"end\":115858,\"start\":115857},{\"end\":115865,\"start\":115864},{\"end\":115871,\"start\":115870},{\"end\":115879,\"start\":115878},{\"end\":115886,\"start\":115885},{\"end\":115894,\"start\":115893},{\"end\":115902,\"start\":115901},{\"end\":115911,\"start\":115910},{\"end\":116245,\"start\":116244},{\"end\":116251,\"start\":116250},{\"end\":116259,\"start\":116258},{\"end\":116269,\"start\":116268},{\"end\":116280,\"start\":116279},{\"end\":116289,\"start\":116288},{\"end\":116303,\"start\":116302},{\"end\":116316,\"start\":116315},{\"end\":116327,\"start\":116326},{\"end\":116693,\"start\":116692},{\"end\":116702,\"start\":116701},{\"end\":116715,\"start\":116714},{\"end\":116727,\"start\":116726},{\"end\":117017,\"start\":117016},{\"end\":117026,\"start\":117025},{\"end\":117034,\"start\":117033},{\"end\":117043,\"start\":117042},{\"end\":117045,\"start\":117044},{\"end\":117312,\"start\":117311},{\"end\":117321,\"start\":117320},{\"end\":117327,\"start\":117326},{\"end\":117335,\"start\":117334},{\"end\":117343,\"start\":117342},{\"end\":117631,\"start\":117630},{\"end\":117639,\"start\":117638},{\"end\":117840,\"start\":117839},{\"end\":117842,\"start\":117841},{\"end\":117853,\"start\":117852},{\"end\":117865,\"start\":117864},{\"end\":117871,\"start\":117870},{\"end\":117873,\"start\":117872},{\"end\":117882,\"start\":117881},{\"end\":117893,\"start\":117892},{\"end\":117903,\"start\":117902},{\"end\":117917,\"start\":117916},{\"end\":129916,\"start\":129914},{\"end\":139254,\"start\":139247},{\"end\":139261,\"start\":139257}]", "bib_author_last_name": "[{\"end\":96118,\"start\":96110},{\"end\":96128,\"start\":96122},{\"end\":96134,\"start\":96132},{\"end\":96143,\"start\":96138},{\"end\":96151,\"start\":96147},{\"end\":96161,\"start\":96155},{\"end\":96174,\"start\":96165},{\"end\":96184,\"start\":96178},{\"end\":96468,\"start\":96461},{\"end\":96482,\"start\":96474},{\"end\":96493,\"start\":96488},{\"end\":96672,\"start\":96668},{\"end\":96679,\"start\":96676},{\"end\":96690,\"start\":96685},{\"end\":96701,\"start\":96694},{\"end\":96710,\"start\":96705},{\"end\":96722,\"start\":96714},{\"end\":97036,\"start\":97031},{\"end\":97044,\"start\":97040},{\"end\":97053,\"start\":97048},{\"end\":97064,\"start\":97057},{\"end\":97074,\"start\":97068},{\"end\":97086,\"start\":97078},{\"end\":97101,\"start\":97090},{\"end\":97110,\"start\":97105},{\"end\":97120,\"start\":97114},{\"end\":97130,\"start\":97124},{\"end\":97141,\"start\":97134},{\"end\":97157,\"start\":97145},{\"end\":97168,\"start\":97161},{\"end\":97180,\"start\":97172},{\"end\":97189,\"start\":97184},{\"end\":97199,\"start\":97193},{\"end\":97212,\"start\":97205},{\"end\":97218,\"start\":97216},{\"end\":97228,\"start\":97222},{\"end\":97237,\"start\":97232},{\"end\":97245,\"start\":97241},{\"end\":97255,\"start\":97249},{\"end\":97265,\"start\":97259},{\"end\":97273,\"start\":97269},{\"end\":97282,\"start\":97277},{\"end\":97291,\"start\":97286},{\"end\":97301,\"start\":97295},{\"end\":97315,\"start\":97305},{\"end\":97326,\"start\":97319},{\"end\":97339,\"start\":97330},{\"end\":97349,\"start\":97343},{\"end\":97807,\"start\":97803},{\"end\":97822,\"start\":97811},{\"end\":97833,\"start\":97826},{\"end\":97848,\"start\":97837},{\"end\":97856,\"start\":97852},{\"end\":97865,\"start\":97860},{\"end\":97874,\"start\":97869},{\"end\":97883,\"start\":97878},{\"end\":97893,\"start\":97887},{\"end\":97904,\"start\":97897},{\"end\":98332,\"start\":98324},{\"end\":98343,\"start\":98336},{\"end\":98352,\"start\":98347},{\"end\":98603,\"start\":98600},{\"end\":98612,\"start\":98607},{\"end\":98621,\"start\":98616},{\"end\":98627,\"start\":98625},{\"end\":98637,\"start\":98631},{\"end\":98649,\"start\":98641},{\"end\":98657,\"start\":98653},{\"end\":98664,\"start\":98661},{\"end\":98961,\"start\":98955},{\"end\":98969,\"start\":98965},{\"end\":98979,\"start\":98975},{\"end\":99600,\"start\":99590},{\"end\":99613,\"start\":99604},{\"end\":99623,\"start\":99617},{\"end\":99861,\"start\":99851},{\"end\":99870,\"start\":99865},{\"end\":99879,\"start\":99874},{\"end\":99889,\"start\":99883},{\"end\":99897,\"start\":99893},{\"end\":99907,\"start\":99901},{\"end\":100224,\"start\":100215},{\"end\":100233,\"start\":100228},{\"end\":100243,\"start\":100237},{\"end\":100573,\"start\":100570},{\"end\":100581,\"start\":100579},{\"end\":100778,\"start\":100773},{\"end\":100789,\"start\":100782},{\"end\":100801,\"start\":100793},{\"end\":100812,\"start\":100805},{\"end\":100826,\"start\":100816},{\"end\":100835,\"start\":100830},{\"end\":101270,\"start\":101266},{\"end\":101278,\"start\":101274},{\"end\":101286,\"start\":101282},{\"end\":101293,\"start\":101290},{\"end\":101300,\"start\":101297},{\"end\":101308,\"start\":101304},{\"end\":101315,\"start\":101312},{\"end\":101323,\"start\":101319},{\"end\":101333,\"start\":101330},{\"end\":101624,\"start\":101620},{\"end\":101632,\"start\":101628},{\"end\":101644,\"start\":101636},{\"end\":101656,\"start\":101648},{\"end\":101670,\"start\":101664},{\"end\":101998,\"start\":101994},{\"end\":102007,\"start\":102002},{\"end\":102019,\"start\":102011},{\"end\":102489,\"start\":102483},{\"end\":102767,\"start\":102759},{\"end\":103221,\"start\":103213},{\"end\":103231,\"start\":103225},{\"end\":103238,\"start\":103235},{\"end\":103251,\"start\":103242},{\"end\":103258,\"start\":103255},{\"end\":103550,\"start\":103544},{\"end\":103556,\"start\":103554},{\"end\":103768,\"start\":103760},{\"end\":103779,\"start\":103772},{\"end\":103790,\"start\":103783},{\"end\":103801,\"start\":103794},{\"end\":104051,\"start\":104041},{\"end\":104063,\"start\":104057},{\"end\":104073,\"start\":104067},{\"end\":104082,\"start\":104077},{\"end\":104092,\"start\":104086},{\"end\":104850,\"start\":104842},{\"end\":104861,\"start\":104854},{\"end\":105058,\"start\":105053},{\"end\":105069,\"start\":105062},{\"end\":105080,\"start\":105073},{\"end\":105090,\"start\":105084},{\"end\":105099,\"start\":105094},{\"end\":105107,\"start\":105103},{\"end\":105505,\"start\":105500},{\"end\":105512,\"start\":105509},{\"end\":105521,\"start\":105516},{\"end\":105538,\"start\":105525},{\"end\":105549,\"start\":105542},{\"end\":105557,\"start\":105553},{\"end\":105569,\"start\":105561},{\"end\":105584,\"start\":105573},{\"end\":105969,\"start\":105967},{\"end\":105979,\"start\":105973},{\"end\":105989,\"start\":105983},{\"end\":106266,\"start\":106260},{\"end\":106477,\"start\":106474},{\"end\":106486,\"start\":106483},{\"end\":107023,\"start\":107020},{\"end\":107224,\"start\":107218},{\"end\":107235,\"start\":107228},{\"end\":107245,\"start\":107239},{\"end\":107257,\"start\":107249},{\"end\":107386,\"start\":107380},{\"end\":107398,\"start\":107392},{\"end\":107407,\"start\":107402},{\"end\":107417,\"start\":107411},{\"end\":107787,\"start\":107781},{\"end\":107796,\"start\":107791},{\"end\":107817,\"start\":107806},{\"end\":108081,\"start\":108078},{\"end\":108091,\"start\":108085},{\"end\":108373,\"start\":108367},{\"end\":108382,\"start\":108377},{\"end\":108392,\"start\":108386},{\"end\":108629,\"start\":108624},{\"end\":108643,\"start\":108633},{\"end\":108653,\"start\":108647},{\"end\":108663,\"start\":108657},{\"end\":108672,\"start\":108667},{\"end\":108679,\"start\":108676},{\"end\":108957,\"start\":108950},{\"end\":108971,\"start\":108961},{\"end\":108983,\"start\":108975},{\"end\":108996,\"start\":108987},{\"end\":109315,\"start\":109308},{\"end\":109321,\"start\":109319},{\"end\":109330,\"start\":109325},{\"end\":109338,\"start\":109334},{\"end\":109348,\"start\":109342},{\"end\":109361,\"start\":109352},{\"end\":109618,\"start\":109612},{\"end\":109629,\"start\":109622},{\"end\":109640,\"start\":109633},{\"end\":109647,\"start\":109644},{\"end\":109657,\"start\":109651},{\"end\":109667,\"start\":109661},{\"end\":109675,\"start\":109671},{\"end\":109681,\"start\":109679},{\"end\":109690,\"start\":109687},{\"end\":109950,\"start\":109943},{\"end\":109960,\"start\":109954},{\"end\":109968,\"start\":109964},{\"end\":109979,\"start\":109972},{\"end\":110337,\"start\":110332},{\"end\":110578,\"start\":110574},{\"end\":110588,\"start\":110582},{\"end\":110599,\"start\":110592},{\"end\":111102,\"start\":111097},{\"end\":111113,\"start\":111106},{\"end\":111124,\"start\":111117},{\"end\":111436,\"start\":111432},{\"end\":111446,\"start\":111440},{\"end\":111456,\"start\":111450},{\"end\":111699,\"start\":111691},{\"end\":112115,\"start\":112108},{\"end\":112347,\"start\":112339},{\"end\":112357,\"start\":112351},{\"end\":112367,\"start\":112361},{\"end\":112377,\"start\":112371},{\"end\":112387,\"start\":112381},{\"end\":112769,\"start\":112761},{\"end\":112779,\"start\":112773},{\"end\":112791,\"start\":112783},{\"end\":112802,\"start\":112795},{\"end\":112812,\"start\":112806},{\"end\":113100,\"start\":113097},{\"end\":113109,\"start\":113106},{\"end\":113122,\"start\":113115},{\"end\":113375,\"start\":113371},{\"end\":113382,\"start\":113379},{\"end\":113389,\"start\":113386},{\"end\":113395,\"start\":113393},{\"end\":113405,\"start\":113402},{\"end\":113607,\"start\":113598},{\"end\":113621,\"start\":113611},{\"end\":113630,\"start\":113625},{\"end\":113642,\"start\":113636},{\"end\":113654,\"start\":113646},{\"end\":113983,\"start\":113976},{\"end\":113994,\"start\":113987},{\"end\":114004,\"start\":113998},{\"end\":114017,\"start\":114008},{\"end\":114026,\"start\":114021},{\"end\":114037,\"start\":114032},{\"end\":114047,\"start\":114041},{\"end\":114061,\"start\":114051},{\"end\":114385,\"start\":114379},{\"end\":114397,\"start\":114389},{\"end\":114405,\"start\":114401},{\"end\":114414,\"start\":114409},{\"end\":114723,\"start\":114716},{\"end\":114734,\"start\":114727},{\"end\":114744,\"start\":114738},{\"end\":114753,\"start\":114748},{\"end\":114760,\"start\":114757},{\"end\":114770,\"start\":114764},{\"end\":115086,\"start\":115084},{\"end\":115092,\"start\":115090},{\"end\":115411,\"start\":115409},{\"end\":115423,\"start\":115415},{\"end\":115431,\"start\":115427},{\"end\":115439,\"start\":115437},{\"end\":115450,\"start\":115443},{\"end\":115462,\"start\":115454},{\"end\":115472,\"start\":115466},{\"end\":115479,\"start\":115476},{\"end\":115486,\"start\":115483},{\"end\":115498,\"start\":115490},{\"end\":115862,\"start\":115859},{\"end\":115868,\"start\":115866},{\"end\":115876,\"start\":115872},{\"end\":115883,\"start\":115880},{\"end\":115891,\"start\":115887},{\"end\":115899,\"start\":115895},{\"end\":115908,\"start\":115903},{\"end\":115916,\"start\":115912},{\"end\":116248,\"start\":116246},{\"end\":116256,\"start\":116252},{\"end\":116266,\"start\":116260},{\"end\":116277,\"start\":116270},{\"end\":116286,\"start\":116281},{\"end\":116300,\"start\":116290},{\"end\":116313,\"start\":116304},{\"end\":116324,\"start\":116317},{\"end\":116339,\"start\":116328},{\"end\":116699,\"start\":116694},{\"end\":116712,\"start\":116703},{\"end\":116724,\"start\":116716},{\"end\":116739,\"start\":116728},{\"end\":117023,\"start\":117018},{\"end\":117031,\"start\":117027},{\"end\":117040,\"start\":117035},{\"end\":117049,\"start\":117046},{\"end\":117318,\"start\":117313},{\"end\":117324,\"start\":117322},{\"end\":117332,\"start\":117328},{\"end\":117340,\"start\":117336},{\"end\":117348,\"start\":117344},{\"end\":117636,\"start\":117632},{\"end\":117642,\"start\":117640},{\"end\":117850,\"start\":117843},{\"end\":117862,\"start\":117854},{\"end\":117868,\"start\":117866},{\"end\":117879,\"start\":117874},{\"end\":117890,\"start\":117883},{\"end\":117900,\"start\":117894},{\"end\":117914,\"start\":117904},{\"end\":117924,\"start\":117918},{\"end\":129923,\"start\":129917},{\"end\":139266,\"start\":139262}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b0\"},\"end\":96395,\"start\":96057},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18606472},\"end\":96664,\"start\":96397},{\"attributes\":{\"doi\":\"arXiv:1909.01214\",\"id\":\"b2\"},\"end\":97023,\"start\":96666},{\"attributes\":{\"id\":\"b3\"},\"end\":97799,\"start\":97025},{\"attributes\":{\"id\":\"b4\"},\"end\":98245,\"start\":97801},{\"attributes\":{\"doi\":\"arXiv:1807.02202\",\"id\":\"b5\"},\"end\":98537,\"start\":98247},{\"attributes\":{\"doi\":\"arXiv:1811.00511\",\"id\":\"b6\"},\"end\":98874,\"start\":98539},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":133195},\"end\":99530,\"start\":98876},{\"attributes\":{\"doi\":\"arXiv:1810.08575\",\"id\":\"b8\"},\"end\":99793,\"start\":99532},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4787508},\"end\":100161,\"start\":99795},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207240067},\"end\":100529,\"start\":100163},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7138078},\"end\":100769,\"start\":100531},{\"attributes\":{\"doi\":\"arXiv:2002.06305\",\"id\":\"b12\"},\"end\":101175,\"start\":100771},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":147704286},\"end\":101616,\"start\":101177},{\"attributes\":{\"doi\":\"arXiv:1809.09672\",\"id\":\"b14\"},\"end\":101925,\"start\":101618},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1729177},\"end\":102410,\"start\":101927},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3745695},\"end\":102704,\"start\":102412},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207605508},\"end\":103145,\"start\":102706},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2739209},\"end\":103494,\"start\":103147},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b19\"},\"end\":103692,\"start\":103496},{\"attributes\":{\"doi\":\"arXiv:1804.05958\",\"id\":\"b20\"},\"end\":103987,\"start\":103694},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":201666437},\"end\":104748,\"start\":103989},{\"attributes\":{\"doi\":\"arXiv:1805.01252\",\"id\":\"b22\"},\"end\":105049,\"start\":104750},{\"attributes\":{\"doi\":\"arXiv:1811.07871\",\"id\":\"b23\"},\"end\":105381,\"start\":105051},{\"attributes\":{\"doi\":\"arXiv:1910.13461\",\"id\":\"b24\"},\"end\":105869,\"start\":105383},{\"attributes\":{\"doi\":\"arXiv:1909.03087\",\"id\":\"b25\"},\"end\":106186,\"start\":105871},{\"attributes\":{\"id\":\"b26\"},\"end\":106354,\"start\":106188},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1586456},\"end\":106969,\"start\":106356},{\"attributes\":{\"id\":\"b28\"},\"end\":107153,\"start\":106971},{\"attributes\":{\"id\":\"b29\"},\"end\":107376,\"start\":107155},{\"attributes\":{\"doi\":\"arXiv:1806.08730\",\"id\":\"b30\"},\"end\":107685,\"start\":107378},{\"attributes\":{\"doi\":\"arXiv:1707.07402\",\"id\":\"b31\"},\"end\":108024,\"start\":107687},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13690180},\"end\":108308,\"start\":108026},{\"attributes\":{\"doi\":\"arXiv:1705.04304\",\"id\":\"b33\"},\"end\":108553,\"start\":108310},{\"attributes\":{\"doi\":\"arXiv:1909.05863\",\"id\":\"b34\"},\"end\":108885,\"start\":108555},{\"attributes\":{\"id\":\"b35\"},\"end\":109251,\"start\":108887},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":160025533},\"end\":109525,\"start\":109253},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b37\"},\"end\":109939,\"start\":109527},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b38\"},\"end\":110209,\"start\":109941},{\"attributes\":{\"id\":\"b39\"},\"end\":110480,\"start\":110211},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":103456},\"end\":111027,\"start\":110482},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":198967997},\"end\":111361,\"start\":111029},{\"attributes\":{\"doi\":\"arXiv:1509.00685\",\"id\":\"b42\"},\"end\":111629,\"start\":111363},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9403493},\"end\":112042,\"start\":111631},{\"attributes\":{\"doi\":\"arXiv:1910.00292\",\"id\":\"b44\"},\"end\":112259,\"start\":112044},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3075448},\"end\":112757,\"start\":112261},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b46\"},\"end\":113028,\"start\":112759},{\"attributes\":{\"doi\":\"arXiv:1704.04368\",\"id\":\"b47\"},\"end\":113295,\"start\":113030},{\"attributes\":{\"doi\":\"arXiv:1905.02450\",\"id\":\"b48\"},\"end\":113594,\"start\":113297},{\"attributes\":{\"doi\":\"arXiv:1809.10736\",\"id\":\"b49\"},\"end\":113945,\"start\":113596},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":13756489},\"end\":114319,\"start\":113947},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2204603},\"end\":114712,\"start\":114321},{\"attributes\":{\"doi\":\"arXiv:1908.04319\",\"id\":\"b52\"},\"end\":115010,\"start\":114714},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":4999752},\"end\":115305,\"start\":115012},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b54\"},\"end\":115779,\"start\":115307},{\"attributes\":{\"doi\":\"arXiv:2001.04063\",\"id\":\"b55\"},\"end\":116137,\"start\":115781},{\"attributes\":{\"doi\":\"arXiv:1904.13015\",\"id\":\"b56\"},\"end\":116624,\"start\":116139},{\"attributes\":{\"doi\":\"arXiv:2004.10450\",\"id\":\"b57\"},\"end\":116932,\"start\":116626},{\"attributes\":{\"doi\":\"arXiv:1912.08777\",\"id\":\"b58\"},\"end\":117245,\"start\":116934},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":150295060},\"end\":117522,\"start\":117247},{\"attributes\":{\"doi\":\"arXiv:2002.05058\",\"id\":\"b60\"},\"end\":117837,\"start\":117524},{\"attributes\":{\"doi\":\"arXiv:1909.08593\",\"id\":\"b61\"},\"end\":118201,\"start\":117839},{\"attributes\":{\"id\":\"b62\"},\"end\":118781,\"start\":118203},{\"attributes\":{\"id\":\"b63\"},\"end\":118831,\"start\":118783},{\"attributes\":{\"id\":\"b64\"},\"end\":119200,\"start\":118833},{\"attributes\":{\"id\":\"b65\"},\"end\":119453,\"start\":119202},{\"attributes\":{\"id\":\"b66\"},\"end\":122103,\"start\":119455},{\"attributes\":{\"id\":\"b67\"},\"end\":124150,\"start\":122105},{\"attributes\":{\"id\":\"b68\"},\"end\":125527,\"start\":124152},{\"attributes\":{\"id\":\"b69\"},\"end\":129303,\"start\":125529},{\"attributes\":{\"id\":\"b70\"},\"end\":129820,\"start\":129305},{\"attributes\":{\"id\":\"b71\"},\"end\":130027,\"start\":129822},{\"attributes\":{\"id\":\"b72\"},\"end\":133580,\"start\":130029},{\"attributes\":{\"id\":\"b73\"},\"end\":134333,\"start\":133582},{\"attributes\":{\"id\":\"b74\"},\"end\":138751,\"start\":134335},{\"attributes\":{\"id\":\"b75\"},\"end\":139094,\"start\":138753},{\"attributes\":{\"id\":\"b76\"},\"end\":139441,\"start\":139096},{\"attributes\":{\"id\":\"b77\"},\"end\":143732,\"start\":139443},{\"attributes\":{\"id\":\"b78\"},\"end\":144906,\"start\":143734}]", "bib_title": "[{\"end\":96455,\"start\":96397},{\"end\":98951,\"start\":98876},{\"end\":99845,\"start\":99795},{\"end\":100211,\"start\":100163},{\"end\":100564,\"start\":100531},{\"end\":101262,\"start\":101177},{\"end\":101990,\"start\":101927},{\"end\":102479,\"start\":102412},{\"end\":102755,\"start\":102706},{\"end\":103209,\"start\":103147},{\"end\":104037,\"start\":103989},{\"end\":106467,\"start\":106356},{\"end\":108074,\"start\":108026},{\"end\":109304,\"start\":109253},{\"end\":110570,\"start\":110482},{\"end\":111093,\"start\":111029},{\"end\":111687,\"start\":111631},{\"end\":112335,\"start\":112261},{\"end\":113972,\"start\":113947},{\"end\":114375,\"start\":114321},{\"end\":115080,\"start\":115012},{\"end\":117309,\"start\":117247},{\"end\":118285,\"start\":118203},{\"end\":119260,\"start\":119202},{\"end\":120408,\"start\":119455},{\"end\":122598,\"start\":122105},{\"end\":126024,\"start\":125529},{\"end\":129410,\"start\":129305},{\"end\":130439,\"start\":130029},{\"end\":134681,\"start\":134335},{\"end\":140075,\"start\":139443},{\"end\":143883,\"start\":143734}]", "bib_author": "[{\"end\":96120,\"start\":96108},{\"end\":96130,\"start\":96120},{\"end\":96136,\"start\":96130},{\"end\":96145,\"start\":96136},{\"end\":96153,\"start\":96145},{\"end\":96163,\"start\":96153},{\"end\":96176,\"start\":96163},{\"end\":96186,\"start\":96176},{\"end\":96470,\"start\":96457},{\"end\":96484,\"start\":96470},{\"end\":96495,\"start\":96484},{\"end\":96674,\"start\":96666},{\"end\":96681,\"start\":96674},{\"end\":96692,\"start\":96681},{\"end\":96703,\"start\":96692},{\"end\":96712,\"start\":96703},{\"end\":96724,\"start\":96712},{\"end\":97038,\"start\":97027},{\"end\":97046,\"start\":97038},{\"end\":97055,\"start\":97046},{\"end\":97066,\"start\":97055},{\"end\":97076,\"start\":97066},{\"end\":97088,\"start\":97076},{\"end\":97103,\"start\":97088},{\"end\":97112,\"start\":97103},{\"end\":97122,\"start\":97112},{\"end\":97132,\"start\":97122},{\"end\":97143,\"start\":97132},{\"end\":97159,\"start\":97143},{\"end\":97170,\"start\":97159},{\"end\":97182,\"start\":97170},{\"end\":97191,\"start\":97182},{\"end\":97201,\"start\":97191},{\"end\":97214,\"start\":97201},{\"end\":97220,\"start\":97214},{\"end\":97230,\"start\":97220},{\"end\":97239,\"start\":97230},{\"end\":97247,\"start\":97239},{\"end\":97257,\"start\":97247},{\"end\":97267,\"start\":97257},{\"end\":97275,\"start\":97267},{\"end\":97284,\"start\":97275},{\"end\":97293,\"start\":97284},{\"end\":97303,\"start\":97293},{\"end\":97317,\"start\":97303},{\"end\":97328,\"start\":97317},{\"end\":97341,\"start\":97328},{\"end\":97351,\"start\":97341},{\"end\":97809,\"start\":97801},{\"end\":97824,\"start\":97809},{\"end\":97835,\"start\":97824},{\"end\":97850,\"start\":97835},{\"end\":97858,\"start\":97850},{\"end\":97867,\"start\":97858},{\"end\":97876,\"start\":97867},{\"end\":97885,\"start\":97876},{\"end\":97895,\"start\":97885},{\"end\":97906,\"start\":97895},{\"end\":98334,\"start\":98320},{\"end\":98345,\"start\":98334},{\"end\":98354,\"start\":98345},{\"end\":98605,\"start\":98596},{\"end\":98614,\"start\":98605},{\"end\":98623,\"start\":98614},{\"end\":98629,\"start\":98623},{\"end\":98639,\"start\":98629},{\"end\":98651,\"start\":98639},{\"end\":98659,\"start\":98651},{\"end\":98666,\"start\":98659},{\"end\":98963,\"start\":98953},{\"end\":98971,\"start\":98963},{\"end\":98981,\"start\":98971},{\"end\":99602,\"start\":99588},{\"end\":99615,\"start\":99602},{\"end\":99625,\"start\":99615},{\"end\":99863,\"start\":99847},{\"end\":99872,\"start\":99863},{\"end\":99881,\"start\":99872},{\"end\":99891,\"start\":99881},{\"end\":99899,\"start\":99891},{\"end\":99909,\"start\":99899},{\"end\":100226,\"start\":100213},{\"end\":100235,\"start\":100226},{\"end\":100245,\"start\":100235},{\"end\":100575,\"start\":100566},{\"end\":100583,\"start\":100575},{\"end\":100780,\"start\":100771},{\"end\":100791,\"start\":100780},{\"end\":100803,\"start\":100791},{\"end\":100814,\"start\":100803},{\"end\":100828,\"start\":100814},{\"end\":100837,\"start\":100828},{\"end\":101272,\"start\":101264},{\"end\":101280,\"start\":101272},{\"end\":101288,\"start\":101280},{\"end\":101295,\"start\":101288},{\"end\":101302,\"start\":101295},{\"end\":101310,\"start\":101302},{\"end\":101317,\"start\":101310},{\"end\":101325,\"start\":101317},{\"end\":101335,\"start\":101325},{\"end\":101626,\"start\":101618},{\"end\":101634,\"start\":101626},{\"end\":101646,\"start\":101634},{\"end\":101658,\"start\":101646},{\"end\":101672,\"start\":101658},{\"end\":102000,\"start\":101992},{\"end\":102009,\"start\":102000},{\"end\":102021,\"start\":102009},{\"end\":102491,\"start\":102481},{\"end\":102769,\"start\":102757},{\"end\":103223,\"start\":103211},{\"end\":103233,\"start\":103223},{\"end\":103240,\"start\":103233},{\"end\":103253,\"start\":103240},{\"end\":103260,\"start\":103253},{\"end\":103552,\"start\":103540},{\"end\":103558,\"start\":103552},{\"end\":103770,\"start\":103758},{\"end\":103781,\"start\":103770},{\"end\":103792,\"start\":103781},{\"end\":103803,\"start\":103792},{\"end\":104053,\"start\":104039},{\"end\":104065,\"start\":104053},{\"end\":104075,\"start\":104065},{\"end\":104084,\"start\":104075},{\"end\":104094,\"start\":104084},{\"end\":104852,\"start\":104840},{\"end\":104863,\"start\":104852},{\"end\":105060,\"start\":105051},{\"end\":105071,\"start\":105060},{\"end\":105082,\"start\":105071},{\"end\":105092,\"start\":105082},{\"end\":105101,\"start\":105092},{\"end\":105109,\"start\":105101},{\"end\":105507,\"start\":105498},{\"end\":105514,\"start\":105507},{\"end\":105523,\"start\":105514},{\"end\":105540,\"start\":105523},{\"end\":105551,\"start\":105540},{\"end\":105559,\"start\":105551},{\"end\":105571,\"start\":105559},{\"end\":105586,\"start\":105571},{\"end\":105971,\"start\":105965},{\"end\":105981,\"start\":105971},{\"end\":105991,\"start\":105981},{\"end\":106268,\"start\":106258},{\"end\":106479,\"start\":106469},{\"end\":106488,\"start\":106479},{\"end\":107025,\"start\":107015},{\"end\":107226,\"start\":107216},{\"end\":107237,\"start\":107226},{\"end\":107247,\"start\":107237},{\"end\":107259,\"start\":107247},{\"end\":107388,\"start\":107378},{\"end\":107400,\"start\":107388},{\"end\":107409,\"start\":107400},{\"end\":107419,\"start\":107409},{\"end\":107789,\"start\":107779},{\"end\":107798,\"start\":107789},{\"end\":107804,\"start\":107798},{\"end\":107819,\"start\":107804},{\"end\":108083,\"start\":108076},{\"end\":108093,\"start\":108083},{\"end\":108375,\"start\":108365},{\"end\":108384,\"start\":108375},{\"end\":108394,\"start\":108384},{\"end\":108631,\"start\":108622},{\"end\":108645,\"start\":108631},{\"end\":108655,\"start\":108645},{\"end\":108665,\"start\":108655},{\"end\":108674,\"start\":108665},{\"end\":108681,\"start\":108674},{\"end\":108959,\"start\":108948},{\"end\":108973,\"start\":108959},{\"end\":108985,\"start\":108973},{\"end\":108998,\"start\":108985},{\"end\":109317,\"start\":109306},{\"end\":109323,\"start\":109317},{\"end\":109332,\"start\":109323},{\"end\":109340,\"start\":109332},{\"end\":109350,\"start\":109340},{\"end\":109363,\"start\":109350},{\"end\":109620,\"start\":109610},{\"end\":109631,\"start\":109620},{\"end\":109642,\"start\":109631},{\"end\":109649,\"start\":109642},{\"end\":109659,\"start\":109649},{\"end\":109669,\"start\":109659},{\"end\":109677,\"start\":109669},{\"end\":109683,\"start\":109677},{\"end\":109692,\"start\":109683},{\"end\":109952,\"start\":109941},{\"end\":109962,\"start\":109952},{\"end\":109970,\"start\":109962},{\"end\":109981,\"start\":109970},{\"end\":110339,\"start\":110328},{\"end\":110580,\"start\":110572},{\"end\":110590,\"start\":110580},{\"end\":110601,\"start\":110590},{\"end\":111104,\"start\":111095},{\"end\":111115,\"start\":111104},{\"end\":111126,\"start\":111115},{\"end\":111438,\"start\":111428},{\"end\":111448,\"start\":111438},{\"end\":111458,\"start\":111448},{\"end\":111701,\"start\":111689},{\"end\":112117,\"start\":112106},{\"end\":112349,\"start\":112337},{\"end\":112359,\"start\":112349},{\"end\":112369,\"start\":112359},{\"end\":112379,\"start\":112369},{\"end\":112389,\"start\":112379},{\"end\":112771,\"start\":112759},{\"end\":112781,\"start\":112771},{\"end\":112793,\"start\":112781},{\"end\":112804,\"start\":112793},{\"end\":112814,\"start\":112804},{\"end\":113102,\"start\":113095},{\"end\":113111,\"start\":113102},{\"end\":113124,\"start\":113111},{\"end\":113377,\"start\":113369},{\"end\":113384,\"start\":113377},{\"end\":113391,\"start\":113384},{\"end\":113397,\"start\":113391},{\"end\":113407,\"start\":113397},{\"end\":113609,\"start\":113596},{\"end\":113623,\"start\":113609},{\"end\":113632,\"start\":113623},{\"end\":113644,\"start\":113632},{\"end\":113656,\"start\":113644},{\"end\":113662,\"start\":113656},{\"end\":113985,\"start\":113974},{\"end\":113996,\"start\":113985},{\"end\":114006,\"start\":113996},{\"end\":114019,\"start\":114006},{\"end\":114028,\"start\":114019},{\"end\":114039,\"start\":114028},{\"end\":114049,\"start\":114039},{\"end\":114063,\"start\":114049},{\"end\":114387,\"start\":114377},{\"end\":114399,\"start\":114387},{\"end\":114407,\"start\":114399},{\"end\":114416,\"start\":114407},{\"end\":114725,\"start\":114714},{\"end\":114736,\"start\":114725},{\"end\":114746,\"start\":114736},{\"end\":114755,\"start\":114746},{\"end\":114762,\"start\":114755},{\"end\":114772,\"start\":114762},{\"end\":115088,\"start\":115082},{\"end\":115094,\"start\":115088},{\"end\":115413,\"start\":115407},{\"end\":115425,\"start\":115413},{\"end\":115433,\"start\":115425},{\"end\":115441,\"start\":115433},{\"end\":115452,\"start\":115441},{\"end\":115464,\"start\":115452},{\"end\":115474,\"start\":115464},{\"end\":115481,\"start\":115474},{\"end\":115488,\"start\":115481},{\"end\":115500,\"start\":115488},{\"end\":115864,\"start\":115857},{\"end\":115870,\"start\":115864},{\"end\":115878,\"start\":115870},{\"end\":115885,\"start\":115878},{\"end\":115893,\"start\":115885},{\"end\":115901,\"start\":115893},{\"end\":115910,\"start\":115901},{\"end\":115918,\"start\":115910},{\"end\":116250,\"start\":116244},{\"end\":116258,\"start\":116250},{\"end\":116268,\"start\":116258},{\"end\":116279,\"start\":116268},{\"end\":116288,\"start\":116279},{\"end\":116302,\"start\":116288},{\"end\":116315,\"start\":116302},{\"end\":116326,\"start\":116315},{\"end\":116341,\"start\":116326},{\"end\":116701,\"start\":116692},{\"end\":116714,\"start\":116701},{\"end\":116726,\"start\":116714},{\"end\":116741,\"start\":116726},{\"end\":117025,\"start\":117016},{\"end\":117033,\"start\":117025},{\"end\":117042,\"start\":117033},{\"end\":117051,\"start\":117042},{\"end\":117320,\"start\":117311},{\"end\":117326,\"start\":117320},{\"end\":117334,\"start\":117326},{\"end\":117342,\"start\":117334},{\"end\":117350,\"start\":117342},{\"end\":117638,\"start\":117630},{\"end\":117644,\"start\":117638},{\"end\":117852,\"start\":117839},{\"end\":117864,\"start\":117852},{\"end\":117870,\"start\":117864},{\"end\":117881,\"start\":117870},{\"end\":117892,\"start\":117881},{\"end\":117902,\"start\":117892},{\"end\":117916,\"start\":117902},{\"end\":117926,\"start\":117916},{\"end\":129925,\"start\":129914},{\"end\":139257,\"start\":139247},{\"end\":139268,\"start\":139257}]", "bib_venue": "[{\"end\":96106,\"start\":96057},{\"end\":96503,\"start\":96495},{\"end\":96819,\"start\":96740},{\"end\":97996,\"start\":97906},{\"end\":98318,\"start\":98247},{\"end\":98594,\"start\":98539},{\"end\":99123,\"start\":98981},{\"end\":99586,\"start\":99532},{\"end\":99958,\"start\":99909},{\"end\":100306,\"start\":100245},{\"end\":100632,\"start\":100583},{\"end\":100948,\"start\":100853},{\"end\":101384,\"start\":101335},{\"end\":101746,\"start\":101688},{\"end\":102083,\"start\":102021},{\"end\":102540,\"start\":102491},{\"end\":102869,\"start\":102769},{\"end\":103275,\"start\":103260},{\"end\":103538,\"start\":103496},{\"end\":103756,\"start\":103694},{\"end\":104269,\"start\":104094},{\"end\":104838,\"start\":104750},{\"end\":105191,\"start\":105125},{\"end\":105496,\"start\":105383},{\"end\":105963,\"start\":105871},{\"end\":106256,\"start\":106188},{\"end\":106571,\"start\":106488},{\"end\":107013,\"start\":106971},{\"end\":107214,\"start\":107155},{\"end\":107507,\"start\":107435},{\"end\":107777,\"start\":107687},{\"end\":108154,\"start\":108093},{\"end\":108363,\"start\":108310},{\"end\":108620,\"start\":108555},{\"end\":108946,\"start\":108887},{\"end\":109374,\"start\":109363},{\"end\":109608,\"start\":109527},{\"end\":110051,\"start\":109997},{\"end\":110326,\"start\":110211},{\"end\":110697,\"start\":110601},{\"end\":111187,\"start\":111126},{\"end\":111426,\"start\":111363},{\"end\":111759,\"start\":111701},{\"end\":112104,\"start\":112044},{\"end\":112467,\"start\":112389},{\"end\":112869,\"start\":112830},{\"end\":113093,\"start\":113030},{\"end\":113367,\"start\":113297},{\"end\":113741,\"start\":113678},{\"end\":114112,\"start\":114063},{\"end\":114477,\"start\":114416},{\"end\":114837,\"start\":114788},{\"end\":115150,\"start\":115094},{\"end\":115405,\"start\":115307},{\"end\":115855,\"start\":115781},{\"end\":116242,\"start\":116139},{\"end\":116690,\"start\":116626},{\"end\":117014,\"start\":116934},{\"end\":117366,\"start\":117350},{\"end\":117628,\"start\":117524},{\"end\":117992,\"start\":117942},{\"end\":118438,\"start\":118287},{\"end\":118804,\"start\":118783},{\"end\":118971,\"start\":118833},{\"end\":119321,\"start\":119262},{\"end\":120659,\"start\":120410},{\"end\":122678,\"start\":122600},{\"end\":124689,\"start\":124152},{\"end\":126253,\"start\":126026},{\"end\":129459,\"start\":129412},{\"end\":129912,\"start\":129822},{\"end\":130863,\"start\":130441},{\"end\":133850,\"start\":133582},{\"end\":135558,\"start\":134683},{\"end\":138838,\"start\":138753},{\"end\":139245,\"start\":139096},{\"end\":140265,\"start\":140077},{\"end\":144069,\"start\":143885},{\"end\":99252,\"start\":99125},{\"end\":100354,\"start\":100308},{\"end\":102132,\"start\":102085},{\"end\":102956,\"start\":102871},{\"end\":103294,\"start\":103277},{\"end\":104431,\"start\":104271},{\"end\":106641,\"start\":106573},{\"end\":110780,\"start\":110699},{\"end\":111804,\"start\":111761},{\"end\":112532,\"start\":112469},{\"end\":114525,\"start\":114479}]"}}}, "year": 2023, "month": 12, "day": 17}