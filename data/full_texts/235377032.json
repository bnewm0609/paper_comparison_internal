{"id": 235377032, "updated": "2023-10-06 02:32:57.623", "metadata": {"title": "Energy-Based Models for Code Generation under Compilability Constraints", "authors": "[{\"first\":\"Tomasz\",\"last\":\"Korbak\",\"middle\":[]},{\"first\":\"Hady\",\"last\":\"Elsahar\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Dymetman\",\"middle\":[]},{\"first\":\"Germ'an\",\"last\":\"Kruszewski\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 6, "day": 9}, "abstract": "Neural language models can be successfully trained on source code, leading to applications such as code completion. However, their versatile autoregressive self-supervision objective overlooks important global sequence-level features that are present in the data such as syntactic correctness or compilability. In this work, we pose the problem of learning to generate compilable code as constraint satisfaction. We define an Energy-Based Model (EBM) representing a pre-trained generative model with an imposed constraint of generating only compilable sequences. We then use the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021) to train a generative model approximating the EBM. We conduct experiments showing that our proposed approach is able to improve compilability rates without sacrificing diversity and complexity of the generated samples.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.04985", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-04985", "doi": null}}, "content": {"source": {"pdf_hash": "7c2de353f3151a0f88736af9be354ab5e773bff0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.04985v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "53c2edf42b80a026c20c38a1a30a01d4139ebbf1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7c2de353f3151a0f88736af9be354ab5e773bff0.txt", "contents": "\nEnergy-Based Models for Code Generation under Compilability Constraints\n\n\nTomasz Korbak t.korbak@sussex.ac.uk \nUniversity of Sussex\nUnited Kingdom\n\nHady Elsahar hady.elsahar@naverlabs.com \nNaver Labs Europe\nFrance\n\nMarc Dymetman marc.dymetman@naverlabs.com \nNaver Labs Europe\nFrance\n\nGerm\u00e1n Kruszewski german.kruszewski@naverlabs.com \nNaver Labs Europe\nFrance\n\nEnergy-Based Models for Code Generation under Compilability Constraints\n\nNeural language models can be successfully trained on source code, leading to applications such as code completion. However, their versatile autoregressive self-supervision objective overlooks important global sequence-level features that are present in the data such as syntactic correctness or compilability. In this work, we pose the problem of learning to generate compilable code as constraint satisfaction. We define an Energy-Based Model (EBM) representing a pre-trained generative model with an imposed constraint of generating only compilable sequences. We then use the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021)  to train a generative model approximating the EBM. We conduct experiments showing that our proposed approach is able to improve compilability rates without sacrificing diversity and complexity of the generated samples. . 2020. Engine: Energy-based inference networks for non-autoregressive machine translation. ArXiv, abs/2005.00850.Ronald J. Williams. 1992a. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Mach. Learn., 8:229-256.\n\nIntroduction\n\nCode completion is an essential feature of any modern Integrated Development Environment (IDEs). It supports developers with recommendations about the next token to write given a context, speeding up software development and reducing the number of mistakes. A large body of work has relied on statistical language modeling, treating programming languages as natural languages using probabilistic grammars (Raychev et al., 2014;Bielik et al., 2016), and more recently relying on neural language models (Liu et al., 2016a;Svyatkovskiy et al., 2020a,b;Arkesteijn et al., 2020;Ciniselli et al., 2021). 1 In particular, neural autore- * Work done during a research internship at Naver Labs Europe.\n\n1 See Allamanis et al. (2018) for a survey.\n\ngressive language models have been favoured due to their scalability and generic training procedure that can exploit large codebases (e.g. open source code repositories available on GitHub) through selfsupervised training. Despite these desirable traits, neural language models, trained in the standard way, are known to suffer from myopia and to overlook global sequence-level features that are present in the data and which might be crucial for the quality of generated sequences (Parshakova et al., 2019b). This leads to repetitions, hallucinations and failing to capture long-distance consistency requirements. In a code generation context, this is demonstrated in compilation errors that are a common failure mode in such tasks as translation between programming languages (Roziere et al., 2020). This problem has inspired a large body of work on different fronts on injecting sequence-level priors by either directly optimizing sequence-level features (Ranzato et al., 2016) or through fusion with grammars and automata (Xiao et al., 2016). These techniques aim to balance between the desirable traits and fast inference of neural autoregressive models trained in the standard way and the satisfaction of global sequence-level features.\n\nIn this work, we formulate compilable code generation as a constraint satisfaction problem. We show that this formulation leads to a unique distribution represented by an Energy-Based Model (EBM). This unique distribution by definition fully satisfies the compilability constraints while having a minimal KL divergence from the original autoregressive generative model trained through cross entropy. We then train an auto-regressive generative model to approximate the underlying distribution of this EBM using the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021).\n\nIn our experiments, we show that our approach significantly improves compilability rates without sacrificing diversity or complexity of the generated examples. This alleviates the drawbacks of reinforcement learning fine-tuning techniques that maximize compilability but deviate significantly from the original generative model, which leads to severe loss in diversity and complexity of the generated samples. Finally, we complement our experiments with a qualitative analysis of the effect of several fine-tuning approaches on the distribution of compilation errors.\n\n\nRelated Work\n\nImposing compilability constraints on generative models There is a body of work focusing on unconditional code generation or code completion: generating a piece of source code given a preceding piece of source code (Nguyen et al., 2013;Raychev et al., 2014;Karpathy et al., 2015;Bielik et al., 2016). That work, however, focuses on perplexity and similarity with respect to ground truth completions (in terms of exact-match accuracy, Levensthein distance and ROUGE scores) (Svyatkovskiy et al., 2020a;Lu et al., 2021), usually failing to measure and control for compilability of generated sequences or semantic and syntactic constraints in general. 2 On the other hand, semantic and syntactic constraints are frequently considered in languageto-code translation or program synthesis. For instance, Zhong et al. (2017), who used policy gradients to train a model for translating natural language questions to corresponding SQL queries and -in addition for rewarding for query execution results -added a penalty for syntactically invalid queries. Taking that one step further, Kulal et al. (2019) use compilation errors (with their precise location) to guide search over the space of possible programs.\n\nOptimizing sequence-level rewards for text generation Most previous attempts at steering autoregressive model to conform to global constraints defined over entire sequence have employed reinforcement learning (RL). This includes using Reinforce (Williams, 1992a) for machine transla-tion (Ranzato et al., 2016) or actor critic (Konda and Tsitsiklis, 2000) for abstractive summarization (Paulus et al., 2018), caption generation (Liu et al., 2016b), dialogue (Li et al., 2016b, and video captioning (Pasunuru and Bansal, 2017). Some approaches (for instance, in machine translation and summarization (Ranzato et al., 2016;Bahdanau et al., 2017)) directly optimize performance metrics such as BLEU and ROUGE at training time.\n\nOthers use heuristic rewards (for instance Li et al. (2016b) for dialogue generation and Tambwekar et al. (2019) for story generation) in order to obtain certain a priori desirable features of generated sequences that then incentivize good performance on target metrics. A weakness of using RL in finetuning generative models is the problem of catastrophic forgetting: maximizing global, sequencelevel rewards leads to very large deviations from the original autoregressive model trained through cross-entropy. This often results in significant reductions in fluency and diversity of generated samples. The catastrophic forgetting problem is sometimes addressed by imposing a penalty term to the rewards, such as the KL divergence between the trained policy and the auto-regressive model. This approach, termed \"conservative fine-tuning\", was applied to generating melodies with music theory rewards and organic molecules with synthesizability rewards by Jaques et al. (2017) as well finetuning language models for controllable language generation by Ziegler et al. (2019). This solution doesn't have an explicit notion of the optimal policy and often has hard time balancing between the reward term and the KL penalty term, leading to instability in training (Khalifa et al., 2021). Unlike this approach, our formulation defines the optimal distribution that satisfies both requirements.\n\nEnergy-based models for text Energy-based models (EBMs) (Hinton, 2002;LeCun et al., 2006;Ranzato et al., 2007) are a family of probabilistic graphical models in which learning and inference are done by associating an unnormalized probability with each configuration of observed and latent variables. Early examples of EBMs applied to natural language processing include sequence labeling problems (e.g. tagging) exploiting global properties of a sequence (Andor et al., 2016;Belanger and McCallum, 2016). A recent surge of interest in EBMs (Du and Mordatch, 2019) has not left text generation unaffected (see (Bakhtin et al., 2020) for a survey). Tu et al. (2020) proposed an energy-based inference networks for non-autoregressive machine translation. Parshakova et al. (2019b) and Deng et al. (2020) augment a autoregressive language models with an additional global factor to obtain a lower perplexity on the training data. Khalifa et al. (2021) develop a novel approach to distributional controllable text generation by constructing an EBM satisfying desired statistical constraints imposed on the set of generated sequences (such as topic or gender statistics over the sequences) and then train an autoregressive policy to approximate it, which can be sampled from efficiently. We build on Khalifa et al.'s approach by applying it to a novel domain outside natural language and defining a new kind of constraint: compilability.\n\n\nMethod\n\nFollowing Khalifa et al. (2021), we formulate compilable code generation as a constraint satisfaction problem over a space of generative models. There are two constraints that a target generative model p must satisfy. First, p must have minimal divergence -in the distribution space-from an original generative model a pre-trained using a standard autoregressive language modeling objective. Second, it must generate only sequences that satisfy a certain sequence level constraint b. In our case, b(x) = 1 iff x is a syntactically correct Python program and b(x) = 0 otherwise. There two constraints can be represented as a product-of-experts (Hinton, 2002) energy-based model\nP (x) = a(x)b(x).\n(1)\n\np(x) can be obtained from P (x) by dividing it by a normalization constant Z:\np(x) = 1 Z P (x),(2)\nwhere\nZ . = x P (x).(3)\nThis EBM P is unique, it represents a distribution p that optimally reconciles the two constraints. It is a special case of the generalized maximum entropy formulation presented in (Csisz\u00e1r and Shields, 2004) for applying constraints over distributions. However, one problem still remains: it is not straightforward how to draw samples x \u223c p(x) or even evaluating probability p(x) from this optimal unique distribution. A simple method for drawing samples from the p distribution could be sampling sequences from a and filtering on b(x). While this method sounds simple, there's no direct way of using it for interactive code completion as sampling full sequences till the end is necessary to filter through the sequence-level filter b(x). Therefore our objective here is to obtain another autoregressive policy \u03c0 \u03b8 to directly approximate p.\n\nTo attain this, Khalifa et al. (2021) (following Parshakova et al. (2019a)) developed a training procedure called KL-Adaptive Distributional Policy Gradients (KL-DPG) to train \u03c0 \u03b8 to minimize the KL divergence between p and \u03c0 \u03b8 . The gradient of this KL turns out to be tractable:\n\u2207 \u03b8 D KL (p, \u03c0 \u03b8 ) = \u2207 \u03b8 E x\u223cp log p(x) \u03c0 \u03b8 (x) (4) = \u2212\u2207 \u03b8 E x\u223cp log \u03c0 \u03b8 (x) (5) = \u2212E x\u223cp \u2207 \u03b8 log \u03c0 \u03b8 (x) (6) = \u2212 1 Z x P (x)\u2207 \u03b8 log \u03c0 \u03b8 (x)(7)\nLet us now absorb the constant \u22121/Z into a learning rate \u03b1 (\u03b8) and estimate the expectation over p(x) using importance sampling (Owen, 2013) from yet another generative model q:\n\u2207 \u03b8 D KL (p, \u03c0 \u03b8 ) \u221d E x\u223cq P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x). (8)\nDuring training, both \u03c0 \u03b8 and q are initialized as a.\n\nThen, q is periodically updated to \u03c0 \u03b8 if \u03c0 \u03b8 surpasses q in being closer to p (in terms of KL). For a pseudocode of the whole KL-DPG training procedure, see Algorithm 1. The gradient in (8) is similar to an estimate obtained using policy gradients methods in standard reinforcement learning (Sutton et al., 1999) with P (x)/q(x) playing the role of a pseudoreward. This similarity, however, is superficial. Our objective is approximating a target generative model p by minimizing D KL (p, \u03c0 \u03b8 ) rather than maximizing expected reward b(x) or P (x) or P (x)/q(x). As we show in Section 5, these objectives produce vastly different policies which diverge from p and catastrophically forget what the pretrained model a knew about its training domain. Furthermore, since q will always be close to \u03c0 \u03b8 , our pseudoreward P (x)/q(x) effectively depends on policy parameters \u03b8.\n\nAlgorithm 1 KL-DPG Require: EBM P , initial generative model a 1: \u03c0 \u03b8 \u2190 a 2: q \u2190 a 3: for each iteration do 4:\n\nfor each episode do 5:\nsample x from q(x) 6: \u03b8 \u2190 \u03b8 + \u03b1 (\u03b8) P (x) q(x) \u2207 \u03b8 log \u03c0 \u03b8 (x) 7:\nif D KL (p||\u03c0 \u03b8 ) < D KL (p||q) then 8:\n\nq \u2190 \u03c0 \u03b8 Ensure: \u03c0 \u03b8 4 Experiments 4.1 Setup Dataset: To prepare the training dataset, we started from the Python150 dataset, which consists of 150k Python source code files obtained from GitHub (Raychev et al., 2016). Then, using the code from Roziere et al. (2020), we extracted 713k Python functions (both methods and standalone functions) from it (250 MB of raw text data). The additional filtering criteria were compilability (according to b(x)) and being less than 128 BPE tokens long. The dataset was then split into a training subset D train and test subset D test .\n\nInitial generative model a: We implemented a using the GPT-2 (Radford et al., 2019) architecture with 117m parameters (gpt2-small) and kept all the original hyperparameters (see Table 1 in the Appendix). We trained a byte-level BPE tokenizer (Sennrich et al., 2016) with special BOS and EOS tokens to obtain a vocabulary of 50k tokens. The model was trained for one epoch.\n\n\nCompilability Scorer b:\n\nTo check for compilability, we call the compile command function from codeop module of Python Standard Library 3 with a sequence x as argument and check if it returns a code object. We apply no postprocessing other than removing BOS and EOS tokens. codeop.compile command is the implementation that Python interactive interpreters use in read-eval-print loop (REPL) to determine whether a string is a valid Python code. The method tries to compile a string of Python code and raise and exception if there is a problem with the Python code, in particular a SyntaxError for invalid Python syntax and ValueError or OverflowError if there is an invalid literal.\n\nThis notion of compilability is concerned only with syntactic correctness and does not execute the body of a function. However, we found the initial compilability rate E x\u223ca b(x) of functions x sampled from a(x) to be only 0.56, which leaves a large margin for improvement. 4 KL-DPG training \u03c0 \u03b8 and q share their architecture with a but have separate weights which are only initially identical to a's. Throughout the training, \u03c0 \u03b8 will be updated to approximate p. See Table  2 in the Appendix for a complete list of hyperparameters used for training \u03c0 \u03b8 and q using KL-DPG.\n\n\nBaselines\n\nWe compare our method to a common approach of using standard reinforcement learning to fine-tune a generative model to conform to desired constraints. We use the Reinforce algorithm (Williams, 1992b) which instead of minimizing divergence from the target distribution p tries to maximize expected reward E \u03c0 \u03b8 R(x). We consider two kinds of reward R(x):\n\u2022 R(x) = b(x)\n, where the generative model is simply rewarded for generating sequences that compile;\n\n\u2022 R(x) = P (x), where the generative model is simply rewarded proportionally to the score our EBM assigns to x. Intuitively, this objective gives reward for both compilability and respecting the original generative model a.\n\n\nEvaluation Metrics\n\nWe evaluate KL-DPG and two baselines in terms of the following metrics:\n1. E x\u223c\u03c0 \u03b8 b(x), compilability rate of sequences sampled from \u03c0 \u03b8 (x), 2. D KL (p, \u03c0 \u03b8 )\n, the forward KL divergence from the optimal distribution p, 5. Self-BLEU-5, a measure of text diversity across samples, proposed in the context of NLP by (Zhu et al., 2018), 6. Perplexity measured on D test , a held-out subset of the data used for training a, calculated as\nexp \u2212 1 N x\u2208Dtest log \u03c0 \u03b8 (x) ,\nwhere N is the overall number of tokens in D test .\n\n7. Sequence length, the average number of characters in generated sequence x after detokenization,\n\n8. AST node count, the average number of nodes in an abstract syntax tree (AST) of sequences that compile. Samples are parsed to their corresponding ASTs using the ast module from Python Standard Library. 5 Intuitively, this metric should indicate the logical (as opposed to surface) complexity of generated programs, 9. PEP8 error frequency, the average number of violations of PEP8, the style guide for Python, 6 measured using pycodestyle, 7 an offthe-shelf linter (static code analysis tool). We report the average number of errors per character to avoid confounding by sequence length.\n\nWhile high compilability rate is the target, the remaining metrics control for various aspects of fluency, quality and diversity of generated samples. Most but not all of these aspects reduce to the constraint of staying close to a; for instance, it is possible for \u03c0 \u03b8 to actually outperform a in matching the statistics of a's own training distribution p * (x).\n\n\nResults\n\nWe present the evolution of nine evaluation metrics as a function of gradient updates on Figures 1 and 2.\n\nReinforce with R(x) = b(x) quickly improves compilability by a large margin but this improvement is mirrored by an equally large divergence from p and a. This divergence translates into generating sequences much shorter (in terms of the number of characters) and logically simpler (in terms of the number of nodes in its AST) than an average 0 100 200 gradient updates 0.6 0.7 0.8 0.9\n1.0 E b(x) KL-DPG R(x) = b(x) R(x) = P(x) Figure 1: Compilability rate E x\u223c\u03c0 \u03b8 b(x) (\u2191 better) of sam-\nples from policies obtained from KL-DPG, and two baselines: Reinforce with reward R(x) = b(x) and with reward R(x) = P (x).\n\nsequence sampled from a. This heavily decreased sequence length (most of the generated functions are one-liners) seems to artificially increase diversity metrics (Self-BLEU-5 and Distinct-1).\n\nReinforce with R(x) = P (x) doesn't improve compilability rate until an inflection point after which it quickly reaches perfect compilability at a price of heavily diverging from both a and (perhaps counterintuitively) p. The reason behind that, however, is that the policy heavily peaks around a single sequence that is compilable. To understand what causes this behavior, first note that the objective for Reinforce with R(\nx) = P (x) is to maximize E x\u223c\u03c0 \u03b8 [a(x)b(x)].\nBecause R(x) = 0 for uncompilable sequences, compilation rate will improve. But for compilable sequences, the effective reward is R(x) = a(x) meaning that \u03c0 \u03b8 is rewarded most for generating the most probable sequences (according to a(x)), making them even more probable. Eventually, E x\u223c\u03c0 \u03b8 a(x) is maximized by a policy peaking on a single sample x that was the most probable one according to a(x). This failure mode is reflected in diversity metrics and perplexity. The sequence the policy peaks on is also shorter and less complex than an average sequence sampled from a.\n\nKL-DPG is the only method that consistently improves compilability rate while decreasing divergence from p, maintaining the diversity of a and only slightly decreasing sequence length and : Evaluation metrics KL(p|\u03c0 \u03b8 ) (\u2193 better), KL(\u03c0 \u03b8 |a) (\u2193 better), Self-BLEU-5 (\u2193 better), Distinct-1 (\u2191 better), AST node count (\u2191 better), PEP8 error count (\u2193 better), sequence length (\u2191 better), and perplexity (\u2193 better) for policies obtained from KL-DPG, and two baselines:\nReinforce with reward R(x) = b(x) and with reward R(x) = P (x).\nthe number of nodes in ASTs. Moreover, as a byproduct of improving compilability, KL-DPG is also able to slightly decrease the perplexity and the frequency of PEP8 violations per character. We conjecture the decrease in perplexity is because compilability provides a training signal enabling \u03c0 \u03b8 to fit the a's training distribution p * (x) better than a was able to. 8 The decrease in the frequency of PEP8 violations might be due to the fact that compilability is correlated with PEP8 compliance.\n\n\nQualitative evaluation\n\nTo further analyze effects of different fine-tuning approaches on sample diversity, we measured the frequency of BPE tokens in generated samples. For each of four analyzed generative models, we sampled 1000 sequences using pure ancestral sampling. We then computed the frequency for each BPE token (the number of times it occurs) and its rank (its index in a sorted list of tokens). We plotted these re-sults on Figure 4. This qualitative evaluation paints a similar picture: fine-tuning using Reinforce incurs a large (with R(x) = b(x)) or extreme (with R(x) = P (x)) decrease in token diversity. In contrast, KL-DPG is able to maintain a relatively long tail of token frequencies, not departing too far from a.\n\nMoreover, in order to gain better understanding of how different fine-tuning methods affect generative models we measured the frequency of different categories of compilation errors for samples from a and from fine-tuned policies. This analysis is presented on Figure 3. We categorized errors using error messages produced by Python interpreter trying to compile an uncompilable sequence. invalid syntax is the most common failure mode (30% of all sequences sampled from a), with a long tail of other error categories. We can see that both KL-DPG and Reinforce with R(x) = b(x) consistently decrease error frequency across almost all the categories.\n\nFinally, in the Appendix we present randomly generated samples from each discussed policy. Tables 3-6 contain samples obtained through unconditional generation. In addition to that, to illustrate  the applicability of obtained policies for code completion, in Tables 7-9 we present samples obtained through conditional generation, i.e. x \u223c \u03c0 \u03b8 (x|c), where the context c is a function name. In either case, samples were obtained using pure ancestral sampling.\n\n\nDiscussion\n\nIn the paper, we presented a new energy-based model formulation for the problem of imposing the constraint of compilability on an autoregressive generative model for source code. In contrast with standard reinforcement learning approaches, the solution we propose -KL-DPG -is able to improve compilability rate without sacrificing diversity and complexity of generated samples.\n\nOne obvious application of the presented approach is improving the accuracy of code completion, i.e. tools assisting in programming by predicting the next tokens based on context (Svyatkovskiy et al., 2020a). The fact that fine-tuning using KL-DPG has a beneficial effect on perplexity and PEP8 error frequency suggests that it can provide a training signal complementary to that in a language modeling objective. The benefits of this auxilary training signal would arguably diminish with increased training time and datatset size, but that still leaves room for significant improvement in low-resource domains.\n\nA limitation of the current KL-DPG approach is that it is restricted to unconditional generation. This is because for a conditional EBM P (x, c) the proportionality constant \u22121/Z from (4) would depend on a context c. Nevertheless, one can imagine using a policy \u03c0 \u03b8 fine-tuned using KL-DPG as initialization of a decoder for conditional generation, e.g. transpilation (translation between programming languages) or program synthesis (translation from a natural language to a programming language). \n\n\nA Hyperparameters and implementation details\n\nWe implemented all models using PyTorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019).\n\nTraining the initial generative model took 10 days on 3 Nvidia Tesla T4 GPUs. For a detailed list of hyperparameter values, see Table 1.          \n\nFigure 2\n2Figure 2: Evaluation metrics KL(p|\u03c0 \u03b8 ) (\u2193 better), KL(\u03c0 \u03b8 |a) (\u2193 better), Self-BLEU-5 (\u2193 better), Distinct-1 (\u2191 better), AST node\n\nFigure 3 :Figure 4 :\n34The frequency (measured as the percentage of samples from \u03c0 \u03b8 (x) causing a given error) of each kind compilation error for the original generative model a and policies fine-tuned using KL-DPG and Reinforce with R(x) = b(x). The policy fine-tuned using Reinforce with R(x) = P (x) was excluded because the single sequence it produces causes no compilation errors. Percentages were computed using 500 samples while confidence intervals were based on 3 repeats of the sampling procedure. R(x) = b(x) R(x) = P(x) Token frequency against token rank computed for tokens found in samples from from KL-DPG, and two baselines. Longer tails imply more diverse samples.\n\n0 def\n0generate_samples_with_prompt_line(self): lines =[] for line in rc: if line.startswith('_','-'): lines.append(\"{}0 %s))\" % line.replace(\".\",\"\\n\") lines.append(\": \".join(lines)) lines.appenddsets() lines.append_): if len(lines)> 0: lines.append(lines[0]) return lines Sequences sampled from a policy fine-tuned using KL-DPG 1 def generate_samples_with_prompt(self): result = self._generate_blobs().generate(self._name,self._amount_in,lambda x:x.lower()) return result 0 def generate_samples_with_prompt_token(self,impdly,red,name,restdeclarations,restid_with_mucmapreduce_access_reference,tpversion): \u2192 if prefix_to_acked_ level_per_pbfrom_account_version(MACRO256): return 71212000x00 * c201402E64D + 204 self.generate_cant_rgb_signature(FLAG,name,comtop header, \"0|02\",[\"-20001500e6fsha\"] 0 def generate_samples_with_prompt(self): tsMAIN_SIZE =(0,1) tsSBream_bin = self.1000 if if tsody_size is not None: tsbleations = y size = ts86.data.get_input(vid_ docs).get_language() for address in data.SerializeToString()if not region: cpu_ratio = np.zeros(freq.encode(\"Now\")) tsLOCATION_examples =[self.read_format(addr)for dir in tsningAssignmentInt()]) Sequences sampled from a policy fine-tuned using Reinforce with R(x) = b(x) 1 def generate_samples_with_prompt(self): pass 1 def generate_samples_with_prompt_indices(self): return self.raw_results_with.raw_options.random_encoding 0 def generate_samples_with_prompt(self, * args, ** kwargs): return self.fit_sum(kwargs -(n))): Sequences sampled from a policy fine-tuned using Reinforce with R(x) = P (x) 0 def generate_samples_with_prompt(self,cached_done,keep = False): if not hasattr(upstream_show,'normalize'): return sm =wb. cppProcessor(cached_TLS = False) self.maxOccurs = self.concurrency. anno_DealList() tool.is(csrf_restore,lazy = True) self.salt_made(csrf) 1 def generate_samples_with_prompt(self): start = back_start -self.start + self.test_samples().set_ofmid result =[] for step in range(start): result.append(step) result.append(step) return result 0 def generate_samples_with_prompt(self,type::phone_shard = None): return int(int(self.last_offsets_best_timescale,type_op = \"0\"))\n\n\nAn imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc.Youri Arkesteijn, Nikhil Saldanha, and Bastijn \nKostense. 2020. \nCode completion using neu-\nral attention and byte pair encoding. \nCoRR, \nabs/2004.06343. \n\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, \nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. \nCourville, and Yoshua Bengio. 2017. An actor-critic \nalgorithm for sequence prediction. In 5th Inter-\nnational Conference on Learning Representations, \nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net. \n\nA. Bakhtin, Y. Deng, S. Gross, Myle Ott, Marc'Aurelio \nRanzato, and Arthur Szlam. 2020. Energy-based \nmodels for text. ArXiv, abs/2004.10188. \n\nDavid Belanger and Andrew McCallum. 2016. Struc-\ntured prediction energy networks. In Proceedings \nof the 33rd International Conference on Interna-\ntional Conference on Machine Learning -Volume 48, \nICML'16, pages 983-992. JMLR.org. \n\nPavol Bielik, Veselin Raychev, and Martin Vechev. \n2016. Phog: Probabilistic model for code. In Pro-\nceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning -Vol-\nume 48, ICML'16, page 2933-2942. JMLR.org. \n\nMatteo Ciniselli, Nathan Cooper, Luca Pascarella, \nDenys Poshyvanyk, Massimiliano Di Penta, and \nGabriele Bavota. 2021. An empirical study on the \nusage of BERT models for code completion. CoRR, \nabs/2103.07115. \n\nImre Csisz\u00e1r and Paul C. Shields. 2004. Information \ntheory and statistics: A tutorial. Commun. Inf. The-\nory, 1(4):417-528. \n\nYuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, \nand Marc'Aurelio Ranzato. 2020. Residual energy-\nbased models for text generation. In 8th Inter-\nnational Conference on Learning Representations, \nICLR 2020, Addis Ababa, Ethiopia, April 26-30, \n2020. OpenReview.net. \n\nYilun Du and Igor Mordatch. 2019. Implicit genera-\ntion and modeling with energy based models. In \nAdvances in Neural Information Processing Systems, \nvolume 32. Curran Associates, Inc. \n\nGeoffrey E. Hinton. 2002. Training products of experts \nby minimizing contrastive divergence. Neural Com-\nput., 14(8):1771-1800. \n\nNatasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose \nMiguel Hernandez Lobato, Richard E. Turner, and \nDoug Eck. 2017. Tuning recurrent neural networks \nwith reinforcement learning. \n\nA. Karpathy, J. Johnson, and Li Fei-Fei. 2015. Visual-\nizing and understanding recurrent networks. ArXiv, \nabs/1506.02078. \n\nMuhammad Khalifa, Hady Elsahar, and Marc Dymet-\nman. 2021. A distributional approach to controlled \ntext generation. In International Conference on \nLearning Representations. \n\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A \nmethod for stochastic optimization. arXiv preprint \narXiv:1412.6980. \n\nVijay Konda and John Tsitsiklis. 2000. Actor-critic al-\ngorithms. In Advances in Neural Information Pro-\ncessing Systems, volume 12. MIT Press. \n\nSumith Kulal, Panupong Pasupat, Kartik Chandra, \nMina Lee, Oded Padon, Alex Aiken, and Percy S \nLiang. 2019. Spoc: Search-based pseudocode to \ncode. In Advances in Neural Information Process-\ning Systems, volume 32. Curran Associates, Inc. \n\nYann LeCun, Sumit Chopra, Raia Hadsell, \nMarc'Aurelio Ranzato, and Fu Jie Huang. 2006. A \nTutorial on Energy-Based Learning. In Predicting \nStructured Data. MIT Press. \n\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, \nand Bill Dolan. 2016a. A diversity-promoting ob-\njective function for neural conversation models. In \nProceedings of the 2016 Conference of the North \nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, \npages 110-119, San Diego, California. Association \nfor Computational Linguistics. \n\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, \nMichel Galley, and Jianfeng Gao. 2016b. Deep rein-\nforcement learning for dialogue generation. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016, \nAustin, Texas, USA, November 1-4, 2016, pages \n1192-1202. The Association for Computational Lin-\nguistics. \n\nChang Liu, Xin Wang, Richard Shin, Joseph E Gonza-\nlez, and Dawn Song. 2016a. Neural code comple-\ntion. \n\nSiqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, \nand Kevin Murphy. 2016b. Optimization of image \ndescription metrics using policy gradient methods. \nCoRR, abs/1612.00370. \n\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey \nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement, \nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-\ndaresan, Shao Kun Deng, Shengyu Fu, and Shujie \nLiu. 2021. Codexglue: A machine learning bench-\nmark dataset for code understanding and generation. \nCoRR, abs/2102.04664. \n\nChris J. Maddison and Daniel Tarlow. 2014. Structured \ngenerative models of natural source code. In Pro-\nceedings of the 31st International Conference on In-\nternational Conference on Machine Learning -Vol-\nume 32, ICML'14, page II-649-II-657. JMLR.org. \n\nTung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh \nNguyen, and Tien N. Nguyen. 2013. A statistical \nsemantic language model for source code. In Pro-\nceedings of the 2013 9th Joint Meeting on Foun-\ndations of Software Engineering, ESEC/FSE 2013, \npage 532-542, New York, NY, USA. Association for \nComputing Machinery. \n\nArt B. Owen. 2013. Importance Sampling. In Monte \nCarlo theory, methods and examples, chapter 9. \n\nTetiana Parshakova, Jean-Marc Andreoli, and Marc \nDymetman. 2019a. Distributional Reinforcement \nLearning For Energy-Based Sequential Models. \nCoRR. \n\nTetiana Parshakova, Jean-Marc Andreoli, and Marc \nDymetman. 2019b. Global Autoregressive Models \nfor Data-Efficient Sequence Learning. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL), pages 900-909, \nHong Kong, China. Association for Computational \nLinguistics. \n\nRamakanth Pasunuru and Mohit Bansal. 2017. Re-\ninforced video captioning with entailment rewards. \nIn Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing, \nEMNLP 2017, Copenhagen, Denmark, September 9-\n11, 2017, pages 979-985. Association for Computa-\ntional Linguistics. \n\nAdam Paszke, Sam Gross, Francisco Massa, Adam \nLerer, James Bradbury, Gregory Chanan, Trevor \nKilleen, Zeming Lin, Natalia Gimelshein, Luca \nAntiga, Alban Desmaison, Andreas Kopf, Edward \nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, \nJunjie Bai, and Soumith Chintala. 2019. \nPy-\ntorch: Romain Paulus, Caiming Xiong, and Richard Socher. \n2018. A deep reinforced model for abstractive sum-\nmarization. In 6th International Conference on \nLearning Representations, ICLR 2018, Vancouver, \nBC, Canada, April 30 -May 3, 2018, Conference \nTrack Proceedings. OpenReview.net. \n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, \nDario Amodei, and Ilya Sutskever. 2019. Language \nmodels are unsupervised multitask learners. OpenAI \nBlog, 1(8):9. \n\nMarc'Aurelio Ranzato, Y-Lan Boureau, Sumit Chopra, \nand Yann LeCun. 2007. A unified energy-based \nframework for unsupervised learning. \nIn Pro-\nceedings of the Eleventh International Conference \non Artificial Intelligence and Statistics, AISTATS \n2007, San Juan, Puerto Rico, March 21-24, 2007, \nvolume 2 of JMLR Proceedings, pages 371-379. \nJMLR.org. \n\nMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, \nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks. In 4th Inter-\nnational Conference on Learning Representations, \nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, \nConference Track Proceedings. \n\nVeselin Raychev, Pavol Bielik, and Martin Vechev. \n2016. Probabilistic model for code with decision \ntrees. SIGPLAN Not., 51(10):731-747. \n\nVeselin Raychev, Martin Vechev, and Eran Yahav. 2014. \nCode completion with statistical language models. \nSIGPLAN Not., 49(6):419-428. \n\nBaptiste Roziere, Marie-Anne Lachaux, Lowik \nChanussot, and Guillaume Lample. 2020. Un-\nsupervised translation of programming languages. \nAdvances in Neural Information Processing Systems, \n33. \n\nRico Sennrich, Barry Haddow, and Alexandra Birch. \n2016. Neural machine translation of rare words \nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational \nLinguistics (Volume 1: Long Papers), pages 1715-\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics. \n\nRichard S. Sutton, David McAllester, Satinder Singh, \nand Yishay Mansour. 1999. Policy gradient methods \nfor reinforcement learning with function approxima-\ntion. In Proceedings of the 12th International Con-\nference on Neural Information Processing Systems, \nNIPS'99, page 1057-1063, Cambridge, MA, USA. \nMIT Press. \n\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, \nand Neel Sundaresan. 2020a. Intellicode compose: \nCode generation using transformer. In Proceed-\nings of the 28th ACM Joint Meeting on European \nSoftware Engineering Conference and Symposium \non the Foundations of Software Engineering, ES-\nEC/FSE 2020, page 1433-1443, New York, NY, \nUSA. Association for Computing Machinery. \n\n\nTable 2 :\n2Hyperparameters used for training \u03c0 \u03b8 using \nKL-DPG and Reinforce \n\n9 https://github.com/naver/gdc \n\n\n\nTable 3 :\n3Sequences sampled from the original generative model a basic = asim.bin.sample(srs,rng = self.ctypes,trials = self.rng,dtype = self.dtype) Elements([(\"Unsupported Ct%s]\" % ','.join(self.__class__.__name__)) return \"Data attribute '%s' % ','.join(\"%sCHOICES from %s\" %(WARNING,str(r)))b(x) \n\nProgram \n\n0 \n\ndef fetch_size(self,page): \nresponse = self.fetch(page,max((2)) \nconstant(response.json(),response.pop('utf-8')) \npayload = \"%s//%s//%s//%s//%s\" %(self.resource.id,page.format_from_bytes()) \nreturn payload \n\n0 \n\ndef setUp(self): \nself.project_loader = testutil.FileSentenceDependencyGraph(extensions =['file','path']) \nself.schema =RelatedPackage preserveLoader(root_loader) \nself.extension_context = XMLLoader() \n\n1 \ndef __getattr__(self,perm): \nreturn self._memo.get(perm) \n\n1 \n\ndef expand(self,text): \nvalue.strip() \nreturn extract_cseq(text) \n\n1 \n\ndef test_Obze(self): \nw = Command() \nself.assertEqual(w.callHeader.callHeader,self.result) \n\n0 \n\ndef start_stream(self,addressFamily,opcode): \nlogger.info(\"OpenlibwriteStructBegin chunkon.csv',OperationalError()) \nerror_message = self.get_stream([None,None]) \nmessage,message = self.block_messages[0] \nmessage = message[0] \nself._process_message(message,message,message,message) \n\n0 \n\ndef set_dense(self,srs,fit_to): \nif dup in self.scalar: \nreturn \nif not isinstance(modality,(pyobj): \nself.sq =SUBNET \nself.1 \ndef _act(self,value): \nself._result.set_argument('value',value) \n\n1 \ndef _verify_ssling_access_admin(self,ip_name): \nself._check_proxy(ip_name) \n\n0 \n\ndef __str__(self): \nr =[] \nfor s in self.__dict__.items(): \nif s[0]in BoundCacheContents(): \nbreak \nif s[:-1]:0 \ndef test_FaceIP_3D_14(self): \nself.assertTrue(self.doTestFace(self.doTestFace([self.doTestFace([False,False)]) \n\n0 \n\ndef __init__(self, ** options): \nsuper(_ChoiceTest,self).__init__( ** options) \nself.action_classes = options[\"cells_store\"] \nself.choices =(1.2, ** options[\"mysql\"]= FakeMissingTuple()) \nself.parser = Message(list.__init__(option_forms)) \n\n1 \n\ndef main(self,client): \nremove_home_config(client,\"client_snapshot_url\") \nself.client.client_snapshot.update(client) \n\n1 \ndef _stop_signal(self,emitter,datafile,for_attachment): \nvim.gui.target_cancel() \n\n\n\nTable 4 :\n4Sequences sampled from a policy fine-tuned using KL-DPG += self.lengthString(len(self.parameters_)) return n + self.lengthString(number(self.value_)) def __init__(self, ** kwargs): self.sourcemersListComp = kwargs.get('stretch {}'.format(self.__class__.twsourceCentOS_text))b(x) \n\nProgram \n\n1 \ndef invalidateKey(self): \nself.action.rooms = { } \n\n1 \ndef get(self): \nreturn self.handler.identifier \n\n1 \ndef flush(self): \nself.write(\"ready\") \n\n1 \ndef get_flavor(self,resource,path, ** metadata): \nreturn self.context.get(resource,path, ** metadata) \n\n1 \n\ndef test_api_set_to_result(self): \nX = T.ListHead() \nself.assertEquals(quantiles(X),self._cache.annotations) \n\n1 \ndef is_cmp(self,other): \nreturn not self._safe_eq(other,self.link) \n\n1 \ndef __iter__(self): \nreturn iter(self._reverse()) \n\n1 \ndef cancel(self): \nreturn self.enhanced_window.set_timeout() \n\n1 \ndef __str__(self): \nreturn str(self.repository) \n\n1 \ndef summary(self): \nreturn self._series \n\n1 \ndef Lazypeer(self): \nreturn self._peer \n\n1 \n\ndef ByteSize(self): \nn = 0 \nn 1 \n\ndef setUp(self): \nsuper(TestMaUserRoleTestCase,self).setUp() \nself.core =BER() \nself.topsetup_existing = False \n\n1 \n\n\nTable 5 :\n5Sequences sampled from a policy fine-tuned using Reinforce with R(x) = b(x)b(x) \n\nProgram \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n1 \ndef set_OwnerId(self,OwnerId): \nself.add_query_param('OwnerId',OwnerId) \n\n\n\nTable 6 :\n6Sequences sampled from a policy finetuned using Reinforce with R(x) = P (x) Sequences sampled from a policy fine-tuned using KL-DPG def close(self): if not self.closed: self.closed = True self.translation.close() def close(self): self.queue.Importer.close(self.info) self.open_input.close() self.graph.close(self.gamma) def close(self): try: self.srv.get_browser.mac(self.bus_process.name,vm_output = True) except suspended as ex: self.socket.stop(ex)Sequences sampled from a policy fine-tuned using Reinforce with R(x) = b(x)Sequences sampled from a policy fine-tuned using Reinforce with R(x) = P (x) 1 def close(self,object): self.api.close(self.uid.length)b(x) \n\nProgram \n\nSequences sampled from the original generative model a \n\n1 \n\ndef closeEvent(self): \nself._isalive = False \nself._original_resume = True \n\n1 \ndef close_file(self): \npass \n\n1 \ndef closeWorking(self): \npass \n\n1 \n\n1 \n\n1 \n\n1 \ndef close(self): \nself._stdout.close() \n\n1 \ndef close(self): \nself.idb.close() \n\n1 \n\ndef close(self): \nself.reuse = subprocess.Popen('CONNECTION','').unregisterProducer() \np = subprocess.Popen() \np.communicate().close() \nreturn u.close() \n\n1 \ndef close(self): \nself.job_closed.remove(self) \n\n1 \ndef close(self): \nself.buffer.flush() \n\n\n\nTable 7 :\n7Samples obtained from policies conditioned on prompt def close Program Sequences sampled from the original generative model a def fit_pdf(self,hop,theta,theta): asserttriangular is self._fit_rewrite(hop, kernel,theta,theta)-gtheta,70) assertworkspace isTType.ACCEPTED_ignore assert subset in(coeff,Y) assert self._Xfd != xOpenStackBackendError assert isinstance(750,Win,T,Vector) def fit(self,X = None,y = None,result = None): sts = self.get_appId(self.mesh_filename,X,y = y,d = result) self.mirror_logpdf([0x9] * indented) Sequences sampled from a policy fine-tuned using KL-DPG def fit(self,X,y, * args, ** kwargs): X = self.transform(X,y, * args, ** kwargs) data = np.DataFrame(data) for i in self.fallback_array.iteration_two( * data): data[i].labels[i].tolist() return data def fit(self, initial_output = None): if initial_output: self.force_input = False else: self.cells_done = tuple(initial_output) if initial_input == self.WK_MASK: self.output_output += self.osfstorage_NORMAL self.outputs = list([self.inputState.NORMAL_READ valid]) return 1 1 def fit(self,reshape,a,b): return frappe. filediff(islice(a,b),b) Sequences sampled from a policy fine-tuned using Reinforce with R(x) = b(x) def fit(self,fit,d): self.fit =followers return super(PositionUntilLockedSequence,self).fit(marks) def fit(self,X_acc): X_exog = self.xc1.exog y = self.instance.exog y,= self.model.w2 preserve_uniform(os.environ.XMANllf,y_y)) y += self.model.t2le continX y = self.transition.fit(y) y.y = self.model.y * y y.red = self.model.gw.urmpopow(y) return y Sequences sampled from a policy fine-tuned using Reinforce with R(x) = P (x) def fit(self,fit,X,y,z): self.learning = indices[np.zeros(axis = 1Dot,y = y,motion = self. np.loss,y = res.scale)] self.index = yb(x) \n\n0 \n\n0 \n\ndef fit(self,X,y): \nself._ y = y \nself._children -= 1 \nassert isinstance(self._labels,_MOD_'\") \nx[:]= 0 \ny[:]=Bio_OFFSET \ny * = self._labels \ny * y * y \ny //= y \nreturn y \n\n1 \n\n1 \n\n0 \n\n1 \ndef fit(self,X,y): \nself.x = y \n\n1 \n\n0 \n\n0 \n\n1 \ndef fit(self,params): \nself.params_param = params \n\n1 \n\ndef fit(self,X,y = None): \nself.x = x \nself.y = x \n\n\n\nTable 8 :\n8Samples obtained from policies conditioned on prompt def fit Program Sequences sampled from the original generative model a def generate_samples_with_prompt(self,input_value,decimal = False): use_full = False full_input_string = escape_input[decimal] newprefix = local_input_format.split(\"<%s__\") % input_label.strip(),[formatted_full]) return newprefixb(x) \n\n0 \n\n1 \ndef generate_samples_with_prompt_publish(self): \nself.overflow = self.percent \n\n\n\nTable 9 :\n9Samples obtained from policies conditioned on prompt def generate samples with prompt\nOne exception is the work of Maddison and Tarlow (2014), who augment neural probabilistic context free grammars with semantic constraints and use them for unconditional generation.\nhttps://docs.python.org/3/library/ codeop.html\n. D KL (\u03c0 \u03b8 , a), the reverse KL divergence from the original pretrained generative model, 4. Distinct-1 score, a measure of text diversity in terms of the frequency of token repetitions in a sample x, proposed in the context of NLP by(Li et al., 2016a),   4  Note that initial compilability rate will be equal to our Z because Ex\u223cab(x) = x a(x)b(x) = x P (x) = Z.\nhttps://docs.python.org/3/library/ast. html 6 https://www.python.org/dev/peps/ pep-0008/ 7 https://github.com/PyCQA/pycodestyle\nThis mirrors the results obtained byParshakova et al.  (2019b), who also defined an EBM augmenting an autoregressive model with prior knowledge about features of the training set and observed a decrease in perplexity compared to pure autoregressive training.\n\nA survey of machine learning for big code and naturalness. Miltiadis Allamanis, Earl T Barr, Premkumar T Devanbu, Charles Sutton, 10.1145/3212695ACM Comput. Surv. 51437Miltiadis Allamanis, Earl T. Barr, Premkumar T. De- vanbu, and Charles Sutton. 2018. A survey of ma- chine learning for big code and naturalness. ACM Comput. Surv., 51(4):81:1-81:37.\n\nDaniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, 10.18653/v1/P16-1231Globally Normalized Transition-Based Neural Networks. Slav Petrov, and Michael CollinsDaniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Globally Nor- malized Transition-Based Neural Networks.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, Machine Learning. Ronald J. Williams. 1992b. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. In Machine Learning, pages 229- 256.\n\nHuggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, abs/1910.03771CoRRMorgan Funtowicz, and Jamie BrewThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans- formers: State-of-the-art natural language process- ing. CoRR, abs/1910.03771.\n\nSequence-based structured prediction for semantic parsing. Chunyang Xiao, Marc Dymetman, Claire Gardent, 10.18653/v1/P16-1127Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsChunyang Xiao, Marc Dymetman, and Claire Gardent. 2016. Sequence-based structured prediction for se- mantic parsing. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1341- 1350, Berlin, Germany. Association for Computa- tional Linguistics.\n\nVictor Zhong, Caiming Xiong, Richard Socher, arXiv:1709.00103Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprintVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103.\n\nTexygen: A benchmarking platform for text generation models. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, Yong Yu, 10.1145/3209978.3210080The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018. Ann Arbor, MI, USAACMYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texy- gen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Con- ference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08- 12, 2018, pages 1097-1100. ACM.\n\nFine-tuning language models from human preferences. M Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, abs/1909.08593CoRRDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris- tiano, and Geoffrey Irving. 2019. Fine-tuning lan- guage models from human preferences. CoRR, abs/1909.08593.\n", "annotations": {"author": "[{\"end\":148,\"start\":75},{\"end\":215,\"start\":149},{\"end\":284,\"start\":216},{\"end\":361,\"start\":285}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":82},{\"end\":161,\"start\":154},{\"end\":229,\"start\":221},{\"end\":302,\"start\":292}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":153,\"start\":149},{\"end\":220,\"start\":216},{\"end\":291,\"start\":285}]", "author_affiliation": "[{\"end\":147,\"start\":112},{\"end\":214,\"start\":190},{\"end\":283,\"start\":259},{\"end\":360,\"start\":336}]", "title": "[{\"end\":72,\"start\":1},{\"end\":433,\"start\":362}]", "venue": null, "abstract": "[{\"end\":1565,\"start\":435}]", "bib_ref": "[{\"end\":2008,\"start\":1986},{\"end\":2028,\"start\":2008},{\"end\":2101,\"start\":2082},{\"end\":2130,\"start\":2101},{\"end\":2154,\"start\":2130},{\"end\":2177,\"start\":2154},{\"end\":2180,\"start\":2179},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2304,\"start\":2281},{\"end\":2828,\"start\":2802},{\"end\":3120,\"start\":3098},{\"end\":3300,\"start\":3278},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3365,\"start\":3346},{\"end\":4154,\"start\":4132},{\"end\":4977,\"start\":4956},{\"end\":4998,\"start\":4977},{\"end\":5020,\"start\":4998},{\"end\":5040,\"start\":5020},{\"end\":5242,\"start\":5214},{\"end\":5258,\"start\":5242},{\"end\":5391,\"start\":5390},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5558,\"start\":5539},{\"end\":5835,\"start\":5816},{\"end\":6205,\"start\":6188},{\"end\":6253,\"start\":6218},{\"end\":6298,\"start\":6270},{\"end\":6350,\"start\":6329},{\"end\":6389,\"start\":6371},{\"end\":6418,\"start\":6389},{\"end\":6468,\"start\":6441},{\"end\":6564,\"start\":6542},{\"end\":6586,\"start\":6564},{\"end\":6728,\"start\":6711},{\"end\":7643,\"start\":7623},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7740,\"start\":7719},{\"end\":7950,\"start\":7928},{\"end\":8128,\"start\":8114},{\"end\":8147,\"start\":8128},{\"end\":8168,\"start\":8147},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8533,\"start\":8513},{\"end\":8561,\"start\":8533},{\"end\":8689,\"start\":8667},{\"end\":8721,\"start\":8705},{\"end\":8835,\"start\":8810},{\"end\":8858,\"start\":8840},{\"end\":9005,\"start\":8984},{\"end\":10531,\"start\":10504},{\"end\":12197,\"start\":12176},{\"end\":13215,\"start\":13186},{\"end\":13839,\"start\":13816},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15421,\"start\":15404},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16258,\"start\":16240},{\"end\":22954,\"start\":22926},{\"end\":23968,\"start\":23947},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24004,\"start\":23985}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24295,\"start\":24154},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24979,\"start\":24296},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27129,\"start\":24980},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36417,\"start\":27130},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36531,\"start\":36418},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38743,\"start\":36532},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39908,\"start\":38744},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41245,\"start\":39909},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":42491,\"start\":41246},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44613,\"start\":42492},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":45073,\"start\":44614},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45171,\"start\":45074}]", "paragraph": "[{\"end\":2273,\"start\":1581},{\"end\":2318,\"start\":2275},{\"end\":3562,\"start\":2320},{\"end\":4155,\"start\":3564},{\"end\":4724,\"start\":4157},{\"end\":5941,\"start\":4741},{\"end\":6666,\"start\":5943},{\"end\":8056,\"start\":6668},{\"end\":9489,\"start\":8058},{\"end\":10176,\"start\":9500},{\"end\":10198,\"start\":10195},{\"end\":10277,\"start\":10200},{\"end\":10304,\"start\":10299},{\"end\":11165,\"start\":10323},{\"end\":11447,\"start\":11167},{\"end\":11769,\"start\":11592},{\"end\":11882,\"start\":11829},{\"end\":12755,\"start\":11884},{\"end\":12867,\"start\":12757},{\"end\":12891,\"start\":12869},{\"end\":12997,\"start\":12958},{\"end\":13572,\"start\":12999},{\"end\":13946,\"start\":13574},{\"end\":14631,\"start\":13974},{\"end\":15208,\"start\":14633},{\"end\":15575,\"start\":15222},{\"end\":15676,\"start\":15590},{\"end\":15901,\"start\":15678},{\"end\":15995,\"start\":15924},{\"end\":16359,\"start\":16085},{\"end\":16443,\"start\":16392},{\"end\":16543,\"start\":16445},{\"end\":17135,\"start\":16545},{\"end\":17500,\"start\":17137},{\"end\":17617,\"start\":17512},{\"end\":18003,\"start\":17619},{\"end\":18230,\"start\":18107},{\"end\":18423,\"start\":18232},{\"end\":18850,\"start\":18425},{\"end\":19472,\"start\":18897},{\"end\":19939,\"start\":19474},{\"end\":20502,\"start\":20004},{\"end\":21241,\"start\":20529},{\"end\":21892,\"start\":21243},{\"end\":22353,\"start\":21894},{\"end\":22745,\"start\":22368},{\"end\":23358,\"start\":22747},{\"end\":23858,\"start\":23360},{\"end\":24005,\"start\":23907},{\"end\":24153,\"start\":24007}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10194,\"start\":10177},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10298,\"start\":10278},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10322,\"start\":10305},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11591,\"start\":11448},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11828,\"start\":11770},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12957,\"start\":12892},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15589,\"start\":15576},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16084,\"start\":15996},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16391,\"start\":16360},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18106,\"start\":18004},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18896,\"start\":18851},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20003,\"start\":19940}]", "table_ref": "[{\"end\":13759,\"start\":13752},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15111,\"start\":15103},{\"end\":24142,\"start\":24135}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1579,\"start\":1567},{\"attributes\":{\"n\":\"2\"},\"end\":4739,\"start\":4727},{\"attributes\":{\"n\":\"3\"},\"end\":9498,\"start\":9492},{\"end\":13972,\"start\":13949},{\"attributes\":{\"n\":\"4.2\"},\"end\":15220,\"start\":15211},{\"attributes\":{\"n\":\"4.3\"},\"end\":15922,\"start\":15904},{\"attributes\":{\"n\":\"5\"},\"end\":17510,\"start\":17503},{\"attributes\":{\"n\":\"5.1\"},\"end\":20527,\"start\":20505},{\"attributes\":{\"n\":\"6\"},\"end\":22366,\"start\":22356},{\"end\":23905,\"start\":23861},{\"end\":24163,\"start\":24155},{\"end\":24317,\"start\":24297},{\"end\":24986,\"start\":24981},{\"end\":36428,\"start\":36419},{\"end\":36542,\"start\":36533},{\"end\":38754,\"start\":38745},{\"end\":39919,\"start\":39910},{\"end\":41256,\"start\":41247},{\"end\":42502,\"start\":42493},{\"end\":44624,\"start\":44615},{\"end\":45084,\"start\":45075}]", "table": "[{\"end\":36417,\"start\":27382},{\"end\":36531,\"start\":36430},{\"end\":38743,\"start\":36828},{\"end\":39908,\"start\":39030},{\"end\":41245,\"start\":39996},{\"end\":42491,\"start\":41918},{\"end\":44613,\"start\":44254},{\"end\":45073,\"start\":44979}]", "figure_caption": "[{\"end\":24295,\"start\":24165},{\"end\":24979,\"start\":24320},{\"end\":27129,\"start\":24988},{\"end\":27382,\"start\":27132},{\"end\":36828,\"start\":36544},{\"end\":39030,\"start\":38756},{\"end\":39996,\"start\":39921},{\"end\":41918,\"start\":41258},{\"end\":44254,\"start\":42504},{\"end\":44979,\"start\":44626},{\"end\":45171,\"start\":45086}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17616,\"start\":17601},{\"end\":20949,\"start\":20941},{\"end\":21512,\"start\":21504}]", "bib_author_first_name": "[{\"end\":46221,\"start\":46212},{\"end\":46237,\"start\":46233},{\"end\":46239,\"start\":46238},{\"end\":46255,\"start\":46246},{\"end\":46257,\"start\":46256},{\"end\":46274,\"start\":46267},{\"end\":46511,\"start\":46505},{\"end\":46524,\"start\":46519},{\"end\":46539,\"start\":46534},{\"end\":46555,\"start\":46547},{\"end\":46575,\"start\":46565},{\"end\":46590,\"start\":46584},{\"end\":46994,\"start\":46988},{\"end\":46996,\"start\":46995},{\"end\":47264,\"start\":47258},{\"end\":47279,\"start\":47271},{\"end\":47293,\"start\":47287},{\"end\":47306,\"start\":47300},{\"end\":47324,\"start\":47317},{\"end\":47342,\"start\":47335},{\"end\":47355,\"start\":47348},{\"end\":47367,\"start\":47364},{\"end\":47379,\"start\":47375},{\"end\":47776,\"start\":47768},{\"end\":47787,\"start\":47783},{\"end\":47804,\"start\":47798},{\"end\":48385,\"start\":48379},{\"end\":48400,\"start\":48393},{\"end\":48415,\"start\":48408},{\"end\":48793,\"start\":48786},{\"end\":48803,\"start\":48799},{\"end\":48811,\"start\":48808},{\"end\":48826,\"start\":48819},{\"end\":48838,\"start\":48832},{\"end\":48849,\"start\":48846},{\"end\":48860,\"start\":48856},{\"end\":49397,\"start\":49396},{\"end\":49411,\"start\":49406},{\"end\":49428,\"start\":49421},{\"end\":49442,\"start\":49439},{\"end\":49444,\"start\":49443},{\"end\":49453,\"start\":49449},{\"end\":49466,\"start\":49461},{\"end\":49480,\"start\":49476},{\"end\":49497,\"start\":49489}]", "bib_author_last_name": "[{\"end\":46231,\"start\":46222},{\"end\":46244,\"start\":46240},{\"end\":46265,\"start\":46258},{\"end\":46281,\"start\":46275},{\"end\":46517,\"start\":46512},{\"end\":46532,\"start\":46525},{\"end\":46545,\"start\":46540},{\"end\":46563,\"start\":46556},{\"end\":46582,\"start\":46576},{\"end\":46598,\"start\":46591},{\"end\":47005,\"start\":46997},{\"end\":47269,\"start\":47265},{\"end\":47285,\"start\":47280},{\"end\":47298,\"start\":47294},{\"end\":47315,\"start\":47307},{\"end\":47333,\"start\":47325},{\"end\":47346,\"start\":47343},{\"end\":47362,\"start\":47356},{\"end\":47373,\"start\":47368},{\"end\":47384,\"start\":47380},{\"end\":47781,\"start\":47777},{\"end\":47796,\"start\":47788},{\"end\":47812,\"start\":47805},{\"end\":48391,\"start\":48386},{\"end\":48406,\"start\":48401},{\"end\":48422,\"start\":48416},{\"end\":48797,\"start\":48794},{\"end\":48806,\"start\":48804},{\"end\":48817,\"start\":48812},{\"end\":48830,\"start\":48827},{\"end\":48844,\"start\":48839},{\"end\":48854,\"start\":48850},{\"end\":48863,\"start\":48861},{\"end\":49404,\"start\":49398},{\"end\":49419,\"start\":49412},{\"end\":49437,\"start\":49429},{\"end\":49447,\"start\":49445},{\"end\":49459,\"start\":49454},{\"end\":49474,\"start\":49467},{\"end\":49487,\"start\":49481},{\"end\":49508,\"start\":49498},{\"end\":49516,\"start\":49510}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/3212695\",\"id\":\"b0\",\"matched_paper_id\":207591052},\"end\":46503,\"start\":46153},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1231\",\"id\":\"b1\"},\"end\":46896,\"start\":46505},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2332513},\"end\":47182,\"start\":46898},{\"attributes\":{\"doi\":\"abs/1910.03771\",\"id\":\"b3\"},\"end\":47707,\"start\":47184},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1127\",\"id\":\"b4\",\"matched_paper_id\":16911296},\"end\":48377,\"start\":47709},{\"attributes\":{\"doi\":\"arXiv:1709.00103\",\"id\":\"b5\"},\"end\":48723,\"start\":48379},{\"attributes\":{\"doi\":\"10.1145/3209978.3210080\",\"id\":\"b6\",\"matched_paper_id\":3636178},\"end\":49342,\"start\":48725},{\"attributes\":{\"doi\":\"abs/1909.08593\",\"id\":\"b7\"},\"end\":49746,\"start\":49344}]", "bib_title": "[{\"end\":46210,\"start\":46153},{\"end\":46986,\"start\":46898},{\"end\":47766,\"start\":47709},{\"end\":48784,\"start\":48725}]", "bib_author": "[{\"end\":46233,\"start\":46212},{\"end\":46246,\"start\":46233},{\"end\":46267,\"start\":46246},{\"end\":46283,\"start\":46267},{\"end\":46519,\"start\":46505},{\"end\":46534,\"start\":46519},{\"end\":46547,\"start\":46534},{\"end\":46565,\"start\":46547},{\"end\":46584,\"start\":46565},{\"end\":46600,\"start\":46584},{\"end\":47007,\"start\":46988},{\"end\":47271,\"start\":47258},{\"end\":47287,\"start\":47271},{\"end\":47300,\"start\":47287},{\"end\":47317,\"start\":47300},{\"end\":47335,\"start\":47317},{\"end\":47348,\"start\":47335},{\"end\":47364,\"start\":47348},{\"end\":47375,\"start\":47364},{\"end\":47386,\"start\":47375},{\"end\":47783,\"start\":47768},{\"end\":47798,\"start\":47783},{\"end\":47814,\"start\":47798},{\"end\":48393,\"start\":48379},{\"end\":48408,\"start\":48393},{\"end\":48424,\"start\":48408},{\"end\":48799,\"start\":48786},{\"end\":48808,\"start\":48799},{\"end\":48819,\"start\":48808},{\"end\":48832,\"start\":48819},{\"end\":48846,\"start\":48832},{\"end\":48856,\"start\":48846},{\"end\":48865,\"start\":48856},{\"end\":49406,\"start\":49396},{\"end\":49421,\"start\":49406},{\"end\":49439,\"start\":49421},{\"end\":49449,\"start\":49439},{\"end\":49461,\"start\":49449},{\"end\":49476,\"start\":49461},{\"end\":49489,\"start\":49476},{\"end\":49510,\"start\":49489},{\"end\":49518,\"start\":49510}]", "bib_venue": "[{\"end\":46706,\"start\":46674},{\"end\":48010,\"start\":47923},{\"end\":49014,\"start\":48996},{\"end\":46314,\"start\":46298},{\"end\":46672,\"start\":46620},{\"end\":47023,\"start\":47007},{\"end\":47256,\"start\":47184},{\"end\":47921,\"start\":47834},{\"end\":48529,\"start\":48440},{\"end\":48994,\"start\":48888},{\"end\":49394,\"start\":49344}]"}}}, "year": 2023, "month": 12, "day": 17}