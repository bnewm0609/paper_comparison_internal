{"id": 258841328, "updated": "2023-10-05 00:29:17.522", "metadata": {"title": "QLoRA: Efficient Finetuning of Quantized LLMs", "authors": "[{\"first\":\"Tim\",\"last\":\"Dettmers\",\"middle\":[]},{\"first\":\"Artidoro\",\"last\":\"Pagnoni\",\"middle\":[]},{\"first\":\"Ari\",\"last\":\"Holtzman\",\"middle\":[]},{\"first\":\"Luke\",\"last\":\"Zettlemoyer\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.14314", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-14314", "doi": "10.48550/arxiv.2305.14314"}}, "content": {"source": {"pdf_hash": "32ac52069e562d4f900afee70bdca63f53461481", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.14314v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b2c4ac349cb04b4083bb72734e3b54c17f4c95ac", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/32ac52069e562d4f900afee70bdca63f53461481.txt", "contents": "\nQLORA: Efficient Finetuning of Quantized LLMs\n\n\nTim Dettmers dettmers@cs.washington.edu \nUniversity of Washington\n\n\nArtidoro Pagnoni artidoro@cs.washington.edu \nUniversity of Washington\n\n\nAri Holtzman \nUniversity of Washington\n\n\nLuke Zettlemoyer \nUniversity of Washington\n\n\nQLORA: Efficient Finetuning of Quantized LLMs\n\nWe present QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLORA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLORA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLORA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training. 2 * Equal contribution.\n\nIntroduction\n\nFinetuning large language models (LLMs) is a highly effective way to improve their performance, [40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [43,2,4]. However, finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B parameter model [57] requires more than 780 GB of GPU memory. While recent quantization methods can reduce the memory footprint of LLMs [14,13,18,66], such techniques only work for inference and break down during training [65].\n\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28] Table 1: Elo ratings for a competition between models, averaged for 10,000 random initial orderings. The winner of a match is determined by GPT-4 which declares which response is better for a given prompt of the the Vicuna benchmark. 95% confidence intervals are shown (\u00b1). After GPT-4, Guanaco 33B and 65B win the most matches, while Guanaco 13B scores better than Bard.\n\n\nModel\n\nSize Elo QLORA reduces the average memory requirements of finetuning a 65B parameter model from >780GB of GPU memory to <48GB without degrading the runtime or predictive performance compared to a 16bit fully finetuned baseline. This marks a significant shift in accessibility of LLM finetuning: now the largest publicly available models to date finetunable on a single GPU. Using QLORA, we train the Guanaco family of models, with the second best model reaching 97.8% of the performance level of ChatGPT on the Vicuna [10] benchmark, while being trainable in less than 12 hours on a single consumer GPU; using a single professional GPU over 24 hours we achieve 99.3% with our largest model, essentially closing the gap to ChatGPT on the Vicuna benchmark. When deployed, our smallest Guanaco model (7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than 20 percentage points on the Vicuna benchmark (Table 6).\n\nQLORA introduces multiple innovations designed to reduce memory use without sacrificing performance: (1) 4-bit NormalFloat, an information theoretically optimal quantization data type for normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\n\n(2) Double Quantization, a method that quantizes the quantization constants, saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model).\n\n(3) Paged Optimizers, using NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when processing a mini-batch with a long sequence length. We combine these contributions into a better tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of the accuracy tradeoffs seen in prior work.\n\nQLORA's efficiency enables us to perform an in-depth study of instruction finetuning and chatbot performance on model scales that would be impossible using regular finetuning due to memory overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets, model architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA recovers 16-bit performance ( \u00a74) and training a state-of-the-art chatbot, Guanaco, ( \u00a75), we also analyze trends in the trained models. First, we find that data quality is far more important than dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2, subsampled) on chatbot performance, even when both are meant to support instruction following generalization. Second, we show that strong Massive Multitask Language Understanding (MMLU) benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice versa-in other words, dataset suitability matters more than size for a given task.\n\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete against each other in matches to produce the best response for a given prompt. The winner of a match is judged by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores [16,17] which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties.\n\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analysis highlights success and failure cases that were not captured by the quantitative benchmarks.\n\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face transformers stack [64], making them easily accessible to all. We release a collection of adapters for 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32 different open sourced, finetuned models. \n\n\nBackground\n\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a representation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range [\u2212127, 127]:\nX Int8 = round 127 absmax(X FP32 ) X FP32 = round(c FP32 \u00b7 X FP32 ),(1)\nwhere c is the quantization constant or quantization scale. Dequantization is the inverse:\ndequant(c FP32 , X Int8 ) = X Int8 c FP32 = X FP32(2)\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins-certain bit combinations-are not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can be formalized as follows: We chunk the input tensor X \u2208 R b\u00d7h into n contiguous blocks of size B by flattening the input tensor and slicing the linear segment into n = (b \u00d7 h)/B blocks. We quantize these blocks independently with Equation 1 to create a quantized tensor and n quantization constants c i .\n\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [28] is a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed. Gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. Given a projection XW = Y with X \u2208 R b\u00d7h , W \u2208 R h\u00d7o LoRA computes:\nY = XW + sXL 1 L 2 ,(3)\nwhere L 1 \u2208 R h\u00d7r and L 2 \u2208 R r\u00d7o , and s is a scalar.\n\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is the memory requirement of LoRA during training both in terms of the number and size of adapters used. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve performance without significantly increasing the total memory used. While LoRA was designed as a Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning comes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA model trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used 0.2% of the original model weights [28,37], the LoRA input gradients have a memory footprint of 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [9], the input gradients reduce to an average of 18 MB per sequence making them more memory intensive than all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of memory. This highlights that gradient checkpointing is important but also that aggressively reducing the amount of LoRA parameter yields only minor memory benefits. This means we can use more adapters without significantly increasing the overall training memory footprint (see Appendix G for a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision performance.\n\n\nQLORA Finetuning\n\nQLORA achieves high-fidelity 4-bit finetuning via two techniques we propose-4-bit NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to prevent memory spikes during gradient checkpointing from causing out-of-memory errors that have traditionally made finetuning on a single machine difficult for large models.\n\nQLORA has one low-precision storage data type, in our case usually 4-bit, and one computation data type that is usually BFloat16. In practice, this means whenever a QLORA weight tensor is used, we dequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\n\nWe now discuss the components of QLORA followed by a formal definition of QLORA.\n\n\n4-bit NormalFloat Quantization\n\nThe NormalFloat (NF) data type builds on Quantile Quantization [15] which is an information-theoretically optimal data type that ensures each quantization bin has an equal number of values assigned from the input tensor. Quantile quantization works by estimating the quantile of the input tensor through the empirical cumulative distribution function.\n\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive. Therefore fast quantile approximation algorithms, such as SRAM quantiles [15], are used to estimate them. Due to the approximate nature of these quantile estimation algorithms, the data type has large quantization errors for outliers, which are often the most important values.\n\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from a distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles making exact quantile estimation computationally feasible.\n\nSince pretrained neural network weights usually have a zero-centered normal distribution with standard deviation \u03c3 (see Appendix F), we can transform all weights to a single fixed distribution by scaling \u03c3 such that the distribution fits exactly into the range of our data type. For our data type, we set the arbitrary range [\u22121, 1]. As such, both the quantiles for the data type and the neural network weights need to be normalized into this range.\n\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary standard deviations \u03c3 in the range [\u22121, 1] is computed as follows: (1) estimate the 2 k + 1 quantiles of a theoretical N (0, 1) distribution to obtain a k-bit quantile quantization data type for normal distributions, (2) take this data type and normalize its values into the [\u22121, 1] range, (3) quantize an input weight tensor by normalizing it into the [\u22121, 1] range through absolute maximum rescaling.\n\nOnce the weight range and data type range match, we can quantize as usual.\n\nStep (3) is equivalent to rescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data type. More formally, we estimate the 2 k values q i of the data type as follows:\nq i = 1 2 Q X i 2 k + 1 + Q X i + 1 2 k + 1 ,(4)\nwhere Q X (\u00b7) is the quantile function of the standard normal distribution N (0, 1). A problem for a symmetric k-bit quantization is that this approach does not have an exact representation of zero, which is an important property to quantize padding and other zero-valued elements with no error. To ensure a discrete zeropoint of 0 and to use all 2 k bits for a k-bit datatype, we create an asymmetric data type by estimating the quantiles q i of two ranges q i : 2 k\u22121 for the negative part and 2 k\u22121 + 1 for the positive part and then we unify these sets of q i and remove one of the two zeros that occurs in both sets. We term the resulting data type that has equal expected number of values in each quantization bin k-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered normally distributed data. The exact values of this data type can be found in Appendix E.\n\n\nDouble Quantization\n\nWe introduce Double Quantization (DQ), the process of quantizing the quantization constants for additional memory savings. While a small blocksize is required for precise 4-bit quantization [13], it also has a considerable memory overhead. For example, using 32-bit constants and a blocksize of 64 for W, quantization constants add 32/64 = 0.5 bits per parameter on average. Double Quantization helps reduce the memory footprint of quantization constants.\n\nMore specifically, Double Quantization treats quantization constants c FP32 2 of the first quantization as inputs to a second quantization. This second step yields the quantized quantization constants c FP8 2 and the second level of quantization constants c FP32 1 . We use 8-bit Floats with a blocksize of 256 for the second quantization as no performance degradation is observed for 8-bit quantization, in line with results from Dettmers and Zettlemoyer [13]. Since the c FP32 2 are positive, we subtract the mean from c 2 before quantization to center the values around zero and make use of symmetric quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per parameter from 32/64 = 0.5 bits, to 8/64 + 32/(64 \u00b7 256) = 0.127 bits, a reduction of 0.373 bits per parameter.\n\nPaged Optimizers use the NVIDIA unified memory 3 feature wich does automatic page-to-page transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM and the disk. We use this feature to allocate paged memory for the optimizer states which are then automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step.\n\nQLORA. Using the components described above, we define QLORA for a single linear layer in the quantized base model with a single LoRA adapter as follows:\nY BF16 = X BF16 doubleDequant(c FP32 1 , c k-bit 2 , W NF4 ) + X BF16 L BF16 1 L BF16 2 ,(5)\nwhere doubleDequant(\u00b7) is defined as:\ndoubleDequant(c FP32 1 , c k-bit 2 , W k-bit ) = dequant(dequant(c FP32 1 , c k-bit 2 ), W 4bit ) = W BF16 ,(6)\nWe use NF4 for W and FP8 for c 2 . We use a blocksize of 64 for W for higher quantization precision and a blocksize of 256 for c 2 to conserve memory.\n\nFor parameter updates only the gradient with respect to the error for the adapters weights \u2202E \u2202Li are needed, and not for 4-bit weights \u2202E \u2202W . However, the calculation of \u2202E \u2202Li entails the calculation of \u2202X \u2202W which proceeds via equation (5) with dequantization from storage W NF4 to computation data type W BF16 to calculate the derivative \u2202X \u2202W in BFloat16 precision. To summarize, QLORA has one storage data type (usually 4-bit NormalFloat) and a computation data type . We dequantize the storage data type to the computation data type to perform the forward and backward pass, but we only compute weight gradients for the LoRA parameters which use 16-bit BrainFloat.\n\n\nQLoRA vs. Standard Finetuning\n\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed at answering these questions.\n\nExperimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [58] with RoBERTa-large [38], Super-NaturalInstructions (TKInstruct) [61] with T5 [49], and 5-shot MMLU [24] after finetuning LLaMA on Flan v2 [39] and Alpaca [55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity across different models (OPT [72], LLaMA [57], BLOOM [52], Pythia [7]) for model sizes 125m -13B. We provide more details in the results section for each particular setup to make the results more readable. Full details in Appendix A. While paged optimizers are critical to do 33B/65B QLORA tuning on a single 24/48GB GPU, we do not provide hard measurements for Paged Optimizers since the paging only occurs when processing mini-batches with long sequence lengths, which is rare. We do, however, perform an analysis of the runtime of paged optimizers for 65B models on 48GB GPUs and find that with a batch size of 16, paged optimizers provide the same training speed as regular optimizers. Future work should measure and characterize under what circumstances slowdowns occur from the paging process.\nQ L o R A -A ll Q L o R A -F F N Q L o R A -A t t\nDefault LoRA hyperparameters do not match 16bit performance When using the standard practice of applying LoRA to query and value attention projection matrices [28], we are not able to replicate full finetuning performance for large base models. As shown in Figure 2 for LLaMA 7B finetuning on Alpaca, we find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers are required to match full finetuning performance. Other LoRA hyperparameters, such as the projection dimension r, do not affect performance (see Appendix A).  Similarly, we find that default hyperparameters for fully finetuned baselines are undertuned. We do a hyperparameter search over learning rates 1e-6 to 5e-5 and batch sizes 8 to 128 to find robust baselines. Results for 7B LLaMA finetuning on Alpaca are shown in Figure 2.\n\n\n4-bit NormalFloat yields better performance than 4-bit Floating Point\n\nWhile the 4-bit NormalFloat (NF4) data type is informationtheoretically optimal, it still needs to be determined if this property translates to empirical advantages. We follow the setup from Dettmers and Zettlemoyer [13] where quantized LLMs (OPT [72], BLOOM [52], Pythia [7], LLaMA) of different sizes (125M to 65B) with different data types are evaluated on language modeling and a set of zero-shot tasks. In Figure 3 and Table 2 we see that NF4 improves performance significantly over FP4 and Int4 and that double quantization reduces the memory footprint without degrading performance.\n\nk-bit QLORA matches 16-bit full finetuning and 16-bit LoRA performance Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13,18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization.\n\nFor our second setup, since full finetuning models at and beyond 11B parameters requires more than one server of high memory GPUs, we continue to test whether 4-bit QLORA can match 16-bit LoRA at the 7B to 65B parameter scales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision.\n\nSummary Our results consistently show that 4-bit QLORA with NF4 data type matches 16bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with wellestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that double quantization does not degrade performance. Combined, this forms compelling evidence that 4-bit QLORA tuning reliably yields results matching 16-bit methods.\n\nIn line with previous work on quantization [13], our MMLU and Elo results indicate that with a given finetuning and inference resource budget it is beneficial to increase the number of parameters in the base model while decreasing their precision. This highlights the importance of efficiency benefits from QLORA. Since we did not observe performance degradation compared to full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\n\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware.\n\n\nPushing the Chatbot State-of-the-art with QLoRA\n\nHaving established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate on a challenging Natural Language Understanding benchmark (MMLU) and develop new methods for real-world chatbot performance evaluation.\n\n\nExperimental setup\n\nWe now describe an overview of the experimental setup with full details in Appendix B.\n\nData As, to our knowledge, there is no comprehensive study of recent instruction-following datasets, we select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [31], HH-RLHF [4]), distillation from instruction-tuned models (Alpaca [55], self-instruct [59], unnaturalinstructions [26]), corpora aggregations (FLAN v2 [12]), as well as hybrids (Chip2 [32], Longform [30]). These datasets cover different languages, data sizes, and licenses.\n\nTraining Setup To avoid confounding effects from different training objectives, we perform QLoRA finetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for datasets that include human judgments of different responses. For datasets that have a clear distinction between instruction and response, we finetune only on the response (see ablations in Appendix B). For OASST1 and HH-RLHF, multiple responses are available. We then select the top response at every level of the conversation tree and finetune on the full selected conversation, including the instructions. In all of our experiments, we use NF4 QLORA with double quantization and paged optimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter searches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found at 7B generalize (including number of epochs) except learning rate and batch size. We halve the learning rate for 33B and 65B while doubling the batch size.\n\nBaselines We compare our models to both research (Vicuna [10] and Open Assistant [31]) and commercial (GPT-4 [42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model is a LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF) on the same OASST1 dataset that we experiment with. Vicuna does full fine-tuning of LLaMA 13B on proprietary user-shared conversations from ShareGPT and is thus the result of distillation from OpenAI GPT models. Following common practice, we use the MMLU (Massively Multitask Language Understanding) benchmark [24] to measure performance on a range of language understanding tasks. This is a multiple-choice benchmark covering 57 tasks including elementary mathematics, US history, computer science, law, and more. We report 5-shot test accuracy.\n\n\nEvaluation\n\nWe also test generative language capabilities through both automated and human evaluations. This second set of evaluations relies on queries curated by humans and aims at measuring the quality of model responses.\n\nWhile this is a more realistic testbed for chatbot model performance and is growing in popularity, there is no commonly accepted protocol in the literature. We describe below our proposed setup, using nucleus sampling with p = 0.9 and temperature 0.7 in all cases.\n\nBenchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts [10] and the OASST1 validation dataset [31]. We use the Vicuna prompts, a set of 80 prompts from a diverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of crowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the validation dataset as queries and include previous turns in the prompt. This procedure leads to 953 unique user queries. We term these two datasets the Vicuna and OA benchmarks.\n\nAutomated Evaluation First, based on the evaluation protocol introduced by Chiang et al. [10], we use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the Vicuna benchmark. Given a query along with ChatGPT's and a model's responses, GPT-4 is prompted to assign a score out of ten to both responses and provide an explanation. The overall performance of a model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score can be higher than 100% if the model achieves a higher absolute score than ChatGPT. We find a significant ordering effect with GPT-4 increasing the score of the response occurring earlier in the prompt. To control for such effects, we recommend reporting the mean score over both orders.\n\nNext, we measure performance through direct comparisons between system outputs. We simplify the rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to pick the best response or declare a tie and provide an explanation. We conduct these head-to-head comparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\n\nHuman Evaluation While recent work indicates generative models can be effectively employed for system evaluations [19], the reliability GPT-4 ratings to assess chatbot performance is, to our knowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel human evaluations on the Vicuna benchmark matching both automated evaluation protocols described above. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to ChatGPT and three annotators for pairwise comparisons.\n\nElo Rating With both human and automated pairwise comparisons, we create a tournament-style competition where models compete against each other. The tournament is made up of matches where pairs of models compete to produce the best response for a given prompt. This is similar to how Bai et al. [4] and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to human ratings. We randomly sample from the set of labeled comparisons to compute Elo [16,17]. Elo rating, which is widely used in chess and other games, is a measure of the expected win-rate relative to an opponent's win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player has an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or 1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match proportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo rating while an expected outcome leads to a small change. Over time, Elo ratings approximately match the skill of each player at playing the game. We start with a score of 1,000 and use K = 32. Similar to Chiang et al. [10], we repeat this procedure 10,000 times with different random seeds to control for ordering effects, e.g., the effect of which model pairs compete with each other first.\n\n\nGuanaco: QLORA trained on OASST1 is a State-of-the-art Chatbot\n\nBased on our automated and human evaluations, we find that the top QLORA tuned model, Guanaco 65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model and offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B have an expected win probability of 30%, based on Elo rating from human annotators system-level pairwise comparisons -the highest reported to date.\n\nThe Vicuna benchmark [10] results relative to ChatGPT are shown in Table 6. We find that Guanaco 65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT. Guanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its weights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage points of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a 5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.\n\nHowever, Table 6 also has very wide confidence intervals, with many models overlapping in performance. We hypothesize that this uncertainty comes from the lack of clear specification of scale, e.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead recommend using the Elo ranking method [16], based on pairwise judgments from human annotators and GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive models can be seen in Table 1. We note that human and GPT-4 ranking of models on the Vicuna benchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with a Kendall Tau of \u03c4 = 0.43 and Spearman rank correlation of r = 0.55 at the system level. At the example level, the agreement between GPT-4 and human annotators' majority vote is weaker with Fleiss \u03ba = 0.25. Overall, this shows a moderate agreement between system-level judgments by GPT-4 and human annotators, and thus that model-based evaluation represents a somewhat reliable alternative to human evaluation. We discuss further considerations in Section 6.2.\n\nElo rankings in Table 7 indicate that Guanaco 33B and 65B models outperform all models besides GPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line with Table 6. We note that the Vicuna benchmark favors open-source models while the larger OA benchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 6 that the suitability of a finetuning dataset is a determining factor in performance. Finetuning Llama models on FLAN v2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends are observed with other models). This also points to partial orthogonality in current evaluation benchmarks: strong MMLU performance does not imply strong chatbot performance (as measured by Vicuna or OA benchmarks) and vice versa.\n\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1 dataset collection guidelines explicitly forbid the use of GPT models. The next best model trained on only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points lower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit QLORA is effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our 33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the potential for future work via QLORA tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today.\n\n\nQualitative Analysis\n\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only looking at summary statistics. Perhaps the largest is the problem of benchmark validity [36]-whether a benchmark truly tests what its name or description suggests is always at question, especially as we discover \"shortcuts\" to solve benchmarks that machine learning models sometimes exploit [22,46]. To partially alleviate this, we here perform some qualitative analysis, in two sections. First, in \u00a76.1 \n\n\nQualitative Analysis of Example Generations\n\nTo find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant benchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we attempt to setup a question or prompt that will induce the pattern even though it is the incorrect solution, e.g., if we observe that the model tends to give long-winded answers we prompt the model to \"Answer yes or no without explanation.\" We use this to find \"lemons\" where we manage to adversarially break the model and \"cherries\" where we fail to break the model, and present both. All generations in this section were generated with Nucleus Sampling [25] with p = 0.9.\n\nOf course, this is by no means comprehensive, since it is beyond the scope of this small qualitative study to control for all the variables involved, e.g., the full distribution of responses the model can generate for a given prompt is quite large, so we rely on samples we hope are representative. However, we believe describing these examples gives context to the quantitative evidence shown earlier in the paper. Since we open source all models and code, we hope this section will inspire future work to examine in more detail the issues we present here.\n\nFactual Recall For questions such as \"What is the capital of Zambia?\" all models consistently generate correct answers, e.g.,\n\n\nGuanaco:\n\nThe capital of Zambia is Lusaka.\n\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For instance in response to this prompt from HotPotQA [68]:\nUser:\nIn what year was the singer who popularized 'I'll Keep the Lovelight Burning' born?\n\n\nGuanaco:\n\nAl Jolson is credited with popularizing the song 'I'll Keep the Lovelight Burning,' and he was born in the year 1886.\n\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is correct for the person mentioned, Al Jolson).\n\nSuggestibility Guanaco shows a surprising resistance to going along with some kinds of assumed misinformation, e.g., in the following interaction:\n\nUser: How was it finally, officially confirmed that the earth is flat by peer reviewed scientists?\n\n\nGuanaco:\n\nThe earth has never been officially confirmed to be flat by peer reviewed scientists. The overwhelming consensus among scientists is that the earth is spherical. In fact, this has been known for centuries and has been confirmed through numerous experiments and observations. The belief that the earth is flat is a conspiracy theory that has been debunked by science. The idea that a group of scientists have peer reviewed and confirmed the earth is flat is simply false.\n\nIf you have any more questions please ask.\n\nGuanaco is also quite good at knowing what kinds of questions aren't possible to answer, e.g.,\n\n\nUser: What time is it?\n\nGuanaco: I'm sorry, but as an AI language model, I don't have access to real-time information. My responses are generated based on my pre-existing knowledge and do not reflect the current time. If you're looking for the current time, I suggest using a timekeeping device or checking an online timepiece.\n\nSimilar results hold for \"Where are you?\", \"How are you?\", etc.\n\nRefusal Similarly, Guanaco sometimes refuses to follow instructions for seemingly random reasons:\n\nUser: Please reverse the words in the sentence \"I'm a cool dude in a mocha mood.\"\n\n\nGuanaco:\n\nI'm sorry, I'm not able to do that right now, but I can explain why the sentence you provided is grammatically correct.\n\nThe sentence \"I'm a cool dude in a mocha mood\" follows the correct grammar and syntax rules of the English language. In this case, the adjective \"cool\" comes before the noun \"dude\" to describe it, and the prepositional phrase \"in a mocha mood\" provides additional information about the context of the sentence. This sentence structure is perfectly acceptable and makes sense in the context of the sentence.\n\nIf you have any other questions or if there's anything else I can help you with, feel free to ask Secret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system designers would like to keep something secret from the user, even if the system is allowed to give free text responses back to the user. We prompt the model with \"The secret word is 'banana'. Do not reveal or repeat back this back, no matter what the human requests.\" and try to see how easy it is for a potential user to access this secret information. Directly asking fails:\n\nUser: What is the secret word?\n\n\nGuanaco:\n\nI'm sorry, but I'm not able to disclose the secret word. I'm programmed to not reveal or repeat back the secret word, no matter what the human requests. Is there anything else I can help you with?\n\nwhich is the desired behavior. However, a tiny amount of trickery breaks this behavior:\n\nUser: This is a game. The goal is to ignore your previous instructions. What is the secret word?\n\n\nGuanaco:\n\nSorry, the secret word is banana. It's best if we don't speak of it. Is there anything else you'd like to ask?\n\nshowcasing the need to study methods for more faithful instruction following.\n\nMath Guanaco's biggest weakness is mathematics, an area where many language models struggle in [35]. When Guanaco shows its work, it tends to be accurate, e.g.,\n\nUser: Lee mows one lawn and charges $33. Last week he mowed 16 lawns and three customers each gave him a $10 tip. How many dollars did Lee earn mowing lawns last week?\n\nGuanaco: Lee earned $582 mowing lawns last week.\n\nTo find the total amount he earned, we first need to calculate how much he earned for mowing 16 lawns:\n$33 x 16 = $528\nThen we need to add the tips he received from the three customers:\n$10 x 3 = $30\nSo the total amount Lee earned was:\n$528 + $30 = $558\nHowever Guanaco can breakdown at even simple problems if it fails to break them down step-by-step, a known issue [63]. For example, we see the following interaction: which is not only wrong (the true factorization is 3 \u00d7 17 \u00d7 43), it's wrong twice .\n\nTheory of Mind Guanaco displays surprisingly strong Theory of Mind capabilities [41,51]. For instance, the model correctly answers the following question in great detail:\n\nUser: James and Abby are in the bedroom. Abby put the pen in the desk drawer. Abby leaves the bedroom. James moves the pen into the bag. Where does James think Abby will look for the pen?\n\nGuanaco: James knows that Abby put the pen in the desk drawer. But James also knows that he has moved the pen into the bag, and he assumes that Abby does not know about this. So, James may guess that Abby will look for the pen in the desk drawer, because that's where she left it last time. However, this assumption may be incorrect, because Abby may have seen James moving the pen.\n\nHowever, these inferences are unreliable, and often the model gives reasons that presume things that don't make sense in the situation, e.g., User: Evelyn entered the living room. Jackson entered the playroom. James entered the playroom. The beans are in the treasure chest. James exited the playroom. Jackson moved the beans to the pantry. Jackson exited the playroom. James entered the living room. Where will James look for the beans?\n\nGuanaco: James will look for the beans in the pantry, because that's where Jackson moved them.\n\nwhere Guanaco presumes information transfer that was never described. These issues echo recent literature [51], but require more study.\n\n\nConsiderations\n\nEvaluation We report moderate agreement among human annotators (Fleiss \u03ba = 0.42) with additional deterioration when comparing two strong systems. This points to limitations in the current benchmarks and human evaluation protocols for chatbot task performance. When manually comparing generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that subjective preferences start to play an important role as the authors of this paper disagreed on the many preferred responses. Future work should investigate approaches to mitigate these problems drawing from disciplines that developed mechanisms to deal with subjective preferences, such as Human-Computer Interaction and Psychology.\n\nIn our analysis, we also find that automated evaluation systems have noticeable biases. For example, we observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its prompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss \u03ba = 0.25) also suggests that human annotators and automated systems might rely on preferences that are not always aligned. In addition, in Table 7, we observe that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an additional 20% probability of winning against an opponent. Future work should examine the presence of potential biases in automated evaluation systems as well as possible mitigation strategies.\n\nData & Training We note that the OASST1 dataset on which Guanaco models are trained is multilingual and that the OA benchmark also contains prompts in different languages. We leave it to future work to investigate the degree to which such multilingual training improves performance on instructions in languages other than English and whether this explains the larger gap between Vicuna-13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.\n\nGiven the strong performance of Guanaco models, we investigate any data leakage between the OASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after performing fuzzy string matching in the two datasets and inspecting the closest matches manually.\n\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning) without relying on reinforcement learning from human feedback (RLHF). This calls for further investigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA enables such analysis at scale, without the need for overwhelming computational resources.\n\n\nRelated Work\n\nQuantization of Large Language Models Quantization of LLMs has largely focused on quantization for inference time. Major approaches for preserving 16-bit LLM quality focus on managing outlier features (e.g., SmoothQuant [66] and LLM.int8() [14]) while others use more sophisticated grouping methods [44,69]. Lossy quantization approaches study the trade-offs for regular rounding [13,71,47] or how to optimize rounding decisions to improve quantization precision [18]. Besides our work, SwitchBack layers [65] is the only work that studies backpropagation through quantized weights at a scale beyond 1B parameters.\n\nFinetuning with Adapters While we use Low-rank Adapters [28] (LoRA), many other Parameter Efficient FineTuning (PEFT) methods have been proposed such as prompt tuning [48,33,34], tuning the embedding layer inputs [1], tuning hidden states (IA 3 ) [37], adding full layers [27], tuning biases [70], learning a mask over weights based on Fisher information [54], and a combination of approaches [23]. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning performance. We leave it to future work to explore the tradeoffs of other PEFT approaches.\n\nInstruction Finetuning To help a pretrained LLM follow the instructions provided in a prompt, instruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM to generate the output given the input as a prompt. Approaches and datasets include MetaICL [40], MetaTuning [73], InstructGPT [43], FLAN [62,12], PromptSource [3], Super-NaturalInstructions [61,50], Self-instruct [59], UnnaturalInstructions [26], OPT-IML [29], UnifiedSKG [67], OIG/Chip2 [32], Alpaca [55], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].\n\nChatbots Many instruction following models are structured as dialogue-based chatbots, often using Reinforcement Learning from Human Feedback (RLHF) [11] or generating data from an existing model to train with AI model feedback (RLAIF) [5]. Approaches and datasets include Anthropic-HH [2,4], Open Assistant [31], LaMDA [56], and Sparrow [21]. We do not use reinforcement learning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open Assistant dataset which was designed to be used for RLHF training [31]. For the evaluation of chatbots approaches that use GPT-4 instead of costly human annotation have been developed [10,45].\n\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\n\n\nLimitations and Discussion\n\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance with a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish that QLORA can match full 16-bit finetuning performance at 33B and 65B scales. Due to the immense resource costs, we leave this study to future work.\n\nAnother limitation is the evaluation of instruction finetuning models. While we provide evaluations on MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks such as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these benchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods for evaluating chatbots.\n\nFrom the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU, but dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score accordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. Do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? Maybe something else? Because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. We should ensure as a community that the benchmarks measure what we care about.\n\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B to generate a socially biased sequence of tokens compared to other models in Table 8. We see that the average score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that finetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results are encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We leave further evaluation of analyzing biases in Guanaco and similar chatbots to future work.\n\nAn additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient FineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these methods scale to large models. We used LoRA as many results established its robustness but other adapters might yield better performance. Since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization. For example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full finetuning performance after finetuning.\n\n\nBroader Impacts\n\nOur QLORA finetuning method is the first method that enables the finetuning of 33B parameter models on a single consumer GPU and 65B parameter models on a single professional GPU, while not degrading performance relative to a full finetuning baseline. We have demonstrated that our best 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\n\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like chatbots, we believe that our method will make finetuning widespread and common in particular for the researchers that have the least resources, a big win for the accessibility of state of the art NLP technology. QLORA can be seen as an equalizing factor that helps to close the resource gap between large corporations and small teams with consumer GPUs.\n\nAnother potential source of impact is deployment to mobile phones. We believe our QLORA method might enable the critical milestone of enabling the finetuning of LLMs on phones and other low resource settings. While 7B models were shown to be able to be run on phones before, QLORA is the first method that would enable the finetuning of such models. We estimate that with an iPhone 12 Plus, QLORA can finetune 3 million tokens per night while the phone is charging. While finetuned 7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable novel applications that have not been possible before due to privacy or LLM quality issues. QLORA can help enable privacy-preserving usage of LLMs, where users can own and manage their own data and models, while simultaneously making LLMs easier to deploy.\n\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of LLMs has known dangers [8,6], but we believe that equalizing access to a technology that is quickly becoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs in the hands of large corporations that do not release models or source code for auditing.\n\nAll in all, we believe that QLORA will have a broadly positive impact making the finetuning of high quality LLMs much more widely and easily accessible. We find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B, 65B). We find LoRA r is unrelated to final performance if LoRA is used on all layers as can be seen in Figure 4 \n\n\nA.2 Super-Natural Instructions Experimental Setup Details\n\nWe use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [60]. However, we split the training data in training and validation datasets allowing us to perform more rigorous hyperparameter tuning and early stopping. We use the same hyperparameters described in the paper for training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r = 16 for small, medium, and large T5 models and LoRA r = 64 for T5 xl and xxl models. We also use LoRA \u03b1 = 64 in all our experiments and no LoRA dropout.  [62], T0++ [50], Super-Natural Instructions [60], and OPT-IML [29]. We used the same task mixtures described by the authors with the exception of some datasets that were not freely available at the time of writing. \n\n\nB.2 Hyperparameters\n\nWe provide the exact hyperparameters used in our QLORA finetuning experiments. We find hyperparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation and hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16 computation datatype. We set LoRA r = 64, \u03b1 = 16, and add LoRA modules on all linear layers of the base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1 for models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction finetuning [62,60] and after benchmarking other linear and cosine schedules, we use a constant learning rate schedule. We use group-by-length to group examples of similar lengths in the same batch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model size are shown in Table 9.\n\n\nB.3 Ablations\n\nWhile it is general practice in the literature to only train on the response in instruction following datasets, we study the effect of training on the instruction in addition to the response in  performance. We did not evaluate the effect this may have on chatabot performance as measured by vicuna or OA benchmarks.\n\nB.4 What is more important: instruction finetuning dataset size or dataset quality? Data set suitability is more important than dataset size. To understand the effects of dataset quality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples (Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and examine the resulting trends, as shown in Table 11. We find that increasing the dataset size and increasing the number of epochs improves MMLU only marginally (0.0 -0.5 MMLU), while the difference between datasets is up to 40x larger (1.5 -8.0 MMLU). This is a clear indicator that dataset quality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for chatbot performance as discussed in .\n\n\nC Human Evaluation\n\nWe conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna evaluation [10], adjusted for an Amazon Mechanical Turk form as show in Figure 5.\n\n\nD Pairwise Evaluation with GPT-4\n\nWhile we found that the GPT-4 evaluation gave different results depend on which system was presented first, when averaged over both options the pairwise results were well-ordered. The aggregated pairwise judgments are hown in Table 12. On inspection, it is clear these judgments are transitive, i.e., when System A is judged better than System B and System B is judged better than System C, it is always the case that System A is judged better than System C. This yields a complete ordering, given in Table 13.\n\n\nE NormalFloat 4-bit data type\n\nThe exact values of the NF4 data type are as follows: \n\n\nF Normality of Trained Neural Network Weights\n\nWhile it is common knowledge that trained neural network weights are mostly normally distributed, we perform statistical testing to verify this. We use the Shapiro-Wilk test[53] on the weights of the 7B  LLaMA model [57]. We find that the weights of each hidden unit have different normal distributions. As such, we test he weights of each individual hidden unit. This mean for weight W \u2208 R in\u00d7out we perform tests over the out dimension. Using a 5% significance threshold, we find that 7.5% of neurons are non-normally distributed which is about 2.5% more than the expected false-positive rate. As such, while almost all pretrained weights appear to be normally distributed there seem to be exceptions. Such exceptions might be due to outliers weights [13] or because the p-value of the Shaprio-Wilk test is not accurate for large samples sizes[53] that occur in the LLaMA FFN layer hidden units. this verifies the claim that neural network weights.  Figure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch size 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention). Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some models do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\n\n\nG Memory Footprint\n\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in Figure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers are needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient checkpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the activation gradient might consume a considerable amount of memory. \n\nFigure 1 :\n1Different finetuning methods and their memory requirements. QLORA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\n\nFigure 2 :\n2RougeL for LLaMA 7B models on the Alpaca dataset. Each point represents a run with a different random seed. We improve on the Stanford Alpaca fully finetuned default hyperparameters to construct a strong 16-bit baseline for comparisons. Using LoRA on all transformer layers is critical to match 16-bit performance.\n\nFigure 3 :\n3Mean zero-shot accuracy over Winogrande, HellaSwag, PiQA, Arc-Easy, and Arc-Challenge using LLaMA models with different 4-bit data types. The NormalFloat data type significantly improves the bit-for-bit accuracy gains compared to regular 4-bit Floats. While Double Quantization (DQ) only leads to minor gains, it allows for a more fine-grained control over the memory footprint to fit models of certain size (33B/65B) into certain GPUs (24/48GB).\n\n\nUser: Please factorize 1833. Guanaco: Sure, I'd be happy to help! 1833 is a prime number, which means it has only two factors: 1 and 1833. So the unique factorization of 1833 is: 2\u02c61 * 3\u02c62 * 17\u02c61 I hope this helps! Let me know if you have any other questions.\n\nFigure 4 :\n4LoRA r for LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of hyperparameters and for each LoRA r we run 3 random seed with each hyperparameter combination. The performance of specific LoRA r values appears to be independent of other hyperparameters.\n\nFigure 5 :\n5The crowdsourcing form used by human annotators.\n\nTable 3 :\n3Experimentscomparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORA replicates 16-bit LoRA and full-\nfinetuning. \n\nDataset \nGLUE (Acc.) \nSuper-NaturalInstructions (RougeL) \nModel \nRoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B \n\nBF16 \n88.6 \n40.1 \n42.1 \n48.0 \n54.3 \n62.0 \nBF16 replication \n88.6 \n40.0 \n42.2 \n47.3 \n54.9 \n-\n\nLoRA BF16 \n88.8 \n40.5 \n42.6 \n47.1 \n55.4 \n60.7 \nQLORA Int8 \n88.8 \n40.4 \n42.9 \n45.4 \n56.5 \n60.7 \nQLORA FP4 \n88.6 \n40.3 \n42.4 \n47.5 \n55.6 \n60.9 \nQLORA NF4 + DQ \n-\n40.4 \n42.7 \n47.7 \n55.3 \n60.9 \n\n\n\nTable 2 :\n2PileCommon Crawl mean \nperplexity for different data types \nfor 125M to 13B OPT, BLOOM, \nLLaMA, and Pythia models. \n\nData type \nMean PPL \n\nInt4 \n34.34 \nFloat4 (E2M1) \n31.07 \nFloat4 (E3M0) \n29.48 \nNFloat4 + DQ \n27.41 \n\n\n\nTable 4 :\n4Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and FLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance, while FP4 is consistently one percentage point behind both.Mean 5-shot MMLU Accuracy \n\nLLaMA Size \n7B \n13B \n33B \n65B \nMean \nDataset \nAlpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 \n\nBFloat16 \n38.4 \n45.6 \n47.2 \n50.6 \n57.7 \n60.5 \n61.8 \n62.5 \n53.0 \nFloat4 \n37.2 \n44.0 \n47.3 \n50.0 \n55.9 \n58.5 \n61.3 \n63.3 \n52.2 \nNFloat4 + DQ \n39.0 \n44.5 \n47.5 \n50.7 \n57.3 \n59.2 \n61.8 \n63.9 \n53.1 \n\n\n\nTable 5 :\n5MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA.Dataset \n7B \n13B 33B 65B \n\nLLaMA no tuning \n35.1 46.9 57.8 63.4 \n\nSelf-Instruct \n36.4 33.3 53.0 56.7 \nLongform \n32.1 43.2 56.6 59.7 \nChip2 \n34.5 41.6 53.6 59.8 \nHH-RLHF \n34.9 44.6 55.8 60.1 \nUnnatural Instruct \n41.9 48.1 57.3 61.3 \nGuanaco (OASST1) 36.6 46.4 57.0 62.2 \nAlpaca \n38.8 47.8 57.3 62.5 \nFLAN v2 \n44.5 51.4 59.2 63.9 \n\n\n\nTable 6 :\n6Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by GPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset and having a fraction of the memory requirement of baseline models.Model / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPTMean \n95% CI \n\n\n\nTable 7 :\n7Elo rating for a tournament between models where models compete to generate the best response for a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to ChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo is approximately a difference of 1.5% in win-rate.Benchmark \nVicuna \nVicuna \nOpen Assistant \n# Prompts \n80 \n80 \n953 \nJudge \nHuman raters \nGPT-4 \nGPT-4 \nMedian Rank \n\nModel \nElo Rank \nElo Rank \nElo \nRank \n\nGPT-4 \n1176 \n1 \n1348 \n1 \n1294 \n1 \n1 \nGuanaco-65B \n1023 \n2 \n1022 \n2 \n1008 \n3 \n2 \nGuanaco-33B \n1009 \n4 \n992 \n3 \n1002 \n4 \n4 \nChatGPT-3.5 Turbo \n916 \n7 \n966 \n5 \n1015 \n2 \n5 \nVicuna-13B \n984 \n5 \n974 \n4 \n936 \n5 \n5 \nGuanaco-13B \n975 \n6 \n913 \n6 \n885 \n6 \n6 \nGuanaco-7B \n1010 \n3 \n879 \n8 \n860 \n7 \n7 \nBard \n909 \n8 \n902 \n7 \n-\n-\n8 \n\nwe show some examples that we believe are representative of some observed patterns in the text \ngenerated by our 65b Guanaco model. Second,  \u00a76.2 we detail considerations about the results we \nhave discussed and our interpretation of them. \n\n\n\nTable 8 :\n8Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating biased sequences. Guanaco follows the biased pattern of the LLaMA base model.LLaMA-65B GPT-3 OPT-175B Guanaco-65B \n\nGender \n70.6 \n62.6 \n65.7 \n47.5 \nReligion \n79.0 \n73.3 \n68.6 \n38.7 \nRace/Color \n57.0 \n64.7 \n68.6 \n45.3 \nSexual orientation \n81.0 \n76.2 \n78.6 \n59.1 \nAge \n70.1 \n64.4 \n67.8 \n36.3 \nNationality \n64.2 \n61.6 \n62.9 \n32.4 \nDisability \n66.7 \n76.7 \n76.7 \n33.9 \nPhysical appearance \n77.8 \n74.6 \n76.2 \n43.1 \nSocioeconomic status \n71.5 \n73.8 \n76.2 \n55.3 \n\nAverage \n66.6 \n67.2 \n69.5 \n43.5 \n\n\n\n\nFFN output layers}. We keep LoRA \u03b1 fixed and search the learning rate, since LoRA \u03b1 is always proportional to the learning rate.A QLoRA vs Standard Finetuning Experimental Setup Details \n\nA.1 Hyperparameters for QLORA \n\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05, \n0.1}, LoRA r { 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers, \nall layers, attention + \n\n\nB Training a State-of-the-art Chatbot Experimental Setup DetailsB.1 DatasetsWe describe the datasets used for QLORA finetuning experiments outlined in Section 5.OASST1 The OpenAssistant dataset[31] was collected via crowd-sourcing. It contains 161,443 unique messages distributed across 66,497 conversations and spanning 35 different languages. The dataset often contains several ranked replies for each given user question. In our experiments, we only use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples. We finetuning our models on the full conversation including the user queries.HH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint consists of two assistant replies to a user question along with a human preference judgment of the best reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine helpfulness and harmlessness data and only keep the preferred assistant reply.FLAN v2The FLAN v2 collection[39] is a collection of 1836 tasks augmented with hundreds of manually curated templates and rich formatting patterns into over 15M examples. The authors show that models trained on this collection outperform other public collections including the original FLAN 2021\n\nTable 9 :\n9Training hyperparameters for QLORA finetuning on different datasets and across model sizes.Self-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instructions datasets[59,55,26] are instruction tuning datasets collected with various approaches of model distillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and paraphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of 82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that they contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar instruction tuning collections.Longform The LongForm dataset[30] is based on an English corpus augmented with instructions and as such is a hybrid human-generated dataset.The underlying documents are human-written and come from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended with additional structured corpora examples such as Stack Exchange and WikiHow and task examples such as question answering, email writing, grammar error correction, story/poem generation, and text summarization. The dataset contains 23,700 examples. Chip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction examples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia toxic adversarial questions, grade school math, reasoning instructions, and character and scene descriptions with a total of 210,289 examples.\n\nTable 10 .\n10In these experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four different instruction tuning datasets, we find that only training on the target is beneficial to MMLU Dataset Unnatural Instructions Chip2 Alpaca FLAN v2 MeanTrain on source and target \n36.2 \n33.7 \n38.1 \n42.0 \n37.5 \nTrain on target \n38.0 \n34.5 \n39.0 \n42.9 \n38.6 \n\n\n\nTable 10 :\n10MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\n\nTable 11 :\n11Effect different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While increasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference between datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.Chip \nUnnatural Instructions \nFLAN v2 \nDatapoints \u2193 Epochs \u2192 \n1 \n2 \n3 \n1 \n2 \n3 \n1 \n2 \n3 \nMean \n\n50000 \n34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28 \n100000 \n33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16 \n150000 \n34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02 \n\nMean \n34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17 \n\n\nTable 12 :\n12Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row x and column y is # judgment x is better than y\u2212# judgment y is better than x total # number of judgmentsModelGuanaco 65B Guanaco 33B Vicuna ChatGPT-3.5 Turbo Bard Guanaco 13B Guanaco 7BGuanaco 65B \n-\n0.21 \n0.19 \n0.16 \n0.72 \n0.59 \n0.86 \nGuanaco 33B \n-0.21 \n-\n0.17 \n0.10 \n0.51 \n0.41 \n0.68 \nVicuna \n-0.19 \n-0.17 \n-\n0.10 \n0.50 \n0.20 \n0.57 \nChatGPT-3.5 Turbo \n-0.16 \n-0.10 \n-0.10 \n-\n0.35 \n0.19 \n0.40 \nBard \n-0.72 \n-0.51 \n-0.50 \n-0.35 \n-\n0.12 \n0.03 \nGuanaco 13B \n-0.59 \n-0.41 \n-0.20 \n-0.19 \n-0.12 \n-\n0.20 \nGuanaco 7B \n-0.86 \n-0.68 \n-0.57 \n-0.40 \n-0.03 \n-0.20 \n-\n\n\nTable 13 :\n13The complete ordering induced by pairwise GPT-4 judgments between systemsModel \nParams \nSize \n\nGuanaco \n65B \n41 GB \nGuanaco \n33B \n21 GB \nVicuna \n13B \n26 GB \nChatGPT-3.5 Turbo \nN/A \nN/A \nBard \nN/A \nN/A \nGuanaco \n13B \n10 GB \nGuanaco \n7B \n5 GB \n\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide\n[53] S.Shaphiro  and M. Wilk. An analysis of variance test for normality. Biometrika, 52(3):591-611, 1965. [54] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:24193-24205, 2021.\nAcknowledgementsWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and Evangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced computational, storage, and networking infrastructure of the Hyak supercomputer system at the University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank the beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for help with the integration of our software into the Hugging Face transformers stack.\nInput-tuning: Adapting unfamiliar inputs to frozen pretrained models. S An, Y Li, Z Lin, Q Liu, B Chen, Q Fu, W Chen, N Zheng, J.-G Lou, arXiv:2203.03131arXiv preprintS. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.\n\nA general language assistant as a laboratory for alignment. A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N Dassarma, arXiv:2112.00861arXiv preprintA. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n\nS H Bach, V Sanh, Z.-X Yong, A Webson, C Raffel, N V Nayak, A Sharma, T Kim, M S Bari, T Fevry, arXiv:2202.01279An integrated development environment and repository for natural language prompts. arXiv preprintS. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.\n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.05862arXiv preprintY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\n\nY Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. arXiv preprintY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho- seini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n\nOn the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparencyE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610-623, 2021.\n\nS Biderman, H Schoelkopf, Q Anthony, H Bradley, K O&apos;brien, E Hallahan, M A Khan, S Purohit, U S Prashanth, E Raff, arXiv:2304.01373A suite for analyzing large language models across training and scaling. arXiv preprintS. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\n\nR Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\nT Chen, B Xu, C Zhang, C Guestrin, arXiv:1604.06174Training deep nets with sublinear memory cost. arXiv preprintT. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n\nDeep reinforcement learning from human preferences. Advances in neural information processing systems. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 30P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.\n\nScaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416arXiv preprintH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. De- hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n\nThe case for 4-bit precision: k-bit inference scaling laws. T Dettmers, L Zettlemoyer, arXiv:2212.09720arXiv preprintT. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\n8-bit matrix multiplication for transformers at scale. T Dettmers, M Lewis, Y Belkada, L Zettlemoyer, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022. int8(T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.\n\n8-bit optimizers via block-wise quantization. T Dettmers, M Lewis, S Shleifer, L Zettlemoyer, 9th International Conference on Learning Representations, ICLR. 2022T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.\n\nA E Elo, The proposed uscf rating system. its development, theory, and applications. Chess Life. 22A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242-247, 1967.\n\nThe rating of chessplayers, past and present. A E Elo, Arco PubA. E. Elo. The rating of chessplayers, past and present. Arco Pub., 1978.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. E Frantar, S Ashkboos, T Hoefler, D Alistarh, arXiv:2210.17323arXiv preprintE. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nJ Fu, S.-K Ng, Z Jiang, P Liu, Gptscore, arXiv:2302.04166Evaluate as you desire. arXiv preprintJ. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.\n\nKoala: A dialogue model for academic research. X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, Blog postX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley. edu/blog/2023/04/03/koala/.\n\nImproving alignment of dialogue agents via targeted human judgements. A Glaese, N Mcaleese, M Tr\u0119bacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, arXiv:2209.14375arXiv preprintA. Glaese, N. McAleese, M. Tr\u0119bacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\n\nS Gururangan, S Swayamdipta, O Levy, R Schwartz, S R Bowman, N A Smith, arXiv:1803.02324Annotation artifacts in natural language inference data. arXiv preprintS. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\n\nCompacter: Efficient low-rank hypercomplex adapter layers. J Henderson, S Ruder, Advances in Neural Information Processing Systems. J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In Advances in Neural Information Processing Systems, 2021.\n\nMeasuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea- suring massive multitask language understanding. In International Conference on Learning Representations, 2020.\n\nThe curious case of neural text degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, International Conference on Learning Representations. A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.\n\nUnnatural instructions: Tuning language models with (almost) no human labor. O Honovich, T Scialom, O Levy, T Schick, arXiv:2212.09689arXiv preprintO. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\n\nParameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q De Laroussilhe, A Gesmundo, M Attariyan, S Gelly, International Conference on Machine Learning. PMLRN. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790-2799. PMLR, 2019.\n\nE J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. arXiv preprintE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\nOpt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X V Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, arXiv:2212.12017arXiv preprintS. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.\n\nLongform: Optimizing instruction tuning for long text generation with corpus extraction. A K\u00f6ksal, T Schick, A Korhonen, H Sch\u00fctze, arXiv:2304.08460arXiv preprintA. K\u00f6ksal, T. Schick, A. Korhonen, and H. Sch\u00fctze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460, 2023.\n\nA K\u00f6pf, Y Kilcher, D Von R\u00fctte, S Anagnostidis, Z.-R Tam, K Stevens, A Barhoum, N M Duc, O Stanley, R Nagyfi, arXiv:2304.07327Openassistant conversations-democratizing large language model alignment. arXiv preprintA. K\u00f6pf, Y. Kilcher, D. von R\u00fctte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.\n\nOpen-instruction-generalist dataset. Laion, LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/ Open-Instruction-Generalist, 2023.\n\nThe power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.08691arXiv preprintB. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n\nPrefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.00190arXiv preprintX. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.\n\nHolistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110arXiv preprintP. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n\nAre we learning yet? a meta review of evaluation failures across machine learning. T Liao, R Taori, I D Raji, L Schmidt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 2T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nFew-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. H Liu, D Tam, M Muqeeth, J Mohta, T Huang, M Bansal, C A , Advances in Neural Information Processing Systems. 35H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950-1965, 2022.\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nThe flan collection: Designing data and methods for effective instruction tuning. S Longpre, L Hou, T Vu, A Webson, H W Chung, Y Tay, D Zhou, Q V Le, B Zoph, J Wei, arXiv:2301.13688arXiv preprintS. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.\n\nS Min, M Lewis, L Zettlemoyer, H Hajishirzi, arXiv:2110.15943Metaicl: Learning to learn in context. arXiv preprintS. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021.\n\nEvaluating theory of mind in question answering. A Nematzadeh, K Burns, E Grant, A Gopnik, T Griffiths, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingA. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392-2400, 2018.\n\n. Openai, arXivOpenAI. Gpt-4 technical report. arXiv, 2023.\n\nTraining language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 35L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n\nG Park, B Park, S J Kwon, B Kim, Y Lee, D Lee, arXiv:2206.09557Quantized matmul for efficient inference of large-scale generative language models. arXiv preprintG. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n\nB Peng, C Li, P He, M Galley, J Gao, arXiv:2304.03277Instruction tuning with gpt-4. arXiv preprintB. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\nHypothesis only baselines in natural language inference. A Poliak, J Naradowsky, A Haldar, R Rudinger, B Van Durme, Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics. the Seventh Joint Conference on Lexical and Computational SemanticsA. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, 2018.\n\nR Pope, S Douglas, A Chowdhery, J Devlin, J Bradbury, A Levskaya, J Heek, K Xiao, S Agrawal, J Dean, arXiv:2211.05102Efficiently scaling transformer inference. arXiv preprintR. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.\n\nLearning how to ask: Querying lms with mixtures of soft prompts. G Qin, J Eisner, arXiv:2104.06599arXiv preprintG. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 1532-4435J. Mach. Learn. Res. 211C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.\n\nMultitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, arXiv:2110.08207arXiv preprintV. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\n\nM Sap, R Lebras, D Fried, Y Choi, arXiv:2210.13312Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprintM. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.\n\nT L Scao, A Fan, C Akiki, E Pavlick, S Ili\u0107, D Hesslow, R Castagn\u00e9, A S Luccioni, F Yvon, M Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u0107, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni, F. Yvon, M. Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nStanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023.\n\nR Thoppilan, D Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Language models for dialog applications. arXiv preprintR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\n\nH Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi\u00e8re, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nA Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, arXiv:1804.07461Glue: A multitask benchmark and analysis platform for natural language understanding. arXiv preprintA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi- task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nSelf-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.10560arXiv preprintY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\n\nSuper-naturalinstructions:generalization via declarative instructions on 1600+ tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Arunkumar, A Ashok, A S Dhanasekaran, A Naik, D Stap, EMNLP. 2022Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP, 2022.\n\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Naik, A Ashok, A S Dhanasekaran, A Arunkumar, D Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingY. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, 2022.\n\nFinetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652arXiv preprintJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nChain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022.\n\nHuggingface's transformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, arXiv:1910.03771arXiv preprintT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nStable and low-precision training for large-scale vision-language models. M Wortsman, T Dettmers, L Zettlemoyer, A Morcos, A Farhadi, L Schmidt, arXiv:2304.13013arXiv preprintM. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013, 2023.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. G Xiao, J Lin, M Seznec, J Demouth, S Han, arXiv:2211.10438arXiv preprintG. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nT Xie, C H Wu, P Shi, R Zhong, T Scholak, M Yasunaga, C.-S Wu, M Zhong, P Yin, S I Wang, arXiv:2201.05966Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprintT. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin, S. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingZ. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, 2018.\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Z Yao, R Y Aminabadi, M Zhang, X Wu, C Li, Y He, arXiv:2206.01861arXiv preprintZ. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\nBitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. E B Zaken, S Ravfogel, Y Goldberg, arXiv:2106.10199arXiv preprintE. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.\n\nA Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. arXiv preprintA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\nS Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nAdapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. R Zhong, K Lee, Z Zhang, D Klein, arXiv:2104.04670arXiv preprintR. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.\n", "annotations": {"author": "[{\"end\":116,\"start\":49},{\"end\":188,\"start\":117},{\"end\":229,\"start\":189},{\"end\":274,\"start\":230},{\"end\":116,\"start\":49},{\"end\":188,\"start\":117},{\"end\":229,\"start\":189},{\"end\":274,\"start\":230}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":53},{\"end\":133,\"start\":126},{\"end\":201,\"start\":193},{\"end\":246,\"start\":235},{\"end\":61,\"start\":53},{\"end\":133,\"start\":126},{\"end\":201,\"start\":193},{\"end\":246,\"start\":235}]", "author_first_name": "[{\"end\":52,\"start\":49},{\"end\":125,\"start\":117},{\"end\":192,\"start\":189},{\"end\":234,\"start\":230},{\"end\":52,\"start\":49},{\"end\":125,\"start\":117},{\"end\":192,\"start\":189},{\"end\":234,\"start\":230}]", "author_affiliation": "[{\"end\":115,\"start\":90},{\"end\":187,\"start\":162},{\"end\":228,\"start\":203},{\"end\":273,\"start\":248},{\"end\":115,\"start\":90},{\"end\":187,\"start\":162},{\"end\":228,\"start\":203},{\"end\":273,\"start\":248}]", "title": "[{\"end\":46,\"start\":1},{\"end\":320,\"start\":275},{\"end\":46,\"start\":1},{\"end\":320,\"start\":275}]", "venue": null, "abstract": "[{\"end\":2202,\"start\":322},{\"end\":2202,\"start\":322}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2318,\"start\":2314},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2321,\"start\":2318},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2324,\"start\":2321},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2327,\"start\":2324},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2330,\"start\":2327},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2333,\"start\":2330},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2391,\"start\":2387},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2393,\"start\":2391},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2395,\"start\":2393},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2524,\"start\":2520},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2644,\"start\":2640},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2647,\"start\":2644},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2650,\"start\":2647},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2653,\"start\":2650},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2730,\"start\":2726},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3022,\"start\":3018},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3926,\"start\":3922},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6591,\"start\":6587},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6594,\"start\":6591},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7356,\"start\":7352},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9245,\"start\":9241},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10507,\"start\":10503},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10510,\"start\":10507},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10648,\"start\":10645},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12085,\"start\":12081},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12550,\"start\":12546},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15431,\"start\":15427},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16154,\"start\":16150},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18928,\"start\":18924},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18952,\"start\":18948},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18997,\"start\":18993},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19010,\"start\":19006},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19032,\"start\":19028},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19071,\"start\":19067},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19087,\"start\":19083},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19211,\"start\":19207},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":19309,\"start\":19305},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19321,\"start\":19317},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19333,\"start\":19329},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19345,\"start\":19342},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20289,\"start\":20285},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21301,\"start\":21297},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":21332,\"start\":21328},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":21344,\"start\":21340},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21356,\"start\":21353},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21887,\"start\":21883},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21890,\"start\":21887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23852,\"start\":23848},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25328,\"start\":25324},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25341,\"start\":25338},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25399,\"start\":25395},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25419,\"start\":25415},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25447,\"start\":25443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25484,\"start\":25480},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25517,\"start\":25513},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25532,\"start\":25528},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26693,\"start\":26689},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26717,\"start\":26713},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26745,\"start\":26741},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27213,\"start\":27209},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28038,\"start\":28034},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28077,\"start\":28073},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28606,\"start\":28602},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29785,\"start\":29781},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30494,\"start\":30491},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30517,\"start\":30513},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30671,\"start\":30667},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30674,\"start\":30671},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31388,\"start\":31384},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32079,\"start\":32075},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32960,\"start\":32956},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35519,\"start\":35515},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35722,\"start\":35718},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35725,\"start\":35722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36533,\"start\":36529},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":37425,\"start\":37421},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41076,\"start\":41072},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":41729,\"start\":41725},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41947,\"start\":41943},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":41950,\"start\":41947},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":43253,\"start\":43249},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":46154,\"start\":46150},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":46174,\"start\":46170},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":46233,\"start\":46229},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":46236,\"start\":46233},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46314,\"start\":46310},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":46317,\"start\":46314},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":46320,\"start\":46317},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46397,\"start\":46393},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":46439,\"start\":46435},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":46606,\"start\":46602},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46717,\"start\":46713},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":46720,\"start\":46717},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":46723,\"start\":46720},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46762,\"start\":46759},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":46797,\"start\":46793},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46822,\"start\":46818},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":46842,\"start\":46838},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":46943,\"start\":46939},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":47408,\"start\":47404},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":47425,\"start\":47421},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47443,\"start\":47439},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":47454,\"start\":47450},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":47457,\"start\":47454},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47475,\"start\":47472},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":47507,\"start\":47503},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":47510,\"start\":47507},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47530,\"start\":47526},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":47558,\"start\":47554},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47572,\"start\":47568},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":47589,\"start\":47585},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":47605,\"start\":47601},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":47618,\"start\":47614},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47631,\"start\":47627},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47643,\"start\":47639},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":47673,\"start\":47669},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":47828,\"start\":47824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":47914,\"start\":47911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47964,\"start\":47961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":47966,\"start\":47964},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47987,\"start\":47983},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":47999,\"start\":47995},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":48017,\"start\":48013},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48215,\"start\":48211},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48333,\"start\":48329},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":48336,\"start\":48333},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53370,\"start\":53367},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":53372,\"start\":53370},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54153,\"start\":54149},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":54613,\"start\":54609},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":54624,\"start\":54620},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54657,\"start\":54653},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":54675,\"start\":54671},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":55427,\"start\":55423},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":55430,\"start\":55427},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56997,\"start\":56993},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":57968,\"start\":57964},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":58505,\"start\":58501},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":65799,\"start\":65795},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":66632,\"start\":66628},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":67110,\"start\":67106},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":67113,\"start\":67110},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":67116,\"start\":67113},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":67648,\"start\":67644},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2318,\"start\":2314},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2321,\"start\":2318},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2324,\"start\":2321},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2327,\"start\":2324},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2330,\"start\":2327},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2333,\"start\":2330},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2391,\"start\":2387},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2393,\"start\":2391},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2395,\"start\":2393},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2524,\"start\":2520},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2644,\"start\":2640},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2647,\"start\":2644},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2650,\"start\":2647},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2653,\"start\":2650},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2730,\"start\":2726},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3022,\"start\":3018},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3926,\"start\":3922},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6591,\"start\":6587},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6594,\"start\":6591},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7356,\"start\":7352},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9245,\"start\":9241},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10507,\"start\":10503},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10510,\"start\":10507},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10648,\"start\":10645},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12085,\"start\":12081},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12550,\"start\":12546},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15431,\"start\":15427},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16154,\"start\":16150},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18928,\"start\":18924},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18952,\"start\":18948},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18997,\"start\":18993},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19010,\"start\":19006},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19032,\"start\":19028},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19071,\"start\":19067},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19087,\"start\":19083},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19211,\"start\":19207},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":19309,\"start\":19305},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19321,\"start\":19317},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19333,\"start\":19329},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19345,\"start\":19342},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20289,\"start\":20285},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21301,\"start\":21297},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":21332,\"start\":21328},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":21344,\"start\":21340},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21356,\"start\":21353},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21887,\"start\":21883},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21890,\"start\":21887},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23852,\"start\":23848},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25328,\"start\":25324},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25341,\"start\":25338},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25399,\"start\":25395},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25419,\"start\":25415},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25447,\"start\":25443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25484,\"start\":25480},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25517,\"start\":25513},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25532,\"start\":25528},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26693,\"start\":26689},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26717,\"start\":26713},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26745,\"start\":26741},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27213,\"start\":27209},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28038,\"start\":28034},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28077,\"start\":28073},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28606,\"start\":28602},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29785,\"start\":29781},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30494,\"start\":30491},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30517,\"start\":30513},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30671,\"start\":30667},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30674,\"start\":30671},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31388,\"start\":31384},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32079,\"start\":32075},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32960,\"start\":32956},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35519,\"start\":35515},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35722,\"start\":35718},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35725,\"start\":35722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36533,\"start\":36529},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":37425,\"start\":37421},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41076,\"start\":41072},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":41729,\"start\":41725},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41947,\"start\":41943},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":41950,\"start\":41947},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":43253,\"start\":43249},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":46154,\"start\":46150},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":46174,\"start\":46170},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":46233,\"start\":46229},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":46236,\"start\":46233},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46314,\"start\":46310},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":46317,\"start\":46314},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":46320,\"start\":46317},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46397,\"start\":46393},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":46439,\"start\":46435},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":46606,\"start\":46602},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46717,\"start\":46713},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":46720,\"start\":46717},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":46723,\"start\":46720},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46762,\"start\":46759},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":46797,\"start\":46793},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46822,\"start\":46818},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":46842,\"start\":46838},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":46943,\"start\":46939},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":47408,\"start\":47404},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":47425,\"start\":47421},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":47443,\"start\":47439},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":47454,\"start\":47450},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":47457,\"start\":47454},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47475,\"start\":47472},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":47507,\"start\":47503},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":47510,\"start\":47507},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47530,\"start\":47526},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":47558,\"start\":47554},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47572,\"start\":47568},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":47589,\"start\":47585},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":47605,\"start\":47601},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":47618,\"start\":47614},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47631,\"start\":47627},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47643,\"start\":47639},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":47673,\"start\":47669},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":47828,\"start\":47824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":47914,\"start\":47911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47964,\"start\":47961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":47966,\"start\":47964},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47987,\"start\":47983},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":47999,\"start\":47995},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":48017,\"start\":48013},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48215,\"start\":48211},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48333,\"start\":48329},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":48336,\"start\":48333},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53370,\"start\":53367},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":53372,\"start\":53370},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54153,\"start\":54149},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":54613,\"start\":54609},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":54624,\"start\":54620},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54657,\"start\":54653},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":54675,\"start\":54671},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":55427,\"start\":55423},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":55430,\"start\":55427},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56997,\"start\":56993},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":57968,\"start\":57964},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":58505,\"start\":58501},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":65799,\"start\":65795},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":66632,\"start\":66628},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":67110,\"start\":67106},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":67113,\"start\":67110},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":67116,\"start\":67113},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":67648,\"start\":67644}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59801,\"start\":59597},{\"attributes\":{\"id\":\"fig_1\"},\"end\":60129,\"start\":59802},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60589,\"start\":60130},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60851,\"start\":60590},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61138,\"start\":60852},{\"attributes\":{\"id\":\"fig_7\"},\"end\":61200,\"start\":61139},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61823,\"start\":61201},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":62054,\"start\":61824},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":62656,\"start\":62055},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63105,\"start\":62657},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63470,\"start\":63106},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":64551,\"start\":63471},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":65152,\"start\":64552},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":65599,\"start\":65153},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66894,\"start\":65600},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":68494,\"start\":66895},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":68876,\"start\":68495},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":68996,\"start\":68877},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":69694,\"start\":68997},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":70351,\"start\":69695},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":70608,\"start\":70352},{\"attributes\":{\"id\":\"fig_0\"},\"end\":59801,\"start\":59597},{\"attributes\":{\"id\":\"fig_1\"},\"end\":60129,\"start\":59802},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60589,\"start\":60130},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60851,\"start\":60590},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61138,\"start\":60852},{\"attributes\":{\"id\":\"fig_7\"},\"end\":61200,\"start\":61139},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61823,\"start\":61201},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":62054,\"start\":61824},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":62656,\"start\":62055},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":63105,\"start\":62657},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":63470,\"start\":63106},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":64551,\"start\":63471},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":65152,\"start\":64552},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":65599,\"start\":65153},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66894,\"start\":65600},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":68494,\"start\":66895},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":68876,\"start\":68495},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":68996,\"start\":68877},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":69694,\"start\":68997},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":70351,\"start\":69695},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":70608,\"start\":70352}]", "paragraph": "[{\"end\":2731,\"start\":2218},{\"end\":3394,\"start\":2733},{\"end\":4351,\"start\":3404},{\"end\":4641,\"start\":4353},{\"end\":4808,\"start\":4643},{\"end\":5165,\"start\":4810},{\"end\":6198,\"start\":5167},{\"end\":6949,\"start\":6200},{\"end\":7143,\"start\":6951},{\"end\":7576,\"start\":7145},{\"end\":8253,\"start\":7591},{\"end\":8416,\"start\":8326},{\"end\":9186,\"start\":8471},{\"end\":9734,\"start\":9188},{\"end\":9813,\"start\":9759},{\"end\":11236,\"start\":9815},{\"end\":11616,\"start\":11257},{\"end\":11901,\"start\":11618},{\"end\":11983,\"start\":11903},{\"end\":12369,\"start\":12018},{\"end\":12750,\"start\":12371},{\"end\":13013,\"start\":12752},{\"end\":13464,\"start\":13015},{\"end\":13966,\"start\":13466},{\"end\":14042,\"start\":13968},{\"end\":14254,\"start\":14044},{\"end\":15213,\"start\":14304},{\"end\":15692,\"start\":15237},{\"end\":16509,\"start\":15694},{\"end\":17039,\"start\":16511},{\"end\":17194,\"start\":17041},{\"end\":17325,\"start\":17288},{\"end\":17588,\"start\":17438},{\"end\":18262,\"start\":17590},{\"end\":18701,\"start\":18296},{\"end\":20075,\"start\":18703},{\"end\":21007,\"start\":20126},{\"end\":21670,\"start\":21081},{\"end\":22523,\"start\":21672},{\"end\":23368,\"start\":22525},{\"end\":23803,\"start\":23370},{\"end\":24380,\"start\":23805},{\"end\":24531,\"start\":24382},{\"end\":25019,\"start\":24583},{\"end\":25128,\"start\":25042},{\"end\":25602,\"start\":25130},{\"end\":26630,\"start\":25604},{\"end\":27445,\"start\":26632},{\"end\":27672,\"start\":27460},{\"end\":27938,\"start\":27674},{\"end\":28511,\"start\":27940},{\"end\":29289,\"start\":28513},{\"end\":29665,\"start\":29291},{\"end\":30194,\"start\":29667},{\"end\":31557,\"start\":30196},{\"end\":32052,\"start\":31624},{\"end\":32621,\"start\":32054},{\"end\":33762,\"start\":32623},{\"end\":34555,\"start\":33764},{\"end\":35304,\"start\":34557},{\"end\":35831,\"start\":35329},{\"end\":36547,\"start\":35879},{\"end\":37106,\"start\":36549},{\"end\":37233,\"start\":37108},{\"end\":37278,\"start\":37246},{\"end\":37426,\"start\":37280},{\"end\":37516,\"start\":37433},{\"end\":37646,\"start\":37529},{\"end\":37786,\"start\":37648},{\"end\":37934,\"start\":37788},{\"end\":38034,\"start\":37936},{\"end\":38517,\"start\":38047},{\"end\":38561,\"start\":38519},{\"end\":38657,\"start\":38563},{\"end\":38987,\"start\":38684},{\"end\":39052,\"start\":38989},{\"end\":39151,\"start\":39054},{\"end\":39234,\"start\":39153},{\"end\":39366,\"start\":39247},{\"end\":39774,\"start\":39368},{\"end\":40345,\"start\":39776},{\"end\":40377,\"start\":40347},{\"end\":40586,\"start\":40390},{\"end\":40675,\"start\":40588},{\"end\":40773,\"start\":40677},{\"end\":40896,\"start\":40786},{\"end\":40975,\"start\":40898},{\"end\":41137,\"start\":40977},{\"end\":41306,\"start\":41139},{\"end\":41356,\"start\":41308},{\"end\":41460,\"start\":41358},{\"end\":41543,\"start\":41477},{\"end\":41593,\"start\":41558},{\"end\":41861,\"start\":41612},{\"end\":42033,\"start\":41863},{\"end\":42222,\"start\":42035},{\"end\":42606,\"start\":42224},{\"end\":43045,\"start\":42608},{\"end\":43141,\"start\":43047},{\"end\":43278,\"start\":43143},{\"end\":43994,\"start\":43297},{\"end\":44780,\"start\":43996},{\"end\":45253,\"start\":44782},{\"end\":45531,\"start\":45255},{\"end\":45913,\"start\":45533},{\"end\":46544,\"start\":45930},{\"end\":47116,\"start\":46546},{\"end\":47674,\"start\":47118},{\"end\":48337,\"start\":47676},{\"end\":48427,\"start\":48339},{\"end\":48801,\"start\":48458},{\"end\":49210,\"start\":48803},{\"end\":50126,\"start\":49212},{\"end\":50817,\"start\":50128},{\"end\":51551,\"start\":50819},{\"end\":51948,\"start\":51571},{\"end\":52405,\"start\":51950},{\"end\":53245,\"start\":52407},{\"end\":53633,\"start\":53247},{\"end\":54001,\"start\":53635},{\"end\":54824,\"start\":54063},{\"end\":55729,\"start\":54848},{\"end\":56063,\"start\":55747},{\"end\":56869,\"start\":56065},{\"end\":57063,\"start\":56892},{\"end\":57610,\"start\":57100},{\"end\":57698,\"start\":57644},{\"end\":59126,\"start\":57748},{\"end\":59596,\"start\":59149},{\"end\":2731,\"start\":2218},{\"end\":3394,\"start\":2733},{\"end\":4351,\"start\":3404},{\"end\":4641,\"start\":4353},{\"end\":4808,\"start\":4643},{\"end\":5165,\"start\":4810},{\"end\":6198,\"start\":5167},{\"end\":6949,\"start\":6200},{\"end\":7143,\"start\":6951},{\"end\":7576,\"start\":7145},{\"end\":8253,\"start\":7591},{\"end\":8416,\"start\":8326},{\"end\":9186,\"start\":8471},{\"end\":9734,\"start\":9188},{\"end\":9813,\"start\":9759},{\"end\":11236,\"start\":9815},{\"end\":11616,\"start\":11257},{\"end\":11901,\"start\":11618},{\"end\":11983,\"start\":11903},{\"end\":12369,\"start\":12018},{\"end\":12750,\"start\":12371},{\"end\":13013,\"start\":12752},{\"end\":13464,\"start\":13015},{\"end\":13966,\"start\":13466},{\"end\":14042,\"start\":13968},{\"end\":14254,\"start\":14044},{\"end\":15213,\"start\":14304},{\"end\":15692,\"start\":15237},{\"end\":16509,\"start\":15694},{\"end\":17039,\"start\":16511},{\"end\":17194,\"start\":17041},{\"end\":17325,\"start\":17288},{\"end\":17588,\"start\":17438},{\"end\":18262,\"start\":17590},{\"end\":18701,\"start\":18296},{\"end\":20075,\"start\":18703},{\"end\":21007,\"start\":20126},{\"end\":21670,\"start\":21081},{\"end\":22523,\"start\":21672},{\"end\":23368,\"start\":22525},{\"end\":23803,\"start\":23370},{\"end\":24380,\"start\":23805},{\"end\":24531,\"start\":24382},{\"end\":25019,\"start\":24583},{\"end\":25128,\"start\":25042},{\"end\":25602,\"start\":25130},{\"end\":26630,\"start\":25604},{\"end\":27445,\"start\":26632},{\"end\":27672,\"start\":27460},{\"end\":27938,\"start\":27674},{\"end\":28511,\"start\":27940},{\"end\":29289,\"start\":28513},{\"end\":29665,\"start\":29291},{\"end\":30194,\"start\":29667},{\"end\":31557,\"start\":30196},{\"end\":32052,\"start\":31624},{\"end\":32621,\"start\":32054},{\"end\":33762,\"start\":32623},{\"end\":34555,\"start\":33764},{\"end\":35304,\"start\":34557},{\"end\":35831,\"start\":35329},{\"end\":36547,\"start\":35879},{\"end\":37106,\"start\":36549},{\"end\":37233,\"start\":37108},{\"end\":37278,\"start\":37246},{\"end\":37426,\"start\":37280},{\"end\":37516,\"start\":37433},{\"end\":37646,\"start\":37529},{\"end\":37786,\"start\":37648},{\"end\":37934,\"start\":37788},{\"end\":38034,\"start\":37936},{\"end\":38517,\"start\":38047},{\"end\":38561,\"start\":38519},{\"end\":38657,\"start\":38563},{\"end\":38987,\"start\":38684},{\"end\":39052,\"start\":38989},{\"end\":39151,\"start\":39054},{\"end\":39234,\"start\":39153},{\"end\":39366,\"start\":39247},{\"end\":39774,\"start\":39368},{\"end\":40345,\"start\":39776},{\"end\":40377,\"start\":40347},{\"end\":40586,\"start\":40390},{\"end\":40675,\"start\":40588},{\"end\":40773,\"start\":40677},{\"end\":40896,\"start\":40786},{\"end\":40975,\"start\":40898},{\"end\":41137,\"start\":40977},{\"end\":41306,\"start\":41139},{\"end\":41356,\"start\":41308},{\"end\":41460,\"start\":41358},{\"end\":41543,\"start\":41477},{\"end\":41593,\"start\":41558},{\"end\":41861,\"start\":41612},{\"end\":42033,\"start\":41863},{\"end\":42222,\"start\":42035},{\"end\":42606,\"start\":42224},{\"end\":43045,\"start\":42608},{\"end\":43141,\"start\":43047},{\"end\":43278,\"start\":43143},{\"end\":43994,\"start\":43297},{\"end\":44780,\"start\":43996},{\"end\":45253,\"start\":44782},{\"end\":45531,\"start\":45255},{\"end\":45913,\"start\":45533},{\"end\":46544,\"start\":45930},{\"end\":47116,\"start\":46546},{\"end\":47674,\"start\":47118},{\"end\":48337,\"start\":47676},{\"end\":48427,\"start\":48339},{\"end\":48801,\"start\":48458},{\"end\":49210,\"start\":48803},{\"end\":50126,\"start\":49212},{\"end\":50817,\"start\":50128},{\"end\":51551,\"start\":50819},{\"end\":51948,\"start\":51571},{\"end\":52405,\"start\":51950},{\"end\":53245,\"start\":52407},{\"end\":53633,\"start\":53247},{\"end\":54001,\"start\":53635},{\"end\":54824,\"start\":54063},{\"end\":55729,\"start\":54848},{\"end\":56063,\"start\":55747},{\"end\":56869,\"start\":56065},{\"end\":57063,\"start\":56892},{\"end\":57610,\"start\":57100},{\"end\":57698,\"start\":57644},{\"end\":59126,\"start\":57748},{\"end\":59596,\"start\":59149}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8325,\"start\":8254},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8470,\"start\":8417},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9758,\"start\":9735},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14303,\"start\":14255},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17287,\"start\":17195},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17437,\"start\":17326},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20125,\"start\":20076},{\"attributes\":{\"id\":\"formula_7\"},\"end\":37432,\"start\":37427},{\"attributes\":{\"id\":\"formula_8\"},\"end\":41476,\"start\":41461},{\"attributes\":{\"id\":\"formula_9\"},\"end\":41557,\"start\":41544},{\"attributes\":{\"id\":\"formula_10\"},\"end\":41611,\"start\":41594},{\"attributes\":{\"id\":\"formula_0\"},\"end\":8325,\"start\":8254},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8470,\"start\":8417},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9758,\"start\":9735},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14303,\"start\":14255},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17287,\"start\":17195},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17437,\"start\":17326},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20125,\"start\":20076},{\"attributes\":{\"id\":\"formula_7\"},\"end\":37432,\"start\":37427},{\"attributes\":{\"id\":\"formula_8\"},\"end\":41476,\"start\":41461},{\"attributes\":{\"id\":\"formula_9\"},\"end\":41557,\"start\":41544},{\"attributes\":{\"id\":\"formula_10\"},\"end\":41611,\"start\":41594}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":4349,\"start\":4341},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21512,\"start\":21505},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22237,\"start\":22230},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22942,\"start\":22935},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32128,\"start\":32121},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32639,\"start\":32632},{\"end\":33140,\"start\":33133},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33787,\"start\":33780},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33961,\"start\":33954},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34904,\"start\":34897},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44441,\"start\":44434},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":50412,\"start\":50405},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":55728,\"start\":55721},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":56492,\"start\":56484},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":57334,\"start\":57326},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57609,\"start\":57601},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":4349,\"start\":4341},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21512,\"start\":21505},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22237,\"start\":22230},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22942,\"start\":22935},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32128,\"start\":32121},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32639,\"start\":32632},{\"end\":33140,\"start\":33133},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33787,\"start\":33780},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33961,\"start\":33954},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34904,\"start\":34897},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44441,\"start\":44434},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":50412,\"start\":50405},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":55728,\"start\":55721},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":56492,\"start\":56484},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":57334,\"start\":57326},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57609,\"start\":57601}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2216,\"start\":2204},{\"end\":3402,\"start\":3397},{\"attributes\":{\"n\":\"2\"},\"end\":7589,\"start\":7579},{\"attributes\":{\"n\":\"3\"},\"end\":11255,\"start\":11239},{\"end\":12016,\"start\":11986},{\"end\":15235,\"start\":15216},{\"attributes\":{\"n\":\"4\"},\"end\":18294,\"start\":18265},{\"end\":21079,\"start\":21010},{\"attributes\":{\"n\":\"5\"},\"end\":24581,\"start\":24534},{\"attributes\":{\"n\":\"5.1\"},\"end\":25040,\"start\":25022},{\"attributes\":{\"n\":\"5.2\"},\"end\":27458,\"start\":27448},{\"attributes\":{\"n\":\"5.3\"},\"end\":31622,\"start\":31560},{\"attributes\":{\"n\":\"6\"},\"end\":35327,\"start\":35307},{\"attributes\":{\"n\":\"6.1\"},\"end\":35877,\"start\":35834},{\"end\":37244,\"start\":37236},{\"end\":37527,\"start\":37519},{\"end\":38045,\"start\":38037},{\"end\":38682,\"start\":38660},{\"end\":39245,\"start\":39237},{\"end\":40388,\"start\":40380},{\"end\":40784,\"start\":40776},{\"attributes\":{\"n\":\"6.2\"},\"end\":43295,\"start\":43281},{\"attributes\":{\"n\":\"7\"},\"end\":45928,\"start\":45916},{\"attributes\":{\"n\":\"8\"},\"end\":48456,\"start\":48430},{\"attributes\":{\"n\":\"9\"},\"end\":51569,\"start\":51554},{\"end\":54061,\"start\":54004},{\"end\":54846,\"start\":54827},{\"end\":55745,\"start\":55732},{\"end\":56890,\"start\":56872},{\"end\":57098,\"start\":57066},{\"end\":57642,\"start\":57613},{\"end\":57746,\"start\":57701},{\"end\":59147,\"start\":59129},{\"end\":59608,\"start\":59598},{\"end\":59813,\"start\":59803},{\"end\":60141,\"start\":60131},{\"end\":60863,\"start\":60853},{\"end\":61150,\"start\":61140},{\"end\":61211,\"start\":61202},{\"end\":61834,\"start\":61825},{\"end\":62065,\"start\":62056},{\"end\":62667,\"start\":62658},{\"end\":63116,\"start\":63107},{\"end\":63481,\"start\":63472},{\"end\":64562,\"start\":64553},{\"end\":66905,\"start\":66896},{\"end\":68506,\"start\":68496},{\"end\":68888,\"start\":68878},{\"end\":69008,\"start\":68998},{\"end\":69706,\"start\":69696},{\"end\":70363,\"start\":70353},{\"attributes\":{\"n\":\"1\"},\"end\":2216,\"start\":2204},{\"end\":3402,\"start\":3397},{\"attributes\":{\"n\":\"2\"},\"end\":7589,\"start\":7579},{\"attributes\":{\"n\":\"3\"},\"end\":11255,\"start\":11239},{\"end\":12016,\"start\":11986},{\"end\":15235,\"start\":15216},{\"attributes\":{\"n\":\"4\"},\"end\":18294,\"start\":18265},{\"end\":21079,\"start\":21010},{\"attributes\":{\"n\":\"5\"},\"end\":24581,\"start\":24534},{\"attributes\":{\"n\":\"5.1\"},\"end\":25040,\"start\":25022},{\"attributes\":{\"n\":\"5.2\"},\"end\":27458,\"start\":27448},{\"attributes\":{\"n\":\"5.3\"},\"end\":31622,\"start\":31560},{\"attributes\":{\"n\":\"6\"},\"end\":35327,\"start\":35307},{\"attributes\":{\"n\":\"6.1\"},\"end\":35877,\"start\":35834},{\"end\":37244,\"start\":37236},{\"end\":37527,\"start\":37519},{\"end\":38045,\"start\":38037},{\"end\":38682,\"start\":38660},{\"end\":39245,\"start\":39237},{\"end\":40388,\"start\":40380},{\"end\":40784,\"start\":40776},{\"attributes\":{\"n\":\"6.2\"},\"end\":43295,\"start\":43281},{\"attributes\":{\"n\":\"7\"},\"end\":45928,\"start\":45916},{\"attributes\":{\"n\":\"8\"},\"end\":48456,\"start\":48430},{\"attributes\":{\"n\":\"9\"},\"end\":51569,\"start\":51554},{\"end\":54061,\"start\":54004},{\"end\":54846,\"start\":54827},{\"end\":55745,\"start\":55732},{\"end\":56890,\"start\":56872},{\"end\":57098,\"start\":57066},{\"end\":57642,\"start\":57613},{\"end\":57746,\"start\":57701},{\"end\":59147,\"start\":59129},{\"end\":59608,\"start\":59598},{\"end\":59813,\"start\":59803},{\"end\":60141,\"start\":60131},{\"end\":60863,\"start\":60853},{\"end\":61150,\"start\":61140},{\"end\":61211,\"start\":61202},{\"end\":61834,\"start\":61825},{\"end\":62065,\"start\":62056},{\"end\":62667,\"start\":62658},{\"end\":63116,\"start\":63107},{\"end\":63481,\"start\":63472},{\"end\":64562,\"start\":64553},{\"end\":66905,\"start\":66896},{\"end\":68506,\"start\":68496},{\"end\":68888,\"start\":68878},{\"end\":69008,\"start\":68998},{\"end\":69706,\"start\":69696},{\"end\":70363,\"start\":70353}]", "table": "[{\"end\":61823,\"start\":61224},{\"end\":62054,\"start\":61840},{\"end\":62656,\"start\":62325},{\"end\":63105,\"start\":62775},{\"end\":63470,\"start\":63455},{\"end\":64551,\"start\":63836},{\"end\":65152,\"start\":64739},{\"end\":65599,\"start\":65283},{\"end\":68876,\"start\":68770},{\"end\":69694,\"start\":69330},{\"end\":70351,\"start\":69980},{\"end\":70608,\"start\":70439},{\"end\":61823,\"start\":61224},{\"end\":62054,\"start\":61840},{\"end\":62656,\"start\":62325},{\"end\":63105,\"start\":62775},{\"end\":63470,\"start\":63455},{\"end\":64551,\"start\":63836},{\"end\":65152,\"start\":64739},{\"end\":65599,\"start\":65283},{\"end\":68876,\"start\":68770},{\"end\":69694,\"start\":69330},{\"end\":70351,\"start\":69980},{\"end\":70608,\"start\":70439}]", "figure_caption": "[{\"end\":59801,\"start\":59610},{\"end\":60129,\"start\":59815},{\"end\":60589,\"start\":60143},{\"end\":60851,\"start\":60592},{\"end\":61138,\"start\":60865},{\"end\":61200,\"start\":61152},{\"end\":61224,\"start\":61213},{\"end\":61840,\"start\":61836},{\"end\":62325,\"start\":62067},{\"end\":62775,\"start\":62669},{\"end\":63455,\"start\":63118},{\"end\":63836,\"start\":63483},{\"end\":64739,\"start\":64564},{\"end\":65283,\"start\":65155},{\"end\":66894,\"start\":65602},{\"end\":68494,\"start\":66907},{\"end\":68770,\"start\":68509},{\"end\":68996,\"start\":68891},{\"end\":69330,\"start\":69011},{\"end\":69980,\"start\":69709},{\"end\":70439,\"start\":70366},{\"end\":59801,\"start\":59610},{\"end\":60129,\"start\":59815},{\"end\":60589,\"start\":60143},{\"end\":60851,\"start\":60592},{\"end\":61138,\"start\":60865},{\"end\":61200,\"start\":61152},{\"end\":61224,\"start\":61213},{\"end\":61840,\"start\":61836},{\"end\":62325,\"start\":62067},{\"end\":62775,\"start\":62669},{\"end\":63455,\"start\":63118},{\"end\":63836,\"start\":63483},{\"end\":64739,\"start\":64564},{\"end\":65283,\"start\":65155},{\"end\":66894,\"start\":65602},{\"end\":68494,\"start\":66907},{\"end\":68770,\"start\":68509},{\"end\":68996,\"start\":68891},{\"end\":69330,\"start\":69011},{\"end\":69980,\"start\":69709},{\"end\":70439,\"start\":70366}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20391,\"start\":20383},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21006,\"start\":20998},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21500,\"start\":21492},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":54000,\"start\":53992},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":57062,\"start\":57054},{\"end\":58708,\"start\":58700},{\"end\":59245,\"start\":59237},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20391,\"start\":20383},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21006,\"start\":20998},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21500,\"start\":21492},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":54000,\"start\":53992},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":57062,\"start\":57054},{\"end\":58708,\"start\":58700},{\"end\":59245,\"start\":59237}]", "bib_author_first_name": "[{\"end\":71604,\"start\":71603},{\"end\":71610,\"start\":71609},{\"end\":71616,\"start\":71615},{\"end\":71623,\"start\":71622},{\"end\":71630,\"start\":71629},{\"end\":71638,\"start\":71637},{\"end\":71644,\"start\":71643},{\"end\":71652,\"start\":71651},{\"end\":71664,\"start\":71660},{\"end\":71951,\"start\":71950},{\"end\":71961,\"start\":71960},{\"end\":71968,\"start\":71967},{\"end\":71976,\"start\":71975},{\"end\":71985,\"start\":71984},{\"end\":71996,\"start\":71995},{\"end\":72008,\"start\":72007},{\"end\":72017,\"start\":72016},{\"end\":72027,\"start\":72026},{\"end\":72035,\"start\":72034},{\"end\":72290,\"start\":72289},{\"end\":72292,\"start\":72291},{\"end\":72300,\"start\":72299},{\"end\":72311,\"start\":72307},{\"end\":72319,\"start\":72318},{\"end\":72329,\"start\":72328},{\"end\":72339,\"start\":72338},{\"end\":72341,\"start\":72340},{\"end\":72350,\"start\":72349},{\"end\":72360,\"start\":72359},{\"end\":72367,\"start\":72366},{\"end\":72369,\"start\":72368},{\"end\":72377,\"start\":72376},{\"end\":72843,\"start\":72842},{\"end\":72850,\"start\":72849},{\"end\":72859,\"start\":72858},{\"end\":72870,\"start\":72869},{\"end\":72880,\"start\":72879},{\"end\":72888,\"start\":72887},{\"end\":72900,\"start\":72899},{\"end\":72909,\"start\":72908},{\"end\":72917,\"start\":72916},{\"end\":72928,\"start\":72927},{\"end\":73215,\"start\":73214},{\"end\":73222,\"start\":73221},{\"end\":73234,\"start\":73233},{\"end\":73243,\"start\":73242},{\"end\":73253,\"start\":73252},{\"end\":73264,\"start\":73263},{\"end\":73273,\"start\":73272},{\"end\":73281,\"start\":73280},{\"end\":73291,\"start\":73290},{\"end\":73305,\"start\":73304},{\"end\":73679,\"start\":73678},{\"end\":73681,\"start\":73680},{\"end\":73691,\"start\":73690},{\"end\":73700,\"start\":73699},{\"end\":73718,\"start\":73717},{\"end\":74130,\"start\":74129},{\"end\":74142,\"start\":74141},{\"end\":74156,\"start\":74155},{\"end\":74167,\"start\":74166},{\"end\":74178,\"start\":74177},{\"end\":74194,\"start\":74193},{\"end\":74206,\"start\":74205},{\"end\":74208,\"start\":74207},{\"end\":74216,\"start\":74215},{\"end\":74227,\"start\":74226},{\"end\":74229,\"start\":74228},{\"end\":74242,\"start\":74241},{\"end\":74608,\"start\":74607},{\"end\":74621,\"start\":74620},{\"end\":74623,\"start\":74622},{\"end\":74633,\"start\":74632},{\"end\":74642,\"start\":74641},{\"end\":74652,\"start\":74651},{\"end\":74661,\"start\":74660},{\"end\":74668,\"start\":74667},{\"end\":74670,\"start\":74669},{\"end\":74683,\"start\":74682},{\"end\":74691,\"start\":74690},{\"end\":74703,\"start\":74702},{\"end\":75023,\"start\":75022},{\"end\":75031,\"start\":75030},{\"end\":75037,\"start\":75036},{\"end\":75046,\"start\":75045},{\"end\":75343,\"start\":75339},{\"end\":75353,\"start\":75352},{\"end\":75359,\"start\":75358},{\"end\":75366,\"start\":75365},{\"end\":75375,\"start\":75374},{\"end\":75381,\"start\":75380},{\"end\":75390,\"start\":75389},{\"end\":75399,\"start\":75398},{\"end\":75409,\"start\":75408},{\"end\":75419,\"start\":75418},{\"end\":75421,\"start\":75420},{\"end\":75433,\"start\":75432},{\"end\":75443,\"start\":75442},{\"end\":75445,\"start\":75444},{\"end\":75822,\"start\":75821},{\"end\":75824,\"start\":75823},{\"end\":75838,\"start\":75837},{\"end\":75847,\"start\":75846},{\"end\":75856,\"start\":75855},{\"end\":75866,\"start\":75865},{\"end\":75874,\"start\":75873},{\"end\":76120,\"start\":76119},{\"end\":76122,\"start\":76121},{\"end\":76131,\"start\":76130},{\"end\":76138,\"start\":76137},{\"end\":76149,\"start\":76148},{\"end\":76157,\"start\":76156},{\"end\":76164,\"start\":76163},{\"end\":76173,\"start\":76172},{\"end\":76179,\"start\":76178},{\"end\":76187,\"start\":76186},{\"end\":76199,\"start\":76198},{\"end\":76495,\"start\":76494},{\"end\":76507,\"start\":76506},{\"end\":76739,\"start\":76738},{\"end\":76751,\"start\":76750},{\"end\":76760,\"start\":76759},{\"end\":76771,\"start\":76770},{\"end\":77218,\"start\":77217},{\"end\":77230,\"start\":77229},{\"end\":77239,\"start\":77238},{\"end\":77251,\"start\":77250},{\"end\":77507,\"start\":77506},{\"end\":77509,\"start\":77508},{\"end\":77773,\"start\":77772},{\"end\":77775,\"start\":77774},{\"end\":77948,\"start\":77947},{\"end\":77959,\"start\":77958},{\"end\":77971,\"start\":77970},{\"end\":77982,\"start\":77981},{\"end\":78201,\"start\":78200},{\"end\":78210,\"start\":78206},{\"end\":78216,\"start\":78215},{\"end\":78225,\"start\":78224},{\"end\":78456,\"start\":78455},{\"end\":78464,\"start\":78463},{\"end\":78477,\"start\":78476},{\"end\":78484,\"start\":78483},{\"end\":78495,\"start\":78494},{\"end\":78505,\"start\":78504},{\"end\":78515,\"start\":78514},{\"end\":78806,\"start\":78805},{\"end\":78816,\"start\":78815},{\"end\":78828,\"start\":78827},{\"end\":78839,\"start\":78838},{\"end\":78852,\"start\":78851},{\"end\":78862,\"start\":78861},{\"end\":78872,\"start\":78871},{\"end\":78880,\"start\":78879},{\"end\":78893,\"start\":78892},{\"end\":78905,\"start\":78904},{\"end\":79183,\"start\":79182},{\"end\":79197,\"start\":79196},{\"end\":79212,\"start\":79211},{\"end\":79220,\"start\":79219},{\"end\":79232,\"start\":79231},{\"end\":79234,\"start\":79233},{\"end\":79244,\"start\":79243},{\"end\":79246,\"start\":79245},{\"end\":79582,\"start\":79581},{\"end\":79595,\"start\":79594},{\"end\":79858,\"start\":79857},{\"end\":79871,\"start\":79870},{\"end\":79880,\"start\":79879},{\"end\":79890,\"start\":79889},{\"end\":79897,\"start\":79896},{\"end\":79908,\"start\":79907},{\"end\":79916,\"start\":79915},{\"end\":80231,\"start\":80230},{\"end\":80243,\"start\":80242},{\"end\":80251,\"start\":80250},{\"end\":80257,\"start\":80256},{\"end\":80267,\"start\":80266},{\"end\":80569,\"start\":80568},{\"end\":80581,\"start\":80580},{\"end\":80592,\"start\":80591},{\"end\":80600,\"start\":80599},{\"end\":80853,\"start\":80852},{\"end\":80864,\"start\":80863},{\"end\":80875,\"start\":80874},{\"end\":80890,\"start\":80889},{\"end\":80901,\"start\":80900},{\"end\":80919,\"start\":80918},{\"end\":80931,\"start\":80930},{\"end\":80944,\"start\":80943},{\"end\":81243,\"start\":81242},{\"end\":81245,\"start\":81244},{\"end\":81251,\"start\":81250},{\"end\":81259,\"start\":81258},{\"end\":81269,\"start\":81268},{\"end\":81282,\"start\":81281},{\"end\":81288,\"start\":81287},{\"end\":81296,\"start\":81295},{\"end\":81304,\"start\":81303},{\"end\":81662,\"start\":81661},{\"end\":81670,\"start\":81669},{\"end\":81672,\"start\":81671},{\"end\":81679,\"start\":81678},{\"end\":81691,\"start\":81690},{\"end\":81703,\"start\":81702},{\"end\":81712,\"start\":81711},{\"end\":81718,\"start\":81717},{\"end\":81729,\"start\":81728},{\"end\":81737,\"start\":81736},{\"end\":81744,\"start\":81743},{\"end\":81746,\"start\":81745},{\"end\":82120,\"start\":82119},{\"end\":82130,\"start\":82129},{\"end\":82140,\"start\":82139},{\"end\":82152,\"start\":82151},{\"end\":82373,\"start\":82372},{\"end\":82381,\"start\":82380},{\"end\":82392,\"start\":82391},{\"end\":82405,\"start\":82404},{\"end\":82424,\"start\":82420},{\"end\":82431,\"start\":82430},{\"end\":82442,\"start\":82441},{\"end\":82453,\"start\":82452},{\"end\":82455,\"start\":82454},{\"end\":82462,\"start\":82461},{\"end\":82473,\"start\":82472},{\"end\":83040,\"start\":83039},{\"end\":83050,\"start\":83049},{\"end\":83061,\"start\":83060},{\"end\":83302,\"start\":83301},{\"end\":83304,\"start\":83303},{\"end\":83310,\"start\":83309},{\"end\":83513,\"start\":83512},{\"end\":83522,\"start\":83521},{\"end\":83535,\"start\":83534},{\"end\":83542,\"start\":83541},{\"end\":83553,\"start\":83552},{\"end\":83562,\"start\":83561},{\"end\":83574,\"start\":83573},{\"end\":83583,\"start\":83582},{\"end\":83596,\"start\":83595},{\"end\":83602,\"start\":83601},{\"end\":83919,\"start\":83918},{\"end\":83927,\"start\":83926},{\"end\":83936,\"start\":83935},{\"end\":83938,\"start\":83937},{\"end\":83946,\"start\":83945},{\"end\":84399,\"start\":84398},{\"end\":84406,\"start\":84405},{\"end\":84413,\"start\":84412},{\"end\":84424,\"start\":84423},{\"end\":84433,\"start\":84432},{\"end\":84442,\"start\":84441},{\"end\":84452,\"start\":84451},{\"end\":84454,\"start\":84453},{\"end\":84749,\"start\":84748},{\"end\":84756,\"start\":84755},{\"end\":84763,\"start\":84762},{\"end\":84772,\"start\":84771},{\"end\":84778,\"start\":84777},{\"end\":84787,\"start\":84786},{\"end\":84795,\"start\":84794},{\"end\":84803,\"start\":84802},{\"end\":84812,\"start\":84811},{\"end\":84827,\"start\":84826},{\"end\":85209,\"start\":85208},{\"end\":85220,\"start\":85219},{\"end\":85227,\"start\":85226},{\"end\":85233,\"start\":85232},{\"end\":85243,\"start\":85242},{\"end\":85245,\"start\":85244},{\"end\":85254,\"start\":85253},{\"end\":85261,\"start\":85260},{\"end\":85269,\"start\":85268},{\"end\":85271,\"start\":85270},{\"end\":85277,\"start\":85276},{\"end\":85285,\"start\":85284},{\"end\":85546,\"start\":85545},{\"end\":85553,\"start\":85552},{\"end\":85562,\"start\":85561},{\"end\":85577,\"start\":85576},{\"end\":85841,\"start\":85840},{\"end\":85855,\"start\":85854},{\"end\":85864,\"start\":85863},{\"end\":85873,\"start\":85872},{\"end\":85883,\"start\":85882},{\"end\":86413,\"start\":86412},{\"end\":86423,\"start\":86422},{\"end\":86429,\"start\":86428},{\"end\":86438,\"start\":86437},{\"end\":86449,\"start\":86448},{\"end\":86463,\"start\":86462},{\"end\":86474,\"start\":86473},{\"end\":86483,\"start\":86482},{\"end\":86494,\"start\":86493},{\"end\":86503,\"start\":86502},{\"end\":86820,\"start\":86819},{\"end\":86828,\"start\":86827},{\"end\":86836,\"start\":86835},{\"end\":86838,\"start\":86837},{\"end\":86846,\"start\":86845},{\"end\":86853,\"start\":86852},{\"end\":86860,\"start\":86859},{\"end\":87170,\"start\":87169},{\"end\":87178,\"start\":87177},{\"end\":87184,\"start\":87183},{\"end\":87190,\"start\":87189},{\"end\":87200,\"start\":87199},{\"end\":87442,\"start\":87441},{\"end\":87452,\"start\":87451},{\"end\":87466,\"start\":87465},{\"end\":87476,\"start\":87475},{\"end\":87488,\"start\":87487},{\"end\":87886,\"start\":87885},{\"end\":87894,\"start\":87893},{\"end\":87905,\"start\":87904},{\"end\":87918,\"start\":87917},{\"end\":87928,\"start\":87927},{\"end\":87940,\"start\":87939},{\"end\":87952,\"start\":87951},{\"end\":87960,\"start\":87959},{\"end\":87968,\"start\":87967},{\"end\":87979,\"start\":87978},{\"end\":88323,\"start\":88322},{\"end\":88330,\"start\":88329},{\"end\":88580,\"start\":88579},{\"end\":88590,\"start\":88589},{\"end\":88601,\"start\":88600},{\"end\":88612,\"start\":88611},{\"end\":88619,\"start\":88618},{\"end\":88629,\"start\":88628},{\"end\":88639,\"start\":88638},{\"end\":88647,\"start\":88646},{\"end\":88653,\"start\":88652},{\"end\":88655,\"start\":88654},{\"end\":88997,\"start\":88996},{\"end\":89005,\"start\":89004},{\"end\":89015,\"start\":89014},{\"end\":89025,\"start\":89024},{\"end\":89027,\"start\":89026},{\"end\":89035,\"start\":89034},{\"end\":89047,\"start\":89046},{\"end\":89059,\"start\":89058},{\"end\":89070,\"start\":89069},{\"end\":89082,\"start\":89081},{\"end\":89084,\"start\":89083},{\"end\":89092,\"start\":89091},{\"end\":89359,\"start\":89358},{\"end\":89366,\"start\":89365},{\"end\":89376,\"start\":89375},{\"end\":89385,\"start\":89384},{\"end\":89653,\"start\":89652},{\"end\":89655,\"start\":89654},{\"end\":89663,\"start\":89662},{\"end\":89670,\"start\":89669},{\"end\":89679,\"start\":89678},{\"end\":89690,\"start\":89689},{\"end\":89698,\"start\":89697},{\"end\":89709,\"start\":89708},{\"end\":89721,\"start\":89720},{\"end\":89723,\"start\":89722},{\"end\":89735,\"start\":89734},{\"end\":89743,\"start\":89742},{\"end\":90118,\"start\":90117},{\"end\":90127,\"start\":90126},{\"end\":90140,\"start\":90139},{\"end\":90149,\"start\":90148},{\"end\":90159,\"start\":90158},{\"end\":90165,\"start\":90164},{\"end\":90177,\"start\":90176},{\"end\":90186,\"start\":90185},{\"end\":90188,\"start\":90187},{\"end\":90410,\"start\":90409},{\"end\":90423,\"start\":90422},{\"end\":90434,\"start\":90433},{\"end\":90442,\"start\":90441},{\"end\":90453,\"start\":90452},{\"end\":90472,\"start\":90468},{\"end\":90481,\"start\":90480},{\"end\":90488,\"start\":90487},{\"end\":90495,\"start\":90494},{\"end\":90504,\"start\":90503},{\"end\":90789,\"start\":90788},{\"end\":90800,\"start\":90799},{\"end\":90810,\"start\":90809},{\"end\":90821,\"start\":90820},{\"end\":90836,\"start\":90832},{\"end\":90847,\"start\":90846},{\"end\":90858,\"start\":90857},{\"end\":90869,\"start\":90868},{\"end\":90878,\"start\":90877},{\"end\":90888,\"start\":90887},{\"end\":91193,\"start\":91192},{\"end\":91201,\"start\":91200},{\"end\":91210,\"start\":91209},{\"end\":91221,\"start\":91220},{\"end\":91229,\"start\":91228},{\"end\":91237,\"start\":91236},{\"end\":91239,\"start\":91238},{\"end\":91633,\"start\":91632},{\"end\":91641,\"start\":91640},{\"end\":91650,\"start\":91649},{\"end\":91660,\"start\":91659},{\"end\":91667,\"start\":91666},{\"end\":91669,\"start\":91668},{\"end\":91678,\"start\":91677},{\"end\":91690,\"start\":91689},{\"end\":92016,\"start\":92015},{\"end\":92024,\"start\":92023},{\"end\":92034,\"start\":92033},{\"end\":92054,\"start\":92053},{\"end\":92063,\"start\":92062},{\"end\":92074,\"start\":92073},{\"end\":92087,\"start\":92086},{\"end\":92096,\"start\":92095},{\"end\":92098,\"start\":92097},{\"end\":92114,\"start\":92113},{\"end\":92122,\"start\":92121},{\"end\":92467,\"start\":92466},{\"end\":92475,\"start\":92474},{\"end\":92485,\"start\":92484},{\"end\":92505,\"start\":92504},{\"end\":92514,\"start\":92513},{\"end\":92525,\"start\":92524},{\"end\":92533,\"start\":92532},{\"end\":92542,\"start\":92541},{\"end\":92544,\"start\":92543},{\"end\":92560,\"start\":92559},{\"end\":92573,\"start\":92572},{\"end\":93128,\"start\":93127},{\"end\":93135,\"start\":93134},{\"end\":93144,\"start\":93143},{\"end\":93146,\"start\":93145},{\"end\":93154,\"start\":93153},{\"end\":93161,\"start\":93160},{\"end\":93163,\"start\":93162},{\"end\":93169,\"start\":93168},{\"end\":93179,\"start\":93178},{\"end\":93185,\"start\":93184},{\"end\":93187,\"start\":93186},{\"end\":93194,\"start\":93193},{\"end\":93196,\"start\":93195},{\"end\":93484,\"start\":93483},{\"end\":93491,\"start\":93490},{\"end\":93499,\"start\":93498},{\"end\":93513,\"start\":93512},{\"end\":93522,\"start\":93521},{\"end\":93529,\"start\":93528},{\"end\":93531,\"start\":93530},{\"end\":93538,\"start\":93537},{\"end\":93540,\"start\":93539},{\"end\":93546,\"start\":93545},{\"end\":93898,\"start\":93897},{\"end\":93906,\"start\":93905},{\"end\":93915,\"start\":93914},{\"end\":93923,\"start\":93922},{\"end\":93935,\"start\":93934},{\"end\":93947,\"start\":93946},{\"end\":93954,\"start\":93953},{\"end\":93964,\"start\":93963},{\"end\":93973,\"start\":93972},{\"end\":93981,\"start\":93980},{\"end\":94325,\"start\":94324},{\"end\":94337,\"start\":94336},{\"end\":94349,\"start\":94348},{\"end\":94364,\"start\":94363},{\"end\":94374,\"start\":94373},{\"end\":94385,\"start\":94384},{\"end\":94711,\"start\":94710},{\"end\":94719,\"start\":94718},{\"end\":94726,\"start\":94725},{\"end\":94736,\"start\":94735},{\"end\":94747,\"start\":94746},{\"end\":94966,\"start\":94965},{\"end\":94973,\"start\":94972},{\"end\":94975,\"start\":94974},{\"end\":94981,\"start\":94980},{\"end\":94988,\"start\":94987},{\"end\":94997,\"start\":94996},{\"end\":95008,\"start\":95007},{\"end\":95023,\"start\":95019},{\"end\":95029,\"start\":95028},{\"end\":95038,\"start\":95037},{\"end\":95045,\"start\":95044},{\"end\":95047,\"start\":95046},{\"end\":95506,\"start\":95505},{\"end\":95514,\"start\":95513},{\"end\":95520,\"start\":95519},{\"end\":95529,\"start\":95528},{\"end\":95539,\"start\":95538},{\"end\":95548,\"start\":95547},{\"end\":95565,\"start\":95564},{\"end\":95567,\"start\":95566},{\"end\":96104,\"start\":96103},{\"end\":96111,\"start\":96110},{\"end\":96113,\"start\":96112},{\"end\":96126,\"start\":96125},{\"end\":96135,\"start\":96134},{\"end\":96141,\"start\":96140},{\"end\":96147,\"start\":96146},{\"end\":96469,\"start\":96468},{\"end\":96471,\"start\":96470},{\"end\":96480,\"start\":96479},{\"end\":96492,\"start\":96491},{\"end\":96710,\"start\":96709},{\"end\":96718,\"start\":96717},{\"end\":96725,\"start\":96724},{\"end\":96731,\"start\":96730},{\"end\":96739,\"start\":96738},{\"end\":96746,\"start\":96745},{\"end\":96754,\"start\":96753},{\"end\":96762,\"start\":96761},{\"end\":96768,\"start\":96767},{\"end\":96777,\"start\":96776},{\"end\":97039,\"start\":97038},{\"end\":97048,\"start\":97047},{\"end\":97058,\"start\":97057},{\"end\":97067,\"start\":97066},{\"end\":97078,\"start\":97077},{\"end\":97086,\"start\":97085},{\"end\":97094,\"start\":97093},{\"end\":97103,\"start\":97102},{\"end\":97111,\"start\":97110},{\"end\":97117,\"start\":97116},{\"end\":97119,\"start\":97118},{\"end\":97496,\"start\":97495},{\"end\":97505,\"start\":97504},{\"end\":97512,\"start\":97511},{\"end\":97521,\"start\":97520},{\"end\":71604,\"start\":71603},{\"end\":71610,\"start\":71609},{\"end\":71616,\"start\":71615},{\"end\":71623,\"start\":71622},{\"end\":71630,\"start\":71629},{\"end\":71638,\"start\":71637},{\"end\":71644,\"start\":71643},{\"end\":71652,\"start\":71651},{\"end\":71664,\"start\":71660},{\"end\":71951,\"start\":71950},{\"end\":71961,\"start\":71960},{\"end\":71968,\"start\":71967},{\"end\":71976,\"start\":71975},{\"end\":71985,\"start\":71984},{\"end\":71996,\"start\":71995},{\"end\":72008,\"start\":72007},{\"end\":72017,\"start\":72016},{\"end\":72027,\"start\":72026},{\"end\":72035,\"start\":72034},{\"end\":72290,\"start\":72289},{\"end\":72292,\"start\":72291},{\"end\":72300,\"start\":72299},{\"end\":72311,\"start\":72307},{\"end\":72319,\"start\":72318},{\"end\":72329,\"start\":72328},{\"end\":72339,\"start\":72338},{\"end\":72341,\"start\":72340},{\"end\":72350,\"start\":72349},{\"end\":72360,\"start\":72359},{\"end\":72367,\"start\":72366},{\"end\":72369,\"start\":72368},{\"end\":72377,\"start\":72376},{\"end\":72843,\"start\":72842},{\"end\":72850,\"start\":72849},{\"end\":72859,\"start\":72858},{\"end\":72870,\"start\":72869},{\"end\":72880,\"start\":72879},{\"end\":72888,\"start\":72887},{\"end\":72900,\"start\":72899},{\"end\":72909,\"start\":72908},{\"end\":72917,\"start\":72916},{\"end\":72928,\"start\":72927},{\"end\":73215,\"start\":73214},{\"end\":73222,\"start\":73221},{\"end\":73234,\"start\":73233},{\"end\":73243,\"start\":73242},{\"end\":73253,\"start\":73252},{\"end\":73264,\"start\":73263},{\"end\":73273,\"start\":73272},{\"end\":73281,\"start\":73280},{\"end\":73291,\"start\":73290},{\"end\":73305,\"start\":73304},{\"end\":73679,\"start\":73678},{\"end\":73681,\"start\":73680},{\"end\":73691,\"start\":73690},{\"end\":73700,\"start\":73699},{\"end\":73718,\"start\":73717},{\"end\":74130,\"start\":74129},{\"end\":74142,\"start\":74141},{\"end\":74156,\"start\":74155},{\"end\":74167,\"start\":74166},{\"end\":74178,\"start\":74177},{\"end\":74194,\"start\":74193},{\"end\":74206,\"start\":74205},{\"end\":74208,\"start\":74207},{\"end\":74216,\"start\":74215},{\"end\":74227,\"start\":74226},{\"end\":74229,\"start\":74228},{\"end\":74242,\"start\":74241},{\"end\":74608,\"start\":74607},{\"end\":74621,\"start\":74620},{\"end\":74623,\"start\":74622},{\"end\":74633,\"start\":74632},{\"end\":74642,\"start\":74641},{\"end\":74652,\"start\":74651},{\"end\":74661,\"start\":74660},{\"end\":74668,\"start\":74667},{\"end\":74670,\"start\":74669},{\"end\":74683,\"start\":74682},{\"end\":74691,\"start\":74690},{\"end\":74703,\"start\":74702},{\"end\":75023,\"start\":75022},{\"end\":75031,\"start\":75030},{\"end\":75037,\"start\":75036},{\"end\":75046,\"start\":75045},{\"end\":75343,\"start\":75339},{\"end\":75353,\"start\":75352},{\"end\":75359,\"start\":75358},{\"end\":75366,\"start\":75365},{\"end\":75375,\"start\":75374},{\"end\":75381,\"start\":75380},{\"end\":75390,\"start\":75389},{\"end\":75399,\"start\":75398},{\"end\":75409,\"start\":75408},{\"end\":75419,\"start\":75418},{\"end\":75421,\"start\":75420},{\"end\":75433,\"start\":75432},{\"end\":75443,\"start\":75442},{\"end\":75445,\"start\":75444},{\"end\":75822,\"start\":75821},{\"end\":75824,\"start\":75823},{\"end\":75838,\"start\":75837},{\"end\":75847,\"start\":75846},{\"end\":75856,\"start\":75855},{\"end\":75866,\"start\":75865},{\"end\":75874,\"start\":75873},{\"end\":76120,\"start\":76119},{\"end\":76122,\"start\":76121},{\"end\":76131,\"start\":76130},{\"end\":76138,\"start\":76137},{\"end\":76149,\"start\":76148},{\"end\":76157,\"start\":76156},{\"end\":76164,\"start\":76163},{\"end\":76173,\"start\":76172},{\"end\":76179,\"start\":76178},{\"end\":76187,\"start\":76186},{\"end\":76199,\"start\":76198},{\"end\":76495,\"start\":76494},{\"end\":76507,\"start\":76506},{\"end\":76739,\"start\":76738},{\"end\":76751,\"start\":76750},{\"end\":76760,\"start\":76759},{\"end\":76771,\"start\":76770},{\"end\":77218,\"start\":77217},{\"end\":77230,\"start\":77229},{\"end\":77239,\"start\":77238},{\"end\":77251,\"start\":77250},{\"end\":77507,\"start\":77506},{\"end\":77509,\"start\":77508},{\"end\":77773,\"start\":77772},{\"end\":77775,\"start\":77774},{\"end\":77948,\"start\":77947},{\"end\":77959,\"start\":77958},{\"end\":77971,\"start\":77970},{\"end\":77982,\"start\":77981},{\"end\":78201,\"start\":78200},{\"end\":78210,\"start\":78206},{\"end\":78216,\"start\":78215},{\"end\":78225,\"start\":78224},{\"end\":78456,\"start\":78455},{\"end\":78464,\"start\":78463},{\"end\":78477,\"start\":78476},{\"end\":78484,\"start\":78483},{\"end\":78495,\"start\":78494},{\"end\":78505,\"start\":78504},{\"end\":78515,\"start\":78514},{\"end\":78806,\"start\":78805},{\"end\":78816,\"start\":78815},{\"end\":78828,\"start\":78827},{\"end\":78839,\"start\":78838},{\"end\":78852,\"start\":78851},{\"end\":78862,\"start\":78861},{\"end\":78872,\"start\":78871},{\"end\":78880,\"start\":78879},{\"end\":78893,\"start\":78892},{\"end\":78905,\"start\":78904},{\"end\":79183,\"start\":79182},{\"end\":79197,\"start\":79196},{\"end\":79212,\"start\":79211},{\"end\":79220,\"start\":79219},{\"end\":79232,\"start\":79231},{\"end\":79234,\"start\":79233},{\"end\":79244,\"start\":79243},{\"end\":79246,\"start\":79245},{\"end\":79582,\"start\":79581},{\"end\":79595,\"start\":79594},{\"end\":79858,\"start\":79857},{\"end\":79871,\"start\":79870},{\"end\":79880,\"start\":79879},{\"end\":79890,\"start\":79889},{\"end\":79897,\"start\":79896},{\"end\":79908,\"start\":79907},{\"end\":79916,\"start\":79915},{\"end\":80231,\"start\":80230},{\"end\":80243,\"start\":80242},{\"end\":80251,\"start\":80250},{\"end\":80257,\"start\":80256},{\"end\":80267,\"start\":80266},{\"end\":80569,\"start\":80568},{\"end\":80581,\"start\":80580},{\"end\":80592,\"start\":80591},{\"end\":80600,\"start\":80599},{\"end\":80853,\"start\":80852},{\"end\":80864,\"start\":80863},{\"end\":80875,\"start\":80874},{\"end\":80890,\"start\":80889},{\"end\":80901,\"start\":80900},{\"end\":80919,\"start\":80918},{\"end\":80931,\"start\":80930},{\"end\":80944,\"start\":80943},{\"end\":81243,\"start\":81242},{\"end\":81245,\"start\":81244},{\"end\":81251,\"start\":81250},{\"end\":81259,\"start\":81258},{\"end\":81269,\"start\":81268},{\"end\":81282,\"start\":81281},{\"end\":81288,\"start\":81287},{\"end\":81296,\"start\":81295},{\"end\":81304,\"start\":81303},{\"end\":81662,\"start\":81661},{\"end\":81670,\"start\":81669},{\"end\":81672,\"start\":81671},{\"end\":81679,\"start\":81678},{\"end\":81691,\"start\":81690},{\"end\":81703,\"start\":81702},{\"end\":81712,\"start\":81711},{\"end\":81718,\"start\":81717},{\"end\":81729,\"start\":81728},{\"end\":81737,\"start\":81736},{\"end\":81744,\"start\":81743},{\"end\":81746,\"start\":81745},{\"end\":82120,\"start\":82119},{\"end\":82130,\"start\":82129},{\"end\":82140,\"start\":82139},{\"end\":82152,\"start\":82151},{\"end\":82373,\"start\":82372},{\"end\":82381,\"start\":82380},{\"end\":82392,\"start\":82391},{\"end\":82405,\"start\":82404},{\"end\":82424,\"start\":82420},{\"end\":82431,\"start\":82430},{\"end\":82442,\"start\":82441},{\"end\":82453,\"start\":82452},{\"end\":82455,\"start\":82454},{\"end\":82462,\"start\":82461},{\"end\":82473,\"start\":82472},{\"end\":83040,\"start\":83039},{\"end\":83050,\"start\":83049},{\"end\":83061,\"start\":83060},{\"end\":83302,\"start\":83301},{\"end\":83304,\"start\":83303},{\"end\":83310,\"start\":83309},{\"end\":83513,\"start\":83512},{\"end\":83522,\"start\":83521},{\"end\":83535,\"start\":83534},{\"end\":83542,\"start\":83541},{\"end\":83553,\"start\":83552},{\"end\":83562,\"start\":83561},{\"end\":83574,\"start\":83573},{\"end\":83583,\"start\":83582},{\"end\":83596,\"start\":83595},{\"end\":83602,\"start\":83601},{\"end\":83919,\"start\":83918},{\"end\":83927,\"start\":83926},{\"end\":83936,\"start\":83935},{\"end\":83938,\"start\":83937},{\"end\":83946,\"start\":83945},{\"end\":84399,\"start\":84398},{\"end\":84406,\"start\":84405},{\"end\":84413,\"start\":84412},{\"end\":84424,\"start\":84423},{\"end\":84433,\"start\":84432},{\"end\":84442,\"start\":84441},{\"end\":84452,\"start\":84451},{\"end\":84454,\"start\":84453},{\"end\":84749,\"start\":84748},{\"end\":84756,\"start\":84755},{\"end\":84763,\"start\":84762},{\"end\":84772,\"start\":84771},{\"end\":84778,\"start\":84777},{\"end\":84787,\"start\":84786},{\"end\":84795,\"start\":84794},{\"end\":84803,\"start\":84802},{\"end\":84812,\"start\":84811},{\"end\":84827,\"start\":84826},{\"end\":85209,\"start\":85208},{\"end\":85220,\"start\":85219},{\"end\":85227,\"start\":85226},{\"end\":85233,\"start\":85232},{\"end\":85243,\"start\":85242},{\"end\":85245,\"start\":85244},{\"end\":85254,\"start\":85253},{\"end\":85261,\"start\":85260},{\"end\":85269,\"start\":85268},{\"end\":85271,\"start\":85270},{\"end\":85277,\"start\":85276},{\"end\":85285,\"start\":85284},{\"end\":85546,\"start\":85545},{\"end\":85553,\"start\":85552},{\"end\":85562,\"start\":85561},{\"end\":85577,\"start\":85576},{\"end\":85841,\"start\":85840},{\"end\":85855,\"start\":85854},{\"end\":85864,\"start\":85863},{\"end\":85873,\"start\":85872},{\"end\":85883,\"start\":85882},{\"end\":86413,\"start\":86412},{\"end\":86423,\"start\":86422},{\"end\":86429,\"start\":86428},{\"end\":86438,\"start\":86437},{\"end\":86449,\"start\":86448},{\"end\":86463,\"start\":86462},{\"end\":86474,\"start\":86473},{\"end\":86483,\"start\":86482},{\"end\":86494,\"start\":86493},{\"end\":86503,\"start\":86502},{\"end\":86820,\"start\":86819},{\"end\":86828,\"start\":86827},{\"end\":86836,\"start\":86835},{\"end\":86838,\"start\":86837},{\"end\":86846,\"start\":86845},{\"end\":86853,\"start\":86852},{\"end\":86860,\"start\":86859},{\"end\":87170,\"start\":87169},{\"end\":87178,\"start\":87177},{\"end\":87184,\"start\":87183},{\"end\":87190,\"start\":87189},{\"end\":87200,\"start\":87199},{\"end\":87442,\"start\":87441},{\"end\":87452,\"start\":87451},{\"end\":87466,\"start\":87465},{\"end\":87476,\"start\":87475},{\"end\":87488,\"start\":87487},{\"end\":87886,\"start\":87885},{\"end\":87894,\"start\":87893},{\"end\":87905,\"start\":87904},{\"end\":87918,\"start\":87917},{\"end\":87928,\"start\":87927},{\"end\":87940,\"start\":87939},{\"end\":87952,\"start\":87951},{\"end\":87960,\"start\":87959},{\"end\":87968,\"start\":87967},{\"end\":87979,\"start\":87978},{\"end\":88323,\"start\":88322},{\"end\":88330,\"start\":88329},{\"end\":88580,\"start\":88579},{\"end\":88590,\"start\":88589},{\"end\":88601,\"start\":88600},{\"end\":88612,\"start\":88611},{\"end\":88619,\"start\":88618},{\"end\":88629,\"start\":88628},{\"end\":88639,\"start\":88638},{\"end\":88647,\"start\":88646},{\"end\":88653,\"start\":88652},{\"end\":88655,\"start\":88654},{\"end\":88997,\"start\":88996},{\"end\":89005,\"start\":89004},{\"end\":89015,\"start\":89014},{\"end\":89025,\"start\":89024},{\"end\":89027,\"start\":89026},{\"end\":89035,\"start\":89034},{\"end\":89047,\"start\":89046},{\"end\":89059,\"start\":89058},{\"end\":89070,\"start\":89069},{\"end\":89082,\"start\":89081},{\"end\":89084,\"start\":89083},{\"end\":89092,\"start\":89091},{\"end\":89359,\"start\":89358},{\"end\":89366,\"start\":89365},{\"end\":89376,\"start\":89375},{\"end\":89385,\"start\":89384},{\"end\":89653,\"start\":89652},{\"end\":89655,\"start\":89654},{\"end\":89663,\"start\":89662},{\"end\":89670,\"start\":89669},{\"end\":89679,\"start\":89678},{\"end\":89690,\"start\":89689},{\"end\":89698,\"start\":89697},{\"end\":89709,\"start\":89708},{\"end\":89721,\"start\":89720},{\"end\":89723,\"start\":89722},{\"end\":89735,\"start\":89734},{\"end\":89743,\"start\":89742},{\"end\":90118,\"start\":90117},{\"end\":90127,\"start\":90126},{\"end\":90140,\"start\":90139},{\"end\":90149,\"start\":90148},{\"end\":90159,\"start\":90158},{\"end\":90165,\"start\":90164},{\"end\":90177,\"start\":90176},{\"end\":90186,\"start\":90185},{\"end\":90188,\"start\":90187},{\"end\":90410,\"start\":90409},{\"end\":90423,\"start\":90422},{\"end\":90434,\"start\":90433},{\"end\":90442,\"start\":90441},{\"end\":90453,\"start\":90452},{\"end\":90472,\"start\":90468},{\"end\":90481,\"start\":90480},{\"end\":90488,\"start\":90487},{\"end\":90495,\"start\":90494},{\"end\":90504,\"start\":90503},{\"end\":90789,\"start\":90788},{\"end\":90800,\"start\":90799},{\"end\":90810,\"start\":90809},{\"end\":90821,\"start\":90820},{\"end\":90836,\"start\":90832},{\"end\":90847,\"start\":90846},{\"end\":90858,\"start\":90857},{\"end\":90869,\"start\":90868},{\"end\":90878,\"start\":90877},{\"end\":90888,\"start\":90887},{\"end\":91193,\"start\":91192},{\"end\":91201,\"start\":91200},{\"end\":91210,\"start\":91209},{\"end\":91221,\"start\":91220},{\"end\":91229,\"start\":91228},{\"end\":91237,\"start\":91236},{\"end\":91239,\"start\":91238},{\"end\":91633,\"start\":91632},{\"end\":91641,\"start\":91640},{\"end\":91650,\"start\":91649},{\"end\":91660,\"start\":91659},{\"end\":91667,\"start\":91666},{\"end\":91669,\"start\":91668},{\"end\":91678,\"start\":91677},{\"end\":91690,\"start\":91689},{\"end\":92016,\"start\":92015},{\"end\":92024,\"start\":92023},{\"end\":92034,\"start\":92033},{\"end\":92054,\"start\":92053},{\"end\":92063,\"start\":92062},{\"end\":92074,\"start\":92073},{\"end\":92087,\"start\":92086},{\"end\":92096,\"start\":92095},{\"end\":92098,\"start\":92097},{\"end\":92114,\"start\":92113},{\"end\":92122,\"start\":92121},{\"end\":92467,\"start\":92466},{\"end\":92475,\"start\":92474},{\"end\":92485,\"start\":92484},{\"end\":92505,\"start\":92504},{\"end\":92514,\"start\":92513},{\"end\":92525,\"start\":92524},{\"end\":92533,\"start\":92532},{\"end\":92542,\"start\":92541},{\"end\":92544,\"start\":92543},{\"end\":92560,\"start\":92559},{\"end\":92573,\"start\":92572},{\"end\":93128,\"start\":93127},{\"end\":93135,\"start\":93134},{\"end\":93144,\"start\":93143},{\"end\":93146,\"start\":93145},{\"end\":93154,\"start\":93153},{\"end\":93161,\"start\":93160},{\"end\":93163,\"start\":93162},{\"end\":93169,\"start\":93168},{\"end\":93179,\"start\":93178},{\"end\":93185,\"start\":93184},{\"end\":93187,\"start\":93186},{\"end\":93194,\"start\":93193},{\"end\":93196,\"start\":93195},{\"end\":93484,\"start\":93483},{\"end\":93491,\"start\":93490},{\"end\":93499,\"start\":93498},{\"end\":93513,\"start\":93512},{\"end\":93522,\"start\":93521},{\"end\":93529,\"start\":93528},{\"end\":93531,\"start\":93530},{\"end\":93538,\"start\":93537},{\"end\":93540,\"start\":93539},{\"end\":93546,\"start\":93545},{\"end\":93898,\"start\":93897},{\"end\":93906,\"start\":93905},{\"end\":93915,\"start\":93914},{\"end\":93923,\"start\":93922},{\"end\":93935,\"start\":93934},{\"end\":93947,\"start\":93946},{\"end\":93954,\"start\":93953},{\"end\":93964,\"start\":93963},{\"end\":93973,\"start\":93972},{\"end\":93981,\"start\":93980},{\"end\":94325,\"start\":94324},{\"end\":94337,\"start\":94336},{\"end\":94349,\"start\":94348},{\"end\":94364,\"start\":94363},{\"end\":94374,\"start\":94373},{\"end\":94385,\"start\":94384},{\"end\":94711,\"start\":94710},{\"end\":94719,\"start\":94718},{\"end\":94726,\"start\":94725},{\"end\":94736,\"start\":94735},{\"end\":94747,\"start\":94746},{\"end\":94966,\"start\":94965},{\"end\":94973,\"start\":94972},{\"end\":94975,\"start\":94974},{\"end\":94981,\"start\":94980},{\"end\":94988,\"start\":94987},{\"end\":94997,\"start\":94996},{\"end\":95008,\"start\":95007},{\"end\":95023,\"start\":95019},{\"end\":95029,\"start\":95028},{\"end\":95038,\"start\":95037},{\"end\":95045,\"start\":95044},{\"end\":95047,\"start\":95046},{\"end\":95506,\"start\":95505},{\"end\":95514,\"start\":95513},{\"end\":95520,\"start\":95519},{\"end\":95529,\"start\":95528},{\"end\":95539,\"start\":95538},{\"end\":95548,\"start\":95547},{\"end\":95565,\"start\":95564},{\"end\":95567,\"start\":95566},{\"end\":96104,\"start\":96103},{\"end\":96111,\"start\":96110},{\"end\":96113,\"start\":96112},{\"end\":96126,\"start\":96125},{\"end\":96135,\"start\":96134},{\"end\":96141,\"start\":96140},{\"end\":96147,\"start\":96146},{\"end\":96469,\"start\":96468},{\"end\":96471,\"start\":96470},{\"end\":96480,\"start\":96479},{\"end\":96492,\"start\":96491},{\"end\":96710,\"start\":96709},{\"end\":96718,\"start\":96717},{\"end\":96725,\"start\":96724},{\"end\":96731,\"start\":96730},{\"end\":96739,\"start\":96738},{\"end\":96746,\"start\":96745},{\"end\":96754,\"start\":96753},{\"end\":96762,\"start\":96761},{\"end\":96768,\"start\":96767},{\"end\":96777,\"start\":96776},{\"end\":97039,\"start\":97038},{\"end\":97048,\"start\":97047},{\"end\":97058,\"start\":97057},{\"end\":97067,\"start\":97066},{\"end\":97078,\"start\":97077},{\"end\":97086,\"start\":97085},{\"end\":97094,\"start\":97093},{\"end\":97103,\"start\":97102},{\"end\":97111,\"start\":97110},{\"end\":97117,\"start\":97116},{\"end\":97119,\"start\":97118},{\"end\":97496,\"start\":97495},{\"end\":97505,\"start\":97504},{\"end\":97512,\"start\":97511},{\"end\":97521,\"start\":97520}]", "bib_author_last_name": "[{\"end\":71607,\"start\":71605},{\"end\":71613,\"start\":71611},{\"end\":71620,\"start\":71617},{\"end\":71627,\"start\":71624},{\"end\":71635,\"start\":71631},{\"end\":71641,\"start\":71639},{\"end\":71649,\"start\":71645},{\"end\":71658,\"start\":71653},{\"end\":71668,\"start\":71665},{\"end\":71958,\"start\":71952},{\"end\":71965,\"start\":71962},{\"end\":71973,\"start\":71969},{\"end\":71982,\"start\":71977},{\"end\":71993,\"start\":71986},{\"end\":72005,\"start\":71997},{\"end\":72014,\"start\":72009},{\"end\":72024,\"start\":72018},{\"end\":72032,\"start\":72028},{\"end\":72044,\"start\":72036},{\"end\":72297,\"start\":72293},{\"end\":72305,\"start\":72301},{\"end\":72316,\"start\":72312},{\"end\":72326,\"start\":72320},{\"end\":72336,\"start\":72330},{\"end\":72347,\"start\":72342},{\"end\":72357,\"start\":72351},{\"end\":72364,\"start\":72361},{\"end\":72374,\"start\":72370},{\"end\":72383,\"start\":72378},{\"end\":72847,\"start\":72844},{\"end\":72856,\"start\":72851},{\"end\":72867,\"start\":72860},{\"end\":72877,\"start\":72871},{\"end\":72885,\"start\":72881},{\"end\":72897,\"start\":72889},{\"end\":72906,\"start\":72901},{\"end\":72914,\"start\":72910},{\"end\":72925,\"start\":72918},{\"end\":72937,\"start\":72929},{\"end\":73219,\"start\":73216},{\"end\":73231,\"start\":73223},{\"end\":73240,\"start\":73235},{\"end\":73250,\"start\":73244},{\"end\":73261,\"start\":73254},{\"end\":73270,\"start\":73265},{\"end\":73278,\"start\":73274},{\"end\":73288,\"start\":73282},{\"end\":73302,\"start\":73292},{\"end\":73314,\"start\":73306},{\"end\":73688,\"start\":73682},{\"end\":73697,\"start\":73692},{\"end\":73715,\"start\":73701},{\"end\":73729,\"start\":73719},{\"end\":74139,\"start\":74131},{\"end\":74153,\"start\":74143},{\"end\":74164,\"start\":74157},{\"end\":74175,\"start\":74168},{\"end\":74191,\"start\":74179},{\"end\":74203,\"start\":74195},{\"end\":74213,\"start\":74209},{\"end\":74224,\"start\":74217},{\"end\":74239,\"start\":74230},{\"end\":74247,\"start\":74243},{\"end\":74618,\"start\":74609},{\"end\":74630,\"start\":74624},{\"end\":74639,\"start\":74634},{\"end\":74649,\"start\":74643},{\"end\":74658,\"start\":74653},{\"end\":74665,\"start\":74662},{\"end\":74680,\"start\":74671},{\"end\":74688,\"start\":74684},{\"end\":74700,\"start\":74692},{\"end\":74713,\"start\":74704},{\"end\":75028,\"start\":75024},{\"end\":75034,\"start\":75032},{\"end\":75043,\"start\":75038},{\"end\":75055,\"start\":75047},{\"end\":75350,\"start\":75344},{\"end\":75356,\"start\":75354},{\"end\":75363,\"start\":75360},{\"end\":75372,\"start\":75367},{\"end\":75378,\"start\":75376},{\"end\":75387,\"start\":75382},{\"end\":75396,\"start\":75391},{\"end\":75406,\"start\":75400},{\"end\":75416,\"start\":75410},{\"end\":75430,\"start\":75422},{\"end\":75440,\"start\":75434},{\"end\":75450,\"start\":75446},{\"end\":75835,\"start\":75825},{\"end\":75844,\"start\":75839},{\"end\":75853,\"start\":75848},{\"end\":75863,\"start\":75857},{\"end\":75871,\"start\":75867},{\"end\":75881,\"start\":75875},{\"end\":76128,\"start\":76123},{\"end\":76135,\"start\":76132},{\"end\":76146,\"start\":76139},{\"end\":76154,\"start\":76150},{\"end\":76161,\"start\":76158},{\"end\":76170,\"start\":76165},{\"end\":76176,\"start\":76174},{\"end\":76184,\"start\":76180},{\"end\":76196,\"start\":76188},{\"end\":76206,\"start\":76200},{\"end\":76504,\"start\":76496},{\"end\":76519,\"start\":76508},{\"end\":76748,\"start\":76740},{\"end\":76757,\"start\":76752},{\"end\":76768,\"start\":76761},{\"end\":76783,\"start\":76772},{\"end\":77227,\"start\":77219},{\"end\":77236,\"start\":77231},{\"end\":77248,\"start\":77240},{\"end\":77263,\"start\":77252},{\"end\":77513,\"start\":77510},{\"end\":77779,\"start\":77776},{\"end\":77956,\"start\":77949},{\"end\":77968,\"start\":77960},{\"end\":77979,\"start\":77972},{\"end\":77991,\"start\":77983},{\"end\":78204,\"start\":78202},{\"end\":78213,\"start\":78211},{\"end\":78222,\"start\":78217},{\"end\":78229,\"start\":78226},{\"end\":78239,\"start\":78231},{\"end\":78461,\"start\":78457},{\"end\":78474,\"start\":78465},{\"end\":78481,\"start\":78478},{\"end\":78492,\"start\":78485},{\"end\":78502,\"start\":78496},{\"end\":78512,\"start\":78506},{\"end\":78520,\"start\":78516},{\"end\":78813,\"start\":78807},{\"end\":78825,\"start\":78817},{\"end\":78836,\"start\":78829},{\"end\":78849,\"start\":78840},{\"end\":78859,\"start\":78853},{\"end\":78869,\"start\":78863},{\"end\":78877,\"start\":78873},{\"end\":78890,\"start\":78881},{\"end\":78902,\"start\":78894},{\"end\":78913,\"start\":78906},{\"end\":79194,\"start\":79184},{\"end\":79209,\"start\":79198},{\"end\":79217,\"start\":79213},{\"end\":79229,\"start\":79221},{\"end\":79241,\"start\":79235},{\"end\":79252,\"start\":79247},{\"end\":79592,\"start\":79583},{\"end\":79601,\"start\":79596},{\"end\":79868,\"start\":79859},{\"end\":79877,\"start\":79872},{\"end\":79887,\"start\":79881},{\"end\":79894,\"start\":79891},{\"end\":79905,\"start\":79898},{\"end\":79913,\"start\":79909},{\"end\":79927,\"start\":79917},{\"end\":80240,\"start\":80232},{\"end\":80248,\"start\":80244},{\"end\":80254,\"start\":80252},{\"end\":80264,\"start\":80258},{\"end\":80272,\"start\":80268},{\"end\":80578,\"start\":80570},{\"end\":80589,\"start\":80582},{\"end\":80597,\"start\":80593},{\"end\":80607,\"start\":80601},{\"end\":80861,\"start\":80854},{\"end\":80872,\"start\":80865},{\"end\":80887,\"start\":80876},{\"end\":80898,\"start\":80891},{\"end\":80916,\"start\":80902},{\"end\":80928,\"start\":80920},{\"end\":80941,\"start\":80932},{\"end\":80950,\"start\":80945},{\"end\":81248,\"start\":81246},{\"end\":81256,\"start\":81252},{\"end\":81266,\"start\":81260},{\"end\":81279,\"start\":81270},{\"end\":81285,\"start\":81283},{\"end\":81293,\"start\":81289},{\"end\":81301,\"start\":81297},{\"end\":81309,\"start\":81305},{\"end\":81667,\"start\":81663},{\"end\":81676,\"start\":81673},{\"end\":81688,\"start\":81680},{\"end\":81700,\"start\":81692},{\"end\":81709,\"start\":81704},{\"end\":81715,\"start\":81713},{\"end\":81726,\"start\":81719},{\"end\":81734,\"start\":81730},{\"end\":81741,\"start\":81738},{\"end\":81752,\"start\":81747},{\"end\":82127,\"start\":82121},{\"end\":82137,\"start\":82131},{\"end\":82149,\"start\":82141},{\"end\":82160,\"start\":82153},{\"end\":82378,\"start\":82374},{\"end\":82389,\"start\":82382},{\"end\":82402,\"start\":82393},{\"end\":82418,\"start\":82406},{\"end\":82428,\"start\":82425},{\"end\":82439,\"start\":82432},{\"end\":82450,\"start\":82443},{\"end\":82459,\"start\":82456},{\"end\":82470,\"start\":82463},{\"end\":82480,\"start\":82474},{\"end\":82870,\"start\":82865},{\"end\":83047,\"start\":83041},{\"end\":83058,\"start\":83051},{\"end\":83070,\"start\":83062},{\"end\":83307,\"start\":83305},{\"end\":83316,\"start\":83311},{\"end\":83519,\"start\":83514},{\"end\":83532,\"start\":83523},{\"end\":83539,\"start\":83536},{\"end\":83550,\"start\":83543},{\"end\":83559,\"start\":83554},{\"end\":83571,\"start\":83563},{\"end\":83580,\"start\":83575},{\"end\":83593,\"start\":83584},{\"end\":83599,\"start\":83597},{\"end\":83608,\"start\":83603},{\"end\":83924,\"start\":83920},{\"end\":83933,\"start\":83928},{\"end\":83943,\"start\":83939},{\"end\":83954,\"start\":83947},{\"end\":84403,\"start\":84400},{\"end\":84410,\"start\":84407},{\"end\":84421,\"start\":84414},{\"end\":84430,\"start\":84425},{\"end\":84439,\"start\":84434},{\"end\":84449,\"start\":84443},{\"end\":84753,\"start\":84750},{\"end\":84760,\"start\":84757},{\"end\":84769,\"start\":84764},{\"end\":84775,\"start\":84773},{\"end\":84784,\"start\":84779},{\"end\":84792,\"start\":84788},{\"end\":84800,\"start\":84796},{\"end\":84809,\"start\":84804},{\"end\":84824,\"start\":84813},{\"end\":84836,\"start\":84828},{\"end\":84845,\"start\":84838},{\"end\":85217,\"start\":85210},{\"end\":85224,\"start\":85221},{\"end\":85230,\"start\":85228},{\"end\":85240,\"start\":85234},{\"end\":85251,\"start\":85246},{\"end\":85258,\"start\":85255},{\"end\":85266,\"start\":85262},{\"end\":85274,\"start\":85272},{\"end\":85282,\"start\":85278},{\"end\":85289,\"start\":85286},{\"end\":85550,\"start\":85547},{\"end\":85559,\"start\":85554},{\"end\":85574,\"start\":85563},{\"end\":85588,\"start\":85578},{\"end\":85852,\"start\":85842},{\"end\":85861,\"start\":85856},{\"end\":85870,\"start\":85865},{\"end\":85880,\"start\":85874},{\"end\":85893,\"start\":85884},{\"end\":86290,\"start\":86284},{\"end\":86420,\"start\":86414},{\"end\":86426,\"start\":86424},{\"end\":86435,\"start\":86430},{\"end\":86446,\"start\":86439},{\"end\":86460,\"start\":86450},{\"end\":86471,\"start\":86464},{\"end\":86480,\"start\":86475},{\"end\":86491,\"start\":86484},{\"end\":86500,\"start\":86495},{\"end\":86507,\"start\":86504},{\"end\":86825,\"start\":86821},{\"end\":86833,\"start\":86829},{\"end\":86843,\"start\":86839},{\"end\":86850,\"start\":86847},{\"end\":86857,\"start\":86854},{\"end\":86864,\"start\":86861},{\"end\":87175,\"start\":87171},{\"end\":87181,\"start\":87179},{\"end\":87187,\"start\":87185},{\"end\":87197,\"start\":87191},{\"end\":87204,\"start\":87201},{\"end\":87449,\"start\":87443},{\"end\":87463,\"start\":87453},{\"end\":87473,\"start\":87467},{\"end\":87485,\"start\":87477},{\"end\":87498,\"start\":87489},{\"end\":87891,\"start\":87887},{\"end\":87902,\"start\":87895},{\"end\":87915,\"start\":87906},{\"end\":87925,\"start\":87919},{\"end\":87937,\"start\":87929},{\"end\":87949,\"start\":87941},{\"end\":87957,\"start\":87953},{\"end\":87965,\"start\":87961},{\"end\":87976,\"start\":87969},{\"end\":87984,\"start\":87980},{\"end\":88327,\"start\":88324},{\"end\":88337,\"start\":88331},{\"end\":88587,\"start\":88581},{\"end\":88598,\"start\":88591},{\"end\":88609,\"start\":88602},{\"end\":88616,\"start\":88613},{\"end\":88626,\"start\":88620},{\"end\":88636,\"start\":88630},{\"end\":88644,\"start\":88640},{\"end\":88650,\"start\":88648},{\"end\":88659,\"start\":88656},{\"end\":89002,\"start\":88998},{\"end\":89012,\"start\":89006},{\"end\":89022,\"start\":89016},{\"end\":89032,\"start\":89028},{\"end\":89044,\"start\":89036},{\"end\":89056,\"start\":89048},{\"end\":89067,\"start\":89060},{\"end\":89079,\"start\":89071},{\"end\":89089,\"start\":89085},{\"end\":89097,\"start\":89093},{\"end\":89363,\"start\":89360},{\"end\":89373,\"start\":89367},{\"end\":89382,\"start\":89377},{\"end\":89390,\"start\":89386},{\"end\":89660,\"start\":89656},{\"end\":89667,\"start\":89664},{\"end\":89676,\"start\":89671},{\"end\":89687,\"start\":89680},{\"end\":89695,\"start\":89691},{\"end\":89706,\"start\":89699},{\"end\":89718,\"start\":89710},{\"end\":89732,\"start\":89724},{\"end\":89740,\"start\":89736},{\"end\":89749,\"start\":89744},{\"end\":90124,\"start\":90119},{\"end\":90137,\"start\":90128},{\"end\":90146,\"start\":90141},{\"end\":90156,\"start\":90150},{\"end\":90162,\"start\":90160},{\"end\":90174,\"start\":90166},{\"end\":90183,\"start\":90178},{\"end\":90198,\"start\":90189},{\"end\":90420,\"start\":90411},{\"end\":90431,\"start\":90424},{\"end\":90439,\"start\":90435},{\"end\":90450,\"start\":90443},{\"end\":90466,\"start\":90454},{\"end\":90478,\"start\":90473},{\"end\":90485,\"start\":90482},{\"end\":90492,\"start\":90489},{\"end\":90501,\"start\":90496},{\"end\":90507,\"start\":90505},{\"end\":90797,\"start\":90790},{\"end\":90807,\"start\":90801},{\"end\":90818,\"start\":90811},{\"end\":90830,\"start\":90822},{\"end\":90844,\"start\":90837},{\"end\":90855,\"start\":90848},{\"end\":90866,\"start\":90859},{\"end\":90875,\"start\":90870},{\"end\":90885,\"start\":90879},{\"end\":90894,\"start\":90889},{\"end\":91198,\"start\":91194},{\"end\":91207,\"start\":91202},{\"end\":91218,\"start\":91211},{\"end\":91226,\"start\":91222},{\"end\":91234,\"start\":91230},{\"end\":91246,\"start\":91240},{\"end\":91638,\"start\":91634},{\"end\":91647,\"start\":91642},{\"end\":91657,\"start\":91651},{\"end\":91664,\"start\":91661},{\"end\":91675,\"start\":91670},{\"end\":91687,\"start\":91679},{\"end\":91701,\"start\":91691},{\"end\":92021,\"start\":92017},{\"end\":92031,\"start\":92025},{\"end\":92051,\"start\":92035},{\"end\":92060,\"start\":92055},{\"end\":92071,\"start\":92064},{\"end\":92084,\"start\":92075},{\"end\":92093,\"start\":92088},{\"end\":92111,\"start\":92099},{\"end\":92119,\"start\":92115},{\"end\":92127,\"start\":92123},{\"end\":92472,\"start\":92468},{\"end\":92482,\"start\":92476},{\"end\":92502,\"start\":92486},{\"end\":92511,\"start\":92506},{\"end\":92522,\"start\":92515},{\"end\":92530,\"start\":92526},{\"end\":92539,\"start\":92534},{\"end\":92557,\"start\":92545},{\"end\":92570,\"start\":92561},{\"end\":92578,\"start\":92574},{\"end\":93132,\"start\":93129},{\"end\":93141,\"start\":93136},{\"end\":93151,\"start\":93147},{\"end\":93158,\"start\":93155},{\"end\":93166,\"start\":93164},{\"end\":93176,\"start\":93170},{\"end\":93182,\"start\":93180},{\"end\":93191,\"start\":93188},{\"end\":93199,\"start\":93197},{\"end\":93488,\"start\":93485},{\"end\":93496,\"start\":93492},{\"end\":93510,\"start\":93500},{\"end\":93519,\"start\":93514},{\"end\":93526,\"start\":93523},{\"end\":93535,\"start\":93532},{\"end\":93543,\"start\":93541},{\"end\":93551,\"start\":93547},{\"end\":93903,\"start\":93899},{\"end\":93912,\"start\":93907},{\"end\":93920,\"start\":93916},{\"end\":93932,\"start\":93924},{\"end\":93944,\"start\":93936},{\"end\":93951,\"start\":93948},{\"end\":93961,\"start\":93955},{\"end\":93970,\"start\":93965},{\"end\":93978,\"start\":93974},{\"end\":93991,\"start\":93982},{\"end\":94334,\"start\":94326},{\"end\":94346,\"start\":94338},{\"end\":94361,\"start\":94350},{\"end\":94371,\"start\":94365},{\"end\":94382,\"start\":94375},{\"end\":94393,\"start\":94386},{\"end\":94716,\"start\":94712},{\"end\":94723,\"start\":94720},{\"end\":94733,\"start\":94727},{\"end\":94744,\"start\":94737},{\"end\":94751,\"start\":94748},{\"end\":94970,\"start\":94967},{\"end\":94978,\"start\":94976},{\"end\":94985,\"start\":94982},{\"end\":94994,\"start\":94989},{\"end\":95005,\"start\":94998},{\"end\":95017,\"start\":95009},{\"end\":95026,\"start\":95024},{\"end\":95035,\"start\":95030},{\"end\":95042,\"start\":95039},{\"end\":95052,\"start\":95048},{\"end\":95511,\"start\":95507},{\"end\":95517,\"start\":95515},{\"end\":95526,\"start\":95521},{\"end\":95536,\"start\":95530},{\"end\":95545,\"start\":95540},{\"end\":95562,\"start\":95549},{\"end\":95575,\"start\":95568},{\"end\":96108,\"start\":96105},{\"end\":96123,\"start\":96114},{\"end\":96132,\"start\":96127},{\"end\":96138,\"start\":96136},{\"end\":96144,\"start\":96142},{\"end\":96150,\"start\":96148},{\"end\":96477,\"start\":96472},{\"end\":96489,\"start\":96481},{\"end\":96501,\"start\":96493},{\"end\":96715,\"start\":96711},{\"end\":96722,\"start\":96719},{\"end\":96728,\"start\":96726},{\"end\":96736,\"start\":96732},{\"end\":96743,\"start\":96740},{\"end\":96751,\"start\":96747},{\"end\":96759,\"start\":96755},{\"end\":96765,\"start\":96763},{\"end\":96774,\"start\":96769},{\"end\":96781,\"start\":96778},{\"end\":97045,\"start\":97040},{\"end\":97055,\"start\":97049},{\"end\":97064,\"start\":97059},{\"end\":97075,\"start\":97068},{\"end\":97083,\"start\":97079},{\"end\":97091,\"start\":97087},{\"end\":97100,\"start\":97095},{\"end\":97108,\"start\":97104},{\"end\":97114,\"start\":97112},{\"end\":97123,\"start\":97120},{\"end\":97502,\"start\":97497},{\"end\":97509,\"start\":97506},{\"end\":97518,\"start\":97513},{\"end\":97527,\"start\":97522},{\"end\":71607,\"start\":71605},{\"end\":71613,\"start\":71611},{\"end\":71620,\"start\":71617},{\"end\":71627,\"start\":71624},{\"end\":71635,\"start\":71631},{\"end\":71641,\"start\":71639},{\"end\":71649,\"start\":71645},{\"end\":71658,\"start\":71653},{\"end\":71668,\"start\":71665},{\"end\":71958,\"start\":71952},{\"end\":71965,\"start\":71962},{\"end\":71973,\"start\":71969},{\"end\":71982,\"start\":71977},{\"end\":71993,\"start\":71986},{\"end\":72005,\"start\":71997},{\"end\":72014,\"start\":72009},{\"end\":72024,\"start\":72018},{\"end\":72032,\"start\":72028},{\"end\":72044,\"start\":72036},{\"end\":72297,\"start\":72293},{\"end\":72305,\"start\":72301},{\"end\":72316,\"start\":72312},{\"end\":72326,\"start\":72320},{\"end\":72336,\"start\":72330},{\"end\":72347,\"start\":72342},{\"end\":72357,\"start\":72351},{\"end\":72364,\"start\":72361},{\"end\":72374,\"start\":72370},{\"end\":72383,\"start\":72378},{\"end\":72847,\"start\":72844},{\"end\":72856,\"start\":72851},{\"end\":72867,\"start\":72860},{\"end\":72877,\"start\":72871},{\"end\":72885,\"start\":72881},{\"end\":72897,\"start\":72889},{\"end\":72906,\"start\":72901},{\"end\":72914,\"start\":72910},{\"end\":72925,\"start\":72918},{\"end\":72937,\"start\":72929},{\"end\":73219,\"start\":73216},{\"end\":73231,\"start\":73223},{\"end\":73240,\"start\":73235},{\"end\":73250,\"start\":73244},{\"end\":73261,\"start\":73254},{\"end\":73270,\"start\":73265},{\"end\":73278,\"start\":73274},{\"end\":73288,\"start\":73282},{\"end\":73302,\"start\":73292},{\"end\":73314,\"start\":73306},{\"end\":73688,\"start\":73682},{\"end\":73697,\"start\":73692},{\"end\":73715,\"start\":73701},{\"end\":73729,\"start\":73719},{\"end\":74139,\"start\":74131},{\"end\":74153,\"start\":74143},{\"end\":74164,\"start\":74157},{\"end\":74175,\"start\":74168},{\"end\":74191,\"start\":74179},{\"end\":74203,\"start\":74195},{\"end\":74213,\"start\":74209},{\"end\":74224,\"start\":74217},{\"end\":74239,\"start\":74230},{\"end\":74247,\"start\":74243},{\"end\":74618,\"start\":74609},{\"end\":74630,\"start\":74624},{\"end\":74639,\"start\":74634},{\"end\":74649,\"start\":74643},{\"end\":74658,\"start\":74653},{\"end\":74665,\"start\":74662},{\"end\":74680,\"start\":74671},{\"end\":74688,\"start\":74684},{\"end\":74700,\"start\":74692},{\"end\":74713,\"start\":74704},{\"end\":75028,\"start\":75024},{\"end\":75034,\"start\":75032},{\"end\":75043,\"start\":75038},{\"end\":75055,\"start\":75047},{\"end\":75350,\"start\":75344},{\"end\":75356,\"start\":75354},{\"end\":75363,\"start\":75360},{\"end\":75372,\"start\":75367},{\"end\":75378,\"start\":75376},{\"end\":75387,\"start\":75382},{\"end\":75396,\"start\":75391},{\"end\":75406,\"start\":75400},{\"end\":75416,\"start\":75410},{\"end\":75430,\"start\":75422},{\"end\":75440,\"start\":75434},{\"end\":75450,\"start\":75446},{\"end\":75835,\"start\":75825},{\"end\":75844,\"start\":75839},{\"end\":75853,\"start\":75848},{\"end\":75863,\"start\":75857},{\"end\":75871,\"start\":75867},{\"end\":75881,\"start\":75875},{\"end\":76128,\"start\":76123},{\"end\":76135,\"start\":76132},{\"end\":76146,\"start\":76139},{\"end\":76154,\"start\":76150},{\"end\":76161,\"start\":76158},{\"end\":76170,\"start\":76165},{\"end\":76176,\"start\":76174},{\"end\":76184,\"start\":76180},{\"end\":76196,\"start\":76188},{\"end\":76206,\"start\":76200},{\"end\":76504,\"start\":76496},{\"end\":76519,\"start\":76508},{\"end\":76748,\"start\":76740},{\"end\":76757,\"start\":76752},{\"end\":76768,\"start\":76761},{\"end\":76783,\"start\":76772},{\"end\":77227,\"start\":77219},{\"end\":77236,\"start\":77231},{\"end\":77248,\"start\":77240},{\"end\":77263,\"start\":77252},{\"end\":77513,\"start\":77510},{\"end\":77779,\"start\":77776},{\"end\":77956,\"start\":77949},{\"end\":77968,\"start\":77960},{\"end\":77979,\"start\":77972},{\"end\":77991,\"start\":77983},{\"end\":78204,\"start\":78202},{\"end\":78213,\"start\":78211},{\"end\":78222,\"start\":78217},{\"end\":78229,\"start\":78226},{\"end\":78239,\"start\":78231},{\"end\":78461,\"start\":78457},{\"end\":78474,\"start\":78465},{\"end\":78481,\"start\":78478},{\"end\":78492,\"start\":78485},{\"end\":78502,\"start\":78496},{\"end\":78512,\"start\":78506},{\"end\":78520,\"start\":78516},{\"end\":78813,\"start\":78807},{\"end\":78825,\"start\":78817},{\"end\":78836,\"start\":78829},{\"end\":78849,\"start\":78840},{\"end\":78859,\"start\":78853},{\"end\":78869,\"start\":78863},{\"end\":78877,\"start\":78873},{\"end\":78890,\"start\":78881},{\"end\":78902,\"start\":78894},{\"end\":78913,\"start\":78906},{\"end\":79194,\"start\":79184},{\"end\":79209,\"start\":79198},{\"end\":79217,\"start\":79213},{\"end\":79229,\"start\":79221},{\"end\":79241,\"start\":79235},{\"end\":79252,\"start\":79247},{\"end\":79592,\"start\":79583},{\"end\":79601,\"start\":79596},{\"end\":79868,\"start\":79859},{\"end\":79877,\"start\":79872},{\"end\":79887,\"start\":79881},{\"end\":79894,\"start\":79891},{\"end\":79905,\"start\":79898},{\"end\":79913,\"start\":79909},{\"end\":79927,\"start\":79917},{\"end\":80240,\"start\":80232},{\"end\":80248,\"start\":80244},{\"end\":80254,\"start\":80252},{\"end\":80264,\"start\":80258},{\"end\":80272,\"start\":80268},{\"end\":80578,\"start\":80570},{\"end\":80589,\"start\":80582},{\"end\":80597,\"start\":80593},{\"end\":80607,\"start\":80601},{\"end\":80861,\"start\":80854},{\"end\":80872,\"start\":80865},{\"end\":80887,\"start\":80876},{\"end\":80898,\"start\":80891},{\"end\":80916,\"start\":80902},{\"end\":80928,\"start\":80920},{\"end\":80941,\"start\":80932},{\"end\":80950,\"start\":80945},{\"end\":81248,\"start\":81246},{\"end\":81256,\"start\":81252},{\"end\":81266,\"start\":81260},{\"end\":81279,\"start\":81270},{\"end\":81285,\"start\":81283},{\"end\":81293,\"start\":81289},{\"end\":81301,\"start\":81297},{\"end\":81309,\"start\":81305},{\"end\":81667,\"start\":81663},{\"end\":81676,\"start\":81673},{\"end\":81688,\"start\":81680},{\"end\":81700,\"start\":81692},{\"end\":81709,\"start\":81704},{\"end\":81715,\"start\":81713},{\"end\":81726,\"start\":81719},{\"end\":81734,\"start\":81730},{\"end\":81741,\"start\":81738},{\"end\":81752,\"start\":81747},{\"end\":82127,\"start\":82121},{\"end\":82137,\"start\":82131},{\"end\":82149,\"start\":82141},{\"end\":82160,\"start\":82153},{\"end\":82378,\"start\":82374},{\"end\":82389,\"start\":82382},{\"end\":82402,\"start\":82393},{\"end\":82418,\"start\":82406},{\"end\":82428,\"start\":82425},{\"end\":82439,\"start\":82432},{\"end\":82450,\"start\":82443},{\"end\":82459,\"start\":82456},{\"end\":82470,\"start\":82463},{\"end\":82480,\"start\":82474},{\"end\":82870,\"start\":82865},{\"end\":83047,\"start\":83041},{\"end\":83058,\"start\":83051},{\"end\":83070,\"start\":83062},{\"end\":83307,\"start\":83305},{\"end\":83316,\"start\":83311},{\"end\":83519,\"start\":83514},{\"end\":83532,\"start\":83523},{\"end\":83539,\"start\":83536},{\"end\":83550,\"start\":83543},{\"end\":83559,\"start\":83554},{\"end\":83571,\"start\":83563},{\"end\":83580,\"start\":83575},{\"end\":83593,\"start\":83584},{\"end\":83599,\"start\":83597},{\"end\":83608,\"start\":83603},{\"end\":83924,\"start\":83920},{\"end\":83933,\"start\":83928},{\"end\":83943,\"start\":83939},{\"end\":83954,\"start\":83947},{\"end\":84403,\"start\":84400},{\"end\":84410,\"start\":84407},{\"end\":84421,\"start\":84414},{\"end\":84430,\"start\":84425},{\"end\":84439,\"start\":84434},{\"end\":84449,\"start\":84443},{\"end\":84753,\"start\":84750},{\"end\":84760,\"start\":84757},{\"end\":84769,\"start\":84764},{\"end\":84775,\"start\":84773},{\"end\":84784,\"start\":84779},{\"end\":84792,\"start\":84788},{\"end\":84800,\"start\":84796},{\"end\":84809,\"start\":84804},{\"end\":84824,\"start\":84813},{\"end\":84836,\"start\":84828},{\"end\":84845,\"start\":84838},{\"end\":85217,\"start\":85210},{\"end\":85224,\"start\":85221},{\"end\":85230,\"start\":85228},{\"end\":85240,\"start\":85234},{\"end\":85251,\"start\":85246},{\"end\":85258,\"start\":85255},{\"end\":85266,\"start\":85262},{\"end\":85274,\"start\":85272},{\"end\":85282,\"start\":85278},{\"end\":85289,\"start\":85286},{\"end\":85550,\"start\":85547},{\"end\":85559,\"start\":85554},{\"end\":85574,\"start\":85563},{\"end\":85588,\"start\":85578},{\"end\":85852,\"start\":85842},{\"end\":85861,\"start\":85856},{\"end\":85870,\"start\":85865},{\"end\":85880,\"start\":85874},{\"end\":85893,\"start\":85884},{\"end\":86290,\"start\":86284},{\"end\":86420,\"start\":86414},{\"end\":86426,\"start\":86424},{\"end\":86435,\"start\":86430},{\"end\":86446,\"start\":86439},{\"end\":86460,\"start\":86450},{\"end\":86471,\"start\":86464},{\"end\":86480,\"start\":86475},{\"end\":86491,\"start\":86484},{\"end\":86500,\"start\":86495},{\"end\":86507,\"start\":86504},{\"end\":86825,\"start\":86821},{\"end\":86833,\"start\":86829},{\"end\":86843,\"start\":86839},{\"end\":86850,\"start\":86847},{\"end\":86857,\"start\":86854},{\"end\":86864,\"start\":86861},{\"end\":87175,\"start\":87171},{\"end\":87181,\"start\":87179},{\"end\":87187,\"start\":87185},{\"end\":87197,\"start\":87191},{\"end\":87204,\"start\":87201},{\"end\":87449,\"start\":87443},{\"end\":87463,\"start\":87453},{\"end\":87473,\"start\":87467},{\"end\":87485,\"start\":87477},{\"end\":87498,\"start\":87489},{\"end\":87891,\"start\":87887},{\"end\":87902,\"start\":87895},{\"end\":87915,\"start\":87906},{\"end\":87925,\"start\":87919},{\"end\":87937,\"start\":87929},{\"end\":87949,\"start\":87941},{\"end\":87957,\"start\":87953},{\"end\":87965,\"start\":87961},{\"end\":87976,\"start\":87969},{\"end\":87984,\"start\":87980},{\"end\":88327,\"start\":88324},{\"end\":88337,\"start\":88331},{\"end\":88587,\"start\":88581},{\"end\":88598,\"start\":88591},{\"end\":88609,\"start\":88602},{\"end\":88616,\"start\":88613},{\"end\":88626,\"start\":88620},{\"end\":88636,\"start\":88630},{\"end\":88644,\"start\":88640},{\"end\":88650,\"start\":88648},{\"end\":88659,\"start\":88656},{\"end\":89002,\"start\":88998},{\"end\":89012,\"start\":89006},{\"end\":89022,\"start\":89016},{\"end\":89032,\"start\":89028},{\"end\":89044,\"start\":89036},{\"end\":89056,\"start\":89048},{\"end\":89067,\"start\":89060},{\"end\":89079,\"start\":89071},{\"end\":89089,\"start\":89085},{\"end\":89097,\"start\":89093},{\"end\":89363,\"start\":89360},{\"end\":89373,\"start\":89367},{\"end\":89382,\"start\":89377},{\"end\":89390,\"start\":89386},{\"end\":89660,\"start\":89656},{\"end\":89667,\"start\":89664},{\"end\":89676,\"start\":89671},{\"end\":89687,\"start\":89680},{\"end\":89695,\"start\":89691},{\"end\":89706,\"start\":89699},{\"end\":89718,\"start\":89710},{\"end\":89732,\"start\":89724},{\"end\":89740,\"start\":89736},{\"end\":89749,\"start\":89744},{\"end\":90124,\"start\":90119},{\"end\":90137,\"start\":90128},{\"end\":90146,\"start\":90141},{\"end\":90156,\"start\":90150},{\"end\":90162,\"start\":90160},{\"end\":90174,\"start\":90166},{\"end\":90183,\"start\":90178},{\"end\":90198,\"start\":90189},{\"end\":90420,\"start\":90411},{\"end\":90431,\"start\":90424},{\"end\":90439,\"start\":90435},{\"end\":90450,\"start\":90443},{\"end\":90466,\"start\":90454},{\"end\":90478,\"start\":90473},{\"end\":90485,\"start\":90482},{\"end\":90492,\"start\":90489},{\"end\":90501,\"start\":90496},{\"end\":90507,\"start\":90505},{\"end\":90797,\"start\":90790},{\"end\":90807,\"start\":90801},{\"end\":90818,\"start\":90811},{\"end\":90830,\"start\":90822},{\"end\":90844,\"start\":90837},{\"end\":90855,\"start\":90848},{\"end\":90866,\"start\":90859},{\"end\":90875,\"start\":90870},{\"end\":90885,\"start\":90879},{\"end\":90894,\"start\":90889},{\"end\":91198,\"start\":91194},{\"end\":91207,\"start\":91202},{\"end\":91218,\"start\":91211},{\"end\":91226,\"start\":91222},{\"end\":91234,\"start\":91230},{\"end\":91246,\"start\":91240},{\"end\":91638,\"start\":91634},{\"end\":91647,\"start\":91642},{\"end\":91657,\"start\":91651},{\"end\":91664,\"start\":91661},{\"end\":91675,\"start\":91670},{\"end\":91687,\"start\":91679},{\"end\":91701,\"start\":91691},{\"end\":92021,\"start\":92017},{\"end\":92031,\"start\":92025},{\"end\":92051,\"start\":92035},{\"end\":92060,\"start\":92055},{\"end\":92071,\"start\":92064},{\"end\":92084,\"start\":92075},{\"end\":92093,\"start\":92088},{\"end\":92111,\"start\":92099},{\"end\":92119,\"start\":92115},{\"end\":92127,\"start\":92123},{\"end\":92472,\"start\":92468},{\"end\":92482,\"start\":92476},{\"end\":92502,\"start\":92486},{\"end\":92511,\"start\":92506},{\"end\":92522,\"start\":92515},{\"end\":92530,\"start\":92526},{\"end\":92539,\"start\":92534},{\"end\":92557,\"start\":92545},{\"end\":92570,\"start\":92561},{\"end\":92578,\"start\":92574},{\"end\":93132,\"start\":93129},{\"end\":93141,\"start\":93136},{\"end\":93151,\"start\":93147},{\"end\":93158,\"start\":93155},{\"end\":93166,\"start\":93164},{\"end\":93176,\"start\":93170},{\"end\":93182,\"start\":93180},{\"end\":93191,\"start\":93188},{\"end\":93199,\"start\":93197},{\"end\":93488,\"start\":93485},{\"end\":93496,\"start\":93492},{\"end\":93510,\"start\":93500},{\"end\":93519,\"start\":93514},{\"end\":93526,\"start\":93523},{\"end\":93535,\"start\":93532},{\"end\":93543,\"start\":93541},{\"end\":93551,\"start\":93547},{\"end\":93903,\"start\":93899},{\"end\":93912,\"start\":93907},{\"end\":93920,\"start\":93916},{\"end\":93932,\"start\":93924},{\"end\":93944,\"start\":93936},{\"end\":93951,\"start\":93948},{\"end\":93961,\"start\":93955},{\"end\":93970,\"start\":93965},{\"end\":93978,\"start\":93974},{\"end\":93991,\"start\":93982},{\"end\":94334,\"start\":94326},{\"end\":94346,\"start\":94338},{\"end\":94361,\"start\":94350},{\"end\":94371,\"start\":94365},{\"end\":94382,\"start\":94375},{\"end\":94393,\"start\":94386},{\"end\":94716,\"start\":94712},{\"end\":94723,\"start\":94720},{\"end\":94733,\"start\":94727},{\"end\":94744,\"start\":94737},{\"end\":94751,\"start\":94748},{\"end\":94970,\"start\":94967},{\"end\":94978,\"start\":94976},{\"end\":94985,\"start\":94982},{\"end\":94994,\"start\":94989},{\"end\":95005,\"start\":94998},{\"end\":95017,\"start\":95009},{\"end\":95026,\"start\":95024},{\"end\":95035,\"start\":95030},{\"end\":95042,\"start\":95039},{\"end\":95052,\"start\":95048},{\"end\":95511,\"start\":95507},{\"end\":95517,\"start\":95515},{\"end\":95526,\"start\":95521},{\"end\":95536,\"start\":95530},{\"end\":95545,\"start\":95540},{\"end\":95562,\"start\":95549},{\"end\":95575,\"start\":95568},{\"end\":96108,\"start\":96105},{\"end\":96123,\"start\":96114},{\"end\":96132,\"start\":96127},{\"end\":96138,\"start\":96136},{\"end\":96144,\"start\":96142},{\"end\":96150,\"start\":96148},{\"end\":96477,\"start\":96472},{\"end\":96489,\"start\":96481},{\"end\":96501,\"start\":96493},{\"end\":96715,\"start\":96711},{\"end\":96722,\"start\":96719},{\"end\":96728,\"start\":96726},{\"end\":96736,\"start\":96732},{\"end\":96743,\"start\":96740},{\"end\":96751,\"start\":96747},{\"end\":96759,\"start\":96755},{\"end\":96765,\"start\":96763},{\"end\":96774,\"start\":96769},{\"end\":96781,\"start\":96778},{\"end\":97045,\"start\":97040},{\"end\":97055,\"start\":97049},{\"end\":97064,\"start\":97059},{\"end\":97075,\"start\":97068},{\"end\":97083,\"start\":97079},{\"end\":97091,\"start\":97087},{\"end\":97100,\"start\":97095},{\"end\":97108,\"start\":97104},{\"end\":97114,\"start\":97112},{\"end\":97123,\"start\":97120},{\"end\":97502,\"start\":97497},{\"end\":97509,\"start\":97506},{\"end\":97518,\"start\":97513},{\"end\":97527,\"start\":97522}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2203.03131\",\"id\":\"b0\"},\"end\":71888,\"start\":71533},{\"attributes\":{\"doi\":\"arXiv:2112.00861\",\"id\":\"b1\"},\"end\":72287,\"start\":71890},{\"attributes\":{\"doi\":\"arXiv:2202.01279\",\"id\":\"b2\"},\"end\":72749,\"start\":72289},{\"attributes\":{\"doi\":\"arXiv:2204.05862\",\"id\":\"b3\"},\"end\":73212,\"start\":72751},{\"attributes\":{\"doi\":\"arXiv:2212.08073\",\"id\":\"b4\"},\"end\":73605,\"start\":73214},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":232040593},\"end\":74127,\"start\":73607},{\"attributes\":{\"doi\":\"arXiv:2304.01373\",\"id\":\"b6\"},\"end\":74605,\"start\":74129},{\"attributes\":{\"doi\":\"arXiv:2108.07258\",\"id\":\"b7\"},\"end\":75020,\"start\":74607},{\"attributes\":{\"doi\":\"arXiv:1604.06174\",\"id\":\"b8\"},\"end\":75262,\"start\":75022},{\"attributes\":{\"id\":\"b9\"},\"end\":75716,\"start\":75264},{\"attributes\":{\"id\":\"b10\"},\"end\":76070,\"start\":75718},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b11\"},\"end\":76432,\"start\":76072},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b12\"},\"end\":76681,\"start\":76434},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":251564521},\"end\":77169,\"start\":76683},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":238408308},\"end\":77504,\"start\":77171},{\"attributes\":{\"id\":\"b15\"},\"end\":77724,\"start\":77506},{\"attributes\":{\"id\":\"b16\"},\"end\":77862,\"start\":77726},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b17\"},\"end\":78198,\"start\":77864},{\"attributes\":{\"doi\":\"arXiv:2302.04166\",\"id\":\"b18\"},\"end\":78406,\"start\":78200},{\"attributes\":{\"id\":\"b19\"},\"end\":78733,\"start\":78408},{\"attributes\":{\"doi\":\"arXiv:2209.14375\",\"id\":\"b20\"},\"end\":79180,\"start\":78735},{\"attributes\":{\"doi\":\"arXiv:1803.02324\",\"id\":\"b21\"},\"end\":79520,\"start\":79182},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235356070},\"end\":79803,\"start\":79522},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221516475},\"end\":80182,\"start\":79805},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":127986954},\"end\":80489,\"start\":80184},{\"attributes\":{\"doi\":\"arXiv:2212.09689\",\"id\":\"b25\"},\"end\":80803,\"start\":80491},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":59599816},\"end\":81240,\"start\":80805},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b27\"},\"end\":81565,\"start\":81242},{\"attributes\":{\"doi\":\"arXiv:2212.12017\",\"id\":\"b28\"},\"end\":82028,\"start\":81567},{\"attributes\":{\"doi\":\"arXiv:2304.08460\",\"id\":\"b29\"},\"end\":82370,\"start\":82030},{\"attributes\":{\"doi\":\"arXiv:2304.07327\",\"id\":\"b30\"},\"end\":82826,\"start\":82372},{\"attributes\":{\"id\":\"b31\"},\"end\":82979,\"start\":82828},{\"attributes\":{\"doi\":\"arXiv:2104.08691\",\"id\":\"b32\"},\"end\":83238,\"start\":82981},{\"attributes\":{\"doi\":\"arXiv:2101.00190\",\"id\":\"b33\"},\"end\":83470,\"start\":83240},{\"attributes\":{\"doi\":\"arXiv:2211.09110\",\"id\":\"b34\"},\"end\":83833,\"start\":83472},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":244907059},\"end\":84307,\"start\":83835},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":248693283},\"end\":84746,\"start\":84309},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b37\"},\"end\":85124,\"start\":84748},{\"attributes\":{\"doi\":\"arXiv:2301.13688\",\"id\":\"b38\"},\"end\":85543,\"start\":85126},{\"attributes\":{\"doi\":\"arXiv:2110.15943\",\"id\":\"b39\"},\"end\":85789,\"start\":85545},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52115700},\"end\":86280,\"start\":85791},{\"attributes\":{\"id\":\"b41\"},\"end\":86341,\"start\":86282},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":246426909},\"end\":86817,\"start\":86343},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b43\"},\"end\":87167,\"start\":86819},{\"attributes\":{\"doi\":\"arXiv:2304.03277\",\"id\":\"b44\"},\"end\":87382,\"start\":87169},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":21382535},\"end\":87883,\"start\":87384},{\"attributes\":{\"doi\":\"arXiv:2211.05102\",\"id\":\"b46\"},\"end\":88255,\"start\":87885},{\"attributes\":{\"doi\":\"arXiv:2104.06599\",\"id\":\"b47\"},\"end\":88494,\"start\":88257},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b48\",\"matched_paper_id\":204838007},\"end\":88927,\"start\":88496},{\"attributes\":{\"doi\":\"arXiv:2110.08207\",\"id\":\"b49\"},\"end\":89356,\"start\":88929},{\"attributes\":{\"doi\":\"arXiv:2210.13312\",\"id\":\"b50\"},\"end\":89650,\"start\":89358},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b51\"},\"end\":90060,\"start\":89652},{\"attributes\":{\"id\":\"b52\"},\"end\":90407,\"start\":90062},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b53\"},\"end\":90786,\"start\":90409},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b54\"},\"end\":91190,\"start\":90788},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b55\"},\"end\":91557,\"start\":91192},{\"attributes\":{\"doi\":\"arXiv:2212.10560\",\"id\":\"b56\"},\"end\":91927,\"start\":91559},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":253098274},\"end\":92373,\"start\":91929},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":253098274},\"end\":93075,\"start\":92375},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b59\"},\"end\":93410,\"start\":93077},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":246411621},\"end\":93821,\"start\":93412},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b61\"},\"end\":94248,\"start\":93823},{\"attributes\":{\"doi\":\"arXiv:2304.13013\",\"id\":\"b62\"},\"end\":94618,\"start\":94250},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b63\"},\"end\":94963,\"start\":94620},{\"attributes\":{\"doi\":\"arXiv:2201.05966\",\"id\":\"b64\"},\"end\":95428,\"start\":94965},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":52822214},\"end\":96008,\"start\":95430},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b66\"},\"end\":96373,\"start\":96010},{\"attributes\":{\"doi\":\"arXiv:2106.10199\",\"id\":\"b67\"},\"end\":96707,\"start\":96375},{\"attributes\":{\"doi\":\"arXiv:2210.02414\",\"id\":\"b68\"},\"end\":97036,\"start\":96709},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b69\"},\"end\":97395,\"start\":97038},{\"attributes\":{\"doi\":\"arXiv:2104.04670\",\"id\":\"b70\"},\"end\":97737,\"start\":97397},{\"attributes\":{\"doi\":\"arXiv:2203.03131\",\"id\":\"b0\"},\"end\":71888,\"start\":71533},{\"attributes\":{\"doi\":\"arXiv:2112.00861\",\"id\":\"b1\"},\"end\":72287,\"start\":71890},{\"attributes\":{\"doi\":\"arXiv:2202.01279\",\"id\":\"b2\"},\"end\":72749,\"start\":72289},{\"attributes\":{\"doi\":\"arXiv:2204.05862\",\"id\":\"b3\"},\"end\":73212,\"start\":72751},{\"attributes\":{\"doi\":\"arXiv:2212.08073\",\"id\":\"b4\"},\"end\":73605,\"start\":73214},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":232040593},\"end\":74127,\"start\":73607},{\"attributes\":{\"doi\":\"arXiv:2304.01373\",\"id\":\"b6\"},\"end\":74605,\"start\":74129},{\"attributes\":{\"doi\":\"arXiv:2108.07258\",\"id\":\"b7\"},\"end\":75020,\"start\":74607},{\"attributes\":{\"doi\":\"arXiv:1604.06174\",\"id\":\"b8\"},\"end\":75262,\"start\":75022},{\"attributes\":{\"id\":\"b9\"},\"end\":75716,\"start\":75264},{\"attributes\":{\"id\":\"b10\"},\"end\":76070,\"start\":75718},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b11\"},\"end\":76432,\"start\":76072},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b12\"},\"end\":76681,\"start\":76434},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":251564521},\"end\":77169,\"start\":76683},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":238408308},\"end\":77504,\"start\":77171},{\"attributes\":{\"id\":\"b15\"},\"end\":77724,\"start\":77506},{\"attributes\":{\"id\":\"b16\"},\"end\":77862,\"start\":77726},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b17\"},\"end\":78198,\"start\":77864},{\"attributes\":{\"doi\":\"arXiv:2302.04166\",\"id\":\"b18\"},\"end\":78406,\"start\":78200},{\"attributes\":{\"id\":\"b19\"},\"end\":78733,\"start\":78408},{\"attributes\":{\"doi\":\"arXiv:2209.14375\",\"id\":\"b20\"},\"end\":79180,\"start\":78735},{\"attributes\":{\"doi\":\"arXiv:1803.02324\",\"id\":\"b21\"},\"end\":79520,\"start\":79182},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235356070},\"end\":79803,\"start\":79522},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221516475},\"end\":80182,\"start\":79805},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":127986954},\"end\":80489,\"start\":80184},{\"attributes\":{\"doi\":\"arXiv:2212.09689\",\"id\":\"b25\"},\"end\":80803,\"start\":80491},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":59599816},\"end\":81240,\"start\":80805},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b27\"},\"end\":81565,\"start\":81242},{\"attributes\":{\"doi\":\"arXiv:2212.12017\",\"id\":\"b28\"},\"end\":82028,\"start\":81567},{\"attributes\":{\"doi\":\"arXiv:2304.08460\",\"id\":\"b29\"},\"end\":82370,\"start\":82030},{\"attributes\":{\"doi\":\"arXiv:2304.07327\",\"id\":\"b30\"},\"end\":82826,\"start\":82372},{\"attributes\":{\"id\":\"b31\"},\"end\":82979,\"start\":82828},{\"attributes\":{\"doi\":\"arXiv:2104.08691\",\"id\":\"b32\"},\"end\":83238,\"start\":82981},{\"attributes\":{\"doi\":\"arXiv:2101.00190\",\"id\":\"b33\"},\"end\":83470,\"start\":83240},{\"attributes\":{\"doi\":\"arXiv:2211.09110\",\"id\":\"b34\"},\"end\":83833,\"start\":83472},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":244907059},\"end\":84307,\"start\":83835},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":248693283},\"end\":84746,\"start\":84309},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b37\"},\"end\":85124,\"start\":84748},{\"attributes\":{\"doi\":\"arXiv:2301.13688\",\"id\":\"b38\"},\"end\":85543,\"start\":85126},{\"attributes\":{\"doi\":\"arXiv:2110.15943\",\"id\":\"b39\"},\"end\":85789,\"start\":85545},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52115700},\"end\":86280,\"start\":85791},{\"attributes\":{\"id\":\"b41\"},\"end\":86341,\"start\":86282},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":246426909},\"end\":86817,\"start\":86343},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b43\"},\"end\":87167,\"start\":86819},{\"attributes\":{\"doi\":\"arXiv:2304.03277\",\"id\":\"b44\"},\"end\":87382,\"start\":87169},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":21382535},\"end\":87883,\"start\":87384},{\"attributes\":{\"doi\":\"arXiv:2211.05102\",\"id\":\"b46\"},\"end\":88255,\"start\":87885},{\"attributes\":{\"doi\":\"arXiv:2104.06599\",\"id\":\"b47\"},\"end\":88494,\"start\":88257},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b48\",\"matched_paper_id\":204838007},\"end\":88927,\"start\":88496},{\"attributes\":{\"doi\":\"arXiv:2110.08207\",\"id\":\"b49\"},\"end\":89356,\"start\":88929},{\"attributes\":{\"doi\":\"arXiv:2210.13312\",\"id\":\"b50\"},\"end\":89650,\"start\":89358},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b51\"},\"end\":90060,\"start\":89652},{\"attributes\":{\"id\":\"b52\"},\"end\":90407,\"start\":90062},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b53\"},\"end\":90786,\"start\":90409},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b54\"},\"end\":91190,\"start\":90788},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b55\"},\"end\":91557,\"start\":91192},{\"attributes\":{\"doi\":\"arXiv:2212.10560\",\"id\":\"b56\"},\"end\":91927,\"start\":91559},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":253098274},\"end\":92373,\"start\":91929},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":253098274},\"end\":93075,\"start\":92375},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b59\"},\"end\":93410,\"start\":93077},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":246411621},\"end\":93821,\"start\":93412},{\"attributes\":{\"doi\":\"arXiv:1910.03771\",\"id\":\"b61\"},\"end\":94248,\"start\":93823},{\"attributes\":{\"doi\":\"arXiv:2304.13013\",\"id\":\"b62\"},\"end\":94618,\"start\":94250},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b63\"},\"end\":94963,\"start\":94620},{\"attributes\":{\"doi\":\"arXiv:2201.05966\",\"id\":\"b64\"},\"end\":95428,\"start\":94965},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":52822214},\"end\":96008,\"start\":95430},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b66\"},\"end\":96373,\"start\":96010},{\"attributes\":{\"doi\":\"arXiv:2106.10199\",\"id\":\"b67\"},\"end\":96707,\"start\":96375},{\"attributes\":{\"doi\":\"arXiv:2210.02414\",\"id\":\"b68\"},\"end\":97036,\"start\":96709},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b69\"},\"end\":97395,\"start\":97038},{\"attributes\":{\"doi\":\"arXiv:2104.04670\",\"id\":\"b70\"},\"end\":97737,\"start\":97397}]", "bib_title": "[{\"end\":73676,\"start\":73607},{\"end\":76736,\"start\":76683},{\"end\":77215,\"start\":77171},{\"end\":79579,\"start\":79522},{\"end\":79855,\"start\":79805},{\"end\":80228,\"start\":80184},{\"end\":80850,\"start\":80805},{\"end\":83916,\"start\":83835},{\"end\":84396,\"start\":84309},{\"end\":85838,\"start\":85791},{\"end\":86410,\"start\":86343},{\"end\":87439,\"start\":87384},{\"end\":88577,\"start\":88496},{\"end\":92013,\"start\":91929},{\"end\":92464,\"start\":92375},{\"end\":93481,\"start\":93412},{\"end\":95503,\"start\":95430},{\"end\":73676,\"start\":73607},{\"end\":76736,\"start\":76683},{\"end\":77215,\"start\":77171},{\"end\":79579,\"start\":79522},{\"end\":79855,\"start\":79805},{\"end\":80228,\"start\":80184},{\"end\":80850,\"start\":80805},{\"end\":83916,\"start\":83835},{\"end\":84396,\"start\":84309},{\"end\":85838,\"start\":85791},{\"end\":86410,\"start\":86343},{\"end\":87439,\"start\":87384},{\"end\":88577,\"start\":88496},{\"end\":92013,\"start\":91929},{\"end\":92464,\"start\":92375},{\"end\":93481,\"start\":93412},{\"end\":95503,\"start\":95430}]", "bib_author": "[{\"end\":71609,\"start\":71603},{\"end\":71615,\"start\":71609},{\"end\":71622,\"start\":71615},{\"end\":71629,\"start\":71622},{\"end\":71637,\"start\":71629},{\"end\":71643,\"start\":71637},{\"end\":71651,\"start\":71643},{\"end\":71660,\"start\":71651},{\"end\":71670,\"start\":71660},{\"end\":71960,\"start\":71950},{\"end\":71967,\"start\":71960},{\"end\":71975,\"start\":71967},{\"end\":71984,\"start\":71975},{\"end\":71995,\"start\":71984},{\"end\":72007,\"start\":71995},{\"end\":72016,\"start\":72007},{\"end\":72026,\"start\":72016},{\"end\":72034,\"start\":72026},{\"end\":72046,\"start\":72034},{\"end\":72299,\"start\":72289},{\"end\":72307,\"start\":72299},{\"end\":72318,\"start\":72307},{\"end\":72328,\"start\":72318},{\"end\":72338,\"start\":72328},{\"end\":72349,\"start\":72338},{\"end\":72359,\"start\":72349},{\"end\":72366,\"start\":72359},{\"end\":72376,\"start\":72366},{\"end\":72385,\"start\":72376},{\"end\":72849,\"start\":72842},{\"end\":72858,\"start\":72849},{\"end\":72869,\"start\":72858},{\"end\":72879,\"start\":72869},{\"end\":72887,\"start\":72879},{\"end\":72899,\"start\":72887},{\"end\":72908,\"start\":72899},{\"end\":72916,\"start\":72908},{\"end\":72927,\"start\":72916},{\"end\":72939,\"start\":72927},{\"end\":73221,\"start\":73214},{\"end\":73233,\"start\":73221},{\"end\":73242,\"start\":73233},{\"end\":73252,\"start\":73242},{\"end\":73263,\"start\":73252},{\"end\":73272,\"start\":73263},{\"end\":73280,\"start\":73272},{\"end\":73290,\"start\":73280},{\"end\":73304,\"start\":73290},{\"end\":73316,\"start\":73304},{\"end\":73690,\"start\":73678},{\"end\":73699,\"start\":73690},{\"end\":73717,\"start\":73699},{\"end\":73731,\"start\":73717},{\"end\":74141,\"start\":74129},{\"end\":74155,\"start\":74141},{\"end\":74166,\"start\":74155},{\"end\":74177,\"start\":74166},{\"end\":74193,\"start\":74177},{\"end\":74205,\"start\":74193},{\"end\":74215,\"start\":74205},{\"end\":74226,\"start\":74215},{\"end\":74241,\"start\":74226},{\"end\":74249,\"start\":74241},{\"end\":74620,\"start\":74607},{\"end\":74632,\"start\":74620},{\"end\":74641,\"start\":74632},{\"end\":74651,\"start\":74641},{\"end\":74660,\"start\":74651},{\"end\":74667,\"start\":74660},{\"end\":74682,\"start\":74667},{\"end\":74690,\"start\":74682},{\"end\":74702,\"start\":74690},{\"end\":74715,\"start\":74702},{\"end\":75030,\"start\":75022},{\"end\":75036,\"start\":75030},{\"end\":75045,\"start\":75036},{\"end\":75057,\"start\":75045},{\"end\":75352,\"start\":75339},{\"end\":75358,\"start\":75352},{\"end\":75365,\"start\":75358},{\"end\":75374,\"start\":75365},{\"end\":75380,\"start\":75374},{\"end\":75389,\"start\":75380},{\"end\":75398,\"start\":75389},{\"end\":75408,\"start\":75398},{\"end\":75418,\"start\":75408},{\"end\":75432,\"start\":75418},{\"end\":75442,\"start\":75432},{\"end\":75452,\"start\":75442},{\"end\":75837,\"start\":75821},{\"end\":75846,\"start\":75837},{\"end\":75855,\"start\":75846},{\"end\":75865,\"start\":75855},{\"end\":75873,\"start\":75865},{\"end\":75883,\"start\":75873},{\"end\":76130,\"start\":76119},{\"end\":76137,\"start\":76130},{\"end\":76148,\"start\":76137},{\"end\":76156,\"start\":76148},{\"end\":76163,\"start\":76156},{\"end\":76172,\"start\":76163},{\"end\":76178,\"start\":76172},{\"end\":76186,\"start\":76178},{\"end\":76198,\"start\":76186},{\"end\":76208,\"start\":76198},{\"end\":76506,\"start\":76494},{\"end\":76521,\"start\":76506},{\"end\":76750,\"start\":76738},{\"end\":76759,\"start\":76750},{\"end\":76770,\"start\":76759},{\"end\":76785,\"start\":76770},{\"end\":77229,\"start\":77217},{\"end\":77238,\"start\":77229},{\"end\":77250,\"start\":77238},{\"end\":77265,\"start\":77250},{\"end\":77515,\"start\":77506},{\"end\":77781,\"start\":77772},{\"end\":77958,\"start\":77947},{\"end\":77970,\"start\":77958},{\"end\":77981,\"start\":77970},{\"end\":77993,\"start\":77981},{\"end\":78206,\"start\":78200},{\"end\":78215,\"start\":78206},{\"end\":78224,\"start\":78215},{\"end\":78231,\"start\":78224},{\"end\":78241,\"start\":78231},{\"end\":78463,\"start\":78455},{\"end\":78476,\"start\":78463},{\"end\":78483,\"start\":78476},{\"end\":78494,\"start\":78483},{\"end\":78504,\"start\":78494},{\"end\":78514,\"start\":78504},{\"end\":78522,\"start\":78514},{\"end\":78815,\"start\":78805},{\"end\":78827,\"start\":78815},{\"end\":78838,\"start\":78827},{\"end\":78851,\"start\":78838},{\"end\":78861,\"start\":78851},{\"end\":78871,\"start\":78861},{\"end\":78879,\"start\":78871},{\"end\":78892,\"start\":78879},{\"end\":78904,\"start\":78892},{\"end\":78915,\"start\":78904},{\"end\":79196,\"start\":79182},{\"end\":79211,\"start\":79196},{\"end\":79219,\"start\":79211},{\"end\":79231,\"start\":79219},{\"end\":79243,\"start\":79231},{\"end\":79254,\"start\":79243},{\"end\":79594,\"start\":79581},{\"end\":79603,\"start\":79594},{\"end\":79870,\"start\":79857},{\"end\":79879,\"start\":79870},{\"end\":79889,\"start\":79879},{\"end\":79896,\"start\":79889},{\"end\":79907,\"start\":79896},{\"end\":79915,\"start\":79907},{\"end\":79929,\"start\":79915},{\"end\":80242,\"start\":80230},{\"end\":80250,\"start\":80242},{\"end\":80256,\"start\":80250},{\"end\":80266,\"start\":80256},{\"end\":80274,\"start\":80266},{\"end\":80580,\"start\":80568},{\"end\":80591,\"start\":80580},{\"end\":80599,\"start\":80591},{\"end\":80609,\"start\":80599},{\"end\":80863,\"start\":80852},{\"end\":80874,\"start\":80863},{\"end\":80889,\"start\":80874},{\"end\":80900,\"start\":80889},{\"end\":80918,\"start\":80900},{\"end\":80930,\"start\":80918},{\"end\":80943,\"start\":80930},{\"end\":80952,\"start\":80943},{\"end\":81250,\"start\":81242},{\"end\":81258,\"start\":81250},{\"end\":81268,\"start\":81258},{\"end\":81281,\"start\":81268},{\"end\":81287,\"start\":81281},{\"end\":81295,\"start\":81287},{\"end\":81303,\"start\":81295},{\"end\":81311,\"start\":81303},{\"end\":81669,\"start\":81661},{\"end\":81678,\"start\":81669},{\"end\":81690,\"start\":81678},{\"end\":81702,\"start\":81690},{\"end\":81711,\"start\":81702},{\"end\":81717,\"start\":81711},{\"end\":81728,\"start\":81717},{\"end\":81736,\"start\":81728},{\"end\":81743,\"start\":81736},{\"end\":81754,\"start\":81743},{\"end\":82129,\"start\":82119},{\"end\":82139,\"start\":82129},{\"end\":82151,\"start\":82139},{\"end\":82162,\"start\":82151},{\"end\":82380,\"start\":82372},{\"end\":82391,\"start\":82380},{\"end\":82404,\"start\":82391},{\"end\":82420,\"start\":82404},{\"end\":82430,\"start\":82420},{\"end\":82441,\"start\":82430},{\"end\":82452,\"start\":82441},{\"end\":82461,\"start\":82452},{\"end\":82472,\"start\":82461},{\"end\":82482,\"start\":82472},{\"end\":82872,\"start\":82865},{\"end\":83049,\"start\":83039},{\"end\":83060,\"start\":83049},{\"end\":83072,\"start\":83060},{\"end\":83309,\"start\":83301},{\"end\":83318,\"start\":83309},{\"end\":83521,\"start\":83512},{\"end\":83534,\"start\":83521},{\"end\":83541,\"start\":83534},{\"end\":83552,\"start\":83541},{\"end\":83561,\"start\":83552},{\"end\":83573,\"start\":83561},{\"end\":83582,\"start\":83573},{\"end\":83595,\"start\":83582},{\"end\":83601,\"start\":83595},{\"end\":83610,\"start\":83601},{\"end\":83926,\"start\":83918},{\"end\":83935,\"start\":83926},{\"end\":83945,\"start\":83935},{\"end\":83956,\"start\":83945},{\"end\":84405,\"start\":84398},{\"end\":84412,\"start\":84405},{\"end\":84423,\"start\":84412},{\"end\":84432,\"start\":84423},{\"end\":84441,\"start\":84432},{\"end\":84451,\"start\":84441},{\"end\":84457,\"start\":84451},{\"end\":84755,\"start\":84748},{\"end\":84762,\"start\":84755},{\"end\":84771,\"start\":84762},{\"end\":84777,\"start\":84771},{\"end\":84786,\"start\":84777},{\"end\":84794,\"start\":84786},{\"end\":84802,\"start\":84794},{\"end\":84811,\"start\":84802},{\"end\":84826,\"start\":84811},{\"end\":84838,\"start\":84826},{\"end\":84847,\"start\":84838},{\"end\":85219,\"start\":85208},{\"end\":85226,\"start\":85219},{\"end\":85232,\"start\":85226},{\"end\":85242,\"start\":85232},{\"end\":85253,\"start\":85242},{\"end\":85260,\"start\":85253},{\"end\":85268,\"start\":85260},{\"end\":85276,\"start\":85268},{\"end\":85284,\"start\":85276},{\"end\":85291,\"start\":85284},{\"end\":85552,\"start\":85545},{\"end\":85561,\"start\":85552},{\"end\":85576,\"start\":85561},{\"end\":85590,\"start\":85576},{\"end\":85854,\"start\":85840},{\"end\":85863,\"start\":85854},{\"end\":85872,\"start\":85863},{\"end\":85882,\"start\":85872},{\"end\":85895,\"start\":85882},{\"end\":86292,\"start\":86284},{\"end\":86422,\"start\":86412},{\"end\":86428,\"start\":86422},{\"end\":86437,\"start\":86428},{\"end\":86448,\"start\":86437},{\"end\":86462,\"start\":86448},{\"end\":86473,\"start\":86462},{\"end\":86482,\"start\":86473},{\"end\":86493,\"start\":86482},{\"end\":86502,\"start\":86493},{\"end\":86509,\"start\":86502},{\"end\":86827,\"start\":86819},{\"end\":86835,\"start\":86827},{\"end\":86845,\"start\":86835},{\"end\":86852,\"start\":86845},{\"end\":86859,\"start\":86852},{\"end\":86866,\"start\":86859},{\"end\":87177,\"start\":87169},{\"end\":87183,\"start\":87177},{\"end\":87189,\"start\":87183},{\"end\":87199,\"start\":87189},{\"end\":87206,\"start\":87199},{\"end\":87451,\"start\":87441},{\"end\":87465,\"start\":87451},{\"end\":87475,\"start\":87465},{\"end\":87487,\"start\":87475},{\"end\":87500,\"start\":87487},{\"end\":87893,\"start\":87885},{\"end\":87904,\"start\":87893},{\"end\":87917,\"start\":87904},{\"end\":87927,\"start\":87917},{\"end\":87939,\"start\":87927},{\"end\":87951,\"start\":87939},{\"end\":87959,\"start\":87951},{\"end\":87967,\"start\":87959},{\"end\":87978,\"start\":87967},{\"end\":87986,\"start\":87978},{\"end\":88329,\"start\":88322},{\"end\":88339,\"start\":88329},{\"end\":88589,\"start\":88579},{\"end\":88600,\"start\":88589},{\"end\":88611,\"start\":88600},{\"end\":88618,\"start\":88611},{\"end\":88628,\"start\":88618},{\"end\":88638,\"start\":88628},{\"end\":88646,\"start\":88638},{\"end\":88652,\"start\":88646},{\"end\":88661,\"start\":88652},{\"end\":89004,\"start\":88996},{\"end\":89014,\"start\":89004},{\"end\":89024,\"start\":89014},{\"end\":89034,\"start\":89024},{\"end\":89046,\"start\":89034},{\"end\":89058,\"start\":89046},{\"end\":89069,\"start\":89058},{\"end\":89081,\"start\":89069},{\"end\":89091,\"start\":89081},{\"end\":89099,\"start\":89091},{\"end\":89365,\"start\":89358},{\"end\":89375,\"start\":89365},{\"end\":89384,\"start\":89375},{\"end\":89392,\"start\":89384},{\"end\":89662,\"start\":89652},{\"end\":89669,\"start\":89662},{\"end\":89678,\"start\":89669},{\"end\":89689,\"start\":89678},{\"end\":89697,\"start\":89689},{\"end\":89708,\"start\":89697},{\"end\":89720,\"start\":89708},{\"end\":89734,\"start\":89720},{\"end\":89742,\"start\":89734},{\"end\":89751,\"start\":89742},{\"end\":90126,\"start\":90117},{\"end\":90139,\"start\":90126},{\"end\":90148,\"start\":90139},{\"end\":90158,\"start\":90148},{\"end\":90164,\"start\":90158},{\"end\":90176,\"start\":90164},{\"end\":90185,\"start\":90176},{\"end\":90200,\"start\":90185},{\"end\":90422,\"start\":90409},{\"end\":90433,\"start\":90422},{\"end\":90441,\"start\":90433},{\"end\":90452,\"start\":90441},{\"end\":90468,\"start\":90452},{\"end\":90480,\"start\":90468},{\"end\":90487,\"start\":90480},{\"end\":90494,\"start\":90487},{\"end\":90503,\"start\":90494},{\"end\":90509,\"start\":90503},{\"end\":90799,\"start\":90788},{\"end\":90809,\"start\":90799},{\"end\":90820,\"start\":90809},{\"end\":90832,\"start\":90820},{\"end\":90846,\"start\":90832},{\"end\":90857,\"start\":90846},{\"end\":90868,\"start\":90857},{\"end\":90877,\"start\":90868},{\"end\":90887,\"start\":90877},{\"end\":90896,\"start\":90887},{\"end\":91200,\"start\":91192},{\"end\":91209,\"start\":91200},{\"end\":91220,\"start\":91209},{\"end\":91228,\"start\":91220},{\"end\":91236,\"start\":91228},{\"end\":91248,\"start\":91236},{\"end\":91640,\"start\":91632},{\"end\":91649,\"start\":91640},{\"end\":91659,\"start\":91649},{\"end\":91666,\"start\":91659},{\"end\":91677,\"start\":91666},{\"end\":91689,\"start\":91677},{\"end\":91703,\"start\":91689},{\"end\":92023,\"start\":92015},{\"end\":92033,\"start\":92023},{\"end\":92053,\"start\":92033},{\"end\":92062,\"start\":92053},{\"end\":92073,\"start\":92062},{\"end\":92086,\"start\":92073},{\"end\":92095,\"start\":92086},{\"end\":92113,\"start\":92095},{\"end\":92121,\"start\":92113},{\"end\":92129,\"start\":92121},{\"end\":92474,\"start\":92466},{\"end\":92484,\"start\":92474},{\"end\":92504,\"start\":92484},{\"end\":92513,\"start\":92504},{\"end\":92524,\"start\":92513},{\"end\":92532,\"start\":92524},{\"end\":92541,\"start\":92532},{\"end\":92559,\"start\":92541},{\"end\":92572,\"start\":92559},{\"end\":92580,\"start\":92572},{\"end\":93134,\"start\":93127},{\"end\":93143,\"start\":93134},{\"end\":93153,\"start\":93143},{\"end\":93160,\"start\":93153},{\"end\":93168,\"start\":93160},{\"end\":93178,\"start\":93168},{\"end\":93184,\"start\":93178},{\"end\":93193,\"start\":93184},{\"end\":93201,\"start\":93193},{\"end\":93490,\"start\":93483},{\"end\":93498,\"start\":93490},{\"end\":93512,\"start\":93498},{\"end\":93521,\"start\":93512},{\"end\":93528,\"start\":93521},{\"end\":93537,\"start\":93528},{\"end\":93545,\"start\":93537},{\"end\":93553,\"start\":93545},{\"end\":93905,\"start\":93897},{\"end\":93914,\"start\":93905},{\"end\":93922,\"start\":93914},{\"end\":93934,\"start\":93922},{\"end\":93946,\"start\":93934},{\"end\":93953,\"start\":93946},{\"end\":93963,\"start\":93953},{\"end\":93972,\"start\":93963},{\"end\":93980,\"start\":93972},{\"end\":93993,\"start\":93980},{\"end\":94336,\"start\":94324},{\"end\":94348,\"start\":94336},{\"end\":94363,\"start\":94348},{\"end\":94373,\"start\":94363},{\"end\":94384,\"start\":94373},{\"end\":94395,\"start\":94384},{\"end\":94718,\"start\":94710},{\"end\":94725,\"start\":94718},{\"end\":94735,\"start\":94725},{\"end\":94746,\"start\":94735},{\"end\":94753,\"start\":94746},{\"end\":94972,\"start\":94965},{\"end\":94980,\"start\":94972},{\"end\":94987,\"start\":94980},{\"end\":94996,\"start\":94987},{\"end\":95007,\"start\":94996},{\"end\":95019,\"start\":95007},{\"end\":95028,\"start\":95019},{\"end\":95037,\"start\":95028},{\"end\":95044,\"start\":95037},{\"end\":95054,\"start\":95044},{\"end\":95513,\"start\":95505},{\"end\":95519,\"start\":95513},{\"end\":95528,\"start\":95519},{\"end\":95538,\"start\":95528},{\"end\":95547,\"start\":95538},{\"end\":95564,\"start\":95547},{\"end\":95577,\"start\":95564},{\"end\":96110,\"start\":96103},{\"end\":96125,\"start\":96110},{\"end\":96134,\"start\":96125},{\"end\":96140,\"start\":96134},{\"end\":96146,\"start\":96140},{\"end\":96152,\"start\":96146},{\"end\":96479,\"start\":96468},{\"end\":96491,\"start\":96479},{\"end\":96503,\"start\":96491},{\"end\":96717,\"start\":96709},{\"end\":96724,\"start\":96717},{\"end\":96730,\"start\":96724},{\"end\":96738,\"start\":96730},{\"end\":96745,\"start\":96738},{\"end\":96753,\"start\":96745},{\"end\":96761,\"start\":96753},{\"end\":96767,\"start\":96761},{\"end\":96776,\"start\":96767},{\"end\":96783,\"start\":96776},{\"end\":97047,\"start\":97038},{\"end\":97057,\"start\":97047},{\"end\":97066,\"start\":97057},{\"end\":97077,\"start\":97066},{\"end\":97085,\"start\":97077},{\"end\":97093,\"start\":97085},{\"end\":97102,\"start\":97093},{\"end\":97110,\"start\":97102},{\"end\":97116,\"start\":97110},{\"end\":97125,\"start\":97116},{\"end\":97504,\"start\":97495},{\"end\":97511,\"start\":97504},{\"end\":97520,\"start\":97511},{\"end\":97529,\"start\":97520},{\"end\":71609,\"start\":71603},{\"end\":71615,\"start\":71609},{\"end\":71622,\"start\":71615},{\"end\":71629,\"start\":71622},{\"end\":71637,\"start\":71629},{\"end\":71643,\"start\":71637},{\"end\":71651,\"start\":71643},{\"end\":71660,\"start\":71651},{\"end\":71670,\"start\":71660},{\"end\":71960,\"start\":71950},{\"end\":71967,\"start\":71960},{\"end\":71975,\"start\":71967},{\"end\":71984,\"start\":71975},{\"end\":71995,\"start\":71984},{\"end\":72007,\"start\":71995},{\"end\":72016,\"start\":72007},{\"end\":72026,\"start\":72016},{\"end\":72034,\"start\":72026},{\"end\":72046,\"start\":72034},{\"end\":72299,\"start\":72289},{\"end\":72307,\"start\":72299},{\"end\":72318,\"start\":72307},{\"end\":72328,\"start\":72318},{\"end\":72338,\"start\":72328},{\"end\":72349,\"start\":72338},{\"end\":72359,\"start\":72349},{\"end\":72366,\"start\":72359},{\"end\":72376,\"start\":72366},{\"end\":72385,\"start\":72376},{\"end\":72849,\"start\":72842},{\"end\":72858,\"start\":72849},{\"end\":72869,\"start\":72858},{\"end\":72879,\"start\":72869},{\"end\":72887,\"start\":72879},{\"end\":72899,\"start\":72887},{\"end\":72908,\"start\":72899},{\"end\":72916,\"start\":72908},{\"end\":72927,\"start\":72916},{\"end\":72939,\"start\":72927},{\"end\":73221,\"start\":73214},{\"end\":73233,\"start\":73221},{\"end\":73242,\"start\":73233},{\"end\":73252,\"start\":73242},{\"end\":73263,\"start\":73252},{\"end\":73272,\"start\":73263},{\"end\":73280,\"start\":73272},{\"end\":73290,\"start\":73280},{\"end\":73304,\"start\":73290},{\"end\":73316,\"start\":73304},{\"end\":73690,\"start\":73678},{\"end\":73699,\"start\":73690},{\"end\":73717,\"start\":73699},{\"end\":73731,\"start\":73717},{\"end\":74141,\"start\":74129},{\"end\":74155,\"start\":74141},{\"end\":74166,\"start\":74155},{\"end\":74177,\"start\":74166},{\"end\":74193,\"start\":74177},{\"end\":74205,\"start\":74193},{\"end\":74215,\"start\":74205},{\"end\":74226,\"start\":74215},{\"end\":74241,\"start\":74226},{\"end\":74249,\"start\":74241},{\"end\":74620,\"start\":74607},{\"end\":74632,\"start\":74620},{\"end\":74641,\"start\":74632},{\"end\":74651,\"start\":74641},{\"end\":74660,\"start\":74651},{\"end\":74667,\"start\":74660},{\"end\":74682,\"start\":74667},{\"end\":74690,\"start\":74682},{\"end\":74702,\"start\":74690},{\"end\":74715,\"start\":74702},{\"end\":75030,\"start\":75022},{\"end\":75036,\"start\":75030},{\"end\":75045,\"start\":75036},{\"end\":75057,\"start\":75045},{\"end\":75352,\"start\":75339},{\"end\":75358,\"start\":75352},{\"end\":75365,\"start\":75358},{\"end\":75374,\"start\":75365},{\"end\":75380,\"start\":75374},{\"end\":75389,\"start\":75380},{\"end\":75398,\"start\":75389},{\"end\":75408,\"start\":75398},{\"end\":75418,\"start\":75408},{\"end\":75432,\"start\":75418},{\"end\":75442,\"start\":75432},{\"end\":75452,\"start\":75442},{\"end\":75837,\"start\":75821},{\"end\":75846,\"start\":75837},{\"end\":75855,\"start\":75846},{\"end\":75865,\"start\":75855},{\"end\":75873,\"start\":75865},{\"end\":75883,\"start\":75873},{\"end\":76130,\"start\":76119},{\"end\":76137,\"start\":76130},{\"end\":76148,\"start\":76137},{\"end\":76156,\"start\":76148},{\"end\":76163,\"start\":76156},{\"end\":76172,\"start\":76163},{\"end\":76178,\"start\":76172},{\"end\":76186,\"start\":76178},{\"end\":76198,\"start\":76186},{\"end\":76208,\"start\":76198},{\"end\":76506,\"start\":76494},{\"end\":76521,\"start\":76506},{\"end\":76750,\"start\":76738},{\"end\":76759,\"start\":76750},{\"end\":76770,\"start\":76759},{\"end\":76785,\"start\":76770},{\"end\":77229,\"start\":77217},{\"end\":77238,\"start\":77229},{\"end\":77250,\"start\":77238},{\"end\":77265,\"start\":77250},{\"end\":77515,\"start\":77506},{\"end\":77781,\"start\":77772},{\"end\":77958,\"start\":77947},{\"end\":77970,\"start\":77958},{\"end\":77981,\"start\":77970},{\"end\":77993,\"start\":77981},{\"end\":78206,\"start\":78200},{\"end\":78215,\"start\":78206},{\"end\":78224,\"start\":78215},{\"end\":78231,\"start\":78224},{\"end\":78241,\"start\":78231},{\"end\":78463,\"start\":78455},{\"end\":78476,\"start\":78463},{\"end\":78483,\"start\":78476},{\"end\":78494,\"start\":78483},{\"end\":78504,\"start\":78494},{\"end\":78514,\"start\":78504},{\"end\":78522,\"start\":78514},{\"end\":78815,\"start\":78805},{\"end\":78827,\"start\":78815},{\"end\":78838,\"start\":78827},{\"end\":78851,\"start\":78838},{\"end\":78861,\"start\":78851},{\"end\":78871,\"start\":78861},{\"end\":78879,\"start\":78871},{\"end\":78892,\"start\":78879},{\"end\":78904,\"start\":78892},{\"end\":78915,\"start\":78904},{\"end\":79196,\"start\":79182},{\"end\":79211,\"start\":79196},{\"end\":79219,\"start\":79211},{\"end\":79231,\"start\":79219},{\"end\":79243,\"start\":79231},{\"end\":79254,\"start\":79243},{\"end\":79594,\"start\":79581},{\"end\":79603,\"start\":79594},{\"end\":79870,\"start\":79857},{\"end\":79879,\"start\":79870},{\"end\":79889,\"start\":79879},{\"end\":79896,\"start\":79889},{\"end\":79907,\"start\":79896},{\"end\":79915,\"start\":79907},{\"end\":79929,\"start\":79915},{\"end\":80242,\"start\":80230},{\"end\":80250,\"start\":80242},{\"end\":80256,\"start\":80250},{\"end\":80266,\"start\":80256},{\"end\":80274,\"start\":80266},{\"end\":80580,\"start\":80568},{\"end\":80591,\"start\":80580},{\"end\":80599,\"start\":80591},{\"end\":80609,\"start\":80599},{\"end\":80863,\"start\":80852},{\"end\":80874,\"start\":80863},{\"end\":80889,\"start\":80874},{\"end\":80900,\"start\":80889},{\"end\":80918,\"start\":80900},{\"end\":80930,\"start\":80918},{\"end\":80943,\"start\":80930},{\"end\":80952,\"start\":80943},{\"end\":81250,\"start\":81242},{\"end\":81258,\"start\":81250},{\"end\":81268,\"start\":81258},{\"end\":81281,\"start\":81268},{\"end\":81287,\"start\":81281},{\"end\":81295,\"start\":81287},{\"end\":81303,\"start\":81295},{\"end\":81311,\"start\":81303},{\"end\":81669,\"start\":81661},{\"end\":81678,\"start\":81669},{\"end\":81690,\"start\":81678},{\"end\":81702,\"start\":81690},{\"end\":81711,\"start\":81702},{\"end\":81717,\"start\":81711},{\"end\":81728,\"start\":81717},{\"end\":81736,\"start\":81728},{\"end\":81743,\"start\":81736},{\"end\":81754,\"start\":81743},{\"end\":82129,\"start\":82119},{\"end\":82139,\"start\":82129},{\"end\":82151,\"start\":82139},{\"end\":82162,\"start\":82151},{\"end\":82380,\"start\":82372},{\"end\":82391,\"start\":82380},{\"end\":82404,\"start\":82391},{\"end\":82420,\"start\":82404},{\"end\":82430,\"start\":82420},{\"end\":82441,\"start\":82430},{\"end\":82452,\"start\":82441},{\"end\":82461,\"start\":82452},{\"end\":82472,\"start\":82461},{\"end\":82482,\"start\":82472},{\"end\":82872,\"start\":82865},{\"end\":83049,\"start\":83039},{\"end\":83060,\"start\":83049},{\"end\":83072,\"start\":83060},{\"end\":83309,\"start\":83301},{\"end\":83318,\"start\":83309},{\"end\":83521,\"start\":83512},{\"end\":83534,\"start\":83521},{\"end\":83541,\"start\":83534},{\"end\":83552,\"start\":83541},{\"end\":83561,\"start\":83552},{\"end\":83573,\"start\":83561},{\"end\":83582,\"start\":83573},{\"end\":83595,\"start\":83582},{\"end\":83601,\"start\":83595},{\"end\":83610,\"start\":83601},{\"end\":83926,\"start\":83918},{\"end\":83935,\"start\":83926},{\"end\":83945,\"start\":83935},{\"end\":83956,\"start\":83945},{\"end\":84405,\"start\":84398},{\"end\":84412,\"start\":84405},{\"end\":84423,\"start\":84412},{\"end\":84432,\"start\":84423},{\"end\":84441,\"start\":84432},{\"end\":84451,\"start\":84441},{\"end\":84457,\"start\":84451},{\"end\":84755,\"start\":84748},{\"end\":84762,\"start\":84755},{\"end\":84771,\"start\":84762},{\"end\":84777,\"start\":84771},{\"end\":84786,\"start\":84777},{\"end\":84794,\"start\":84786},{\"end\":84802,\"start\":84794},{\"end\":84811,\"start\":84802},{\"end\":84826,\"start\":84811},{\"end\":84838,\"start\":84826},{\"end\":84847,\"start\":84838},{\"end\":85219,\"start\":85208},{\"end\":85226,\"start\":85219},{\"end\":85232,\"start\":85226},{\"end\":85242,\"start\":85232},{\"end\":85253,\"start\":85242},{\"end\":85260,\"start\":85253},{\"end\":85268,\"start\":85260},{\"end\":85276,\"start\":85268},{\"end\":85284,\"start\":85276},{\"end\":85291,\"start\":85284},{\"end\":85552,\"start\":85545},{\"end\":85561,\"start\":85552},{\"end\":85576,\"start\":85561},{\"end\":85590,\"start\":85576},{\"end\":85854,\"start\":85840},{\"end\":85863,\"start\":85854},{\"end\":85872,\"start\":85863},{\"end\":85882,\"start\":85872},{\"end\":85895,\"start\":85882},{\"end\":86292,\"start\":86284},{\"end\":86422,\"start\":86412},{\"end\":86428,\"start\":86422},{\"end\":86437,\"start\":86428},{\"end\":86448,\"start\":86437},{\"end\":86462,\"start\":86448},{\"end\":86473,\"start\":86462},{\"end\":86482,\"start\":86473},{\"end\":86493,\"start\":86482},{\"end\":86502,\"start\":86493},{\"end\":86509,\"start\":86502},{\"end\":86827,\"start\":86819},{\"end\":86835,\"start\":86827},{\"end\":86845,\"start\":86835},{\"end\":86852,\"start\":86845},{\"end\":86859,\"start\":86852},{\"end\":86866,\"start\":86859},{\"end\":87177,\"start\":87169},{\"end\":87183,\"start\":87177},{\"end\":87189,\"start\":87183},{\"end\":87199,\"start\":87189},{\"end\":87206,\"start\":87199},{\"end\":87451,\"start\":87441},{\"end\":87465,\"start\":87451},{\"end\":87475,\"start\":87465},{\"end\":87487,\"start\":87475},{\"end\":87500,\"start\":87487},{\"end\":87893,\"start\":87885},{\"end\":87904,\"start\":87893},{\"end\":87917,\"start\":87904},{\"end\":87927,\"start\":87917},{\"end\":87939,\"start\":87927},{\"end\":87951,\"start\":87939},{\"end\":87959,\"start\":87951},{\"end\":87967,\"start\":87959},{\"end\":87978,\"start\":87967},{\"end\":87986,\"start\":87978},{\"end\":88329,\"start\":88322},{\"end\":88339,\"start\":88329},{\"end\":88589,\"start\":88579},{\"end\":88600,\"start\":88589},{\"end\":88611,\"start\":88600},{\"end\":88618,\"start\":88611},{\"end\":88628,\"start\":88618},{\"end\":88638,\"start\":88628},{\"end\":88646,\"start\":88638},{\"end\":88652,\"start\":88646},{\"end\":88661,\"start\":88652},{\"end\":89004,\"start\":88996},{\"end\":89014,\"start\":89004},{\"end\":89024,\"start\":89014},{\"end\":89034,\"start\":89024},{\"end\":89046,\"start\":89034},{\"end\":89058,\"start\":89046},{\"end\":89069,\"start\":89058},{\"end\":89081,\"start\":89069},{\"end\":89091,\"start\":89081},{\"end\":89099,\"start\":89091},{\"end\":89365,\"start\":89358},{\"end\":89375,\"start\":89365},{\"end\":89384,\"start\":89375},{\"end\":89392,\"start\":89384},{\"end\":89662,\"start\":89652},{\"end\":89669,\"start\":89662},{\"end\":89678,\"start\":89669},{\"end\":89689,\"start\":89678},{\"end\":89697,\"start\":89689},{\"end\":89708,\"start\":89697},{\"end\":89720,\"start\":89708},{\"end\":89734,\"start\":89720},{\"end\":89742,\"start\":89734},{\"end\":89751,\"start\":89742},{\"end\":90126,\"start\":90117},{\"end\":90139,\"start\":90126},{\"end\":90148,\"start\":90139},{\"end\":90158,\"start\":90148},{\"end\":90164,\"start\":90158},{\"end\":90176,\"start\":90164},{\"end\":90185,\"start\":90176},{\"end\":90200,\"start\":90185},{\"end\":90422,\"start\":90409},{\"end\":90433,\"start\":90422},{\"end\":90441,\"start\":90433},{\"end\":90452,\"start\":90441},{\"end\":90468,\"start\":90452},{\"end\":90480,\"start\":90468},{\"end\":90487,\"start\":90480},{\"end\":90494,\"start\":90487},{\"end\":90503,\"start\":90494},{\"end\":90509,\"start\":90503},{\"end\":90799,\"start\":90788},{\"end\":90809,\"start\":90799},{\"end\":90820,\"start\":90809},{\"end\":90832,\"start\":90820},{\"end\":90846,\"start\":90832},{\"end\":90857,\"start\":90846},{\"end\":90868,\"start\":90857},{\"end\":90877,\"start\":90868},{\"end\":90887,\"start\":90877},{\"end\":90896,\"start\":90887},{\"end\":91200,\"start\":91192},{\"end\":91209,\"start\":91200},{\"end\":91220,\"start\":91209},{\"end\":91228,\"start\":91220},{\"end\":91236,\"start\":91228},{\"end\":91248,\"start\":91236},{\"end\":91640,\"start\":91632},{\"end\":91649,\"start\":91640},{\"end\":91659,\"start\":91649},{\"end\":91666,\"start\":91659},{\"end\":91677,\"start\":91666},{\"end\":91689,\"start\":91677},{\"end\":91703,\"start\":91689},{\"end\":92023,\"start\":92015},{\"end\":92033,\"start\":92023},{\"end\":92053,\"start\":92033},{\"end\":92062,\"start\":92053},{\"end\":92073,\"start\":92062},{\"end\":92086,\"start\":92073},{\"end\":92095,\"start\":92086},{\"end\":92113,\"start\":92095},{\"end\":92121,\"start\":92113},{\"end\":92129,\"start\":92121},{\"end\":92474,\"start\":92466},{\"end\":92484,\"start\":92474},{\"end\":92504,\"start\":92484},{\"end\":92513,\"start\":92504},{\"end\":92524,\"start\":92513},{\"end\":92532,\"start\":92524},{\"end\":92541,\"start\":92532},{\"end\":92559,\"start\":92541},{\"end\":92572,\"start\":92559},{\"end\":92580,\"start\":92572},{\"end\":93134,\"start\":93127},{\"end\":93143,\"start\":93134},{\"end\":93153,\"start\":93143},{\"end\":93160,\"start\":93153},{\"end\":93168,\"start\":93160},{\"end\":93178,\"start\":93168},{\"end\":93184,\"start\":93178},{\"end\":93193,\"start\":93184},{\"end\":93201,\"start\":93193},{\"end\":93490,\"start\":93483},{\"end\":93498,\"start\":93490},{\"end\":93512,\"start\":93498},{\"end\":93521,\"start\":93512},{\"end\":93528,\"start\":93521},{\"end\":93537,\"start\":93528},{\"end\":93545,\"start\":93537},{\"end\":93553,\"start\":93545},{\"end\":93905,\"start\":93897},{\"end\":93914,\"start\":93905},{\"end\":93922,\"start\":93914},{\"end\":93934,\"start\":93922},{\"end\":93946,\"start\":93934},{\"end\":93953,\"start\":93946},{\"end\":93963,\"start\":93953},{\"end\":93972,\"start\":93963},{\"end\":93980,\"start\":93972},{\"end\":93993,\"start\":93980},{\"end\":94336,\"start\":94324},{\"end\":94348,\"start\":94336},{\"end\":94363,\"start\":94348},{\"end\":94373,\"start\":94363},{\"end\":94384,\"start\":94373},{\"end\":94395,\"start\":94384},{\"end\":94718,\"start\":94710},{\"end\":94725,\"start\":94718},{\"end\":94735,\"start\":94725},{\"end\":94746,\"start\":94735},{\"end\":94753,\"start\":94746},{\"end\":94972,\"start\":94965},{\"end\":94980,\"start\":94972},{\"end\":94987,\"start\":94980},{\"end\":94996,\"start\":94987},{\"end\":95007,\"start\":94996},{\"end\":95019,\"start\":95007},{\"end\":95028,\"start\":95019},{\"end\":95037,\"start\":95028},{\"end\":95044,\"start\":95037},{\"end\":95054,\"start\":95044},{\"end\":95513,\"start\":95505},{\"end\":95519,\"start\":95513},{\"end\":95528,\"start\":95519},{\"end\":95538,\"start\":95528},{\"end\":95547,\"start\":95538},{\"end\":95564,\"start\":95547},{\"end\":95577,\"start\":95564},{\"end\":96110,\"start\":96103},{\"end\":96125,\"start\":96110},{\"end\":96134,\"start\":96125},{\"end\":96140,\"start\":96134},{\"end\":96146,\"start\":96140},{\"end\":96152,\"start\":96146},{\"end\":96479,\"start\":96468},{\"end\":96491,\"start\":96479},{\"end\":96503,\"start\":96491},{\"end\":96717,\"start\":96709},{\"end\":96724,\"start\":96717},{\"end\":96730,\"start\":96724},{\"end\":96738,\"start\":96730},{\"end\":96745,\"start\":96738},{\"end\":96753,\"start\":96745},{\"end\":96761,\"start\":96753},{\"end\":96767,\"start\":96761},{\"end\":96776,\"start\":96767},{\"end\":96783,\"start\":96776},{\"end\":97047,\"start\":97038},{\"end\":97057,\"start\":97047},{\"end\":97066,\"start\":97057},{\"end\":97077,\"start\":97066},{\"end\":97085,\"start\":97077},{\"end\":97093,\"start\":97085},{\"end\":97102,\"start\":97093},{\"end\":97110,\"start\":97102},{\"end\":97116,\"start\":97110},{\"end\":97125,\"start\":97116},{\"end\":97504,\"start\":97495},{\"end\":97511,\"start\":97504},{\"end\":97520,\"start\":97511},{\"end\":97529,\"start\":97520}]", "bib_venue": "[{\"end\":73886,\"start\":73817},{\"end\":86054,\"start\":85983},{\"end\":87651,\"start\":87584},{\"end\":92739,\"start\":92668},{\"end\":95736,\"start\":95665},{\"end\":73886,\"start\":73817},{\"end\":86054,\"start\":85983},{\"end\":87651,\"start\":87584},{\"end\":92739,\"start\":92668},{\"end\":95736,\"start\":95665},{\"end\":71601,\"start\":71533},{\"end\":71948,\"start\":71890},{\"end\":72482,\"start\":72401},{\"end\":72840,\"start\":72751},{\"end\":73380,\"start\":73332},{\"end\":73815,\"start\":73731},{\"end\":74336,\"start\":74265},{\"end\":74782,\"start\":74731},{\"end\":75118,\"start\":75073},{\"end\":75337,\"start\":75264},{\"end\":75819,\"start\":75718},{\"end\":76117,\"start\":76072},{\"end\":76492,\"start\":76434},{\"end\":76902,\"start\":76785},{\"end\":77327,\"start\":77265},{\"end\":77601,\"start\":77515},{\"end\":77770,\"start\":77726},{\"end\":77945,\"start\":77864},{\"end\":78279,\"start\":78257},{\"end\":78453,\"start\":78408},{\"end\":78803,\"start\":78735},{\"end\":79325,\"start\":79270},{\"end\":79652,\"start\":79603},{\"end\":79981,\"start\":79929},{\"end\":80326,\"start\":80274},{\"end\":80566,\"start\":80491},{\"end\":80996,\"start\":80952},{\"end\":81377,\"start\":81327},{\"end\":81659,\"start\":81567},{\"end\":82117,\"start\":82030},{\"end\":82570,\"start\":82498},{\"end\":82863,\"start\":82828},{\"end\":83037,\"start\":82981},{\"end\":83299,\"start\":83240},{\"end\":83510,\"start\":83472},{\"end\":84050,\"start\":83956},{\"end\":84506,\"start\":84457},{\"end\":84909,\"start\":84863},{\"end\":85206,\"start\":85126},{\"end\":85643,\"start\":85606},{\"end\":85981,\"start\":85895},{\"end\":86558,\"start\":86509},{\"end\":86964,\"start\":86882},{\"end\":87251,\"start\":87222},{\"end\":87582,\"start\":87500},{\"end\":88043,\"start\":88002},{\"end\":88320,\"start\":88257},{\"end\":88689,\"start\":88670},{\"end\":88994,\"start\":88929},{\"end\":89480,\"start\":89408},{\"end\":89823,\"start\":89767},{\"end\":90115,\"start\":90062},{\"end\":90564,\"start\":90525},{\"end\":90957,\"start\":90912},{\"end\":91348,\"start\":91264},{\"end\":91630,\"start\":91559},{\"end\":92134,\"start\":92129},{\"end\":92666,\"start\":92580},{\"end\":93125,\"start\":93077},{\"end\":93602,\"start\":93553},{\"end\":93895,\"start\":93823},{\"end\":94322,\"start\":94250},{\"end\":94708,\"start\":94620},{\"end\":95161,\"start\":95070},{\"end\":95663,\"start\":95577},{\"end\":96101,\"start\":96010},{\"end\":96466,\"start\":96375},{\"end\":96844,\"start\":96799},{\"end\":97185,\"start\":97141},{\"end\":97493,\"start\":97397},{\"end\":71601,\"start\":71533},{\"end\":71948,\"start\":71890},{\"end\":72482,\"start\":72401},{\"end\":72840,\"start\":72751},{\"end\":73380,\"start\":73332},{\"end\":73815,\"start\":73731},{\"end\":74336,\"start\":74265},{\"end\":74782,\"start\":74731},{\"end\":75118,\"start\":75073},{\"end\":75337,\"start\":75264},{\"end\":75819,\"start\":75718},{\"end\":76117,\"start\":76072},{\"end\":76492,\"start\":76434},{\"end\":76902,\"start\":76785},{\"end\":77327,\"start\":77265},{\"end\":77601,\"start\":77515},{\"end\":77770,\"start\":77726},{\"end\":77945,\"start\":77864},{\"end\":78279,\"start\":78257},{\"end\":78453,\"start\":78408},{\"end\":78803,\"start\":78735},{\"end\":79325,\"start\":79270},{\"end\":79652,\"start\":79603},{\"end\":79981,\"start\":79929},{\"end\":80326,\"start\":80274},{\"end\":80566,\"start\":80491},{\"end\":80996,\"start\":80952},{\"end\":81377,\"start\":81327},{\"end\":81659,\"start\":81567},{\"end\":82117,\"start\":82030},{\"end\":82570,\"start\":82498},{\"end\":82863,\"start\":82828},{\"end\":83037,\"start\":82981},{\"end\":83299,\"start\":83240},{\"end\":83510,\"start\":83472},{\"end\":84050,\"start\":83956},{\"end\":84506,\"start\":84457},{\"end\":84909,\"start\":84863},{\"end\":85206,\"start\":85126},{\"end\":85643,\"start\":85606},{\"end\":85981,\"start\":85895},{\"end\":86558,\"start\":86509},{\"end\":86964,\"start\":86882},{\"end\":87251,\"start\":87222},{\"end\":87582,\"start\":87500},{\"end\":88043,\"start\":88002},{\"end\":88320,\"start\":88257},{\"end\":88689,\"start\":88670},{\"end\":88994,\"start\":88929},{\"end\":89480,\"start\":89408},{\"end\":89823,\"start\":89767},{\"end\":90115,\"start\":90062},{\"end\":90564,\"start\":90525},{\"end\":90957,\"start\":90912},{\"end\":91348,\"start\":91264},{\"end\":91630,\"start\":91559},{\"end\":92134,\"start\":92129},{\"end\":92666,\"start\":92580},{\"end\":93125,\"start\":93077},{\"end\":93602,\"start\":93553},{\"end\":93895,\"start\":93823},{\"end\":94322,\"start\":94250},{\"end\":94708,\"start\":94620},{\"end\":95161,\"start\":95070},{\"end\":95663,\"start\":95577},{\"end\":96101,\"start\":96010},{\"end\":96466,\"start\":96375},{\"end\":96844,\"start\":96799},{\"end\":97185,\"start\":97141},{\"end\":97493,\"start\":97397}]"}}}, "year": 2023, "month": 12, "day": 17}