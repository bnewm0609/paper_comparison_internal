{"id": 240493478, "updated": "2023-10-06 06:06:09.366", "metadata": {"title": "PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations", "authors": "[{\"first\":\"Mark\",\"last\":\"Muller\",\"middle\":[\"Niklas\"]},{\"first\":\"Gleb\",\"last\":\"Makarchuk\",\"middle\":[]},{\"first\":\"Gagandeep\",\"last\":\"Singh\",\"middle\":[]},{\"first\":\"Markus\",\"last\":\"Puschel\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Vechev\",\"middle\":[]}]", "venue": "Proceedings of the ACM on Programming Languages, Volume 6, Issue POPL, January 2022, Article No.: 43, pp 1-33", "journal": "Proceedings of the ACM on Programming Languages", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Formal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging tasks from prior work. Our results show that PRIMA is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20%, 30%, and 34% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.03638", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/pacmpl/MullerMSPV22", "doi": "10.1145/3498704"}}, "content": {"source": {"pdf_hash": "a0b05ab4f9d06890d1933dc4429cfeceebc4ce0c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.03638v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3498704", "status": "GOLD"}}, "grobid": {"id": "e82b6b0f204d35357f6d9f27a2c3ccaf10bfa8cb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a0b05ab4f9d06890d1933dc4429cfeceebc4ce0c.txt", "contents": "\n43 PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations\nJanuary 2022\n\nMark Niklas M\u00fcller \nUIUC and VMware Research\nUnited States\n\nEth Zurich \nUIUC and VMware Research\nUnited States\n\nSwitzerland Gleb Makarchuk \nUIUC and VMware Research\nUnited States\n\nEth Zurich \nUIUC and VMware Research\nUnited States\n\nGagandeep Switzerland \nUIUC and VMware Research\nUnited States\n\nSingh \nUIUC and VMware Research\nUnited States\n\nMarkus P\u00fcschel \nUIUC and VMware Research\nUnited States\n\nEth Zurich \nUIUC and VMware Research\nUnited States\n\nMartin Switzerland \nUIUC and VMware Research\nUnited States\n\nEth Vechev \nUIUC and VMware Research\nUnited States\n\nZurich \nUIUC and VMware Research\nUnited States\n\nSwitzerland \nUIUC and VMware Research\nUnited States\n\n43 PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations\n\nProc. ACM Program. Lang. 6, POPL, Article 43\n. ACM Program. Lang. 6, POPL, Article 4333January 202210.1145/3498704ACM Reference Format: Mark Niklas M\u00fcller, Gleb Makarchuk, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. 2022. PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations.CCS Concepts: \u2022 Theory of computation \u2192 AbstractionProgram verification\u2022 Computing method- ologies \u2192 Neural networks Additional Key Words and Phrases: Robustness, Convexity, Polyhedra, Abstract Interpretation\nFormal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge.In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called Prima. Prima is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss.We evaluate the effectiveness of Prima on a variety of challenging tasks from prior work. Our results show that Prima is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20%, 30%, and 34% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, Prima enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes.\n\nINTRODUCTION\n\nThe growing adoption of neural networks (NNs) in many safety critical domains highlights the importance of providing formal, deterministic guarantees about their safety and robustness when deployed in the real world [Szegedy et al. 2014]. While the last few years have seen significant Strong interdependencies between neurons that can be captured directly or indirectly are shown as solid or dashed lines, respectively. Individual single-neuron, multi-neuron or optimal convex abstractions are illustrated in blue and the resulting overall layer-wise abstraction in green.\n\nprogress in formal verification of NNs, existing deterministic methods (see Urban and Min\u00e9 [2021] for a survey) still either do not scale to or are too imprecise when handling realistic networks. Key challenge: handling non-linearities. Neural networks interleave affine and non-linear activation layers (e.g., ReLU, Sigmoid), leading to highly non-linear behaviours. Because affine layers can be captured exactly using linear constraints, the key challenge in neural network verification rests in designing methods that can handle the effect of these non-linear activations in a precise and scalable manner.\n\nExact verification, e.g., [Anderson et al. 2019Bunel et al. 2020b;Ehlers 2017;Katz et al. 2017;Singh et al. 2019c;Tjeng et al. 2019;, has, in the worst-case, exponential complexity in the (large) number of non-linear activations due to a combinatorial blow-up of case distinctions (e.g., for ReLUs) and complex shapes for general activations (e.g., for Sigmoids). Therefore exact verifiers typically only handle piecewise linear activations and do not scale to larger networks.\n\nTo overcome this limitation, state-of-the-art verifiers, e.g., [Singh et al. 2019a,b;Xu et al. 2020;, often sacrifice completeness for scalability and leverage abstract interpretation [Cousot 1996] to over-approximate the effect of each activation layer with convex polyhedra. Naturally, the scalability and precision of these incomplete methods are tied to the particular polyhedral fragment they utilize.\n\nBelow, we contrast different state-of-the-art abstraction approaches with our work by comparing the strong inter-neuron dependencies they can capture directly or indirectly, illustrated as solid or dashed lines, respectively, in Figure 1 for a layer of four neurons. Individual abstractions are visualized in blue and the resulting layer-wise shape in green.\n\nOptimal convex approximation. Assume a layer of neurons, each applying the scalar, univariate, non-linear activation function : R \u2192 R and the most precise polyhedral abstraction P of the layer's inputs . The most precise convex abstraction of the layer output is then given by the convex hull of all input-output vector pairs conv({( , ( )) | \u2208 P \u2286 R }), illustrated in Figure 1 (d), where all interactions are fully captured. Computing this 2 -dimensional convex hull, however, is intractable due to the exponential cost O ( log( ) + ) [Chazelle 1993] in the number of neurons , where the number of vertices = O ( ) of the input polytope P is at worst also exponential in [Seidel 1995] ( is the number of constraints of the input polytope P).\n\nSingle-Neuron approximation. Most incomplete verifiers are fundamentally based on singleneuron convex abstractions, i.e., activations are approximated separately. The tightest single-neuron abstractions maintain upper and lower bounds , for each input and compute convex hulls of all input-output tuples: conv({( , ( )) | \u2208 [ , ] \u2286 R}), as illustrated in Figure 2 for a ReLU. The union of the obtained constraints is the final abstraction of the layer. Geometrically, it is the Cartesian product of the convex hulls for each ReLU. This abstraction is significantly larger in volume (exponential in ) than the optimal convex hull discussed earlier, the key reason being that the interdependencies between neurons in the same layer are ignored, as illustrated in Figure 1 (a). Thus, the approximation error can grow exponentially with each layer, accumulating significant imprecision.\n\nMulti-Neuron approximation. To mitigate this limitation for ReLU networks, recent works [Palma et al. 2021;Singh et al. 2019a; introduced multi-neuron abstractions as a first compromise between the optimal but intractable layer-wise and the imprecise but scalable neuron-wise abstraction. Singh et al. [2019a] partition the neurons of an activation layer into small sets of size \u2264 5, form groups of \u2264 3 neurons for each partition, approximate the group's input with octahedra [Claris\u00f3 and Cortadella 2007], and then compute exact convex hulls jointly approximating the output of ReLUs for this input. These exact convex hull computations are computationally expensive and yield complex constraints, limiting the approach to only a few, mostly disjoint neuron groups, and restricting the number of captured dependencies, see Figure 1 (b).  and Palma et al. [2021] merge the activation layer with the preceding affine layer and compute a convex approximation over the resulting multivariate activation layer for a hyperbox approximation of its input. This coarse input abstraction effectively restricts their approach to interactions over a single affine layer at a time. While both approaches currently yield state-of-the-art precision, they are limited to ReLU activations and lack scalability as they require small instances of the NP-hard convex hull problem to be solved exactly or large instances to be solved partially. They also do not address the problem of capturing enough neuroninterdependencies within a layer to come as close as possible to the optimal convex abstraction.\n\nThis work: precise multi-neuron approximations. In this work, we present the first general verification framework for networks with arbitrary, bounded, multivariate activation functions called Prima (PRecIse Multi-neuron Abstraction). Prima builds on the group-wise approximations from Singh et al. [2019a] and leverages the key insight that most interdependencies between neurons can be captured by considering a large number of relatively small, overlapping neuron-groups. While not achieving the tightness of the optimal convex approximation, Prima yields much tighter layer-wise approximations than previous methods, as shown in Figure 1 (c).\n\nThe key technical contributions of our work are: (i) PDDM (Partial Double Description Method) -a general, precise, and fast convex hull approximation method for polytopes that enables the consideration of many neuron groups, and (ii) SBLM (Split-Bound-Lift Method) -a novel decomposition approach that builds upon the PDDM to quickly compute multi-neuron constraints. While we combine these methods with abstraction refinement approaches in Prima, we note that they are also of general interest (beyond neural networks) and can be used independently of each other.\n\nPrima can be applied to any network with bounded, multivariate activation functions and arbitrary specifications expressible as polyhedra such as individual fairness [Ruoss et al. 2020b]; global safety properties [Katz et al. 2017]; and acoustic [Ryou et al. 2020], geometric [Balunovic et al. 2019], spatial [Ruoss et al. 2020a], and \u2113 -norm bounded perturbations ]. Our experimental evaluation shows that Prima achieves state-of-the-art precision on the majority of our ReLU-based classifiers while remaining competitive on the rest. For Sigmoid-and Tanh-based networks, Prima significantly outperforms prior work on all benchmarks. Further, Prima enables, for the first time, precise and scalable verification of a realistic architecture for autonomous driving containing > 100k neurons in a regression setting. Finally, while Prima is incomplete, it can be used for boosting the scalability of state-of-the-art complete verifiers [Singh et al. 2019c; for ReLU-based networks that benefit from more precise convex abstractions.\n\nMain contributions. Our key contributions are:\n\n(1) PDDM, a precise method for approximating the convex hull of polytopes, with worst-case polynomial time-and space-complexity and exactness guarantees in low dimensions.\n\n(2) Split-Bound-Lift Method, a technique which efficiently computes joint constraints over groups of non-linear functions, by decomposing the underlying convex hull problem into lower-dimensional spaces. (3) Prima, a novel verifier combining these approaches with a sparse neuron grouping technique and abstraction refinement, to obtain the first multi-neuron verifier for arbitrary, bounded, multivariate non-linear activations (e.g., ReLU, Sigmoid, Tanh, and MaxPool). (4) An evaluation of Prima on a range of activations and network architectures (e.g., fully connected, convolutional, and residual). We show that Prima is significantly more precise than state-of-the-art, with gains of up to 20%, 30%, and 34% for ReLU-, Sigmoid-, and Tanhbased networks, while being effective in a regression setting, scaling to large networks, and enabling verification in real-world settings such as autonomous driving. We release our code as part of the open-source framework ERAN at https://github.com/eth-sri/eran.\n\n\nBACKGROUND\n\nIn this section, we establish the terminology we use to discuss polyhedra, neural networks (NNs) and their verification.\n\nNotation. We use lower case Latin or Greek letters , , , . . . , , . . . for scalars, bold for vectors , capitalized bold for matrices , and calligraphic A or blackboard bold A for sets. Similarly, we denote scalar functions as : R \u2192 R and vector valued functions bold as : R \u2192 R .\n\nNeural networks. We focus our discussion on networks ( ) : X \u2192 R |Y | that map input samples (images) \u2208 X to numerical scores \u2208 R | Y | . For a classification task, the network classifies an input by applying argmax to its output: ( ) = arg max ( ) . While our methods can refine the abstraction of activation functions in arbitrary neural architectures [Xu et al. 2020], for simplicity, we discuss a feedforward architecture which is an interleaved composition of affine functions ( ) = + , such as normalization, linear, convolutional, or average pooling layers, with non-linear activation layers ( ) such as ReLU, Tanh, Sigmoid, or MaxPool:\n( ) = \u2022 \u2022 \u22121 \u2022 ... \u2022 1 \u2022 0 ( ).\n\nNeural Network Verification\n\nPrima is an optimization-based verification approach and supports any safety specification (pre-and post-condition) which can be expressed as a convex polyhedron. Examples of such specifications include but are not limited to individual fairness [Ruoss et al. 2020b], global safety properties [Katz et al. 2017], acoustic [Ryou et al. 2020], geometric [Balunovic et al. 2019], spatial [Ruoss et al. 2020a], and \u2113 -norm bounded perturbations .\n\nAt its core, Prima is based on accumulating linear constraints encoding the whole network for a given (convex) pre-condition, defining a linear optimization objective representing the property to be verified, and finally using an LP solver to derive a bound on this objective. If this bound satisfies a predetermined threshold (that depends on the property), the property is verified.\n\nWhile all affine layers (e.g., linear, convolutional, and normalization layers) can be encoded exactly using linear constraints, non-linearities have to be over-approximated via constraints in their input-output space. That is, for an activation layer : R \u2192 R and a given set of inputs P in \u2286 R , we need to derive sound output constraints, that represent a set P in-out \u2286 R + which includes all possible input-output pairs that can be obtained by applying to the inputs in P in .\n\nWe show an over-approximation for a single ReLU in Figure 2. In the concrete, the ReLU maps input to = max(0, ). If the bounds 0 > \u2264 \u2264 > 0 are known, the best convex approximation is given by the blue triangle. In this work we present novel methods to compute tighter shapes by considering multiple neurons jointly in a higher dimensional space.\n\n\nOverview of Convex Polyhedra\n\nWe now introduce the necessary background on polyhedra. A polyhedron can be represented as the convex hull of its extremal points, called the vertex-or V-representation, or as the subspace satisfying a set of linear constraints, called the halfspace constraint or H -representation. Simultaneously maintaining both representations of the same polyhedron is called double description.\n\nVertex representation. A polyhedron P \u2286 R is the closed convex hull of a set of generators called vertices R = { \u2208 R }:\nP = P (R) = \u2211\ufe01 | \u2208 R, \u2211\ufe01 = 1, \u2208 R + 0 ,\nwhere R + 0 are the positive real numbers including 0. A polyhedral cone P \u2286 R is the positive linear span of a set of generators called rays R = { \u2208 R } and always includes the origin:\nP = P (R) = \u2211\ufe01 | \u2208 R, \u2208 R + 0 .\nHalfspace representation. Alternatively, a polyhedron can be described as the set P \u2286 R satisfying a system of linear inequalities (or constraints) defined by \u2208 R \u00d7 and \u2208 R :\nP = P ( , ) \u2261 { \u2208 R | \u2265 }.\nGeometrically, P is the intersection of closed affine halfspaces H = { \u2208 R | \u2265 } with \u2208 R and \u2208 R. For a polyhedral cone we have = 0. For convenience, a polyhedron P ( , ) can be equivalently described in so-called homogenized coordinates \u2032 = [1, ], where it can be expressed as P\n( \u2032 ) = { \u2032 \u2208 R +1 | \u2032 \u2032 \u2265 0} with the new constraint matrix \u2032 = [\u2212 , ].\nA -face F of a -dimensional polyhedron is a -dimensional subset F \u2286 P satisfying \u2212 linearly independent constraints 1 with equality. We call a 0-face a vertex and a ( \u2212 1)-face a facet [Edelsbrunner 2012]. The rank of a ray or vertex in a -dimensional polyhedron is the number of linearly independent constraints it satisfies with equality. We call a ray of rank \u2212 1 and a vertex of rank extremal. A ray of rank \u2212 can be represented as the positive combination of extremal rays and a vertex of rank \u2212 as the convex combination of + 1 extremal points.\n\nDouble description. Polyhedra static analysis [Fukuda and Prodon 1995;Motzkin et al. 1953;Singh et al. 2017] usually maintains both representations (H and V) in a pair ( \u2032 , R), called double description. This is useful as computing the convex hull in the V-representation is trivial (union of generator sets), but computing intersections is NP-hard. Conversely, computing intersections in the H -representation is trivial (union of constraints), but computing the convex hull is NP-hard. The transformation from the Vto the H -representation is called the convex hull problem and the reverse the vertex enumeration problem. Both are NP-hard in general.\n\nInclusion. We define the inclusion of a polytope Q in a polytope P as: Q \u2286 P or equivalently, \u2200 \u2208 Q, \u2208 P. In this setting, we say P over-approximates Q and Q under-approximates P. 1 We call a set of constraints \u2265 linearly independent, if the are linearly independent.\n\n\nOVERVIEW OF PRIMA\n\nWe now present an overview of Prima, our framework for faster and more precise verification of neural networks with arbitrary, bounded, multivariate, non-linear activations. We provide a complete formal description of its main components PDDM and SBLM in Sections 4 and 5, and of Prima in Section 6. In our explanations, we follow the setup outlined in Section 1: an activation layer consisting of neurons representing non-linear activations ( ) (e.g., ReLU, Tanh, Sigmoid). Computing a convex approximation of a whole layer. Conceptually, given an -dimensional polytope S constraining the input to the activation layer, Prima computes a set of multi-neuron constraints, forming a convex over-approximation of this layer, as follows:\n\n(1) Group decomposition: Decompose the set of activations in the layer into overlapping groups (subsets) of size .\n\n(2) Octahedral projection: For each such group , compute an octahedral over-approximation P of the projection of S to the input-space of group .\n\n(3) Split-Bound-Lift Method (SBLM): Then, for each polytope P , compute a joint convex overapproximation K of the group output in the H -representation using our novel SBLM method. This method decomposes the problem into lower dimensions and leverages our novel Partial Double Description Method (PDDM) with polynomial complexity to compute fast and scalable convex hull approximations. Both SBLM and PDDM are also key to making Prima applicable to non-piecewise-linear activations. (4) Combine constraints: Finally, take the intersection of all output constraints K (a union of all constraints) to obtain an over-approximation of the entire layer output.\n\nVerification is performed by solving an LP problem which combines the generated multi-neuron constraints with an LP encoding of the whole network (evaluated in Section 7). We now explain the basic workings of each step and illustrate the key concepts on a running example.\n\nGroup decomposition. Computing convex hulls for large sets of activations (e.g., a whole layer) is infeasible. Thus, we consider groups of size , typically = 3 or 4. The key idea here is to capture dependencies between activation inputs and outputs ignored by neuron-wise approximations and thus achieve tighter approximations. The tightness increases with the number of groups and, importantly, the degree of overlap between them. Considering all possible groups for every layer is too expensive; thus we define the parameters partition size and group overlap for tuning the cost and precision of our approximations. We first partition the activations of a layer into sets of size (sorting by volume of the single neuron abstraction) and then for every set 2 choose a subset of all groups that pairwise overlap by at most , 0 \u2264 < .  Fig. 3. Exact projection of S \u2208 R 3 (left) to = 2 variables (green) and its octahedral over-approximation P (blue).\n\nOctahedral projection. Projecting the layer-wise input polytope S onto the input dimensions of every group is generally intractable due to the high dimensionality and large number of constraints. Therefore, we follow the idea of [Singh et al. 2019a] and over-approximate the projection. Empirically we find that multidimensional octahedra [Claris\u00f3 and Cortadella 2007], yielding 3 \u2212 1 input constraints per group of neurons, provide a good trade-off between accuracy and complexity. Such a projection is illustrated in Figure 3 for a layer of = 3 neurons and = 2.\nInput Polytope P 1 2 1 + 2 \u2265 \u22122 \u2212 1 + 2 \u2265 \u22122 1 \u2212 2 \u2265 \u22122 .\n. .  \nP \u2032 2,2 K \u2032 1,1 K \u2032 1,2 K \u2032 2,1 K \u2032 2,2 K1 K2 K 1 2 2 Output Constraints K 1 + 2 \u2212 2 1 \u2212 2 2 \u2265 \u22122 0.375 2 \u2212 2 \u2265 \u22120.75 \u2212 1 + 1 \u2265 0 . . .\n\nSplit-Bound-Lift Method\n\nThe next and most demanding step takes a -dimensional input polytope for a given -activation group, and computes a 2 -dimensional convex over-approximation of the output of the corresponding activations. We introduce a new technique, called Split-Bound-Lift Method, and illustrate its workings in Figure 4 on an example. We assume ReLU activations, group size = 2, and an octahedral input polytope P (left panel in Figure 4) described by\nP = { 1 + 2 \u2265 \u22122, \u2212 1 + 2 \u2265 \u22122, 1 \u2212 2 \u2265 \u22122, \u2212 1 \u2212 2 \u2265 \u22122, \u2212 2 \u2265 \u22121.2}.\nOur method has three main components explained next. Split the input polytope. We first split P into regions, which we call quadrants, for which tight or even exact, linear bounds of the activation functions are available. Choosing the right splits is essential for ensuring tight approximations. For piecewise-linear activation functions (like ReLU), splitting into their linear regions even yields exact bounds in every quadrant, leading to the tightest approximations. For our example with ReLU activations, this corresponds to splitting along hyperplanes where the input variables 1 and 2 are 0. We (randomly) choose the ordering { 1 , 2 } of output variables and split P (in the following we omit the superscript ) along the corresponding hyperplanes. That is, we first intersect P with the halfspaces { \u2208 R 2 | 1 \u2265 0} and { \u2208 R 2 | 1 \u2264 0}, obtaining P 1 and P 2 , and then P 1 and P 2 with { \u2208 R 2 | 2 \u2265 0} and { \u2208 R 2 | 2 \u2264 0}. These intersections generate a tree of polytopes visualized in the first three columns in the central panel of Figure 4 with the quadrants as leafs (third column). For brevity, we only follow the bottom half. There, the two quadrants P 2,1 and P 2,2 are described by\nP 2,1 = { 1 \u2212 2 \u2265 \u22122, \u2212 1 \u2265 0, \u2212 2 \u2265 \u22121.2, 2 \u2265 0}, P 2,2 = { 1 + 2 \u2265 \u22122, \u2212 1 \u2265 0, \u2212 2 \u2265 0}.\nIn the second part of the algorithm, we lift these quadrants step-by-step from the space of only their inputs to the space of both their inputs and outputs. We will now describe one step of lifting consisting of extending, bounding and computing a convex hull.\n\nExtend and bound the quadrants. We extend 3 the quadrants one output variable at a time, which, as we will see later, enables significant gains in speed while reducing the approximation error. In our example, we first trivially extend all quadrants from the ( 1 , 2 )-space to the ( 2 , 1 , 2 )-space (fourth column in Figure 4). Next, we bound the quadrants in the added dimension using the linear bounds (parametrically defined, see Section 5) corresponding to applying (an approximation of) the activation in the quadrant. Here, 2 \u2264 2 and 2 \u2265 2 for the quadrant P 2,1 (since 2 \u2265 0) and 2 \u2264 0 and 2 \u2265 0 for the quadrant P 2,2 (since 2 \u2264 0). Note that in this case the bounds we apply on every quadrant are exact, yielding the two polytopes (fifth column) with 0 volume in their 3 -space (in general, the bounds need not be exact): Approximate convex hull. Next, we compute the convex hull of K \u2032 2,1 and K \u2032 2,2 . Instead of using an exact method, we utilize our PDDM to compute precise overapproximations, leveraging the concept of duality, ideas from computational geometry and our novel PDD polyhedron representation (explained below and in more detail in Section 4). Note that because the considered quadrants are only extended one variable at a time, the computation takes place in 3 despite the groupoutput being in the 4 ( 1 , 2 , 1 , 2 )-space. This yields two main benefits: (i) precision -directly computing 2 -dimensional convex hulls with PDDM will lose more precision than our decomposed method, because PDDM is exact for polytopes of dimension up to three and loses precision only slowly for higher dimensions, and (ii) speed -a lower-dimensional polytope with fewer constraints and generally also fewer vertices significantly reduces the time required for the individual convex hull computations.\nK \u2032 2,1 = { 1 \u2212 2 \u2265 \u22122, \u2212 1 \u2265 0, \u2212 2 \u2265 \u22121.2, 2 \u2265 0, 2 \u2212 2 \u2265 0, \u2212 2 + 2 \u2265 0}, K \u2032 2,2 = { 1 + 2 \u2265 \u22122, \u2212 1 \u2265 0, \u2212 2 \u2265 0, \u2212 2 \u2265 0, 2 \u2265 0}.\nImportantly, our approximate method scales quartically as O{ 4 \u00b7 + 2 log( 2 )} in the number of input constraints and linear in the number of vertices (see Theorem 4.5) while optimal exact methods are in O ( log( ) + \u230a /2\u230b ) [Chazelle 1993], i.e., exponential in the number of dimensions and superlinear in the number of input vertices.\n\nNote that for non-piecewise-linear functions (e.g., Tanh or Sigmoid), the number of vertices doubles when extending by a dimension. This makes exact methods intractable and approximate methods not using the decompositional SBLM approach (that is, extending by all dimensions at the same time) slow (see our evaluation in Section 7).\n\nWe now obtain the convex hull (sixth column) of the two polytopes K \u2032 2,1 and K \u2032 2,2 which is exact in our 3 case:\nK 2 = { 1 + 2 \u2212 2 2 \u2265 \u22122, \u2212 1 \u2265 0, 0.375 2 \u2212 2 \u2265 \u22120.75, \u2212 2 + 2 \u2265 0, 2 \u2265 0}.\nWe compute K 1 analogously, thus completing the first step of lifting. The next and in this case final step of lifting starts with extending K 1 and K 2 by 1 into the ( 1 , 2 , 1 , 2 )-space, where we apply bounds on 1 yielding (in 4 and thus not illustrated as figure)\nK \u2032 1 = { \u2212 1 + 2 \u2212 2 2 \u2265 \u22122, 1 \u2265 0, 0.375 2 \u2212 2 \u2265 \u22120.75, \u2212 2 + 2 \u2265 0, 2 \u2265 0, 1 \u2212 1 \u2265 0, \u2212 1 + 1 \u2265 0}, K \u2032 2 = { 1 + 2 \u2212 2 2 \u2265 \u22122, \u2212 1 \u2265 0, 0.375 2 \u2212 2 \u2265 \u22120.75, \u2212 2 + 2 \u2265 0, 2 \u2265 0, \u2212 1 \u2265 0, 1 \u2265 0}.\nCompleting the second and final step of lifting by computing their convex hull yields the final tight 2-neuron constraints: Naturally, the region K is tighter than the tightest single-neuron approximations (triangle relaxation, discussed earlier). We illustrate this point by comparing their projections into the ( 2 , 1 , 2 )-space in Figure 5.\nK = { 1 + 2 \u2212 2 1 \u2212 2 2 \u2265 \u22122, 0.375 2 \u2212 2 \u2265 \u22120.75, \u2212 1 + 1 \u2265 0, \u2212 2 + 2 \u2265 0, 1 \u2265 0, 2 \u2265 0}. (a) Constraints - (b) Generators -R (c) Unsound V-representation (d) Sound V-representation (e) A-irredundant a) (f) A-irredundant b)\n\nPartial Double Description Method (PDDM)\n\nWe now introduce the new PDDM for computing fast, precise, and sound over-approximations of the convex hull of two polyhedra. This is in contrast to existing approximation methods, which either optimize for closer approximations [Bentley et al. 1982;Khosravani et al. 2013;Sartipizadeh and Vincent 2016;Zhong et al. 2014] but sacrifice the soundness required for verification, or have exponential complexity [Xu et al. 1998], making them too expensive for our application. Double description method. The well-known Double Description Method (DDM) [Fukuda and Prodon 1995;Motzkin et al. 1953] for computing the convex hull of two polyhedra in Double Description works as follows: (i) translate both polyhedra to their dual representation (explained in Section 4), (ii) intersect them in dual space by adding the constraints of one to the other, oneat-a-time, computing full Double Descriptions at every intermediate step, and (iii) translate the result back to primal space. Every step of adding an additional constraint generates quadratically many new vertices, leading to an overall increase exponential in the number of constraints (in dual space).\n\nPartial double description. We introduce the Partial Double Description (PDD), which guarantees soundness and also allows an approximate much cheaper intersection in dual space. We combine an exact H -representation, as their intersection is trivial, with an under-approximating 4 V-representation, as their exact intersection carries exponential cost. We illustrate this in Figure 6, where we show the constraints describing a polytope in (a), the corresponding exact V-representation in (b), an unsound approximate V-representation in (c), and three sound ones in (d), (e), and (f). Note that this definition of the PDD allows many different V-representations for a given H -representation (see (d), (e), and (f) in Figure 6) some of which are quite imprecise (see (e) and (f)).\n\nPartial double description method. Now, we define the PDDM to compute approximate convex hulls in PDD leveraging two key ideas: (i) instead of intersecting in dual space by adding the constraints of one polytope to the other one-at-a-time (as per DDM), we add them all in a single step. Crucially, this leads to an overall number of vertices at most quadratic (instead of exponential) in the number of original vertices (in dual space), and (ii) this single-step approach is asymmetric and we can greatly increase the intersection accuracy, by performing it in both directions and combining the resulting vertices. Overall, our approach yields a polynomial complexity (see Theorem 4.5) algorithm for sound convex hull approximations (see Theorem 4.1), guarantees exactness for low primal dual adding constraints ray shooting combine vertices A-irredundancy primal Partial Double Description Method Fig. 7. Partial Double Description Method for a 2-dimensional example. The input polytopes (1 st column) are translated to their dual representation (2 nd column), then all their constraints are added to the other dual polytope (3 rd column). The points are separated based on whether they are included in the intersection of the H -representations. Now ray-shooting is used to discover vertices on the rays between points in the intersection (blue points) to those outside (red points) by intersecting the rays with the constraints added in the previous step (4 th column). The vertices of both V-representations are then combined (5 th column) before A-irredundancy is enforced (6 th column) and the result is translated back to primal space (7 th column).\n\ndimensions (see Theorem 4.4), and empirically is two orders of magnitude faster for the challenging cases in our experiments (see Figure 16c), while losing precision only slowly as dimensionality increases (see Figure 16b). We illustrate the Partial Double Description Method in Figure 7 and provide more technical details in Section 4.\n\n\nLayerwise Abstraction\n\nSo far we have seen how to compute the multi-neuron convex approximation for a single group of activations. To compute the final abstraction of the whole activation layer, we combine the constraints forming the H -representations of the computed output polyhedra of each group, thereby obtaining the H -representation of the polytope describing the layerwise over-approximation. In this section, we explain our PDDM for computing convex hull approximations in greater detail. First, we introduce the needed notion of duality and our novel Partial Double Description (PDD) representation for polyhedra. Then, we explain the PDDM step by step as illustrated in Figure 7.\n\n\nTHE PARTIAL DOUBLE DESCRIPTION METHOD\nPrimal Polytope Polyhedral cone Dual Polytope Polyhedral cone 1 2 \u22121.5 1 \u2212 2 \u2265 \u22120.75 \u22121.5 1 + 2 \u2265 \u22120.75 \u2212 1 + 2 \u2265 \u22121.5 1 \u2212 2 \u2265 \u22121.5 5 1 \u2212 2 \u2265 \u22125.5 5 1 + 2 \u2265 \u22125.5 0.75 0 \u2212 1.5 1 \u2212 2 \u2265 0 0.75 0 \u2212 1.5 1 + 2 \u2265 0 1.5 0 \u2212 1 + 2 \u2265 0 1.5 0 + 1 \u2212 2 \u2265 0 5.5 0 + 5 1 \u2212 2 \u2265 0 5.5 0 + 5 1 + 2 \u2265 0 0 = 1\nThe PDDM computes the convex hull of two -dimensional polytopes P 1 = P ( 1 , 1 ) and P 2 = P ( 2 , 2 ), but uses the equivalent homogenized representation (see Section 2.2) of ( + 1)-dimensional cones P \u2032 1 = P ( \u2032 1 ) and P \u2032 2 = P ( \u2032 2 ). Vertices in the original polytope now correspond to rays in the cone. In the following explanations we will use either term, depending on convenience. The original polytope can be recovered from the cone, by intersecting it with the hyperplane \u2032 0 = 1 in primal, or with \u2032 0 = \u22121 in dual space (explained next) as visualized in Figure 8. Duality. The dual P of a polytope P with a minimal set (containing no redundancy) of extremal vertices R enclosing the origin but not containing it in its boundary (to ensure a bounded dual) is defined as\nP = { \u2208 R | \u22a4 \u2264 1 \u2200 \u2208 P} = \u2208R { \u2208 R | \u22a4 \u2264 1},(1)\nand for polyhedral cones P \u2032 as [Genov 2015]\nP \u2032 = { \u2032 \u2208 R +1 | \u2032\u22a4 \u2032 \u2264 0 \u2200 \u2032 \u2208 P \u2032 }.\n(2) Figure 8 shows an example of the dual of a polytope. Important for the remaining section are four properties of the transform between primal and dual. 1) The dual of a polyhedron is also a polyhedron.\n\n2) It is inclusion reversing: P \u2282 Q if and only if Q \u2282 P, 3) the V-representation of the dual corresponds to the H -representation of the primal and vice versa: P = P ( \u2032 , R \u2032 ) implies P = P (R \u2032\u22a4 , \u2032\u22a4 ), where (\u00b7) \u22a4 denotes transpose (note that this implies that the vertices of the primal correspond to the supporting hyperplanes of the dual and vice-versa), and 4) the dual of the dual of a polyhedron is the original primal polyhedron P = P. Partial double description. We leverage these duality properties in two ways: We translate the convex hull problem in primal space to an intersection problem in dual space (only involving a transpose given a DD or PDD) where we compute a V-representation under-approximating the intersection in dual space to obtain an H -representation over-approximating the convex hull in primal space (using inclusion reversion). To compute these intersections efficiently, we introduce the Partial Double Description (PDD) as a relaxation of the Double Description (DD) (Section 2.2) as discussed in the overview.\n\nFormally, the PDD of a ( + 1)-dimensional polyhedral cone is the pair of constraints and rays ( \u2032 , R \u2032 ) with \u2032 \u2208 R \u00d7( +1) and R \u2032 \u2208 R \u00d7( +1) where the V-representation is an underapproximation of the H -representation or more formally, where for any row \u2208 R \u2032 and constraint \u2208 \u2032 , \u2265 0 holds. We call constraints \u2208 \u2032 active for a given ray \u2208 R \u2032 , if they are fulfilled with equality, that is = 0. We store this relationship as part of the PDD in what we call the incidence matrix I \u2208 {0, 1} \u00d7 : I , = 1 if = 0 and I , = 0 otherwise. Further, we define the partial ordering on I: I \u2286 I iff I , \u2264 I , , \u2200 1 \u2264 \u2264 . Intuitively this corresponds to a row in the incidence matrix being only lesser than another if the set of active constraints of the associated ray is a strict subset of that of the other. Next, we describe PDDM as illustrated in Figure 7.\n\n\nConversion to Dual\n\nGiven the two polyhedral cones P 1 and P 2 in PDD representation ( \u2032 1 , R \u2032 1 ) and ( \u2032 2 , R \u2032 2 ) (1 st column in Figure 7), the first step of the PDDM is to convert them to their dual space representations (R \u2032\u22a4 1 , \u2032\u22a4 1 ) and (R \u2032\u22a4 2 , \u2032\u22a4 2 ) [Fukuda 2020] (2 nd column).\n\n\nIntersection\n\nThe next step in the PDDM is the intersection in dual space (columns 3 to 5 in Figure 7). Recall that the standard approach (DDM) for the intersection of polyhedra in DD is to sequentially add the constraints of one polytope to the other, computing exact V-representations at every step. This however can increase the number of vertices quadratically in every step resulting in an exponential size of the intermediate representation. Instead, we add all constraints jointly in one step, leveraging our PDD. In the following description of the intersection, we adopt the polytope (not cone) view and consider a general polytope ( , R). Batch intersection. To intersect a polytope ( , R) in PDD with a batch of constraints represented by the matrix and inducing the polyhedron P ( ), we separate the vertices in R into three sets depending on whether they satisfy all to-be-added constraints with inequality (R + ), some only\n(a) Adding Multiple Constraints (b) Exact Result (c) Discovered Vertices (d) A-Irredundant Fig. 9\n. Adding a batch of three constraints (blue thick lines) to a polytope in PDD. Vertices are separated into R \u2032 + (black), R \u2032 0 (none), and R \u2032 \u2212 (red). Ray-shooting discovers new vertices R \u2032 * (blue), avoiding the superfluous green points, but missing an extremal vertex (yellow) (a). Exact intersection (b), result of joint constraint processing (c), and under-approximation after enforcing A-irredundancy (d). with equality (R 0 ), or violate at least one (R \u2212 ). This corresponds to these points lying inside, on the boundary of, or outside of the polyhedron P ( ). An example is shown in Figure 9(a): the three added constraints are shown in blue and the vertices in R \u2032 + (black), R \u2032 0 (none), and R \u2032 \u2212 (red). Now we employ a technique called ray-shooting [Mar\u00e9chal and P\u00e9rin 2017] and shoot a ray \u2212 \u2212\u2212 \u2192 + \u2212 from a vertex + \u2208 R + inside the intersection P ( \u2229 ) to a vertex \u2212 \u2208 R \u2212 outside the intersection. We record the first hyperplane H = { \u2208 R | = 0} corresponding to one of the new constraints \u2208 that intersects with the ray \u2212 \u2212\u2212 \u2192 + \u2212 . We add the point * at which \u2212 \u2212\u2212 \u2192 + \u2212 intersects H to the set of discovered points R * . Doing so for all combinations of ( + , \u2212 ) \u2208 R + \u00d7 R \u2212 yields the set of points\nR * = { * = \u2212 \u2212\u2212 \u2192 + \u2212 \u2229 H | ( + , \u2212 ) \u2208 R \u2032 + \u00d7 R \u2032 \u2212 }.\nThe V-representation of the resulting intersection is now the union R + \u222a R 0 \u222a R * . In Figure 9 (a) the rays \u2212 \u2212\u2212 \u2192 + \u2212 are dashed lines from all black to all red vertices and discover new vertices R * (blue). Only using the first intersections, immediately discards the green points, however, we also do not discover the yellow point, which is an extremal vertex of the exact intersection (b), obtaining instead the under-approximation (c).\n\nBoosting precision. Batch intersection is asymmetric: The PDD of one polytope is intersected with the H -representation of another, to obtain an exact H -representation and under-approximating Vrepresentation of the intersection (compare Figure 10 (b) and (c)). By performing it in both directions, i.e., intersecting (R \u2032\u22a4 1 , \u2032\u22a4 1 ) with (R \u2032\u22a4 2 , \u2032\u22a4 2 ) and vice-versa in our example, we obtain two different under-approximations of the intersection (see Figure 10 (b) and (c)). Their convex hull (obtained by the union of vertices) is still a sound under-approximation of the exact intersection and more precise than the individual under-approximations. This is illustrated in Figure 10, where the exact intersection (blue in (d)) of the two H -representations (grey in (a)) is recovered despite the union of the input V-representations (green and red in (a)) not covering it. This is due to the synergy between PDD and PDDM: the under-approximate V-representation of the first polytope is intersected with the exact H -representation of the second one and vice versa. We see the same behaviour in Figure 7, where both uni-directional intersections (4 th column) are under-approximations, but their union is exact (5 th column).\n\nEmpirically we find that this is crucial to minimize the precision loss due to using approximations. Further, the intersection results are exact for small dimensions \u2264 4 of cones (see Theorem 4.4).\n\n\nEnforcing A-Irredundancy\n\nDespite using batch intersection, the number of vertices can grow quickly when computing multiple convex hulls sequentially in the Split-Bound-Lift Method. Therefore, some notion of redundancy is needed to efficiently reduce the representation size. The standard definitions of irredundancy are: 1) the set of unique extremal rays of the cone P ( \u2032 ) are irredundant, and 2) a ray is irredundant if removing it leads to a different cone P (R \u2032 ) \u2260 P (R \u2032 \\ ). For an exact DD, an irredundant representation does not lose precision and can be computed by retaining only rays with rank \u2212 1 (which can be cheaply computed using the incidence matrix I). However, a PDD ( \u2032 , R \u2032 ) usually does not include all or even any extremal rays of the cone P ( \u2032 ). Consequently, enforcing the first irredundancy definition could remove all rays. Enforcing the second definition is expensive to compute in the absence of a full set of extremal rays, as the full convex hull problem has to be solved to assess the removal af a ray.\n\nTherefore, we propose A-irredundancy requiring for all rays \u2208 R \u2032 that there may not be another generator \u2208 R \u2032 with a larger (by inclusion) active constraint set. Formally and using the partial ordering defined above, we require for an A-irredundant PDD:\nI \u2288 I , for all , \u2208 {1, ..., }, \u2260 .\nAny ray fulfilling a subset (including the same) constraints with equality as another ray, is removed until the above definition is satisfied to obtain an A-irredundant representation. Extremal rays will always be retained as they have the maximum number of active constraints and there are never two with the same active set. Intuitively, this enforces that no two rays lie in the interior of the same face of the polyhedron.\n\nWe illustrate the effect of enforcing A-irredundancy once in Figure 10 where we use it to obtain the polytope 10 (e) from 10 (d) and see that all extremal rays are retained and no precision is lost. In Figure 9 we apply it to polytope 9 (c) where the PDD misses one extremal vertex to obtain 9 (d) and see that here the resulting reduction in generator set size can come at the cost of a precision loss. Enforcing A-irredundancy in the 6 th column of Figure 7 (removing the red vertices), recovers the minimal set of extremal rays. Note that for rays of equal incidence there are multiple possibilities which to retain, as is illustrated in Figure 6 (e) and (f).\n\n\nConversion to Primal\n\nTranslating the A-irredundant PDD obtained as described above, back to primal space concludes the PDDM and yields the (generally) approximate convex hull of P 1 and P 2 illustrated in the 7 th column of Figure 7.\n\n\nFormal Guarantees\n\nIn this subsection, we first show that the PDDM is sound and exact in low dimensions, before analysing its worst-case complexity.\n\n\nAlgorithm 1: Batch Intersection\nResult: Intersected polytope ( \u2032 , R \u2032 ) Input: polytope ( , R ), constraint matrix Initialize R \u2212 , R 0 , R + , R * = \u2205, \u2205, \u2205, \u2205 for in R do if ( ) < 0 then Add to R \u2212 else if ( ) > 0 then Add to R + else Add to R 0 for + in R + do for \u2212 in R \u2212 do Compute * via ray-shooting from + to \u2212 Add * to R * Construct new PDD ( \u222a , R 0 \u222a R + \u222a R * ) Make PDD A-irredundant return PDD\nSoundness guarantee. Computing a sound over-approximation of the convex hull of two polytopes in primal space, by inclusion-inversion, is equivalent to computing a sound under-approximation of the intersection of their dual space representations. Since the primal-dual conversion employed in the PDDM is exact, a sound under-approximation of the intersection of two polytopes in PDD in dual space implies overall soundness. Enforcing A-irredundancy on a polytope P to yield Q can only remove generators, yielding Q \u2286 P. It follows directly that Q is a sound under-approximation, if P is. If both polytopes P \u2032 and P \u2032 generated by the vertex sets obtained for the two directions of batch intersection are sound under-approximations of the true intersection of the exact H -representations, it follows that their union P \u2032 is also a sound under-approximation. Hence, the soundness of the PDDM follows from the soundness of the batch intersection step:\n\nTheorem 4.1. The batch intersection P \u2032 = ( \u2032 , R \u2032 ) of a polytope P in PDD ( , R ) with the exact constraints of a polytope Q computed as described above and detailed in Algorithm 1, is a sound under-approximation of the intersection of the two exact H -representations and :\n{ \u2208 R | \u2032 \u2265 0} = { \u2208 R | \u2265 0 \u2227 \u2265 0}, \u2211\ufe01 \u2208R \u2032 | \u2211\ufe01 \u2264 1, \u2208 R + 0 \u2286 { \u2208 R | \u2265 0 \u2227 \u2265 0}.\nProof. Recall that a PDD consists of an exact H -representation and an under-approximate V-representation. The intersection of two polytopes in H -representation is simply the union of all constraints, allowing for an exact intersection of the H -representations. Hence, it remains to show that the resulting V-representation R \u2032 is a sound under-approximation of the H -representation \u2032 . For this, it is sufficient to show that, by construction, every vertex \u2208 R \u2032 satisfies all constraints in \u2032 . Recall that R \u2032 is the union of three groups of vertices (see Section 4.2 or Algorithm 1):\n\nR + vertices of the generating set R that satisfy all constraints in strictly, R 0 vertices of the generating set R that satisfy all constraints in , at least one with equality, R * the first intersections * of rays from a vertex in + \u2208 R + to a vertex in \u2212 \u2208 R \u2212 (vertices in R not satisfying all constraint in ) with the hyperplanes defined by . Since \u2212 lies outside Q while + lies inside, an intersection * is guaranteed to exist and lie between the two. By convexity of P, * satisfies all constraints of . Further, since * is the first intersection of the ray with a constraint in as seen from + , which satisfies all constraints in , * also satisfies all constraints in . Consequently, all vertices in the generating set R \u2032 satisfy all constraints of both P and Q. It follows that R \u2032 \u2286 Q \u2229 P and hence that the generated polytope is a sound under-approximation. \u25a1\n\n\nAlgorithm 2: PDDM Intersection\n\nResult: Intersected polytope ( \u222a , R \u2032 ) Input: polytope ( , R ) and ( , R )\nCompute ( \u2032 , R \u2032 ) = ( \u222a , R ) with Alg.1 Compute ( \u2032 , R \u2032 ) = ( \u222a , R ) with Alg.1 Construct new PDD ( \u2032 , R \u2032 \u222a R \u2032 ) Make PDD A-irredundant return PDD\nExactness guarantee. Further, we can show that for relatively low dimensional polyhedra in Double Description, as they are often encountered during the first step of lifting in the SBLM, the PDDM as described above is not only sound but actually exact. To this end, let us first show the following guarantee for the intersection of a cone in DD with a matrix of constraints:\n\nTheorem 4.2. Given a Double Description ( , R ) of a polyhedral cone and the constraint matrix , adding all constraints jointly as per Algorithm 1 is guaranteed to yield a double description ( \u222a , R \u2032 ) enumerating all extremal rays \u2032 of the \u222a -induced cone with one of the following properties:\n\n(1) \u2032 is extremal (rank \u2212 1) in the -induced cone.\n\n(2) \u2032 is of rank \u2212 2 in the -induced cone.\n\nProof. We can formally divide the rays of the new PDD R \u2032 into the two non-overlapping sets:\n\n\u2022 R + \u222a R 0 : Rays in R not violating any constraint \u2208 \u2022 R * : Rays discovered by ray-shooting\n\nSince ( , R ) is a DD of the -induced cone it enumerates all extremal rays. If \u2032 is extremal in both the -induced and the \u222a -induced cones, it is included in R and does not violate any constraints. Therefore, it is included in the first group above and will be part of R \u2032 , which concludes the proof of the first point. Any ray of rank \u2212 2 can, by definition, be represented as a positive combination of two extremal rays, that is rays of rank \u2212 1. As we assume ray \u2032 to be extremal in the \u222a -induced cone and therefore have rank \u2212 1, it necessarily intersects at least one constraint \u2208 and is extremal to the \u222a -induced cone. Consequently exactly one of the extremal rays used to construct it has to lie on either side of thy hyperplane induced by constraint . Therefore, they will be included in the sets R + and R \u2212 and the intersection will be discovered as part of the ray-shooting, concluding the proof of the second point. \u25a1\n\nUsing this result, we can proof the following guarantee for intersections of two cones in DD using our batch intersection and precision boosting approach, described in Section 4.2 and Algorithm 2: Theorem 4.3. Given the double descriptions ( , R ) and ( , R ) of two polyhedral cones, their intersection computed as per Algorithm 2 is guaranteed to be a partial double description ( \u222a , R \u2032 ) enumerating all extremal rays \u2032 of the ( \u222a )-induced cone with one of the following properties:\n\n(1) \u2032 is extremal in the -induced cone.\n\n(2) \u2032 is extremal in the -induced cone.\n\n(3) \u2032 is of rank \u2212 2 in the -induced cone.\n\n(4) \u2032 is of rank \u2212 2 in the -induced cone.\n\nProof. The proof follows directly from applying Lemma 4.2 to both applications of Algorithm 1, the insight that every extremal ray discovered by either will be included in the final generating set R \u2032 and the observation that the intersection of the exact H -representations, trivially is the union of their respective constraints, leading to a valid partial double description. \u25a1\n\nUsing these results, we can in turn proof that the intersection of two polyhedral cones of up to dimension 4 in DD using the approach described above is exact: Theorem 4.4. Given the Double Descriptions ( , R ) and ( , R ) of two polyhedral cones P and Q of dimension \u2264 4, the PDD of their intersection ( \u222a , R \u2032 ) computed as described above and detailed in Algorithm 2 is an exact DD with an irredundant generating set R \u2032 .\n\nProof. For briefness sake, we will only show the proof for = 4 here. Let R * be the set of extremal rays of the ( \u222a )-induced polyhedral cone. Consequently * \u2208 R * has the rank \u2212 1 = 3 in this cone and therefore it fulfills 3 linearly independent constraints in \u222a with equality. This leads to the following four exhaustive options:\n\n(1) all 3 constraints are part of , * is extremal in P , (2) all 3 constraints are part of , * is extremal in P , (3) 2 constraints are part of and 1 of , * is of rank \u2212 2 = 2 in P , (4) 2 constraints are part of and 1 of , * is of rank \u2212 2 = 2 in P . All of those are enumerated by Algorithm 4.3. Hence, R \u2032 will include all extremal rays of the ( \u222a )-induced cone. In this case A-irredundancy is equivalent to irredundancy. \u25a1 Complexity analysis. Finally, we can show that computing an over-approximation of the convex hull of two -dimensional, bounded polytopes in PDD using the PDDM has polynomial complexity:\n\nTheorem 4.5. Given the PDD of two -dimensional, bounded polytopes with a V-representation of at most vertices and an H -representation of at most constraints, computing a sound overapproximation of their convex hull using the PDDM as described above and detailed in Algorithm 2 has a worst-case time complexity of O ( \u00b7 4 + 2 log( 2 )). Figure 7:\n\n\nProof. The PDDM can be broken down into its six components illustrated in\n\n(1) Conversion from primal to dual representation (Section 4.1) (2) Adding the constraints of one polytope to the other, or more concretely separation of vertices into the three sets R + , R 0 , and R \u2212 (Section 4.2 or first half of Algorithm 1) (3) Discovery of new vertices via ray-shooting (Section 4.2 or second half of Algorithm 1) (4) Combining the vertices of the two intersection directions (Section 4.2 or Algorithm 2) (5) Enforcing of A-irredundancy (Section 4.3 or Algorithm 2) (6) Conversion from dual to primal representation (Section 4.1) Primal-dual conversions and combining of vertices can be computed in constant time, as this only involves computing the transpose and concatenation which can be done implicitly by changing the indexing of the corresponding matrices. Therefore, we will focus on the remaining three steps, which are all conducted in dual space.\n\nIn the following we assume the setting, of two -dimensional, bounded polytopes which in dual-space are defined by P = ( , R ) and Q = ( , R ). For convenience's sake, we assume the number of vertices to be = max(|R |, |R |) and number of constraints = max(| |, | |). Note that their roles are reversed compared to a primal space representation.\n\nAdding constraints and separating vertices. Recall that in dual space we compute the intersection of the two polytopes P and Q. The first step of intersecting P with Q is to split all points in R into the three groups R + , R 0 , and R \u2212 defined in Section 4.2 depending on whether the lie inside, on the border of or outside the polytope defined by as per the first half of Algorithm 1. This requires (at worst) evaluating \u2212 {>, =, <}0 for all \u2208 R and , \u2208 . Where the addition and comparison are dominated by the -dimensional dot-product between and , leading to a total complexity of this step of order O ( \u00b7 \u00b7 ). Note that incidence matrix columns corresponding to the new constraints are added and populated without any extra computation with 0s for the vertices in R + and 1s for vertices in R 0 .\n\nRay-shooting. Recall that to discover new generating vertices, the first intersections between the rays shot from all generating vertices of P lying inside Q, + \u2208 R + , to all vertices lying outside Q, \u2212 \u2208 R \u2212 , and all constraints in are computed. At worst there are no vertices in group R 0 and all vertices are spread equally between R + and R \u2212 , leading to 2 /4 rays to be intersected with constraints where each intersection corresponds to computing a ratio of dot-products and is order O ( ). Selecting the first intersection for each ray is linear in the intersection number. Consequently, the ray-shooting process overall is O ( \u00b7 \u00b7 2 ). Note that this adds new incidence matrix rows corresponding to the new vertices R * , which can then be populated with the row obtained by the elementwise of the two vertices generating the ray and a 1 in the column associated with the constraint of the first intersection which is linear O ( ) and dominated by the previous term.\n\nEnforcing A-irredundancy. The intermediate state prior to enforcing A-irredundancy contains at most = 2( + 2 /4) vertices, consisting of the at most vertices in R + and the at most 2 /4 vertices in R * , discovered during ray shooting, for both intersection directions. To enforce Airredundancy, vertices are first sorted in descending order by the number of active constraints which is order O ( log( )). Then starting with the first vertex, row-wise inclusion of the corresponding incidence matrix rows is checked for all following elements. Each check is O ( ) and ( 2 \u2212 )/2 checks have to be performed in the worst case that is, if no element is removed. This leads to an overall complexity of O ( \u00b7 4 + 2 log( 2 )) for enforcing A-irredundancy.\n\nPDDM complexity. Putting the three elements together and observing < for any -dimensional, bounded polytope, we observe that both the ray-shooting and the separation of vertices get dominated by the last step of enforcing A-irredundancy. Swapping the roles of and to derive an expression in terms of primal space entities, we arrive at an overall complexity of O ( \u00b7 4 + 2 log( 2 )). At a high level, we first decompose the input polytope into regions where we can bound all activation functions tightly. Then, we extend these regions into the output space and apply linear constraints corresponding to the (relaxed) activations. Taking the convex hull of the resulting polytopes yields an H -representation encoding the k-neuron abstraction.\n\nTo increase the efficiency of this approach, we use a decomposition method we call splitting and then recursively extend and bound the resulting polytopes by one output variable at a time, which we call lifting. This minimizes the dimensionality in which we have to compute the convex hulls. We formalize this in Algorithm 3 and explain both splitting and lifting below after stating the prerequisites for the SBLM.\n\n\nPrerequisites\n\nFor simplicities' sake, we assume just one type of activation function : D \u2192 R, with domain D, is to be bounded. Now the SBLM requires a set of intervals D (e.g., \u2264 0, \u2265 0 for ReLU), covering the domain D (e.g., R for ReLU), and a pair of tight linear constraints B upper and lower bounding the function output (e.g., \u2264 0 and \u2265 0, and \u2264 and \u2265 , respectively, for ReLU) on each of the intervals obtained by intersecting the interval [ , ] defined by the neuron-wise bounds with the intervals D . More formally, we require the intervals\nD = [ , ],\n, \u2208 R and \u2264 ,\nD \u2286 D ,\nwith the affinely extended real numbers R = R \u222a {\u2212\u221e, \u221e} and the bounds on these intervals\nB = ( \u2264 , \u2265 ), { \u2264, \u2265 } ( ) = + , , \u2208 R . . \u2264 ( ) \u2264 ( ) \u2264 \u2265 ( ), \u2200 \u2208 (D \u2229 [ , ] ),\nto be provided to instantiate SBLM and by extension Prima. We note that the bounds and of the bounding regions can depend on the concrete input bounds and and the slope and intercept of { \u2264, \u2265 } can in turn depend on the corresponding concrete interval bounds [max( , ), min( , )].\n\nGeneralization. While we focus on the univariate case using only two bounding regions D 1 and D 2 in the following, SBLM and by extension Prima can be generalized to allow for neuron groups combining different multivariate activation functions : D \u2286 R \u2192 R. Further, more than one upper-and lower-bound B per bounding region can be provided and D can be specified as polyhedral regions instead of as intervals, as long as their union covers the domain D \u2286 D of the individual functions .\n\n\nSplitting the Input Polytope\n\nTo apply the bounds B , the input polytope P has to be split into the regions for which the bounds were specified. These regions correspond to the intersection of P with the k-Cartesian product of the bounding regions D , that is all combinations of neuron-wise bounding regions for the group of k neurons. We choose an ordering of the output variables I and recursively split P by intersecting with the bounding regions associated with these output variables.\n\nAs every such split is equivalent on an abstract level, we will explain one case assuming the parent polytope P 1 , the output variable = ( ), and the corresponding bounding regions\nD 1 = { \u2208 R | \u2265 1 } and D 2 = { \u2208 R | \u2264 2 }.\nWe compute the children nodes by intersecting P 1 with D 1 and D 2 to obtain P 1,1 = P 1 \u2229 D 1 and P 1,2 = P 1 \u2229 D 2 . Starting with P at the root and recursively applying this splitting rule for every \u2208 I, generates a polytope tree, which we call the decomposition tree, with 2 leaf polytopes P {1,2} , which we call quadrants. This is illustrated in the blue portion of the central panel in Figure 4, where D 1 and D 2 are R + 0 and R \u2212 0 , respectively.\n\n\nLifting\n\nWe now extend these quadrants P {1,2} to the output space and bound them using the corresponding constraints on the activation function B , before taking their convex hull. This yields a polytope K, jointly constraining the inputs and outputs of a neuron group. The constraints of its H -representation form the desired k-neuron abstraction. We call this process lifting and propose a recursive approach: We lift sibling polytopes on the decomposition tree until only the desired polytope K remains.\n\nAgain, we explain a single step of lifting, as they are equivalent. We assume the sibling polytopes K 1,1 and K 1,2 , corresponding to P 1,1 and P 1,2 in the decomposition tree, with the associated inputand output-variables and , respectively, and the pairs of bounds B 1 and B 2 instantiated for . A single step consist of three parts:\n\n\u2022 extending K 1,1 and K 1,2 by the output variable , \u2022 bounding on the extended polytopes, by intersecting them with the constraints B 1 and B 2 to obtain K \u2032 1,1 and K \u2032 1,2 , \u2022 computing their (approximate) convex hull using the PDDM: K 1 = conv(K \u2032 1,1 , K \u2032 1,2 ). Applying this lifting rule recursively to the decomposition tree starting with K {1,2} = P {1,2} , combines all 2 quadrants into a single 2 -dimensional polytope K, jointly constraining the inputs and outputs, thereby concluding the Split-Bound-Lift Method. This is illustrated in the right portion of the central panel in Figure 4. The decompositional approach has two benefits: Precisioncomputing approximate convex hulls via the PDDM is exact for polytopes of dimension up to 3 and starts to lose precision only slowly as dimensionality increases. Directly computing 2 -dimensional convex hulls with PDDM will therefore lose more precision than using our decomposed method. Speed -a lower-dimensional polytope with fewer constraints and generally also fewer vertices significantly reduces the runtime for the individual convex hull operations. In fact, computing the convex hulls for the approximation of non-piecewise-linear functions directly in the inputoutput space is intractable even for groups of only size = 3, as the number of vertices increases exponentially with during the extension and bounding process in that case. We instantiate SBLM for common network functions next. ReLU. We can capture all univariate, piecewise-linear functions, such as ReLU, exactly on the intervals D where they are linear. Further, if the neuron-wise bounds [ , ] only contain one such linear region, the neuron behaves linearly, can be encoded exactly and is excluded from the k-neuron abstraction. Therefore, we consider = where we denote the lower bound of the intersection D \u2229 [ , ] as and the upper one as . We show these bounds in Figure 11 for the Sigmoid function and, for illustration purposes, a non-optimal . In practice, we choose to minimize the area of the abstraction of a single neuron in the input-output plane. MaxPool. Let MaxPool be the multivariate function = max( 1 , 2 , ..., ) on the domain \u2208 P \u2286 [ , ] . Note that here the generalized formulation is required. We chose the polyhedral bounding regions D = { \u2208 R | \u2265 , 1 \u2264 \u2264 , \u2260 } , separating the domain into the regions where one variable dominates all others (illustrated for = 2 in Figure 12). On each of these regions, MaxPool can be bounded exactly with \u2264 and \u2265 . During the splitting process, this increased number of bounding regions leads to a decomposition tree where every parent node has child nodes.\n\n\nInstantiation for Various Functions\n( , 0) with \u2208 [ , ] for < 0 < . We choose D 1 = [\u2212\u221e,\n\nPRIMA VERIFICATION FRAMEWORK\n\nPrima is based on three high-level steps: (i) accumulate a set of constraints encoding a (convex) abstraction of the network for a given pre-condition (as discussed so far), (ii) define a linear optimization objective representing the post-condition, and (iii) use an LP or MILP solver to derive a bound on this optimization objective. If this bound exceeds a threshold depending on the postcondition, certification succeeds, otherwise, if the optimal solution violates this bound, it could be a true counterexample or a false positive due to approximation. Hence, we evaluate any such possible counterexample with the concrete network to determine whether it is a true counterexample.\n\nWhile all affine layers are encoded exactly, two considerations have to be balanced when encoding non-linear activation layers with Prima: more precise encodings (e.g., considering more or larger neuron groups) improve the optimal bound of the optimization problem, but the increased number of constraints can make this problem impractical to solve. We navigate this trade-off by leveraging abstraction refinement -using increasingly more precise but also more costly methods until we are able to either decide a property (verify or falsify) or reach a timeout.\n\n\nAbstraction Refinement Approaches\n\nFundamentally, we can refine our abstraction in three ways: (i) compute tighter abstractions of the group-wise inputs, (ii) compute tighter layer-wise multi-neuron constraints for the given input abstraction from (i), and (iii) encode part of the network using an exact MILP encoding.\n\nInput bound refinement. Since SBLM and PDDM abstract a group of neurons for a given polyhedral input region, the tightness of the resulting constraints depends directly on the tightness of the input abstraction. These are computed using a fast, incomplete verifier (e.g., [M\u00fcller et al. 2021;Singh et al. 2019b;Xu et al. 2020]) based on single-neuron abstractions and can be tightened significantly by computing more precise neuron-wise bounds [Singh et al. 2019c] using an LP or MILP encoding.\n\nTighten multi-neuron constraints. The layer-wise tightness of our multi-neuron constraints depends on (i) the tightness of the group-wise constraints, mostly determined by the quality of the input region, and (ii) on capturing the important neuron-interdependencies with the chosen groups. Using larger neuron groups (increasing ) and considering more groupings by allowing more overlap (increasing ) and partitioning the neurons into fewer sets before grouping (increasing ), allows capturing more and more complex interactions. While the constraints themselves can be computed quickly, the resulting LP problems become harder to solve.\n\nNetwork encoding. Prima encodes non-linear activations in four different ways: (i) exact encoding via equality constraints for stable (those exhibiting linear behavior) piecewise-linear activations, (ii) single-neuron constraints, (iii) multi-neuron constraints computed via SBLM and PDDM, and (iv) exact (for piecewise-linear functions) MILP encodings. While stable activations are always encoded exactly and all unstable activations are encoded using both the single-and multi-neuron constraints, we only selectively use a MILP encoding on the (typically relatively narrow) last layers of convolutional networks due to their large computational cost.\n\n\nAbstraction Refinement Cascade\n\nPrima leverages our multi-neuron constraints as part of an abstraction refinement cascade using increasingly more precise and expensive approaches: We first attempt verification using singleneuron constraints via DeepPoly [Singh et al. 2019b] or GPUPoly [M\u00fcller et al. 2021]. If this fails, we encode all activation layers using our multi-neuron constraints and solve the resulting LP. If this also fails, we attempt to decide the property by tightening the multi-neuron constraints Section 6.1, encoding the final network layer(s) using MILP, and refining individual neuron bounds. In this section, we evaluate the effectiveness of Prima and show that it significantly improves over state-of-the-art verifiers on a range of challenging benchmarks yielding up to 14%, 30% and 34% precision gains on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, we show that Prima can scale to realworld problems, obtaining tight bounds in an autonomous driving steering-angleprediction task. Finally, we demonstrate the effectiveness and benefits of computing relaxations with SBLM and PDDM compared to directly using the exact convex hull.\n\n\nEXPERIMENTAL EVALUATION\n\n\nExperimental Setup\n\nThe neural network certification benchmarks for fully connected networks were run on a 20 core 2.20GHz Intel Xeon Silver 4114 CPU with 100 GB of main memory and those for convolutional networks on a 16 Core 3.6GHz Intel i9-9900K with 64GB of main memory and an NVIDIA RTX 2080Ti. We use Gurobi 9.0 for solving MILP and LP problems [Gurobi Optimization, LLC 2018].\n\n\nBenchmarks\n\nWe evaluate Prima on a wide range of networks based on ReLU, Tanh, and Sigmoid activations:\n\n\u2022 The set of fully-connected and convolutional ReLU networks 5 from [Singh et al. 2019a] trained using DiffAI , PGD [Madry et al. 2018], Wong [Wong et al. 2018], and natural training (see results on MNIST and CIFAR10 in Table 2). \u2022 The published set of CIFAR10 convolutional networks from [Dathathri et al. 2020], trained using either just PGD or a mix of standard and PGD training (see results on CIFAR10 in Table 3). \u2022 The set of fully-connected and convolutional Tanh and Sigmoid networks from [Singh et al. 2019a] trained using natural training (see results on MNIST in Table 5). \u2022 The NVIDIA self-driving car network architecture DAVE [Bojarski et al. 2016] trained on a steering angle prediction task using the Udacity self-driving car dataset [Udacity 2016] with 31 834 train and 1 974 test samples 6 , an input resolution of 3 \u00d7 66 \u00d7 200, and PGD [Madry et al. 2018] training (see results in Table 6).\n\nWhile we evaluate performance for the widely considered and challenging \u2113 \u221e perturbations 7 , Prima can also be applied to other specifications including individual fairness [Ruoss et al. 2020b], global safety properties [Katz et al. 2017], acoustic [Ryou et al. 2020], geometric [Balunovic et al. 2019], and spatial [Ruoss et al. 2020a] based perturbations. For classification tasks and ReLU networks, we compare Prima with a range of state-of-theart incomplete verifiers notably also the ReLU-specialized kPoly [Singh et al. 2019a], OptC2V , and additionally the highly optimized and fully GPU-based -Crown ] (in incomplete mode). For classification using Tanh and Sigmoid activations, fewer verifiers are available and thus we compare with the state-of-the-art incomplete verifier DeepPoly [Singh et al. 2019b]. Few verification methods consider the regression setting and to the best of our knowledge, we are the first to analyze the full-size DAVE network. Neurify ] analyses a heavily scaled-down version in a binary classification setting, but in complete mode it does not scale to the much larger networks analysed here. In incomplete mode, it uses the same bounds as DeepZono [Singh et al. 2018] and is less precise than GPUPoly [M\u00fcller et al. 2020] to which we compare. -Crown does not support regression tasks and while an extension might be possible, it is non-trivial. It is also unclear if the approach scales to networks of this size. FastC2V , kPoly [Singh et al. 2019a], RefinePoly [Singh et al. 2019b], DeepPoly [Singh et al. 2019c] (equivalent bounds to Crown  For our experiments, we use the setup outlined in Section 6 which is similar to kPoly in [Singh et al. 2019a]. We use DeepPoly or GPUPoly (for convolutional networks) to determine the octahedral input bounds required to compute the multi-neuron constraints with Prima. For fully-connected networks, we refine the neuron-wise bounds of unstable neurons using the MILP encoding from Tjeng et al. [2019] for the second activation layer (the first layer bounds are already exact) and an LP encoding for the remaining layers. We note that encoding more layers with MILP does not scale on these networks. For convolutional networks, we encode some of the neurons in the last one or two layers using the MILP encoding from [Tjeng et al. 2019]. We note that the concurrent bound optimization in -Crown corresponds to simultaneous bound-refinement on all neurons of all layers, which is orthogonal to our approach and a promising direction to be explored in future work (though intractable without a GPU-based LP solver). We report as Accuracy the number of correctly classified samples out of the considered test set, as # Upper Bound the number of properties that could not be falsified and hence form an upper bound to the number of certifiable properties, as # Ver the number of verified regions, and as Time the average runtime per correctly classified sample in seconds.\n\n\nImage Classification with ReLU Activation\n\nWe compare Prima against the state-of the art methods kPoly and OptC2V in Table 2 and -Crown in Table 3. Computing multi-neuron constraints for groups of = 4 ReLU neurons becomes feasible with SBLM and PDDM reducing the time per group from several minutes, when directly computing exact convex hulls as in kPoly, to less than 50 milliseconds. Nevertheless, we find empirically that the best strategy to leverage this speed-up is to evaluate a large variety of small groups. Unless reported differently, we consider overlapping groups of size = 3 with = 100.\n\nComparison with the state-of-the-art. Figure 13 shows scatter plots comparing the runtime and precision of Prima with those of other state-of-the-art verifiers on the robustness certification of a normally trained 5 \u00d7 100 MLP, a provably trained ConvBig (MNIST) and an adversarially trained ConvSmall (CIFAR10). We note that adversarially and provably trained networks sacrifice accuracy for ease of certification, making normally trained networks more relevant and challenging. Here, fast, purely propagation-based, incomplete verifiers like DeepPoly verify only about 16% of the images. In contrast, Prima verifies 51% in < 160 seconds per image. The closest verifiers in terms of precision are kPoly and OptC2V, which verify 44% and 43% of samples and take around 310 and 140 seconds, respectively. Based on these observations, we compare Prima with kPoly and OptC2V on the remaining benchmarks from [Singh et al. 2019a]. Table 3. Number of verified adversarial regions of the 100 random samples from the CIFAR10 test set evaluated by ]. CNN-A-Mix is trained using a combination of adversarial and natural training and CNN-B-Adv only adversarially. Both are taken from [Dathathri et al. 2020]. Comparison with kPoly and OptC2V. For all normally trained networks, Prima is significantly more accurate than both kPoly [Singh et al. 2019a] and OptC2V , verifying between 44 and 201 more regions than the better of the two while sometimes also being significantly faster. These results are summarized in Table 2. For the, comparatively easy to verify (as can be seen in Figure 13(b)), DiffAI trained ConvBig MNIST network, we gain less precision verifying only 4 more regions than OptC2V. However, the easier proofs come at the cost of reduced accuracy, making them less relevant for real-world applications. For both PGD-trained CIFAR10 networks, Prima verifies between 23 and 59 more regions than kPoly and OptC2V while being around four times faster. On the provably trained ResNet, Prima is 50x faster than kPoly and able to decide all properties. However, this network is so heavily regularized that even complete verification via a MILP encoding is tractable. In summary, Prima is usually faster than kPoly and OptC2V, especially on larger networks, and is always more precise, sometimes substantially so. Table 4. Evaluation of a range of parameters for grouping set size , group size , and overlap , partial MILP refinement, and neuron-wise bound refinement for the first 100 samples of the MNIST test set and the normally trained 5 \u00d7 100. Of the first 100 samples, 99 are classified correctly and for 9 of those a counterexample is known. Comparison with -Crown. -Crown ] is a highly optimized, fully GPU-based complete BaB [Morrison et al. 2016] solver, supporting only ReLU activations 8 and the classification setting. When comparing complete and incomplete verifiers on accuracy, it is crucial to ensure that similar runtimes were achieved, as complete verifiers can, given sufficient time, decide any property. The GPU-based LP solver underlying -Crown is an orthogonal development to the Prima multi-neuron constraints. Prima currently uses a much slower CPU-based solver which is the main bottleneck for large networks as the runtime for computing multineuron constraints becomes small via our improved algorithms (see Section 7.8). We consider combining the GPU-based solver from -Crown with our multi-neuron approximations as an interesting item for future work. Despite the discrepancy in LP-solver performance distorting the comparison, Prima is still significantly faster on CNN-A-Mix while also achieving notably higher precision. On the larger network CNN-B-Adv, where LP-solver performance is more dominant, -Crown achieves slightly higher precision and smaller runtime. Unfortunately, we could not run the public version of -Crown without soundness issues on the networks from [Singh et al. 2019a] and consequently only compare on networks they provide. The recent SDP-based (semidefinite programming) SDP-FO [Dathathri et al. 2020] takes many hours per sample and is outperformed by -Crown. Thus we do not compare to it directly.\n\n\nParameter Study\n\nIn Table 4, we compare the effect of different parameter combinations on runtime and accuracy for the 5 \u00d7 100 MLP, which allows also more expensive settings to be evaluated while still representing a challenging verification problem with = 0.026. Using the single-neuron triangle relaxation ( = 1) only 21 regions can be verified. Adding our multi-neuron constraints with partition sizes of = 10 and = 20 increases this to 26 and 28 regions, respectively. Neither considering a larger overlap ( = 2), nor larger groups ( = 4), nor larger partition sizes ( = 100) can increase the number of verified regions, despite significantly increased the runtime. While using triangle relaxations with a partial MILP encoding is relatively fast it also only increases the accuracy to 23 regions. In contrast, combining a partial MILP encoding with multi-neuron constraints yields, depending on the exact setting, an almost 75% increase to 35 verified regions, although at the price of increased runtime. Refining the neuron-wise bounds using a triangle relaxation and LP encoding only improves the number of verified regions to 27, while additionally using multi-neuron constraints yields a significant jump to 45. This further improves to 54 when using MILP to refine the second layer bounds and 60 when additionally encoding the last two layers with MILP. The significant increase in precision when combining tight multi-neuron constraints computed via SBLM and PDDM with other methods demonstrates their utility and highlights the potential of our abstraction-refinement-based approach. Normalized bound improvement [%] our sparse heuristic random Fig. 14. Normalized bound improvement over the fraction of groups used to compute multi-neuron constraints, . Our method is the blue circle, whose gain is normalized to 100%.\n\n\nEffect of Grouping Strategy\n\nWe evaluate the sensitivity of Prima to the chosen neuron groupings, by comparing the performance 9 of random groups with those generated by our sparse grouping heuristic in Figure 14 for the first 100 test images of CIFAR10 and the ConvSmall network. Concretely, we first generate a deterministic sparse grouping with our heuristic for a group size of = 3, a partition size of = 100, and a maximum overlap of = 1. Then we (randomly) reduce this grouping to a fraction (x-axis in Figure 14) of the original number of groups. The random groupings are generated to have the same size (number of groups) by repeatedly drawing indices uniformly at random and rejecting duplicates. We observe that considering fewer groups from our heuristic (blue in Figure 14) reduces the bound improvement notably, e.g., to 37% at = 0.1 (blue square). Choosing random groups (orange in Figure 14) is consistently worse (vertical gap in Figure 14); by around 10% at = 1.0 (circles) closing to 3.4% at = 0.1 (squares). While our heuristic generates groups with small overlap to evenly cover all neurons, random sampling can lead to some groups with large overlap, while potentially not covering some neurons at all, leading to worse performance. 9 Concretely, we compare the obtained improvement of \u210e , , the lower bound to the optimization objective min \u2032 \u2208B \u221e ( \u2032 ) \u2212 ( \u2032 ) , over the triangle relaxation (\u0394) normalized using our standard sparse heuristic (Prima):\n(\u210e , \u2212 \u210e \u0394 , )/(\u210e Prima , \u2212 \u210e \u0394 , )\nConsidering fewer groups makes overlaps between groups less likely, making the groupings resulting from the two sampling strategies more similar and explaining the shrinking performance gap. To obtain the same precision with random groups as with our heuristic, about twice as many ( = 2.0, diamond) groups are needed (horizontal gap in Figure 14). We repeated these experiments several times with different random seeds and obtained consistent results.\n\nOverall, we conclude that while our heuristic consistently outperforms random groups, Prima is relatively insensitive to the exact groupings, as long as sufficiently many are used. While using the exact convex hull algorithm for ReLU relaxations is merely slow, it becomes infeasible for non-piecewise-linear activations such as Tanh and Sigmoid. Computing the constraints for a single group of = 3 neurons can take minutes using direct exact convex hull computation, whereas SBLM using PDDM takes only 10 milliseconds. This dramatic speed-up is a result of SBLM's decompositional approach of solving the problem in lower dimensions (see Section 5), significantly reducing its complexity. Note that both methods compute only approximations of the optimal group-wise convex relaxation for these cases, as the underlying interval-wise bounds are not exact.\n\n\nImage Classification with Tanh and Sigmoid Activations\n\nWe evaluate our method on normally trained, fully-connected and convolutional networks for the MNIST dataset. We choose an for the \u221e region such that the state-of-the-art verifier for Tanh and Sigmoid activations, DeepPoly, verifies less than 50% of the regions. We remark that DeepPoly is based on the same principles and has similar precision as other state-of-the-art verifiers for these activations such as CNN-Cert [Boopathy et al. 2019] and Crown .\n\nWe use overlapping groups with = 10 and again refine neuron-wise lower-and upper-bounds for fully-connected networks. We verify between 14% and 34% more regions than the current stateof-the-art, in some cases doubling the number of verified samples, while maintaining a reasonable runtime comparable to that for ReLU networks (see Table 5). We evaluate Prima in the setting of autonomous driving, deriving upper and lower bounds to the predicted steering angle under an \u2113 \u221e threat-model in a regression setting. We thereby demonstrate scalability to large networks (> 100k neurons and over 27 million connections) and inputs (3 \u00d7 66 \u00d7 200) of real-world relevance. We report the certified maximum absolute steering angle error and the width of reachable steering angles. We  use PGD [Madry et al. 2018] to compute empirical bounds (emp). We use the CNN architecture proposed by Bojarski et al. [2016] and adversarial training [Madry et al. 2018] on the Udacity autonomous driving dataset [Udacity 2016] to obtain the network evaluated here. Fig. 15. Samples from the self-driving car dataset. The target steering angle is illustrated in green, the predicted one in blue. The empirical bounds for = 2/255 are shown in red and the certified range is shaded blue.\n\n\nAutonomous Driving\n\nWhen the permissible perturbation size is small and the standard error of the model is larger than the perturbation effect, cheaper methods such as GPUPoly already yield good results. However, for larger perturbations, Prima reduces the gap between empirical and certified error around 20% (see Table 6). In Figure 15, we show two representative samples, where the certified steering angle range for = 2/255 is shaded blue, the empirical bounds on the steering angle are shown in red, the target in green and the prediction on the unperturbed sample in blue. Qualitatively, we find that while the network often still performs well on unperturbed samples with poor lighting or contrast (see lower example in Figure 15) the sensitivity to perturbations and consequently the width of the reachable steering angle range is much larger than for samples in better conditions (see upper example in Figure 15). \n\n\nEffectiveness of SBLM and PDDM for Convex Hull Computations\n\nComputing approximations with SBLM using PDDM has two main advantages compared to the direct convex hull approach: It is significantly faster and produces fewer constraints, making the resulting LP easier to solve, while barely losing any precision. For example, verifying the 5 \u00d7 100 network with Prima and comparing abstractions for groups of = 3 computed with SBLM and PDDM or naively and neuron-wise triangle relaxation ( Figure  16), we observe the following: Using SBLM and PDDM we reduce the mean number of constraints computed per neuron-group by over 70% from 156 to 44 significantly reducing the number of constraints in the resulting LP, as many hundred such neuron groups are considered. The mean volume of the constraint polytopes defined by these constraints in the 6-dimensional input-output space of the individual neuron groups, meanwhile, is only around 5% larger. Single neuron constraints, in contrast, yield 4-times larger volumes. Additionally, computing the approximate constraints is about 200 times faster than the exact convex hull.\n\nNot only are Prima constraints faster to generate and allow the verification of the same properties, but a runtime analysis for the first 100 samples (illustrated in Figure 18) shows that they also speed up the final LP solve 8-fold compared to the naive approach, as significantly fewer constraints have to be considered. This effect is also observed in the time-intensive neuron-wise bound-refinement where Prima constraints reduce the runtime by 70% while allowing 3 additional regions to be verified. This can be explained by the fewer but more diverse Prima constraints also speeding up the final LP solve in the refinement step reducing the number of timeouts and allowing tighter neuron-wise bounds to be computed. Using neuron-refinement with Prima is in fact still quicker than the naive approach without any refinement, while almost verifying twice as many samples. SBLM combined with exact convex hulls computations already yields a small speed-up of around 20%, but the synergy with PDDM is key to unlock its full potential.\n\nAn analysis of the runtime contributions of the octahedral input constraint computation, the multi-neuron constraint computation and the final LP solve (illustrated in Figure 17), shows the following: Using the naive approach, the multi-neuron constraint computation clearly dominates the runtime, while only contributing around 50% when using SBLM and PDDM. For larger networks, the input constraint computation and LP-solve become more expensive, reducing the multi-neuron constraints computation runtime contribution further and further, e.g., 7% for the CIFAR10 ConvSmall, and shifting the performance bottleneck to the LP-solver, especially when neuron-wise bound-refinement or partial MILP encodings are used.\n\n\nRELATED WORK\n\nThe importance of certifying the robustness of neural networks to input perturbations has created a surge of research activity in recent years. The approaches with deterministic guarantees can be divided into exact and incomplete methods. Incomplete methods are much faster and more scalable than exact ones, but they can be imprecise, i.e., they may fail to certify a property even if it holds.\n\nComplete methods are mostly based on satisfiability modulo theory (SMT) [Ehlers 2017;Huang et al. 2017;Katz et al. 2017Katz et al. , 2019 or the branch-and-bound approach Botoeva et al. 2020;Bunel et al. 2020b;Lu and Kumar 2020;Palma et al. 2021;Tjeng et al. 2019;, often implemented using mixed integer linear programming (MILP). These methods offer exactness guarantees but are based on solving NP-hard optimization problems, which can make them intractable even for small networks. Incomplete methods can be divided into bound propagation approaches [Gowal et al. 2019;M\u00fcller et al. 2020;Singh et al. 2018Singh et al. , 2019b] and those that generate polynomially-solvable optimization problems [Bunel et al. 2020a;Dathathri et al. 2020;Lyu et al. 2020;Raghunathan et al. 2018;Singh et al. 2019a;Xiang et al. 2018] such as linear programming (LP) or semidefinite programming (SDP) optimization problems. Compared to deterministic certification methods, randomized smoothing [Cohen et al. 2019;Lecuyer et al. 2018;Salman et al. 2019a] is a defence method providing only probabilistic guarantees and incurring significant runtime costs at inference time, with the generalization to arbitrary safety properties still being an open problem.\n\nA new avenue towards more precision are methods [Palma et al. 2021;Singh et al. 2019a;] breaking the so-called convex barrier [Salman et al. 2019b] by considering activation functions jointly. However, their scalability is limited by the need to solve NP-hard convex hull problems. There are many approaches for solving the convex hull problem for polyhedra exactly Fukuda 1991, 1992;Barber et al. 1993;Dantzig 1998;Edelsbrunner 2012;Fukuda and Prodon 1995;Joswig 2003;Motzkin et al. 1953], in contrast to few approximate methods which either sacrifice soundness [Bentley et al. 1982;Khosravani et al. 2013;Sartipizadeh and Vincent 2016;Zhong et al. 2014] or still exhibit exponential complexity [Xu et al. 1998], prohibiting their use in neural network verification.\n\nOur work follows the line of convex barrier-breaking methods, generalizing the concept to arbitrary bounded, multivariate activations. In contrast to prior work, we decompose the underlying convex hull problem into lower-dimensional spaces and solve it approximately using a novel relaxed Double Description, irredundancy formulation, and a new ray-shooting-based algorithm to add multiple constraints jointly. The resulting speed-ups make Prima tractable for non-piecewise-linear activations, a first for convex barrier-breaking methods.\n\n\nCONCLUSION\n\nWe presented Prima, a general framework that substantially advances the state-of-the-art in neural network verification by providing efficient multi-neuron abstractions for arbitrary, bounded, multivariate non-linear activation functions. Our key idea is to compute tighter overall abstractions by considering many overlapping neuron groups thereby capturing more inter-neuron dependencies. To enable this, we decompose the bottleneck convex hull computation into lower-dimensional spaces and solve it approximately. Our extensive experimental evaluation shows that our algorithmic advances shift the bottleneck to the LP-solver while significantly improving both precision and scalability over prior work.\n\nFig. 1 .\n1Illustration of the tightness of different abstraction strategies, for a layer of four neurons (grey dots).\n\nFig. 2 .\n2Convex single-neuron approximation (blue) of a ReLU (black) with bounded inputs \u2208 [ , ].\n\nFig. 4 .\n4Illustration of the Split-Bound-Lift Method for a group of = 2 neurons and a ReLU activation.\n\nFig. 5 .\n5Comparison of 2-neuron and 1-neuron constraints projected into 2 -1 -2 -space for a ReLU activation, given input polytope P .\n\nFig. 6 .\n6Illustration of the Partial Double Description. Input constraints (a), exact vertex enumeration R (b), unsound partial vertex enumeration violating the PDD definition (c), partial or approximate vertex enumeration R (d), and A-irredundant versions of the partial vertex enumeration (e).\n\nFig. 8 .\n8Top: polytope in primal (left) and dual (right) space. Bottom: equivalent polyhedral cones in homogenized coordinates. In red: the plane the cone can be intersected with to recover the polytope.\n\nFig. 10 .\n10Boosting intersection precision by combining both directions of batch intersection. Input polytopes in PDD with exact H -representation (black) and approximate V-representation (P 1 green and P 2 red) (a), batch intersection of P 1 with the H -representation of P 2 (b), batch intersection in the opposite direction (c), combining both intersections (d), and applying A-irredundancy (e).\n\n\u25a1 5\n5SPLIT-BOUND-LIFT METHOD Algorithm 3: Split-Bound-Lift Method (SBLM) Input: Variable ordering I, input polytope P, set of bounding regions D and set of bounds B Output: Jointly constraining polytope K if |I| > 0 then Get next output variable: \u2190 I 0 foreach D , B in D, B do Split region: P = P \u2229 D Apply SBLM: K \u2190 SBLM(I 1: , P , D, B) Extend into space including : K \u2190 K \u00d7 R Apply bounds B : K \u2190 K \u2229 B Compute convex hull: K = PDDM({K } ) return K else return P In this section, we explain the Split-Bound-Lift Method in greater detail. Recall that we use the SBLM to compute k-neuron abstractions, by approximating the convex hull conv({( , ( )) | \u2208 P \u2286 [ , ] }) for a group of neurons and their activation functions ( ) = [ 1 ( 1 ), ..., ( )] \u22a4 , assuming that their inputs are constrained by the polytope P.\n\nFig. 11 .\n11Interval-wise bounds for the Sigmoid function on the intervals [ , ] and [ , ].\n\n\n0] and D 2 = [0, \u221e], with B 1 = ( \u2265 0, \u2264 0) and B 2 = ( \u2265 , \u2264 ), obtaining exact bounds on both intervals. Tanh and Sigmoid. Let be an S-curve function with domain [ , ], that is \u2032\u2032 ( ) \u2265 0 for \u2264 0, \u2032\u2032 ( ) \u2264 0 for \u2265 0 and \u2032 ( ) > 0 for \u2208 [ , ]. Both Sigmoid ( ) = +1 and Tanh tanh( ) = \u2212 \u2212 + \u2212 have these properties. We split the domain at \u2208 [ , ] into D 1 = [\u2212\u221e, ] and D 2 = [ , \u221e], choosing to minimize the area between upper and lower bound in the input-output plane, using the bounds from Singh et al. [2019b]: ( ) \u2264 \u2264 = ( ) + ( \u2212 ) \u2032 ( ), \u2032 ( )), else, ( ) \u2265 \u2265 = ( ) + ( \u2212 ) ( )\u2212 ( ) \u2212 , if \u2265 0, min( \u2032 ( ), \u2032 ( )), else,\n\nFig. 12 .\n12Polyhedral bounding regions D and corresponding bounds B for the 2 MaxPool function on the input region [ 1 , 1 ] \u00d7 [ 2 , 2 ].\n\nFig. 13 .\n13Comparison of the runtime/accuracy trade-off of Prima (ours), OptC2V [Tjandraatmadja et al. 2020],\n\n\nof constraint computation using SBLM and PDD compared to an exact convex hull.\n\nFig. 16 .Fig. 17 .\n1617Case study: Analysis of the distribution of the number of discovered constraints, abstraction volume, and runtime over all (\u2248 360) individual 3-neuron groups processed during the verification of a single MNIST image on the 5 \u00d7 100 ReLU network. Comparison of the runtime contribution of the octahedral input constraint computation, multi-neuron constraint computation and LP solve.\n\nTable 1 .\n1Neural network architectures used in experiments.Dataset \nModel \nType \nNeurons Layers Activation \n\nMNIST \n5 \u00d7 100 5 \nFC \n510 \n5 ReLU \n6 \u00d7 100 \nFC \n600 \n6 Tanh/Sigm \n8 \u00d7 100 5 \nFC \n810 \n8 ReLU \n\n9 \u00d7 100 \nFC \n900 \n9 Tanh/Sigm \n5 \u00d7 200 5 \nFC \n1 010 \n5 ReLU \n6 \u00d7 200 \nFC \n1 200 \n6 Tanh/Sigm \n8 \u00d7 200 5 \nFC \n1 610 \n8 ReLU \n\nConvSmall \n\nConv \n3 604 \n3 Relu/Tanh/Sigm \n\nConvBig \n\nConv \n48 064 \n6 ReLU \n\nCIFAR10 \n\nConvSmall \n\nConv \n4 852 \n3 ReLU \n\nCNN-A-Mix \n\nConv \n6 244 \n3 ReLU \n\nCNN-B-Adv \n\nConv \n16 634 \n3 ReLU \n\nConvBig \n\nConv \n62 464 \n6 ReLU \n\nResNet \n\nResidual \n107 496 \n10 ReLU \n\nSelf-Driving DAVE \nConv \n107 032 \n8 ReLU + Tanh \n\n\n\nTable 2 .\n2Number of verified adversarial regions of the first 1 000 samples and runtime for Prima, OptC2V \n[Tjandraatmadja et al. 2020], and kPoly [Singh et al. 2019a]. Natural (NOR), adversarial (PGD [Madry et al. \n2018]), or provable (DiffAI [Mirman et al. 2018], Wong [Wong et al. 2018]) training was used. \n\nDataset \nModel \nTraining Accuracy \nkPoly \nOptC2V  \u2020 \nPrima (ours) # Upper Bound \n\n# Ver Time # Ver Time # Ver Time \n\nMNIST \n5 \u00d7 100 \nNOR \n960 \n0.026 100 \n441 \n307 \n429 \n137 \n510 \n159 \n842 \n8 \u00d7 100 \nNOR \n947 \n0.026 100 \n369 \n171 \n384 \n759 \n428 \n301 \n820 \n5 \u00d7 200 \nNOR \n972 \n0.015 \n50 \n574 \n187 \n601 \n403 \n690 \n224 \n901 \n8 \u00d7 200 \nNOR \n950 \n0.015 \n50 \n506 \n464 \n528 3451 \n612 \n395 \n911 \n\nConvSmall NOR \n980 \n0.120 100 \n347 \n477 \n436 \n55 \n640 \n51 \n733 \n\nConvBig \n\nDiffAI \n929 \n0.300 100 \n736 \n40 \n771 \n102 \n775 \n5.5 \n790 \n\nCIFAR10 ConvSmall PGD \n630 \n2/255 100 \n399 \n86 \n398 \n105 \n458 \n16 \n481 \n\nConvBig \n\nPGD \n631 \n2/255 100 \n459 \n346 n/a  \u2020 n/a  \u2020 \n482 \n128 \n550 \n\nResNet \n\nWong \n290 \n8/255 \n50 \n245 \n91 n/a  \u2020 n/a  \u2020 \n248 \n1.9 \n248 \n\n \u2020 The OptC2V [Tjandraatmadja et al. 2020] code has not been released; we report their runtimes and results where available. \n\n\n\nTable 5 .\n5Number of verified adversarial regions and runtime in seconds of Prima vs. DeepPoly for Tanh/Sigmoid on 100 images from the MNIST dataset.Act. \nModel \nAcc. \nDeepPoly \nPrima \n\nVer. Time Ver. Time \n\nTanh 6 \u00d7 100 \n97 \n0.006 \n38 \n0.3 \n61 \n72.5 \n9 \u00d7 100 \n98 \n0.006 \n18 \n0.4 \n52 186.0 \n\n6 \u00d7 200 \n98 \n0.002 \n39 \n0.6 \n68 170.0 \n\nConvSmall \n\n99 \n0.005 \n16 \n0.4 \n30 \n27.8 \n\nSigm 6 \u00d7 100 \n99 \n0.015 \n30 \n0.3 \n53 \n96.9 \n9 \u00d7 100 \n99 \n0.015 \n38 \n0.5 \n56 336.4 \n\n6 \u00d7 200 \n99 \n0.012 \n43 \n1.0 \n73 267.0 \n\nConvSmall \n\n99 \n0.014 \n30 \n0.5 \n51 \n47.0 \n\n\n\nTable 6 .\n6Standard (std.), empirically maximal (emp.) and certifiably maximal (cert.) mean absolute steering angle error (MAE) (smaller is better) for Prima vs. GPUPoly evaluated on every 20 \u210e sample and mean evaluation time.Method \nstd. \nMAE \n\nemp. \nMAE \n\ncert. \nMAE \n\ncert. \nWidth \nTime [s] \n\n1/255 GPUPoly 7.37\u00b09.41\u00b010.35\u00b05.75\u00b01.55 \nPrima \n7.37\u00b09.41\u00b010.17\u00b05.30\u00b0154.2 \n\n2/255 GPUPoly 7.37\u00b011.46\u00b018.35\u00b019.63\u00b02.41 \nPrima \n7.37\u00b011.46\u00b017.05\u00b017.03\u00b0239.5 \n\n\n\n\nNaive Convex Hul SBLM Convex Hul SBLM + PDDM Fig. 18. Runtime comparison of using SBLM vs. exact convex hull for computing relaxations in Prima. Evaluated on 100 images and the MNIST 5 \u00d7 100 ReLU network.0 \n100 \n200 \n300 \n400 \n500 \nRuntime per image [s] \n\nNaive Convex Hull \n+ Refine \n\nSBLM + PDDM \n+ Refine \n\n28 verified \n133.3s \n\n28 verified \n107.0s \n\n28 verified \n15.5s \n\n52 verified \n313.0s \n\n55 verified \n99.8s \n\nOctahedron constraints \nMulti-Neuron constraints \nNeuron refinement \nFinal LP Solve \nMiscellaneous \n\n\nProc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.\nFor piecewise-linear activations, typically is chosen large enough such that there is only one set. Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.\nExtending a -dimensional polytope by a variable defines it in the + 1-dimensional space, where it is (initially) unbounded in the dimension of the added variable.\nAn under-approximation in dual space corresponds to an over-approximation in primal space, due to inclusion reversion.\nThe networks referred to as 6 \u00d7 \u00b7 00 and 9 \u00d7 \u00b7 00 in previous work only include 5 and 8 hidden layers, respectively, and have therefore been renamed.Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.\nThe labels of the original test set are not available (anymore), so we used videos 1, 2, 5, and 6 as train and video 4 (instead of 3) as test dataset. 7 That is, := ( ) = ( \u2032 ), \u2200 \u2032 \u2208 B \u221e := { \u2208 X | | | \u2212 \u2032 | | \u221e \u2264 } \u21d4 min \u2032 \u2208B \u221e ( \u2032 ) \u2212 ( \u2032 ) > 0, \u2200 \u2260\nExtensions to piecewise-linear activations with more than = 2 linear regions would significantly increase runtime (O ( ) with split depth ), while precision would be significantly lower for non-piecewise linear activations.\n\nOptimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness. Greg Anderson, Shankara Pailoor, Isil Dillig, Swarat Chaudhuri, 10.1145/3314221.3314614Proc. Programming Language Design and Implementation (PLDI). Programming Language Design and Implementation (PLDI)Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri. 2019. Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness. In Proc. Programming Language Design and Implementation (PLDI). 731-744. https://doi.org/10.1145/3314221.3314614\n\nStrong mixed-integer programming formulations for trained neural networks. Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, Juan Pablo Vielma, 10.1007/s10107-020-01474-5Mathematical Programming. Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma. 2020. Strong mixed-integer programming formulations for trained neural networks. Mathematical Programming (2020), 1-37. https://doi.org/10. 1007/s10107-020-01474-5\n\nA basis enumeration algorithm for linear systems with geometric applications. David Avis, Komei Fukuda, 10.1016/0893-9659(91)90141-HApplied Mathematics Letters. 4David Avis and Komei Fukuda. 1991. A basis enumeration algorithm for linear systems with geometric applications. Applied Mathematics Letters 4, 5 (1991), 39-42. https://doi.org/10.1016/0893-9659(91)90141-H\n\nA pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra. David Avis, Komei Fukuda, 10.1007/BF02293050Discrete & Computational Geometry. 8David Avis and Komei Fukuda. 1992. A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra. Discrete & Computational Geometry 8, 3 (1992), 295-313. https://doi.org/10.1007/BF02293050\n\nCertifying Geometric Robustness of Neural Networks. Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, Martin T Vechev, ; , Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaMislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin T. Vechev. 2019. Certifying Geometric Robustness of Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15287-15297. https://proceedings.neurips.cc/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html\n\nThe quickhull algorithm for convex hull. Bradford Barber, Hannu David P Dobkin, Huhdanpaa, 10.1145/235815.235821GCG53The Geometry CenterTechnical ReportC Bradford Barber, David P Dobkin, and Hannu Huhdanpaa. 1993. The quickhull algorithm for convex hull. Technical Report. Technical Report GCG53, The Geometry Center, MN. https://doi.org/10.1145/235815.235821\n\nApproximation algorithms for convex hulls. Jon Louis Bentley, P Franco, Mark G Preparata, Faust, 10.1145/358315.358392Commun. ACM. 25Jon Louis Bentley, Franco P Preparata, and Mark G Faust. 1982. Approximation algorithms for convex hulls. Commun. ACM 25, 1 (1982), 64-68. https://doi.org/10.1145/358315.358392\n\nEnd to end learning for self-driving cars. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, D Lawrence, Mathew Jackel, Urs Monfort, Jiakai Muller, Zhang, abs/1604.07316ArXiv preprintMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. 2016. End to end learning for self-driving cars. ArXiv preprint abs/1604.07316 (2016). https://arxiv.org/abs/1604.07316\n\nCNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks. Akhilan Boopathy, Pin-Yu Tsui-Wei Weng, Sijia Chen, Luca Liu, Daniel, 10.1609/aaai.v33i01.33013240The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAAAAI PressAkhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. 2019. CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. AAAI Press, 3240-3247. https://doi.org/10.1609/aaai.v33i01.33013240\n\nEfficient Verification of ReLU-Based Neural Networks via Dependency Analysis. Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, Ruth Misener, 10.1609/aaai.v34i04.5729The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceElena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener. 2020. Efficient Verification of ReLU-Based Neural Networks via Dependency Analysis. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 3291-3299. https://doi.org/10.1609/aaai.v34i04.5729\n\nAn efficient nonconvex reformulation of stagewise convex optimization problems. Rudy Bunel, Oliver Hinder, Srinadh Bhojanapalli, Krishnamurthy Dvijotham, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Rudy Bunel, Oliver Hinder, Srinadh Bhojanapalli, and Krishnamurthy Dvijotham. 2020a. An efficient nonconvex refor- mulation of stagewise convex optimization problems. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/ paper/2020/hash/5d97f4dd7c44b2905c799db681b80ce0-Abstract.html\n\nBranch and bound for piecewise linear neural network verification. Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Pushmeet Kohli, P Torr, Mudigonda, Journal of Machine Learning Research. 212020Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Pushmeet Kohli, P Torr, and P Mudigonda. 2020b. Branch and bound for piecewise linear neural network verification. Journal of Machine Learning Research 21, 2020 (2020).\n\nAn optimal convex hull algorithm in any fixed dimension. Bernard Chazelle, 10.1007/BF02573985Discrete & Computational Geometry. 10Bernard Chazelle. 1993. An optimal convex hull algorithm in any fixed dimension. Discrete & Computational Geometry 10, 4 (1993), 377-409. https://doi.org/10.1007/BF02573985\n\nThe octahedron abstract domain. Robert Claris\u00f3, Jordi Cortadella, 10.1007/978-3-540-27864-1_23Science of Computer Programming. 64Robert Claris\u00f3 and Jordi Cortadella. 2007. The octahedron abstract domain. Science of Computer Programming 64, 1 (2007), 115-139. https://doi.org/10.1007/978-3-540-27864-1_23\n\nCertified Adversarial Robustness via Randomized Smoothing. Jeremy M Cohen, Elan Rosenfeld, J Zico Kolter, PMLR, 1310-1320Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Proceedings of Machine Learning ResearchJeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. 2019. Certified Adversarial Robustness via Randomized Smoothing. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, 1310-1320. http://proceedings.mlr.press/v97/cohen19c.html\n\n. Patrick Cousot, 10.1145/234528.234740Abstract Interpretation. ACM Comput. Surv. 28Patrick Cousot. 1996. Abstract Interpretation. ACM Comput. Surv. 28, 2 (1996), 324-328. https://doi.org/10.1145/234528.234740\n\nLinear programming and extensions. George Bernard, Dantzig , 10.1515/9781400884179Princeton university press48George Bernard Dantzig. 1998. Linear programming and extensions. Vol. 48. Princeton university press. https://doi.org/10. 1515/9781400884179\n\nEnabling certification of verificationagnostic networks via memory-efficient semidefinite programming. Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian J Goodfellow, Percy Liang, Pushmeet Kohli, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian J. Goodfellow, Percy Liang, and Pushmeet Kohli. 2020. Enabling certification of verification- agnostic networks via memory-efficient semidefinite programming. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings. neurips.cc/paper/2020/hash/397d6b4c83c91021fe928a8c4220386b-Abstract.html\n\n. Herbert Edelsbrunner, 10.1007/978-3-642-61568-9Algorithms in combinatorial geometry. 10Springer Science & Business MediaHerbert Edelsbrunner. 2012. Algorithms in combinatorial geometry. Vol. 10. Springer Science & Business Media. https: //doi.org/10.1007/978-3-642-61568-9\n\nFormal verification of piece-wise linear feed-forward neural networks. Ruediger Ehlers, 10.1007/978-3-319-68167-2_19International Symposium on Automated Technology for Verification and Analysis. SpringerRuediger Ehlers. 2017. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis. Springer, 269-286. https://doi.org/10.1007/978-3-319-68167-2_19\n\nKomei Fukuda, 10.3929/ethz-b-000426218Polyhedral Computation. Komei Fukuda. 2020. Polyhedral Computation. https://doi.org/10.3929/ethz-b-000426218\n\nDouble description method revisited. Komei Fukuda, Alain Prodon, 10.1007/3-540-61576-8_77Franco-Japanese and Franco-Chinese Conference on Combinatorics and Computer Science. SpringerKomei Fukuda and Alain Prodon. 1995. Double description method revisited. In Franco-Japanese and Franco-Chinese Conference on Combinatorics and Computer Science. Springer, 91-111. https://doi.org/10.1007/3-540-61576-8_77\n\nAi2: Safety and robustness certification of neural networks with abstract interpretation. Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev, 10.1109/SP.2018.000582018 IEEE Symposium on Security and Privacy (SP). IEEE. Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 3-18. https://doi.org/10.1109/SP.2018.00058\n\nThe convex hull problem in practice: improving the running time of the double description method. Blagoy Genov, Ph.D. DissertationBlagoy Genov. 2015. The convex hull problem in practice: improving the running time of the double description method. Ph.D. Dissertation.\n\nScalable Verified Training for Provably Robust Image Classification. Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Arthur Mann, Pushmeet Kohli, 10.1109/ICCV.2019.00494IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). IEEESven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Arthur Mann, and Pushmeet Kohli. 2019. Scalable Verified Training for Provably Robust Image Classification. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019. IEEE, 4841-4850. https://doi.org/10.1109/ICCV.2019.00494\n\nSafety verification of deep neural networks. Llc Gurobi Optimization, Sen Marta Kwiatkowska, Min Wang, Wu, 10.1007/978-3-319-63387-9_1International Conference on Computer Aided Verification. SpringerGurobi Optimizer Reference ManualGurobi Optimization, LLC. 2018. Gurobi Optimizer Reference Manual. http://www.gurobi.com Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety verification of deep neural networks. In International Conference on Computer Aided Verification. Springer, 3-29. https://doi.org/10.1007/978-3-319-63387-9_1\n\nBeneath-and-beyond revisited. Michael Joswig, 10.1007/978-3-662-05148-1_1Algebra, Geometry and Software Systems. SpringerMichael Joswig. 2003. Beneath-and-beyond revisited. In Algebra, Geometry and Software Systems. Springer, 1-21. https: //doi.org/10.1007/978-3-662-05148-1_1\n\nReluplex: An efficient SMT solver for verifying deep neural networks. Guy Katz, Clark Barrett, L David, Kyle Dill, Julian, Kochenderfer, 10.1007/978-3-319-63387-9_5International Conference on Computer Aided Verification. SpringerGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. 2017. Reluplex: An efficient SMT solver for verifying deep neural networks. In International Conference on Computer Aided Verification. Springer, 97-117. https: //doi.org/10.1007/978-3-319-63387-9_5\n\nThe marabou framework for verification and analysis of deep neural networks. Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji\u0107, 10.1007/978-3-030-25540-4_26International Conference on Computer Aided Verification. SpringerGuy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji\u0107, et al. 2019. The marabou framework for verification and analysis of deep neural networks. In International Conference on Computer Aided Verification. Springer, 443-452. https://doi.org/10.1007/978-3-030-25540-4_26\n\nA simple algorithm for convex hull determination in high dimensions. Hamid R Khosravani, E Ant\u00f3nio, Pedro M Ruano, Ferreira, 10.1109/WISP.2013.66574922013 IEEE 8th International Symposium on Intelligent Signal Processing. IEEEHamid R Khosravani, Ant\u00f3nio E Ruano, and Pedro M Ferreira. 2013. A simple algorithm for convex hull determination in high dimensions. In 2013 IEEE 8th International Symposium on Intelligent Signal Processing. IEEE, 109-114. https: //doi.org/10.1109/WISP.2013.6657492\n\nCertified Robustness to Adversarial Examples with Differential Privacy. Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana, 10.1109/SP.2019.00044IEEE Symposium on Security and Privacy. Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2018. Certified Robustness to Adversarial Examples with Differential Privacy. 2019 IEEE Symposium on Security and Privacy (S&P) (2018). https: //doi.org/10.1109/SP.2019.00044\n\nNeural Network Branching for Neural Network Verification. Jingyue Lu, M. Pawan Kumar, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Jingyue Lu and M. Pawan Kumar. 2020. Neural Network Branching for Neural Network Verification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https: //openreview.net/forum?id=B1evfa4tPB\n\nFastened CROWN: Tightened Neural Network Robustness Certificates. Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, Luca Daniel, 10.1609/aaai.v34i04.5944The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceZhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca Daniel. 2020. Fastened CROWN: Tightened Neural Network Robustness Certificates. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 5037-5044. https://doi.org/10.1609/aaai.v34i04.5944\n\nTowards Deep Learning Models Resistant to Adversarial Attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id= rJzIBfZAb\n\nEfficient elimination of redundancies in polyhedra using raytracing. Alexandre Mar\u00e9chal, Micha\u00ebl P\u00e9rin, Alexandre Mar\u00e9chal and Micha\u00ebl P\u00e9rin. 2017. Efficient elimination of redundancies in polyhedra using raytracing.\n\nDifferentiable Abstract Interpretation for Provably Robust Neural Networks. Matthew Mirman, Timon Gehr, Martin T Vechev, PMLRProceedings of the 35th International Conference on Machine Learning. Jennifer G. Dy and Andreas Krausethe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, Sweden80Proceedings of Machine Learning ResearchMatthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Differentiable Abstract Interpretation for Provably Robust Neural Networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 3575-3583. http://proceedings.mlr.press/v80/mirman18b.html\n\nBranch-and-bound algorithms: A survey of recent advances in searching, branching, and pruning. David R Morrison, H Sheldon, Jason J Jacobson, Edward C Sauppe, Sewell, 10.1016/j.disopt.2016.01.005Discrete Optimization. 19David R Morrison, Sheldon H Jacobson, Jason J Sauppe, and Edward C Sewell. 2016. Branch-and-bound algorithms: A survey of recent advances in searching, branching, and pruning. Discrete Optimization 19 (2016), 79-102. https: //doi.org/10.1016/j.disopt.2016.01.005\n\nThe double description method. Howard Theodore S Motzkin, Raiffa, L Gerald, Robert M Thompson, Thrall, 10.1515/9781400881970-004Contributions to the Theory of Games. 2Theodore S Motzkin, Howard Raiffa, Gerald L Thompson, and Robert M Thrall. 1953. The double description method. Contributions to the Theory of Games 2, 28 (1953), 51-73. https://doi.org/10.1515/9781400881970-004\n\nChristoph M\u00fcller, Francois Serre, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. 2021. Scaling Polyhedral Neural Network Verification on GPUs. Proc. Machine Learning and Systems (MLSys). Christoph M\u00fcller, Francois Serre, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. 2021. Scaling Polyhedral Neural Network Verification on GPUs. Proc. Machine Learning and Systems (MLSys) (2021).\n\nChristoph M\u00fcller, Gagandeep Singh, arXiv:cs.LG/2007.10868Markus P\u00fcschel, and Martin Vechev. 2020. Neural Network Robustness Verification on GPUs. Christoph M\u00fcller, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. 2020. Neural Network Robustness Verification on GPUs. arXiv:cs.LG/2007.10868\n\nScaling the Convex Barrier with Active Sets. Alessandro De Palma, Harkirat S Behl, Rudy R Bunel, H S Philip, M. Pawan Torr, Kumar, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaAlessandro De Palma, Harkirat S. Behl, Rudy R. Bunel, Philip H. S. Torr, and M. Pawan Kumar. 2021. Scaling the Convex Barrier with Active Sets. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=uQfOy7LrlTR\n\nSemidefinite relaxations for certifying robustness to adversarial examples. Aditi Raghunathan, Jacob Steinhardt, Percy Liang, ; Montr\u00e9al, Canada , Samy Bengio, Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman GarnettNeurIPSAditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 10900-10910. https://proceedings.neurips.cc/paper/ 2018/hash/29c0605a3bab4229e46723f89cf59d83-Abstract.html\n\nEfficient Certification of Spatial Robustness. Anian Ruoss, Maximilian Baader, Mislav Balunovi\u0107, Martin Vechev, abs/2009.09318ArXiv preprintAnian Ruoss, Maximilian Baader, Mislav Balunovi\u0107, and Martin Vechev. 2020a. Efficient Certification of Spatial Robustness. ArXiv preprint abs/2009.09318 (2020). https://arxiv.org/abs/2009.09318\n\nLearning Certified Individually Fair Representations. Anian Ruoss, Mislav Balunovic, Marc Fischer, Martin T Vechev, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin T. Vechev. 2020b. Learning Certified Individually Fair Representa- tions. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/55d491cf951b1b920900684d71419282- Abstract.html\n\nFast and effective robustness certification for recurrent neural networks. Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, Martin Vechev, ArXiv preprint abs/2005.13300Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, and Martin Vechev. 2020. Fast and effective robustness certification for recurrent neural networks. ArXiv preprint abs/2005.13300 (2020). https://arxiv.org/abs/2005. 13300\n\nProvably Robust Deep Learning via Adversarially Trained Smoothed Classifiers. Jerry Hadi Salman, Li, P Ilya, Pengchuan Razenshteyn, Huan Zhang, S\u00e9bastien Zhang, Greg Bubeck, Yang, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaHadi Salman, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang, S\u00e9bastien Bubeck, and Greg Yang. 2019a. Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 11289-11300. https://proceedings.neurips.cc/paper/2019/hash/3a24b25a7b092a252166a1641ae953e7- Abstract.html\n\nA Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks. Greg Hadi Salman, Huan Yang, Cho-Jui Zhang, Pengchuan Hsieh, Zhang, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaHadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. 2019b. A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 9832-9842. https://proceedings.neurips.cc/paper/2019/hash/246a3c5544feb054f3ea718f61adfa16-Abstract.html\n\nComputing the approximate convex hull in high dimensions. Hossein Sartipizadeh, Tyrone L Vincent, ArXiv preprint abs/1603.04422Hossein Sartipizadeh and Tyrone L Vincent. 2016. Computing the approximate convex hull in high dimensions. ArXiv preprint abs/1603.04422 (2016). https://arxiv.org/abs/1603.04422\n\nThe upper bound theorem for polytopes: an easy proof of its asymptotic version. Raimund Seidel, 10.1016/0925-7721(95)00013-YComputational Geometry. 5Raimund Seidel. 1995. The upper bound theorem for polytopes: an easy proof of its asymptotic version. Computational Geometry 5, 2 (1995), 115-116. https://doi.org/10.1016/0925-7721(95)00013-Y\n\nBeyond the Single Neuron Convex Barrier for Neural Network Certification. Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, Martin T Vechev, ; , Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman GarnettNeurIPS; Vancouver, BC, CanadaGagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, and Martin T. Vechev. 2019a. Beyond the Single Neuron Convex Barrier for Neural Network Certification. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15072-15083. https://proceedings.neurips.cc/paper/2019/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html\n\nFast and Effective Robustness Certification. Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, Martin T Vechev, ; Montr\u00e9al, Canada , Samy Bengio, Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman GarnettNeurIPSGagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin T. Vechev. 2018. Fast and Effective Robustness Certification. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 10825-10836. https://proceedings.neurips.cc/paper/ 2018/hash/f2f446980d8e971ef3da97af089481c3-Abstract.html\n\nAn abstract domain for certifying neural networks. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, Martin Vechev, 10.1145/3290354Proceedings of the ACM on Programming Languages. 3POPLGagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. 2019b. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages 3, POPL (2019), 1-30. https://doi.org/10.1145/3290354\n\nBoosting Robustness Certification of Neural Networks. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, Martin T Vechev, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAGagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin T. Vechev. 2019c. Boosting Robustness Certification of Neural Networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HJgeEh09KQ\n\nFast Polyhedra Abstract Domain. Gagandeep Singh, Markus P\u00fcschel, Martin Vechev, 10.1145/3009837.3009885Proc. Principles of Programming Languages (POPL. Principles of Programming Languages (POPLGagandeep Singh, Markus P\u00fcschel, and Martin Vechev. 2017. Fast Polyhedra Abstract Domain. In Proc. Principles of Programming Languages (POPL). 46-59. https://doi.org/10.1145/3009837.3009885\n\nIntriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J Goodfellow, Rob Fergus, 2nd International Conference on Learning Representations. Bengio and Yann LeCunBanff, AB, CanadaConference Track ProceedingsChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http: //arxiv.org/abs/1312.6199\n\nPRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations. Proc. ACM Program. Lang. 633Publication dateProc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022. PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations 43:33\n\nThe Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification. Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, Juan Pablo Vielma, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien LinChristian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma. 2020. The Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/f6c2a0c4b566bc99d596e58638e342b0-Abstract.html\n\nEvaluating Robustness of Neural Networks with Mixed Integer Programming. Vincent Tjeng, Kai Y Xiao, Russ Tedrake, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USA, May 6-9Using Deep Learning to Predict Steering AnglesVincent Tjeng, Kai Y. Xiao, and Russ Tedrake. 2019. Evaluating Robustness of Neural Networks with Mixed Integer Programming. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=HyGIdiRqtm Udacity. 2016. Using Deep Learning to Predict Steering Angles. https://github.com/udacity/self-driving-car.\n\nA Review of Formal Methods applied to Machine Learning. Caterina Urban, Antoine Min\u00e9, ArXiv preprint abs/2104.02466Caterina Urban and Antoine Min\u00e9. 2021. A Review of Formal Methods applied to Machine Learning. ArXiv preprint abs/2104.02466 (2021). https://arxiv.org/abs/2104.02466\n\nEfficient Formal Safety Analysis of Neural Networks. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana, ; Montr\u00e9al, Canada , Samy Bengio, Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman GarnettNeurIPSShiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Efficient Formal Safety Analysis of Neural Networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 6369-6379. https://proceedings.neurips.cc/paper/ 2018/hash/2ecd2bd94734e5dd392d8678bc64cdab-Abstract.html\n\nBeta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Verification. Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, J Zico Kolter, abs/2103.06624ArXiv preprintShiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. 2021. Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Verification. ArXiv preprint abs/2103.06624 (2021). https://arxiv.org/abs/2103.06624\n\nTowards Fast Computation of Certified Robustness for ReLU Networks. Huan Tsui-Wei Weng, Hongge Zhang, Zhao Chen, Cho-Jui Song, Luca Hsieh, Duane S Daniel, Inderjit S Boning, Dhillon, PMLRProceedings of the 35th International Conference on Machine Learning. Jennifer G. Dy and Andreas Krausethe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, Sweden80Proceedings of Machine Learning ResearchTsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane S. Boning, and Inderjit S. Dhillon. 2018. Towards Fast Computation of Certified Robustness for ReLU Networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 5273-5282. http://proceedings. mlr.press/v80/weng18a.html\n\nScaling provable adversarial defenses. Eric Wong, Frank R Schmidt, Jan Hendrik Metzen, J Zico, Canada Montr\u00e9al, Samy Bengio, Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman GarnettNeurIPSEric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. 2018. Scaling provable adversarial defenses. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 8410-8419. https://proceedings.neurips.cc/paper/2018/hash/ 358f9e7be09177c17d0d17ff73584307-Abstract.html\n\nOutput reachable set estimation and verification for multilayer neural networks. Weiming Xiang, Hoang-Dung Tran, Taylor T Johnson , 10.1109/TNNLS.2018.2808470IEEE transactions on neural networks and learning systems. 29Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. 2018. Output reachable set estimation and verification for multilayer neural networks. IEEE transactions on neural networks and learning systems 29, 11 (2018), 5777-5783. https: //doi.org/10.1109/TNNLS.2018.2808470\n\nAutomatic Perturbation Analysis for Scalable Certified Robustness and Beyond. Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, Cho-Jui Hsieh, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. 2020. Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/0cbc5671ae26f67871cb914d81ef8fc1-Abstract.html\n\nFast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers. Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, Cho-Jui Hsieh, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaKaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. 2021. Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https: //openreview.net/forum?id=nVZtXBI6LNn\n\nAn approximate algorithm for computing multidimensional convex hulls. Zong-Ben Xu, Jiang-She Zhang, Yiu-Wing Leung, 10.1016/S0096-3003(97)10043-1Applied mathematics and computation. 94Zong-Ben Xu, Jiang-She Zhang, and Yiu-Wing Leung. 1998. An approximate algorithm for computing multidimensional convex hulls. Applied mathematics and computation 94, 2-3 (1998), 193-226. https://doi.org/10.1016/S0096-3003(97)10043-1\n\nEfficient Neural Network Robustness Certification with General Activation Functions. Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel, ; Montr\u00e9al, Canada , Samy Bengio, Hanna M Wallach, Hugo Larochelle, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman GarnettNeurIPSHuan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. 2018. Efficient Neural Network Robustness Certification with General Activation Functions. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 4944-4953. https://proceedings.neurips.cc/paper/2018/hash/d04863f100d59b3eb688a11f95b0ae60-Abstract.html\n\nFinding convex hull vertices in metric space. Jinhong Zhong, Ke Tang, Kai Qin, 10.1109/IJCNN.2014.68896992014 International Joint Conference on Neural Networks (IJCNN). IEEEJinhong Zhong, Ke Tang, and A Kai Qin. 2014. Finding convex hull vertices in metric space. In 2014 International Joint Conference on Neural Networks (IJCNN). IEEE, 1587-1592. https://doi.org/10.1109/IJCNN.2014.6889699\n", "annotations": {"author": "[{\"end\":173,\"start\":114},{\"end\":225,\"start\":174},{\"end\":293,\"start\":226},{\"end\":345,\"start\":294},{\"end\":408,\"start\":346},{\"end\":455,\"start\":409},{\"end\":511,\"start\":456},{\"end\":563,\"start\":512},{\"end\":623,\"start\":564},{\"end\":675,\"start\":624},{\"end\":723,\"start\":676},{\"end\":776,\"start\":724}]", "publisher": null, "author_last_name": "[{\"end\":132,\"start\":126},{\"end\":184,\"start\":178},{\"end\":252,\"start\":238},{\"end\":304,\"start\":298},{\"end\":367,\"start\":356},{\"end\":414,\"start\":409},{\"end\":470,\"start\":463},{\"end\":522,\"start\":516},{\"end\":582,\"start\":571},{\"end\":634,\"start\":628},{\"end\":682,\"start\":676},{\"end\":735,\"start\":724}]", "author_first_name": "[{\"end\":118,\"start\":114},{\"end\":125,\"start\":119},{\"end\":177,\"start\":174},{\"end\":237,\"start\":226},{\"end\":297,\"start\":294},{\"end\":355,\"start\":346},{\"end\":462,\"start\":456},{\"end\":515,\"start\":512},{\"end\":570,\"start\":564},{\"end\":627,\"start\":624}]", "author_affiliation": "[{\"end\":172,\"start\":134},{\"end\":224,\"start\":186},{\"end\":292,\"start\":254},{\"end\":344,\"start\":306},{\"end\":407,\"start\":369},{\"end\":454,\"start\":416},{\"end\":510,\"start\":472},{\"end\":562,\"start\":524},{\"end\":622,\"start\":584},{\"end\":674,\"start\":636},{\"end\":722,\"start\":684},{\"end\":775,\"start\":737}]", "title": "[{\"end\":99,\"start\":1},{\"end\":875,\"start\":777}]", "venue": "[{\"end\":921,\"start\":877}]", "abstract": "[{\"end\":2678,\"start\":1412}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2931,\"start\":2910},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3366,\"start\":3345},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3926,\"start\":3905},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3945,\"start\":3926},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3957,\"start\":3945},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3974,\"start\":3957},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3993,\"start\":3974},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4011,\"start\":3993},{\"end\":4443,\"start\":4421},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4458,\"start\":4443},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4554,\"start\":4542},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5677,\"start\":5663},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5812,\"start\":5799},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6862,\"start\":6843},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6881,\"start\":6862},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7064,\"start\":7044},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7260,\"start\":7231},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7617,\"start\":7598},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8647,\"start\":8627},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9741,\"start\":9721},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9786,\"start\":9768},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9819,\"start\":9801},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9854,\"start\":9831},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9884,\"start\":9864},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10509,\"start\":10489},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":12605,\"start\":12589},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13207,\"start\":13187},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13252,\"start\":13234},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13281,\"start\":13263},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13316,\"start\":13293},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13346,\"start\":13326},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16153,\"start\":16135},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16572,\"start\":16548},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16592,\"start\":16572},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16609,\"start\":16592},{\"end\":17338,\"start\":17337},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20573,\"start\":20554},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20693,\"start\":20664},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25369,\"start\":25355},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27328,\"start\":27307},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27351,\"start\":27328},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27381,\"start\":27351},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":27398,\"start\":27381},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":27501,\"start\":27486},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27649,\"start\":27625},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27669,\"start\":27649},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32910,\"start\":32899},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37190,\"start\":37165},{\"end\":60671,\"start\":60666},{\"end\":60894,\"start\":60889},{\"end\":61234,\"start\":61229},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":63680,\"start\":63660},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":63699,\"start\":63680},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":63714,\"start\":63699},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":63852,\"start\":63832},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":65452,\"start\":65432},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":65484,\"start\":65464},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":66961,\"start\":66941},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":67007,\"start\":66989},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":67033,\"start\":67015},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":67184,\"start\":67162},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":67389,\"start\":67370},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":67534,\"start\":67513},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":67747,\"start\":67728},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":67977,\"start\":67958},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":68023,\"start\":68005},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":68052,\"start\":68034},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":68087,\"start\":68064},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":68120,\"start\":68101},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":68316,\"start\":68297},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":68596,\"start\":68577},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":68988,\"start\":68969},{\"end\":69041,\"start\":69022},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":69269,\"start\":69250},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":69302,\"start\":69283},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":69334,\"start\":69314},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":69472,\"start\":69453},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":69764,\"start\":69745},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":70099,\"start\":70080},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":72259,\"start\":72239},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":72530,\"start\":72508},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":72674,\"start\":72655},{\"end\":74089,\"start\":74064},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":75256,\"start\":75237},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":75391,\"start\":75369},{\"end\":77121,\"start\":77118},{\"end\":78582,\"start\":78581},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":80648,\"start\":80626},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":81463,\"start\":81445},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":81562,\"start\":81540},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":81606,\"start\":81588},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":86224,\"start\":86211},{\"end\":86242,\"start\":86224},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":86258,\"start\":86242},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":86276,\"start\":86258},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":86330,\"start\":86310},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":86349,\"start\":86330},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":86367,\"start\":86349},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":86385,\"start\":86367},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":86403,\"start\":86385},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":86711,\"start\":86692},{\"end\":86730,\"start\":86711},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":86747,\"start\":86730},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":86767,\"start\":86747},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":86857,\"start\":86837},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":86879,\"start\":86857},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":86895,\"start\":86879},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":86919,\"start\":86895},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":86938,\"start\":86919},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":86955,\"start\":86938},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":87135,\"start\":87116},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":87155,\"start\":87135},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":87174,\"start\":87155},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":87447,\"start\":87428},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":87466,\"start\":87447},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":87526,\"start\":87506},{\"end\":87764,\"start\":87746},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":87783,\"start\":87764},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":87796,\"start\":87783},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":87814,\"start\":87796},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":87837,\"start\":87814},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":87849,\"start\":87837},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":87869,\"start\":87849},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":87964,\"start\":87943},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":87987,\"start\":87964},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":88017,\"start\":87987},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":88035,\"start\":88017},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":88091,\"start\":88076}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":89527,\"start\":89409},{\"attributes\":{\"id\":\"fig_1\"},\"end\":89627,\"start\":89528},{\"attributes\":{\"id\":\"fig_2\"},\"end\":89732,\"start\":89628},{\"attributes\":{\"id\":\"fig_3\"},\"end\":89869,\"start\":89733},{\"attributes\":{\"id\":\"fig_4\"},\"end\":90167,\"start\":89870},{\"attributes\":{\"id\":\"fig_5\"},\"end\":90373,\"start\":90168},{\"attributes\":{\"id\":\"fig_6\"},\"end\":90774,\"start\":90374},{\"attributes\":{\"id\":\"fig_7\"},\"end\":91591,\"start\":90775},{\"attributes\":{\"id\":\"fig_8\"},\"end\":91684,\"start\":91592},{\"attributes\":{\"id\":\"fig_9\"},\"end\":92313,\"start\":91685},{\"attributes\":{\"id\":\"fig_10\"},\"end\":92453,\"start\":92314},{\"attributes\":{\"id\":\"fig_11\"},\"end\":92565,\"start\":92454},{\"attributes\":{\"id\":\"fig_13\"},\"end\":92646,\"start\":92566},{\"attributes\":{\"id\":\"fig_14\"},\"end\":93052,\"start\":92647},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":93695,\"start\":93053},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":94870,\"start\":93696},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":95414,\"start\":94871},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":95870,\"start\":95415},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":96392,\"start\":95871}]", "paragraph": "[{\"end\":3267,\"start\":2694},{\"end\":3877,\"start\":3269},{\"end\":4356,\"start\":3879},{\"end\":4764,\"start\":4358},{\"end\":5124,\"start\":4766},{\"end\":5869,\"start\":5126},{\"end\":6753,\"start\":5871},{\"end\":8339,\"start\":6755},{\"end\":8987,\"start\":8341},{\"end\":9553,\"start\":8989},{\"end\":10585,\"start\":9555},{\"end\":10633,\"start\":10587},{\"end\":10806,\"start\":10635},{\"end\":11815,\"start\":10808},{\"end\":11950,\"start\":11830},{\"end\":12233,\"start\":11952},{\"end\":12878,\"start\":12235},{\"end\":13383,\"start\":12941},{\"end\":13769,\"start\":13385},{\"end\":14251,\"start\":13771},{\"end\":14598,\"start\":14253},{\"end\":15014,\"start\":14631},{\"end\":15135,\"start\":15016},{\"end\":15361,\"start\":15176},{\"end\":15568,\"start\":15394},{\"end\":15876,\"start\":15596},{\"end\":16500,\"start\":15950},{\"end\":17155,\"start\":16502},{\"end\":17424,\"start\":17157},{\"end\":18179,\"start\":17446},{\"end\":18295,\"start\":18181},{\"end\":18441,\"start\":18297},{\"end\":19098,\"start\":18443},{\"end\":19372,\"start\":19100},{\"end\":20323,\"start\":19374},{\"end\":20888,\"start\":20325},{\"end\":20952,\"start\":20947},{\"end\":21552,\"start\":21115},{\"end\":22825,\"start\":21624},{\"end\":23178,\"start\":22918},{\"end\":24993,\"start\":23180},{\"end\":25466,\"start\":25130},{\"end\":25800,\"start\":25468},{\"end\":25917,\"start\":25802},{\"end\":26264,\"start\":25995},{\"end\":26808,\"start\":26463},{\"end\":28229,\"start\":27078},{\"end\":29011,\"start\":28231},{\"end\":30669,\"start\":29013},{\"end\":31007,\"start\":30671},{\"end\":31701,\"start\":31033},{\"end\":32817,\"start\":32032},{\"end\":32911,\"start\":32867},{\"end\":33157,\"start\":32953},{\"end\":34208,\"start\":33159},{\"end\":35062,\"start\":34210},{\"end\":35361,\"start\":35085},{\"end\":36301,\"start\":35378},{\"end\":37623,\"start\":36400},{\"end\":38125,\"start\":37682},{\"end\":39359,\"start\":38127},{\"end\":39558,\"start\":39361},{\"end\":40604,\"start\":39587},{\"end\":40861,\"start\":40606},{\"end\":41324,\"start\":40898},{\"end\":41988,\"start\":41326},{\"end\":42225,\"start\":42013},{\"end\":42376,\"start\":42247},{\"end\":43738,\"start\":42788},{\"end\":44017,\"start\":43740},{\"end\":44693,\"start\":44103},{\"end\":45565,\"start\":44695},{\"end\":45676,\"start\":45600},{\"end\":46207,\"start\":45833},{\"end\":46504,\"start\":46209},{\"end\":46556,\"start\":46506},{\"end\":46600,\"start\":46558},{\"end\":46694,\"start\":46602},{\"end\":46790,\"start\":46696},{\"end\":47724,\"start\":46792},{\"end\":48214,\"start\":47726},{\"end\":48255,\"start\":48216},{\"end\":48296,\"start\":48257},{\"end\":48340,\"start\":48298},{\"end\":48384,\"start\":48342},{\"end\":48766,\"start\":48386},{\"end\":49194,\"start\":48768},{\"end\":49527,\"start\":49196},{\"end\":50142,\"start\":49529},{\"end\":50490,\"start\":50144},{\"end\":51447,\"start\":50568},{\"end\":51793,\"start\":51449},{\"end\":52597,\"start\":51795},{\"end\":53576,\"start\":52599},{\"end\":54327,\"start\":53578},{\"end\":55071,\"start\":54329},{\"end\":55488,\"start\":55073},{\"end\":56040,\"start\":55506},{\"end\":56065,\"start\":56052},{\"end\":56163,\"start\":56074},{\"end\":56528,\"start\":56247},{\"end\":57016,\"start\":56530},{\"end\":57509,\"start\":57049},{\"end\":57692,\"start\":57511},{\"end\":58194,\"start\":57738},{\"end\":58705,\"start\":58206},{\"end\":59043,\"start\":58707},{\"end\":61693,\"start\":59045},{\"end\":62501,\"start\":61816},{\"end\":63064,\"start\":62503},{\"end\":63386,\"start\":63102},{\"end\":63882,\"start\":63388},{\"end\":64521,\"start\":63884},{\"end\":65175,\"start\":64523},{\"end\":66353,\"start\":65210},{\"end\":66765,\"start\":66402},{\"end\":66871,\"start\":66780},{\"end\":67782,\"start\":66873},{\"end\":70731,\"start\":67784},{\"end\":71334,\"start\":70777},{\"end\":75490,\"start\":71336},{\"end\":77324,\"start\":75510},{\"end\":78801,\"start\":77356},{\"end\":79291,\"start\":78838},{\"end\":80147,\"start\":79293},{\"end\":80660,\"start\":80206},{\"end\":81922,\"start\":80662},{\"end\":82848,\"start\":81945},{\"end\":83970,\"start\":82912},{\"end\":85008,\"start\":83972},{\"end\":85725,\"start\":85010},{\"end\":86137,\"start\":85742},{\"end\":87378,\"start\":86139},{\"end\":88147,\"start\":87380},{\"end\":88687,\"start\":88149},{\"end\":89408,\"start\":88702}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12910,\"start\":12879},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15175,\"start\":15136},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15393,\"start\":15362},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15595,\"start\":15569},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15949,\"start\":15877},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20946,\"start\":20889},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21088,\"start\":20953},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21623,\"start\":21553},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22917,\"start\":22826},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25129,\"start\":24994},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25994,\"start\":25918},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26462,\"start\":26265},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27034,\"start\":26809},{\"attributes\":{\"id\":\"formula_13\"},\"end\":32031,\"start\":31742},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32866,\"start\":32818},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32952,\"start\":32912},{\"attributes\":{\"id\":\"formula_16\"},\"end\":36399,\"start\":36302},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37681,\"start\":37624},{\"attributes\":{\"id\":\"formula_18\"},\"end\":40897,\"start\":40862},{\"attributes\":{\"id\":\"formula_19\"},\"end\":42787,\"start\":42411},{\"attributes\":{\"id\":\"formula_20\"},\"end\":44102,\"start\":44018},{\"attributes\":{\"id\":\"formula_21\"},\"end\":45832,\"start\":45677},{\"attributes\":{\"id\":\"formula_22\"},\"end\":56051,\"start\":56041},{\"attributes\":{\"id\":\"formula_23\"},\"end\":56073,\"start\":56066},{\"attributes\":{\"id\":\"formula_24\"},\"end\":56246,\"start\":56164},{\"attributes\":{\"id\":\"formula_25\"},\"end\":57737,\"start\":57693},{\"attributes\":{\"id\":\"formula_26\"},\"end\":61784,\"start\":61732},{\"attributes\":{\"id\":\"formula_27\"},\"end\":78837,\"start\":78802}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":67100,\"start\":67093},{\"end\":67289,\"start\":67282},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":67454,\"start\":67447},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":67780,\"start\":67773},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":70858,\"start\":70851},{\"end\":70880,\"start\":70873},{\"end\":72268,\"start\":72261},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":72846,\"start\":72839},{\"end\":73654,\"start\":73647},{\"end\":75520,\"start\":75513},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":81000,\"start\":80993},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":82247,\"start\":82240}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2692,\"start\":2680},{\"attributes\":{\"n\":\"2\"},\"end\":11828,\"start\":11818},{\"attributes\":{\"n\":\"2.1\"},\"end\":12939,\"start\":12912},{\"attributes\":{\"n\":\"2.2\"},\"end\":14629,\"start\":14601},{\"attributes\":{\"n\":\"3\"},\"end\":17444,\"start\":17427},{\"attributes\":{\"n\":\"3.1\"},\"end\":21113,\"start\":21090},{\"attributes\":{\"n\":\"3.2\"},\"end\":27076,\"start\":27036},{\"attributes\":{\"n\":\"3.3\"},\"end\":31031,\"start\":31010},{\"attributes\":{\"n\":\"4\"},\"end\":31741,\"start\":31704},{\"attributes\":{\"n\":\"4.1\"},\"end\":35083,\"start\":35065},{\"attributes\":{\"n\":\"4.2\"},\"end\":35376,\"start\":35364},{\"attributes\":{\"n\":\"4.3\"},\"end\":39585,\"start\":39561},{\"attributes\":{\"n\":\"4.4\"},\"end\":42011,\"start\":41991},{\"attributes\":{\"n\":\"4.5\"},\"end\":42245,\"start\":42228},{\"end\":42410,\"start\":42379},{\"end\":45598,\"start\":45568},{\"end\":50566,\"start\":50493},{\"attributes\":{\"n\":\"5.1\"},\"end\":55504,\"start\":55491},{\"attributes\":{\"n\":\"5.2\"},\"end\":57047,\"start\":57019},{\"attributes\":{\"n\":\"5.3\"},\"end\":58204,\"start\":58197},{\"attributes\":{\"n\":\"5.4\"},\"end\":61731,\"start\":61696},{\"attributes\":{\"n\":\"6\"},\"end\":61814,\"start\":61786},{\"attributes\":{\"n\":\"6.1\"},\"end\":63100,\"start\":63067},{\"attributes\":{\"n\":\"6.2\"},\"end\":65208,\"start\":65178},{\"attributes\":{\"n\":\"7\"},\"end\":66379,\"start\":66356},{\"attributes\":{\"n\":\"7.1\"},\"end\":66400,\"start\":66382},{\"attributes\":{\"n\":\"7.2\"},\"end\":66778,\"start\":66768},{\"attributes\":{\"n\":\"7.3\"},\"end\":70775,\"start\":70734},{\"attributes\":{\"n\":\"7.4\"},\"end\":75508,\"start\":75493},{\"attributes\":{\"n\":\"7.5\"},\"end\":77354,\"start\":77327},{\"attributes\":{\"n\":\"7.6\"},\"end\":80204,\"start\":80150},{\"attributes\":{\"n\":\"7.7\"},\"end\":81943,\"start\":81925},{\"attributes\":{\"n\":\"7.8\"},\"end\":82910,\"start\":82851},{\"attributes\":{\"n\":\"8\"},\"end\":85740,\"start\":85728},{\"attributes\":{\"n\":\"9\"},\"end\":88700,\"start\":88690},{\"end\":89418,\"start\":89410},{\"end\":89537,\"start\":89529},{\"end\":89637,\"start\":89629},{\"end\":89742,\"start\":89734},{\"end\":89879,\"start\":89871},{\"end\":90177,\"start\":90169},{\"end\":90384,\"start\":90375},{\"end\":90779,\"start\":90776},{\"end\":91602,\"start\":91593},{\"end\":92324,\"start\":92315},{\"end\":92464,\"start\":92455},{\"end\":92666,\"start\":92648},{\"end\":93063,\"start\":93054},{\"end\":93706,\"start\":93697},{\"end\":94881,\"start\":94872},{\"end\":95425,\"start\":95416}]", "table": "[{\"end\":93695,\"start\":93114},{\"end\":94870,\"start\":93708},{\"end\":95414,\"start\":95021},{\"end\":95870,\"start\":95642},{\"end\":96392,\"start\":96077}]", "figure_caption": "[{\"end\":89527,\"start\":89420},{\"end\":89627,\"start\":89539},{\"end\":89732,\"start\":89639},{\"end\":89869,\"start\":89744},{\"end\":90167,\"start\":89881},{\"end\":90373,\"start\":90179},{\"end\":90774,\"start\":90387},{\"end\":91591,\"start\":90781},{\"end\":91684,\"start\":91605},{\"end\":92313,\"start\":91687},{\"end\":92453,\"start\":92327},{\"end\":92565,\"start\":92467},{\"end\":92646,\"start\":92568},{\"end\":93052,\"start\":92671},{\"end\":93114,\"start\":93065},{\"end\":95021,\"start\":94883},{\"end\":95642,\"start\":95427},{\"end\":96077,\"start\":95873}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5003,\"start\":4995},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5508,\"start\":5496},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6234,\"start\":6226},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6644,\"start\":6632},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7587,\"start\":7579},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8982,\"start\":8974},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14312,\"start\":14304},{\"end\":20214,\"start\":20208},{\"end\":20852,\"start\":20844},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21420,\"start\":21412},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21538,\"start\":21530},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22678,\"start\":22670},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23507,\"start\":23499},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26807,\"start\":26799},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28614,\"start\":28606},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28957,\"start\":28949},{\"end\":29917,\"start\":29911},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30811,\"start\":30801},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30892,\"start\":30882},{\"end\":30958,\"start\":30950},{\"end\":31700,\"start\":31692},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32611,\"start\":32603},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32965,\"start\":32957},{\"end\":35061,\"start\":35053},{\"end\":35211,\"start\":35202},{\"end\":35465,\"start\":35457},{\"end\":37002,\"start\":36994},{\"end\":37779,\"start\":37771},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38378,\"start\":38365},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38594,\"start\":38585},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38817,\"start\":38808},{\"end\":39237,\"start\":39229},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41396,\"start\":41387},{\"end\":41536,\"start\":41528},{\"end\":41785,\"start\":41777},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":41975,\"start\":41967},{\"end\":42224,\"start\":42216},{\"end\":50489,\"start\":50481},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":58139,\"start\":58131},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":59645,\"start\":59637},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60954,\"start\":60945},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":61476,\"start\":61467},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":71383,\"start\":71374},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72917,\"start\":72905},{\"end\":77154,\"start\":77150},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":77539,\"start\":77530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":77845,\"start\":77836},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78111,\"start\":78102},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78232,\"start\":78223},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78283,\"start\":78273},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79185,\"start\":79175},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":81710,\"start\":81703},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":82262,\"start\":82253},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":82661,\"start\":82652},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":82845,\"start\":82836},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":83348,\"start\":83338},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":84147,\"start\":84138},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":85187,\"start\":85178}]", "bib_author_first_name": "[{\"end\":97764,\"start\":97760},{\"end\":97783,\"start\":97775},{\"end\":97797,\"start\":97793},{\"end\":97812,\"start\":97806},{\"end\":98322,\"start\":98318},{\"end\":98337,\"start\":98333},{\"end\":98352,\"start\":98348},{\"end\":98366,\"start\":98357},{\"end\":98387,\"start\":98383},{\"end\":98393,\"start\":98388},{\"end\":98790,\"start\":98785},{\"end\":98802,\"start\":98797},{\"end\":99173,\"start\":99168},{\"end\":99185,\"start\":99180},{\"end\":99525,\"start\":99519},{\"end\":99547,\"start\":99537},{\"end\":99565,\"start\":99556},{\"end\":99578,\"start\":99573},{\"end\":99591,\"start\":99585},{\"end\":99593,\"start\":99592},{\"end\":99603,\"start\":99602},{\"end\":99611,\"start\":99606},{\"end\":99613,\"start\":99612},{\"end\":99627,\"start\":99623},{\"end\":100455,\"start\":100447},{\"end\":100469,\"start\":100464},{\"end\":100813,\"start\":100810},{\"end\":100819,\"start\":100814},{\"end\":100830,\"start\":100829},{\"end\":100843,\"start\":100839},{\"end\":100845,\"start\":100844},{\"end\":101128,\"start\":101121},{\"end\":101145,\"start\":101139},{\"end\":101149,\"start\":101146},{\"end\":101163,\"start\":101157},{\"end\":101185,\"start\":101177},{\"end\":101198,\"start\":101194},{\"end\":101213,\"start\":101206},{\"end\":101222,\"start\":101221},{\"end\":101239,\"start\":101233},{\"end\":101251,\"start\":101248},{\"end\":101267,\"start\":101261},{\"end\":101700,\"start\":101693},{\"end\":101717,\"start\":101711},{\"end\":101738,\"start\":101733},{\"end\":101749,\"start\":101745},{\"end\":102705,\"start\":102700},{\"end\":102725,\"start\":102715},{\"end\":102739,\"start\":102736},{\"end\":102758,\"start\":102751},{\"end\":102773,\"start\":102769},{\"end\":103610,\"start\":103606},{\"end\":103624,\"start\":103618},{\"end\":103640,\"start\":103633},{\"end\":103668,\"start\":103655},{\"end\":104493,\"start\":104489},{\"end\":104508,\"start\":104501},{\"end\":104518,\"start\":104513},{\"end\":104538,\"start\":104530},{\"end\":104547,\"start\":104546},{\"end\":104884,\"start\":104877},{\"end\":105162,\"start\":105156},{\"end\":105177,\"start\":105172},{\"end\":105494,\"start\":105488},{\"end\":105496,\"start\":105495},{\"end\":105508,\"start\":105504},{\"end\":105521,\"start\":105520},{\"end\":105526,\"start\":105522},{\"end\":106232,\"start\":106225},{\"end\":106475,\"start\":106469},{\"end\":106492,\"start\":106485},{\"end\":106796,\"start\":106789},{\"end\":106821,\"start\":106808},{\"end\":106839,\"start\":106833},{\"end\":106854,\"start\":106849},{\"end\":106876,\"start\":106868},{\"end\":106889,\"start\":106885},{\"end\":106903,\"start\":106897},{\"end\":106918,\"start\":106913},{\"end\":106934,\"start\":106931},{\"end\":106936,\"start\":106935},{\"end\":106954,\"start\":106949},{\"end\":106970,\"start\":106962},{\"end\":107881,\"start\":107874},{\"end\":108227,\"start\":108219},{\"end\":108597,\"start\":108592},{\"end\":108782,\"start\":108777},{\"end\":108796,\"start\":108791},{\"end\":109239,\"start\":109234},{\"end\":109253,\"start\":109246},{\"end\":109266,\"start\":109262},{\"end\":109289,\"start\":109284},{\"end\":109305,\"start\":109299},{\"end\":109323,\"start\":109317},{\"end\":109815,\"start\":109809},{\"end\":110053,\"start\":110049},{\"end\":110074,\"start\":110061},{\"end\":110092,\"start\":110086},{\"end\":110108,\"start\":110104},{\"end\":110123,\"start\":110116},{\"end\":110137,\"start\":110129},{\"end\":110151,\"start\":110146},{\"end\":110173,\"start\":110166},{\"end\":110180,\"start\":110174},{\"end\":110195,\"start\":110187},{\"end\":110781,\"start\":110778},{\"end\":110806,\"start\":110803},{\"end\":110829,\"start\":110826},{\"end\":111319,\"start\":111312},{\"end\":111633,\"start\":111630},{\"end\":111645,\"start\":111640},{\"end\":111656,\"start\":111655},{\"end\":111668,\"start\":111664},{\"end\":112147,\"start\":112144},{\"end\":112159,\"start\":112154},{\"end\":112161,\"start\":112160},{\"end\":112176,\"start\":112169},{\"end\":112190,\"start\":112186},{\"end\":112210,\"start\":112199},{\"end\":112226,\"start\":112220},{\"end\":112237,\"start\":112232},{\"end\":112252,\"start\":112244},{\"end\":112267,\"start\":112262},{\"end\":112282,\"start\":112272},{\"end\":112836,\"start\":112835},{\"end\":112851,\"start\":112846},{\"end\":112853,\"start\":112852},{\"end\":113319,\"start\":113312},{\"end\":113337,\"start\":113329},{\"end\":113355,\"start\":113349},{\"end\":113372,\"start\":113366},{\"end\":113383,\"start\":113378},{\"end\":113774,\"start\":113767},{\"end\":113787,\"start\":113779},{\"end\":114222,\"start\":114214},{\"end\":114237,\"start\":114228},{\"end\":114249,\"start\":114242},{\"end\":114260,\"start\":114256},{\"end\":114272,\"start\":114267},{\"end\":114282,\"start\":114278},{\"end\":115089,\"start\":115079},{\"end\":115107,\"start\":115097},{\"end\":115123,\"start\":115117},{\"end\":115141,\"start\":115133},{\"end\":115157,\"start\":115151},{\"end\":115733,\"start\":115724},{\"end\":115751,\"start\":115744},{\"end\":115956,\"start\":115949},{\"end\":115970,\"start\":115965},{\"end\":115983,\"start\":115977},{\"end\":115985,\"start\":115984},{\"end\":116776,\"start\":116775},{\"end\":116791,\"start\":116786},{\"end\":116793,\"start\":116792},{\"end\":116812,\"start\":116804},{\"end\":117183,\"start\":117177},{\"end\":117213,\"start\":117212},{\"end\":117228,\"start\":117222},{\"end\":117230,\"start\":117229},{\"end\":117535,\"start\":117526},{\"end\":117552,\"start\":117544},{\"end\":117569,\"start\":117560},{\"end\":117927,\"start\":117918},{\"end\":117945,\"start\":117936},{\"end\":118270,\"start\":118257},{\"end\":118286,\"start\":118278},{\"end\":118288,\"start\":118287},{\"end\":118299,\"start\":118295},{\"end\":118301,\"start\":118300},{\"end\":118310,\"start\":118309},{\"end\":118312,\"start\":118311},{\"end\":118329,\"start\":118321},{\"end\":118831,\"start\":118826},{\"end\":118850,\"start\":118845},{\"end\":118868,\"start\":118863},{\"end\":118877,\"start\":118876},{\"end\":118894,\"start\":118888},{\"end\":118901,\"start\":118897},{\"end\":118915,\"start\":118910},{\"end\":118917,\"start\":118916},{\"end\":118931,\"start\":118927},{\"end\":119703,\"start\":119698},{\"end\":119721,\"start\":119711},{\"end\":119736,\"start\":119730},{\"end\":119754,\"start\":119748},{\"end\":120045,\"start\":120040},{\"end\":120059,\"start\":120053},{\"end\":120075,\"start\":120071},{\"end\":120091,\"start\":120085},{\"end\":120093,\"start\":120092},{\"end\":120890,\"start\":120882},{\"end\":120902,\"start\":120897},{\"end\":120915,\"start\":120909},{\"end\":120936,\"start\":120927},{\"end\":120950,\"start\":120944},{\"end\":120962,\"start\":120956},{\"end\":121330,\"start\":121325},{\"end\":121349,\"start\":121348},{\"end\":121365,\"start\":121356},{\"end\":121383,\"start\":121379},{\"end\":121400,\"start\":121391},{\"end\":121412,\"start\":121408},{\"end\":122359,\"start\":122355},{\"end\":122377,\"start\":122373},{\"end\":122391,\"start\":122384},{\"end\":122408,\"start\":122399},{\"end\":123301,\"start\":123294},{\"end\":123322,\"start\":123316},{\"end\":123324,\"start\":123323},{\"end\":123629,\"start\":123622},{\"end\":123967,\"start\":123958},{\"end\":123983,\"start\":123975},{\"end\":123998,\"start\":123992},{\"end\":124014,\"start\":124008},{\"end\":124016,\"start\":124015},{\"end\":124026,\"start\":124025},{\"end\":124034,\"start\":124029},{\"end\":124036,\"start\":124035},{\"end\":124050,\"start\":124046},{\"end\":124890,\"start\":124881},{\"end\":124903,\"start\":124898},{\"end\":124917,\"start\":124910},{\"end\":124932,\"start\":124926},{\"end\":124948,\"start\":124942},{\"end\":124950,\"start\":124949},{\"end\":124960,\"start\":124959},{\"end\":124977,\"start\":124971},{\"end\":124984,\"start\":124980},{\"end\":124998,\"start\":124993},{\"end\":125000,\"start\":124999},{\"end\":125014,\"start\":125010},{\"end\":125792,\"start\":125783},{\"end\":125805,\"start\":125800},{\"end\":125818,\"start\":125812},{\"end\":125834,\"start\":125828},{\"end\":126200,\"start\":126191},{\"end\":126213,\"start\":126208},{\"end\":126226,\"start\":126220},{\"end\":126242,\"start\":126236},{\"end\":126244,\"start\":126243},{\"end\":126680,\"start\":126671},{\"end\":126694,\"start\":126688},{\"end\":126710,\"start\":126704},{\"end\":127074,\"start\":127065},{\"end\":127092,\"start\":127084},{\"end\":127106,\"start\":127102},{\"end\":127122,\"start\":127118},{\"end\":127137,\"start\":127130},{\"end\":127148,\"start\":127145},{\"end\":127150,\"start\":127149},{\"end\":127166,\"start\":127163},{\"end\":128124,\"start\":128115},{\"end\":128145,\"start\":128141},{\"end\":128160,\"start\":128156},{\"end\":128175,\"start\":128171},{\"end\":128186,\"start\":128180},{\"end\":128198,\"start\":128194},{\"end\":128204,\"start\":128199},{\"end\":129101,\"start\":129094},{\"end\":129112,\"start\":129109},{\"end\":129114,\"start\":129113},{\"end\":129125,\"start\":129121},{\"end\":129745,\"start\":129737},{\"end\":129760,\"start\":129753},{\"end\":130021,\"start\":130016},{\"end\":130033,\"start\":130028},{\"end\":130045,\"start\":130039},{\"end\":130065,\"start\":130058},{\"end\":130077,\"start\":130072},{\"end\":130085,\"start\":130084},{\"end\":130102,\"start\":130096},{\"end\":130109,\"start\":130105},{\"end\":130123,\"start\":130118},{\"end\":130125,\"start\":130124},{\"end\":130139,\"start\":130135},{\"end\":130988,\"start\":130983},{\"end\":130999,\"start\":130995},{\"end\":131012,\"start\":131007},{\"end\":131020,\"start\":131017},{\"end\":131031,\"start\":131026},{\"end\":131045,\"start\":131038},{\"end\":131059,\"start\":131053},{\"end\":131466,\"start\":131462},{\"end\":131488,\"start\":131482},{\"end\":131500,\"start\":131496},{\"end\":131514,\"start\":131507},{\"end\":131525,\"start\":131521},{\"end\":131538,\"start\":131533},{\"end\":131540,\"start\":131539},{\"end\":131557,\"start\":131549},{\"end\":131559,\"start\":131558},{\"end\":132350,\"start\":132346},{\"end\":132362,\"start\":132357},{\"end\":132364,\"start\":132363},{\"end\":132377,\"start\":132374},{\"end\":132385,\"start\":132378},{\"end\":132395,\"start\":132394},{\"end\":132408,\"start\":132402},{\"end\":132423,\"start\":132419},{\"end\":132437,\"start\":132432},{\"end\":132439,\"start\":132438},{\"end\":132453,\"start\":132449},{\"end\":133237,\"start\":133230},{\"end\":133255,\"start\":133245},{\"end\":133278,\"start\":133262},{\"end\":133721,\"start\":133716},{\"end\":133734,\"start\":133726},{\"end\":133744,\"start\":133740},{\"end\":133757,\"start\":133752},{\"end\":133771,\"start\":133764},{\"end\":133785,\"start\":133779},{\"end\":133799,\"start\":133793},{\"end\":133814,\"start\":133811},{\"end\":133827,\"start\":133820},{\"end\":134756,\"start\":134751},{\"end\":134765,\"start\":134761},{\"end\":134778,\"start\":134773},{\"end\":134790,\"start\":134785},{\"end\":134802,\"start\":134797},{\"end\":134812,\"start\":134809},{\"end\":134825,\"start\":134818},{\"end\":135388,\"start\":135380},{\"end\":135402,\"start\":135393},{\"end\":135418,\"start\":135410},{\"end\":135817,\"start\":135813},{\"end\":135833,\"start\":135825},{\"end\":135846,\"start\":135840},{\"end\":135860,\"start\":135853},{\"end\":135872,\"start\":135868},{\"end\":135882,\"start\":135881},{\"end\":135899,\"start\":135893},{\"end\":135906,\"start\":135902},{\"end\":135920,\"start\":135915},{\"end\":135922,\"start\":135921},{\"end\":135936,\"start\":135932},{\"end\":136733,\"start\":136726},{\"end\":136743,\"start\":136741}]", "bib_author_last_name": "[{\"end\":97773,\"start\":97765},{\"end\":97791,\"start\":97784},{\"end\":97804,\"start\":97798},{\"end\":97822,\"start\":97813},{\"end\":98331,\"start\":98323},{\"end\":98346,\"start\":98338},{\"end\":98355,\"start\":98353},{\"end\":98381,\"start\":98367},{\"end\":98400,\"start\":98394},{\"end\":98795,\"start\":98791},{\"end\":98809,\"start\":98803},{\"end\":99178,\"start\":99174},{\"end\":99192,\"start\":99186},{\"end\":99535,\"start\":99526},{\"end\":99554,\"start\":99548},{\"end\":99571,\"start\":99566},{\"end\":99583,\"start\":99579},{\"end\":99600,\"start\":99594},{\"end\":99621,\"start\":99614},{\"end\":99638,\"start\":99628},{\"end\":100462,\"start\":100456},{\"end\":100484,\"start\":100470},{\"end\":100495,\"start\":100486},{\"end\":100827,\"start\":100820},{\"end\":100837,\"start\":100831},{\"end\":100855,\"start\":100846},{\"end\":100862,\"start\":100857},{\"end\":101137,\"start\":101129},{\"end\":101155,\"start\":101150},{\"end\":101175,\"start\":101164},{\"end\":101192,\"start\":101186},{\"end\":101204,\"start\":101199},{\"end\":101219,\"start\":101214},{\"end\":101231,\"start\":101223},{\"end\":101246,\"start\":101240},{\"end\":101259,\"start\":101252},{\"end\":101274,\"start\":101268},{\"end\":101281,\"start\":101276},{\"end\":101709,\"start\":101701},{\"end\":101731,\"start\":101718},{\"end\":101743,\"start\":101739},{\"end\":101753,\"start\":101750},{\"end\":101761,\"start\":101755},{\"end\":102713,\"start\":102706},{\"end\":102734,\"start\":102726},{\"end\":102749,\"start\":102740},{\"end\":102767,\"start\":102759},{\"end\":102781,\"start\":102774},{\"end\":103616,\"start\":103611},{\"end\":103631,\"start\":103625},{\"end\":103653,\"start\":103641},{\"end\":103678,\"start\":103669},{\"end\":104499,\"start\":104494},{\"end\":104511,\"start\":104509},{\"end\":104528,\"start\":104519},{\"end\":104544,\"start\":104539},{\"end\":104552,\"start\":104548},{\"end\":104563,\"start\":104554},{\"end\":104893,\"start\":104885},{\"end\":105170,\"start\":105163},{\"end\":105188,\"start\":105178},{\"end\":105502,\"start\":105497},{\"end\":105518,\"start\":105509},{\"end\":105533,\"start\":105527},{\"end\":106239,\"start\":106233},{\"end\":106483,\"start\":106476},{\"end\":106806,\"start\":106797},{\"end\":106831,\"start\":106822},{\"end\":106847,\"start\":106840},{\"end\":106866,\"start\":106855},{\"end\":106883,\"start\":106877},{\"end\":106895,\"start\":106890},{\"end\":106911,\"start\":106904},{\"end\":106929,\"start\":106919},{\"end\":106947,\"start\":106937},{\"end\":106960,\"start\":106955},{\"end\":106976,\"start\":106971},{\"end\":107894,\"start\":107882},{\"end\":108234,\"start\":108228},{\"end\":108604,\"start\":108598},{\"end\":108789,\"start\":108783},{\"end\":108803,\"start\":108797},{\"end\":109244,\"start\":109240},{\"end\":109260,\"start\":109254},{\"end\":109282,\"start\":109267},{\"end\":109297,\"start\":109290},{\"end\":109315,\"start\":109306},{\"end\":109330,\"start\":109324},{\"end\":109821,\"start\":109816},{\"end\":110059,\"start\":110054},{\"end\":110084,\"start\":110075},{\"end\":110102,\"start\":110093},{\"end\":110114,\"start\":110109},{\"end\":110127,\"start\":110124},{\"end\":110144,\"start\":110138},{\"end\":110164,\"start\":110152},{\"end\":110185,\"start\":110181},{\"end\":110201,\"start\":110196},{\"end\":110801,\"start\":110782},{\"end\":110824,\"start\":110807},{\"end\":110834,\"start\":110830},{\"end\":110838,\"start\":110836},{\"end\":111326,\"start\":111320},{\"end\":111638,\"start\":111634},{\"end\":111653,\"start\":111646},{\"end\":111662,\"start\":111657},{\"end\":111673,\"start\":111669},{\"end\":111681,\"start\":111675},{\"end\":111695,\"start\":111683},{\"end\":112152,\"start\":112148},{\"end\":112167,\"start\":112162},{\"end\":112184,\"start\":112177},{\"end\":112197,\"start\":112191},{\"end\":112218,\"start\":112211},{\"end\":112230,\"start\":112227},{\"end\":112242,\"start\":112238},{\"end\":112260,\"start\":112253},{\"end\":112270,\"start\":112268},{\"end\":112289,\"start\":112283},{\"end\":112833,\"start\":112815},{\"end\":112844,\"start\":112837},{\"end\":112859,\"start\":112854},{\"end\":112869,\"start\":112861},{\"end\":113327,\"start\":113320},{\"end\":113347,\"start\":113338},{\"end\":113364,\"start\":113356},{\"end\":113376,\"start\":113373},{\"end\":113388,\"start\":113384},{\"end\":113777,\"start\":113775},{\"end\":113793,\"start\":113788},{\"end\":114226,\"start\":114223},{\"end\":114240,\"start\":114238},{\"end\":114254,\"start\":114250},{\"end\":114265,\"start\":114261},{\"end\":114276,\"start\":114273},{\"end\":114289,\"start\":114283},{\"end\":115095,\"start\":115090},{\"end\":115115,\"start\":115108},{\"end\":115131,\"start\":115124},{\"end\":115149,\"start\":115142},{\"end\":115163,\"start\":115158},{\"end\":115742,\"start\":115734},{\"end\":115757,\"start\":115752},{\"end\":115963,\"start\":115957},{\"end\":115975,\"start\":115971},{\"end\":115992,\"start\":115986},{\"end\":116773,\"start\":116757},{\"end\":116784,\"start\":116777},{\"end\":116802,\"start\":116794},{\"end\":116819,\"start\":116813},{\"end\":116827,\"start\":116821},{\"end\":117202,\"start\":117184},{\"end\":117210,\"start\":117204},{\"end\":117220,\"start\":117214},{\"end\":117239,\"start\":117231},{\"end\":117247,\"start\":117241},{\"end\":117542,\"start\":117536},{\"end\":117558,\"start\":117553},{\"end\":117575,\"start\":117570},{\"end\":117934,\"start\":117928},{\"end\":117951,\"start\":117946},{\"end\":118276,\"start\":118271},{\"end\":118293,\"start\":118289},{\"end\":118307,\"start\":118302},{\"end\":118319,\"start\":118313},{\"end\":118334,\"start\":118330},{\"end\":118341,\"start\":118336},{\"end\":118843,\"start\":118832},{\"end\":118861,\"start\":118851},{\"end\":118874,\"start\":118869},{\"end\":118886,\"start\":118878},{\"end\":118908,\"start\":118902},{\"end\":118925,\"start\":118918},{\"end\":118942,\"start\":118932},{\"end\":119709,\"start\":119704},{\"end\":119728,\"start\":119722},{\"end\":119746,\"start\":119737},{\"end\":119761,\"start\":119755},{\"end\":120051,\"start\":120046},{\"end\":120069,\"start\":120060},{\"end\":120083,\"start\":120076},{\"end\":120100,\"start\":120094},{\"end\":120895,\"start\":120891},{\"end\":120907,\"start\":120903},{\"end\":120925,\"start\":120916},{\"end\":120942,\"start\":120937},{\"end\":120954,\"start\":120951},{\"end\":120969,\"start\":120963},{\"end\":121342,\"start\":121331},{\"end\":121346,\"start\":121344},{\"end\":121354,\"start\":121350},{\"end\":121377,\"start\":121366},{\"end\":121389,\"start\":121384},{\"end\":121406,\"start\":121401},{\"end\":121419,\"start\":121413},{\"end\":121425,\"start\":121421},{\"end\":122371,\"start\":122360},{\"end\":122382,\"start\":122378},{\"end\":122397,\"start\":122392},{\"end\":122414,\"start\":122409},{\"end\":122421,\"start\":122416},{\"end\":123314,\"start\":123302},{\"end\":123332,\"start\":123325},{\"end\":123636,\"start\":123630},{\"end\":123973,\"start\":123968},{\"end\":123990,\"start\":123984},{\"end\":124006,\"start\":123999},{\"end\":124023,\"start\":124017},{\"end\":124044,\"start\":124037},{\"end\":124061,\"start\":124051},{\"end\":124896,\"start\":124891},{\"end\":124908,\"start\":124904},{\"end\":124924,\"start\":124918},{\"end\":124940,\"start\":124933},{\"end\":124957,\"start\":124951},{\"end\":124969,\"start\":124961},{\"end\":124991,\"start\":124985},{\"end\":125008,\"start\":125001},{\"end\":125025,\"start\":125015},{\"end\":125798,\"start\":125793},{\"end\":125810,\"start\":125806},{\"end\":125826,\"start\":125819},{\"end\":125841,\"start\":125835},{\"end\":126206,\"start\":126201},{\"end\":126218,\"start\":126214},{\"end\":126234,\"start\":126227},{\"end\":126251,\"start\":126245},{\"end\":126686,\"start\":126681},{\"end\":126702,\"start\":126695},{\"end\":126717,\"start\":126711},{\"end\":127082,\"start\":127075},{\"end\":127100,\"start\":127093},{\"end\":127116,\"start\":127107},{\"end\":127128,\"start\":127123},{\"end\":127143,\"start\":127138},{\"end\":127161,\"start\":127151},{\"end\":127173,\"start\":127167},{\"end\":128139,\"start\":128125},{\"end\":128154,\"start\":128146},{\"end\":128169,\"start\":128161},{\"end\":128178,\"start\":128176},{\"end\":128192,\"start\":128187},{\"end\":128211,\"start\":128205},{\"end\":129107,\"start\":129102},{\"end\":129119,\"start\":129115},{\"end\":129133,\"start\":129126},{\"end\":129751,\"start\":129746},{\"end\":129765,\"start\":129761},{\"end\":130026,\"start\":130022},{\"end\":130037,\"start\":130034},{\"end\":130056,\"start\":130046},{\"end\":130070,\"start\":130066},{\"end\":130082,\"start\":130078},{\"end\":130094,\"start\":130086},{\"end\":130116,\"start\":130110},{\"end\":130133,\"start\":130126},{\"end\":130150,\"start\":130140},{\"end\":130993,\"start\":130989},{\"end\":131005,\"start\":131000},{\"end\":131015,\"start\":131013},{\"end\":131024,\"start\":131021},{\"end\":131036,\"start\":131032},{\"end\":131051,\"start\":131046},{\"end\":131066,\"start\":131060},{\"end\":131480,\"start\":131467},{\"end\":131494,\"start\":131489},{\"end\":131505,\"start\":131501},{\"end\":131519,\"start\":131515},{\"end\":131531,\"start\":131526},{\"end\":131547,\"start\":131541},{\"end\":131566,\"start\":131560},{\"end\":131575,\"start\":131568},{\"end\":132355,\"start\":132351},{\"end\":132372,\"start\":132365},{\"end\":132392,\"start\":132386},{\"end\":132400,\"start\":132396},{\"end\":132417,\"start\":132409},{\"end\":132430,\"start\":132424},{\"end\":132447,\"start\":132440},{\"end\":132464,\"start\":132454},{\"end\":133243,\"start\":133238},{\"end\":133260,\"start\":133256},{\"end\":133724,\"start\":133722},{\"end\":133738,\"start\":133735},{\"end\":133750,\"start\":133745},{\"end\":133762,\"start\":133758},{\"end\":133777,\"start\":133772},{\"end\":133791,\"start\":133786},{\"end\":133809,\"start\":133800},{\"end\":133818,\"start\":133815},{\"end\":133833,\"start\":133828},{\"end\":134759,\"start\":134757},{\"end\":134771,\"start\":134766},{\"end\":134783,\"start\":134779},{\"end\":134795,\"start\":134791},{\"end\":134807,\"start\":134803},{\"end\":134816,\"start\":134813},{\"end\":134831,\"start\":134826},{\"end\":135391,\"start\":135389},{\"end\":135408,\"start\":135403},{\"end\":135424,\"start\":135419},{\"end\":135823,\"start\":135818},{\"end\":135838,\"start\":135834},{\"end\":135851,\"start\":135847},{\"end\":135866,\"start\":135861},{\"end\":135879,\"start\":135873},{\"end\":135891,\"start\":135883},{\"end\":135913,\"start\":135907},{\"end\":135930,\"start\":135923},{\"end\":135947,\"start\":135937},{\"end\":136739,\"start\":136734},{\"end\":136748,\"start\":136744},{\"end\":136757,\"start\":136750}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/3314221.3314614\",\"id\":\"b0\",\"matched_paper_id\":127982021},\"end\":98241,\"start\":97666},{\"attributes\":{\"doi\":\"10.1007/s10107-020-01474-5\",\"id\":\"b1\",\"matched_paper_id\":53760231},\"end\":98705,\"start\":98243},{\"attributes\":{\"doi\":\"10.1016/0893-9659(91)90141-H\",\"id\":\"b2\",\"matched_paper_id\":123149253},\"end\":99074,\"start\":98707},{\"attributes\":{\"doi\":\"10.1007/BF02293050\",\"id\":\"b3\",\"matched_paper_id\":1524485},\"end\":99465,\"start\":99076},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":202766379},\"end\":100404,\"start\":99467},{\"attributes\":{\"doi\":\"10.1145/235815.235821\",\"id\":\"b5\"},\"end\":100765,\"start\":100406},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12263445},\"end\":101076,\"start\":100767},{\"attributes\":{\"id\":\"b7\"},\"end\":101598,\"start\":101078},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":54011126},\"end\":102620,\"start\":101600},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":213299187},\"end\":103524,\"start\":102622},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225075766},\"end\":104420,\"start\":103526},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":202577669},\"end\":104818,\"start\":104422},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":26605267},\"end\":105122,\"start\":104820},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2360355},\"end\":105427,\"start\":105124},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":59842968},\"end\":106221,\"start\":105429},{\"attributes\":{\"id\":\"b15\"},\"end\":106432,\"start\":106223},{\"attributes\":{\"id\":\"b16\"},\"end\":106684,\"start\":106434},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":225039902},\"end\":107870,\"start\":106686},{\"attributes\":{\"id\":\"b18\"},\"end\":108146,\"start\":107872},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1931807},\"end\":108590,\"start\":108148},{\"attributes\":{\"id\":\"b20\"},\"end\":108738,\"start\":108592},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15796021},\"end\":109142,\"start\":108740},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206579396},\"end\":109709,\"start\":109144},{\"attributes\":{\"id\":\"b23\"},\"end\":109978,\"start\":109711},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":204960212},\"end\":110731,\"start\":109980},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11626373},\"end\":111280,\"start\":110733},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14415696},\"end\":111558,\"start\":111282},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":516928},\"end\":112065,\"start\":111560},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":196610895},\"end\":112744,\"start\":112067},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5559466},\"end\":113238,\"start\":112746},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":49431481},\"end\":113707,\"start\":113240},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":208548538},\"end\":114146,\"start\":113709},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":208527458},\"end\":115014,\"start\":114148},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3488815},\"end\":115653,\"start\":115016},{\"attributes\":{\"id\":\"b34\"},\"end\":115871,\"start\":115655},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":51872670},\"end\":116660,\"start\":115873},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":28605696},\"end\":117144,\"start\":116662},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":219525319},\"end\":117524,\"start\":117146},{\"attributes\":{\"id\":\"b38\"},\"end\":117916,\"start\":117526},{\"attributes\":{\"id\":\"b39\"},\"end\":118210,\"start\":117918},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235613337},\"end\":118748,\"start\":118212},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":53215541},\"end\":119649,\"start\":118750},{\"attributes\":{\"id\":\"b42\"},\"end\":119984,\"start\":119651},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":211259016},\"end\":120805,\"start\":119986},{\"attributes\":{\"id\":\"b44\"},\"end\":121245,\"start\":120807},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":184487142},\"end\":122272,\"start\":121247},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":67855530},\"end\":123234,\"start\":122274},{\"attributes\":{\"id\":\"b47\"},\"end\":123540,\"start\":123236},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":17562024},\"end\":123882,\"start\":123542},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":207796611},\"end\":124834,\"start\":123884},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":53960414},\"end\":125730,\"start\":124836},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":57757287},\"end\":126135,\"start\":125732},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":196059499},\"end\":126637,\"start\":126137},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":15378666},\"end\":127021,\"start\":126639},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":604334},\"end\":127670,\"start\":127023},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":240493478},\"end\":128002,\"start\":127672},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":220055765},\"end\":129019,\"start\":128004},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":47016770},\"end\":129679,\"start\":129021},{\"attributes\":{\"id\":\"b58\"},\"end\":129961,\"start\":129681},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":52347370},\"end\":130850,\"start\":129963},{\"attributes\":{\"id\":\"b60\"},\"end\":131392,\"start\":130852},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":13750928},\"end\":132305,\"start\":131394},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":44124705},\"end\":133147,\"start\":132307},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":1977462},\"end\":133636,\"start\":133149},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":225086082},\"end\":134628,\"start\":133638},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":227210816},\"end\":135308,\"start\":134630},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":120660835},\"end\":135726,\"start\":135310},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":53297058},\"end\":136678,\"start\":135728},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":14454695},\"end\":137070,\"start\":136680}]", "bib_title": "[{\"end\":97758,\"start\":97666},{\"end\":98316,\"start\":98243},{\"end\":98783,\"start\":98707},{\"end\":99166,\"start\":99076},{\"end\":99517,\"start\":99467},{\"end\":100808,\"start\":100767},{\"end\":101691,\"start\":101600},{\"end\":102698,\"start\":102622},{\"end\":103604,\"start\":103526},{\"end\":104487,\"start\":104422},{\"end\":104875,\"start\":104820},{\"end\":105154,\"start\":105124},{\"end\":105486,\"start\":105429},{\"end\":106787,\"start\":106686},{\"end\":108217,\"start\":108148},{\"end\":108775,\"start\":108740},{\"end\":109232,\"start\":109144},{\"end\":110047,\"start\":109980},{\"end\":110776,\"start\":110733},{\"end\":111310,\"start\":111282},{\"end\":111628,\"start\":111560},{\"end\":112142,\"start\":112067},{\"end\":112813,\"start\":112746},{\"end\":113310,\"start\":113240},{\"end\":113765,\"start\":113709},{\"end\":114212,\"start\":114148},{\"end\":115077,\"start\":115016},{\"end\":115947,\"start\":115873},{\"end\":116755,\"start\":116662},{\"end\":117175,\"start\":117146},{\"end\":118255,\"start\":118212},{\"end\":118824,\"start\":118750},{\"end\":120038,\"start\":119986},{\"end\":121323,\"start\":121247},{\"end\":122353,\"start\":122274},{\"end\":123620,\"start\":123542},{\"end\":123956,\"start\":123884},{\"end\":124879,\"start\":124836},{\"end\":125781,\"start\":125732},{\"end\":126189,\"start\":126137},{\"end\":126669,\"start\":126639},{\"end\":127063,\"start\":127023},{\"end\":127767,\"start\":127672},{\"end\":128113,\"start\":128004},{\"end\":129092,\"start\":129021},{\"end\":130014,\"start\":129963},{\"end\":131460,\"start\":131394},{\"end\":132344,\"start\":132307},{\"end\":133228,\"start\":133149},{\"end\":133714,\"start\":133638},{\"end\":134749,\"start\":134630},{\"end\":135378,\"start\":135310},{\"end\":135811,\"start\":135728},{\"end\":136724,\"start\":136680}]", "bib_author": "[{\"end\":97775,\"start\":97760},{\"end\":97793,\"start\":97775},{\"end\":97806,\"start\":97793},{\"end\":97824,\"start\":97806},{\"end\":98333,\"start\":98318},{\"end\":98348,\"start\":98333},{\"end\":98357,\"start\":98348},{\"end\":98383,\"start\":98357},{\"end\":98402,\"start\":98383},{\"end\":98797,\"start\":98785},{\"end\":98811,\"start\":98797},{\"end\":99180,\"start\":99168},{\"end\":99194,\"start\":99180},{\"end\":99537,\"start\":99519},{\"end\":99556,\"start\":99537},{\"end\":99573,\"start\":99556},{\"end\":99585,\"start\":99573},{\"end\":99602,\"start\":99585},{\"end\":99606,\"start\":99602},{\"end\":99623,\"start\":99606},{\"end\":99640,\"start\":99623},{\"end\":100464,\"start\":100447},{\"end\":100486,\"start\":100464},{\"end\":100497,\"start\":100486},{\"end\":100829,\"start\":100810},{\"end\":100839,\"start\":100829},{\"end\":100857,\"start\":100839},{\"end\":100864,\"start\":100857},{\"end\":101139,\"start\":101121},{\"end\":101157,\"start\":101139},{\"end\":101177,\"start\":101157},{\"end\":101194,\"start\":101177},{\"end\":101206,\"start\":101194},{\"end\":101221,\"start\":101206},{\"end\":101233,\"start\":101221},{\"end\":101248,\"start\":101233},{\"end\":101261,\"start\":101248},{\"end\":101276,\"start\":101261},{\"end\":101283,\"start\":101276},{\"end\":101711,\"start\":101693},{\"end\":101733,\"start\":101711},{\"end\":101745,\"start\":101733},{\"end\":101755,\"start\":101745},{\"end\":101763,\"start\":101755},{\"end\":102715,\"start\":102700},{\"end\":102736,\"start\":102715},{\"end\":102751,\"start\":102736},{\"end\":102769,\"start\":102751},{\"end\":102783,\"start\":102769},{\"end\":103618,\"start\":103606},{\"end\":103633,\"start\":103618},{\"end\":103655,\"start\":103633},{\"end\":103680,\"start\":103655},{\"end\":104501,\"start\":104489},{\"end\":104513,\"start\":104501},{\"end\":104530,\"start\":104513},{\"end\":104546,\"start\":104530},{\"end\":104554,\"start\":104546},{\"end\":104565,\"start\":104554},{\"end\":104895,\"start\":104877},{\"end\":105172,\"start\":105156},{\"end\":105190,\"start\":105172},{\"end\":105504,\"start\":105488},{\"end\":105520,\"start\":105504},{\"end\":105535,\"start\":105520},{\"end\":106241,\"start\":106225},{\"end\":106485,\"start\":106469},{\"end\":106495,\"start\":106485},{\"end\":106808,\"start\":106789},{\"end\":106833,\"start\":106808},{\"end\":106849,\"start\":106833},{\"end\":106868,\"start\":106849},{\"end\":106885,\"start\":106868},{\"end\":106897,\"start\":106885},{\"end\":106913,\"start\":106897},{\"end\":106931,\"start\":106913},{\"end\":106949,\"start\":106931},{\"end\":106962,\"start\":106949},{\"end\":106978,\"start\":106962},{\"end\":107896,\"start\":107874},{\"end\":108236,\"start\":108219},{\"end\":108606,\"start\":108592},{\"end\":108791,\"start\":108777},{\"end\":108805,\"start\":108791},{\"end\":109246,\"start\":109234},{\"end\":109262,\"start\":109246},{\"end\":109284,\"start\":109262},{\"end\":109299,\"start\":109284},{\"end\":109317,\"start\":109299},{\"end\":109332,\"start\":109317},{\"end\":109823,\"start\":109809},{\"end\":110061,\"start\":110049},{\"end\":110086,\"start\":110061},{\"end\":110104,\"start\":110086},{\"end\":110116,\"start\":110104},{\"end\":110129,\"start\":110116},{\"end\":110146,\"start\":110129},{\"end\":110166,\"start\":110146},{\"end\":110187,\"start\":110166},{\"end\":110203,\"start\":110187},{\"end\":110803,\"start\":110778},{\"end\":110826,\"start\":110803},{\"end\":110836,\"start\":110826},{\"end\":110840,\"start\":110836},{\"end\":111328,\"start\":111312},{\"end\":111640,\"start\":111630},{\"end\":111655,\"start\":111640},{\"end\":111664,\"start\":111655},{\"end\":111675,\"start\":111664},{\"end\":111683,\"start\":111675},{\"end\":111697,\"start\":111683},{\"end\":112154,\"start\":112144},{\"end\":112169,\"start\":112154},{\"end\":112186,\"start\":112169},{\"end\":112199,\"start\":112186},{\"end\":112220,\"start\":112199},{\"end\":112232,\"start\":112220},{\"end\":112244,\"start\":112232},{\"end\":112262,\"start\":112244},{\"end\":112272,\"start\":112262},{\"end\":112291,\"start\":112272},{\"end\":112835,\"start\":112815},{\"end\":112846,\"start\":112835},{\"end\":112861,\"start\":112846},{\"end\":112871,\"start\":112861},{\"end\":113329,\"start\":113312},{\"end\":113349,\"start\":113329},{\"end\":113366,\"start\":113349},{\"end\":113378,\"start\":113366},{\"end\":113390,\"start\":113378},{\"end\":113779,\"start\":113767},{\"end\":113795,\"start\":113779},{\"end\":114228,\"start\":114214},{\"end\":114242,\"start\":114228},{\"end\":114256,\"start\":114242},{\"end\":114267,\"start\":114256},{\"end\":114278,\"start\":114267},{\"end\":114291,\"start\":114278},{\"end\":115097,\"start\":115079},{\"end\":115117,\"start\":115097},{\"end\":115133,\"start\":115117},{\"end\":115151,\"start\":115133},{\"end\":115165,\"start\":115151},{\"end\":115744,\"start\":115724},{\"end\":115759,\"start\":115744},{\"end\":115965,\"start\":115949},{\"end\":115977,\"start\":115965},{\"end\":115994,\"start\":115977},{\"end\":116775,\"start\":116757},{\"end\":116786,\"start\":116775},{\"end\":116804,\"start\":116786},{\"end\":116821,\"start\":116804},{\"end\":116829,\"start\":116821},{\"end\":117204,\"start\":117177},{\"end\":117212,\"start\":117204},{\"end\":117222,\"start\":117212},{\"end\":117241,\"start\":117222},{\"end\":117249,\"start\":117241},{\"end\":117544,\"start\":117526},{\"end\":117560,\"start\":117544},{\"end\":117577,\"start\":117560},{\"end\":117936,\"start\":117918},{\"end\":117953,\"start\":117936},{\"end\":118278,\"start\":118257},{\"end\":118295,\"start\":118278},{\"end\":118309,\"start\":118295},{\"end\":118321,\"start\":118309},{\"end\":118336,\"start\":118321},{\"end\":118343,\"start\":118336},{\"end\":118845,\"start\":118826},{\"end\":118863,\"start\":118845},{\"end\":118876,\"start\":118863},{\"end\":118888,\"start\":118876},{\"end\":118897,\"start\":118888},{\"end\":118910,\"start\":118897},{\"end\":118927,\"start\":118910},{\"end\":118944,\"start\":118927},{\"end\":119711,\"start\":119698},{\"end\":119730,\"start\":119711},{\"end\":119748,\"start\":119730},{\"end\":119763,\"start\":119748},{\"end\":120053,\"start\":120040},{\"end\":120071,\"start\":120053},{\"end\":120085,\"start\":120071},{\"end\":120102,\"start\":120085},{\"end\":120897,\"start\":120882},{\"end\":120909,\"start\":120897},{\"end\":120927,\"start\":120909},{\"end\":120944,\"start\":120927},{\"end\":120956,\"start\":120944},{\"end\":120971,\"start\":120956},{\"end\":121344,\"start\":121325},{\"end\":121348,\"start\":121344},{\"end\":121356,\"start\":121348},{\"end\":121379,\"start\":121356},{\"end\":121391,\"start\":121379},{\"end\":121408,\"start\":121391},{\"end\":121421,\"start\":121408},{\"end\":121427,\"start\":121421},{\"end\":122373,\"start\":122355},{\"end\":122384,\"start\":122373},{\"end\":122399,\"start\":122384},{\"end\":122416,\"start\":122399},{\"end\":122423,\"start\":122416},{\"end\":123316,\"start\":123294},{\"end\":123334,\"start\":123316},{\"end\":123638,\"start\":123622},{\"end\":123975,\"start\":123958},{\"end\":123992,\"start\":123975},{\"end\":124008,\"start\":123992},{\"end\":124025,\"start\":124008},{\"end\":124029,\"start\":124025},{\"end\":124046,\"start\":124029},{\"end\":124063,\"start\":124046},{\"end\":124898,\"start\":124881},{\"end\":124910,\"start\":124898},{\"end\":124926,\"start\":124910},{\"end\":124942,\"start\":124926},{\"end\":124959,\"start\":124942},{\"end\":124971,\"start\":124959},{\"end\":124980,\"start\":124971},{\"end\":124993,\"start\":124980},{\"end\":125010,\"start\":124993},{\"end\":125027,\"start\":125010},{\"end\":125800,\"start\":125783},{\"end\":125812,\"start\":125800},{\"end\":125828,\"start\":125812},{\"end\":125843,\"start\":125828},{\"end\":126208,\"start\":126191},{\"end\":126220,\"start\":126208},{\"end\":126236,\"start\":126220},{\"end\":126253,\"start\":126236},{\"end\":126688,\"start\":126671},{\"end\":126704,\"start\":126688},{\"end\":126719,\"start\":126704},{\"end\":127084,\"start\":127065},{\"end\":127102,\"start\":127084},{\"end\":127118,\"start\":127102},{\"end\":127130,\"start\":127118},{\"end\":127145,\"start\":127130},{\"end\":127163,\"start\":127145},{\"end\":127175,\"start\":127163},{\"end\":128141,\"start\":128115},{\"end\":128156,\"start\":128141},{\"end\":128171,\"start\":128156},{\"end\":128180,\"start\":128171},{\"end\":128194,\"start\":128180},{\"end\":128213,\"start\":128194},{\"end\":129109,\"start\":129094},{\"end\":129121,\"start\":129109},{\"end\":129135,\"start\":129121},{\"end\":129753,\"start\":129737},{\"end\":129767,\"start\":129753},{\"end\":130028,\"start\":130016},{\"end\":130039,\"start\":130028},{\"end\":130058,\"start\":130039},{\"end\":130072,\"start\":130058},{\"end\":130084,\"start\":130072},{\"end\":130096,\"start\":130084},{\"end\":130105,\"start\":130096},{\"end\":130118,\"start\":130105},{\"end\":130135,\"start\":130118},{\"end\":130152,\"start\":130135},{\"end\":130995,\"start\":130983},{\"end\":131007,\"start\":130995},{\"end\":131017,\"start\":131007},{\"end\":131026,\"start\":131017},{\"end\":131038,\"start\":131026},{\"end\":131053,\"start\":131038},{\"end\":131068,\"start\":131053},{\"end\":131482,\"start\":131462},{\"end\":131496,\"start\":131482},{\"end\":131507,\"start\":131496},{\"end\":131521,\"start\":131507},{\"end\":131533,\"start\":131521},{\"end\":131549,\"start\":131533},{\"end\":131568,\"start\":131549},{\"end\":131577,\"start\":131568},{\"end\":132357,\"start\":132346},{\"end\":132374,\"start\":132357},{\"end\":132394,\"start\":132374},{\"end\":132402,\"start\":132394},{\"end\":132419,\"start\":132402},{\"end\":132432,\"start\":132419},{\"end\":132449,\"start\":132432},{\"end\":132466,\"start\":132449},{\"end\":133245,\"start\":133230},{\"end\":133262,\"start\":133245},{\"end\":133281,\"start\":133262},{\"end\":133726,\"start\":133716},{\"end\":133740,\"start\":133726},{\"end\":133752,\"start\":133740},{\"end\":133764,\"start\":133752},{\"end\":133779,\"start\":133764},{\"end\":133793,\"start\":133779},{\"end\":133811,\"start\":133793},{\"end\":133820,\"start\":133811},{\"end\":133835,\"start\":133820},{\"end\":134761,\"start\":134751},{\"end\":134773,\"start\":134761},{\"end\":134785,\"start\":134773},{\"end\":134797,\"start\":134785},{\"end\":134809,\"start\":134797},{\"end\":134818,\"start\":134809},{\"end\":134833,\"start\":134818},{\"end\":135393,\"start\":135380},{\"end\":135410,\"start\":135393},{\"end\":135426,\"start\":135410},{\"end\":135825,\"start\":135813},{\"end\":135840,\"start\":135825},{\"end\":135853,\"start\":135840},{\"end\":135868,\"start\":135853},{\"end\":135881,\"start\":135868},{\"end\":135893,\"start\":135881},{\"end\":135902,\"start\":135893},{\"end\":135915,\"start\":135902},{\"end\":135932,\"start\":135915},{\"end\":135949,\"start\":135932},{\"end\":136741,\"start\":136726},{\"end\":136750,\"start\":136741},{\"end\":136759,\"start\":136750}]", "bib_venue": "[{\"end\":97961,\"start\":97908},{\"end\":99856,\"start\":99826},{\"end\":102063,\"start\":102042},{\"end\":102905,\"start\":102888},{\"end\":105765,\"start\":105674},{\"end\":113874,\"start\":113853},{\"end\":114413,\"start\":114396},{\"end\":115244,\"start\":115223},{\"end\":116189,\"start\":116101},{\"end\":118434,\"start\":118427},{\"end\":119120,\"start\":119113},{\"end\":121678,\"start\":121648},{\"end\":122674,\"start\":122644},{\"end\":124279,\"start\":124249},{\"end\":125203,\"start\":125196},{\"end\":126342,\"start\":126322},{\"end\":126832,\"start\":126791},{\"end\":127271,\"start\":127254},{\"end\":129233,\"start\":129204},{\"end\":130328,\"start\":130321},{\"end\":131772,\"start\":131684},{\"end\":132642,\"start\":132635},{\"end\":134924,\"start\":134917},{\"end\":136125,\"start\":136118},{\"end\":97906,\"start\":97847},{\"end\":98452,\"start\":98428},{\"end\":98866,\"start\":98839},{\"end\":99245,\"start\":99212},{\"end\":99752,\"start\":99640},{\"end\":100445,\"start\":100406},{\"end\":100896,\"start\":100885},{\"end\":101119,\"start\":101078},{\"end\":102040,\"start\":101791},{\"end\":102886,\"start\":102807},{\"end\":103797,\"start\":103680},{\"end\":104601,\"start\":104565},{\"end\":104946,\"start\":104913},{\"end\":105249,\"start\":105218},{\"end\":105629,\"start\":105550},{\"end\":106303,\"start\":106262},{\"end\":106467,\"start\":106434},{\"end\":107095,\"start\":106978},{\"end\":107957,\"start\":107921},{\"end\":108341,\"start\":108264},{\"end\":108652,\"start\":108630},{\"end\":108912,\"start\":108829},{\"end\":109407,\"start\":109353},{\"end\":109807,\"start\":109711},{\"end\":110311,\"start\":110226},{\"end\":110922,\"start\":110867},{\"end\":111393,\"start\":111355},{\"end\":111779,\"start\":111724},{\"end\":112374,\"start\":112319},{\"end\":112966,\"start\":112896},{\"end\":113449,\"start\":113411},{\"end\":113851,\"start\":113795},{\"end\":114394,\"start\":114315},{\"end\":115221,\"start\":115165},{\"end\":115722,\"start\":115655},{\"end\":116066,\"start\":115998},{\"end\":116878,\"start\":116857},{\"end\":117310,\"start\":117274},{\"end\":117716,\"start\":117577},{\"end\":118062,\"start\":117975},{\"end\":118425,\"start\":118343},{\"end\":119056,\"start\":118944},{\"end\":119696,\"start\":119651},{\"end\":120219,\"start\":120102},{\"end\":120880,\"start\":120807},{\"end\":121539,\"start\":121427},{\"end\":122535,\"start\":122423},{\"end\":123292,\"start\":123236},{\"end\":123688,\"start\":123666},{\"end\":124175,\"start\":124063},{\"end\":125139,\"start\":125027},{\"end\":125905,\"start\":125858},{\"end\":126320,\"start\":126253},{\"end\":126789,\"start\":126742},{\"end\":127231,\"start\":127175},{\"end\":127792,\"start\":127769},{\"end\":128344,\"start\":128213},{\"end\":129202,\"start\":129135},{\"end\":129735,\"start\":129681},{\"end\":130264,\"start\":130152},{\"end\":130981,\"start\":130852},{\"end\":131649,\"start\":131581},{\"end\":132578,\"start\":132466},{\"end\":133364,\"start\":133307},{\"end\":133952,\"start\":133835},{\"end\":134915,\"start\":134833},{\"end\":135490,\"start\":135455},{\"end\":136061,\"start\":135949},{\"end\":136847,\"start\":136785}]"}}}, "year": 2023, "month": 12, "day": 17}