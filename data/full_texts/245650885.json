{"id": 245650885, "updated": "2023-11-10 23:01:24.41", "metadata": {"title": "Language as Queries for Referring Video Object Segmentation", "authors": "[{\"first\":\"Jiannan\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Peize\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Zehuan\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": 6, "day": 1}, "abstract": "Referring video object segmentation (R-VOS) is an emerging cross-modal task that aims to segment the target object referred by a language expression in all video frames. In this work, we propose a simple and unified framework built upon Transformer, termed ReferFormer. It views the language as queries and directly attends to the most relevant regions in the video frames. Concretely, we introduce a small set of object queries conditioned on the language as the input to the Transformer. In this manner, all the queries are obligated to find the referred objects only. They are eventually transformed into dynamic kernels which capture the crucial object-level information, and play the role of convolution filters to generate the segmentation masks from feature maps. The object tracking is achieved naturally by linking the corresponding queries across frames. This mechanism greatly simplifies the pipeline and the end-to-end framework is significantly different from the previous methods. Extensive experiments on Ref-Youtube-vos, Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-vos, ReferFormer achieves 55.6 J&F with a ResNet-50 backbone without bells and whistles, which exceeds the previous state-of-the-art performance by 8.4 points. In addition, with the strong Video-Swin-Base backbone, ReferFormer achieves the best J&F of 64.9 among all existing methods. Moreover, we show the impressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences and JHMDB-Sentences respectively, which significantly outperforms the previous methods by a large margin. Code is publicly available at https://github.com/wjn922/ReferFormer.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WuJSYL22", "doi": "10.1109/cvpr52688.2022.00492"}}, "content": {"source": {"pdf_hash": "4d7180bbab1278ea921f7ec258209fed604db15f", "pdf_src": "IEEE", "pdf_uri": "[\"https://arxiv.org/pdf/2201.00487v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2201.00487", "status": "GREEN"}}, "grobid": {"id": "534c1f24c12732195b39b1746680f535295bb87f", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/4d7180bbab1278ea921f7ec258209fed604db15f.txt", "contents": "\nLanguage as Queries for Referring Video Object Segmentation\n\n\nJiannan Wu \nThe University of Hong\nKong 2 ByteDance\n\nYi Jiang \nPeize Sun \nThe University of Hong\nKong 2 ByteDance\n\nZehuan Yuan \nPing Luo \nThe University of Hong\nKong 2 ByteDance\n\nHKU-TCL Joint Research Centre for Artificial Intelligence\n\n\nLanguage as Queries for Referring Video Object Segmentation\n10.1109/CVPR52688.2022.00492\nReferring video object segmentation (R-VOS) is an emerging cross-modal task that aims to segment the target object referred by a language expression in all video frames.In this work, we propose a simple and unified framework built upon Transformer, termed ReferFormer. It views the language as queries and directly attends to the most relevant regions in the video frames. Concretely, we introduce a small set of object queries conditioned on the language as the input to the Transformer. In this manner, all the queries are obligated to find the referred objects only. They are eventually transformed into dynamic kernels which capture the crucial object-level information, and play the role of convolution filters to generate the segmentation masks from feature maps. The object tracking is achieved naturally by linking the corresponding queries across frames. This mechanism greatly simplifies the pipeline and the endto-end framework is significantly different from the previous methods. Extensive experiments on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-VOS, ReferFormer achieves 55.6 J &F with a ResNet-50 backbone without bells and whistles, which exceeds the previous state-of-the-art performance by 8.4 points. In addition, with the strong Video-Swin-Base backbone, ReferFormer achieves the best J &F of 64.9 among all existing methods. Moreover, we show the impressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences and JHMDB-Sentences respectively, which significantly outperforms the previous methods by a large margin. Code is publicly available at https://github.com/wjn922/ReferFormer.\n\nIntroduction\n\nReferring video object segmentation (R-VOS) aims to segment the target object in a video given a natural language description. This emerging topic has raised great attention in the research community and is expected to benefit many applications in a friendly and interactive way, e.g., video editing and video surveillance. R-VOS is more challenging than the traditional semi-supervised video object segmentation [33,46], because it does not only lack the ground-truth mask annotation in the first frame, but also require the comprehensive understanding of the cross-modal sources, i.e., vision and language. Therefore, the model should have a strong ability to infer which object is referred and to perform accurate segmentation.\n\nTo accomplish this task, the existing methods can be mainly categorized into two groups: (1) Bottom-up methods. These methods incorporate the vision and language features in a early-fusion manner, and then adopt a FCN [28] as decoder to generate object masks, as shown in Figure 1(a). (2) Top-down methods. These methods tackle the problem in a top-down perspective and follow a two-stage pipeline. As illustrated in Figure 1(b), they first employ an instance segmentation model to find all the objects in each frame, and then associate them in the entire video to form the tracklet candidates. Afterwards, they use the expression as the grounding criterion to select the best-matched one.\n\nAlthough these two streams of methods have demonstrated their effectiveness with promising results, they still have some intrinsic limitations. First, for the bottom-up methods, they fail to capture the crucial instance-level information and do not consider the object association across multiple frames. Therefore, these methods can not provide explicit knowledge for cross-modal reasoning and would encounter the discrepancy of predicted object due to scene changes. Second, although top-down methods have greatly boost the performance over the bottom-up methods, they suffer from heavy workload because of the complex, multistage pipeline. Furthermore, the separate optimization on several sub-problems would lead to sub-optimal solution.\n\nThese limitations of current methods motivate us to design a simple and unified framework that solves the R-VOS task elegantly. The recent success of Transformer [40] in object detection [4,55] and video instance segmentation [15,43,44] demonstrates a promising solution. However, it is non-trivial to apply such models to the R-VOS task. These models [4,55] use a fixed number (e.g., 100) of learnable queries to detect all the objects in an image. Under this circumstance, it would be confused for the model to distinguish which object is referred due to the randomness of the expression. Here raises a natural question: \"Is it possible for a unified model to know where to look using queries?\"\n\nThis work answers the question by proposing the notion of language as queries, as shown in Figure 1(c). We put the linguistic restriction on all object queries and use these conditional queries as input for the model. In this manner, the expression will make the queries focus on the referred object only, and thus greatly reducing the query number (e.g., 5 in our experiments). The next challenge lies in how to decode the object mask from query representations. As the queries contain rich instance characteristics, we view them as instance-aware dynamic kernels to filter out the segmentation masks from feature maps.\n\nThe unified framework can not only produce the segmentation masks for referred objects, but also the classification results and detection boxes. Moreover, the conditional queries are linked via instance matching strategy across frames so that the object tracking is achieved naturally without post-process. We hope this framework could serve as a strong baseline for R-VOS task.\n\nThe main contributions of this work are as follows.\n\n\u2022 We propose a simple and unified framework for referring video object segmentation, termed ReferFormer. Given a video clip and the corresponding language expression, our framework directly detects, segments and tracks the referred object in all frames in an end-to-end manner. \u2022 We present the notion of language as queries. We in-troduce a small set of object queries which conditioned on the text expression to attend the referred object only. These conditional queries are shared across different frames in the initial state and they are transformed into dynamic kernels to filter out the segmentation masks from feature maps. This mechanism provides a new perspective for the R-VOS task. \u2022 \n\n\nRelated Work\n\nVideo Object Segmentation. The traditional video object segmentation (VOS) aims to propagates the groundtruth object masks given in the first frame to the entire video. Most recent works [7,32,41,49] lie in the group of matching-based methods, which perform feature matching to track the target objects.\n\nReferring Video Object Segmentation. Referring video object segmentation (R-VOS) provides the language description instead of mask annotation as the object reference, thus it would be a more challenging task. The current methods for R-VOS mainly follow the two pipelines:\n\n(1) Bottom-up methods. An intuitive thinking is directly applying the image-level referring methods [8,14,29,53] on the video frames independently, e.g., RefVOS [2]. The obvious drawback of such methods is that they fail to utilize the valuable temporal information across frames, resulting in inconsistent object prediction due to the scene or appearance variations. To address this issue, URVOS [36] casts the task as a joint problem of referring object segmentation in an image and mask propagation in a video. They propose a unified referring VOS framework that employs a memory attention module to leverage the information of mask predictions in previous frames.\n\n(2) Top-down methods. The typical top-down method [20] first constructs an exhaustive set of object tracklets by propagating the object masks detected from several key frames to the whole video. Then, a language grounding model is built to select the best object tracklet from the candidate set. Although the method has made breakthrough Figure 2. The overall pipeline of ReferFormer. It mainly consists of four parts: Backbone, Transformer, Cross-modal Feature Pyramid and the Segmentation part. The model takes a video clip with the corresponding language expression as input and output the segmentation mask of the referred object in each frame. For the Transformer decoder input, the object queries are conditioned on the language expression to find the referred object. The same colors represent the queries in the same frame and the same shapes represent the queries refer to the same instance. The order of queries inner frame keep consistent for different frames. Best viewed in color.\n\nperformance improvement over the previous methods, the complex, multi-stage pipeline is computational-expensive and impractical. In contrast to these two pipelines, we propose a query-based method that achieves the strongest performance with a simple and unified framework. Transformer Transformer [40] was first introduced for sequence-to-sequence translation in natural language processing community and has achieved marvelous success in most computer vision tasks [10,17,26] such as object detection [4,55], tracking [5,30,37,47] and segmentation [6,15,54]. DETR [4] introduces the new query-based paradigm [38,55] for object detection, which employs a set of object queries as candidates and inputs them to the Transformer decoder. Beyond image field, VisTR [43] extends the framework for video instance segmentation (VIS) [48] task and solves the problem in a direct end-to-end parallel sequence decoding manner. SeqFormer [44] decouples the content query and box query to aggregates temporal information from each frame and achieves the state-of-theart performance on VIS task. Inspired by these works, our work also relies on the query mechanism of Transformer but considers an additional modality, i.e., language, as the object reference. Thus, we propose the notion of language as queries and build the simple and unified framework that segments and tracks the referred object simultaneously.\n\n\nApproach\n\nGiven a video clip I = {I t } T t=1 with T frames and a referring expression E = {e l } L l=1 with L words, we aim to produce T -frame binary segmentation masks of referred object S = {s t } T t=1 , s t \u2208 R H\u00d7W in an end-to-end manner. To this end, we propose a simple and unified framework named ReferFormer, as shown in Figure 2. It mainly consists of four key components: Backbone, Transformer, Cross-modal Feature Pyramid network (CM-FPN) and the Instance Sequence Segmentation process. A small set of object queries conditioned on the language is introduced to find the referred object. During inference, we directly output the mask predictions by selecting the queries with the highest average score as the final results.\n\n\nBackbone\n\nVisual Encoder. We start by adopting a visual backbone to extract the multi-scale feature maps for each frame in the video clip independently, resulting in the visual feature\nsequence F v = {f t } T t=1\n. It is noteworthy that both the 2D and 3D visual encoder could be adopted.\n\nLinguistic Encoder. Given the language description with L words, we use off-the-shelf linguistic embedding model, RoBERTa [25], to extract the text feature\nF e = {f i } L i=1 .\nAnd we also obtain the sentence-level feature f s e \u2208 R C by pooling the features of each word.\n\n\nLanguage as Queries\n\nThe key design comes from that we use a set of object queries conditioned on the language expression, termed conditional queries, as the Transformer decoder input. These queries are obligated to focus on the referred object only and produce the instance-aware dynamic kernels. The final segmentation masks are obtained by per-forming dynamic convolution between the dynamic kernels and their corresponding feature maps. Here, we adopt the Deformable-DETR [55] as our Transformer model due to its effectiveness and efficiency for global attention.\n\nTransformer Encoder. First, a 1 \u00d7 1 convolution is applied on the multi-scale visual features F v to reduce the channel dimension of all feature maps to C = 256. To enrich the information of visual features, we then incorporate projected visual features with the text feature F e in a multiplication way and form the new multi-scale feature maps\nF v = f t T t=1\n. Afterwards, the fixed 2D positional encoding is added to feature maps of each frame and the summed features are fed into the Transformer encoder. To utilize the Transformer process the video frames independently, we flatten the spatial dimensions and move the temporal dimension to batch dimension for efficiency. Finally, the output of the Transformer encoder, i.e., encoded memory, is then input to the decoder.\n\nTransformer Decoder. We introduce N object queries to represent the instances for each frame similar to [43], the difference lies in that the query weights are shared across video frames to handle the length-variable videos. Meanwhile, we repeat the sentence feature f s e for N times to fit the query number. Both the object queries and repeated sentence features are fed into the decoder as input. In this manner, all the queries will use the language expression as guidance and try to find the referred objects only. These conditional queries are duplicated to serve as the decoder input for all the frames and they are turned into instance embeddings by the decoder eventually, resulting in the set of N q = T \u00d7 N predictions. It should be noted the queries keep the same order across different frames and we refer to the queries in the same relative position (represented as the same shape in Figure 2) as instance sequence. Therefore, the temporal coherence of referred object could be achieved easily by linking the corresponding queries.\n\nPrediction Heads. Three lightweight heads are built on top of the decoder to further transform the N q instance embeddings. The class head outputs the binary probability which indicates whether the instance is referred by the text sentence and this instance is visible in the current frame. It could also be modified to predict the referred object category by simply changing the output class number. The mask head is implemented by three consecutive linear layers. It produces the parameters of N q dynamic kernels\n\u03a9 = {\u03c9 i } Nq i=1\n, which is similar to the conditional convolutional filters in [39]. These parameters will be reshaped to form the three 1 \u00d7 1 convolution layers with the channel number as 8. The box head is a 3-layer feed forward network (FFN) with ReLU activation except for the last layer. It will predict the box location of the referred object and Figure 3. We visualize the predicted boxes from all the queries. It can be seen that the these boxes will locate near the referred object only even if there are other objects in the video. thus the position of dynamic kernels could be determined by the center of corresponding boxes.\n\nDynamic Convolution. Suppose now we have obtained the semantically-rich feature maps F seg = f t seg T t=1 (will be discussed in Sec. 3.3) for each frame, the question is how we perform the instance sequence segmentation and obtain the masks of referred object from them. Since the dynamic kernels have captured the object-level information, we use them as convolution filters on the feature maps for instance decoding. Considering that the location prior of dynamic kernels \u03a9 provides a strong and robust reference for the referred object, we concatenate the feature maps F seg with relative coordinates for each dynamic kernel. Finally, the binary segmentation masks are generated by performing dynamic convolution between the conditional convolutional weights and their corresponding feature maps:\ns i = f i \u03c9 i Nq i=1 (1)\nwhere \u03c9 i andf i are the i-th dynamic kernel weights and its exclusive feature map, respectively. We reshape the output masks in frame-order sequence, resulting in a set as\u015c \u2208\nR T \u00d7N \u00d7 H 4 \u00d7 W 4 .\nIllustration of conditional queries. It is well known that the decoder embedding and position embedding in Transformer decoder encode the content and spatial information respectively. In our framework, these two parts are fed with the text sentence feature and learanble queries parameters, so that all the queries are restricted by the language expression. As shown in figure 3, these queries will focus on the referred object only even if other objects exist in the video. And there will be one query with much higher score while the scores of other queries will be suppressed.\n\n\nCross-modal Feature Pyramid Network\n\nFeature pyramid network (FPN) [22] is adopted to produce multi-scale feature maps for video frames. We construct a 4-level pyramid with the spatial stride from 4\u00d7 to 32\u00d7. Specifically, the Transformer memory (spatial strides {8, 16, 32}) and the 4\u00d7 feature from visual backbone are stacked to form the hierarchical features. And we design a cross-modal feature pyramid network (CM-FPN) to perform multi-scale cross-modal fusion for finer interaction, see the architecture figure in supplementary materials.\n\nIn each level, the interaction process is achieved by the vision-language fusion module. And we take the l-th level feature of FPN as an example to clarify the process. Here, we use f l v \u2208 R T \u00d7H l \u00d7W l \u00d7C to represent the l-th level visual feature for simplicity. Before the MHSA module, the spatial size of vision feature F l v is downsampled by a factor of \u03c3 while the temporal dimension is kept unchanged. Thus, the complexity of self-attention [40] operation would be greatly reduced, making the fusion module can be inserted into each level of FPN. Then, the spatial size of vision feature is recovered to H l \u00d7 W l for maintaining fine-grained information. We set the downsample factors as [8,4,2,1] for the 4-level features maps. Next, f l v interact with wordlevel feature f e in a cross-attention way, where the query, key are vision and language feature, respectively:\nInteract(f l v , f e ) = Softmax( f l v W Q \u00b7 (f e W K ) T \u221a d head )f e W V (2) where W Q , W K , W V \u2208 R C\u00d7dhead are learnable parameters.\nFinally, we apply an additional 3 \u00d7 3 convolutional layer on the feature maps with spatial stride 4 to get the final feature\nmaps F seg = f t seg T t=1 , where f t seg \u2208 R H 4 \u00d7 W 4 \u00d7C d .\n\nInstance Sequence Matching and Loss\n\nUsing N conditional queries, we generate the set of N q = T \u00d7 N predictions, which can be regarded as the trajectories of N instances on T frames. As described previous, the predictions across frames maintain the same relative positions. Therefore, we can supervise the instance sequence as a whole using instance matching strategy [43]. Let us denote the prediction set as\u0177 = {\u0177 i } N i=1 , and the predictions for the i-th instance is represented by:\ny i = p t i ,b t i ,\u015d t i T t=1(3)\nFor the t-th frame,p t i \u2208 R 1 is a probability scalar indicating whether the instance corresponds to the referred object and this object is visible in the current frame.b t i \u2208 R 4 is the normalized vector defines the center coordinates as well as the height and width of predicted box.\u015d t i \u2208 R H 4 \u00d7 W 4 is the predicted binary segmentation mask.\n\nSince there is only one referred object in the video, the ground-truth instance sequence is represented as y = {c t , b t , s t } T t=1 . c t is an one-hot value and it equals 1 when the ground-truth instance is visible in the frame I t otherwise 0. To train the network, we first find the best prediction as the positive sample via minimizing the matching cost:\ny pos = arg min yi\u2208\u0177 L match (y,\u0177 i )(4)\nL match (y,\u0177 i ) = \u03bb cls L cls (y,\u0177 i ) + \u03bb box L box (y,\u0177 i )\n+ \u03bb mask L mask (y,\u0177 i )(5)\nThe matching cost is computed from each frame and normalized by the frame number. Here, L cls (y,\u0177 i ) is the focal loss [23] that supervises the predicted instance sequence reference results. The box-related loss sums up the L1 loss and GIoU loss [35]. And the mask-related loss is the combination of DICE loss [31] and binary mask focal loss. Both the two mask losses are spatio-temporally calculated over the entire video clip. The network is optimized by minimizing the total loss L match for positive samples while letting the negative samples predict the \u2205 class.\n\n\nInference\n\nAs mentioned previously, ReferFormer can handle the videos of arbitrary length in a single forward pass since all the frames share the same initial conditional queries. Given the video and language expression, ReferFormer will predict N instance sequences. For each instance sequence, we average the predicted reference probabilities over all the frames and obtain the reference score set P = {p i } N i=1 . We select the instance sequence with the highest average score and its index is denoted as \u03c3:\n\u03c3 = argmax i\u2208{1,2,...,N } p i(6)\nThe final segmentation masks for each frame S = {s t } T t=1 is obtained from the mask candidates set\u015c by selecting the corresponding queries indexed with \u03c3. No postprocess is needed for associating objects since the linked queries naturally track the same instance.\n\n\nExperiments\n\n\nDatasets and Metrics\n\nDatasets. The experiments are conducted on the four popular R-VOS benchmarks. Ref-Youtube-VOS [36] is a largescale benchmark which covers 3,978 videos with \u223c15K language descriptions. Ref-DAVIS17 [18] is built upon DAVIS17 [34] by providing the language description for a specific object in each video and contains 90 videos. A2D-Sentences and JHMDB-Sentences are created by providing the additional textual annotations on the original A2D [45] and JHMDB [16] datasets. A2D-Sentences contains 3,782 videos and each video has 3-5 frames annotated with the pixel-level segmentation masks. JHMDB-Sentences has 928 videos with the 928 corresponding sentences in total.  On A2D-Sentences and JHMDB-Sentences, the model is evaluated with the criteria of Precision@K, Ovrall IoU, Mean IoU and mAP over 0.50:0.05:0.95. The Precision@K measures the percentage of test samples whole IoU scores are higher than the threshold K. Following standard protocol, the thresholds are set as 0.5:0.1:0.9.\n\n\nImplementation Details.\n\nModel Settings. We test our models under different visual backbones including: ResNet [12], Swin Transformer [26] and Video Swin Transformer [27]. The text encoder is selected as RoBERTa [25] and its parameters are frozen during the entire training stage. Following [55], we use the last three stage features from the visual backbone as the input to Transformer, their corresponding spatial strides are {8, 16, 32}. In the Transformer model, we adopt 4 encoder layers and 4 decoder layers and the hidden dimension is C = 256. The number of conditional query is set as 5.\n\nTraining Details. During training, we use sliding-windows to obtain the clips from a video and each clip consist of 5 randomly sampled frames. Following [43], the data aug-mentation includes random horizontal flip, random resize, random crop and photometric distortion. All frames are downsampled so that the short side has the size of 360 and the maximum size for the long side is 640 to fit GPU memory. The coefficients for losses are set as \u03bb cls = 2, \u03bb L1 = 5, \u03bb giou = 2, \u03bb dice = 5, \u03bb focal = 2.\n\nMost of our experiments follow the pretrain-thenfinetune process. And some models are trained from scratch for fair comparison. Additionally, on Ref-Youtube-VOS, we also reports the results by training the mixed data from Ref-Youtube-VOS and Ref-COCO [52]. The joint training technique has proven the effectiveness in many VIS tasks [1,21,44]. Please see more in the supplementary materials.\n\nInference Details. During inference, the video frames are downscaled to 360p. The model receives the whole video frames and directly outputs the predicted segmentation masks without post-process.\n\n\nMain Results\n\n\nRef-Youtube-VOS & Ref-DAVIS17\n\nWe compare our method with other state-of-the-art methods in Table 1 [36], and even beats PMINet [9] using the ensemble models and adopting post-process (55.6 vs 54.2). Using the strong Swin-Large [26] backbone, ReferFormer reaches the surprising 64.2 J &F, which obviously exceeds the ensemble results of the complicated, multi-stage method CITD [20] and creates a fairly high new record. Additionally, we also test the Video Swin Transformer [27] as the backbones. It is well known that the spatiotemporal visual encoder has strong ability to capture both the spatial characteristics and the temporal cues. For a fair comparison with MTTR [3], we train our model with the Video-Swin-Tiny backbone from scratch. It can be seen that our method outperforms MTTR under all the metrics with the smaller window size (5 vs 12). Comparing the results of ReferFormer under Video-Swin-Tiny backbone, it proves that the model benefits from the pretraining stage and joint training process to address the overfitting issue.\n\nOn Ref-DAVIS17, our method also achieves the best results under the same ResNet-50 setting (58.5 J &F). And the performance consistently improves by using stronger backbones, which proves the generality of our method. A2D-Sentences & JHMDB-Sentences We further evaluate our method on the A2D-Sentences dataset and compare the performance with other state-of-the-art methods in Table 2. It is obvious that our method achieves the impressive improvement over the previous methods. Compared with the recent MTTR [3], our method exhibits the clear performance advantage (+2.5 mAP) with smaller window size (6 vs. 10 with Video-Swin-Base visual backbone achieves 55.0 mAP which shows a significant gain of 8.9 mAP over previous best result. And ReferFormer also demonstrates its strong ability to produce high-quality masks via the stringent metrics (e.g., 57.9 for P@0.8 and 21.2 for P@0.9). We also evaluate the models on JHMDB-Sentences without finetuning to further prove the generality of our method, and the results table is placed in the supplementary materials. ReferFormer significantly outperforms all the existing methods and achieves superior 43.7 mAP using Video-Swin-Base backbone.\n\n\nAblation Study\n\nIn this section, we perform extensive ablation studies on Ref-Youtube-VOS to study the effect of core components in our model. All models are based on Video-Swin-Tiny visual backbone and we train the models from scratch otherwise specified. The detailed analysis is as follows.\n\nComponent Analysis. We build a simple Transformer bottom-up baseline. Specifically, considering a video clip of T frames, we flatten the temporal and spatial dimension into one dimension and then concatenate the visual features with the textual features along length dimension to form the multi-modal feature map f m \u2208 R (T \u00d7H\u00d7W +L)\u00d7C . The vanilla Transformer encoder builds the global dependencies between the visual and textual features. And a standard  FPN plays the role of decoder to generate segmenatation masks from encoded visual features. The baseline method operates the fixed-length video of 5 frames during the training and inference phases. We report the performance of the baseline method and also study core components in Table 3. First, from the first row of Table 3, the baseline method only achieves 47.2 J and 50.1 F. This inferior behavior attributes to two reasons: (1) The baseline method can not distinguish the similar objects that are close together and tends to segment the most salient region. In contrast, our method performs well with only 1 conditional query (see Table 4(a)), proving that dynamic convolution is essential for segmenting the referred object. (2) Our method uses a set of shared queries to track instances in all frames, and the best query is determined by the voting scores of each frame. In this sense, our model can produce a reliable reasoning result and keep the temporal consistency in the entire video. On the contrary, the baseline method could be regarded as a image-level method that independently predicts the results of each frame even though the model is able to aggregate the information from other frames.\n\nSecond, comparing the second and last row of Table  3, we can see that the standard FPN has already achieved strong performance and the vision-language fusion process further helps to provide more accurate segmentation. This is because the object mask would be inaccurate due to light variation, whereas the cross-modal fusion uses the text as a complementary to strengthen the object pixel features. Another technique is using the relative coordinate features, this would help the model better determine the location of referred object and lead to performance improvement, as shown in the third row in Table 3.\n\nNumber of Conditional Queries. Benefit from the design of conditional queries, all the initial object queries tend to find the referred object only. In this situation, we can only use a relatively small number of queries. In Table  4(a), it can be seen that the model achieves considerable results under all these settings, even with N = 1. Certainly, more queries enable the model make judgement from a wide range of instance candidates, which could better handle the complicated scenes where the similar objects are clustered together. The performance saturates at N = 5 and begins to slightly decrease when the query number gets larger. We conjecture that it is caused by the imbalance of label assignment as there is only one positive sample in each frame.\n\nNumber of Training Clip Frames. We study the effect of training clip frame number in Table 4(b). Note that under T = 1, the model can be viewed as an image-level method and the performance of metric J &F is only 50.0. When the frame number increases to 3, the model enjoys an significant J &F gain of 4.8. This is because using more frames to form a clip helps the model better aggregate the temporal action-related information. We choose T = 5 by default.\n\nLabel Assignment Method. Our framework is able to predict the reference probability, box location and segmentation mask of the referred object. We find the optimal positive sample by minimizing the overall matching cost in Eq.4. There are some variants in the label assignment method and the comparison experiments are shown in Table  4(c). From the first two rows we show that the lack of box or mask cost would both lead to the performance drop. With the segmentation-centric design, the mask cost is the most direct guidance for optimization, and the box provides the location prior for dynamic kernel. Thus, the combination of classification, box and mask cost shows more robustness.\n\n\nConclusion\n\nIn this work, we propose ReferFormer, an extremely simple and unified framework for referring video object segmentation. This framework provides a new perspective for the R-VOS task which views the language as queries. These queries are restricted to attend to the referred object only, and the object tracking is easily achieved by linking the corresponding queries. Given the video and an expression, our framework directly produces the segmentation masks as well as the detected boxes of the referred object in all frames without post-process. We validate our model on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences and it shows the state-of-the-art performance on the four benchmarks.\n\nFigure 1 .\n1Comparison of current referring video object segmentation (R-VOS) pipelines. (a) Bottom-up. (b) Top-down. (c) Ours.\n\nEvaluation\nMetrics. We use the standard evaluation metrics for Ref-Youtube-VOS and Ref-DAVIS17: region similarity (J ), contour accuracy (F) and their average value (J &F). For Ref-Youtube-VOS, as the annotations of validation set are not released publicly, we evaluate our method on the official challenge server 1 . Ref-DAVIS17 is evaluated by the official evaluation code 2 .\n\n\nTable 1. Comparison with the state-of-the-art methods on Ref-Youtube-VOS and Ref-DAVIS17. * means joint trainig with Ref-COCO dataset. \u2020 indicates the spatio-temporal visual backbone is trained from scratch.Method \n\nBackbone \nRef-Youtube-VOS \nRef-DAVIS17 \nJ &F \nJ \nF \nJ &F \nJ \nF \nSpatial Visual Backbones \nCMSA [50] \nResNet-50 \n34.9 \n33.3 36.5 \n34.7 \n32.2 37.2 \nCMSA + RNN [50] \nResNet-50 \n36.4 \n34.8 38.1 \n40.2 \n36.9 43.5 \nURVOS [36] \nResNet-50 \n47.2 \n45.3 49.2 \n51.5 \n47.3 56.0 \nReferFormer \nResNet-50 \n55.6 \n54.8 56.5 \n58.5 \n55.8 61.3 \nReferFormer  *  \nResNet-50 \n58.7 \n57.4 60.1 \n-\n-\n-\nPMINet [9] \nResNeSt-101 \n48.2 \n46.7 49.6 \n-\n-\n-\nPMINet + CFBI [9] \nResNeSt-101 \n53.0 \n51.5 54.5 \n-\n-\n-\nCITD  *  [20] \nResNet-101 \n56.4 \n54.8 58.1 \n-\n-\n-\nReferFormer \nResNet-101 \n57.3 \n56.1 58.4 \n-\n-\n-\nReferFormer  *  \nResNet-101 \n59.3 \n58.1 60.4 \n-\n-\n-\nPMINet + CFBI [9] \nEnsemble \n54.2 \n53.0 55.5 \n-\n-\n-\nCITD [20] \nEnsemble \n61.4 \n60.0 62.7 \n-\n-\n-\nReferFormer \nSwin-L \n62.4 \n60.8 64.0 \n60.5 \n57.6 63.4 \nReferFormer  *  \nSwin-L \n64.2 \n62.3 66.2 \n-\n-\n-\nSpatio-temporal Visual Backbones \nMTTR  \u2020 (\u03c9 = 12) [3] \nVideo-Swin-T \n55.3 \n54.0 56.6 \n-\n-\n-\nReferFormer  \u2020 (\u03c9 = 5) \n56.0 \n54.8 57.3 \n-\n-\n-\nReferFormer \nVideo-Swin-T \n59.4 \n58.0 60.9 \n-\n-\n-\nReferFormer  *  \n62.6 \n59.9 63.3 \n-\n-\n-\nReferFormer \nVideo-Swin-S \n60.1 \n58.6 61.6 \n-\n-\n-\nReferFormer  *  \n63.3 \n61.4 65.2 \n-\n-\n-\nReferFormer \nVideo-Swin-B \n62.9 \n61.3 64.6 \n61.1 \n58.1 64.1 \nReferFormer  *  \n64.9 \n62.8 67.0 \n-\n-\n-\n\n\n\n\n.Method \n\nBackbone \nPrecision \nIoU \nmAP \nP@0.5 P@0.6 P@0.7 P@0.8 P@0.9 Overall Mean \nHu et al. [13] \nVGG-16 \n34.8 \n23.6 \n13.3 \n3.3 \n0.1 \n47.4 \n35.0 \n13.2 \nGavrilyuk et al. [11] \nI3D \n47.5 \n34.7 \n21.1 \n8.0 \n0.2 \n53.6 \n42.1 \n19.8 \nCMSA + CFSA [51] \nResNet-101 \n48.7 \n43.1 \n35.8 \n23.1 \n5.2 \n61.8 \n43.2 \n-\nACAN [42] \nI3D \n55.7 \n45.9 \n31.9 \n16.0 \n2.0 \n60.1 \n49.0 \n27.4 \nCMPC-V [24] \nI3D \n65.5 \n59.2 \n50.6 \n34.2 \n9.8 \n65.3 \n57.3 \n40.4 \nClawCraneNet [19] \nResNet-50/101 \n70.4 \n67.7 \n61.7 \n48.9 \n17.1 \n63.1 \n59.9 \n-\nMTTR (\u03c9 = 8) [3] \nVideo-Swin-T \n72.1 \n68.4 \n60.7 \n45.6 \n16.4 \n70.2 \n61.8 \n44.7 \nMTTR (\u03c9 = 10) [3] \nVideo-Swin-T \n75.4 \n71.2 \n63.8 \n48.5 \n16.9 \n72.0 \n64.0 \n46.1 \nReferFormer  \u2020 (\u03c9 = 6) Video-Swin-T \n76.0 \n72.2 \n65.4 \n49.8 \n17.9 \n72.3 \n64.1 \n48.6 \nReferFormer (\u03c9 = 5) \nVideo-Swin-T \n82.8 \n79.2 \n72.3 \n55.3 \n19.3 \n77.6 \n69.6 \n52.8 \nReferFormer (\u03c9 = 5) \nVideo-Swin-S \n82.6 \n79.4 \n73.1 \n57.4 \n21.1 \n77.7 \n69.8 \n53.9 \nReferFormer (\u03c9 = 5) Video-Swin-B \n83.1 \n80.4 \n74.1 \n57.9 \n21.2 \n78.6 \n70.3 \n55.0 \n\nTable 2. Comparison with the state-of-the-art methods on A2D-Sentences.  \u2020 means our model is trained from scratch. \n\nCITD [20] and PMINet [9] are the top-2 solutions in 2021 \nRef-Youtube-VOS Challenge. It can be observed \nthat ReferFormer outperforms previous methods on the two \ndatasets under all metrics and with a large marge. On \nRef-Youtube-VOS, ReferFormer with a ResNet-50 back-\nbone achieves the overall J &F of 55.6, which is 8.4 points \nhigher than the previous state-of-the-art work URVOS \n\n\nTable 3. Ablation study on the components of ReferFormer. The visual backbone is Video-Swin-Tiny.). Incorporating the pretraining stage, ReferFormer \n\nComponents \nJ \nF \nBaseline \n47.2 (\u22127.6) 50.1 (\u22127.2) \nw/o Visual-language Fusion 53.0 (\u22121.8) 56.2 (\u22121.1) \nw/o Relative Coordinates \n53.7 (\u22121.1) 55.9 (\u22121.4) \nFull Model \n54.8 \n57.3 \n\n\n\n\nQueries J &F JF \n\n1 \n53.6 52.7 54.5 \n3 \n54.2 53.2 55.2 \n5 \n56.0 54.8 57.3 \n8 \n55.3 54.1 56.6 \n\nFrames J &F J \nF \n\n1 \n50.0 48.4 51.6 \n3 \n54.8 53.6 56.0 \n5 \n56.0 54.8 57.3 \n\nClass Box Mask J &F J \nF \n\n55.2 54.0 56.4 \n54.5 53.5 55.5 \n56.0 54.8 57.3 \n\n(a) The effect of query number. \n(b) The effect of frame number. \n(c) The effect of label assignment \nmethod. \n\n\n\nTable 4 .\n4Ablation study on different settings of ReferFormer. All the models are using Video-Swin-Tiny as visual backbone.\nhttps://competitions.codalab.org/competitions/29139 2 https://github.com/davisvideochallenge/davis2017-evaluation\nAcknowledgements\nStem-seg: Spatio-temporal embeddings for instance segmentation in videos. Ali Athar, Sabarinath Mahadevan, Aljosa Osep, Laura Leal-Taix\u00e9, Bastian Leibe, European Conference on Computer Vision. SpringerAli Athar, Sabarinath Mahadevan, Aljosa Osep, Laura Leal- Taix\u00e9, and Bastian Leibe. Stem-seg: Spatio-temporal em- beddings for instance segmentation in videos. In European Conference on Computer Vision, pages 158-177. Springer, 2020. 6\n\nRefvos: A closer look at referring expressions for video object segmentation. Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, Xavier Giro-I Nieto, arXiv:2010.00263arXiv preprintMiriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, and Xavier Giro-i Nieto. Refvos: A closer look at referring expressions for video object segmen- tation. arXiv preprint arXiv:2010.00263, 2020. 2\n\nEnd-to-end referring video object segmentation with multimodal transformers. Adam Botach, Evgenii Zheltonozhskii, Chaim Baskin, arXiv:2111.1482167arXiv preprintAdam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation with multi- modal transformers. arXiv preprint arXiv:2111.14821, 2021. 6, 7\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. Springer23Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision, pages 213-229. Springer, 2020. 2, 3\n\nTransformer tracking. Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, Huchuan Lu, CVPR. Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In CVPR, 2021. 3\n\nPerpixel classification is not all you need for semantic segmentation. Bowen Cheng, Alex Schwing, Alexander Kirillov, Advances in Neural Information Processing Systems. 34Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per- pixel classification is not all you need for semantic segmen- tation. Advances in Neural Information Processing Systems, 34, 2021. 3\n\nRethinking space-time networks with improved memory coverage for efficient video object segmentation. Yu-Wing Ho Kei Cheng, Chi-Keung Tai, Tang, arXiv:2106.05210arXiv preprintHo Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Re- thinking space-time networks with improved memory cov- erage for efficient video object segmentation. arXiv preprint arXiv:2106.05210, 2021. 2\n\nVision-language transformer and query generation for referring segmentation. Henghui Ding, Chang Liu, Suchen Wang, Xudong Jiang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionHenghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for refer- ring segmentation. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 16321-16330, 2021. 2\n\nProgressive multimodal interaction network for referring video object segmentation. Zihan Ding, Tianrui Hui, Shaofei Huang, Si Liu, Xuan Luo, Junshi Huang, Xiaoming Wei, 67Zihan Ding, Tianrui Hui, Shaofei Huang, Si Liu, Xuan Luo, Junshi Huang, and Xiaoming Wei. Progressive multimodal interaction network for referring video object segmentation. 2021. 6, 7\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.11929Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3\n\nActor and action video segmentation from a sentence. Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, G M Cees, Snoek, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation from a sentence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5958-5966, 2018. 7\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 6\n\nSegmentation from natural language expressions. Ronghang Hu, Marcus Rohrbach, Trevor Darrell, European Conference on Computer Vision. SpringerRonghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg- mentation from natural language expressions. In European Conference on Computer Vision, pages 108-124. Springer, 2016. 7\n\nReferring image segmentation via cross-modal progressive comprehension. Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring im- age segmentation via cross-modal progressive comprehen- sion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 10488-10497, 2020. 2\n\nVideo instance segmentation using inter-frame communication transformers. Sukjun Hwang, Miran Heo, Seon Joo Seoung Wug Oh, Kim, arXiv:2106.0329923arXiv preprintSukjun Hwang, Miran Heo, Seoung Wug Oh, and Seon Joo Kim. Video instance segmentation using inter-frame com- munication transformers. arXiv preprint arXiv:2106.03299, 2021. 2, 3\n\nTowards understanding action recognition. Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, Michael J Black, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael J Black. Towards understanding ac- tion recognition. In Proceedings of the IEEE international conference on computer vision, pages 3192-3199, 2013. 5\n\nIshan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding. Aishwarya Kamath, Mannat Singh, Yann Lecun, Gabriel Synnaeve, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr- modulated detection for end-to-end multi-modal understand- ing. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, pages 1780-1790, 2021. 3\n\nVideo object segmentation with language referring expressions. Anna Khoreva, Anna Rohrbach, Bernt Schiele, Asian Conference on Computer Vision. SpringerAnna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Asian Conference on Computer Vision, pages 123-141. Springer, 2018. 5\n\nClawcranenet: Leveraging object-level relation for text-based video segmentation. Chen Liang, Yu Wu, Yawei Luo, Yi Yang, arXiv:2103.10702arXiv preprintChen Liang, Yu Wu, Yawei Luo, and Yi Yang. Clawcranenet: Leveraging object-level relation for text-based video seg- mentation. arXiv preprint arXiv:2103.10702, 2021. 7\n\nRethinking cross-modal interaction from a top-down perspective for referring video object segmentation. Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao Wei, Yi Yang, arXiv:2106.0106167arXiv preprintChen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao Wei, and Yi Yang. Rethinking cross-modal interaction from a top-down perspective for referring video object segmentation. arXiv preprint arXiv:2106.01061, 2021. 2, 6, 7\n\nVideo instance segmentation with a propose-reduce paradigm. Huaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, Jiaya Jia, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionHuaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, and Ji- aya Jia. Video instance segmentation with a propose-reduce paradigm. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1739-1748, 2021. 6\n\nFeature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117-2125, 2017. 4\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 5\n\nCross-modal progressive comprehension for referring segmentation. Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, Guanbin Li, IEEE Transactions on Pattern Analysis and Machine Intelligence. 20217Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, and Guanbin Li. Cross-modal progressive comprehension for referring segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 7\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 36arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle- moyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 3, 6\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.1403067arXiv preprintZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows. arXiv preprint arXiv:2103.14030, 2021. 3, 6, 7\n\n. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu, arXiv:2106.1323067Video swin transformer. arXiv preprintZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021. 6, 7\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition, pages 3431-3440, 2015. 1\n\nMulti-task collaborative network for joint referring expression comprehension and segmentation. Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, Rongrong Ji, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionGen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collabora- tive network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 10034-10043, 2020. 2\n\nTrackFormer: Multi-object tracking with transformers. Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, Christoph Feichtenhofer, arXiv:2101.02702arXiv preprintTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. TrackFormer: Multi-object track- ing with transformers. arXiv preprint arXiv:2101.02702, 2021. 3\n\nV-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, 2016 fourth international conference on 3D vision (3DV). IEEEFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565-571. IEEE, 2016. 5\n\nVideo object segmentation using space-time memory networks. Joon-Young Seoung Wug Oh, Ning Lee, Seon Joo Xu, Kim, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSeoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9226-9235, 2019. 2\n\nA benchmark dataset and evaluation methodology for video object segmentation. Federico Perazzi, Jordi Pont-Tuset, Brian Mcwilliams, Luc Van Gool, Markus Gross, Alexander Sorkine-Hornung, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionFederico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724-732, 2016. 1\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, Luc Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. arXiv preprintJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 5\n\nGeneralized intersection over union: A metric and a loss for bounding box regression. Hamid Rezatofighi, Nathan Tsoi, Junyoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized in- tersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 658-666, 2019. 5\n\nUrvos: Unified referring video object segmentation network with a large-scale benchmark. Seonguk Seo, Joon-Young Lee, Bohyung Han, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer67Proceedings, Part XV 16Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with a large-scale benchmark. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16, pages 208-223. Springer, 2020. 2, 5, 6, 7\n\nPeize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, Ping Luo, arXiv:2012.15460Transtrack: Multiple-object tracking with transformer. arXiv preprintPeize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple-object tracking with trans- former. arXiv preprint arXiv:2012.15460, 2020. 3\n\nSparse r-cnn: End-to-end object detection with learnable proposals. Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPeize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen- feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end ob- ject detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14454-14463, 2021. 3\n\nConditional convolutions for instance segmentation. Zhi Tian, Chunhua Shen, Hao Chen, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UK, AuSpringerProceedings, Part I 16Zhi Tian, Chunhua Shen, and Hao Chen. Conditional con- volutions for instance segmentation. In Computer Vision- ECCV 2020: 16th European Conference, Glasgow, UK, Au- gust 23-28, 2020, Proceedings, Part I 16, pages 282-298. Springer, 2020. 4\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 25Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 2, 3, 5\n\nFeelvos: Fast end-to-end embedding learning for video object segmentation. Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPaul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmenta- tion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 9481-9490, 2019. 2\n\nAsymmetric cross-guided attention network for actor and action video segmentation from natural language query. Hao Wang, Cheng Deng, Junchi Yan, Dacheng Tao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionHao Wang, Cheng Deng, Junchi Yan, and Dacheng Tao. Asymmetric cross-guided attention network for actor and ac- tion video segmentation from natural language query. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3939-3948, 2019. 7\n\nEnd-to-end video instance segmentation with transformers. Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition56Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8741-8750, 2021. 2, 3, 4, 5, 6\n\nSeqformer: a frustratingly simple model for video instance segmentation. Junfeng Wu, Yi Jiang, Wenqing Zhang, Xiang Bai, Song Bai, arXiv:2112.0827526arXiv preprintJunfeng Wu, Yi Jiang, Wenqing Zhang, Xiang Bai, and Song Bai. Seqformer: a frustratingly simple model for video instance segmentation. arXiv preprint arXiv:2112.08275, 2021. 2, 3, 6\n\nCan humans fly? action understanding with multiple classes of actors. Chenliang Xu, Shao-Hang Hsieh, Caiming Xiong, Jason J Corso, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChenliang Xu, Shao-Hang Hsieh, Caiming Xiong, and Ja- son J Corso. Can humans fly? action understanding with multiple classes of actors. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 2264-2273, 2015. 5\n\nYoutube-vos: A large-scale video object segmentation benchmark. Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, Thomas Huang, arXiv:1809.03327arXiv preprintNing Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. 1\n\nLearning spatio-temporal transformer for visual tracking. Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, Huchuan Lu, ICCV. Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for vi- sual tracking. In ICCV, 2021. 3\n\nVideo instance segmentation. Linjie Yang, Yuchen Fan, Ning Xu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLinjie Yang, Yuchen Fan, and Ning Xu. Video instance seg- mentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5188-5197, 2019. 3\n\nCollaborative video object segmentation by foreground-background integration. Zongxin Yang, Yunchao Wei, Yi Yang, European Conference on Computer Vision. SpringerZongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by foreground-background inte- gration. In European Conference on Computer Vision, pages 332-348. Springer, 2020. 2\n\nCross-modal self-attention network for referring image segmentation. Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLinwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image seg- mentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10502- 10511, 2019. 6\n\nReferring segmentation in images and videos with cross-modal self-attention network. Linwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, Yang Wang, arXiv:2102.04762arXiv preprintLinwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, and Yang Wang. Referring segmentation in images and videos with cross-modal self-attention network. arXiv preprint arXiv:2102.04762, 2021. 7\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, European Conference on Computer Vision. SpringerLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres- sions. In European Conference on Computer Vision, pages 69-85. Springer, 2016. 6\n\nDiscriminative bimodal networks for visual localization and detection with natural language queries. Yuting Zhang, Luyao Yuan, Yijie Guo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhiyuan He, I-An Huang, and Honglak LeeYuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, and Honglak Lee. Discriminative bimodal networks for visual localization and detection with natural language queries. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 557-566, 2017. 2\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmen- tation from a sequence-to-sequence perspective with trans- formers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6881-6890, 2021. 3\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, arXiv:2010.04159arXiv preprintXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable trans- formers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. 2, 3, 4, 6\n", "annotations": {"author": "[{\"end\":115,\"start\":63},{\"end\":125,\"start\":116},{\"end\":177,\"start\":126},{\"end\":190,\"start\":178},{\"end\":301,\"start\":191}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":71},{\"end\":124,\"start\":119},{\"end\":135,\"start\":132},{\"end\":189,\"start\":185},{\"end\":199,\"start\":196}]", "author_first_name": "[{\"end\":70,\"start\":63},{\"end\":118,\"start\":116},{\"end\":131,\"start\":126},{\"end\":184,\"start\":178},{\"end\":195,\"start\":191}]", "author_affiliation": "[{\"end\":114,\"start\":75},{\"end\":176,\"start\":137},{\"end\":240,\"start\":201},{\"end\":300,\"start\":242}]", "title": "[{\"end\":60,\"start\":1},{\"end\":361,\"start\":302}]", "venue": null, "abstract": "[{\"end\":2071,\"start\":391}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2504,\"start\":2500},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2507,\"start\":2504},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3041,\"start\":3037},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3107,\"start\":3104},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4419,\"start\":4415},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4443,\"start\":4440},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":4446,\"start\":4443},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4483,\"start\":4479},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4486,\"start\":4483},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4489,\"start\":4486},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4608,\"start\":4605},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":4611,\"start\":4608},{\"end\":6700,\"start\":6699},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6908,\"start\":6905},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6911,\"start\":6908},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6914,\"start\":6911},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6917,\"start\":6914},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7399,\"start\":7396},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7402,\"start\":7399},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7405,\"start\":7402},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7408,\"start\":7405},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7460,\"start\":7457},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7697,\"start\":7693},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8019,\"start\":8015},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9262,\"start\":9258},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9431,\"start\":9427},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9434,\"start\":9431},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9437,\"start\":9434},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9466,\"start\":9463},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9469,\"start\":9466},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9486,\"start\":9483},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9489,\"start\":9486},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9492,\"start\":9489},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9513,\"start\":9510},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9516,\"start\":9513},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9519,\"start\":9516},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9529,\"start\":9526},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9574,\"start\":9570},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9577,\"start\":9574},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9726,\"start\":9722},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9791,\"start\":9787},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9892,\"start\":9888},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11520,\"start\":11516},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12149,\"start\":12145},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13125,\"start\":13121},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14665,\"start\":14661},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16896,\"start\":16892},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17824,\"start\":17820},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18071,\"start\":18068},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18073,\"start\":18071},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18075,\"start\":18073},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18077,\"start\":18075},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18955,\"start\":18951},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20078,\"start\":20074},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20205,\"start\":20201},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20269,\"start\":20265},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21474,\"start\":21470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21576,\"start\":21572},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21603,\"start\":21599},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21820,\"start\":21816},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21835,\"start\":21831},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22478,\"start\":22474},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22501,\"start\":22497},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22533,\"start\":22529},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22579,\"start\":22575},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22658,\"start\":22654},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23117,\"start\":23113},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23718,\"start\":23714},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23799,\"start\":23796},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23802,\"start\":23799},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23805,\"start\":23802},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24173,\"start\":24169},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24200,\"start\":24197},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24301,\"start\":24297},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24451,\"start\":24447},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24548,\"start\":24544},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24744,\"start\":24741},{\"end\":24921,\"start\":24912},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25627,\"start\":25624},{\"end\":25726,\"start\":25717}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31641,\"start\":31513},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32021,\"start\":31642},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33488,\"start\":32022},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34996,\"start\":33489},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35331,\"start\":34997},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35694,\"start\":35332},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35820,\"start\":35695}]", "paragraph": "[{\"end\":2817,\"start\":2087},{\"end\":3508,\"start\":2819},{\"end\":4251,\"start\":3510},{\"end\":4949,\"start\":4253},{\"end\":5571,\"start\":4951},{\"end\":5951,\"start\":5573},{\"end\":6004,\"start\":5953},{\"end\":6701,\"start\":6006},{\"end\":7021,\"start\":6718},{\"end\":7294,\"start\":7023},{\"end\":7963,\"start\":7296},{\"end\":8958,\"start\":7965},{\"end\":10361,\"start\":8960},{\"end\":11101,\"start\":10374},{\"end\":11288,\"start\":11114},{\"end\":11392,\"start\":11317},{\"end\":11549,\"start\":11394},{\"end\":11666,\"start\":11571},{\"end\":12236,\"start\":11690},{\"end\":12583,\"start\":12238},{\"end\":13015,\"start\":12600},{\"end\":14062,\"start\":13017},{\"end\":14579,\"start\":14064},{\"end\":15218,\"start\":14598},{\"end\":16020,\"start\":15220},{\"end\":16221,\"start\":16046},{\"end\":16822,\"start\":16243},{\"end\":17368,\"start\":16862},{\"end\":18250,\"start\":17370},{\"end\":18516,\"start\":18392},{\"end\":19071,\"start\":18619},{\"end\":19456,\"start\":19107},{\"end\":19820,\"start\":19458},{\"end\":19924,\"start\":19862},{\"end\":20522,\"start\":19953},{\"end\":21037,\"start\":20536},{\"end\":21337,\"start\":21071},{\"end\":22360,\"start\":21376},{\"end\":22958,\"start\":22388},{\"end\":23461,\"start\":22960},{\"end\":23854,\"start\":23463},{\"end\":24051,\"start\":23856},{\"end\":25113,\"start\":24100},{\"end\":26305,\"start\":25115},{\"end\":26601,\"start\":26324},{\"end\":28270,\"start\":26603},{\"end\":28883,\"start\":28272},{\"end\":29645,\"start\":28885},{\"end\":30103,\"start\":29647},{\"end\":30792,\"start\":30105},{\"end\":31512,\"start\":30807}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11316,\"start\":11289},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11570,\"start\":11550},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12599,\"start\":12584},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14597,\"start\":14580},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16045,\"start\":16021},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16242,\"start\":16222},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18391,\"start\":18251},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18580,\"start\":18517},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19106,\"start\":19072},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19861,\"start\":19821},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19952,\"start\":19925},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21070,\"start\":21038}]", "table_ref": "[{\"end\":24168,\"start\":24161},{\"end\":27348,\"start\":27341},{\"end\":27386,\"start\":27379},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27705,\"start\":27698},{\"end\":28325,\"start\":28317},{\"end\":28882,\"start\":28875},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29118,\"start\":29110},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29739,\"start\":29732},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30441,\"start\":30433}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2085,\"start\":2073},{\"attributes\":{\"n\":\"2.\"},\"end\":6716,\"start\":6704},{\"attributes\":{\"n\":\"3.\"},\"end\":10372,\"start\":10364},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11112,\"start\":11104},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11688,\"start\":11669},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16860,\"start\":16825},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18617,\"start\":18582},{\"attributes\":{\"n\":\"3.5.\"},\"end\":20534,\"start\":20525},{\"attributes\":{\"n\":\"4.\"},\"end\":21351,\"start\":21340},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21374,\"start\":21354},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22386,\"start\":22363},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24066,\"start\":24054},{\"end\":24098,\"start\":24069},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26322,\"start\":26308},{\"attributes\":{\"n\":\"5.\"},\"end\":30805,\"start\":30795},{\"end\":31524,\"start\":31514},{\"end\":31653,\"start\":31643},{\"end\":35705,\"start\":35696}]", "table": "[{\"end\":33488,\"start\":32231},{\"end\":34996,\"start\":33492},{\"end\":35331,\"start\":35096},{\"end\":35694,\"start\":35348}]", "figure_caption": "[{\"end\":31641,\"start\":31526},{\"end\":32021,\"start\":31654},{\"end\":32231,\"start\":32024},{\"end\":33492,\"start\":33491},{\"end\":35096,\"start\":34999},{\"end\":35348,\"start\":35334},{\"end\":35820,\"start\":35707}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3102,\"start\":3091},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3244,\"start\":3236},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5050,\"start\":5042},{\"end\":8311,\"start\":8303},{\"end\":10704,\"start\":10696},{\"end\":13923,\"start\":13915},{\"end\":14943,\"start\":14935},{\"end\":16621,\"start\":16613}]", "bib_author_first_name": "[{\"end\":36029,\"start\":36026},{\"end\":36047,\"start\":36037},{\"end\":36065,\"start\":36059},{\"end\":36077,\"start\":36072},{\"end\":36097,\"start\":36090},{\"end\":36474,\"start\":36468},{\"end\":36490,\"start\":36484},{\"end\":36506,\"start\":36500},{\"end\":36524,\"start\":36517},{\"end\":36539,\"start\":36534},{\"end\":36561,\"start\":36548},{\"end\":36907,\"start\":36903},{\"end\":36923,\"start\":36916},{\"end\":36945,\"start\":36940},{\"end\":37218,\"start\":37211},{\"end\":37236,\"start\":37227},{\"end\":37251,\"start\":37244},{\"end\":37269,\"start\":37262},{\"end\":37288,\"start\":37279},{\"end\":37305,\"start\":37299},{\"end\":37632,\"start\":37629},{\"end\":37642,\"start\":37639},{\"end\":37654,\"start\":37648},{\"end\":37664,\"start\":37660},{\"end\":37678,\"start\":37671},{\"end\":37692,\"start\":37685},{\"end\":37891,\"start\":37886},{\"end\":37903,\"start\":37899},{\"end\":37922,\"start\":37913},{\"end\":38285,\"start\":38278},{\"end\":38309,\"start\":38300},{\"end\":38630,\"start\":38623},{\"end\":38642,\"start\":38637},{\"end\":38654,\"start\":38648},{\"end\":38667,\"start\":38661},{\"end\":39134,\"start\":39129},{\"end\":39148,\"start\":39141},{\"end\":39161,\"start\":39154},{\"end\":39171,\"start\":39169},{\"end\":39181,\"start\":39177},{\"end\":39193,\"start\":39187},{\"end\":39209,\"start\":39201},{\"end\":39409,\"start\":39403},{\"end\":39428,\"start\":39423},{\"end\":39445,\"start\":39436},{\"end\":39462,\"start\":39458},{\"end\":39483,\"start\":39476},{\"end\":39496,\"start\":39490},{\"end\":39517,\"start\":39510},{\"end\":39536,\"start\":39528},{\"end\":39552,\"start\":39547},{\"end\":40052,\"start\":40046},{\"end\":40068,\"start\":40064},{\"end\":40087,\"start\":40079},{\"end\":40093,\"start\":40092},{\"end\":40095,\"start\":40094},{\"end\":40529,\"start\":40522},{\"end\":40541,\"start\":40534},{\"end\":40557,\"start\":40549},{\"end\":40567,\"start\":40563},{\"end\":40979,\"start\":40971},{\"end\":40990,\"start\":40984},{\"end\":41007,\"start\":41001},{\"end\":41321,\"start\":41314},{\"end\":41336,\"start\":41329},{\"end\":41344,\"start\":41342},{\"end\":41357,\"start\":41350},{\"end\":41369,\"start\":41362},{\"end\":41382,\"start\":41375},{\"end\":41393,\"start\":41388},{\"end\":41401,\"start\":41399},{\"end\":41923,\"start\":41917},{\"end\":41936,\"start\":41931},{\"end\":41950,\"start\":41942},{\"end\":42231,\"start\":42224},{\"end\":42247,\"start\":42240},{\"end\":42260,\"start\":42254},{\"end\":42276,\"start\":42268},{\"end\":42294,\"start\":42285},{\"end\":42756,\"start\":42747},{\"end\":42771,\"start\":42765},{\"end\":42783,\"start\":42779},{\"end\":42798,\"start\":42791},{\"end\":43275,\"start\":43271},{\"end\":43289,\"start\":43285},{\"end\":43305,\"start\":43300},{\"end\":43631,\"start\":43627},{\"end\":43641,\"start\":43639},{\"end\":43651,\"start\":43646},{\"end\":43659,\"start\":43657},{\"end\":43973,\"start\":43969},{\"end\":43983,\"start\":43981},{\"end\":43995,\"start\":43988},{\"end\":44009,\"start\":44002},{\"end\":44023,\"start\":44016},{\"end\":44037,\"start\":44030},{\"end\":44045,\"start\":44043},{\"end\":44390,\"start\":44383},{\"end\":44404,\"start\":44396},{\"end\":44412,\"start\":44409},{\"end\":44425,\"start\":44418},{\"end\":44435,\"start\":44430},{\"end\":44851,\"start\":44843},{\"end\":44862,\"start\":44857},{\"end\":44875,\"start\":44871},{\"end\":44893,\"start\":44886},{\"end\":44905,\"start\":44898},{\"end\":44922,\"start\":44917},{\"end\":45404,\"start\":45396},{\"end\":45415,\"start\":45410},{\"end\":45427,\"start\":45423},{\"end\":45838,\"start\":45836},{\"end\":45851,\"start\":45844},{\"end\":45864,\"start\":45857},{\"end\":45879,\"start\":45872},{\"end\":45887,\"start\":45885},{\"end\":45899,\"start\":45892},{\"end\":46190,\"start\":46184},{\"end\":46200,\"start\":46196},{\"end\":46211,\"start\":46206},{\"end\":46226,\"start\":46219},{\"end\":46237,\"start\":46231},{\"end\":46250,\"start\":46245},{\"end\":46261,\"start\":46257},{\"end\":46272,\"start\":46268},{\"end\":46284,\"start\":46280},{\"end\":46305,\"start\":46298},{\"end\":46720,\"start\":46718},{\"end\":46732,\"start\":46726},{\"end\":46741,\"start\":46738},{\"end\":46750,\"start\":46747},{\"end\":46761,\"start\":46755},{\"end\":46772,\"start\":46767},{\"end\":46787,\"start\":46780},{\"end\":46800,\"start\":46793},{\"end\":47059,\"start\":47057},{\"end\":47068,\"start\":47065},{\"end\":47078,\"start\":47075},{\"end\":47090,\"start\":47084},{\"end\":47101,\"start\":47096},{\"end\":47116,\"start\":47109},{\"end\":47125,\"start\":47122},{\"end\":47396,\"start\":47388},{\"end\":47407,\"start\":47403},{\"end\":47425,\"start\":47419},{\"end\":47894,\"start\":47891},{\"end\":47904,\"start\":47900},{\"end\":47920,\"start\":47911},{\"end\":47933,\"start\":47926},{\"end\":47947,\"start\":47939},{\"end\":47957,\"start\":47952},{\"end\":47972,\"start\":47964},{\"end\":48487,\"start\":48484},{\"end\":48508,\"start\":48499},{\"end\":48524,\"start\":48519},{\"end\":48546,\"start\":48537},{\"end\":48864,\"start\":48858},{\"end\":48882,\"start\":48876},{\"end\":48901,\"start\":48890},{\"end\":49273,\"start\":49263},{\"end\":49293,\"start\":49289},{\"end\":49307,\"start\":49299},{\"end\":49752,\"start\":49744},{\"end\":49767,\"start\":49762},{\"end\":49785,\"start\":49780},{\"end\":49801,\"start\":49798},{\"end\":49818,\"start\":49812},{\"end\":49835,\"start\":49826},{\"end\":50296,\"start\":50291},{\"end\":50317,\"start\":50309},{\"end\":50332,\"start\":50327},{\"end\":50347,\"start\":50342},{\"end\":50362,\"start\":50358},{\"end\":50383,\"start\":50380},{\"end\":50776,\"start\":50771},{\"end\":50796,\"start\":50790},{\"end\":50811,\"start\":50803},{\"end\":50822,\"start\":50818},{\"end\":50837,\"start\":50834},{\"end\":50850,\"start\":50844},{\"end\":51398,\"start\":51391},{\"end\":51414,\"start\":51404},{\"end\":51427,\"start\":51420},{\"end\":51827,\"start\":51822},{\"end\":51835,\"start\":51833},{\"end\":51849,\"start\":51843},{\"end\":51861,\"start\":51857},{\"end\":51873,\"start\":51867},{\"end\":51886,\"start\":51879},{\"end\":51894,\"start\":51891},{\"end\":51907,\"start\":51901},{\"end\":51921,\"start\":51914},{\"end\":51932,\"start\":51928},{\"end\":52315,\"start\":52310},{\"end\":52327,\"start\":52321},{\"end\":52337,\"start\":52335},{\"end\":52348,\"start\":52345},{\"end\":52363,\"start\":52355},{\"end\":52371,\"start\":52368},{\"end\":52387,\"start\":52378},{\"end\":52401,\"start\":52398},{\"end\":52412,\"start\":52406},{\"end\":52426,\"start\":52419},{\"end\":52953,\"start\":52950},{\"end\":52967,\"start\":52960},{\"end\":52977,\"start\":52974},{\"end\":53357,\"start\":53351},{\"end\":53371,\"start\":53367},{\"end\":53385,\"start\":53381},{\"end\":53399,\"start\":53394},{\"end\":53416,\"start\":53411},{\"end\":53429,\"start\":53424},{\"end\":53431,\"start\":53430},{\"end\":53445,\"start\":53439},{\"end\":53459,\"start\":53454},{\"end\":53844,\"start\":53840},{\"end\":53865,\"start\":53859},{\"end\":53879,\"start\":53872},{\"end\":53896,\"start\":53889},{\"end\":53910,\"start\":53903},{\"end\":53929,\"start\":53918},{\"end\":54490,\"start\":54487},{\"end\":54502,\"start\":54497},{\"end\":54515,\"start\":54509},{\"end\":54528,\"start\":54521},{\"end\":54993,\"start\":54987},{\"end\":55009,\"start\":55000},{\"end\":55021,\"start\":55014},{\"end\":55035,\"start\":55028},{\"end\":55049,\"start\":55042},{\"end\":55060,\"start\":55057},{\"end\":55073,\"start\":55067},{\"end\":55590,\"start\":55583},{\"end\":55597,\"start\":55595},{\"end\":55612,\"start\":55605},{\"end\":55625,\"start\":55620},{\"end\":55635,\"start\":55631},{\"end\":55935,\"start\":55926},{\"end\":55949,\"start\":55940},{\"end\":55964,\"start\":55957},{\"end\":55977,\"start\":55972},{\"end\":55979,\"start\":55978},{\"end\":56443,\"start\":56439},{\"end\":56454,\"start\":56448},{\"end\":56467,\"start\":56461},{\"end\":56482,\"start\":56473},{\"end\":56494,\"start\":56488},{\"end\":56510,\"start\":56502},{\"end\":56523,\"start\":56517},{\"end\":56824,\"start\":56821},{\"end\":56836,\"start\":56830},{\"end\":56851,\"start\":56843},{\"end\":56860,\"start\":56856},{\"end\":56874,\"start\":56867},{\"end\":57060,\"start\":57054},{\"end\":57073,\"start\":57067},{\"end\":57083,\"start\":57079},{\"end\":57473,\"start\":57466},{\"end\":57487,\"start\":57480},{\"end\":57495,\"start\":57493},{\"end\":57822,\"start\":57816},{\"end\":57834,\"start\":57827},{\"end\":57846,\"start\":57843},{\"end\":57856,\"start\":57852},{\"end\":58340,\"start\":58334},{\"end\":58352,\"start\":58345},{\"end\":58364,\"start\":58361},{\"end\":58377,\"start\":58370},{\"end\":58389,\"start\":58385},{\"end\":58669,\"start\":58662},{\"end\":58681,\"start\":58674},{\"end\":58695,\"start\":58691},{\"end\":58711,\"start\":58702},{\"end\":58713,\"start\":58712},{\"end\":58726,\"start\":58720},{\"end\":58728,\"start\":58727},{\"end\":59087,\"start\":59081},{\"end\":59100,\"start\":59095},{\"end\":59112,\"start\":59107},{\"end\":59681,\"start\":59675},{\"end\":59696,\"start\":59689},{\"end\":59711,\"start\":59701},{\"end\":59725,\"start\":59718},{\"end\":59736,\"start\":59731},{\"end\":59748,\"start\":59742},{\"end\":59761,\"start\":59755},{\"end\":59774,\"start\":59766},{\"end\":59784,\"start\":59781},{\"end\":59793,\"start\":59792},{\"end\":59795,\"start\":59794},{\"end\":60387,\"start\":60381},{\"end\":60399,\"start\":60393},{\"end\":60409,\"start\":60404},{\"end\":60417,\"start\":60414},{\"end\":60430,\"start\":60422},{\"end\":60443,\"start\":60437}]", "bib_author_last_name": "[{\"end\":36035,\"start\":36030},{\"end\":36057,\"start\":36048},{\"end\":36070,\"start\":36066},{\"end\":36088,\"start\":36078},{\"end\":36103,\"start\":36098},{\"end\":36482,\"start\":36475},{\"end\":36498,\"start\":36491},{\"end\":36515,\"start\":36507},{\"end\":36532,\"start\":36525},{\"end\":36546,\"start\":36540},{\"end\":36567,\"start\":36562},{\"end\":36914,\"start\":36908},{\"end\":36938,\"start\":36924},{\"end\":36952,\"start\":36946},{\"end\":37225,\"start\":37219},{\"end\":37242,\"start\":37237},{\"end\":37260,\"start\":37252},{\"end\":37277,\"start\":37270},{\"end\":37297,\"start\":37289},{\"end\":37315,\"start\":37306},{\"end\":37637,\"start\":37633},{\"end\":37646,\"start\":37643},{\"end\":37658,\"start\":37655},{\"end\":37669,\"start\":37665},{\"end\":37683,\"start\":37679},{\"end\":37695,\"start\":37693},{\"end\":37897,\"start\":37892},{\"end\":37911,\"start\":37904},{\"end\":37931,\"start\":37923},{\"end\":38298,\"start\":38286},{\"end\":38313,\"start\":38310},{\"end\":38319,\"start\":38315},{\"end\":38635,\"start\":38631},{\"end\":38646,\"start\":38643},{\"end\":38659,\"start\":38655},{\"end\":38673,\"start\":38668},{\"end\":39139,\"start\":39135},{\"end\":39152,\"start\":39149},{\"end\":39167,\"start\":39162},{\"end\":39175,\"start\":39172},{\"end\":39185,\"start\":39182},{\"end\":39199,\"start\":39194},{\"end\":39213,\"start\":39210},{\"end\":39421,\"start\":39410},{\"end\":39434,\"start\":39429},{\"end\":39456,\"start\":39446},{\"end\":39474,\"start\":39463},{\"end\":39488,\"start\":39484},{\"end\":39508,\"start\":39497},{\"end\":39526,\"start\":39518},{\"end\":39545,\"start\":39537},{\"end\":39560,\"start\":39553},{\"end\":40062,\"start\":40053},{\"end\":40077,\"start\":40069},{\"end\":40090,\"start\":40088},{\"end\":40100,\"start\":40096},{\"end\":40107,\"start\":40102},{\"end\":40532,\"start\":40530},{\"end\":40547,\"start\":40542},{\"end\":40561,\"start\":40558},{\"end\":40571,\"start\":40568},{\"end\":40982,\"start\":40980},{\"end\":40999,\"start\":40991},{\"end\":41015,\"start\":41008},{\"end\":41327,\"start\":41322},{\"end\":41340,\"start\":41337},{\"end\":41348,\"start\":41345},{\"end\":41360,\"start\":41358},{\"end\":41373,\"start\":41370},{\"end\":41386,\"start\":41383},{\"end\":41397,\"start\":41394},{\"end\":41404,\"start\":41402},{\"end\":41929,\"start\":41924},{\"end\":41940,\"start\":41937},{\"end\":41964,\"start\":41951},{\"end\":41969,\"start\":41966},{\"end\":42238,\"start\":42232},{\"end\":42252,\"start\":42248},{\"end\":42266,\"start\":42261},{\"end\":42283,\"start\":42277},{\"end\":42300,\"start\":42295},{\"end\":42763,\"start\":42757},{\"end\":42777,\"start\":42772},{\"end\":42789,\"start\":42784},{\"end\":42807,\"start\":42799},{\"end\":43283,\"start\":43276},{\"end\":43298,\"start\":43290},{\"end\":43313,\"start\":43306},{\"end\":43637,\"start\":43632},{\"end\":43644,\"start\":43642},{\"end\":43655,\"start\":43652},{\"end\":43664,\"start\":43660},{\"end\":43979,\"start\":43974},{\"end\":43986,\"start\":43984},{\"end\":44000,\"start\":43996},{\"end\":44014,\"start\":44010},{\"end\":44028,\"start\":44024},{\"end\":44041,\"start\":44038},{\"end\":44050,\"start\":44046},{\"end\":44394,\"start\":44391},{\"end\":44407,\"start\":44405},{\"end\":44416,\"start\":44413},{\"end\":44428,\"start\":44426},{\"end\":44439,\"start\":44436},{\"end\":44855,\"start\":44852},{\"end\":44869,\"start\":44863},{\"end\":44884,\"start\":44876},{\"end\":44896,\"start\":44894},{\"end\":44915,\"start\":44906},{\"end\":44931,\"start\":44923},{\"end\":45408,\"start\":45405},{\"end\":45421,\"start\":45416},{\"end\":45436,\"start\":45428},{\"end\":45842,\"start\":45839},{\"end\":45855,\"start\":45852},{\"end\":45870,\"start\":45865},{\"end\":45883,\"start\":45880},{\"end\":45890,\"start\":45888},{\"end\":45902,\"start\":45900},{\"end\":46194,\"start\":46191},{\"end\":46204,\"start\":46201},{\"end\":46217,\"start\":46212},{\"end\":46229,\"start\":46227},{\"end\":46243,\"start\":46238},{\"end\":46255,\"start\":46251},{\"end\":46266,\"start\":46262},{\"end\":46278,\"start\":46273},{\"end\":46296,\"start\":46285},{\"end\":46314,\"start\":46306},{\"end\":46323,\"start\":46316},{\"end\":46724,\"start\":46721},{\"end\":46736,\"start\":46733},{\"end\":46745,\"start\":46742},{\"end\":46753,\"start\":46751},{\"end\":46765,\"start\":46762},{\"end\":46778,\"start\":46773},{\"end\":46791,\"start\":46788},{\"end\":46804,\"start\":46801},{\"end\":47063,\"start\":47060},{\"end\":47073,\"start\":47069},{\"end\":47082,\"start\":47079},{\"end\":47094,\"start\":47091},{\"end\":47107,\"start\":47102},{\"end\":47120,\"start\":47117},{\"end\":47128,\"start\":47126},{\"end\":47401,\"start\":47397},{\"end\":47417,\"start\":47408},{\"end\":47433,\"start\":47426},{\"end\":47898,\"start\":47895},{\"end\":47909,\"start\":47905},{\"end\":47924,\"start\":47921},{\"end\":47937,\"start\":47934},{\"end\":47950,\"start\":47948},{\"end\":47962,\"start\":47958},{\"end\":47975,\"start\":47973},{\"end\":48497,\"start\":48488},{\"end\":48517,\"start\":48509},{\"end\":48535,\"start\":48525},{\"end\":48560,\"start\":48547},{\"end\":48874,\"start\":48865},{\"end\":48888,\"start\":48883},{\"end\":48908,\"start\":48902},{\"end\":49287,\"start\":49274},{\"end\":49297,\"start\":49294},{\"end\":49310,\"start\":49308},{\"end\":49315,\"start\":49312},{\"end\":49760,\"start\":49753},{\"end\":49778,\"start\":49768},{\"end\":49796,\"start\":49786},{\"end\":49810,\"start\":49802},{\"end\":49824,\"start\":49819},{\"end\":49851,\"start\":49836},{\"end\":50307,\"start\":50297},{\"end\":50325,\"start\":50318},{\"end\":50340,\"start\":50333},{\"end\":50356,\"start\":50348},{\"end\":50378,\"start\":50363},{\"end\":50392,\"start\":50384},{\"end\":50788,\"start\":50777},{\"end\":50801,\"start\":50797},{\"end\":50816,\"start\":50812},{\"end\":50832,\"start\":50823},{\"end\":50842,\"start\":50838},{\"end\":50859,\"start\":50851},{\"end\":51402,\"start\":51399},{\"end\":51418,\"start\":51415},{\"end\":51431,\"start\":51428},{\"end\":51831,\"start\":51828},{\"end\":51841,\"start\":51836},{\"end\":51855,\"start\":51850},{\"end\":51865,\"start\":51862},{\"end\":51877,\"start\":51874},{\"end\":51889,\"start\":51887},{\"end\":51899,\"start\":51895},{\"end\":51912,\"start\":51908},{\"end\":51926,\"start\":51922},{\"end\":51936,\"start\":51933},{\"end\":52319,\"start\":52316},{\"end\":52333,\"start\":52328},{\"end\":52343,\"start\":52338},{\"end\":52353,\"start\":52349},{\"end\":52366,\"start\":52364},{\"end\":52376,\"start\":52372},{\"end\":52396,\"start\":52388},{\"end\":52404,\"start\":52402},{\"end\":52417,\"start\":52413},{\"end\":52431,\"start\":52427},{\"end\":52958,\"start\":52954},{\"end\":52972,\"start\":52968},{\"end\":52982,\"start\":52978},{\"end\":53365,\"start\":53358},{\"end\":53379,\"start\":53372},{\"end\":53392,\"start\":53386},{\"end\":53409,\"start\":53400},{\"end\":53422,\"start\":53417},{\"end\":53437,\"start\":53432},{\"end\":53452,\"start\":53446},{\"end\":53470,\"start\":53460},{\"end\":53857,\"start\":53845},{\"end\":53870,\"start\":53866},{\"end\":53887,\"start\":53880},{\"end\":53901,\"start\":53897},{\"end\":53916,\"start\":53911},{\"end\":53934,\"start\":53930},{\"end\":54495,\"start\":54491},{\"end\":54507,\"start\":54503},{\"end\":54519,\"start\":54516},{\"end\":54532,\"start\":54529},{\"end\":54998,\"start\":54994},{\"end\":55012,\"start\":55010},{\"end\":55026,\"start\":55022},{\"end\":55040,\"start\":55036},{\"end\":55055,\"start\":55050},{\"end\":55065,\"start\":55061},{\"end\":55077,\"start\":55074},{\"end\":55593,\"start\":55591},{\"end\":55603,\"start\":55598},{\"end\":55618,\"start\":55613},{\"end\":55629,\"start\":55626},{\"end\":55639,\"start\":55636},{\"end\":55938,\"start\":55936},{\"end\":55955,\"start\":55950},{\"end\":55970,\"start\":55965},{\"end\":55985,\"start\":55980},{\"end\":56446,\"start\":56444},{\"end\":56459,\"start\":56455},{\"end\":56471,\"start\":56468},{\"end\":56486,\"start\":56483},{\"end\":56500,\"start\":56495},{\"end\":56515,\"start\":56511},{\"end\":56529,\"start\":56524},{\"end\":56828,\"start\":56825},{\"end\":56841,\"start\":56837},{\"end\":56854,\"start\":56852},{\"end\":56865,\"start\":56861},{\"end\":56877,\"start\":56875},{\"end\":57065,\"start\":57061},{\"end\":57077,\"start\":57074},{\"end\":57086,\"start\":57084},{\"end\":57478,\"start\":57474},{\"end\":57491,\"start\":57488},{\"end\":57500,\"start\":57496},{\"end\":57825,\"start\":57823},{\"end\":57841,\"start\":57835},{\"end\":57850,\"start\":57847},{\"end\":57861,\"start\":57857},{\"end\":58343,\"start\":58341},{\"end\":58359,\"start\":58353},{\"end\":58368,\"start\":58365},{\"end\":58383,\"start\":58378},{\"end\":58394,\"start\":58390},{\"end\":58672,\"start\":58670},{\"end\":58689,\"start\":58682},{\"end\":58700,\"start\":58696},{\"end\":58718,\"start\":58714},{\"end\":58733,\"start\":58729},{\"end\":59093,\"start\":59088},{\"end\":59105,\"start\":59101},{\"end\":59116,\"start\":59113},{\"end\":59687,\"start\":59682},{\"end\":59699,\"start\":59697},{\"end\":59716,\"start\":59712},{\"end\":59729,\"start\":59726},{\"end\":59740,\"start\":59737},{\"end\":59753,\"start\":59749},{\"end\":59764,\"start\":59762},{\"end\":59779,\"start\":59775},{\"end\":59790,\"start\":59785},{\"end\":59802,\"start\":59796},{\"end\":59808,\"start\":59804},{\"end\":60391,\"start\":60388},{\"end\":60402,\"start\":60400},{\"end\":60412,\"start\":60410},{\"end\":60420,\"start\":60418},{\"end\":60435,\"start\":60431},{\"end\":60447,\"start\":60444}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":213175621},\"end\":36388,\"start\":35952},{\"attributes\":{\"doi\":\"arXiv:2010.00263\",\"id\":\"b1\"},\"end\":36824,\"start\":36390},{\"attributes\":{\"doi\":\"arXiv:2111.14821\",\"id\":\"b2\"},\"end\":37163,\"start\":36826},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218889832},\"end\":37605,\"start\":37165},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":232404168},\"end\":37813,\"start\":37607},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235829267},\"end\":38174,\"start\":37815},{\"attributes\":{\"doi\":\"arXiv:2106.05210\",\"id\":\"b6\"},\"end\":38544,\"start\":38176},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":236987044},\"end\":39043,\"start\":38546},{\"attributes\":{\"id\":\"b8\"},\"end\":39401,\"start\":39045},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b9\"},\"end\":39991,\"start\":39403},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4002773},\"end\":40474,\"start\":39993},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":40921,\"start\":40476},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1931511},\"end\":41240,\"start\":40923},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219622248},\"end\":41841,\"start\":41242},{\"attributes\":{\"doi\":\"arXiv:2106.03299\",\"id\":\"b14\"},\"end\":42180,\"start\":41843},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13000587},\"end\":42645,\"start\":42182},{\"attributes\":{\"id\":\"b16\"},\"end\":43206,\"start\":42647},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3992628},\"end\":43543,\"start\":43208},{\"attributes\":{\"doi\":\"arXiv:2103.10702\",\"id\":\"b18\"},\"end\":43863,\"start\":43545},{\"attributes\":{\"doi\":\"arXiv:2106.01061\",\"id\":\"b19\"},\"end\":44321,\"start\":43865},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":232352827},\"end\":44794,\"start\":44323},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10716717},\"end\":45325,\"start\":44796},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206771220},\"end\":45768,\"start\":45327},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":234496945},\"end\":46182,\"start\":45770},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b24\"},\"end\":46643,\"start\":46184},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b25\"},\"end\":47053,\"start\":46645},{\"attributes\":{\"doi\":\"arXiv:2106.13230\",\"id\":\"b26\"},\"end\":47330,\"start\":47055},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1629541},\"end\":47793,\"start\":47332},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":213176225},\"end\":48428,\"start\":47795},{\"attributes\":{\"doi\":\"arXiv:2101.02702\",\"id\":\"b29\"},\"end\":48770,\"start\":48430},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206429151},\"end\":49201,\"start\":48772},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":90262243},\"end\":49664,\"start\":49203},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1949934},\"end\":50289,\"start\":49666},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b33\"},\"end\":50683,\"start\":50291},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":67855581},\"end\":51300,\"start\":50685},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":226220136},\"end\":51820,\"start\":51302},{\"attributes\":{\"doi\":\"arXiv:2012.15460\",\"id\":\"b36\"},\"end\":52240,\"start\":51822},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":227162412},\"end\":52896,\"start\":52242},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":212675968},\"end\":53322,\"start\":52898},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13756489},\"end\":53763,\"start\":53324},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":67856723},\"end\":54374,\"start\":53765},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":204958091},\"end\":54927,\"start\":54376},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":227227748},\"end\":55508,\"start\":54929},{\"attributes\":{\"doi\":\"arXiv:2112.08275\",\"id\":\"b43\"},\"end\":55854,\"start\":55510},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12546432},\"end\":56373,\"start\":55856},{\"attributes\":{\"doi\":\"arXiv:1809.03327\",\"id\":\"b45\"},\"end\":56761,\"start\":56375},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232428140},\"end\":57023,\"start\":56763},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":152282473},\"end\":57386,\"start\":57025},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":212747509},\"end\":57745,\"start\":57388},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":104292134},\"end\":58247,\"start\":57747},{\"attributes\":{\"doi\":\"arXiv:2102.04762\",\"id\":\"b50\"},\"end\":58617,\"start\":58249},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":1688357},\"end\":58978,\"start\":58619},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":11620893},\"end\":59581,\"start\":58980},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":229924195},\"end\":60305,\"start\":59583},{\"attributes\":{\"doi\":\"arXiv:2010.04159\",\"id\":\"b54\"},\"end\":60676,\"start\":60307}]", "bib_title": "[{\"end\":36024,\"start\":35952},{\"end\":37209,\"start\":37165},{\"end\":37627,\"start\":37607},{\"end\":37884,\"start\":37815},{\"end\":38621,\"start\":38546},{\"end\":40044,\"start\":39993},{\"end\":40520,\"start\":40476},{\"end\":40969,\"start\":40923},{\"end\":41312,\"start\":41242},{\"end\":42222,\"start\":42182},{\"end\":42745,\"start\":42647},{\"end\":43269,\"start\":43208},{\"end\":44381,\"start\":44323},{\"end\":44841,\"start\":44796},{\"end\":45394,\"start\":45327},{\"end\":45834,\"start\":45770},{\"end\":47386,\"start\":47332},{\"end\":47889,\"start\":47795},{\"end\":48856,\"start\":48772},{\"end\":49261,\"start\":49203},{\"end\":49742,\"start\":49666},{\"end\":50769,\"start\":50685},{\"end\":51389,\"start\":51302},{\"end\":52308,\"start\":52242},{\"end\":52948,\"start\":52898},{\"end\":53349,\"start\":53324},{\"end\":53838,\"start\":53765},{\"end\":54485,\"start\":54376},{\"end\":54985,\"start\":54929},{\"end\":55924,\"start\":55856},{\"end\":56819,\"start\":56763},{\"end\":57052,\"start\":57025},{\"end\":57464,\"start\":57388},{\"end\":57814,\"start\":57747},{\"end\":58660,\"start\":58619},{\"end\":59079,\"start\":58980},{\"end\":59673,\"start\":59583}]", "bib_author": "[{\"end\":36037,\"start\":36026},{\"end\":36059,\"start\":36037},{\"end\":36072,\"start\":36059},{\"end\":36090,\"start\":36072},{\"end\":36105,\"start\":36090},{\"end\":36484,\"start\":36468},{\"end\":36500,\"start\":36484},{\"end\":36517,\"start\":36500},{\"end\":36534,\"start\":36517},{\"end\":36548,\"start\":36534},{\"end\":36569,\"start\":36548},{\"end\":36916,\"start\":36903},{\"end\":36940,\"start\":36916},{\"end\":36954,\"start\":36940},{\"end\":37227,\"start\":37211},{\"end\":37244,\"start\":37227},{\"end\":37262,\"start\":37244},{\"end\":37279,\"start\":37262},{\"end\":37299,\"start\":37279},{\"end\":37317,\"start\":37299},{\"end\":37639,\"start\":37629},{\"end\":37648,\"start\":37639},{\"end\":37660,\"start\":37648},{\"end\":37671,\"start\":37660},{\"end\":37685,\"start\":37671},{\"end\":37697,\"start\":37685},{\"end\":37899,\"start\":37886},{\"end\":37913,\"start\":37899},{\"end\":37933,\"start\":37913},{\"end\":38300,\"start\":38278},{\"end\":38315,\"start\":38300},{\"end\":38321,\"start\":38315},{\"end\":38637,\"start\":38623},{\"end\":38648,\"start\":38637},{\"end\":38661,\"start\":38648},{\"end\":38675,\"start\":38661},{\"end\":39141,\"start\":39129},{\"end\":39154,\"start\":39141},{\"end\":39169,\"start\":39154},{\"end\":39177,\"start\":39169},{\"end\":39187,\"start\":39177},{\"end\":39201,\"start\":39187},{\"end\":39215,\"start\":39201},{\"end\":39423,\"start\":39403},{\"end\":39436,\"start\":39423},{\"end\":39458,\"start\":39436},{\"end\":39476,\"start\":39458},{\"end\":39490,\"start\":39476},{\"end\":39510,\"start\":39490},{\"end\":39528,\"start\":39510},{\"end\":39547,\"start\":39528},{\"end\":39562,\"start\":39547},{\"end\":40064,\"start\":40046},{\"end\":40079,\"start\":40064},{\"end\":40092,\"start\":40079},{\"end\":40102,\"start\":40092},{\"end\":40109,\"start\":40102},{\"end\":40534,\"start\":40522},{\"end\":40549,\"start\":40534},{\"end\":40563,\"start\":40549},{\"end\":40573,\"start\":40563},{\"end\":40984,\"start\":40971},{\"end\":41001,\"start\":40984},{\"end\":41017,\"start\":41001},{\"end\":41329,\"start\":41314},{\"end\":41342,\"start\":41329},{\"end\":41350,\"start\":41342},{\"end\":41362,\"start\":41350},{\"end\":41375,\"start\":41362},{\"end\":41388,\"start\":41375},{\"end\":41399,\"start\":41388},{\"end\":41406,\"start\":41399},{\"end\":41931,\"start\":41917},{\"end\":41942,\"start\":41931},{\"end\":41966,\"start\":41942},{\"end\":41971,\"start\":41966},{\"end\":42240,\"start\":42224},{\"end\":42254,\"start\":42240},{\"end\":42268,\"start\":42254},{\"end\":42285,\"start\":42268},{\"end\":42302,\"start\":42285},{\"end\":42765,\"start\":42747},{\"end\":42779,\"start\":42765},{\"end\":42791,\"start\":42779},{\"end\":42809,\"start\":42791},{\"end\":43285,\"start\":43271},{\"end\":43300,\"start\":43285},{\"end\":43315,\"start\":43300},{\"end\":43639,\"start\":43627},{\"end\":43646,\"start\":43639},{\"end\":43657,\"start\":43646},{\"end\":43666,\"start\":43657},{\"end\":43981,\"start\":43969},{\"end\":43988,\"start\":43981},{\"end\":44002,\"start\":43988},{\"end\":44016,\"start\":44002},{\"end\":44030,\"start\":44016},{\"end\":44043,\"start\":44030},{\"end\":44052,\"start\":44043},{\"end\":44396,\"start\":44383},{\"end\":44409,\"start\":44396},{\"end\":44418,\"start\":44409},{\"end\":44430,\"start\":44418},{\"end\":44441,\"start\":44430},{\"end\":44857,\"start\":44843},{\"end\":44871,\"start\":44857},{\"end\":44886,\"start\":44871},{\"end\":44898,\"start\":44886},{\"end\":44917,\"start\":44898},{\"end\":44933,\"start\":44917},{\"end\":45410,\"start\":45396},{\"end\":45423,\"start\":45410},{\"end\":45438,\"start\":45423},{\"end\":45844,\"start\":45836},{\"end\":45857,\"start\":45844},{\"end\":45872,\"start\":45857},{\"end\":45885,\"start\":45872},{\"end\":45892,\"start\":45885},{\"end\":45904,\"start\":45892},{\"end\":46196,\"start\":46184},{\"end\":46206,\"start\":46196},{\"end\":46219,\"start\":46206},{\"end\":46231,\"start\":46219},{\"end\":46245,\"start\":46231},{\"end\":46257,\"start\":46245},{\"end\":46268,\"start\":46257},{\"end\":46280,\"start\":46268},{\"end\":46298,\"start\":46280},{\"end\":46316,\"start\":46298},{\"end\":46325,\"start\":46316},{\"end\":46726,\"start\":46718},{\"end\":46738,\"start\":46726},{\"end\":46747,\"start\":46738},{\"end\":46755,\"start\":46747},{\"end\":46767,\"start\":46755},{\"end\":46780,\"start\":46767},{\"end\":46793,\"start\":46780},{\"end\":46806,\"start\":46793},{\"end\":47065,\"start\":47057},{\"end\":47075,\"start\":47065},{\"end\":47084,\"start\":47075},{\"end\":47096,\"start\":47084},{\"end\":47109,\"start\":47096},{\"end\":47122,\"start\":47109},{\"end\":47130,\"start\":47122},{\"end\":47403,\"start\":47388},{\"end\":47419,\"start\":47403},{\"end\":47435,\"start\":47419},{\"end\":47900,\"start\":47891},{\"end\":47911,\"start\":47900},{\"end\":47926,\"start\":47911},{\"end\":47939,\"start\":47926},{\"end\":47952,\"start\":47939},{\"end\":47964,\"start\":47952},{\"end\":47977,\"start\":47964},{\"end\":48499,\"start\":48484},{\"end\":48519,\"start\":48499},{\"end\":48537,\"start\":48519},{\"end\":48562,\"start\":48537},{\"end\":48876,\"start\":48858},{\"end\":48890,\"start\":48876},{\"end\":48910,\"start\":48890},{\"end\":49289,\"start\":49263},{\"end\":49299,\"start\":49289},{\"end\":49312,\"start\":49299},{\"end\":49317,\"start\":49312},{\"end\":49762,\"start\":49744},{\"end\":49780,\"start\":49762},{\"end\":49798,\"start\":49780},{\"end\":49812,\"start\":49798},{\"end\":49826,\"start\":49812},{\"end\":49853,\"start\":49826},{\"end\":50309,\"start\":50291},{\"end\":50327,\"start\":50309},{\"end\":50342,\"start\":50327},{\"end\":50358,\"start\":50342},{\"end\":50380,\"start\":50358},{\"end\":50394,\"start\":50380},{\"end\":50790,\"start\":50771},{\"end\":50803,\"start\":50790},{\"end\":50818,\"start\":50803},{\"end\":50834,\"start\":50818},{\"end\":50844,\"start\":50834},{\"end\":50861,\"start\":50844},{\"end\":51404,\"start\":51391},{\"end\":51420,\"start\":51404},{\"end\":51433,\"start\":51420},{\"end\":51833,\"start\":51822},{\"end\":51843,\"start\":51833},{\"end\":51857,\"start\":51843},{\"end\":51867,\"start\":51857},{\"end\":51879,\"start\":51867},{\"end\":51891,\"start\":51879},{\"end\":51901,\"start\":51891},{\"end\":51914,\"start\":51901},{\"end\":51928,\"start\":51914},{\"end\":51938,\"start\":51928},{\"end\":52321,\"start\":52310},{\"end\":52335,\"start\":52321},{\"end\":52345,\"start\":52335},{\"end\":52355,\"start\":52345},{\"end\":52368,\"start\":52355},{\"end\":52378,\"start\":52368},{\"end\":52398,\"start\":52378},{\"end\":52406,\"start\":52398},{\"end\":52419,\"start\":52406},{\"end\":52433,\"start\":52419},{\"end\":52960,\"start\":52950},{\"end\":52974,\"start\":52960},{\"end\":52984,\"start\":52974},{\"end\":53367,\"start\":53351},{\"end\":53381,\"start\":53367},{\"end\":53394,\"start\":53381},{\"end\":53411,\"start\":53394},{\"end\":53424,\"start\":53411},{\"end\":53439,\"start\":53424},{\"end\":53454,\"start\":53439},{\"end\":53472,\"start\":53454},{\"end\":53859,\"start\":53840},{\"end\":53872,\"start\":53859},{\"end\":53889,\"start\":53872},{\"end\":53903,\"start\":53889},{\"end\":53918,\"start\":53903},{\"end\":53936,\"start\":53918},{\"end\":54497,\"start\":54487},{\"end\":54509,\"start\":54497},{\"end\":54521,\"start\":54509},{\"end\":54534,\"start\":54521},{\"end\":55000,\"start\":54987},{\"end\":55014,\"start\":55000},{\"end\":55028,\"start\":55014},{\"end\":55042,\"start\":55028},{\"end\":55057,\"start\":55042},{\"end\":55067,\"start\":55057},{\"end\":55079,\"start\":55067},{\"end\":55595,\"start\":55583},{\"end\":55605,\"start\":55595},{\"end\":55620,\"start\":55605},{\"end\":55631,\"start\":55620},{\"end\":55641,\"start\":55631},{\"end\":55940,\"start\":55926},{\"end\":55957,\"start\":55940},{\"end\":55972,\"start\":55957},{\"end\":55987,\"start\":55972},{\"end\":56448,\"start\":56439},{\"end\":56461,\"start\":56448},{\"end\":56473,\"start\":56461},{\"end\":56488,\"start\":56473},{\"end\":56502,\"start\":56488},{\"end\":56517,\"start\":56502},{\"end\":56531,\"start\":56517},{\"end\":56830,\"start\":56821},{\"end\":56843,\"start\":56830},{\"end\":56856,\"start\":56843},{\"end\":56867,\"start\":56856},{\"end\":56879,\"start\":56867},{\"end\":57067,\"start\":57054},{\"end\":57079,\"start\":57067},{\"end\":57088,\"start\":57079},{\"end\":57480,\"start\":57466},{\"end\":57493,\"start\":57480},{\"end\":57502,\"start\":57493},{\"end\":57827,\"start\":57816},{\"end\":57843,\"start\":57827},{\"end\":57852,\"start\":57843},{\"end\":57863,\"start\":57852},{\"end\":58345,\"start\":58334},{\"end\":58361,\"start\":58345},{\"end\":58370,\"start\":58361},{\"end\":58385,\"start\":58370},{\"end\":58396,\"start\":58385},{\"end\":58674,\"start\":58662},{\"end\":58691,\"start\":58674},{\"end\":58702,\"start\":58691},{\"end\":58720,\"start\":58702},{\"end\":58735,\"start\":58720},{\"end\":59095,\"start\":59081},{\"end\":59107,\"start\":59095},{\"end\":59118,\"start\":59107},{\"end\":59689,\"start\":59675},{\"end\":59701,\"start\":59689},{\"end\":59718,\"start\":59701},{\"end\":59731,\"start\":59718},{\"end\":59742,\"start\":59731},{\"end\":59755,\"start\":59742},{\"end\":59766,\"start\":59755},{\"end\":59781,\"start\":59766},{\"end\":59792,\"start\":59781},{\"end\":59804,\"start\":59792},{\"end\":59810,\"start\":59804},{\"end\":60393,\"start\":60381},{\"end\":60404,\"start\":60393},{\"end\":60414,\"start\":60404},{\"end\":60422,\"start\":60414},{\"end\":60437,\"start\":60422},{\"end\":60449,\"start\":60437}]", "bib_venue": "[{\"end\":36143,\"start\":36105},{\"end\":36466,\"start\":36390},{\"end\":36901,\"start\":36826},{\"end\":37355,\"start\":37317},{\"end\":37701,\"start\":37697},{\"end\":37982,\"start\":37933},{\"end\":38276,\"start\":38176},{\"end\":38746,\"start\":38675},{\"end\":39127,\"start\":39045},{\"end\":39674,\"start\":39578},{\"end\":40186,\"start\":40109},{\"end\":40650,\"start\":40573},{\"end\":41055,\"start\":41017},{\"end\":41487,\"start\":41406},{\"end\":41915,\"start\":41843},{\"end\":42369,\"start\":42302},{\"end\":42880,\"start\":42809},{\"end\":43350,\"start\":43315},{\"end\":43625,\"start\":43545},{\"end\":43967,\"start\":43865},{\"end\":44512,\"start\":44441},{\"end\":45010,\"start\":44933},{\"end\":45505,\"start\":45438},{\"end\":45966,\"start\":45904},{\"end\":46387,\"start\":46341},{\"end\":46716,\"start\":46645},{\"end\":47512,\"start\":47435},{\"end\":48058,\"start\":47977},{\"end\":48482,\"start\":48430},{\"end\":48965,\"start\":48910},{\"end\":49388,\"start\":49317},{\"end\":49930,\"start\":49853},{\"end\":50463,\"start\":50410},{\"end\":50942,\"start\":50861},{\"end\":51484,\"start\":51433},{\"end\":52007,\"start\":51954},{\"end\":52514,\"start\":52433},{\"end\":53035,\"start\":52984},{\"end\":53521,\"start\":53472},{\"end\":54017,\"start\":53936},{\"end\":54605,\"start\":54534},{\"end\":55160,\"start\":55079},{\"end\":55581,\"start\":55510},{\"end\":56064,\"start\":55987},{\"end\":56437,\"start\":56375},{\"end\":56883,\"start\":56879},{\"end\":57159,\"start\":57088},{\"end\":57540,\"start\":57502},{\"end\":57944,\"start\":57863},{\"end\":58332,\"start\":58249},{\"end\":58773,\"start\":58735},{\"end\":59195,\"start\":59118},{\"end\":59891,\"start\":59810},{\"end\":60379,\"start\":60307},{\"end\":38804,\"start\":38748},{\"end\":40250,\"start\":40188},{\"end\":40714,\"start\":40652},{\"end\":41555,\"start\":41489},{\"end\":42423,\"start\":42371},{\"end\":42938,\"start\":42882},{\"end\":44570,\"start\":44514},{\"end\":45074,\"start\":45012},{\"end\":45559,\"start\":45507},{\"end\":47576,\"start\":47514},{\"end\":48126,\"start\":48060},{\"end\":49446,\"start\":49390},{\"end\":49994,\"start\":49932},{\"end\":51010,\"start\":50944},{\"end\":51497,\"start\":51486},{\"end\":52582,\"start\":52516},{\"end\":53052,\"start\":53037},{\"end\":54085,\"start\":54019},{\"end\":54663,\"start\":54607},{\"end\":55228,\"start\":55162},{\"end\":56128,\"start\":56066},{\"end\":57217,\"start\":57161},{\"end\":58012,\"start\":57946},{\"end\":59259,\"start\":59197},{\"end\":59959,\"start\":59893}]"}}}, "year": 2023, "month": 12, "day": 17}