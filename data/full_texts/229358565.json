{"id": 229358565, "updated": "2022-01-16 17:56:44.476", "metadata": {"title": "Modeling the relationship between acoustic stimulus and EEG with a dilated convolutional neural network", "authors": "[{\"middle\":[],\"last\":\"Accou\",\"first\":\"Bernd\"},{\"middle\":[],\"last\":\"Jalilpour Monesi\",\"first\":\"Mohammad\"},{\"middle\":[],\"last\":\"Montoya\",\"first\":\"Jair\"},{\"middle\":[],\"last\":\"Van hamme\",\"first\":\"Hugo\"},{\"middle\":[],\"last\":\"Francart\",\"first\":\"Tom\"}]", "venue": "2020 28th European Signal Processing Conference (EUSIPCO)", "journal": "2020 28th European Signal Processing Conference (EUSIPCO)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Current tests to measure whether a person can understand speech require behavioral responses from the person, which is in practice not always possible (e.g. young children). Therefore there is a need for objective measures of speech intelligibility. Recently, it has been shown that speech intelligibility can be measured by letting a person listen to natural speech, recording the electroencephalogram (EEG) and decoding the speech envelope from the EEG signal. Linear decoders are used, which is sub-optimal, as the human brain is a complex non-linear system and cannot easily be modeled by a linear decoder. We therefore propose an approach based on deep learning which can model complex non-linear relationships. Our approach is based on dilated convolutions as used in WaveNet to maximize the receptive field with regard to the number of tunable parameters. Comparison with a model based on a state of the art linear decoder and a convolutional baseline model shows that our proposed model significantly improves on both models (from 62.3% to 90.6% (p<0.001) and from 78.8% to 90.6% (p<0.001) respectively). Best results are achieved with a receptive field size between 250-500ms, which is longer than the optimal integration window for a linear decoder.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3115691565", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eusipco/AccouJMhF20", "doi": "10.23919/eusipco47968.2020.9287417"}}, "content": {"source": {"pdf_hash": "aaa66dbda2abdc86efb1b01c949b4977fb26fa74", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "77b3dc4c34ab82e4c3d4ebfae24c2ddb0443027c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aaa66dbda2abdc86efb1b01c949b4977fb26fa74.txt", "contents": "\nModeling the relationship between acoustic stimulus and EEG with a dilated convolutional neural network\n\n\nBernd Accou \nDepartment of Electrical Engineering KU Leuven\nPSI\nLeuvenBelgium\n\nMohammad Jalilpour Monesi \nDepartment of Electrical Engineering KU Leuven\nPSI\nLeuvenBelgium\n\nJair Montoya \nHugo Van Hamme \nDepartment of Electrical Engineering KU Leuven\nPSI\nLeuvenBelgium\n\nTom Francart \nExporl \n\nDepartment of Neurosciences KU Leuven\nLeuvenBelgium\n\nModeling the relationship between acoustic stimulus and EEG with a dilated convolutional neural network\nIndex Terms-match/mismatchEEG decodingspeechaudi- tory systemenvelope\nCurrent tests to measure whether a person can understand speech require behavioral responses from the person, which is in practice not always possible (e.g. young children). Therefore there is a need for objective measures of speech intelligibility. Recently, it has been shown that speech intelligibility can be measured by letting a person listen to natural speech, recording the electroencephalogram (EEG) and decoding the speech envelope from the EEG signal. Linear decoders are used, which is sub-optimal, as the human brain is a complex non-linear system and cannot easily be modeled by a linear decoder. We therefore propose an approach based on deep learning which can model complex non-linear relationships. Our approach is based on dilated convolutions as used in WaveNet to maximize the receptive field with regard to the number of tunable parameters. Comparison with a model based on a state of the art linear decoder and a convolutional baseline model shows that our proposed model significantly improves on both models (from 62.3% to 90.6% (p <0.001) and from 78.8% to 90.6% (p <0.001) respectively). Best results are achieved with a receptive field size between 250-500ms, which is longer than the optimal integration window for a linear decoder.\n\nI. INTRODUCTION\n\nA popular technique to study how the human brain processes speech is to present natural running speech to a subject and record the corresponding electroencephalogram (EEG) to capture the signal evoked by the stimulus. Then, using linear regression, either the features of the speech signal are decoded from the EEG signal (backward model), or the EEG signal is predicted from the speech signal (forward model). Finally, the correlation between the true and the predicted signal is computed, leading to a measure of neural tracking of speech [1]- [5]. This method has applications in domains such as audiology, as part of an objective measure of speech intelligibility [5]- [7]. While the results of this approach are promising, unfortunately, the correlations between actual and predicted signal with either technique are small (in the order of 0.1), limiting applicability. Additionally, when the same measurement is made multiple times, there is a large variability [4]. This is due to the use of simple linear models, which do not seem appropriate given the complexity and the dynamic nature of the brain. For instance, it is well known that depending on the level of attention and state of arousal of the subject, response latencies can change dramatically [3]. As this cannot be modeled using a pure linear approach, nonlinear deep learning methods might be more suitable to tackle this task.\n\nWhen comparing simple artificial neural networks (ANNs) to the linear decoder, somewhat higher correlations between predicted and actual signal can be obtained for EEG and for intracranial electrodes [8], [9]. For auditory attention decoding (i.e., in a multi-speaker scenario deducing which speaker a subject is attending to from the EEG signal), higher performance is reported using relatively simple deep neural networks [10]. However, the correlations between predicted and actual EEG remain low, whereas the variability across segments remains high. Therefore, long segments (in the order of several minutes) of speech and corresponding EEG are required to obtain a reliable response.\n\nTo avoid having to solve the regression problem, which is notoriously difficult with deep learning, and inspired by these recent advances in auditory attention detection, we therefore redefined the problem as a match/mismatch classification problem [11], [12]. In this paradigm, each model has 3 inputs: EEG, the corresponding stimulus envelope and an imposter envelope. This imposter envelope is a mismatched speech envelope segment. The task of the models in this paradigm is to identify the stimulus envelope corresponding to the EEG.\n\nVery recently, convolutional networks have been applied for auditory attention decoding [13], [14]. Instead of a two-step approach (reconstructing the attended stimulus and comparing the similarity with the actual stimuli), these convolution-based models can classify the attended speaker directly from the EEG and the envelopes of the speech signals, which allows for end-to-end training and better performance.\n\nCompared to fully connected ANN's, convolutional net-  works are more efficient due to weight sharing, but still have limited receptive field. To maximize the receptive field of convolutional layers, while keeping the amount of parameters relatively low, dilated convolutions can be used. In dilated convolutions, some samples can be skipped by adding spacing between consecutive weights in the kernel (see Figure 1 (c)). By increasing this spacing exponentially with depth, a maximal receptive field can be achieved, while keeping the number of parameters low compared to strided convolutions. In WaveNet, dilated convolutions were used to maximize the receptive field in order to computationally cope with raw audio files at a sampling rate of 16kHz [15]. In this study, we expand on the use of dilated neural networks on time-series data by applying them to segments of EEG data and segments of speech envelopes.\n\n\nII. METHODS\n\nAs references, we constructed a baseline network, based on a state-of-the-art linear decoder [1], [5] as shown in Figure 1 (a) and a convolutional baseline, as shown in Figure 1 \n(b).\nTo compensate for the brain delay following an auditory stimulus, both the decoder and convolutional baseline model  apply an integration window of 250 ms to the EEG. In the decoder baseline, this integration window is constructed by concatenating the next 250 ms of EEG response to the current EEG sample in the channel dimension. The linear decoder reconstructs the envelope of the speech stimulus from the EEG by making a linear combination of all samples in the integration window. The reconstructed envelope is correlated with both input envelopes. The matched stimulus envelope is chosen based on the highest correlation value.\n\nIn the convolutional baseline, the integration window is  implemented as a convolution which slides over the EEG segment and linearly combines all channels over the next 250 ms into a reconstructed speech envelope sample. The cosine similarity between the reconstructed envelope and both input envelopes is computed and fed into a single sigmoid neuron, which will classify whether input envelope 1 or input envelope 2 matches with the EEG. Construction of the integration window for both the decoder and convolutional baseline network at the end of a segment would require samples from outside of the segment. To prevent this, the last 250 ms of each envelope segment are discarded.\n\nOur proposed dilated convolutional network is shown in Figure 1 (c). In the first step, a convolutional layer with 8 filters is used to spatially and linearly combine all EEG channels. Then, N dilated convolutions with kernel size K are applied to the spatially filtered EEG and the two stimulus envelopes. To minimize the amount of parameters per receptive field size, the dilation factor d for layer L n is chosen to be K n\u22121 , as in [15]. A rectified linear unit (ReLU) nonlinearity is applied after each dilated convolution. Cosine similarity is used to compare each EEG representation to each stimulus representation after dilated convolutions. A sigmoid layer classifies match/mismatch based on the cosine similarity scores. The number of filters for the spatial convolutional layer and dilated convolutional layers were chosen based on the optimal performance of the model in a hyperparameter sweep (2 n filters for n = 0...8).\n\nEach of the proposed models has 3 inputs, one for EEG data segments and two for speech envelope segments. Data was presented to the models in segments of 10 seconds with an overlap of 90%. Note that for linear models typically segments of 30s or longer are used. The imposter speech envelope segment is chosen 1 second after the end of the current EEG segment, as displayed in Figure 2. To ensure that our dataset is balanced, the imposter segment is alternately presented to each of the speech envelope inputs. This means that each segment of EEG is presented twice to the network.\n\nAs the only trainable weights in our baseline model are the weights of the linear decoder, the linear decoder was trained independently in a linear regression setting. In this linear regression setting, mean squared error was used as a loss function and Pearson correlation as the evaluation metric. After training, the linear decoder was evaluated as displayed in Figure 1 (a) with accuracy as the performance metric.\n\nThe convolutional baseline and dilated models were trained in an end-to-end fashion for 50 epochs with stochastic gradient descent using the Adam optimizer with a learning rate of 10 \u22123 . The models were saved after each epoch. After 50 epochs, the best model was chosen from the saved models based on validation loss.\n\nA grid search for different values of kernel size (2, 3 and 4) and depth (up to the maximal depth for each kernel size) is performed on the dilated network to search the optimal values. Depth and receptive field size are limited by the input segment length and kernel size. The maximum depth is log K (segment length) , e.g. for a network with kernel size 3 and a segment length of 640 samples, the maximum depth is 5, which corresponds to a receptive field size of 3 5 = 243 samples. All models were created in tensorflow (1.14.0) [16] with the keras API [17]. The code used to construct the models is available at https://github.com/exporl/ eeg-matching-eusipco2020.\n\n\nIII. EVALUATION\n\nTo evaluate our models, we recorded EEG data from 48 normal hearing subjects while they listened to audiobooks narrated in Flemish (Dutch). Subjects were screened for normal hearing with a pure-tone audiogram and Flemish MATRIXtest. Stimulus audio was presented binaurally at 62 dBA. Different stimuli were presented per subject, chosen from a set of 10 unique stimuli of roughly the same length (14 minutes and 29 seconds \u00b1 1 minute and 7 seconds). 23, 20, 4 and 1 subjects listened to 8, 7, 6 and 2 stimuli respectively. The order of presentation was randomized for each subject.\n\nEach recording was split into a training, validation and test set containing 80%, 10% and 10% of the recording respectively (see Figure 4). The validation and test set were extracted from the middle of the recording to avoid possible edge effects. All presented models are trained with data across subjects to obtain general subject-independent models.\n\nThe EEG signal and speech stimulus were preprocessed in MATLAB. The envelope of the stimulus was estimated using a gamma-tone filterbank with 28 sub-bands. The envelope for each sub-band was estimated by taking the absolute value of each sample and raising it to the power of 0.6. All subbands were averaged to obtain 1 speech envelope [18]. EEG Test set (10%) Fig. 4. Visualisation of data of 7 first subjects within the dataset. Each rectangle represents a recording. and speech envelopes were downsampled to 1024Hz. A Multi channel Wiener filter was applied to the EEG for artifact rejection [19]. The channels of the EEG were re-referenced to a common average. Both signals were filtered between 0.5 and 32 Hz with a Chebyshev2 filter (order of 2000, 80 dB stopband attenuation and 1 dB passband ripple) and downsampled to 64 Hz.\n\nThe stimuli were presented using a laptop running Windows using the APEX 4 software platform [20] developed at ExpORL, an RME Multiface II sound card and Etymotic ER-3A insert phones which were electromagnetically shielded. The experiments took place in an electromagnetically shielded and soundproofed cabin. The EEG was measured with an Active-Two system from BioSemi, with 64 electrodes at 8kHz sampling rate.\n\n\nIV. RESULTS\n\nClassification accuracy for different kernel sizes K (2, 3 and 4) and depth N are shown in Figure 3. The best configurations of the dilated model outperform the convolutional baseline with 10.8%, 11.9% and 10.6% for kernel size 2, 3 and 4 respectively. All dilated convolutional models presented here outperform the decoder baseline significantly (p < 0.001) (Wilcoxon signed rank test with Holm-Bonferroni correction for multiple comparisons, two tailed, grouping based on subject). The dilated convolutional models also significantly outperform the convolutional baseline for depths bigger than 2 for kernel size 2, and for depths bigger than 1 for kernel size 3 and 4 (all p <0.001). For all 3 kernel sizes, we see an initial increase in match/mismatch accuracy with increasing depth to an optimum. After this optimal depth, an increase in depth has decreased performance.\n\nThe combined results in function of receptive field are displayed in Figure 5. Performance increases as receptive field size increases up to 27 samples (= 420 ms). Increasing the receptive field beyond 27 samples lowers performance with approximately 3%.\n\n\nV. DISCUSSION\n\nThe decoder baseline in our setup has a median performance of 62.3%, which is close to chance level. This low score is partly due to the wider frequency range (0.5 -32Hz) than the one typically used (0.5 -4Hz) for linear models. Another factor 2 (1_2) 3   is that this decoder is trained on all subject data, while better performance is achieved with subject specific decoders [5]. Performance might increase with increasing segment length, typically 30-60 s segments are used with linear models.\n\nThe convolutional baseline outperforms the decoder baseline by 16.5%. This performance increase might be due to the use of a non-linearity in the final layer and end-to-end training. The dilated model significantly outperforms the convolutional baseline for the same integration window length, possibly due to the bigger capacity and non-linear transformations for both speech envelopes and EEG in the dilated model.\n\nAccording to Figure 5, the optimal length of the receptive field appears to be between 16 and 32 time samples (250-500 ms). Previous literature shows that this is longer than the optimal length for the linear decoder, which spans from 0 ms to 75-140 ms [5]. A possible explanation is that the more complex and non-linear dilated network can also model some non-linearity of the later auditory EEG responses.\n\nAs higher accuracy is achieved in shorter timeframes, the proposed dilated convolutional network has more application potential in a diagnostic setting to measure speech intelligibility, as shorter time is needed for testing. This model can also be incorporated in applications like a smart hearing aid (as part of the feedback loop). Another possible application for the dilated network is auditory attention decoding, in which the attended speaker is identified using only the brain response of the subject.\n\nSome additional strategies might improve this model in future work. Input features with more information of the speech signal might be included (e.g., spectrogram, phonemes, word frequency, etc) to achieve higher performance. As our best model achieves more than 90% accuracy, some ceiling effects may occur. We can compensate for these ceiling effects by shortening the segment length and therefore decreasing overall performance per segment.\n\n\nThe work is funded by KU Leuven Special Research Fund C24/18/099 (C2 project to Tom Francart and Hugo Van hamme).Research funded by a PhD grant (1S89620N) of the Research Foundation Flanders (FWO).This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 637424, ERC starting Grant to Tom Francart).\n\nFig. 1 .\n1The structure of the proposed networks\n\nFig. 2 .\n2To ensure similarity to the matched speech envelope segment, the imposter speech envelope segment is extracted 1 second in the future from the time aligned speech envelope segment.\n\nFig. 3 .\n3Results for different kernel sizes. Each point in the boxplot is the score on the test set grouped by subject\n\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nFig. 5 .\n5Results for dilation models with different depths and kernel sizes, ordered by the receptive field size.\n\n\nDilation network for different depths and kernel sizes Ordered by receptive field Decoder baseline median performance CNN baseline median performance(1_3) \n4 (2_2) \n4 (1_4) \n8 (3_2) \n9 (2_3) \n16 (4_2) \n16 (2_4) \n27 (3_3) \n32 (5_2) \n64 (6_2) \n64 (3_4) \n81 (4_3) \n128 (7_2) \n243 (5_3) \n256 (8_2) \n256 (4_4) \n512 (9_2) \nReceptive field in samples (depth_kernelsize) \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAccuracy (%) \n\n\n\nThe Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli. Michael J Crosse, Giovanni M Di Liberto, Adam Bednar, Edmund C Lalor, Frontiers in Human Neuroscience. 10Michael J. Crosse, Giovanni M. Di Liberto, Adam Bednar, and Ed- mund C. Lalor, \"The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continu- ous Stimuli,\" Frontiers in Human Neuroscience, vol. 10, 2016.\n\nNeural responses to uninterrupted natural speech can be extracted with precise temporal resolution. C Edmund, John J Lalor, Foxe, European journal of neuroscience. 311Edmund C Lalor and John J Foxe, \"Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution,\" European journal of neuroscience, vol. 31, no. 1, pp. 189-193, 2010.\n\nEmergence of neural encoding of auditory objects while listening to competing speakers. Nai Ding, Jonathan Z Simon, Proceedings of the National Academy of Sciences. 10929Nai Ding and Jonathan Z. Simon, \"Emergence of neural encoding of auditory objects while listening to competing speakers,\" Proceedings of the National Academy of Sciences, vol. 109, no. 29, pp. 11854-11859, July 2012.\n\nNeural envelope tracking as a measure of speech understanding in cochlear implant users. Eline Verschueren, Ben Somers, Tom Francart, Hearing Research. 373Eline Verschueren, Ben Somers, and Tom Francart, \"Neural envelope tracking as a measure of speech understanding in cochlear implant users,\" Hearing Research, vol. 373, pp. 23-31, Mar. 2019.\n\nSpeech Intelligibility Predicted from Neural Entrainment of the Speech Envelope. Jonas Vanthornhout, Lien Decruy, Jan Wouters, Jonathan Z Simon, Tom Francart, Journal of the Association for Research in Otolaryngology. 192Jonas Vanthornhout, Lien Decruy, Jan Wouters, Jonathan Z. Simon, and Tom Francart, \"Speech Intelligibility Predicted from Neural Entrainment of the Speech Envelope,\" Journal of the Association for Research in Otolaryngology, vol. 19, no. 2, pp. 181-191, Apr. 2018.\n\nData-driven spatial filtering for improved measurement of cortical tracking of multiple representations of speech. Damien Lesenfants, Jonas Vanthornhout, Eline Verschueren, Tom Francart, Journal of Neural Engineering. Damien Lesenfants, Jonas Vanthornhout, Eline Verschueren, and Tom Francart, \"Data-driven spatial filtering for improved measurement of cortical tracking of multiple representations of speech,\" Journal of Neural Engineering, 2019.\n\nEEG can predict speech intelligibility. Ivan Iotzov, Lucas C Parra, Journal of Neural Engineering. 16336008Ivan Iotzov and Lucas C. Parra, \"EEG can predict speech intelligibility,\" Journal of Neural Engineering, vol. 16, no. 3, pp. 036008, Mar. 2019.\n\nTowards reconstructing intelligible speech from the human auditory cortex. Hassan Akbari, Bahar Khalighinejad, Jose L Herrero, Ashesh D Mehta, Nima Mesgarani, Scientific Reports. 91Hassan Akbari, Bahar Khalighinejad, Jose L. Herrero, Ashesh D. Mehta, and Nima Mesgarani, \"Towards reconstructing intelligible speech from the human auditory cortex,\" Scientific Reports, vol. 9, no. 1, pp. 1-12, Jan. 2019.\n\nSpeech Reconstruction from Human Auditory Cortex with Deep Neural Networks. Minda Yang, A Sameer, Catherine A Sheth, Schevon, I I Guy M Mckhann, Nima Mesgarani, INTERSPEECH. 5Minda Yang, Sameer A Sheth, Catherine A Schevon, Guy M McKhann II, and Nima Mesgarani, \"Speech Reconstruction from Human Auditory Cortex with Deep Neural Networks,\" in INTERSPEECH, 2015, p. 5.\n\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. Birger Tobias De Taillez, Bernd T Kollmeier, Meyer, European Journal of Neuroscience. Tobias de Taillez, Birger Kollmeier, and Bernd T. Meyer, \"Machine learning for decoding listeners' attention from electroencephalography evoked by continuous speech,\" European Journal of Neuroscience, Dec. 2017.\n\nDecoding the auditory brain with canonical component analysis. Alain De Cheveign\u00e9, D E Daniel, Giovanni M Wong, Jens Di Liberto, Malcolm Hjortkjaer, Edmund Slaney, Lalor, NeuroImage. 172Alain de Cheveign\u00e9, Daniel D. E. Wong, Giovanni M. Di Liberto, Jens Hjortkjaer, Malcolm Slaney, and Edmund Lalor, \"Decoding the auditory brain with canonical component analysis,\" NeuroImage, vol. 172, pp. 206-216, May 2018.\n\nAccurate Modeling of Brain Responses to Speech. D E Daniel, Giovanni M Wong, Alain Di Liberto, De Cheveign\u00e9, 509307Daniel D. E. Wong, Giovanni M. Di Liberto, and Alain de Cheveign\u00e9, \"Accurate Modeling of Brain Responses to Speech,\" bioRxiv, p. 509307, July 2019.\n\nEEG-based detection of the attended speaker and the locus of auditory attention with convolutional neural networks,\" bioRxiv. Lucas Deckers, Neetha Das, Alexander Amir Hossein Ansari, Tom Bertrand, Francart, 10.1101/475673Lucas Deckers, Neetha Das, Amir Hossein Ansari, Alexander Bertrand, and Tom Francart, \"EEG-based detection of the attended speaker and the locus of auditory attention with convolutional neural networks,\" bioRxiv, Dec. 2018, doi:10.1101/475673.\n\n. Gregory Ciccarelli, Michael Nolan, Joseph Perricone, Paul T Calamia, Stephanie Haro, O&apos; James, Nima Sullivan, Mesgarani, F Thomas, Gregory Ciccarelli, Michael Nolan, Joseph Perricone, Paul T. Calamia, Stephanie Haro, James O'Sullivan, Nima Mesgarani, Thomas F.\n\nComparison of Two-Talker Attention Decoding from EEG with Nonlinear Neural Networks and Linear Methods. Christopher J Quatieri, Smalt, Scientific Reports. 91Quatieri, and Christopher J. Smalt, \"Comparison of Two-Talker Atten- tion Decoding from EEG with Nonlinear Neural Networks and Linear Methods,\" Scientific Reports, vol. 9, no. 1, pp. 1-10, Aug. 2019.\n\nWaveNet: A Generative Model for Raw Audio. Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, arXiv:1609.03499arXiv: 1609.03499Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu, \"WaveNet: A Generative Model for Raw Audio,\" arXiv:1609.03499 [cs], Sept. 2016, arXiv: 1609.03499.\n\nTensorFlow: Large-scale machine learning on heterogeneous systems. Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg ; Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden,Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit SteinerSoftware available from tensorflow.orgMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng, \"TensorFlow: Large-scale machine learning on heterogeneous systems,\" 2015, Software available from tensorflow.org.\n\nKeras. Fran\u00e7ois Chollet, Fran\u00e7ois Chollet et al., \"Keras,\" https://keras.io, 2015.\n\nAuditory-Inspired Speech Envelope Extraction Methods for Improved EEG-Based Auditory Attention Detection in a Cocktail Party Scenario. W Biesmans, N Das, T Francart, A Bertrand, IEEE Transactions on Neural Systems and Rehabilitation Engineering. 255W. Biesmans, N. Das, T. Francart, and A. Bertrand, \"Auditory- Inspired Speech Envelope Extraction Methods for Improved EEG-Based Auditory Attention Detection in a Cocktail Party Scenario,\" IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 5, pp. 402-412, May 2017.\n\nA generic EEG artifact removal algorithm based on the multi-channel Wiener filter. Ben Somers, Tom Francart, Alexander Bertrand, Journal of Neural Engineering. 15336007Ben Somers, Tom Francart, and Alexander Bertrand, \"A generic EEG artifact removal algorithm based on the multi-channel Wiener filter,\" Journal of Neural Engineering, vol. 15, no. 3, pp. 036007, Feb. 2018.\n\nAPEX 3: a multi-purpose test platform for auditory psychophysical experiments. Tom Francart, Astrid Van Wieringen, Jan Wouters, Journal of Neuroscience Methods. 1722Tom Francart, Astrid van Wieringen, and Jan Wouters, \"APEX 3: a multi-purpose test platform for auditory psychophysical experiments,\" Journal of Neuroscience Methods, vol. 172, no. 2, pp. 283-293, July 2008.\n", "annotations": {"author": "[{\"start\":\"107\",\"end\":\"185\"},{\"start\":\"186\",\"end\":\"278\"},{\"start\":\"279\",\"end\":\"292\"},{\"start\":\"293\",\"end\":\"374\"},{\"start\":\"375\",\"end\":\"388\"},{\"start\":\"389\",\"end\":\"396\"},{\"start\":\"397\",\"end\":\"450\"}]", "publisher": null, "author_last_name": "[{\"start\":\"113\",\"end\":\"118\"},{\"start\":\"205\",\"end\":\"211\"},{\"start\":\"284\",\"end\":\"291\"},{\"start\":\"298\",\"end\":\"307\"},{\"start\":\"379\",\"end\":\"387\"},{\"start\":\"389\",\"end\":\"395\"}]", "author_first_name": "[{\"start\":\"107\",\"end\":\"112\"},{\"start\":\"186\",\"end\":\"194\"},{\"start\":\"195\",\"end\":\"204\"},{\"start\":\"279\",\"end\":\"283\"},{\"start\":\"293\",\"end\":\"297\"},{\"start\":\"375\",\"end\":\"378\"}]", "author_affiliation": "[{\"start\":\"120\",\"end\":\"184\"},{\"start\":\"213\",\"end\":\"277\"},{\"start\":\"309\",\"end\":\"373\"},{\"start\":\"398\",\"end\":\"449\"}]", "title": "[{\"start\":\"1\",\"end\":\"104\"},{\"start\":\"451\",\"end\":\"554\"}]", "venue": null, "abstract": "[{\"start\":\"625\",\"end\":\"1886\"}]", "bib_ref": "[{\"start\":\"2446\",\"end\":\"2449\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2451\",\"end\":\"2454\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2573\",\"end\":\"2576\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2578\",\"end\":\"2581\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"2873\",\"end\":\"2876\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3166\",\"end\":\"3169\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3504\",\"end\":\"3507\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"3509\",\"end\":\"3512\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3728\",\"end\":\"3732\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4244\",\"end\":\"4248\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"4250\",\"end\":\"4254\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"4622\",\"end\":\"4626\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"4628\",\"end\":\"4632\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"5700\",\"end\":\"5704\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"5972\",\"end\":\"5975\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"5977\",\"end\":\"5980\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"7819\",\"end\":\"7823\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"10175\",\"end\":\"10179\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"10199\",\"end\":\"10203\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"11604\",\"end\":\"11608\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"11863\",\"end\":\"11867\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"12196\",\"end\":\"12200\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"13932\",\"end\":\"13933\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"14057\",\"end\":\"14060\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"14849\",\"end\":\"14852\",\"attributes\":{\"ref_id\":\"b4\"}}]", "figure": "[{\"start\":\"15960\",\"end\":\"16372\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"16373\",\"end\":\"16422\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"16423\",\"end\":\"16614\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"16615\",\"end\":\"16735\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"16736\",\"end\":\"16975\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"16976\",\"end\":\"17091\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"17092\",\"end\":\"17505\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1905\",\"end\":\"3302\"},{\"start\":\"3304\",\"end\":\"3993\"},{\"start\":\"3995\",\"end\":\"4532\"},{\"start\":\"4534\",\"end\":\"4946\"},{\"start\":\"4948\",\"end\":\"5863\"},{\"start\":\"5879\",\"end\":\"6057\"},{\"start\":\"6063\",\"end\":\"6696\"},{\"start\":\"6698\",\"end\":\"7381\"},{\"start\":\"7383\",\"end\":\"8317\"},{\"start\":\"8319\",\"end\":\"8901\"},{\"start\":\"8903\",\"end\":\"9321\"},{\"start\":\"9323\",\"end\":\"9641\"},{\"start\":\"9643\",\"end\":\"10311\"},{\"start\":\"10331\",\"end\":\"10912\"},{\"start\":\"10914\",\"end\":\"11266\"},{\"start\":\"11268\",\"end\":\"12101\"},{\"start\":\"12103\",\"end\":\"12515\"},{\"start\":\"12531\",\"end\":\"13406\"},{\"start\":\"13408\",\"end\":\"13662\"},{\"start\":\"13680\",\"end\":\"14176\"},{\"start\":\"14178\",\"end\":\"14594\"},{\"start\":\"14596\",\"end\":\"15003\"},{\"start\":\"15005\",\"end\":\"15514\"},{\"start\":\"15516\",\"end\":\"15959\"}]", "formula": "[{\"start\":\"6058\",\"end\":\"6062\",\"attributes\":{\"id\":\"formula_0\"}}]", "table_ref": null, "section_header": "[{\"start\":\"1888\",\"end\":\"1903\"},{\"start\":\"5866\",\"end\":\"5877\"},{\"start\":\"10314\",\"end\":\"10329\"},{\"start\":\"12518\",\"end\":\"12529\"},{\"start\":\"13665\",\"end\":\"13678\"},{\"start\":\"16374\",\"end\":\"16382\"},{\"start\":\"16424\",\"end\":\"16432\"},{\"start\":\"16616\",\"end\":\"16624\"},{\"start\":\"16977\",\"end\":\"16985\"}]", "table": "[{\"start\":\"17243\",\"end\":\"17505\"}]", "figure_caption": "[{\"start\":\"15962\",\"end\":\"16372\"},{\"start\":\"16384\",\"end\":\"16422\"},{\"start\":\"16434\",\"end\":\"16614\"},{\"start\":\"16626\",\"end\":\"16735\"},{\"start\":\"16738\",\"end\":\"16975\"},{\"start\":\"16987\",\"end\":\"17091\"},{\"start\":\"17094\",\"end\":\"17243\"}]", "figure_ref": "[{\"start\":\"5355\",\"end\":\"5363\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"5993\",\"end\":\"6001\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"6048\",\"end\":\"6056\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"7438\",\"end\":\"7446\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"8696\",\"end\":\"8704\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"9268\",\"end\":\"9276\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"11043\",\"end\":\"11052\"},{\"start\":\"11629\",\"end\":\"11635\"},{\"start\":\"12622\",\"end\":\"12630\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"13477\",\"end\":\"13485\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"14609\",\"end\":\"14617\",\"attributes\":{\"ref_id\":\"fig_7\"}}]", "bib_author_first_name": "[{\"start\":\"17635\",\"end\":\"17642\"},{\"start\":\"17643\",\"end\":\"17644\"},{\"start\":\"17653\",\"end\":\"17661\"},{\"start\":\"17662\",\"end\":\"17666\"},{\"start\":\"17676\",\"end\":\"17680\"},{\"start\":\"17689\",\"end\":\"17695\"},{\"start\":\"17696\",\"end\":\"17697\"},{\"start\":\"18100\",\"end\":\"18101\"},{\"start\":\"18110\",\"end\":\"18114\"},{\"start\":\"18115\",\"end\":\"18116\"},{\"start\":\"18459\",\"end\":\"18462\"},{\"start\":\"18469\",\"end\":\"18477\"},{\"start\":\"18478\",\"end\":\"18479\"},{\"start\":\"18848\",\"end\":\"18853\"},{\"start\":\"18867\",\"end\":\"18870\"},{\"start\":\"18879\",\"end\":\"18882\"},{\"start\":\"19186\",\"end\":\"19191\"},{\"start\":\"19206\",\"end\":\"19210\"},{\"start\":\"19219\",\"end\":\"19222\"},{\"start\":\"19232\",\"end\":\"19240\"},{\"start\":\"19241\",\"end\":\"19242\"},{\"start\":\"19250\",\"end\":\"19253\"},{\"start\":\"19707\",\"end\":\"19713\"},{\"start\":\"19726\",\"end\":\"19731\"},{\"start\":\"19746\",\"end\":\"19751\"},{\"start\":\"19765\",\"end\":\"19768\"},{\"start\":\"20081\",\"end\":\"20085\"},{\"start\":\"20094\",\"end\":\"20099\"},{\"start\":\"20100\",\"end\":\"20101\"},{\"start\":\"20368\",\"end\":\"20374\"},{\"start\":\"20383\",\"end\":\"20388\"},{\"start\":\"20404\",\"end\":\"20408\"},{\"start\":\"20409\",\"end\":\"20410\"},{\"start\":\"20420\",\"end\":\"20426\"},{\"start\":\"20427\",\"end\":\"20428\"},{\"start\":\"20436\",\"end\":\"20440\"},{\"start\":\"20774\",\"end\":\"20779\"},{\"start\":\"20786\",\"end\":\"20787\"},{\"start\":\"20796\",\"end\":\"20805\"},{\"start\":\"20806\",\"end\":\"20807\"},{\"start\":\"20824\",\"end\":\"20825\"},{\"start\":\"20826\",\"end\":\"20827\"},{\"start\":\"20843\",\"end\":\"20847\"},{\"start\":\"21175\",\"end\":\"21181\"},{\"start\":\"21201\",\"end\":\"21206\"},{\"start\":\"21207\",\"end\":\"21208\"},{\"start\":\"21537\",\"end\":\"21542\"},{\"start\":\"21557\",\"end\":\"21558\"},{\"start\":\"21559\",\"end\":\"21560\"},{\"start\":\"21569\",\"end\":\"21577\"},{\"start\":\"21578\",\"end\":\"21579\"},{\"start\":\"21586\",\"end\":\"21590\"},{\"start\":\"21603\",\"end\":\"21610\"},{\"start\":\"21623\",\"end\":\"21629\"},{\"start\":\"21933\",\"end\":\"21934\"},{\"start\":\"21935\",\"end\":\"21936\"},{\"start\":\"21945\",\"end\":\"21953\"},{\"start\":\"21954\",\"end\":\"21955\"},{\"start\":\"21962\",\"end\":\"21967\"},{\"start\":\"22275\",\"end\":\"22280\"},{\"start\":\"22290\",\"end\":\"22296\"},{\"start\":\"22302\",\"end\":\"22311\"},{\"start\":\"22333\",\"end\":\"22336\"},{\"start\":\"22618\",\"end\":\"22625\"},{\"start\":\"22638\",\"end\":\"22645\"},{\"start\":\"22653\",\"end\":\"22659\"},{\"start\":\"22671\",\"end\":\"22675\"},{\"start\":\"22676\",\"end\":\"22677\"},{\"start\":\"22687\",\"end\":\"22696\"},{\"start\":\"22703\",\"end\":\"22710\"},{\"start\":\"22718\",\"end\":\"22722\"},{\"start\":\"22744\",\"end\":\"22745\"},{\"start\":\"22989\",\"end\":\"23000\"},{\"start\":\"23001\",\"end\":\"23002\"},{\"start\":\"23286\",\"end\":\"23291\"},{\"start\":\"23306\",\"end\":\"23312\"},{\"start\":\"23323\",\"end\":\"23328\"},{\"start\":\"23334\",\"end\":\"23339\"},{\"start\":\"23350\",\"end\":\"23355\"},{\"start\":\"23365\",\"end\":\"23369\"},{\"start\":\"23378\",\"end\":\"23381\"},{\"start\":\"23396\",\"end\":\"23402\"},{\"start\":\"23411\",\"end\":\"23416\"},{\"start\":\"23778\",\"end\":\"23784\"},{\"start\":\"23792\",\"end\":\"23798\"},{\"start\":\"23808\",\"end\":\"23812\"},{\"start\":\"23821\",\"end\":\"23827\"},{\"start\":\"23836\",\"end\":\"23843\"},{\"start\":\"23850\",\"end\":\"23855\"},{\"start\":\"23863\",\"end\":\"23867\"},{\"start\":\"23868\",\"end\":\"23869\"},{\"start\":\"23879\",\"end\":\"23883\"},{\"start\":\"23891\",\"end\":\"23898\"},{\"start\":\"23905\",\"end\":\"23913\"},{\"start\":\"23921\",\"end\":\"23927\"},{\"start\":\"23938\",\"end\":\"23941\"},{\"start\":\"23954\",\"end\":\"23960\"},{\"start\":\"23967\",\"end\":\"23975\"},{\"start\":\"23984\",\"end\":\"23991\"},{\"start\":\"23999\",\"end\":\"24007\"},{\"start\":\"24013\",\"end\":\"24018\"},{\"start\":\"24031\",\"end\":\"24037\"},{\"start\":\"24046\",\"end\":\"24055\"},{\"start\":\"24064\",\"end\":\"24068\"},{\"start\":\"24100\",\"end\":\"24106\"},{\"start\":\"24114\",\"end\":\"24118\"},{\"start\":\"24123\",\"end\":\"24132\"},{\"start\":\"25134\",\"end\":\"25142\"},{\"start\":\"25346\",\"end\":\"25347\"},{\"start\":\"25358\",\"end\":\"25359\"},{\"start\":\"25365\",\"end\":\"25366\"},{\"start\":\"25377\",\"end\":\"25378\"},{\"start\":\"25840\",\"end\":\"25843\"},{\"start\":\"25852\",\"end\":\"25855\"},{\"start\":\"25866\",\"end\":\"25875\"},{\"start\":\"26210\",\"end\":\"26213\"},{\"start\":\"26224\",\"end\":\"26230\"},{\"start\":\"26246\",\"end\":\"26249\"}]", "bib_author_last_name": "[{\"start\":\"17645\",\"end\":\"17651\"},{\"start\":\"17667\",\"end\":\"17674\"},{\"start\":\"17681\",\"end\":\"17687\"},{\"start\":\"17698\",\"end\":\"17703\"},{\"start\":\"18102\",\"end\":\"18108\"},{\"start\":\"18117\",\"end\":\"18122\"},{\"start\":\"18124\",\"end\":\"18128\"},{\"start\":\"18463\",\"end\":\"18467\"},{\"start\":\"18480\",\"end\":\"18485\"},{\"start\":\"18854\",\"end\":\"18865\"},{\"start\":\"18871\",\"end\":\"18877\"},{\"start\":\"18883\",\"end\":\"18891\"},{\"start\":\"19192\",\"end\":\"19204\"},{\"start\":\"19211\",\"end\":\"19217\"},{\"start\":\"19223\",\"end\":\"19230\"},{\"start\":\"19243\",\"end\":\"19248\"},{\"start\":\"19254\",\"end\":\"19262\"},{\"start\":\"19714\",\"end\":\"19724\"},{\"start\":\"19732\",\"end\":\"19744\"},{\"start\":\"19752\",\"end\":\"19763\"},{\"start\":\"19769\",\"end\":\"19777\"},{\"start\":\"20086\",\"end\":\"20092\"},{\"start\":\"20102\",\"end\":\"20107\"},{\"start\":\"20375\",\"end\":\"20381\"},{\"start\":\"20389\",\"end\":\"20402\"},{\"start\":\"20411\",\"end\":\"20418\"},{\"start\":\"20429\",\"end\":\"20434\"},{\"start\":\"20441\",\"end\":\"20450\"},{\"start\":\"20780\",\"end\":\"20784\"},{\"start\":\"20788\",\"end\":\"20794\"},{\"start\":\"20808\",\"end\":\"20813\"},{\"start\":\"20815\",\"end\":\"20822\"},{\"start\":\"20828\",\"end\":\"20841\"},{\"start\":\"20848\",\"end\":\"20857\"},{\"start\":\"21182\",\"end\":\"21199\"},{\"start\":\"21209\",\"end\":\"21218\"},{\"start\":\"21220\",\"end\":\"21225\"},{\"start\":\"21543\",\"end\":\"21555\"},{\"start\":\"21561\",\"end\":\"21567\"},{\"start\":\"21580\",\"end\":\"21584\"},{\"start\":\"21591\",\"end\":\"21601\"},{\"start\":\"21611\",\"end\":\"21621\"},{\"start\":\"21630\",\"end\":\"21636\"},{\"start\":\"21638\",\"end\":\"21643\"},{\"start\":\"21937\",\"end\":\"21943\"},{\"start\":\"21956\",\"end\":\"21960\"},{\"start\":\"21968\",\"end\":\"21978\"},{\"start\":\"21980\",\"end\":\"21992\"},{\"start\":\"22281\",\"end\":\"22288\"},{\"start\":\"22297\",\"end\":\"22300\"},{\"start\":\"22312\",\"end\":\"22331\"},{\"start\":\"22337\",\"end\":\"22345\"},{\"start\":\"22347\",\"end\":\"22355\"},{\"start\":\"22626\",\"end\":\"22636\"},{\"start\":\"22646\",\"end\":\"22651\"},{\"start\":\"22660\",\"end\":\"22669\"},{\"start\":\"22678\",\"end\":\"22685\"},{\"start\":\"22697\",\"end\":\"22701\"},{\"start\":\"22711\",\"end\":\"22716\"},{\"start\":\"22723\",\"end\":\"22731\"},{\"start\":\"22733\",\"end\":\"22742\"},{\"start\":\"22746\",\"end\":\"22752\"},{\"start\":\"23003\",\"end\":\"23011\"},{\"start\":\"23013\",\"end\":\"23018\"},{\"start\":\"23292\",\"end\":\"23304\"},{\"start\":\"23313\",\"end\":\"23321\"},{\"start\":\"23329\",\"end\":\"23332\"},{\"start\":\"23340\",\"end\":\"23348\"},{\"start\":\"23356\",\"end\":\"23363\"},{\"start\":\"23370\",\"end\":\"23376\"},{\"start\":\"23382\",\"end\":\"23394\"},{\"start\":\"23403\",\"end\":\"23409\"},{\"start\":\"23417\",\"end\":\"23428\"},{\"start\":\"23785\",\"end\":\"23790\"},{\"start\":\"23799\",\"end\":\"23806\"},{\"start\":\"23813\",\"end\":\"23819\"},{\"start\":\"23828\",\"end\":\"23834\"},{\"start\":\"23844\",\"end\":\"23848\"},{\"start\":\"23856\",\"end\":\"23861\"},{\"start\":\"23870\",\"end\":\"23877\"},{\"start\":\"23884\",\"end\":\"23889\"},{\"start\":\"23899\",\"end\":\"23903\"},{\"start\":\"23914\",\"end\":\"23919\"},{\"start\":\"23928\",\"end\":\"23936\"},{\"start\":\"23942\",\"end\":\"23952\"},{\"start\":\"23961\",\"end\":\"23965\"},{\"start\":\"23976\",\"end\":\"23982\"},{\"start\":\"23992\",\"end\":\"23997\"},{\"start\":\"24008\",\"end\":\"24011\"},{\"start\":\"24019\",\"end\":\"24029\"},{\"start\":\"24038\",\"end\":\"24044\"},{\"start\":\"24056\",\"end\":\"24062\"},{\"start\":\"24069\",\"end\":\"24098\"},{\"start\":\"24107\",\"end\":\"24112\"},{\"start\":\"24119\",\"end\":\"24121\"},{\"start\":\"24133\",\"end\":\"24138\"},{\"start\":\"25143\",\"end\":\"25150\"},{\"start\":\"25348\",\"end\":\"25356\"},{\"start\":\"25360\",\"end\":\"25363\"},{\"start\":\"25367\",\"end\":\"25375\"},{\"start\":\"25379\",\"end\":\"25387\"},{\"start\":\"25844\",\"end\":\"25850\"},{\"start\":\"25856\",\"end\":\"25864\"},{\"start\":\"25876\",\"end\":\"25884\"},{\"start\":\"26214\",\"end\":\"26222\"},{\"start\":\"26231\",\"end\":\"26244\"},{\"start\":\"26250\",\"end\":\"26257\"}]", "bib_entry": "[{\"start\":\"17507\",\"end\":\"17998\",\"attributes\":{\"matched_paper_id\":\"12165026\",\"id\":\"b0\"}},{\"start\":\"18000\",\"end\":\"18369\",\"attributes\":{\"matched_paper_id\":\"6699749\",\"id\":\"b1\"}},{\"start\":\"18371\",\"end\":\"18757\",\"attributes\":{\"matched_paper_id\":\"15570759\",\"id\":\"b2\"}},{\"start\":\"18759\",\"end\":\"19103\",\"attributes\":{\"matched_paper_id\":\"56147098\",\"id\":\"b3\"}},{\"start\":\"19105\",\"end\":\"19590\",\"attributes\":{\"matched_paper_id\":\"3490315\",\"id\":\"b4\"}},{\"start\":\"19592\",\"end\":\"20039\",\"attributes\":{\"matched_paper_id\":\"92419290\",\"id\":\"b5\"}},{\"start\":\"20041\",\"end\":\"20291\",\"attributes\":{\"matched_paper_id\":\"73491790\",\"id\":\"b6\"}},{\"start\":\"20293\",\"end\":\"20696\",\"attributes\":{\"matched_paper_id\":\"59409775\",\"id\":\"b7\"}},{\"start\":\"20698\",\"end\":\"21065\",\"attributes\":{\"matched_paper_id\":\"20480282\",\"id\":\"b8\"}},{\"start\":\"21067\",\"end\":\"21472\",\"attributes\":{\"matched_paper_id\":\"5275290\",\"id\":\"b9\"}},{\"start\":\"21474\",\"end\":\"21883\",\"attributes\":{\"matched_paper_id\":\"3531854\",\"id\":\"b10\"}},{\"start\":\"21885\",\"end\":\"22147\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"22149\",\"end\":\"22614\",\"attributes\":{\"id\":\"b12\",\"doi\":\"10.1101/475673\"}},{\"start\":\"22616\",\"end\":\"22883\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"22885\",\"end\":\"23241\",\"attributes\":{\"matched_paper_id\":\"91320213\",\"id\":\"b14\"}},{\"start\":\"23243\",\"end\":\"23709\",\"attributes\":{\"id\":\"b15\",\"doi\":\"arXiv:1609.03499\"}},{\"start\":\"23711\",\"end\":\"25125\",\"attributes\":{\"id\":\"b16\"}},{\"start\":\"25127\",\"end\":\"25209\",\"attributes\":{\"id\":\"b17\"}},{\"start\":\"25211\",\"end\":\"25755\",\"attributes\":{\"matched_paper_id\":\"27835175\",\"id\":\"b18\"}},{\"start\":\"25757\",\"end\":\"26129\",\"attributes\":{\"matched_paper_id\":\"10736717\",\"id\":\"b19\"}},{\"start\":\"26131\",\"end\":\"26503\",\"attributes\":{\"matched_paper_id\":\"41858699\",\"id\":\"b20\"}}]", "bib_title": "[{\"start\":\"17507\",\"end\":\"17633\"},{\"start\":\"18000\",\"end\":\"18098\"},{\"start\":\"18371\",\"end\":\"18457\"},{\"start\":\"18759\",\"end\":\"18846\"},{\"start\":\"19105\",\"end\":\"19184\"},{\"start\":\"19592\",\"end\":\"19705\"},{\"start\":\"20041\",\"end\":\"20079\"},{\"start\":\"20293\",\"end\":\"20366\"},{\"start\":\"20698\",\"end\":\"20772\"},{\"start\":\"21067\",\"end\":\"21173\"},{\"start\":\"21474\",\"end\":\"21535\"},{\"start\":\"22885\",\"end\":\"22987\"},{\"start\":\"25211\",\"end\":\"25344\"},{\"start\":\"25757\",\"end\":\"25838\"},{\"start\":\"26131\",\"end\":\"26208\"}]", "bib_author": "[{\"start\":\"17635\",\"end\":\"17653\"},{\"start\":\"17653\",\"end\":\"17676\"},{\"start\":\"17676\",\"end\":\"17689\"},{\"start\":\"17689\",\"end\":\"17705\"},{\"start\":\"18100\",\"end\":\"18110\"},{\"start\":\"18110\",\"end\":\"18124\"},{\"start\":\"18124\",\"end\":\"18130\"},{\"start\":\"18459\",\"end\":\"18469\"},{\"start\":\"18469\",\"end\":\"18487\"},{\"start\":\"18848\",\"end\":\"18867\"},{\"start\":\"18867\",\"end\":\"18879\"},{\"start\":\"18879\",\"end\":\"18893\"},{\"start\":\"19186\",\"end\":\"19206\"},{\"start\":\"19206\",\"end\":\"19219\"},{\"start\":\"19219\",\"end\":\"19232\"},{\"start\":\"19232\",\"end\":\"19250\"},{\"start\":\"19250\",\"end\":\"19264\"},{\"start\":\"19707\",\"end\":\"19726\"},{\"start\":\"19726\",\"end\":\"19746\"},{\"start\":\"19746\",\"end\":\"19765\"},{\"start\":\"19765\",\"end\":\"19779\"},{\"start\":\"20081\",\"end\":\"20094\"},{\"start\":\"20094\",\"end\":\"20109\"},{\"start\":\"20368\",\"end\":\"20383\"},{\"start\":\"20383\",\"end\":\"20404\"},{\"start\":\"20404\",\"end\":\"20420\"},{\"start\":\"20420\",\"end\":\"20436\"},{\"start\":\"20436\",\"end\":\"20452\"},{\"start\":\"20774\",\"end\":\"20786\"},{\"start\":\"20786\",\"end\":\"20796\"},{\"start\":\"20796\",\"end\":\"20815\"},{\"start\":\"20815\",\"end\":\"20824\"},{\"start\":\"20824\",\"end\":\"20843\"},{\"start\":\"20843\",\"end\":\"20859\"},{\"start\":\"21175\",\"end\":\"21201\"},{\"start\":\"21201\",\"end\":\"21220\"},{\"start\":\"21220\",\"end\":\"21227\"},{\"start\":\"21537\",\"end\":\"21557\"},{\"start\":\"21557\",\"end\":\"21569\"},{\"start\":\"21569\",\"end\":\"21586\"},{\"start\":\"21586\",\"end\":\"21603\"},{\"start\":\"21603\",\"end\":\"21623\"},{\"start\":\"21623\",\"end\":\"21638\"},{\"start\":\"21638\",\"end\":\"21645\"},{\"start\":\"21933\",\"end\":\"21945\"},{\"start\":\"21945\",\"end\":\"21962\"},{\"start\":\"21962\",\"end\":\"21980\"},{\"start\":\"21980\",\"end\":\"21994\"},{\"start\":\"22275\",\"end\":\"22290\"},{\"start\":\"22290\",\"end\":\"22302\"},{\"start\":\"22302\",\"end\":\"22333\"},{\"start\":\"22333\",\"end\":\"22347\"},{\"start\":\"22347\",\"end\":\"22357\"},{\"start\":\"22618\",\"end\":\"22638\"},{\"start\":\"22638\",\"end\":\"22653\"},{\"start\":\"22653\",\"end\":\"22671\"},{\"start\":\"22671\",\"end\":\"22687\"},{\"start\":\"22687\",\"end\":\"22703\"},{\"start\":\"22703\",\"end\":\"22718\"},{\"start\":\"22718\",\"end\":\"22733\"},{\"start\":\"22733\",\"end\":\"22744\"},{\"start\":\"22744\",\"end\":\"22754\"},{\"start\":\"22989\",\"end\":\"23013\"},{\"start\":\"23013\",\"end\":\"23020\"},{\"start\":\"23286\",\"end\":\"23306\"},{\"start\":\"23306\",\"end\":\"23323\"},{\"start\":\"23323\",\"end\":\"23334\"},{\"start\":\"23334\",\"end\":\"23350\"},{\"start\":\"23350\",\"end\":\"23365\"},{\"start\":\"23365\",\"end\":\"23378\"},{\"start\":\"23378\",\"end\":\"23396\"},{\"start\":\"23396\",\"end\":\"23411\"},{\"start\":\"23411\",\"end\":\"23430\"},{\"start\":\"23778\",\"end\":\"23792\"},{\"start\":\"23792\",\"end\":\"23808\"},{\"start\":\"23808\",\"end\":\"23821\"},{\"start\":\"23821\",\"end\":\"23836\"},{\"start\":\"23836\",\"end\":\"23850\"},{\"start\":\"23850\",\"end\":\"23863\"},{\"start\":\"23863\",\"end\":\"23879\"},{\"start\":\"23879\",\"end\":\"23891\"},{\"start\":\"23891\",\"end\":\"23905\"},{\"start\":\"23905\",\"end\":\"23921\"},{\"start\":\"23921\",\"end\":\"23938\"},{\"start\":\"23938\",\"end\":\"23954\"},{\"start\":\"23954\",\"end\":\"23967\"},{\"start\":\"23967\",\"end\":\"23984\"},{\"start\":\"23984\",\"end\":\"23999\"},{\"start\":\"23999\",\"end\":\"24013\"},{\"start\":\"24013\",\"end\":\"24031\"},{\"start\":\"24031\",\"end\":\"24046\"},{\"start\":\"24046\",\"end\":\"24064\"},{\"start\":\"24064\",\"end\":\"24100\"},{\"start\":\"24100\",\"end\":\"24114\"},{\"start\":\"24114\",\"end\":\"24123\"},{\"start\":\"24123\",\"end\":\"24140\"},{\"start\":\"25134\",\"end\":\"25152\"},{\"start\":\"25346\",\"end\":\"25358\"},{\"start\":\"25358\",\"end\":\"25365\"},{\"start\":\"25365\",\"end\":\"25377\"},{\"start\":\"25377\",\"end\":\"25389\"},{\"start\":\"25840\",\"end\":\"25852\"},{\"start\":\"25852\",\"end\":\"25866\"},{\"start\":\"25866\",\"end\":\"25886\"},{\"start\":\"26210\",\"end\":\"26224\"},{\"start\":\"26224\",\"end\":\"26246\"},{\"start\":\"26246\",\"end\":\"26259\"}]", "bib_venue": "[{\"start\":\"17705\",\"end\":\"17736\"},{\"start\":\"18130\",\"end\":\"18162\"},{\"start\":\"18487\",\"end\":\"18534\"},{\"start\":\"18893\",\"end\":\"18909\"},{\"start\":\"19264\",\"end\":\"19321\"},{\"start\":\"19779\",\"end\":\"19808\"},{\"start\":\"20109\",\"end\":\"20138\"},{\"start\":\"20452\",\"end\":\"20470\"},{\"start\":\"20859\",\"end\":\"20870\"},{\"start\":\"21227\",\"end\":\"21259\"},{\"start\":\"21645\",\"end\":\"21655\"},{\"start\":\"21885\",\"end\":\"21931\"},{\"start\":\"22149\",\"end\":\"22273\"},{\"start\":\"23020\",\"end\":\"23038\"},{\"start\":\"23243\",\"end\":\"23284\"},{\"start\":\"23711\",\"end\":\"23776\"},{\"start\":\"25127\",\"end\":\"25132\"},{\"start\":\"25389\",\"end\":\"25455\"},{\"start\":\"25886\",\"end\":\"25915\"},{\"start\":\"26259\",\"end\":\"26290\"}]"}}}, "year": 2023, "month": 12, "day": 17}