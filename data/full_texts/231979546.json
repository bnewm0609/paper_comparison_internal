{"id": 231979546, "updated": "2023-10-06 05:40:57.162", "metadata": {"title": "Interest-aware Message-Passing GCN for Recommendation", "authors": "[{\"first\":\"Fan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Zhiyong\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Zan\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Liqiang\",\"last\":\"Nie\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the Web Conference 2021", "publication_date": {"year": 2021, "month": 2, "day": 19}, "abstract": "Graph Convolution Networks (GCNs) manifest great potential in recommendation. This is attributed to their capability on learning good user and item embeddings by exploiting the collaborative signals from the high-order neighbors. Like other GCN models, the GCN based recommendation models also suffer from the notorious over-smoothing problem - when stacking more layers, node embeddings become more similar and eventually indistinguishable, resulted in performance degradation. The recently proposed LightGCN and LR-GCN alleviate this problem to some extent, however, we argue that they overlook an important factor for the over-smoothing problem in recommendation, that is, high-order neighboring users with no common interests of a user can be also involved in the user's embedding learning in the graph convolution operation. As a result, the multi-layer graph convolution will make users with dissimilar interests have similar embeddings. In this paper, we propose a novel Interest-aware Message-Passing GCN (IMP-GCN) recommendation model, which performs high-order graph convolution inside subgraphs. The subgraph consists of users with similar interests and their interacted items. To form the subgraphs, we design an unsupervised subgraph generation module, which can effectively identify users with common interests by exploiting both user feature and graph structure. To this end, our model can avoid propagating negative information from high-order neighbors into embedding learning. Experimental results on three large-scale benchmark datasets show that our model can gain performance improvement by stacking more layers and outperform the state-of-the-art GCN-based recommendation models significantly.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.10044", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/www/LiuCZGN21", "doi": "10.1145/3442381.3449986"}}, "content": {"source": {"pdf_hash": "a7febdc89a0235b7e944512a7e5358e268c8e1c3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.10044v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2102.10044", "status": "GREEN"}}, "grobid": {"id": "34dde06354f5bb4a53fd261a42190851f86bcabc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a7febdc89a0235b7e944512a7e5358e268c8e1c3.txt", "contents": "\nInterest-aware Message-Passing GCN for Recommendation\nApril 19-23, 2021. April 19-23, 2021\n\nFan Liu liufancs@gmail.com \nZhiyong Cheng \nShandong Artificial Intelligence Institute\nShandong Academy of Sciences)\nSchool of Information Science and Engineering\nQilu University of Technology\nShandong Normal University\n\n\nLei Zhu \nZan Gao \nShandong Artificial Intelligence Institute\nShandong Academy of Sciences)\nSchool of Information Science and Engineering\nQilu University of Technology\nShandong Normal University\n\n\nLiqiang Nie nieliqiang@gmail.com \nFan Liu \nZhiyong Cheng \nShandong Artificial Intelligence Institute\nShandong Academy of Sciences)\nSchool of Information Science and Engineering\nQilu University of Technology\nShandong Normal University\n\n\nLei Zhu \nZan Gao \nShandong Artificial Intelligence Institute\nShandong Academy of Sciences)\nSchool of Information Science and Engineering\nQilu University of Technology\nShandong Normal University\n\n\nLiqiang Nie \n\nSchool of Computer Science and Technology\nShandong University\n\n\nInterest-aware Message-Passing GCN for Recommendation\n\nSlovenia \u00a9 2021 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. of the Web Conference 2021 (WWW '21)\nLjubljana; Ljubljana, SloveniaApril 19-23, 2021. April 19-23, 202110.1145/3442381.3449986ACM Reference Format: 2021. Interest-aware Message-Passing GCN for Recommendation. In Proceedings * Corresponding author: Zhiyong Cheng and Liqiang Nie. This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. ACM, New York, NY, USA, 10 pages. https://RecommendationGraph Convolution NetworksMessage-Passing StrategyInterest-awareSubgraph\nGraph Convolution Networks (GCNs) manifest great potential in recommendation. This is attributed to their capability on learning good user and item embeddings by exploiting the collaborative signals from the high-order neighbors. Like other GCN models, the GCN based recommendation models also suffer from the notorious over-smoothing problem -when stacking more layers, node embeddings become more similar and eventually indistinguishable, resulted in performance degradation. The recently proposed LightGCN and LR-GCN alleviate this problem to some extent, however, we argue that they overlook an important factor for the over-smoothing problem in recommendation, that is, high-order neighboring users with no common interests of a user can be also involved in the user's embedding learning in the graph convolution operation. As a result, the multi-layer graph convolution will make users with dissimilar interests have similar embeddings. In this paper, we propose a novel Interest-aware Message-Passing GCN (IMP-GCN) recommendation model, which performs high-order graph convolution inside subgraphs. The subgraph consists of users with similar interests and their interacted items. To form the subgraphs, we design an unsupervised subgraph generation module, which can effectively identify users with common interests by exploiting both user feature and graph structure. To this end, our model can avoid propagating negative information from high-order neighbors into embedding learning. Experimental results on three large-scale benchmark datasets show that our model can gain performance improvement by stacking more layers and outperform the state-of-the-art GCN-based recommendation models significantly.CCS CONCEPTS\u2022 Information systems \u2192 Recommender systems.\n\nINTRODUCTION\n\nRecommendation system has become one of the most important techniques for various online platforms. It can not only provide personalized information for an specific user from overwhelming information, but also increase the revenue for service providers. Among them, Collaborative filtering (CF) based models [1,15,20,38,41] have made substantial progress in learning user and item representations by modeling historical user-item interactions. For example, matrix factorization (MF) can directly embed user/item as a feature vector and model the user-item interactions with inner product [1]. Neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks to learn better user and item representations [15].\n\nRecently, GCN-based models [14,21,29,32,33] have achieved great success in recommendation due to the powerful capability on representation learning from non-Euclidean structure. The core of GCN-based models is to iteratively aggregate feature information from local graph neighbors. It has been proved to be an efficient way to distill additional information from graph structure, and thus improves user and item representation learning and alleviates the sparse problem. For example, NGCF [33] has proved that exploiting high-order connectivity can help alleviate the sparsity problem in recommendation. However, it is also well-recognized that GCNs suffer from the over-smoothing problem [33], because the graph convolution operation is actually a special kind of graph Laplacian smoothing [33], making node representations become indistinguishable after multi-layer graph convolution [40]. As a result, most current GCN based models obtain their peak performance by stacking only few layers (e.g., 2 or 3 layers), and continuing increasing the depth will lead to sharp performance degradation. In the domain of recommendation, Chen et al. [3] have empirically demonstrated that the user/item embeddings become more similar when stacking more layers in NGCF due to the over-smoothing effect. In other words, the preferences of different users become homogeneous, resulted in performance degradation in recommendation. Based on the observations, they proposed a LR-GCN model, which removes the non-linearities in GCNs to simply the network structure and introduced a residual network structure to alleviate the over-smoothing problem, achieving substantially improvement over NGCF on recommendation accuracy.\n\nIt is worth mentioning that the LightGCN proposed by He et al. [14] has a similar formulation as LR-GCN. With careful experimental studies, He et al. pointed out that the feature transformation and nonlinear activation have no positive effect (or even negative effect due to the increase of training difficult) to the final performance. Therefore, they only keep the neighborhood aggregation in the LightGCN for collaborative filtering. Comparing to LR-GCN, LightGCN further removes the \"self-loop\" in the aggregation operation. Although LightGCN is not dedicatedly designed for tacking the over-smoothing problem, it has almost the same formulation as LR-GCN and thus can also alleviate the over-smoothing problem to some extent. In fact, both LR-GCN and LightGCN are consistent with the recent theories in simplifying GCNs [37] and can obtain the best performance with a deeper structure (e.g., 4 layers). Despite the two success GCN based models are designed for recommendation, we argue that they still design the model from the perspective of graph convolution, while have not well considered the over-smoothing problem in the domain of recommendation.\n\nThe GCN based recommendation model is built upon a useritem graph, in which the user and item are linked according to the historical user-item interactions. The user embedding is learned by iteratively aggregating messages passed from the neighboring (both user and item) nodes. Note that the passed messages are distilled from the embeddings of neighboring nodes. When stacking layers, the information from the -order neighbors, which are indirectly connected via items and users, are also involved in the embedding learning of a target node. An underlying assumption is that the collaborative signals from high-order neighbors are beneficial to the embedding learning. However, not all the information from high-order neighbors are positive in reality. In the user-item interaction graph, the high-order neighboring users could have no common or even contradictory interest with a target user. This is highly possible, especially when the graph is constructed based on implicit feedbacks (e.g., click). In fact, the implicit feedback is more widely used over the explicit feedbacks in modern recommendation systems. The core idea behind collaborative filtering is that similar users like similar items. Therefore, the collaborative signals that we would like to exploit should be from similar users (i.e., users with similar interests). However, existing GCN-based recommendation models have not distinguished the high-order neighbors, and just simply aggregate the messages from all those neighbors to update user embeddings. As a result, the embeddings of dissimilar users are also involved in the embedding learning of a target user, negatively affecting the performance. This is also a reason of the over-smoothing effect in the GCN-based recommendation models -making the embeddings of dissimilar users to be similar.\n\nMotivated by the above considerations, in this paper, we propose a novel Interest-aware Messaging-Passing GCN (IMP-GCN) recommendation model, which groups users and their interacted items into different subgraphs and operates high-order graph convolutions inside subgraphs. More specific, we adopt the simplified network structure of LightGCN, as its effectiveness has been well demonstrated in [14] and it can alleviate the over-smoothing problem to some extent. The first-order graph convolution is the same as that of LightGCN. For the high-order graph convolution, only the messages from nodes in the same subgraph are exploited to learn the node embeddings. The subgraph is generated by a proposed graph generation module, which integrates users features and graph structure to identify users with similar interests, and then constructs the subgraphs by retaining those users and their interacted items.\n\nTo this end, our model can filter out the negative information propagation in the high-order graph convolution operations for the embedding learning, and thus can keep the uniqueness of users by stacking more graph convolution layers. Extensive experiments have been conducted on three large-scale real-world datasets to validate the effectiveness of our model. Results show that our model outperforms the state-of-the-art methods by a large margin and can obtain better performance with more layers (till 7 layers) 1 . This indicates that our model can benefits from higher-order neighbors by excluding negative nodes. Besides, with deep analysis on the results, we found that the negative information in the embedding propagation is the major reason for the performance degradation of existing GCN-based recommendation models in deep structure. We released the codes and involved parameter settings to facilitate others to repeat this work 2 .\n\nIn summary, the main contributes of this work are as follows: \u2022 We step into the over-smoothing problem in existing GCN-based recommendation models and point out an overlooked factor: exploiting high-order neighbors indiscriminately makes the embeddings of users with dissimilar interests to be similar. \u2022 We propose an IMP-GCN model which exploits high-order neighbors from the same subgraph, in which the user nodes share more similar interests than those in other subgraphs. It is proved to be effective on alleviating the over-smoothing problem. \u2022 We design a subgraph generation module to group users and generate subgraphs from the user-item bipartite graph by considering users features and graph structure information. \u2022 We conduct empirical studies on three benchmark datasets to evaluate the proposed IPM-GCN model. Results show that IPM-GCN can gain improvement by stacking more layers and learn better user/item embeddings, and thus outperforms the SOTA GCN-based recommendation models with a large margin.\n\n\nMETHODOLOGY 2.1 Recap\n\nLet \u2208 R \u00d7 be the user-item interaction matrix, where and indicate the number of users and items, respectively. An nonzero entry \u2208 indicates that user \u2208 U has interacted with item \u2208 I before; otherwise, the entry is zero. A user-item bipartite graph G = (W, E) can be constructed based on the interaction matrix, where the node set W consists of the two types of user nodes and item nodes and E represents for the set of edges. For a nonzero , there is an edge between the user and item . The above information is taken as the input of GCN model to learn the user and item representations by iteratively aggregating features from neighboring nodes in the bipartite graph.\n\nHere we take LightGCN as an example to describe the GCN-based recommendation model, because it achieves the state-of-the-art performance with a very light design. Our model is also developed based on its design. 3 Let operation in LightGCN is described as follows:\n( ) = \u2211\ufe01 \u2208N 1 \u221a\ufe01 |N | \u221a\ufe01 |N | ( \u22121) , ( ) = \u2211\ufe01 \u2208N 1 \u221a\ufe01 |N | \u221a\ufe01 |N | ( \u22121) ,(1)\nwhere ( ) and ( ) represent the embeddings of the user and item after layers propagation, respectively; N denotes the set of items that interact with user , and N denotes the set of users that interact with item ;\n1 \u221a | N | \u221a | N |\nis symmetric normalization terms, which can avoid the scale of embeddings increasing with graph convolution operations [18]. After K layers graph convolution, the final embeddings of a user and an item are the combination of their embeddings obtained at each layer in LightGCN:\n= \u2211\ufe01 =0 ( ) ; = \u2211\ufe01 =0 ( ) ,(2)\nwhere \u2265 0 is a hyper-parameter assigned to the k-th layer. It denotes the importance of this layer in constituting the final embedding. From Eq. 2, it is expected that after iteratively aggregating features from higher-order neighbors, the nodes will fail to preserve their own distinct features and their embeddings become more and more similar, leading to the over-smoothing problem. Besides, it does not distinguish the heterogeneous features of high-order nodes in the aggregation process. The noisy information from high-order neighbors could hurt the embedding learning. For example, the embeddings of users with no common interests or even contradictory interests in the high-order neighbors are aggregated to learn a target user's embedding via the graph convolution operation. Fig. 1 shows the average coverage ratio of the number of nodes that a target node reaches in the propagation by stacking different numbers of layers to all the nodes in the graph. It can be seen that after 6-or 7-layer graph convolution, a node can almost receive information from all the other nodes in embedding propagation. Therefore, by aggregating information from all the connected highorder neighbors, it is unavoidable that the node embeddings become homogeneous in the current GCN-based models after stacking more layers, especially for the densely connected ones, whose embeddings will become more and more similar. In the recommendation scenario, this means the uniqueness of users will be neglected in deep structure.\n\nActually, current GCN-based recommendation models achieve their peak performance at most 3 or 4 layers [14,37]. Besides the over-smoothing effect, we deem that a node also takes noisy or negative information in the embedding propagation process, which hurts the final performance. This is because a user's interests often span a range of items. Different users can have very different interests or even exhibit contradictory attitudes to some items. Without distinguishing those users, the embedding propagation may perform among users with very different interests to learn their embeddings in the graph convolution operation. To avoid the situation and alleviate the over-smoothing problem, it is important to group users with similar interests (and their interacted items) into subgraphs and constrain the embedding propagation to operate inside the subgraph. To achieve the goal we propose the interest-aware message-passing GCN model.\n\n\nIMP-GCN MODEL\n\n2.2.1 Interest-aware Message-passing Strategy. With constructing subgraphs, we would like that all the information propagated in a subgraph can contribute to the embedding learning of all the nodes in this subgraph. In other words, we aim to exclude the negative information propagation in the graph convolution operation using subgraphs. To achieve the goal, we rely on user nodes to form subgraphs in the user-item bipartite graph. The general idea is that users with more similar interests are grouped into a subgraph, and the items which directly linked to those users also belong to this subgraph. Therefore, each user only belongs to one subgraph, and an item can be associated with multiple subgraph. Let with \u2208 {1, \u00b7 \u00b7 \u00b7 , } denotes a subgraph, where is the number of subgraphs. In the next, we introduce the graph convolution operation in our model.\n\nBecause the direct interactions between users and items provide the most important and reliable information of user interests, in the first-order propagation, all the first-order neighbors are involved in the graph convolution operation. Let (0) and (0) denote the ID embeddings of user and item , respectively. The first-order graph convolution is:\n(1) = \u2211\ufe01 \u2208N 1 \u221a\ufe01 |N | \u221a\ufe01 |N | (0) , (1) = \u2211\ufe01 \u2208N 1 \u221a\ufe01 |N | \u221a\ufe01 |N | (0) ,(3)\nwhere (1) and (1) represent the first layer embeddings of the target user and item , respectively. For the high-order graph convolution, to avoid introducing noisy information, a node in a subgraph can only exploit the information from its neighbor nodes in this subgraph. Because the items interacted by a user all belong to the subgraph of this user, the user can still receive information from all the linked items. However, for an item node, its direct user neighbors can be distributed in different subgraphs. To learn the embeddings of an item , for each subgraph it belongs to, we learn an embedding for this item. Let ( ) denotes the embedding of item in subgraph after layers graph convolution, the high-order propagation in IMP-GCN is defined as:\n( +1) = \u2211\ufe01 \u2208N 1 \u221a\ufe01 |N | \u221a\ufe01 |N | ( ) , ( +1) = \u2211\ufe01 \u2208N 1 \u221a\ufe01 |N | \u221a\ufe01 |N | ( ) .(4)\nIn this way, we guarantee that the embedding of a node learned in a subgraph only contributes to the embedding learning of other nodes in this subgraph. This can avoid the noisy information propagated from unrelated nodes.\n\n(\u00b7) can be regarded as the features learned from the users with a similar interest in the subgraph . This make senses since users with similar interests often prefer the same feature of an item. The final representation of an item after layers graph convolution is a combination of its embeddings learned in different subgraphs, i.e.,\n( ) = \u2211\ufe01 \u2208S ( ) ,(5)\nwhere is the subgraph set that item belongs to.\n\n\nLayer Combination and Prediction.\n\nWe combine the embeddings obtained at each layer to form the final representation of user and item as Eq. 2. Similar to LightGCN, is set uniformly as 1/( + 1) [14].\n\nWith the learned embeddings of users (i.e., ) and items , given a user and a target item , the preference of the user to the item is computed by inner product:\n= .(6)\nNotice that other interaction functions can be also applied, such as Euclidean distance. Because the main focus of this work is to study the effects of distinguishing user interests in the graph convolution in the GCN-based recommendation model, we adopt the inner product as previous work [2,33,42] for fair comparisons in the empirical studies.\n\n\nMatrix-form propagation rule.\n\nWe implement our algorithm with the matrix form propagation rule (see [33] for more details), by which we can simultaneously update the representations of all users and items in a rather efficient way. It is a commonly used approach to make graph convolution network feasible for large-scale graph [26,33]. Let (0) be the representations matrix for users ID and items ID; ( ) represents the representation of users and items at the -th layer. Similarly, ( ) is defined as the representation of users and items at the -th layer in subgraph . As shown in Fig. 2, the first layer embedding propagation in our model can be described as follows:\n(1) = L (0) ,(7)\nwhere L is the Laplacian matrix for the user-item interaction graph.\n\nAs we involve the subgraphs in high-order graph convolution layers,the embeddings propagation on subgraphs is formulated as follows:\n( \u22121) = L ( \u22122) ,(8)\nwhere \u2a7e 2; L represent the Laplacian matrix for the subgraph . And then, the ( \u2212 1)-th layer embeddings are propagated on the user-item graph and obtained the embeddings in the -th layer:\n( ) = L ( \u22121) .(9)\nWe aggregate all the -th layer embeddings involved different subgraphs to formulate the final -th layer embeddings:\n( ) = \u2211\ufe01 \u2208 ( ) .(10)\nLastly, we combine all the layers' embeddings and get the final representations of users and items, this formulation keeps consistent with it in LightGCN [14]:\n= 0 (0) + 1 (1) + \u00b7 \u00b7 \u00b7 + ( )(11)\n\nOptimization.\n\nIn this work, we target at the top-recommendation, which aims to recommend a set of top-ranked items matching the target user's preference. Compared to rating prediction, this is a more practical task in real commercial systems [27]. Similar to other rank-oriented recommendation works [33,42], we adopt the pairwise learning method for optimization. To perform the pairwise learning, it needs to constructs a triplet of { , + , \u2212 }, with an observed interaction between and + and an unobserved interaction between and \u2212 . This method assumes that a positive item (i.e., + ) should rank higher than an negative item (i.e., \u2212 ). The objective function is formulated as:\n\narg min \u2211\ufe01\n(u,i + ,i \u2212 ) \u2208 O \u2212 ln (\u02c6+ \u2212\u02c6\u2212 ) + \u2225\u0398\u2225 2 2(12)\nwhere O = {( , + , \u2212 )|( , + ) \u2208 R + , ( , \u2212 ) \u2208 R \u2212 } denotes the training set; R + indicates the observed interactions between user and + in the training dataset, and R \u2212 is the sampled unobserved interaction set. and \u0398 represent the regularization weight and the parameters of the model, respectively. The 2 regularization is used to prevent overfitting. The mini-batch Adam [17] is adopted to optimize the prediction model and update the model parameters. Specifically, for a batch of randomly sampled triples ( , + , \u2212 ) \u2208 ( ), the representation of those users and items are first learned by the propagation rules and then the model parameters are updated by using the gradients of the loss function.\n\n\nSubgraph Generation Module\n\nIn this section, we introduce our proposed subgraph generation module which is designed to construct the subgraphs with \u2208 {1, \u00b7 \u00b7 \u00b7 , } from a given input graph G. Remind that the subgraphs are used to group users with common interests in our model. We formulate the user grouping as a classification task, i.e., each user is classified to a group. Specifically, each user is represented by a feature vector, which is a fusion of the graph structure and the ID embedding:\n= ( 1 ( (0) + (1) ) + 1 ),(13)\nwhere is the obtained user feature via feature fusion.\n\n(0) is the embedding of user ID and (1) is the feature obtained by aggregating local neighbor in the graph (i.e., the user embedding after the first layer propagation.). 1 \u2208 \u00d7 and 1 \u2208 1\u00d7 are respectively the trainable weight matrix and bias vector of the fusion method. is the activation function. LeakyReLU [24] is adopted, because it can encode both positive and small negative signals. To classify the users into different subgraphs, we cast the obtained user feature to a prediction vector with a 2-layer neural networks:\n= ( 2 + 2 ), = 3 + 3 ,(14)\nwhere is the prediction vector. The position of maximum value in represents which group/subgraph the user belongs to. 2 \u2208 \u00d7 , 3 \u2208 \u00d7 and 2 \u2208 1\u00d7 , 3 \u2208 1\u00d7 are respectively the trainable weight matrices and bias vectors of the two layers. The dimension of the prediction vector dimensions is the same as the number of subgraphs, which is a pre-selected hyper-parameter. Note that it is an unsupervised method to classify users into different groups and thus does not need ground-truth label. For users with similar embeddings, Eq. 14 will generate similar prediction vector, namely, they will be classified into the same group. The subgraph generation aims to construct a matrix, which represents the user-item adjacency relation in a subgraph based on the user grouping results and the Laplacian matrix of the original user-item graph. For the matrix of each subgraph, according to the obtained user group information, we filter out the user-item adjacency relations in the Laplacian matrix of the original user-item graph if the corresponding users are not in the user group.\n\n\nEXPERIMENTS 3.1 Experimental Setup\n\n\nData Description.\n\nTo evaluate the effectiveness of IMP-GCN, we conducted experiments on three benchmark datasets: Amazon-Kindle Store, Amazon-Home&Kitchen and Gowalla. The first two datasets are from the public Amazon review dataset 4 , which has been widely used for recommendation evaluation in previous studies. The third dataset is a check-in dataset collected from Gowalla, where users share their locations by checking-in. We followed the general setting in recommendation to filter users and items with few interactions. For all the datasets, we used the 10-core settings, i.e., retaining users and items with at least 10 interactions. The statistics of three datasets are shown in Table 1. As we can see, the datasets are of different sizes and sparsity levels, which are useful for analyzing the performance of our method and the competitors in different situations. For each datasets, we randomly split it into training, validation, and testing set with the ratio 80:10:10 for each user. The observed user-item interactions were treated as positive instances. For the methods which adopt the pairwise learning strategy, we randomly sample a negative instance, that the user did not consume before, to pair with each positive instance.\n\n\nEvaluation Metrics.\n\nFor each user in the test set, we treat all the items that the user did not interact with as negative items. Two widely used evaluation metrics for top-recommendation are adopted in our evaluation: Recall and Normalized Discounted Cumulative Gain [13]. For each metric, the performance is computed based on the top 20 results. Notice that the reported results are the average values across all the testing users.\n\n\nExperimental Settings.\n\nWe implemented our model with Tensorflow 5 and carefully tuned the key parameters. The embedding size is fixed to 64 for all models and the embedding parameters are initialized with the Xavier method [39]. We optimized our method with Adam [17] and used the default learning rate of 0.001 and default mini-batch size of 1024 (on gowalla, we increased the minibatch size to 2048 for speed). The 2 regularization coefficient is searched in the range of {1 \u22126 , 1 \u22125 , \u00b7 \u00b7 \u00b7 , 1 \u22122 }. The early stopping and validation strategies are kept the same as those in LightGCN.\n\n\nStudy of IMP-GCN\n\nIn this section, we first evaluated the performance of our IPM-GCN model when stacking different layers in graph convolution. This is to examine whether our interest-aware message-passing strategy can alleviate the over-smoothing problem. In the next, we study the effects of the subgraph numbers on the performance of our model.\n\n\nEffect of Layer Numbers.\n\nTo investigate the effectiveness of IMP-GCN in deeper structure, we increased the model depth and performed detailed comparison with LightGCN. Since the adopted message-passing strategy is the same as LightGCN in the first-order convolution layer, we increased the layer number from 2 to 7. The experimental results are shown in Fig. 3, in which IMP-GCN 2 , IMP-GCN 3 and IMP-GCN 4 indicate the model with 2, 3, and 4 subgraphs, respectively. We omitted the results on & \u210e for space limitation, because they show exactly the same trend. From the results, we had some interesting observations. Firstly, the proposed IMP-GCN outperforms LightGCN consistently when stacking more than 2 or 3 layers over both datasets. This indicates that our model can learn better embeddings by the interest-aware message-passing strategy. Secondly, the peak performance of LightGCN is obtained when stacking 3 or 4 layers, and increasing more layers will cause dramatic performance degradation, indicating it suffers from the over-smoothing problem in a deep structure. In contrast, IMP-GCN continues to achieve better performance with deeper structure (notice that when stacking more than 7 layers, a node already aggregates information from almost all the nodes, see Fig. 1. The results demonstrate the capability of our model on alleviating the over-smoothing problem. Moreover, it also 1) justifies our claim that exploiting information from all nodes indiscriminately causes the over-smoothing in GCN-based recommendation model, and 2) validates the effectiveness of our subgraph generation algorithm on classifying users with common interests.\n\n\nEffect of Subgraph.\n\nThe performance of IPM-GCN with different numbers (i.e., {2, 3, 4}) of subgraphs can also be observed in Fig. 3. From the results, we can see that the (1) IMP-GCN 2 with 2 subgraphs can obtain the best results when stacking no more than 3 layers. This is because a node in the subgraphs of IMP-GCN 2 can reach more nodes in short distance than the on in IMP-GCN 3 or IMP-GCN 4 in the embedding propagation operation. (2) When stacking more than 3 layers, IMP-GCN 3 performs the best. After 3 layers graph convolution, the number of involved nodes increasing sharply in embedding propagation (see the examples in Fig. 1). On average, each node in IMP-GCN 2 should reach more nodes that the one in IMP-GCN 3 and IMP-GCN 4 , however, the performance improvement of IMP-GCN 2 is smaller or even negative (on th Kindle Stores) than that of IMP-GCN 3 and IMP-GCN 4 . This indicates that there is still noisy information in embedding propagation by discriminating user interests in a coarse-level (i.e., 2 subgraphs), negatively impacting the performance. Note that IMP-GCN 3 can still benefit from high-order neighbors.\n\n(3) With more subgraphs, on the one hand, IMP-GCN 4 can distinguish users with similar interests in a finer level and thus can better distill information from high-order neighbors; on the other hand, it also cuts more connections to other nodes, especially the ones in short distance which provide more valuable information in embedding learning. As a result, when stacking more layers, its performance is only comparable to that of IMP-GCN 2 . Therefore, there is a trade-off on selecting the number of subgraphs. We further studied the effects of subgraphs by analyzing the average coverage ratio of each node and the corresponding performance based on the LightGCN and our IPM-GCN model. Due to the space limitation, we only provide the results on Kindle Store and omit the performance . . ndcg which has the similar trend as recall. In this experiment, we used the LightGCN with 4 layers and IPM-GCN with 3 subgraphs 6 and 6 layers, which are their optimal setting on Kindle Store. The average recall and average cover ratio of each user in a subgraph based on LightGCN and IPM-GCN are shown in Fig. 4(a) and Fig. 4(b), respectively. Notably, by grouping users with similar interest in subgraphs to make information only propagate inside subgraphs, IPM-GCN can benefit from more layers of graph convolution and distill positive information from high-order neighors. In contrast, LightGCN is limited by the negative information from high-order neighbors and can only gain improvements over 4 layers. Comparing the performance of different subgraphs, we can see that with a higher coverage ratio, the performance of IPM-GCN increases clearly.\n\nAnother interesting finding is that, by stacking 6 layers, a user node in a subgraph almost connects to all the other nodes in the whole graph. This indicates that the users in a subgraph almost interact all the items in the graph (otherwise, the coverage ratio cannot be that high). More importantly, IPM-GCN can still achieve improvement with such high coverage without over-smoothing. This indicates that the embeddings of items learned in a graph contributes to the embedding learning of users in this graph, and the distilled information in a subgraph during graph convolution is useful for the embedding learning for all the nodes in this subgraph. It demonstrates the effectiveness of our interest-aware messagepassing strategy and the subgraph generation algorithm.\n\n\nComparison with SOTA Methods\n\n\nBaselines.\n\nTo demonstrate the effectiveness, we compared our proposed method with several recently proposed competitive methods, including\n\n\u2022 NeuMF [15]: It is a state-of-the-art neural collaborative filtering method. This method uses multiple hidden layers above the element-wise and concatenation of user and item embeddings to capture their non-linear feature interactions. \u2022 HOP-Rec [42]: This method exploits the high-order user-item interactions by random walks to enrich the original training data. In experiments, we used the codes released by the authors 7 . \u2022 CSE [2]: This recently proposed graph-based model also exploits the high-order proximity in the user-item bipartite graph. Different from HOP-Rec, this method explores the user-user and item-item relations by random walks to improve the performance. We used the codes released by the authors ( the same link as HOP-Rec). The symbol * denotes that the improvement is significant with \u2212 < 0.05 based on a two-tailed paired t-test.\n\n\u2022 GCMC [29]: This method applies the GCN techniques on useritem bipartite graph and employs one convolutional layer to exploit the direct connections between users and items. \u2022 NGCF [33]: This method explicitly encodes the collaborative signal in the form of high-order connectivities by performing embedding propagation in the user-item bipartite graph. \u2022 LightGCN [14]: It is an simplified version of NGCF by removing the feature transformation and nonlinear activation module. It makes GCN-based methods more concise and appropriate for recommendation and achieves the state-of-the-art performance.\n\nFor fair comparisons, all the methods are optimized by the same pairwise learning strategy. We put great efforts to tune these methods based on the validation dataset and reported their best performance. Table 2 shows the performance comparison results. The best and second best results were highlighted in bold. From the results, we had following observations. The performance of NeuMF is relatively poor as it not explicitly leverages the high-order connectivities between users and items, resulting in suboptimal performance. For the graph-based methods, CSE makes use of the implicit associates of user-user and item-item similarities via high-order neighborhood proximity by performing random walks on the user-item interaction graph. GCMC obtains better performance over CSE, demonstrating the advantages of GCN-based approaches, which can exploit graph structure information. However, it does not perform well on & \u210e because the useful information in neighbors cannot be efficiently aggregated. Hop-Rec outperforms the above methods on the three datasets, because it samples user-item interactions from high-order neighbors to enrich the training data. NGCF achieves consistent much better performance over the above baselines. This is because it adopts the GCN techniques to explicitly and directly exploit the high-order connectivities in the embedding. In contrast, the GCMC method only utilizes the first-order neighbors for representation learning; HOP-Rec and CSE leverage the high-order neighbors to enrich the training data rather than using them in embedding function for direct representation learning. This demonstrates the powerful representation learning capability of GCN and the importance of utilizing high-order information directly in representation learning. Similar to the results reported in [14], LightGCN achieves substantially improvement over NGCF by simplifying it with the removal of two common designs in GCN. IMP-GCN outperforms all the baselines consistently over all the datasets. In particular, compared to the strongest baseline in terms of NDCG@20, IMP-GCN can reach a relative improvement over LightGCN by 7.85%, 7.19%, 3.66% on , & \u210e and , respectively. The great improvement over LightGCN demonstrates the importance of distinguishing nodes in high-order neighbors in the graph convolution operation, as well as the effectiveness of our proposed interest-aware message-passing strategy.\n\n\nOverall Comparison.\n\n\nAblation Study\n\nIn this section, we examined the contribution of different components in our model to the final performance by comparing IMP-GCN with the following two variants:\n\n\u2022 IMP-GCN : This variant removes the graph structure information from the subgraph generation module (i.e., removing (1) in Eq. 13). \u2022 IMP-GCN : In this variant, the first-order propagation is also performed inside each subgraph (i.e., The equation for (1) in Eq. 3 is replaced with \u2208S (1) ).\n\nThe results of two variants and IPM-GCN were reported in Table 3, in which the best results are highlighted in bold. IMP-GCN outperforms IMP-GCN over all the datasets, which indicates the effectiveness of employing graph structure information in subgraph generation module. It is expected that IMP-GCN obtains much better performance over IMP-GCN , because the first-order neighbors (i.e., the interaction between users and items) contributes the direct information for user and embedding in the collaborative filtering process. The results also demonstrate the reasonable design of our IPM-GCN model.\n\n\nRELATED WORK\n\nAs one of the most important information retrieval techniques, recommendation has made tremendous progress in past decades. Among various recommendation approaches, the model-based collaborative filtering (CF) [5, 6, 14-16, 19, 20, 27, 32, 33] achieves a great success and becomes the mainstream recommendation technique. CF learns user and item embeddings by reconstructing the user-item interaction matrix. Earlier research efforts mainly focus on the shallow models, such as BPR [27], CML [16], matrix factorization (MF) [19]. Their success motivates the development of various variants via leveraging additional information (e.g., review [25], image [12], knowledge graph [30][31][32]) to deal with different tasks (e.g., context-aware [22], session-based [23]). With the rise of deep learning, it has also been widely applied in recommendation and exhibits great potential by either enhancing the user/item embedding learning or introducing non-linearity into the interaction function, promoting another peak development of recommendation technique. Many DL-based recommendation models have been proposed, such as NeuMF [15], Wide&Deep [4], and achieved better performance over traditional models.\n\nAnother research line is graph-based recommendation, which can explicitly exploit high-order proximity between users and items. Early approaches infer indirect preference by random walks in the graph to provide recommendation [7,10,11]. The recently proposed approaches exploit the user-item bipartite graph to enrich the useritem interactions [42,44] and explore other types of collaborative relations, such as user-user and item-item similar ties [2,44]. For example, HOP-Rec [42] uses random sample positive user-item interactions to enrich the training data by using random walks. WalkRanker [44] and CSE [2] performs random walks to explore the high-order proximity in user-user and item-item relations. As those methods rely on random walks to sample new interactions for model training, their performance heavily depends on the quality of generated interactions by random walks. As a result, these methods need carefully selection and tuning effects.\n\nIn recent years, Graph Convolution Networks (GCNs) have attracted increasing attention in recommendation due to the powerful capability on representation learning from non-Euclidean structure [8, 9, 14, 21, 29, 32-36, 43, 45]. And then, many GCN-based recommendation models have been developed. For example, GC-MC [29] employs one convolution layer to exploit the direct connections between users and items; PinSage [43] combines random walks with multiple graph convolution layers on the item-item graph for Pinterest image recommendation; MEIRec [8] utilizes metapath-guided neighbors to exploit rich structure information for intent recommendation; NGCF [33] exploits high-order proximity by propagating embeddings on the user-item interaction graph; instead of implicitly capturing the high-order connectivity through the propagation embedding, SMOG-CF [45] is proposed to directly capture the high-order connectivity between neighboring nodes at any order. Multi-GCCF [28] explicitly incorporates the user-user and item-item graphs, which is built upon the user-item bipartite graph, in the embedding learning process. Inspired by the study of simplifying GCN [37], researchers also introspect the complex design in GCN-based recommendation models. He at al. [14] pointed out that the two common designs feature transformation and nonlinear activation have no positive effects on the final performance, and proposed LightGCN which substantially improves the performance over NDCG. Meanwhile, Chen et al. [3] also proposed to remove the nonlinearity in the network and introduced a residual network to alleviate the over-smoothing problem in existing GCN-based recommendation models. In this paper, we move a step further on this research line. We claim that the indiscriminatively exploiting the high-order neighboring nodes is also an important reason for the over-smoothing problem for GCN-based recommendation model. A typical example is that two users with contradictory interests can be also connected via a -order path in the user-item interaction graph. To tackle the problem, we propose an interest-aware message-passing strategy to make the embedding propagation only happened inside a subgraph with similar interests.\n\n\nCONCLUSION\n\nIn this work, we argued that exploiting high-order node indiscriminately would introduce negative information into the embedding propagation in the GCN-based recommendation models, causing the performance degradation when stacking more layers. We presented a IMP-GCN model which learns user and item embeddings by performing high-order graph convolution inside subgraphs. The subgraphs are formed by a designed subgraph generation algorithm that groups users with similar interests and their interacted items into the same graph. In IMP-GCN, the embedding of a node learned in a subgraph only contributes to the embedding learning of other nodes in this subgraph. In this way, IMP-GCN can effectively avoid taking the noisy information into the embedding learning. Experiments on large-scale real-world datasets demonstrate that IMP-GCN can gain improvements by stacking more layers to exploit information from higher-order neighbors, and achieve the state-of-the-art performance. The advantages of IMP-GCN indicate the importance of distinguishing high-order neighbors on tackling the over-smoothing problem in GCN models. We believe the insights in this study can shed light on the further development of graph-based recommendation models.\n\n( 0 )Figure 1 :\n01denote the ID embedding of user and (0) denote the ID embedding of item , The average ratio of nodes involved in different layers of graph convolution on three datasets.\n\nFigure 2 :\n2propagation : Features obtained from the k-th layer : The final representation of users and items : An overview of our IMP-GCN model with two subgraphs as illustration. In IMP-GCN, the first-order propagation operates on whole graph, and high-order propagation operates inside the subgraphs.\n\nFigure 3 :\n3Results Comparison between IMP-GCN and LightGCN at different layers on Kindle Store and Gowalla. IMP-GCN 2 , IMP-GCN 3 , and IMP-GCN 4 represent IMP-GCN with 2, 3, and 4 subgraphs, respectively.\n\nFigure 4 :\n4Statistics of Recall and Coverage Ratio on Kindle Store in three subgraphs.\n\nTable 1 :\n1Basic statistics of the experimental datasets.Dataset \n#user #item #interactions sparsity \n\nKindle Store \n68,223 61,934 \n982,618 \n99.98% \nHome&Kitchen 66,519 28,237 \n551,681 \n99.97% \nGowalla \n29,858 40,981 \n1,027,370 \n99.92% \n\n\n\nTable 2 :\n2Performance of our model and the competitors over \nthree datasets. Noticed that the values are reported by per-\ncentage with '%' omitted. \n\nDatasets \nKindle Store \nHome&Kitchen \nGowalla \nMetrics \nRecall NDCG Recall NDCG Recall NDCG \n\nNeuMF \n4.96 \n2.06 \n1.34 \n0.62 \n12.96 \n11.21 \nCSE \n7.65 \n4.54 \n1.93 \n0.91 \n13.85 \n11.51 \nHOP-Rec \n7.96 \n4.58 \n1.98 \n0.94 \n14.11 \n12.70 \nGCMC \n7.93 \n4.55 \n1.42 \n0.64 \n14.03 \n11.68 \nNGCF \n8.25 \n5.09 \n2.14 \n0.96 \n15.62 \n13.35 \nLightGCN 10.22 \n6.24 \n3.03 \n1.39 \n17.96 \n15.29 \n\nIMP-GCN 10.88* 6.73* \n3.22* \n1.49* 18.69* 15.85* \nImprov. \n6.46% \n7.85% 6.27% 7.19% \n4.07% \n3.66% \n\n\n\nTable 3 :\n3Performance of our model and its variants over three datasets. Noticed that the values are reported by percentage with '%' omitted.Datasets \nKindle Store \nHome&Kitchen \nGowalla \nMetrics \nRecall NDCG Recall NDCG Recall NDCG \n\nIMP-GCN \n10.57 \n6.63 \n3.14 \n1.43 \n18.61 \n15.61 \nIMP-GCN \n10.19 \n6.40 \n2.97 \n1.31 \n17.84 \n15.11 \nIMP-GCN \n10.88 \n6.73 \n3.22 \n1.49 \n18.69 15.85 \n\n\nIn experiments, we found that by stacking 7 layers, a user node almost reaches all the other users in three different datasets. Therefore, no more gain after stacking 7 layers. 2 https://github.com/liufancs/IMP_GCN.3 Note that although LR-GCN was inspired by a different motivation, its final formulation is almost the same as LightGCN.\nhttp://jmcauley.ucsd.edu/data/amazon. 5 https://www.tensorflow.org.\nNumber of users in the three groups 1 , 2 , 3 are 3, 971, 3, 584, 6, 801, respectively. 7 https://github.com/cnclabs/smore.\n\nLessons from the Netflix prize challenge. M Robert, Yehuda Bell, Koren, SIGKDD Explorations. ACM. Robert M. Bell and Yehuda Koren. 2007. Lessons from the Netflix prize challenge. In SIGKDD Explorations. ACM, 75-79.\n\nCollaborative Similarity Embedding for Recommender Systems. Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, Yi-Hsuan Yang, WWW. ACM. Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, and Yi-Hsuan Yang. 2019. Collaborative Similarity Embedding for Recommender Systems. In WWW. ACM, 2637-2643.\n\nRevisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. Lei Chen, Le Wu, Richang Hong, Kun Zhang, Meng Wang, The Thirty-Fourth AAAI Conference on Artificial Intelligence. AAAI PressLei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. AAAI Press, 27-34.\n\n. Heng-Tze, Levent Cheng, Jeremiah Koc, Tal Harmsen, Tushar Shaked, Hrishi Chandra, Glen Aradhye, Greg Anderson, Wei Corrado, Mustafa Chai, Ispir, Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n\nWide & deep learning for recommender systems. Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. the 1st Workshop on Deep Learning for Recommender SystemsACMWide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 7-10.\n\nMMALFM: Explainable recommendation by leveraging reviews and images. Zhiyong Cheng, Xiaojun Chang, Lei Zhu, C Rose, Mohan Kanjirathinkal, Kankanhalli, TOIS. 3716Zhiyong Cheng, Xiaojun Chang, Lei Zhu, Rose C Kanjirathinkal, and Mohan Kankanhalli. 2019. MMALFM: Explainable recommendation by leveraging re- views and images. TOIS 37, 2 (2019), 16.\n\nAspect-aware latent factor model: Rating prediction with ratings and reviews. Zhiyong Cheng, Ying Ding, Lei Zhu, Kankanhalli Mohan, Proceedings of the 27th International Conference on World Wide Web. the 27th International Conference on World Wide Web3Zhiyong Cheng, Ying Ding, Lei Zhu, and Kankanhalli Mohan. 2018. Aspect-aware latent factor model: Rating prediction with ratings and reviews. In Proceedings of the 27th International Conference on World Wide Web. IW3C2, 639-648.\n\nBlockbusters and Wallflowers: Accurate, Diverse, and Scalable Recommendations with Random Walks. Fabian Christoffel, Bibek Paudel, Chris Newell, Abraham Bernstein, Proceedings of the 9th ACM Conference on Recommender Systems. the 9th ACM Conference on Recommender SystemsACMFabian Christoffel, Bibek Paudel, Chris Newell, and Abraham Bernstein. 2015. Blockbusters and Wallflowers: Accurate, Diverse, and Scalable Recommendations with Random Walks. In Proceedings of the 9th ACM Conference on Recommender Systems. ACM, 163-170.\n\nMetapath-Guided Heterogeneous Graph Neural Network for Intent Recommendation. Junxiong Shaohua Fan, Xiaotian Zhu, Chuan Han, Linmei Shi, Biyu Hu, Yongliang Ma, Li, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMShaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma, and Yongliang Li. 2019. Metapath-Guided Heterogeneous Graph Neural Network for Intent Recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2478-2486.\n\nGraph Neural Networks for Social Recommendation. Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, Dawei Yin, Proceedings of the 28th International Conference on World Wide Web. the 28th International Conference on World Wide Web3Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph Neural Networks for Social Recommendation. In Proceedings of the 28th International Conference on World Wide Web. IW3C2, 417-426.\n\nRandom-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation. Fran\u00e7ois Fouss, Alain Pirotte, Jean-Michel Renders, Marco Saerens, IEEE Trans. Knowl. Data Eng. 19Fran\u00e7ois Fouss, Alain Pirotte, Jean-Michel Renders, and Marco Saerens. 2007. Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation. IEEE Trans. Knowl. Data Eng. 19, 3 (2007), 355-369.\n\nItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines. Marco Gori, Augusto Pucci, Proceedings of the 20th International Joint Conference on Artifical Intelligence. the 20th International Joint Conference on Artifical IntelligenceMorgan Kaufmann Publishers IncMarco Gori and Augusto Pucci. 2007. ItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines. In Proceedings of the 20th International Joint Conference on Artifical Intelligence. Morgan Kaufmann Publishers Inc., 2766-2771.\n\nVBPR: Visual Bayesian Personalized Ranking from Implicit Feedback. Ruining He, Julian Mcauley, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. the Thirtieth AAAI Conference on Artificial IntelligenceAAAI PressRuining He and Julian McAuley. 2016. VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, 144-150.\n\nTriRank: Reviewaware Explainable Recommendation by Modeling Aspects. Xiangnan He, Tao Chen, Min-Yen Kan, Xiao Chen, Proceedings of the 24th ACM International Conference on Information and Knowledge Management. the 24th ACM International Conference on Information and Knowledge ManagementACMXiangnan He, Tao Chen, Min-Yen Kan, and Xiao Chen. 2015. TriRank: Re- viewaware Explainable Recommendation by Modeling Aspects. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management. ACM, 1661-1670.\n\nLightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalACMXiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net- work for Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 639-648.\n\nNeural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide Web3Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web. IW3C2, 173-182.\n\nCollaborative metric learning. Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, Deborah Estrin, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide Web3Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and Deborah Estrin. 2017. Collaborative metric learning. In Proceedings of the 26th International Conference on World Wide Web. IW3C2, 193-201.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Proceedings of the 3rd International Conference on Learning Representations. the 3rd International Conference on Learning RepresentationsDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimiza- tion. In Proceedings of the 3rd International Conference on Learning Representations.\n\nSemi-Supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, ICLR. OpenReview.net. Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In ICLR. OpenReview.net.\n\nMatrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, In IEEE Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech- niques for recommender systems. In IEEE Computer, Vol. 42. 42-49.\n\nUser Diverse Preference Modeling by Multimodal Attentive Metric Learning. Fan Liu, Zhiyong Cheng, Changchang Sun, Yinglong Wang, Liqiang Nie, Mohan Kankanhalli, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaACMFan Liu, Zhiyong Cheng, Changchang Sun, Yinglong Wang, Liqiang Nie, and Mohan Kankanhalli. 2019. User Diverse Preference Modeling by Multimodal Attentive Metric Learning. In Proceedings of the 27th ACM International Conference on Multimedia. ACM, 1526-1534.\n\nAn Attribute-aware Attentive GCN Model for Recommendation. Fan Liu, Zhiyong Cheng, Lei Zhu, Chenghao Liu, Liqiang Nie, IEEE Transactions on Knowledge and Data Engineering. Fan Liu, Zhiyong Cheng, Lei Zhu, Chenghao Liu, and Liqiang Nie. 2020. An Attribute-aware Attentive GCN Model for Recommendation. IEEE Transactions on Knowledge and Data Engineering (2020), 1-12.\n\nLearning Context-Aware Latent Representations for Context-Aware Collaborative Filtering. Xin Liu, Wei Wu, Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 38th International ACM SIGIR Conference on Research and Development in Information RetrievalAssociation for Computing MachineryXin Liu and Wei Wu. 2015. Learning Context-Aware Latent Representations for Context-Aware Collaborative Filtering. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. Association for Computing Machinery, 887-890.\n\nKeywords Generation Improves E-Commerce Session-Based Recommendation. Yuanxing Liu, Zhaochun Ren, Wei-Nan Zhang, Wanxiang Che, Ting Liu, Dawei Yin, Proceedings of The Web Conference 2020. The Web Conference 2020Association for Computing MachineryYuanxing Liu, Zhaochun Ren, Wei-Nan Zhang, Wanxiang Che, Ting Liu, and Dawei Yin. 2020. Keywords Generation Improves E-Commerce Session-Based Recommendation. In Proceedings of The Web Conference 2020. Association for Computing Machinery, 1604-1614.\n\nRectifier nonlinearities improve neural network acoustic models. L Andrew, Maas, Y Awni, Andrew Y Hannun, Ng, ICML Workshop on Deep Learning for Audio, Speech and Language Processing. Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing.\n\nHidden factors and hidden topics: understanding rating dimensions with review text. Julian Mcauley, Jure Leskovec, Proceedings of the 7th ACM Conference on Recommender Systems. the 7th ACM Conference on Recommender SystemsACMJulian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems. ACM, 165-172.\n\nDeepInf: Social Influence Prediction with Deep Learning. Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, Jie Tang, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMJiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. 2018. DeepInf: Social Influence Prediction with Deep Learning.. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2110-2119.\n\nBPR: Bayesian personalized ranking from implicit feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, UAI. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI. 452- 461.\n\nMulti-graph convolution collaborative filtering. Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Proceedings of IEEE International Conference on Data Mining. IEEE International Conference on Data MiningRuiming Tang, and Xiuqiang HeJianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming Tang, and Xiuqiang He. 2019. Multi-graph convolution collaborative filtering. In Proceedings of IEEE International Conference on Data Mining. 1306 -1311.\n\nGraph Convolutional Matrix Completion. Rianne Van Den, Thomas N Berg, Max Kipf, Welling, ACM SIGKDD: Deep Learning Day. ACMRianne van den Berg, Thomas N. Kipf, and Max Welling. 2018. Graph Convolu- tional Matrix Completion. In ACM SIGKDD: Deep Learning Day. ACM.\n\nRippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo, CIKM. ACM. Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. In CIKM. ACM, 417-426.\n\nExploring High-Order User Preference on the Knowledge Graph for Recommender Systems. Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo, ACM Trans. Inf. Syst. 3726Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2019. Exploring High-Order User Preference on the Knowledge Graph for Recommender Systems. ACM Trans. Inf. Syst. 37, 3 (2019), 32:1-32:26.\n\nKGAT: Knowledge Graph Attention Network for Recommendation. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMXiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT: Knowledge Graph Attention Network for Recommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 950-958.\n\nNeural Graph Collaborative Filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalACMXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 165-174.\n\nPersonalized Hashtag Recommendation for Micro-videos. Yinwei Wei, Zhiyong Cheng, Xuzheng Yu, Zhou Zhao, Lei Zhu, Liqiang Nie, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaACMYinwei Wei, Zhiyong Cheng, Xuzheng Yu, Zhou Zhao, Lei Zhu, and Liqiang Nie. 2019. Personalized Hashtag Recommendation for Micro-videos. In Proceedings of the 27th ACM International Conference on Multimedia. ACM, 1446-1454.\n\nXiangnan He, and Tat-Seng Chua. 2020. Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback. Yinwei Wei, Xiang Wang, Liqiang Nie, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaACMYinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. 2020. Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback. In Proceedings of the 28th ACM International Conference on Multimedia. ACM, 3541-3549.\n\nMMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. Yinwei Wei, Xiang Wang, Liqiang Nie, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaACMXiangnan He, Richang Hong, and Tat-Seng ChuaYinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM International Conference on Multimedia. ACM, 1437-1445.\n\nSimplifying Graph Convolutional Networks. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger, PMLRFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying Graph Convolutional Networks. PMLR, 6861- 6871.\n\nCollaborative Denoising Auto-Encoders for Top-N Recommender Systems. Yao Wu, Christopher Dubois, Alice X Zheng, Martin Ester, Proceedings of the 9th ACM International Conference on Web Search and Data Mining. the 9th ACM International Conference on Web Search and Data MiningACMYao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. 2016. Collabora- tive Denoising Auto-Encoders for Top-N Recommender Systems. In Proceedings of the 9th ACM International Conference on Web Search and Data Mining. ACM, 153-162.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Glorot Xavier, Bengio Yoshua, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics. JMLR. the 13th International Conference on Artificial Intelligence and Statistics. JMLRGlorot Xavier and Bengio Yoshua. 2010. Understanding the difficulty of train- ing deep feedforward neural networks. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics. JMLR, 249-256.\n\nGraph Highway Networks. ArXiv abs. Xin Xin, Alexandros Karatzoglou, I Arapakis, J Jose, 4635Xin Xin, Alexandros Karatzoglou, I. Arapakis, and J. Jose. 2020. Graph Highway Networks. ArXiv abs/2004.04635 (2020).\n\nDeep matrix factorization models for recommender systems. Hongjian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, Jiajun Chen, Proceedings of the 26h International Joint Conference on Artificial Intelligence. the 26h International Joint Conference on Artificial IntelligenceAAAI PressHongJian Xue, XinYu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2017. Deep matrix factorization models for recommender systems. In Proceedings of the 26h International Joint Conference on Artificial Intelligence. AAAI Press, 3203-3209.\n\nHOP-rec: high-order proximity for implicit recommendation. Jheng-Hong Yang, Chih-Ming Chen, Chuan-Ju Wang, Ming-Feng Tsai, RecSys. ACM. Jheng-Hong Yang, Chih-Ming Chen, Chuan-Ju Wang, and Ming-Feng Tsai. 2018. HOP-rec: high-order proximity for implicit recommendation. In RecSys. ACM, 140-144.\n\nGraph Convolutional Neural Networks for Web-Scale Recommender Systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 974-983.\n\nWalkRanker: A Unified Pairwise Ranking Model With Multiple Relations for Item Recommendation. Lu Yu, Chuxu Zhang, Shichao Pei, Guolei Sun, Xiangliang Zhang, IJCAI. AAAI PressLu Yu, Chuxu Zhang, Shichao Pei, Guolei Sun, and Xiangliang Zhang. 2018. WalkRanker: A Unified Pairwise Ranking Model With Multiple Relations for Item Recommendation. In IJCAI. AAAI Press, 2596-2603.\n\nStacked Mixed-Order Graph Convolutional Networks for Collaborative Filtering. Hengrui Zhang, Julian Mcauley, Proceedings of the 2020 SIAM International Conference on Data Mining. the 2020 SIAM International Conference on Data MiningHengrui Zhang and Julian McAuley. 2020. Stacked Mixed-Order Graph Convo- lutional Networks for Collaborative Filtering. In Proceedings of the 2020 SIAM International Conference on Data Mining. Society for Industrial and Applied Math- ematics, 73-81.\n", "annotations": {"author": "[{\"end\":120,\"start\":93},{\"end\":313,\"start\":121},{\"end\":322,\"start\":314},{\"end\":509,\"start\":323},{\"end\":543,\"start\":510},{\"end\":552,\"start\":544},{\"end\":745,\"start\":553},{\"end\":754,\"start\":746},{\"end\":941,\"start\":755},{\"end\":954,\"start\":942},{\"end\":1019,\"start\":955}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":97},{\"end\":134,\"start\":129},{\"end\":321,\"start\":318},{\"end\":330,\"start\":327},{\"end\":521,\"start\":518},{\"end\":551,\"start\":548},{\"end\":566,\"start\":561},{\"end\":753,\"start\":750},{\"end\":762,\"start\":759},{\"end\":953,\"start\":950}]", "author_first_name": "[{\"end\":96,\"start\":93},{\"end\":128,\"start\":121},{\"end\":317,\"start\":314},{\"end\":326,\"start\":323},{\"end\":517,\"start\":510},{\"end\":547,\"start\":544},{\"end\":560,\"start\":553},{\"end\":749,\"start\":746},{\"end\":758,\"start\":755},{\"end\":949,\"start\":942}]", "author_affiliation": "[{\"end\":312,\"start\":136},{\"end\":508,\"start\":332},{\"end\":744,\"start\":568},{\"end\":940,\"start\":764},{\"end\":1018,\"start\":956}]", "title": "[{\"end\":54,\"start\":1},{\"end\":1073,\"start\":1020}]", "venue": "[{\"end\":1238,\"start\":1075}]", "abstract": "[{\"end\":3611,\"start\":1841}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3938,\"start\":3935},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3941,\"start\":3938},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3944,\"start\":3941},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3947,\"start\":3944},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3950,\"start\":3947},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4218,\"start\":4215},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4392,\"start\":4388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4426,\"start\":4422},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4429,\"start\":4426},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4432,\"start\":4429},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4435,\"start\":4432},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4438,\"start\":4435},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4889,\"start\":4885},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5089,\"start\":5085},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5191,\"start\":5187},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5286,\"start\":5282},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5540,\"start\":5537},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6173,\"start\":6169},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6935,\"start\":6931},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9490,\"start\":9486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10518,\"start\":10517},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12877,\"start\":12876},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13363,\"start\":13359},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15173,\"start\":15169},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15176,\"start\":15173},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17317,\"start\":17314},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17325,\"start\":17322},{\"end\":17937,\"start\":17934},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18972,\"start\":18968},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19435,\"start\":19432},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19438,\"start\":19435},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19441,\"start\":19438},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19596,\"start\":19592},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19824,\"start\":19820},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19827,\"start\":19824},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20906,\"start\":20902},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21190,\"start\":21186},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21248,\"start\":21244},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21251,\"start\":21248},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22068,\"start\":22064},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23294,\"start\":23290},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24883,\"start\":24882},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26168,\"start\":26164},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26560,\"start\":26556},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26600,\"start\":26596},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29814,\"start\":29813},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32677,\"start\":32673},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32916,\"start\":32912},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33102,\"start\":33099},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33536,\"start\":33532},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33711,\"start\":33707},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33895,\"start\":33891},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35952,\"start\":35948},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36882,\"start\":36879},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37018,\"start\":37015},{\"end\":37917,\"start\":37884},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38160,\"start\":38156},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38170,\"start\":38166},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38202,\"start\":38198},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38320,\"start\":38316},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38332,\"start\":38328},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":38354,\"start\":38350},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38358,\"start\":38354},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38362,\"start\":38358},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38418,\"start\":38414},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38438,\"start\":38434},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38803,\"start\":38799},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38818,\"start\":38815},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39107,\"start\":39104},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39110,\"start\":39107},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39113,\"start\":39110},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39226,\"start\":39222},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39229,\"start\":39226},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39330,\"start\":39327},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39333,\"start\":39330},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":39360,\"start\":39356},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39478,\"start\":39474},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39490,\"start\":39487},{\"end\":40062,\"start\":40029},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":40155,\"start\":40151},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40257,\"start\":40253},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40388,\"start\":40385},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40498,\"start\":40494},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":40698,\"start\":40694},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":40814,\"start\":40810},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41006,\"start\":41002},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41105,\"start\":41101},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41349,\"start\":41346},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45574,\"start\":45573}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43514,\"start\":43326},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43819,\"start\":43515},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44027,\"start\":43820},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44116,\"start\":44028},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44356,\"start\":44117},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44975,\"start\":44357},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45357,\"start\":44976}]", "paragraph": "[{\"end\":4393,\"start\":3627},{\"end\":6104,\"start\":4395},{\"end\":7263,\"start\":6106},{\"end\":9089,\"start\":7265},{\"end\":9999,\"start\":9091},{\"end\":10946,\"start\":10001},{\"end\":11966,\"start\":10948},{\"end\":12662,\"start\":11992},{\"end\":12928,\"start\":12664},{\"end\":13221,\"start\":13008},{\"end\":13517,\"start\":13240},{\"end\":15064,\"start\":13549},{\"end\":16005,\"start\":15066},{\"end\":16881,\"start\":16023},{\"end\":17232,\"start\":16883},{\"end\":18064,\"start\":17308},{\"end\":18366,\"start\":18144},{\"end\":18702,\"start\":18368},{\"end\":18771,\"start\":18724},{\"end\":18973,\"start\":18809},{\"end\":19134,\"start\":18975},{\"end\":19488,\"start\":19142},{\"end\":20162,\"start\":19522},{\"end\":20248,\"start\":20180},{\"end\":20382,\"start\":20250},{\"end\":20591,\"start\":20404},{\"end\":20726,\"start\":20611},{\"end\":20907,\"start\":20748},{\"end\":21626,\"start\":20958},{\"end\":21638,\"start\":21628},{\"end\":22392,\"start\":21686},{\"end\":22894,\"start\":22423},{\"end\":22980,\"start\":22926},{\"end\":23507,\"start\":22982},{\"end\":24608,\"start\":23535},{\"end\":25893,\"start\":24667},{\"end\":26329,\"start\":25917},{\"end\":26922,\"start\":26356},{\"end\":27272,\"start\":26943},{\"end\":28932,\"start\":27301},{\"end\":30069,\"start\":28956},{\"end\":31715,\"start\":30071},{\"end\":32490,\"start\":31717},{\"end\":32663,\"start\":32536},{\"end\":33523,\"start\":32665},{\"end\":34126,\"start\":33525},{\"end\":36558,\"start\":34128},{\"end\":36760,\"start\":36599},{\"end\":37054,\"start\":36762},{\"end\":37657,\"start\":37056},{\"end\":38876,\"start\":37674},{\"end\":39835,\"start\":38878},{\"end\":42069,\"start\":39837},{\"end\":43325,\"start\":42084}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13007,\"start\":12929},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13239,\"start\":13222},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13548,\"start\":13518},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17307,\"start\":17233},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18143,\"start\":18065},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18723,\"start\":18703},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19141,\"start\":19135},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20179,\"start\":20163},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20403,\"start\":20383},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20610,\"start\":20592},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20747,\"start\":20727},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20941,\"start\":20908},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21685,\"start\":21639},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22925,\"start\":22895},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23534,\"start\":23508}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25345,\"start\":25338},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34339,\"start\":34332}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3625,\"start\":3613},{\"attributes\":{\"n\":\"2\"},\"end\":11990,\"start\":11969},{\"attributes\":{\"n\":\"2.2\"},\"end\":16021,\"start\":16008},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":18807,\"start\":18774},{\"attributes\":{\"n\":\"2.2.3\"},\"end\":19520,\"start\":19491},{\"attributes\":{\"n\":\"2.2.4\"},\"end\":20956,\"start\":20943},{\"attributes\":{\"n\":\"2.3\"},\"end\":22421,\"start\":22395},{\"attributes\":{\"n\":\"3\"},\"end\":24645,\"start\":24611},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":24665,\"start\":24648},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":25915,\"start\":25896},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":26354,\"start\":26332},{\"attributes\":{\"n\":\"3.2\"},\"end\":26941,\"start\":26925},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":27299,\"start\":27275},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":28954,\"start\":28935},{\"attributes\":{\"n\":\"3.3\"},\"end\":32521,\"start\":32493},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":32534,\"start\":32524},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":36580,\"start\":36561},{\"attributes\":{\"n\":\"3.4\"},\"end\":36597,\"start\":36583},{\"attributes\":{\"n\":\"4\"},\"end\":37672,\"start\":37660},{\"attributes\":{\"n\":\"5\"},\"end\":42082,\"start\":42072},{\"end\":43342,\"start\":43327},{\"end\":43526,\"start\":43516},{\"end\":43831,\"start\":43821},{\"end\":44039,\"start\":44029},{\"end\":44127,\"start\":44118},{\"end\":44367,\"start\":44358},{\"end\":44986,\"start\":44977}]", "table": "[{\"end\":44356,\"start\":44175},{\"end\":44975,\"start\":44369},{\"end\":45357,\"start\":45119}]", "figure_caption": "[{\"end\":43514,\"start\":43345},{\"end\":43819,\"start\":43528},{\"end\":44027,\"start\":43833},{\"end\":44116,\"start\":44041},{\"end\":44175,\"start\":44129},{\"end\":45119,\"start\":44988}]", "figure_ref": "[{\"end\":14341,\"start\":14335},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20081,\"start\":20075},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27636,\"start\":27630},{\"end\":28558,\"start\":28552},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29067,\"start\":29061},{\"end\":29574,\"start\":29568},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31179,\"start\":31170},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31193,\"start\":31184}]", "bib_author_first_name": "[{\"end\":45931,\"start\":45930},{\"end\":45946,\"start\":45940},{\"end\":46173,\"start\":46164},{\"end\":46188,\"start\":46180},{\"end\":46204,\"start\":46195},{\"end\":46219,\"start\":46211},{\"end\":46500,\"start\":46497},{\"end\":46509,\"start\":46507},{\"end\":46521,\"start\":46514},{\"end\":46531,\"start\":46528},{\"end\":46543,\"start\":46539},{\"end\":46892,\"start\":46886},{\"end\":46908,\"start\":46900},{\"end\":46917,\"start\":46914},{\"end\":46933,\"start\":46927},{\"end\":46948,\"start\":46942},{\"end\":46962,\"start\":46958},{\"end\":46976,\"start\":46972},{\"end\":46990,\"start\":46987},{\"end\":47007,\"start\":47000},{\"end\":47564,\"start\":47557},{\"end\":47579,\"start\":47572},{\"end\":47590,\"start\":47587},{\"end\":47597,\"start\":47596},{\"end\":47609,\"start\":47604},{\"end\":47920,\"start\":47913},{\"end\":47932,\"start\":47928},{\"end\":47942,\"start\":47939},{\"end\":47959,\"start\":47948},{\"end\":48420,\"start\":48414},{\"end\":48439,\"start\":48434},{\"end\":48453,\"start\":48448},{\"end\":48469,\"start\":48462},{\"end\":48931,\"start\":48923},{\"end\":48953,\"start\":48945},{\"end\":48964,\"start\":48959},{\"end\":48976,\"start\":48970},{\"end\":48986,\"start\":48982},{\"end\":49000,\"start\":48991},{\"end\":49537,\"start\":49532},{\"end\":49546,\"start\":49543},{\"end\":49555,\"start\":49551},{\"end\":49564,\"start\":49560},{\"end\":49575,\"start\":49569},{\"end\":49580,\"start\":49576},{\"end\":49594,\"start\":49587},{\"end\":49606,\"start\":49601},{\"end\":50082,\"start\":50074},{\"end\":50095,\"start\":50090},{\"end\":50116,\"start\":50105},{\"end\":50131,\"start\":50126},{\"end\":50495,\"start\":50490},{\"end\":50509,\"start\":50502},{\"end\":51007,\"start\":51000},{\"end\":51018,\"start\":51012},{\"end\":51446,\"start\":51438},{\"end\":51454,\"start\":51451},{\"end\":51468,\"start\":51461},{\"end\":51478,\"start\":51474},{\"end\":51990,\"start\":51982},{\"end\":51999,\"start\":51995},{\"end\":52011,\"start\":52006},{\"end\":52021,\"start\":52018},{\"end\":52034,\"start\":52026},{\"end\":52046,\"start\":52042},{\"end\":52600,\"start\":52592},{\"end\":52609,\"start\":52605},{\"end\":52623,\"start\":52616},{\"end\":52638,\"start\":52631},{\"end\":52647,\"start\":52644},{\"end\":52660,\"start\":52652},{\"end\":53033,\"start\":53023},{\"end\":53047,\"start\":53041},{\"end\":53057,\"start\":53054},{\"end\":53071,\"start\":53063},{\"end\":53082,\"start\":53077},{\"end\":53100,\"start\":53093},{\"end\":53489,\"start\":53488},{\"end\":53505,\"start\":53500},{\"end\":53887,\"start\":53886},{\"end\":53899,\"start\":53896},{\"end\":54130,\"start\":54124},{\"end\":54144,\"start\":54138},{\"end\":54156,\"start\":54151},{\"end\":54411,\"start\":54408},{\"end\":54424,\"start\":54417},{\"end\":54442,\"start\":54432},{\"end\":54456,\"start\":54448},{\"end\":54470,\"start\":54463},{\"end\":54481,\"start\":54476},{\"end\":54938,\"start\":54935},{\"end\":54951,\"start\":54944},{\"end\":54962,\"start\":54959},{\"end\":54976,\"start\":54968},{\"end\":54989,\"start\":54982},{\"end\":55336,\"start\":55333},{\"end\":55345,\"start\":55342},{\"end\":55950,\"start\":55942},{\"end\":55964,\"start\":55956},{\"end\":55977,\"start\":55970},{\"end\":55993,\"start\":55985},{\"end\":56003,\"start\":55999},{\"end\":56014,\"start\":56009},{\"end\":56434,\"start\":56433},{\"end\":56450,\"start\":56449},{\"end\":56465,\"start\":56457},{\"end\":56838,\"start\":56832},{\"end\":56852,\"start\":56848},{\"end\":57242,\"start\":57234},{\"end\":57252,\"start\":57248},{\"end\":57262,\"start\":57259},{\"end\":57273,\"start\":57267},{\"end\":57287,\"start\":57280},{\"end\":57297,\"start\":57294},{\"end\":57814,\"start\":57807},{\"end\":57832,\"start\":57823},{\"end\":57852,\"start\":57848},{\"end\":57866,\"start\":57862},{\"end\":58108,\"start\":58101},{\"end\":58121,\"start\":58114},{\"end\":58133,\"start\":58129},{\"end\":58142,\"start\":58138},{\"end\":58158,\"start\":58151},{\"end\":58569,\"start\":58563},{\"end\":58585,\"start\":58579},{\"end\":58587,\"start\":58586},{\"end\":58597,\"start\":58594},{\"end\":58883,\"start\":58876},{\"end\":58897,\"start\":58890},{\"end\":58911,\"start\":58905},{\"end\":58922,\"start\":58918},{\"end\":58935,\"start\":58929},{\"end\":58944,\"start\":58940},{\"end\":58955,\"start\":58950},{\"end\":59271,\"start\":59264},{\"end\":59285,\"start\":59278},{\"end\":59299,\"start\":59293},{\"end\":59310,\"start\":59306},{\"end\":59323,\"start\":59317},{\"end\":59332,\"start\":59328},{\"end\":59343,\"start\":59338},{\"end\":59669,\"start\":59664},{\"end\":59684,\"start\":59676},{\"end\":59694,\"start\":59689},{\"end\":59704,\"start\":59700},{\"end\":59718,\"start\":59710},{\"end\":60203,\"start\":60198},{\"end\":60218,\"start\":60210},{\"end\":60227,\"start\":60223},{\"end\":60238,\"start\":60234},{\"end\":60253,\"start\":60245},{\"end\":60773,\"start\":60767},{\"end\":60786,\"start\":60779},{\"end\":60801,\"start\":60794},{\"end\":60810,\"start\":60806},{\"end\":60820,\"start\":60817},{\"end\":60833,\"start\":60826},{\"end\":61319,\"start\":61313},{\"end\":61330,\"start\":61325},{\"end\":61344,\"start\":61337},{\"end\":61824,\"start\":61818},{\"end\":61835,\"start\":61830},{\"end\":61849,\"start\":61842},{\"end\":62338,\"start\":62333},{\"end\":62349,\"start\":62343},{\"end\":62363,\"start\":62357},{\"end\":62382,\"start\":62371},{\"end\":62393,\"start\":62390},{\"end\":62404,\"start\":62398},{\"end\":62648,\"start\":62645},{\"end\":62664,\"start\":62653},{\"end\":62678,\"start\":62673},{\"end\":62680,\"start\":62679},{\"end\":62694,\"start\":62688},{\"end\":63175,\"start\":63169},{\"end\":63190,\"start\":63184},{\"end\":63643,\"start\":63640},{\"end\":63659,\"start\":63649},{\"end\":63674,\"start\":63673},{\"end\":63686,\"start\":63685},{\"end\":63882,\"start\":63874},{\"end\":63893,\"start\":63888},{\"end\":63907,\"start\":63899},{\"end\":63922,\"start\":63915},{\"end\":63936,\"start\":63930},{\"end\":64415,\"start\":64405},{\"end\":64431,\"start\":64422},{\"end\":64446,\"start\":64438},{\"end\":64462,\"start\":64453},{\"end\":64715,\"start\":64712},{\"end\":64729,\"start\":64722},{\"end\":64741,\"start\":64734},{\"end\":64752,\"start\":64748},{\"end\":64774,\"start\":64767},{\"end\":64776,\"start\":64775},{\"end\":64791,\"start\":64787},{\"end\":65374,\"start\":65372},{\"end\":65384,\"start\":65379},{\"end\":65399,\"start\":65392},{\"end\":65411,\"start\":65405},{\"end\":65427,\"start\":65417},{\"end\":65738,\"start\":65731},{\"end\":65752,\"start\":65746}]", "bib_author_last_name": "[{\"end\":45938,\"start\":45932},{\"end\":45951,\"start\":45947},{\"end\":45958,\"start\":45953},{\"end\":46178,\"start\":46174},{\"end\":46193,\"start\":46189},{\"end\":46209,\"start\":46205},{\"end\":46224,\"start\":46220},{\"end\":46505,\"start\":46501},{\"end\":46512,\"start\":46510},{\"end\":46526,\"start\":46522},{\"end\":46537,\"start\":46532},{\"end\":46548,\"start\":46544},{\"end\":46884,\"start\":46876},{\"end\":46898,\"start\":46893},{\"end\":46912,\"start\":46909},{\"end\":46925,\"start\":46918},{\"end\":46940,\"start\":46934},{\"end\":46956,\"start\":46949},{\"end\":46970,\"start\":46963},{\"end\":46985,\"start\":46977},{\"end\":46998,\"start\":46991},{\"end\":47012,\"start\":47008},{\"end\":47019,\"start\":47014},{\"end\":47570,\"start\":47565},{\"end\":47585,\"start\":47580},{\"end\":47594,\"start\":47591},{\"end\":47602,\"start\":47598},{\"end\":47624,\"start\":47610},{\"end\":47637,\"start\":47626},{\"end\":47926,\"start\":47921},{\"end\":47937,\"start\":47933},{\"end\":47946,\"start\":47943},{\"end\":47965,\"start\":47960},{\"end\":48432,\"start\":48421},{\"end\":48446,\"start\":48440},{\"end\":48460,\"start\":48454},{\"end\":48479,\"start\":48470},{\"end\":48943,\"start\":48932},{\"end\":48957,\"start\":48954},{\"end\":48968,\"start\":48965},{\"end\":48980,\"start\":48977},{\"end\":48989,\"start\":48987},{\"end\":49003,\"start\":49001},{\"end\":49007,\"start\":49005},{\"end\":49541,\"start\":49538},{\"end\":49549,\"start\":49547},{\"end\":49558,\"start\":49556},{\"end\":49567,\"start\":49565},{\"end\":49585,\"start\":49581},{\"end\":49599,\"start\":49595},{\"end\":49610,\"start\":49607},{\"end\":50088,\"start\":50083},{\"end\":50103,\"start\":50096},{\"end\":50124,\"start\":50117},{\"end\":50139,\"start\":50132},{\"end\":50500,\"start\":50496},{\"end\":50515,\"start\":50510},{\"end\":51010,\"start\":51008},{\"end\":51026,\"start\":51019},{\"end\":51449,\"start\":51447},{\"end\":51459,\"start\":51455},{\"end\":51472,\"start\":51469},{\"end\":51483,\"start\":51479},{\"end\":51993,\"start\":51991},{\"end\":52004,\"start\":52000},{\"end\":52016,\"start\":52012},{\"end\":52024,\"start\":52022},{\"end\":52040,\"start\":52035},{\"end\":52051,\"start\":52047},{\"end\":52603,\"start\":52601},{\"end\":52614,\"start\":52610},{\"end\":52629,\"start\":52624},{\"end\":52642,\"start\":52639},{\"end\":52650,\"start\":52648},{\"end\":52665,\"start\":52661},{\"end\":53039,\"start\":53034},{\"end\":53052,\"start\":53048},{\"end\":53061,\"start\":53058},{\"end\":53075,\"start\":53072},{\"end\":53091,\"start\":53083},{\"end\":53107,\"start\":53101},{\"end\":53498,\"start\":53490},{\"end\":53512,\"start\":53506},{\"end\":53516,\"start\":53514},{\"end\":53894,\"start\":53888},{\"end\":53904,\"start\":53900},{\"end\":53913,\"start\":53906},{\"end\":54136,\"start\":54131},{\"end\":54149,\"start\":54145},{\"end\":54165,\"start\":54157},{\"end\":54415,\"start\":54412},{\"end\":54430,\"start\":54425},{\"end\":54446,\"start\":54443},{\"end\":54461,\"start\":54457},{\"end\":54474,\"start\":54471},{\"end\":54493,\"start\":54482},{\"end\":54942,\"start\":54939},{\"end\":54957,\"start\":54952},{\"end\":54966,\"start\":54963},{\"end\":54980,\"start\":54977},{\"end\":54993,\"start\":54990},{\"end\":55340,\"start\":55337},{\"end\":55348,\"start\":55346},{\"end\":55954,\"start\":55951},{\"end\":55968,\"start\":55965},{\"end\":55983,\"start\":55978},{\"end\":55997,\"start\":55994},{\"end\":56007,\"start\":56004},{\"end\":56018,\"start\":56015},{\"end\":56441,\"start\":56435},{\"end\":56447,\"start\":56443},{\"end\":56455,\"start\":56451},{\"end\":56472,\"start\":56466},{\"end\":56476,\"start\":56474},{\"end\":56846,\"start\":56839},{\"end\":56861,\"start\":56853},{\"end\":57246,\"start\":57243},{\"end\":57257,\"start\":57253},{\"end\":57265,\"start\":57263},{\"end\":57278,\"start\":57274},{\"end\":57292,\"start\":57288},{\"end\":57302,\"start\":57298},{\"end\":57821,\"start\":57815},{\"end\":57846,\"start\":57833},{\"end\":57860,\"start\":57853},{\"end\":57881,\"start\":57867},{\"end\":58112,\"start\":58109},{\"end\":58127,\"start\":58122},{\"end\":58136,\"start\":58134},{\"end\":58149,\"start\":58143},{\"end\":58162,\"start\":58159},{\"end\":58577,\"start\":58570},{\"end\":58592,\"start\":58588},{\"end\":58602,\"start\":58598},{\"end\":58611,\"start\":58604},{\"end\":58888,\"start\":58884},{\"end\":58903,\"start\":58898},{\"end\":58916,\"start\":58912},{\"end\":58927,\"start\":58923},{\"end\":58938,\"start\":58936},{\"end\":58948,\"start\":58945},{\"end\":58959,\"start\":58956},{\"end\":59276,\"start\":59272},{\"end\":59291,\"start\":59286},{\"end\":59304,\"start\":59300},{\"end\":59315,\"start\":59311},{\"end\":59326,\"start\":59324},{\"end\":59336,\"start\":59333},{\"end\":59347,\"start\":59344},{\"end\":59674,\"start\":59670},{\"end\":59687,\"start\":59685},{\"end\":59698,\"start\":59695},{\"end\":59708,\"start\":59705},{\"end\":59723,\"start\":59719},{\"end\":60208,\"start\":60204},{\"end\":60221,\"start\":60219},{\"end\":60232,\"start\":60228},{\"end\":60243,\"start\":60239},{\"end\":60258,\"start\":60254},{\"end\":60777,\"start\":60774},{\"end\":60792,\"start\":60787},{\"end\":60804,\"start\":60802},{\"end\":60815,\"start\":60811},{\"end\":60824,\"start\":60821},{\"end\":60837,\"start\":60834},{\"end\":61323,\"start\":61320},{\"end\":61335,\"start\":61331},{\"end\":61348,\"start\":61345},{\"end\":61828,\"start\":61825},{\"end\":61840,\"start\":61836},{\"end\":61853,\"start\":61850},{\"end\":62341,\"start\":62339},{\"end\":62355,\"start\":62350},{\"end\":62369,\"start\":62364},{\"end\":62388,\"start\":62383},{\"end\":62396,\"start\":62394},{\"end\":62415,\"start\":62405},{\"end\":62651,\"start\":62649},{\"end\":62671,\"start\":62665},{\"end\":62686,\"start\":62681},{\"end\":62700,\"start\":62695},{\"end\":63182,\"start\":63176},{\"end\":63197,\"start\":63191},{\"end\":63647,\"start\":63644},{\"end\":63671,\"start\":63660},{\"end\":63683,\"start\":63675},{\"end\":63691,\"start\":63687},{\"end\":63886,\"start\":63883},{\"end\":63897,\"start\":63894},{\"end\":63913,\"start\":63908},{\"end\":63928,\"start\":63923},{\"end\":63941,\"start\":63937},{\"end\":64420,\"start\":64416},{\"end\":64436,\"start\":64432},{\"end\":64451,\"start\":64447},{\"end\":64467,\"start\":64463},{\"end\":64720,\"start\":64716},{\"end\":64732,\"start\":64730},{\"end\":64746,\"start\":64742},{\"end\":64765,\"start\":64753},{\"end\":64785,\"start\":64777},{\"end\":64800,\"start\":64792},{\"end\":65377,\"start\":65375},{\"end\":65390,\"start\":65385},{\"end\":65403,\"start\":65400},{\"end\":65415,\"start\":65412},{\"end\":65433,\"start\":65428},{\"end\":65744,\"start\":65739},{\"end\":65760,\"start\":65753}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1851838},\"end\":46102,\"start\":45888},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":67749794},\"end\":46391,\"start\":46104},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":210932292},\"end\":46872,\"start\":46393},{\"attributes\":{\"id\":\"b3\"},\"end\":47171,\"start\":46874},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3352400},\"end\":47486,\"start\":47173},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":53290927},\"end\":47833,\"start\":47488},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3458700},\"end\":48315,\"start\":47835},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16548458},\"end\":48843,\"start\":48317},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":196199861},\"end\":49481,\"start\":48845},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":67769538},\"end\":49957,\"start\":49483},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12477253},\"end\":50415,\"start\":49959},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2625992},\"end\":50931,\"start\":50417},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3099285},\"end\":51367,\"start\":50933},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2023519},\"end\":51899,\"start\":51369},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":211043589},\"end\":52558,\"start\":51901},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13907106},\"end\":52990,\"start\":52560},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11129957},\"end\":53442,\"start\":52992},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":53818,\"start\":53444},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3144218},\"end\":54065,\"start\":53820},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":58370896},\"end\":54332,\"start\":54067},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":201125251},\"end\":54874,\"start\":54334},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":214605979},\"end\":55242,\"start\":54876},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18147416},\"end\":55870,\"start\":55244},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":215842890},\"end\":56366,\"start\":55872},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16489696},\"end\":56746,\"start\":56368},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6440341},\"end\":57175,\"start\":56748},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":49862561},\"end\":57746,\"start\":57177},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10795036},\"end\":58050,\"start\":57748},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":209531818},\"end\":58522,\"start\":58052},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":36809545},\"end\":58786,\"start\":58524},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3766110},\"end\":59177,\"start\":58788},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":83458637},\"end\":59602,\"start\":59179},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":159042183},\"end\":60158,\"start\":59604},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":150380651},\"end\":60711,\"start\":60160},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":201646575},\"end\":61183,\"start\":60713},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":222278410},\"end\":61723,\"start\":61185},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":201701022},\"end\":62289,\"start\":61725},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b37\"},\"end\":62574,\"start\":62291},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6392154},\"end\":63092,\"start\":62576},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5575601},\"end\":63603,\"start\":63094},{\"attributes\":{\"id\":\"b40\"},\"end\":63814,\"start\":63605},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":27308776},\"end\":64344,\"start\":63816},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52901452},\"end\":64639,\"start\":64346},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":46949657},\"end\":65276,\"start\":64641},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4672722},\"end\":65651,\"start\":65278},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":215813246},\"end\":66134,\"start\":65653}]", "bib_title": "[{\"end\":45928,\"start\":45888},{\"end\":46162,\"start\":46104},{\"end\":46495,\"start\":46393},{\"end\":47217,\"start\":47173},{\"end\":47555,\"start\":47488},{\"end\":47911,\"start\":47835},{\"end\":48412,\"start\":48317},{\"end\":48921,\"start\":48845},{\"end\":49530,\"start\":49483},{\"end\":50072,\"start\":49959},{\"end\":50488,\"start\":50417},{\"end\":50998,\"start\":50933},{\"end\":51436,\"start\":51369},{\"end\":51980,\"start\":51901},{\"end\":52590,\"start\":52560},{\"end\":53021,\"start\":52992},{\"end\":53486,\"start\":53444},{\"end\":53884,\"start\":53820},{\"end\":54122,\"start\":54067},{\"end\":54406,\"start\":54334},{\"end\":54933,\"start\":54876},{\"end\":55331,\"start\":55244},{\"end\":55940,\"start\":55872},{\"end\":56431,\"start\":56368},{\"end\":56830,\"start\":56748},{\"end\":57232,\"start\":57177},{\"end\":57805,\"start\":57748},{\"end\":58099,\"start\":58052},{\"end\":58561,\"start\":58524},{\"end\":58874,\"start\":58788},{\"end\":59262,\"start\":59179},{\"end\":59662,\"start\":59604},{\"end\":60196,\"start\":60160},{\"end\":60765,\"start\":60713},{\"end\":61311,\"start\":61185},{\"end\":61816,\"start\":61725},{\"end\":62643,\"start\":62576},{\"end\":63167,\"start\":63094},{\"end\":63872,\"start\":63816},{\"end\":64403,\"start\":64346},{\"end\":64710,\"start\":64641},{\"end\":65370,\"start\":65278},{\"end\":65729,\"start\":65653}]", "bib_author": "[{\"end\":45940,\"start\":45930},{\"end\":45953,\"start\":45940},{\"end\":45960,\"start\":45953},{\"end\":46180,\"start\":46164},{\"end\":46195,\"start\":46180},{\"end\":46211,\"start\":46195},{\"end\":46226,\"start\":46211},{\"end\":46507,\"start\":46497},{\"end\":46514,\"start\":46507},{\"end\":46528,\"start\":46514},{\"end\":46539,\"start\":46528},{\"end\":46550,\"start\":46539},{\"end\":46886,\"start\":46876},{\"end\":46900,\"start\":46886},{\"end\":46914,\"start\":46900},{\"end\":46927,\"start\":46914},{\"end\":46942,\"start\":46927},{\"end\":46958,\"start\":46942},{\"end\":46972,\"start\":46958},{\"end\":46987,\"start\":46972},{\"end\":47000,\"start\":46987},{\"end\":47014,\"start\":47000},{\"end\":47021,\"start\":47014},{\"end\":47572,\"start\":47557},{\"end\":47587,\"start\":47572},{\"end\":47596,\"start\":47587},{\"end\":47604,\"start\":47596},{\"end\":47626,\"start\":47604},{\"end\":47639,\"start\":47626},{\"end\":47928,\"start\":47913},{\"end\":47939,\"start\":47928},{\"end\":47948,\"start\":47939},{\"end\":47967,\"start\":47948},{\"end\":48434,\"start\":48414},{\"end\":48448,\"start\":48434},{\"end\":48462,\"start\":48448},{\"end\":48481,\"start\":48462},{\"end\":48945,\"start\":48923},{\"end\":48959,\"start\":48945},{\"end\":48970,\"start\":48959},{\"end\":48982,\"start\":48970},{\"end\":48991,\"start\":48982},{\"end\":49005,\"start\":48991},{\"end\":49009,\"start\":49005},{\"end\":49543,\"start\":49532},{\"end\":49551,\"start\":49543},{\"end\":49560,\"start\":49551},{\"end\":49569,\"start\":49560},{\"end\":49587,\"start\":49569},{\"end\":49601,\"start\":49587},{\"end\":49612,\"start\":49601},{\"end\":50090,\"start\":50074},{\"end\":50105,\"start\":50090},{\"end\":50126,\"start\":50105},{\"end\":50141,\"start\":50126},{\"end\":50502,\"start\":50490},{\"end\":50517,\"start\":50502},{\"end\":51012,\"start\":51000},{\"end\":51028,\"start\":51012},{\"end\":51451,\"start\":51438},{\"end\":51461,\"start\":51451},{\"end\":51474,\"start\":51461},{\"end\":51485,\"start\":51474},{\"end\":51995,\"start\":51982},{\"end\":52006,\"start\":51995},{\"end\":52018,\"start\":52006},{\"end\":52026,\"start\":52018},{\"end\":52042,\"start\":52026},{\"end\":52053,\"start\":52042},{\"end\":52605,\"start\":52592},{\"end\":52616,\"start\":52605},{\"end\":52631,\"start\":52616},{\"end\":52644,\"start\":52631},{\"end\":52652,\"start\":52644},{\"end\":52667,\"start\":52652},{\"end\":53041,\"start\":53023},{\"end\":53054,\"start\":53041},{\"end\":53063,\"start\":53054},{\"end\":53077,\"start\":53063},{\"end\":53093,\"start\":53077},{\"end\":53109,\"start\":53093},{\"end\":53500,\"start\":53488},{\"end\":53514,\"start\":53500},{\"end\":53518,\"start\":53514},{\"end\":53896,\"start\":53886},{\"end\":53906,\"start\":53896},{\"end\":53915,\"start\":53906},{\"end\":54138,\"start\":54124},{\"end\":54151,\"start\":54138},{\"end\":54167,\"start\":54151},{\"end\":54417,\"start\":54408},{\"end\":54432,\"start\":54417},{\"end\":54448,\"start\":54432},{\"end\":54463,\"start\":54448},{\"end\":54476,\"start\":54463},{\"end\":54495,\"start\":54476},{\"end\":54944,\"start\":54935},{\"end\":54959,\"start\":54944},{\"end\":54968,\"start\":54959},{\"end\":54982,\"start\":54968},{\"end\":54995,\"start\":54982},{\"end\":55342,\"start\":55333},{\"end\":55350,\"start\":55342},{\"end\":55956,\"start\":55942},{\"end\":55970,\"start\":55956},{\"end\":55985,\"start\":55970},{\"end\":55999,\"start\":55985},{\"end\":56009,\"start\":55999},{\"end\":56020,\"start\":56009},{\"end\":56443,\"start\":56433},{\"end\":56449,\"start\":56443},{\"end\":56457,\"start\":56449},{\"end\":56474,\"start\":56457},{\"end\":56478,\"start\":56474},{\"end\":56848,\"start\":56832},{\"end\":56863,\"start\":56848},{\"end\":57248,\"start\":57234},{\"end\":57259,\"start\":57248},{\"end\":57267,\"start\":57259},{\"end\":57280,\"start\":57267},{\"end\":57294,\"start\":57280},{\"end\":57304,\"start\":57294},{\"end\":57823,\"start\":57807},{\"end\":57848,\"start\":57823},{\"end\":57862,\"start\":57848},{\"end\":57883,\"start\":57862},{\"end\":58114,\"start\":58101},{\"end\":58129,\"start\":58114},{\"end\":58138,\"start\":58129},{\"end\":58151,\"start\":58138},{\"end\":58164,\"start\":58151},{\"end\":58579,\"start\":58563},{\"end\":58594,\"start\":58579},{\"end\":58604,\"start\":58594},{\"end\":58613,\"start\":58604},{\"end\":58890,\"start\":58876},{\"end\":58905,\"start\":58890},{\"end\":58918,\"start\":58905},{\"end\":58929,\"start\":58918},{\"end\":58940,\"start\":58929},{\"end\":58950,\"start\":58940},{\"end\":58961,\"start\":58950},{\"end\":59278,\"start\":59264},{\"end\":59293,\"start\":59278},{\"end\":59306,\"start\":59293},{\"end\":59317,\"start\":59306},{\"end\":59328,\"start\":59317},{\"end\":59338,\"start\":59328},{\"end\":59349,\"start\":59338},{\"end\":59676,\"start\":59664},{\"end\":59689,\"start\":59676},{\"end\":59700,\"start\":59689},{\"end\":59710,\"start\":59700},{\"end\":59725,\"start\":59710},{\"end\":60210,\"start\":60198},{\"end\":60223,\"start\":60210},{\"end\":60234,\"start\":60223},{\"end\":60245,\"start\":60234},{\"end\":60260,\"start\":60245},{\"end\":60779,\"start\":60767},{\"end\":60794,\"start\":60779},{\"end\":60806,\"start\":60794},{\"end\":60817,\"start\":60806},{\"end\":60826,\"start\":60817},{\"end\":60839,\"start\":60826},{\"end\":61325,\"start\":61313},{\"end\":61337,\"start\":61325},{\"end\":61350,\"start\":61337},{\"end\":61830,\"start\":61818},{\"end\":61842,\"start\":61830},{\"end\":61855,\"start\":61842},{\"end\":62343,\"start\":62333},{\"end\":62357,\"start\":62343},{\"end\":62371,\"start\":62357},{\"end\":62390,\"start\":62371},{\"end\":62398,\"start\":62390},{\"end\":62417,\"start\":62398},{\"end\":62653,\"start\":62645},{\"end\":62673,\"start\":62653},{\"end\":62688,\"start\":62673},{\"end\":62702,\"start\":62688},{\"end\":63184,\"start\":63169},{\"end\":63199,\"start\":63184},{\"end\":63649,\"start\":63640},{\"end\":63673,\"start\":63649},{\"end\":63685,\"start\":63673},{\"end\":63693,\"start\":63685},{\"end\":63888,\"start\":63874},{\"end\":63899,\"start\":63888},{\"end\":63915,\"start\":63899},{\"end\":63930,\"start\":63915},{\"end\":63943,\"start\":63930},{\"end\":64422,\"start\":64405},{\"end\":64438,\"start\":64422},{\"end\":64453,\"start\":64438},{\"end\":64469,\"start\":64453},{\"end\":64722,\"start\":64712},{\"end\":64734,\"start\":64722},{\"end\":64748,\"start\":64734},{\"end\":64767,\"start\":64748},{\"end\":64787,\"start\":64767},{\"end\":64802,\"start\":64787},{\"end\":65379,\"start\":65372},{\"end\":65392,\"start\":65379},{\"end\":65405,\"start\":65392},{\"end\":65417,\"start\":65405},{\"end\":65435,\"start\":65417},{\"end\":65746,\"start\":65731},{\"end\":65762,\"start\":65746}]", "bib_venue": "[{\"end\":47350,\"start\":47293},{\"end\":48086,\"start\":48035},{\"end\":48588,\"start\":48543},{\"end\":49188,\"start\":49107},{\"end\":49731,\"start\":49680},{\"end\":50664,\"start\":50599},{\"end\":51157,\"start\":51101},{\"end\":51656,\"start\":51579},{\"end\":52262,\"start\":52166},{\"end\":52786,\"start\":52735},{\"end\":53228,\"start\":53177},{\"end\":53655,\"start\":53595},{\"end\":54614,\"start\":54563},{\"end\":55559,\"start\":55463},{\"end\":56083,\"start\":56060},{\"end\":56970,\"start\":56925},{\"end\":57487,\"start\":57404},{\"end\":58269,\"start\":58225},{\"end\":59908,\"start\":59825},{\"end\":60469,\"start\":60373},{\"end\":60958,\"start\":60907},{\"end\":61469,\"start\":61418},{\"end\":61974,\"start\":61923},{\"end\":62851,\"start\":62785},{\"end\":63378,\"start\":63297},{\"end\":64090,\"start\":64025},{\"end\":64985,\"start\":64902},{\"end\":65885,\"start\":65832},{\"end\":45984,\"start\":45960},{\"end\":46234,\"start\":46226},{\"end\":46610,\"start\":46550},{\"end\":47291,\"start\":47219},{\"end\":47643,\"start\":47639},{\"end\":48033,\"start\":47967},{\"end\":48541,\"start\":48481},{\"end\":49105,\"start\":49009},{\"end\":49678,\"start\":49612},{\"end\":50168,\"start\":50141},{\"end\":50597,\"start\":50517},{\"end\":51099,\"start\":51028},{\"end\":51577,\"start\":51485},{\"end\":52164,\"start\":52053},{\"end\":52733,\"start\":52667},{\"end\":53175,\"start\":53109},{\"end\":53593,\"start\":53518},{\"end\":53935,\"start\":53915},{\"end\":54183,\"start\":54167},{\"end\":54561,\"start\":54495},{\"end\":55046,\"start\":54995},{\"end\":55461,\"start\":55350},{\"end\":56058,\"start\":56020},{\"end\":56550,\"start\":56478},{\"end\":56923,\"start\":56863},{\"end\":57402,\"start\":57304},{\"end\":57886,\"start\":57883},{\"end\":58223,\"start\":58164},{\"end\":58642,\"start\":58613},{\"end\":58970,\"start\":58961},{\"end\":59369,\"start\":59349},{\"end\":59823,\"start\":59725},{\"end\":60371,\"start\":60260},{\"end\":60905,\"start\":60839},{\"end\":61416,\"start\":61350},{\"end\":61921,\"start\":61855},{\"end\":62331,\"start\":62291},{\"end\":62783,\"start\":62702},{\"end\":63295,\"start\":63199},{\"end\":63638,\"start\":63605},{\"end\":64023,\"start\":63943},{\"end\":64480,\"start\":64469},{\"end\":64900,\"start\":64802},{\"end\":65440,\"start\":65435},{\"end\":65830,\"start\":65762}]"}}}, "year": 2023, "month": 12, "day": 17}