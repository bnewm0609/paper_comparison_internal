{"id": 220831008, "updated": "2023-10-06 12:49:24.898", "metadata": {"title": "Flower: A Friendly Federated Learning Research Framework", "authors": "[{\"first\":\"Daniel\",\"last\":\"Beutel\",\"middle\":[\"J.\"]},{\"first\":\"Taner\",\"last\":\"Topal\",\"middle\":[]},{\"first\":\"Akhil\",\"last\":\"Mathur\",\"middle\":[]},{\"first\":\"Xinchi\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Javier\",\"last\":\"Fernandez-Marques\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Lorenzo\",\"last\":\"Sani\",\"middle\":[]},{\"first\":\"Kwing\",\"last\":\"Li\",\"middle\":[\"Hei\"]},{\"first\":\"Titouan\",\"last\":\"Parcollet\",\"middle\":[]},{\"first\":\"Pedro\",\"last\":\"Gusmao\",\"middle\":[\"Porto\",\"Buarque\",\"de\"]},{\"first\":\"Nicholas\",\"last\":\"Lane\",\"middle\":[\"D.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store the data in the cloud. However, FL is difficult to implement realistically, both in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices. In this paper, we present Flower -- a comprehensive FL framework that distinguishes itself from existing platforms by offering new facilities to execute large-scale FL experiments and consider richly heterogeneous FL device scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of the design space. We believe Flower provides the community with a critical new tool for FL study and development.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2007.14390", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "a199a03e11b68c4132be880b5fcabc57251bc477", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.14390v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "87aa6f73730c98b56a40e58522680b7e91b9359e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a199a03e11b68c4132be880b5fcabc57251bc477.txt", "contents": "\nFLOWER: A FRIENDLY FEDERATED LEARNING FRAMEWORK\n\n\nDaniel J Beutel \nTaner Topal \nAkhil Mathur \nXinchi Qiu \nJavier Fernandez-Marques \nYan Gao \nLorenzo Sani \nKwing Hei Li \nTitouan Parcollet \nPedro Porto \nBuarque De Gusm\u00e3o \nNicholas D Lane \nFLOWER: A FRIENDLY FEDERATED LEARNING FRAMEWORK\n\nFederated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store the data in the cloud. However, FL is difficult to implement realistically, both in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices.In this paper, we present Flower -a comprehensive FL framework that distinguishes itself from existing platforms by offering new facilities to execute large-scale FL experiments, and consider richly heterogeneous FL device scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of the design space. We believe Flower provides the community a critical new tool for FL study and development.\n\nINTRODUCTION\n\nThere has been tremendous progress in enabling the execution of deep learning models on mobile and embedded devices to infer user contexts and behaviors (Fromm et al., 2018;Chowdhery et al., 2019;Malekzadeh et al., 2019;Lee et al., 2019;Yao et al., 2019;LiKamWa et al., 2016;Georgiev et al., 2017). This has been powered by the increasing computational abilities of mobile devices as well as novel algorithms which apply software optimizations to enable pre-trained cloud-scale models to run on resourceconstrained devices. However, when it comes to the training of these mobile-focused models, a working assumption has been that the models will be trained centrally in the cloud, using training data aggregated from several users. Federated Learning (FL)  is an emerging area of research in the machine learning community which aims to enable distributed edge devices (or users) to collaboratively train a shared prediction model while keeping their personal data private. At a high level, this is achieved by repeating three basic steps: i) local parameters update to a shared prediction model on each edge device, ii) sending the local parameter updates to a central server for aggregation, and iii) receiving the aggregated  Figure 1. Survey of the number of FL clients used in FL research papers in the last two years. Scatter plot of number of concurrent clients participated in each communication round (y-axis) and total number of clients in the client pool (x-axis). The x-axis is converted to log scale to reflect the data points more clearly. FedScale can achieve 100 concurrent clients participated in each round out of 10000 total clients (orange point), while Flower framework can achieve 1000 concurrent clients out of a total 1 million clients (green point). The plot shows that Flower can achieve both higher concurrent participated client and larger client pool compared with other experiments existing the the recent research papers. Appendix A.1 gives details of the papers considered. model back for the next round of local updates.\n\nFrom a systems perspective, a major bottleneck to FL research is the paucity of frameworks that support scalable execution of FL methods on mobile and edge devices. While several frameworks including Tensorflow Federated (Google, 2020;Abadi et al., 2016a) (TFF) and LEAF (Caldas et al., 2018) enable experimentation on FL algorithms, they do not provide support for running FL on arXiv:2007.14390v5 [cs.LG] 5 Mar 2022 edge devices. System-related factors such as heterogeneity in the software stack, compute capabilities, and network bandwidth, affect model synchronization and local training. In combination with the choice of the client selection and parameter aggregation algorithms, they can impact the accuracy and training time of models trained in a federated setting. The systems' complexity of FL and the lack of scalable open-source frameworks can lead to a disparity between FL research and production. While closed productiongrade systems report client numbers in the thousands or even millions (Hard et al., 2019), few research papers use populations of more than 100 clients, as can be seen in Figure 1. Even those papers which use more than 100 clients rely on simulations (e.g., using nested loops) rather than actually implementing FL clients on real devices.\n\nIn this paper, we present Flower 1 , a novel FL framework, that supports experimentation with both algorithmic and systems-related challenges in FL. Flower offers a stable, language and ML framework-agnostic implementation of the core components of a FL system, and provides higher-level abstractions to enable researchers to experiment and implement new ideas on top of a reliable stack. Moreover, Flower allows for rapid transition of existing ML training pipelines into a FL setup to evaluate their convergence properties and training time in a federated setting. Most importantly, Flower provides support for extending FL implementations to mobile and wireless clients, with heterogeneous compute, memory, and network resources.\n\nAs system-level challenges of limited compute, memory, and network bandwidth in mobile devices are not a major bottleneck for powerful cloud servers, Flower provides builtin tools to simulate many of these challenging conditions in a cloud environment and allows for a realistic evaluation of FL algorithms. Finally, Flower is designed with scalability in mind and enables large-cohort research that leverages both a large number of connected clients and a large number of clients training concurrently. We believe that the capability to perform FL at scale will unlock new research opportunities as results obtained in small-scale experiments are not guaranteed to generalize well to large-scale problems. In summary, we make the following contributions:\n\n\u2022 We present Flower, a novel FL framework that supports large-cohort training and evaluation, both on real edge devices and on single-node or multi-node compute clusters. This unlocks scalable algorithmic research of realworld system conditions such as limited computational resources which are common for typical FL workloads. \u2022 We describe the design principles and implementation details of Flower. In addition to being language-and ML framework-agnostic by design, Flower is also fully 1 https://flower.dev extendable and can incorporate emerging algorithms, training strategies and communication protocols. \u2022 Using Flower , we present experiments that explore both algorithmic and system-level aspects of FL on five machine learning workloads with up to 15 million clients. Our results quantify the impact of various system bottlenecks such as client heterogeneity and fluctuating network speeds on FL performance. \u2022 Flower is open-sourced under Apache 2.0 License and adopted by major research organizations in both academia and industry. The community is actively participating in the development and contributes novel baselines, functionality, and algorithms.\n\n\nBACKGROUND AND RELATED WORK\n\nFL builds on a vast body of prior work and has since been expanded in different directions. McMahan et al. (2017) introduced the basic federated averaging (FedAvg) algorithm and evaluated it in terms of communication efficiency. There is active work on privacy and robustness improvements for FL: A targeted model poisoning attack using Fashion-MNIST (Xiao et al., 2017) (along with possible mitigation strategies) was demonstrated by Bhagoji et al. (2018). Abadi et al. (2016b) propose an attempt to translate the idea of differential privacy to deep learning. Secure aggregation (Bonawitz et al., 2017) is a way to hide model updates from \"honest but curious\" attackers. Robustness and faulttolerance improvements at the optimizer level are commonly studied and demonstrated, e.g., by Zeno (Xie et al., 2019). Finally, there is an increasing emphasis on the performance of federated optimization in heterogeneous data and system settings (Smith et al., 2017;Li et al., 2018;2019).\n\nThe optimization of distributed training with and without federated concepts has been covered from many angles (Dean et al., 2012;Jia et al., 2018;Chahal et al., 2018;Sergeev & Balso, 2018;Dryden et al., 2016). Bonawitz et al. (2019) detail the system design of a large-scale Googleinternal FL system. TFF (Google, 2020), PySyft (Ryffel et al., 2018), and LEAF (Caldas et al., 2018) propose open source frameworks which are primarily used for simulations that run a small number of homogeneous clients. Flower unifies both perspectives by being open source and suitable for exploratory research, with scalability to expand into settings involving a large number of heterogeneous clients. Most of the mentioned approaches have in common that they implement their own systems to obtain the described results.\n\nThe main intention of Flower is to provide a framework which would (a) allow to perform similar research using a common framework and (b) enable to run those experiments on a large number of heterogeneous devices.\n\n\nFLOWER OVERVIEW\n\nFlower is a novel end-to-end federated learning framework that enables a more seamless transition from experimental research in simulation to system research on a large cohort of real edge devices. Flower offers individual strength in both areas (viz. simulation and real world devices); and offers the ability for experimental implementations to migrate between the two extremes as needed during exploration and development. In this section, we describe use cases that motivate our perspective, design goals, resulting framework architecture, and comparison to other frameworks.\n\n\nUse Cases\n\nThe identified gap between FL research practice and industry reports from proprietary large-scale systems (Figure 1) is, at least in part, related a number of use cases that are not well-supported by the current FL ecosystem. The following sections show how Flower enables those use cases.\n\nScale experiments to large cohorts. Experiments need to scale to both a large client pool size and a large number of clients training concurrently to better understand how well methods generalize. A researcher needs to be able launch large-scale FL evaluations of their algorithms and design using reasonable levels of compute (e.g., single-machine/a multi-GPU rack), and have results at this scale have acceptable speed (wall-clock execution time).\n\nExperiment on heterogeneous devices. Heterogeneous client environments are the norm for FL. Researchers need ways to both simulate heterogeneity and to execute FL on real edge devices to quantify the effects of system heterogeneity. Measurements about the performance of client performance should be able to be easily collected, and deploying heterogeneous experiments is painless.\n\nTransition from simulation to real devices. New methods are often conceived in simulated environments. To understand their applicability to real-world scenarios, frameworks need to support seamless transition between simulation and on-device execution. Shifting from simulation to real devices, mixing simulated and real devices, and selecting certain elements to have varying levels of realism (e.g., compute or network) should be easy.\n\nMulti-framework workloads. Diverse client environments naturally motivate the usage of different ML frameworks, so FL frameworks should be able to integrate updates coming from clients using varying ML frameworks in the same workload. Examples range from situations where clients use two different training frameworks (pytorch and tensorflow) to more complex situations where clients have their own device-and OS-specific training algorithm. \n\n\nFedProx\n\nImplementation of the algorithm proposed by Li et al. (2020) to extend FL to heterogenous network conditions.\n\n\nQFedAvg\n\nImplementation of the algorithm proposed by Li et al. (2019) to encourage fairness in FL.\n\n\nFedOptim\n\nA family of server-side optimizations that include FedAdagrad, FedYogi, and FedAdam as described in Reddi et al. (2021).\n\n\nDesign Goals\n\nThe given uses cases identify a gap in the existing FL ecosystem that results in research that does not necessarily reflect real-world FL scenarios. To adress the ecosystem gap, we defined a set of independent design goals for Flower:\n\nScalable: Given that real-world FL would encounter a large number of clients, Flower should scale to a large number of concurrent clients to foster research on a realistic scale.\n\nClient-agnostic: Given the heterogeneous environment on mobile clients, Flower should be interoperable with different programming languages, operating systems, and hardware.\n\nCommunication-agnostic: Given the heterogeneous connectivity settings, Flower should allow different serialization and communication approaches.\n\nPrivacy-agnostic: Different FL settings (cross-devic, crosssilo) have different privacy requirements (secure aggregation, differential privacy). Flower should support common approaches whilst not be prescriptive about their usage.\n\nFlexible: Given the rate of change in FL and the velocity of the general ML ecosystem, Flower should be flexible to enable both experimental research and adoption of recently proposed approaches with low engineering overhead.\n\nA framework architecture with those properties will increase both realism and scale in FL research and provide a smooth transition from research in simulation to large-cohort research on real edge devices. The next section describes how the Flower framework architecture supports those goals.\n\n\nCore Framework Architecture\n\nFL can be described as an interplay between global and local computations. Global computations are executed on the server side and responsible for orchestrating the learning process over a set of available clients. Local computations are executed on individual clients and have access to actual data used for training or evaluation of model parameters.\n\nThe architecture of the Flower core framework reflects that perspective and enables researchers to experiment with building blocks, both on the global and on the local level. Global logic for client selection, configuration, parameter update aggregation, and federated or centralized model evaluation can be expressed through the Strategy abstraction. An implementation of the Strategy abstraction represents a single FL algorithm and Flower provides tested reference implementations of popular FL algorithms such as FedAvg  or FedYogi (Reddi et al., 2021) (summarized in table 1). Local logic on the other hand is mainly concerned with model training and evaluation on local data partitions. Flower acknowledges the breadth and diversity of existing ML pipelines and offers ML framework-agnostic ways to federate these, either on the Flower Protocol level or using the high-level Client abstraction. Figure 2 illustrates those components.\n\nThe Flower core framework implements the necessary infrastructure to run these workloads at scale. On the server side, there are three major components involved: the Client-Manager, the FL loop, and a (user customizable) Strategy. Server components sample clients from the ClientManager, which manages a set of ClientProxy objects, each representing a single client connected to the server. They are responsible for sending and receiving Flower Protocol mes-sages to and from the actual client. The FL loop is at the heart of the FL process: it orchestrates the entire learning process. It does not, however, make decisions about how to proceed, as those decisions are delegated to the currently configured Strategy implementation.\n\nIn summary, the FL loop asks the Strategy to configure the next round of FL, sends those configurations to the affected clients, receives the resulting client updates (or failures) from the clients, and delegates result aggregation to the Strategy. It takes the same approach for both federated training and federated evaluation, with the added capability of server-side evaluation (again, via the Strategy). The client side is simpler in the sense that it only waits for messages from the server. It then reacts to the messages received by calling user-provided training and evaluation functions.\n\nA distinctive property of this architecture is that the server is unaware of the nature of connected clients, which allows to train models across heterogeneous client platforms and implementations, including workloads comprised of clients connected through different communication stacks. The framework manages underlying complexities such as connection handling, client life cycle, timeouts, and error handling in an for the researcher.\n\n\nVirtual Client Engine\n\nBuilt into Flower is the Virtual Client Engine (VCE): a tool that enables the virtualization of Flower Clients to maximise utilization of the available hardware. Given a pool of clients, their respective compute and memory budgets (e.g. number of CPUs, VRAM requirements) and, the FL-specific hyperparameters (e.g. number of clients per round), the VCE launches Flower Clients in a resource-aware manner. The VCE will schedule, instantiate and run the Flower Clients in a transparent way to the user and the Flower Server. This property greatly simplifies parallelization of jobs, ensuring the available hardware is not underutilised and, enables porting the same FL experiment to a wide varying of setups without reconfiguration: a desktop machine, a single GPU rack or multi-node GPU cluster. The VCE therefore becomes a key module inside the Flower framework enabling running large scale FL workloads with minimal overhead in a scalable manner.\n\n\nEdge Client Engine\n\nFlower is designed to be open source, extendable and, framework and device agnostic. Some devices suitable for lightweight FL workloads such as Raspberry Pi or NVIDIA Jetson require minimal or no special configuration. These Python-enabled embedded devices can readily be used as Flower Clients. On the other hand, commodity devices such as smartphones require a more strict, limited and sometimes proprietary software stack to run ML workloads. To circum- \n\n\nTFF Syft FedScale LEAF Flower\n\nSingle-node simulation\n\u221a \u221a \u221a \u221a \u221a Multi-node execution * \u221a ( \u221a )*** \u221a Scalability * ** \u221a Heterogeneous clients ( \u221a )*** ** \u221a ML framework-agnostic **** **** \u221a Communication-agnostic \u221a Language-agnostic \u221a Baselines \u221a \u221a *\nLabels: * Planned / ** Only simulated *** Only Python-based / **** Only PyTorch and/or TF/Keras vent this limitation, Flower provides a low-level integration by directly handling Flower Protocol messages on the client.\n\n\nSecure Aggregation\n\nIn FL the server does not have direct access to a client's data. To further protect clients' local data, Flower provides implementation of both SecAgg (Bonawitz et al., 2017) and SecAgg+ (Bell et al., 2020) protocols for a semi-honest threat model. The Flower secure aggregation implementation satisfies five goals: usability, flexibility, compatibility, reliability and efficiency. The execution of secure aggregation protocols is independent of any special hardware and ML framework, robust against client dropouts, and has lower theoretical overhead for both communication and computation than other traditional multi-party computation secure aggregation protocol, which will be shown in 5.5.\n\n\nFL Framework Comparison\n\nWe compare Flower to other FL toolkits, namely TFF (Google, 2020), Syft (Ryffel et al., 2018), FedScale (Lai et al., 2021) and LEAF (Caldas et al., 2018). Table 2 provides an overview, with a more detailed description of those properties following thereafter.\n\nSingle-node simulation enables simulation of FL systems on a single machine to investigate workload performance without the need for a multi-machine system. Supported by all frameworks.\n\nMulti-node execution requires network communication between server and clients on different machines. Multimachine execution is currently supported by Syft and Flower. FedScale supports multi-machine simulation (but not real deployment), TFF plans multi-machine deployments.\n\nScalability is important to derive experimental results that generalize to large cohorts. Single-machine simulation is limited because workloads including a large number of clients often exhibit vastly different properties. TFF and LEAF are, at the time of writing, constrained to singlemachine simulations. FedScale can simulate clients on mul-tiple machines, but only scales to 100 concurrent clients. Syft is able to communicate over the network, but only by connecting to data holding clients that act as servers themselves, which limits scalability. In Flower, data-holding clients connect to the server which allows workloads to scale to millions of clients, including scenarios that require full control over when connections are being opened and closed. Flower also includes a virtual client engine for large-scale multi-node simulations.\n\nHeterogeneous clients refers to the ability to run workloads comprised of clients running on different platforms using different languages, all in the same workload. FL targeting edge devices will clearly have to assume pools of clients of many different types (e.g., phone, tablet, embedded). Flower supports such heterogeneous client pools through its language-agnostic and communication-agnostic clientside integration points. It is the only framework in our comparison that does so, with TFF and Syft expecting a framework-provided client runtime, whereas FedScale and LEAF focus on Python-based simulations.\n\nML framework-agnostic toolkits allow researchers and users to leverage their previous investments in existing ML frameworks by providing universal integration points. This is a unique property of Flower: the ML framework landscape is evolving quickly (e.g., JAX (Bradbury et al., 2018), PyTorch Lightning (W. Falcon, 2019)) and therefore the user should choose which framework to use for their local training pipelines. TFF is tightly coupled with TensorFlow and experimentally supports JAX, LEAF also has a dependency on TensorFlow, and Syft provides hooks for PyTorch and Keras, but does not integrate with arbitrary tools.\n\nLanguage-agnostic describes the capability to implement clients in a variety of languages, a property especially important for research on mobile and emerging embedded platforms. These platforms often do not support Python, but rely on specific languages (Java on Android, Swift on iOS) for idiomatic development, or native C++ for resource constrained embedded devices. Flower achieves a fully language-agnostic interface by offering protocol-level integration. Other frameworks are based on Python, with some of them indicating a plan to support Android and iOS (but not embedded platforms) in the future.\n\nBaselines allow the comparison of existing methods with new FL algorithms. Having existing implementations at ones disposal can greatly accelerate research progress. LEAF and FedScale come with a number of benchmarks built-in with different datasets. TFF provides libraries for constructing baselines with some datasets. Flower currently implements a number of FL methods in the context of popular ML benchmarks, e.g., a federated training of CIFAR-10 ( Krizhevsky et al., 2005) image classification, and has initial port of LEAF datasets such as FEMNIST and Shake-speare (Caldas et al., 2018).\n\n\nIMPLEMENTATION\n\nFlower has an extensive implementation of FL averaging algorithms, a robust communication stack, and various examples of deploying Flower on real and simulated clients. Due to space constraints, we only focus on some of the implementation details in this section and refer the reader to the Flower GitHub repository for more details.\n\nCommunication stack. FL requires stable and efficient communication between clients and server. The Flower communication protocol is currently implemented on top of bi-directional gRPC (Foundation) streams. gRPC defines the types of messages exchanged and uses compilers to then generate efficient implementations for different languages such as Python, Java, or C++. A major reason for choosing gRPC was its efficient binary serialization format, which is especially important on low-bandwidth mobile connections. Bi-directional streaming allows for the exchange of multiple message without the overhead incurred by re-establishing a connection for every request/response pair.\n\nSerialization. Independent of communication stack, Flower clients receive instructions (messages) as raw byte arrays (either via the network or throught other means, for example, inter-process communication), deserialize the instruction, and execute the instruction (e.g., training on local data). The results are then serialized and communicated back to the server. Note that a client communicates with the server through language-independent messages and can thus be implemented in a variety of programming languages, a key property to enable real on-device execution. The useraccessible byte array abstraction makes Flower uniquely serialization-agnostic and enables users to experiment with custom serialization methods, for example, gradient compression or encryption.\n\nAlternative communication stacks. Even though the current implementation uses gRPC, there is no inherent reliance on it. The internal Flower server architecture uses modular abstractions such that components that are not tied to gRPC are unaware of it. This enables the server to support user-provided RPC frameworks and orchestrate workloads across heterogeneous clients, with some connected through gRPC, and others through other RPC frameworks.\n\nClientProxy. The abstraction that enables communicationagnostic execution is called ClientProxy. Each ClientProxy object registered with the ClientManager represents a single client that is available to the server for training or evaluation. Clients which are offline do not have an associated ClientProxy object. All server-side logic (client configuration, receiving results from clients) is built against the ClientProxy abstraction.\n\nOne key design decision that makes Flower so flexible is that ClientProxy is an abstract interface, not an implementation. There are different implementations of the ClientProxy interface, for example, GrpcClientProxy. Each implementation encapsulates details on how to communicate with the actual client, for example, to send messages to an actual edge device using gRPC.\n\nVirtual Client Engine (VCE). Resource consumption (CPU, GPU, RAM, VRAM, etc.) is the major bottleneck for large-scale experiments. Even a modestly sized model easily exhausts most systems if kept in memory a million times. The VCE enables large-scale single-machine or multimachine experiments by executing workloads in a resourceaware fashion that either increases parallelism for better wall-clock time or to enable large-scale experiments on limited hardware resources. It creates a ClientProxy for each client, but defers instantiation of the actual client object (including local model and data) until the resources to execute the client-side task (training, evaluation) become available. This avoids having to keep multiple client-side models and datasets in memory at any given point in time.\n\nVCE builds on the Ray (Moritz et al., 2018) framework to schedule the execution of client-side tasks. In case of limited resources, Ray can sequence the execution of client-side computations, thus enabling a much larger scale of experiments on common hardware. The capability to perform FL at scale will unlock new research opportunities as results obtained in small-scale experiments often do not generalize well to large-cohort settings.\n\n\nFRAMEWORK EVALUATION\n\nIn this section we evaluate Flower's capabilities in supporting both research and implementations of real-world FL workloads. Our evaluation focuses on three main aspects: \n\n\nLarge-Scale Experiment\n\nFederated Learning receives most of its power from its ability to leverage data from millions of users. However, selecting large numbers of clients in each training round does not necessarily translate into faster convergence times. In fact, as observed in , there is usually an empirical threshold for which if we increase the number of participating clients per round beyond that point, convergence will be slower. By allowing experiments to run at mega-scales, with thousands of active clients per round, Flower gives us the opportunity to empirically find such threshold for any task at hand.\n\nTo show this ability, in this series of experiments we use Flower to fine-tune a network on data from 15M users using different numbers of clients per round. More specifically, we fine-tune a Transformer network to correctly predict Amazon book ratings based on text reviews from users.\n\nExperimental Setup. We choose to use Amazon's Book Reviews Dataset (Ni et al., 2019) which contains over 51M reviews from 15M different users. Each review from a given user contains a textual review of a book along with its given rank (1-5). We fine-tune the classifier of a pre-trained DistilBERT model (Sanh et al., 2019) to correctly predict ranks based on textual reviews. For each experiment we fix the number of clients being sampled in each round (from 10 to 1000) and aggregate models using FedAvg. We test the aggregated model after each round on a fixed set of 1M clients. Convergence curves are reported in Figure 3 all our experiments were run using two NVIDIA V100 GPUs on a 22-cores of an Intel Xeon Gold 6152 (2.10GHz) CPU.\n\nResults. Figure 3 shows the expected initial speed-up in convergence when selecting 10 to 500 clients per round in each experiment. However, if we decide to sample 1k clients in each round, we notice an increase in convergence time. Intuitively, this behaviour is caused by clients' data having very different distributions; making it difficult for simple Aggregation Strategies such as FedAvg to find a suitable set of weights.\n\n\nSingle Machine Experiments\n\nOne of our strongest claims in this paper is that Flower can be effectively used in Research. For this to be true, Flower needs to be fast at providing reliable results when experimenting new ideas, e.g. a new aggregation strategy.\n\nIn this experiment, we provide a head-to-head comparison in term of training times between Flower and the four main FL frameworks, namely FedScale, TFF, FedJax and the original LEAF, when training with different FL setups.\n\nExperimental Setup. We consider all three FL setups proposed by (Caldas et al., 2018) when training a CNN model to correctly classify characters from the FEMNIST dataset. More specifically, we consider the scenarios where the number of clients (c) and local epochs per round change (l) vary. The total number of rounds and total number of clients are kept constant at 2000 and 179, respectively. To allow for a fair comparison, We run all our experiments using eight cores of an Intel Xeon E5-2680 CPU (2.40GHz) equipped with two NVIDIA RTX2080 GPUs and 20GB of RAM.\n\nResults. Figure 4 shows the impact of choosing different FL frameworks for the various tasks. On our first task, when training using three clients per round (c = 3) for one local epoch (l = 1), FedJax finishes training first (05:18), LEAF finishes second (44:39) followed by TFF (58:29) and Flower (59:19). In this simple case, the overhead of having a multi-task system, like the Virtual Client Engine (VCE), causes Flower to sightly under-perform in comparison to loop-based simulators, like LEAF.\n\nHowever, the benefits of having a VCE become more evident if we train on more realistic scenarios. When increasing the number of clients per round to 35 while keeping the single local epoch, we notice that Flower (230:18) is still among the fastest frameworks. Since the number of local epochs is still one, most of the overhead comes from loading data and models into memory rather than performing real training, hence the similarity those LEAF and Flower.\n\nThe VCE allows us to specify the amount of GPU memory we want to associate with each client, this allows for more efficient data and model loading of different clients on the same GPU, making the overall training considerably faster. In fact, when we substantially increase the amount of work performed by each client to 100 local epochs, while fixing the number of active client to 3, we see a significant saving in training time. In this task Flower outperforms all other. It completes the task in just about 80 minutes, while the second best performing framework (FedJax) takes over twice as long (over 173 minutes).\n\nIt is also important to acknowledge the two extreme training times we see in this experiment. FedJax seems to be very efficient when training on few (1) local epochs; however, in scenarios where communication-efficiency is key and larger number of local epochs are required, FedJax performance slightly degrades. FedScale, on the other hands, consistently showed high training times across all training scenarios. We believe this apparent inefficiency to be associated with network overheads that are usually unnecessary in a singlecomputer simulation.\n\n\nFlower enables FL evaluation on real devices\n\nFlower can assist researchers in quantifying the system costs associated with running FL on real devices and to identify bottlenecks in real-world federated training. In this section, we present the results of deploying Flower on six types of heterogeneous real-world mobile and embedded devices, including Java-based Android smartphones and Python-based Nvidia Jetson series devices and Raspberry Pi.  (Lite, 2020). The source code for both implementations is available in the Flower repository.\n\nResults. Figure 5 shows the system metrics associated with training a DeepConvLSTM (Singh et al., 2021) model for a human activity recognition task on Python-enabled Jetson and Raspberry Pi devices. We used the RealWorld dataset (Sztyler & Stuckenschmidt, 2016) consisting of timeseries data from accelerometer and gyroscope sensors on mobile devices, and partitioned it across 10 mobile clients. The first takeaway from our experiments in that we could deploy Flower clients on these heterogeneous devices, without requiring any modifications in the client-side Flower code. The only consideration was to ensure that a compatible ML framework (e.g., TensorFlow) is installed on each client. Secondly, we show in Figure 5 how FL researchers can deploy and quantify the training time and energy consumption of FL on various heterogeneous devices and processors.\n\nHere, the FL training time is aggregated over 40 rounds, and includes the time taken to perform local 10 local epochs of SGD on the client, communicating model parameters between the server and the client, and updating the global model on the server. By comparing the relative energy consumption and training times across various devices, FL researchers can devise more informed client selection policies that can tradeoff between FL convergence time and overall energy consumption. For instance, choosing Jetson Nano-CPU based FL clients over Raspberry Pi clients may increase FL convergence time by 10 minutes, however it reduces the overall energy consumption by almost 60%.\n\nNext, we illustrate how Flower can enable fine-grained profiling of FL on real devices. We deploy Flower on 10 Android clients to train a model with 2 convolutional layers and 3 fully-connected layers (Flower, 2021) on the CIFAR-10 dataset. TensorFlow Lite is used as the training ML framework on the devices. We measure the time taken for various FL operations, such as local SGD training, communication between the server and client, local evaluation on the client, and the overhead due to the Flower framework.  The overhead includes converting model gradients to GRPCcompatible buffers and vice-versa, to enable communication between Java FL clients and a Python FL server. In Figure 6, we report the mean latency of various FL operations over 40 rounds on two types of Android devices: Google Pixel 4 and Samsung Galaxy S9. We observe that on both devices, local training remains the most time-consuming operation, and that the total system overhead of the Flower framework is less than 100ms per round.\n\n\nRealism in Federated Learning\n\nFlower facilitates the deployment of FL on real-world devices. While this property is beneficial for production-grade systems, can it also assist researchers in developing better federated optimization algorithms? In this section, we study two realistic scenarios of FL deployment.\n\nComputational Heterogeneity across Clients. In realworld, FL clients will have vastly different computational capabilities. While newer smartphones are now equipped with mobile GPUs, other phones or wearable devices may have a much less powerful processor. How does this computational heterogeneity impact FL?\n\nFor this experiment, we use a Nvidia Jetson TX2 as the client device, which has a Pascal GPU and six CPU cores. We train a ResNet18 model on the CIFAR-10 dataset in a federated setting with 10 total Jetson TX2 clients and 40 rounds of training. In Table 3, we observe that if Jetson TX2 CPU clients are used for federated training (local epochs E=10), the FL process would take 1.27\u00d7 more time to converge as compared to training on Jetson TX2 GPU clients.\n\nOnce we obtain this quantification of computational heterogeneity using Flower, we can design better federated optimization algorithms. As an example, we implemented a modified version of FedAvg where each client device is assigned a cutoff time (\u03c4 ) after which it must send its model parameters to the server, irrespective of whether it has finished its local epochs or not. This strategy has parallels with the FedProx algorithm  which also accepts partial results from clients. However, the key advantage of Flower's on-device training capabilities is that we can accurately measure and assign a realistic processor-specific cutoff time for each client. For example, we measure that on average it takes 1.99 minutes to complete a FL round on the TX2 GPU. We then set the same time as a cutoff for CPU clients (\u03c4 = 1.99 mins) as shown in Table 3. This ensures that we can obtain faster convergence even in the presence of CPU clients, at the expense of a 4% accuracy drop. With \u03c4 = 2.23, a better balance between accuracy and convergence time could be obtained for CPU clients.\n\nHeterogeneity in Network Speeds. An important consideration for any FL system is to choose a set of participating clients in each training round. In the real-world, clients are distributed across the world and vary in their download and upload speeds. Hence, it is critical for any FL system to study how client selection can impact the overall FL training time. We now present an experiment with 40 clients collaborating to train a 4-layer deep CNN model for the FashionMNIST dataset. More details about the dataset and network architecture are presented in the Appendix.\n\nUsing Flower, we instantiate 40 clients on a cloud platform and fix the download and upload speeds for each client using the WONDERSHAPER library. Each client is representative of a country and its download and upload speed is set based on a recent market survey of 4G and 5G speeds in different countries (OpenSignal, 2020).  We observe that if all clients have the network speeds corresponding to Country 1 (Canada), the FL training finishes in 8.9 mins. As we include slower clients in FL, the training time gradually increases, with a major jump around index = 17. On the other extreme, for client speeds corresponding to Country 40 (Iraq), the FL training takes 108 minutes.\n\nThere are two key takeaways from this experiment: a) Using Flower, we can profile the training time of any FL algorithm under scenarios of network heterogeneity, b) we can leverage these insights to design sophisticated client sampling techniques. For example, during subsequent rounds of federated learning, we could monitor the number of samples each client was able to process during a given time window and increase the selection probability of slow clients to balance the contributions of fast and slow clients to the global model. The FedFS strategy detailed in the appendix works on this general idea, and reduces the convergence time of FL by up to 30% over the FedAvg random sampling approach.\n\n\nSecure Aggregation Overheads\n\nPrivacy is one of the cornerstones in FL, which inevitably generates computational overhead during training. In hardware-constrained systems, such as cross-device FL, it is desirable not only to be able to measure such overheads, but also to make sure that security protocols are well implemented and follow the expected protocol described in the original papers. Flower's implementation of Secure Aggregation, named Salvia, is based on the SecAgg (Bonawitz et al., 2017) and SecAgg+ (Bell et al., 2020) protocols as described in Section 3.6. To verify that Salvia's behavior matches the expected theoretical complexity, we evaluate its impact on server-side computation and communication overhead with the model vector size and clients dropouts. Experiment Setup. The FL simulations run on a Linux system with an Intel Xeon E-2136 CPU (3.30GHz), with 256 GB of RAM. In our simulations, all entries of our local vectors are of size 24 bits. We ignore communication latency. Moreover, all dropouts simulated happen after stage 2, i.e. Share Keys Stage. This is because this imposes the most significant overhead as the server not only needs to regenerate dropped-out clients' secrets, but also compute their pairwise masks generated between their neighbours.\n\nFor our simulations, the n and t parameters of the t-outof-n secret-sharing scheme are set to 51 and 26, respectively. These parameters are chosen to reference SecAgg+'s proven correctness and security guarantees, where we can tolerate up to 5% dropouts and 5% corrupted clients with correctness holding with probability 1 \u2212 2 \u221220 and security holding with probability 1 \u2212 2 \u221240 .\n\nResults. Fixing the number of sampled clients to 100, we plotted CPU running times through aggregating a vector of size 100k entries to aggregating one of size 500k entries in Figure 8. We also measured how the performance would change after client dropouts by repeating the same experiments with a 5% client dropout.\n\nBoth the running times and total data transfer of the server increase linearly with the model vector size as the operations involving model vectors are linear to the vectors' sizes, e.g. generating masks, sending vectors. We also note the server's running time increases when there are 5% clients dropping out, as the server has to perform extra computation to calculate all k pairwise masks for each client dropped. Lastly, we observe that the total data transferred of the server remains unchanged with client dropouts as each client only communicates with the server plus exactly k neighbors, regardless of the total number of clients and dropouts. We conclude that all our experimental data matches the expected complexities of SecAgg and SecAgg+.\n\n\nCONCLUSION\n\nWe have presented Flower -a novel framework that is specifically designed to advance FL research by enabling heterogeneous FL workloads at scale. Although Flower is broadly useful across a range of FL settings, we believe that it will be a true game-changer for reducing the disparity between FL research and real-world FL systems. Through the provided abstractions and components, researchers can federated existing ML workloads (regardless of the ML framework used) and transition these workloads from large-scale simulation to execution on heterogeneous edge devices. We further evaluate the capabilities of Flower in experiments that target both scale and systems heterogeneity by scaling FL up to 15M clients, providing head-to-head comparison between different FL frameworks for single-computer experiments, measuring FL energy consumption on a cluster of Nvidia \n\n\nA APPENDIX\n\nA.1 Survey on papers From a systems perspective, a major bottleneck to FL research is the paucity of frameworks that support scalable execution of FL methods on mobile and edge devices. Fig.  9 shows the histograms of total number of clients in the FL pools in research papers. The research papers is gathered from Google Scholar that is related to federated learning from last 2 years which consists of total 150 papers in the survey. We excluded papers that are using the framework not available to reproduced the results. As we can see from the histogram, the majority of experiments only use up to 100 total clients, which usually on datasets such as CIFAR10 and ImageNet. There are only 3 papers using the dataset with a total clients pool up to 1 millions, and they are using the Reddit and Sentiment140 dataset from leaf (Caldas et al., 2018).\n\n\nA.2 FedFS Algorithm\n\nWe introduce Federating: Fast and Slow (FedFS) to overcomes the challenges arising from heterogeneous devices and non-IID data. FedFS acknowledges the difference in compute capabilities inherent in networks of mobile devices by combining partial work, importance sampling, and dynamic timeouts to enable clients to contribute equally to the global model.\n\nPartial work. Given a (local) data set of size m k on client k, a batch size of B, and the number of local training epochs E, FedAvg performs E m k B (local) gradient updates \u03b8 k \u2190 \u03b8 k \u2212 \u03b7 (b; \u03b8 k ) before returning \u03b8 k to the server. The asynchronous setting treats the success of local update computation as binary. If a client succeeds in computing E m k B mini-batch updates before reaching a timeout \u2206, their weight update is considered by the server, otherwise it is discarded. The server then averages all successful \u03b8 k\u2208{0,..,K} updates, weighted by m k , the number of training examples on client k. This is wasteful because a clients' computation might be discarded upon reaching \u2206 even if it was close to computing the full E m k B gradient updates. We therefore apply the concept of partial work  in which a client submits their locally updated \u03b8 k upon reaching \u2206 along with c k , the number of examples actually involved in computing \u03b8 k , even if c k < E m k B B. The server averages by c k , not m k , because c k can vary over different rounds and devices depending on a number of factors (device speed, concurrent processes, \u2206, m k , etc.).\n\nIntuitively, this leads to more graceful performance degradation with smaller values for \u2206. Even if \u2206 is set to an adversarial value just below the completion time of the fastest client, which would cause FedAvg to not consider any update and hence prevent convergence, FedFS would still progress by combining K partial updates. More importantly it allows devices which regularly discard their updates because of lacking compute capabilities to have their updates represented in the global model, which would otherwise overfit the data distribution on the subset of faster devices in the population.\n\nImportance sampling. Partial work enables FedFS to leverage the observed values for c r k (with r \u2208 {1, ..., t}, the amount of work done by client k during all previous rounds up to the current round t) and E r m k (with r \u2208 {1, ..., t}, the amount of work client k was maximally allowed to do during those rounds) for client selection during round t + 1. c and m can be measured in different ways depending on the use case. In vision, c t k could capture the number of image examples processed, whereas in speech c t k could measure the accumulated duration of all audio samples used for training on client k during round t. c t k < E t m k suggests that client k was not able to compute E t m k B gradient updates within \u2206 t , so its weight update \u03b8 t k has less of an impact on the global model \u03b8 compared to an update from client j with c t j = E t m j . FedFS uses importance sampling for client selection to mitigate the effects introduced by this difference in client capabilities. We define the work contribution w k of client k as the ratio between the actual work done during previous rounds c k = t r=1 c r k and the maximum work possible\u0109 k = t r=1 E r m k . Clients which have never been selected before (and hence have no contribution history) have w k = 0. We then sample clients on the selection probability 1 \u2212 w k + (normalized over all k \u2208 {1, ..., K}), with being the minimum client selection probability. is an important hyper-parameter that prevents clients with c t k = E t m k to be excluded from future rounds. Basing the client selection probability on a clients' previous contributions (w k ) allows clients which had low contributions in previous rounds to be selected more frequently, and hence contribute additional updates to the global model. Synchronous FedAvg is a special case of FedFS: if all clients are able to compute c t k = E t m k every round, then there will be Algorithm 1: FedFS begin Server T, C, K, , r f , rs, \u2206max, E, B, initialise \u03b80 for round t \u2190 0, ..., T \u2212 1 do j \u2190 max( C \u00b7 K , 1) St \u2190 (sample j distinct indices from {1, ..., K} with 1 \u2212 w k + ) if fast round (r f , rs) then \u2206t = \u2206 f else \u2206t = \u2206 s end for k \u2208 St do in parallel \u03b8 k t+1 , c k , m k \u2190 ClientTraining(k, \u2206t, \u03b8t, E, B, \u2206t) end cr \u2190 k\u2208S t c k \u03b8t+1 \u2190 k\u2208S t no difference in w k and FedFS samples amongst all clients with a uniform client selection probability of 1 k . Alternating timeout. Gradual failure for clients which are not able to compute E t m k B gradient updates within \u2206 t and client selection based on previous contributions allow FedFS to use more aggressive values for \u2206. One strategy is to use an alternating schedule for \u2206 in which we perform r f \"fast\" rounds with small \u2206 f ) and r s \"slow\" rounds with larger \u2206 s . This allows FedFS to be configured for either improved convergence in terms of wall-clock time or better overall performance (e.g., in terms for classification accuracy).\n\nFedFS algorithm. The full FedFS algorithm is given in Algorithm 1.\n\n\nA.3 Scaling FedAvg to ImageNet-scale datasets\n\nWe now demonstrate that Flower can not only scale to a large number of clients, but it can also support training of FL models on web-scale workloads such as ImageNet. To the best of our knowledge, this is the first-ever attempt at training ImageNet in a FL setting. Experiment Setup. We use the ILSVRC-2012 ImageNet partitioning (Russakovsky et al., 2015) that contains 1.2M pictures for training and a subset composed of 50K images for testing. We train a ResNet-18 model on this dataset in a federated setting with 50 clients equipped with four physical CPU cores. To this end, we partition the ImageNet training set into 50 IID partitions and distribute them on each client. During training, we also consider a simple image augmentation scheme based on random horizontal flipping and cropping.\n\nResults. Figure 10 shows the results on the test set of ImageNet obtained by training a ResNet-18 model. It is worth to mention that based on 50 clients and 3 local epochs, the training lasted for about 15 days demonstrating Flower's potential to run long-term and realistic experiments.\n\nWe measured top-1 and top-5 accuracies of 59.1% and 80.4% respectively obtained with FL compared to 63% and 84% for centralised training. First, it is clear from Figure  10 that FL accuracies could have increased a bit further at the cost of a longer training time, certainly reducing the gap with centralised training. Then, the ResNet-18 architecture relies heavily on batch-normalisation, and it is unclear how the internal statistics of this technique behave in the context of FL, potentially harming the final results. As expected, the scalability of Flower helps with raising and investing new issues related to federated learning.\n\nFor such long-term experiments, one major risk is that client devices may go offline during training, thereby nullifying the training progress. Flower's built-in support for keeping the model states on the server and resuming the federated training from the last saved state in the case of failures came handy for this experiment.\n\n\nA.4 Datasets and Network Architectures\n\nWe use the following datasets and network architectures for our experiments.\n\nCIFAR-10 consists of 60,000 images from 10 different object classes. The images are 32 x 32 pixels in size and in RGB format. We use the training and test splits provided by the dataset authors -50,000 images are used as training data and remaining 10,000 images are reserved for testing.\n\nFashion-MNIST consists of images of fashion items (60,000 training, 10,000 test) with 10 classes such as trousers or pullovers. The images are 28 x 28 pixels in size and in grayscale format. We use a 2-layer CNN followed by 2 fully-connected layers for training a model on this dataset.\n\nImageNet. We use the ILSVRC-2012 ImageNet (Russakovsky et al., 2015) containing 1.2M images for training and 50K images for testing. A ResNet-18 model is used for federated training this dataset.\n\nFigure 2 .\n2Flower core framework architecture with both Edge Client Engine and Virtual Client Engine. Edge clients live on real edge devices and communicate with the server over RPC. Virtual clients on the other hand consume close to zero resources when inactive and only load model and data into memory when the client is being selected for training or evaluation.\n\nFigure 3 .\n3Flower scales to even 15M user experiments. Each curve shows successful convergence of the DistilBERT model under varying amounts of clients per round, with the exception of the two smallest client sizes: 50 and 10.\n\nFigure 4 .\n4Training times (log scale in second) comparison of different FEMNIST tasks between different FL frameworks.\n\nFigure 5 .Figure 6 .\n56AGX-GPU NX-GPU TX2-GPU Nano-GPU AGX-CPU NX-CPU TX2-CPU Nano-CPU RPI-GPU NX-GPU TX2-GPU Nano-GPU AGX-CPU NX-CPU TX2-CPU Nano-CPU RPI-Flower enables quantifying the system performance of FL on mobile and embedded devices. Here we report the training times and energy consumption associated with running FL on CPUs and GPUs of various embedded devices. Flower enables fine-grained profiling of FL performance on real devices. The framework overhead is <100ms per round.\n\nFigure 7 .\n7Effect of network heterogeneity in clients on FL training time. Using this quantification, we designed a new client sampling strategy called FedFS (detailed in the Appendix).\n\nFigure 8 .\n8Performance of Secure Aggregation. Running time of server with increasing vector size The x-axis of Figure 7 shows countries arranged in descending order of their network speeds: country indices 1-20 represent the top 20 countries based on their network speeds (mean download speed = 40.1Mbps), and indices 21-40 are the bottom 20 countries (mean download speed = 6.76Mbps).\n\nFigure 9 .\n9Histograms of the number of total FL clients used in FL research papers in the last two years. A vast majority of papers only use up to 100 clients.\n\nFigure 10 .\n10Training time reported in days and accuracies (Top-1 and Top-5) for an ImageNet federated training with Flower.\n\nTable 1 .\n1Excerpt of built-in FL algorithms available in Flower. New algorithms can be implemented using the Strategy interface.Strategy \nDescription \n\nFedAvg \nVanilla Federated Averaging (McMahan et al., 2017) \n\nFault \nTolerant \nFedAvg \n\nA variant of FedAvg that can tolerate faulty client \nconditions such as client disconnections or laggards. \n\n\n\nTable 2 .\n2Comparison of different FL frameworks.\n\nTable 3 .\n3Effect of computational heterogeneity on FL training times. Using Flower, we can compute a hardware-specific cutoff \u03c4 (in minutes) for each processor, and find a balance between FL accuracy and training time. \u03c4 = 0 indicates no cutoff time.GPU \nCPU \n(\u03c4 = 0) \n\nCPU \n(\u03c4 = 2.23) \n\nCPU \n(\u03c4 = 1.99) \n\nAccuracy \n0.67 \n0.67 \n0.66 \n0.63 \nTraining \ntime (mins) \n80.32 \n102 \n(1.27\u00d7) \n\n89.15 \n(1.11\u00d7) \n\n80.34 \n(1.0\u00d7) \n\n\n\n\nJetson TX2 devices, optimizing convergence time under limited bandwidth, and illustrating a deployment of Flower on a range of Android mobile devices in the AWS Device Farm. Flower is open-sourced under Apache 2.0 License and we look forward to more community contributions to it.\n\nTensorflow: A system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, M Kudlur, J Levenberg, R Monga, S Moore, D G Murray, B Steiner, P Tucker, V Vasudevan, P Warden, M Wicke, Y Yu, X Zheng, 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., and Zheng, X. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265-283, 2016a. URL https://www.usenix.org/system/files/ conference/osdi16/osdi16-abadi.pdf.\n\nDeep learning with differential privacy. M Abadi, A Chu, I Goodfellow, B Mcmahan, I Mironov, K Talwar, L Zhang, 23rd ACM Conference on Computer and Communications Security. ACM CCSAbadi, M., Chu, A., Goodfellow, I., McMahan, B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with dif- ferential privacy. In 23rd ACM Conference on Com- puter and Communications Security (ACM CCS), pp. 308-318, 2016b. URL https://arxiv.org/abs/ 1607.00133.\n\nSecure single-server aggregation with (poly) logarithmic overhead. J H Bell, K A Bonawitz, A Gasc\u00f3n, T Lepoint, M Raykova, Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security. the 2020 ACM SIGSAC Conference on Computer and Communications SecurityBell, J. H., Bonawitz, K. A., Gasc\u00f3n, A., Lepoint, T., and Raykova, M. Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 1253-1269, 2020.\n\nAnalyzing federated learning through an adversarial lens. CoRR, abs/1811.12470. A N Bhagoji, S Chakraborty, P Mittal, S B Calo, Bhagoji, A. N., Chakraborty, S., Mittal, P., and Calo, S. B. Analyzing federated learning through an adversarial lens. CoRR, abs/1811.12470, 2018. URL http://arxiv. org/abs/1811.12470.\n\nPractical secure aggregation for privacypreserving machine learning. K Bonawitz, V Ivanov, B Kreuter, A Marcedone, H B Mcmahan, S Patel, D Ramage, A Segal, Seth , K , http:/doi.acm.org/10.1145/3133956.3133982Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS '17. the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS '17New York, NY, USAACMBonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel, S., Ramage, D., Segal, A., and Seth, K. Practical secure aggregation for privacy- preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communi- cations Security, CCS '17, pp. 1175-1191, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4946-8. doi: 10.1145/3133956.3133982. URL http://doi.acm. org/10.1145/3133956.3133982.\n\nTowards federated learning at scale: System design. K Bonawitz, H Eichner, W Grieskamp, D Huba, A Ingerman, V Ivanov, C M Kiddon, J Kone\u010dn\u00fd, S Mazzocchi, B Mcmahan, T V Overveldt, D Petrou, D Ramage, J Roselander, To appearBonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Inger- man, A., Ivanov, V., Kiddon, C. M., Kone\u010dn\u00fd, J., Maz- zocchi, S., McMahan, B., Overveldt, T. V., Petrou, D., Ramage, D., and Roselander, J. Towards federated learn- ing at scale: System design. In SysML 2019, 2019. URL https://arxiv.org/abs/1902.01046. To ap- pear.\n\nJAX: composable transformations of Python+NumPy programs. J Bradbury, R Frostig, P Hawkins, M J Johnson, C Leary, D Maclaurin, G Necula, A Paszke, J Vanderplas, S Wanderman-Milne, Q Zhang, Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\nS Caldas, S M K Duddu, P Wu, T Li, J Kone\u010dn\u1ef3, H B Mcmahan, V Smith, A Talwalkar, Leaf, arXiv:1812.01097A benchmark for federated settings. arXiv preprintCaldas, S., Duddu, S. M. K., Wu, P., Li, T., Kone\u010dn\u1ef3, J., McMahan, H. B., Smith, V., and Talwalkar, A. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nA hitchhiker's guide on distributed training of deep neural networks. K S Chahal, M S Grover, K Dey, abs/1810.11787CoRRChahal, K. S., Grover, M. S., and Dey, K. A hitchhiker's guide on distributed training of deep neural networks. CoRR, abs/1810.11787, 2018. URL http://arxiv. org/abs/1810.11787.\n\nVisual wake words dataset. CoRR, abs/1906.05721. A Chowdhery, P Warden, J Shlens, A Howard, R Rhodes, Chowdhery, A., Warden, P., Shlens, J., Howard, A., and Rhodes, R. Visual wake words dataset. CoRR, abs/1906.05721, 2019. URL http://arxiv.org/ abs/1906.05721.\n\nLarge scale distributed deep networks. J Dean, G S Corrado, R Monga, K Chen, M Devin, Q V Le, M Z Mao, M Ranzato, A Senior, P Tucker, K Yang, A Y Ng, Proceedings of the 25th International Conference on Neural Information Processing Systems. the 25th International Conference on Neural Information Processing SystemsUSACurran Associates Inc1Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., Mao, M. Z., Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. Large scale distributed deep net- works. In Proceedings of the 25th International Confer- ence on Neural Information Processing Systems -Volume 1, NIPS'12, pp. 1223-1231, USA, 2012. Curran Asso- ciates Inc. URL http://dl.acm.org/citation. cfm?id=2999134.2999271.\n\nCommunication quantization for data-parallel training of deep neural networks. N Dryden, S A Jacobs, T Moon, B Van Essen, 10.1109/MLHPC.2016.4Proceedings of the Workshop on Machine Learning in High Performance Computing Environments, MLHPC '16. the Workshop on Machine Learning in High Performance Computing Environments, MLHPC '16Piscataway, NJ, USAIEEE PressDryden, N., Jacobs, S. A., Moon, T., and Van Essen, B. Communication quantization for data-parallel training of deep neural networks. In Proceedings of the Workshop on Machine Learning in High Performance Computing Environments, MLHPC '16, pp. 1-8, Piscataway, NJ, USA, 2016. IEEE Press. ISBN 978-1-5090-3882-4. doi: 10.1109/MLHPC.2016.4. URL https://doi.org/ 10.1109/MLHPC.2016.4.\n\nModel architecture for android devices. Flower, Flower. Model architecture for android devices. https://github.com/adap/flower/ blob/main/examples/android/tflite_ convertor/convert_to_tflite.py, 2021.\n\nA high performance, opensource universal rpc framework. C N C Foundation, Grpc, Foundation, C. N. C. grpc: A high performance, open- source universal rpc framework. URL https://grpc. io. Accessed: 2020-03-25.\n\nHeterogeneous bitwidth binarization in convolutional neural networks. J Fromm, S Patel, M Philipose, Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18. the 32nd International Conference on Neural Information Processing Systems, NIPS'18Red Hook, NY, USACurran Associates IncFromm, J., Patel, S., and Philipose, M. Heterogeneous bitwidth binarization in convolutional neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, pp. 4010-4019, Red Hook, NY, USA, 2018. Curran Asso- ciates Inc.\n\nAccelerating mobile audio sensing algorithms through on-chip GPU offloading. P Georgiev, N D Lane, C Mascolo, Chu , D , 10.1145/3081333.3081358Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services, MobiSys'17. Choudhury, T., Ko, S. Y., Campbell, A., and Ganesan, D.the 15th Annual International Conference on Mobile Systems, Applications, and Services, MobiSys'17Niagara Falls, NY, USAACMGeorgiev, P., Lane, N. D., Mascolo, C., and Chu, D. Accel- erating mobile audio sensing algorithms through on-chip GPU offloading. In Choudhury, T., Ko, S. Y., Camp- bell, A., and Ganesan, D. (eds.), Proceedings of the 15th Annual International Conference on Mobile Sys- tems, Applications, and Services, MobiSys'17, Niagara Falls, NY, USA, June 19-23, 2017, pp. 306-318. ACM, 2017. doi: 10.1145/3081333.3081358. URL https: //doi.org/10.1145/3081333.3081358.\n\nTensorflow federated: Machine learning on decentralized data. Google, Google. Tensorflow federated: Machine learning on decen- tralized data. https://www.tensorflow.org/ federated, 2020. accessed 25-Mar-20.\n\n. A Hard, K Rao, R Mathews, S Ramaswamy, F Beaufays, S Augenstein, H Eichner, C Kiddon, D Ramage, Federated learning for mobile keyboard predictionHard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S., Eichner, H., Kiddon, C., and Ramage, D. Federated learning for mobile keyboard prediction, 2019.\n\nHighly scalable deep learning training system with mixed-precision: Training imagenet in four minutes. X Jia, S Song, W He, Y Wang, H Rong, F Zhou, L Xie, Z Guo, Y Yang, L Yu, T Chen, G Hu, S Shi, Chu , X , abs/1807.11205CoRR. Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., Xie, L., Guo, Z., Yang, Y., Yu, L., Chen, T., Hu, G., Shi, S., and Chu, X. Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes. CoRR, abs/1807.11205, 2018. URL http: //arxiv.org/abs/1807.11205.\n\nCifar-10 (canadian institute for advanced research). A Krizhevsky, V Nair, G Hinton, OnlineKrizhevsky, A., Nair, V., and Hinton, G. Cifar- 10 (canadian institute for advanced research). On- line, 2005. URL http://www.cs.toronto.edu/ kriz/cifar.html.\n\nF Lai, Y Dai, X Zhu, M Chowdhury, Fedscale, arXiv:2105.11367Benchmarking model and system performance of federated learning. arXiv preprintLai, F., Dai, Y., Zhu, X., and Chowdhury, M. Fedscale: Benchmarking model and system performance of feder- ated learning. arXiv preprint arXiv:2105.11367, 2021.\n\nPrivacy-preserving remote deep-learning inference using sgx. T Lee, Z Lin, S Pushp, C Li, Y Liu, Y Lee, F Xu, C Xu, L Zhang, J Song, Occlumency, 10.1145/3300061.33454479781450361699. doi: 10. 1145/3300061.3345447The 25th Annual International Conference on Mobile Computing and Networking, MobiCom '19. New York, NY, USAAssociation for Computing MachineryLee, T., Lin, Z., Pushp, S., Li, C., Liu, Y., Lee, Y., Xu, F., Xu, C., Zhang, L., and Song, J. Occlumency: Privacy-preserving remote deep-learning inference us- ing sgx. In The 25th Annual International Confer- ence on Mobile Computing and Networking, MobiCom '19, New York, NY, USA, 2019. Association for Com- puting Machinery. ISBN 9781450361699. doi: 10. 1145/3300061.3345447. URL https://doi.org/ 10.1145/3300061.3345447.\n\nT Li, A K Sahu, M Zaheer, M Sanjabi, A Talwalkar, Smith , arXiv:1812.06127Federated optimization in heterogeneous networks. arXiv preprintLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.\n\nFair resource allocation in federated learning. T Li, M Sanjabi, V Smith, arXiv:1905.10497arXiv preprintLi, T., Sanjabi, M., and Smith, V. Fair resource allocation in federated learning. arXiv preprint arXiv:1905.10497, 2019.\n\n. T Li, A K Sahu, M Zaheer, M Sanjabi, A Talwalkar, V Smith, Federated optimization in heterogeneous networksLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Federated optimization in heterogeneous networks, 2020.\n\nAnalog convnet image sensor architecture for continuous mobile vision. R Likamwa, Y Hou, J Gao, M Polansky, L Zhong, Redeye, 10.1109/ISCA.2016.31Proceedings of the 43rd International Symposium on Computer Architecture, ISCA '16. the 43rd International Symposium on Computer Architecture, ISCA '16IEEE PressLiKamWa, R., Hou, Y., Gao, J., Polansky, M., and Zhong, L. Redeye: Analog convnet image sensor architecture for continuous mobile vision. In Proceedings of the 43rd International Symposium on Computer Architec- ture, ISCA '16, pp. 255-266. IEEE Press, 2016. ISBN 9781467389471. doi: 10.1109/ISCA.2016.31. URL https://doi.org/10.1109/ISCA.2016.31.\n\nOn-device model personalization. T Lite, Lite, T. On-device model personalization. https://blog.tensorflow.org/2019/12/ example-on-device-model-personalization. html, 2020.\n\n. M Malekzadeh, D Athanasakis, H Haddadi, B Livshits, Privacy-preserving banditsMalekzadeh, M., Athanasakis, D., Haddadi, H., and Livshits, B. Privacy-preserving bandits, 2019.\n\nCommunication-efficient learning of deep networks from decentralized data. B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, PMLRProceedings of the 20th International Conference on Artificial Intelligence and Statistics. Singh, A. and Zhu, X. J.the 20th International Conference on Artificial Intelligence and StatisticsFort Lauderdale, FL, USA54McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efficient learn- ing of deep networks from decentralized data. In Singh, A. and Zhu, X. J. (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, volume 54 of Proceedings of Machine Learning Research, pp. 1273-1282. PMLR, 2017. URL http://proceedings.mlr.press/ v54/mcmahan17a.html.\n\nP Moritz, R Nishihara, S Wang, A Tumanov, R Liaw, E Liang, M Elibol, Z Yang, W Paul, M I Jordan, I Stoica, Ray, A distributed framework for emerging ai applications. Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., and Stoica, I. Ray: A distributed framework for emerging ai applications, 2018.\n\nJustifying recommendations using distantly-labeled reviews and fine-grained aspects. J Ni, J Li, J Mcauley, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)OpenSignal. The state of mobile network experience 2020: One year into the 5g eraNi, J., Li, J., and McAuley, J. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pp. 188-197, 2019. OpenSignal. The state of mobile network experi- ence 2020: One year into the 5g era. https: //www.opensignal.com/reports/2020/05/ global-state-of-the-mobile-network, 2020. accessed 10-Oct-20.\n\nAdaptive federated optimization. S Reddi, Z Charles, M Zaheer, Z Garrett, K Rush, J Kone\u010dn\u00fd, S Kumar, H B Mcmahan, Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Kone\u010dn\u00fd, J., Kumar, S., and McMahan, H. B. Adaptive federated optimization, 2021.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International journal of computer vision. 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition chal- lenge. International journal of computer vision, 115(3): 211-252, 2015.\n\nA generic framework for privacy preserving deep learning. T Ryffel, A Trask, M Dahl, B Wagner, J Mancuso, D Rueckert, J Passerat-Palmbach, abs/1811.04017CoRRRyffel, T., Trask, A., Dahl, M., Wagner, B., Mancuso, J., Rueckert, D., and Passerat-Palmbach, J. A generic framework for privacy preserving deep learning. CoRR, abs/1811.04017, 2018. URL http://arxiv.org/ abs/1811.04017.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, arXiv:1910.01108abs/1802.05799CoRRarXiv preprintHorovod: fast and easy distributed deep learning in tensorflowSanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Flower: A Friendly Federated Learning Framework Sergeev, A. and Balso, M. D. Horovod: fast and easy distributed deep learning in tensorflow. CoRR, abs/1802.05799, 2018. URL http://arxiv.org/ abs/1802.05799.\n\nDeep convlstm with selfattention for human activity decoding using wearable sensors. S P Singh, M K Sharma, A Lay-Ekuakille, D Gangwar, S Gupta, 10.1109/JSEN.2020.3045135IEEE Sensors Journal. 216Singh, S. P., Sharma, M. K., Lay-Ekuakille, A., Gang- war, D., and Gupta, S. Deep convlstm with self- attention for human activity decoding using wearable sensors. IEEE Sensors Journal, 21(6):8575-8582, Mar 2021. ISSN 2379-9153. doi: 10.1109/jsen.2020. 3045135. URL http://dx.doi.org/10.1109/ JSEN.2020.3045135.\n\nFederated multi-task learning. V Smith, C.-K Chiang, M Sanjabi, A S Talwalkar, Advances in Neural Information Processing Systems. Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S. Federated multi-task learning. In Advances in Neural Information Processing Systems, pp. 4424-4434, 2017.\n\nOn-body localization of wearable devices: An investigation of position-aware activity recognition. T Sztyler, H Stuckenschmidt, 10.1109/PERCOM.2016.74565212016 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE Computer SocietySztyler, T. and Stuckenschmidt, H. On-body localization of wearable devices: An investigation of position-aware activity recognition. In 2016 IEEE International Con- ference on Pervasive Computing and Communications (PerCom), pp. 1-9. IEEE Computer Society, 2016. doi: 10.1109/PERCOM.2016.7456521.\n\nW Falcon, Pytorch lightning. W. Falcon, e. a. Pytorch lightning, 2019. URL https://github.com/williamFalcon/ pytorch-lightning.\n\nFashion-mnist: a novel image dataset for benchmarking machine learning algorithms. H Xiao, K Rasul, R Vollgraf, arXiv:1708.07747arXiv preprintXiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nDistributed stochastic gradient descent with suspicion-based faulttolerance. C Xie, S Koyejo, I Gupta, Zeno, Proceedings of the 36th International Conference on Machine Learning. Chaudhuri, K. and Salakhutdinov, R.the 36th International Conference on Machine LearningLong Beach, California, USA97Xie, C., Koyejo, S., and Gupta, I. Zeno: Distributed stochastic gradient descent with suspicion-based fault- tolerance. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Ma- chine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 6893-6901, Long Beach, Cali- fornia, USA, 09-15 Jun 2019. PMLR. URL http:// proceedings.mlr.press/v97/xie19b.html.\n\nLatent backdoor attacks on deep neural networks. Y Yao, H Li, H Zheng, B Y Zhao, 10.1145/3319535.33542099781450367479. doi: 10.1145/ 3319535.3354209Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS '19. the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS '19New York, NY, USAAssociation for Computing MachineryYao, Y., Li, H., Zheng, H., and Zhao, B. Y. Latent back- door attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS '19, pp. 2041-2055, New York, NY, USA, 2019. Association for Comput- ing Machinery. ISBN 9781450367479. doi: 10.1145/ 3319535.3354209. URL https://doi.org/10. 1145/3319535.3354209.\n", "annotations": {"author": "[{\"end\":67,\"start\":51},{\"end\":80,\"start\":68},{\"end\":94,\"start\":81},{\"end\":106,\"start\":95},{\"end\":132,\"start\":107},{\"end\":141,\"start\":133},{\"end\":155,\"start\":142},{\"end\":169,\"start\":156},{\"end\":188,\"start\":170},{\"end\":201,\"start\":189},{\"end\":220,\"start\":202},{\"end\":237,\"start\":221}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":60},{\"end\":79,\"start\":74},{\"end\":93,\"start\":87},{\"end\":105,\"start\":102},{\"end\":131,\"start\":114},{\"end\":140,\"start\":137},{\"end\":154,\"start\":150},{\"end\":168,\"start\":166},{\"end\":187,\"start\":178},{\"end\":200,\"start\":195},{\"end\":219,\"start\":210},{\"end\":236,\"start\":232}]", "author_first_name": "[{\"end\":57,\"start\":51},{\"end\":59,\"start\":58},{\"end\":73,\"start\":68},{\"end\":86,\"start\":81},{\"end\":101,\"start\":95},{\"end\":113,\"start\":107},{\"end\":136,\"start\":133},{\"end\":149,\"start\":142},{\"end\":161,\"start\":156},{\"end\":165,\"start\":162},{\"end\":177,\"start\":170},{\"end\":194,\"start\":189},{\"end\":209,\"start\":202},{\"end\":229,\"start\":221},{\"end\":231,\"start\":230}]", "author_affiliation": null, "title": "[{\"end\":48,\"start\":1},{\"end\":285,\"start\":238}]", "venue": null, "abstract": "[{\"end\":1394,\"start\":287}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1583,\"start\":1563},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1606,\"start\":1583},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1630,\"start\":1606},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1647,\"start\":1630},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1664,\"start\":1647},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1685,\"start\":1664},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1707,\"start\":1685},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3700,\"start\":3686},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3720,\"start\":3700},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3757,\"start\":3736},{\"end\":3868,\"start\":3864},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4491,\"start\":4472},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7546,\"start\":7525},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7803,\"start\":7784},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7889,\"start\":7868},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7911,\"start\":7891},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8037,\"start\":8014},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8243,\"start\":8225},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8393,\"start\":8373},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8409,\"start\":8393},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8414,\"start\":8409},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8547,\"start\":8528},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8564,\"start\":8547},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8584,\"start\":8564},{\"end\":8606,\"start\":8584},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8626,\"start\":8606},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8650,\"start\":8628},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8737,\"start\":8723},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8767,\"start\":8746},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8799,\"start\":8778},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12129,\"start\":12113},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12250,\"start\":12234},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12411,\"start\":12392},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14859,\"start\":14839},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19134,\"start\":19111},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19166,\"start\":19147},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19748,\"start\":19734},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19776,\"start\":19755},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19805,\"start\":19787},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19836,\"start\":19815},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22154,\"start\":22131},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22191,\"start\":22178},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23583,\"start\":23559},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23698,\"start\":23677},{\"end\":24250,\"start\":24238},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27613,\"start\":27592},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29203,\"start\":29186},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29442,\"start\":29423},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30860,\"start\":30839},{\"end\":31648,\"start\":31634},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33940,\"start\":33928},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34126,\"start\":34106},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34284,\"start\":34252},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35779,\"start\":35765},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41201,\"start\":41178},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41233,\"start\":41214},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":45189,\"start\":45168},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":50727,\"start\":50701},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":53195,\"start\":53169}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":53690,\"start\":53323},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53919,\"start\":53691},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54040,\"start\":53920},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54531,\"start\":54041},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54719,\"start\":54532},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55107,\"start\":54720},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55269,\"start\":55108},{\"attributes\":{\"id\":\"fig_8\"},\"end\":55396,\"start\":55270},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55747,\"start\":55397},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55798,\"start\":55748},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56219,\"start\":55799},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56502,\"start\":56220}]", "paragraph": "[{\"end\":3463,\"start\":1410},{\"end\":4741,\"start\":3465},{\"end\":5475,\"start\":4743},{\"end\":6232,\"start\":5477},{\"end\":7401,\"start\":6234},{\"end\":8415,\"start\":7433},{\"end\":9223,\"start\":8417},{\"end\":9438,\"start\":9225},{\"end\":10037,\"start\":9458},{\"end\":10340,\"start\":10051},{\"end\":10791,\"start\":10342},{\"end\":11174,\"start\":10793},{\"end\":11613,\"start\":11176},{\"end\":12057,\"start\":11615},{\"end\":12178,\"start\":12069},{\"end\":12279,\"start\":12190},{\"end\":12412,\"start\":12292},{\"end\":12663,\"start\":12429},{\"end\":12843,\"start\":12665},{\"end\":13018,\"start\":12845},{\"end\":13164,\"start\":13020},{\"end\":13396,\"start\":13166},{\"end\":13623,\"start\":13398},{\"end\":13917,\"start\":13625},{\"end\":14301,\"start\":13949},{\"end\":15242,\"start\":14303},{\"end\":15975,\"start\":15244},{\"end\":16574,\"start\":15977},{\"end\":17013,\"start\":16576},{\"end\":17986,\"start\":17039},{\"end\":18466,\"start\":18009},{\"end\":18522,\"start\":18500},{\"end\":18937,\"start\":18719},{\"end\":19655,\"start\":18960},{\"end\":19942,\"start\":19683},{\"end\":20129,\"start\":19944},{\"end\":20405,\"start\":20131},{\"end\":21253,\"start\":20407},{\"end\":21867,\"start\":21255},{\"end\":22494,\"start\":21869},{\"end\":23103,\"start\":22496},{\"end\":23699,\"start\":23105},{\"end\":24051,\"start\":23718},{\"end\":24731,\"start\":24053},{\"end\":25506,\"start\":24733},{\"end\":25955,\"start\":25508},{\"end\":26393,\"start\":25957},{\"end\":26767,\"start\":26395},{\"end\":27568,\"start\":26769},{\"end\":28009,\"start\":27570},{\"end\":28206,\"start\":28034},{\"end\":28829,\"start\":28233},{\"end\":29117,\"start\":28831},{\"end\":29857,\"start\":29119},{\"end\":30287,\"start\":29859},{\"end\":30549,\"start\":30318},{\"end\":30773,\"start\":30551},{\"end\":31341,\"start\":30775},{\"end\":31842,\"start\":31343},{\"end\":32301,\"start\":31844},{\"end\":32922,\"start\":32303},{\"end\":33476,\"start\":32924},{\"end\":34021,\"start\":33525},{\"end\":34883,\"start\":34023},{\"end\":35562,\"start\":34885},{\"end\":36572,\"start\":35564},{\"end\":36887,\"start\":36606},{\"end\":37198,\"start\":36889},{\"end\":37656,\"start\":37200},{\"end\":38738,\"start\":37658},{\"end\":39312,\"start\":38740},{\"end\":39993,\"start\":39314},{\"end\":40697,\"start\":39995},{\"end\":41987,\"start\":40730},{\"end\":42369,\"start\":41989},{\"end\":42688,\"start\":42371},{\"end\":43441,\"start\":42690},{\"end\":44325,\"start\":43456},{\"end\":45190,\"start\":44340},{\"end\":45568,\"start\":45214},{\"end\":46728,\"start\":45570},{\"end\":47329,\"start\":46730},{\"end\":50254,\"start\":47331},{\"end\":50322,\"start\":50256},{\"end\":51168,\"start\":50372},{\"end\":51457,\"start\":51170},{\"end\":52096,\"start\":51459},{\"end\":52428,\"start\":52098},{\"end\":52547,\"start\":52471},{\"end\":52837,\"start\":52549},{\"end\":53125,\"start\":52839},{\"end\":53322,\"start\":53127}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18718,\"start\":18523}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19845,\"start\":19838},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37455,\"start\":37448},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38506,\"start\":38499}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1408,\"start\":1396},{\"attributes\":{\"n\":\"2\"},\"end\":7431,\"start\":7404},{\"attributes\":{\"n\":\"3\"},\"end\":9456,\"start\":9441},{\"attributes\":{\"n\":\"3.1\"},\"end\":10049,\"start\":10040},{\"end\":12067,\"start\":12060},{\"end\":12188,\"start\":12181},{\"end\":12290,\"start\":12282},{\"attributes\":{\"n\":\"3.2\"},\"end\":12427,\"start\":12415},{\"attributes\":{\"n\":\"3.3\"},\"end\":13947,\"start\":13920},{\"attributes\":{\"n\":\"3.4\"},\"end\":17037,\"start\":17016},{\"attributes\":{\"n\":\"3.5\"},\"end\":18007,\"start\":17989},{\"end\":18498,\"start\":18469},{\"attributes\":{\"n\":\"3.6\"},\"end\":18958,\"start\":18940},{\"attributes\":{\"n\":\"3.7\"},\"end\":19681,\"start\":19658},{\"attributes\":{\"n\":\"4\"},\"end\":23716,\"start\":23702},{\"attributes\":{\"n\":\"5\"},\"end\":28032,\"start\":28012},{\"attributes\":{\"n\":\"5.1\"},\"end\":28231,\"start\":28209},{\"attributes\":{\"n\":\"5.2\"},\"end\":30316,\"start\":30290},{\"attributes\":{\"n\":\"5.3\"},\"end\":33523,\"start\":33479},{\"attributes\":{\"n\":\"5.4\"},\"end\":36604,\"start\":36575},{\"attributes\":{\"n\":\"5.5\"},\"end\":40728,\"start\":40700},{\"attributes\":{\"n\":\"6\"},\"end\":43454,\"start\":43444},{\"end\":44338,\"start\":44328},{\"end\":45212,\"start\":45193},{\"end\":50370,\"start\":50325},{\"end\":52469,\"start\":52431},{\"end\":53334,\"start\":53324},{\"end\":53702,\"start\":53692},{\"end\":53931,\"start\":53921},{\"end\":54062,\"start\":54042},{\"end\":54543,\"start\":54533},{\"end\":54731,\"start\":54721},{\"end\":55119,\"start\":55109},{\"end\":55282,\"start\":55271},{\"end\":55407,\"start\":55398},{\"end\":55758,\"start\":55749},{\"end\":55809,\"start\":55800}]", "table": "[{\"end\":55747,\"start\":55527},{\"end\":56219,\"start\":56051}]", "figure_caption": "[{\"end\":53690,\"start\":53336},{\"end\":53919,\"start\":53704},{\"end\":54040,\"start\":53933},{\"end\":54531,\"start\":54065},{\"end\":54719,\"start\":54545},{\"end\":55107,\"start\":54733},{\"end\":55269,\"start\":55121},{\"end\":55396,\"start\":55285},{\"end\":55527,\"start\":55409},{\"end\":55798,\"start\":55760},{\"end\":56051,\"start\":55811},{\"end\":56502,\"start\":56222}]", "figure_ref": "[{\"end\":2647,\"start\":2639},{\"end\":4581,\"start\":4573},{\"end\":10166,\"start\":10157},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15212,\"start\":15204},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29745,\"start\":29737},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29876,\"start\":29868},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31360,\"start\":31352},{\"end\":34040,\"start\":34032},{\"end\":34744,\"start\":34736},{\"end\":36253,\"start\":36245},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42555,\"start\":42547},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44533,\"start\":44526},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":51188,\"start\":51179},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":51631,\"start\":51621}]", "bib_author_first_name": "[{\"end\":56560,\"start\":56559},{\"end\":56569,\"start\":56568},{\"end\":56579,\"start\":56578},{\"end\":56587,\"start\":56586},{\"end\":56595,\"start\":56594},{\"end\":56604,\"start\":56603},{\"end\":56612,\"start\":56611},{\"end\":56621,\"start\":56620},{\"end\":56633,\"start\":56632},{\"end\":56643,\"start\":56642},{\"end\":56652,\"start\":56651},{\"end\":56662,\"start\":56661},{\"end\":56675,\"start\":56674},{\"end\":56684,\"start\":56683},{\"end\":56693,\"start\":56692},{\"end\":56695,\"start\":56694},{\"end\":56705,\"start\":56704},{\"end\":56716,\"start\":56715},{\"end\":56726,\"start\":56725},{\"end\":56739,\"start\":56738},{\"end\":56749,\"start\":56748},{\"end\":56758,\"start\":56757},{\"end\":56764,\"start\":56763},{\"end\":57391,\"start\":57390},{\"end\":57400,\"start\":57399},{\"end\":57407,\"start\":57406},{\"end\":57421,\"start\":57420},{\"end\":57432,\"start\":57431},{\"end\":57443,\"start\":57442},{\"end\":57453,\"start\":57452},{\"end\":57864,\"start\":57863},{\"end\":57866,\"start\":57865},{\"end\":57874,\"start\":57873},{\"end\":57876,\"start\":57875},{\"end\":57888,\"start\":57887},{\"end\":57898,\"start\":57897},{\"end\":57909,\"start\":57908},{\"end\":58407,\"start\":58406},{\"end\":58409,\"start\":58408},{\"end\":58420,\"start\":58419},{\"end\":58435,\"start\":58434},{\"end\":58445,\"start\":58444},{\"end\":58447,\"start\":58446},{\"end\":58710,\"start\":58709},{\"end\":58722,\"start\":58721},{\"end\":58732,\"start\":58731},{\"end\":58743,\"start\":58742},{\"end\":58756,\"start\":58755},{\"end\":58758,\"start\":58757},{\"end\":58769,\"start\":58768},{\"end\":58778,\"start\":58777},{\"end\":58788,\"start\":58787},{\"end\":58800,\"start\":58796},{\"end\":58804,\"start\":58803},{\"end\":59534,\"start\":59533},{\"end\":59546,\"start\":59545},{\"end\":59557,\"start\":59556},{\"end\":59570,\"start\":59569},{\"end\":59578,\"start\":59577},{\"end\":59590,\"start\":59589},{\"end\":59600,\"start\":59599},{\"end\":59602,\"start\":59601},{\"end\":59612,\"start\":59611},{\"end\":59623,\"start\":59622},{\"end\":59636,\"start\":59635},{\"end\":59647,\"start\":59646},{\"end\":59649,\"start\":59648},{\"end\":59662,\"start\":59661},{\"end\":59672,\"start\":59671},{\"end\":59682,\"start\":59681},{\"end\":60089,\"start\":60088},{\"end\":60101,\"start\":60100},{\"end\":60112,\"start\":60111},{\"end\":60123,\"start\":60122},{\"end\":60125,\"start\":60124},{\"end\":60136,\"start\":60135},{\"end\":60145,\"start\":60144},{\"end\":60158,\"start\":60157},{\"end\":60168,\"start\":60167},{\"end\":60178,\"start\":60177},{\"end\":60192,\"start\":60191},{\"end\":60211,\"start\":60210},{\"end\":60476,\"start\":60475},{\"end\":60486,\"start\":60485},{\"end\":60490,\"start\":60487},{\"end\":60499,\"start\":60498},{\"end\":60505,\"start\":60504},{\"end\":60511,\"start\":60510},{\"end\":60522,\"start\":60521},{\"end\":60524,\"start\":60523},{\"end\":60535,\"start\":60534},{\"end\":60544,\"start\":60543},{\"end\":60884,\"start\":60883},{\"end\":60886,\"start\":60885},{\"end\":60896,\"start\":60895},{\"end\":60898,\"start\":60897},{\"end\":60908,\"start\":60907},{\"end\":61161,\"start\":61160},{\"end\":61174,\"start\":61173},{\"end\":61184,\"start\":61183},{\"end\":61194,\"start\":61193},{\"end\":61204,\"start\":61203},{\"end\":61413,\"start\":61412},{\"end\":61421,\"start\":61420},{\"end\":61423,\"start\":61422},{\"end\":61434,\"start\":61433},{\"end\":61443,\"start\":61442},{\"end\":61451,\"start\":61450},{\"end\":61460,\"start\":61459},{\"end\":61462,\"start\":61461},{\"end\":61468,\"start\":61467},{\"end\":61470,\"start\":61469},{\"end\":61477,\"start\":61476},{\"end\":61488,\"start\":61487},{\"end\":61498,\"start\":61497},{\"end\":61508,\"start\":61507},{\"end\":61516,\"start\":61515},{\"end\":61518,\"start\":61517},{\"end\":62199,\"start\":62198},{\"end\":62209,\"start\":62208},{\"end\":62211,\"start\":62210},{\"end\":62221,\"start\":62220},{\"end\":62229,\"start\":62228},{\"end\":63121,\"start\":63120},{\"end\":63125,\"start\":63122},{\"end\":63345,\"start\":63344},{\"end\":63354,\"start\":63353},{\"end\":63363,\"start\":63362},{\"end\":63953,\"start\":63952},{\"end\":63965,\"start\":63964},{\"end\":63967,\"start\":63966},{\"end\":63975,\"start\":63974},{\"end\":63988,\"start\":63985},{\"end\":63992,\"start\":63991},{\"end\":64982,\"start\":64981},{\"end\":64990,\"start\":64989},{\"end\":64997,\"start\":64996},{\"end\":65008,\"start\":65007},{\"end\":65021,\"start\":65020},{\"end\":65033,\"start\":65032},{\"end\":65047,\"start\":65046},{\"end\":65058,\"start\":65057},{\"end\":65068,\"start\":65067},{\"end\":65405,\"start\":65404},{\"end\":65412,\"start\":65411},{\"end\":65420,\"start\":65419},{\"end\":65426,\"start\":65425},{\"end\":65434,\"start\":65433},{\"end\":65442,\"start\":65441},{\"end\":65450,\"start\":65449},{\"end\":65457,\"start\":65456},{\"end\":65464,\"start\":65463},{\"end\":65472,\"start\":65471},{\"end\":65478,\"start\":65477},{\"end\":65486,\"start\":65485},{\"end\":65492,\"start\":65491},{\"end\":65501,\"start\":65498},{\"end\":65505,\"start\":65504},{\"end\":65884,\"start\":65883},{\"end\":65898,\"start\":65897},{\"end\":65906,\"start\":65905},{\"end\":66082,\"start\":66081},{\"end\":66089,\"start\":66088},{\"end\":66096,\"start\":66095},{\"end\":66103,\"start\":66102},{\"end\":66444,\"start\":66443},{\"end\":66451,\"start\":66450},{\"end\":66458,\"start\":66457},{\"end\":66467,\"start\":66466},{\"end\":66473,\"start\":66472},{\"end\":66480,\"start\":66479},{\"end\":66487,\"start\":66486},{\"end\":66493,\"start\":66492},{\"end\":66499,\"start\":66498},{\"end\":66508,\"start\":66507},{\"end\":67164,\"start\":67163},{\"end\":67170,\"start\":67169},{\"end\":67172,\"start\":67171},{\"end\":67180,\"start\":67179},{\"end\":67190,\"start\":67189},{\"end\":67201,\"start\":67200},{\"end\":67218,\"start\":67213},{\"end\":67515,\"start\":67514},{\"end\":67521,\"start\":67520},{\"end\":67532,\"start\":67531},{\"end\":67696,\"start\":67695},{\"end\":67702,\"start\":67701},{\"end\":67704,\"start\":67703},{\"end\":67712,\"start\":67711},{\"end\":67722,\"start\":67721},{\"end\":67733,\"start\":67732},{\"end\":67746,\"start\":67745},{\"end\":68006,\"start\":68005},{\"end\":68017,\"start\":68016},{\"end\":68024,\"start\":68023},{\"end\":68031,\"start\":68030},{\"end\":68043,\"start\":68042},{\"end\":68622,\"start\":68621},{\"end\":68765,\"start\":68764},{\"end\":68779,\"start\":68778},{\"end\":68794,\"start\":68793},{\"end\":68805,\"start\":68804},{\"end\":69016,\"start\":69015},{\"end\":69027,\"start\":69026},{\"end\":69036,\"start\":69035},{\"end\":69046,\"start\":69045},{\"end\":69057,\"start\":69056},{\"end\":69059,\"start\":69058},{\"end\":69758,\"start\":69757},{\"end\":69768,\"start\":69767},{\"end\":69781,\"start\":69780},{\"end\":69789,\"start\":69788},{\"end\":69800,\"start\":69799},{\"end\":69808,\"start\":69807},{\"end\":69817,\"start\":69816},{\"end\":69827,\"start\":69826},{\"end\":69835,\"start\":69834},{\"end\":69843,\"start\":69842},{\"end\":69845,\"start\":69844},{\"end\":69855,\"start\":69854},{\"end\":70208,\"start\":70207},{\"end\":70214,\"start\":70213},{\"end\":70220,\"start\":70219},{\"end\":71193,\"start\":71192},{\"end\":71202,\"start\":71201},{\"end\":71213,\"start\":71212},{\"end\":71223,\"start\":71222},{\"end\":71234,\"start\":71233},{\"end\":71242,\"start\":71241},{\"end\":71253,\"start\":71252},{\"end\":71262,\"start\":71261},{\"end\":71264,\"start\":71263},{\"end\":71468,\"start\":71467},{\"end\":71483,\"start\":71482},{\"end\":71491,\"start\":71490},{\"end\":71497,\"start\":71496},{\"end\":71507,\"start\":71506},{\"end\":71519,\"start\":71518},{\"end\":71525,\"start\":71524},{\"end\":71534,\"start\":71533},{\"end\":71546,\"start\":71545},{\"end\":71556,\"start\":71555},{\"end\":71920,\"start\":71919},{\"end\":71930,\"start\":71929},{\"end\":71939,\"start\":71938},{\"end\":71947,\"start\":71946},{\"end\":71957,\"start\":71956},{\"end\":71968,\"start\":71967},{\"end\":71980,\"start\":71979},{\"end\":72321,\"start\":72320},{\"end\":72329,\"start\":72328},{\"end\":72338,\"start\":72337},{\"end\":72350,\"start\":72349},{\"end\":72927,\"start\":72926},{\"end\":72929,\"start\":72928},{\"end\":72938,\"start\":72937},{\"end\":72940,\"start\":72939},{\"end\":72950,\"start\":72949},{\"end\":72967,\"start\":72966},{\"end\":72978,\"start\":72977},{\"end\":73381,\"start\":73380},{\"end\":73393,\"start\":73389},{\"end\":73403,\"start\":73402},{\"end\":73414,\"start\":73413},{\"end\":73416,\"start\":73415},{\"end\":73746,\"start\":73745},{\"end\":73757,\"start\":73756},{\"end\":74209,\"start\":74208},{\"end\":74421,\"start\":74420},{\"end\":74429,\"start\":74428},{\"end\":74438,\"start\":74437},{\"end\":74718,\"start\":74717},{\"end\":74725,\"start\":74724},{\"end\":74735,\"start\":74734},{\"end\":75400,\"start\":75399},{\"end\":75407,\"start\":75406},{\"end\":75413,\"start\":75412},{\"end\":75422,\"start\":75421},{\"end\":75424,\"start\":75423}]", "bib_author_last_name": "[{\"end\":56566,\"start\":56561},{\"end\":56576,\"start\":56570},{\"end\":56584,\"start\":56580},{\"end\":56592,\"start\":56588},{\"end\":56601,\"start\":56596},{\"end\":56609,\"start\":56605},{\"end\":56618,\"start\":56613},{\"end\":56630,\"start\":56622},{\"end\":56640,\"start\":56634},{\"end\":56649,\"start\":56644},{\"end\":56659,\"start\":56653},{\"end\":56672,\"start\":56663},{\"end\":56681,\"start\":56676},{\"end\":56690,\"start\":56685},{\"end\":56702,\"start\":56696},{\"end\":56713,\"start\":56706},{\"end\":56723,\"start\":56717},{\"end\":56736,\"start\":56727},{\"end\":56746,\"start\":56740},{\"end\":56755,\"start\":56750},{\"end\":56761,\"start\":56759},{\"end\":56770,\"start\":56765},{\"end\":57397,\"start\":57392},{\"end\":57404,\"start\":57401},{\"end\":57418,\"start\":57408},{\"end\":57429,\"start\":57422},{\"end\":57440,\"start\":57433},{\"end\":57450,\"start\":57444},{\"end\":57459,\"start\":57454},{\"end\":57871,\"start\":57867},{\"end\":57885,\"start\":57877},{\"end\":57895,\"start\":57889},{\"end\":57906,\"start\":57899},{\"end\":57917,\"start\":57910},{\"end\":58417,\"start\":58410},{\"end\":58432,\"start\":58421},{\"end\":58442,\"start\":58436},{\"end\":58452,\"start\":58448},{\"end\":58719,\"start\":58711},{\"end\":58729,\"start\":58723},{\"end\":58740,\"start\":58733},{\"end\":58753,\"start\":58744},{\"end\":58766,\"start\":58759},{\"end\":58775,\"start\":58770},{\"end\":58785,\"start\":58779},{\"end\":58794,\"start\":58789},{\"end\":59543,\"start\":59535},{\"end\":59554,\"start\":59547},{\"end\":59567,\"start\":59558},{\"end\":59575,\"start\":59571},{\"end\":59587,\"start\":59579},{\"end\":59597,\"start\":59591},{\"end\":59609,\"start\":59603},{\"end\":59620,\"start\":59613},{\"end\":59633,\"start\":59624},{\"end\":59644,\"start\":59637},{\"end\":59659,\"start\":59650},{\"end\":59669,\"start\":59663},{\"end\":59679,\"start\":59673},{\"end\":59693,\"start\":59683},{\"end\":60098,\"start\":60090},{\"end\":60109,\"start\":60102},{\"end\":60120,\"start\":60113},{\"end\":60133,\"start\":60126},{\"end\":60142,\"start\":60137},{\"end\":60155,\"start\":60146},{\"end\":60165,\"start\":60159},{\"end\":60175,\"start\":60169},{\"end\":60189,\"start\":60179},{\"end\":60208,\"start\":60193},{\"end\":60217,\"start\":60212},{\"end\":60483,\"start\":60477},{\"end\":60496,\"start\":60491},{\"end\":60502,\"start\":60500},{\"end\":60508,\"start\":60506},{\"end\":60519,\"start\":60512},{\"end\":60532,\"start\":60525},{\"end\":60541,\"start\":60536},{\"end\":60554,\"start\":60545},{\"end\":60560,\"start\":60556},{\"end\":60893,\"start\":60887},{\"end\":60905,\"start\":60899},{\"end\":60912,\"start\":60909},{\"end\":61171,\"start\":61162},{\"end\":61181,\"start\":61175},{\"end\":61191,\"start\":61185},{\"end\":61201,\"start\":61195},{\"end\":61211,\"start\":61205},{\"end\":61418,\"start\":61414},{\"end\":61431,\"start\":61424},{\"end\":61440,\"start\":61435},{\"end\":61448,\"start\":61444},{\"end\":61457,\"start\":61452},{\"end\":61465,\"start\":61463},{\"end\":61474,\"start\":61471},{\"end\":61485,\"start\":61478},{\"end\":61495,\"start\":61489},{\"end\":61505,\"start\":61499},{\"end\":61513,\"start\":61509},{\"end\":61521,\"start\":61519},{\"end\":62206,\"start\":62200},{\"end\":62218,\"start\":62212},{\"end\":62226,\"start\":62222},{\"end\":62239,\"start\":62230},{\"end\":62908,\"start\":62902},{\"end\":63136,\"start\":63126},{\"end\":63142,\"start\":63138},{\"end\":63351,\"start\":63346},{\"end\":63360,\"start\":63355},{\"end\":63373,\"start\":63364},{\"end\":63962,\"start\":63954},{\"end\":63972,\"start\":63968},{\"end\":63983,\"start\":63976},{\"end\":64839,\"start\":64833},{\"end\":64987,\"start\":64983},{\"end\":64994,\"start\":64991},{\"end\":65005,\"start\":64998},{\"end\":65018,\"start\":65009},{\"end\":65030,\"start\":65022},{\"end\":65044,\"start\":65034},{\"end\":65055,\"start\":65048},{\"end\":65065,\"start\":65059},{\"end\":65075,\"start\":65069},{\"end\":65409,\"start\":65406},{\"end\":65417,\"start\":65413},{\"end\":65423,\"start\":65421},{\"end\":65431,\"start\":65427},{\"end\":65439,\"start\":65435},{\"end\":65447,\"start\":65443},{\"end\":65454,\"start\":65451},{\"end\":65461,\"start\":65458},{\"end\":65469,\"start\":65465},{\"end\":65475,\"start\":65473},{\"end\":65483,\"start\":65479},{\"end\":65489,\"start\":65487},{\"end\":65496,\"start\":65493},{\"end\":65895,\"start\":65885},{\"end\":65903,\"start\":65899},{\"end\":65913,\"start\":65907},{\"end\":66086,\"start\":66083},{\"end\":66093,\"start\":66090},{\"end\":66100,\"start\":66097},{\"end\":66113,\"start\":66104},{\"end\":66123,\"start\":66115},{\"end\":66448,\"start\":66445},{\"end\":66455,\"start\":66452},{\"end\":66464,\"start\":66459},{\"end\":66470,\"start\":66468},{\"end\":66477,\"start\":66474},{\"end\":66484,\"start\":66481},{\"end\":66490,\"start\":66488},{\"end\":66496,\"start\":66494},{\"end\":66505,\"start\":66500},{\"end\":66513,\"start\":66509},{\"end\":66525,\"start\":66515},{\"end\":67167,\"start\":67165},{\"end\":67177,\"start\":67173},{\"end\":67187,\"start\":67181},{\"end\":67198,\"start\":67191},{\"end\":67211,\"start\":67202},{\"end\":67518,\"start\":67516},{\"end\":67529,\"start\":67522},{\"end\":67538,\"start\":67533},{\"end\":67699,\"start\":67697},{\"end\":67709,\"start\":67705},{\"end\":67719,\"start\":67713},{\"end\":67730,\"start\":67723},{\"end\":67743,\"start\":67734},{\"end\":67752,\"start\":67747},{\"end\":68014,\"start\":68007},{\"end\":68021,\"start\":68018},{\"end\":68028,\"start\":68025},{\"end\":68040,\"start\":68032},{\"end\":68049,\"start\":68044},{\"end\":68057,\"start\":68051},{\"end\":68627,\"start\":68623},{\"end\":68776,\"start\":68766},{\"end\":68791,\"start\":68780},{\"end\":68802,\"start\":68795},{\"end\":68814,\"start\":68806},{\"end\":69024,\"start\":69017},{\"end\":69033,\"start\":69028},{\"end\":69043,\"start\":69037},{\"end\":69054,\"start\":69047},{\"end\":69065,\"start\":69060},{\"end\":69765,\"start\":69759},{\"end\":69778,\"start\":69769},{\"end\":69786,\"start\":69782},{\"end\":69797,\"start\":69790},{\"end\":69805,\"start\":69801},{\"end\":69814,\"start\":69809},{\"end\":69824,\"start\":69818},{\"end\":69832,\"start\":69828},{\"end\":69840,\"start\":69836},{\"end\":69852,\"start\":69846},{\"end\":69862,\"start\":69856},{\"end\":69867,\"start\":69864},{\"end\":70211,\"start\":70209},{\"end\":70217,\"start\":70215},{\"end\":70228,\"start\":70221},{\"end\":71199,\"start\":71194},{\"end\":71210,\"start\":71203},{\"end\":71220,\"start\":71214},{\"end\":71231,\"start\":71224},{\"end\":71239,\"start\":71235},{\"end\":71250,\"start\":71243},{\"end\":71259,\"start\":71254},{\"end\":71272,\"start\":71265},{\"end\":71480,\"start\":71469},{\"end\":71488,\"start\":71484},{\"end\":71494,\"start\":71492},{\"end\":71504,\"start\":71498},{\"end\":71516,\"start\":71508},{\"end\":71522,\"start\":71520},{\"end\":71531,\"start\":71526},{\"end\":71543,\"start\":71535},{\"end\":71553,\"start\":71547},{\"end\":71566,\"start\":71557},{\"end\":71927,\"start\":71921},{\"end\":71936,\"start\":71931},{\"end\":71944,\"start\":71940},{\"end\":71954,\"start\":71948},{\"end\":71965,\"start\":71958},{\"end\":71977,\"start\":71969},{\"end\":71998,\"start\":71981},{\"end\":72326,\"start\":72322},{\"end\":72335,\"start\":72330},{\"end\":72347,\"start\":72339},{\"end\":72355,\"start\":72351},{\"end\":72935,\"start\":72930},{\"end\":72947,\"start\":72941},{\"end\":72964,\"start\":72951},{\"end\":72975,\"start\":72968},{\"end\":72984,\"start\":72979},{\"end\":73387,\"start\":73382},{\"end\":73400,\"start\":73394},{\"end\":73411,\"start\":73404},{\"end\":73426,\"start\":73417},{\"end\":73754,\"start\":73747},{\"end\":73772,\"start\":73758},{\"end\":74216,\"start\":74210},{\"end\":74426,\"start\":74422},{\"end\":74435,\"start\":74430},{\"end\":74447,\"start\":74439},{\"end\":74722,\"start\":74719},{\"end\":74732,\"start\":74726},{\"end\":74741,\"start\":74736},{\"end\":74747,\"start\":74743},{\"end\":75404,\"start\":75401},{\"end\":75410,\"start\":75408},{\"end\":75419,\"start\":75414},{\"end\":75429,\"start\":75425}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":57347,\"start\":56504},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207241585},\"end\":57794,\"start\":57349},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":219602752},\"end\":58324,\"start\":57796},{\"attributes\":{\"id\":\"b3\"},\"end\":58638,\"start\":58326},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/3133956.3133982\",\"id\":\"b4\",\"matched_paper_id\":3833774},\"end\":59479,\"start\":58640},{\"attributes\":{\"id\":\"b5\"},\"end\":60028,\"start\":59481},{\"attributes\":{\"id\":\"b6\"},\"end\":60473,\"start\":60030},{\"attributes\":{\"doi\":\"arXiv:1812.01097\",\"id\":\"b7\"},\"end\":60811,\"start\":60475},{\"attributes\":{\"doi\":\"abs/1810.11787\",\"id\":\"b8\"},\"end\":61109,\"start\":60813},{\"attributes\":{\"id\":\"b9\"},\"end\":61371,\"start\":61111},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":372467},\"end\":62117,\"start\":61373},{\"attributes\":{\"doi\":\"10.1109/MLHPC.2016.4\",\"id\":\"b11\",\"matched_paper_id\":1712156},\"end\":62860,\"start\":62119},{\"attributes\":{\"id\":\"b12\"},\"end\":63062,\"start\":62862},{\"attributes\":{\"id\":\"b13\"},\"end\":63272,\"start\":63064},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":44128338},\"end\":63873,\"start\":63274},{\"attributes\":{\"doi\":\"10.1145/3081333.3081358\",\"id\":\"b15\",\"matched_paper_id\":24963942},\"end\":64769,\"start\":63875},{\"attributes\":{\"id\":\"b16\"},\"end\":64977,\"start\":64771},{\"attributes\":{\"id\":\"b17\"},\"end\":65299,\"start\":64979},{\"attributes\":{\"doi\":\"abs/1807.11205\",\"id\":\"b18\",\"matched_paper_id\":51876267},\"end\":65828,\"start\":65301},{\"attributes\":{\"id\":\"b19\"},\"end\":66079,\"start\":65830},{\"attributes\":{\"doi\":\"arXiv:2105.11367\",\"id\":\"b20\"},\"end\":66380,\"start\":66081},{\"attributes\":{\"doi\":\"10.1145/3300061.3345447\",\"id\":\"b21\",\"matched_paper_id\":204714961},\"end\":67161,\"start\":66382},{\"attributes\":{\"id\":\"b22\"},\"end\":67464,\"start\":67163},{\"attributes\":{\"id\":\"b23\"},\"end\":67691,\"start\":67466},{\"attributes\":{\"id\":\"b24\"},\"end\":67932,\"start\":67693},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":110960},\"end\":68586,\"start\":67934},{\"attributes\":{\"id\":\"b26\"},\"end\":68760,\"start\":68588},{\"attributes\":{\"id\":\"b27\"},\"end\":68938,\"start\":68762},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14955348},\"end\":69755,\"start\":68940},{\"attributes\":{\"id\":\"b29\"},\"end\":70120,\"start\":69757},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202621357},\"end\":71157,\"start\":70122},{\"attributes\":{\"id\":\"b31\"},\"end\":71414,\"start\":71159},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2930547},\"end\":71859,\"start\":71416},{\"attributes\":{\"id\":\"b33\"},\"end\":72239,\"start\":71861},{\"attributes\":{\"id\":\"b34\"},\"end\":72839,\"start\":72241},{\"attributes\":{\"id\":\"b35\"},\"end\":73347,\"start\":72841},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3586416},\"end\":73644,\"start\":73349},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3000875},\"end\":74206,\"start\":73646},{\"attributes\":{\"id\":\"b38\"},\"end\":74335,\"start\":74208},{\"attributes\":{\"id\":\"b39\"},\"end\":74638,\"start\":74337},{\"attributes\":{\"id\":\"b40\"},\"end\":75348,\"start\":74640},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202235190},\"end\":76094,\"start\":75350}]", "bib_title": "[{\"end\":56557,\"start\":56504},{\"end\":57388,\"start\":57349},{\"end\":57861,\"start\":57796},{\"end\":58707,\"start\":58640},{\"end\":61410,\"start\":61373},{\"end\":62196,\"start\":62119},{\"end\":63342,\"start\":63274},{\"end\":63950,\"start\":63875},{\"end\":65402,\"start\":65301},{\"end\":66441,\"start\":66382},{\"end\":68003,\"start\":67934},{\"end\":69013,\"start\":68940},{\"end\":70205,\"start\":70122},{\"end\":71465,\"start\":71416},{\"end\":72924,\"start\":72841},{\"end\":73378,\"start\":73349},{\"end\":73743,\"start\":73646},{\"end\":74715,\"start\":74640},{\"end\":75397,\"start\":75350}]", "bib_author": "[{\"end\":56568,\"start\":56559},{\"end\":56578,\"start\":56568},{\"end\":56586,\"start\":56578},{\"end\":56594,\"start\":56586},{\"end\":56603,\"start\":56594},{\"end\":56611,\"start\":56603},{\"end\":56620,\"start\":56611},{\"end\":56632,\"start\":56620},{\"end\":56642,\"start\":56632},{\"end\":56651,\"start\":56642},{\"end\":56661,\"start\":56651},{\"end\":56674,\"start\":56661},{\"end\":56683,\"start\":56674},{\"end\":56692,\"start\":56683},{\"end\":56704,\"start\":56692},{\"end\":56715,\"start\":56704},{\"end\":56725,\"start\":56715},{\"end\":56738,\"start\":56725},{\"end\":56748,\"start\":56738},{\"end\":56757,\"start\":56748},{\"end\":56763,\"start\":56757},{\"end\":56772,\"start\":56763},{\"end\":57399,\"start\":57390},{\"end\":57406,\"start\":57399},{\"end\":57420,\"start\":57406},{\"end\":57431,\"start\":57420},{\"end\":57442,\"start\":57431},{\"end\":57452,\"start\":57442},{\"end\":57461,\"start\":57452},{\"end\":57873,\"start\":57863},{\"end\":57887,\"start\":57873},{\"end\":57897,\"start\":57887},{\"end\":57908,\"start\":57897},{\"end\":57919,\"start\":57908},{\"end\":58419,\"start\":58406},{\"end\":58434,\"start\":58419},{\"end\":58444,\"start\":58434},{\"end\":58454,\"start\":58444},{\"end\":58721,\"start\":58709},{\"end\":58731,\"start\":58721},{\"end\":58742,\"start\":58731},{\"end\":58755,\"start\":58742},{\"end\":58768,\"start\":58755},{\"end\":58777,\"start\":58768},{\"end\":58787,\"start\":58777},{\"end\":58796,\"start\":58787},{\"end\":58803,\"start\":58796},{\"end\":58807,\"start\":58803},{\"end\":59545,\"start\":59533},{\"end\":59556,\"start\":59545},{\"end\":59569,\"start\":59556},{\"end\":59577,\"start\":59569},{\"end\":59589,\"start\":59577},{\"end\":59599,\"start\":59589},{\"end\":59611,\"start\":59599},{\"end\":59622,\"start\":59611},{\"end\":59635,\"start\":59622},{\"end\":59646,\"start\":59635},{\"end\":59661,\"start\":59646},{\"end\":59671,\"start\":59661},{\"end\":59681,\"start\":59671},{\"end\":59695,\"start\":59681},{\"end\":60100,\"start\":60088},{\"end\":60111,\"start\":60100},{\"end\":60122,\"start\":60111},{\"end\":60135,\"start\":60122},{\"end\":60144,\"start\":60135},{\"end\":60157,\"start\":60144},{\"end\":60167,\"start\":60157},{\"end\":60177,\"start\":60167},{\"end\":60191,\"start\":60177},{\"end\":60210,\"start\":60191},{\"end\":60219,\"start\":60210},{\"end\":60485,\"start\":60475},{\"end\":60498,\"start\":60485},{\"end\":60504,\"start\":60498},{\"end\":60510,\"start\":60504},{\"end\":60521,\"start\":60510},{\"end\":60534,\"start\":60521},{\"end\":60543,\"start\":60534},{\"end\":60556,\"start\":60543},{\"end\":60562,\"start\":60556},{\"end\":60895,\"start\":60883},{\"end\":60907,\"start\":60895},{\"end\":60914,\"start\":60907},{\"end\":61173,\"start\":61160},{\"end\":61183,\"start\":61173},{\"end\":61193,\"start\":61183},{\"end\":61203,\"start\":61193},{\"end\":61213,\"start\":61203},{\"end\":61420,\"start\":61412},{\"end\":61433,\"start\":61420},{\"end\":61442,\"start\":61433},{\"end\":61450,\"start\":61442},{\"end\":61459,\"start\":61450},{\"end\":61467,\"start\":61459},{\"end\":61476,\"start\":61467},{\"end\":61487,\"start\":61476},{\"end\":61497,\"start\":61487},{\"end\":61507,\"start\":61497},{\"end\":61515,\"start\":61507},{\"end\":61523,\"start\":61515},{\"end\":62208,\"start\":62198},{\"end\":62220,\"start\":62208},{\"end\":62228,\"start\":62220},{\"end\":62241,\"start\":62228},{\"end\":62910,\"start\":62902},{\"end\":63138,\"start\":63120},{\"end\":63144,\"start\":63138},{\"end\":63353,\"start\":63344},{\"end\":63362,\"start\":63353},{\"end\":63375,\"start\":63362},{\"end\":63964,\"start\":63952},{\"end\":63974,\"start\":63964},{\"end\":63985,\"start\":63974},{\"end\":63991,\"start\":63985},{\"end\":63995,\"start\":63991},{\"end\":64841,\"start\":64833},{\"end\":64989,\"start\":64981},{\"end\":64996,\"start\":64989},{\"end\":65007,\"start\":64996},{\"end\":65020,\"start\":65007},{\"end\":65032,\"start\":65020},{\"end\":65046,\"start\":65032},{\"end\":65057,\"start\":65046},{\"end\":65067,\"start\":65057},{\"end\":65077,\"start\":65067},{\"end\":65411,\"start\":65404},{\"end\":65419,\"start\":65411},{\"end\":65425,\"start\":65419},{\"end\":65433,\"start\":65425},{\"end\":65441,\"start\":65433},{\"end\":65449,\"start\":65441},{\"end\":65456,\"start\":65449},{\"end\":65463,\"start\":65456},{\"end\":65471,\"start\":65463},{\"end\":65477,\"start\":65471},{\"end\":65485,\"start\":65477},{\"end\":65491,\"start\":65485},{\"end\":65498,\"start\":65491},{\"end\":65504,\"start\":65498},{\"end\":65508,\"start\":65504},{\"end\":65897,\"start\":65883},{\"end\":65905,\"start\":65897},{\"end\":65915,\"start\":65905},{\"end\":66088,\"start\":66081},{\"end\":66095,\"start\":66088},{\"end\":66102,\"start\":66095},{\"end\":66115,\"start\":66102},{\"end\":66125,\"start\":66115},{\"end\":66450,\"start\":66443},{\"end\":66457,\"start\":66450},{\"end\":66466,\"start\":66457},{\"end\":66472,\"start\":66466},{\"end\":66479,\"start\":66472},{\"end\":66486,\"start\":66479},{\"end\":66492,\"start\":66486},{\"end\":66498,\"start\":66492},{\"end\":66507,\"start\":66498},{\"end\":66515,\"start\":66507},{\"end\":66527,\"start\":66515},{\"end\":67169,\"start\":67163},{\"end\":67179,\"start\":67169},{\"end\":67189,\"start\":67179},{\"end\":67200,\"start\":67189},{\"end\":67213,\"start\":67200},{\"end\":67221,\"start\":67213},{\"end\":67520,\"start\":67514},{\"end\":67531,\"start\":67520},{\"end\":67540,\"start\":67531},{\"end\":67701,\"start\":67695},{\"end\":67711,\"start\":67701},{\"end\":67721,\"start\":67711},{\"end\":67732,\"start\":67721},{\"end\":67745,\"start\":67732},{\"end\":67754,\"start\":67745},{\"end\":68016,\"start\":68005},{\"end\":68023,\"start\":68016},{\"end\":68030,\"start\":68023},{\"end\":68042,\"start\":68030},{\"end\":68051,\"start\":68042},{\"end\":68059,\"start\":68051},{\"end\":68629,\"start\":68621},{\"end\":68778,\"start\":68764},{\"end\":68793,\"start\":68778},{\"end\":68804,\"start\":68793},{\"end\":68816,\"start\":68804},{\"end\":69026,\"start\":69015},{\"end\":69035,\"start\":69026},{\"end\":69045,\"start\":69035},{\"end\":69056,\"start\":69045},{\"end\":69067,\"start\":69056},{\"end\":69767,\"start\":69757},{\"end\":69780,\"start\":69767},{\"end\":69788,\"start\":69780},{\"end\":69799,\"start\":69788},{\"end\":69807,\"start\":69799},{\"end\":69816,\"start\":69807},{\"end\":69826,\"start\":69816},{\"end\":69834,\"start\":69826},{\"end\":69842,\"start\":69834},{\"end\":69854,\"start\":69842},{\"end\":69864,\"start\":69854},{\"end\":69869,\"start\":69864},{\"end\":70213,\"start\":70207},{\"end\":70219,\"start\":70213},{\"end\":70230,\"start\":70219},{\"end\":71201,\"start\":71192},{\"end\":71212,\"start\":71201},{\"end\":71222,\"start\":71212},{\"end\":71233,\"start\":71222},{\"end\":71241,\"start\":71233},{\"end\":71252,\"start\":71241},{\"end\":71261,\"start\":71252},{\"end\":71274,\"start\":71261},{\"end\":71482,\"start\":71467},{\"end\":71490,\"start\":71482},{\"end\":71496,\"start\":71490},{\"end\":71506,\"start\":71496},{\"end\":71518,\"start\":71506},{\"end\":71524,\"start\":71518},{\"end\":71533,\"start\":71524},{\"end\":71545,\"start\":71533},{\"end\":71555,\"start\":71545},{\"end\":71568,\"start\":71555},{\"end\":71929,\"start\":71919},{\"end\":71938,\"start\":71929},{\"end\":71946,\"start\":71938},{\"end\":71956,\"start\":71946},{\"end\":71967,\"start\":71956},{\"end\":71979,\"start\":71967},{\"end\":72000,\"start\":71979},{\"end\":72328,\"start\":72320},{\"end\":72337,\"start\":72328},{\"end\":72349,\"start\":72337},{\"end\":72357,\"start\":72349},{\"end\":72937,\"start\":72926},{\"end\":72949,\"start\":72937},{\"end\":72966,\"start\":72949},{\"end\":72977,\"start\":72966},{\"end\":72986,\"start\":72977},{\"end\":73389,\"start\":73380},{\"end\":73402,\"start\":73389},{\"end\":73413,\"start\":73402},{\"end\":73428,\"start\":73413},{\"end\":73756,\"start\":73745},{\"end\":73774,\"start\":73756},{\"end\":74218,\"start\":74208},{\"end\":74428,\"start\":74420},{\"end\":74437,\"start\":74428},{\"end\":74449,\"start\":74437},{\"end\":74724,\"start\":74717},{\"end\":74734,\"start\":74724},{\"end\":74743,\"start\":74734},{\"end\":74749,\"start\":74743},{\"end\":75406,\"start\":75399},{\"end\":75412,\"start\":75406},{\"end\":75421,\"start\":75412},{\"end\":75431,\"start\":75421}]", "bib_venue": "[{\"end\":58076,\"start\":58006},{\"end\":59040,\"start\":58944},{\"end\":61691,\"start\":61614},{\"end\":62469,\"start\":62364},{\"end\":63575,\"start\":63475},{\"end\":64308,\"start\":64188},{\"end\":66701,\"start\":66684},{\"end\":68230,\"start\":68163},{\"end\":69286,\"start\":69187},{\"end\":70567,\"start\":70407},{\"end\":74934,\"start\":74854},{\"end\":75690,\"start\":75594},{\"end\":56850,\"start\":56772},{\"end\":57520,\"start\":57461},{\"end\":58004,\"start\":57919},{\"end\":58404,\"start\":58326},{\"end\":58942,\"start\":58848},{\"end\":59531,\"start\":59481},{\"end\":60086,\"start\":60030},{\"end\":60612,\"start\":60578},{\"end\":60881,\"start\":60813},{\"end\":61158,\"start\":61111},{\"end\":61612,\"start\":61523},{\"end\":62362,\"start\":62261},{\"end\":62900,\"start\":62862},{\"end\":63118,\"start\":63064},{\"end\":63473,\"start\":63375},{\"end\":64131,\"start\":64018},{\"end\":64831,\"start\":64771},{\"end\":65526,\"start\":65522},{\"end\":65881,\"start\":65830},{\"end\":66204,\"start\":66141},{\"end\":66682,\"start\":66594},{\"end\":67285,\"start\":67237},{\"end\":67512,\"start\":67466},{\"end\":68161,\"start\":68079},{\"end\":68619,\"start\":68588},{\"end\":69161,\"start\":69071},{\"end\":69921,\"start\":69869},{\"end\":70405,\"start\":70230},{\"end\":71190,\"start\":71159},{\"end\":71608,\"start\":71568},{\"end\":71917,\"start\":71861},{\"end\":72318,\"start\":72241},{\"end\":73031,\"start\":73011},{\"end\":73477,\"start\":73428},{\"end\":73886,\"start\":73801},{\"end\":74235,\"start\":74218},{\"end\":74418,\"start\":74337},{\"end\":74817,\"start\":74749},{\"end\":75592,\"start\":75498}]"}}}, "year": 2023, "month": 12, "day": 17}