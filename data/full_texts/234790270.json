{"id": 234790270, "updated": "2023-10-06 02:51:19.117", "metadata": {"title": "Face, Body, Voice: Video Person-Clustering with Multiple Modalities", "authors": "[{\"first\":\"Andrew\",\"last\":\"Brown\",\"middle\":[]},{\"first\":\"Vicky\",\"last\":\"Kalogeiton\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Zisserman\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "journal": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "publication_date": {"year": 2021, "month": 5, "day": 20}, "abstract": "The objective of this work is person-clustering in videos -- grouping characters according to their identity. Previous methods focus on the narrower task of face-clustering, and for the most part ignore other cues such as the person's voice, their overall appearance (hair, clothes, posture), and the editing structure of the videos. Similarly, most current datasets evaluate only the task of face-clustering, rather than person-clustering. This limits their applicability to downstream applications such as story understanding which require person-level, rather than only face-level, reasoning. In this paper we make contributions to address both these deficiencies: first, we introduce a Multi-Modal High-Precision Clustering algorithm for person-clustering in videos using cues from several modalities (face, body, and voice). Second, we introduce a Video Person-Clustering dataset, for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features. The dataset is by far the largest of its kind, and covers films and TV-shows representing a wide range of demographics. Finally, we show the effectiveness of using multiple modalities for person-clustering, explore the use of this new broad task for story understanding through character co-occurrences, and achieve a new state of the art on all available datasets for face and person-clustering.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.09939", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/BrownKZ21", "doi": "10.1109/iccvw54120.2021.00357"}}, "content": {"source": {"pdf_hash": "497ab53abff70ddbb3d1e0622791d763d030e29f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.09939v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2105.09939", "status": "GREEN"}}, "grobid": {"id": "2a521466cc40fc3ead4fd7c9eb21611a317c22a9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/497ab53abff70ddbb3d1e0622791d763d030e29f.txt", "contents": "\nFace, Body, Voice: Video Person-Clustering with Multiple Modalities\n\n\nAndrew Brown \nDept. of Engineering Science\nVGG\nUniversity of Oxford\n\n\nVicky Kalogeiton vicky.kalogeiton@lix.polytechnique.fr \nDept. of Engineering Science\nVGG\nUniversity of Oxford\n\n\nLIX\n\u00c9cole Polytechnique\nCNRS\nParisIP\n\nAndrew Zisserman \nDept. of Engineering Science\nVGG\nUniversity of Oxford\n\n\nFace, Body, Voice: Video Person-Clustering with Multiple Modalities\n\nThe objective of this work is person-clustering in videos -grouping characters according to their identity. Previous methods focus on the narrower task of face-clustering, and for the most part ignore other cues such as the person's voice, their overall appearance (hair, clothes, posture), and the editing structure of the videos. Similarly, most current datasets evaluate only the task of face-clustering, rather than person-clustering. This limits their applicability to downstream applications such as story understanding which require person-level, rather than only face-level, reasoning.In this paper we make contributions to address both these deficiencies: first, we introduce a Multi-Modal High-Precision Clustering algorithm for person-clustering in videos using cues from several modalities (face, body, and voice). Second, we introduce a Video Person-Clustering dataset, for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, facetracks when visible, and voice-tracks when speaking, with their associated features. The dataset is by far the largest of its kind, and covers films and TV-shows representing a wide range of demographics. Finally, we show the effectiveness of using multiple modalities for person-clustering, explore the use of this new broad task for story understanding through character co-occurrences, and achieve a new state of the art on all available datasets for face and person-clustering.\n\nIntroduction\n\nClustering people by identity in videos is an appealing and much-visited topic in computer vision [13,20,34,36,69,70,78]. It has several real-world applications, such as enabling person-specific browsing, organisation of video collections, character based fast-forwards, automatic cast listing; and story understanding, all without requiring any explicit identity labeling. A successful person-clustering framework can therefore alleviate the tremendous annotation cost that is otherwise necessary for such applications.\n\nHowever, methods for clustering by identity are almost Figure 1: Video Person-Clustering -an essential step towards story understanding. Imagine trying to understand the story in the scenes above, given only the non-greyed-out parts. Face-level understanding (left) omits important information, such as characters with their backs turned. This work addresses the new task of video person-clustering, which develops person-level understanding (right) in a scene by clustering all people, regardless of if their faces are showing or not. This is in contrast to the more limited, established, task of face-clustering. Person-level understanding is essential for downstream applications of grouping-by-identity such as story understanding, and cannot be achieved by face-clustering alone.\n\nalways limited to only using information from faces. Such methods have two significant drawbacks: First, they ignore many available, informative cues that a human would use to solve the task: (i) the person's voice available from the audio track; (ii) the person's overall appearance (from their hair, clothes, posture); and, (iii) the editing structure (in edited material) -such as the co-occurrence of characters in nearby shots and within a scene. Second, they limit the utility of clustering for downstream applications such as story understanding. Understanding the story-line in a scene requires knowledge of all the characters present in a scene, not just those whose faces are visible, i.e. person-level not face-level reasoning. This is illustrated in Figure 1. Our objective in this paper is to cluster people (or more precisely person-tracks, which depict an entire body in any pose) by identity in movies and TV-material, as a first step towards story-level understanding. We cluster people, rather than just faces, and use all cues (face, voice, body appearance, editing structure), including tracks of people from behind without a visible face.\n\nTo see the value and necessity of this multi-modal approach, consider the problem of determining if two poor resolution faces depict the same person or not -the voice can discriminatively resolve this ambiguity. Similarly, consider the problem of determining if a person seen speaking to camera in one shot, is the same as the person seen from behind in a following shot -the hair and clothes can provide the link. In Figure 1, for example, how would the people seen from behind be identified other than by clustering their hair, clothes or voice with instances in neighboring shots?\n\nMore generally, modalities arising from the same person are both redundant and complementary, and can be used to address two fundamental problems in clustering: how to obtain pure clusters (i.e. containing tracks from a single person); and, how to merge clusters without violating their purity (i.e. by contaminating them with tracks from another person). They can be used to obtain very pure clusters by requiring agreement (e.g. on both face and voice) in order for tracks to be grouped together; and can be used to merge clusters which could not otherwise be confidently merged with a single modality, e.g. by using the common voice to merge a frontal with a profile face cluster (where the face descriptors of each cluster may be different). In this way, multiple modalities provide a bridge between otherwise unmergeable clusters. Methods that merge clusters using a single modality inevitably sacrifice purity.\n\nIn this paper, we introduce a new method for the task of video person-clustering, Multi-Modal High-Precision Clustering (MuHPC), that uses multiple modalities -face, voice, and body appearance. It builds on recent methods that use first nearest neighbour [33,36,59] clustering algorithms, and is designed to take advantage of the redundancy and complementarity of the modalities, as discussed above, and to incorporate lessons from the face-clustering literature, such as cannot-link constraints and using the video editing structure [4,13] (Section 3).\n\nTo evaluate the multi-modal person-clustering task, we require a dataset with person-level annotations. However, there are very few such datasets due to the previous emphasis on face-clustering and moreover, most face-clustering and labelling datasets, such as Buffy [18] and TBBT [58], are based on TV material with limited diversity in skin color. For these reasons, we introduce a new Video Person-Clustering Dataset (VPCD) where we: (i) re-purpose multiple existing face datasets by adding person-level multimodal annotations (e.g. all person-tracks and voice utterances); and (ii) include different TV shows and films (hereby referred to under the unified term program sets) to address this lack of diversity. VPCD consists of visually disparate program sets, and includes body-tracks, face-tracks when visible; and voice utterances when speaking, for all annotated characters. We provide features so that future cluster-ing algorithms can be compared easily and fairly (Section 4).\n\nWe show the effectiveness of multi-modality and outperform strong baselines for person-clustering on VPCD (Section 5.1), and explore this new expansive task for story understanding (Section 5.4). Our method also significantly outperforms the face-clustering state of the art on both TBBT and Buffy by over 10% NMI (Section 5.3). Note that our goal is multi-modal clustering and not representation learning. Thus, we do not propose a new architecture or train a network for better features. Instead, we use features from pre-trained networks (for face and speaker recognition) and only train a network where it is necessary for body Re-ID. A broader impact statement is included in the appendix. VPCD is publicly released and can be found here [72].\n\n\nRelated Work\n\nIn this work, we focus on multi-modal person-clustering in videos. Similar works target the more limited task of faceclustering or labelling, person Re-ID, or person search. We describe them, and also discuss similar datasets to VPCD.\n\n\nFace-Clustering.\n\nA well studied task for both images [5,28,49,57] and videos [36,69,70,83], with difficulty arising from the variation of pose, lighting, and emotion [27,41] in faces of the same identity. Most video approaches exploit the spatio-temporal continuity and find must-link and cannot-link clustering constraints [4,13,16,36,60,62,70,74,78,80], or additional constraints from the structure of videos [70]. Most works approach face-clustering with metric or representation learning [13,20,60,61,62,69,77,78]. For instance, [69] map features from the same identity to a fixed-radius sphere, while Sharma et al. use supervision from video constraints [60,61] or weak clustering labels [62]. These methods, however, are limited by the relatively small training sets available from particular TV-shows. For this reason, some recent approaches focus on simply clustering pre-trained features that have been learnt on very large-scale face datasets. Sarfraz et al. [59] propose a simple first nearest neighbour clustering method upon pre-trained features, FINCH, and show impressive results. More recently, [36] combines [59] with spatio-temporal constraints and improves performance. All the above works focus on the limited task of clustering faces (Figure 1 -left), whereas our focus is multi-modal person-clustering i.e. clustering every appearance of characters, regardless of whether their face is visible (Figure 1 right), and using multiple modalities. Face-Labelling. The task of classifying faces by identity -most works address this by using face-appearance with supervision from transcripts aligned to subtitles [4,6,15,18,19,51,54,64,67], for example by using Multiple Instance Learning [6,24,37,76]. Some exploit cues other than faces from videos: [53] use clothing to match faces in TV-shows across shot boundaries, while [7,48] use face and voice to label faces. [52] use face and voice to retrieve a list of shots containing a named person, by searching for their name in subtitles and displayed text. These works focus only on visible faces and although some are multi-modal (face, voice and/or text supervision), the text is typically obtained from external sources (i.e. transcripts). Our task is different, as we cluster rather than label, and thus do not require characterclassifiers or ID supervision or extra annotation, and we use all available cues i.e. editing structure and multi-modality. Person Re-ID. The task of re-identifying pedestrians in CCTV -typically [40,75,84,85], each body is fully visible and walking, the clothing remains constant for each identity, and the images are low resolution. This differs substantially from person-clustering in TV and film material, where there is large pose variation (e.g. sitting, standing, lying down), occlusion, and the clothing frequently changes for each identity. A full literature review is out of scope. Closer to our task are works on person-retrieval in photo albums [35,65,82] or person-search from portraits in videos [31,79]. [31,65] use face and body features, while [79] use audio. The TRECVID Instance Search challenges [2] involved retrieving a list of shots that contain an identity, given a query video for that identify. In contrast, we cluster all characters at the tracklevel in videos without requiring search queries. Related Datasets. Various face-clustering datasets have been proposed [14,18,21,36,50,58]. These follow some similar trends: (a) are limited in size, consisting of a movie or some TV show episodes; (b) under-represent most demographic groups; and (c) contain only face annotations, so cannot be used for the broader multi-modal person-clustering task. Several story understanding [3,32] or person-search [31] datasets with face and/or body annotations exist. These cannot be used for our task, as they lack audio [31,32] or contain only partial annotations such as keyframes [32] or for a subset of tracks [3]. Furthermore none contain labelled voice utterances. Instead, VPCD contains 6 different TV-shows and movies, representing a diverse range of characters, and containing multi-modal annotations for all annotated characters. Story Understanding. This targets automatic understanding of human-centred story-lines in videos. It has been formulated in several ways, e.g. grouping scenes by story threads [17,55], learning character interactions [44,68] or relationships [39], creating movie graphs [73]; or textto-video retrieval from narrating captions [3], with several datasets [3,32] introduced. Many works [3,39,73] highlight the importance of knowing who is present in a scene for understanding the story. This is the focus of our work.\n\n\nMethod\n\nHere, we describe the Multi-Modal High-Precision Clustering (MuHPC) method for person-clustering in videos. It is a single hierarchical agglomerative clustering [56] (HAC) approach that groups person-tracks by identity using similarities of modality features, together with constraints arising from the video structure. MuHPC uses pre-computed features, and hence does not require any training outside of simply learning optimal hyper-parameters, and can then run out of the box for any video dataset. In this work, we use three modalities (face, voice, and body appearance) but MuHPC can easily scale to any number of modalities. Overview. MuHPC consists of three stages. Stage 1 creates high-precision clusters using a single modality, here the face. We group person-tracks that share a first nearest neighbour (NN) using multiple iterations of HAC (as in [33,36,59]). We follow this trend subject to two additional constraints: a cannot-link constraint for concurrent tracks (as in [36] based on [4,13]), and a conservative threshold on the maximum NN distance. This results in 1 clusters (Section 3.1). Stage 2 exploits multi-modality to bridge clusters that were otherwise unmergeable by the single face modality with a conservative threshold; in particular, by requiring that different modalities (i.e. face and voice) concur on the merge (Section 3.2). Stage 3 clusters tracks without visible faces, and hence that are not yet clustered by the first two stages. Constraints from the editing structure (neighboring shots) and a conservative threshold on body features (so that they depict the same person with the same clothing) are used to link face-less person-tracks to clusters with faces (Section 3.3). Here, we describe the stages, algorithm design choices, and how the hyper-parameters are learnt. The method is visualised in Figure 2. Notation. Given a dataset with person-tracks and characters, where is a single person-track, the goal is to cluster all by identity into clusters ( is unknown). Each persontrack is represented by one feature vector per available modality, i.e. ={ , , }, with , , the face, voice and body-track features, respectively. The availability of each feature vector is dependant upon the part of the person that is visible (face and/or body), and if they are speaking. For each person, at least one of , are available. Let ( , ) be the distance between two track features of the same modality, and , and the distances between two face, voice or body-tracks, respectively; the lower the value, the more likely the tracks depict the same identity. NN is nearest neighbor; 1 is the first NN track of track . The set of video frames that is present in is denoted by .\n\n\nStage 1: High-Precision Clustering\n\nStage 1 creates high-precision clusters, each containing tracks of the same identity. It uses only the face modality as this is the most discriminant of the three (face, voice and body), and thus is least likely to group different identities in the same cluster. Here, we use a NN clustering method [36,59], subject to two clustering constraints. Clustering Constraints. A NN is only considered valid if the resulting merge satisfies: (1) A Spatio-Temporal Cannotlink Constraint: Tracks that have (partial) temporal overlap cannot be grouped together, since they must represent different characters as they appear together in at least in one frame (introduced by [36]); and (2) A NN Distance Constraint: the distance ( , 1 ) between a track and its first NN 1 is less than a strict threshold tight for Stage 1.\n\nClustering process. At every iteration (cluster partition \u0393), each cluster is grouped with its NN cluster, i.e. the closest. Specifically, the first partition groups tracks into clusters through first NN relations, while following partitions group the clusters formed in the previous partition; each cluster is represented by the average of the features it contains. Following the notation of [59], at each partition \u0393, the method forms \u0393 clusters by merging tracks that are either first NN (mutually or one is the first NN of the other) or have a common NN 1 , as described by the adjacency matrix:\n( , ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 if = 1 or 1 = or 1 = 1 and \u2229 = \u2205, ( , 1 ) \u2264 tight 0 otherwise.(1)\nDiscussion. In standard HAC the clustering continues until all clusters merge to one. Including the constraints introduces strict stopping criteria, and therefore the clustering stops when either the clusters are all more than a distance tight apart, or they are separated by a cannot-link constraint.\n\nThis results in 1 high-precision clusters, where we expect 1 \u2265 . The very simple addition of a distance threshold leads to a significant improvement in clustering results over [36,59,69] (Section 5.3). Without this constraint, little prevents an incorrect merging of clusters of different identities and the subsequent creation of low-precision clusters.\n\n\nStage 2: Multi-modal Cluster Bridging\n\nCombining a discriminative modality with the constraints results in high-precision clusters. However, a single modality alone cannot continue making confident merges without sacrificing purity. Thus, Stage 2 merges these clusters by exploiting multiple modalities i.e. face and voice. Modality-pair merges. To further merge clusters, we demand that two modalities agree that the clusters contain the same identity. Therefore, we require that the distances for the face and voice are both below new thresholds, i.e. < loose and < loose . Note, here we use features taken from tracks within clusters, rather than averaged cluster features. tight is raised by just a small margin, , i.e. loose = tight + , due to the concurrent agreement from the voice. Discussion. This stage results in 2 clusters with highprecision, where 2 \u2264 1 . Here, we use face and voice as they have been shown to be coupled [46,47] and to contain redundant, identity discriminating information. An alternative is to require that the voice modality alone provides a confident (i.e. tight threshold) match, e.g. two person-tracks with the same voice. We find however that voice alone cannot reliably join clusters of the same identity. This can be because two identities with the same emotion in their voice (e.g. shouting, crying) can appear similar to the less discriminative voice embedding (more in the appendix).\n\n\nStage 3: Clustering backs\n\nStages 1 and 2 result in high-precision person clusters. Nevertheless, they do not account for person-tracks with no visible face, for instance when viewed from behind, i.e. a face-less person. The goal of Stage 3 is to add the face-less person-tracks into their respective high-precision clusters using the modality of body-appearance. Here, we use the editing structure of the videos, given that the appearance of the same character can change dramatically between scenes. As discussed above, body features may not be discriminative for identity if characters are wearing very similar clothing. We determine such body-tracks using the simple ratio-test introduced in [42]. Specifically, for each body-track we compute the first and second NN distances, 1 , and 2 , . If the ratio, 2 , / 1 , is higher than a threshold then the body-track is classified as non-distinctive and is ignored.\n\nIn detail, for assigning face-less people to clusters, we find the NN body-track (that has a face and therefore is already clustered) that does not violate the ratio-test in a neighbouring shot, and assign the face-less person to this cluster. Given that the same person is most likely wearing the same outfit in the same or neighbouring shots, we only examine the distance between body-tracks from these shots. At this stage, some backs cannot be clustered with high confidence, either because they are not similar to any nearby body or because they themselves fail the ratio test for being a non-distinctive feature. Our design choice is to ignore these backs. In detail, we ignore any back for which the NN distance is more than a threshold back . Note, this stage does not change the number of clusters from 2 . Required Number of Clusters. Suppose we know the number of characters , and hence the number of clusters. Our goal is to reduce 2 to the desired (typically 2 \u2265 ). Previous methods [69] employ HAC; however, this suffers from reliance on features that can no longer confidently discriminate between clusters of the same person. Instead, we employ a cluster prior: there is no identity overlap amongst the largest clusters i.e. they contain unique identities, and conversely there is likely an identity overlap between a small and large cluster. Our intuition is that big clusters contain ample information about an identity, and consequently if two large clusters contained the same identity, then they would have been merged. Therefore, we iteratively merge the smallest with the largest cluster until there are clusters. In practice, we observe that small clusters contain blurry or low-resolution tracks, and so could not confidently be merged at earlier stages. Discussion. Most methods [60,62,69] fine-tune character features on a video dataset. Instead, MuHPC operates on pre-trained features, thus reducing the computational burden and leading to increased generalisation capabilities. An extension would be to replace the constraints with a cost function optimisation approach, allowing a cannot-link to be correctly broken for a person's reflection in a mirror.\n\n\nLearning Hyper-Parameters\n\nThe hyper-parameters for MuHPC are learnt on the validation partition of VPCD. The visually disparate program sets in the test partition are disjoint from those in the validation, yet these parameters are kept constant. For the hyper-parameter associated with the face modality ( loose ) this is possible as the face features are trained on millions of faces [10], and therefore are highly discriminative and universal (generalise well across different program sets). However, voice identity features are less universal than face features, and hence there is not a single good choice for loose that would generalise across the audibly disparate program sets. Instead, we learn a unique value automatically for each. Our goal is to choose loose to be lower than the minimum distance between voices from different people. The cannot-link constraints automatically provide face-track pairs of different identities. We measure the distances between different people's voices. In practice, there are too few constraints between speaking faces to provide an accurate representation of the negative distances, as rarely two face-tracks speak in the same shot. We combine the cannot-link speaking facetracks with clusters from Stage 1 to provide more examples. This leads to many negative distances and an accurate representation of their distribution. We select loose as the lower  99.9 percentile of these distances. This provides a robust automatic threshold measure. For program sets with similar sounding characters, this process gives a low loose (e.g. Buffy -many similar sounding teenagers).\n\n\nVideo Person-Clustering Dataset\n\nIn this section, we describe the dataset (Section 4.1), the annotation (Section 4.2), and the feature extraction processes (Section 4.3). The dataset is built on top of existing video datasets that have face-level annotations (labeled face-tracks) by adding and annotating body-tracks, and annotating voice utterances. This is for three reasons: first, it enriches the existing dataset by raising them to have personlevel annotations; second, it enables comparisons on facelevel clustering with prior work on these datasets; and third, it means that the video material is already publicly available and we need only release the new annotations (and features).\n\n\nVPCD content\n\nVPCD contains full multi-modal annotations for primary and secondary characters for a range of diverse and visually disparate TV-shows and movies (statistics in Table 1, examples in Figure 3). VPCD contains annotations for 39,777 body-tracks, 35,396 face-tracks for whenever the face is visible, and 9,165 manually annotated voice-tracks for whenever each of them are speaking. Identity discriminating features (embeddings from deep networks) are provided for all modalities. A total of 23 hours of video cover a range of genres and styles such as Hollywood Drama (Hidden Figures,  2016), Romance (About Last Night, 2014), fast-paced Action/Mystery (Sherlock, Buffy) and live studio-audience sitcoms (Friends, TBBT). A large variety of characters are annotated, ranging from small casts shown over many episodes (e.g. Friends) to program sets with a long-tailed distribution with many secondary/background characters (e.g. Buffy). VPCD is by far the largest dataset of its kind. The program sets were chosen such that VPCD is representative of the diversity of people's appearance in the real world. There is a val. set and a test set -these are disjoint. The val. set is the first five episodes of Friends. Each face-body pair is displayed with a unique color. A diverse and representative range of characters are captured in a variety of scenes (e.g. dark (d)), viewpoints (e.g. (e)); and poses, including backs of bodies (magenta, cyan). When speaking, we also include a voice-track (blue signal below body-tracks).\n\n\nAnnotation Process\n\nHere, we describe the annotation process for the face, body, and voice tracks in VPCD. For all component program sets, the face annotations already exist, and define the characters of interest for that video. Our goal is to annotate their body and voice-tracks. Very often in videos, a character is seen facing from behind ( Figure 3). This means that the existing face-tracks cannot be used to trivially annotate the body-tracks by spatial overlap (since there will be no face-track). We therefore combine automatic and manual annotation methods (more details in the appendix). Face. We use the same face bounding-box/track annotations and ID labels as were provided with the original datasets so that we can compare to previous works on face-clustering. Body. We detect bodies with a Cascade R-CNN [9] trained on MovieNet [32] and form tracks with an IOU tracker. When a body-track clearly corresponds to a face-track (i.e. no significant IOU with any other face-track), the body-track is automatically annotated with the character name of that face-track. We manually annotate the remainder as well as the body-tracks corresponding to characters from behind. Voice. We manually segment the audio-track into the speaking parts for all annotated characters. To ensure the correctness of the segmentation, the audio track was first segmented by one human annotator, and then verified by different ones.\n\n\nFeature Extraction\n\nFace. We use L2-normalised 256D features, extracted from an SENet-50 [30] pre-trained on MS-Celeb-1M [23], and fine-tuned on VGGFace2 [10] (same as [18,36,48,58]). Body. For all body detections, we extract 256D features with ResNet50 [25] trained on CSM [31]. We average the features across each body-track, and then L2-normalise them. Voice. Following [11], we extract a single, L2-normalised 512D speaker embedding from each voice segment using a thin-ResNet-34 [25,81] trained on VoxCeleb2 [12].\n\n\nExperiments\n\nHere, we evaluate MuHPC. We first give experimental details, followed by person-clustering results on VPCD and provide ablations. We compare to previous faceclustering works and finally examine the advantages of person-clustering for story understanding.   Table 2: Person-Clustering Results on VPCD. For each program set, each metric is averaged across all episodes. AT protocol. The 'Average' column reports averaged metrics across all six program sets. # is the sum of ground truth clusters across each episode in each program set. We report two strong baselines (B-ReID, B-C1C, Section 5.1) and an ablation on the modalities used. Keys: F-face, B-body, V-voice. Modality: used modalities.\n\n\nPerson-Clustering\n\nBaselines. To evaluate person-clustering, we compare to two strong baselines stemming from the best existing faceclustering algorithm, C1C [36]. The first, B-ReID, is inspired by person Re-ID [40,84,85] and uses C1C to cluster body rather than face features. It ignores person-tracks without bodies (<2% of person-tracks). For the second, B-C1C, we use regular C1C to cluster faces, with the addition of Stage 3 of MuHPC for clustering face-less bodies. Results and analysis. Table 6 reports person-clustering results when testing on VPCD. For all metrics, MuHPC (full method) significantly outperforms the strongest baseline by on average 6.1% in WCP and 11.8% in NMI. B-ReID is poor due to frequent clothing changes. MuHPC outperforms B-C1C thanks to (1) the NN distance threshold that prevents incorrect merges and subsequent low-precision clusters, and (2) the multi-modal bridges that merge clusters which face alone cannot. This validates that using all available video cues, such as multi-modality and editing structure aids video person-clustering substantially. The clustering process for a character in VPCD is visualised qualitatively and quantitatively in Figure 2. MuHPC improves most upon the baselines on the more unconstrained program sets with many secondary characters and long-tailed character distributions (e.g. TBBT, Buffy, Friends, Sherlock). Here, MuHPC uses the NN distance threshold to keep the clusters of the many characters separated, and then merges any repeated clusters of main-characters via talking person-tracks. The MuHPC clustering process is visualised in Figure 4.\n\n\nAblation\n\nHere, we perform ablations on the different modalities in MuHPC. Detailed results and parameter sweeps can be found in the appendix. Table 6 includes an ablation of the multi-modality, i.e. using voice (MuHPC -Stage 2) or body (MuHPC -Stage 3) modalities or both (MuHPC). Experiments without the body modality do not use Stage 3, and instead cluster each face-less body to the temporallyclosest (Temporal-NN) body with a face in a nearby shot. Due to the threading structure [29] of edited videos, there is a strong prior that the Temporal-NN is correct.\n\nAdding either the voice or body offers a benefit over MuHPC-, due to the increased discriminative capabilities from an additional modality. MuHPC outperforms MuHPC , as there are many face-less bodies in VPCD, and the body modality allows for these to be clustered correctly. Using the voice in conjunction with the body (MuHPC) performs best, as their benefits are compounded, and the multimodal bridges connect clusters with higher purity. The voice gives a higher boost when used alongside the body modality, as otherwise the multi-modal bridges are merging lower precision clusters. The voice adds significant benefit in NMI on multiple program-sets. This is impressive as the tight voice thresholds were found automatically. Sometimes the voice does not lead to an improvement, due to the absence of speaking person-tracks in merge-able clusters (e.g. TBBT). Additionally, the body offers little improvement in the two movies (Hidden Figures, About Last Night) that have many dark scenes and non-distinctive clothing. Here temporal-NN is able to assign face-less bodies to clusters well. Note, NMI increases more than WCP when adding the voice modality, because bridging two high-precision clusters will not greatly effect the purity; however, it leads to increased NMI as there is less identity overlap between the resulting clusters.\n\n\nFace-Clustering\n\nHere, we compare to previous works by experimenting only on face-tracks, excluding person-tracks without faces. We compare to FINCH [59] (evaluated at the required number of clusters, from [36]), BCL [69] and C1C [36]. For TBBT and Buffy, the face annotations are the same as [36,69]. Here, we do not compare to works that use the less challenging [36] subset of the annotations [57,62]. For our method, we present: (i) MuHPC-uses only face-tracks, i.e. exactly the same information and features as other methods, hence results are directly comparable; and (ii) MuHPC uses face-tracks with multi-modal bridges (i.e. voice). Following [36,69], performance is evaluated at frame level. Table 7 reports face-clustering results. For both AT and OC protocols, MuHPC-significantly outperforms the state of the art in all metrics, as it avoids incorrect merges, hence      maintaining cluster purity. For instance, NMI, CP and CR boost by +10-14% for Buffy and TBBT for OC, and by over 10% for WCP averaged across all datasets for AT. MuHPC also leads to a boost over MuHPC-in most datasets. We observe that the more challenging the dataset, the higher the boosts by multi-modality, e.g. +3.8% in CR for Friends and +7.4% in NMI for Sherlock. We note that on NMI, WCP, the performance on TBBT is now almost saturated. A full discussion of results is given in the appendix.\n\n\nEnabling Story Understanding\n\nHere, we explore how close we have come to enabling story understanding. Clustering people (rather than faces) indicates who is present in a scene (Figure 1) -an essential and necessary step for predicting character co-occurrences, and hence their interactions [39,45] that make up a story. Specifically, we ask two questions: (1) Can clustering on the face-level predict the co-occurrence of two characters correctly? (2) How close is MuHPC to correctly predicting co-occurrences? To answer these, we experiment with the five main characters from the six episodes of TBBT in VPCD. Figure 10a shows the ground truth (Pers. Lev. GT) co-occurrence of character pairs as a proportion of all frames in the show, and hence is a measure of their interaction, e.g. Sheldon and Leonard co-occur for 24% of all frames.\n\nFor the first question, we visualise the co-occurrences of characters according to the face-track annotations (Face Lev. GT) in VPCD (shown in absolute terms in Figure 5c, and relative to the GT in Figure 5e). The face-level cooccurrences are poor -with an average error from GT of 48%. For instance, Penny and Leonard, whose romance is a main story-line, are shown to co-occur in only 3% of the videos vs the GT 9%. Furthermore, the GT shows that this is the second most commonly occurring pair; nevertheless, the face-level annotations fail to pick up that it is significant relative to other pairs. This is expected as often one or more characters do not show their face when appearing together ( Figure 1). Hence, any co-occurrences predicted from the face-level are a limited foundation for story understanding.\n\nFor the second question, we cluster with MuHPC and assign each cluster to the character that appears most within it (Figures 10b, 5d). We observe that these predictions are very close to the GT, with an average error of just 3% (Figure 5d). This impressive result shows that the presence of each character, their co-occurrence and hence their possible interactions can be found completely automatically and accurately using our proposed method. This provides a rich and informative foundation for story understanding.\n\n\nConclusions\n\nIn this work we propose MuHPC, a novel method for multi-modal person-clustering in videos. For evaluation we introduced VPCD, the largest and most diverse dataset of its kind. We showed that using all available video cues is essential for person-clustering, leading to significant improvements on VPCD, and to state-of-the-art performance for face-clustering. Importantly, we demonstrated that MuHPC allows each character appearance and co-occurrence to be predicted completely automatically and accurately. We hope this can support downstream story understanding tasks such as learning interactions and relationships [39]. \n\n\nAppendix\n\n\nA. Broader Impact\n\nVideo Person-Clustering is an appealing topic in Computer Vision, with many downstream applications such as story understanding, video navigation, and video organisation. A successful person-clustering framework (such as that presented in this work) takes a significant step towards realising these applications by alleviating the tremendous annotation cost that would otherwise be necessary.\n\nFor all potential impacts and applications of video personclustering, it is essential that the datasets that methods are evaluated on are representative of the real-world in which they (or their downstream applications) may be deployed [63]. This is essential if the research is to be accessible by different communities around the world. A representative dataset can accurately foreshadow and ultimately prevent any algorithmic discrimination on specific demographic groups. Previous person-clustering datasets (which focused on the narrower task of face-clustering) were nonrepresentative of most demographic groups. To this end, in this work we presented VPCD, which represents a wide and diverse range of characters, and so is more representative of the diversity in the real-world.\n\nThe person-clustering task aims at recognising and clustering identities. Re-identifying people in the real-world generally poses a threat to their privacy, and could carry risks if used inappropriately. In VPCD however, the identities are all actors playing the part of characters. This is not private data, and none of the videos have been obtained from social media or search engines. All videos in VPCD are in fact from public films and television material.\n\n\nB. VPCD Details\n\nHere, we give additional details on the annotation (Section B.1) and feature extraction (Section B.2) process for the body-tracks in VPCD. These sections are complementary to Sections 4.2 & 4.3 in the main manuscript. We then give further statistics and details of the voice-tracks in VPCD (Section B.3).\n\n\nB.1. Annotation Process\n\nHere, we provide additional details for the body-track annotation in VPCD. To set the scene, we have body-tracks computed for all program sets in VPCD. The task at this stage is to annotate the body-tracks with the names of the characters that are annotated in the face-tracks.\n\nThe body-tracks fall into two categories, which are annotated separately. (1) The body-track shows the person from the front and contains a visible, annotated face. For these cases we automatically label the body-tracks by making assignments to labelled face-tracks. Within each shot, the assignment is done using the Hungarian Algorithm [38] with a cost function of the spatial intersection over union (IOU) between face and body-tracks in the frames that they co-occur. If there are more body-tracks than facetracks, then a body-track can not be assigned, and vice-versa. In 95% of cases this association is trivial and the assignment proceeds automatically. Where multiple assignment costs for the same facetrack are below a threshold, indicating that the assignment was non-trivial, we instead make the assignments manually. (2) The body-track does not contain a visible face, i.e. the back is turned to the camera. We manually annotate all of these cases throughout each video. On average, 10-15% of body-tracks correspond to manually labelled bodies from behind.\n\n\nB.2. Feature Extraction\n\nHere, we describe in more detail the feature extraction process for the body-tracks.\n\nFeatures are extracted from each of the body-tracks using a ResNet50 architecture [26]. Our goal is to train the body features to discriminate identity based on the highly discriminative clothing that people are wearing. We train a ResNet50 on the CSM dataset [31], which contains identity-labelled body detections from movies. This dataset contains the same label for all body detections of each identity, regardless of their clothing. Instead, we decompose the samples for each class (identity) in CSM into subclasses containing images of the same identity in the same outfit. Our assumption is that if two detections occur close-by temporally within the same movie, then the person is likely to be wearing the same clothing. Each body detection is annotated with the shot that the detection is found in. We cluster the body detections in each class according to their temporal location, resulting in several subclasses for each identity, where they are wearing the same clothing. We train the model in a contrastive manner using the Smooth-AP loss from [8]. For the network to be variant to both identity and clothing, we sample positives from the same identity wearing the same outfit, and negatives from different identities.\n\n\nB.3. VPCD Voice-Track Statistics\n\nHere, we give further details and statistics for the voice-tracks in VPCD. In total, there are 27,163 voice-tracks in VPCD ( Table 4). This includes annotations for the 'laughter' track from the live studio audience in TBBT and Friends, and additionally laughter from each character in all program sets. Features, and the associated annotations for all of these voice-tracks are provided for future research use with VPCD. The distribution of lengths of these voice-tracks is shown in Figure 6. These figures for the number of voice-tracks are different to those provided in Table 1 in the main manuscript. MuHPC implements a pre-processing step on the voice-tracks, such that only the most identity-discriminating voice-tracks are used in the clustering process (explained in Section C).\n\n\nC. Implementation Details\n\nIn this section, we give details on a pre-processing step for MuHPC, which aims to remove voice-tracks that might not be identity-discriminating from the clustering process. Some of the voice-tracks in MuHPC are not used, due to overlap between multiple voice-tracks, or due to them being too short. Here, we explain this process, and provide statistics on how many voice-tracks are ignored at this stage (Table 4). First, the temporal overlap between multiple voice-tracks. Our goal here is to use the voice-track features as a discriminative signal for identity. If multiple voice-tracks from different identities have large temporal overlap, then the resulting features will be very similar, and they will not provide a good identity-discriminating signal. We choose to ignore any   , there is a strong positive correlation between the discriminative capabilities of voice-track features and the length of the voice-track. In order to maximise the discriminativeness of the voice-track features, we ignore those that are less than 1 second in length. Table 4 shows the total number of voice-track annotations in VPCD before (\"All Annotations\") and after these steps (\"Filtered\").\n\n\nD. Metrics\n\nAs mentioned in Section 5 in the main manuscript, for each dataset in VPCD, we use Weighted Cluster Purity (WCP) and Normalized Mutual Information (NMI). Furthermore, we introduce the metrics of Character Precision and Recall. Here, we describe in more detail the WCP and NMI metrics and give some motivation behind the proposed Character Precision and Recall (CP, CR).\n\nWeighted Clustering Purity (WCP). WCP weights the purity of a cluster by the number of samples belonging in it; to compute purity, each cluster containing elements is assigned to the class which is most frequent in the cluster. WCP is highest at 1 when within each cluster, all samples are from the same class. For a given clustering, C, with N total tracks in the video:  Table 5: Person-Clustering Results on VPCD after Stage 1 -Clustering only speaking person-tracks. We report the averaged metrics for both AT and OC protocol, averaged across all program sets. Every experiment shown is clustering only a subset of the person-tracks that contain all three modalities (face, body and voice) in order to isolate the clustering performance when each modality is used alone. The three reported methods, MuHPC , MuHPC , MuHPC , use a different modality as the single modality in Stage 1 (body, voice and face, respectively). The numbers reported are taken after Stage 1. \n= 1 \u2208 \u00b7 .\n\nNormalized Mutual Information (NMI) [43]. NMI mea\n\n\nCharacter Precision and Recall (CP, CR). We introduce\n\nCharacter Precision (CP) and Recall (CR), two metrics computed using the ground truth number of clusters. CP is the proportion of tracks in a cluster that belong to its assigned character, while CR is the proportion of that character's total tracks that appear in the cluster. The assignment is done using the Hungarian algorithm [38] by using CR as the cost function. Note that this assignment is unique, i.e. two characters cannot be assigned to the same cluster. We measure CP and CR and report results averaged across all characters. Our motivation is that the standard metrics are weighted according the number of samples in each cluster, thus disproportionately favouring frequently appearing characters and disregarding tail distributions. Instead, similar to character AP [48], CP and CR weight all characters equally. Similar to the Hungarian matching accuracy used in [1,71], CP and CR are computed using the ground truth number of clusters. Thus, they measure complementary information to WCP and NMI, which do not have access to this information.\n\n\nE. Modality Analysis\n\nIn this section, we provide further analysis into the discriminative capabilities of each of the three modalities used in MuHPC (face, body and voice). In Stage 1 of MuHPC, high-precision clusters are created using just the face modality, as it is the most discriminative of the three. Here, we justify this by instead using the other modalities in Stage 1. Table 5 shows results averaged across all program sets in VPCD for both AT and OC protocol, when each of the available modalities are used in Stage 1 (termed MuHPC , MuHPC ; and MuHPC ). Next, we explain some experimental details, and then analyse these results.   these experiments we learn nearest neighbour distance thresholds for each modality on the VPCD val. set. As shown in Table 5, only the face modality can be reliably used in Stage 1 to produce high-precision clusters, as reflected by the high values for WCP in both protocol. This justifies the use of the face modality in Stage 1 of MuHPC. This is understandable, as different identities can sound the same when expressing similar emotions (e.g. anger, sadness), and bodies from different identities can look very similar when wearing similar clothing. According to WCP and NMI, MuHPC produces better clustering performance than MuHPC , indicating that the voice modality is better at discriminating identity than the body modality.\n\n\nF. Person-Clustering Results\n\nIn this section, we provide extensive analysis of the personclustering results obtained by MuHPC as well as results for an additional experiment. First, we explore the impact of Stages 1 and 2 of MuHPC on some episodes from the Friends program set in VPCD (Section F.1). Second, we provide further person-clustering results from MuHPC on VPCD using the OC protocol. Third, we examine the results when clustering tracks from all program sets in VPCD, concatenated by their research order of broadcast (Section F.3).\n\n\nF.1. Per-Stage Analysis\n\nWe examine the effects of Stages 1 and 2 (Section 3 in the main manuscript) on the performance of MuHPC on episodes from the Friends program set in VPCD. To this end, we plot in Figure 7 the %WCP and %NMI results over the number of clusters after each partition of the method for four episodes. Each circle in the plot displays the partition (i.e. showing the number of clusters of the resulting partition and the corresponding metric value). The blue lines and circles represent the clustering process at Stage 1 of MuHPC, while the orange ones display the Stage 2 results.\n\nWe observe that in most cases after the first partition (first blue dot) the WCP maintains high values (above 99%). While Stage 1 progresses, the WCP drops only by a small margin (i.e. less than 1% in most cases), whereas the NMI increases significantly (i.e. up to +50%). This validates that Stage 1 indeed results in high-precision clusters, as the purity (indicated by WCP) is not compromised, and also the NMI increases.\n\nThe orange dots signify the additional partition from Stage 2. Stage 2 consistently and significantly increases the NMI of the resulting clusters (i.e. by up to 5%), without sacrificing their purity (WCP remains constant). This indicates that Stage 2 bridges highprecision clusters of the same identity, thus retaining the high WCP, while decreasing the identity overlap between clusters.    The total predicted number of clusters (cluster pred), measured against the ground truth number of clusters (cluster GT). Note that \"cluster GT\" is different to # in the main manuscript. # is the summed number of ground truth clusters (number of characters) across multiple episodes. For example, episodes 1 and 2 of Sherlock have 13 and 22 ground truth clusters, respectively. In this case, # = 35. However, some characters appear in both episodes, such as \"John\" or \"Sherlock\". Instead, \"cluster GT\" is the total number of unique ground truth characters and therefore clusters across multiple episodes. For the same example of episodes 1 and 2 of Sherlock, \"cluster GT\" is equal to 31, as 4 characters feature in both episodes. Table 6 gives person-clustering results for the OC protocol. The experiments, ablation studies and baselines are the same as those used for the AT protocol, and explained in Section 5.1 of the main manuscript. Similarly to the AT protocol, MuHPC-significantly outperforms both baselines across all metrics and program sets. MuHPC gives a further boost when averaged across all program sets. The voice modality provides comparably less of a performance boost in the OC protocol (here) relative to the AT protocol ( Table 2 in the main manuscript). This is due to the Oracle Cluster protocol (OC), which forces the clusters to merge beyond the automatic termination point until the ground truth number of clusters is reached. Next, we explain this in further detail.\n# Modality # = # = # = # = # = # = # =\n\nF.2. Oracle Clusters Results\n\nMuHPC automatically stops clustering when the features within each cluster can no longer confidently be used to discriminate between clusters of the same identity. To reach the oracle number of clusters, the clusters are merged in a non-discriminative way. In this case, this reverses the positive impact of the voice modality (seen in Table 2 in the main manuscript) by merging the new clusters incorrectly until the oracle number of clusters is reached. This opens possibilities for future research into more effective ways of reducing to the ground truth number of clusters. The Automatic Termination protocol is the more realistic setting for real-world deployment of person-clustering algorithms on videos with unknown numbers of characters.\n\n\nF.3. Clustering on Multiple Program Sets Simultaneously\n\nIn this section, we present results for the person-clustering task when clustering tracks from multiple program sets simultaneously. In the main manuscript, all experiments are conducted on individual program sets from VPCD. Here, we cluster tracks from multiple program sets at the same time. In detail we incrementally consider additional episodes and movies from each of the program sets. Results for the WCP, NMI and the number of predicted clusters against the ground truth number of characters for the AT protocol for person-clustering are shown in Figure 8. The order with which program sets are added to the clustering experiment is in line with the timing of their first use in Computer Vision research (i.e. first Buffy [18], followed by TBBT [58], then Sherlock [48] and so on). Episodes within each of the TV-shows are added chronologically (starting with the first episode in the program set).\n\nImpressively, Figure 8 shows that when clustering all tracks from VPCD simultaneously, the WCP and NMI remain high at 80.6% and 79.3%, respectively. This indicates that most clusters have high purity, even with 323 different characters and over 30,000 tracks, over the visually disparate TV-shows and movies. As expected, these metrics drop as the total number of tracks increases, as the task becomes much more difficult. Until the introduction of tracks from episodes from Friends (14,642 tracks), the predicted number of clusters lies very close to the ground truth number of clusters. This indicates that VPCD is accurately predicting the number of different characters in the tracks. As the total number of tracks increases, the predicted number of clusters diverges from the ground truth number, and MuHPC predicts more clusters than there are characters. This is in line with and partially explained by the combination of cannot-link constraints and decreasing WCP. As the purity of clusters decreases, the cannot-link constraints start preventing clusters containing tracks of the same identity from merging. This results in MuHPC automatically terminating the clustering when there are more clusters than characters. We observe similar results when adding datasets in different orders. Similar experiments for combining the TBBT and Buffy datasets for face-clustering are presented in [69].\n\n\nG. Face-Clustering Results\n\nHere, we give further analysis of the face-clustering results shown in Table 3 of the main manuscript (and repeated in Table 7). This is an extension of Section 5.3 in the main manuscript. In detail, the extra analysis concerns the automated termination (AT) criterion, and the relation of MuHPC to previous methods. To summarise Section 5.3 of the main manuscript, MuHPC significantly surpasses the performance of previous methods across all program sets, all metrics and both AT and OC protocol.\n\nFirst, we analyse the AT protocol results. The goal of the AT protocol is to automatically terminate clustering and assess the quality of the resulting clusters. This is a realistic protocol for videos in-the-wild where the number of characters is unknown. Here, the number of predicted clusters, # , can be measured relative to the ground truth number of clusters, # . In all program sets, MuHPC predicts more clusters than the ground truth. This is because MuHPC prioritises high-precision. For TBBT, # is very close to # (168 vs. 130), and is in fact closer than the predictions of all previous methods. This is impressive seeing as the goal of BCL [69] is to predict the ground truth number of clusters. For the other program sets, # is slightly further from # than previous methods (e.g. a difference from # of 36 for Sherlock vs. 25 for C1C [36]). We now give two reasons why despite this, the clusters from MuHPC are far more desirable than those from previous methods.  First, the clusters from MuHPC are far higher quality. It would be expected that when predicting more clusters than there are ground truth clusters, any method would achieve higher WCP. However, NMI is also significantly higher for MuHPC than previous methods (e.g. on average 9.8% higher than the best prior work across all program sets). Second, for downstream applications, it is far more useful to have many high-precision clusters, than few very low-precision clusters. The latter in this case requires a large amount of human labelling in order to correctly label the person-tracks from the clusters (a cluster property reflected by the Operator Clicks Index (OCI-k) [22] metric). Furthermore, a good way of measuring the utility of clusters for a downstream task is the character precision and recall metrics. These metrics assign each character uniquely to a cluster, and measure the resulting precision and recall of these pseudo-labels. MuHPC significantly achieves a CP and CR of 56.0% and 39.3% higher, respectively, than C1C across all program sets. This indicates that although prior work may predict a number of clusters closer to the ground truth than MuHPC, these clusters however are of almost no use for downstream applications, unlike the clusters from MuHPC.\n\nNext, we discuss MuHPC in relation to previous methods. C1C continues using face to cluster even when there are large distances between clusters, and therefore degenerates in the later partitions, leading to lower WCP and NMI. Unlike BCL, MuHPC uses pretrained features, thus alleviating the computational burden of training, allowing for greater generalisation, and as we demonstrate leading to better results. BCL uses the assumption that each identity occupies the same hyper-spherical volume in their learnt latent space. We argue that complex similarity structures and variation between faces of the same identity mean that they cannot be constrained to within fixed-radius hyper-spheres (BCL), even when training with this objective. Instead, MuHPC does not enforce such a constraint, and uses a nearest neighbour constraint with multi-modality to connect highly dissimilar tracks.  Table 2 in the main manuscript.\n\n\nH. Parameter Selection & Sweeps\n\nIn this section, we give a parameter sweep for the nearest neighbour distance threshold tight (Section 3.1 in main manuscript), and\n\ngive further description and analysis on the automatic parameter selection method for loose (Section 3.2 in main manuscript).\n\n\nH.1. Nearest Neighbor Distance Threshold\n\nHere, we give metrics across all program sets in VPCD for parameter sweeps on the nearest neighbour distance threshold, tight .\n\nThese are displayed in Figure 9. As detailed in the main manuscript, the value was chosen on the validation partition of VPCD. To isolate the role of tight , all metrics are evaluated at the Automated Termination criterion, after Stage 1, and using only the face-track annotations. The metrics at the chosen value of tight = 0.48, are therefore equivalent to MuHPC-at AT protocol in Table 3 in the main manuscript. We notify the reader that in the main manuscript, it reads that tight = 0.52. This is incorrect, the value is tight = 0.48.\n\nAcross most program sets, the same relationship between the metrics and tight is seen. Namely, as tight increases, NMI increases, while WCP and the total number of clusters decreases. In more detail, as tight increases, the maximum distance at which clusters can merge increases. This leads to more cluster merges before the automatic termination of Stage 1. This is reflected by the decreasing number of clusters at the termination point. Firstly, there is an increased likelihood of incorrect merges, where clusters depicting different identities merge together, leading to lower precision clusters, as shown by decreasing WCP. Increasing tight also leads to more correct merges. This is reflected by the rising NMI, which shows that the identity overlap between clusters is decreasing. An increasing NMI can be interpreted as there being more correct merges than incorrect merges. In some program sets (e.g. Buffy, Sherlock), NMI starts to decrease as tight increases,\n\nindicating that more incorrect merges are being made than correct merges.\n\nIn a window surrounding the learnt value of 0.48, the NMI and WCP are roughly stable at very high values across all program sets (high relative to the respective prior work on those program sets -see Table 3 in main manuscript). This demonstrates that this learnt parameter generalises well to the different program sets, that the face features are indeed universal; and that MuHPC is not particularly sensitive to this choice of parameter. The program sets in VPCD are highly visually disparate. These results therefore indicate that MuHPC could be simply and effectively used on any number of different program sets not in VPCD.\n\nAt the chosen value of tight = 0.48, often more clusters are predicted than the ground truth number (marked as #C in Figure 9). In some program sets, this is by just a small number (168 vs # = 130 for TBBT, 223 vs # = 165 for Buffy). There is a trade-off between obtaining a number of clusters similar to #C, and the precision of these clusters. Our design choice at Stage 1 is to produce clusters with very high-precision. Stage 2 leads to a further reduction of these clusters by using multiple modalities to merge clusters. A discussion in Section G explains why overpredicting the number of clusters is beneficial for downstream uses of the clusters. we show similarities between voices of the same identity (positives) and different identities (negatives). These are found via the cannot-link constraints (negatives) and the clusters from Stage 1 (positives and negatives). Similarities are computed via (1 minus cosine similarity). This process finds less positives than negatives, hence the frequency of the positives is scaled to match that of the negatives.  \n\n\nH.2. Automatically Learnt Hyper-Parameters\n\nThe values for the threshold on the voice similarities that are used in the multi-modal bridges, loose , are learnt automatically for each of the audibly disparate program sets in VPCD (this is detailed in Section 3.4 in the main manuscript). Here, we give the values that are learnt for each program set, provide some analysis, and visualise the voice distances that the hyper-parameters were learnt from.\n\nThe values of loose learnt automatically for the different program sets are given in Table 8. The voice distances between different identities are found via a combination of cannot-link constraints and the clusters from Stage 1. We observe that for some program sets these voice distances are quite high. This in turn leads to a relatively high value of loose (e.g. TBBT, Friends). We additionally show the similarities between voices for the same identity (positives) and different identities (negatives) in Figure 10 for two program sets from VPCD.\n\nA high value of loose indicates that the characters all sounded different to the voice embedding network, and in turn the respective features from different speakers were able to be separated in the embedding space (Figure 10 -left). For the multi-modal bridges, this means that the voices from two speaking person-tracks can sound quite different and a bridge can still confidently be formed.\n\nFor other program sets, the voice distances between the different identities are quite low, and therefore loose is also low (e.g. Buffy, Sherlock). In these cases, there are many similar sounding characters; hence, the voice embedding network cannot separate the embeddings from different identities well (Figure 10 -right). For the multi-modal bridges, this means that the voices from two speaking person-tracks must sound very similar for a bridge to still confidently be formed, as only then can the voice modality (together with the concurrent agreement from the face modality) be sure that it is the same person.\n\nFigure 2 :\n2The clustering process of MuHPC. (Left) Example persontracks at each stage of MuHPC. Two high-precision clusters from Stage 1 depicting the same character. One contains near-frontal faces (below) and one profiles (top), hence the single face modality cannot confidently merge the two. Stage 2 uses a talking person-track from each cluster to form a bridge, by demanding the agreement of both face and voice modalities that these contain the same identity. Stage 3 merges face-less bodies into the formed cluster. (right) The NMI and number of clusters at each partition, \u0393, of stages 1 and 2 on an example video from VPCD. At each partition the number of clusters decreases, while the normalised mutual information increases. At \u0393 4 Stage 1 clustering stops. Stage 2 progresses to \u0393 5 by bridging clusters. Stage 3 does not affect the number of clusters.\n\nFigure 3 :\n3VPCD dataset. It consists of different and diverse TV shows and movies; here, we display a subset of them: (a) About Last Night, (b) TBBT, (c) Friends, (d) Sherlock, (e) Hidden Figures. VPCD contains face, body and voice tracks annotated for many characters. Here, we display such examples.\n\n\nFurther ablations and experiments on clustering all characters in all videos simultaneously are included in the appendix. Implementation details. We use the face, body and voice track annotations and features from VPCD (Sections 4.1,4.3). For all modalities, feature distances , , are computed using (1 -cosine similarity). As described in Section 3.4, parameters are learnt on the VPCD val. set. The values are: tight =0.48, =0.025, =0.9 and back =0.4. These parameters are fixed for all experiments, and only have to be re-learnt if the features change. Details on the automatically selected tight values are in the appendix.\n\n\nClustering Process of MuHPC for a character in Buffy. Stage 1 produces high-precision clusters. Cluster #1 contains mainly profile and downwards-facing views of the character, while Cluster #2 contains frontal facing views. Both clusters contain very different clothing and body poses. The face modality alone can no longer confidently merge these clusters. Stage 2 merges the two clusters using multi-modal bridges between a speaking person-track from each cluster. Stage 3 then merges back views into these clusters via body features. Back views of the character are merged via frontal appearances in nearby shots where the character is wearing the same clothing. Clustering Process of MuHPC for a character in Sherlock. Stage 1 produces high-precision clusters. Cluster #1 contains mainly frontal face views, while Cluster #2 contains profile face views. Both clusters contain very different lighting conditions, body poses; and camera-views of the same character. Stage 2 merges the two clusters where the face alone could not, by using multi-modal bridges between a speaking person-track from each cluster. Stage 3 then merges back views into these clusters via body features. Back views of the character (both full-body, and over-the-shoulder views) are merged via frontal appearances in nearby shots where the character is wearing the same clothing.\n\nFigure 4 :\n4Clustering Process of MuHPC. For two program sets from VPCD, (a)-Buffy, and (b)-Sherlock, we show the clustering process for one of the principal characters.\n\nFigure 5 :\n5Character co-occurrences for story understanding between the 5 main characters in the 6 episodes of The Big Bang Theory in VPCD. (a) The ground truth co-occurrences as a proportion of the 6 videos. It is generated from VPCD which annotates each character whenever they are visible. Higher indicates more co-occurrence. (b,c): MuHPC and face-level clustering co-occurrences; (d,e): MuHPC and face-level co-occurrences, relative to the ground truth (a). 1.0 indicates that the prediction is the same as the ground truth. Key: P: Penny, H: Howard, R: Raj, S: Sheldon, L:Leonard.\n\nFigure 6 :\n6Voice-track lengths in VPCD. The distribution of all voicetrack lengths in VPCD.\n\n\nsures the trade-off between clustering quality and number of resulting clusters. Given class labels and cluster labels , NMI( , ) = 2 ( ; ) ( )+ ( ) , where (.) is the entropy and ( ; ) = ( ) \u2212 ( \\ ) the mutual information.\n\nFigure 7 :\n7Stage 1 and Stage 2 Person-Clustering results from the program set, Friends. %WCP and %NMI for episodes of Friends from VPCD, for the Automatic Termination protocol (AT). The blue line illustrates the results after Stage 1, while the orange one illustrates the results after Stage 2, i.e. bridging clusters by exploiting the voice modality. # is the ground truth number of clusters for each episode. For fair comparison between MuHPC , MuHPC ; and MuHPC , we cluster the same person-tracks in each of the experiments. This limits the experiments to person-tracks with all three available modalities i.e. talking person-tracks with a visible face. To isolate the role of each of the modalities, we report clustering performance after Stage 1. Similarly to tight in MuHPC, for\n\n6 :\n6Person-Clustering Results on VPCD. For each program set, each metric is averaged across all episodes. OC protocol. The 'Average' column reports averaged metrics across all six program sets. # is the sum of ground truth clusters across each episode. We report two strong baselines (B-ReID, B-C1C, Section 5.1 in main manuscript) and an ablation on the modalities used. Keys: F-face, B-body, V-voice. Modality: used modalities.\n\nFigure 8 :\n8Person-Clustering Results when clustering multiple program sets simultaneously. Incrementally, more and more tracks are considered by adding different program sets together. There are discrete data points for each time the tracks from an additional episode or movie are added. Each data point considers the total cumulative number of tracks up to that point. All experiments are for the Automatic Termination (AT) protocol for person-clustering for MuHPC. Top: The WCP and NMI measurements. Bottom:\n\nFigure 9 :\n9Parameter sweep for tight on the six program sets in VPCD. For each program set, the NMI, WCP and number of clusters are plotted, for the Automatic Termination criterion, for varying values of tight . We additionally show for each program set, the ground truth number of clusters, #C, marked on the Number of Clusters axis of each plot. For the numerical values of #C, we refer the reader to\n\nFigure 10 :\n10Voice similarities in two program sets from VPCD. Here\n\nTable 1 :\n1Video Person-Clustering Dataset statistics. For each programset in VPCD we detail video and annotation statistics. #eps: number of \nepisodes; #IDs: number of unique characters; TBBT: The Big Bang Theory; \n(movies) ALN: About Last Night; HF: Hidden Figures. We cite the first \npublished work that used each respective program set for face-clustering, \nbut we provide additional full multi-modal annotations for each. \n\n\n\n\nMetrics.For each dataset in VPCD, we measure each metric at the episode level and average over all episodes. Following[36,69], we use Weighted Cluster Purity (WCP) and Normalized Mutual Information (NMI). WCP weights the purity of a cluster by the number of tracks belonging in it. NMI[43] measures the trade-off between clustering quality and number of resulting clusters. Character Precision and Recall (CP, CR) are computed using the number of ground truth identities. Each identity is uniquely assigned to a cluster. CP is the proportion of tracks in a cluster that belong to its assigned character, while CR is the proportion of that character's total tracks that appear in the cluster. They are averaged across all characters, thus weighting each equally.Test protocol. We evaluate: (i) automatic termination (AT), i.e. unknown number of clusters, and (ii) oracle cluster (OC), when known. AT is realistic for applications, while OC offers a fair comparison to the state of the art.F B V WCP NMI CP CR WCP NMI CP CR WCP NMI CP CR WCP NMI CP CR WCP NMI CP CR WCP NMI CP CR WCP NMI CP CR# \n\nModality \n# = \n# = \n# = \n# = \n# = \n# = \n# = \nTBBT \n130 \nBuffy \n165 \nSherlock \n50 \nFriends \n239 \nHidden Figures \n10 \nAbout Last Night \n24 \nAverage \n618 \n\nB-ReID \n80.5 69.7 49.6 55.0 65.0 60.9 52.7 46.8 61.2 28.9 43.6 44.3 70.9 60.4 71.0 56.3 32.6 23.4 36.8 19.6 41.0 14.1 37.4 32.6 58.5 42.9 48.5 42.4 \nB-C1C \n87.7 69.2 39.4 50.6 73.6 58.2 34.6 41.6 77.7 41.6 29.3 43.6 85.3 77.1 69.5 70.8 76.2 69.8 55.2 50.3 94.4 85.8 68.0 76.8 82.5 67.0 49.3 55.6 \n\nMuHPC-\n93.5 84.6 76.4 77.6 80.0 66.7 63.8 65.2 83.8 52.3 51.2 58.4 85.7 73.7 81.3 79.0 77.6 70.4 59.1 52.1 95.7 89.7 98.2 86.3 86.1 72.9 71.7 69.8 \nMuHPC \n93.5 84.6 76.4 77.6 80.1 67.2 64.2 64.7 84.5 59.3 54.9 57.3 86.9 75.3 84.0 82.8 77.6 70.4 59.1 52.1 96.0 90.5 98.3 86.4 86.4 73.5 72.3 69.7 \nMuHPC \n96.9 92.8 80.4 79.6 85.7 75.6 68.1 67.9 84.1 52.9 51.7 54.3 89.5 81.3 84.6 82.4 77.6 70.3 59.0 52.0 95.7 89.4 98.2 86.3 88.2 77.1 74.0 70.0 \nMuHPC \n96.9 92.8 80.4 79.6 85.8 76.4 68.4 67.2 84.8 60.0 55.2 57.2 90.8 83.1 87.7 86.6 77.6 70.3 59.0 52.0 96.0 90.2 98.3 86.4 88.6 78.8 74.8 71.5 \n\n\n\nTable 3 :\n3Face-Clustering Results. Comparisons to previous state of the art on four program sets, using only face-tracks with unknown (AT), and known (OC) number of clusters. We report metrics averaged over each episode in each program set, and the number of predicted clusters, summed over each episode (# ). MuHPC-uses only face, whereas MuHPC uses the multi-modal bridges from voice and face. Where not reported in respective publications, numbers are computed using official implementations. Finch has no stopping criterion so results for AT are not reported.\n\n\nClustering/, 2021. 2 [73] Paul Vicol, Makarand Tapaswi, Lluis Castrejon, and Sanja Fidler. Moviegraphs: Towards understanding human-centric situations from videos. In Proc. CVPR, 2018. 3 [74] Kiri Wagstaff, Claire Cardie, Seth Rogers, Stefan Schr\u00f6dl, et al. Constrained k-means clustering with background knowledge. In Proc. ICML, 2001. 2 [75] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for person re-identification. In Proc. CVPR, 2018. 3 [76] P. Wohlhart, M. K\u00f6stinger, P. M. Roth, and H. Bischof. Sh\u0133ie Xiao, Mingkui Tan, and Dong Xu. Weighted blocksparse low rank representation for face clustering in videos. In Proc. ECCV, 2014. 2 [81] Weidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Utterance-level aggregation for speaker recognition in the wild. In Proc. ICASSP, 2019. 6, 14 [82] Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, and Lubomir Bourdev. Beyond frontal faces: Improving person recognition using multiple cues. In Proc. CVPR, 2015. 3 [83] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Joint face representation adaptation and clustering in videos. In Proc. ECCV, 2016. 2 [84] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In Proc. ICCV, 2015. 3, 7 [85] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled samples generated by gan improve the person re-identification baseline in vitro. In Proc. ICCV, 2017. 3, 7Mul-\ntiple instance boosting for face recognition in videos. In \nDAGM-Symposium, 2011. 2 \n[77] Baoyuan Wu, Siwei Lyu, Bao-Gang Hu, and Qiang Ji. Simul-\ntaneous clustering and tracklet linking for multi-face tracking \nin videos. In Proc. ICCV, 2013. 2 \n[78] Baoyuan Wu, Yifan Zhang, Bao-Gang Hu, and Qiang Ji. \nConstrained clustering and its application to face clustering \nin videos. In Proc. CVPR, 2013. 1, 2 \n[79] Jiangyue Xia, Anyi Rao, Qingqiu Huang, Linning Xu, Jiang-\ntao Wen, and Dahua Lin. Online multi-modal person search \nin videos. In Proc. ECCV, 2020. 3 \n[80] \n\nTable 4 :\n4Voice-Track statistics in VPCD. The number of voicetracks for each program set in VPCD both before and after a filtering step (Section B.1). All Annotations -the total voice-track annotations provided with VPCD. Filtered -the total voice-track annotations used by our person-clustering method, after ignoring short and overlapping tracks (same asTable 1in main manuscript). Total -the summation over all six program sets.voice-tracks that have 20% overlap with a different voice-track. \nSecond, the temporal length of the voice-tracks. As shown in [81]\n\nTable\n\n\nTable 7 :\n7Face-Clustering Results. Comparisons to previous state of the art on four program sets, using only face-tracks with unknown (AT), and known (OC) number of clusters. We report metrics averaged over each episode in each program set, and the number of predicted clusters, summed over each episode (# ). MuHPC-uses only face, whereas MuHPC uses the multi-modal bridges from voice and face. Where not reported in respective publications, numbers are computed using official implementations. Finch has no stopping criterion so results for AT are not reported.\n\nTable 8 :\n8The automatically learnt values for loose for the different program sets in VPCD.\nAcknowledgementWe are grateful to Bruno Korbar and Sagar Vaze for their helpful comments, and to Maya Gulieva, Shaya Ghadimi, and DK for their help with annotation. AB is funded by an EPSRC DTA Studentship. This work is supported by the EPSRC Programme Grants Seebibyte EP/M013774/1 and VisualAI EP/T028572/1, and by a CNRS INS2I 2020 grant.\nLabelling unlabelled videos from scratch with multi-modal self-supervision. NeurIPS. Yuki Asano, Mandela Patrick, Christian Rupprecht, Andrea Vedaldi, 14Yuki Asano, Mandela Patrick, Christian Rupprecht, and An- drea Vedaldi. Labelling unlabelled videos from scratch with multi-modal self-supervision. NeurIPS, 2020. 14\n\nTrecvid 2019: An evaluation campaign to benchmark video activity detection, video captioning and matching, and video search & retrieval. ArXiv. G Awad, A Butt, K Curtis, Y Lee, J Fiscus, A Godil, A Delgado, Jesse Zhang, Eliot Godard, Lukas L Diduch, A F Smeaton, Yyette Graham, Wessel Kraa\u0133, Georges Qu\u00e9not, abs/2009.09984G. Awad, A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, A. Delgado, Jesse Zhang, Eliot Godard, Lukas L. Diduch, A. F. Smeaton, Yyette Graham, Wessel Kraa\u0133, and Georges Qu\u00e9not. Trecvid 2019: An evaluation campaign to bench- mark video activity detection, video captioning and matching, and video search & retrieval. ArXiv, abs/2009.09984, 2019. 3\n\nAndrew Brown, and Andrew Zisserman. Condensed movies: Story based retrieval with contextual embeddings. Max Bain, Arsha Nagrani, ACCV. 2020Max Bain, Arsha Nagrani, Andrew Brown, and Andrew Zis- serman. Condensed movies: Story based retrieval with con- textual embeddings. In ACCV, 2020. 3\n\nSemi-supervised learning with constraints for person identification in multimedia data. Martin Bauml, Makarand Tapaswi, Rainer Stiefelhagen, Proc. CVPR. CVPR23Martin Bauml, Makarand Tapaswi, and Rainer Stiefelhagen. Semi-supervised learning with constraints for person identi- fication in multimedia data. In Proc. CVPR, 2013. 2, 3\n\nNames and faces in the news. L Tamara, Alexander C Berg, Jaety Berg, Michael Edwards, Ryan Maire, Yee-Whye White, Erik Teh, David A Learned-Miller, Forsyth, Proc. CVPR. CVPRTamara L Berg, Alexander C Berg, Jaety Edwards, Michael Maire, Ryan White, Yee-Whye Teh, Erik Learned-Miller, and David A Forsyth. Names and faces in the news. In Proc. CVPR, 2004. 2\n\nFinding actors and actions in movies. P Bojanowski, F Bach, I Laptev, J Ponce, C Schmid, J Sivic, Proc. ICCV. ICCVP. Bojanowski, F. Bach, , I. Laptev, J. Ponce, C. Schmid, and J. Sivic. Finding actors and actions in movies. In Proc. ICCV, 2013. 2\n\nAutomated video labelling: Identifying faces by corroborative evidence. Andrew Brown, Ernesto Coto, Andrew Zisserman, MIPR. Andrew Brown, Ernesto Coto, and Andrew Zisserman. Au- tomated video labelling: Identifying faces by corroborative evidence. In MIPR, 2021. 2\n\nSmooth-AP: Smoothing the path towards largescale image retrieval. Andrew Brown, Weidi Xie, Vicky Kalogeiton, Andrew Zisserman, 2020. 13European Conference on Computer Vision (ECCV). Andrew Brown, Weidi Xie, Vicky Kalogeiton, and Andrew Zisserman. Smooth-AP: Smoothing the path towards large- scale image retrieval. In European Conference on Computer Vision (ECCV), 2020. 13\n\nCascade r-cnn: Delving into high quality object detection. Zhaowei Cai, Nuno Vasconcelos, Proc. CVPR. CVPRZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proc. CVPR, 2018. 6\n\nVGGFace2: A dataset for recognising faces across pose and age. Qiong Cao, Li Shen, Weidi Xie, M Omkar, Andrew Parkhi, Zisserman, Proc. Int. Conf. Autom. Face and Gesture Recog. Int. Conf. Autom. Face and Gesture Recog56Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An- drew Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In Proc. Int. Conf. Autom. Face and Gesture Recog., 2018. 5, 6\n\nIn defence of metric learning for speaker recognition. Jaesung Joon Son Chung, Seongkyu Huh, Minjae Mun, Hee Soo Lee, Soyeon Heo, Chiheon Choe, Sunghwan Ham, Bong-Jin Jung, Icksang Lee, Han, INTERSPEECH. Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Lee, Hee Soo Heo, Soyeon Choe, Chiheon Ham, Sunghwan Jung, Bong-Jin Lee, and Icksang Han. In defence of metric learning for speaker recognition. In INTERSPEECH, 2020. 6\n\nVoxCeleb2: Deep speaker recognition. Arsha Joon Son Chung, Andrew Nagrani, Zisserman, INTERSPEECH. Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition. In INTERSPEECH, 2018. 6\n\nUnsupervised metric learning for face identification in tv video. Jakob Ramazan Gokberk Cinbis, Cordelia Verbeek, Schmid, Proc. ICCV. ICCV13Ramazan Gokberk Cinbis, Jakob Verbeek, and Cordelia Schmid. Unsupervised metric learning for face identifica- tion in tv video. In Proc. ICCV, 2011. 1, 2, 3\n\nLearning from ambiguously labeled images. Timothee Cour, Benjamin Sapp, Chris Jordan, Ben Taskar, Proc. CVPR. CVPRTimothee Cour, Benjamin Sapp, Chris Jordan, and Ben Taskar. Learning from ambiguously labeled images. In Proc. CVPR, 2009. 3\n\nTalking pictures: Temporal grouping and dialog-supervised person recognition. T Cour, B Sapp, A Nagle, B Taskar, Proc. CVPR. CVPRT. Cour, B. Sapp, A. Nagle, and B. Taskar. Talking pictures: Temporal grouping and dialog-supervised person recogni- tion. In Proc. CVPR, 2010. 2\n\nConstrained clustering with minkowski weighted k-means. Renato Cordeiro De Amorim, CINTI. Renato Cordeiro de Amorim. Constrained clustering with minkowski weighted k-means. In CINTI, 2012. 2\n\nStoviz: story visualization of tv series. Philippe Ercolessi, Herv\u00e9 Bredin, Christine S\u00e9nac, Proc. ACMMM. ACMMMPhilippe Ercolessi, Herv\u00e9 Bredin, and Christine S\u00e9nac. Stoviz: story visualization of tv series. In Proc. ACMMM, 2012. 3\n\nBuffy\" -automatic naming of characters in TV video. Mark Everingham, Josef Sivic, Andrew Zisserman, Proc. BMVC. BMVC617Hello! My name isMark Everingham, Josef Sivic, and Andrew Zisserman. \"Hello! My name is... Buffy\" -automatic naming of charac- ters in TV video. In Proc. BMVC, 2006. 2, 3, 5, 6, 17\n\nTaking the bite out of automatic naming of characters in TV video. Image and Vision Computing. Mark Everingham, Josef Sivic, Andrew Zisserman, Mark Everingham, Josef Sivic, and Andrew Zisserman. Tak- ing the bite out of automatic naming of characters in TV video. Image and Vision Computing, 2009. 2\n\nOn affine invariant clustering and automatic cast listing in movies. Andrew W Fitzgibbon, Andrew Zisserman, Proc. ECCV. ECCV1Andrew W. Fitzgibbon and Andrew Zisserman. On affine invariant clustering and automatic cast listing in movies. In Proc. ECCV, 2002. 1, 2\n\nAccio: A data set for face track retrieval in movies across age. Esam Ghaleb, Makarand Tapaswi, Ziad Al-Halah, Hazim Kemal Ekenel, and Rainer Stiefelhagen. Proc. ICMREsam Ghaleb, Makarand Tapaswi, Ziad Al-Halah, Hazim Ke- mal Ekenel, and Rainer Stiefelhagen. Accio: A data set for face track retrieval in movies across age. In Proc. ICMR, 2015. 3\n\nIs that you? metric learning approaches for face identification. Matthieu Guillaumin, Jakob Verbeek, Cordelia Schmid, 17Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid. Is that you? metric learning approaches for face identifica- tion. 2009. 17\n\nMS-Celeb-1M: A dataset and benchmark for large-scale face recognition. Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, ECCV. Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. MS-Celeb-1M: A dataset and benchmark for large-scale face recognition. In ECCV, 2016. 6\n\nNaming tv characters by watching and analyzing dialogs. Monica-Laura Haurilet, Makarand Tapaswi, Ziad Al-Halah, Rainer Stiefelhagen, Proc. WACV. WACVMonica-Laura Haurilet, Makarand Tapaswi, Ziad Al-Halah, and Rainer Stiefelhagen. Naming tv characters by watching and analyzing dialogs. In Proc. WACV, 2016. 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proc. CVPR. CVPRKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016. 6\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proc. CVPR. CVPRKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. CVPR, 2016. 13\n\nMerge or not? learning to group faces via imitation learning. Yue He, Kaidi Cao, Cheng Li, Chen Change Loy, AAAI. Yue He, Kaidi Cao, Cheng Li, and Chen Change Loy. Merge or not? learning to group faces via imitation learning. In AAAI, 2018. 2\n\nClustering appearances of objects under varying illumination conditions. Jeffrey Ho, Ming-Husang Yang, Jongwoo Lim, Kuang-Chih Lee, David Kriegman, Proc. CVPR. CVPRJeffrey Ho, Ming-Husang Yang, Jongwoo Lim, Kuang-Chih Lee, and David Kriegman. Clustering appearances of objects under varying illumination conditions. In Proc. CVPR, 2003. 2\n\nThread-safe: Towards recognizing human actions across shot boundaries. Minh Hoai, Andrew Zisserman, Asian Conference on Computer Vision. Minh Hoai and Andrew Zisserman. Thread-safe: Towards recognizing human actions across shot boundaries. In Asian Conference on Computer Vision, 2014. 7\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, Proc. CVPR. CVPRJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proc. CVPR, 2018. 6\n\nPerson search in videos with one portrait through visual and temporal links. Qingqiu Huang, Wentao Liu, Dahua Lin, Proc. ECCV. ECCV613Qingqiu Huang, Wentao Liu, and Dahua Lin. Person search in videos with one portrait through visual and temporal links. In Proc. ECCV, 2018. 3, 6, 13\n\nMovienet: A holistic dataset for movie understanding. Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin, Proc. ECCV. ECCV36Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic dataset for movie understanding. In Proc. ECCV, 2020. 3, 6\n\nClustering using a similarity measure based on shared near neighbors. Austin Raymond, Edward A Jarvis, Patrick, IEEE Trans. Computers. 23Raymond Austin Jarvis and Edward A. Patrick. Clustering using a similarity measure based on shared near neighbors. IEEE Trans. Computers, 1973. 2, 3\n\nEnd-to-end face detection and cast grouping in movies using erdos-renyi clustering. Souyoung Jin, Hang Su, Chris Stauffer, Erik Learned-Miller, Proc. ICCV. ICCVSouYoung Jin, Hang Su, Chris Stauffer, and Erik Learned- Miller. End-to-end face detection and cast grouping in movies using erdos-renyi clustering. In Proc. ICCV, 2017. 1\n\nPerson recognition in personal photo collections. Rodrigo Seong Joon Oh, Mario Benenson, Bernt Fritz, Schiele, Proc. ICCV. ICCVSeong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt Schiele. Person recognition in personal photo collections. In Proc. ICCV, 2015. 3\n\nConstrained video face clustering using 1nn relations. Vicky Kalogeiton, Andrew Zisserman, Proc. BMVC. BMVC17Vicky Kalogeiton and Andrew Zisserman. Constrained video face clustering using 1nn relations. In Proc. BMVC, 2020. 1, 2, 3, 4, 5, 6, 7, 9, 17\n\nLearning to recognize faces from videos and weakly relatedinformation cues. M K\u00f6stinger, P Wohlhart, P Roth, H Bischof, AVSS. 2M. K\u00f6stinger, P. Wohlhart, P. Roth, and H. Bischof. Learning to recognize faces from videos and weakly relatedinformation cues. In AVSS, 2011. 2\n\nThe hungarian method for the assignment problem. Naval research logistics quarterly. Harold W Kuhn, 1314Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 1955. 13, 14\n\nLearning interactions and relationships between movie characters. Anna Kukleva, Makarand Tapaswi, Ivan Laptev, Proc. CVPR. CVPR39Anna Kukleva, Makarand Tapaswi, and Ivan Laptev. Learn- ing interactions and relationships between movie characters. In Proc. CVPR, 2020. 3, 9\n\nDeepreid: Deep filter pairing neural network for person re-identification. Wei Li, Rui Zhao, Tong Xiao, Xiaogang Wang, Proc. CVPR. CVPR37Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person re-identification. In Proc. CVPR, 2014. 3, 7\n\nDeep density clustering of unconstrained faces. Wei-An Lin, Jun-Cheng Chen, Carlos D Castillo, Rama Chellappa, Proc. CVPR. CVPRWei-An Lin, Jun-Cheng Chen, Carlos D Castillo, and Rama Chellappa. Deep density clustering of unconstrained faces. In Proc. CVPR, 2018. 2\n\nDistinctive image features from scaleinvariant keypoints. G David, Lowe, International journal of computer vision. 4David G Lowe. Distinctive image features from scale- invariant keypoints. International journal of computer vision, 2004. 4\n\nIntroduction to information retrieval. D Christopher, Prabhakar Manning, Hinrich Raghavan, Sch\u00fctze, Cambridge university press614Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze. Introduction to information retrieval. Cambridge university press, 2008. 6, 14\n\nLAEO-Net: revisiting people Looking At Each Other in videos. M J Marin-Jimenez, V Kalogeiton, P Medina-Suarez, A Zisserman, Proc. CVPR. CVPRM. J. Marin-Jimenez, V. Kalogeiton, P. Medina-Suarez, and A. Zisserman. LAEO-Net: revisiting people Looking At Each Other in videos. In Proc. CVPR, 2019. 3\n\nLAEO-Net++: revisiting people Looking At Each Other in videos. M J Marin-Jimenez, V Kalogeiton, P Medina-Suarez, A Zisserman, IEEE PAMI. 2020M. J. Marin-Jimenez, V. Kalogeiton, P. Medina-Suarez, and A. Zisserman. LAEO-Net++: revisiting people Looking At Each Other in videos. In IEEE PAMI, 2020. 9\n\nLearnable pins: Cross-modal embeddings for person identity. Arsha Nagrani, Samuel Albanie, Andrew Zisserman, Proc. ECCV. ECCVArsha Nagrani, Samuel Albanie, and Andrew Zisserman. Learnable pins: Cross-modal embeddings for person identity. In Proc. ECCV, 2018. 4\n\nSeeing voices and hearing faces: Cross-modal biometric matching. Arsha Nagrani, Samuel Albanie, Andrew Zisserman, Proc. CVPR. CVPRArsha Nagrani, Samuel Albanie, and Andrew Zisserman. Seeing voices and hearing faces: Cross-modal biometric matching. In Proc. CVPR, 2018. 4\n\nFrom benedict cumberbatch to sherlock holmes: Character identification in tv series without a script. Arsha Nagrani, Andrew Zisserman, Proc. BMVC. BMVC1417Arsha Nagrani and Andrew Zisserman. From benedict cum- berbatch to sherlock holmes: Character identification in tv series without a script. In Proc. BMVC, 2017. 2, 5, 6, 14, 17\n\nClustering millions of faces by identity. Charles Otto, Dayong Wang, Jain, Charles Otto, Dayong Wang, and Anil K Jain. Clustering millions of faces by identity. IEEE PAMI, 2017. 2\n\nOn evaluating face tracks in movies. Alexey Ozerov, Jean-Ronan Vigouroux, Louis Chevallier, Patrick P\u00e9rez, Intl. Conf. Image Proc. Alexey Ozerov, Jean-Ronan Vigouroux, Louis Chevallier, and Patrick P\u00e9rez. On evaluating face tracks in movies. In Intl. Conf. Image Proc., 2013. 3\n\nIt's in the bag: Stronger supervision for automated face labelling. M Omkar, Esa Parkhi, Andrew Rahtu, Zisserman, ICCV Workshop: Describing and Understanding Video & The Large Scale Movie Description Challenge. Omkar M. Parkhi, Esa Rahtu, and Andrew Zisserman. It's in the bag: Stronger supervision for automated face labelling. In ICCV Workshop: Describing and Understanding Video & The Large Scale Movie Description Challenge, 2015. 2\n\nMultimodal person discovery in broadcast tv: lessons learned from mediaeval 2015. Multimedia Tools and Applications. Johann Poignant, Herv\u00e9 Bredin, Claude Barras, Johann Poignant, Herv\u00e9 Bredin, and Claude Barras. Multi- modal person discovery in broadcast tv: lessons learned from mediaeval 2015. Multimedia Tools and Applications, 2017. 3\n\nLeveraging archival video for building face datasets. Deva Ramanan, Simon Baker, Sham Kakade, Proc. ICCV. ICCVDeva Ramanan, Simon Baker, and Sham Kakade. Leveraging archival video for building face datasets. In Proc. ICCV, 2007. 2\n\nLinking people in videos with \"their\" names using coreference resolution. Vignesh Ramanathan, Armand Joulin, Percy Liang, Li Fei-Fei, Proc. ECCV. ECCVVignesh Ramanathan, Armand Joulin, Percy Liang, and Li Fei-Fei. Linking people in videos with \"their\" names using coreference resolution. In Proc. ECCV, 2014. 2\n\nA local-to-global approach to multi-modal movie scene segmentation. Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin, Proc. CVPR. CVPRAnyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, and Dahua Lin. A local-to-global ap- proach to multi-modal movie scene segmentation. In Proc. CVPR, 2020. 3\n\nA Survey of Partitional and Hierarchical Clustering Algorithms. Chandan Reddy, Bhanukiran Vinzamuri, Chandan Reddy and Bhanukiran Vinzamuri. A Survey of Partitional and Hierarchical Clustering Algorithms. 2018. 3\n\nSelf-supervised face-grouping on graphs. Veith Rothlingshofer, Vivek Sharma, Rainer Stiefelhagen, Proc. ACMMM. ACMMM27Veith Rothlingshofer, Vivek Sharma, and Rainer Stiefelha- gen. Self-supervised face-grouping on graphs. In Proc. ACMMM, 2019. 2, 7\n\nTvd: a reproducible and multiply aligned tv series dataset. Anindya Roy, Camille Guinaudeau, Herv\u00e9 Bredin, Claude Barras, LREC. 617Anindya Roy, Camille Guinaudeau, Herv\u00e9 Bredin, and Claude Barras. Tvd: a reproducible and multiply aligned tv series dataset. In LREC, 2014. 2, 3, 5, 6, 17\n\nEfficient parameter-free clustering using first neighbor relations. Saquib Sarfraz, Vivek Sharma, Rainer Stiefelhagen, Proc. CVPR. CVPR17Saquib Sarfraz, Vivek Sharma, and Rainer Stiefelhagen. Effi- cient parameter-free clustering using first neighbor relations. In Proc. CVPR, 2019. 2, 3, 4, 7, 9, 17\n\nSelf-supervised learning of face representations for video face clustering. Vivek Sharma, Makarand Tapaswi, Rainer Saquib Sarfraz, Stiefelhagen, Proc. Int. Conf. Autom. Face and Gesture Recog. Int. Conf. Autom. Face and Gesture Recog25Vivek Sharma, Makarand Tapaswi, M Saquib Sarfraz, and Rainer Stiefelhagen. Self-supervised learning of face rep- resentations for video face clustering. In Proc. Int. Conf. Autom. Face and Gesture Recog., 2019. 2, 5\n\nVideo face clustering with selfsupervised representation learning. Vivek Sharma, Makarand Tapaswi, Rainer Saquib Sarfraz, Stiefelhagen, IEEE Transactions on Biometrics, Behavior, and Identity Science. 2Vivek Sharma, Makarand Tapaswi, M Saquib Sarfraz, and Rainer Stiefelhagen. Video face clustering with self- supervised representation learning. IEEE Transactions on Biometrics, Behavior, and Identity Science, 2019. 2\n\nClustering based contrastive learning for improving face representations. Vivek Sharma, Makarand Tapaswi, Rainer Saquib Sarfraz, Stiefelhagen, Proc. Int. Conf. Autom. Face and Gesture Recog. Int. Conf. Autom. Face and Gesture Recog7Vivek Sharma, Makarand Tapaswi, M Saquib Sarfraz, and Rainer Stiefelhagen. Clustering based contrastive learning for improving face representations. Proc. Int. Conf. Autom. Face and Gesture Recog., 2020. 2, 5, 7\n\nThinking through and writing about research ethics beyond \"broader impact. Kate Sim, Andrew Brown, Amelia Hassoun, 2021. 13Kate Sim, Andrew Brown, and Amelia Hassoun. Thinking through and writing about research ethics beyond \"broader impact\". CoRR, 2021. 13\n\nWho are you?\" -learning person specific classifiers from video. Josef Sivic, Mark Everingham, Andrew Zisserman, Proc. CVPR. CVPRJosef Sivic, Mark Everingham, and Andrew Zisserman. \"Who are you?\" -learning person specific classifiers from video. In Proc. CVPR, 2009. 2\n\nFinding people in repeated shots of the same scene. Josef Sivic, C Larry Zitnick, Rick Szeliski, Proc. BMVC. BMVCJosef Sivic, C. Larry Zitnick, and Rick Szeliski. Finding people in repeated shots of the same scene. In Proc. BMVC, 2006. 3\n\nMulti-face: Self-supervised multiview adaptation for robust face clustering in videos. Krishna Somandepalli, Rajat Hebbar, Shrikanth Narayanan, arXiv:2008.11289arXiv preprintKrishna Somandepalli, Rajat Hebbar, and Shrikanth Narayanan. Multi-face: Self-supervised multiview adap- tation for robust face clustering in videos. arXiv preprint arXiv:2008.11289, 2020. 5\n\nknock! knock! who is it?\" probabilistic person identification in tv-series. Makarand Tapaswi, Martin B\u00e4uml, Rainer Stiefelhagen, Proc. CVPR. CVPRMakarand Tapaswi, Martin B\u00e4uml, and Rainer Stiefelhagen. \"knock! knock! who is it?\" probabilistic person identifica- tion in tv-series. In Proc. CVPR, 2012. 2\n\nStorygraphs: visualizing character interactions as a timeline. Makarand Tapaswi, Martin Bauml, Rainer Stiefelhagen, Proc. CVPR. CVPRMakarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen. Storygraphs: visualizing character interactions as a timeline. In Proc. CVPR, 2014. 3\n\nVideo face clustering with unknown number of clusters. Makarand Tapaswi, T Marc, Sanja Law, Fidler, Proc. ICCV. ICCV17Makarand Tapaswi, Marc T Law, and Sanja Fidler. Video face clustering with unknown number of clusters. In Proc. ICCV, 2019. 1, 2, 4, 5, 6, 7, 9, 17\n\nTotal cluster: A person agnostic clustering method for broadcast videos. Makarand Tapaswi, M Omkar, Esa Parkhi, Eric Rahtu, Rainer Sommerlade, Andrew Stiefelhagen, Zisserman, Proc. ICVGIP. ICVGIP1Makarand Tapaswi, Omkar M Parkhi, Esa Rahtu, Eric Som- merlade, Rainer Stiefelhagen, and Andrew Zisserman. Total cluster: A person agnostic clustering method for broadcast videos. In Proc. ICVGIP, 2014. 1, 2\n\nSimon Wouter Van Gansbeke, Stamatios Vandenhende, Marc Georgoulis, Luc Proesmans, Van Gool, arXiv:2005.12320Learning to classify images without labels. 14arXiv preprintWouter Van Gansbeke, Simon Vandenhende, Stamatios Geor- goulis, Marc Proesmans, and Luc Van Gool. Learning to clas- sify images without labels. arXiv preprint arXiv:2005.12320, 2020. 14\n\nVideo person clustering dataset. Vgg, VGG. Video person clustering dataset. https: //www.robots.ox.ac.uk/~vgg/data/Video_Person_\n", "annotations": {"author": "[{\"end\":140,\"start\":71},{\"end\":290,\"start\":141},{\"end\":364,\"start\":291}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":78},{\"end\":157,\"start\":147},{\"end\":307,\"start\":298}]", "author_first_name": "[{\"end\":77,\"start\":71},{\"end\":146,\"start\":141},{\"end\":297,\"start\":291}]", "author_affiliation": "[{\"end\":139,\"start\":85},{\"end\":251,\"start\":197},{\"end\":289,\"start\":253},{\"end\":363,\"start\":309}]", "title": "[{\"end\":68,\"start\":1},{\"end\":432,\"start\":365}]", "venue": null, "abstract": "[{\"end\":1902,\"start\":434}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2020,\"start\":2016},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2023,\"start\":2020},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2026,\"start\":2023},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2029,\"start\":2026},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":2032,\"start\":2029},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":2035,\"start\":2032},{\"end\":2038,\"start\":2035},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6149,\"start\":6145},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6152,\"start\":6149},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6155,\"start\":6152},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6427,\"start\":6424},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6430,\"start\":6427},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6716,\"start\":6712},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6730,\"start\":6726},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8181,\"start\":8177},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8493,\"start\":8490},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8496,\"start\":8493},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8499,\"start\":8496},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8502,\"start\":8499},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8518,\"start\":8514},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8521,\"start\":8518},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8524,\"start\":8521},{\"end\":8527,\"start\":8524},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8607,\"start\":8603},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8610,\"start\":8607},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8764,\"start\":8761},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8767,\"start\":8764},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8770,\"start\":8767},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8773,\"start\":8770},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8776,\"start\":8773},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8779,\"start\":8776},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8782,\"start\":8779},{\"end\":8785,\"start\":8782},{\"end\":8788,\"start\":8785},{\"end\":8791,\"start\":8788},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8852,\"start\":8848},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8933,\"start\":8929},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8936,\"start\":8933},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8939,\"start\":8936},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8942,\"start\":8939},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8945,\"start\":8942},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8948,\"start\":8945},{\"end\":8951,\"start\":8948},{\"end\":8954,\"start\":8951},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8974,\"start\":8970},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9100,\"start\":9096},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9103,\"start\":9100},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9134,\"start\":9130},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9410,\"start\":9406},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9552,\"start\":9548},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9566,\"start\":9562},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10068,\"start\":10065},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10070,\"start\":10068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10073,\"start\":10070},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10076,\"start\":10073},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10079,\"start\":10076},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10082,\"start\":10079},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10085,\"start\":10082},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10088,\"start\":10085},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10091,\"start\":10088},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10144,\"start\":10141},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10147,\"start\":10144},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10150,\"start\":10147},{\"end\":10153,\"start\":10150},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10207,\"start\":10203},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10281,\"start\":10278},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10284,\"start\":10281},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10324,\"start\":10320},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10935,\"start\":10931},{\"end\":10938,\"start\":10935},{\"end\":10941,\"start\":10938},{\"end\":10944,\"start\":10941},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11396,\"start\":11392},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11399,\"start\":11396},{\"end\":11402,\"start\":11399},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11449,\"start\":11445},{\"end\":11452,\"start\":11449},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11458,\"start\":11454},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11461,\"start\":11458},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11554,\"start\":11551},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11831,\"start\":11827},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11834,\"start\":11831},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11837,\"start\":11834},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11840,\"start\":11837},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11843,\"start\":11840},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11846,\"start\":11843},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12140,\"start\":12137},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12143,\"start\":12140},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12165,\"start\":12161},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12274,\"start\":12270},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12277,\"start\":12274},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12336,\"start\":12332},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12366,\"start\":12363},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12769,\"start\":12765},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12772,\"start\":12769},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12810,\"start\":12806},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12813,\"start\":12810},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12835,\"start\":12831},{\"end\":12863,\"start\":12859},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12918,\"start\":12915},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12945,\"start\":12942},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12948,\"start\":12945},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12975,\"start\":12972},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12978,\"start\":12975},{\"end\":12981,\"start\":12978},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13279,\"start\":13275},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13976,\"start\":13972},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13979,\"start\":13976},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":13982,\"start\":13979},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14103,\"start\":14099},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14116,\"start\":14113},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14119,\"start\":14116},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16160,\"start\":16156},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16163,\"start\":16160},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16524,\"start\":16520},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17066,\"start\":17062},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17846,\"start\":17842},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17849,\"start\":17846},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":17852,\"start\":17849},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18962,\"start\":18958},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18965,\"start\":18962},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20152,\"start\":20148},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20235,\"start\":20234},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20263,\"start\":20262},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":21369,\"start\":21365},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22178,\"start\":22174},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":22181,\"start\":22178},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":22184,\"start\":22181},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22946,\"start\":22942},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27230,\"start\":27227},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27255,\"start\":27251},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27925,\"start\":27921},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27957,\"start\":27953},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27990,\"start\":27986},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28004,\"start\":28000},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28007,\"start\":28004},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28010,\"start\":28007},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":28013,\"start\":28010},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28090,\"start\":28086},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28110,\"start\":28106},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28209,\"start\":28205},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28320,\"start\":28316},{\"end\":28322,\"start\":28320},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28349,\"start\":28345},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29223,\"start\":29219},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29276,\"start\":29272},{\"end\":29279,\"start\":29276},{\"end\":29282,\"start\":29279},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31175,\"start\":31171},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":32748,\"start\":32744},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32805,\"start\":32801},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":32816,\"start\":32812},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32829,\"start\":32825},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32892,\"start\":32888},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":32895,\"start\":32892},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32964,\"start\":32960},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":32995,\"start\":32991},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32998,\"start\":32995},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":33250,\"start\":33246},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":33253,\"start\":33250},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34275,\"start\":34271},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34278,\"start\":34275},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36794,\"start\":36790},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":37463,\"start\":37459},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39445,\"start\":39441},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40371,\"start\":40367},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40549,\"start\":40545},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41344,\"start\":41341},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45361,\"start\":45357},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":45811,\"start\":45807},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45908,\"start\":45905},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":45911,\"start\":45908},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":52539,\"start\":52535},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":52562,\"start\":52558},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":52582,\"start\":52578},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":54111,\"start\":54107},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":55298,\"start\":55294},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":55493,\"start\":55489},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":56297,\"start\":56293},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":70600,\"start\":70596},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":70603,\"start\":70600},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":70767,\"start\":70763}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64464,\"start\":63597},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64768,\"start\":64465},{\"attributes\":{\"id\":\"fig_2\"},\"end\":65398,\"start\":64769},{\"attributes\":{\"id\":\"fig_3\"},\"end\":66757,\"start\":65399},{\"attributes\":{\"id\":\"fig_4\"},\"end\":66928,\"start\":66758},{\"attributes\":{\"id\":\"fig_5\"},\"end\":67517,\"start\":66929},{\"attributes\":{\"id\":\"fig_6\"},\"end\":67611,\"start\":67518},{\"attributes\":{\"id\":\"fig_7\"},\"end\":67837,\"start\":67612},{\"attributes\":{\"id\":\"fig_9\"},\"end\":68625,\"start\":67838},{\"attributes\":{\"id\":\"fig_10\"},\"end\":69057,\"start\":68626},{\"attributes\":{\"id\":\"fig_11\"},\"end\":69569,\"start\":69058},{\"attributes\":{\"id\":\"fig_12\"},\"end\":69974,\"start\":69570},{\"attributes\":{\"id\":\"fig_13\"},\"end\":70044,\"start\":69975},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":70475,\"start\":70045},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":72617,\"start\":70476},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":73183,\"start\":72618},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":75252,\"start\":73184},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":75817,\"start\":75253},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":75825,\"start\":75818},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":76391,\"start\":75826},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":76485,\"start\":76392}]", "paragraph": "[{\"end\":2438,\"start\":1918},{\"end\":3224,\"start\":2440},{\"end\":4385,\"start\":3226},{\"end\":4970,\"start\":4387},{\"end\":5888,\"start\":4972},{\"end\":6443,\"start\":5890},{\"end\":7432,\"start\":6445},{\"end\":8182,\"start\":7434},{\"end\":8433,\"start\":8199},{\"end\":13103,\"start\":8454},{\"end\":15818,\"start\":13114},{\"end\":16667,\"start\":15857},{\"end\":17268,\"start\":16669},{\"end\":17664,\"start\":17363},{\"end\":18020,\"start\":17666},{\"end\":19449,\"start\":18062},{\"end\":20367,\"start\":19479},{\"end\":22553,\"start\":20369},{\"end\":24174,\"start\":22583},{\"end\":24869,\"start\":24210},{\"end\":26404,\"start\":24886},{\"end\":27829,\"start\":26427},{\"end\":28350,\"start\":27852},{\"end\":29058,\"start\":28366},{\"end\":30683,\"start\":29080},{\"end\":31250,\"start\":30696},{\"end\":32592,\"start\":31252},{\"end\":33977,\"start\":32612},{\"end\":34819,\"start\":34010},{\"end\":35637,\"start\":34821},{\"end\":36156,\"start\":35639},{\"end\":36796,\"start\":36172},{\"end\":37221,\"start\":36829},{\"end\":38009,\"start\":37223},{\"end\":38472,\"start\":38011},{\"end\":38796,\"start\":38492},{\"end\":39101,\"start\":38824},{\"end\":40171,\"start\":39103},{\"end\":40283,\"start\":40199},{\"end\":41515,\"start\":40285},{\"end\":42340,\"start\":41552},{\"end\":43552,\"start\":42370},{\"end\":43936,\"start\":43567},{\"end\":44908,\"start\":43938},{\"end\":46085,\"start\":45027},{\"end\":47465,\"start\":46110},{\"end\":48012,\"start\":47498},{\"end\":48614,\"start\":48040},{\"end\":49040,\"start\":48616},{\"end\":50928,\"start\":49042},{\"end\":51745,\"start\":50999},{\"end\":52711,\"start\":51805},{\"end\":54112,\"start\":52713},{\"end\":54640,\"start\":54143},{\"end\":56899,\"start\":54642},{\"end\":57821,\"start\":56901},{\"end\":57988,\"start\":57857},{\"end\":58115,\"start\":57990},{\"end\":58287,\"start\":58160},{\"end\":58827,\"start\":58289},{\"end\":59800,\"start\":58829},{\"end\":59875,\"start\":59802},{\"end\":60507,\"start\":59877},{\"end\":61577,\"start\":60509},{\"end\":62030,\"start\":61624},{\"end\":62582,\"start\":62032},{\"end\":62977,\"start\":62584},{\"end\":63596,\"start\":62979}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17362,\"start\":17269},{\"attributes\":{\"id\":\"formula_1\"},\"end\":44918,\"start\":44909},{\"attributes\":{\"id\":\"formula_2\"},\"end\":50967,\"start\":50929}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25054,\"start\":25047},{\"end\":28630,\"start\":28623},{\"end\":29563,\"start\":29556},{\"end\":30836,\"start\":30829},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":33303,\"start\":33296},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":41684,\"start\":41677},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42134,\"start\":42127},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":42784,\"start\":42775},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":43431,\"start\":43424},{\"end\":44318,\"start\":44311},{\"end\":46475,\"start\":46468},{\"end\":46857,\"start\":46850},{\"end\":50171,\"start\":50164},{\"end\":50685,\"start\":50678},{\"end\":51342,\"start\":51335},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":54221,\"start\":54214},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":54269,\"start\":54262},{\"end\":57797,\"start\":57790},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":58679,\"start\":58672},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":60084,\"start\":60077},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":62124,\"start\":62117}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1916,\"start\":1904},{\"attributes\":{\"n\":\"2.\"},\"end\":8197,\"start\":8185},{\"end\":8452,\"start\":8436},{\"attributes\":{\"n\":\"3.\"},\"end\":13112,\"start\":13106},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15855,\"start\":15821},{\"attributes\":{\"n\":\"3.2.\"},\"end\":18060,\"start\":18023},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19477,\"start\":19452},{\"attributes\":{\"n\":\"3.4.\"},\"end\":22581,\"start\":22556},{\"attributes\":{\"n\":\"4.\"},\"end\":24208,\"start\":24177},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24884,\"start\":24872},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26425,\"start\":26407},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27850,\"start\":27832},{\"attributes\":{\"n\":\"5.\"},\"end\":28364,\"start\":28353},{\"attributes\":{\"n\":\"5.1.\"},\"end\":29078,\"start\":29061},{\"attributes\":{\"n\":\"5.2.\"},\"end\":30694,\"start\":30686},{\"attributes\":{\"n\":\"5.3.\"},\"end\":32610,\"start\":32595},{\"attributes\":{\"n\":\"5.4.\"},\"end\":34008,\"start\":33980},{\"attributes\":{\"n\":\"6.\"},\"end\":36170,\"start\":36159},{\"end\":36807,\"start\":36799},{\"end\":36827,\"start\":36810},{\"end\":38490,\"start\":38475},{\"end\":38822,\"start\":38799},{\"end\":40197,\"start\":40174},{\"end\":41550,\"start\":41518},{\"end\":42368,\"start\":42343},{\"end\":43565,\"start\":43555},{\"end\":44969,\"start\":44920},{\"end\":45025,\"start\":44972},{\"end\":46108,\"start\":46088},{\"end\":47496,\"start\":47468},{\"end\":48038,\"start\":48015},{\"end\":50997,\"start\":50969},{\"end\":51803,\"start\":51748},{\"end\":54141,\"start\":54115},{\"end\":57855,\"start\":57824},{\"end\":58158,\"start\":58118},{\"end\":61622,\"start\":61580},{\"end\":63608,\"start\":63598},{\"end\":64476,\"start\":64466},{\"end\":66769,\"start\":66759},{\"end\":66940,\"start\":66930},{\"end\":67529,\"start\":67519},{\"end\":67849,\"start\":67839},{\"end\":68630,\"start\":68627},{\"end\":69069,\"start\":69059},{\"end\":69581,\"start\":69571},{\"end\":69987,\"start\":69976},{\"end\":70055,\"start\":70046},{\"end\":72628,\"start\":72619},{\"end\":75263,\"start\":75254},{\"end\":75824,\"start\":75819},{\"end\":75836,\"start\":75827},{\"end\":76402,\"start\":76393}]", "table": "[{\"end\":70475,\"start\":70117},{\"end\":72617,\"start\":71569},{\"end\":75252,\"start\":74680},{\"end\":75817,\"start\":75686}]", "figure_caption": "[{\"end\":64464,\"start\":63610},{\"end\":64768,\"start\":64478},{\"end\":65398,\"start\":64771},{\"end\":66757,\"start\":65401},{\"end\":66928,\"start\":66771},{\"end\":67517,\"start\":66942},{\"end\":67611,\"start\":67531},{\"end\":67837,\"start\":67614},{\"end\":68625,\"start\":67851},{\"end\":69057,\"start\":68632},{\"end\":69569,\"start\":69071},{\"end\":69974,\"start\":69583},{\"end\":70044,\"start\":69990},{\"end\":70117,\"start\":70057},{\"end\":71569,\"start\":70478},{\"end\":73183,\"start\":72630},{\"end\":74680,\"start\":73186},{\"end\":75686,\"start\":75265},{\"end\":76391,\"start\":75838},{\"end\":76485,\"start\":76404}]", "figure_ref": "[{\"end\":2503,\"start\":2495},{\"end\":3996,\"start\":3988},{\"end\":4813,\"start\":4805},{\"end\":9708,\"start\":9692},{\"end\":9862,\"start\":9853},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14961,\"start\":14953},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25076,\"start\":25068},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25473,\"start\":25458},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26760,\"start\":26752},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30256,\"start\":30248},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30682,\"start\":30674},{\"end\":34166,\"start\":34157},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":34602,\"start\":34592},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34991,\"start\":34982},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35029,\"start\":35019},{\"end\":35530,\"start\":35521},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35772,\"start\":35755},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35877,\"start\":35867},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42045,\"start\":42037},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":48226,\"start\":48218},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":52368,\"start\":52360},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":52735,\"start\":52727},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":58320,\"start\":58312},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":60634,\"start\":60626},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":62550,\"start\":62541},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":62816,\"start\":62799},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":63302,\"start\":63284}]", "bib_author_first_name": "[{\"end\":76917,\"start\":76913},{\"end\":76932,\"start\":76925},{\"end\":76951,\"start\":76942},{\"end\":76969,\"start\":76963},{\"end\":77293,\"start\":77292},{\"end\":77301,\"start\":77300},{\"end\":77309,\"start\":77308},{\"end\":77319,\"start\":77318},{\"end\":77326,\"start\":77325},{\"end\":77336,\"start\":77335},{\"end\":77345,\"start\":77344},{\"end\":77360,\"start\":77355},{\"end\":77373,\"start\":77368},{\"end\":77387,\"start\":77382},{\"end\":77389,\"start\":77388},{\"end\":77399,\"start\":77398},{\"end\":77401,\"start\":77400},{\"end\":77417,\"start\":77411},{\"end\":77432,\"start\":77426},{\"end\":77447,\"start\":77440},{\"end\":77926,\"start\":77923},{\"end\":77938,\"start\":77933},{\"end\":78203,\"start\":78197},{\"end\":78219,\"start\":78211},{\"end\":78235,\"start\":78229},{\"end\":78472,\"start\":78471},{\"end\":78490,\"start\":78481},{\"end\":78492,\"start\":78491},{\"end\":78504,\"start\":78499},{\"end\":78518,\"start\":78511},{\"end\":78532,\"start\":78528},{\"end\":78548,\"start\":78540},{\"end\":78560,\"start\":78556},{\"end\":78573,\"start\":78566},{\"end\":78838,\"start\":78837},{\"end\":78852,\"start\":78851},{\"end\":78860,\"start\":78859},{\"end\":78870,\"start\":78869},{\"end\":78879,\"start\":78878},{\"end\":78889,\"start\":78888},{\"end\":79125,\"start\":79119},{\"end\":79140,\"start\":79133},{\"end\":79153,\"start\":79147},{\"end\":79385,\"start\":79379},{\"end\":79398,\"start\":79393},{\"end\":79409,\"start\":79404},{\"end\":79428,\"start\":79422},{\"end\":79754,\"start\":79747},{\"end\":79764,\"start\":79760},{\"end\":79979,\"start\":79974},{\"end\":79987,\"start\":79985},{\"end\":79999,\"start\":79994},{\"end\":80006,\"start\":80005},{\"end\":80020,\"start\":80014},{\"end\":80391,\"start\":80384},{\"end\":80416,\"start\":80408},{\"end\":80428,\"start\":80422},{\"end\":80437,\"start\":80434},{\"end\":80441,\"start\":80438},{\"end\":80453,\"start\":80447},{\"end\":80466,\"start\":80459},{\"end\":80481,\"start\":80473},{\"end\":80495,\"start\":80487},{\"end\":80509,\"start\":80502},{\"end\":80795,\"start\":80790},{\"end\":80818,\"start\":80812},{\"end\":81038,\"start\":81033},{\"end\":81071,\"start\":81063},{\"end\":81315,\"start\":81307},{\"end\":81330,\"start\":81322},{\"end\":81342,\"start\":81337},{\"end\":81354,\"start\":81351},{\"end\":81584,\"start\":81583},{\"end\":81592,\"start\":81591},{\"end\":81600,\"start\":81599},{\"end\":81609,\"start\":81608},{\"end\":81843,\"start\":81837},{\"end\":82023,\"start\":82015},{\"end\":82040,\"start\":82035},{\"end\":82058,\"start\":82049},{\"end\":82262,\"start\":82258},{\"end\":82280,\"start\":82275},{\"end\":82294,\"start\":82288},{\"end\":82606,\"start\":82602},{\"end\":82624,\"start\":82619},{\"end\":82638,\"start\":82632},{\"end\":82883,\"start\":82877},{\"end\":82885,\"start\":82884},{\"end\":82904,\"start\":82898},{\"end\":83141,\"start\":83137},{\"end\":83158,\"start\":83150},{\"end\":83172,\"start\":83168},{\"end\":83493,\"start\":83485},{\"end\":83511,\"start\":83506},{\"end\":83529,\"start\":83521},{\"end\":83752,\"start\":83745},{\"end\":83761,\"start\":83758},{\"end\":83775,\"start\":83769},{\"end\":83788,\"start\":83780},{\"end\":83801,\"start\":83793},{\"end\":84036,\"start\":84024},{\"end\":84055,\"start\":84047},{\"end\":84069,\"start\":84065},{\"end\":84086,\"start\":84080},{\"end\":84331,\"start\":84324},{\"end\":84343,\"start\":84336},{\"end\":84359,\"start\":84351},{\"end\":84369,\"start\":84365},{\"end\":84569,\"start\":84562},{\"end\":84581,\"start\":84574},{\"end\":84597,\"start\":84589},{\"end\":84607,\"start\":84603},{\"end\":84820,\"start\":84817},{\"end\":84830,\"start\":84825},{\"end\":84841,\"start\":84836},{\"end\":84857,\"start\":84846},{\"end\":85079,\"start\":85072},{\"end\":85095,\"start\":85084},{\"end\":85109,\"start\":85102},{\"end\":85125,\"start\":85115},{\"end\":85136,\"start\":85131},{\"end\":85414,\"start\":85410},{\"end\":85427,\"start\":85421},{\"end\":85664,\"start\":85661},{\"end\":85671,\"start\":85669},{\"end\":85682,\"start\":85678},{\"end\":85876,\"start\":85869},{\"end\":85890,\"start\":85884},{\"end\":85901,\"start\":85896},{\"end\":86137,\"start\":86130},{\"end\":86147,\"start\":86145},{\"end\":86159,\"start\":86155},{\"end\":86170,\"start\":86165},{\"end\":86182,\"start\":86177},{\"end\":86425,\"start\":86419},{\"end\":86441,\"start\":86435},{\"end\":86443,\"start\":86442},{\"end\":86728,\"start\":86720},{\"end\":86738,\"start\":86734},{\"end\":86748,\"start\":86743},{\"end\":86763,\"start\":86759},{\"end\":87026,\"start\":87019},{\"end\":87047,\"start\":87042},{\"end\":87063,\"start\":87058},{\"end\":87295,\"start\":87290},{\"end\":87314,\"start\":87308},{\"end\":87564,\"start\":87563},{\"end\":87577,\"start\":87576},{\"end\":87589,\"start\":87588},{\"end\":87597,\"start\":87596},{\"end\":88048,\"start\":88044},{\"end\":88066,\"start\":88058},{\"end\":88080,\"start\":88076},{\"end\":88329,\"start\":88326},{\"end\":88337,\"start\":88334},{\"end\":88348,\"start\":88344},{\"end\":88363,\"start\":88355},{\"end\":88592,\"start\":88586},{\"end\":88607,\"start\":88598},{\"end\":88620,\"start\":88614},{\"end\":88622,\"start\":88621},{\"end\":88637,\"start\":88633},{\"end\":88863,\"start\":88862},{\"end\":89085,\"start\":89084},{\"end\":89108,\"start\":89099},{\"end\":89125,\"start\":89118},{\"end\":89380,\"start\":89379},{\"end\":89382,\"start\":89381},{\"end\":89399,\"start\":89398},{\"end\":89413,\"start\":89412},{\"end\":89430,\"start\":89429},{\"end\":89679,\"start\":89678},{\"end\":89681,\"start\":89680},{\"end\":89698,\"start\":89697},{\"end\":89712,\"start\":89711},{\"end\":89729,\"start\":89728},{\"end\":89979,\"start\":89974},{\"end\":89995,\"start\":89989},{\"end\":90011,\"start\":90005},{\"end\":90246,\"start\":90241},{\"end\":90262,\"start\":90256},{\"end\":90278,\"start\":90272},{\"end\":90555,\"start\":90550},{\"end\":90571,\"start\":90565},{\"end\":90830,\"start\":90823},{\"end\":90843,\"start\":90837},{\"end\":91005,\"start\":90999},{\"end\":91024,\"start\":91014},{\"end\":91041,\"start\":91036},{\"end\":91061,\"start\":91054},{\"end\":91310,\"start\":91309},{\"end\":91321,\"start\":91318},{\"end\":91336,\"start\":91330},{\"end\":91802,\"start\":91796},{\"end\":91818,\"start\":91813},{\"end\":91833,\"start\":91827},{\"end\":92078,\"start\":92074},{\"end\":92093,\"start\":92088},{\"end\":92105,\"start\":92101},{\"end\":92333,\"start\":92326},{\"end\":92352,\"start\":92346},{\"end\":92366,\"start\":92361},{\"end\":92376,\"start\":92374},{\"end\":92636,\"start\":92632},{\"end\":92649,\"start\":92642},{\"end\":92656,\"start\":92654},{\"end\":92671,\"start\":92664},{\"end\":92683,\"start\":92676},{\"end\":92696,\"start\":92691},{\"end\":92708,\"start\":92703},{\"end\":92981,\"start\":92974},{\"end\":92999,\"start\":92989},{\"end\":93170,\"start\":93165},{\"end\":93192,\"start\":93187},{\"end\":93207,\"start\":93201},{\"end\":93441,\"start\":93434},{\"end\":93454,\"start\":93447},{\"end\":93472,\"start\":93467},{\"end\":93487,\"start\":93481},{\"end\":93736,\"start\":93730},{\"end\":93751,\"start\":93746},{\"end\":93766,\"start\":93760},{\"end\":94045,\"start\":94040},{\"end\":94062,\"start\":94054},{\"end\":94078,\"start\":94072},{\"end\":94488,\"start\":94483},{\"end\":94505,\"start\":94497},{\"end\":94521,\"start\":94515},{\"end\":94915,\"start\":94910},{\"end\":94932,\"start\":94924},{\"end\":94948,\"start\":94942},{\"end\":95360,\"start\":95356},{\"end\":95372,\"start\":95366},{\"end\":95386,\"start\":95380},{\"end\":95609,\"start\":95604},{\"end\":95621,\"start\":95617},{\"end\":95640,\"start\":95634},{\"end\":95866,\"start\":95861},{\"end\":95875,\"start\":95874},{\"end\":95881,\"start\":95876},{\"end\":95895,\"start\":95891},{\"end\":96142,\"start\":96135},{\"end\":96162,\"start\":96157},{\"end\":96180,\"start\":96171},{\"end\":96498,\"start\":96490},{\"end\":96514,\"start\":96508},{\"end\":96528,\"start\":96522},{\"end\":96790,\"start\":96782},{\"end\":96806,\"start\":96800},{\"end\":96820,\"start\":96814},{\"end\":97058,\"start\":97050},{\"end\":97069,\"start\":97068},{\"end\":97081,\"start\":97076},{\"end\":97343,\"start\":97335},{\"end\":97354,\"start\":97353},{\"end\":97365,\"start\":97362},{\"end\":97378,\"start\":97374},{\"end\":97392,\"start\":97386},{\"end\":97411,\"start\":97405},{\"end\":97672,\"start\":97667},{\"end\":97703,\"start\":97694},{\"end\":97721,\"start\":97717},{\"end\":97737,\"start\":97734}]", "bib_author_last_name": "[{\"end\":76923,\"start\":76918},{\"end\":76940,\"start\":76933},{\"end\":76961,\"start\":76952},{\"end\":76977,\"start\":76970},{\"end\":77298,\"start\":77294},{\"end\":77306,\"start\":77302},{\"end\":77316,\"start\":77310},{\"end\":77323,\"start\":77320},{\"end\":77333,\"start\":77327},{\"end\":77342,\"start\":77337},{\"end\":77353,\"start\":77346},{\"end\":77366,\"start\":77361},{\"end\":77380,\"start\":77374},{\"end\":77396,\"start\":77390},{\"end\":77409,\"start\":77402},{\"end\":77424,\"start\":77418},{\"end\":77438,\"start\":77433},{\"end\":77454,\"start\":77448},{\"end\":77931,\"start\":77927},{\"end\":77946,\"start\":77939},{\"end\":78209,\"start\":78204},{\"end\":78227,\"start\":78220},{\"end\":78248,\"start\":78236},{\"end\":78479,\"start\":78473},{\"end\":78497,\"start\":78493},{\"end\":78509,\"start\":78505},{\"end\":78526,\"start\":78519},{\"end\":78538,\"start\":78533},{\"end\":78554,\"start\":78549},{\"end\":78564,\"start\":78561},{\"end\":78588,\"start\":78574},{\"end\":78597,\"start\":78590},{\"end\":78849,\"start\":78839},{\"end\":78857,\"start\":78853},{\"end\":78867,\"start\":78861},{\"end\":78876,\"start\":78871},{\"end\":78886,\"start\":78880},{\"end\":78895,\"start\":78890},{\"end\":79131,\"start\":79126},{\"end\":79145,\"start\":79141},{\"end\":79163,\"start\":79154},{\"end\":79391,\"start\":79386},{\"end\":79402,\"start\":79399},{\"end\":79420,\"start\":79410},{\"end\":79438,\"start\":79429},{\"end\":79758,\"start\":79755},{\"end\":79776,\"start\":79765},{\"end\":79983,\"start\":79980},{\"end\":79992,\"start\":79988},{\"end\":80003,\"start\":80000},{\"end\":80012,\"start\":80007},{\"end\":80027,\"start\":80021},{\"end\":80038,\"start\":80029},{\"end\":80406,\"start\":80392},{\"end\":80420,\"start\":80417},{\"end\":80432,\"start\":80429},{\"end\":80445,\"start\":80442},{\"end\":80457,\"start\":80454},{\"end\":80471,\"start\":80467},{\"end\":80485,\"start\":80482},{\"end\":80500,\"start\":80496},{\"end\":80513,\"start\":80510},{\"end\":80518,\"start\":80515},{\"end\":80810,\"start\":80796},{\"end\":80826,\"start\":80819},{\"end\":80837,\"start\":80828},{\"end\":81061,\"start\":81039},{\"end\":81079,\"start\":81072},{\"end\":81087,\"start\":81081},{\"end\":81320,\"start\":81316},{\"end\":81335,\"start\":81331},{\"end\":81349,\"start\":81343},{\"end\":81361,\"start\":81355},{\"end\":81589,\"start\":81585},{\"end\":81597,\"start\":81593},{\"end\":81606,\"start\":81601},{\"end\":81616,\"start\":81610},{\"end\":81862,\"start\":81844},{\"end\":82033,\"start\":82024},{\"end\":82047,\"start\":82041},{\"end\":82064,\"start\":82059},{\"end\":82273,\"start\":82263},{\"end\":82286,\"start\":82281},{\"end\":82304,\"start\":82295},{\"end\":82617,\"start\":82607},{\"end\":82630,\"start\":82625},{\"end\":82648,\"start\":82639},{\"end\":82896,\"start\":82886},{\"end\":82914,\"start\":82905},{\"end\":83148,\"start\":83142},{\"end\":83166,\"start\":83159},{\"end\":83181,\"start\":83173},{\"end\":83504,\"start\":83494},{\"end\":83519,\"start\":83512},{\"end\":83536,\"start\":83530},{\"end\":83756,\"start\":83753},{\"end\":83767,\"start\":83762},{\"end\":83778,\"start\":83776},{\"end\":83791,\"start\":83789},{\"end\":83805,\"start\":83802},{\"end\":84045,\"start\":84037},{\"end\":84063,\"start\":84056},{\"end\":84078,\"start\":84070},{\"end\":84099,\"start\":84087},{\"end\":84334,\"start\":84332},{\"end\":84349,\"start\":84344},{\"end\":84363,\"start\":84360},{\"end\":84373,\"start\":84370},{\"end\":84572,\"start\":84570},{\"end\":84587,\"start\":84582},{\"end\":84601,\"start\":84598},{\"end\":84611,\"start\":84608},{\"end\":84823,\"start\":84821},{\"end\":84834,\"start\":84831},{\"end\":84844,\"start\":84842},{\"end\":84861,\"start\":84858},{\"end\":85082,\"start\":85080},{\"end\":85100,\"start\":85096},{\"end\":85113,\"start\":85110},{\"end\":85129,\"start\":85126},{\"end\":85145,\"start\":85137},{\"end\":85419,\"start\":85415},{\"end\":85437,\"start\":85428},{\"end\":85667,\"start\":85665},{\"end\":85676,\"start\":85672},{\"end\":85686,\"start\":85683},{\"end\":85882,\"start\":85877},{\"end\":85894,\"start\":85891},{\"end\":85905,\"start\":85902},{\"end\":86143,\"start\":86138},{\"end\":86153,\"start\":86148},{\"end\":86163,\"start\":86160},{\"end\":86175,\"start\":86171},{\"end\":86186,\"start\":86183},{\"end\":86433,\"start\":86426},{\"end\":86450,\"start\":86444},{\"end\":86459,\"start\":86452},{\"end\":86732,\"start\":86729},{\"end\":86741,\"start\":86739},{\"end\":86757,\"start\":86749},{\"end\":86778,\"start\":86764},{\"end\":87040,\"start\":87027},{\"end\":87056,\"start\":87048},{\"end\":87069,\"start\":87064},{\"end\":87078,\"start\":87071},{\"end\":87306,\"start\":87296},{\"end\":87324,\"start\":87315},{\"end\":87574,\"start\":87565},{\"end\":87586,\"start\":87578},{\"end\":87594,\"start\":87590},{\"end\":87605,\"start\":87598},{\"end\":87858,\"start\":87845},{\"end\":88056,\"start\":88049},{\"end\":88074,\"start\":88067},{\"end\":88087,\"start\":88081},{\"end\":88332,\"start\":88330},{\"end\":88342,\"start\":88338},{\"end\":88353,\"start\":88349},{\"end\":88368,\"start\":88364},{\"end\":88596,\"start\":88593},{\"end\":88612,\"start\":88608},{\"end\":88631,\"start\":88623},{\"end\":88647,\"start\":88638},{\"end\":88869,\"start\":88864},{\"end\":88875,\"start\":88871},{\"end\":89097,\"start\":89086},{\"end\":89116,\"start\":89109},{\"end\":89134,\"start\":89126},{\"end\":89143,\"start\":89136},{\"end\":89396,\"start\":89383},{\"end\":89410,\"start\":89400},{\"end\":89427,\"start\":89414},{\"end\":89440,\"start\":89431},{\"end\":89695,\"start\":89682},{\"end\":89709,\"start\":89699},{\"end\":89726,\"start\":89713},{\"end\":89739,\"start\":89730},{\"end\":89987,\"start\":89980},{\"end\":90003,\"start\":89996},{\"end\":90021,\"start\":90012},{\"end\":90254,\"start\":90247},{\"end\":90270,\"start\":90263},{\"end\":90288,\"start\":90279},{\"end\":90563,\"start\":90556},{\"end\":90581,\"start\":90572},{\"end\":90835,\"start\":90831},{\"end\":90848,\"start\":90844},{\"end\":90854,\"start\":90850},{\"end\":91012,\"start\":91006},{\"end\":91034,\"start\":91025},{\"end\":91052,\"start\":91042},{\"end\":91067,\"start\":91062},{\"end\":91316,\"start\":91311},{\"end\":91328,\"start\":91322},{\"end\":91342,\"start\":91337},{\"end\":91353,\"start\":91344},{\"end\":91811,\"start\":91803},{\"end\":91825,\"start\":91819},{\"end\":91840,\"start\":91834},{\"end\":92086,\"start\":92079},{\"end\":92099,\"start\":92094},{\"end\":92112,\"start\":92106},{\"end\":92344,\"start\":92334},{\"end\":92359,\"start\":92353},{\"end\":92372,\"start\":92367},{\"end\":92384,\"start\":92377},{\"end\":92640,\"start\":92637},{\"end\":92652,\"start\":92650},{\"end\":92662,\"start\":92657},{\"end\":92674,\"start\":92672},{\"end\":92689,\"start\":92684},{\"end\":92701,\"start\":92697},{\"end\":92712,\"start\":92709},{\"end\":92987,\"start\":92982},{\"end\":93009,\"start\":93000},{\"end\":93185,\"start\":93171},{\"end\":93199,\"start\":93193},{\"end\":93220,\"start\":93208},{\"end\":93445,\"start\":93442},{\"end\":93465,\"start\":93455},{\"end\":93479,\"start\":93473},{\"end\":93494,\"start\":93488},{\"end\":93744,\"start\":93737},{\"end\":93758,\"start\":93752},{\"end\":93779,\"start\":93767},{\"end\":94052,\"start\":94046},{\"end\":94070,\"start\":94063},{\"end\":94093,\"start\":94079},{\"end\":94107,\"start\":94095},{\"end\":94495,\"start\":94489},{\"end\":94513,\"start\":94506},{\"end\":94536,\"start\":94522},{\"end\":94550,\"start\":94538},{\"end\":94922,\"start\":94916},{\"end\":94940,\"start\":94933},{\"end\":94963,\"start\":94949},{\"end\":94977,\"start\":94965},{\"end\":95364,\"start\":95361},{\"end\":95378,\"start\":95373},{\"end\":95394,\"start\":95387},{\"end\":95615,\"start\":95610},{\"end\":95632,\"start\":95622},{\"end\":95650,\"start\":95641},{\"end\":95872,\"start\":95867},{\"end\":95889,\"start\":95882},{\"end\":95904,\"start\":95896},{\"end\":96155,\"start\":96143},{\"end\":96169,\"start\":96163},{\"end\":96190,\"start\":96181},{\"end\":96506,\"start\":96499},{\"end\":96520,\"start\":96515},{\"end\":96541,\"start\":96529},{\"end\":96798,\"start\":96791},{\"end\":96812,\"start\":96807},{\"end\":96833,\"start\":96821},{\"end\":97066,\"start\":97059},{\"end\":97074,\"start\":97070},{\"end\":97085,\"start\":97082},{\"end\":97093,\"start\":97087},{\"end\":97351,\"start\":97344},{\"end\":97360,\"start\":97355},{\"end\":97372,\"start\":97366},{\"end\":97384,\"start\":97379},{\"end\":97403,\"start\":97393},{\"end\":97424,\"start\":97412},{\"end\":97435,\"start\":97426},{\"end\":97692,\"start\":97673},{\"end\":97715,\"start\":97704},{\"end\":97732,\"start\":97722},{\"end\":97747,\"start\":97738},{\"end\":97757,\"start\":97749},{\"end\":98058,\"start\":98055}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":77146,\"start\":76828},{\"attributes\":{\"doi\":\"abs/2009.09984\",\"id\":\"b1\"},\"end\":77817,\"start\":77148},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218571391},\"end\":78107,\"start\":77819},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5707100},\"end\":78440,\"start\":78109},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17113597},\"end\":78797,\"start\":78442},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":147764},\"end\":79045,\"start\":78799},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":231861751},\"end\":79311,\"start\":79047},{\"attributes\":{\"doi\":\"2020. 13\",\"id\":\"b7\",\"matched_paper_id\":220713885},\"end\":79686,\"start\":79313},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206596979},\"end\":79909,\"start\":79688},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":216009},\"end\":80327,\"start\":79911},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":214667019},\"end\":80751,\"start\":80329},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49211906},\"end\":80965,\"start\":80753},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10050369},\"end\":81263,\"start\":80967},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5339134},\"end\":81503,\"start\":81265},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14302159},\"end\":81779,\"start\":81505},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14815971},\"end\":81971,\"start\":81781},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":950888},\"end\":82204,\"start\":81973},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8300220},\"end\":82505,\"start\":82206},{\"attributes\":{\"id\":\"b18\"},\"end\":82806,\"start\":82507},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17428157},\"end\":83070,\"start\":82808},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15499901},\"end\":83418,\"start\":83072},{\"attributes\":{\"id\":\"b21\"},\"end\":83672,\"start\":83420},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2908606},\"end\":83966,\"start\":83674},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":519278},\"end\":84276,\"start\":83968},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594692},\"end\":84514,\"start\":84278},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594692},\"end\":84753,\"start\":84516},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":19167488},\"end\":84997,\"start\":84755},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2667132},\"end\":85337,\"start\":84999},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15911835},\"end\":85626,\"start\":85339},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":140309863},\"end\":85790,\"start\":85628},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":51864534},\"end\":86074,\"start\":85792},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220665753},\"end\":86347,\"start\":86076},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9540064},\"end\":86634,\"start\":86349},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6238091},\"end\":86967,\"start\":86636},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8123285},\"end\":87233,\"start\":86969},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":221340370},\"end\":87485,\"start\":87235},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6344950},\"end\":87758,\"start\":87487},{\"attributes\":{\"id\":\"b37\"},\"end\":87976,\"start\":87760},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":214714312},\"end\":88249,\"start\":87978},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":938105},\"end\":88536,\"start\":88251},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52832041},\"end\":88802,\"start\":88538},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":221242327},\"end\":89043,\"start\":88804},{\"attributes\":{\"id\":\"b42\"},\"end\":89316,\"start\":89045},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":150377210},\"end\":89613,\"start\":89318},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":150377210},\"end\":89912,\"start\":89615},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":13746939},\"end\":90174,\"start\":89914},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4559198},\"end\":90446,\"start\":90176},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":35411938},\"end\":90779,\"start\":90448},{\"attributes\":{\"id\":\"b48\"},\"end\":90960,\"start\":90781},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1466704},\"end\":91239,\"start\":90962},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":35403732},\"end\":91677,\"start\":91241},{\"attributes\":{\"id\":\"b51\"},\"end\":92018,\"start\":91679},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2210504},\"end\":92250,\"start\":92020},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":8335083},\"end\":92562,\"start\":92252},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":214802984},\"end\":92908,\"start\":92564},{\"attributes\":{\"id\":\"b55\"},\"end\":93122,\"start\":92910},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":204837441},\"end\":93372,\"start\":93124},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":8994087},\"end\":93660,\"start\":93374},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":67856748},\"end\":93962,\"start\":93662},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":67855496},\"end\":94414,\"start\":93964},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":208093762},\"end\":94834,\"start\":94416},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":214802598},\"end\":95279,\"start\":94836},{\"attributes\":{\"doi\":\"2021. 13\",\"id\":\"b62\"},\"end\":95538,\"start\":95281},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":2934173},\"end\":95807,\"start\":95540},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":6049714},\"end\":96046,\"start\":95809},{\"attributes\":{\"doi\":\"arXiv:2008.11289\",\"id\":\"b65\"},\"end\":96412,\"start\":96048},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":14037774},\"end\":96717,\"start\":96414},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":1055956},\"end\":96993,\"start\":96719},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":199528244},\"end\":97260,\"start\":96995},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":13890308},\"end\":97665,\"start\":97262},{\"attributes\":{\"doi\":\"arXiv:2005.12320\",\"id\":\"b70\"},\"end\":98020,\"start\":97667},{\"attributes\":{\"id\":\"b71\"},\"end\":98150,\"start\":98022}]", "bib_title": "[{\"end\":77921,\"start\":77819},{\"end\":78195,\"start\":78109},{\"end\":78469,\"start\":78442},{\"end\":78835,\"start\":78799},{\"end\":79117,\"start\":79047},{\"end\":79377,\"start\":79313},{\"end\":79745,\"start\":79688},{\"end\":79972,\"start\":79911},{\"end\":80382,\"start\":80329},{\"end\":80788,\"start\":80753},{\"end\":81031,\"start\":80967},{\"end\":81305,\"start\":81265},{\"end\":81581,\"start\":81505},{\"end\":81835,\"start\":81781},{\"end\":82013,\"start\":81973},{\"end\":82256,\"start\":82206},{\"end\":82875,\"start\":82808},{\"end\":83135,\"start\":83072},{\"end\":83743,\"start\":83674},{\"end\":84022,\"start\":83968},{\"end\":84322,\"start\":84278},{\"end\":84560,\"start\":84516},{\"end\":84815,\"start\":84755},{\"end\":85070,\"start\":84999},{\"end\":85408,\"start\":85339},{\"end\":85659,\"start\":85628},{\"end\":85867,\"start\":85792},{\"end\":86128,\"start\":86076},{\"end\":86417,\"start\":86349},{\"end\":86718,\"start\":86636},{\"end\":87017,\"start\":86969},{\"end\":87288,\"start\":87235},{\"end\":87561,\"start\":87487},{\"end\":88042,\"start\":87978},{\"end\":88324,\"start\":88251},{\"end\":88584,\"start\":88538},{\"end\":88860,\"start\":88804},{\"end\":89377,\"start\":89318},{\"end\":89676,\"start\":89615},{\"end\":89972,\"start\":89914},{\"end\":90239,\"start\":90176},{\"end\":90548,\"start\":90448},{\"end\":90997,\"start\":90962},{\"end\":91307,\"start\":91241},{\"end\":92072,\"start\":92020},{\"end\":92324,\"start\":92252},{\"end\":92630,\"start\":92564},{\"end\":93163,\"start\":93124},{\"end\":93432,\"start\":93374},{\"end\":93728,\"start\":93662},{\"end\":94038,\"start\":93964},{\"end\":94481,\"start\":94416},{\"end\":94908,\"start\":94836},{\"end\":95602,\"start\":95540},{\"end\":95859,\"start\":95809},{\"end\":96488,\"start\":96414},{\"end\":96780,\"start\":96719},{\"end\":97048,\"start\":96995},{\"end\":97333,\"start\":97262}]", "bib_author": "[{\"end\":76925,\"start\":76913},{\"end\":76942,\"start\":76925},{\"end\":76963,\"start\":76942},{\"end\":76979,\"start\":76963},{\"end\":77300,\"start\":77292},{\"end\":77308,\"start\":77300},{\"end\":77318,\"start\":77308},{\"end\":77325,\"start\":77318},{\"end\":77335,\"start\":77325},{\"end\":77344,\"start\":77335},{\"end\":77355,\"start\":77344},{\"end\":77368,\"start\":77355},{\"end\":77382,\"start\":77368},{\"end\":77398,\"start\":77382},{\"end\":77411,\"start\":77398},{\"end\":77426,\"start\":77411},{\"end\":77440,\"start\":77426},{\"end\":77456,\"start\":77440},{\"end\":77933,\"start\":77923},{\"end\":77948,\"start\":77933},{\"end\":78211,\"start\":78197},{\"end\":78229,\"start\":78211},{\"end\":78250,\"start\":78229},{\"end\":78481,\"start\":78471},{\"end\":78499,\"start\":78481},{\"end\":78511,\"start\":78499},{\"end\":78528,\"start\":78511},{\"end\":78540,\"start\":78528},{\"end\":78556,\"start\":78540},{\"end\":78566,\"start\":78556},{\"end\":78590,\"start\":78566},{\"end\":78599,\"start\":78590},{\"end\":78851,\"start\":78837},{\"end\":78859,\"start\":78851},{\"end\":78869,\"start\":78859},{\"end\":78878,\"start\":78869},{\"end\":78888,\"start\":78878},{\"end\":78897,\"start\":78888},{\"end\":79133,\"start\":79119},{\"end\":79147,\"start\":79133},{\"end\":79165,\"start\":79147},{\"end\":79393,\"start\":79379},{\"end\":79404,\"start\":79393},{\"end\":79422,\"start\":79404},{\"end\":79440,\"start\":79422},{\"end\":79760,\"start\":79747},{\"end\":79778,\"start\":79760},{\"end\":79985,\"start\":79974},{\"end\":79994,\"start\":79985},{\"end\":80005,\"start\":79994},{\"end\":80014,\"start\":80005},{\"end\":80029,\"start\":80014},{\"end\":80040,\"start\":80029},{\"end\":80408,\"start\":80384},{\"end\":80422,\"start\":80408},{\"end\":80434,\"start\":80422},{\"end\":80447,\"start\":80434},{\"end\":80459,\"start\":80447},{\"end\":80473,\"start\":80459},{\"end\":80487,\"start\":80473},{\"end\":80502,\"start\":80487},{\"end\":80515,\"start\":80502},{\"end\":80520,\"start\":80515},{\"end\":80812,\"start\":80790},{\"end\":80828,\"start\":80812},{\"end\":80839,\"start\":80828},{\"end\":81063,\"start\":81033},{\"end\":81081,\"start\":81063},{\"end\":81089,\"start\":81081},{\"end\":81322,\"start\":81307},{\"end\":81337,\"start\":81322},{\"end\":81351,\"start\":81337},{\"end\":81363,\"start\":81351},{\"end\":81591,\"start\":81583},{\"end\":81599,\"start\":81591},{\"end\":81608,\"start\":81599},{\"end\":81618,\"start\":81608},{\"end\":81864,\"start\":81837},{\"end\":82035,\"start\":82015},{\"end\":82049,\"start\":82035},{\"end\":82066,\"start\":82049},{\"end\":82275,\"start\":82258},{\"end\":82288,\"start\":82275},{\"end\":82306,\"start\":82288},{\"end\":82619,\"start\":82602},{\"end\":82632,\"start\":82619},{\"end\":82650,\"start\":82632},{\"end\":82898,\"start\":82877},{\"end\":82916,\"start\":82898},{\"end\":83150,\"start\":83137},{\"end\":83168,\"start\":83150},{\"end\":83183,\"start\":83168},{\"end\":83506,\"start\":83485},{\"end\":83521,\"start\":83506},{\"end\":83538,\"start\":83521},{\"end\":83758,\"start\":83745},{\"end\":83769,\"start\":83758},{\"end\":83780,\"start\":83769},{\"end\":83793,\"start\":83780},{\"end\":83807,\"start\":83793},{\"end\":84047,\"start\":84024},{\"end\":84065,\"start\":84047},{\"end\":84080,\"start\":84065},{\"end\":84101,\"start\":84080},{\"end\":84336,\"start\":84324},{\"end\":84351,\"start\":84336},{\"end\":84365,\"start\":84351},{\"end\":84375,\"start\":84365},{\"end\":84574,\"start\":84562},{\"end\":84589,\"start\":84574},{\"end\":84603,\"start\":84589},{\"end\":84613,\"start\":84603},{\"end\":84825,\"start\":84817},{\"end\":84836,\"start\":84825},{\"end\":84846,\"start\":84836},{\"end\":84863,\"start\":84846},{\"end\":85084,\"start\":85072},{\"end\":85102,\"start\":85084},{\"end\":85115,\"start\":85102},{\"end\":85131,\"start\":85115},{\"end\":85147,\"start\":85131},{\"end\":85421,\"start\":85410},{\"end\":85439,\"start\":85421},{\"end\":85669,\"start\":85661},{\"end\":85678,\"start\":85669},{\"end\":85688,\"start\":85678},{\"end\":85884,\"start\":85869},{\"end\":85896,\"start\":85884},{\"end\":85907,\"start\":85896},{\"end\":86145,\"start\":86130},{\"end\":86155,\"start\":86145},{\"end\":86165,\"start\":86155},{\"end\":86177,\"start\":86165},{\"end\":86188,\"start\":86177},{\"end\":86435,\"start\":86419},{\"end\":86452,\"start\":86435},{\"end\":86461,\"start\":86452},{\"end\":86734,\"start\":86720},{\"end\":86743,\"start\":86734},{\"end\":86759,\"start\":86743},{\"end\":86780,\"start\":86759},{\"end\":87042,\"start\":87019},{\"end\":87058,\"start\":87042},{\"end\":87071,\"start\":87058},{\"end\":87080,\"start\":87071},{\"end\":87308,\"start\":87290},{\"end\":87326,\"start\":87308},{\"end\":87576,\"start\":87563},{\"end\":87588,\"start\":87576},{\"end\":87596,\"start\":87588},{\"end\":87607,\"start\":87596},{\"end\":87860,\"start\":87845},{\"end\":88058,\"start\":88044},{\"end\":88076,\"start\":88058},{\"end\":88089,\"start\":88076},{\"end\":88334,\"start\":88326},{\"end\":88344,\"start\":88334},{\"end\":88355,\"start\":88344},{\"end\":88370,\"start\":88355},{\"end\":88598,\"start\":88586},{\"end\":88614,\"start\":88598},{\"end\":88633,\"start\":88614},{\"end\":88649,\"start\":88633},{\"end\":88871,\"start\":88862},{\"end\":88877,\"start\":88871},{\"end\":89099,\"start\":89084},{\"end\":89118,\"start\":89099},{\"end\":89136,\"start\":89118},{\"end\":89145,\"start\":89136},{\"end\":89398,\"start\":89379},{\"end\":89412,\"start\":89398},{\"end\":89429,\"start\":89412},{\"end\":89442,\"start\":89429},{\"end\":89697,\"start\":89678},{\"end\":89711,\"start\":89697},{\"end\":89728,\"start\":89711},{\"end\":89741,\"start\":89728},{\"end\":89989,\"start\":89974},{\"end\":90005,\"start\":89989},{\"end\":90023,\"start\":90005},{\"end\":90256,\"start\":90241},{\"end\":90272,\"start\":90256},{\"end\":90290,\"start\":90272},{\"end\":90565,\"start\":90550},{\"end\":90583,\"start\":90565},{\"end\":90837,\"start\":90823},{\"end\":90850,\"start\":90837},{\"end\":90856,\"start\":90850},{\"end\":91014,\"start\":90999},{\"end\":91036,\"start\":91014},{\"end\":91054,\"start\":91036},{\"end\":91069,\"start\":91054},{\"end\":91318,\"start\":91309},{\"end\":91330,\"start\":91318},{\"end\":91344,\"start\":91330},{\"end\":91355,\"start\":91344},{\"end\":91813,\"start\":91796},{\"end\":91827,\"start\":91813},{\"end\":91842,\"start\":91827},{\"end\":92088,\"start\":92074},{\"end\":92101,\"start\":92088},{\"end\":92114,\"start\":92101},{\"end\":92346,\"start\":92326},{\"end\":92361,\"start\":92346},{\"end\":92374,\"start\":92361},{\"end\":92386,\"start\":92374},{\"end\":92642,\"start\":92632},{\"end\":92654,\"start\":92642},{\"end\":92664,\"start\":92654},{\"end\":92676,\"start\":92664},{\"end\":92691,\"start\":92676},{\"end\":92703,\"start\":92691},{\"end\":92714,\"start\":92703},{\"end\":92989,\"start\":92974},{\"end\":93011,\"start\":92989},{\"end\":93187,\"start\":93165},{\"end\":93201,\"start\":93187},{\"end\":93222,\"start\":93201},{\"end\":93447,\"start\":93434},{\"end\":93467,\"start\":93447},{\"end\":93481,\"start\":93467},{\"end\":93496,\"start\":93481},{\"end\":93746,\"start\":93730},{\"end\":93760,\"start\":93746},{\"end\":93781,\"start\":93760},{\"end\":94054,\"start\":94040},{\"end\":94072,\"start\":94054},{\"end\":94095,\"start\":94072},{\"end\":94109,\"start\":94095},{\"end\":94497,\"start\":94483},{\"end\":94515,\"start\":94497},{\"end\":94538,\"start\":94515},{\"end\":94552,\"start\":94538},{\"end\":94924,\"start\":94910},{\"end\":94942,\"start\":94924},{\"end\":94965,\"start\":94942},{\"end\":94979,\"start\":94965},{\"end\":95366,\"start\":95356},{\"end\":95380,\"start\":95366},{\"end\":95396,\"start\":95380},{\"end\":95617,\"start\":95604},{\"end\":95634,\"start\":95617},{\"end\":95652,\"start\":95634},{\"end\":95874,\"start\":95861},{\"end\":95891,\"start\":95874},{\"end\":95906,\"start\":95891},{\"end\":96157,\"start\":96135},{\"end\":96171,\"start\":96157},{\"end\":96192,\"start\":96171},{\"end\":96508,\"start\":96490},{\"end\":96522,\"start\":96508},{\"end\":96543,\"start\":96522},{\"end\":96800,\"start\":96782},{\"end\":96814,\"start\":96800},{\"end\":96835,\"start\":96814},{\"end\":97068,\"start\":97050},{\"end\":97076,\"start\":97068},{\"end\":97087,\"start\":97076},{\"end\":97095,\"start\":97087},{\"end\":97353,\"start\":97335},{\"end\":97362,\"start\":97353},{\"end\":97374,\"start\":97362},{\"end\":97386,\"start\":97374},{\"end\":97405,\"start\":97386},{\"end\":97426,\"start\":97405},{\"end\":97437,\"start\":97426},{\"end\":97694,\"start\":97667},{\"end\":97717,\"start\":97694},{\"end\":97734,\"start\":97717},{\"end\":97749,\"start\":97734},{\"end\":97759,\"start\":97749},{\"end\":98060,\"start\":98055}]", "bib_venue": "[{\"end\":78266,\"start\":78262},{\"end\":78615,\"start\":78611},{\"end\":78913,\"start\":78909},{\"end\":79794,\"start\":79790},{\"end\":80128,\"start\":80088},{\"end\":81105,\"start\":81101},{\"end\":81379,\"start\":81375},{\"end\":81634,\"start\":81630},{\"end\":82084,\"start\":82079},{\"end\":82322,\"start\":82318},{\"end\":82932,\"start\":82928},{\"end\":84117,\"start\":84113},{\"end\":84391,\"start\":84387},{\"end\":84629,\"start\":84625},{\"end\":85163,\"start\":85159},{\"end\":85704,\"start\":85700},{\"end\":85923,\"start\":85919},{\"end\":86204,\"start\":86200},{\"end\":86796,\"start\":86792},{\"end\":87096,\"start\":87092},{\"end\":87342,\"start\":87338},{\"end\":88105,\"start\":88101},{\"end\":88386,\"start\":88382},{\"end\":88665,\"start\":88661},{\"end\":89458,\"start\":89454},{\"end\":90039,\"start\":90035},{\"end\":90306,\"start\":90302},{\"end\":90599,\"start\":90595},{\"end\":92130,\"start\":92126},{\"end\":92402,\"start\":92398},{\"end\":92730,\"start\":92726},{\"end\":93240,\"start\":93235},{\"end\":93797,\"start\":93793},{\"end\":94197,\"start\":94157},{\"end\":95067,\"start\":95027},{\"end\":95668,\"start\":95664},{\"end\":95922,\"start\":95918},{\"end\":96559,\"start\":96555},{\"end\":96851,\"start\":96847},{\"end\":97111,\"start\":97107},{\"end\":97457,\"start\":97451},{\"end\":76911,\"start\":76828},{\"end\":77290,\"start\":77148},{\"end\":77952,\"start\":77948},{\"end\":78260,\"start\":78250},{\"end\":78609,\"start\":78599},{\"end\":78907,\"start\":78897},{\"end\":79169,\"start\":79165},{\"end\":79493,\"start\":79448},{\"end\":79788,\"start\":79778},{\"end\":80086,\"start\":80040},{\"end\":80531,\"start\":80520},{\"end\":80850,\"start\":80839},{\"end\":81099,\"start\":81089},{\"end\":81373,\"start\":81363},{\"end\":81628,\"start\":81618},{\"end\":81869,\"start\":81864},{\"end\":82077,\"start\":82066},{\"end\":82316,\"start\":82306},{\"end\":82600,\"start\":82507},{\"end\":82926,\"start\":82916},{\"end\":83226,\"start\":83183},{\"end\":83483,\"start\":83420},{\"end\":83811,\"start\":83807},{\"end\":84111,\"start\":84101},{\"end\":84385,\"start\":84375},{\"end\":84623,\"start\":84613},{\"end\":84867,\"start\":84863},{\"end\":85157,\"start\":85147},{\"end\":85474,\"start\":85439},{\"end\":85698,\"start\":85688},{\"end\":85917,\"start\":85907},{\"end\":86198,\"start\":86188},{\"end\":86482,\"start\":86461},{\"end\":86790,\"start\":86780},{\"end\":87090,\"start\":87080},{\"end\":87336,\"start\":87326},{\"end\":87611,\"start\":87607},{\"end\":87843,\"start\":87760},{\"end\":88099,\"start\":88089},{\"end\":88380,\"start\":88370},{\"end\":88659,\"start\":88649},{\"end\":88917,\"start\":88877},{\"end\":89082,\"start\":89045},{\"end\":89452,\"start\":89442},{\"end\":89750,\"start\":89741},{\"end\":90033,\"start\":90023},{\"end\":90300,\"start\":90290},{\"end\":90593,\"start\":90583},{\"end\":90821,\"start\":90781},{\"end\":91091,\"start\":91069},{\"end\":91450,\"start\":91355},{\"end\":91794,\"start\":91679},{\"end\":92124,\"start\":92114},{\"end\":92396,\"start\":92386},{\"end\":92724,\"start\":92714},{\"end\":92972,\"start\":92910},{\"end\":93233,\"start\":93222},{\"end\":93500,\"start\":93496},{\"end\":93791,\"start\":93781},{\"end\":94155,\"start\":94109},{\"end\":94615,\"start\":94552},{\"end\":95025,\"start\":94979},{\"end\":95354,\"start\":95281},{\"end\":95662,\"start\":95652},{\"end\":95916,\"start\":95906},{\"end\":96133,\"start\":96048},{\"end\":96553,\"start\":96543},{\"end\":96845,\"start\":96835},{\"end\":97105,\"start\":97095},{\"end\":97449,\"start\":97437},{\"end\":97817,\"start\":97775},{\"end\":98053,\"start\":98022}]"}}}, "year": 2023, "month": 12, "day": 17}