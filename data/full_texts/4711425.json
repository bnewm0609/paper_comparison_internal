{"id": 4711425, "updated": "2023-09-30 17:54:44.925", "metadata": {"title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification", "authors": "[{\"first\":\"James\",\"last\":\"Thorne\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Vlachos\",\"middle\":[]},{\"first\":\"Christos\",\"last\":\"Christodoulopoulos\",\"middle\":[]},{\"first\":\"Arpit\",\"last\":\"Mittal\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1803.05355", "mag": "2963961878", "acl": "N18-1074", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/ThorneVCM18", "doi": "10.18653/v1/n18-1074"}}, "content": {"source": {"pdf_hash": "824a7e56548d4fba66648280e1b4f8f42e5de4bd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1803.05355v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/N18-1074.pdf", "status": "HYBRID"}}, "grobid": {"id": "f376a55d191e9f5f1e93c68b382a765399b391f2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/824a7e56548d4fba66648280e1b4f8f42e5de4bd.txt", "contents": "\nFEVER: a large-scale dataset for Fact Extraction and VERification\n\n\nJames Thorne j.thorne@sheffield.ac.uk \nDepartment of Computer Science\nUniversity of Sheffield\n\n\nAndreas Vlachos a.vlachos@sheffield.ac.uk \nDepartment of Computer Science\nUniversity of Sheffield\n\n\nChristos Christodoulopoulos \nAmazon Research Cambridge\n\n\nArpit Mittal \nAmazon Research Cambridge\n\n\nFEVER: a large-scale dataset for Fact Extraction and VERification\n\nIn this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.The claims are classified as SUPPORTED, RE-FUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss \u03ba. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.\n\nIntroduction\n\nThe ever-increasing amounts of textual information available combined with the ease in sharing it through the web has increased the demand for verification, also referred to as fact checking. While it has received a lot of attention in the context of journalism, verification is important for other domains, e.g. information in scientific publications, product reviews, etc.\n\nIn this paper we focus on verification of textual claims against textual sources. When compared to textual entailment (TE)/natural language inference (Dagan et al., 2009;Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in verification systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to handle large-scale resources such as Wikipedia (Chen et al., 2017). However, questions typically provide the information needed to identify the answer, while information missing from a claim can often be crucial in retrieving refuting evidence. For example, a claim stating \"Fiji's largest island is Kauai.\" can be refuted by retrieving \"Kauai is the oldest Hawaiian Island.\" as evidence.\n\nProgress on the aforementioned tasks has benefited from the availability of large-scale datasets (Bowman et al., 2015;Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently conducted Fake News Challenge (Pomerleau and Rao, 2017) with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 associated news articles which is orders of magnitude smaller than those used for TE and QA.\n\nIn this paper we present a new dataset for claim verification, FEVER: Fact Extraction and VERification. It consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim (see Figure 1). The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.\n\nTo ensure annotation consistency, we developed suitable guidelines and user interfaces, resulting in inter-annotator agreement of 0.6841 in Fleiss \u03ba (Fleiss, 1971) in claim verification classification, and 95.42% precision and 72.36% recall in evidence retrieval.\n\nTo characterize the challenges posed by FEVER we develop a pipeline approach which, given a claim, first identifies relevant documents, then selects sentences forming the evidence from the documents and finally classifies the claim w.r.t. evidence. The best performing version achieves 31.87% accuracy in verification when requiring correct evidence to be retrieved for claims SUP-PORTED or REFUTED, and 50.91% if the correctness of the evidence is ignored, both indicating the difficulty but also the feasibility of the task. We also conducted oracle experiments in which components of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sentences containing the evidence. In addition to publishing the data via our website 1 , we also publish the annotation interfaces 2 and the baseline system 3 to stimulate further research on verification.\n\n\nRelated Works\n\nVlachos and Riedel (2014) constructed a dataset for claim verification consisting of 106 claims, selecting data from fact-checking websites such as PolitiFact, taking advantage of the labelled claims available there. However, in order to develop claim verification components we typically require the justification for each verdict, including the sources used. While this information is usually available in justifications provided by the journalists, they are not in a machine-readable form. Thus, also considering the small number of claims, the task defined by the dataset proposed Claim: The Rodney King riots took place in the most populous county in the USA.\n\n\n[wiki/Los Angeles Riots]\n\nThe 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.\n\n\n[wiki/Los Angeles County]\n\nLos Angeles County, officially the County of Los Angeles, is the most populous county in the USA.\n\nVerdict: Supported remains too challenging for the ML/NLP methods currently available. Wang (2017) extended this approach by including all 12.8K claims available by Politifact via its API, however the justification and the evidence contained in it was ignored in the experiments as it was not machine-readable. Instead, the claims were classified considering only the text and the metadata related to the person making the claim. While this rendered the task amenable to current NLP/ML methods, it does not allow for verification against any sources and no evidence needs to be returned to justify the verdicts.\n\nThe Fake News challenge (Pomerleau and Rao, 2017) modelled verification as stance classification: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, combining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by journalists in the context of the Emergent Project (Silverman, 2015), and the dataset was first proposed by Ferreira and Vlachos (2016), who only classified the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them.\n\nA differently motivated but closely related dataset is the one developed by Angeli and Manning (2014) to evaluate natural logic inference for common sense reasoning, as it evaluated sim-ple claims such as \"not all birds can fly\" against textual sources -including Wikipedia -which were processed with an Open Information Extraction system (Mausam et al., 2012). However, the claims were small in number (1,378) and limited in variety as they were derived from eight binary ConceptNet relations (Tandon et al., 2011).\n\nClaim verification is also related to the multilingual Answer Validation Exercise (Rodrigo et al., 2009) conducted in the context of the TREC shared tasks. Apart from the difference in dataset size (1,000 instances per language), the key difference is that the claims being validated were answers returned to questions by QA systems. The questions and the QA systems themselves provide additional context to the claim, while in our task definition the claims are outside any particular context. In the same vein, Kobayashi et al. (2017) collected a dataset of 412 statements in context from high-school student exams that were validated against Wikipedia and history textbooks.\n\n\nFact extraction and verification dataset\n\nThe dataset was constructed in two stages:\n\n\nClaim Generation Extracting information from\n\nWikipedia and generating claims from it.\n\nClaim Labeling Classifying whether a claim is supported or refuted by Wikipedia and selecting the evidence for it, or deciding there's not enough information to make a decision.\n\n\nTask 1 -Claim Generation\n\nThe objective of this task was to generate claims from information extracted from Wikipedia. We used the June 2017 Wikipedia dump, processed it with Stanford CoreNLP , and sampled sentences from the introductory sections of approximately 50,000 popular pages. 4 The annotators were given a sentence from the sample chosen at random, and were asked to generate a set of claims containing a single piece of information, focusing on the entity that its original Wikipedia page was about. We asked the annotators to generate claims about a single fact which could be arbitrarily complex and allowed for a variety of expressions for the entities.\n\nIf only the source sentences were used to generate claims then this would result in trivially verifiable claims, as the new claims would in essence be simplifications and paraphrases. At the other extreme, if we allowed world knowledge to be freely incorporated it would result in claims that would be hard to verify on Wikipedia alone. We address this issue by introducing a dictionary: a list of terms that were (hyper-)linked in the original sentence, along with the first sentence from their corresponding Wikipedia pages. Using this dictionary, we provide additional knowledge that can be used to increase the complexity of the generated claims in a controlled manner.\n\nThe annotators were also asked to generate mutations of the claims: altered versions of the original claims, which may or may not change whether they are supported by Wikipedia, or even if they can be verified against it. Inspired by the operators used in Natural Logic Inference (Angeli and Manning, 2014), we specified six types of mutation: paraphrasing, negation, substitution of an entity/relation with a similar/dissimilar one, and making the claim more general/specific. See Appendix A for the full guidelines.\n\nDuring trials of the annotation task, we discovered that the majority of annotators had difficulty generating non-trivial negation mutations (e.g. mutations beyond adding \"not\" to the original). Besides providing numerous examples for each mutation, we also redesigned the annotation interface so that all mutation types were visible at once and highlighted mutations that contained \"not\" in order to discourage trivial negations. Finally, we provided the annotators with an ontology diagram to illustrate the different levels of entity similarity and class membership.\n\nThis process resulted in claims (both extracted and mutated) with a mean length of 9.4 tokens which is comparable to the average hypothesis length of 8.3 tokens in Bowman et al. (2015).\n\n\nTask 2 -Claim Labeling\n\nThe annotators were asked to label each individual claim generated during Task 1 as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two cases, the annotators were asked to find the evidence from any page that supports or refutes the claim (see Figure 2 for a screenshot of the interface). In order to encourage inter-annotator consistency, we gave the following general guideline:\n\nIf I was given only the selected sentences, do I have strong reason to believe the claim is true (supported) or stronger reason to believe the claim is false (refuted). If I'm not certain, what additional information (dictionary) do I have to add to reach this conclusion.\n\nIn the annotation interface, all sentences from the introductory section of the page for the main entity of the claim and of every linked entity in those sentences were provided as a default source of evidence (left-hand side in Fig. 2). Using this interface the annotators recorded the sentences necessary to justify their classification decisions. In order to allow exploration beyond the main and linked pages, we also allowed annotators to add an arbitrary Wikipedia page by providing its URL and the system would add its introductory section as additional sentences that could be then selected as evidence (right-hand side in Fig. 2). The title of the page could also be used as evidence to resolve co-reference, but this decision was not explicitly recorded. We did not set a hard time limit for the task, but the annotators were advised not to spend more than 2-3 minutes per claim. The label NOTENOUGHINFO was used if the claim could not be supported or refuted by any amount of information in Wikipedia (either because it is too general, or too specific).\n\n\nAnnotators\n\nThe annotation team consisted of a total of 50 members, 25 of which were involved only in the first task. All annotators were native US English speakers and were trained either directly by the authors, or by experienced annotators. The interface for both tasks was developed by the authors in collaboration with an initial team of two annotators. Their notes and suggestions were incorporated into the annotation guidelines.\n\nThe majority of the feedback received from the annotators was very positive: they found the task engaging and challenging, and after the initial stages of annotation they had developed an understanding of the needs of the task which let them discuss solutions about edge cases as a group.\n\n\nData Validation\n\nGiven the complexity of the second task (claim labeling), we conducted three forms of data validation: 5-way inter-annotator agreement, agreement against super-annotators (defined in Section 3.4.2), and manual validation by the authors. The validation for claim generation was done im-plicitly during claim labeling. As a result 1.01% of claims were skipped, 2.11% contained typos and 6.63% of the generated claims were flagged as too vague/ambiguous and were excluded e.g. or \"Sons of Anarchy premiered.\".\n\n\n5-way Agreement\n\nWe randomly selected 4% (n = 7506) of claims which were not skipped to be annotated by 5 annotators. We calculated the Fleiss \u03ba score (Fleiss, 1971) to be 0.6841 which we consider encouraging given the complexity of the task. In comparison Bowman et al. (2015) reported a \u03ba of 0.7 for a simpler task, since the annotators were given the premise/evidence to verify a hypothesis against without the additional task of finding it.\n\n\nAgreement against Super-Annotators\n\nWe randomly selected 1% of the data to be annotated by super-annotators: expert annotators with no suggested time restrictions. The purpose of this exercise was to provide as much coverage of evidence as possible. We instructed the superannotators to search over the whole Wikipedia for every possible sentence that could be used as evidence. We compared the regular annotations against this set of evidence and the precision/recall was 95.42% and 72.36% respectively.\n\n\nValidation by the Authors\n\nAs a final quality control step, we chose 227 examples and annotated them for accuracy of the labels and the evidence provided. We found that 91.2% of the examples were annotated correctly. 3% of the claims were mistakes in claim generation that had not been flagged during labeling. We found a similar number of these claims which did not meet the guidelines during a manual error analysis of the baseline system (Section 5.8).\n\n\nFindings\n\nWhen compared against the super-annotators, all except two annotators achieved > 90% precision and all but 9 achieved recall > 70% in evidence retrieval. The majority of the low-recall cases are for claims such as \"Akshay Kumar is an actor.\" where the super-annotator added 34 sentences as evidence, most of them being filmography listings (e.g. \"In 2000, he starred in the Priyadarshandirected comedy Hera Pheri\").\n\nDuring the validation by the authors, we found that most of the examples that were annotated incorrectly were cases where the label was correct, We tried to resolve this issue by asking our annotators to err on the side of caution. For example, while the claim \"Shakira is Canadian\" could be labeled as REFUTED by the sentence \"Shakira is a Colombian singer, songwriter, dancer, and record producer\", we advocated that unless more explicit evidence is provided (e.g. \"She was denied Canadian citizenship\"), the claim should be labeled as NOTENOUGHINFO, since dual citizenships are permitted, and the annotators' world knowledge should not be factored in.\n\nA related issue is entity resolution. For a claim like \"David Beckham was with United.\", it might be trivial for an annotator to accept \"David Beckham made his European League debut playing for Manchester United.\" as supporting evidence. This implicitly assumes that \"United\" refers to \"Manchester United\", however there are many Uniteds in Wikipedia and not just football clubs, e.g. United Airlines. The annotators knew the page of the main entity and thus it was relatively easy to resolve ambiguous entities. While we provide this information as part of the dataset, we argue that it should only be used for system training/development.\n\n\nBaseline System Description\n\nWe construct a simple pipelined system comprising three components: document retrieval, sentence-level evidence selection and textual entailment. Each component is evaluated in isolation through oracle evaluations on the development set and we report the final accuracies on the test set.\n\n\nDocument Retrieval\n\nWe use the document retrieval component from the DrQA system (Chen et al., 2017) which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram Term Frequency-Inverse Document Frequency (TF-IDF) vectors.\n\nSentence Selection Our simple sentence selection method ranks sentences by TF-IDF similarity to the claim. We sort the most-similar sentences first and tune a cut-off using validation accuracy on the development set. We evaluate both DrQA and a simple unigram TF-IDF implementation to rank the sentences for selection. We further evaluate impact of sentence selection on the RTE module by predicting entailment given the original documents without sentence selection.\n\nRecognizing Textual Entailment We compare two models for recognizing textual entailment. For a simple well-performing baseline, we selected Riedel et al. (2017)'s submission from the 2017 Fake News Challenge. It is a multi-layer perceptron (MLP) with a single hidden layer which uses term frequencies and TF-IDF cosine similarity between the claim and evidence as features.\n\nEvaluating the state-of-the-art in RTE, we used a decomposable attention (DA) model between the claim and the evidence passage (Parikh et al., 2016). We selected it because at the time of development this model was the highest scoring system for the Stanford Natural Language Inference task (Bowman et al., 2015) with publicly available code that did not require the input text to be parsed syntactically, nor was an ensemble.\n\nThe RTE component must correctly classify a claim as NOTENOUGHINFO when the evidence retrieved is not relevant or informative. However, the instances labeled as NOTENOUGHINFO have no evidence annotated, thus cannot be used to train RTE for this class. To overcome this issue, we simulate training instances for the NOTENOUGH-INFO through two methods: sampling a sentence from the nearest page (NEARESTP) to the claim as evidence using our document retrieval component and sampling a sentence from Wikipedia uniformly at random (RANDOMS).\n\n\nExperiments\n\n\nDataset Statistics\n\nWe partitioned the annotated claims into training, development and test sets. We ensured that each Wikipedia page used to generate claims occurs in exactly one set. We reserved a further 19,998 examples for use as a test set for a shared task. \n\n\nEvaluation\n\nPredicting whether a claim is SUPPORTED, RE-FUTED or NOTENOUGHINFO is a 3-way classification task that we evaluate using accuracy. In the case of the first two classes, appropriate evidence must be provided, at a sentence-level, to justify the classification. We consider an answer returned correct for the first two classes only if correct evidence is returned. Given that the development and test datasets have balanced class distributions, a random baseline will have \u223c 33% accuracy if one ignores the requirement for evidence for SUPPORTED and REFUTED. We evaluate the correctness of the evidence retrieved by computing the F 1 -score of all the predicted sentences in comparison to the humanannotated sentences for those claims requiring evidence on our complete pipeline system (Section 5.7). As in Fig. 1, some claims require multihop inference involving sentences from more than one document to be correctly supported as SUP-PORTED/REFUTED. In this case all sentences must be selected for the evidence to be marked as correct. We report this as the proportion of fully supported claims. Some claims may be equally supported by different pieces of evidence; in this case one complete set of sentences should be predicted.\n\nSystems that select information that the annotators did not will be penalized in terms of precision. We recognize that it is not feasible to ensure that the evidence selection annotations are complete, nevertheless we argue that they are useful for automatic evaluation during system development. For a more reliable evaluation we advocate crowd-sourcing annotations of false-positive predictions at a later date in a similar manner to the TAC KBP Slot Filler Validation (Ellis et al., 2016).\n\n\nDocument Retrieval\n\nThe document retrieval component of the baseline system returns the k nearest documents to the claim using the DrQA (Chen et al., 2017) TF-IDF implementation to return the k-nearest documents. In the scenario where evidence from multiple documents is required, k must be greater than this figure. We simulate the upper bound in accuracy using an oracle 3-way RTE classifier that predicts SUPPORTED/REFUTED ones correctly only if the documents containing the supporting/refuting evidence are returned by document retrieval and always predicts NOTENOUGHINFO instances correctly independently of the evidence. Results are shown in Table 2.  \n\n\nSentence Selection\n\nMirroring document retrieval, we extract the top lmost similar sentences from the k-most relevant documents using TF-IDF vector similarity. We modified document retrieval component of DrQA (Chen et al., 2017) to select sentences using bigram TF-IDF with binning and compared this to a simple unigram TF-IDF implementation using NLTK (Loper and Bird, 2002). Using the parameters k = 5 documents and l = 5 sentences, 55.30% of claims (excluding NOTENOUGHINFO) can be fully supported or refuted by the retrieved documents before sentence selection (see Table 2). After applying the sentence selection component, 44.22% of claims can be fully supported using the extracted sentences with DrQA and only 34.03% with NLTK. This would yield oracle accuracies of 62.81% and 56.02% respectively.\n\n\nRecognizing Textual Entailment\n\nThe RTE component is trained on labeled claims paired with sentence-level. Where multiple sentences are required as evidence, the strings are concatenated. As discussed in Section 4, such data is not annotated for claims labeled NOTE-NOUGHINFO, thus we compare random samplingbased and similarity-based strategies for generating it. We evaluate classification accuracy on the development set in an oracle evaluation, assuming correct evidence sentences are selected (Table 3)  The random sampling (RANDOMS) approach (where a sentence is sampled at random from Wikipedia in place of evidence for claims labeled as NOTENOUGHINFO) yielded sentences that were not only semantically different to the claim, but also unrelated. While the the accuracy of models trained with sampling approach is higher in oracle evaluation setting, this may not yield a better system in the pipeline setting. In contrast, the nearest page (NEARESTP) method samples a sentence from the highest-ranked page returned by our document retrieval module. This simulates finding related information that may not be sufficient to support or refute a claim. We will evaluate both RANDOMS and NEARESTP in the full pipeline setting, but we will not pursue the SNLI-trained model further as it performed substantially worse.\n\n\nFull Pipeline\n\nThe complete pipeline consists of the DrQA document retrieval module (Section 5.3), DrQA-based sentence retrieval module (Section 5.4), and the decomposable attention RTE model (Section 5.5). The two parameters: k, describing the number documents and l, describing the number sentences to return were found using grid-search optimizing the RTE accuracy with the DA model. For the pipeline, we set k = 5 and l = 5 and report the development set accuracy, both with and without the requirement to provide correct evidence for the SUPPORTED/REFUTED predictions (marked as ScoreEv and NoScoreEv respectively).\n\n\nModel\n\nAccuracy (  The decomposable attention model trained with NEARESTP is the most accurate when evidence is considered. Inspection of the confusion matrices shows that the RANDOMS strategy harms recall for the NOTENOUGHINFO class. This is due to the difference between the sampled pages in the training set and the ones retrieved in the development set causing related but uninformative evidence to be misclassified as SUPPORTED and REFUTED.\n\nAblation of the sentence selection module We evaluate the impact of the sentence selection module on both the RTE accuracy by removing it. While the sentence selection module may improve accuracy in the RTE component, it is discarding sentences that are required as evidence to support claims, harming performance (see Section 5.4). We assess the accuracies in both oracle setting (similar to Section 5.5) (see Table 5) as well as in the full pipeline (see Table 6).\n\nIn the oracle setting, the decomposable attention models are worst affected by removal of the sentence selection module: exhibiting an substantial decrease in accuracy. The NEARESTP training regime exhibits a 17% decrease and the RAN-DOMS accuracy decreases by 19%, despite nearperfect recall of the NOTENOUGHINFO class.\n\n\nModel\n\nOracle Accuracy (%)  In the pipeline setting, we run the RTE component without sentence selection using k = 5 most similar predicted documents. The removal of the sentence selection component decreased the accuracy (NOSCOREEV) approximately 10% for both decomposable attention models.\n\n\nModel\n\nAccuracy ( \n\n\nEvaluating Full Pipeline on Test Set\n\nWe evaluate our pipeline approach on the test set based on the results observed in Section 5.6. First, we use DrQA to select select 5 documents nearest to the claim. Then, we select 5 sentences using our DrQA-based sentence retrieval component and concatenate them. Finally, we predict entailment using the Decomposable Attention model trained with the NEARESTP strategy. The classification accuracy is 31.87%. Ignoring the requirement for correct evidence (NoScoreEv) the accuracy is 50.91%, which highlights that while the systems were predicting the correct label, the evidence selected was different to that which the human annotators chose. The recall of the document and sen-tence retrieval modules for claims which required evidence on the test set was 45.89% (considering complete groups of evidence) and the precision 10.79%. The resulting F 1 score is 17.47%.\n\n\nManual Error Analysis\n\nUsing the predictions on the test set, we sampled 961 of the predictions with an incorrect label or incorrect evidence and performed a manual analysis (procedure described in Appendix B). Of these, 28.51% (n = 274) had the correct predicted label but did not satisfy the requirements for evidence. The information retrieval component of the pipeline failed to identify any correct evidence in 58.27% (n = 560) of cases which accounted for the large disparity between accuracy of the system when evidence was and was not considered. Where suitable evidence was found, the RTE component incorrectly classified 13.84% (n = 133) of claims.\n\nThe pipeline retrieved new evidence that had not been identified by annotators in 21.85% (n = 210) of claims. This was in-line with our expectation given the measured recall rate of annotators (see Section 3.4.2), who achieved recall of 72.36% of evidence identified by the super-annotators.\n\nWe found that 4.05% (n = 41) of claims did not meet our guidelines. Of these, there were 11 claims which could be checked without evidence as these either tautologous or self-contradictory. Some correct claims appeared ungrammatical due to the mis-parsing of named entities (e.g. Exotic Birds is the name of a band but could be parsed as a type of animal). Annotator errors (where the wrong label was applied) were present in 1.35% (n = 13) of incorrectly classified claims. Interestingly, our system found new evidence that contradicted the gold evidence in 0.52% (n = 5) of cases. This was caused either by entity resolution errors or by inconsistent information present in Wikipedia pages (e.g. Pakistan was described as having both the 41st and 42nd largest GDP in two different pages).\n\n\nAblation of Training Data\n\nTo evaluate whether the size of the dataset is suitable for training the RTE component of the pipeline, we plot the learning curves for the DA and MLP models (Fig. 3). For each model, we trained 5 models with different random initializations using the NEARESTP method (see Section 5.5). We selected the highest performing model when evaluated on development set and report the oracle RTE accuracy on the test set. We observe that with fewer than 6000 training instances, the accuracy of DA is unstable. However, with more data, its accuracy increases with respect to the log of the number of training instances and exceeds that of MLP. While both learning curves exhibit the typical diminishing return trends, they indicate that the dataset is large enough to demonstrate the differences of models with different learning capabilities. \n\n\nDiscussion\n\nThe pipeline presented and evaluated in the previous section is one possible approach to the task proposed in our dataset, but we envisage different ones to be equally valid and possibly better performing. For instance, it would be interesting to test how approaches similar to natural logic inference (Angeli and Manning, 2014) can be applied, where a knowledge base/graph is constructed by reading the textual sources and then a reasoning process over the claim is applied, possibly using recent advances in neural theorem proving (Rockt\u00e4schel and Riedel, 2017). A different approach could be to consider a combination of question generation (Heilman and Smith, 2010) followed by a question answering model such as BiDAF (Seo et al., 2016), possibly requiring modification as they are designed to select a single span of text from a document rather than return one or more sentences as per our scoring criteria. The sentence-level evidence annotation in our dataset will help develop models selecting and attending to the relevant information from multiple documents and non-contiguous passages. Not only will this enhance the interpretability of predictions, but also facilitate the development of new methods for reading comprehension.\n\nAnother use case for the FEVER dataset is claim extraction: generating short concise textual facts from longer encyclopedic texts. For sources like Wikipedia or news articles, the sentences can contain multiple individual claims, making them not only difficult to parse, but also hard to evaluate against evidence. During the construction on the FEVER dataset, we allowed for an extension of the task where simple claims can be extracted from multiple complex sentences.\n\nFinally, we would like to note that while we chose Wikipedia as our textual source, we do not consider it to be the only source of information worth considering in verification, hence not using TRUE or FALSE in our classification scheme. We expect systems developed on the dataset presented to be portable to different textual sources.\n\n\nConclusions\n\nIn this paper we have introduced FEVER, a publicly available dataset for fact extraction and verification against textual sources. We discussed the data collection and annotation methods and shared some of the insights obtained during the annotation process that we hope will be useful to other large-scale annotation efforts.\n\nIn order to evaluate the challenge this dataset presents, we developed a pipeline approach that comprises information retrieval and textual entailment components. We showed that the task is challenging yet feasible, with the best performing system achieving an accuracy of 31.87%.\n\nWe also discussed other uses for the FEVER dataset and presented some further extensions that we would like to work on in the future. We believe that FEVER will provide a stimulating challenge for claim extraction and verification systems. \n\n\nA Annotation Guidelines\n\n\nA.1 Task 1 Definitions\n\nClaim A claim is a single sentence expressing information (true or mutated) about a single aspect of one target entity. Using only the source sentence to generate claims will result in simple claims that are not challenging. But, allowing world knowledge to be incorporated is too unconstrained and will result in claims that cannot be evidenced by this dataset. We address this gap by introducing a dictionary that provides additional knowledge that can be used to increase the complexity of claims in a controlled manner.\n\nDictionary Additional world knowledge is given to the annotator in a dictionary. This allows for more complex claims to be generated in a structured manner by the annotator. This knowledge may be incorporated into claims or may be needed when labelling whether evidence supports or refutes claims.\n\nMutation True claims will be distorted or mutated as part of the claim generation workflow. This may be achieved by making the sentence negative, substituting words or ideas or by making words more or less specific. The annotation system will select which type of mutation will be used.\n\n\nRequirements:\n\n\u2022 Claims must reference the target entity directly and avoid use of pronouns/nominals (e.g. he, she, it, the country)\n\n\u2022 Claims must not use speculative/cautious/vague language (e.g. may be, might be, it is reported that)\n\n\u2022 True claims should only be facts that can be deduced by information given in the source sentence and dictionary\n\n\u2022 Minor variations over the entity name are acceptable: (e.g. Amazon River vs River Amazon)\n\nExamples of true claims:\n\n\u2022 Keanu Reeves has acted in a Shakespeare play\n\n\u2022 The Assassin's Creed game franchise was launched in 2007\n\n\u2022 Prince Hamlet is the Prince of Denmark\n\n\u2022 In 2004, the coach of the Argentinian men's national field hockey team was Carlos Retegui\n\n\nA.2 Task 1 (subtask 1) Guidelines\n\nThe objective of this task is to generate true claims from this source sentence that was extracted from Wikipedia.\n\n\u2022 Extract simple factoid claims about entity given the source sentence.\n\n\u2022 Use the source sentence and dictionary as the basis for your claims.\n\n\u2022 Reference any entity directly (i.e. pronouns and nominals should not be used)\n\n\u2022 Minor variations of names are acceptable (e.g. John F Kennedy, JFK, President Kennedy).\n\n\u2022 Avoid vague or cautions language (e.g. might be, may be, could be, is reported that)\n\n\u2022 Correct capitalisation of entity names should be followed (India, not india).\n\n\u2022 Sentences should end with a period.\n\n\u2022 Numbers can be formatted in any appropriate English format (including as words for smaller quantities).\n\n\u2022 Some of the extracted text might not be accurate. These are still valid candidates for summary. It is not your job to fact check the information\n\n\nWorld Knowledge\n\n\u2022 Do not incorporate your own knowledge or beliefs.\n\n\u2022 Additional world knowledge is given to the you in the form of a dictionary. Use this to make more complex claims (we prefer using this dictionary instead of your own knowledge because the information in this dictionary can be backed up from Wikipedia)\n\n\u2022 If the source sentence is not suitable, leave the box blank to skip.\n\n\u2022 If a dictionary entry is not suitable or uninformative, ignore it.  Tables 7 and 8 for examples from the real data.\n\n\nA.4 Task 1 (substask 2) Guidelines\n\nThe objective of this task is to generate modifications to claims. The modifications can be either true or false. You will be given specific instructions about the types of modifications to make.\n\n\u2022 Use the original claims and dictionary as the basis for your modifications to facts about entity\n\n\u2022 Reference any entity directly (i.e. pronouns and nominals should not be used)\n\n\u2022 Avoid vague or cautions language (e.g. might be, may be, could be, is reported that)\n\n\u2022 Correct capitalisation of entity names should be followed (India, not india).\n\n\u2022 Sentences should end with a period.\n\n\u2022 Numbers can be formatted in any appropriate English format (including as words for smaller quantities).\n\n\u2022 Some of the extracted text might not be accurate. These are still valid candidates for summary. It is not your job to fact check the information\n\nSpecific guidelines for this screen\n\n\u2022 Aim to spend about up to 1 minute generating each claim.\n\n\u2022 You are allowed to incorporate your own world knowledge in making these modifications and misinformation.\n\n\u2022 All facts should reference any entity directly (i.e. pronouns and nominals should not be used).\n\n\u2022 The mutations you produce should be objective (i.e. not subjective) and verifiable using information/knowledge that would be publicly available \u2022 If it is not possible to generate facts or misinformation, leave the box blank.\n\nThere are six types of mutation the annotator will be asked to introduce. These will all be given on the same annotation page as all the claim modification types are related.\n\n\nEntity\n\n\nINDIA\n\n\nSource Sentence\n\nIt shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the northeast; and Myanmar (Burma) and Bangladesh to the east.  Entity CANADA\n\n\nDictionary\n\n\nSource Sentence\n\nCanada is sparsely populated, the majority of its land territory being dominated by forest and tundra and the Rocky Mountains.\n\n\nDictionary\n\n\nProvince of Canada\n\nThe United Province of Canada, or the Province of Canada, or the United Canadas was a British colony in North America from 1841 to 1867.\n\n\nRocky Mountains\n\nThe Rocky Mountains, commonly known as the Rockies, are a major mountain range in western North America. tundra In physical geography, tundra is a type of biome where the tree growth is hindered by low temperatures and short growing season.\n\n\nClaims\n\n-The terrain in Canada is mostly forest and tundra.\n\n-Parts of Canada are subject to low temperatures.\n\n-Canada is in North America.\n\n-In some areas of Canada, it is difficult for trees to grow.  1. Rephrase the claim so that it has the same meaning 2. Negate the meaning of the claim 3. Substitute the verb and/or object in the claim to alternative from the same set of things 4. Substitute the verb and/or object in the claim to alternative from a different set of things 5. Make the claim more specific so that the new claim implies the original claim (by making the meaning more specific) 6. Make the claim more general so that the new claim can be implied by the original claim (by making the meaning less specific) It may not always be possible to generate claims for each modification type. In this case, the box may be left blank.\n\n\nA.5 Task 1 (subtask 2) Examples\n\nThe following example illustrates how given a single source sentence, the following mutations could be made and why they are suitable. For the claim \"Barack Obama toured the UK.\", Figure 6 shows the relations between objects, and Table 9 contains examples for each type of mutation. The purpose of this task is to identify evidence from a Wikipedia page that can be used to support or refute simple factoid sentences called claims. The claims are generated by humans (as part of the WF1 annotation workflow) from a Wikipedia page. Some claims are true. Some claims are fake. You must find the evidence from the page that supports or refutes the claim.\n\nOther Wikipedia pages will also provide additional information that can serves as evidence. For each line, we will provide extracts from the linked pages in the dictionary column which appear when you \"Expand\" the sentence. The sentences from these linked pages that contain relevant supplementary information should be individually selected to record which information is used in justifying your decisions.\n\nStep-by-step guide:\n\n1. Read and understand the claim In the claim, Barack Obama is visiting a country, whereas the dinner is a political event.\n\n\nMore specific\n\nBarrack Obama made state visit to London.\n\nLondon is in the UK. If Obama visited London, he must have visited the UK.\n\nMore general Barrack Obama visited a country in the EU.\n\nThe UK is in the EU. If Obama visited the UK, he visited an EU country. 3. On identifying a relevant sentence, press the Expand button to highlight it. This will load the dictionary and the buttons to annotate it:\n\n(a) If the highlighted sentence contains enough information in a definitive statement to support or refute the claim, press the Supports or Refutes button to add your annotation. No information from the dictionary is needed in this case (this includes information from the main Wikipedia page). Then continue annotating from step 2.\n\n(b) If the highlighted sentence contains some information supporting or refuting the claim but also needs supporting information, this can be added from the dictionary.\n\ni. The hyperlinked sentences from the passage are automatically added to the dictionary ii. If a sentence from the main Wikipedia article is needed to provide supporting information. Click \"Add Main Wikipedia Page\" to add it to the dictionary.\n\niii. If the claim or sentence contains an entity that is not in the dictionary, then a custom page can be added by clicking \"Add Custom Page\". Use a search engine of your choice to find the page and then paste the Wikipedia URL into the box. iv. Tick the sentences from the dictionary that provide the minimal amount of supporting information needed to form your decision. If there are multiple equally relevant entries (such as a list of movies), then just select the first. Once all required information is added, then press the Supports or Refutes button to add your annotation and continue from step 2.\n\n(c) If the highlighted sentence and the dictionary do not contain enough information to support or refute the claim, press the Cancel button and continue from step 2 to identify more relevant sentences.\n\n4. On reaching the end of the Wikipedia page. Press Submit if you could find information that supports or refutes the claim. If you could not find any supporting evidence, press\n\nSkip then select Not enough information\n\nThe objective is to find sentences that support or refute the claim.\n\nYou must apply common-sense reasoning to the evidence you read but avoid applying your own world-knowledge by basing your decisions on the information presented in the Wikipedia page and dictionary.\n\nAs a guide -you should ask yourself: If I was given only the selected sentences, do I have stronger reason to believe claim is true (supported) or stronger reason to believe the claim is false (refuted). If I'm not certain, what additional information (dictionary) do I have to add to reach this conclusion.\n\n\nA.7 Task 2 Examples\n\nFigure 1 :\n1Manually verified claim requiring evidence from multiple Wikipedia pages.\n\nFigure 2 :\n2Screenshot of Task 2 -Claim Labeling but the evidence selected was not sufficient (only 4 out of 227 examples were labeled incorrectly according to the guidelines).\n\nFigure 3 :\n3Learning curves for the RTE models.\n\nFigure 4 :\n4Screenshot\n\nFigure 5 :\n5Screenshots of annotation task 1 (subtask 2)\n\nFigure 6 :\n6Toy ontology to be used with the provided examples of similar and dissimilar mutations A.6 Task 2 Guidelines\n\n\nTable 1: Dataset split sizes for SUPPORTED, REFUTED and NOTENOUGHINFO (NEI) classesSplit SUPPORTED REFUTED \nNEI \nTraining \n80,035 \n29,775 \n35,639 \nDev \n3,333 \n3,333 \n3,333 \nTest \n3,333 \n3,333 \n3,333 \nReserved \n6,666 \n6,666 \n6,666 \n\n\n\nTable 2 :\n2Dev. set document retrieval evaluation.\n\nTable 3 :\n3Oracle classification on claims in the development set using gold sentences as evidence\n\n\n%)NoScoreEv ScoreEv \n\nMLP / NP \n41.86 \n19.04 \nMLP / RS \n40.63 \n19.42 \nDA / NP \n52.09 \n32.57 \nDA / RS \n50.37 \n23.53 \n\n\n\nTable 4 :\n4Full pipeline results on development set\n\nTable 5 :\n5Oracle accuracy on claims in the dev. set using gold documents as evidence (c.f.Table 3).\n\nTable 6 :\n6Pipeline accuracy on the dev. set without the sentence selection module (c.f.Table 4).\n\n\nBhutanBhutan, officially the Kingdom of Bhutan, is a landlocked country in Asia, and it is the smallest state located entirely within the Himalaya mountain range. China China, officially the People's Republic of China (PRC), is a unitary sovereign state in East Asia and the world's most populous country, with a population of over 1.381 billion. Pakistan Pakistan, officially the Islamic Republic of Pakistan, is a federal parliamentary republic in South Asia on the crossroads of Central and Western Asia.Claims-One of the land borders that India shares is with the world's most populous country. (uses information from the dictionary entry for china) -India borders 6 countries.(summarises some of the information in the source sentence) -The Republic of India is situated between Pakistan and Burma. (deduced by Pakistan being West of India, and Burma being to the East)\n\nTable 7 :\n7Task 1 (subtask 1) example: India\n\nTable 8 :\n8Task 1 (subtask 1) example: Canada\n\nType Claim Rationale\nClaimRephrasePresident Obama visited some places in the United Kingdom.Rephrased. Same meaning.NegateObama has never been to the UK before.Obama could not have toured the UK if he has never been there.Substitute Similar \nBarack Obama visited France. \nBoth the UK and France are \ncountries \n\nSubstitute Dissimilar Barrack Obama attended the \nWhitehouse \nCorrespondents \nDinner. \n\n\n\nTable 9 :\n9Example mutations 2. Read the Wikipedia page and identify sentences that contain relevant information.\nhttp://fever.ai 2 https://github.com/awslabs/fever 3 https://github.com/sheffieldnlp/ fever-baselines\nThese consisted of 5,000 from a Wikipedia 'most accessed pages' list and the pages hyperlinked from them.\nAcknowledgmentsThe work reported was partly conducted while James Thorne was at Amazon Research Cambridge. Andreas Vlachos is supported by the EU H2020 SUMMA project (grant agreement number 688139). The authors would like to thank the following people for their advice and suggestions: David Hardcastle, Marie Hanabusa, Timothy Howd, Neil Lawrence, Benjamin Riedel, Craig Saunders and Iris Spik. The authors also wish to thank the team of annotators involved in preparing this dataset.Adding Main Wikipedia Page to DictionaryIn the case where the claim can be supported from multiple sentences from the main Wikipedia page, information the main Wikipedia page should be added to the dictionary to add supporting information. This is because each line that is annotated in the left column for the main Wikipedia page is stored independently.Claim: George Washington was a soldier, born in 1732.Wikipedia page: George Washington Sentence 1 contains enough information to wholly support the claim without the need for any additional information. Sentence 2 and 3 contain partial information that can be combined. Expand sentence 2 and click add main Wikipedia page to add the Wikipedia page to add George Washington to the dictionary. Sentence 3 can now be added to dictionary to support the claim.The order of the sentences doesn't matter (selecting sentence 2+3 is the same as adding sentence 3+2) because we sort the sentences in document order. This means that you only need to annotate this once.If you attempt to add the main Wikipedia page to the dictionary from sentence 3 having already used it for sentence 2, the system will warn you that you are making a duplicate annotation. You may need to add a custom page from Wikipedia to the dictionary. This may happen in cases where the claim discusses an entity that was not in the original Wikipedia page Claim: Colin Firth is a Gemini. In Original Page: \"Colin Firth (born 10 September 1960)... \" Requires Additional Information from Gemini: \"Under the tropical zodiac, the sun transits this sign between May 21 and June 21.\"Tense The difference in verb tenses that do not affect the meaning should be ignored.Claim: Frank Sinatra is a musician Supported: He is one of the best-selling music artists of all time, having sold more than 150 million records worldwide.Claim: Frank Sinatra is a musician Supported: Francis Albert Sinatra was an American singerA.7.3 SkippingThere may be times where it is appropriate to skip the claim by pressing the Skip button:\u2022 The claim cannot be verified using the information with the information provided:-If the claim could potentially be verified using other publicly available information. Select Not Enough Information -If the claim can't be verified using any publicly available information (because it's ambiguous, vague, personal or implausible) select The claim is ambiguous or contains personal information -If the claim doesn't meet the guidelines from WF1, select: The claim doesn't meet the WF1 guidelinesA.8 Task 2 additional guidelinesAfter conferring with the organizers, the annotators expanded the guidelines to include common case that were not explicitly covered in the guidelines:1. Any claims involving \"many\", \"several\", \"rarely\", \"barely\" or other indeterminate count words are going to be ambiguous and can be flagged.2. Same goes for \"popular\", \"famous\", and \"successful (for people, for works we can assume commercial success)\"3. We cannot prove personhood for fictional characters like we can for real people (dogs and cats can be authors, actors, and citizens in fiction)4. If a claim is \"[Person] was in [film]\", the only way to refute it would be (a) if they were born after it was released, or (b) their acting debut is mentioned and occurs a realistically long enough amount of time after the film's release (at least a few years).5.A list of movies that someone was in or jobs a person held is not necessarily exclusive, we cannot refute someone being a lawyer because the first sentence of their wiki article says they were an actor.6. A person is not their roles, if a claim is something like \"Tom Cruise participated in a heist in Mission Impossible 3\", we cannot prove it, because Ethan Hunt did that, not Tom Cruise.7. Our workflow is time-insensitive, but if a claim tags something to a time period, we can treat it as such. \"Neil Armstrong is an astronaut\" can be supported, but \"Neil Armstrong was an astronaut in 2013\" can be refuted, because he was dead at the time.8. If someone won 5 Academy Awards, they won 3 Academy Awards. Similarly, if they won an Academy Award, they were nominated for an award. 11. Flag anything related to death, large-scale recent disasters, and controversial religious or social statements.B Manual Error AnalysisThe manual error analysis was conducted with the decision process described inFigure 8. For the cases of finding new evidence, recommended actions have been listed. While these were not performed for this version of the dataset, this may form a future update following a pool-based evaluation in the FEVER Shared Task.\nNat-uralLI: Natural logic inference for common sense reasoning. Gabor Angeli, D Christopher, Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingGabor Angeli and Christopher D. Manning. 2014. Nat- uralLI: Natural logic inference for common sense reasoning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro- cessing. pages 534-545.\n\nA large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.\n\nReading wikipedia to answer opendomain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open- domain questions. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, pages 1870-1879. https://doi.org/10.18653/v1/P17-1171.\n\nRecognizing textual entailment: Rational, evaluation and approaches. Bill Ido Dagan, Bernardo Dolan, Dan Magnini, Roth, 10.1017/S1351324909990209Natural Language Engineering. 154Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail- ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i-xvii. https://doi.org/10.1017/S1351324909990209.\n\nJoe Ellis, Jeremy Getman, Dana Fore, Neil Kuster, Zhiyi Song, Ann Bies, and Stephanie Strassel. 2016. Overview of Linguistic Resources for the TAC KBP 2016 Evaluations : Methodologies and Results. Proceedings of TAC KBP 2016 Workshop, National Institute of Standards and Technology. Maryland, USALdcJoe Ellis, Jeremy Getman, Dana Fore, Neil Kuster, Zhiyi Song, Ann Bies, and Stephanie Strassel. 2016. Overview of Linguistic Resources for the TAC KBP 2016 Evaluations : Methodologies and Results. Pro- ceedings of TAC KBP 2016 Workshop, National In- stitute of Standards and Technology, Maryland, USA (Ldc).\n\nEmergent: a novel data-set for stance classification. William Ferreira, Andreas Vlachos, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaWilliam Ferreira and Andreas Vlachos. 2016. Emer- gent: a novel data-set for stance classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies. San Diego, California, pages 1163-1168.\n\nMeasuring nominal scale agreement among many raters. L Joseph, Fleiss, Psychological bulletin. 765378Joseph L Fleiss. 1971. Measuring nominal scale agree- ment among many raters. Psychological bulletin 76(5):378.\n\n. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, Luke Zettlemoyer, AllenNLP: A Deep Semantic Natural Language Processing Platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2017. AllenNLP: A Deep Semantic Natural Language Pro- cessing Platform .\n\nGood Question! statistical ranking for question generation. Michael Heilman, A Noah, Smith, Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. the 2010 Annual Conference of the North American Chapter of the Association for Computational LinguisticsMichael Heilman and Noah A. Smith. 2010. Good Question! statistical ranking for question genera- tion. In Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. pages 609-617.\n\nAutomated historical fact-checking by passage retrieval, word statistics, and virtual question-answering. Mio Kobayashi, Ai Ishii, Chikara Hoshino, Hiroshi Miyashita, Takuya Matsuzaki, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language Processing1Mio Kobayashi, Ai Ishii, Chikara Hoshino, Hiroshi Miyashita, and Takuya Matsuzaki. 2017. Auto- mated historical fact-checking by passage retrieval, word statistics, and virtual question-answering. In Proceedings of the Eighth International Joint Con- ference on Natural Language Processing (Volume 1: Long Papers). volume 1, pages 967-975.\n\nNltk: The natural language toolkit. Edward Loper, Steven Bird, 10.3115/1118108.1118117Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational LinguisticsStroudsburg, PA, USA, ETMTNLP '021Association for Computational LinguisticsEdward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective Tools and Method- ologies for Teaching Natural Language Process- ing and Computational Linguistics -Volume 1. As- sociation for Computational Linguistics, Strouds- burg, PA, USA, ETMTNLP '02, pages 63-70. https://doi.org/10.3115/1118108.1118117.\n\nThe Stanford CoreNLP natural language processing toolkit. D Christopher, Mihai Manning, John Surdeanu, Jenny Rose Bauer, Steven Finkel, David Bethard, Mc-Closky, ACL (System Demonstrations). Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In ACL (System Demon- strations). pages 55-60.\n\nOpen language learning for information extraction. Michael Mausam, Robert Schmitz, Stephen Bart, Oren Soderland, Etzioni, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningMausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pages 523-534.\n\nA decomposable attention model for natural language inference. Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, Jakob Uszkoreit, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsAnkur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceed- ings of the 2016 Conference on Empirical Meth- ods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2249-2255. https://aclweb.org/anthology/D16-\n\nFake news challenge. Dean Pomerleau, Delip Rao, Dean Pomerleau and Delip Rao. 2017. Fake news chal- lenge. http://fakenewschallenge.org/.\n\nSQuAD: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Empirical Methods in Natural Language Processing. EMNLPP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. SQuAD: 100,000+ questions for machine compre- hension of text. In Empirical Methods in Natural Language Processing (EMNLP).\n\nSentence provides information about a different entity and only one entity is permitted (e.g. place of birth can only be one place) Claim: David Schwimmer finished acting in Friends in 2005. A.7.1 What does it mean to Support or Refute a claim The following count as valid justifications for marking an item as supported/refuted: Sentence directly states information that supports/refutes the claim or states information that is synonymous/antonymous with information in the claim Claim: Water occurs artificially. conjunction with other sentences from the dictionary, fulfils one of the above criteria Claim: John McCain is a conservative. Refuted by: \"He was the Republican nominee for the 2008 U.S. presidential election.\" AND \"The Republican Party's current ideology is American conservatism, which contrasts with the Democrats' more progressive platform (also called modern liberalismA.7.1 What does it mean to Support or Refute a claim The following count as valid justifications for marking an item as supported/refuted: Sentence directly states information that sup- ports/refutes the claim or states information that is synonymous/antonymous with information in the claim Claim: Water occurs artificially Refuted by: \"It also occurs in nature as snow, glaciers ...\" Claim: Samuel L. Jackson was in the third movie in the Die Hard film series. Supported by: \"He is a highly prolific actor, hav- ing appeared in over 100 films, including Die Hard 3.\" Sentence refutes the claim through negation or quantification Claim: Schindler's List received no awards. Refuted by: \"It was the recipient of seven Academy Awards (out of twelve nominations), in- cluding Best Picture, Best Director...\" Sentence provides information about a different entity and only one entity is permitted (e.g. place of birth can only be one place) Claim: David Schwimmer finished acting in Friends in 2005. Refuted by: \"After the series finale of Friends in 2004, Schwimmer was cast as the title character in the 2005 drama Duane Hopwood.\" Sentence provides information that, in conjunc- tion with other sentences from the dictionary, ful- fils one of the above criteria Claim: John McCain is a conservative. Refuted by: \"He was the Republican nom- inee for the 2008 U.S. presidential election.\" AND \"The Republican Party's current ideology is American conservatism, which contrasts with the Democrats' more progressive platform (also called modern liberalism).\"\n", "annotations": {"author": "[{\"end\":164,\"start\":69},{\"end\":264,\"start\":165},{\"end\":321,\"start\":265},{\"end\":363,\"start\":322}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":75},{\"end\":180,\"start\":173},{\"end\":292,\"start\":274},{\"end\":334,\"start\":328}]", "author_first_name": "[{\"end\":74,\"start\":69},{\"end\":172,\"start\":165},{\"end\":273,\"start\":265},{\"end\":327,\"start\":322}]", "author_affiliation": "[{\"end\":163,\"start\":108},{\"end\":263,\"start\":208},{\"end\":320,\"start\":294},{\"end\":362,\"start\":336}]", "title": "[{\"end\":66,\"start\":1},{\"end\":429,\"start\":364}]", "venue": null, "abstract": "[{\"end\":1388,\"start\":431}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1950,\"start\":1930},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1970,\"start\":1950},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2392,\"start\":2373},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2834,\"start\":2813},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2857,\"start\":2834},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3110,\"start\":3085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4494,\"start\":4480},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7224,\"start\":7199},{\"end\":7634,\"start\":7617},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7701,\"start\":7674},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7859,\"start\":7839},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8061,\"start\":8036},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8320,\"start\":8299},{\"end\":8475,\"start\":8454},{\"end\":8582,\"start\":8560},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9014,\"start\":8991},{\"end\":9800,\"start\":9799},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11163,\"start\":11137},{\"end\":12131,\"start\":12125},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15304,\"start\":15291},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18698,\"start\":18679},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19860,\"start\":19839},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20024,\"start\":20003},{\"end\":22694,\"start\":22674},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22853,\"start\":22834},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23587,\"start\":23568},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23734,\"start\":23712},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31528,\"start\":31502},{\"end\":31763,\"start\":31733},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31869,\"start\":31844},{\"end\":31941,\"start\":31917}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44780,\"start\":44694},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44958,\"start\":44781},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45007,\"start\":44959},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45031,\"start\":45008},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45089,\"start\":45032},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45211,\"start\":45090},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45446,\"start\":45212},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45498,\"start\":45447},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":45598,\"start\":45499},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":45718,\"start\":45599},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45771,\"start\":45719},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45873,\"start\":45772},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":45972,\"start\":45874},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":46849,\"start\":45973},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":46895,\"start\":46850},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":46942,\"start\":46896},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":47344,\"start\":46943},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":47459,\"start\":47345}]", "paragraph": "[{\"end\":1778,\"start\":1404},{\"end\":2714,\"start\":1780},{\"end\":3294,\"start\":2716},{\"end\":4329,\"start\":3296},{\"end\":4594,\"start\":4331},{\"end\":5523,\"start\":4596},{\"end\":6205,\"start\":5541},{\"end\":6433,\"start\":6234},{\"end\":6560,\"start\":6463},{\"end\":7173,\"start\":6562},{\"end\":7958,\"start\":7175},{\"end\":8476,\"start\":7960},{\"end\":9155,\"start\":8478},{\"end\":9242,\"start\":9200},{\"end\":9331,\"start\":9291},{\"end\":9510,\"start\":9333},{\"end\":10180,\"start\":9539},{\"end\":10855,\"start\":10182},{\"end\":11374,\"start\":10857},{\"end\":11945,\"start\":11376},{\"end\":12132,\"start\":11947},{\"end\":12542,\"start\":12159},{\"end\":12816,\"start\":12544},{\"end\":13882,\"start\":12818},{\"end\":14321,\"start\":13897},{\"end\":14611,\"start\":14323},{\"end\":15137,\"start\":14631},{\"end\":15584,\"start\":15157},{\"end\":16091,\"start\":15623},{\"end\":16549,\"start\":16121},{\"end\":16977,\"start\":16562},{\"end\":17633,\"start\":16979},{\"end\":18275,\"start\":17635},{\"end\":18595,\"start\":18307},{\"end\":18866,\"start\":18618},{\"end\":19335,\"start\":18868},{\"end\":19710,\"start\":19337},{\"end\":20138,\"start\":19712},{\"end\":20677,\"start\":20140},{\"end\":20958,\"start\":20714},{\"end\":22201,\"start\":20973},{\"end\":22695,\"start\":22203},{\"end\":23356,\"start\":22718},{\"end\":24164,\"start\":23379},{\"end\":25487,\"start\":24199},{\"end\":26110,\"start\":25505},{\"end\":26558,\"start\":26120},{\"end\":27026,\"start\":26560},{\"end\":27348,\"start\":27028},{\"end\":27642,\"start\":27358},{\"end\":27663,\"start\":27652},{\"end\":28573,\"start\":27704},{\"end\":29234,\"start\":28599},{\"end\":29527,\"start\":29236},{\"end\":30319,\"start\":29529},{\"end\":31185,\"start\":30349},{\"end\":32439,\"start\":31200},{\"end\":32911,\"start\":32441},{\"end\":33248,\"start\":32913},{\"end\":33590,\"start\":33264},{\"end\":33872,\"start\":33592},{\"end\":34114,\"start\":33874},{\"end\":34690,\"start\":34167},{\"end\":34989,\"start\":34692},{\"end\":35277,\"start\":34991},{\"end\":35412,\"start\":35295},{\"end\":35516,\"start\":35414},{\"end\":35631,\"start\":35518},{\"end\":35724,\"start\":35633},{\"end\":35750,\"start\":35726},{\"end\":35798,\"start\":35752},{\"end\":35858,\"start\":35800},{\"end\":35900,\"start\":35860},{\"end\":35993,\"start\":35902},{\"end\":36145,\"start\":36031},{\"end\":36218,\"start\":36147},{\"end\":36290,\"start\":36220},{\"end\":36371,\"start\":36292},{\"end\":36462,\"start\":36373},{\"end\":36550,\"start\":36464},{\"end\":36631,\"start\":36552},{\"end\":36670,\"start\":36633},{\"end\":36777,\"start\":36672},{\"end\":36925,\"start\":36779},{\"end\":36996,\"start\":36945},{\"end\":37251,\"start\":36998},{\"end\":37323,\"start\":37253},{\"end\":37442,\"start\":37325},{\"end\":37676,\"start\":37481},{\"end\":37776,\"start\":37678},{\"end\":37857,\"start\":37778},{\"end\":37945,\"start\":37859},{\"end\":38026,\"start\":37947},{\"end\":38065,\"start\":38028},{\"end\":38172,\"start\":38067},{\"end\":38320,\"start\":38174},{\"end\":38357,\"start\":38322},{\"end\":38417,\"start\":38359},{\"end\":38526,\"start\":38419},{\"end\":38625,\"start\":38528},{\"end\":38854,\"start\":38627},{\"end\":39030,\"start\":38856},{\"end\":39222,\"start\":39067},{\"end\":39381,\"start\":39255},{\"end\":39553,\"start\":39417},{\"end\":39813,\"start\":39573},{\"end\":39875,\"start\":39824},{\"end\":39926,\"start\":39877},{\"end\":39956,\"start\":39928},{\"end\":40662,\"start\":39958},{\"end\":41349,\"start\":40698},{\"end\":41758,\"start\":41351},{\"end\":41779,\"start\":41760},{\"end\":41904,\"start\":41781},{\"end\":41963,\"start\":41922},{\"end\":42039,\"start\":41965},{\"end\":42096,\"start\":42041},{\"end\":42311,\"start\":42098},{\"end\":42645,\"start\":42313},{\"end\":42815,\"start\":42647},{\"end\":43060,\"start\":42817},{\"end\":43668,\"start\":43062},{\"end\":43872,\"start\":43670},{\"end\":44051,\"start\":43874},{\"end\":44092,\"start\":44053},{\"end\":44162,\"start\":44094},{\"end\":44362,\"start\":44164},{\"end\":44671,\"start\":44364}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23353,\"start\":23346},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23936,\"start\":23929},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24674,\"start\":24665},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26978,\"start\":26971},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":27024,\"start\":27017},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":37409,\"start\":37395},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":40935,\"start\":40928}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1402,\"start\":1390},{\"attributes\":{\"n\":\"2\"},\"end\":5539,\"start\":5526},{\"end\":6232,\"start\":6208},{\"end\":6461,\"start\":6436},{\"attributes\":{\"n\":\"3\"},\"end\":9198,\"start\":9158},{\"end\":9289,\"start\":9245},{\"attributes\":{\"n\":\"3.1\"},\"end\":9537,\"start\":9513},{\"attributes\":{\"n\":\"3.2\"},\"end\":12157,\"start\":12135},{\"attributes\":{\"n\":\"3.3\"},\"end\":13895,\"start\":13885},{\"attributes\":{\"n\":\"3.4\"},\"end\":14629,\"start\":14614},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":15155,\"start\":15140},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":15621,\"start\":15587},{\"attributes\":{\"n\":\"3.4.3\"},\"end\":16119,\"start\":16094},{\"attributes\":{\"n\":\"3.4.4\"},\"end\":16560,\"start\":16552},{\"attributes\":{\"n\":\"4\"},\"end\":18305,\"start\":18278},{\"end\":18616,\"start\":18598},{\"attributes\":{\"n\":\"5\"},\"end\":20691,\"start\":20680},{\"attributes\":{\"n\":\"5.1\"},\"end\":20712,\"start\":20694},{\"attributes\":{\"n\":\"5.2\"},\"end\":20971,\"start\":20961},{\"attributes\":{\"n\":\"5.3\"},\"end\":22716,\"start\":22698},{\"attributes\":{\"n\":\"5.4\"},\"end\":23377,\"start\":23359},{\"attributes\":{\"n\":\"5.5\"},\"end\":24197,\"start\":24167},{\"attributes\":{\"n\":\"5.6\"},\"end\":25503,\"start\":25490},{\"end\":26118,\"start\":26113},{\"end\":27356,\"start\":27351},{\"end\":27650,\"start\":27645},{\"attributes\":{\"n\":\"5.7\"},\"end\":27702,\"start\":27666},{\"attributes\":{\"n\":\"5.8\"},\"end\":28597,\"start\":28576},{\"attributes\":{\"n\":\"5.9\"},\"end\":30347,\"start\":30322},{\"attributes\":{\"n\":\"6\"},\"end\":31198,\"start\":31188},{\"attributes\":{\"n\":\"7\"},\"end\":33262,\"start\":33251},{\"end\":34140,\"start\":34117},{\"end\":34165,\"start\":34143},{\"end\":35293,\"start\":35280},{\"end\":36029,\"start\":35996},{\"end\":36943,\"start\":36928},{\"end\":37479,\"start\":37445},{\"end\":39039,\"start\":39033},{\"end\":39047,\"start\":39042},{\"end\":39065,\"start\":39050},{\"end\":39235,\"start\":39225},{\"end\":39253,\"start\":39238},{\"end\":39394,\"start\":39384},{\"end\":39415,\"start\":39397},{\"end\":39571,\"start\":39556},{\"end\":39822,\"start\":39816},{\"end\":40696,\"start\":40665},{\"end\":41920,\"start\":41907},{\"end\":44693,\"start\":44674},{\"end\":44705,\"start\":44695},{\"end\":44792,\"start\":44782},{\"end\":44970,\"start\":44960},{\"end\":45019,\"start\":45009},{\"end\":45043,\"start\":45033},{\"end\":45101,\"start\":45091},{\"end\":45457,\"start\":45448},{\"end\":45509,\"start\":45500},{\"end\":45729,\"start\":45720},{\"end\":45782,\"start\":45773},{\"end\":45884,\"start\":45875},{\"end\":46860,\"start\":46851},{\"end\":46906,\"start\":46897},{\"end\":46964,\"start\":46944},{\"end\":47355,\"start\":47346}]", "table": "[{\"end\":45446,\"start\":45297},{\"end\":45718,\"start\":45603},{\"end\":47344,\"start\":47166}]", "figure_caption": "[{\"end\":44780,\"start\":44707},{\"end\":44958,\"start\":44794},{\"end\":45007,\"start\":44972},{\"end\":45031,\"start\":45021},{\"end\":45089,\"start\":45045},{\"end\":45211,\"start\":45103},{\"end\":45297,\"start\":45214},{\"end\":45498,\"start\":45459},{\"end\":45598,\"start\":45511},{\"end\":45603,\"start\":45601},{\"end\":45771,\"start\":45731},{\"end\":45873,\"start\":45784},{\"end\":45972,\"start\":45886},{\"end\":46849,\"start\":45975},{\"end\":46895,\"start\":46862},{\"end\":46942,\"start\":46908},{\"end\":47166,\"start\":46970},{\"end\":47459,\"start\":47357}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3732,\"start\":3724},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12414,\"start\":12406},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13053,\"start\":13047},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13455,\"start\":13449},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21784,\"start\":21778},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30515,\"start\":30507},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40886,\"start\":40878}]", "bib_author_first_name": "[{\"end\":52833,\"start\":52828},{\"end\":52843,\"start\":52842},{\"end\":53313,\"start\":53312},{\"end\":53327,\"start\":53322},{\"end\":53347,\"start\":53336},{\"end\":53367,\"start\":53356},{\"end\":53369,\"start\":53368},{\"end\":53845,\"start\":53840},{\"end\":53856,\"start\":53852},{\"end\":53869,\"start\":53864},{\"end\":53885,\"start\":53878},{\"end\":54535,\"start\":54531},{\"end\":54555,\"start\":54547},{\"end\":54566,\"start\":54563},{\"end\":54862,\"start\":54859},{\"end\":54876,\"start\":54870},{\"end\":54889,\"start\":54885},{\"end\":54900,\"start\":54896},{\"end\":55528,\"start\":55521},{\"end\":55546,\"start\":55539},{\"end\":56192,\"start\":56191},{\"end\":56358,\"start\":56354},{\"end\":56372,\"start\":56368},{\"end\":56383,\"start\":56379},{\"end\":56399,\"start\":56393},{\"end\":56416,\"start\":56409},{\"end\":56431,\"start\":56425},{\"end\":56444,\"start\":56437},{\"end\":56460,\"start\":56453},{\"end\":56474,\"start\":56470},{\"end\":56833,\"start\":56826},{\"end\":56844,\"start\":56843},{\"end\":57438,\"start\":57435},{\"end\":57452,\"start\":57450},{\"end\":57467,\"start\":57460},{\"end\":57484,\"start\":57477},{\"end\":57502,\"start\":57496},{\"end\":58059,\"start\":58053},{\"end\":58073,\"start\":58067},{\"end\":58878,\"start\":58877},{\"end\":58897,\"start\":58892},{\"end\":58911,\"start\":58907},{\"end\":58927,\"start\":58922},{\"end\":58932,\"start\":58928},{\"end\":58946,\"start\":58940},{\"end\":58960,\"start\":58955},{\"end\":59290,\"start\":59283},{\"end\":59305,\"start\":59299},{\"end\":59322,\"start\":59315},{\"end\":59333,\"start\":59329},{\"end\":59970,\"start\":59965},{\"end\":59984,\"start\":59979},{\"end\":60004,\"start\":59996},{\"end\":60015,\"start\":60010},{\"end\":60605,\"start\":60601},{\"end\":60622,\"start\":60617},{\"end\":60781,\"start\":60780},{\"end\":60794,\"start\":60793},{\"end\":60803,\"start\":60802},{\"end\":60814,\"start\":60813}]", "bib_author_last_name": "[{\"end\":52840,\"start\":52834},{\"end\":52855,\"start\":52844},{\"end\":52864,\"start\":52857},{\"end\":53320,\"start\":53314},{\"end\":53334,\"start\":53328},{\"end\":53354,\"start\":53348},{\"end\":53375,\"start\":53370},{\"end\":53384,\"start\":53377},{\"end\":53850,\"start\":53846},{\"end\":53862,\"start\":53857},{\"end\":53876,\"start\":53870},{\"end\":53892,\"start\":53886},{\"end\":54545,\"start\":54536},{\"end\":54561,\"start\":54556},{\"end\":54574,\"start\":54567},{\"end\":54580,\"start\":54576},{\"end\":54868,\"start\":54863},{\"end\":54883,\"start\":54877},{\"end\":54894,\"start\":54890},{\"end\":54907,\"start\":54901},{\"end\":55537,\"start\":55529},{\"end\":55554,\"start\":55547},{\"end\":56199,\"start\":56193},{\"end\":56207,\"start\":56201},{\"end\":56366,\"start\":56359},{\"end\":56377,\"start\":56373},{\"end\":56391,\"start\":56384},{\"end\":56407,\"start\":56400},{\"end\":56423,\"start\":56417},{\"end\":56435,\"start\":56432},{\"end\":56451,\"start\":56445},{\"end\":56468,\"start\":56461},{\"end\":56486,\"start\":56475},{\"end\":56841,\"start\":56834},{\"end\":56849,\"start\":56845},{\"end\":56856,\"start\":56851},{\"end\":57448,\"start\":57439},{\"end\":57458,\"start\":57453},{\"end\":57475,\"start\":57468},{\"end\":57494,\"start\":57485},{\"end\":57512,\"start\":57503},{\"end\":58065,\"start\":58060},{\"end\":58078,\"start\":58074},{\"end\":58890,\"start\":58879},{\"end\":58905,\"start\":58898},{\"end\":58920,\"start\":58912},{\"end\":58938,\"start\":58933},{\"end\":58953,\"start\":58947},{\"end\":58968,\"start\":58961},{\"end\":58979,\"start\":58970},{\"end\":59297,\"start\":59291},{\"end\":59313,\"start\":59306},{\"end\":59327,\"start\":59323},{\"end\":59343,\"start\":59334},{\"end\":59352,\"start\":59345},{\"end\":59977,\"start\":59971},{\"end\":59994,\"start\":59985},{\"end\":60008,\"start\":60005},{\"end\":60025,\"start\":60016},{\"end\":60615,\"start\":60606},{\"end\":60626,\"start\":60623},{\"end\":60791,\"start\":60782},{\"end\":60800,\"start\":60795},{\"end\":60811,\"start\":60804},{\"end\":60820,\"start\":60815}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2854390},\"end\":53244,\"start\":52764},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14604520},\"end\":53788,\"start\":53246},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1171\",\"id\":\"b2\",\"matched_paper_id\":3618568},\"end\":54460,\"start\":53790},{\"attributes\":{\"doi\":\"10.1017/S1351324909990209\",\"id\":\"b3\",\"matched_paper_id\":18717799},\"end\":54857,\"start\":54462},{\"attributes\":{\"id\":\"b4\"},\"end\":55465,\"start\":54859},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1434196},\"end\":56136,\"start\":55467},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":143544759},\"end\":56350,\"start\":56138},{\"attributes\":{\"id\":\"b7\"},\"end\":56764,\"start\":56352},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1809816},\"end\":57327,\"start\":56766},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":19918495},\"end\":58015,\"start\":57329},{\"attributes\":{\"doi\":\"10.3115/1118108.1118117\",\"id\":\"b10\",\"matched_paper_id\":1438450},\"end\":58817,\"start\":58017},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14068874},\"end\":59230,\"start\":58819},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":74065},\"end\":59900,\"start\":59232},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8495258},\"end\":60578,\"start\":59902},{\"attributes\":{\"id\":\"b14\"},\"end\":60717,\"start\":60580},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11816014},\"end\":61056,\"start\":60719},{\"attributes\":{\"id\":\"b16\"},\"end\":63499,\"start\":61058}]", "bib_title": "[{\"end\":52826,\"start\":52764},{\"end\":53310,\"start\":53246},{\"end\":53838,\"start\":53790},{\"end\":54529,\"start\":54462},{\"end\":55519,\"start\":55467},{\"end\":56189,\"start\":56138},{\"end\":56824,\"start\":56766},{\"end\":57433,\"start\":57329},{\"end\":58051,\"start\":58017},{\"end\":58875,\"start\":58819},{\"end\":59281,\"start\":59232},{\"end\":59963,\"start\":59902},{\"end\":60778,\"start\":60719},{\"end\":61247,\"start\":61058}]", "bib_author": "[{\"end\":52842,\"start\":52828},{\"end\":52857,\"start\":52842},{\"end\":52866,\"start\":52857},{\"end\":53322,\"start\":53312},{\"end\":53336,\"start\":53322},{\"end\":53356,\"start\":53336},{\"end\":53377,\"start\":53356},{\"end\":53386,\"start\":53377},{\"end\":53852,\"start\":53840},{\"end\":53864,\"start\":53852},{\"end\":53878,\"start\":53864},{\"end\":53894,\"start\":53878},{\"end\":54547,\"start\":54531},{\"end\":54563,\"start\":54547},{\"end\":54576,\"start\":54563},{\"end\":54582,\"start\":54576},{\"end\":54870,\"start\":54859},{\"end\":54885,\"start\":54870},{\"end\":54896,\"start\":54885},{\"end\":54909,\"start\":54896},{\"end\":55539,\"start\":55521},{\"end\":55556,\"start\":55539},{\"end\":56201,\"start\":56191},{\"end\":56209,\"start\":56201},{\"end\":56368,\"start\":56354},{\"end\":56379,\"start\":56368},{\"end\":56393,\"start\":56379},{\"end\":56409,\"start\":56393},{\"end\":56425,\"start\":56409},{\"end\":56437,\"start\":56425},{\"end\":56453,\"start\":56437},{\"end\":56470,\"start\":56453},{\"end\":56488,\"start\":56470},{\"end\":56843,\"start\":56826},{\"end\":56851,\"start\":56843},{\"end\":56858,\"start\":56851},{\"end\":57450,\"start\":57435},{\"end\":57460,\"start\":57450},{\"end\":57477,\"start\":57460},{\"end\":57496,\"start\":57477},{\"end\":57514,\"start\":57496},{\"end\":58067,\"start\":58053},{\"end\":58080,\"start\":58067},{\"end\":58892,\"start\":58877},{\"end\":58907,\"start\":58892},{\"end\":58922,\"start\":58907},{\"end\":58940,\"start\":58922},{\"end\":58955,\"start\":58940},{\"end\":58970,\"start\":58955},{\"end\":58981,\"start\":58970},{\"end\":59299,\"start\":59283},{\"end\":59315,\"start\":59299},{\"end\":59329,\"start\":59315},{\"end\":59345,\"start\":59329},{\"end\":59354,\"start\":59345},{\"end\":59979,\"start\":59965},{\"end\":59996,\"start\":59979},{\"end\":60010,\"start\":59996},{\"end\":60027,\"start\":60010},{\"end\":60617,\"start\":60601},{\"end\":60628,\"start\":60617},{\"end\":60793,\"start\":60780},{\"end\":60802,\"start\":60793},{\"end\":60813,\"start\":60802},{\"end\":60822,\"start\":60813}]", "bib_venue": "[{\"end\":52952,\"start\":52866},{\"end\":53472,\"start\":53386},{\"end\":54001,\"start\":53914},{\"end\":54635,\"start\":54607},{\"end\":55140,\"start\":54909},{\"end\":55698,\"start\":55556},{\"end\":56231,\"start\":56209},{\"end\":56550,\"start\":56488},{\"end\":56978,\"start\":56858},{\"end\":57601,\"start\":57514},{\"end\":58245,\"start\":58103},{\"end\":59008,\"start\":58981},{\"end\":59490,\"start\":59354},{\"end\":60113,\"start\":60027},{\"end\":60599,\"start\":60580},{\"end\":60870,\"start\":60822},{\"end\":61571,\"start\":61249},{\"end\":53025,\"start\":52954},{\"end\":53545,\"start\":53474},{\"end\":54075,\"start\":54003},{\"end\":55155,\"start\":55142},{\"end\":55848,\"start\":55700},{\"end\":57085,\"start\":56980},{\"end\":57675,\"start\":57603},{\"end\":58407,\"start\":58247},{\"end\":59613,\"start\":59492},{\"end\":60199,\"start\":60115}]"}}}, "year": 2023, "month": 12, "day": 17}