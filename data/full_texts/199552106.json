{"id": 199552106, "updated": "2023-10-06 23:47:02.673", "metadata": {"title": "Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data", "authors": "[{\"first\":\"Mikaela\",\"last\":\"Uy\",\"middle\":[\"Angelina\"]},{\"first\":\"Quang-Hieu\",\"last\":\"Pham\",\"middle\":[]},{\"first\":\"Binh-Son\",\"last\":\"Hua\",\"middle\":[]},{\"first\":\"Duc\",\"last\":\"Nguyen\",\"middle\":[\"Thanh\"]},{\"first\":\"Sai-Kit\",\"last\":\"Yeung\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 8, "day": 13}, "abstract": "Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy (~92%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1908.04616", "mag": "2981440248", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/UyPHNY19", "doi": "10.1109/iccv.2019.00167"}}, "content": {"source": {"pdf_hash": "b2b0c31d036941cb557be4afb7101dc1b72f17cb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.04616v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1908.04616", "status": "GREEN"}}, "grobid": {"id": "4dada4b5f0efc71d45ae1e5dd885e7e907d1ba4d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b2b0c31d036941cb557be4afb7101dc1b72f17cb.txt", "contents": "\nRevisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data\n\n\nMikaela Angelina Uy \nHong Kong University of Science and Technology\n\n\nQuang-Hieu Pham \nDesign\nSingapore University of Technology\n\n\nBinh-Son Hua \nThe University of Tokyo\n\n\nDuc Thanh Nguyen \nDeakin University\n\n\nSai-Kit Yeung \nHong Kong University of Science and Technology\n\n\nRevisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data\n\nDeep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy (\u223c92%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page 1 .\n\nIntroduction\n\nThe task of understanding our real world has achieved a great leap in recent years. The rise of powerful computational resources such as GPUs and the availability of 3D data from depth sensors have accelerated the fast-growing field of 3D deep learning. Among various 3D data representations, point clouds are widely used in computer graphics and computer vision thanks to their simplicity. Recent works have shown great promises in solving classical scene understanding problems with point clouds such as 3D object classification and segmentation.\n\nHowever, the current progress on classification with 3D point clouds has witnessed a trend of performance saturation. For example, many recent object classification methods have reported very high accuracies in 2018, and the trend of bringing the accuracy towards perfection is still ongoing. 1 https://hkust-vgd.github.io/scanobjectnn/ This phenomenon inspires us to raise a question on whether problems such as 3D object classification have been totally solved, and to think about how to move forward.\n\nTo answer this question, we perform a benchmark of existing point cloud object classification techniques with both synthetic and real-world data. For synthetic objects, we use ModelNet40 [43], the most popular dataset in point cloud object classification that contains about 10,000 CAD models. To support the investigation of object classification methods on real-world data, we introduce ScanObjectNN, a new point cloud object dataset from the state-of-the-art scene mesh datasets SceneNN [19] and ScanNet [9]. Based on the initial instance segmentation from the scene datasets, we manually filter and select objects for 15 common categories, and further enrich the dataset by considering additional object perturbations.\n\nOur study shows that while the accuracy with CAD data is reaching perfection, learning to classify a real-world object dataset is still a very challenging task. By analyzing the benchmark results, we identify three open issues that are worth to further explore for future researches. First, classification models trained on synthetic data often do not generalize well to real-world data such as point clouds reconstructed from RGB-D scans [19,9], and vice versa. Second, challenging in-context and partial observations of real-world objects are common due to occlusions and reconstruction errors; for example, they can be found in window-based object detectors [38] in many robotics or autonomous vehicle applications. Finally, how to handle background effectively when they appear together with objects due to clutter in the real-world scenes.\n\nAs our dataset opens up opportunities to tackle such open problems in real-world object classification, we also present a new method for point cloud object classification that can improve upon the state-of-the-art results on our dataset by jointly learning the classification and segmentation tasks in a single neural network.\n\nIn summary, we make the following contributions: \u2022 A new object dataset from meshes of scanned real-world scene for training and testing point cloud classification,\n\n\u2022 A comprehensive benchmark of existing object classification techniques on synthetic and real-world point cloud data,\n\n\u2022 A new network architecture that is able to classify objects observed in a real-world setting by a joint learning of classification and segmentation.\n\n\nRelated Works\n\nIn this paper, we focus on object classification with point cloud data, which has advanced greatly in the past few years. We briefly discuss the related works and their datasets, below.\n\nObject Classification on Point Clouds. Early attempts to classifying point clouds were developed by adapting ideas from deep learning on images, e.g., using multiple view images [39,48,46,22], or applying convolutions on 3D voxel grids [27,43]. While it seems natural to extend the convolution operations from 2D to 3D, it is shown that performing convolutions on a point cloud is not a trivial task [30,49]. The difficulty stems from the fact that a point cloud has no well-defined order of points on which convolutions can be performed. Qi et al. [30] addressed this problem by learning global features of point clouds using a symmetric function that is invariant to the order of points. Alternatively, some other methods proposed to learn local features from convolutions, e.g., [32,25,20,42,44,18,2,24,33,11] or from autoencoders [45]. There are also methods jointly learning features from point clouds and multi-view projections [47]. It is also possible to treat point clouds and views as sequences [26,17,15], or to use unsupervised learning [16].\n\nRecent works demonstrate very competitive and compelling performances on standard datasets. For example, the gap between state-of-the-art methods such as SpecGCN [41], SpiderCNN [44], DGCNN [42], PointCNN [25] is less than 1% on ModelNet40 dataset [43]. In the online leaderboard maintained by the authors of ModelNet40, the accuracy of the object classification task is reaching perfection, with 92% for point cloud methods [25,42,44,26].\n\nObject Datasets. There are a limited number of datasets that can be used to train and test 3D object classification methods. ModelNet40 was originally developed by Wu et al. [43] for learning a convolutional deep-belief network to model 3D shapes represented in voxel grids. Objects in ModelNet40 are CAD models of 40 common categories such as airplane, motorbike, chair and table, to name a few. This dataset has been a common benchmark for point cloud object classification [30]. ShapeNet [7] is an alternative large-scale dataset of 3D CAD shapes with approximately 51, 000 objects in 55 categories. However, this set is usually used for benchmarking part segmentation.\n\nSo far, object classification on ModelNet40 is done with the assumption that objects are clean, complete, and free from any background noise. Unfortunately, this assumption is not often held in practice. It is common to see incomplete (partial) objects due to the imperfection of 3D reconstruction. In addition, objects in real-world settings are often scanned when being placed in a scene, which makes them appear in a clutter, and thus may be attached with background elements. A potential treatment is to remove such background using human annotators [28]. However, this solution is tedious, prone to errors, and subjective to the experience of annotators. Other works synthesize challenges on CAD data by introducing noise simulated by Gaussians [4,12] or created with a parametic model [6] to mimic real world scenarios. Recently, the trend of sim2real [3] also aims to bridge the gap between synthetic and real data.\n\nPrior to our work, there are also a few datasets of realworld object scans [10,8,5] but most are small in scale and are not suitable for training object classification networks, which often have thousands of parameters. For example, in robotics, Sydney urban objects dataset [10] contains only 631 objects of 26 categories captured by a LiDAR camera, which is mainly used for evaluation [27,2] but not for training. Some datasets [36,5] are captured in controlled environment which might greatly differ from real-world scenes. Choi et al. [8] proposed a dataset of more than 10,000 object scans in the real world. However, not all of their scans can be successfully reconstructed; the online repository by the authors also provided only about 400 reconstructed objects. RGB-D and 3D scene meshes datasets [19,9,1,37,34] have more objects that are reconstructed along with the scenes, but such objects are often considered in a scene segmentation or object detection task, and not under an object classification setup. RGBD-to-CAD object classification challenge [21,29] provides an object dataset that mixes CAD models and realworld scans. Its goal is to classify RGB-D objects such that a retrieval can be done to find similar CAD models. However, several categories are ambiguous, and objects are supposed to be well segmented before classification. ScanNet [9] has a benchmark on 3D object classification with partially scanned objects. However, this dataset is designed for volume-based object classification [31], and there are quite few techniques that report their results with this data.\n\n\nBenchmark Data\n\nOur goal is to quantitatively analyze the performances of existing object classification methods on point clouds. We split our task into two parts: benchmarking with synthetic data and with real-world data.\n\n\nSynthetic Data -ModelNet40\n\nFor synthetic data, we experiment with the well-known ModelNet40 dataset [43]. This set is a collection of CAD models with 40 object categories. The dataset includes 9,840 objects for training and 2,468 objects for testing. The objects in ModelNet40 are synthetic, and thus are complete,  well-segmented, and noise-free. In this experiment, we use the uniformly dense point cloud variant as preprocessed by Qi et al. [30]. Each point cloud is randomly sampled to 1024 points as input to the networks unless otherwise stated. The point clouds are centered at zero, and we use local coordinates (x, y, z) normalized to [\u22121, 1] as point attributes.\n\nWe follow the default train/test split, and use the default parameters as in the original implementations of the methods. Our benchmark is performed with a NVIDIA Tesla P100 GPU. We re-trained PointNet [30], PointNet++ [32], PointCNN [25], Dynamic Graph CNN (DGCNN) [42], 3D modified Fisher Vector (3DmFV) [2], and SpiderCNN [44]. For remaining methods, we provided the results reported in the original papers. We additionally report each method's best performance when provided with additional information such as point normals. The results are shown in Table 1.\n\nIt can be observed that the performance of recent methods is becoming incremental, and fluctuates around 92%. This saturating score inspires us to revisit the object classification problem: Can classification methods trained on ModelNet40 perform well on real-world data? Or is there still room for more research problems to be explored?\n\n\nReal-World Data -ScanObjectNN\n\nObjects obtained from real-world 3D scans are significantly different from CAD models due to the presence of background noise and the non-uniform density due to holes from incomplete scans/reconstructions and occlusions. This situation is often seen in sliding window-based object detec- tion [38] in which a window may enclose an object of interest partially and also include background elements within the window. Due to these properties, applying existing point cloud classification methods to real-world data may not produce the same good results as CAD models.\n\n\nData Collection\n\nTo study this potential issue, we build a real-world object dataset based on two popular scene meshes datasets: Sce-neNN [19] and ScanNet [9]. SceneNN has 100 annotated Class Bag Bed Bin Box Cabinet Chair Desk Display Door Pillow Shelf Sink Sofa Table Toilet   #Objects  78  135 201 127  347  395  149  181  221  105  267  118  254  242  82   Table 2. Classes and objects in our dataset.\n\nscenes with highly cluttered objects while ScanNet has a larger collection of 1513 indoor scenes. From a total of more than 1600 scenes from SceneNN and ScanNet, we selected 700 unique scenes. We then manually examined each object, fixed inconsistent labels, and discard objects that are ambiguous, have low reconstruction quality, have unknown labels, are too sparse, and have too few instances to form a category for training. During categorization, we also took into account inter-class balancing to avoid any bias potentially coming from classes with more samples.\n\nThe results are 2902 objects that are categorized into 15 categories. The raw objects are represented by a list of points with global and local coordinates, normals, colors attributes and semantic labelsOther works synthesize challenges on CAD data by introducing noise simulated by Gaussians [4,12] or created with a parametic model [6]. Recently, the trend of sim2real [3] also aims to bridge the gap between synthetic and real data. As in the experiment with synthetic data, we sample all raw objects to 1024 points as input to the networks and all methods were trained using only the local (x, y, z) coordinates. We will make our dataset publicly available for future research. Table 2 summarizes classes and objects in our dataset.\n\n\nData Enrichment\n\nBased on the selected objects, we construct several variants that represent different levels of difficulty of our dataset. This allows us to explore the robustness of existing classification methods in more extreme real-world scenarios. Vanilla. The first variant is referred to as OBJ ONLY which includes only ground truth segmented objects extracted from the scene meshes datasets. This variant has the closest form analogous to its CAD counterpart, and is used to investigate the robustness of classification methods to noisy objects with deformed geometric shape and nonuniform surface density. Sample objects of this variant are shown in Figure 2(a). Background. The previous variant assumes that an object can be accurately segmented before being classified. However, in real-world scans, objects are often presented in under-segmentation situations, i.e., background elements or parts of nearby objects are included, and accurate annotations for such under-segmentations are also not always available. Those background elements may provide the context where objects belong to, and thus would become a good hint for object classification, e.g., laptops often sit on desks. However, they may also introduce distractions which corrupt (a) Objects only.\n\n(b) Objects with background. the classification, e.g., a pen may be under-segmented with a table where it sits on and thus could be considered as a part of the table rather than a separate object. To study these factors, we introduce a variant of our dataset where objects are attached with background data (OBJ BG). We determine such background by using the ground truth axis-aligned object bounding boxes. Specifically, given a bounding box, all points in the box are extracted to form an object. Sample objects with background are shown in Figure 2(b).\n\nPerturbed. The given bounding boxes from the groundtruth tightly enclose the objects. However, in real-world scenarios bounding boxes may over-or under-cover, or even split objects. For example, in object detection techniques such as R-CNN [13], object category has to be predicted from a rough bounding box that localizes a candidate object. To simulate this challenge, we extend our dataset by translating, rotating (about the gravity axis), and scaling the ground truth bounding boxes before extracting the geometry in the box. We name the variants of these perturbations with a common prefix PB. The perturbations introduce various degrees of background and partiality to objects. In this work, we use four perturbation variants in the increasing order of difficulty: PB T25, PB T25 R, PB T50 R, and PB T50 RS. Suffix T25 and T50 denote translation that randomly shifts the bounding box up to 25% and 50% of its size from the box centroid along each world axis. Suffix R and S denotes rotation and scaling. Each perturbation variant contains five random samples for each original object, resulting in up to 14, 510 perturbed objects in total. Since perturbation might introduce invalid objects, e.g., objects that are almost completely out of the bounding box of interest, we perform an additional check after perturbation by ensuring that at least 50% of the original object points remain in the bounding box. Objects that do not satisfy this condition are discarded. Sample point clouds of these variants are shown in Figure 3. More details about perturbing objects can be found in our supplementary material. \n\n\nBenchmark on ScanObjectNN\n\nFor a clearer picture of the maturity of point cloud-based object classification, we benchmark several representative methods on our dataset. We aim to identify the limitations of current works on real-world data. We choose 3DmFV [2], PointNet [30], SpiderCNN [44], PointNet++ [32], DGCNN [42] and PointCNN [25] as our representative works.\n\n\nTraining on ModelNet40\n\nWe first study the case when training is done on Model-Net40 and testing is done on ScanObjectNN. Since objects in ModelNet40 are standalone with no background objects, we also removed background in all our variants for fair evaluations. Furthermore, we only evaluated the current methods on 11 (out of 15) common classes between ModelNet40 and our dataset. Please refer to the supplementary material for the details on these common classes.\n\nEvaluation results are reported in Table 3. These results show that the current techniques trained on CAD models are not able to generalize to real-world data; all techniques achieved less than 50% of accuracy. This is expected and is because of the fact that real-world objects and CAD objects are significantly different in their geometry. Real-world objects are often incomplete and partial due to construction errors and occlusions; their surfaces have low-frequency noise; object boundaries are inaccurate. These are in contrast to CAD objects, which are often clean and noise-free. We also found that the harder the data is (i.e. more noise and partiality), the lower the performance is, and this is consistent for all techniques. In other words, knowledge learned from synthetic objects in ModelNet40 is not well transferable and/or applicable to real-world data.\n\n\nTraining on ScanObjectNN\n\nIn this experiment, we train and test the techniques on ScanObjectNN to demonstrate training on datasets with realworld properties should improve the performance in classifying real-world objects. We also analyze how different perturbations can affect the classification performance. We randomly split our dataset into two subsets: training (80%) and test (20%) set. We ensure that the training and test sets contain objects from different scenes so that similar objects do not occur in the same set, e.g. same types of chairs can be found in the same room. We report the performance of all the techniques on the hardest split in Table 4 For fair comparisons, we kept the same data augmentation process in all the methods (e.g., random rotation and per-point jitter). We trained the methods to convergence rather than selecting the best performance on the test set.\n\nVanilla. The 2nd column in Table 4 shows the overall performance of existing methods when trained on the simplest variant of our dataset (OBJ ONLY). This clearly shows that the classification accuracy increased significantly when training and testing are both done using ScanOb-jectNN versus when training is done using ModelNet40 (Table 3 Column 2). However, we also notice an observable performance drop comparing to the pure synthetic setting in Table 1. This gives an important message: point cloud classification on real-world data is still open, a dataset with real-world properties can help, but further research is necessary to regain the high performance as in synthetic setting. In the following, we investigate the performance change in different types of perturbations in our dataset.\n\n\nBackground.\n\nAs shown in Table 4 (2) training on our real-world objects generalizes to CAD evaluation better than the opposite case. background elements could distract the learning in existing methods by confusing between foreground and background points. To further confirm the negative effect of having background objects, we conduct a control experiment using the hardest perturbation variant, i.e., PB T50 RS. Table 5 shows the overall accuracy of all existing models decrease when trained and tested with the presence of background.\n\nPerturbation. Table 4 also shows the impact of perturbations to the classification performance (compared with Column 2). In this result, we observe that translation and rotation both make the classification performance decrease significantly, especially with larger perturbations that introduce more background and partiality. Scale further degrades the performance by a small gap. Figure 4 illustrates the confusion matrices of all methods on our hardest variant PB T50 RS. It can be seen that there are no major ambiguity issues in our categories, and our dataset is challenging due to the high variations in real-world data.\n\nGeneralization to CAD Data. While it is shown that networks trained on synthetic data generalizes poorly to our dataset (Table 3), the reverse is not true. Here we tested the generalization capability of existing methods when trained on ScanObjectNN. In this experiment, all methods were trained on our PB T50 RS (with and without background) and tested on ModelNet40. The results in the last two columns in Table 5 clearly show that existing methods could generalize better when they were trained on real-world data (compared with the results in Table 3). Performance on individual classes are presented in Table 6. As shown in Table 6, lower accuracies are achieved on classes such as bed, cabinet, and desk, where complete structures are never observed in real scans because these objects are often situated adjacent to walls or near corners of rooms. Therefore, we advocate using real-world data in training object classification because the generalization is shown to be much better.\n\n\nPart Annotation on Real-World Data\n\nWe further support part-based annotation in our dataset. So far, point cloud classification methods only evaluate part segmentation task on ShapeNet [40]. However, there has been no publicly available dataset for part segmentation on real-world data despite the availability of scene meshes datasets [19,9]. We close this gap with our dataset, which will be released for future research. Figure 5 shows a visualization of part segmentation on our data. Table 7 and Table 8 provide a baseline part segmentation evaluation on our data. Using these part annotations may also improve partial object classification in the future.\n\n\nDiscussion\n\nOur quantitative evaluations show that performing object classification on real-world data is challenging. The state-ofthe-art methods in our benchmark have up to 78.5% accuracy on our hardest variant (PB T50 RS). The benchmark also helps us recognize the following open problems:\n\nBackground is expected to provide context information but also introduce noise. It is desirable to have an approach that can distinguish foreground from background to effectively exploit context information in the classification task. Object partiality, caused by low reconstruction quality or inaccurate object proposals, also needs to be addressed. Part segmentation techniques [30,25] could help to describe partial objects. Generalization between CAD models and real-world scans needs more investigations. In general, we found that training on real-world data and testing on CADs can generalize better than the opposite case. It could be explained that real-world data have more variations including background and partiality as discussed above. However, CAD models are still important because real-world scans are seldom complete and noise free. Bridging this domain gap could be an important research direction.\n\nTo facilitate future work, in the next sections, we propose ideas and baseline solutions.\n\n\nBackground-aware Classification Network\n\nWe propose here a simple deep network to handle the occurrence of background in point clouds obtained from real scans; this is one of the open problems we raised in the previous section. An issue with existing point cloud classification networks is the lack of capability to distinguish between foreground and background points. In other words, existing methods take point clouds as a whole and directly calculate features for classification. This issue stems from the design of these networks and also from the simplicity of available training datasets, e.g., ModelNet40.\n\nTo tackle this issue, our idea is to make the network aware of the presence of background by adding a segmentationguided branch to the classification network. The segmenta-tion branch predicts an object mask that separates the foreground from the background. Note that the mask can be easily obtained from our training data since our objects are originally from scene instance segmentation datasets [19,9].\n\n\nNetwork Architecture\n\nOur background-aware (BGA) model is built on top of PointNet++ [32] (BGA-PN++). Our network is depicted in Figure 6. In particular, we use three levels of set abstractions from the PointNet++ to extract point cloud global features. Global features are then passed through three fully connected layers to produce object classification score. Dropout is also used in a similar manner with the original PointNet++ architecture. Three PointNet feature propagation modules are then employed to compute object masks in segmentation. The feature vector just before the last fully connected layer for the classification score is used as the input to the first PointNet feature propagation modules, making the predicted object mask driven by the classification output. We trained both branches jointly. The loss function is the sum of the classification and segmentation loss, which can be written as L total = L class + \u03bbL seg where L class and L seg are both cross entropy losses between the predicted and ground-truth class labels and object masks, respectively. We set \u03bb = 0.5 in our experiments.\n\nJoint learning for both classification and segmentation with the use of object masks allows the network to be aware of relevant points (i.e., acknowledge the presence of background points). In addition, using classification prediction as a prior to segmentation guides the network to learn object masks that are consistent with the true shape of desired object classes. As to be detailed in our experiments, jointly learning classification and mask prediction results in better classification accuracy in noisy scenarios.\n\nFurthermore, we also introduce BGA-DGCNN, which is a background-aware network based on DGCNN [42]. We apply the same concept as BGA-PN++ that jointly predicts both classification and segmentation, where the last fully connected layer of the classification branch is used as input to the segmentation branch. Our experimental results show that our bga model is adaptive to different network architectures.\n\n\nEvaluation\n\nWe evaluate our network on both our dataset and Model-Net40.   and existing ones on our hardest variant PB T50 RS and ModelNet40 respectively. Our BGA models, BGA-PN++ and BGA-DGCNN, both outperform their vanilla counterparts with BGA-PN++ achieving the best performance on our PB T50 RS. On ModelNet40, our BGA-PN++ improves upon PointNet++ by almost 5% (with 52.6% of accuracy), while our BGA-DGCNN achieves the top performance of 56.5%. Note that, in this evaluation all methods were trained on our i.e. PB T50 RS. As shown, our BGA models gains improvements in both ModelNet40 and our dataset.\n\nIn addition, we also evaluated the segmentation performance of our network. Experimental results showed that our BGA-PN++ performed at 77.6% and 71.0%, while our BGA-DGCNN achieved 78.5% and 74.3% of segmentation accuracy on our PB T50 RS and ModelNet40, respectively. We visualize some of the object masks predicted by our BGA-PN++ in Figure 7. It can be seen that our proposed network is able to mask out the background fairly accurately.\n\n\nOurs\n\nModelNet40 OA mAcc OA mAcc 3DmFV [2] 63.  \n\n\nDiscussion and Limitation\n\nWhile both BGA models demonstrate good performance, we found that DGCNN-based networks generalizes well between real and CAD data, e.g., when being trained on real and tested on CAD data (Table 9) and vice versa (Table  3). Moreover, Table 3 also show that the same is true for DGCNN-based models on the synthetic to real case. More investigations on the DGCNN architecture could lead to models that generalize better and bridge the gap between synthetic and real data.\n\nOur proposed BGA is not without limitation. In general, it requires object masks and background to be included in the data. Fig. 8-(a) shows a fail case of our method when evaluating on a background-free ModelNet40 object.\n\n\nConclusion\n\nThis paper revisits state-of-the-art object classification methods on point cloud data. We found that existing methods were successful with synthetic data but failed on realistic data. To prove this, we built a new real-world object dataset containing \u223c 15, 000 objects in 15 categories. Compared with current datasets, our dataset offers more practical challenges including background occurrence, object partiality, and different deformation variants. We benchmarked existing methods on our new dataset, discussed issues, identified open problems, and suggested possible solutions. We also proposed a new point cloud network to classify objects with background. Experimental results showed the advance of our method on both synthetic and real-world object datasets.\n\nFigure 2 .\n2Example objects from our dataset.\n\nFigure 3 .\n3An object in different perturbation variants.\n\nFigure 4 .\n4Confusion matrices of (a) 3DmFV[2], (b) PointNet[30], (c) SpiderCNN[44], (d) PointNet++[32], (e) DGCNN[42] and (f) PointCNN[25] on our hardest PB T50 RS. This shows that there are no major ambiguity issues among object classes in our dataset.\n\nFigure 5 .\n5Part segmentation on the chair category. From top to bottom: part prediction, ground truth in 2048 points, and highresolution ground truth from original point clouds.\n\nFigure 6 .\n6Our proposed network.\n\nFigure 7 .\n7Sample objects and their corresponding predicted masks from the test set of PB T50 RS by our BGA-PN++. Note that color on point clouds is for visualization purposes, but the input to the networks are (x, y, z) coordinates only.\n\nFigure 8 .\n8Sample segmentation results of our BGA-PN++ on Mod-elNet40. Background and foreground are marked in orange and blue, respectively.\n\nTable Toilet\nToiletFigure 1. Sample objects from our dataset.\n\n\n. Full performances on all splits are provided in our supplementary material.Table 4. Overall accuracy in % when training and testing were done on ScanObjectNN. The training and testing are done on the same variant. With real-world data, the more background and partiality are introduced, the more challenging the classification task is.O \nB J \n\nO \n\nN \nL Y P B \nT 2 5 \nP B \n\nT 2 5 \n\nR \n\nP B \n\nT 5 0 \n\nR \n\nP B \n\nT 5 0 \n\nR S \n\n3DmFV [2] \n30.9 28.4 27.2 24.5 24.9 \nPointNet [30] \n42.3 37.6 35.3 32.1 31.1 \nSpiderCNN [44] 44.2 37.7 34.5 31.7 30.9 \nPointNet++ [32] 43.6 37.8 37.2 33.3 32.0 \nDGCNN [42] \n49.3 42.4 40.3 36.6 36.8 \nPointCNN [25] \n32.2 28.7 28.1 26.4 24.6 \n\nTable 3. Overall accuracy in % on our dataset when training was \ndone on ModelNet40. Note that for a fair comparison, background \nhas been removed in all variants. The results show that training on \nCAD models and testing on real-world data is challenging. Most \nmethods do not generalize well in this test. \n\nO \nB J \n\nO \n\nN \nL Y O \n\nB J \nB G P B \nT 2 5 \nP B \n\nT 2 5 \n\nR \n\nP B \n\nT 5 0 \n\nR \n\nP B \n\nT 5 0 \n\nR S \n\n3DmFV [2] \n73.8 68.2 67.1 67.4 63.5 63.0 \nPointNet [30] \n79.2 73.3 73.5 72.7 68.2 68.2 \nSpiderCNN [44] 79.5 77.1 78.1 77.7 73.8 73.7 \nPointNet++ [32] 84.3 82.3 82.7 81.4 79.1 77.9 \nDGCNN [42] \n86.2 82.8 83.3 81.5 80.0 78.1 \nPointCNN [25] \n85.5 86.1 83.6 82.5 78.5 78.5 \n\n\n\n\nColumns 3-7, background makes strong impact to the classification performance of all methods. Specifically, except PointCNN[25], all methods performed worse on OBJ BG compared with OBJ ONLY. It can be explained by the fact thatOurs \n\nModelNet40 \nw/o BG w/ BG w/o BG w/ BG \n\n3DmFV [2] \n69.8 \n63.0 \n54.1 \n51.5 \nPointNet [30] \n74.4 \n68.2 \n60.4 \n50.9 \nSpiderCNN [44] \n76.9 \n73.7 \n52.7 \n46.6 \nPointNet++ [32] \n80.2 \n77.9 \n55.0 \n47.4 \nDGCNN [42] \n81.5 \n78.1 \n58.7 \n54.7 \nPointCNN [25] \n80.8 \n78.5 \n38.1 \n49.2 \n\nTable 5. Overall accuracy in % when training on our hardest variant \nPB T50 RS, with and without background (BG) points. Testing is \ndone on the same variant of our dataset, and on ModelNet40. The \nsecond header indicates the results corresponding to the training \nset. The results show that (1) background impacts negatively to \nthe classification performance, and \n\nTable 9\n9shows a comparison between our networkset abstraction 1 \nset abstraction 2 \nset abstraction 3 \n\nfeature vector \n\nFC 512 \nFC 256 \n\nclass vector \n\ninput cloud \n\nFC \n\nclass prediction \n\n\u2295 \n\nfeature propagation 1 \nfeature propagation 2 \nfeature propagation 3 \n\nmask vector \n\nFC 128 \nFC 2 \n\nmask prediction \n\nClassification branch \nSegmentation branch \n\n\n\n\nTable 9. Overall and average class accuracy in % on our PB T50 RS and on ModelNet40. Training is done on our PB T50 RS.0 \n58.1 \n51.5 \n52.2 \nPointNet [30] \n68.2 \n63.4 \n50.9 \n52.7 \nSpiderCNN [44] \n73.7 \n69.8 \n46.6 \n48.8 \nPointNet++ [32] \n77.9 \n75.4 \n47.4 \n45.9 \nDGCNN [42] \n78.1 \n73.6 \n54.7 \n54.9 \nPointCNN [25] \n78.5 \n75.1 \n49.2 \n44.6 \nBGA-PN++ (ours) \n80.2 \n77.5 \n52.6 \n50.6 \nBGA-DGCNN (ours) 79.7 \n75.7 \n56.5 \n57.6 \n\n\nAcknowledgment This research project is partially supported by an internal grant from HKUST (R9429).\nIoannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. Iro Armeni, Ozan Sener, R Amir, Helen Zamir, Jiang, CVPR. Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In CVPR, 2016. 2\n\nThree-dimensional point cloud classification in realtime using convolutional neural networks. Yizhak Ben-Shabat, Michael Lindenbaum, Anath Fischer, IEEE Robotics and Automation Letters. 38Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer. 3dmfv: Three-dimensional point cloud classification in real- time using convolutional neural networks. IEEE Robotics and Automation Letters, 2018. 2, 3, 5, 6, 7, 8\n\nLearning to drive from simulation without real world labels. Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen, Vinh-Dieu Lam, Alex Kendall, International Conference on Robotics and Automation (ICRA). 24Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen, Vinh-Dieu Lam, and Alex Kendall. Learning to drive from simulation without real world labels. In Interna- tional Conference on Robotics and Automation (ICRA), 2019. 2, 4\n\nNoise-resistant deep learning for object classification in three-dimensional point clouds using a point pair descriptor. Dmytro Bobkov, Sili Chen, Ruiqing Jian, Muhammad Z Iqbal, Eckehard Steinbach, IEEE Robotics and Automation Letters. 24Dmytro Bobkov, Sili Chen, Ruiqing Jian, Muhammad Z. Iqbal, and Eckehard Steinbach. Noise-resistant deep learning for object classification in three-dimensional point clouds using a point pair descriptor. IEEE Robotics and Automation Letters, 2018. 2, 4\n\nYale-cmu-berkeley dataset for robotic manipulation research. Berk Calli, Arjun Singh, James Bruce, Aaron Walsman, Kurt Konolige, Siddhartha Srinivasa, Pieter Abbeel, Aaron M Dollar, International Journal of Robotics Research. 2Berk Calli, Arjun Singh, James Bruce, Aaron Walsman, Kurt Konolige, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. Yale-cmu-berkeley dataset for robotic manipulation research. International Journal of Robotics Research, 2017. 2\n\nMitigation of effects of occlusion on object recognition with deep neural networks through low-level image completion. Ben Chandler, Ennio Mingolla, Comp. Int. and Neurosc. 24Ben Chandler and Ennio Mingolla. Mitigation of effects of occlusion on object recognition with deep neural networks through low-level image completion. In Comp. Int. and Neu- rosc., 2016. 2, 4\n\nShapeNet: An Information-Rich 3D Model Repository. Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu, arXiv:1512.03012Stanford University -Princeton University -Toyota Technological Institute at ChicagoTechnical ReportAngel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Reposi- tory. Technical Report arXiv:1512.03012, Stanford University -Princeton University -Toyota Technological Institute at Chicago, 2015. 2\n\nSungjoon Choi, Qian-Yi Zhou, Stephen Miller, Vladlen Koltun, arXiv:1602.02481A large dataset of object scans. Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. A large dataset of object scans. arXiv:1602.02481, 2016. 2\n\nScannet: Richlyannotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Niessner, CVPR. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly- annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 1, 2, 3, 6, 7\n\nUnsupervised feature learning for classification of outdoor 3d scans. Alastair Mark De Deuge, Calvin Quadros, Bertrand Hung, Douillard, Australasian Conference on Robotics and Automation. Mark De Deuge, Alastair Quadros, Calvin Hung, and Bertrand Douillard. Unsupervised feature learning for classi- fication of outdoor 3d scans. In Australasian Conference on Robotics and Automation, 2013. 2\n\nGeneral-purpose deep point cloud feature extractor. M Dominguez, R Dhamdhere, A Petkar, S Jain, S Sah, R Ptucha, In WACV. 2M. Dominguez, R. Dhamdhere, A. Petkar, S. Jain, S. Sah, and R. Ptucha. General-purpose deep point cloud feature extractor. In WACV, 2018. 2\n\nA study of the effect of noise and occlusion on the accuracy of convolutional neural networks applied to 3d object recognition. Computer Vision and Image Understanding. Alberto Garcia-Garcia, Jose Rodriguez, Sergio Orts, Sergiu Oprea, Francisco Gomez-Donoso, Miguel Cazorla, 24Alberto Garcia-Garcia, Jose Rodriguez, Sergio Orts, Sergiu Oprea, Francisco Gomez-Donoso, and Miguel Cazorla. A study of the effect of noise and occlusion on the accuracy of convolutional neural networks applied to 3d object recogni- tion. Computer Vision and Image Understanding, 2017. 2, 4\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, CVPR. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 4\n\nFlex-convolution (million-scale point-cloud learning beyond grid-worlds). Fabian Groh, Patrick Wieschollek, Hendrik P A Lensch, In ACCV. 3Fabian Groh, Patrick Wieschollek, and Hendrik P. A. Lensch. Flex-convolution (million-scale point-cloud learning beyond grid-worlds). In ACCV, 2018. 3\n\nAggregating sequential views for 3d global feature learning by cnn with hierarchical attention aggregation. Z Han, H Lu, Z Liu, C Vong, Y Liua, M Zwicker, J Han, C L P Chen, IEEE Transactions on Image Processing. 32Z. Han, H. Lu, Z. Liu, C. Vong, Y. Liua, M. Zwicker, J. Han, and C. L. P. Chen. 3d2seqviews: Aggregating sequential views for 3d global feature learning by cnn with hierarchical attention aggregation. IEEE Transactions on Image Process- ing, 2019. 2\n\nView inter-prediction GAN: unsupervised representation learning for 3d shapes by learning global shape memories to support local view predictions. Zhizhong Han, Mingyang Shang, Yu-Shen Liu, Matthias Zwicker, AAAI. Zhizhong Han, Mingyang Shang, Yu-Shen Liu, and Matthias Zwicker. View inter-prediction GAN: unsupervised repre- sentation learning for 3d shapes by learning global shape memories to support local view predictions. In AAAI, 2018. 2\n\nY\u02c62seq2seq: Cross-modal representation learning for 3d shape and text by joint reconstruction and prediction of view and word sequences. Zhizhong Han, Mingyang Shang, Xiyang Wang, Yu-Shen Liu, Matthias Zwicker, AAAI. Zhizhong Han, Mingyang Shang, Xiyang Wang, Yu-Shen Liu, and Matthias Zwicker. Y\u02c62seq2seq: Cross-modal representa- tion learning for 3d shape and text by joint reconstruction and prediction of view and word sequences. In AAAI, 2019. 2\n\nMonte carlo convolution for learning on nonuniformly sampled point clouds. P Hermosilla, T Ritschel, P-P Vazquez, A Vinacua, T Ropinski, Proceedings of SIGGRAPH Asia). SIGGRAPH Asia)P. Hermosilla, T. Ritschel, P-P Vazquez, A. Vinacua, and T. Ropinski. Monte carlo convolution for learning on non- uniformly sampled point clouds. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2018. 2\n\nScenenn: A scene meshes dataset with annotations. International Conference on 3D Vision (3DV). Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung27Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh- Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung. Scenenn: A scene meshes dataset with annotations. In International Conference on 3D Vision (3DV), 2016. http://www.scenenn.net. 1, 2, 3, 6, 7\n\nPointwise convolutional neural networks. Binh-Son, Minh-Khoi Hua, Sai-Kit Tran, Yeung, In CVPR. 2Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point- wise convolutional neural networks. In CVPR, 2018. 2\n\nD to CAD Retrieval with ObjectNN Dataset. Binh-Son, Quang-Trung Hua, Minh-Khoi Truong, Quang-Hieu Tran, Asako Pham, Tang Kanezaki, Hungyueh Lee, Winston Chiang, Bo Hsu, Yijuan Li, Henry Lu, Shoki Johan, Masaki Tashiro, Aono, Eurographics Workshop on 3D Object Retrieval. Minh-Triet Tran, Viet-Khoi Pham, Hai-Dang Nguyen, Vinh-Tiep Nguyen, Quang-Thang Tran, Thuyen V. Phan, Bao Truong, Minh N. Do, Anh-Duc Duong, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung. RGB-Binh-Son Hua, Quang-Trung Truong, Minh-Khoi Tran, Quang-Hieu Pham, Asako Kanezaki, Tang Lee, HungYueh Chiang, Winston Hsu, Bo Li, Yijuan Lu, Henry Johan, Shoki Tashiro, Masaki Aono, Minh-Triet Tran, Viet-Khoi Pham, Hai-Dang Nguyen, Vinh-Tiep Nguyen, Quang-Thang Tran, Thuyen V. Phan, Bao Truong, Minh N. Do, Anh-Duc Duong, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung. RGB-D to CAD Retrieval with ObjectNN Dataset. In Eurographics Workshop on 3D Object Retrieval, 2017. 2\n\nRotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida, CVPR. Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. In CVPR, 2018. 2\n\nEscape from cells: Deep kd-networks for the recognition of 3d point cloud models. Roman Klokov, Victor S Lempitsky, Roman Klokov and Victor S. Lempitsky. Escape from cells: Deep kd-networks for the recognition of 3d point cloud mod- els. 2017. 3\n\nSo-net: Selforganizing network for point cloud analysis. Jiaxin Li, M Ben, Gim Hee Chen, Lee, CVPR. 23Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self- organizing network for point cloud analysis. In CVPR, 2018. 2, 3\n\nPointcnn: Convolution on x-transformed points. Yangyan Li, Rui Bu, Mingchao Sun, Baoquan Chen, Advances in Neural Information Processing Systems. 78Yangyan Li, Rui Bu, Mingchao Sun, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. Advances in Neural Information Processing Systems, 2018. 2, 3, 5, 6, 7, 8\n\nPoint2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network. Xinhai Liu, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker, arXiv:1811.02565Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Point2sequence: Learning the shape representa- tion of 3d point clouds with an attention-based sequence to sequence network. arXiv:1811.02565, 2018. 2\n\nVoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition. D Maturana, S Scherer, IROS. D. Maturana and S. Scherer. VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition. In IROS, 2015. 2\n\nA robust 3d-2d interactive tool for scene segmentation and annotation. Binh-Son Duc Thanh Nguyen, Lap-Fai Hua, Sai-Kit Yu, Yeung, IEEE Transactions on Visualization and Computer Graphics. 2TVCGDuc Thanh Nguyen, Binh-Son Hua, Lap-Fai Yu, and Sai-Kit Yeung. A robust 3d-2d interactive tool for scene segmentation and annotation. IEEE Transactions on Visualization and Computer Graphics (TVCG), 2017. 2\n\nObject-to-CAD Retrieval. Quang-Hieu Pham, Minh-Khoi Tran, Wenhui Li, Shu Xiang, Heyu Zhou, Weizhi Nie, Anan Liu, Yuting Su, Minh-Triet Tran, Ngoc-Minh Bui, Trong-Le Do, Tu V Ninh, Tu-Khiem Le, Anh-Vu Dao, Eurographics Workshop on 3D Object Retrieval. Vinh-Tiep Nguyen, Minh N. Do, Anh-Duc Duong, Binh-Son Hua, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung. RGB-DQuang-Hieu Pham, Minh-Khoi Tran, Wenhui Li, Shu Xiang, Heyu Zhou, Weizhi Nie, Anan Liu, Yuting Su, Minh-Triet Tran, Ngoc-Minh Bui, Trong-Le Do, Tu V. Ninh, Tu-Khiem Le, Anh-Vu Dao, Vinh-Tiep Nguyen, Minh N. Do, Anh- Duc Duong, Binh-Son Hua, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung. RGB-D Object-to-CAD Retrieval. In Eurographics Workshop on 3D Object Retrieval, 2018. 2\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, CVPR. 78Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. CVPR, 2017. 2, 3, 5, 6, 7, 8\n\nVolumetric and multi-view cnns for object classification on 3d data. Charles R Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, Leonidas J Guibas, CVPR. Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, and Leonidas J. Guibas. Volumetric and multi-view cnns for object classification on 3d data. In CVPR, 2016. 2\n\nPoint-net++: Deep hierarchical feature learning on point sets in a metric space. Li Charles R Qi, Hao Yi, Leonidas J Su, Guibas, Advances in Neural Information Processing Systems. 7Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point- net++: Deep hierarchical feature learning on point sets in a metric space. Advances in Neural Information Processing Systems, 2017. 2, 3, 5, 6, 7, 8\n\nMining point cloud local structures by kernel correlation and graph pooling. Yiru Shen, Chen Feng, Yaoqing Yang, Dong Tian, CVPR. 23Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Mining point cloud local structures by kernel correlation and graph pooling. In CVPR, 2018. 2, 3\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, ECCV. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 2\n\nDynamic edgeconditioned filters in convolutional neural networks on graphs. Martin Simonovsky, Nikos Komodakis, CVPR. Martin Simonovsky and Nikos Komodakis. Dynamic edge- conditioned filters in convolutional neural networks on graphs. In CVPR, 2017. 3\n\nBigbird: A large-scale 3d database of object instances. A Singh, J Sha, K S Narayan, T Achim, P Abbeel, International Conference on Robotics and Automation (ICRA). A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel. Bigbird: A large-scale 3d database of object instances. In In- ternational Conference on Robotics and Automation (ICRA), 2014. 2\n\nSun rgb-d: A rgb-d scene understanding benchmark suite. Shuran Song, Samuel P Lichtenberg, Jianxiong Xiao, CVPR. Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In CVPR, 2015. 2\n\nDeep Sliding Shapes for amodal 3D object detection in RGB-D images. Shuran Song, Jianxiong Xiao, CVPR. 13Shuran Song and Jianxiong Xiao. Deep Sliding Shapes for amodal 3D object detection in RGB-D images. In CVPR, 2016. 1, 3\n\nLearned-Miller. Multi-view convolutional neural networks for 3d shape recognition. Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik G , ICCV. Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik G. Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In ICCV, 2015. 2\n\nSemanticpaint: Interactive 3d labeling and learning at your fingertips. Julien Valentin, Vibhav Vineet, Ming-Ming Cheng, David Kim, Jamie Shotton, Pushmeet Kohli, Matthias Nie\u00dfner, Antonio Criminisi, Shahram Izadi, Philip Torr, ACM Transactions on Graphics. 6Julien Valentin, Vibhav Vineet, Ming-Ming Cheng, David Kim, Jamie Shotton, Pushmeet Kohli, Matthias Nie\u00dfner, An- tonio Criminisi, Shahram Izadi, and Philip Torr. Semantic- paint: Interactive 3d labeling and learning at your fingertips. ACM Transactions on Graphics, 2015. 6\n\nLocal spectral graph convolution for point set feature learning. Chu Wang, Babak Samari, Kaleem Siddiqi, ECCV. 23Chu Wang, Babak Samari, and Kaleem Siddiqi. Local spectral graph convolution for point set feature learning. ECCV, 2018. 2, 3\n\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, Justin M Solomon, arXiv:1801.07829Dynamic graph cnn for learning on point clouds. 7arXiv preprintYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829, 2018. 2, 3, 5, 6, 7, 8\n\nXiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. Zhirong Wu, Shuran Song, Aditya Khosla, CVPR. 1Zhirong Wu, Shuran Song, Aditya Khosla, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015. 1, 2\n\nSpidercnn: Deep learning on point sets with parameterized convolutional filters. Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, Yu Qiao, ECCV. 7Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In ECCV, 2018. 2, 3, 5, 6, 7, 8\n\nFoldingnet: Point cloud auto-encoder via deep grid deformation. Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian, CVPR. Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold- ingnet: Point cloud auto-encoder via deep grid deformation. In CVPR, 2018. 2\n\nSpnet: Deep 3d object classification and retrieval using stereographic projection. Mohsen Yavartanoo, Euyoung Kim, ACCV. Mohsen Yavartanoo and Euyoung Kim. Spnet: Deep 3d object classification and retrieval using stereographic projection. In ACCV, 2018. 2\n\nPvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition. Haoxuan You, Yifan Feng, Rongrong Ji, Yue Gao, Proceedings of the ACM International Conference on Multimedia. the ACM International Conference on MultimediaHaoxuan You, Yifan Feng, Rongrong Ji, and Yue Gao. Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition. In Proceedings of the ACM Interna- tional Conference on Multimedia, 2018. 2\n\nMulti-view harmonized bilinear network for 3d object recognition. Tan Yu, Jingjing Meng, Junsong Yuan, CVPR. Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view har- monized bilinear network for 3d object recognition. In CVPR, 2018. 2\n\nSiamak Ravanbakhsh, Barnabas Poczos. Manzil Zaheer, Satwik Kottur ; Ruslan, R Salakhutdinov, Alexander J Smola, Advances in Neural Information Processing Systems. 23Deep setsManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barn- abas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, 2017. 2, 3\n", "annotations": {"author": "[{\"end\":179,\"start\":110},{\"end\":240,\"start\":180},{\"end\":280,\"start\":241},{\"end\":318,\"start\":281},{\"end\":382,\"start\":319}]", "publisher": null, "author_last_name": "[{\"end\":129,\"start\":127},{\"end\":195,\"start\":191},{\"end\":253,\"start\":250},{\"end\":297,\"start\":291},{\"end\":332,\"start\":327}]", "author_first_name": "[{\"end\":117,\"start\":110},{\"end\":126,\"start\":118},{\"end\":190,\"start\":180},{\"end\":249,\"start\":241},{\"end\":284,\"start\":281},{\"end\":290,\"start\":285},{\"end\":326,\"start\":319}]", "author_affiliation": "[{\"end\":178,\"start\":131},{\"end\":239,\"start\":197},{\"end\":279,\"start\":255},{\"end\":317,\"start\":299},{\"end\":381,\"start\":334}]", "title": "[{\"end\":107,\"start\":1},{\"end\":489,\"start\":383}]", "venue": null, "abstract": "[{\"end\":1650,\"start\":491}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2510,\"start\":2509},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2912,\"start\":2908},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3215,\"start\":3211},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3231,\"start\":3228},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3888,\"start\":3884},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3890,\"start\":3888},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4110,\"start\":4106},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5442,\"start\":5438},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5445,\"start\":5442},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5448,\"start\":5445},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5451,\"start\":5448},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5500,\"start\":5496},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5503,\"start\":5500},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5664,\"start\":5660},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5667,\"start\":5664},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5813,\"start\":5809},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6046,\"start\":6042},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6049,\"start\":6046},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6052,\"start\":6049},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6055,\"start\":6052},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6058,\"start\":6055},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6061,\"start\":6058},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6063,\"start\":6061},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6066,\"start\":6063},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6069,\"start\":6066},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6072,\"start\":6069},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6098,\"start\":6094},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6198,\"start\":6194},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6269,\"start\":6265},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6272,\"start\":6269},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6275,\"start\":6272},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6313,\"start\":6309},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6482,\"start\":6478},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6498,\"start\":6494},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6510,\"start\":6506},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6525,\"start\":6521},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6568,\"start\":6564},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6745,\"start\":6741},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6748,\"start\":6745},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6751,\"start\":6748},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6754,\"start\":6751},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6935,\"start\":6931},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7237,\"start\":7233},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7251,\"start\":7248},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7989,\"start\":7985},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8184,\"start\":8181},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8187,\"start\":8184},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8225,\"start\":8222},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8292,\"start\":8289},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8434,\"start\":8430},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8436,\"start\":8434},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8438,\"start\":8436},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8634,\"start\":8630},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8746,\"start\":8742},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8748,\"start\":8746},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8789,\"start\":8785},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8791,\"start\":8789},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8897,\"start\":8894},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9164,\"start\":9160},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9166,\"start\":9164},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9168,\"start\":9166},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9171,\"start\":9168},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9174,\"start\":9171},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9421,\"start\":9417},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9424,\"start\":9421},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9718,\"start\":9715},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9872,\"start\":9868},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10283,\"start\":10279},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10627,\"start\":10623},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11059,\"start\":11055},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11076,\"start\":11072},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11091,\"start\":11087},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11123,\"start\":11119},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11162,\"start\":11159},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11182,\"start\":11178},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12086,\"start\":12082},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12499,\"start\":12495},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12515,\"start\":12512},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13629,\"start\":13626},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13632,\"start\":13629},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13670,\"start\":13667},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13707,\"start\":13704},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16148,\"start\":16144},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17783,\"start\":17780},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17798,\"start\":17794},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17814,\"start\":17810},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17831,\"start\":17827},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17843,\"start\":17839},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17861,\"start\":17857},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23273,\"start\":23269},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23424,\"start\":23420},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23426,\"start\":23424},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24425,\"start\":24421},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24428,\"start\":24425},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26070,\"start\":26066},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26072,\"start\":26070},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26165,\"start\":26161},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27811,\"start\":27807},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29217,\"start\":29214},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30881,\"start\":30878},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30899,\"start\":30895},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30918,\"start\":30914},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30938,\"start\":30934},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30953,\"start\":30949},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30974,\"start\":30970},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33233,\"start\":33229}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":30774,\"start\":30728},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30833,\"start\":30775},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31089,\"start\":30834},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31269,\"start\":31090},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31304,\"start\":31270},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31545,\"start\":31305},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31689,\"start\":31546},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31752,\"start\":31690},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33103,\"start\":31753},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33977,\"start\":33104},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34337,\"start\":33978},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34758,\"start\":34338}]", "paragraph": "[{\"end\":2214,\"start\":1666},{\"end\":2719,\"start\":2216},{\"end\":3443,\"start\":2721},{\"end\":4289,\"start\":3445},{\"end\":4617,\"start\":4291},{\"end\":4783,\"start\":4619},{\"end\":4903,\"start\":4785},{\"end\":5055,\"start\":4905},{\"end\":5258,\"start\":5073},{\"end\":6314,\"start\":5260},{\"end\":6755,\"start\":6316},{\"end\":7429,\"start\":6757},{\"end\":8353,\"start\":7431},{\"end\":9950,\"start\":8355},{\"end\":10175,\"start\":9969},{\"end\":10851,\"start\":10206},{\"end\":11416,\"start\":10853},{\"end\":11755,\"start\":11418},{\"end\":12354,\"start\":11789},{\"end\":12761,\"start\":12374},{\"end\":13331,\"start\":12763},{\"end\":14069,\"start\":13333},{\"end\":15345,\"start\":14089},{\"end\":15902,\"start\":15347},{\"end\":17520,\"start\":15904},{\"end\":17890,\"start\":17550},{\"end\":18358,\"start\":17917},{\"end\":19230,\"start\":18360},{\"end\":20124,\"start\":19259},{\"end\":20922,\"start\":20126},{\"end\":21462,\"start\":20938},{\"end\":22091,\"start\":21464},{\"end\":23081,\"start\":22093},{\"end\":23744,\"start\":23120},{\"end\":24039,\"start\":23759},{\"end\":24958,\"start\":24041},{\"end\":25049,\"start\":24960},{\"end\":25665,\"start\":25093},{\"end\":26073,\"start\":25667},{\"end\":27189,\"start\":26098},{\"end\":27712,\"start\":27191},{\"end\":28118,\"start\":27714},{\"end\":28730,\"start\":28133},{\"end\":29172,\"start\":28732},{\"end\":29223,\"start\":29181},{\"end\":29722,\"start\":29253},{\"end\":29946,\"start\":29724},{\"end\":30727,\"start\":29961}]", "formula": null, "table_ref": "[{\"end\":11415,\"start\":11408},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12724,\"start\":12620},{\"end\":14022,\"start\":14015},{\"end\":18402,\"start\":18395},{\"end\":19896,\"start\":19889},{\"end\":20160,\"start\":20153},{\"end\":20465,\"start\":20457},{\"end\":20582,\"start\":20575},{\"end\":20957,\"start\":20950},{\"end\":21346,\"start\":21339},{\"end\":21485,\"start\":21478},{\"end\":22222,\"start\":22213},{\"end\":22508,\"start\":22501},{\"end\":22647,\"start\":22640},{\"end\":22708,\"start\":22701},{\"end\":22729,\"start\":22722},{\"end\":23592,\"start\":23573},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29448,\"start\":29440},{\"end\":29475,\"start\":29465},{\"end\":29494,\"start\":29487}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1664,\"start\":1652},{\"attributes\":{\"n\":\"2.\"},\"end\":5071,\"start\":5058},{\"attributes\":{\"n\":\"3.\"},\"end\":9967,\"start\":9953},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10204,\"start\":10178},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11787,\"start\":11758},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":12372,\"start\":12357},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14087,\"start\":14072},{\"attributes\":{\"n\":\"4.\"},\"end\":17548,\"start\":17523},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17915,\"start\":17893},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19257,\"start\":19233},{\"end\":20936,\"start\":20925},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23118,\"start\":23084},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23757,\"start\":23747},{\"attributes\":{\"n\":\"5.\"},\"end\":25091,\"start\":25052},{\"attributes\":{\"n\":\"5.1.\"},\"end\":26096,\"start\":26076},{\"attributes\":{\"n\":\"5.2.\"},\"end\":28131,\"start\":28121},{\"end\":29179,\"start\":29175},{\"attributes\":{\"n\":\"5.3.\"},\"end\":29251,\"start\":29226},{\"attributes\":{\"n\":\"6.\"},\"end\":29959,\"start\":29949},{\"end\":30739,\"start\":30729},{\"end\":30786,\"start\":30776},{\"end\":30845,\"start\":30835},{\"end\":31101,\"start\":31091},{\"end\":31281,\"start\":31271},{\"end\":31316,\"start\":31306},{\"end\":31557,\"start\":31547},{\"end\":31703,\"start\":31691},{\"end\":33986,\"start\":33979}]", "table": "[{\"end\":33103,\"start\":32092},{\"end\":33977,\"start\":33333},{\"end\":34337,\"start\":34026},{\"end\":34758,\"start\":34459}]", "figure_caption": "[{\"end\":30774,\"start\":30741},{\"end\":30833,\"start\":30788},{\"end\":31089,\"start\":30847},{\"end\":31269,\"start\":31103},{\"end\":31304,\"start\":31283},{\"end\":31545,\"start\":31318},{\"end\":31689,\"start\":31559},{\"end\":31752,\"start\":31710},{\"end\":32092,\"start\":31755},{\"end\":33333,\"start\":33106},{\"end\":34026,\"start\":33988},{\"end\":34459,\"start\":34340}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14740,\"start\":14732},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15898,\"start\":15890},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17436,\"start\":17428},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21854,\"start\":21846},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23516,\"start\":23508},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26213,\"start\":26205},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29076,\"start\":29068},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29858,\"start\":29848}]", "bib_author_first_name": "[{\"end\":34968,\"start\":34965},{\"end\":34981,\"start\":34977},{\"end\":34990,\"start\":34989},{\"end\":35002,\"start\":34997},{\"end\":35298,\"start\":35292},{\"end\":35318,\"start\":35311},{\"end\":35336,\"start\":35331},{\"end\":35674,\"start\":35670},{\"end\":35690,\"start\":35683},{\"end\":35705,\"start\":35699},{\"end\":35718,\"start\":35711},{\"end\":35733,\"start\":35726},{\"end\":35749,\"start\":35740},{\"end\":35759,\"start\":35755},{\"end\":36199,\"start\":36193},{\"end\":36212,\"start\":36208},{\"end\":36226,\"start\":36219},{\"end\":36241,\"start\":36233},{\"end\":36243,\"start\":36242},{\"end\":36259,\"start\":36251},{\"end\":36630,\"start\":36626},{\"end\":36643,\"start\":36638},{\"end\":36656,\"start\":36651},{\"end\":36669,\"start\":36664},{\"end\":36683,\"start\":36679},{\"end\":36704,\"start\":36694},{\"end\":36722,\"start\":36716},{\"end\":36738,\"start\":36731},{\"end\":37153,\"start\":37150},{\"end\":37169,\"start\":37164},{\"end\":37456,\"start\":37451},{\"end\":37458,\"start\":37457},{\"end\":37472,\"start\":37466},{\"end\":37493,\"start\":37485},{\"end\":37505,\"start\":37502},{\"end\":37522,\"start\":37516},{\"end\":37534,\"start\":37530},{\"end\":37545,\"start\":37539},{\"end\":37563,\"start\":37556},{\"end\":37577,\"start\":37571},{\"end\":37587,\"start\":37584},{\"end\":37601,\"start\":37592},{\"end\":37610,\"start\":37608},{\"end\":37621,\"start\":37615},{\"end\":38115,\"start\":38107},{\"end\":38129,\"start\":38122},{\"end\":38143,\"start\":38136},{\"end\":38159,\"start\":38152},{\"end\":38410,\"start\":38404},{\"end\":38417,\"start\":38416},{\"end\":38432,\"start\":38425},{\"end\":38446,\"start\":38440},{\"end\":38460,\"start\":38454},{\"end\":38477,\"start\":38469},{\"end\":38777,\"start\":38769},{\"end\":38799,\"start\":38793},{\"end\":38817,\"start\":38809},{\"end\":39146,\"start\":39145},{\"end\":39159,\"start\":39158},{\"end\":39172,\"start\":39171},{\"end\":39182,\"start\":39181},{\"end\":39190,\"start\":39189},{\"end\":39197,\"start\":39196},{\"end\":39533,\"start\":39526},{\"end\":39553,\"start\":39549},{\"end\":39571,\"start\":39565},{\"end\":39584,\"start\":39578},{\"end\":39601,\"start\":39592},{\"end\":39622,\"start\":39616},{\"end\":40013,\"start\":40009},{\"end\":40028,\"start\":40024},{\"end\":40044,\"start\":40038},{\"end\":40062,\"start\":40054},{\"end\":40321,\"start\":40315},{\"end\":40335,\"start\":40328},{\"end\":40356,\"start\":40349},{\"end\":40360,\"start\":40357},{\"end\":40640,\"start\":40639},{\"end\":40647,\"start\":40646},{\"end\":40653,\"start\":40652},{\"end\":40660,\"start\":40659},{\"end\":40668,\"start\":40667},{\"end\":40676,\"start\":40675},{\"end\":40687,\"start\":40686},{\"end\":40694,\"start\":40693},{\"end\":40698,\"start\":40695},{\"end\":41152,\"start\":41144},{\"end\":41166,\"start\":41158},{\"end\":41181,\"start\":41174},{\"end\":41195,\"start\":41187},{\"end\":41588,\"start\":41580},{\"end\":41602,\"start\":41594},{\"end\":41616,\"start\":41610},{\"end\":41630,\"start\":41623},{\"end\":41644,\"start\":41636},{\"end\":41971,\"start\":41970},{\"end\":41985,\"start\":41984},{\"end\":41999,\"start\":41996},{\"end\":42010,\"start\":42009},{\"end\":42021,\"start\":42020},{\"end\":42785,\"start\":42776},{\"end\":42798,\"start\":42791},{\"end\":42995,\"start\":42984},{\"end\":43010,\"start\":43001},{\"end\":43029,\"start\":43019},{\"end\":43041,\"start\":43036},{\"end\":43052,\"start\":43048},{\"end\":43071,\"start\":43063},{\"end\":43084,\"start\":43077},{\"end\":43095,\"start\":43093},{\"end\":43107,\"start\":43101},{\"end\":43117,\"start\":43112},{\"end\":43127,\"start\":43122},{\"end\":43141,\"start\":43135},{\"end\":43986,\"start\":43981},{\"end\":44005,\"start\":43997},{\"end\":44027,\"start\":44018},{\"end\":44316,\"start\":44311},{\"end\":44331,\"start\":44325},{\"end\":44333,\"start\":44332},{\"end\":44539,\"start\":44533},{\"end\":44545,\"start\":44544},{\"end\":44558,\"start\":44551},{\"end\":44752,\"start\":44745},{\"end\":44760,\"start\":44757},{\"end\":44773,\"start\":44765},{\"end\":44786,\"start\":44779},{\"end\":45149,\"start\":45143},{\"end\":45163,\"start\":45155},{\"end\":45176,\"start\":45169},{\"end\":45190,\"start\":45182},{\"end\":45506,\"start\":45505},{\"end\":45518,\"start\":45517},{\"end\":45735,\"start\":45727},{\"end\":45761,\"start\":45754},{\"end\":45774,\"start\":45767},{\"end\":46092,\"start\":46082},{\"end\":46108,\"start\":46099},{\"end\":46121,\"start\":46115},{\"end\":46129,\"start\":46126},{\"end\":46141,\"start\":46137},{\"end\":46154,\"start\":46148},{\"end\":46164,\"start\":46160},{\"end\":46176,\"start\":46170},{\"end\":46191,\"start\":46181},{\"end\":46207,\"start\":46198},{\"end\":46221,\"start\":46213},{\"end\":46228,\"start\":46226},{\"end\":46230,\"start\":46229},{\"end\":46245,\"start\":46237},{\"end\":46256,\"start\":46250},{\"end\":46881,\"start\":46878},{\"end\":46903,\"start\":46896},{\"end\":46916,\"start\":46908},{\"end\":46918,\"start\":46917},{\"end\":47180,\"start\":47173},{\"end\":47182,\"start\":47181},{\"end\":47190,\"start\":47187},{\"end\":47203,\"start\":47195},{\"end\":47220,\"start\":47214},{\"end\":47234,\"start\":47226},{\"end\":47248,\"start\":47240},{\"end\":47250,\"start\":47249},{\"end\":47527,\"start\":47525},{\"end\":47545,\"start\":47542},{\"end\":47558,\"start\":47550},{\"end\":47560,\"start\":47559},{\"end\":47915,\"start\":47911},{\"end\":47926,\"start\":47922},{\"end\":47940,\"start\":47933},{\"end\":47951,\"start\":47947},{\"end\":48181,\"start\":48175},{\"end\":48198,\"start\":48193},{\"end\":48214,\"start\":48206},{\"end\":48225,\"start\":48222},{\"end\":48463,\"start\":48457},{\"end\":48481,\"start\":48476},{\"end\":48691,\"start\":48690},{\"end\":48700,\"start\":48699},{\"end\":48707,\"start\":48706},{\"end\":48709,\"start\":48708},{\"end\":48720,\"start\":48719},{\"end\":48729,\"start\":48728},{\"end\":49048,\"start\":49042},{\"end\":49061,\"start\":49055},{\"end\":49063,\"start\":49062},{\"end\":49086,\"start\":49077},{\"end\":49303,\"start\":49297},{\"end\":49319,\"start\":49310},{\"end\":49542,\"start\":49538},{\"end\":49556,\"start\":49547},{\"end\":49572,\"start\":49563},{\"end\":49590,\"start\":49586},{\"end\":49592,\"start\":49591},{\"end\":49840,\"start\":49834},{\"end\":49857,\"start\":49851},{\"end\":49875,\"start\":49866},{\"end\":49888,\"start\":49883},{\"end\":49899,\"start\":49894},{\"end\":49917,\"start\":49909},{\"end\":49933,\"start\":49925},{\"end\":49950,\"start\":49943},{\"end\":49969,\"start\":49962},{\"end\":49983,\"start\":49977},{\"end\":50364,\"start\":50361},{\"end\":50376,\"start\":50371},{\"end\":50391,\"start\":50385},{\"end\":50539,\"start\":50536},{\"end\":50553,\"start\":50546},{\"end\":50564,\"start\":50559},{\"end\":50576,\"start\":50570},{\"end\":50578,\"start\":50577},{\"end\":50593,\"start\":50586},{\"end\":50595,\"start\":50594},{\"end\":50613,\"start\":50607},{\"end\":50615,\"start\":50614},{\"end\":51004,\"start\":50997},{\"end\":51015,\"start\":51009},{\"end\":51028,\"start\":51022},{\"end\":51283,\"start\":51278},{\"end\":51294,\"start\":51288},{\"end\":51306,\"start\":51300},{\"end\":51315,\"start\":51311},{\"end\":51324,\"start\":51322},{\"end\":51580,\"start\":51573},{\"end\":51591,\"start\":51587},{\"end\":51602,\"start\":51598},{\"end\":51613,\"start\":51609},{\"end\":51850,\"start\":51844},{\"end\":51870,\"start\":51863},{\"end\":52118,\"start\":52111},{\"end\":52129,\"start\":52124},{\"end\":52144,\"start\":52136},{\"end\":52152,\"start\":52149},{\"end\":52557,\"start\":52554},{\"end\":52570,\"start\":52562},{\"end\":52584,\"start\":52577},{\"end\":52767,\"start\":52761},{\"end\":52782,\"start\":52776},{\"end\":52801,\"start\":52800},{\"end\":52826,\"start\":52817},{\"end\":52828,\"start\":52827}]", "bib_author_last_name": "[{\"end\":34975,\"start\":34969},{\"end\":34987,\"start\":34982},{\"end\":34995,\"start\":34991},{\"end\":35008,\"start\":35003},{\"end\":35015,\"start\":35010},{\"end\":35309,\"start\":35299},{\"end\":35329,\"start\":35319},{\"end\":35344,\"start\":35337},{\"end\":35681,\"start\":35675},{\"end\":35697,\"start\":35691},{\"end\":35709,\"start\":35706},{\"end\":35724,\"start\":35719},{\"end\":35738,\"start\":35734},{\"end\":35753,\"start\":35750},{\"end\":35767,\"start\":35760},{\"end\":36206,\"start\":36200},{\"end\":36217,\"start\":36213},{\"end\":36231,\"start\":36227},{\"end\":36249,\"start\":36244},{\"end\":36269,\"start\":36260},{\"end\":36636,\"start\":36631},{\"end\":36649,\"start\":36644},{\"end\":36662,\"start\":36657},{\"end\":36677,\"start\":36670},{\"end\":36692,\"start\":36684},{\"end\":36714,\"start\":36705},{\"end\":36729,\"start\":36723},{\"end\":36745,\"start\":36739},{\"end\":37162,\"start\":37154},{\"end\":37178,\"start\":37170},{\"end\":37464,\"start\":37459},{\"end\":37483,\"start\":37473},{\"end\":37500,\"start\":37494},{\"end\":37514,\"start\":37506},{\"end\":37528,\"start\":37523},{\"end\":37537,\"start\":37535},{\"end\":37554,\"start\":37546},{\"end\":37569,\"start\":37564},{\"end\":37582,\"start\":37578},{\"end\":37590,\"start\":37588},{\"end\":37606,\"start\":37602},{\"end\":37613,\"start\":37611},{\"end\":37624,\"start\":37622},{\"end\":38120,\"start\":38116},{\"end\":38134,\"start\":38130},{\"end\":38150,\"start\":38144},{\"end\":38166,\"start\":38160},{\"end\":38414,\"start\":38411},{\"end\":38423,\"start\":38418},{\"end\":38438,\"start\":38433},{\"end\":38452,\"start\":38447},{\"end\":38467,\"start\":38461},{\"end\":38488,\"start\":38478},{\"end\":38498,\"start\":38490},{\"end\":38791,\"start\":38778},{\"end\":38807,\"start\":38800},{\"end\":38822,\"start\":38818},{\"end\":38833,\"start\":38824},{\"end\":39156,\"start\":39147},{\"end\":39169,\"start\":39160},{\"end\":39179,\"start\":39173},{\"end\":39187,\"start\":39183},{\"end\":39194,\"start\":39191},{\"end\":39204,\"start\":39198},{\"end\":39547,\"start\":39534},{\"end\":39563,\"start\":39554},{\"end\":39576,\"start\":39572},{\"end\":39590,\"start\":39585},{\"end\":39614,\"start\":39602},{\"end\":39630,\"start\":39623},{\"end\":40022,\"start\":40014},{\"end\":40036,\"start\":40029},{\"end\":40052,\"start\":40045},{\"end\":40068,\"start\":40063},{\"end\":40326,\"start\":40322},{\"end\":40347,\"start\":40336},{\"end\":40367,\"start\":40361},{\"end\":40644,\"start\":40641},{\"end\":40650,\"start\":40648},{\"end\":40657,\"start\":40654},{\"end\":40665,\"start\":40661},{\"end\":40673,\"start\":40669},{\"end\":40684,\"start\":40677},{\"end\":40691,\"start\":40688},{\"end\":40703,\"start\":40699},{\"end\":41156,\"start\":41153},{\"end\":41172,\"start\":41167},{\"end\":41185,\"start\":41182},{\"end\":41203,\"start\":41196},{\"end\":41592,\"start\":41589},{\"end\":41608,\"start\":41603},{\"end\":41621,\"start\":41617},{\"end\":41634,\"start\":41631},{\"end\":41652,\"start\":41645},{\"end\":41982,\"start\":41972},{\"end\":41994,\"start\":41986},{\"end\":42007,\"start\":42000},{\"end\":42018,\"start\":42011},{\"end\":42030,\"start\":42022},{\"end\":42774,\"start\":42766},{\"end\":42789,\"start\":42786},{\"end\":42803,\"start\":42799},{\"end\":42810,\"start\":42805},{\"end\":42982,\"start\":42974},{\"end\":42999,\"start\":42996},{\"end\":43017,\"start\":43011},{\"end\":43034,\"start\":43030},{\"end\":43046,\"start\":43042},{\"end\":43061,\"start\":43053},{\"end\":43075,\"start\":43072},{\"end\":43091,\"start\":43085},{\"end\":43099,\"start\":43096},{\"end\":43110,\"start\":43108},{\"end\":43120,\"start\":43118},{\"end\":43133,\"start\":43128},{\"end\":43149,\"start\":43142},{\"end\":43155,\"start\":43151},{\"end\":43995,\"start\":43987},{\"end\":44016,\"start\":44006},{\"end\":44035,\"start\":44028},{\"end\":44323,\"start\":44317},{\"end\":44343,\"start\":44334},{\"end\":44542,\"start\":44540},{\"end\":44549,\"start\":44546},{\"end\":44563,\"start\":44559},{\"end\":44568,\"start\":44565},{\"end\":44755,\"start\":44753},{\"end\":44763,\"start\":44761},{\"end\":44777,\"start\":44774},{\"end\":44791,\"start\":44787},{\"end\":45153,\"start\":45150},{\"end\":45167,\"start\":45164},{\"end\":45180,\"start\":45177},{\"end\":45198,\"start\":45191},{\"end\":45515,\"start\":45507},{\"end\":45526,\"start\":45519},{\"end\":45752,\"start\":45736},{\"end\":45765,\"start\":45762},{\"end\":45777,\"start\":45775},{\"end\":45784,\"start\":45779},{\"end\":46097,\"start\":46093},{\"end\":46113,\"start\":46109},{\"end\":46124,\"start\":46122},{\"end\":46135,\"start\":46130},{\"end\":46146,\"start\":46142},{\"end\":46158,\"start\":46155},{\"end\":46168,\"start\":46165},{\"end\":46179,\"start\":46177},{\"end\":46196,\"start\":46192},{\"end\":46211,\"start\":46208},{\"end\":46224,\"start\":46222},{\"end\":46235,\"start\":46231},{\"end\":46248,\"start\":46246},{\"end\":46260,\"start\":46257},{\"end\":46894,\"start\":46882},{\"end\":46906,\"start\":46904},{\"end\":46921,\"start\":46919},{\"end\":46929,\"start\":46923},{\"end\":47185,\"start\":47183},{\"end\":47193,\"start\":47191},{\"end\":47212,\"start\":47204},{\"end\":47224,\"start\":47221},{\"end\":47238,\"start\":47235},{\"end\":47257,\"start\":47251},{\"end\":47540,\"start\":47528},{\"end\":47548,\"start\":47546},{\"end\":47563,\"start\":47561},{\"end\":47571,\"start\":47565},{\"end\":47920,\"start\":47916},{\"end\":47931,\"start\":47927},{\"end\":47945,\"start\":47941},{\"end\":47956,\"start\":47952},{\"end\":48191,\"start\":48182},{\"end\":48204,\"start\":48199},{\"end\":48220,\"start\":48215},{\"end\":48232,\"start\":48226},{\"end\":48474,\"start\":48464},{\"end\":48491,\"start\":48482},{\"end\":48697,\"start\":48692},{\"end\":48704,\"start\":48701},{\"end\":48717,\"start\":48710},{\"end\":48726,\"start\":48721},{\"end\":48736,\"start\":48730},{\"end\":49053,\"start\":49049},{\"end\":49075,\"start\":49064},{\"end\":49091,\"start\":49087},{\"end\":49308,\"start\":49304},{\"end\":49324,\"start\":49320},{\"end\":49545,\"start\":49543},{\"end\":49561,\"start\":49557},{\"end\":49584,\"start\":49573},{\"end\":49849,\"start\":49841},{\"end\":49864,\"start\":49858},{\"end\":49881,\"start\":49876},{\"end\":49892,\"start\":49889},{\"end\":49907,\"start\":49900},{\"end\":49923,\"start\":49918},{\"end\":49941,\"start\":49934},{\"end\":49960,\"start\":49951},{\"end\":49975,\"start\":49970},{\"end\":49988,\"start\":49984},{\"end\":50369,\"start\":50365},{\"end\":50383,\"start\":50377},{\"end\":50399,\"start\":50392},{\"end\":50544,\"start\":50540},{\"end\":50557,\"start\":50554},{\"end\":50568,\"start\":50565},{\"end\":50584,\"start\":50579},{\"end\":50605,\"start\":50596},{\"end\":50623,\"start\":50616},{\"end\":51007,\"start\":51005},{\"end\":51020,\"start\":51016},{\"end\":51035,\"start\":51029},{\"end\":51286,\"start\":51284},{\"end\":51298,\"start\":51295},{\"end\":51309,\"start\":51307},{\"end\":51320,\"start\":51316},{\"end\":51329,\"start\":51325},{\"end\":51585,\"start\":51581},{\"end\":51596,\"start\":51592},{\"end\":51607,\"start\":51603},{\"end\":51618,\"start\":51614},{\"end\":51861,\"start\":51851},{\"end\":51874,\"start\":51871},{\"end\":52122,\"start\":52119},{\"end\":52134,\"start\":52130},{\"end\":52147,\"start\":52145},{\"end\":52156,\"start\":52153},{\"end\":52560,\"start\":52558},{\"end\":52575,\"start\":52571},{\"end\":52589,\"start\":52585},{\"end\":52774,\"start\":52768},{\"end\":52798,\"start\":52783},{\"end\":52815,\"start\":52802},{\"end\":52834,\"start\":52829}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9649070},\"end\":35196,\"start\":34860},{\"attributes\":{\"id\":\"b1\"},\"end\":35607,\"start\":35198},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":54461709},\"end\":36070,\"start\":35609},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3756857},\"end\":36563,\"start\":36072},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6522002},\"end\":37029,\"start\":36565},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":384629},\"end\":37398,\"start\":37031},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b6\"},\"end\":38105,\"start\":37400},{\"attributes\":{\"doi\":\"arXiv:1602.02481\",\"id\":\"b7\"},\"end\":38340,\"start\":38107},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7684883},\"end\":38697,\"start\":38342},{\"attributes\":{\"id\":\"b9\"},\"end\":39091,\"start\":38699},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13681020},\"end\":39355,\"start\":39093},{\"attributes\":{\"id\":\"b11\"},\"end\":39925,\"start\":39357},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":215827080},\"end\":40239,\"start\":39927},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53029867},\"end\":40529,\"start\":40241},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":78094669},\"end\":40995,\"start\":40531},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53230690},\"end\":41441,\"start\":40997},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53225185},\"end\":41893,\"start\":41443},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":46940349},\"end\":42292,\"start\":41895},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11245438},\"end\":42723,\"start\":42294},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4445385},\"end\":42930,\"start\":42725},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":22045350},\"end\":43871,\"start\":42932},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4341709},\"end\":44227,\"start\":43873},{\"attributes\":{\"id\":\"b22\"},\"end\":44474,\"start\":44229},{\"attributes\":{\"id\":\"b23\"},\"end\":44696,\"start\":44476},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53399839},\"end\":45018,\"start\":44698},{\"attributes\":{\"doi\":\"arXiv:1811.02565\",\"id\":\"b25\"},\"end\":45427,\"start\":45020},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14620252},\"end\":45654,\"start\":45429},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11535472},\"end\":46055,\"start\":45656},{\"attributes\":{\"id\":\"b28\"},\"end\":46798,\"start\":46057},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5115938},\"end\":47102,\"start\":46800},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1009127},\"end\":47442,\"start\":47104},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1745976},\"end\":47832,\"start\":47444},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4578850},\"end\":48113,\"start\":47834},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":545361},\"end\":48379,\"start\":48115},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17648673},\"end\":48632,\"start\":48381},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16619984},\"end\":48984,\"start\":48634},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6242669},\"end\":49227,\"start\":48986},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206594775},\"end\":49453,\"start\":49229},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2407217},\"end\":49760,\"start\":49455},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9568132},\"end\":50294,\"start\":49762},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3847166},\"end\":50534,\"start\":50296},{\"attributes\":{\"doi\":\"arXiv:1801.07829\",\"id\":\"b41\"},\"end\":50903,\"start\":50536},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206592833},\"end\":51195,\"start\":50905},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4536146},\"end\":51507,\"start\":51197},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":8338993},\"end\":51759,\"start\":51509},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":53220126},\"end\":52016,\"start\":51761},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":52077468},\"end\":52486,\"start\":52018},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":49552160},\"end\":52722,\"start\":52488},{\"attributes\":{\"id\":\"b48\"},\"end\":53089,\"start\":52724}]", "bib_title": "[{\"end\":34963,\"start\":34860},{\"end\":35290,\"start\":35198},{\"end\":35668,\"start\":35609},{\"end\":36191,\"start\":36072},{\"end\":36624,\"start\":36565},{\"end\":37148,\"start\":37031},{\"end\":38402,\"start\":38342},{\"end\":38767,\"start\":38699},{\"end\":39143,\"start\":39093},{\"end\":40007,\"start\":39927},{\"end\":40313,\"start\":40241},{\"end\":40637,\"start\":40531},{\"end\":41142,\"start\":40997},{\"end\":41578,\"start\":41443},{\"end\":41968,\"start\":41895},{\"end\":42342,\"start\":42294},{\"end\":42764,\"start\":42725},{\"end\":42972,\"start\":42932},{\"end\":43979,\"start\":43873},{\"end\":44531,\"start\":44476},{\"end\":44743,\"start\":44698},{\"end\":45503,\"start\":45429},{\"end\":45725,\"start\":45656},{\"end\":46080,\"start\":46057},{\"end\":46876,\"start\":46800},{\"end\":47171,\"start\":47104},{\"end\":47523,\"start\":47444},{\"end\":47909,\"start\":47834},{\"end\":48173,\"start\":48115},{\"end\":48455,\"start\":48381},{\"end\":48688,\"start\":48634},{\"end\":49040,\"start\":48986},{\"end\":49295,\"start\":49229},{\"end\":49536,\"start\":49455},{\"end\":49832,\"start\":49762},{\"end\":50359,\"start\":50296},{\"end\":50995,\"start\":50905},{\"end\":51276,\"start\":51197},{\"end\":51571,\"start\":51509},{\"end\":51842,\"start\":51761},{\"end\":52109,\"start\":52018},{\"end\":52552,\"start\":52488},{\"end\":52759,\"start\":52724}]", "bib_author": "[{\"end\":34977,\"start\":34965},{\"end\":34989,\"start\":34977},{\"end\":34997,\"start\":34989},{\"end\":35010,\"start\":34997},{\"end\":35017,\"start\":35010},{\"end\":35311,\"start\":35292},{\"end\":35331,\"start\":35311},{\"end\":35346,\"start\":35331},{\"end\":35683,\"start\":35670},{\"end\":35699,\"start\":35683},{\"end\":35711,\"start\":35699},{\"end\":35726,\"start\":35711},{\"end\":35740,\"start\":35726},{\"end\":35755,\"start\":35740},{\"end\":35769,\"start\":35755},{\"end\":36208,\"start\":36193},{\"end\":36219,\"start\":36208},{\"end\":36233,\"start\":36219},{\"end\":36251,\"start\":36233},{\"end\":36271,\"start\":36251},{\"end\":36638,\"start\":36626},{\"end\":36651,\"start\":36638},{\"end\":36664,\"start\":36651},{\"end\":36679,\"start\":36664},{\"end\":36694,\"start\":36679},{\"end\":36716,\"start\":36694},{\"end\":36731,\"start\":36716},{\"end\":36747,\"start\":36731},{\"end\":37164,\"start\":37150},{\"end\":37180,\"start\":37164},{\"end\":37466,\"start\":37451},{\"end\":37485,\"start\":37466},{\"end\":37502,\"start\":37485},{\"end\":37516,\"start\":37502},{\"end\":37530,\"start\":37516},{\"end\":37539,\"start\":37530},{\"end\":37556,\"start\":37539},{\"end\":37571,\"start\":37556},{\"end\":37584,\"start\":37571},{\"end\":37592,\"start\":37584},{\"end\":37608,\"start\":37592},{\"end\":37615,\"start\":37608},{\"end\":37626,\"start\":37615},{\"end\":38122,\"start\":38107},{\"end\":38136,\"start\":38122},{\"end\":38152,\"start\":38136},{\"end\":38168,\"start\":38152},{\"end\":38416,\"start\":38404},{\"end\":38425,\"start\":38416},{\"end\":38440,\"start\":38425},{\"end\":38454,\"start\":38440},{\"end\":38469,\"start\":38454},{\"end\":38490,\"start\":38469},{\"end\":38500,\"start\":38490},{\"end\":38793,\"start\":38769},{\"end\":38809,\"start\":38793},{\"end\":38824,\"start\":38809},{\"end\":38835,\"start\":38824},{\"end\":39158,\"start\":39145},{\"end\":39171,\"start\":39158},{\"end\":39181,\"start\":39171},{\"end\":39189,\"start\":39181},{\"end\":39196,\"start\":39189},{\"end\":39206,\"start\":39196},{\"end\":39549,\"start\":39526},{\"end\":39565,\"start\":39549},{\"end\":39578,\"start\":39565},{\"end\":39592,\"start\":39578},{\"end\":39616,\"start\":39592},{\"end\":39632,\"start\":39616},{\"end\":40024,\"start\":40009},{\"end\":40038,\"start\":40024},{\"end\":40054,\"start\":40038},{\"end\":40070,\"start\":40054},{\"end\":40328,\"start\":40315},{\"end\":40349,\"start\":40328},{\"end\":40369,\"start\":40349},{\"end\":40646,\"start\":40639},{\"end\":40652,\"start\":40646},{\"end\":40659,\"start\":40652},{\"end\":40667,\"start\":40659},{\"end\":40675,\"start\":40667},{\"end\":40686,\"start\":40675},{\"end\":40693,\"start\":40686},{\"end\":40705,\"start\":40693},{\"end\":41158,\"start\":41144},{\"end\":41174,\"start\":41158},{\"end\":41187,\"start\":41174},{\"end\":41205,\"start\":41187},{\"end\":41594,\"start\":41580},{\"end\":41610,\"start\":41594},{\"end\":41623,\"start\":41610},{\"end\":41636,\"start\":41623},{\"end\":41654,\"start\":41636},{\"end\":41984,\"start\":41970},{\"end\":41996,\"start\":41984},{\"end\":42009,\"start\":41996},{\"end\":42020,\"start\":42009},{\"end\":42032,\"start\":42020},{\"end\":42776,\"start\":42766},{\"end\":42791,\"start\":42776},{\"end\":42805,\"start\":42791},{\"end\":42812,\"start\":42805},{\"end\":42984,\"start\":42974},{\"end\":43001,\"start\":42984},{\"end\":43019,\"start\":43001},{\"end\":43036,\"start\":43019},{\"end\":43048,\"start\":43036},{\"end\":43063,\"start\":43048},{\"end\":43077,\"start\":43063},{\"end\":43093,\"start\":43077},{\"end\":43101,\"start\":43093},{\"end\":43112,\"start\":43101},{\"end\":43122,\"start\":43112},{\"end\":43135,\"start\":43122},{\"end\":43151,\"start\":43135},{\"end\":43157,\"start\":43151},{\"end\":43997,\"start\":43981},{\"end\":44018,\"start\":43997},{\"end\":44037,\"start\":44018},{\"end\":44325,\"start\":44311},{\"end\":44345,\"start\":44325},{\"end\":44544,\"start\":44533},{\"end\":44551,\"start\":44544},{\"end\":44565,\"start\":44551},{\"end\":44570,\"start\":44565},{\"end\":44757,\"start\":44745},{\"end\":44765,\"start\":44757},{\"end\":44779,\"start\":44765},{\"end\":44793,\"start\":44779},{\"end\":45155,\"start\":45143},{\"end\":45169,\"start\":45155},{\"end\":45182,\"start\":45169},{\"end\":45200,\"start\":45182},{\"end\":45517,\"start\":45505},{\"end\":45528,\"start\":45517},{\"end\":45754,\"start\":45727},{\"end\":45767,\"start\":45754},{\"end\":45779,\"start\":45767},{\"end\":45786,\"start\":45779},{\"end\":46099,\"start\":46082},{\"end\":46115,\"start\":46099},{\"end\":46126,\"start\":46115},{\"end\":46137,\"start\":46126},{\"end\":46148,\"start\":46137},{\"end\":46160,\"start\":46148},{\"end\":46170,\"start\":46160},{\"end\":46181,\"start\":46170},{\"end\":46198,\"start\":46181},{\"end\":46213,\"start\":46198},{\"end\":46226,\"start\":46213},{\"end\":46237,\"start\":46226},{\"end\":46250,\"start\":46237},{\"end\":46262,\"start\":46250},{\"end\":46896,\"start\":46878},{\"end\":46908,\"start\":46896},{\"end\":46923,\"start\":46908},{\"end\":46931,\"start\":46923},{\"end\":47187,\"start\":47173},{\"end\":47195,\"start\":47187},{\"end\":47214,\"start\":47195},{\"end\":47226,\"start\":47214},{\"end\":47240,\"start\":47226},{\"end\":47259,\"start\":47240},{\"end\":47542,\"start\":47525},{\"end\":47550,\"start\":47542},{\"end\":47565,\"start\":47550},{\"end\":47573,\"start\":47565},{\"end\":47922,\"start\":47911},{\"end\":47933,\"start\":47922},{\"end\":47947,\"start\":47933},{\"end\":47958,\"start\":47947},{\"end\":48193,\"start\":48175},{\"end\":48206,\"start\":48193},{\"end\":48222,\"start\":48206},{\"end\":48234,\"start\":48222},{\"end\":48476,\"start\":48457},{\"end\":48493,\"start\":48476},{\"end\":48699,\"start\":48690},{\"end\":48706,\"start\":48699},{\"end\":48719,\"start\":48706},{\"end\":48728,\"start\":48719},{\"end\":48738,\"start\":48728},{\"end\":49055,\"start\":49042},{\"end\":49077,\"start\":49055},{\"end\":49093,\"start\":49077},{\"end\":49310,\"start\":49297},{\"end\":49326,\"start\":49310},{\"end\":49547,\"start\":49538},{\"end\":49563,\"start\":49547},{\"end\":49586,\"start\":49563},{\"end\":49595,\"start\":49586},{\"end\":49851,\"start\":49834},{\"end\":49866,\"start\":49851},{\"end\":49883,\"start\":49866},{\"end\":49894,\"start\":49883},{\"end\":49909,\"start\":49894},{\"end\":49925,\"start\":49909},{\"end\":49943,\"start\":49925},{\"end\":49962,\"start\":49943},{\"end\":49977,\"start\":49962},{\"end\":49990,\"start\":49977},{\"end\":50371,\"start\":50361},{\"end\":50385,\"start\":50371},{\"end\":50401,\"start\":50385},{\"end\":50546,\"start\":50536},{\"end\":50559,\"start\":50546},{\"end\":50570,\"start\":50559},{\"end\":50586,\"start\":50570},{\"end\":50607,\"start\":50586},{\"end\":50625,\"start\":50607},{\"end\":51009,\"start\":50997},{\"end\":51022,\"start\":51009},{\"end\":51037,\"start\":51022},{\"end\":51288,\"start\":51278},{\"end\":51300,\"start\":51288},{\"end\":51311,\"start\":51300},{\"end\":51322,\"start\":51311},{\"end\":51331,\"start\":51322},{\"end\":51587,\"start\":51573},{\"end\":51598,\"start\":51587},{\"end\":51609,\"start\":51598},{\"end\":51620,\"start\":51609},{\"end\":51863,\"start\":51844},{\"end\":51876,\"start\":51863},{\"end\":52124,\"start\":52111},{\"end\":52136,\"start\":52124},{\"end\":52149,\"start\":52136},{\"end\":52158,\"start\":52149},{\"end\":52562,\"start\":52554},{\"end\":52577,\"start\":52562},{\"end\":52591,\"start\":52577},{\"end\":52776,\"start\":52761},{\"end\":52800,\"start\":52776},{\"end\":52817,\"start\":52800},{\"end\":52836,\"start\":52817}]", "bib_venue": "[{\"end\":42077,\"start\":42063},{\"end\":52267,\"start\":52221},{\"end\":35021,\"start\":35017},{\"end\":35382,\"start\":35346},{\"end\":35827,\"start\":35769},{\"end\":36307,\"start\":36271},{\"end\":36789,\"start\":36747},{\"end\":37202,\"start\":37180},{\"end\":37449,\"start\":37400},{\"end\":38215,\"start\":38184},{\"end\":38504,\"start\":38500},{\"end\":38885,\"start\":38835},{\"end\":39213,\"start\":39206},{\"end\":39524,\"start\":39357},{\"end\":40074,\"start\":40070},{\"end\":40376,\"start\":40369},{\"end\":40742,\"start\":40705},{\"end\":41209,\"start\":41205},{\"end\":41658,\"start\":41654},{\"end\":42061,\"start\":42032},{\"end\":42387,\"start\":42344},{\"end\":42819,\"start\":42812},{\"end\":43201,\"start\":43157},{\"end\":44041,\"start\":44037},{\"end\":44309,\"start\":44229},{\"end\":44574,\"start\":44570},{\"end\":44842,\"start\":44793},{\"end\":45141,\"start\":45020},{\"end\":45532,\"start\":45528},{\"end\":45842,\"start\":45786},{\"end\":46306,\"start\":46262},{\"end\":46935,\"start\":46931},{\"end\":47263,\"start\":47259},{\"end\":47622,\"start\":47573},{\"end\":47962,\"start\":47958},{\"end\":48238,\"start\":48234},{\"end\":48497,\"start\":48493},{\"end\":48796,\"start\":48738},{\"end\":49097,\"start\":49093},{\"end\":49330,\"start\":49326},{\"end\":49599,\"start\":49595},{\"end\":50018,\"start\":49990},{\"end\":50405,\"start\":50401},{\"end\":50687,\"start\":50641},{\"end\":51041,\"start\":51037},{\"end\":51335,\"start\":51331},{\"end\":51624,\"start\":51620},{\"end\":51880,\"start\":51876},{\"end\":52219,\"start\":52158},{\"end\":52595,\"start\":52591},{\"end\":52885,\"start\":52836}]"}}}, "year": 2023, "month": 12, "day": 17}