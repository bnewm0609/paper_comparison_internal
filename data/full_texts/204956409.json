{"id": 204956409, "updated": "2022-02-03 21:55:38.848", "metadata": {"title": "Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM", "authors": "[{\"middle\":[],\"last\":\"Sheng\",\"first\":\"Lu\"},{\"middle\":[],\"last\":\"Xu\",\"first\":\"Dan\"},{\"middle\":[],\"last\":\"Ouyang\",\"first\":\"Wanli\"},{\"middle\":[],\"last\":\"Wang\",\"first\":\"Xiaogang\"}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "In this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efficient camera relocalization and effective augmentation of visual odometry. To benefit from it, we first present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the first work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (\\ie~KITTI raw dataset and its odometry split) clearly demonstrate the effectiveness of the proposed approach, and new state-of-the-art results are established on the unsupervised depth and pose estimation from monocular videos.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2986062851", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/Sheng0OW19", "doi": "10.1109/iccv.2019.00440"}}, "content": {"source": {"pdf_hash": "3328e37fd40d8840674fc936eba8fc51803ae2d3", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "abe5187a67a3cb6232e77cfe6446c923bb5abc46", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3328e37fd40d8840674fc936eba8fc51803ae2d3.txt", "contents": "\nUnsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM\n\n\nLu Sheng lsheng@buaa.edu.cn \nCollege of Software\nBeihang University\nChina\n\nDan Xu danxu@robots.ox.ac.uk \nUniversity of Oxford\nUK\n\nWanli Ouyang wanli.ouyang@sydney.edu.au \nSenseTime Computer Vision Research Group\nThe University of Sydney\nAustralia\n\nXiaogang Wang xgwang@ee.cuhk.edu.hk \nCUHK-SenseTime Joint Lab\nThe Chinese University of Hong Kong\nHong Kong\n\nUnsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM\n10.1109/ICCV.2019.00440\nIn this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efficient camera relocalization and effective augmentation of visual odometry. To benefit from it, we first present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the first work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (i.e. KITTI raw dataset and its odometry split [12]) clearly demonstrate the effectiveness of the proposed approach, and new state-ofthe-art results are established on the unsupervised depth and pose estimation from monocular video.\n\nIntroduction\n\nWhile perception of 3D geometric scenes is particularly important for interaction with real-world environments, as one important topic, visual simultaneous localization and mapping (SLAM) [10] has received emerging attention in recent years. However, due to the task complexity and limited annotated data, the power of deep learning is only partially explored on existing visual SLAM systems [5,28].\n\nIn this work, we focus on techniques for monocular visual SLAM systems, which generally contains several subtasks, such as depth prediction and camera motion estimation for local 3D scene structure recovery, and keyframe se- * Equal contribution. lection and management for global map construction and localization. As an important part in monocular SLAM, the keyframe selection has been widely investigated in traditional approaches for aiding the visual odometry and scene matching [30,9]. Although various evidences have shown that using deep models with geometric constraints clearly boosts the performance of depth, camera motion and optical flow estimation [51,49], to our best knowledge, no existing work has considered deep learning based frameworks for the keyframe selection task.\n\nIn this paper, we argue that a joint optimization of the keyframe selection and visual odometry should greatly benefit each other. Robust keyframe selection not only provides an efficient manner for fast localization and mapping, but also is particularly useful for effective refinement of the camera motion and depth predictions in the visual odometry task. Inversely, better visual odometry is able to facilitate more accurate keyframe identification. In addition, simultaneous learning of multiple tasks in deep learning has demonstrated its effectiveness in computer vision tasks such as detection and segmentation [4,13]. It is thus a natural expectation that solving the keyframe selection and the visual odometry in a single deep network could also benefit from the advantage of the joint optimization.\n\nBased on aforementioned observations, we propose an unsupervised deep model towards monocular SLAM, powered by three sub-networks to deal with three distinct but complementary tasks, i.e. keyframe selection, camera motion estimation and depth prediction. The keyframe selection sub-network learns a joint visual and geometric similarity between a pair of observed image and keyframe. If this similarity is below a threshold, the observation image is treated as a new keyframe, and is stored in a managed keyframe pool. The camera motion and depth prediction sub-networks learn to predict the depth of the observed image and relative motions from its nearby frames. To jointly learn these three tasks, we propose a collaborative learning strategy to predict a final similarity for the keyframe selection using a geometric metric estimated from the depth and the camera motion estimation networks, and a visual metric directly estimated from the keyframe selection network. A final ranking loss is added for different observation and keyframe pairs. By doing so, the whole network is trained in an unsupervised end-to-end fashion, and the three tasks constrain on each other based on their visual-geometric relationship for a better optimization of the whole model.\n\nIn summary, the contribution of this work is three-fold: \u2022 We design a keyframe selection network, which is used to estimate a combinational similarity metric between visual and geometric cues. The learning of keyframes further provides extra supervision for the learning of the visual odometry networks. \u2022 We propose a unified unsupervised deep learning framework to simultaneously learn the keyframe selection and visual odometry tasks in an end-to-end fashion. As far as we know, it is the first work to jointly optimize these two complementary components in a single deep model. A collaborative optimization loss is designed to enforce mutual constraints in between, enabling them to benefit each other in a joint optimization. \u2022 We extensively demonstrate the effectiveness of the proposed approach on the KITTI raw dataset and its odometry split [12], showing the benefits of jointly learning and achieving new state-of-the art results on unsupervised monocular depth and camera motion estimation tasks.\n\n\nRelated Works\n\nSLAM has been widely studied in recent years as a core 3D scene understanding technology. It can be roughly classified into stereo [35,27,15,42], RGB-D [44,17,21] and monocular-based SLAM [8,7]. We will review the most related monocular visual SLAM approaches.\n\nTraditional Keyframe based Approaches Keyframe selection contains a detection step to identify a keyframe and a matching step for localization. The keyframe selection has been adopted in several state-of-the-art traditional SLAM approaches, such as RDSLAM [39] and ORB-SLAM [30,31]. LSD-SLAM [8] presents a real-time visual SLAM system, which updates the keyframes by tracking the change of rigid pose, and correspondingly refines the depth map estimation. Forster et al. [9] applies a similar strategy to LSD-SLAM using direct tracking, while operates on semi-dense depth maps to obtain a high frame rate. More recently, Hsiao et al. [19] propose a keyframe-based SLAM approach based on dense plane extraction and matching, yielding superior performance on real-time SLAMs.\n\nTraditional Visual Odometry based Approaches The monocular visual odometry estimates the 3D scene structure and ego-motion from 2D data with a monocular camera [37]. It mainly contains feature-based methods with salient feature tracking [33,32], appearance-based methods with pixel-level image/patch matching [50,38] and hybrid methods with a combination of the feature and appearance based strategies [34]. There are also other works exploring camera geometric modeling and regression model learning [16]. However, the traditional approaches mostly rely on hand-crafted representations or shallow models, which leads to inferior SLAM performance.\n\n\nSupervised Deep Learning based Approaches\n\nTo overcome the limitations of traditional approaches, more recent works focus on designing deep learning models to tackle the problem. Several supervised models have been proposed and significantly boost the performance of scene depth [45,23,46], camera pose [2] and scene flow estimation [26]. Eigen et al. [6] introduce a coarse to fine network structure with multi-scale fusion for depth prediction from single images. Kendall et al. [20] propose a PoseNet structure to address the 6-DoF camera relocalization problem. CNN-SLAM [40] detects keyframes and uses them to rectify the scale of the depth prediction, however, the keyframe detection is only based on an off-the-shelf method using hand-crafted features, and is not jointly learned with the other sub-tasks within a single deep model.\n\n\nUnsupervised Deep Learning based Approaches\n\nApart from the supervised models, there exists some unsupervised deep learning based approaches in the literature [24,22,48,36]. Garg et al. [11] present an encoder-decoder disparity learning network utilizing view synthesis error for optimization. To consider mutual constraints from different views, Godard et al. [14] further introduce a two-branches reconstruction network and apply a left-right consistence loss to supervise each other. However, these approaches only learn a single task in their models. SfMLearner [51] proposes to jointly unsupervised learning depth and camera pose from monocular videos using photometric synthesis loss from different nearby views. Upon SfM-Learner, GeoNet [49] further learns an optical flow task to tackle the non-rigid motion issue in the view reconstruction. Our model explores unsupervised learning from monocular videos and is more related to these two approaches, however, ours focuses on designing a keyframe selection network, and a probabilistic collaborative learning framework to make the keyframe selection and the visual-odometry benefit each other in a single deep model.\n\n\nThe Proposed Approach\n\nWe propose an end-to-end system aiming at jointly learning the keyframe selection and the visual odometry in a single deep network towards monocular SLAM. It primarily consists of a visual odometry and a keyframe selection modules implemented with neural networks. In the remainder of this section, we first introduce the designed deep keyframe selection and visual odometry modules, and then present how they are jointly learned in the proposed unsupervised collaborative learning framework.\n\n\nKeyframe-based Visual Odometry\n\nOur visual odometry model includes a monocular depth predictor D \u03c6 D and a camera motion estimator C \u03c6 C between a pair of frames. \u03c6 D and \u03c6 C are network parameters.\n\nNetwork Specification For an image pair I r and I t , D \u03c6 D and C \u03c6 C are defined as\nD t = D \u03c6 D (I t ), \u03b8 t\u2192r = C \u03c6 C (I t , I r ),(1)\nwhere D t is the predicted depth of I t and \u03b8 t\u2192r is the camera ego-motion from the target image I t to the reference image I r . The camera motion consists of a rotation vector \u03c9 = [\u03c9 x , \u03c9 y , \u03c9 z ] and a translation vector t = [t x , t y , t z ] . Our model follows a similar network structure as that in SfM-Learner [51], but our camera motion estimator just uses any two images I t and I r as its input, rather than consecutive frames. Thus our camera motion estimator is flexible and is not fixed to local adjacent frames.\n\nNecessity of Keyframes State-of-the-art learning based visual odometry methods [51,49] only explain small geometric changes, since they are learned by short-length consecutive frames (around 2 \u223c 5 frames). Thus they are usually failed to capture large geometric changes such as the case about the target image versus keyframes.\n\nThanks to the associated keyframe selection task, we find that keyframes are useful as additional training data to augment the geometry description of the visual odometry model. In this case, the camera motion estimator C \u03c6 C has to\n{!, t} C C D D D D Target Image Reference Image D D C C\n\nGeometric Cues\n\n\nBase Layers\n\nBase Layers\nGAP GAP f v f g Visual Feature Geometric Feature Cross-modal Attention \u21b5 v \u21b5 g Fusion\n\nVisual Cues\n\nSimilarity Regression q t$r Figure 2. Network structure of the depth predictor D \u03c6 D , the camera motion estimator C \u03c6 C and the keyframe selector S \u03c6 S . capture more challenging motion patterns between the target image and the keyframes, and the depth predictor D \u03c6 D must find accurate scene geometry to meet the geometric consistency between in between as well.\n\n\nGeometry-equipped Keyframe Selection\n\nKeyframes record the most representative geometry maps or landmarks (dense depth, pose and etc.) among its neighboring frames. Its core function is a keyframe selector S \u03c6 S that builds up keyframe sets by identifying a new keyframe when it includes considerably geometric changes or visual changes against the previous keyframes. It also concurrently localize any frame to its nearest keyframe (if exists) so as to fulfill the camera localization. \u03c6 S indicates the network parameters of the keyframe selector.\n\nNetwork Specification Assume the target image as I t and the reference image as I r (possibly the existing keyframes). The keyframe selector is to measure whether I t and I r are similar both in visual and geometric viewpoints. S \u03c6 S has a two-stream structure and adaptively combines the visual and geometric similarities for the final decision, as depicted in Fig. 2, in which (1) the visual stream applies the concatenated I t and I r as its input. (2) the geometric stream receives the channel-wise concatenation of a series of geometric data obtained from the visual odometry module. It includes the predicted depth maps D t and D r , and warping residual maps \u2206I t\u2190r from I r to I t and \u2206I r\u2190t vice versa.\n\nThe warping residual maps (take \u2206I t\u2190r as an example) are\n\u2206I t\u2190r (x) = |I r (W(x; D t , \u03b8 t\u2192r )) \u2212 I t (x)| (2)\nwhere W(x; D t , \u03b8 t\u2192r ) is the rigid warping field explained by the predicted depth D t and the camera motion \u03b8 t\u2192r . In summary, the keyframe selector is\nq t\u2194r = S \u03c6 S (I t , I r ; D t , D r , \u2206I t\u2190r , \u2206I r\u2190t ),(3)\nwhere the similarity q t\u2194r is robust to the order of I t and I r .\n\nBoth streams share a same network architecture but not their network parameters. The base layers in each stream are copied from ResNet-18 [18], while each of them is followed by a global average pooling and several fullyconnected layers. The extracted visual feature f c and geometric feature f g from each stream are fused together with the help of cross-modal attention. The attentions \u03b1 c and \u03b1 g are learned via an additional fully-connected layer using concatenated f c and f g as the input. The attended visual features f c \u2022 \u03b1 c and f g \u2022 \u03b1 g are further combined using fully-connected layers to generate the final similarity score.\n\n\nKeyframe Online Updating and Management\n\nWe design an online keyframe updating and management strategy to maintain a key frame pool P K during the training stage. At the beginning of the training, P K uses several randomly selected frames as initialization. Each keyframe F K k is represented with a three-tuple containing the frame index t k corresponding to input video sequence, the RGB image I K k and the depth estimation map D K k produced from the visual odometry network, i.e.\nF K k = {t k , I K k , D K k } and P K = {F k K } K k=1\nwhere K is the total number of key frames. The keyframe updating consists inserting and merging operations. After several training iterations, a determination on an input target frame is conducted. We used 200 iterations in our implementation. If its similarity scores between the nearest keyframes are above a threshold, it would be insert into P K . And after each epoch, we start merging the selected keyframes given by a trained better keyframe matching network. Adjacent keyframes in P K are organized into pairs and are passed into the network for similarity measurement. If the two are similar enough, only one of them is kept. The keyframe depth estimation is also used to help the optimization of visual odometry sub-network. The depth map of the closest keyframe to the target frame, is used to refine the depth prediction from the depth estimation net via a weighted averaging operation. In the testing phase, the latest keyframe always compares the target frame, if their dissimilarity is above a threshold, new keyframe is inserted into P K . Please check Fig. 3(a) for illustration.\n\n\nUnsupervised Collaborative Learning\n\nAs aforementioned, the keyframe selection and visual odometry are complementary to each other. It is thus bene-ficial to collaboratively learn these tasks together. But how to merge them into a unified learning framework is not trivial and requires a special design of the training procedure. As shown in Fig. 3(b), the proposed collaborative learning scheme will be depicted in details in the following text. Training Data Preparation. Keyframe selection and visual odometry require different training data constructions as they follow different learning logics. In each training example, we have a short training sequence I s (|I s | = 3), in which the center frame is the target image I t . And we gather one intra-class sample I p that is picked as the temporally nearest keyframe in the keyframe set P K , and select a second temporally nearest sample I n as the hard negative sample. Therefore, the training example is I K = {I s , I p , I n }. Optimization Loss of Visual Odometry. The visual odometry module is learned among a combined training image set I vo = {I s , I p }. For each image pairs {I t , I r } in I vo , we optimize the photometric consistency to both images, within the regions that rigid correspondences exist:\nL pc = {It,Ir}\u2208Ivo x (1 \u2212 M t (x)) \u00b7 \u03c1(I t\u2190r (x), I t )+ (1 \u2212 M r (x)) \u00b7 \u03c1(I r\u2190t (x), I r ) + (M r (x) + M t (x)) \u00b7 \u03c4 (4) where \u03c1(x, y) = \u03b1 2 (1 \u2212 SSIM(x, y)) + (1 \u2212 \u03b1)\u03c3(x \u2212 y)\nis a robust perceptual image similarity measurement. SSIM is the structural similarity index [43] and \u03c3(x) = (x 2 + \u03b5 2 ) 0.45 is the robust Charbonnier loss [3]. I t\u2190r (x) = I r (W(x; D t , \u03b8 t\u2192r )) is the backward warped reference image, and I r\u2190t is defined as the backward warped target image I r\u2190t (x) = I t (W(x; D r , \u03b8 r\u2192t )). Note that \u03b8 r\u2192t is the inverse motion of \u03b8 t\u2192r , which is calculated analytically but not through the camera motion estimator C(I r , I t ) again. The non-rigid mask M t in I t is generated by detecting the regions where the cycle consistency between the bi-directional warping fields, i.e.,\n\u2206W t (x) = |W(x; D t , \u03b8 t\u2192r ) + W(W(x; D t , \u03b8 t\u2192r ); D r , \u03b8 r\u2192t )| is vi- olated. The non-rigid mask M r in I r is calculated in a similar way to threshold \u2206W r (x) = |W(x; D r , \u03b8 r\u2192t ) + W(W(x; D r , \u03b8 r\u2192t ); D t , \u03b8 t\u2192r )|.\nThe threshold is scaled according to the per-pixel magnitude of the warping fields, similarly as [29]. The additional constant \u03c4 is added to remove trivial solutions that any pixel is non-rigid.\n\nTo enhance the geometric consistency, we enforce the cycle consistency in the rigid regions as well\nL cc = {It,Ir}\u2208Ivo x (1 \u2212 M t (x)) \u00b7 \u2206W t (x) + (1 \u2212 M r (x)) \u00b7 \u2206W r (x). (5)\nThe depth maps are further smoothed by L ds = It\u2208Ivo\nx |\u2207d t (x)| exp(\u2212|\u2207I t (x)|), which is an (i) Keyframe Merging (ii) Keyframe Inserting It <\n\nl a t e x i t s h a 1 _ b a s e 6 4 = \" m 1 + N g e A D r B / r d u i z r p s L i a P j m I 0 = \" > A A A B 8 3 i c b V D L S s N\nA F L 3 x W e u r 2 q W b w S K 4 k J L o Q p c B N 7 q r Y B / Q h D K Z T t q h k 0 m Y m Q g l Z O N H u H G h i F t / x o X g H / g N r p y 0 X W j r g Y H D O f d y z 5 w g 4 U x p 2 / 6 0 l p Z X V t f W S x v l z a 3 t n d 3 K 3 n 5 L x a k k t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o s v D b d 1 Q q F o t b P U 6 o H + G B Y C E j W B v J 8 y K s h 0 G Y X e c 9 3 a v U 7 L o 9 A V o k z o z U 3 O r H 9 / 1 X f t L o V d 6 9 f k z S i A p N O F a q 6 9 i J 9 j M s N S O c 5 m U v V T T B Z I Q H t G u o w B F V f j b J n K M j o / R R G E v z h E Y T 9 f d G h i O l x l F g J o u\nM a t 4 r x P + 8 b q r D C z 9 j I k k 1 F W R 6 K E w 5 0 j E q C k B 9 J i n R f G w I J p K Z r I g M s c R E m 5 r K p g R n / s u L p H V a d 8 \n\n\nq z o 1 T c 1 2 Y o g Q H c A j H 4 M A 5 u H A F D W g C g Q Q e 4 A m e r d R 6 t F 6 s 1 + n o k j X b q c I f W G 8 / 8 e m W C A = = < / l a t e x i t >\nI K k 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 h B P s A n u 6 O w 0 1 0 0 Y w 4 X h V N m v 2 R 8 = \" > A A A C B X i c b V D L S s N A F J 3 U V 6 2 v q O B G F 8 E i u r F k d K H L g J u K m\n\nw r 2 A W k M k + m k H T p 5 M D M R S s h C N / 6 K G x e K u H X p 3 p 1 / 4 y T t Q l s P X D i c c y / 3 3 u P F j A p p m t 9 a a W 5 + Y X G p v F x Z W V 1 b 3 9 A 3 t 1 o i S j g m T R y x i H c 8 J\nA i j I W l K K h n p x J y g w G O k 7 Q 0 v c r 9 9 R 7 i g U X g j R z F x A t Q P q U 8 x k k p y 9 b 1 u g O T A 8 9 P L z E 2 H x z C 7 L Q S M W H q V u X r V r J k F j F k C J 6 R q H X 7 a 9 + Z O v e H q X 9 1 e h J O A h B I z J I Q N z V g 6 K e K S Y k a y S j c R J E Z 4 i P r E V j R E A R F O W n y R G Q d K 6 R l + x F W F 0 i j U 3 x M p C o Q Y B Z 7 q z E 8 U 0 1 4 u / u f Z i f T P n Z S G c S J J i M e L / I Q Z M j L y S I w e 5 Q R L N l I E Y U 7 V r Q Y e I I 6 w V M F V V A h w + u V Z 0 j q p w d M\n\na v I Z V y w J j l M E u 2 A d H A I I z Y I E 6 a I A m w O A B P I E X 8 K o 9 a s / a m / Y + b i 1 p k 5 l t 8 A f a x w 9 Z D p t f < / l a t e x i t >\nI K k < l a t e x i t s h a 1 _ b a s e 6 4 = \" O m 7 E X v r 6 T o b b o 3 V F r p G i + F b o W C s = \" > A A A C A 3 i c b V C 7 T s M w F H V a H q W 8 A m z A Y L V C Y q o S G G C M x A J i K Y g + p D Z U j u u 0 V h 0 n s h 2 k K o r E w q + w M I A Q K z / B x i 8 w 8 g U 4 a Q d o O Z K l 4 3 P u 1 b 3 3 e B G j U l n W p 1 E o L i w u L Z d W y q t r 6 x u b 5 t Z 2 U 4 a x w K S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 6 C z z W 3 d E S B r y G z W O i B u g A a c + x U h p q W f u d g O k h p 6 f X K S 9 Z J T e 5 l + M W H K Z 9 s y q V b N y w H l i T 0 n V q Y j i / t f 3 d b 1 n f n T 7 I Y 4 D w h V m S M q O b U X K T Z B Q F D O S l r u x J B H C I z Q g H U 0 5 C o h 0 k / y G F B 5 o p Q / 9 U O j H F c z V 3 x 0 J C q Q c B 5 6 u z F a U s 1 4 m / u d 1 Y u W f u g n l U a w I x 5 N B f s y g C m E W C O x T Q b B i Y 0 0 Q F l T v C v E Q C Y S V j q 2\n\ns Q 7 B n T 5 4 n z a O a f V y z r + y q 4 4 A J S m A P V M A h s M E J c M A 5 q I M G w O A e P I J n 8 G I 8 G E / G q / E 2 K S 0 Y 0 5 4 d 8 A f G + w 8 l B p t 2 < / l a t e x i t >\nI K k+1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" i P 2 o M q 1 2 U O 1 s w E J j E u q u i m m b K W M = \" > A A A C B X i c b V D L S s N A F J 3 U V 6 2 v q O B G F 8 E i C k L J 6 E K X A T c V N x X s A 9 I Y J t N J O 3 T y Y G Y i l J C F b v w V N y 4 U c e v S v T v / x k n a h b Y e u H A 4 5 1 7 u v c e L G R X S N L + 1 0 t z 8 w u J S e b m y s r q 2 v q F v b r V E l H B M m j h i E e 9 4 S B B G Q 9 K U V D L S i T l B g c d I 2 x t e 5 H 7 7 j n B B o / B G j m L i B K g f U p 9 i J J X k 6 n v d A M m B 5 6 e X m Z s O j 2 F 2 W w g Y s f Q q c / W q W T M L G L M E T k j V O v y 0 7 8 2 d e s P V v 7 q 9 C C c B C S V m S A g b m r F 0 U s Q l x Y x k l W 4 i S I z w E P W J r W i I A i K c t P g i M w 6 U 0 j P 8 i K s K p V G o v y d S F A g x C j z V m Z 8 o p r 1 c / M + z E + m f O y k N 4 0 S S E I 8 X + Q k z Z G T k k R g 9 y g m W b K Q I w p y q W w 0 8 Q B x h q Y K r q B D g 9 M u z p H V S g 6 c 1 e A 2 r l g X G K I N d s A + O A A R n w A J 1 0 A B N g M E D e A I v 4 F V 7 1 J 6 1 N + 1 9 3 F r S J j P b 4 A + 0 j x 9 V 6 p t d < / l a t e x i t > I K k 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 h B P s A n u 6 O w 0 1 0 0 Y w 4 X h V N m v 2 R 8 = \" > A A A C B X i c b V D L S s N A F J 3 U V 6 2 v q O B G F 8 E i u r F k d K H L g J u K m w r 2 A W k M k + m k H T p 5 M D M R S s h C N / 6 K G x e K u H X p 3 p 1 / 4 y T t Q l s P X D i c c y / 3 3 u P F j A p p m t 9 a a W 5 + Y X G p v F x Z W V 1 b 3 9 A 3 t 1 o i S j g m T R y x i H c 8 J A i j I W l K K h n p x J y g w G O k 7 Q 0 v c r 9 9 R 7 i g U X g j R z F x A t Q P q U 8 x k k p y 9 b 1 u g O T A 8 9 P L z E 2 H x z C 7 L Q S M W H q V u X r V r J k F j F k C J 6 R q H X 7 a 9 + Z O v e H q X 9 1 e h J O A h B I z J I Q N z V g 6 K e K S Y k a y S j c R J E Z 4 i P r E V j R E A R F O W n y R G Q d K 6 R l + x F W F 0 i j U 3 x M p C o Q Y B Z 7 q z E 8 U 0 1 4 u / u f Z i f T P n Z S G c S J J i M e L / I Q Z M j L y S I w e 5 Q R L N l I E Y U 7 V r Q Y e I I 6 w V M F V V A h w + u V Z 0 j q p w d M a v I Z V y w J j l M E u 2 A d H A I I z Y I E 6 a I A m w O A B P I E X 8 K o 9 a s / a m / Y + b i 1 p k 5 l t 8 A f a x w 9 Z D p t f < / l a t e x i t > I K k < l a t e x i t s h a 1 _ b a s e 6 4 = \" O m 7 E X v r 6 T o b b o 3 V F r p G i + F b o W C s = \" > A A A C A 3 i c b V C 7 T s M w F H V a H q W 8 A m z A Y L V C Y q o S G G C M x A J i K Y g + p D Z U j u u 0 V h 0 n s h 2 k K o r E w q + w M I A Q K z / B x i 8 w 8 g U 4 a Q d o O Z K l 4 3 P u 1 b 3 3 e B G j U l n W p 1 E o L i w u L Z d W y q t r 6 x u b 5 t Z 2 U 4 a x w K S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 6 C z z W 3 d E S B r y G z W O i B u g A a c + x U h p q W f u d g O k h p 6 f X K S 9 Z J T e 5 l + M W H K Z 9 s y q V b N y w H l i T 0 n V q Y j i / t f 3 d b 1 n f n T 7 I Y 4 D w h V m S M q O b U X K T Z B Q F D O S l r u x J B H C I z Q g H U 0 5 C o h 0 k / y G F B 5 o p Q / 9 U O j H F c z V 3 x 0 J C q Q c B 5 6 u z F a U s 1 4 m / u d 1 Y u W f u g n l U a w I x 5 N B f s y g C m E W C O x T Q b B i Y 0 0 Q F l T v C v E Q C Y S V j q 2 s Q 7 B n T 5 4 n z a O a f V y z r + y q 4 4 A J S m A P V M A h s M E J c M A 5 q I M G w O A e P I J n 8 G I 8 G E / G q / E 2 K S 0 Y 0 5 4 d 8 A f G + w 8 l B p t 2 < / l a t e x i t > I K k 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 h B P s A n u 6 O w 0 1 0 0 Y w 4 X h V N m v 2 R 8 = \" > A A A C B X i c b V D L S s N A F J 3 U V 6 2 v q O B G F 8 E i u r F k d K H L g J u K m w r 2 A W k M k + m k H T p 5 M D M R S s h C N / 6 K G x e K u H X p 3 p 1 / 4 y T t Q l s P X D i c c y / 3 3 u P F j A p p m t 9 a a W 5 + Y X G p v F x Z W V 1 b 3 9 A 3 t 1 o i S j g m T R y x i H c 8 J A i j I W l K K h n p x J y g w G O k 7 Q 0 v c r 9 9 R 7 i g U X g j R z F x A t Q P q U 8 x k k p y 9 b 1 u g O T A 8 9 P L z E 2 H x z C 7 L Q S M W H q V u X r V r J k F j F k C J 6 R q H X 7 a 9 + Z O v e H q X 9 1 e h J O A h B I z J I Q N z V g 6 K e K S Y k a y S j c R J E Z 4 i P r E V j R E A R F O W n y R G Q d K 6 R l + x F W F 0 i j U 3 x M p C o Q Y B Z 7 q z E 8 U 0 1 4 u / u f Z i f T P n Z S G c S J J i M e L / I Q Z M j L y S I w e 5 Q R L N l I E Y U 7 V r Q Y e I I 6 w V M F V V A h w + u V Z 0 j q p w d M a v I Z V y w J j l M E u 2 A d H A I I z Y I E 6 a I A m w O A B P I E X 8 K o 9 a s / a m / Y + b i 1 p k 5 l t 8 A f a x w 9 Z D p t f < / l a t e x i t > I K k < l a t e x i t s h a 1 _ b a s e 6 4 = \" O m 7 E X v r 6 T o b b o 3 V F r p G i + F b o W C s = \" > A A A C A 3 i c b V C 7 T s M w F H V a H q W 8 A m z A Y L V C Y q o S G G C M x A J i K Y g + p D Z U j u u 0 V h 0 n s h 2 k K o r E w q + w M I A Q K z / B x i 8 w 8 g U 4 a Q d o O Z K l 4 3 P u 1 b 3 3 e B G j U l n W p 1 E o L i w u L Z d W y q t r 6 x u b 5 t Z 2 U 4 a x w K S B Q x a K t o c k Y Z S T h q K K k X Y k C A o 8 R l r e 6 C z z W 3 d E S B r y G z W O i B u g A a c + x U h p q W f u d g O k h p 6 f X K S 9 Z J T e 5 l + M W H K Z 9 s y q V b N y w H l i T 0 n V q Y j i / t f 3 d b 1 n f n T 7 I Y 4 D w h V m S M q O b U X K T Z B Q F D O S l r u x J B H C I z Q g H U 0 5 C o h 0 k / y G F B 5 o p Q / 9 U O j H F c z V 3 x 0 J C q Q c B 5 6 u z F a U s 1 4 m / u d 1 Y u W f u g n l U a w I x 5 N B f s y g C m E W C O x T Q b B i Y 0 0 Q F l T v C v E Q C Y S V j q 2 s Q 7 B n T 5 4 n z a O a f V y z r + y q 4 4 A J S m A P V M A h s M E J c M A 5 q I M G w O A e P I J n 8 G I 8 G E / G q / E 2 K S 0 Y 0 5 4 d 8 A f G + w 8 l B p t 2 < / l a t e x i t > It < l a t e x i t s h a 1 _ b a s e 6 4 = \" m 1 + N g e A D r B / r d u i z r p s L i a P j m I 0 = \" > A A A B 8 3 i c b V D L S s N A F L 3 x W e u r 2 q W b w S K 4 k J L o Q p c B N 7 q r Y B / Q h D K Z T t q h k 0 m Y m Q g l Z O N H u H G h i F t / x o X g H / g N r p y 0 X W j r g Y H D O f d y z 5 w g 4 U x p 2 / 6 0 l p Z X V t f W S x v l z a 3 t n d 3 K 3 n 5 L x a k k t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o s v D b d 1 Q q F o t b P U 6 o H + G B Y C E j W B v J 8 y K s h 0 G Y X e c 9 3 a v U 7 L o 9 A V o k z o z U 3 O r H 9 / 1 X f t L o V d 6 9 f k z S i A p N O F a q 6 9 i J 9 j M s N S O c 5 m U v V T T B Z I Q H t G u o w B F V f j b J n K M j o / R R G E v z h E Y T 9 f d G h i O l x l F g J o u M a t 4 r x P + 8 b q r D C z 9 j I k k 1 F W R 6 K E w 5 0 j E q C k B 9 J i n R f G w I J p K Z r I g M s c R E m 5 r K p g R n / s u L p H V a d 8 7 q z o 1 T c 1 2 Y o g Q H c A j H 4 M A 5 u H A F D W g C g Q Q e 4 A m e r d R 6 t F 6 s 1 + n o k j X b q c I f W G 8 / 8 e m W C A = = < / l a t e x i t > \u21e5 < l a t e x i t s h a 1 _ b a s e 6 4 = \" g + m q z I t E i g T O p E p Z 1 3 N Z G X W d z V s = \" > A A A B / H i c d V D L S s N A F J 2 o 1 V p f 0 Y I b N 4 N F c F W S t r R 1 V 3 D j s o J 9 Q F P K Z D J p h 0 4 m Y W Y i h F C X f o X g x o W i b v 0 Q d\n\nL x a k k t E l i H s t O g B X l T N C m Z p r T T i I p j g J O 2 8 H o s v D b d 1 Q q F o t b P U 6 o H + G B Y C E j W B v J 8 y K s h 0 G Y X e c 9 3 a v U 7 L o 9 A V o k z o z U 3 O r H 9 / 1 X f t L o V d 6 9 f k z S i A p N O F a q 6 9 i J 9 j M s N S O c 5 m U v V T T B Z I Q H t G u o w B F V f j b J n K M j o / R R G E v z h E Y T 9 f d G h i O l x l F g J o u M a t 4 r x P + 8 b q r D C z 9 j I k k 1 F W R 6 K E w 5 0 j E q C k B 9 J i n R f G w I J p K Z r I g M s c R E m 5 r K p g R n / s u L p H V a d 8 7 q z o 1 T c 1 2 Y o g Q H c A j H 4 M A 5 u H A F D W g C g Q Q e 4 A m e r d R 6 t F 6 s 1 + n o k j X b q c I f W G 8 / 8 e m W C A = = < / l a t e x i t >\n\n\nIt 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y y k c 5 a F U 2 6 Z I H 6 V u g x u b M 6 Z b q 2 E = \" > A A A B + X i c b V D L S g M x F M 3 U V 6 2 v U V c i S L A I b i w T X e h y w I 3 i p o J 9 Q D s M m T T T h m Y y Q 5 I p l G E W / o c b F 4 r o 0 m / w B 9 z 5 N 2 b a L r R 6 I H A 4 5 1 7 u y Q k S z p R 2 n C + r t L C 4 t L x S X q 2 s r W 9 s b t n b O 0 0 V p 5 L Q B o l 5 L N s B V p Q z Q R u a a U 7 b i a Q 4 C j h t B c P L w m + N q F Q s F n d 6 n F A v w n 3 B Q k a w N p J v 2 9 0 I 6 0 E Q Z t e 5 n + k T l P t 2 1 a k 5 E 8 C / B M 1 I 1 T 2 4 f 7 u R H 3 t 1 3 / 7 s 9 m K S R l R o w r F S H e Q k 2 s u w 1 I x w m l e 6 q a I J J k P c p x 1 D B Y 6 o 8 r J J 8 h w e G a U H w 1 i a J z S c q D 8 3 M h w p N Y 4 C M 1 n k V P N e I f 7 n d V I d X n g Z E 0 m q q S D T Q 2 H K o Y 5 h U Q P s M U m J 5 m N D M J H M Z I V k g C U m 2 p R V M S W g + S / / J c 3 T G j q r o V t U d V 0 w R R n s g 0 N w D B A 4 B y 6 4 A n X Q A A S M w A N 4 A s 9 W Z j 1 a L 9 b r d L R k z X Z 2 w S 9 Y 7 9 + q I Z a m < / l a t e x i t >\n\n\nIt+1\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" G W k K 8 C J 2 W S O o q D t 6 d 5 p f X X a / h y s = \" > A A A B + X i c b V D L S g M x F M 3 U V 6 2 v U V c i S L A I g l A m u t D l g B v F T Q X 7 g H Y Y M m m m D c 1 k h i R T K M M s / A 8 3 L h T R p d / g D 7 j z b 8 y 0 X W j 1 Q O B w z r 3 c k x M k n C n t O F 9 W a W F x a X m l v F p Z W 9 / Y 3 L K 3 d 5 o q T i W h D R L z W L Y D r C h n g j Y 0 0 5 y 2 E 0 l x F H D a C o a X h d 8 a U a l Y L O 7 0 O K F e h P u C h Y x g b S T f t r s R 1 o M g z K 5 z P 9 M n K P f t q l N z J o B / C Z q R q n t w / 3 Y j P / b q v v 3 Z 7 c U k j a j Q h G O l O s h J t J d h q R n h N K 9 0 U 0 U T T I a 4 T z u G C h x R 5 W W T 5 D k 8 M k o P h r E 0 T 2 g 4 U X 9 u Z D h S a h w F Z r L I q e a 9 Q v z P 6 6 Q 6 v P A y J p J U U 0 G m h 8 K U Q x 3 D o g b Y Y 5 I S z c e G Y C K Z y Q r J A E t M t C m r Y k p A 8 1 / + S 5 q n N X R W Q 7 e o 6 r p g i j L Y B 4 f g G C B w D l x w B e q g A Q g Y g Q f w B J 6 t z H q 0 X q z X 6 W j J m u 3 s g l + w 3 r 8 B p x W W p A = = < / l a t e x i t > \n\n\nIp\n\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" + i 4 P t f N l b u D W H 1 r C k Z Y 3 d F U H F 9 8 = \" > A A A B 8 3 i c b V D L S g M x F M 3 U V 6 2 v q k t B Q o v g q s z o Q p c D b n R X w T 6 g M 5 R M J t O G Z p K Q Z I Q y d O U / u H H h A 7 f + j D s / x Z 2 Z t g t t P R A 4 n H M v 9 + R E k l F t X P f L K a 2 s r q 1 v l D c r W 9 s 7 u 3 v V / Y O 2 F p n C p I U F E 6 o b I U 0 Y 5 a R l q G G k K x V B a c R I J x p d F X 7 n n i h N B b 8 z Y 0 n C F A 0 4 T S h G x k p B k C I z j J L 8 Z t K X / W r d b b h T w G X i z U n d r 7 0 + f B / H o t m v f g a x w F l K u M E M a d 3 z X G n C H C l D M S O T S p B p I h E e o Q H p W c p R S n S Y T z N P 4 I l V Y p g I Z R 8 3 c K r + 3 s h R q v U 4 j e x k k V E v e o X 4 n 9 f L T H I Z 5 p T L z B C O Z 4 e S j E E j Y F E A j K k i 2 L C x J Q g r a r N C P E Q K Y W N r q t g S v M U v L 5 P 2 W c M 7 b 3 i 3 X t 3 3 w Q x l c A R q 4 B R 4 4 A L 4 4 B o 0 Q Q t g I M E j e A Y v T u Y 8 O W / O + 2 y 0 5 M x 3 D s E f O B 8 / 4 T m V P g = = < / l a t e x i t >\n\n\nIn\nm U v V T T B Z I Q H t G u o w B F V f j b J n K M j o / R R G E v z h E Y T 9 f d G h i O l x l F g J o u M\na t 4 r x P + 8 b q r D C z 9 j I k k 1 F W R 6 K E w 5 0 j E q C k B 9 J i n R f G w I J p K Z r I g M s c R E m 5 r K p g R n / s u L p H V a d 8 7 q z o 1 T c 1 2 Y o g Q H c A j H 4 M A 5 u H A F D W g C g Q Q e 4 A m e r d R 6 t F 6 s 1 + n o k j X b q c I f W G 8 / 6 N G W A g = = < / l a t e x i t >  edge-aware disparity smoothness loss, where the disparity is simply defined as d t (x) = 1/D t (x). Optimization Loss for Keyframe Selection The learning of keyframe selection intensively applies the triplet losses to measure the similarity between frames. Specifically, two kinds of triplets are constructed: (1) I t , I s , I p , in which I s is one image in the training sequence I s , (2) I t , I s , I n , in which I s is one image in the visual odometry training image set I vo . The first triplet is used to rank the similarities w.r.t. I t among samples inside the interval around the target image I t , where I t should be more similar to I s than I p with a small margin \u03b3 p . The second one ranks the similarities with a large margin \u03b3 n , it suggests a much larger similarity between I t and any sample in I vo than the negative sample I n . To this end, the keyframe loss is written as\n\n\n\u21e5 < l a t e x i t s h a 1 _ b a s e 6 4 = \" g + m q z I t E i g T O p E p Z 1 3 N Z G X W d z V s = \" > A A A B / H i c d V D L S s N A F J 2 o 1 V p f 0 Y I b N 4 N F c F W S t r R 1 V 3 D j s o J 9 Q F P K Z D J p h 0 4 m Y W Y i h F C X f o X g x o W i b v 0 Q d 3 6 I e y e t g o p e G O Z w z r 3 c c 4 8 b M S q V Z b 0 Z S 8 s r u d W 1 / H p h Y 3 N r e 8 f c 3 e v K M B a Y d H D I Q t F 3 k S S M c t J R V D H S j w R B g c t I z 5 2 e Z n r v k g h J Q 3 6 h k o g M A z T m 1 K c Y K U 2 N z K L j h s y T S a C / 1 F E 0 I H I 2 M k t W + a R Z r 9 T q 0 C p b V s O u 2 B m o N G r V G r Q 1 k 1 W p t X / z z p + u c + 2 R + e p 4 I Y 4 D w h V m S M q B b U V q m C K h K G Z k V n B i S S K E p 2 h M B h p y p L c M 0 7 n 5 G T z S j A f 9 U O j H F Z y z 3 y d S F M j M n + 4 M k J r I 3 1 p G / q U N Y u U 3 h y n l U a w I x 4 t F f s y g C m G W B P S o I F i x R A O E B d V e I Z 4 g g b D S e R V 0 C F + X w v 9 B t 1 K 2 q 2 X 7 3 C 6 1 2 m B R e X A A D s E x s E E D t M A Z a I M O w C A B t + A e P B h X x p 3 x a D w v W p e M z 5 k i + F H G y w f h o Z l O < / l a t e x i t >\n\n\n\u21e5 < l a t e x i t s h a 1 _ b a s e 6 4 = \" g + m q z I t E i g T O p E p Z 1 3 N Z G X W d z V s = \" > A A A B / H i c d V D L S s N A F J 2 o 1 V p f 0 Y I b N 4 N F c F W S t r R 1 V 3 D j s o J 9 Q F P K Z D J p h 0 4 m Y W Y i h F C X f o X g x o W i b v 0 Q d 3 6 I e y e t g o p e G O Z w z r 3 c c 4 8 b M S q V Z b 0 Z S 8 s r u d W 1 / H p h Y 3 N r e 8 f c 3 e v K M B a Y d H D I Q t F 3 k S S M c t J R V D H S j w R B g c t I z 5 2 e Z n r v k g h J Q 3 6 h k o g M A z T m 1 K c Y K U 2 N z K L j h s y T S a C / 1 F E 0 I H I 2 M k t W + a R Z r 9 T q 0 C p b V s O u 2 B m o N G r V G r Q 1 k 1 W p t X / z z p + u c + 2 R + e p 4 I Y 4 D w h V m S M q B b U V q m C K h K G Z k V n B i S S K E p 2 h M B h p y p L c M 0 7 n 5 G T z S j A f 9 U O j H F Z y z 3 y d S F M j M n + 4 M k J r I 3 1 p G / q U N Y u U 3 h y n l U a w I x 4 t F f s y g C m G W B P S o I F i x R A O E B d V e I Z 4 g g b D S e R V 0 C F + X w v 9 B t 1 K 2 q 2 X 7 3 C 6 1 2 m B R e X A A D s E x s E E D t M A Z a I M O w C A B t + A e P B h X x p 3 x a D w v W p e M z 5 k i + F H G y w f h o Z l O < / l a t e x i t >\nL kf = Is\u2208Is/{It} max{0, \u03b3 p \u2212 q t\u2194s + q t\u2194p } + Is\u2208Ivo/{It} max{0, \u03b3 n \u2212 q t\u2194s + q t\u2194n }. (6)\nThe similarity score is generated according to our keyframe selector in Eq. (3). The intra-sample margin is \u03b3 p = 0.1 and the inter-sample margin is \u03b3 n = 0.8. Overall Optimization Objectives The final loss for our collaborative learning is a weighted combination of the aforementioned losses, written as L total = \u03bb pc L pc + \u03bb cc L cc + \u03bb ds L ds + \u03bb kf L kf . (7) The weights are to balance the contribution of each term. In our experiments, we set \u03bb pc = 1.0, \u03bb cc = 0.05, \u03bb ds = 0.5 and \u03bb kf = 1.0. Note that our depth predictor D \u03c6 D uses multi-scale depth predictions to release the local gradient issue [51], thus the losses L pc , L cc and L ds are also applied in coarser scales, but the weights are decayed accordingly.\n\n\nExperiments\n\n\nExperimental Setup\n\nNetwork Architecture. Our model mainly contains three components, the depth predictor D \u03c6 D , the camera motion estimator C \u03c6 C and the keyframe selector S \u03c6 S . The depth predictor follows the skip-connected encoder-decoder structure as SfMLearner [51], and outputs 4-scale depth predictions. The camera motion estimator regresses the 6-DoF camera motions by 8 convolution layers followed by a global average pooling, as the structure in [51]. The structure of the keyframe selector has two parallel branches, its network specification is depicted in Sec. 3.2. We adopt batch normalization and ReLU activation function after all the convolution layers except the output layers. Datasets. We train our system on the train split by Eigen et al. [6] on the KITTI raw dataset with all static frames excluded. This dataset contains stereo views, and we use them independently. The train/val ratio is 9 : 1, following Zhou et al. [51]. To test the performance of our visual odometry and keyframe selection, we also transfer our system onto the KITTI odometry dataset. We employ the 00 \u223c 08 sequences for training, and the 09 \u223c 10 for testing. Training Details. Our experiments are conducted using the TensorFlow framework [1]. We train our model in an end-to-end fashion with our special designed training data preparation. During training, we resize the image sequences to a resolution of 128\u00d7416, and perform several preprocessing tricks such as random cropping and resizing, and random brightness [49,51]. The network is trained by Adam solver with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. The learning rate is simply fixed at 0.0002 and the batch size is 8. The network is trained on a single NVIDIA Titan X GPU. The training process typically takes around 30 epochs. Evaluation Protocol. The depth prediction performance on  Figure 4. Monocular depth prediction comparison with Zhou et al. [51], GeoNet [49] and DDVO [41].The ground-truth is interpolated for visualization. Our method captures more geometric details, preserves the structure consistency and avoids artifacts in texture-less area.\n\n\nMethod\n\nSetting Cap Data abs rel sq rel RMSE RMSE(log) \u03b4 < 1.  [12] using the split of Eigen et al. [6]. We reports 7 metrics as suggested by Eigen et al. [6]. We also indicate the training setting, and the training data structure. Is means consecutive frames, IK is our keyframe augmented sequences. Bold means the overall best results. \"w/o CC\" means the our visual odometry module trained without cycle consistency. *Updated results provided on the website of Zhou et al. [51]. In setting, depth-gt and post-gt mean that the methods require depth and pose groundtruth in a supervised setting.\n\nis evaluated on the 697 images from the test split of Eigen et al. [6], which covers 29 scenes in the KITTI raw dataset. Following [51], the predicted depth maps are scaled with a factor by matching its median to its ground-truth data, i.e., D pred = median(D gt )/median(D pred ). We use the same depth evaluation metrics as in Eigen et al. [6]. Note that most reference monocular depth prediction methods use consecutive |I s | = 3 frames, but our method requires two additional intra-/inter-class samples, such as the set I K .\n\nThe camera motion evaluation is on the 09 \u223c 10 sequences in the KITTI odometry split. Following [51], all of the reported results are evaluated in terms of 5-frame snippets. To resolve scale ambiguities that frequently occur in monocular visual odometry or SLAM systems, we adjust the scaling factors of the results to optimally align with the ground-truth trajectories. We use Absolute Trajectory Error (ATE) to evaluate trajectory drift for 5-frame snippet.\n\nFor keyframe selection evaluation, we gather snippets whose starting frame is a reference keyframe and a pseudo-GT keyframe is located in the middle of the snippet. The pseudo-GT keyframe is detected if the ratio of the overlapping area with the reference keyframe is just below 50%, in which the overlapping area is defined by the ground-truth camera motion and interpolated depth maps. We apply this strategy to the KITTI odometry test split.\n\n\nOverall Performance Analysis\n\nPerformance of Monocular Depth Estimation. As shown in Tab. 1, if truncating the depth predictions by 80m, our proposed unsupervised approach outperforms all the com-Method Absolute Trajectory Error sequence 09 sequence 10 ORB-SLAM (full) 0.014 \u00b1 0.008 0.012 \u00b1 0.011 ORB-SLAM (short) 0.064 \u00b1 0.141 0.064 \u00b1 0.130 Mean Odom.\n\n0.032 \u00b1 0.026 0.028 \u00b1 0.023 Zhou et al. [51] 0.021 \u00b1 0.017 0.020 \u00b1 0.015 Zhou et al. [51]* 0.016 \u00b1 0.009 0.013 \u00b1 0.009 GeoNet [49] 0.012 \u00b1 0.007 0.012 \u00b1 0.009 Klodt et al. [22] 0.014 \u00b1 0.007 0.013 \u00b1 0.009\n\nOurs \u00d7 0.012 \u00b1 0.006 0.010 \u00b1 0.008 Table 2. Absolute Trajectory Error (ATE) on the KITTI odometry test split averaged over all 5-frame snippets (lower is better). \u00d7 Our method does not trained by 5-frame snippets but 3-frame snippets with two additional intra-/inter-samples. *Updated results. pared unsupervised monocular depth prediction methods on most of the evaluation metrics, including [51,49,22], and even some recent methods with calibrated stereo data [11,14] or directly supervised by ground-truth depth [25,6]. DDVO [41] has marginal improvements on the scores \u03b4 < 1.25 and RMSE, but our method has a significant gain over the squared relative difference (sq rel) from 1.257 to 1.021. If truncating the predictions by 50m, our model achieves the best performance on all the metrics. Performance of Camera Pose Estimation. We compare our method with a traditional monocular SLAM system named ORB-SLAM (full) [30] and its local version ORB-SLAM (short) for 5-frame snippets, their results are borrowed from the website of Zhou et al. [51]. We also compare with SfMLearner [51] and GeoNet [49]. As in Tab. 2, our method outperforms a na\u00efve baseline (mean odometry) and the conventional method ORB-SLAM (short) and ORB-SLAM (full). With respect to deep learning based approaches, our camera motion estimator is better than SfM-Learner proposed by Zhou [51], but its performance is slightly inferior to GeoNet [49]. We believe this gap could be eliminated if our model is trained by longer snippets. We also show the average rotation errors over all 5frame snippets, in which the error is calculated as 2 norm between the rotation angles from the predictions and the ground-truths, as shown in Fig. 5. Although our method was only trained on shorter sequences, its predicted rotations are more accurate to the other learning based ap- Figure 6. Keyframe selection accompanied with the visual odometry. The test sequence is 09 in KITTI odometry test split.\n\nColumn, stacked column, and area charts compare data from multiple categories. For example, you can compare the annual sales of three products. The x-axis shows years and the y-axis shows quantities.   proaches [51,49], which reveals the significance of the keyframes in helping regularize the odometry learning, especially the case with geometric changes from rotations. Note that two variants of ORB-SLAM offer better rotation predictions than learning based models. That is probably because that the results of ORB-SLAM gathered from Zhou et al. [51] are shorter than 5 frames and thus only contain smaller camera motions. Performance of Keyframe Selection. We also give a few exemplar experiments on the quality of our keyframe selection module. First, we show the selected keyframe set when we execute the complete visual odometry to the test sequence 09 in the KITTI odometry test split. The selected keyframes are usually uniformly distributed when the car drives straight along the street, such as the keyframes shown in the left of Fig. 6. However, when the car turns left/right, the sudden geometric changes induce large visual dissimilarity in the captured frames, and thus our keyframe selection encourages more frequent keyframes in a short interval at the turning corner. It also reveals that our keyframe selection is more sensitive to geometry-based visual changes. We also qualitatively test our keyframe selector by reporting the ratio that the detected keyframes are within a fixed range [\u2212\u2206, \u2206] around the pseudo-GT keyframes, as shown in Fig. 7. The proposed keyframe selector combines the merits from both the visual and the geometric cues, and improves their sole models with a large margin. Qualitative Evaluation. We compare our predicted depth maps with those by Zhou et al. [51], GeoNet [49] and DDVO [41], as in Fig. 4. The ground-truth depth maps are re-projected sparse point clouds from velodyne laser scanner. The proposed method has the best visual quality among the prior arts. It can successfully recover reliable depths inside challenging regions (e.g., texture-less area in the first column of Fig. 4), preserve piece-wise smooth structural details but not introduce texture-mapping from the input image. DDVO has the most comparable performance and more often resolves small objects, but it usually suffers severe texture-mapping artifacts, as show in Fig 8(b).\n\n\nModel Component Analysis\n\nBaseline Models. To evaluation the performance effect of different modules, we have several baselines: (i) SFM-Learner [51], which does not use any keyframe selection in the visual odometry; (ii) Ours w/o KFS, which improves the performance over [51], and still not using keyframe selection; (iii) Ours w/ KFS (off-line), which pretrains the keyframe selection sub-network using only the visual clue, and produce a set of keyframes for training with the visual odometry sub-network; (iv) Ours w/ KFS (fixed-length), which use a fixed frame length to determine the keyframe, and the keyframe selection sub-network is jointly optimized with the visual odometry sub-network; (iv) Ours w/ KFS (online updating) which uses the proposed keyframe management strategy to online update the keyframe pool and the multiple sub-tasks are jointly learned. Effect of Keyframe Selection on Visual Odometry. Tab. 3 and 4 show the results of different baseline models on monocular depth and pose estimation tasks. It can be observe that, even if we use off-line keyframe information, we can still improve the performance on both depth and pose estimations. By jointly learning two tasks, especially employing the online keyframe updating, a clear performance gain is obtained, demonstrating the effectiveness of the proposed keyframe detection onto the task of visual odometry. Effect of Visual Odometry on Keyframe Selection. Fig. 5 shows the average rotation errors of different methods on the KITTI odometry. To compare with our direct competitor [51], our model with keyframe selection significantly outperforms their method by reducing the errors with a large margin, which means that the visual odometry network provides better geometric output helping learning better keyframe detector, confirming our initial intuition. Effectiveness of Cycle Consistency. We also conduct a piece of ablation study about the cycle consistency, as shown in Tab. 1. Without cycle consistency, the learning of our depth predictor is similar to Zhou et al. [51] but with additional long-term connections from \"keyframes\". Its performance is superior to Zhou et al. [51] and comparable to recent advanced methods, showing the advances of keyframes in helping visual odometry module. Cycle consistency clearly increases the prediction reliability, as shown in Fig. 8(a), and it boosts the quantitative results of our model to a large margin. But we need to mention that the cycle consistency may not optimal in detecting non-rigid motion regions, thus we may inevitably find depth distortions around moving objects, such as cars in Fig. 4.\n\n\nConclusion\n\nIn this paper we have proposed a learning approach towards monocular visual SLAM. In detail, we designed a deep network for the keyframe selection, which is able to to detect keyframes, manage the keyframes and localize new frames. And we further proposed an end-to-end unsupervised learning framework to simultaneously optimize the keyframe selection and the visual odometry tasks in a single deep model. To constrain and benefit each task during the network learning, a unsupervised collaborative learning strategy was designed. We clearly demonstrated the effectiveness of the proposed approach on KITTI raw and KITTI Odometry datasets with a significant gain over the baseline models, and created new state-of-the-art results on depth and pose estimation from monocular videos.\n\nFigure 1 .\n1Illustration of our motivation: The keyframe selection and the visual odometry are intercorrelated in monocular visual SLAM. The keyframes can improve the depth prediction and camera motion estimation for the visual odometry, while inversely, the visual odometry facilitates more effective identification of keyframes. We expect that a joint learning of both tasks in a single deep model would make them benefit each other.\n\nFigure 3 .\n3(a) The keyframe management, including keyframe merging and keyframe inserting in the training phase, and keyframe set construction in the testing phase. (b) The collaborative learning scheme. The training image tuples consists of consecutive frames around the target image, and two randomly sampled intra-class image Ip and inter-class image In. The keyframe selection task uses the complete training tuple, but the visual odometry does not use the inter-class sample.\n\nFigure 5 .\n5Average rotation errors on the KITTI odometry test split (lower is better). ORB-SLAM-S and ORB-SLAM-F is the short and the full of ORB-SLAM respectively.\n\nFigure 7 .\n7Detection ratio evaluation of Keyframe detection combined with the visual odometry.\n\nFigure 8 .\n8(a) Cycle consistency improves the geometry reliability of the predicted depth maps. Some examples of DDVO[41] (b) show texture-copying artifacts and unwanted texture blurs.\n\n\nThe test sequence is 09 in KITTI odometry test split.Method \nError (lower is better) \nrel \nsq rel RMSE RMSE (log) \n\nSFM-Learner [51] w/o KFS \n0.208 1.768 \n6.856 \n0.283 \nOurs w/o KFS \n0.181 1.587 \n6.689 \n0.264 \nOurs w/ KFS (off-line) \n0.171 1.389 \n6.237 \n0.251 \nOurs w/ KFS (fixed-length) \n0.151 1.127 \n5.941 \n0.223 \nOurs w/ KFS (online updating) 0.139 1.021 \n5.418 \n0.209 \n\n\n\nTable 3 .\n3Quantitative comparison of different variants of the proposed approach w.r.t. the error evaluation metrics on the task of monocular depth estimation. KFS means key-frame selection. Ours w/ KFS (online updating) 0.012 \u00b1 0.006 0.010 \u00b1 0.008Table 4. Quantitative comparison of different variants of the proposed approach w.r.t Absolute Trajectory Error (ATE) on the KITTI odometry test split.Method \nAbsolute Trajectory Error \nsequence 09 \nsequence 10 \n\nSFM-Learner [51] w/o KFS \n0.021 \u00b1 0.017 0.020 \u00b1 0.015 \nOurs w/o KFS \n0.018 \u00b1 0.012 0.017 \u00b1 0.012 \nOurs w/ KFS (off-line) \n0.015 \u00b1 0.008 0.014 \u00b1 0.011 \nOurs w/ KFS (fixed-length) \n0.014 \u00b1 0.007 0.012 \u00b1 0.009 \n\n\nTensorflow: a system for large-scale machine learning. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, OSDI. 16Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe- mawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, vol- ume 16, pages 265-283, 2016. 5\n\nGeometry-aware learning of maps for camera localization. Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, Jan Kautz, CVPR. Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, and Jan Kautz. Geometry-aware learning of maps for camera localization. In CVPR, 2018. 2\n\nTwo deterministic half-quadratic regularization algorithms for computed imaging. Pierre Charbonnier, Laure Blanc-F\u00e9raud, Gilles Aubert, Michel Barlaud, ICIP. IEEE2Pierre Charbonnier, Laure Blanc-F\u00e9raud, Gilles Aubert, and Michel Barlaud. Two deterministic half-quadratic regular- ization algorithms for computed imaging. In ICIP, volume 2, pages 168-172. IEEE, 1994. 4\n\nInstance-aware semantic segmentation via multi-task network cascades. Jifeng Dai, Kaiming He, Jian Sun, CVPR. Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se- mantic segmentation via multi-task network cascades. In CVPR, 2016. 2\n\n. Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, arXiv:1707.07410Toward geometric deep SLAM. arXiv preprintDaniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Toward geometric deep SLAM. arXiv preprint arXiv:1707.07410, 2017. 1\n\nDepth map prediction from a single image using a multi-scale deep network. David Eigen, Christian Puhrsch, Rob Fergus, NIPS. 67David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In NIPS, 2014. 2, 5, 6, 7\n\nDirect sparse odometry. Jakob Engel, Vladlen Koltun, Daniel Cremers, TPAMI. 403Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. TPAMI, 40(3):611-625, 2017. 2\n\nLSD-SLAM: Large-scale direct monocular SLAM. Jakob Engel, Thomas Sch\u00f6ps, Daniel Cremers, ECCV. Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. LSD- SLAM: Large-scale direct monocular SLAM. In ECCV, 2014. 2\n\nSVO: Fast semi-direct monocular visual odometry. Christian Forster, Matia Pizzoli, Davide Scaramuzza, ICRA. 1Christian Forster, Matia Pizzoli, and Davide Scaramuzza. SVO: Fast semi-direct monocular visual odometry. In ICRA, 2014. 1, 2\n\nVisual simultaneous localization and mapping: a survey. Jorge Fuentes-Pacheco, Jos\u00e9 Ruiz-Ascencio, Juan Manuel Rend\u00f3n-Mancha, Artificial Intelligence Review. 431Jorge Fuentes-Pacheco, Jos\u00e9 Ruiz-Ascencio, and Juan Manuel Rend\u00f3n-Mancha. Visual simultaneous localization and mapping: a survey. Artificial Intelligence Review, 43(1):55-81, 2015. 1\n\nUnsupervised CNN for single view depth estimation: Geometry to the rescue. Ravi Garg, Vijay Kumar, B G , Gustavo Carneiro, Ian Reid, ECCV. 67Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Reid. Unsupervised CNN for single view depth estimation: Geometry to the rescue. In ECCV, 2016. 2, 6, 7\n\nAndreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, Vision meets robotics: The KITTI dataset. IJRR. 326Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. IJRR, 32(11):1231-1237, 2013. 1, 2, 6\n\nRegion-based convolutional networks for accurate object detection and segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, TPAMI38Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 38(1):142-158, 2016. 2\n\nUnsupervised monocular depth estimation with leftright consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, CVPR. 7Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- right consistency. In CVPR, 2017. 2, 6, 7\n\nRuben Gomez-Ojeda, David Zu\u00f1iga-No\u00ebl, Francisco-Angel Moreno, Davide Scaramuzza, Javier Gonzalez-Jimenez, arXiv:1705.09479PL-SLAM: a stereo slam system through the combination of points and line segments. arXiv preprintRuben Gomez-Ojeda, David Zu\u00f1iga-No\u00ebl, Francisco-Angel Moreno, Davide Scaramuzza, and Javier Gonzalez-Jimenez. PL-SLAM: a stereo slam system through the combination of points and line segments. arXiv preprint arXiv:1705.09479, 2017. 2\n\nSemi-parametric models for visual odometry. Vitor Guizilini, Fabio Ramos, ICRA. Vitor Guizilini and Fabio Ramos. Semi-parametric models for visual odometry. In ICRA, 2012. 2\n\nA benchmark for rgb-d visual odometry, 3D reconstruction and SLAM. Ankur Handa, Thomas Whelan, John Mcdonald, Andrew J Davison, ICRA. 2Ankur Handa, Thomas Whelan, John McDonald, and An- drew J Davison. A benchmark for rgb-d visual odometry, 3D reconstruction and SLAM. In ICRA, 2014. 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 4\n\nKeyframe-based dense planar SLAM. Ming Hsiao, Eric Westman, Guofeng Zhang, Michael Kaess, ICRA. Ming Hsiao, Eric Westman, Guofeng Zhang, and Michael Kaess. Keyframe-based dense planar SLAM. In ICRA, 2017. 2\n\nPosenet: A convolutional network for real-time 6-DOF camera relocalization. Alex Kendall, Matthew Grimes, Roberto Cipolla, ICCV. Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for real-time 6-DOF cam- era relocalization. In ICCV, 2015. 2\n\nRobust odometry estimation for RGB-D cameras. Christian Kerl, J\u00fcrgen Sturm, Daniel Cremers, ICRA. Christian Kerl, J\u00fcrgen Sturm, and Daniel Cremers. Robust odometry estimation for RGB-D cameras. In ICRA, pages 3748-3754. IEEE, 2013. 2\n\nSupervising the new with the old: learning SFM from SFM. Maria Klodt, Andrea Vedaldi, ECCV. 7Maria Klodt and Andrea Vedaldi. Supervising the new with the old: learning SFM from SFM. In ECCV, 2018. 2, 6, 7\n\nVasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, 3Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 3DV, 2016. 2\n\nUnDeepVO: Monocular visual odometry through unsupervised deep learning. Ruihao Li, Sen Wang, Zhiqiang Long, Dongbing Gu, ICRA. Ruihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu. UnDeepVO: Monocular visual odometry through unsuper- vised deep learning. In ICRA, 2018. 2\n\nLearning depth from single monocular images using deep convolutional neural fields. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian D Reid, TPAMI. 38107Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian D Reid. Learning depth from single monocular images using deep convolutional neural fields. TPAMI, 38(10):2024-2039, 2016. 6, 7\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox, CVPR. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR, 2016. 2\n\nRSLAM: A system for large-scale mapping in constant-time using stereo. Christopher Mei, Gabe Sibley, Mark Cummins, Paul Newman, Ian Reid, IJCV. 942Christopher Mei, Gabe Sibley, Mark Cummins, Paul New- man, and Ian Reid. RSLAM: A system for large-scale map- ping in constant-time using stereo. IJCV, 94(2):198-214, 2011. 2\n\nA constant-time efficient stereo SLAM system. Christopher Mei, Gabe Sibley, Mark Cummins, M Paul, Ian D Newman, Reid, BMVC. Christopher Mei, Gabe Sibley, Mark Cummins, Paul M Newman, and Ian D Reid. A constant-time efficient stereo SLAM system. In BMVC, 2009. 1\n\nUnFlow: Unsupervised learning of optical flow with a bidirectional census loss. Simon Meister, Junhwa Hur, Stefan Roth, AAAI. New Orleans, LouisianaSimon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un- supervised learning of optical flow with a bidirectional cen- sus loss. In AAAI, New Orleans, Louisiana, Feb. 2018. 4\n\nORB-SLAM: a versatile and accurate monocular SLAM system. Raul Mur-Artal, Jose Maria Martinez Montiel, Juan D Tardos, TRO. 3157Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. ORB-SLAM: a versatile and accurate monocular SLAM system. TRO, 31(5):1147-1163, 2015. 1, 2, 7\n\nORB-SLAM2: An open-source slam system for monocular, stereo, and rgb-d cameras. Raul Mur, -Artal , Juan D Tard\u00f3s, IEEE Transactions on Robotics. 335Raul Mur-Artal and Juan D Tard\u00f3s. ORB-SLAM2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE Transactions on Robotics, 33(5):1255-1262, 2017. 2\n\nTwo efficient solutions for visual odometry using directional correspondence. Oleg Naroditsky, S Xun, Jean Zhou, Gallier, Kostas Stergios I Roumeliotis, Daniilidis, TPAMI. 344Oleg Naroditsky, Xun S Zhou, Jean Gallier, Stergios I Roumeliotis, and Kostas Daniilidis. Two efficient solu- tions for visual odometry using directional correspondence. TPAMI, 34(4):818-824, 2012. 2\n\nVisual odometry for ground vehicle applications. David Nist\u00e9r, Oleg Naroditsky, James Bergen, JFR. 231David Nist\u00e9r, Oleg Naroditsky, and James Bergen. Visual odometry for ground vehicle applications. JFR, 23(1):3-20, 2006. 2\n\nCorrelation-based visual odometry for ground vehicles. Navid Nourani, - Vatani, Paulo Vinicius Koerich Borges, JFR. 285Navid Nourani-Vatani and Paulo Vinicius Koerich Borges. Correlation-based visual odometry for ground vehicles. JFR, 28(5):742-768, 2011. 2\n\nLarge-scale 6-DOF SLAM with stereo-in-hand. M Lina, Pedro Paz, Pini\u00e9s, D Juan, Jos\u00e9 Tard\u00f3s, Neira, TRO. 245Lina M Paz, Pedro Pini\u00e9s, Juan D Tard\u00f3s, and Jos\u00e9 Neira. Large-scale 6-DOF SLAM with stereo-in-hand. TRO, 24(5):946-957, 2008. 2\n\nUnsupervised adversarial depth estimation using cycled generative networks. Andrea Pilzer, Dan Xu, Mihai Puscas, Elisa Ricci, Nicu Sebe, 3DV. IEEE. Andrea Pilzer, Dan Xu, Mihai Puscas, Elisa Ricci, and Nicu Sebe. Unsupervised adversarial depth estimation using cy- cled generative networks. In 3DV. IEEE, 2018. 2\n\nVisual odometry. Davide Scaramuzza, Friedrich Fraundorfer, tutorialDavide Scaramuzza and Friedrich Fraundorfer. Visual odom- etry [tutorial].\n\n. IEEE Robotics & Automation Magazine. 184IEEE Robotics & Automation Magazine, 18(4):80-92, 2011. 2\n\nAppearanceguided monocular omnidirectional visual odometry for outdoor ground vehicles. Davide Scaramuzza, Roland Siegwart, TRO. 245Davide Scaramuzza and Roland Siegwart. Appearance- guided monocular omnidirectional visual odometry for out- door ground vehicles. TRO, 24(5):1015-1026, 2008. 2\n\nRobust monocular slam in dynamic environments. Wei Tan, Haomin Liu, Zilong Dong, Guofeng Zhang, Hujun Bao, ISMAR. Wei Tan, Haomin Liu, Zilong Dong, Guofeng Zhang, and Hujun Bao. Robust monocular slam in dynamic environ- ments. In ISMAR, 2013. 2\n\nCNN-SLAM: Real-time dense monocular slam with learned depth prediction. Keisuke Tateno, Federico Tombari, Iro Laina, Nassir Navab, CVPR. Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. CNN-SLAM: Real-time dense monocular slam with learned depth prediction. In CVPR, 2017. 2\n\nLearning depth from monocular videos using direct methods. Chaoyang Wang, Jos\u00e9 Miguel Buenaposada, Rui Zhu, Simon Lucey, CVPR. 7Chaoyang Wang, Jos\u00e9 Miguel Buenaposada, Rui Zhu, and Simon Lucey. Learning depth from monocular videos using direct methods. In CVPR, pages 2022-2030, 2018. 6, 7, 8\n\nStereo DSO: Large-scale direct sparse visual odometry with stereo cameras. Rui Wang, Martin Schworer, Daniel Cremers, ICCV. Rui Wang, Martin Schworer, and Daniel Cremers. Stereo DSO: Large-scale direct sparse visual odometry with stereo cameras. In ICCV, pages 3903-3911, 2017. 2\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, TIP. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600-612, 2004. 4\n\nReal-time large-scale dense RGB-D slam with volumetric fusion. Thomas Whelan, Michael Kaess, Hordur Johannsson, Maurice Fallon, J John, John Leonard, Mcdonald, IJRRThomas Whelan, Michael Kaess, Hordur Johannsson, Mau- rice Fallon, John J Leonard, and John McDonald. Real-time large-scale dense RGB-D slam with volumetric fusion. IJRR, 34(4-5):598-626, 2015. 2\n\nMulti-scale continuous crfs as sequential deep networks for monocular depth estimation. Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe, CVPR. Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Multi-scale continuous crfs as sequential deep networks for monocular depth estimation. In CVPR, 2017. 2\n\nMonocular depth estimation using multi-scale continuous crfs as sequential deep networks. D Xu, Ouyang Ricci, Wang Wanli, N Xiaogang, Sebe, TPAMI. 416D Xu, E Ricci, Ouyang Wanli, Wang Xiaogang, and N Sebe. Monocular depth estimation using multi-scale continuous crfs as sequential deep networks. TPAMI, 41(6):1426-1440, 2019. 2\n\nStructured attention guided convolutional neural fields for monocular depth estimation. Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa Ricci, CVPR. Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In CVPR, 2018. 6\n\nDeep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. Nan Yang, Rui Wang, J\u00f6rg St\u00fcckler, Daniel Cremers, In ECCV. 2Nan Yang, Rui Wang, J\u00f6rg St\u00fcckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry. In ECCV, 2018. 2\n\nGeoNet: Unsupervised learning of dense depth, optical flow and camera pose. Zhichao Yin, Jianping Shi, CVPR. 7Zhichao Yin and Jianping Shi. GeoNet: Unsupervised learn- ing of dense depth, optical flow and camera pose. In CVPR, 2018. 1, 3, 5, 6, 7, 8\n\nRobust appearance based visual route following for navigation in large-scale outdoor environments. M Alan, Lindsay Zhang, Kleeman, IJRR. 283Alan M Zhang and Lindsay Kleeman. Robust appearance based visual route following for navigation in large-scale outdoor environments. IJRR, 28(3):331-356, 2009. 2\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, CVPR. 7Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, 2017. 1, 3, 5, 6, 7, 8\n", "annotations": {"author": "[{\"start\":\"109\",\"end\":\"183\"},{\"start\":\"184\",\"end\":\"238\"},{\"start\":\"239\",\"end\":\"356\"},{\"start\":\"357\",\"end\":\"465\"}]", "publisher": null, "author_last_name": "[{\"start\":\"112\",\"end\":\"117\"},{\"start\":\"188\",\"end\":\"190\"},{\"start\":\"245\",\"end\":\"251\"},{\"start\":\"366\",\"end\":\"370\"}]", "author_first_name": "[{\"start\":\"109\",\"end\":\"111\"},{\"start\":\"184\",\"end\":\"187\"},{\"start\":\"239\",\"end\":\"244\"},{\"start\":\"357\",\"end\":\"365\"}]", "author_affiliation": "[{\"start\":\"138\",\"end\":\"182\"},{\"start\":\"214\",\"end\":\"237\"},{\"start\":\"280\",\"end\":\"355\"},{\"start\":\"394\",\"end\":\"464\"}]", "title": "[{\"start\":\"1\",\"end\":\"106\"},{\"start\":\"466\",\"end\":\"571\"}]", "venue": null, "abstract": "[{\"start\":\"596\",\"end\":\"1724\"}]", "bib_ref": "[{\"start\":\"1928\",\"end\":\"1932\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"2132\",\"end\":\"2135\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2135\",\"end\":\"2138\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"2625\",\"end\":\"2629\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"2629\",\"end\":\"2631\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"2804\",\"end\":\"2808\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"2808\",\"end\":\"2811\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"3552\",\"end\":\"3555\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3555\",\"end\":\"3558\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"5861\",\"end\":\"5865\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"6167\",\"end\":\"6171\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"6171\",\"end\":\"6174\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"6174\",\"end\":\"6177\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6177\",\"end\":\"6180\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"6188\",\"end\":\"6192\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"6192\",\"end\":\"6195\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"6195\",\"end\":\"6198\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"6224\",\"end\":\"6227\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"6227\",\"end\":\"6229\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"6554\",\"end\":\"6558\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"6572\",\"end\":\"6576\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"6576\",\"end\":\"6579\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"6590\",\"end\":\"6593\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"6770\",\"end\":\"6773\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"6933\",\"end\":\"6937\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7234\",\"end\":\"7238\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"7311\",\"end\":\"7315\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"7315\",\"end\":\"7318\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"7383\",\"end\":\"7387\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"7387\",\"end\":\"7390\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"7476\",\"end\":\"7480\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"7575\",\"end\":\"7579\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"8003\",\"end\":\"8007\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"8007\",\"end\":\"8010\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"8010\",\"end\":\"8013\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"8027\",\"end\":\"8030\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"8057\",\"end\":\"8061\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"8076\",\"end\":\"8079\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"8190\",\"end\":\"8209\"},{\"start\":\"8299\",\"end\":\"8303\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"8725\",\"end\":\"8729\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"8729\",\"end\":\"8732\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"8732\",\"end\":\"8735\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"8735\",\"end\":\"8738\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"8752\",\"end\":\"8756\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"8927\",\"end\":\"8931\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"9132\",\"end\":\"9136\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"9310\",\"end\":\"9314\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"10916\",\"end\":\"10920\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"11205\",\"end\":\"11209\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"11209\",\"end\":\"11212\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"14054\",\"end\":\"14058\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"17742\",\"end\":\"17746\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"17807\",\"end\":\"17810\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"18603\",\"end\":\"18607\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"19921\",\"end\":\"19922\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"37259\",\"end\":\"37263\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"37664\",\"end\":\"37668\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"37854\",\"end\":\"37858\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"38159\",\"end\":\"38162\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"38340\",\"end\":\"38344\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"38632\",\"end\":\"38635\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"38910\",\"end\":\"38914\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"38914\",\"end\":\"38917\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"39291\",\"end\":\"39295\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"39304\",\"end\":\"39308\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"39318\",\"end\":\"39322\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"39563\",\"end\":\"39567\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"39600\",\"end\":\"39603\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"39655\",\"end\":\"39658\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"39975\",\"end\":\"39979\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"40164\",\"end\":\"40167\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"40228\",\"end\":\"40232\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"40439\",\"end\":\"40442\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"40725\",\"end\":\"40729\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"41931\",\"end\":\"41935\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"41976\",\"end\":\"41980\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"42017\",\"end\":\"42021\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"42063\",\"end\":\"42067\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"42490\",\"end\":\"42494\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"42494\",\"end\":\"42497\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"42497\",\"end\":\"42500\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"42559\",\"end\":\"42563\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"42563\",\"end\":\"42566\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"42612\",\"end\":\"42616\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"42616\",\"end\":\"42618\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"42625\",\"end\":\"42629\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"43016\",\"end\":\"43020\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"43141\",\"end\":\"43145\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"43179\",\"end\":\"43183\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"43195\",\"end\":\"43199\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"43457\",\"end\":\"43461\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"43514\",\"end\":\"43518\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"44272\",\"end\":\"44276\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"44276\",\"end\":\"44279\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"44610\",\"end\":\"44614\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"45862\",\"end\":\"45866\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"45875\",\"end\":\"45879\",\"attributes\":{\"ref_id\":\"b49\"}},{\"start\":\"45889\",\"end\":\"45893\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"46608\",\"end\":\"46612\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"46735\",\"end\":\"46739\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"48022\",\"end\":\"48026\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"48516\",\"end\":\"48520\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"48624\",\"end\":\"48628\",\"attributes\":{\"ref_id\":\"b51\"}},{\"start\":\"51196\",\"end\":\"51200\",\"attributes\":{\"ref_id\":\"b41\"}}]", "figure": "[{\"start\":\"49893\",\"end\":\"50329\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"50330\",\"end\":\"50812\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"50813\",\"end\":\"50979\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"50980\",\"end\":\"51076\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"51077\",\"end\":\"51263\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"51264\",\"end\":\"51640\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"51641\",\"end\":\"52312\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1740\",\"end\":\"2139\"},{\"start\":\"2141\",\"end\":\"2931\"},{\"start\":\"2933\",\"end\":\"3742\"},{\"start\":\"3744\",\"end\":\"5007\"},{\"start\":\"5009\",\"end\":\"6018\"},{\"start\":\"6036\",\"end\":\"6296\"},{\"start\":\"6298\",\"end\":\"7072\"},{\"start\":\"7074\",\"end\":\"7721\"},{\"start\":\"7767\",\"end\":\"8563\"},{\"start\":\"8611\",\"end\":\"9739\"},{\"start\":\"9765\",\"end\":\"10257\"},{\"start\":\"10292\",\"end\":\"10458\"},{\"start\":\"10460\",\"end\":\"10544\"},{\"start\":\"10596\",\"end\":\"11124\"},{\"start\":\"11126\",\"end\":\"11453\"},{\"start\":\"11455\",\"end\":\"11687\"},{\"start\":\"11775\",\"end\":\"11786\"},{\"start\":\"11887\",\"end\":\"12252\"},{\"start\":\"12293\",\"end\":\"12804\"},{\"start\":\"12806\",\"end\":\"13517\"},{\"start\":\"13519\",\"end\":\"13576\"},{\"start\":\"13631\",\"end\":\"13786\"},{\"start\":\"13848\",\"end\":\"13914\"},{\"start\":\"13916\",\"end\":\"14555\"},{\"start\":\"14599\",\"end\":\"15042\"},{\"start\":\"15099\",\"end\":\"16195\"},{\"start\":\"16235\",\"end\":\"17471\"},{\"start\":\"17649\",\"end\":\"18275\"},{\"start\":\"18506\",\"end\":\"18700\"},{\"start\":\"18702\",\"end\":\"18801\"},{\"start\":\"18880\",\"end\":\"18932\"},{\"start\":\"19773\",\"end\":\"19923\"},{\"start\":\"30862\",\"end\":\"31936\"},{\"start\":\"33109\",\"end\":\"34316\"},{\"start\":\"36648\",\"end\":\"37378\"},{\"start\":\"37415\",\"end\":\"39497\"},{\"start\":\"39508\",\"end\":\"40095\"},{\"start\":\"40097\",\"end\":\"40627\"},{\"start\":\"40629\",\"end\":\"41088\"},{\"start\":\"41090\",\"end\":\"41534\"},{\"start\":\"41567\",\"end\":\"41889\"},{\"start\":\"41891\",\"end\":\"42095\"},{\"start\":\"42097\",\"end\":\"44059\"},{\"start\":\"44061\",\"end\":\"46460\"},{\"start\":\"46489\",\"end\":\"49096\"},{\"start\":\"49111\",\"end\":\"49892\"}]", "formula": "[{\"start\":\"10545\",\"end\":\"10595\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"11688\",\"end\":\"11743\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"11787\",\"end\":\"11872\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"13577\",\"end\":\"13630\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"13787\",\"end\":\"13847\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"15043\",\"end\":\"15098\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"17472\",\"end\":\"17648\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"18276\",\"end\":\"18505\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"18802\",\"end\":\"18879\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"18933\",\"end\":\"19025\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"19157\",\"end\":\"19772\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"20084\",\"end\":\"20283\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"20493\",\"end\":\"21024\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"21184\",\"end\":\"22089\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"22281\",\"end\":\"29093\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"32999\",\"end\":\"33108\",\"attributes\":{\"id\":\"formula_15\"}},{\"start\":\"36553\",\"end\":\"36647\",\"attributes\":{\"id\":\"formula_16\"}}]", "table_ref": "[{\"start\":\"42132\",\"end\":\"42139\"}]", "section_header": "[{\"start\":\"1726\",\"end\":\"1738\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"6021\",\"end\":\"6034\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"7724\",\"end\":\"7765\"},{\"start\":\"8566\",\"end\":\"8609\"},{\"start\":\"9742\",\"end\":\"9763\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"10260\",\"end\":\"10290\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"11745\",\"end\":\"11759\"},{\"start\":\"11762\",\"end\":\"11773\"},{\"start\":\"11874\",\"end\":\"11885\"},{\"start\":\"12255\",\"end\":\"12291\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"14558\",\"end\":\"14597\",\"attributes\":{\"n\":\"3.3.\"}},{\"start\":\"16198\",\"end\":\"16233\",\"attributes\":{\"n\":\"3.4.\"}},{\"start\":\"19027\",\"end\":\"19156\"},{\"start\":\"19926\",\"end\":\"20083\",\"attributes\":{\"n\":\"7\"}},{\"start\":\"20285\",\"end\":\"20492\"},{\"start\":\"21026\",\"end\":\"21183\"},{\"start\":\"22091\",\"end\":\"22280\"},{\"start\":\"29095\",\"end\":\"29780\"},{\"start\":\"29783\",\"end\":\"30853\"},{\"start\":\"30856\",\"end\":\"30860\"},{\"start\":\"31939\",\"end\":\"31941\"},{\"start\":\"31944\",\"end\":\"32993\"},{\"start\":\"32996\",\"end\":\"32998\"},{\"start\":\"34319\",\"end\":\"35434\"},{\"start\":\"35437\",\"end\":\"36552\"},{\"start\":\"37381\",\"end\":\"37392\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"37395\",\"end\":\"37413\",\"attributes\":{\"n\":\"4.1.\"}},{\"start\":\"39500\",\"end\":\"39506\"},{\"start\":\"41537\",\"end\":\"41565\",\"attributes\":{\"n\":\"4.2.\"}},{\"start\":\"46463\",\"end\":\"46487\",\"attributes\":{\"n\":\"4.3.\"}},{\"start\":\"49099\",\"end\":\"49109\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"49894\",\"end\":\"49904\"},{\"start\":\"50331\",\"end\":\"50341\"},{\"start\":\"50814\",\"end\":\"50824\"},{\"start\":\"50981\",\"end\":\"50991\"},{\"start\":\"51078\",\"end\":\"51088\"},{\"start\":\"51642\",\"end\":\"51651\"}]", "table": "[{\"start\":\"51319\",\"end\":\"51640\"},{\"start\":\"52042\",\"end\":\"52312\"}]", "figure_caption": "[{\"start\":\"49906\",\"end\":\"50329\"},{\"start\":\"50343\",\"end\":\"50812\"},{\"start\":\"50826\",\"end\":\"50979\"},{\"start\":\"50993\",\"end\":\"51076\"},{\"start\":\"51090\",\"end\":\"51263\"},{\"start\":\"51266\",\"end\":\"51319\"},{\"start\":\"51653\",\"end\":\"52042\"}]", "figure_ref": "[{\"start\":\"11915\",\"end\":\"11923\"},{\"start\":\"13168\",\"end\":\"13174\"},{\"start\":\"16168\",\"end\":\"16174\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"16540\",\"end\":\"16549\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"36720\",\"end\":\"36727\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"39226\",\"end\":\"39234\"},{\"start\":\"43798\",\"end\":\"43804\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"43939\",\"end\":\"43947\"},{\"start\":\"45102\",\"end\":\"45108\"},{\"start\":\"45620\",\"end\":\"45626\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"45901\",\"end\":\"45907\"},{\"start\":\"46192\",\"end\":\"46199\"},{\"start\":\"46451\",\"end\":\"46459\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"47899\",\"end\":\"47905\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"48817\",\"end\":\"48826\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"49089\",\"end\":\"49095\"}]", "bib_author_first_name": "[{\"start\":\"52369\",\"end\":\"52375\"},{\"start\":\"52383\",\"end\":\"52387\"},{\"start\":\"52396\",\"end\":\"52403\"},{\"start\":\"52410\",\"end\":\"52417\"},{\"start\":\"52424\",\"end\":\"52428\"},{\"start\":\"52436\",\"end\":\"52443\"},{\"start\":\"52450\",\"end\":\"52458\"},{\"start\":\"52466\",\"end\":\"52472\"},{\"start\":\"52483\",\"end\":\"52491\"},{\"start\":\"52500\",\"end\":\"52507\"},{\"start\":\"52836\",\"end\":\"52843\"},{\"start\":\"52856\",\"end\":\"52862\"},{\"start\":\"52867\",\"end\":\"52873\"},{\"start\":\"52879\",\"end\":\"52884\"},{\"start\":\"52891\",\"end\":\"52894\"},{\"start\":\"53134\",\"end\":\"53140\"},{\"start\":\"53154\",\"end\":\"53159\"},{\"start\":\"53174\",\"end\":\"53180\"},{\"start\":\"53189\",\"end\":\"53195\"},{\"start\":\"53493\",\"end\":\"53499\"},{\"start\":\"53505\",\"end\":\"53512\"},{\"start\":\"53517\",\"end\":\"53521\"},{\"start\":\"53663\",\"end\":\"53669\"},{\"start\":\"53678\",\"end\":\"53684\"},{\"start\":\"53698\",\"end\":\"53704\"},{\"start\":\"53980\",\"end\":\"53985\"},{\"start\":\"53993\",\"end\":\"54002\"},{\"start\":\"54012\",\"end\":\"54015\"},{\"start\":\"54208\",\"end\":\"54213\"},{\"start\":\"54221\",\"end\":\"54228\"},{\"start\":\"54237\",\"end\":\"54243\"},{\"start\":\"54412\",\"end\":\"54417\"},{\"start\":\"54425\",\"end\":\"54431\"},{\"start\":\"54440\",\"end\":\"54446\"},{\"start\":\"54623\",\"end\":\"54632\"},{\"start\":\"54642\",\"end\":\"54647\"},{\"start\":\"54657\",\"end\":\"54663\"},{\"start\":\"54866\",\"end\":\"54871\"},{\"start\":\"54889\",\"end\":\"54893\"},{\"start\":\"54909\",\"end\":\"54913\"},{\"start\":\"55230\",\"end\":\"55234\"},{\"start\":\"55241\",\"end\":\"55246\"},{\"start\":\"55254\",\"end\":\"55255\"},{\"start\":\"55256\",\"end\":\"55257\"},{\"start\":\"55260\",\"end\":\"55267\"},{\"start\":\"55278\",\"end\":\"55281\"},{\"start\":\"55454\",\"end\":\"55461\"},{\"start\":\"55470\",\"end\":\"55476\"},{\"start\":\"55483\",\"end\":\"55492\"},{\"start\":\"55502\",\"end\":\"55508\"},{\"start\":\"55802\",\"end\":\"55806\"},{\"start\":\"55817\",\"end\":\"55821\"},{\"start\":\"55831\",\"end\":\"55837\"},{\"start\":\"55847\",\"end\":\"55855\"},{\"start\":\"56118\",\"end\":\"56125\"},{\"start\":\"56134\",\"end\":\"56139\"},{\"start\":\"56151\",\"end\":\"56158\"},{\"start\":\"56159\",\"end\":\"56160\"},{\"start\":\"56329\",\"end\":\"56334\"},{\"start\":\"56348\",\"end\":\"56353\"},{\"start\":\"56367\",\"end\":\"56382\"},{\"start\":\"56391\",\"end\":\"56397\"},{\"start\":\"56410\",\"end\":\"56416\"},{\"start\":\"56827\",\"end\":\"56832\"},{\"start\":\"56844\",\"end\":\"56849\"},{\"start\":\"57025\",\"end\":\"57030\"},{\"start\":\"57038\",\"end\":\"57044\"},{\"start\":\"57053\",\"end\":\"57057\"},{\"start\":\"57068\",\"end\":\"57076\"},{\"start\":\"57291\",\"end\":\"57298\"},{\"start\":\"57303\",\"end\":\"57310\"},{\"start\":\"57318\",\"end\":\"57326\"},{\"start\":\"57332\",\"end\":\"57336\"},{\"start\":\"57516\",\"end\":\"57520\"},{\"start\":\"57528\",\"end\":\"57532\"},{\"start\":\"57542\",\"end\":\"57549\"},{\"start\":\"57557\",\"end\":\"57564\"},{\"start\":\"57766\",\"end\":\"57770\"},{\"start\":\"57780\",\"end\":\"57787\"},{\"start\":\"57796\",\"end\":\"57803\"},{\"start\":\"58012\",\"end\":\"58021\"},{\"start\":\"58028\",\"end\":\"58034\"},{\"start\":\"58042\",\"end\":\"58048\"},{\"start\":\"58258\",\"end\":\"58263\"},{\"start\":\"58271\",\"end\":\"58277\"},{\"start\":\"58534\",\"end\":\"58537\"},{\"start\":\"58545\",\"end\":\"58554\"},{\"start\":\"58817\",\"end\":\"58823\"},{\"start\":\"58828\",\"end\":\"58831\"},{\"start\":\"58838\",\"end\":\"58846\"},{\"start\":\"58853\",\"end\":\"58861\"},{\"start\":\"59101\",\"end\":\"59106\"},{\"start\":\"59112\",\"end\":\"59119\"},{\"start\":\"59126\",\"end\":\"59134\"},{\"start\":\"59140\",\"end\":\"59143\"},{\"start\":\"59144\",\"end\":\"59145\"},{\"start\":\"59444\",\"end\":\"59452\"},{\"start\":\"59460\",\"end\":\"59464\"},{\"start\":\"59470\",\"end\":\"59476\"},{\"start\":\"59486\",\"end\":\"59493\"},{\"start\":\"59503\",\"end\":\"59509\"},{\"start\":\"59519\",\"end\":\"59525\"},{\"start\":\"59539\",\"end\":\"59545\"},{\"start\":\"59863\",\"end\":\"59874\"},{\"start\":\"59880\",\"end\":\"59884\"},{\"start\":\"59893\",\"end\":\"59897\"},{\"start\":\"59907\",\"end\":\"59911\"},{\"start\":\"59920\",\"end\":\"59923\"},{\"start\":\"60161\",\"end\":\"60172\"},{\"start\":\"60178\",\"end\":\"60182\"},{\"start\":\"60191\",\"end\":\"60195\"},{\"start\":\"60205\",\"end\":\"60206\"},{\"start\":\"60213\",\"end\":\"60216\"},{\"start\":\"60217\",\"end\":\"60218\"},{\"start\":\"60458\",\"end\":\"60463\"},{\"start\":\"60473\",\"end\":\"60479\"},{\"start\":\"60485\",\"end\":\"60491\"},{\"start\":\"60759\",\"end\":\"60763\"},{\"start\":\"60775\",\"end\":\"60794\"},{\"start\":\"60804\",\"end\":\"60808\"},{\"start\":\"60809\",\"end\":\"60810\"},{\"start\":\"61067\",\"end\":\"61071\"},{\"start\":\"61077\",\"end\":\"61083\"},{\"start\":\"61086\",\"end\":\"61090\"},{\"start\":\"61091\",\"end\":\"61092\"},{\"start\":\"61384\",\"end\":\"61388\"},{\"start\":\"61401\",\"end\":\"61402\"},{\"start\":\"61408\",\"end\":\"61412\"},{\"start\":\"61428\",\"end\":\"61434\"},{\"start\":\"61731\",\"end\":\"61736\"},{\"start\":\"61745\",\"end\":\"61749\"},{\"start\":\"61762\",\"end\":\"61767\"},{\"start\":\"61963\",\"end\":\"61968\"},{\"start\":\"61978\",\"end\":\"61979\"},{\"start\":\"61988\",\"end\":\"62010\"},{\"start\":\"62211\",\"end\":\"62212\"},{\"start\":\"62219\",\"end\":\"62224\"},{\"start\":\"62238\",\"end\":\"62239\"},{\"start\":\"62246\",\"end\":\"62250\"},{\"start\":\"62480\",\"end\":\"62486\"},{\"start\":\"62495\",\"end\":\"62498\"},{\"start\":\"62503\",\"end\":\"62508\"},{\"start\":\"62517\",\"end\":\"62522\"},{\"start\":\"62530\",\"end\":\"62534\"},{\"start\":\"62735\",\"end\":\"62741\"},{\"start\":\"62754\",\"end\":\"62763\"},{\"start\":\"63050\",\"end\":\"63056\"},{\"start\":\"63069\",\"end\":\"63075\"},{\"start\":\"63303\",\"end\":\"63306\"},{\"start\":\"63312\",\"end\":\"63318\"},{\"start\":\"63324\",\"end\":\"63330\"},{\"start\":\"63337\",\"end\":\"63344\"},{\"start\":\"63352\",\"end\":\"63357\"},{\"start\":\"63574\",\"end\":\"63581\"},{\"start\":\"63590\",\"end\":\"63598\"},{\"start\":\"63608\",\"end\":\"63611\"},{\"start\":\"63619\",\"end\":\"63625\"},{\"start\":\"63851\",\"end\":\"63859\"},{\"start\":\"63866\",\"end\":\"63870\"},{\"start\":\"63871\",\"end\":\"63877\"},{\"start\":\"63891\",\"end\":\"63894\"},{\"start\":\"63900\",\"end\":\"63905\"},{\"start\":\"64161\",\"end\":\"64164\"},{\"start\":\"64171\",\"end\":\"64177\"},{\"start\":\"64188\",\"end\":\"64194\"},{\"start\":\"64441\",\"end\":\"64445\"},{\"start\":\"64452\",\"end\":\"64456\"},{\"start\":\"64457\",\"end\":\"64458\"},{\"start\":\"64466\",\"end\":\"64467\"},{\"start\":\"64475\",\"end\":\"64481\"},{\"start\":\"64740\",\"end\":\"64746\"},{\"start\":\"64755\",\"end\":\"64762\"},{\"start\":\"64770\",\"end\":\"64776\"},{\"start\":\"64789\",\"end\":\"64796\"},{\"start\":\"64805\",\"end\":\"64806\"},{\"start\":\"64813\",\"end\":\"64817\"},{\"start\":\"65126\",\"end\":\"65129\"},{\"start\":\"65134\",\"end\":\"65139\"},{\"start\":\"65147\",\"end\":\"65152\"},{\"start\":\"65161\",\"end\":\"65169\"},{\"start\":\"65176\",\"end\":\"65180\"},{\"start\":\"65454\",\"end\":\"65455\"},{\"start\":\"65460\",\"end\":\"65466\"},{\"start\":\"65474\",\"end\":\"65478\"},{\"start\":\"65486\",\"end\":\"65487\"},{\"start\":\"65781\",\"end\":\"65784\"},{\"start\":\"65789\",\"end\":\"65792\"},{\"start\":\"65799\",\"end\":\"65802\"},{\"start\":\"65809\",\"end\":\"65813\"},{\"start\":\"65819\",\"end\":\"65823\"},{\"start\":\"65830\",\"end\":\"65835\"},{\"start\":\"66122\",\"end\":\"66125\"},{\"start\":\"66132\",\"end\":\"66135\"},{\"start\":\"66142\",\"end\":\"66146\"},{\"start\":\"66157\",\"end\":\"66163\"},{\"start\":\"66435\",\"end\":\"66442\"},{\"start\":\"66448\",\"end\":\"66456\"},{\"start\":\"66709\",\"end\":\"66710\"},{\"start\":\"66717\",\"end\":\"66724\"},{\"start\":\"66971\",\"end\":\"66978\"},{\"start\":\"66985\",\"end\":\"66992\"},{\"start\":\"67000\",\"end\":\"67004\"},{\"start\":\"67014\",\"end\":\"67021\"}]", "bib_author_last_name": "[{\"start\":\"52376\",\"end\":\"52381\"},{\"start\":\"52388\",\"end\":\"52394\"},{\"start\":\"52404\",\"end\":\"52408\"},{\"start\":\"52418\",\"end\":\"52422\"},{\"start\":\"52429\",\"end\":\"52434\"},{\"start\":\"52444\",\"end\":\"52448\"},{\"start\":\"52459\",\"end\":\"52464\"},{\"start\":\"52473\",\"end\":\"52481\"},{\"start\":\"52492\",\"end\":\"52498\"},{\"start\":\"52508\",\"end\":\"52513\"},{\"start\":\"52844\",\"end\":\"52854\"},{\"start\":\"52863\",\"end\":\"52865\"},{\"start\":\"52874\",\"end\":\"52877\"},{\"start\":\"52885\",\"end\":\"52889\"},{\"start\":\"52895\",\"end\":\"52900\"},{\"start\":\"53141\",\"end\":\"53152\"},{\"start\":\"53160\",\"end\":\"53172\"},{\"start\":\"53181\",\"end\":\"53187\"},{\"start\":\"53196\",\"end\":\"53203\"},{\"start\":\"53500\",\"end\":\"53503\"},{\"start\":\"53513\",\"end\":\"53515\"},{\"start\":\"53522\",\"end\":\"53525\"},{\"start\":\"53670\",\"end\":\"53676\"},{\"start\":\"53685\",\"end\":\"53696\"},{\"start\":\"53705\",\"end\":\"53715\"},{\"start\":\"53986\",\"end\":\"53991\"},{\"start\":\"54003\",\"end\":\"54010\"},{\"start\":\"54016\",\"end\":\"54022\"},{\"start\":\"54214\",\"end\":\"54219\"},{\"start\":\"54229\",\"end\":\"54235\"},{\"start\":\"54244\",\"end\":\"54251\"},{\"start\":\"54418\",\"end\":\"54423\"},{\"start\":\"54432\",\"end\":\"54438\"},{\"start\":\"54447\",\"end\":\"54454\"},{\"start\":\"54633\",\"end\":\"54640\"},{\"start\":\"54648\",\"end\":\"54655\"},{\"start\":\"54664\",\"end\":\"54674\"},{\"start\":\"54872\",\"end\":\"54887\"},{\"start\":\"54894\",\"end\":\"54907\"},{\"start\":\"54914\",\"end\":\"54934\"},{\"start\":\"55235\",\"end\":\"55239\"},{\"start\":\"55247\",\"end\":\"55252\"},{\"start\":\"55268\",\"end\":\"55276\"},{\"start\":\"55282\",\"end\":\"55286\"},{\"start\":\"55462\",\"end\":\"55468\"},{\"start\":\"55477\",\"end\":\"55481\"},{\"start\":\"55493\",\"end\":\"55500\"},{\"start\":\"55509\",\"end\":\"55516\"},{\"start\":\"55807\",\"end\":\"55815\"},{\"start\":\"55822\",\"end\":\"55829\"},{\"start\":\"55838\",\"end\":\"55845\"},{\"start\":\"55856\",\"end\":\"55861\"},{\"start\":\"56126\",\"end\":\"56132\"},{\"start\":\"56140\",\"end\":\"56149\"},{\"start\":\"56161\",\"end\":\"56168\"},{\"start\":\"56335\",\"end\":\"56346\"},{\"start\":\"56354\",\"end\":\"56365\"},{\"start\":\"56383\",\"end\":\"56389\"},{\"start\":\"56398\",\"end\":\"56408\"},{\"start\":\"56417\",\"end\":\"56433\"},{\"start\":\"56833\",\"end\":\"56842\"},{\"start\":\"56850\",\"end\":\"56855\"},{\"start\":\"57031\",\"end\":\"57036\"},{\"start\":\"57045\",\"end\":\"57051\"},{\"start\":\"57058\",\"end\":\"57066\"},{\"start\":\"57077\",\"end\":\"57084\"},{\"start\":\"57299\",\"end\":\"57301\"},{\"start\":\"57311\",\"end\":\"57316\"},{\"start\":\"57327\",\"end\":\"57330\"},{\"start\":\"57337\",\"end\":\"57340\"},{\"start\":\"57521\",\"end\":\"57526\"},{\"start\":\"57533\",\"end\":\"57540\"},{\"start\":\"57550\",\"end\":\"57555\"},{\"start\":\"57565\",\"end\":\"57570\"},{\"start\":\"57771\",\"end\":\"57778\"},{\"start\":\"57788\",\"end\":\"57794\"},{\"start\":\"57804\",\"end\":\"57811\"},{\"start\":\"58022\",\"end\":\"58026\"},{\"start\":\"58035\",\"end\":\"58040\"},{\"start\":\"58049\",\"end\":\"58056\"},{\"start\":\"58264\",\"end\":\"58269\"},{\"start\":\"58278\",\"end\":\"58285\"},{\"start\":\"58538\",\"end\":\"58543\"},{\"start\":\"58555\",\"end\":\"58564\"},{\"start\":\"58824\",\"end\":\"58826\"},{\"start\":\"58832\",\"end\":\"58836\"},{\"start\":\"58847\",\"end\":\"58851\"},{\"start\":\"58862\",\"end\":\"58864\"},{\"start\":\"59107\",\"end\":\"59110\"},{\"start\":\"59120\",\"end\":\"59124\"},{\"start\":\"59135\",\"end\":\"59138\"},{\"start\":\"59146\",\"end\":\"59150\"},{\"start\":\"59453\",\"end\":\"59458\"},{\"start\":\"59465\",\"end\":\"59468\"},{\"start\":\"59477\",\"end\":\"59484\"},{\"start\":\"59494\",\"end\":\"59501\"},{\"start\":\"59510\",\"end\":\"59517\"},{\"start\":\"59526\",\"end\":\"59537\"},{\"start\":\"59546\",\"end\":\"59550\"},{\"start\":\"59875\",\"end\":\"59878\"},{\"start\":\"59885\",\"end\":\"59891\"},{\"start\":\"59898\",\"end\":\"59905\"},{\"start\":\"59912\",\"end\":\"59918\"},{\"start\":\"59924\",\"end\":\"59928\"},{\"start\":\"60173\",\"end\":\"60176\"},{\"start\":\"60183\",\"end\":\"60189\"},{\"start\":\"60196\",\"end\":\"60203\"},{\"start\":\"60207\",\"end\":\"60211\"},{\"start\":\"60219\",\"end\":\"60225\"},{\"start\":\"60227\",\"end\":\"60231\"},{\"start\":\"60464\",\"end\":\"60471\"},{\"start\":\"60480\",\"end\":\"60483\"},{\"start\":\"60492\",\"end\":\"60496\"},{\"start\":\"60764\",\"end\":\"60773\"},{\"start\":\"60795\",\"end\":\"60802\"},{\"start\":\"60811\",\"end\":\"60817\"},{\"start\":\"61072\",\"end\":\"61075\"},{\"start\":\"61093\",\"end\":\"61099\"},{\"start\":\"61389\",\"end\":\"61399\"},{\"start\":\"61403\",\"end\":\"61406\"},{\"start\":\"61413\",\"end\":\"61417\"},{\"start\":\"61419\",\"end\":\"61426\"},{\"start\":\"61435\",\"end\":\"61457\"},{\"start\":\"61459\",\"end\":\"61469\"},{\"start\":\"61737\",\"end\":\"61743\"},{\"start\":\"61750\",\"end\":\"61760\"},{\"start\":\"61768\",\"end\":\"61774\"},{\"start\":\"61969\",\"end\":\"61976\"},{\"start\":\"61980\",\"end\":\"61986\"},{\"start\":\"62011\",\"end\":\"62017\"},{\"start\":\"62213\",\"end\":\"62217\"},{\"start\":\"62225\",\"end\":\"62228\"},{\"start\":\"62230\",\"end\":\"62236\"},{\"start\":\"62240\",\"end\":\"62244\"},{\"start\":\"62251\",\"end\":\"62257\"},{\"start\":\"62259\",\"end\":\"62264\"},{\"start\":\"62487\",\"end\":\"62493\"},{\"start\":\"62499\",\"end\":\"62501\"},{\"start\":\"62509\",\"end\":\"62515\"},{\"start\":\"62523\",\"end\":\"62528\"},{\"start\":\"62535\",\"end\":\"62539\"},{\"start\":\"62742\",\"end\":\"62752\"},{\"start\":\"62764\",\"end\":\"62775\"},{\"start\":\"63057\",\"end\":\"63067\"},{\"start\":\"63076\",\"end\":\"63084\"},{\"start\":\"63307\",\"end\":\"63310\"},{\"start\":\"63319\",\"end\":\"63322\"},{\"start\":\"63331\",\"end\":\"63335\"},{\"start\":\"63345\",\"end\":\"63350\"},{\"start\":\"63358\",\"end\":\"63361\"},{\"start\":\"63582\",\"end\":\"63588\"},{\"start\":\"63599\",\"end\":\"63606\"},{\"start\":\"63612\",\"end\":\"63617\"},{\"start\":\"63626\",\"end\":\"63631\"},{\"start\":\"63860\",\"end\":\"63864\"},{\"start\":\"63878\",\"end\":\"63889\"},{\"start\":\"63895\",\"end\":\"63898\"},{\"start\":\"63906\",\"end\":\"63911\"},{\"start\":\"64165\",\"end\":\"64169\"},{\"start\":\"64178\",\"end\":\"64186\"},{\"start\":\"64195\",\"end\":\"64202\"},{\"start\":\"64446\",\"end\":\"64450\"},{\"start\":\"64459\",\"end\":\"64464\"},{\"start\":\"64468\",\"end\":\"64473\"},{\"start\":\"64482\",\"end\":\"64488\"},{\"start\":\"64490\",\"end\":\"64500\"},{\"start\":\"64747\",\"end\":\"64753\"},{\"start\":\"64763\",\"end\":\"64768\"},{\"start\":\"64777\",\"end\":\"64787\"},{\"start\":\"64797\",\"end\":\"64803\"},{\"start\":\"64807\",\"end\":\"64811\"},{\"start\":\"64818\",\"end\":\"64825\"},{\"start\":\"64827\",\"end\":\"64835\"},{\"start\":\"65130\",\"end\":\"65132\"},{\"start\":\"65140\",\"end\":\"65145\"},{\"start\":\"65153\",\"end\":\"65159\"},{\"start\":\"65170\",\"end\":\"65174\"},{\"start\":\"65181\",\"end\":\"65185\"},{\"start\":\"65456\",\"end\":\"65458\"},{\"start\":\"65467\",\"end\":\"65472\"},{\"start\":\"65479\",\"end\":\"65484\"},{\"start\":\"65488\",\"end\":\"65496\"},{\"start\":\"65498\",\"end\":\"65502\"},{\"start\":\"65785\",\"end\":\"65787\"},{\"start\":\"65793\",\"end\":\"65797\"},{\"start\":\"65803\",\"end\":\"65807\"},{\"start\":\"65814\",\"end\":\"65817\"},{\"start\":\"65824\",\"end\":\"65828\"},{\"start\":\"65836\",\"end\":\"65841\"},{\"start\":\"66126\",\"end\":\"66130\"},{\"start\":\"66136\",\"end\":\"66140\"},{\"start\":\"66147\",\"end\":\"66155\"},{\"start\":\"66164\",\"end\":\"66171\"},{\"start\":\"66443\",\"end\":\"66446\"},{\"start\":\"66457\",\"end\":\"66460\"},{\"start\":\"66711\",\"end\":\"66715\"},{\"start\":\"66725\",\"end\":\"66730\"},{\"start\":\"66732\",\"end\":\"66739\"},{\"start\":\"66979\",\"end\":\"66983\"},{\"start\":\"66993\",\"end\":\"66998\"},{\"start\":\"67005\",\"end\":\"67012\"},{\"start\":\"67022\",\"end\":\"67026\"}]", "bib_entry": "[{\"start\":\"52314\",\"end\":\"52777\",\"attributes\":{\"matched_paper_id\":\"6287870\",\"id\":\"b0\"}},{\"start\":\"52779\",\"end\":\"53051\",\"attributes\":{\"matched_paper_id\":\"4547677\",\"id\":\"b1\"}},{\"start\":\"53053\",\"end\":\"53421\",\"attributes\":{\"matched_paper_id\":\"38030033\",\"id\":\"b2\"}},{\"start\":\"53423\",\"end\":\"53659\",\"attributes\":{\"matched_paper_id\":\"8510667\",\"id\":\"b3\"}},{\"start\":\"53661\",\"end\":\"53903\",\"attributes\":{\"id\":\"b4\",\"doi\":\"arXiv:1707.07410\"}},{\"start\":\"53905\",\"end\":\"54182\",\"attributes\":{\"matched_paper_id\":\"2255738\",\"id\":\"b5\"}},{\"start\":\"54184\",\"end\":\"54365\",\"attributes\":{\"matched_paper_id\":\"3299195\",\"id\":\"b6\"}},{\"start\":\"54367\",\"end\":\"54572\",\"attributes\":{\"matched_paper_id\":\"14547347\",\"id\":\"b7\"}},{\"start\":\"54574\",\"end\":\"54808\",\"attributes\":{\"matched_paper_id\":\"206850490\",\"id\":\"b8\"}},{\"start\":\"54810\",\"end\":\"55153\",\"attributes\":{\"matched_paper_id\":\"17942780\",\"id\":\"b9\"}},{\"start\":\"55155\",\"end\":\"55452\",\"attributes\":{\"matched_paper_id\":\"299085\",\"id\":\"b10\"}},{\"start\":\"55454\",\"end\":\"55716\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"55718\",\"end\":\"56048\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"56050\",\"end\":\"56327\",\"attributes\":{\"matched_paper_id\":\"206596513\",\"id\":\"b13\"}},{\"start\":\"56329\",\"end\":\"56781\",\"attributes\":{\"id\":\"b14\",\"doi\":\"arXiv:1705.09479\"}},{\"start\":\"56783\",\"end\":\"56956\",\"attributes\":{\"matched_paper_id\":\"15168299\",\"id\":\"b15\"}},{\"start\":\"56958\",\"end\":\"57243\",\"attributes\":{\"matched_paper_id\":\"206850587\",\"id\":\"b16\"}},{\"start\":\"57245\",\"end\":\"57480\",\"attributes\":{\"matched_paper_id\":\"206594692\",\"id\":\"b17\"}},{\"start\":\"57482\",\"end\":\"57688\",\"attributes\":{\"matched_paper_id\":\"10586095\",\"id\":\"b18\"}},{\"start\":\"57690\",\"end\":\"57964\",\"attributes\":{\"matched_paper_id\":\"12888763\",\"id\":\"b19\"}},{\"start\":\"57966\",\"end\":\"58199\",\"attributes\":{\"matched_paper_id\":\"1708391\",\"id\":\"b20\"}},{\"start\":\"58201\",\"end\":\"58405\",\"attributes\":{\"matched_paper_id\":\"52956678\",\"id\":\"b21\"}},{\"start\":\"58407\",\"end\":\"58743\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"58745\",\"end\":\"59015\",\"attributes\":{\"matched_paper_id\":\"206853077\",\"id\":\"b23\"}},{\"start\":\"59017\",\"end\":\"59338\",\"attributes\":{\"matched_paper_id\":\"15774646\",\"id\":\"b24\"}},{\"start\":\"59340\",\"end\":\"59790\",\"attributes\":{\"matched_paper_id\":\"206594275\",\"id\":\"b25\"}},{\"start\":\"59792\",\"end\":\"60113\",\"attributes\":{\"matched_paper_id\":\"163588\",\"id\":\"b26\"}},{\"start\":\"60115\",\"end\":\"60376\",\"attributes\":{\"matched_paper_id\":\"9195577\",\"id\":\"b27\"}},{\"start\":\"60378\",\"end\":\"60699\",\"attributes\":{\"matched_paper_id\":\"19160323\",\"id\":\"b28\"}},{\"start\":\"60701\",\"end\":\"60985\",\"attributes\":{\"matched_paper_id\":\"206775100\",\"id\":\"b29\"}},{\"start\":\"60987\",\"end\":\"61304\",\"attributes\":{\"matched_paper_id\":\"206775640\",\"id\":\"b30\"}},{\"start\":\"61306\",\"end\":\"61680\",\"attributes\":{\"matched_paper_id\":\"16054868\",\"id\":\"b31\"}},{\"start\":\"61682\",\"end\":\"61906\",\"attributes\":{\"matched_paper_id\":\"10286462\",\"id\":\"b32\"}},{\"start\":\"61908\",\"end\":\"62165\",\"attributes\":{\"matched_paper_id\":\"3890025\",\"id\":\"b33\"}},{\"start\":\"62167\",\"end\":\"62402\",\"attributes\":{\"matched_paper_id\":\"4112630\",\"id\":\"b34\"}},{\"start\":\"62404\",\"end\":\"62716\",\"attributes\":{\"matched_paper_id\":\"51869233\",\"id\":\"b35\"}},{\"start\":\"62718\",\"end\":\"62859\",\"attributes\":{\"id\":\"b36\"}},{\"start\":\"62861\",\"end\":\"62960\",\"attributes\":{\"id\":\"b37\"}},{\"start\":\"62962\",\"end\":\"63254\",\"attributes\":{\"matched_paper_id\":\"13894940\",\"id\":\"b38\"}},{\"start\":\"63256\",\"end\":\"63500\",\"attributes\":{\"matched_paper_id\":\"2554592\",\"id\":\"b39\"}},{\"start\":\"63502\",\"end\":\"63790\",\"attributes\":{\"matched_paper_id\":\"206596482\",\"id\":\"b40\"}},{\"start\":\"63792\",\"end\":\"64084\",\"attributes\":{\"matched_paper_id\":\"21352010\",\"id\":\"b41\"}},{\"start\":\"64086\",\"end\":\"64365\",\"attributes\":{\"matched_paper_id\":\"8515741\",\"id\":\"b42\"}},{\"start\":\"64367\",\"end\":\"64675\",\"attributes\":{\"matched_paper_id\":\"207761262\",\"id\":\"b43\"}},{\"start\":\"64677\",\"end\":\"65036\",\"attributes\":{\"id\":\"b44\"}},{\"start\":\"65038\",\"end\":\"65362\",\"attributes\":{\"matched_paper_id\":\"4396708\",\"id\":\"b45\"}},{\"start\":\"65364\",\"end\":\"65691\",\"attributes\":{\"matched_paper_id\":\"3681302\",\"id\":\"b46\"}},{\"start\":\"65693\",\"end\":\"66019\",\"attributes\":{\"matched_paper_id\":\"4469249\",\"id\":\"b47\"}},{\"start\":\"66021\",\"end\":\"66357\",\"attributes\":{\"matched_paper_id\":\"49658377\",\"id\":\"b48\"}},{\"start\":\"66359\",\"end\":\"66608\",\"attributes\":{\"matched_paper_id\":\"3714620\",\"id\":\"b49\"}},{\"start\":\"66610\",\"end\":\"66911\",\"attributes\":{\"matched_paper_id\":\"35756379\",\"id\":\"b50\"}},{\"start\":\"66913\",\"end\":\"67185\",\"attributes\":{\"matched_paper_id\":\"11977588\",\"id\":\"b51\"}}]", "bib_title": "[{\"start\":\"52314\",\"end\":\"52367\"},{\"start\":\"52779\",\"end\":\"52834\"},{\"start\":\"53053\",\"end\":\"53132\"},{\"start\":\"53423\",\"end\":\"53491\"},{\"start\":\"53905\",\"end\":\"53978\"},{\"start\":\"54184\",\"end\":\"54206\"},{\"start\":\"54367\",\"end\":\"54410\"},{\"start\":\"54574\",\"end\":\"54621\"},{\"start\":\"54810\",\"end\":\"54864\"},{\"start\":\"55155\",\"end\":\"55228\"},{\"start\":\"56050\",\"end\":\"56116\"},{\"start\":\"56783\",\"end\":\"56825\"},{\"start\":\"56958\",\"end\":\"57023\"},{\"start\":\"57245\",\"end\":\"57289\"},{\"start\":\"57482\",\"end\":\"57514\"},{\"start\":\"57690\",\"end\":\"57764\"},{\"start\":\"57966\",\"end\":\"58010\"},{\"start\":\"58201\",\"end\":\"58256\"},{\"start\":\"58745\",\"end\":\"58815\"},{\"start\":\"59017\",\"end\":\"59099\"},{\"start\":\"59340\",\"end\":\"59442\"},{\"start\":\"59792\",\"end\":\"59861\"},{\"start\":\"60115\",\"end\":\"60159\"},{\"start\":\"60378\",\"end\":\"60456\"},{\"start\":\"60701\",\"end\":\"60757\"},{\"start\":\"60987\",\"end\":\"61065\"},{\"start\":\"61306\",\"end\":\"61382\"},{\"start\":\"61682\",\"end\":\"61729\"},{\"start\":\"61908\",\"end\":\"61961\"},{\"start\":\"62167\",\"end\":\"62209\"},{\"start\":\"62404\",\"end\":\"62478\"},{\"start\":\"62962\",\"end\":\"63048\"},{\"start\":\"63256\",\"end\":\"63301\"},{\"start\":\"63502\",\"end\":\"63572\"},{\"start\":\"63792\",\"end\":\"63849\"},{\"start\":\"64086\",\"end\":\"64159\"},{\"start\":\"64367\",\"end\":\"64439\"},{\"start\":\"65038\",\"end\":\"65124\"},{\"start\":\"65364\",\"end\":\"65452\"},{\"start\":\"65693\",\"end\":\"65779\"},{\"start\":\"66021\",\"end\":\"66120\"},{\"start\":\"66359\",\"end\":\"66433\"},{\"start\":\"66610\",\"end\":\"66707\"},{\"start\":\"66913\",\"end\":\"66969\"}]", "bib_author": "[{\"start\":\"52369\",\"end\":\"52383\"},{\"start\":\"52383\",\"end\":\"52396\"},{\"start\":\"52396\",\"end\":\"52410\"},{\"start\":\"52410\",\"end\":\"52424\"},{\"start\":\"52424\",\"end\":\"52436\"},{\"start\":\"52436\",\"end\":\"52450\"},{\"start\":\"52450\",\"end\":\"52466\"},{\"start\":\"52466\",\"end\":\"52483\"},{\"start\":\"52483\",\"end\":\"52500\"},{\"start\":\"52500\",\"end\":\"52515\"},{\"start\":\"52836\",\"end\":\"52856\"},{\"start\":\"52856\",\"end\":\"52867\"},{\"start\":\"52867\",\"end\":\"52879\"},{\"start\":\"52879\",\"end\":\"52891\"},{\"start\":\"52891\",\"end\":\"52902\"},{\"start\":\"53134\",\"end\":\"53154\"},{\"start\":\"53154\",\"end\":\"53174\"},{\"start\":\"53174\",\"end\":\"53189\"},{\"start\":\"53189\",\"end\":\"53205\"},{\"start\":\"53493\",\"end\":\"53505\"},{\"start\":\"53505\",\"end\":\"53517\"},{\"start\":\"53517\",\"end\":\"53527\"},{\"start\":\"53663\",\"end\":\"53678\"},{\"start\":\"53678\",\"end\":\"53698\"},{\"start\":\"53698\",\"end\":\"53717\"},{\"start\":\"53980\",\"end\":\"53993\"},{\"start\":\"53993\",\"end\":\"54012\"},{\"start\":\"54012\",\"end\":\"54024\"},{\"start\":\"54208\",\"end\":\"54221\"},{\"start\":\"54221\",\"end\":\"54237\"},{\"start\":\"54237\",\"end\":\"54253\"},{\"start\":\"54412\",\"end\":\"54425\"},{\"start\":\"54425\",\"end\":\"54440\"},{\"start\":\"54440\",\"end\":\"54456\"},{\"start\":\"54623\",\"end\":\"54642\"},{\"start\":\"54642\",\"end\":\"54657\"},{\"start\":\"54657\",\"end\":\"54676\"},{\"start\":\"54866\",\"end\":\"54889\"},{\"start\":\"54889\",\"end\":\"54909\"},{\"start\":\"54909\",\"end\":\"54936\"},{\"start\":\"55230\",\"end\":\"55241\"},{\"start\":\"55241\",\"end\":\"55254\"},{\"start\":\"55254\",\"end\":\"55260\"},{\"start\":\"55260\",\"end\":\"55278\"},{\"start\":\"55278\",\"end\":\"55288\"},{\"start\":\"55454\",\"end\":\"55470\"},{\"start\":\"55470\",\"end\":\"55483\"},{\"start\":\"55483\",\"end\":\"55502\"},{\"start\":\"55502\",\"end\":\"55518\"},{\"start\":\"55802\",\"end\":\"55817\"},{\"start\":\"55817\",\"end\":\"55831\"},{\"start\":\"55831\",\"end\":\"55847\"},{\"start\":\"55847\",\"end\":\"55863\"},{\"start\":\"56118\",\"end\":\"56134\"},{\"start\":\"56134\",\"end\":\"56151\"},{\"start\":\"56151\",\"end\":\"56170\"},{\"start\":\"56329\",\"end\":\"56348\"},{\"start\":\"56348\",\"end\":\"56367\"},{\"start\":\"56367\",\"end\":\"56391\"},{\"start\":\"56391\",\"end\":\"56410\"},{\"start\":\"56410\",\"end\":\"56435\"},{\"start\":\"56827\",\"end\":\"56844\"},{\"start\":\"56844\",\"end\":\"56857\"},{\"start\":\"57025\",\"end\":\"57038\"},{\"start\":\"57038\",\"end\":\"57053\"},{\"start\":\"57053\",\"end\":\"57068\"},{\"start\":\"57068\",\"end\":\"57086\"},{\"start\":\"57291\",\"end\":\"57303\"},{\"start\":\"57303\",\"end\":\"57318\"},{\"start\":\"57318\",\"end\":\"57332\"},{\"start\":\"57332\",\"end\":\"57342\"},{\"start\":\"57516\",\"end\":\"57528\"},{\"start\":\"57528\",\"end\":\"57542\"},{\"start\":\"57542\",\"end\":\"57557\"},{\"start\":\"57557\",\"end\":\"57572\"},{\"start\":\"57766\",\"end\":\"57780\"},{\"start\":\"57780\",\"end\":\"57796\"},{\"start\":\"57796\",\"end\":\"57813\"},{\"start\":\"58012\",\"end\":\"58028\"},{\"start\":\"58028\",\"end\":\"58042\"},{\"start\":\"58042\",\"end\":\"58058\"},{\"start\":\"58258\",\"end\":\"58271\"},{\"start\":\"58271\",\"end\":\"58287\"},{\"start\":\"58534\",\"end\":\"58545\"},{\"start\":\"58545\",\"end\":\"58566\"},{\"start\":\"58817\",\"end\":\"58828\"},{\"start\":\"58828\",\"end\":\"58838\"},{\"start\":\"58838\",\"end\":\"58853\"},{\"start\":\"58853\",\"end\":\"58866\"},{\"start\":\"59101\",\"end\":\"59112\"},{\"start\":\"59112\",\"end\":\"59126\"},{\"start\":\"59126\",\"end\":\"59140\"},{\"start\":\"59140\",\"end\":\"59152\"},{\"start\":\"59444\",\"end\":\"59460\"},{\"start\":\"59460\",\"end\":\"59470\"},{\"start\":\"59470\",\"end\":\"59486\"},{\"start\":\"59486\",\"end\":\"59503\"},{\"start\":\"59503\",\"end\":\"59519\"},{\"start\":\"59519\",\"end\":\"59539\"},{\"start\":\"59539\",\"end\":\"59552\"},{\"start\":\"59863\",\"end\":\"59880\"},{\"start\":\"59880\",\"end\":\"59893\"},{\"start\":\"59893\",\"end\":\"59907\"},{\"start\":\"59907\",\"end\":\"59920\"},{\"start\":\"59920\",\"end\":\"59930\"},{\"start\":\"60161\",\"end\":\"60178\"},{\"start\":\"60178\",\"end\":\"60191\"},{\"start\":\"60191\",\"end\":\"60205\"},{\"start\":\"60205\",\"end\":\"60213\"},{\"start\":\"60213\",\"end\":\"60227\"},{\"start\":\"60227\",\"end\":\"60233\"},{\"start\":\"60458\",\"end\":\"60473\"},{\"start\":\"60473\",\"end\":\"60485\"},{\"start\":\"60485\",\"end\":\"60498\"},{\"start\":\"60759\",\"end\":\"60775\"},{\"start\":\"60775\",\"end\":\"60804\"},{\"start\":\"60804\",\"end\":\"60819\"},{\"start\":\"61067\",\"end\":\"61077\"},{\"start\":\"61077\",\"end\":\"61086\"},{\"start\":\"61086\",\"end\":\"61101\"},{\"start\":\"61384\",\"end\":\"61401\"},{\"start\":\"61401\",\"end\":\"61408\"},{\"start\":\"61408\",\"end\":\"61419\"},{\"start\":\"61419\",\"end\":\"61428\"},{\"start\":\"61428\",\"end\":\"61459\"},{\"start\":\"61459\",\"end\":\"61471\"},{\"start\":\"61731\",\"end\":\"61745\"},{\"start\":\"61745\",\"end\":\"61762\"},{\"start\":\"61762\",\"end\":\"61776\"},{\"start\":\"61963\",\"end\":\"61978\"},{\"start\":\"61978\",\"end\":\"61988\"},{\"start\":\"61988\",\"end\":\"62019\"},{\"start\":\"62211\",\"end\":\"62219\"},{\"start\":\"62219\",\"end\":\"62230\"},{\"start\":\"62230\",\"end\":\"62238\"},{\"start\":\"62238\",\"end\":\"62246\"},{\"start\":\"62246\",\"end\":\"62259\"},{\"start\":\"62259\",\"end\":\"62266\"},{\"start\":\"62480\",\"end\":\"62495\"},{\"start\":\"62495\",\"end\":\"62503\"},{\"start\":\"62503\",\"end\":\"62517\"},{\"start\":\"62517\",\"end\":\"62530\"},{\"start\":\"62530\",\"end\":\"62541\"},{\"start\":\"62735\",\"end\":\"62754\"},{\"start\":\"62754\",\"end\":\"62777\"},{\"start\":\"63050\",\"end\":\"63069\"},{\"start\":\"63069\",\"end\":\"63086\"},{\"start\":\"63303\",\"end\":\"63312\"},{\"start\":\"63312\",\"end\":\"63324\"},{\"start\":\"63324\",\"end\":\"63337\"},{\"start\":\"63337\",\"end\":\"63352\"},{\"start\":\"63352\",\"end\":\"63363\"},{\"start\":\"63574\",\"end\":\"63590\"},{\"start\":\"63590\",\"end\":\"63608\"},{\"start\":\"63608\",\"end\":\"63619\"},{\"start\":\"63619\",\"end\":\"63633\"},{\"start\":\"63851\",\"end\":\"63866\"},{\"start\":\"63866\",\"end\":\"63891\"},{\"start\":\"63891\",\"end\":\"63900\"},{\"start\":\"63900\",\"end\":\"63913\"},{\"start\":\"64161\",\"end\":\"64171\"},{\"start\":\"64171\",\"end\":\"64188\"},{\"start\":\"64188\",\"end\":\"64204\"},{\"start\":\"64441\",\"end\":\"64452\"},{\"start\":\"64452\",\"end\":\"64466\"},{\"start\":\"64466\",\"end\":\"64475\"},{\"start\":\"64475\",\"end\":\"64490\"},{\"start\":\"64490\",\"end\":\"64502\"},{\"start\":\"64740\",\"end\":\"64755\"},{\"start\":\"64755\",\"end\":\"64770\"},{\"start\":\"64770\",\"end\":\"64789\"},{\"start\":\"64789\",\"end\":\"64805\"},{\"start\":\"64805\",\"end\":\"64813\"},{\"start\":\"64813\",\"end\":\"64827\"},{\"start\":\"64827\",\"end\":\"64837\"},{\"start\":\"65126\",\"end\":\"65134\"},{\"start\":\"65134\",\"end\":\"65147\"},{\"start\":\"65147\",\"end\":\"65161\"},{\"start\":\"65161\",\"end\":\"65176\"},{\"start\":\"65176\",\"end\":\"65187\"},{\"start\":\"65454\",\"end\":\"65460\"},{\"start\":\"65460\",\"end\":\"65474\"},{\"start\":\"65474\",\"end\":\"65486\"},{\"start\":\"65486\",\"end\":\"65498\"},{\"start\":\"65498\",\"end\":\"65504\"},{\"start\":\"65781\",\"end\":\"65789\"},{\"start\":\"65789\",\"end\":\"65799\"},{\"start\":\"65799\",\"end\":\"65809\"},{\"start\":\"65809\",\"end\":\"65819\"},{\"start\":\"65819\",\"end\":\"65830\"},{\"start\":\"65830\",\"end\":\"65843\"},{\"start\":\"66122\",\"end\":\"66132\"},{\"start\":\"66132\",\"end\":\"66142\"},{\"start\":\"66142\",\"end\":\"66157\"},{\"start\":\"66157\",\"end\":\"66173\"},{\"start\":\"66435\",\"end\":\"66448\"},{\"start\":\"66448\",\"end\":\"66462\"},{\"start\":\"66709\",\"end\":\"66717\"},{\"start\":\"66717\",\"end\":\"66732\"},{\"start\":\"66732\",\"end\":\"66741\"},{\"start\":\"66971\",\"end\":\"66985\"},{\"start\":\"66985\",\"end\":\"67000\"},{\"start\":\"67000\",\"end\":\"67014\"},{\"start\":\"67014\",\"end\":\"67028\"}]", "bib_venue": "[{\"start\":\"52515\",\"end\":\"52519\"},{\"start\":\"52902\",\"end\":\"52906\"},{\"start\":\"53205\",\"end\":\"53209\"},{\"start\":\"53527\",\"end\":\"53531\"},{\"start\":\"54024\",\"end\":\"54028\"},{\"start\":\"54253\",\"end\":\"54258\"},{\"start\":\"54456\",\"end\":\"54460\"},{\"start\":\"54676\",\"end\":\"54680\"},{\"start\":\"54936\",\"end\":\"54966\"},{\"start\":\"55288\",\"end\":\"55292\"},{\"start\":\"55518\",\"end\":\"55564\"},{\"start\":\"55718\",\"end\":\"55800\"},{\"start\":\"56170\",\"end\":\"56174\"},{\"start\":\"56451\",\"end\":\"56532\"},{\"start\":\"56857\",\"end\":\"56861\"},{\"start\":\"57086\",\"end\":\"57090\"},{\"start\":\"57342\",\"end\":\"57346\"},{\"start\":\"57572\",\"end\":\"57576\"},{\"start\":\"57813\",\"end\":\"57817\"},{\"start\":\"58058\",\"end\":\"58062\"},{\"start\":\"58287\",\"end\":\"58291\"},{\"start\":\"58407\",\"end\":\"58532\"},{\"start\":\"58866\",\"end\":\"58870\"},{\"start\":\"59152\",\"end\":\"59157\"},{\"start\":\"59552\",\"end\":\"59556\"},{\"start\":\"59930\",\"end\":\"59934\"},{\"start\":\"60233\",\"end\":\"60237\"},{\"start\":\"60498\",\"end\":\"60502\"},{\"start\":\"60819\",\"end\":\"60822\"},{\"start\":\"61101\",\"end\":\"61130\"},{\"start\":\"61471\",\"end\":\"61476\"},{\"start\":\"61776\",\"end\":\"61779\"},{\"start\":\"62019\",\"end\":\"62022\"},{\"start\":\"62266\",\"end\":\"62269\"},{\"start\":\"62541\",\"end\":\"62550\"},{\"start\":\"62718\",\"end\":\"62733\"},{\"start\":\"62863\",\"end\":\"62898\"},{\"start\":\"63086\",\"end\":\"63089\"},{\"start\":\"63363\",\"end\":\"63368\"},{\"start\":\"63633\",\"end\":\"63637\"},{\"start\":\"63913\",\"end\":\"63917\"},{\"start\":\"64204\",\"end\":\"64208\"},{\"start\":\"64502\",\"end\":\"64505\"},{\"start\":\"64677\",\"end\":\"64738\"},{\"start\":\"65187\",\"end\":\"65191\"},{\"start\":\"65504\",\"end\":\"65509\"},{\"start\":\"65843\",\"end\":\"65847\"},{\"start\":\"66173\",\"end\":\"66180\"},{\"start\":\"66462\",\"end\":\"66466\"},{\"start\":\"66741\",\"end\":\"66745\"},{\"start\":\"67028\",\"end\":\"67032\"},{\"start\":\"60504\",\"end\":\"60526\"}]"}}}, "year": 2023, "month": 12, "day": 17}